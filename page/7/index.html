
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/7/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.LG_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.LG_2023_08_14/">cs.LG - 2023-08-14 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift"><a href="#Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift" class="headerlink" title="Distance Matters For Improving Performance Estimation Under Covariate Shift"></a>Distance Matters For Improving Performance Estimation Under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07223">http://arxiv.org/abs/2308.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/melanibe/distance_matters_performance_estimation">https://github.com/melanibe/distance_matters_performance_estimation</a></li>
<li>paper_authors: Mélanie Roschewitz, Ben Glocker</li>
<li>for: 本研究旨在提高 covariate shift 下的性能估算，尤其是在敏感应用场景下。</li>
<li>methods: 该研究提出了一种基于 distance 的方法，通过检查测试样本与预期的训练分布之间的距离，以避免基于不可靠的模型输出来估算性能。</li>
<li>results: 研究在 13 个图像分类任务上进行了实验，并在各种自然和 sintetic 分布shift 下达到了 median 相对 MAE 改进率为 27%，并在 10 个任务中达到了最佳基eline。<details>
<summary>Abstract</summary>
Performance estimation under covariate shift is a crucial component of safe AI model deployment, especially for sensitive use-cases. Recently, several solutions were proposed to tackle this problem, most leveraging model predictions or softmax confidence to derive accuracy estimates. However, under dataset shifts, confidence scores may become ill-calibrated if samples are too far from the training distribution. In this work, we show that taking into account distances of test samples to their expected training distribution can significantly improve performance estimation under covariate shift. Precisely, we introduce a "distance-check" to flag samples that lie too far from the expected distribution, to avoid relying on their untrustworthy model outputs in the accuracy estimation step. We demonstrate the effectiveness of this method on 13 image classification tasks, across a wide-range of natural and synthetic distribution shifts and hundreds of models, with a median relative MAE improvement of 27% over the best baseline across all tasks, and SOTA performance on 10 out of 13 tasks. Our code is publicly available at https://github.com/melanibe/distance_matters_performance_estimation.
</details>
<details>
<summary>摘要</summary>
性能估计下 covariate shift 是安全 AI 模型部署中的一个关键组件，尤其是在敏感应用场景下。最近，一些解决方案被提出来解决这个问题，大多数都是基于模型预测或软max信任来 derive 准确性估计。然而，在数据集 shift 下，信任度分数可能会变得不准确，如果样本太far away from the training distribution。在这种情况下，我们表明可以通过考虑测试样本与预期的训练分布之间的距离来进行性能估计。我们引入了一种"距离检查"来检测测试样本是否位于预期的训练分布中，以避免基于不可靠的模型输出来进行准确性估计。我们在 13 个图像分类任务上进行了实验，包括自然和合成分布 shift，以及多达百个模型， median 相对误差改进率为 27%，并在所有任务上达到最佳基eline的表现。我们的代码可以在 <https://github.com/melanibe/distance_matters_performance_estimation> 上获取。
</details></li>
</ul>
<hr>
<h2 id="AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes"><a href="#AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes" class="headerlink" title="AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes"></a>AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07221">http://arxiv.org/abs/2308.07221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LZH-0225/AudioFormer">https://github.com/LZH-0225/AudioFormer</a></li>
<li>paper_authors: Zhaohui Li, Haitao Wang, Xinghua Jiang</li>
<li>for: 这篇论文是为了学习音频特征表示，通过自然语言理解（NLU）的新角度，并使用神经网络音码器模型生成抽象的音频代码，然后使用这些代码训练马斯克隐藏语言模型（MLM）来获得高质量的音频表示。</li>
<li>methods: 这篇论文使用了一种新的多 positivesample Contrastive（MPC）学习方法，它可以学习多个抽象的音频代码之间的共同表示，从而提高音频表示质量。具体来说，首先使用一个神经网络音码器模型生成抽象的音频代码，然后使用这些代码训练一个马斯克隐藏语言模型（MLM），最后使用MPC学习方法来学习多个抽象的音频代码之间的共同表示。</li>
<li>results: 根据实验结果，AudioFormer在多个数据集上达到了显著提高的性能，甚至超过了一些音视频多模态分类模型。具体来说，AudioFormer在AudioSet（2M,20K）、FSD50K等数据集上的性能分别为53.9、45.1和65.6。<details>
<summary>Abstract</summary>
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures collaborative representations among distinct positive samples. Our research outcomes demonstrate that AudioFormer attains significantly improved performance compared to prevailing monomodal audio classification models across multiple datasets,and even outperforms audio-visual multimodal classification models on select datasets. Specifically,our approach achieves remarkable results on datasets including AudioSet (2M,20K),and FSD50K,with performance scores of 53.9,45.1,and 65.6,respectively. We have openly shared both the code and models: https://github.com/LZH-0225/AudioFormer.git.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法 named AudioFormer，它通过获取逻辑音频编码并进一步练习其为音频分类任务进行学习。我们首先提出了一种新的视角，即视音频分类任务为自然语言理解（NLU）的一种形式。利用现有的神经网络音频编码器模型，我们生成了逻辑音频编码，并使用其训练一个封面语言模型（MLM），从而获得了高质量的音频特征表示。此外，我们还开拓了多个正样本对比（MPC）学习方法的应用。这种方法可以在同一个音频输入中学习多个独立的逻辑音频编码之间的共同表示。在我们的实验中，我们将逻辑音频编码视为文本数据，并使用cloze-like方法训练一个封面语言模型，最终获得了高质量的音频表示。尤其是，MPC学习技术可以有效捕捉多个正样本之间的协作表示。我们的研究结果显示，AudioFormer在多个数据集上达到了 significatively提高的性能，甚至超过了多模态音视频分类模型在一些数据集上。具体来说，我们的方法在AudioSet（2M,20K）、FSD50K等数据集上获得了53.9、45.1和65.6的性能分数。我们已经在 GitHub 上公开了代码和模型：https://github.com/LZH-0225/AudioFormer.git。
</details></li>
</ul>
<hr>
<h2 id="Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data"><a href="#Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data" class="headerlink" title="Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data"></a>Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07940">http://arxiv.org/abs/2308.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taizo Horikomi, Shouji Fujimoto, Atushi Ishikawa, Takayuki Mizuno</li>
<li>for: 本研究用于构建一个基于GPT-2语言模型的深度学习模型，用于生成受环境因素和个人特征 influencing的日常行走路径。</li>
<li>methods: 研究使用了地理坐标转换为特定的位置符号，并将每天的行走路径表示为一个序列符号。通过训练GPT-2架构，实现了从零开始训练一个深度学习模型，用于生成受环境因素和个人特征 influencing的日常行走路径。</li>
<li>results: 研究得出了一个基于GPT-2语言模型的深度学习模型，可以生成受环境因素和个人特征 influencing的日常行走路径。这种模型可以帮助我们更好地理解人们的日常活动行为，并且可以用于评估不同环境和个人特征对行走路径的影响。<details>
<summary>Abstract</summary>
Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
</details>
<details>
<summary>摘要</summary>
据米榊、藤本、石川等人的研究（Front. Phys. 2022），我们将地理坐标表示为纬度和经度转化为不同的空间尺度下的特征位置符号。我们将每个日常路径作为一个序列符号，通过将唯一的时间间隔符号添加到位置符号中来嵌入它。使用GPT-2架构的自然语言模型，我们从零开始训练这个序列符号，以构建一个可以逐步生成个人日常路径的深度学习模型。environmental factor such as weather conditions和个人属性such as gender and age通过特殊符号表示，我们通过在GPT-2架构上训练这些符号和路径，可以生成受环境因素和个人属性 influencing的 trajectory。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data"></a>Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07212">http://arxiv.org/abs/2308.07212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashidhar Reddy Javaji, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这个研究的目的是为了发展用于脑膜癌的诊断技术和治疗方法。</li>
<li>methods: 这个研究使用了深度学习技术，使用了Magnetic Resonance Imaging（MRI）模式，并导入了一种新的组合方法，包括ONet和修改过的UNet，以及新的损失函数。</li>
<li>results: 这个研究获得了2023年BraTS-PEDs挑战赛的精确分类模型。使用扩展资料，包括单独和composite变数，以确保模型的稳定性和准确性。组合策略，结合ONet和UNet模型，展现了更高的效率和精确性。 lesion_wise dice scores为0.52、0.72和0.78，证明了这种组合方法的优势。<details>
<summary>Abstract</summary>
Brain tumors remain a critical global health challenge, necessitating advancements in diagnostic techniques and treatment methodologies. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific features and modeling diverse aspects of the MRI images which result in lesion_wise dice scores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumor labels respectively. Visual comparisons further confirm the superiority of the ensemble method in accurate tumor region coverage. The results indicate that this advanced ensemble approach, building upon the unique strengths of individual models, offers promising prospects for enhanced diagnostic accuracy and effective treatment planning for brain tumors in pediatric brains.
</details>
<details>
<summary>摘要</summary>
脑肿仍然是全球医疗挑战，需要进一步的技术创新和治疗方法。为了应对儿童患者的年龄特定分 segmentation模型的增长需求，本研究利用深度学习技术和Magnetic Resonance Imaging（MRI）模式，探讨一种新的集成方法。通过引入ONet和修改版本的UNet模型，以及创新的损失函数，本研究实现了高精度的分割模型，为BraTS-PEDs 2023 Challenge提供了精准的分割结果。数据扩展，包括单个和复合变换，使模型具有不同扫描协议下的Robustness和准确性。集成策略，将ONet和UNet模型集成在一起，表现出更高的特征捕捉和多样化图像模型化能力，最终得到了lesion_wise dice分割率为0.52、0.72和0.78，用于涉及肿块、肿块核心和整个肿块等标签。视觉比较还证明了集成方法在精准肿块覆盖方面的优势。结果表明，这种高级集成方法，基于各个模型的特点优势，对儿童脑肿的诊断精度和有效的治疗规划具有替代性。
</details></li>
</ul>
<hr>
<h2 id="Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning"><a href="#Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning" class="headerlink" title="Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning"></a>Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07209">http://arxiv.org/abs/2308.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shipeng Bai, Jun Chen, Xintian Shen, Yixuan Qian, Yong Liu</li>
<li>for: 降低神经网络的执行时间和内存占用</li>
<li>methods: 同时施行减少和量化，不需要原始训练数据集</li>
<li>results: 在大规模图像分类任务上实现了显著的改善，比如在ImageNet dataset上与 State-of-the-Art 方法相比，使用30% 减少率和6位量化，ResNet-34 网络上获得20.54%的准确率提升。<details>
<summary>Abstract</summary>
Structured pruning and quantization are promising approaches for reducing the inference time and memory footprint of neural networks. However, most existing methods require the original training dataset to fine-tune the model. This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which performs pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a damaged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then derives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we formulate the reconstruction error between the original network and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and compression methods. For example, we achieve a 20.54% accuracy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization on ResNet-34.
</details>
<details>
<summary>摘要</summary>
《结构化截割和量化是神经网络减少推理时间和内存占用的有力方法。然而，大多数现有方法需要原始训练数据来细化模型，这不仅带来了重量级的资源占用，而且对于敏感或商业机密数据来说，由于隐私和安全问题，无法进行训练。因此，一些无数据方法被提出，但它们只是分别进行无数据截割和量化，没有利用截割和量化的衔接。在本文中，我们提出了一种名为统一无数据压缩（UDFC）的新框架，它在无数据情况下同时进行截割和量化。具体来说，UDFC从假设损坏（例如截割或量化）通道的部分信息可以通过其他通道的线性组合来保留，然后从假设中 derive 重建形式来恢复因压缩而产生的信息损失。最后，我们将重建错误 между 原始网络和压缩后的网络，并理论上解出closed-form解决方案。我们对大规模图像分类任务进行评估，并在不同的网络架构和压缩方法下获得了显著的改进。例如，我们在ImageNet数据集上达到了30%截割率和6位量化的SOTA方法比20.54%的精度提升。》
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-the-Training-of-Neural-Support-Vector-Machines"><a href="#Algorithms-for-the-Training-of-Neural-Support-Vector-Machines" class="headerlink" title="Algorithms for the Training of Neural Support Vector Machines"></a>Algorithms for the Training of Neural Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07204">http://arxiv.org/abs/2308.07204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Lars Simon, Manuel Radons</li>
<li>for: 本研究使用神经支持向量机（NSVM）结构，以汲取领域知识在模型设计中。</li>
<li>methods: 本文提出了一组基于 Pegasos 算法的 NSVM 训练算法，并通过解决一系列标准机器学习任务来证明其效果。</li>
<li>results: 本研究通过实验和分析，证明了 NSVM 在一些标准机器学习任务中的表现，并验证了领域知识的Integration在模型设计中的重要性。<details>
<summary>Abstract</summary>
Neural support vector machines (NSVMs) allow for the incorporation of domain knowledge in the design of the model architecture. In this article we introduce a set of training algorithms for NSVMs that leverage the Pegasos algorithm and provide a proof of concept by solving a set of standard machine learning tasks.
</details>
<details>
<summary>摘要</summary>
神经支持向量机器 (NSVM) 允许在模型建立的架构中包含领域知识。在这篇文章中，我们介绍了一组用 Pegasos 算法进行训练的 NSVM 训练算法，并通过解决一组标准机器学习任务来提供证明。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Neural-Categorical-Priors-for-Physics-Based-Character-Control"><a href="#Neural-Categorical-Priors-for-Physics-Based-Character-Control" class="headerlink" title="Neural Categorical Priors for Physics-Based Character Control"></a>Neural Categorical Priors for Physics-Based Character Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07200">http://arxiv.org/abs/2308.07200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tencent-RoboticsX/NCP">https://github.com/Tencent-RoboticsX/NCP</a></li>
<li>paper_authors: Qingxu Zhu, He Zhang, Mengting Lan, Lei Han</li>
<li>for: 这paper aimed to propose a new learning framework for controlling physics-based characters with naturalistic behaviors.</li>
<li>methods: The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips, and then uses a discrete information bottleneck and prior shifting to generate high-quality life-like behaviors.</li>
<li>results: The proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism, as demonstrated through comprehensive experiments using humanoid characters on two challenging downstream tasks.Here’s the Chinese version of the three key points:</li>
<li>for: 这paper的目标是提出一种新的学习框架，用于控制基于物理的角色表现出自然的行为。</li>
<li>methods: 提议的方法使用了反馈学习（RL）来跟踪和模仿生活中的自然运动，然后使用一种简化信息瓶颈和征识偏移来生成高质量的自然行为。</li>
<li>results: 提议的框架可以控制角色表现出较高质量的行为策略、多样性和真实性，这得到通过对人工智能角色进行了两个复杂的下游任务的实验证明。<details>
<summary>Abstract</summary>
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder's output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.
</details>
<details>
<summary>摘要</summary>
最近的研究发展强化 reuse motion prior 技术已经证明其能够生成自然的行为。在这篇论文中，我们提出一种新的学习框架，用于控制基于物理的人物，并且能够提高 Motion 质量和多样性，至今为止的现有方法。我们使用 reinforcement learning（RL）来初始化和模仿生命like 运动从未结构化运动clip 中提取有用信息，并使用 discrete information bottleneck，与 Vector Quantized Variational AutoEncoder（VQ-VAE）相同。这种结构压缩运动clip 中最重要的信息，并将其压缩成一个紧凑的、有用的幂论空间中。通过在训练过的 categorical prior distribution 中采样代码，可以生成高质量的生命like 行为。尽管这个 prior distribution 可以通过encoder的输出进行训练，但它遵循原始运动clip 的分布，这可能会导致行为偏斜。为了解决这个问题，我们提出了一种名为 prior shifting 的技术，通过 Curiosity-driven RL 来调整 prior distribution。结果显示，我们的方法可以提供足够的行为多样性，并且能够帮助上层策略学习以下渠道任务。我们在人iform 角色上进行了全面的实验，并使用剑盾战斗和两个玩家简易拳击游戏。我们的结果表明，我们的框架能够控制人iform 角色进行较高质量的运动，包括行为策略、多样性和真实性。视频、代码和数据可以在https://tencent-roboticsx.github.io/NCP/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Black-Box-Models-through-Counterfactuals"><a href="#Explaining-Black-Box-Models-through-Counterfactuals" class="headerlink" title="Explaining Black-Box Models through Counterfactuals"></a>Explaining Black-Box Models through Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07198">http://arxiv.org/abs/2308.07198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliatrustworthyai/counterfactualexplanations.jl">https://github.com/juliatrustworthyai/counterfactualexplanations.jl</a></li>
<li>paper_authors: Patrick Altmeyer, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 用于解释人工智能模型的输出</li>
<li>methods: 使用Counterfactual Explanations（CE）和Algorithmic Recourse（AR）生成解释和修复方法</li>
<li>results: 可以提供实用和现实的修复方法，帮助改善模型的输出结果<details>
<summary>Abstract</summary>
We present CounterfactualExplanations.jl: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.
</details>
<details>
<summary>摘要</summary>
我们介绍CounterfactualExplanations.jl：一个用于生成反对方案解释（CE）和算法补救（AR）的 julia 套件。CE 解释了模型对于特定预测所需的输入更改，这些解释可以提供AR：一组可行和有效的改善结果的建议。在这篇文章中，我们讨论了CE 在可解释人工智能中的用途，并详细介绍套件的功能。套件易于使用，设计为可自定义和扩展。我们将它作为 julia 中解释任何预测模型的首选之地。
</details></li>
</ul>
<hr>
<h2 id="gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling"><a href="#gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling" class="headerlink" title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling"></a>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07192">http://arxiv.org/abs/2308.07192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asash/gsasrec">https://github.com/asash/gsasrec</a></li>
<li>paper_authors: Aleksandr Petrov, Craig Macdonald</li>
<li>for: This paper aims to address the issue of overconfidence in recommendation models, specifically in the popular SASRec model, and to propose a novel loss function and improved model that can mitigate overconfidence and improve performance.</li>
<li>methods: The paper proposes a novel Generalised Binary Cross-Entropy Loss function (gBCE) and a modified version of SASRec called gSASRec, which deploys an increased number of negatives and the gBCE loss to mitigate overconfidence.</li>
<li>results: The paper shows through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem, and can outperform BERT4Rec in terms of NDCG score (e.g. +9.47% on the MovieLens-1M dataset) while requiring less training time (e.g. -73% training time on MovieLens-1M). Additionally, gSASRec is suitable for large datasets with more than 1 million items, unlike BERT4Rec.<details>
<summary>Abstract</summary>
A large catalogue size is one of the central challenges in training recommendation models: a large number of items makes them memory and computationally inefficient to compute scores for all items during training, forcing these models to deploy negative sampling. However, negative sampling increases the proportion of positive interactions in the training data, and therefore models trained with negative sampling tend to overestimate the probabilities of positive interactions a phenomenon we call overconfidence. While the absolute values of the predicted scores or probabilities are not important for the ranking of retrieved recommendations, overconfident models may fail to estimate nuanced differences in the top-ranked items, resulting in degraded performance. In this paper, we show that overconfidence explains why the popular SASRec model underperforms when compared to BERT4Rec. This is contrary to the BERT4Rec authors explanation that the difference in performance is due to the bi-directional attention mechanism. To mitigate overconfidence, we propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) and theoretically prove that it can mitigate overconfidence. We further propose the gSASRec model, an improvement over SASRec that deploys an increased number of negatives and the gBCE loss. We show through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem. As a result, gSASRec can outperform BERT4Rec (e.g. +9.47% NDCG on the MovieLens-1M dataset), while requiring less training time (e.g. -73% training time on MovieLens-1M). Moreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets that contain more than 1 million items.
</details>
<details>
<summary>摘要</summary>
庞大的目录大小是训练推荐模型的中心挑战之一：大量的项目使得计算分数的计算成本高昂，使得这些模型不能在训练过程中计算所有项目的分数，因此需要使用负样本。然而，使用负样本增加了正交互动的比例在训练数据中，因此模型受负样本训练后会过度估计正交互动，这种现象我们称为过自信。这会导致模型估计排名顺序中的差异不准确，从而导致性能下降。在这篇论文中，我们表明了SASRec模型在比较BERT4Rec时的下降性能是由于过自信而不是BI-directional attention机制的解释。为了 Mitigate overconfidence，我们提出了一种通用二进制十字积分损失函数（gBCE），并证明了它可以 Mitigate overconfidence。此外，我们还提出了一种改进SASRec模型的gSASRec模型，该模型通过增加负样本数和gBCE损失函数来减少过自信。我们通过对三个数据集进行详细的实验，证明了gSASRec模型不受过自信问题。因此，gSASRec可以在MovieLens-1M数据集上超过BERT4Rec（+9.47% NDCG），同时具有较少的训练时间(-73% 训练时间）。此外，gSASRec模型适用于大于100万个项目的大数据集。
</details></li>
</ul>
<hr>
<h2 id="Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity"><a href="#Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity" class="headerlink" title="Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity"></a>Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07359">http://arxiv.org/abs/2308.07359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Janosch Schneider, Marius Adler, Christoph Ammer-Herrmenau, Alexander Otto König, Ulrich Sax, Jonas Hügel</li>
<li>for: 这篇论文的目的是为了找到类似的病人，以便评估治疗结果和临床决策支持。</li>
<li>methods: 这篇论文使用了世界各地医疗病理分类（ICD）代码，将病人的病理特征转换为数据集，然后使用Semantic Similarity算法进行相似性计算。</li>
<li>results: 这篇论文的结果显示，使用了我们提出的标准化运算符 Lateral Epicritical Density 和 Bipartite Graph Matching 的 комbination，可以实现最高的相似性分析效果，与专家评价的真实相似性相符。<details>
<summary>Abstract</summary>
Finding similar patients is a common objective in precision medicine, facilitating treatment outcome assessment and clinical decision support. Choosing widely-available patient features and appropriate mathematical methods for similarity calculations is crucial. International Statistical Classification of Diseases and Related Health Problems (ICD) codes are used worldwide to encode diseases and are available for nearly all patients. Aggregated as sets consisting of primary and secondary diagnoses they can display a degree of comorbidity and reveal comorbidity patterns. It is possible to compute the similarity of patients based on their ICD codes by using semantic similarity algorithms. These algorithms have been traditionally evaluated using a single-term expert rated data set.   However, real-word patient data often display varying degrees of documented comorbidities that might impair algorithm performance. To account for this, we present a scale term that considers documented comorbidity-variance. In this work, we compared the performance of 80 combinations of established algorithms in terms of semantic similarity based on ICD-code sets. The sets have been extracted from patients with a C25.X (pancreatic cancer) primary diagnosis and provide a variety of different combinations of ICD-codes. Using our scale term we yielded the best results with a combination of level-based information content, Leacock & Chodorow concept similarity and bipartite graph matching for the set similarities reaching a correlation of 0.75 with our expert's ground truth. Our results highlight the importance of accounting for comorbidity variance while demonstrating how well current semantic similarity algorithms perform.
</details>
<details>
<summary>摘要</summary>
在精度医学中，找到类似的患者是一项常见的目标，以便评估治疗结果和临床决策支持。选择广泛可用的患者特征和适当的数学方法进行相似计算是关键。国际疾病分类法（ICD）代码在全球使用，可以为大多数患者提供代码。将这些代码聚合成集合，包括主要和次要诊断，可以显示患者的诊断程度和诊断模式。可以使用语义相似算法计算患者之间的相似性。这些算法通常通过专家评分的数据集来评估。但在实际患者数据中，患者通常有不同程度的记录的相关疾病，这可能会影响算法性能。为解决这个问题，我们提出了一个权重因素，该因素考虑了记录的相关疾病变化。在这种情况下，我们比较了80种已知算法的语义相似性，基于ICD代码集。这些代码集来自患有C25.X（肝癌）主诊断的患者，并提供了不同的ICD代码组合。使用我们的权重因素，我们得到了最佳的结果，与专家的真实ground truth相匹配，相似度为0.75。我们的结果表明了考虑相关疾病变化的重要性，同时也展示了当前语义相似算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks"><a href="#Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks" class="headerlink" title="Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks"></a>Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07358">http://arxiv.org/abs/2308.07358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahnobari/autosurf">https://github.com/ahnobari/autosurf</a></li>
<li>paper_authors: Amin Heyrani Nobari, Justin Rey, Suhas Kodali, Matthew Jones, Faez Ahmed</li>
<li>for: 这个论文的目的是自动生成CFD模型的网格，以提高计算流体力学的精度和效率。</li>
<li>methods: 这个论文使用图解树神经网络（GNN）和专家指导来自动生成CFD模型的网格。它还提出了一种新的3D分割算法，以及一种将预测从3D分割模型项目到CAD表面的方法。</li>
<li>results: 论文通过一个实际案例研究表明，自动生成的网格与专家生成的网格相比较，具有相似的质量，并且使得计算机程序能够正确地计算结果。此外，论文还比较了自动生成网格和适应重新分割的方法，发现自动生成网格比适应重新分割更快。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/ahnobari/AutoSurf%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ahnobari/AutoSurf上获取。</a><details>
<summary>Abstract</summary>
Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation models to CAD surfaces using the conformal predictions method, which provides marginal statistical guarantees and robust uncertainty quantification and handling. We demonstrate that the addition of conformal predictions effectively enables the model to avoid under-refinement, hence failure, in CFD meshing even for weak and less accurate models. Finally, we demonstrate the efficacy of our approach through a real-world case study that demonstrates that our automatically generated mesh is comparable in quality to expert-generated meshes and enables the solver to converge and produce accurate results. Furthermore, we compare our approach to the alternative of adaptive remeshing in the same case study and find that our method is 5 times faster in the overall process of simulation. The code and data for this project are made publicly available at https://github.com/ahnobari/AutoSurf.
</details>
<details>
<summary>摘要</summary>
计算流体动力学（CFD）在不同的工程领域都广泛应用，但是准确的计算受到域的适当精细化的约束。高精度的精细化可能确保准确性，但是来自高计算成本。同时，适应精细化技术需要多次 simulations和高计算成本。这意味着精细化过程取决于专家知识和年代经验。自动生成精细化可以节省很多时间和努力，并且可以加速设计过程。本文提出了基于机器学习的方案，利用图像神经网络（GNN）和专家指导来自动生成飞机模型的CFD精细化。在这个工作中，我们提出了一种新的3D分割算法，其在surface classification方面比PointNet++和PointMLP两种状态态模型更高效。我们还提出了一种将预测从3D分割模型 projet到CAD surface的方法，使用确ensional predictions方法，该方法提供了边缘统计保证和稳定的不确定性评估和处理。我们发现，通过添加确ensional predictions，我们的方法可以避免精细化失败，即下REFINE。最后，我们通过一个实际的案例研究证明了我们的自动生成精细化与专家生成精细化相比质量相同，并且使得计算器能够 converges和生成准确结果。此外，我们与适应精细化的相同案例进行比较，发现我们的方法比适应精细化5倍快。我们将项目的代码和数据公开发布在https://github.com/ahnobari/AutoSurf上。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements"><a href="#Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements" class="headerlink" title="Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements"></a>Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07175">http://arxiv.org/abs/2308.07175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang</li>
<li>for: 学习 $n$-qubit 量子状态，输出由具有最多 $t$ 单位逻辑门的电路。</li>
<li>methods: 使用单Copy测量来学习这类状态，而不需要双Copy测量。</li>
<li>results: 可以在 $\mathsf{poly}(n,2^t,1&#x2F;\epsilon)$ 时间和样本数量内学习这类状态，与之前所有的算法相同。<details>
<summary>Abstract</summary>
Recent work has shown that $n$-qubit quantum states output by circuits with at most $t$ single-qubit non-Clifford gates can be learned to trace distance $\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples. All prior algorithms achieving this runtime use entangled measurements across two copies of the input state. In this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements.
</details>
<details>
<summary>摘要</summary>
最近的工作表明，$n$-粒子量子状态由具有最多$t$个单元素非束地 gates生成的电路可以使用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和样本来跟踪距离$\epsilon$。所有先前的算法达到这个 runtime 都使用了两份输入状态的排合测试。在这种工作中，我们给出了同样的效率的算法，可以使用单份输入状态来学习同一类型的状态。Note:* " $n$-qubit quantum states" is translated as " $n$-粒子量子状态" (n-qubit quantum states)* "circuits with at most $t$ single-qubit non-Clifford gates" is translated as "具有最多$t$个单元素非束地 gates的电路" (circuits with at most t single-qubit non-Clifford gates)* "can be learned to trace distance $\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples" is translated as "可以使用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和样本来跟踪距离$\epsilon$" (can be learned to trace distance ε using polynomial time and samples)* "All prior algorithms achieving this runtime use entangled measurements across two copies of the input state" is translated as "所有先前的算法达到这个 runtime 都使用了两份输入状态的排合测试" (all previous algorithms achieving this runtime use entangled measurements across two copies of the input state)* "In this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements" is translated as "在这种工作中，我们给出了同样的效率的算法，可以使用单份输入状态来学习同一类型的状态" (in this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements)
</details></li>
</ul>
<hr>
<h2 id="PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation"><a href="#PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation" class="headerlink" title="PitchNet: A Fully Convolutional Neural Network for Pitch Estimation"></a>PitchNet: A Fully Convolutional Neural Network for Pitch Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07170">http://arxiv.org/abs/2308.07170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Cochoy</li>
<li>for: 这项研究旨在提高人声中的抑 pitch 检测精度，以便在音乐和语音处理领域中进行更加精准的抑 pitch EXTraction。</li>
<li>methods: 该研究提出了一种基于卷积神经网络的 “PitchNet”，用于从人声中提取抑 pitch。该网络结合自相关函数和深度学习技术，以便优化抑 pitch 检测的精度。</li>
<li>results: 对于 synthetic sounds、opera recordings 和 time-stretched vowels 等数据集的评估表明，PitchNet 能够准确地检测人声中的抑 pitch。这项研究为音乐和语音处理领域中的抑 pitch EXTraction 开创了新的可能性。<details>
<summary>Abstract</summary>
In the domain of music and sound processing, pitch extraction plays a pivotal role. This research introduces "PitchNet", a convolutional neural network tailored for pitch extraction from the human singing voice, including acapella performances. Integrating autocorrelation with deep learning techniques, PitchNet aims to optimize the accuracy of pitch detection. Evaluation across datasets comprising synthetic sounds, opera recordings, and time-stretched vowels demonstrates its efficacy. This work paves the way for enhanced pitch extraction in both music and voice settings.
</details>
<details>
<summary>摘要</summary>
在音乐和声音处理领域中，抽取高度扮演着关键性的角色。本研究介绍“PitchNet”，一种适用于人声歌唱中的声调抽取 convolutional neural network（CNN）。通过对深度学习技术与自相关函数的组合，PitchNet目标是提高声调检测精度。对于 synthetic sounds、opera recording 和时间压缩词汇等数据集进行评估，PitchNet 的效果得到证明。这项工作将为音乐和声音设置中的声调抽取带来进一步的改进。Note: "Simplified Chinese" refers to the standardized form of Chinese used in mainland China, which is different from Traditional Chinese used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models"><a href="#SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models" class="headerlink" title="SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models"></a>SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10997">http://arxiv.org/abs/2308.10997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadeep Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, Sanjiv Kumar</li>
<li>for: 提高文本生成图像模型的计算效率，使其能够更快地生成高质量的图像。</li>
<li>methods: 使用Markov Random Field（MRF）模型来加速 Muse 模型的推理过程，从而提高图像生成的计算效率。</li>
<li>results: 通过使用 MRF 模型，可以significantly reduce the required number of Muse prediction steps，并且在各个空间位置上编码图像元素之间的兼容性，以提高图像质量和计算效率。<details>
<summary>Abstract</summary>
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MRF inference as a differentiable neural-network layer. Our full model, SPEGTI, uses this proposed MRF model to speed up Muse by 1.5X with no loss in output image quality.
</details>
<details>
<summary>摘要</summary>
现代文本到图像生成模型可以生成高质量的图像，这些图像不仅具有摄影真实性，还能够准确地反映文本提示。然而，这种质量来自于费时的计算成本：大多数这些模型都是迭代的，需要多次运行推理，以确保不同区域的图像与文本提示保持一致。在这种情况下，我们提出了一种轻量级的方法，使用Markov随机场（MRF）模型来实现不同区域图像的兼容性。这种方法与最近提出的Muse模型结合使用，并且可以减少Muse预测步骤的数量，从而大幅降低计算成本。我们的全模型SPEGTI使用这种MRF模型，可以帮助Muse快速推理1.5倍，而无需 sacrifi额外的图像质量。
</details></li>
</ul>
<hr>
<h2 id="Pairing-interacting-protein-sequences-using-masked-language-modeling"><a href="#Pairing-interacting-protein-sequences-using-masked-language-modeling" class="headerlink" title="Pairing interacting protein sequences using masked language modeling"></a>Pairing interacting protein sequences using masked language modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07136">http://arxiv.org/abs/2308.07136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bitbol-lab/diffpalm">https://github.com/bitbol-lab/diffpalm</a></li>
<li>paper_authors: Umberto Lupo, Damiano Sgarbossa, Anne-Florence Bitbol</li>
<li>For: The paper aims to predict which proteins interact together from their amino acid sequences.* Methods: The paper uses protein language models trained on multiple sequence alignments, specifically MSA Transformer and the EvoFormer module of AlphaFold, to pair interacting protein sequences.* Results: The proposed method, DiffPALM, outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments and improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer.Here’s the same information in Simplified Chinese:* For: 文章目标是从蛋白质序列中预测哪些蛋白质进行互作。* Methods: 文章使用多个序列对 alignment 训练的蛋白质语言模型，特别是 MSA Transformer 和 AlphaFold 的 EvoFormer 模块，来对互作蛋白质序列进行对应。* Results: 提案的方法 DiffPALM 在难度较高的多个序列对上表现出优于现有的共演化基本方法，并在一些细胞蛋白质复合物的结构预测中达到竞争性表现。<details>
<summary>Abstract</summary>
Predicting which proteins interact together from amino-acid sequences is an important task. We develop a method to pair interacting protein sequences which leverages the power of protein language models trained on multiple sequence alignments, such as MSA Transformer and the EvoFormer module of AlphaFold. We formulate the problem of pairing interacting partners among the paralogs of two protein families in a differentiable way. We introduce a method called DiffPALM that solves it by exploiting the ability of MSA Transformer to fill in masked amino acids in multiple sequence alignments using the surrounding context. MSA Transformer encodes coevolution between functionally or structurally coupled amino acids. We show that it captures inter-chain coevolution, while it was trained on single-chain data, which means that it can be used out-of-distribution. Relying on MSA Transformer without fine-tuning, DiffPALM outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments extracted from ubiquitous prokaryotic protein datasets. It also outperforms an alternative method based on a state-of-the-art protein language model trained on single sequences. Paired alignments of interacting protein sequences are a crucial ingredient of supervised deep learning methods to predict the three-dimensional structure of protein complexes. DiffPALM substantially improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer, without significantly deteriorating any of those we tested. It also achieves competitive performance with using orthology-based pairing.
</details>
<details>
<summary>摘要</summary>
预测 protein sequences 中的互作对是一项重要任务。我们开发了一种方法，可以将 protein sequence 中的互作对级联起来，这种方法利用了多个序列对 alignment（如 MSA Transformer 和 EvoFormer 模块）训练的 protein language model。我们将这个问题转化为一个可导的问题，并提出了一种名为 DiffPALM 的方法来解决它。DiffPALM 利用了 MSA Transformer 可以填充遮盖的氨基酸，通过周围的上下文来填充它们。MSA Transformer 编码了功能或结构上的氨基酸之间的共演化，我们表明它可以在单链数据上训练，并在不需要微调的情况下在多链数据上进行预测。与现有的共演化基本方法相比，DiffPALM 在困难的多链对 alignments 上表现出色，同时也在不需要微调的情况下进行预测。此外，DiffPALM 还可以和一种基于 state-of-the-art 蛋白质语言模型进行比较，并且在一些欧化蛋白质复合物的结构预测中表现出色。Paired alignments of interacting protein sequences 是深度学习方法预测蛋白质复合物的重要组成部分。DiffPALM 在这些复合物的结构预测中提供了重要的改进。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-is-All-a-Graph-Needs"><a href="#Natural-Language-is-All-a-Graph-Needs" class="headerlink" title="Natural Language is All a Graph Needs"></a>Natural Language is All a Graph Needs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07134">http://arxiv.org/abs/2308.07134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang</li>
<li>for: 本研究旨在探讨 Whether large language models (LLMs) can replace graph neural networks (GNNs) as the foundation model for graphs.</li>
<li>methods: 我们提出了 InstructGLM (Instruction-finetuned Graph Language Model)，使用自然语言指令设计了高度可扩展的 prompt，并使用自然语言描述图像的几何结构和节点特征。</li>
<li>results: 我们的方法在 ogbn-arxiv、Cora 和 PubMed 数据集上超过了所有竞争 GNN 基elines，这说明了我们的方法的有效性，并且推照generative大型语言模型为图机器学习的基础模型。<details>
<summary>Abstract</summary>
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.
</details>
<details>
<summary>摘要</summary>
“大规模预训练语言模型，如ChatGPT，对人工智能多种研究领域产生了革命性的影响。基于Transformers的大语言模型（LLM）逐渐取代了CNNs和RNNs，统一了计算机视觉和自然语言处理领域。与独立存在的数据，如图像、视频或文本，相比，图表是一种包含丰富结构和关系信息的数据类型。同时，自然语言作为最表达力强的媒介，能够描述复杂结构。然而，将图学学习问题 integrate into the generative language modeling framework的现有工作很有限。随着大语言模型的重要性不断增长，我们需要探索 Whether LLMs可以取代GNNs作为图学基础模型。本文提出InstructGLM（基于natural language instruction的图语言模型），系统地设计了可扩展的提示，使用自然语言描述图表的结构和节点特征，并使用LLM进行图学学习和推理。我们的方法在ogbn-arxiv、Cora和PubMed数据集上都超过了所有的竞争GNN基elines，这 demonstates了我们的方法的有效性，并照亮了大语言模型作为图学基础模型的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS"><a href="#Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS" class="headerlink" title="Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)"></a>Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08561">http://arxiv.org/abs/2308.08561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yew Kee Wong, Yifan Zhou, Yan Shing Liang, Haichuan Qiu, Yu Xi Wu, Bin He</li>
<li>for: 这份研究目的是为了缩短药物开发过程的时间和成本，以及创新一种能够在三到六个月内完成整个R&amp;D过程，并且只需五十到八十千美元的成本。</li>
<li>methods: 这篇研究使用的方法包括机器学习分子生成（MLMG）和量子模拟（QS），两者共同实现了精确地预测药物的结构和功能。MLMG根据目标蛋白质的分子结构来生成可能的击中者，而QS则对这些分子进行筛选，以确定它们对目标蛋白质的反应和紧缩效果。</li>
<li>results: 这篇研究的结果显示，使用机器学习和量子模拟的融合方法可以快速生成高效的药物材料，并且可以实现对药物的评估和筛选。这些材料可以在几个月内完成整个R&amp;D过程，并且可以降低成本至五十到八十千美元。<details>
<summary>Abstract</summary>
The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction effectiveness and safety, creating a few dozen pre-clinical-trail-ready drugs. This paper is based on our first paper, where we pitched the concept of machine learning combined with quantum simulations. In this paper we will go over the detailed design and framework of QMLS, including MLMG, MLMV, and QS.
</details>
<details>
<summary>摘要</summary>
研发（R&D）阶段是药品开发的长途和昂贵的过程。为了革新这个过程，我们提出了新的概念——量子机器学学习（QMLS），可以缩短整个R&D阶段的时间至3-6个月，并降低成本至50-80万美元。在潜在药物生成（Hit Generation）阶段，机器学学习分子生成（MLMG）根据目标蛋白质的分子结构生成可能的潜在药物，而量子模拟（QS）则从初步试验中筛选出与目标蛋白质具有强烈反应和结合效果的分子。在药物优化阶段，得到的分子 variants 由机器学学习分子变化（MLMV）进行了数十个变化，而其他分子则只进行了几个变化。最后，所有优化后的分子都会经过多轮QS筛选，以确保它们具有高效性和安全性，从而生成数十个前期临床药物。本文是我们之前的第一篇论文的续写，我们在这篇文章中将详细介绍QMLS的设计和框架，包括MLMG、MLMV和QS。
</details></li>
</ul>
<hr>
<h2 id="A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns"><a href="#A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns" class="headerlink" title="A Time-aware tensor decomposition for tracking evolving patterns"></a>A Time-aware tensor decomposition for tracking evolving patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07126">http://arxiv.org/abs/2308.07126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Chatzis, Max Pfeffer, Pedro Lind, Evrim Acar</li>
<li>for: 本研究旨在提取时间序列数据中的慢慢发展模式，并且能够考虑时间序列中的变化。</li>
<li>methods: 本文提出了一种基于PARAFAC2的时间regularization方法，即 temporal PARAFAC2（tPARAFAC2），用于抽取时间序列数据中的慢慢发展模式。</li>
<li>results: 经过广泛的实验 validate that tPARAFAC2可以准确地捕捉时间序列数据中的慢慢发展模式，并且表现比PARAFAC2和时间平滑矩阵因子化regularization方法更好。<details>
<summary>Abstract</summary>
Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate("Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately, performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.") result:时间演化数据集经常可以被视为一个高阶张量，其中一个方向是时间方向。虽然tensor分解已经成功地用于捕捉高阶数据集中的下面模式，但是时间方面通常被忽略，允许时间点的重新排序。在最近的研究中，temporal regularizers被添加到时间方面以解决这个问题。然而，现有的方法仍然不允许下面模式在时间上发生变化（例如，大脑中的空间变化，话题中的上下文变化）。在这篇论文中，我们提议时间PARAFAC2（tPARAFAC2）：基于PARAFAC2的张量分解方法，带有时间正则化，以EXTRACT从时间数据中逐渐发展的模式。通过对 sintetic数据进行了广泛的实验，我们示出了tPARAFAC2可以准确地捕捉下面模式，并且perform better than PARAFAC2和 Coupled Matrix Factorization with temporal smoothness regularization。
</details></li>
</ul>
<hr>
<h2 id="Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers"><a href="#Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers" class="headerlink" title="Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers"></a>Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07121">http://arxiv.org/abs/2308.07121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Rauch, Raphael Schwinger, Moritz Wirth, Bernhard Sick, Sven Tomforde, Christoph Scholz</li>
<li>for: 本研究旨在推动鸟叫声监测领域的终端学习转移，通过结合自动学习（SSL）和深度活动学习（DAL），以便直接处理原始音频数据，并生成高质量鸟叫声表示。</li>
<li>methods: 本研究使用变换器模型，并通过自动学习生成高质量鸟叫声表示，以便加速环境变化评估和决策过程。此外，通过深度活动学习，减少人工标注数据的依赖，提高了鸟叫声识别任务的效果。</li>
<li>results: 本研究通过对不同变换器模型进行比较分析，评估它们在鸟叫声识别任务中的效果。同时，通过使用Huggingface Datasets，生成了一个完整的任务集，以便提高未来的比较性和可重现性。<details>
<summary>Abstract</summary>
We propose a shift towards end-to-end learning in bird sound monitoring by combining self-supervised (SSL) and deep active learning (DAL). Leveraging transformer models, we aim to bypass traditional spectrogram conversions, enabling direct raw audio processing. ActiveBird2Vec is set to generate high-quality bird sound representations through SSL, potentially accelerating the assessment of environmental changes and decision-making processes for wind farms. Additionally, we seek to utilize the wide variety of bird vocalizations through DAL, reducing the reliance on extensively labeled datasets by human experts. We plan to curate a comprehensive set of tasks through Huggingface Datasets, enhancing future comparability and reproducibility of bioacoustic research. A comparative analysis between various transformer models will be conducted to evaluate their proficiency in bird sound recognition tasks. We aim to accelerate the progression of avian bioacoustic research and contribute to more effective conservation strategies.
</details>
<details>
<summary>摘要</summary>
我们提议将学习方法转换为终端学习，通过结合自动生成监督（SSL）和深度活动学习（DAL），利用变换器模型，以直接处理原始音频数据，并不需要传统的spectrogram转换。我们通过ActiveBird2Vec生成高质量的鸟叫表示，通过SSL可能加速环境变化评估和风车决策过程，同时通过DAL减少人工标注数据的依赖，提高生物听音研究的可比性和可重复性。我们计划使用Huggingface集成数据，并进行不同变换器模型之间的比较分析，以评估它们在鸟叫识别任务中的效果。我们希望通过加速鸟类生物听音研究，为生态保护策略做出更有效的贡献。
</details></li>
</ul>
<hr>
<h2 id="Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases"><a href="#Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases" class="headerlink" title="Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases"></a>Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07118">http://arxiv.org/abs/2308.07118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maftej/iisnerf">https://github.com/maftej/iisnerf</a></li>
<li>paper_authors: Eugen Šlapak, Enric Pardo, Matúš Dopiriak, Taras Maksymyuk, Juraj Gazda</li>
<li>for: 这篇论文旨在探讨基于提供训练图像的神经辐射场（NeRF）在不同工业子领域的应用前景，以及未来研究方向。</li>
<li>methods: 本论文使用NeRF来学习3D场景表示，并提供了一系列证明NeRF在工业领域的应用可行性的实验。这些实验包括基于NeRF的视频压缩技术和使用NeRF进行3D运动估计以避免碰撞。</li>
<li>results: 在视频压缩实验中，我们获得了1920x1080和300x168分辨率下的压缩率为48%和74%。在3D动画中使用D-NeRF进行3D运动估计，得到了平均PSNR值为23 dB和SSIM值为0.97。<details>
<summary>Abstract</summary>
The proliferation of technologies, such as extended reality (XR), has increased the demand for high-quality three-dimensional (3D) graphical representations. Industrial 3D applications encompass computer-aided design (CAD), finite element analysis (FEA), scanning, and robotics. However, current methods employed for industrial 3D representations suffer from high implementation costs and reliance on manual human input for accurate 3D modeling. To address these challenges, neural radiance fields (NeRFs) have emerged as a promising approach for learning 3D scene representations based on provided training 2D images. Despite a growing interest in NeRFs, their potential applications in various industrial subdomains are still unexplored. In this paper, we deliver a comprehensive examination of NeRF industrial applications while also providing direction for future research endeavors. We also present a series of proof-of-concept experiments that demonstrate the potential of NeRFs in the industrial domain. These experiments include NeRF-based video compression techniques and using NeRFs for 3D motion estimation in the context of collision avoidance. In the video compression experiment, our results show compression savings up to 48\% and 74\% for resolutions of 1920x1080 and 300x168, respectively. The motion estimation experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF) and achieved an average peak signal-to-noise ratio (PSNR) of disparity map with the value of 23 dB and an structural similarity index measure (SSIM) 0.97.
</details>
<details>
<summary>摘要</summary>
技术的普及，如扩展现实（XR），提高了高品质三维图形表示的需求。工业三维应用包括计算机支持设计（CAD）、Finite Element分析（FEA）、扫描和机器人。然而，现有的工业三维表示方法受到高实施成本和人工输入的限制，以获得准确的三维模型。为解决这些挑战，神经辐射场（NeRF）已经出现为了学习基于提供训练图像的三维场景表示方法。尽管有关NeRF的兴趣在不断增长，但它们在不同的工业子领域的潜在应用仍然未得到了足够的探索。在这篇论文中，我们提供了工业应用场景中NeRF的全面检查，并提供未来研究方向的指导。我们还提供了一系列的证明性实验，以示NeRF在工业领域的潜在应用。这些实验包括基于NeRF的视频压缩技术和使用NeRF进行3D运动估计，以避免碰撞。在视频压缩实验中，我们的结果表明，对于分辨率为1920x1080和300x168的视频，可以实现压缩率为48%和74%。在3D动画中使用D-NeRF进行3D运动估计实验，我们获得了平均的干扰比率（PSNR）为23 dB和结构相似度指标（SSIM）为0.97。
</details></li>
</ul>
<hr>
<h2 id="iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN"><a href="#iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN" class="headerlink" title="iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN"></a>iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07117">http://arxiv.org/abs/2308.07117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Shogo Seki</li>
<li>for: 快速、轻量级、高精度的语音合成</li>
<li>methods: 使用快速和轻量级的1D CNN作为基础网络，并将一些神经网络替换为iSTFT，以提高速度和轻量化。</li>
<li>results: iSTFTNet2比iSTFTNet更快速和轻量级，且音质相对保持不变。可以在<a target="_blank" rel="noopener" href="https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/%E4%B8%AD%E4%B8%8B%E8%BD%BD%E9%9F%B3%E9%A2%91%E6%A0%B7%E6%9C%AC%E3%80%82">https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/中下载音频样本。</a><details>
<summary>Abstract</summary>
The inverse short-time Fourier transform network (iSTFTNet) has garnered attention owing to its fast, lightweight, and high-fidelity speech synthesis. It obtains these characteristics using a fast and lightweight 1D CNN as the backbone and replacing some neural processes with iSTFT. Owing to the difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency dimension is reduced via temporal upsampling. However, this strategy compromises the potential to enhance the speed. Therefore, we propose iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and 2D CNNs to model temporal and spectrogram structures, respectively. We designed a 2D CNN that performs frequency upsampling after conversion in a few-frequency space. This design facilitates the modeling of high-dimensional spectrograms without compromising the speed. The results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（iSTFTNet）在最近引起了关注，因为它具有快速、轻量级和高精度的语音生成特点。它使用快速和轻量级的1D CNN作为基础模型，并将一些神经网络过程替换为iSTFT。由于1D CNN在模型高维спект罗格的问题上难以处理，因此在时间增サンプリング的策略可能会增加速度的约束。为了解决这个问题，我们提出了iSTFTNet2，它是iSTFTNet的改进版本，使用1D-2D CNN来模型时间和спект罗格结构。我们设计了一个2D CNN，它在几个频率空间中进行频率增サンプリング。这种设计允许模型高维спект罗格无需增加速度约束。结果表明，iSTFTNet2使得iSTFTNet更快速和轻量级，同时保持语音质量的同等性。有关audio samples的详细信息请参考https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.
</details></li>
</ul>
<hr>
<h2 id="Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting"><a href="#Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting" class="headerlink" title="Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting"></a>Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07939">http://arxiv.org/abs/2308.07939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo</li>
<li>for: 这个论文是为了解决深度学习模型在动态和复杂环境中学习效率差的问题而写的。</li>
<li>methods: 这个论文提出了一种基于架构的 kontinuous learning 方法，称为 Ada-QPacknet，它通过提取每个任务的子网络来实现。这种方法的关键特点是它的容量，它使用高效的线性和非线性归一化方法来减少模型的大小。</li>
<li>results: 在Well-known CL 场景中，hybrid 8和4位量化实现了浮点子网络的相似准确性。这个方法比大多数 CL 策略在任务和类增量enario中表现出色，并且在Well-known episode combinations 中测试了这个算法，与最流行的 CL 策略进行了比较。<details>
<summary>Abstract</summary>
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combinations and compared with most popular algorithms. Results show that proposed approach outperforms most of the CL strategies in task and class incremental scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach"><a href="#Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach" class="headerlink" title="Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach"></a>Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07356">http://arxiv.org/abs/2308.07356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokul Manoj, Sandeep Singh Sengar, Jac Fredo Agastinose Ronickom</li>
<li>for: 这个研究的目的是为了比较不同年龄组的自闭症诊断使用形态特征（MF）和形态连接特征（MCF）的效果。</li>
<li>methods: 这个研究使用了两个公共可用的数据库—ABIDE-I和ABIDE-II—获得了Structural Magnetic Resonance Imaging（sMRI）数据，并将数据 pré-processed using a standard pipeline，然后将数据分割成根据Destrieux atlas的148个区域，EXTRACTED área、厚度、体积和平均弯曲信息，并使用了统计t检测（p&lt;0.05）来标识特征，然后使用Random Forest（RF）分类器进行训练。</li>
<li>results: 研究结果表明，6岁到11岁的年龄组的性能最高，其次是6岁到18岁和11岁到18岁的年龄组，在MF和MCF中都有高的表现。总的来说，MCF与RF在6岁到11岁的年龄组中表现最好，其准确率、F1 score、回归率和准确率分别为75.8%、83.1%、86%和80.4%。<details>
<summary>Abstract</summary>
Purpose: Age biases have been identified as an essential factor in the diagnosis of ASD. The objective of this study was to compare the effect of different age groups in classifying ASD using morphological features (MF) and morphological connectivity features (MCF). Methods: The structural magnetic resonance imaging (sMRI) data for the study was obtained from the two publicly available databases, ABIDE-I and ABIDE-II. We considered three age groups, 6 to 11, 11 to 18, and 6 to 18, for our analysis. The sMRI data was pre-processed using a standard pipeline and was then parcellated into 148 different regions according to the Destrieux atlas. The area, thickness, volume, and mean curvature information was then extracted for each region which was used to create a total of 592 MF and 10,878 MCF for each subject. Significant features were identified using a statistical t-test (p<0.05) which was then used to train a random forest (RF) classifier. Results: The results of our study suggested that the performance of the 6 to 11 age group was the highest, followed by the 6 to 18 and 11 to 18 ages in both MF and MCF. Overall, the MCF with RF in the 6 to 11 age group performed better in the classification than the other groups and produced an accuracy, F1 score, recall, and precision of 75.8%, 83.1%, 86%, and 80.4%, respectively. Conclusion: Our study thus demonstrates that morphological connectivity and age-related diagnostic model could be an effective approach to discriminating ASD.
</details>
<details>
<summary>摘要</summary>
目的：识别自闭症（ASD）的年龄因素已被证明是关键因素。本研究的目的是比较不同年龄组的分类ASD使用形态特征（MF）和形态连接特征（MCF）的效果。方法：我们从公共数据库ABIDE-I和ABIDE-II中获得了structural magnetic resonance imaging（sMRI）数据。我们分为三个年龄组：6-11岁、11-18岁和6-18岁进行分析。经过标准化处理后，sMRI数据被分割成根据Desitrieux大脑 Atlase所分的148个区域。然后，每个区域中的面积、厚度、体积和平均弯曲信息被提取，并用于创建共计592个MF和10878个MCF。通过统计t检测（p<0.05）进行了特征选择，并用于训练随机森林（RF）分类器。结果：我们的研究结果表明，6-11岁年龄组的性能最高，然后是6-18岁和11-18岁年龄组，在MF和MCF中都是如此。总的来说，在6-11岁年龄组中，MCF与RF的结合使得分类性能更高，其中的准确率、F1分数、回归率和精度分别为75.8%、83.1%、86%和80.4%。结论：这些结果表明，使用形态连接和年龄相关的诊断模型可以有效地识别ASD。
</details></li>
</ul>
<hr>
<h2 id="InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models"><a href="#InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models" class="headerlink" title="#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"></a>#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07074">http://arxiv.org/abs/2308.07074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofa-sys/instag">https://github.com/ofa-sys/instag</a></li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou</li>
<li>for: 这篇论文的目的是提高基础模型的 instruction-following 能力，并通过训练细化（SFT）来实现这一目标。</li>
<li>methods: 该论文使用了一种名为 InsTag 的开源细化标注工具，用于标注 SFT 数据集中的样本，并定义了 instrucion 多样性和复杂性的量化分析。</li>
<li>results: 研究发现，通过使用 InsTag 选择的6000个多样性和复杂性的样本，可以提高基础模型的表现，并且与训练数据量相比，TagLM 模型的表现更高。<details>
<summary>Abstract</summary>
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.
</details>
<details>
<summary>摘要</summary>
基础语言模型通过监督微调（SFT）获得指令遵从能力。多样性和复杂性被视为成功SFT数据集的关键因素，但其定义还未得到明确的量化分析。本工作提出InsTag，一种开放集标记器，用于在SFT数据集中标记样本基于 semantics和意图，并定义指令多样性和复杂性。我们获得了6.6K个标签来描述全面的用户查询。然后我们分析了流行的开源SFT数据集，发现模型能力随着数据集的多样性和复杂性增加。基于这个观察，我们提出了基于InsTag的数据选择器，选择6K个多样性和复杂性最高的样本从开源数据集进行练习。经过微调，我们获得了TagLM模型，其性能在MT-Bench评估中较开源模型高，证明了查询多样性和复杂性的重要性。我们在https://github.com/OFA-Sys/InsTag上开源了InsTag。
</details></li>
</ul>
<hr>
<h2 id="Machine-Unlearning-Solutions-and-Challenges"><a href="#Machine-Unlearning-Solutions-and-Challenges" class="headerlink" title="Machine Unlearning: Solutions and Challenges"></a>Machine Unlearning: Solutions and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07061">http://arxiv.org/abs/2308.07061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Xu, Zihan Wu, Cong Wang, Xiaohua Jia</li>
<li>for: 本研究旨在提供一份系统性的机器学习忘记研究分类和分析，以便为 Selective Data Removal（SDR）技术的发展提供指导。</li>
<li>methods: 本研究分类了现有的机器学习忘记研究，包括精确忘记和近似忘记两种方法。精确忘记方法可以完全除去训练数据的影响，而近似忘记方法可以有效地减少影响。</li>
<li>results: 本研究对现有的机器学习忘记方法进行了 kritische 分析，并提出了未来研究的方向。通过这种分析，研究人员可以更好地了解机器学习忘记技术的优缺点，并为实际应用提供指导。<details>
<summary>Abstract</summary>
Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy violations, security breaches, and performance deterioration. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of machine unlearning research. We categorize existing research into exact unlearning that algorithmically removes data influence entirely and approximate unlearning that efficiently minimizes influence through limited parameter updates. By reviewing the state-of-the-art solutions, we critically discuss their advantages and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning. This paper provides researchers with a roadmap of open problems, encouraging impactful contributions to address real-world needs for selective data removal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习模型可能偶发性记忆敏感、未授权或黑客数据，导致隐私违反、安全泄露和性能下降。为解决这些问题，机器忘记技术已经成为一种重要的解决方案，可以选择性地删除训练模型中具有特定影响的数据点。本文提供了机器忘记的全面分类和分析，并评估了现有的研究。我们将现有的研究分为单精度忘记和近似忘记两种，并评估了它们的优点和限制。此外，我们还提出了未来的方向，以推进机器忘记的发展，并将其视为可靠和适应式机器学习的重要能力。本文为研究人员提供了一个开启问题的路线图，促进了影响性的贡献，以解决实际需求中的选择性数据移除。
</details></li>
</ul>
<hr>
<h2 id="Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review"><a href="#Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review" class="headerlink" title="Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review"></a>Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07052">http://arxiv.org/abs/2308.07052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishabh Tiwari, Jatin Moolchandani, Shamla Mantri</li>
<li>for: 这篇论文主要针对scalp病的诊断和分类。</li>
<li>methods: 该论文使用了深度学习技术，包括Convolutional Neural Networks（CNN）和 Fully Connected Networks（FCN），以及一个APP，以帮助诊断scalp病。</li>
<li>results: 该论文的实验结果表明，使用深度学习模型可以高精度地诊断scalp病，其中一个方法的准确率为97.41%-99.09%，另一个方法的准确率为82.9%，而使用机器学习算法也可以高精度地诊断健康的scalp和脱发病。<details>
<summary>Abstract</summary>
The morbidity of scalp diseases is minuscule compared to other diseases, but the impact on the patient's life is enormous. It is common for people to experience scalp problems that include Dandruff, Psoriasis, Tinea-Capitis, Alopecia and Atopic-Dermatitis. In accordance with WHO research, approximately 70% of adults have problems with their scalp. It has been demonstrated in descriptive research that hair quality is impaired by impaired scalp, but these impacts are reversible with early diagnosis and treatment. Deep Learning advances have demonstrated the effectiveness of CNN paired with FCN in diagnosing scalp and skin disorders. In one proposed Deep-Learning-based scalp inspection and diagnosis system, an imaging microscope and a trained model are combined with an app that classifies scalp disorders accurately with an average precision of 97.41%- 99.09%. Another research dealt with classifying the Psoriasis using the CNN with an accuracy of 82.9%. As part of another study, an ML based algorithm was also employed. It accurately classified the healthy scalp and alopecia areata with 91.4% and 88.9% accuracy with SVM and KNN algorithms. Using deep learning models to diagnose scalp related diseases has improved due to advancements i computation capabilities and computer vision, but there remains a wide horizon for further improvements.
</details>
<details>
<summary>摘要</summary>
scalp 疾病的恶性相对其他疾病较少，但对病人的生活影响巨大。人们常经历披裤屑、 Psoriasis、Tinea-Capitis、Alopecia 和 Atopic-Dermatitis 等 scalp 问题。根据Who研究，约70%的成年人有 scalp 问题。研究表明，损害的毛发质量是由于损害 scalp 引起的，但这些影响可以通过早期诊断和治疗而reverse。深度学习技术的进步使得用 Deep Learning 模型进行 scalp 检查和诊断系统的精度提高了。在一个提议的 Deep-Learning-based scalp 检查和诊断系统中，一个图像镜和一个训练模型被与一个APP结合，可以准确地分类 scalp 疾病，其精度为97.41%-99.09%。另一项研究则是使用 CNN 分类 Psoriasis，其精度为82.9%。在另一项研究中，一种 ML 基本的算法也被应用，它可以准确地分类健康的 scalp 和 Alopecia areata，其精度为91.4% 和 88.9%。使用深度学习模型进行 scalp 相关疾病的诊断，由于计算机能力和计算机视觉的进步，已经得到了进一步改进的空间，但还有很大的可能性空间。
</details></li>
</ul>
<hr>
<h2 id="Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems"><a href="#Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems" class="headerlink" title="Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems"></a>Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07051">http://arxiv.org/abs/2308.07051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bilal Thonnam Thodi, Sai Venkata Ramana Ambadipudi, Saif Eddin Jabari</li>
<li>for: 这个论文是用来研究深度学习方法在交通流动中的应用，特别是用于解决非线性半导体方程的问题。</li>
<li>methods: 这个论文使用了一种名为 нейрон运算器框架，它可以将不同和稀疏的交通数据映射到完整的交通状况中。在训练中，使用了一种名为 физи学信息冲激（π）-FNO的算子，它在训练中添加了一个物理损失函数，以便在训练中提高冲击预测。</li>
<li>results: 通过使用LWR交通流模型， authors发现了在预测环形路网和城市信号灯道路上的density dynamics的高精度预测。此外，他们发现了一个可以使用简单的交通密度动态，例如由2-3个汽车队列和1-2个交通信号ecycle组成的数据，并且可以预测具有不同汽车队列分布和多个交通信号cycle（大于2）的密度动态，并且误差在可接受范围内。在适当的模型架构和训练数据下，插值误差呈线性增长。添加物理正则化可以帮助学习长期交通密度动态，特别是在 periodic boundary data 上。<details>
<summary>Abstract</summary>
Deep learning methods are emerging as popular computational tools for solving forward and inverse problems in traffic flow. In this paper, we study a neural operator framework for learning solutions to nonlinear hyperbolic partial differential equations with applications in macroscopic traffic flow models. In this framework, an operator is trained to map heterogeneous and sparse traffic input data to the complete macroscopic traffic state in a supervised learning setting. We chose a physics-informed Fourier neural operator ($\pi$-FNO) as the operator, where an additional physics loss based on a discrete conservation law regularizes the problem during training to improve the shock predictions. We also propose to use training data generated from random piecewise constant input data to systematically capture the shock and rarefied solutions. From experiments using the LWR traffic flow model, we found superior accuracy in predicting the density dynamics of a ring-road network and urban signalized road. We also found that the operator can be trained using simple traffic density dynamics, e.g., consisting of $2-3$ vehicle queues and $1-2$ traffic signal cycles, and it can predict density dynamics for heterogeneous vehicle queue distributions and multiple traffic signal cycles $(\geq 2)$ with an acceptable error. The extrapolation error grew sub-linearly with input complexity for a proper choice of the model architecture and training data. Adding a physics regularizer aided in learning long-term traffic density dynamics, especially for problems with periodic boundary data.
</details>
<details>
<summary>摘要</summary>
深度学习方法在交通流动问题中得到广泛应用。在这篇论文中，我们研究了一种神经网络框架，用于解决非线性偏微分方程的问题，并应用于大规模交通流模型。在这个框架中，一个算子被训练，以将不同和稀疏的交通数据映射到完整的交通状态中。我们选择了一种带有物理约束的 fourier神经网络（π-FNO）作为算子，其中在训练过程中添加了物理损失，以提高震动预测。我们还提出了使用随机划分的杂ync constant输入数据来系统地捕捉震动和稀疏解。从实验使用LWR交通流模型来看，我们发现了在density动力学中的高精度预测。我们还发现算子可以通过简单的交通密度动力学，例如由2-3辆汽车队列和1-2个交通信号周期组成的系统，来预测密度动力学。此外，我们发现算子可以在不同的汽车队列分布和多个交通信号周期（至少2个）下预测密度动力学，并且误差在输入复杂性增长的速度下逐渐增加。添加物理约束可以帮助学习长期交通密度动力学，特别是在 periodic boundry data 的问题上。
</details></li>
</ul>
<hr>
<h2 id="UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering"><a href="#UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering" class="headerlink" title="UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering"></a>UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07048">http://arxiv.org/abs/2308.07048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Pan, Von-Wun Soo</li>
<li>for: 提高推荐系统的准确率和可解释性</li>
<li>methods: 使用prototype-based matrix factorization方法，即UIPC-MF，其中用户和 Item 都关联有一组原型，以增强推荐的可解释性</li>
<li>results: 相比其他原型基eline方法，UIPC-MF 在三个 dataset 上显示出较高的 Hit Ratio 和 Normalized Discounted Cumulative Gain，同时也提供了更好的透明度。<details>
<summary>Abstract</summary>
Recommending items to potentially interested users has been an important commercial task that faces two main challenges: accuracy and explainability. While most collaborative filtering models rely on statistical computations on a large scale of interaction data between users and items and can achieve high performance, they often lack clear explanatory power. We propose UIPC-MF, a prototype-based matrix factorization method for explainable collaborative filtering recommendations. In UIPC-MF, both users and items are associated with sets of prototypes, capturing general collaborative attributes. To enhance explainability, UIPC-MF learns connection weights that reflect the associative relations between user and item prototypes for recommendations. UIPC-MF outperforms other prototype-based baseline methods in terms of Hit Ratio and Normalized Discounted Cumulative Gain on three datasets, while also providing better transparency.
</details>
<details>
<summary>摘要</summary>
推荐预测已经是电商中一项非常重要的任务，面临着两个主要挑战：准确性和可读性。大多数共同推荐模型通过大规模的用户-物品交互数据进行统计计算，可以达到高性能，但通常缺乏明确的解释力。我们提出了UIPC-MF，一种基于原型的矩阵分解方法，用于可读性推荐。在UIPC-MF中，用户和物品都关联到一组原型，捕捉总的共同特征。为了增强可读性，UIPC-MF学习用户和物品原型之间的关联Weight，用于推荐。UIPC-MF在三个数据集上比基eline方法具有更高的 Hit Ratio 和 Normalized Discounted Cumulative Gain，同时也提供了更好的透明度。
</details></li>
</ul>
<hr>
<h2 id="No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning"><a href="#No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning" class="headerlink" title="No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning"></a>No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07047">http://arxiv.org/abs/2308.07047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Songcan Chen</li>
<li>for: 本研究目的是解决因缺失数据而导致的不完整的分类器（Incomplete LDL，InLDL）性能下降问题，而不需要任何显式正则化。</li>
<li>methods: 我们提出使用分类器自身的标签分布作为正则化的先验知识，并设计了一种权重 schemes来强调小度和缺失度。</li>
<li>results: 我们的模型具有四个优点：1）没有需要显式正则化；2）具有闭式解决方案和易于实现（只需几行代码）；3）与大规模数据集相比，计算复杂度 linear；4）与当前状态对齐水平性能。<details>
<summary>Abstract</summary>
Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we first define a weighted empirical risk and derive upper bounds between the expected risk and the weighted empirical risk, which reveals in principle that weighting plays an implicit regularization role. Then, by using the prior of degrees, we design a weighted scheme and verify its effectiveness. To sum up, our model has four advantages, it is 1) model selection free, as no explicit regularization is imposed; 2) with closed form solution (sub-problem) and easy-to-implement (a few lines of codes); 3) with linear computational complexity in the number of samples, thus scalable to large datasets; 4) competitive with state-of-the-arts even without any explicit regularization.
</details>
<details>
<summary>摘要</summary>
标签分布学习（LDL）将软标签，即学习度，赋予样本。在实际应用中，完整的学习度往往很困难寻求，从而产生了偏差的LDL（InLDL）问题。然而，InLDL经常会导致性能下降。现有方法通常需要一或多个显式正则化，这会增加参数调整的复杂性和额外计算。我们认为标签分布本身可以提供有用的前提，当正确使用时，InLDL问题可以解决无需显式正则化。在本文中，我们提出了一种合理的使用此前提的方法。我们的启发是，大度标签更有可能受到注意，小度标签容易被忽略，而缺失的度标签完全被偏差的InLDL忽略。为了学习准确的标签分布，非常重要不是忽略小 observed degree，而是给它们适当的大量重要，同时逐渐增加缺失的度标签的重要性。我们首先定义了权重化的empirical risk，并 derivated upper bound между预期风险和权重化empirical risk，这 revelas in principle that weighting plays an implicit regularization role。然后，通过使用度标签的前提，我们设计了权重方案，并证明其效果。总之，我们的模型具有以下四个优点：1) 无需显式正则化，因为不需要任何显式正则化; 2) 具有关闭式解（sub-problem）和易于实现（只需几行代码）; 3) 对大量数据集 scales linearly，因此可扩展性好; 4) 与状态 искусственный지标下相当竞争，即无需显式正则化。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Flow-Networks"><a href="#Bayesian-Flow-Networks" class="headerlink" title="Bayesian Flow Networks"></a>Bayesian Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07037">http://arxiv.org/abs/2308.07037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stefanradev93/BayesFlow">https://github.com/stefanradev93/BayesFlow</a></li>
<li>paper_authors: Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez</li>
<li>for: 本研究提出了抽象概率流网络（BFN），一种新的生成模型，其中抽象概率流网络的参数通过 bayesian 推断在噪声数据样本的灯光下进行修改，然后通过神经网络输出第二个相互dependent的分布。</li>
<li>methods: 该研究提出了一种基于 bayesian 推断的生成过程，其中从简单的先验开始，逐步更新两个分布，得到一个类似于反diffusion 模型的生成过程，但是更加简单，不需要前向过程。</li>
<li>results: 在图像模型task上，BFNs  achieved competitive log-likelihoods on dynamically binarized MNIST and CIFAR-10，并在文本8字符级语言模型任务上超越所有已知的批diffusion 模型。<details>
<summary>Abstract</summary>
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文介绍了 bayesian flow networks (BFNs)，一种新的生成模型，其中bayesian inference modify了一组独立分布的参数，然后将这些修改后的分布作为输入传递给神经网络，生成一个第二个、相互关联的分布。这个过程类似于Diffusion模型的反向过程，但是更加简单，不需要前向过程。模型可以处理整数、连续和整数化数据，并且使用本地差分导数，使得梯度导航和几步生成在整数领域如语言模型中变得更加容易。损失函数直接优化数据压缩，并不限制网络架构。在实验中，BFNs在 dynamically binarized MNIST和CIFAR-10上的图像模型 task中 achieved competitive log-likelihoods，并在text8 character-level语言模型任务上超过了所有已知的整数 diffusion models。
</details></li>
</ul>
<hr>
<h2 id="S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields"><a href="#S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields" class="headerlink" title="S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields"></a>S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07032">http://arxiv.org/abs/2308.07032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madaoer/s3im_nerf">https://github.com/madaoer/s3im_nerf</a></li>
<li>paper_authors: Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang, Haoran Wang, Yunfeng Cai, Mingming Sun</li>
<li>for: 该论文旨在提高NeRF和相关神经场方法的可视化质量，使其能够更好地渲染未知视角的场景图像。</li>
<li>methods: 该论文提出了一种非本地多重训练方法，通过一种新的随机结构相似性（S3IM）损失函数，将多个数据点处理为一个整体，而不是独立处理每个输入。</li>
<li>results: 对于八种视角合成任务和八种表面重建任务，S3IM可以减少测试MSE损失率超过90%，提高F1分数198%和Chamfer-$L_{1}$距离64%。此外，S3IM可以在缺少输入、损坏图像和动态场景下保持稳定性。<details>
<summary>Abstract</summary>
Recently, Neural Radiance Field (NeRF) has shown great success in rendering novel-view images of a given scene by learning an implicit representation with only posed RGB images. NeRF and relevant neural field methods (e.g., neural surface representation) typically optimize a point-wise loss and make point-wise predictions, where one data point corresponds to one pixel. Unfortunately, this line of research failed to use the collective supervision of distant pixels, although it is known that pixels in an image or scene can provide rich structural information. To the best of our knowledge, we are the first to design a nonlocal multiplex training paradigm for NeRF and relevant neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss that processes multiple data points as a whole set instead of process multiple inputs independently. Our extensive experiments demonstrate the unreasonable effectiveness of S3IM in improving NeRF and neural surface representation for nearly free. The improvements of quality metrics can be particularly significant for those relatively difficult tasks: e.g., the test MSE loss unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is consistently robust even with sparse inputs, corrupted images, and dynamic scenes.
</details>
<details>
<summary>摘要</summary>
近期，神经辐射场（NeRF）已经取得了在渲染新视图图像中的成功，通过学习含义几何表示，只使用配置好的RGB图像。NeRF和相关的神经场方法（例如神经表面表示）通常是点 wise 损失优化和点 wise 预测，其中一个数据点对应一个像素。尽管这一线索的研究把握不到远程像素的集合supervision，即图像或场景中的像素可以提供丰富的结构信息。据我们所知，我们是首先设计了非本地多样training paradigm for NeRF和相关神经场方法，通过一种新的随机Structural SIMilarity（S3IM）损失函数，将多个数据点处理为一个整体而不是独立处理多个输入。我们的广泛实验表明，S3IM可以减少TensoRF和DVGO的测试MSE损失超过90%，并且 NeuS 的F-score提高198%，Chamfer $L_{1}$ 距离减少64%。此外，S3IM还能够在缺少输入、损坏图像和动态场景下保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer"><a href="#Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer" class="headerlink" title="Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer"></a>Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07352">http://arxiv.org/abs/2308.07352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Nilabh, Fidel Grandia</li>
<li>For: This paper aims to develop a predictive tool for the mobility of engineered nanoparticles (ENPs) in groundwater, to support the development of an efficient remediation strategy for polluted groundwater sites.* Methods: The paper uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the mobility of ENPs within an aquifer, and to quantify the uncertainty in the predictions.* Results: The forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility, and the inverse model output is used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.Here’s the Chinese translation of the three key information points:* For: 这篇论文的目的是为了开发一种能够预测Engineered Nanoparticles（ENPs）在地下水中的移动性，以支持污染地下水站的清理和环境重建。* Methods: 这篇论文使用了一种 Bayesian Physics-Informed Neural Network（B-PINN）框架，来模拟ENPs在aquifer中的移动性，并量化预测结果的不确定性。* Results: 前向模型表明B-PINN在准确预测ENPs移动性的能力，而反向模型输出可以用来预测aquifer中ENPs移动性的主导参数。这项研究 demonstarte了这种工具的能力，可以为开发有效的地下水清理策略提供预测性的信息。<details>
<summary>Abstract</summary>
Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility and quantifying the uncertainty. The inverse model output is then used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.
</details>
<details>
<summary>摘要</summary>
全球有很多污染的地下水点，需要有效的活动整治计划以恢复当地生态环境。工程化的奈米颗粒（ENPs）在地下水中的吸附和分解作用已经在室内实验室中得到了证明，但是在实际场景中的应用仍然受限。奈米颗粒的复杂的运输和保持机制限制了整治策略的发展。因此，一个可预测奈米颗粒的运输和保持行为的工具是非常重要。现有的文献中的工具主要是数值模拟器，它们在缺乏数据和地下水异常性时的灵活性和准确性受到限制。本研究使用泛函神经网络（B-PINN）框架来模拟奈米颗粒在aquifer中的 mobilidad。前向模型的输出结果表明B-PINN在准确预测奈米颗粒 mobilidad和评估不确定性的能力。逆向模型输出被用来预测 governing parameters 的奈米颗粒 mobilidad在小规模 aquifer 中。研究表明工具的可预测性可以为开发有效的地下水整治策略提供先进的预测性 Insight。
</details></li>
</ul>
<hr>
<h2 id="IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse"><a href="#IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse" class="headerlink" title="IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse"></a>IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07351">http://arxiv.org/abs/2308.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Li, Hao Li, Jin Zhang, Zhen Wang, Peng Liu, Chongjie Zhang</li>
<li>for: 本研究旨在提出一种新的转移学习RL方法，以便在新任务上快速解决问题。</li>
<li>methods: 本方法使用actor-critic框架中的Q函数来导引策略选择，选择最大一步改进策略作为目标策略。我们同时实现了优化转移和行为转移（IOB），通过规范学习策略以便模仿指导策略，并将其与行为策略相结合。</li>
<li>results: 我们的方法在标准任务上超越了状态之前的转移RL基准值，并在连续学习场景中提高了最终性和知识传递性。此外，我们证明了我们的优化转移技术可以提高目标策略学习。<details>
<summary>Abstract</summary>
Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source policy with the largest one-step improvement over the current target policy. We integrate optimization transfer and behavior transfer (IOB) by regularizing the learned policy to mimic the guidance policy and combining them as the behavior policy. This integration significantly enhances transfer effectiveness, surpasses state-of-the-art transfer RL baselines in benchmark tasks, and improves final performance and knowledge transferability in continual learning scenarios. Additionally, we show that our optimization transfer technique is guaranteed to improve target policy learning.
</details>
<details>
<summary>摘要</summary>
人类具有 reuse previously learned policies 来解决新任务的能力，同时 reinforcement learning (RL) 代理也可以通过将知识传递到相关的目标任务中来实现这一点。传输 RL 方法可以改变政策优化目标（优化传输）或者影响行为政策（行为传输）使用源政策。然而，在有限样本情况下选择合适的源政策是一大挑战。先前的方法可能会添加额外的组件，如层次政策或源政策价值函数的估计，这可能会导致非站点政策优化或者大量的样本成本，这将导致传输效果减退。为解决这个挑战，我们提出了一种新的传输 RL 方法，不需要训练额外的组件。我们利用 actor-critic 框架中的 Q 函数来导引政策选择，选择目标政策中一步改进最大的源政策。我们将优化传输和行为传输（IOB）相结合，通过规则化学习的政策来模仿指导政策，并将其与行为政策相结合。这种结合显著提高了传输效果，超越了基eline的传输 RL 标准 benchmark 任务，并在连续学习场景中提高了最终性和知识传递性。此外，我们证明了我们的优化传输技术能够提高目标政策学习。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training"><a href="#Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training" class="headerlink" title="Efficient Neural PDE-Solvers using Quantization Aware Training"></a>Efficient Neural PDE-Solvers using Quantization Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07350">http://arxiv.org/abs/2308.07350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Winfried van den Dool, Tijmen Blankevoort, Max Welling, Yuki M. Asano</li>
<li>for: 用 neural networks 代替经典数学方法解决 Partial Differential Equations (PDEs) 的应用，以减少计算成本。</li>
<li>methods: 使用现有的量化方法来降低计算成本，并保持性能。</li>
<li>results: 在四个标准 PDE 数据集和三个网络架构上，发现量化训练可以降低计算成本三个数量级，而且在不同设置下都能够保持性能。此外，我们还证明了在大多数情况下，只有通过包含量化来达到 Pareto 优化。<details>
<summary>Abstract</summary>
In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs performance is almost always achieved only by incorporating quantization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads"><a href="#Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads" class="headerlink" title="Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads"></a>Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07013">http://arxiv.org/abs/2308.07013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingheng Mo, Fanchao Chen, Siqiang Luo, Caihua Shan</li>
<li>For: 提高静态负荷下的系统性能，即使面临动态负荷。* Methods: 使用Reinforcement Learning（RL）引导LSM树转换，并提出了一种新的LSM树结构（FLSM树）以优化压缩策略的转换。* Results: 比较RL和传统的方法，RusKey在多种负荷下显示了4倍的终端性能优势，而无需先知工作负荷知识。<details>
<summary>Abstract</summary>
LSM-trees are widely adopted as the storage backend of key-value stores. However, optimizing the system performance under dynamic workloads has not been sufficiently studied or evaluated in previous work. To fill the gap, we present RusKey, a key-value store with the following new features: (1) RusKey is a first attempt to orchestrate LSM-tree structures online to enable robust performance under the context of dynamic workloads; (2) RusKey is the first study to use Reinforcement Learning (RL) to guide LSM-tree transformations; (3) RusKey includes a new LSM-tree design, named FLSM-tree, for an efficient transition between different compaction policies -- the bottleneck of dynamic key-value stores. We justify the superiority of the new design with theoretical analysis; (4) RusKey requires no prior workload knowledge for system adjustment, in contrast to state-of-the-art techniques. Experiments show that RusKey exhibits strong performance robustness in diverse workloads, achieving up to 4x better end-to-end performance than the RocksDB system under various settings.
</details>
<details>
<summary>摘要</summary>
LSM树是键值存储系统的常用后端存储方式。然而，在动态负荷下优化系统性能尚未得到充分研究和评估。为填补这一空白，我们提出了RusKey，一个具有以下新特点的键值存储系统：1. RusKey是首次在线上调度LSM树结构，以实现对动态负荷下的robust性表现;2. RusKey是首次使用强化学习（RL）引导LSM树转换;3. RusKey包含一种新的LSM树设计，名为FLSM树，用于高效地在不同压缩策略之间进行过渡;4. RusKey不需要先知工作负荷信息，与当前技术相比，更加灵活和易用。我们通过理论分析证明了新设计的优越性。实验结果显示，RusKey在多种工作负荷下表现出强大的性能稳定性，与RocksDB系统在不同设置下达到4倍的终端性能。
</details></li>
</ul>
<hr>
<h2 id="Greedy-online-change-point-detection"><a href="#Greedy-online-change-point-detection" class="headerlink" title="Greedy online change point detection"></a>Greedy online change point detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07012">http://arxiv.org/abs/2308.07012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jou-Hui Ho, Felipe Tobar</li>
<li>for: 寻找时间序列中的变化点，以提高变化点检测的准确率和效率。</li>
<li>methods: 使用Greedy Online Change Point Detection（GOCPD）方法，该方法通过最大化数据来自两个独立模型（temporal）的概率来找到变化点。</li>
<li>results: 在synthetic数据和实际世界单variate和多variate设置中，GOCPD方法能够快速减少false discovery rate，并且在某些情况下比传统方法更高效。<details>
<summary>Abstract</summary>
Standard online change point detection (CPD) methods tend to have large false discovery rates as their detections are sensitive to outliers. To overcome this drawback, we propose Greedy Online Change Point Detection (GOCPD), a computationally appealing method which finds change points by maximizing the probability of the data coming from the (temporal) concatenation of two independent models. We show that, for time series with a single change point, this objective is unimodal and thus CPD can be accelerated via ternary search with logarithmic complexity. We demonstrate the effectiveness of GOCPD on synthetic data and validate our findings on real-world univariate and multivariate settings.
</details>
<details>
<summary>摘要</summary>
常规在线变点检测（CPD）方法通常会有较大的假阳性率，因为它们对异常值敏感。为了解决这个缺点，我们提议了简单在线变点检测（GOCPD）方法，它通过最大化数据来自两个独立模型（时间排序）的概率来检测变点。我们证明，对具有单个变点的时间序列，这个目标函数是单峰型，因此可以通过三元搜索来加速CPD，其复杂度为幂函数。我们在 sintetic 数据上证明了 GOCPD 的有效性，并在实际的单VAR 和多VAR 设置中验证了我们的结论。
</details></li>
</ul>
<hr>
<h2 id="Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning"><a href="#Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning" class="headerlink" title="Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning"></a>Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11636">http://arxiv.org/abs/2308.11636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Liu, Yuanyuan Chen, Anran Li, Yi Ding, Han Yu, Cuntai Guan<br>for: 这个研究旨在解决脑computer interfaces（BCI）建立高性能深度学习模型的长期挑战， BCIs 的数据多样性问题。methods: 这个研究提出了一个弹性联边学习（FLEEG）框架，让不同装备的数据可以在训练过程中合作。每个客户端都有自己的特定数据集，并训练一个层次化的专门化模型，以处理不同的数据格式。服务器则处理训练过程，将来自所有数据集的知识融合，以提高总性能。results: 这个框架在脑意图（MI）分类任务中，与9个由不同设备收集的EEG数据集合作，可以提高分类性能达16.7%。可视化结果显示，提案的框架可以让本地模型专注在任务相关的区域，从而获得更好的性能。<details>
<summary>Abstract</summary>
Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and trains a hierarchical personalized model to manage diverse data formats and facilitate information exchange. Meanwhile, the server coordinates the training procedure to harness knowledge gleaned from all datasets, thus elevating overall performance. The framework has been evaluated in Motor Imagery (MI) classification with nine EEG datasets collected by different devices but implementing the same MI task. Results demonstrate that the proposed frame can boost classification performance up to 16.7% by enabling knowledge sharing between multiple datasets, especially for smaller datasets. Visualization results also indicate that the proposed framework can empower the local models to put a stable focus on task-related areas, yielding better performance. To the best of our knowledge, this is the first end-to-end solution to address this important challenge.
</details>
<details>
<summary>摘要</summary>
BCIs 长期面临不充分数据的挑战，建立高性能深度学习模型困难。虽然许多研究机构和机构收集了多个 EEG 数据集，但是共享多个站点的 EEG 数据仍然困难，主要因为设备的不一致性。这个挑战的重要性无法被过度估计，因为数据多样性对模型的稳定性具有关键作用。然而，现有的研究很少讨论这个问题，通常是在单个数据集内进行模型训练，常在 между subject 或 session 上下文中进行。在这种情况下，我们提出了一种层次个性化 Federated Learning EEG 解码（FLEEG）框架，以解决这个挑战。这种创新的框架标志着 BCIs 新的学习模式，使得不同数据格式的数据集可以在模型训练过程中合作。每个客户端被分配特定数据集，并训练一个层次个性化模型，以处理多个数据格式的多样性，并且促进信息交换。同时，服务器协调训练过程，以利用所有数据集中所获得的知识，从而提高整体性能。我们在 Motor Imagery（MI） 分类任务中使用了 nine EEG 数据集，每个数据集由不同的设备收集，但都实现了相同的 MI 任务。结果表明，我们的框架可以提高分类性能达到 16.7%，尤其是对小数据集的提高。可视化结果还表明，我们的框架可以让本地模型固定焦点于任务相关区域，从而提高性能。到目前为止，我们的解决方案是 BCIs 首次尝试的综合解决方案。
</details></li>
</ul>
<hr>
<h2 id="Deep-convolutional-neural-networks-for-cyclic-sensor-data"><a href="#Deep-convolutional-neural-networks-for-cyclic-sensor-data" class="headerlink" title="Deep convolutional neural networks for cyclic sensor data"></a>Deep convolutional neural networks for cyclic sensor data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06987">http://arxiv.org/abs/2308.06987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Payman Goodarzi, Yannick Robin, Andreas Schütze, Tizian Schneider</li>
<li>for: 本研究旨在探讨基于感测器的维保维护，并应用深度学习技术来解决多感测器系统中的复杂性问题。</li>
<li>methods: 本研究使用了一个 hidraulic system testbed dataset，并比较了三个模型的性能：基线模型使用了 conventional methods，单个CNN模型使用了 early sensor fusion，并且二路CNN模型（2L-CNN）使用了 late sensor fusion。</li>
<li>results: 基线模型使用了 late sensor fusion，可以达到1%的测试错误率，但CNN模型由于感测器的多样性而遇到了问题，导致测试错误率达到20.5%。此外，我们还进行了每感测器都进行独立的训练，并观察到了几个感测器的准确率变化。此外，2L-CNN模型的性能表现了显著的改善，当考虑了最佳和最差的感测器时，错误率下降了33%。本研究重申了多感测器系统中的复杂性问题需要有效地解决。<details>
<summary>Abstract</summary>
Predictive maintenance plays a critical role in ensuring the uninterrupted operation of industrial systems and mitigating the potential risks associated with system failures. This study focuses on sensor-based condition monitoring and explores the application of deep learning techniques using a hydraulic system testbed dataset. Our investigation involves comparing the performance of three models: a baseline model employing conventional methods, a single CNN model with early sensor fusion, and a two-lane CNN model (2L-CNN) with late sensor fusion. The baseline model achieves an impressive test error rate of 1% by employing late sensor fusion, where feature extraction is performed individually for each sensor. However, the CNN model encounters challenges due to the diverse sensor characteristics, resulting in an error rate of 20.5%. To further investigate this issue, we conduct separate training for each sensor and observe variations in accuracy. Additionally, we evaluate the performance of the 2L-CNN model, which demonstrates significant improvement by reducing the error rate by 33% when considering the combination of the least and most optimal sensors. This study underscores the importance of effectively addressing the complexities posed by multi-sensor systems in sensor-based condition monitoring.
</details>
<details>
<summary>摘要</summary>
预测维护在产业系统不间断运行和降低系统故障风险的角色非常重要。本研究使用液压系统测试 datasets 进行探索，并应用深度学习技术进行condition monitoring。我们的调查包括比较三种模型的性能：基eline模型使用 convent ional 方法、单个CNN模型（1L-CNN）在早期整合感知器、以及两个CNN模型（2L-CNN）在晚期整合感知器。基eline模型通过使用晚期整合，实现了1%的测试错误率。然而，CNN模型由于感知器的多样性，导致20.5%的错误率。为了进一步调查这一问题，我们进行了每个感知器分别进行训练，并观察了准确性的变化。此外，我们还评估了2L-CNN模型的性能，其在考虑最佳和最差的感知器组合时显示了33%的下降。这一研究强调了condition monitoring中多感知器系统的复杂性需要得到有效地处理。
</details></li>
</ul>
<hr>
<h2 id="pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems"><a href="#pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems" class="headerlink" title="pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems"></a>pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06983">http://arxiv.org/abs/2308.06983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Momojit Biswas, Himanshu Buckchash, Dilip K. Prasad</li>
<li>for: 本文是为了提高 nearest neighbor 基于自助学习（SSL）的图像识别问题的表现而写的。</li>
<li>methods: 本文使用 nearest neighbor 方法，并引入 pseudo nearest neighbors (pNN) 来控制支持集质量，以提高表现。 另外，文中还使用了一种抽样策略和一种平滑重量更新策略来稳定 nearest neighbor 基于学习的uncertainty。</li>
<li>results: 根据文中的评估结果，提出的方法与基eline nearest neighbor 方法相比，在多个公共图像识别和医学图像识别 dataset 上表现出了8%的提升。此外，该方法与其他之前提出的 SSL 方法相比也具有相似的表现。<details>
<summary>Abstract</summary>
Nearest neighbor (NN) sampling provides more semantic variations than pre-defined transformations for self-supervised learning (SSL) based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline (pNNCLR) to the nearest neighbor based SSL approach (NNCLR). To this end, we introduce pseudo nearest neighbors (pNN) to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.
</details>
<details>
<summary>摘要</summary>
近邻采样（NN）提供了更多的 semantic variation than pre-defined transformation for self-supervised learning（SSL）based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline（pNNCLR）to the nearest neighbor based SSL approach（NNCLR）. To this end, we introduce pseudo nearest neighbors（pNN）to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach"><a href="#Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach" class="headerlink" title="Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach"></a>Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06973">http://arxiv.org/abs/2308.06973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijie He, Ziye Jia, Chao Dong, Wei Wang, Yilu Cao, Yang Yang, Qihui Wu</li>
<li>for: 本文关注在无人航空器（UAV）网络中的路由计划和恢复，以解决UAV网络受到意外攻击的问题。</li>
<li>methods: 本文提出了一种基于节点重要性的攻击模型，并设计了一种节点重要性排名机制，考虑了节点和链接重要性。此外，本文还提出了一种基于强化学习的智能算法，以恢复UAV网络中的路由路径当UAVs被攻击。</li>
<li>results: 数据 simulate 结果表明，提出的机制比其他相关方法更为有效。<details>
<summary>Abstract</summary>
The unmanned aerial vehicle (UAV) network is popular these years due to its various applications. In the UAV network, routing is significantly affected by the distributed network topology, leading to the issue that UAVs are vulnerable to deliberate damage. Hence, this paper focuses on the routing plan and recovery for UAV networks with attacks. In detail, a deliberate attack model based on the importance of nodes is designed to represent enemy attacks. Then, a node importance ranking mechanism is presented, considering the degree of nodes and link importance. However, it is intractable to handle the routing problem by traditional methods for UAV networks, since link connections change with the UAV availability. Hence, an intelligent algorithm based on reinforcement learning is proposed to recover the routing path when UAVs are attacked. Simulations are conducted and numerical results verify the proposed mechanism performs better than other referred methods.
</details>
<details>
<summary>摘要</summary>
“无人航空器（UAV）网络在近年得到广泛应用，但是它们的路由却受到分布网络架构的影响，导致UAV易受到意外攻击。因此，本文关注于UAV网络路由规划和恢复，并对攻击后路由路径的恢复进行了研究。具体来说，我们设计了一种基于节点重要性的攻击模型，并提出了一种考虑节点度和链接重要性的节点重要性排名机制。然而，由于UAV网络中链接的连接变化，传统方法无法有效地处理UAV网络的路由问题。因此，我们提出了一种基于强化学习算法的智能路由恢复方法，以恢复在攻击后的路由路径。我们通过实验和数值结果发现，提议的机制在攻击后路由恢复方面表现更好 than其他已知方法。”Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation"><a href="#AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation" class="headerlink" title="AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation"></a>AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06965">http://arxiv.org/abs/2308.06965</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus">https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus</a></li>
<li>paper_authors: Ziru Liu, Kecheng Chen, Fengyi Song, Bo Chen, Xiangyu Zhao, Huifeng Guo, Ruiming Tang</li>
<li>for: addressing new user IDs or item IDs in streaming recommender systems</li>
<li>methods: utilizes reinforcement learning-driven framework with an Identity Agent and critic network to dynamically determine shared embeddings and retain&#x2F;eliminate ID features</li>
<li>results: significantly enhances recommendation performance and reduces memory usage by approximately 20-30%<details>
<summary>Abstract</summary>
In the domain of streaming recommender systems, conventional methods for addressing new user IDs or item IDs typically involve assigning initial ID embeddings randomly. However, this practice results in two practical challenges: (i) Items or users with limited interactive data may yield suboptimal prediction performance. (ii) Embedding new IDs or low-frequency IDs necessitates consistently expanding the embedding table, leading to unnecessary memory consumption. In light of these concerns, we introduce a reinforcement learning-driven framework, namely AutoAssign+, that facilitates Automatic Shared Embedding Assignment Plus. To be specific, AutoAssign+ utilizes an Identity Agent as an actor network, which plays a dual role: (i) Representing low-frequency IDs field-wise with a small set of shared embeddings to enhance the embedding initialization, and (ii) Dynamically determining which ID features should be retained or eliminated in the embedding table. The policy of the agent is optimized with the guidance of a critic network. To evaluate the effectiveness of our approach, we perform extensive experiments on three commonly used benchmark datasets. Our experiment results demonstrate that AutoAssign+ is capable of significantly enhancing recommendation performance by mitigating the cold-start problem. Furthermore, our framework yields a reduction in memory usage of approximately 20-30%, verifying its practical effectiveness and efficiency for streaming recommender systems.
</details>
<details>
<summary>摘要</summary>
在流媒体推荐系统领域，传统的新用户ID或项目ID处理方法通常是随机分配初始ID embedding。然而，这种做法会导致两个实际挑战：（i）有限交互数据的物品或用户可能会得到下标性的预测性能。（ii）为新ID或低频ID分配 embedding 表需要持续扩展 embedding 表，从而导致不必要的内存浪费。为了解决这些问题，我们提出了一个基于强化学习的框架，即 AutoAssign+。具体来说，AutoAssign+ 使用一个 Identity Agent 作为 actor 网络，该网络在两个角色中发挥作用：（i）通过将低频ID分配到一小set of shared embedding来提高初始化 embedding，并（ii）在 embedding 表中动态确定需要保留或删除的 ID 特征。Policy 网络的优化受到批评网络的指导。为评估我们的方法的效果，我们在三个常用的数据集上进行了广泛的实验。实验结果表明，AutoAssign+ 能够有效解决冷启点问题，并且减少内存使用率约 20-30%，证明我们的方法在流媒体推荐系统中具有实际效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis"><a href="#Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis" class="headerlink" title="Graph Structural Residuals: A Learning Approach to Diagnosis"></a>Graph Structural Residuals: A Learning Approach to Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06961">http://arxiv.org/abs/2308.06961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Lukas Augustin, Oliver Niggemann</li>
<li>for: 提出了一种新的框架，将模型基于诊断结合深度图结构学习。</li>
<li>methods: 利用数据学习系统的下面结构，并通过两个不同的图邻元矩阵来提供动态观察。</li>
<li>results: 通过实验示范了一种数据驱动的诊断方法的潜在优势。<details>
<summary>Abstract</summary>
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
</details>
<details>
<summary>摘要</summary>
传统的模型基于诊断方法是通过构建明确的系统模型来进行，这可能是劳动密集且需要专家知识的。在这篇论文中，我们提出了一种新的框架，它结合了模型基于诊断和深度图结构学习的概念。这种数据驱动的方法利用数据来学习系统的下面结构，并提供动态观察结果，表示为两个不同的图邻接矩阵。我们的工作使得图结构学习与模型基于诊断的集成变得自然和简单，我们的主要贡献包括：1. 重新定义系统表示、观察和缺陷的构造2. 提出了两种不同的自我超级VI持模型建立方法3. 通过对振荡器系统进行实验，证明我们的数据驱动诊断方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks"><a href="#Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks" class="headerlink" title="Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks"></a>Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06960">http://arxiv.org/abs/2308.06960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Wang, Shimin Di, Lei Chen, Xiaofang Zhou</li>
<li>for: 本研究旨在提高预训练GNNs的表现，并设计一种适应性更高的微调策略。</li>
<li>methods: 我们提出了一种基于搜索的微调策略，称为S2PGNN，可以适应不同的下游任务和数据集。</li>
<li>results: 我们在10个著名的预训练GNNs上进行了实验，并证明了S2PGNN可以在不同的下游任务和数据集上提高模型的表现，并且比现有的微调策略更高效。<details>
<summary>Abstract</summary>
Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search to fine-tune pre-trained graph neural networks for graph-level tasks (S2PGNN), which adaptively design a suitable fine-tuning framework for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.
</details>
<details>
<summary>摘要</summary>
近些年来，图 нейрон网络（GNNs）在许多图关联任务中显示出前无似的成功。然而，GNNs still faces the label scarcity issue like other neural networks do. Therefore, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only a few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search for a suitable fine-tuning framework for the given labeled data on the downstream task, which we call S2PGNN. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at [uri].
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II"><a href="#Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II" class="headerlink" title="Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II"></a>Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06959">http://arxiv.org/abs/2308.06959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Kraus, Stefan Feuerriegel, Maytal Saar-Tsechansky</li>
<li>for: 这篇论文的目的是为了提出一个可靠的、成本效益的决策模型，用于分配预防性治疗给有风险的病人。</li>
<li>methods: 本论文使用了Counterfactual推理、机器学习和优化技术，搭建了可扩展的决策模型，可以利用现代医疗记录中的高维度医疗数据。</li>
<li>results: 根据89,191名 prediabetic 病人的电子医疗记录进行评估，我们的决策模型可以与现有的实践相比，实现每年的成本储储11亿美元。此外，我们还分析了不同预算水平下的成本效益。<details>
<summary>Abstract</summary>
Problem Definition. Increasing costs of healthcare highlight the importance of effective disease prevention. However, decision models for allocating preventive care are lacking.   Methodology/Results. In this paper, we develop a data-driven decision model for determining a cost-effective allocation of preventive treatments to patients at risk. Specifically, we combine counterfactual inference, machine learning, and optimization techniques to build a scalable decision model that can exploit high-dimensional medical data, such as the data found in modern electronic health records. Our decision model is evaluated based on electronic health records from 89,191 prediabetic patients. We compare the allocation of preventive treatments (metformin) prescribed by our data-driven decision model with that of current practice. We find that if our approach is applied to the U.S. population, it can yield annual savings of $1.1 billion. Finally, we analyze the cost-effectiveness under varying budget levels.   Managerial Implications. Our work supports decision-making in health management, with the goal of achieving effective disease prevention at lower costs. Importantly, our decision model is generic and can thus be used for effective allocation of preventive care for other preventable diseases.
</details>
<details>
<summary>摘要</summary>
问题定义：医疗费用的增长强调了疾病预防的重要性。然而，疾病预防投入决策模型缺失。方法ология/结果：在这篇论文中，我们开发了基于数据驱动的疾病预防投入决策模型，用于确定对患有风险的患者进行成本效果的预防治疗分配。我们结合了Counterfactual推理、机器学习和优化技术，构建了可扩展的决策模型，可以利用现代电子医疗记录中的高维医疗数据。我们的决策模型通过对89191名 prediabetic 患者的电子医疗记录进行评估。我们比较了我们的数据驱动决策模型与当前做法分配预防治疗（metformin）的情况。我们发现，如果我们的方法应用于美国人口，可以每年节省11亿美元。最后，我们分析了不同预算水平下的成本效果。管理意义：我们的工作支持医疗管理决策，以实现有效的疾病预防，并降低成本。重要的是，我们的决策模型是通用的，可以用于有效地分配预防治疗其他预防性疾病。
</details></li>
</ul>
<hr>
<h2 id="CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets"><a href="#CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets" class="headerlink" title="CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets"></a>CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06957">http://arxiv.org/abs/2308.06957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongik Shin, Beomsuk Kim, Seungjun Baek</li>
<li>for: 这篇论文是用于探讨如何将不同类型的静脉影像融合为一个共同的数据集，以提高医疗影像分类模型的通用能力。</li>
<li>methods: 这篇论文使用的方法是将不同类型的静脉影像融合为一个共同的数据集，然后使用Segment Anything模型（SAM）来进行静脉影像分类。此外，论文还提出了一个名为Condition Embedding block（CEmb）的新方法，可以将不同数据集的特性统计学 Normalization。</li>
<li>results: 实验结果显示，CEmb-SAM比基eline方法在静脉影像分类 задачі中表现更好，特别是在 péripheral nerves和breast cancer领域。这些结果显示了Cemb-SAM在医疗影像分类任务中学习不同数据集的能力。<details>
<summary>Abstract</summary>
Automated segmentation of ultrasound images can assist medical experts with diagnostic and therapeutic procedures. Although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. In this paper, we consider the problem of jointly learning from heterogeneous datasets so that the model can improve generalization abilities by leveraging the inherent variability among datasets. We merge the heterogeneous datasets into one dataset and refer to each component dataset as a subgroup. We propose to train a single segmentation model so that the model can adapt to each sub-group. For robust segmentation, we leverage recently proposed Segment Anything model (SAM) in order to incorporate sub-group information into the model. We propose SAM with Condition Embedding block (CEmb-SAM) which encodes sub-group conditions and combines them with image embeddings from SAM. The conditional embedding block effectively adapts SAM to each image sub-group by incorporating dataset properties through learnable parameters for normalization. Experiments show that CEmb-SAM outperforms the baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer. The experiments highlight the effectiveness of Cemb-SAM in learning from heterogeneous datasets in medical image segmentation tasks.
</details>
<details>
<summary>摘要</summary>
自动分割超音波图像可以帮助医疗专家进行诊断和治疗过程。although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. 在这篇论文中，我们考虑了将异构数据集合在一起，以便模型可以提高通用能力，并且可以利用数据集的内在多样性。我们将异构数据集合为一个数据集，并将每个子组数据集称为子组。我们提议使用单个分割模型，以便模型可以适应每个子组。为了增强分割稳定性，我们利用最近提出的 Segment Anything model (SAM)，并在SAM中添加 Condition Embedding block (CEmb)，以编码子组条件并与图像嵌入结合。 conditional embedding block Effectively adapts SAM to each image subgroup by incorporating dataset properties through learnable parameters for normalization. 实验显示，CEmb-SAM在超音波图像分割任务中超过基eline方法表现，特别是在 péripheral nerves 和乳腺癌中。这些实验证明了 CEmb-SAM 在医学图像分割任务中学习异构数据集的有效性。
</details></li>
</ul>
<hr>
<h2 id="Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels"><a href="#Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels" class="headerlink" title="Channel-Wise Contrastive Learning for Learning with Noisy Labels"></a>Channel-Wise Contrastive Learning for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06952">http://arxiv.org/abs/2308.06952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Kang, Sheng Liu, Huaxi Huang, Tongliang Liu</li>
<li>for: 本文针对受损标签学习（LNL）问题，旨在训练一个能够识别实际类别的分类器。</li>
<li>methods: 本文提出了通道级别对比学习（CWCL）方法，通过对不同通道进行对比学习，分离真实标签信息和噪声。</li>
<li>results: 对多个基准数据集进行评估，本文的方法与现有方法相比，具有更高的识别率和更好的抗噪声性。<details>
<summary>Abstract</summary>
In real-world datasets, noisy labels are pervasive. The challenge of learning with noisy labels (LNL) is to train a classifier that discerns the actual classes from given instances. For this, the model must identify features indicative of the authentic labels. While research indicates that genuine label information is embedded in the learned features of even inaccurately labeled data, it's often intertwined with noise, complicating its direct application. Addressing this, we introduce channel-wise contrastive learning (CWCL). This method distinguishes authentic label information from noise by undertaking contrastive learning across diverse channels. Unlike conventional instance-wise contrastive learning (IWCL), CWCL tends to yield more nuanced and resilient features aligned with the authentic labels. Our strategy is twofold: firstly, using CWCL to extract pertinent features to identify cleanly labeled samples, and secondly, progressively fine-tuning using these samples. Evaluations on several benchmark datasets validate our method's superiority over existing approaches.
</details>
<details>
<summary>摘要</summary>
实际数据集中，噪声标签是普遍存在的。学习噪声标签（LNL）的挑战是训练一个可以从给定的实例中分辨实际的类别的分类器。为此，模型必须标识表示实际标签的特征。虽然研究表明，正确的标签信息在噪声标签的数据中被学习的特征中嵌入，但它通常与噪声杂mix在一起，使其直接应用更加复杂。为解决这个问题，我们引入通道wise contrastive learning（CWCL）。这种方法通过在多个通道进行对比学习来 отличи出实际标签信息和噪声。与传统的实例wise contrastive learning（IWCL）不同，CWCL往往可以生成更加细化和鲜明的特征，与实际标签更加相似。我们的策略是两重的：首先，使用CWCL提取重要的特征，以便从干净标注的样本中分辨实际标签信息；其次，逐渐练化使用这些样本。我们在多个标准 benchmark 数据集上进行了评估，并证明了我们的方法与现有方法相比具有superiority。
</details></li>
</ul>
<hr>
<h2 id="Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding"><a href="#Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding" class="headerlink" title="Knowing Where to Focus: Event-aware Transformer for Video Grounding"></a>Knowing Where to Focus: Event-aware Transformer for Video Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06947">http://arxiv.org/abs/2308.06947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/eatr">https://github.com/jinhyunj/eatr</a></li>
<li>paper_authors: Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon, Kwanghoon Sohn</li>
<li>for: 这 paper 的目的是提出一种事件意识的动态时刻查询方法，以便模型可以根据输入视频的内容和位置信息来更好地预测时刻。</li>
<li>methods: 这 paper 使用了一种叫做槽注意机制的事件分解技术，以及一种名叫网关融合变换层的时刻查询融合技术，来实现事件意识的动态时刻查询。</li>
<li>results: 根据实验结果，这 paper 的事件意识动态时刻查询方法在多个视频锚点识别 benchmark 上表现出了比州前方法更高的效果和效率。<details>
<summary>Abstract</summary>
Recent DETR-based video grounding models have made the model directly predict moment timestamps without any hand-crafted components, such as a pre-defined proposal or non-maximum suppression, by learning moment queries. However, their input-agnostic moment queries inevitably overlook an intrinsic temporal structure of a video, providing limited positional information. In this paper, we formulate an event-aware dynamic moment query to enable the model to take the input-specific content and positional information of the video into account. To this end, we present two levels of reasoning: 1) Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism; and 2) moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps. Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.
</details>
<details>
<summary>摘要</summary>
现代基于DETR的录像落幕模型已经直接预测时刻无需任何手工Component，如预先定义的提案或非最大抑制，通过学习时刻查询。然而，这些输入不具预设的时刻查询无法考虑录像的自然时间结构，仅提供有限的位置信息。在本文中，我们提出了事件意识的动态时刻查询，让模型能够根据输入内容和位置信息进行事件对话。为此，我们提出了两种逻辑：1）事件逻辑，使用槽注意力机制来捕捉录像中的特定事件单位；2）时刻逻辑，将时刻查询与输入句子的数据融合，透过闸道融合对应层来学习时刻查询与录像句子表示之间的互动，以预测时刻。实验结果显示了事件意识的动态时刻查询的有效性和高效性，在多个录像落幕 bencmarks 上出perform state-of-the-art 方法。
</details></li>
</ul>
<hr>
<h2 id="Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis"><a href="#Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis" class="headerlink" title="Semantic-aware Network for Aerial-to-Ground Image Synthesis"></a>Semantic-aware Network for Aerial-to-Ground Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06945">http://arxiv.org/abs/2308.06945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/sanet">https://github.com/jinhyunj/sanet</a></li>
<li>paper_authors: Jinhyun Jang, Taeyong Song, Kwanghoon Sohn</li>
<li>for: 这个论文旨在解决飞行图像与地面图像的同构问题，以实现将飞行图像中的元素转映到地面图像中。</li>
<li>methods: 该论文提出了一个新的框架，通过增强结构对应和Semantic意识来解决这个问题。它 introduce了一种新的Semantic-attentive feature transformation模块，可以重建复杂的地理结构。此外，论文还提出了Semantic-aware的损失函数，通过利用预训练的分类网络，让网络synthesize出realistic的对象。</li>
<li>results: 对比之前的方法和简化研究，论文的方法得到了较高的效果， both qualitatively and quantitatively。<details>
<summary>Abstract</summary>
Aerial-to-ground image synthesis is an emerging and challenging problem that aims to synthesize a ground image from an aerial image. Due to the highly different layout and object representation between the aerial and ground images, existing approaches usually fail to transfer the components of the aerial scene into the ground scene. In this paper, we propose a novel framework to explore the challenges by imposing enhanced structural alignment and semantic awareness. We introduce a novel semantic-attentive feature transformation module that allows to reconstruct the complex geographic structures by aligning the aerial feature to the ground layout. Furthermore, we propose semantic-aware loss functions by leveraging a pre-trained segmentation network. The network is enforced to synthesize realistic objects across various classes by separately calculating losses for different classes and balancing them. Extensive experiments including comparisons with previous methods and ablation studies show the effectiveness of the proposed framework both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:空中到地面图像合成是一个emerging和挑战性的问题，目标是将空中图像转换为地面图像。由于空中和地面图像的布局和对象表示方式之间存在巨大差异，现有的方法通常无法将空中场景中的组件转移到地面场景中。在这篇论文中，我们提出了一个新的框架，以强化结构对Alignment和Semantic意识。我们引入了一个新的启发式Semantic-attentive特征转换模块，以将空中特征转换为地面布局。此外，我们提出了Semantic-aware的损失函数，通过利用预训练的分割网络来强制网络在不同类型的对象上synthesize出真实的对象。我们进行了广泛的实验，包括与前方法进行比较和简要的ablation研究，以证明我们的框架的有效性。
</details></li>
</ul>
<hr>
<h2 id="Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning"><a href="#Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning" class="headerlink" title="Insurance pricing on price comparison websites via reinforcement learning"></a>Insurance pricing on price comparison websites via reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06935">http://arxiv.org/abs/2308.06935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanut Treetanthiploet, Yufei Zhang, Lukasz Szpruch, Isaac Bowers-Barnard, Henrietta Ridley, James Hickey, Chris Pearce</li>
<li>for: 本研究旨在为保险公司在价格比较网站（PCW）上形ulation Effective 价格策略提供技术支持。</li>
<li>methods: 本研究使用了强化学习（RL）框架，通过结合模型基于和模型自由方法来学习价格策略。</li>
<li>results: 比较 experiment 表明，我们的方法在sample efficiency和累积奖励方面超过了6个参考方法，只有在市场情况具有完美信息时才能达到相同水平。<details>
<summary>Abstract</summary>
The emergence of price comparison websites (PCWs) has presented insurers with unique challenges in formulating effective pricing strategies. Operating on PCWs requires insurers to strike a delicate balance between competitive premiums and profitability, amidst obstacles such as low historical conversion rates, limited visibility of competitors' actions, and a dynamic market environment. In addition to this, the capital intensive nature of the business means pricing below the risk levels of customers can result in solvency issues for the insurer. To address these challenges, this paper introduces reinforcement learning (RL) framework that learns the optimal pricing policy by integrating model-based and model-free methods. The model-based component is used to train agents in an offline setting, avoiding cold-start issues, while model-free algorithms are then employed in a contextual bandit (CB) manner to dynamically update the pricing policy to maximise the expected revenue. This facilitates quick adaptation to evolving market dynamics and enhances algorithm efficiency and decision interpretability. The paper also highlights the importance of evaluating pricing policies using an offline dataset in a consistent fashion and demonstrates the superiority of the proposed methodology over existing off-the-shelf RL/CB approaches. We validate our methodology using synthetic data, generated to reflect private commercially available data within real-world insurers, and compare against 6 other benchmark approaches. Our hybrid agent outperforms these benchmarks in terms of sample efficiency and cumulative reward with the exception of an agent that has access to perfect market information which would not be available in a real-world set-up.
</details>
<details>
<summary>摘要</summary>
互联网价格比较网站（PCW）的出现对保险公司带来了独特的挑战。在PCW上运营时，保险公司需要维护一个折衔的平衡，以确保竞争力和利润之间的协调。这些挑战包括历史 conversions 率低、竞争对手动作的有限可见性以及动态的市场环境。此外，保险业务具有资本投入的特点，因此低于客户风险水平的价格可能会导致资本问题。为解决这些挑战，本文提出了一种基于强化学习（RL）框架的价格策略优化方法。RL框架结合模型基于和模型自由两种方法，通过在离线环境中训练代理人，避免冷启始问题，然后在Contextual Bandit（CB）上使用模型自由算法动态更新价格策略，以最大化预期收益。这有助于快速适应市场动态变化，提高算法效率和决策可读性。文章还强调了评估价格策略的离线数据集的一致性，并证明提议的方法在已有的RL/CB方法中表现出色。我们验证了方法使用人工生成的数据，模拟了实际保险公司的私人数据，并与6个参考方法进行比较。我们的混合代理人在样本效益和累积奖励方面都超越参考方法，只有具有完美市场信息的代理人能够在实际场景中匹配我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models"><a href="#Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models" class="headerlink" title="Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models"></a>Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06929">http://arxiv.org/abs/2308.06929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Chapman, Seifey Mohammad, Kimberly Villegas</li>
<li>for: 预测Airbnb租赁价格，帮助hosts优化收益和助旅行者做出 Informed 预订决策。</li>
<li>methods: 使用机器学习模型方法，包括sentiment分析，对Airbnb租赁价格进行预测和分析。</li>
<li>results: 通过对Airbnb租赁价格进行预测和分析，提高了hosts的收益和旅行者的决策能力。<details>
<summary>Abstract</summary>
Our research group wanted to take on the difficult task of predicting prices in a dynamic market. And short term rentals such as Airbnb listings seemed to be the perfect proving ground to do such a thing. Airbnb has revolutionized the travel industry by providing a platform for homeowners to rent out their properties to travelers. The pricing of Airbnb rentals is prone to high fluctuations, with prices changing frequently based on demand, seasonality, and other factors. Accurate prediction of Airbnb rental prices is crucial for hosts to optimize their revenue and for travelers to make informed booking decisions. In this project, we aim to predict the prices of Airbnb rentals using a machine learning modeling approach.   Our project expands on earlier research in the area of analyzing Airbnb rental prices by taking a methodical machine learning approach as well as incorporating sentiment analysis into our feature engineering. We intend to gain a deeper understanding on periodic changes of Airbnb rental prices. The primary objective of this study is to construct an accurate machine learning model for predicting Airbnb rental prices specifically in Austin, Texas. Our project's secondary objective is to identify the key factors that drive Airbnb rental prices and to investigate how these factors vary across different locations and property types.
</details>
<details>
<summary>摘要</summary>
我们的研究小组想要解决难度较大的价格预测问题，并选择短期租赁如Airbnb列表作为证明场景。Airbnb已经革命化旅游业，提供了为房东租出房屋给旅行者的平台。Airbnb租赁价格受到高度波动的影响，价格随着需求、季节和其他因素而变化频繁。正确预测Airbnb租赁价格对房东来说是提高收益的关键，对旅行者来说也是为了做出 Informed 预订决策。在这个项目中，我们使用机器学习模型预测Airbnb租赁价格。我们的项目在对Airbnb租赁价格分析方面进行了深入探索，并且通过包括情感分析在内的特征工程来扩展我们的研究。我们的主要目标是在得克萨斯州奥斯汀建立一个准确的机器学习模型，用于预测Airbnb租赁价格。项目的次要目标是Identify 租赁价格的关键因素，以及这些因素在不同的地点和财产类型中的变化趋势。
</details></li>
</ul>
<hr>
<h2 id="CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor"><a href="#CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor" class="headerlink" title="CBA: Improving Online Continual Learning via Continual Bias Adaptor"></a>CBA: Improving Online Continual Learning via Continual Bias Adaptor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06925">http://arxiv.org/abs/2308.06925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wqza/cba-online-cl">https://github.com/wqza/cba-online-cl</a></li>
<li>paper_authors: Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, Deyu Meng</li>
<li>for: 这篇论文目的是为了解决在线上持续学习（Continual Learning，CL）中的分布变化问题，以确保模型能够稳定地学习新的知识和固化先前学习的知识。</li>
<li>methods: 本文提出了一个增强器（Continual Bias Adaptor，CBA）模组，用于在训练过程中将分类器网络调整为适应不断变化的分布，以避免模型对先前学习的知识忘记和偏向新的任务。</li>
<li>results: 本文透过实验证明了CBA模组能够有效地解决分布变化问题，并且不会增加训练过程中的计算成本和记忆遗传。<details>
<summary>Abstract</summary>
Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases toward the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classifier network is able to learn a stable consolidation of previously learned tasks. In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catastrophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在线 continual learning (CL) 目标是在非站点数据流中学习新知识并固化先前学习的知识。由于训练环境的时间变化，学习到的模型很容易忘记先前学习的知识，偏向新接收的任务。为解决这个问题，我们提议一个 Continual Bias Adaptor (CBA) 模块，用于在训练过程中增强分类网络，以适应不断变化的分布，使得分类网络能够稳定地固化先前学习的任务。在测试阶段，CBA可以被移除，无需额外的计算成本和内存占用。我们理论上解释了我们提议的方法可以有效缓解悬危分布变化的问题，并通过了四种基eline和三个公共 continual learning 标准 benchmark 的实验来证明其效果。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings"><a href="#A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings" class="headerlink" title="A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings"></a>A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10822">http://arxiv.org/abs/2308.10822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wen, Jie Wang, Xiaodong Qiao</li>
<li>for: 本研究旨在提高中文科技论文摘要中的 Move 识别精度。</li>
<li>methods: 该研究提出了一种基于改进预训练模型和扩展网络含attend mechanism的新型加强 Move 识别算法。</li>
<li>results: 实验结果显示，该算法在分 Split 数据集上比原始数据集提高13.37%的精度，并与基本对比模型提高7.55%的精度。<details>
<summary>Abstract</summary>
The recognition of abstracts is crucial for effectively locating the content and clarifying the article. Existing move recognition algorithms lack the ability to learn word position information to obtain contextual semantics. This paper proposes a novel enhanced move recognition algorithm with an improved pre-trained model and a gated network with attention mechanism for unstructured abstracts of Chinese scientific and technological papers. The proposed algorithm first performs summary data segmentation and vocabulary training. The EP-ERNIE$\_$AT-GRU framework is leveraged to incorporate word positional information, facilitating deep semantic learning and targeted feature extraction. Experimental results demonstrate that the proposed algorithm achieves 13.37$\%$ higher accuracy on the split dataset than on the original dataset and a 7.55$\%$ improvement in accuracy over the basic comparison model.
</details>
<details>
<summary>摘要</summary>
“抽象概念识别是科技文献检索和理解的关键。现有的移动识别算法无法学习单词位置信息，导致Contextual semantics的学习受限。本文提出了一种基于EP-ERNIE$\_$AT-GRU框架的增强移动识别算法，用于处理中文科技文献抽象。该算法首先进行摘要数据分 segmentation和词汇训练。实验结果表明，提posed算法在分组数据集上的准确率高于原始数据集13.37%，并且与基本比较模型相比提高7.55%。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CausalLM-is-not-optimal-for-in-context-learning"><a href="#CausalLM-is-not-optimal-for-in-context-learning" class="headerlink" title="CausalLM is not optimal for in-context learning"></a>CausalLM is not optimal for in-context learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06912">http://arxiv.org/abs/2308.06912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, Radu Soricut</li>
<li>for: 本研究探讨了使用前缀语言模型（prefixLM）和 causalLanguage Model（causalLM）在卷积Transformer上的受限学习性能的比较。</li>
<li>methods: 本研究采用了理论方法来分析prefixLM和causalLM的参数构造下的收敛行为。</li>
<li>results: 研究发现，prefixLM在线性回归问题上 converges to its optimal solution，而causalLM的收敛动态类似于在线梯度下降算法，并不一定是最优解，即使数据量无限大。Empirical experiments over synthetic and real tasks verify that causalLM consistently underperforms prefixLM in all settings。<details>
<summary>Abstract</summary>
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.
</details>
<details>
<summary>摘要</summary>
近期实验证据表明，基于转换器的增强学习在使用前缀语言模型（前缀LM）下表现更好，因为所有的增强样本都可以相互听说，而不使用 causalLM（ causalLM），它使用自动循环注意力，禁止增强样本听说未来的样本。虽然这种结果是直观的，但从理论角度不是很了解。在这篇论文中，我们采取了理论方法，分析了 prefixLM 和 causalLM 下的参数构造下的收敛行为。我们的分析表明，两种LM类型都会在一定的参数构造下收敛到其站点点，但是 prefixLM 会收敛到线性回归的优化解，而 causalLM 的收敛动力则类似于在线上的梯度下降算法，这并不是保证优化的，即使样本数量在无穷大。我们通过实验证明， causalLM 在所有设置下一直表现出下降性。
</details></li>
</ul>
<hr>
<h2 id="GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text"><a href="#GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text" class="headerlink" title="GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"></a>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06911">http://arxiv.org/abs/2308.06911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Liu, Yiming Ren, Zhixiang Ren</li>
<li>for: 这个论文主要是为了提出一种多ModalLanguage模型，用于融合Graph、Image和Text信息，以提高分子数据的表示和生成能力。</li>
<li>methods: 这种模型使用GIT-Former架构，可以将所有modalities映射到一个共同准则空间中，以便进行多ModalLanguage的表示和计算。</li>
<li>results: 该研究提出了一种创新的任意语言分子翻译策略，提高了分子描述率10%-15%，提高了属性预测精度5%-10%，并提高了分子生成有效性20%。<details>
<summary>Abstract</summary>
Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. Here, we introduce GIT-Mol, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space. Our study develops an innovative any-to-language molecular translation strategy and achieves a 10%-15% improvement in molecular captioning, a 5%-10% accuracy increase in property prediction, and a 20% boost in molecule generation validity compared to baseline or single-modality models.
</details>
<details>
<summary>摘要</summary>
大型语言模型在自然语言处理方面做出了重要进步，开辟了创新应用，如分子表示和生成。然而，现有的单一模式方法无法捕捉分子数据中的丰富和复杂信息。在这里，我们介绍GIT-Mol，一个多模式大语言模型，它结合结构граф、图像和文本信息，包括简单分子输入系统（SMILES）和分子描述。为了实现多modal分子数据的整合，我们提出GIT-Former，一个新的架构，可以将所有模式转换到一个统一的潜在空间中。我们的研究开发了一种创新的任何语言分子翻译策略，并在分子描述、性能预测和分子生成领域中实现了10%-15%的改进，5%-10%的精度提高和20%的额外验证。
</details></li>
</ul>
<hr>
<h2 id="Generative-Interpretation"><a href="#Generative-Interpretation" class="headerlink" title="Generative Interpretation"></a>Generative Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06907">http://arxiv.org/abs/2308.06907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonathanarbel/generativeinterpretation">https://github.com/yonathanarbel/generativeinterpretation</a></li>
<li>paper_authors: Yonathan A. Arbel, David Hoffman</li>
<li>for: 这篇论文旨在提出一种新的合同理解方法，使用大型自然语言模型来估计合同意义。</li>
<li>methods: 该论文采用了实践案例研究的方法，通过使用AI模型来查看不同的应用场景，并使用实际的合同文档来证明AI模型的能力。</li>
<li>results: 该论文显示了AI模型可以帮助法官和仲裁人员更好地理解合同的意义，量化含义的混淆度，并填充合同中的缺失。同时，该论文还描述了使用这些模型时的限制和风险，以及它们对法律实践和合同理论的影响。<details>
<summary>Abstract</summary>
We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.
</details>
<details>
<summary>摘要</summary>
我们引入生成式解释，一种新的方法来估计合同意义使用大型语言模型。在人工智能胜利的时代，我们采用实地案例来证明这些新工具的能力，每个案例都展示了这些工具在不同方面的能力。我们使用知名合同案例，并提供了实际协议，以示AI模型可以帮助事实发现者在文本上确定常见意义，衡量模糊性，并填充党们的协议中的空白。我们还示出了模型可以计算个别外部证据的证据价值。接着，我们提供了使用这些模型的最佳实践，以及其限制。我们考虑了这些模型在法律实践和合同理论方面的影响，并 argues that these models can become the new workhorse of contractual interpretation.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls"><a href="#Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls" class="headerlink" title="Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls"></a>Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06895">http://arxiv.org/abs/2308.06895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Prakash, Jin Sima, Chao Pan, Eli Chien, Olgica Milenkovic</li>
<li>for: 这个论文是为了研究在几何空间中进行分布式学习，以保护数据隐私。</li>
<li>methods: 该论文提出了一种基于几何空间的分布式学习方法，包括在Poincaré圆柱中分布式的支持向量机学习和一种基于整数B_h序列的标签恢复方法。</li>
<li>results: 该论文的实验结果表明，使用几何空间进行分布式学习可以提高分类精度，并且可以保护数据隐私。<details>
<summary>Abstract</summary>
Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit the communication cost for the hulls, we propose a new quantization method for the Poincar\'e disc coupled with Reed-Solomon-like encoding. Fourth, at server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\sim 11\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces.
</details>
<details>
<summary>摘要</summary>
Hierarchical和树状数据集在许多应用中出现，包括语言处理、图数据挖掘、phylogeny和 genomics。已知树状数据无法在 finite 维 Euclidian 空间中嵌入，这问题可以通过使用抽象空间来缓解。在分布式和隐私化设置下，需要采用新的联邦学习方法，这是一个新的领域。作为这个领域的初步，我们提出了首个在抽象空间上的联邦分类方法。我们的贡献如下：1. 我们开发了分布式版本的凸 Support Vector Machine（SVM）分类器，用于Poincaré盘上的数据。在这个设置中，客户端上的信息是各个客户端数据中的封闭集。2. 为了避免标签交换问题，我们引入了一种数学推理的方法，基于 so-called 整数 $B_h$ 序列。3. 我们计算了抽象空间中的凸闭复杂度，以评估数据泄露程度，同时，我们提出了一种新的归一化方法，用于限制归一化成本。4. 在服务器端，我们引入了一种新的客户端归一化方法，基于平衡图分 partitioning。我们测试了我们的方法在多种多样的数据集上，包括层次单元 RNA-seq 数据集，来自不同的病人和不同的存储库，这些存储库具有严格的隐私限制。我们的方法的分类精度与其欧氏凸缩形相比，提高了约 11%，这表明了隐私保护在抽象空间上的学习的重要性。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders"><a href="#Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders" class="headerlink" title="Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders"></a>Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06885">http://arxiv.org/abs/2308.06885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Kasalický, Rodrigo Alves, Pavel Kordík</li>
<li>for: 研究和比较在线表现的Offline评估指标的选择，以提高live推荐系统中的选择。</li>
<li>methods: 使用减小流行item和考虑交易时间的评估方法，以提高选择的准确性。</li>
<li>results: 五个大型实际live数据中的平均结果，用于帮助学术社区更好地理解Offline评估和优化标准的选择。<details>
<summary>Abstract</summary>
The evaluation of recommendation systems is a complex task. The offline and online evaluation metrics for recommender systems are ambiguous in their true objectives. The majority of recently published papers benchmark their methods using ill-posed offline evaluation methodology that often fails to predict true online performance. Because of this, the impact that academic research has on the industry is reduced. The aim of our research is to investigate and compare the online performance of offline evaluation metrics. We show that penalizing popular items and considering the time of transactions during the evaluation significantly improves our ability to choose the best recommendation model for a live recommender system. Our results, averaged over five large-size real-world live data procured from recommenders, aim to help the academic community to understand better offline evaluation and optimization criteria that are more relevant for real applications of recommender systems.
</details>
<details>
<summary>摘要</summary>
评估推荐系统是一项复杂的任务。在线和离线评估指标对于推荐系统存在很多不确定性。大多数最近发表的论文使用不确定的离线评估方法来评估其方法的性能，这常导致实际上线性能和学术界的影响相差甚大。我们的研究目的是研究和比较离线评估指标对实时推荐系统的在线性能的影响。我们发现，对流行 item 的惩罚和在评估过程中考虑交易时间可以显著提高我们选择最佳推荐模型的能力。我们的结果，基于五个大型实际应用中的真实数据，希望能帮助学术界更好地理解推荐系统的离线评估和优化标准，以便更好地应用于实际应用中。
</details></li>
</ul>
<hr>
<h2 id="Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning"><a href="#Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning" class="headerlink" title="Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning"></a>Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06884">http://arxiv.org/abs/2308.06884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</li>
<li>for: 这 paper 研究了任务对应的通信系统，在 transmitter 与多个 receivers 之间进行任务完成和数据传输。</li>
<li>methods: 这 paper 提出了一种基于多任务深度学习的共同编码器和个别解码器的方法，用于对多个任务和多个接收器进行共同优化。</li>
<li>results: 实验结果表明，相比单任务传输系统，多任务传输系统可以更好地适应变化的通信频道条件，并且可以提高任务特定的目标 completions，同时减少传输负担。<details>
<summary>Abstract</summary>
This paper studies task-oriented, otherwise known as goal-oriented, communications, in a setting where a transmitter communicates with multiple receivers, each with its own task to complete on a dataset, e.g., images, available at the transmitter. A multi-task deep learning approach that involves training a common encoder at the transmitter and individual decoders at the receivers is presented for joint optimization of completing multiple tasks and communicating with multiple receivers. By providing efficient resource allocation at the edge of 6G networks, the proposed approach allows the communications system to adapt to varying channel conditions and achieves task-specific objectives while minimizing transmission overhead. Joint training of the encoder and decoders using multi-task learning captures shared information across tasks and optimizes the communication process accordingly. By leveraging the broadcast nature of wireless communications, multi-receiver task-oriented communications (MTOC) reduces the number of transmissions required to complete tasks at different receivers. Performance evaluation conducted on the MNIST, Fashion MNIST, and CIFAR-10 datasets (with image classification considered for different tasks) demonstrates the effectiveness of MTOC in terms of classification accuracy and resource utilization compared to single-task-oriented communication systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity"><a href="#Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity" class="headerlink" title="Quantifying Outlierness of Funds from their Categories using Supervised Similarity"></a>Quantifying Outlierness of Funds from their Categories using Supervised Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06882">http://arxiv.org/abs/2308.06882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Desai, Ashmita Dhiman, Tushar Sharma, Deepika Sharma, Dhagash Mehta, Stefano Pasquali</li>
<li>for: 本研究旨在量化基金分类错误的影响，以便改善投资管理决策。</li>
<li>methods: 本研究使用机器学习方法，将基金分类错误形式化为距离度量学习问题，并计算每个数据点的类别异常度量。</li>
<li>results: 研究发现，基金分类错误与未来回报之间存在强相关关系，并讨论了这些结果的意义。<details>
<summary>Abstract</summary>
Mutual fund categorization has become a standard tool for the investment management industry and is extensively used by allocators for portfolio construction and manager selection, as well as by fund managers for peer analysis and competitive positioning. As a result, a (unintended) miscategorization or lack of precision can significantly impact allocation decisions and investment fund managers. Here, we aim to quantify the effect of miscategorization of funds utilizing a machine learning based approach. We formulate the problem of miscategorization of funds as a distance-based outlier detection problem, where the outliers are the data-points that are far from the rest of the data-points in the given feature space. We implement and employ a Random Forest (RF) based method of distance metric learning, and compute the so-called class-wise outlier measures for each data-point to identify outliers in the data. We test our implementation on various publicly available data sets, and then apply it to mutual fund data. We show that there is a strong relationship between the outlier measures of the funds and their future returns and discuss the implications of our findings.
</details>
<details>
<summary>摘要</summary>
资金基金分类已成为投资管理行业的标准工具，广泛用于配置股票和选择基金管理人，以及基金管理人对准竞对手的分析和竞争位置。因此，任何不当或精度不够的分类可能会对分配决策产生重大影响，并且对投资基金的管理人也有重要影响。在这种情况下，我们想要量化基金分类错误的影响，并使用机器学习的方法来解决这个问题。我们将基金分类问题定义为一个距离度量学习问题，其中异常值是与其他数据点之间的距离最大的数据点。我们使用随机森林（RF）方法来学习距离度量，并计算每个数据点的类别异常度量来确定异常值。我们在各种公开available的数据集上进行测试，然后应用于基金数据。我们发现基金的异常度量和未来回报之间存在强相关性，并讨论了这些发现的意义。
</details></li>
</ul>
<hr>
<h2 id="AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation"><a href="#AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation" class="headerlink" title="AutoSeqRec: Autoencoder for Efficient Sequential Recommendation"></a>AutoSeqRec: Autoencoder for Efficient Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06878">http://arxiv.org/abs/2308.06878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sliu675/autoseqrec">https://github.com/sliu675/autoseqrec</a></li>
<li>paper_authors: Sijia Liu, Jiahao Liu, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, Ning Gu</li>
<li>for: 这篇论文旨在提出一种适合进行sequential recommendation tasks的增量推荐模型，即AutoSeqRec。</li>
<li>methods: AutoSeqRec 使用 autoencoder 架构，包括一个Encoder和三个Decoder。这些 комponents 考虑了用户-项目互动矩阵和项目转移矩阵的rows和columns。重建用户-项目互动矩阵可以捕捉用户长期偏好，而项目转移矩阵的rows和columns可以表示用户短期的兴趣。</li>
<li>results: 该论文的实验结果显示，AutoSeqRec 在精度方面比较高，并且具有优秀的可靠性和效率。<details>
<summary>Abstract</summary>
Sequential recommendation demonstrates the capability to recommend items by modeling the sequential behavior of users. Traditional methods typically treat users as sequences of items, overlooking the collaborative relationships among them. Graph-based methods incorporate collaborative information by utilizing the user-item interaction graph. However, these methods sometimes face challenges in terms of time complexity and computational efficiency. To address these limitations, this paper presents AutoSeqRec, an incremental recommendation model specifically designed for sequential recommendation tasks. AutoSeqRec is based on autoencoders and consists of an encoder and three decoders within the autoencoder architecture. These components consider both the user-item interaction matrix and the rows and columns of the item transition matrix. The reconstruction of the user-item interaction matrix captures user long-term preferences through collaborative filtering. In addition, the rows and columns of the item transition matrix represent the item out-degree and in-degree hopping behavior, which allows for modeling the user's short-term interests. When making incremental recommendations, only the input matrices need to be updated, without the need to update parameters, which makes AutoSeqRec very efficient. Comprehensive evaluations demonstrate that AutoSeqRec outperforms existing methods in terms of accuracy, while showcasing its robustness and efficiency.
</details>
<details>
<summary>摘要</summary>
sequential recommendation 示示了推荐ITEM的能力，通过模型用户的顺序行为。传统方法通常将用户视为ITEM的序列，忽略了用户之间的协作关系。基于图的方法可以利用用户-ITEM交互图，并 integrable 用户之间的协作信息。然而，这些方法有时会面临时间复杂度和计算效率的限制。为了解决这些限制，本文提出了AutoSeqRec，一种适用于顺序推荐任务的递增推荐模型。AutoSeqRec基于自适应器，包括一个Encoder和三个解码器在自适应器架构中。这些组件考虑了用户-ITEM交互矩阵和用户-ITEM交互矩阵的行列。重建用户-ITEM交互矩阵可以捕捉用户长期的偏好，通过共同筛选。此外，用户-ITEM交互矩阵的行列表示ITEM的出度和入度跳跃行为，可以模型用户短期的兴趣。在进行递增推荐时，只需更新输入矩阵，无需更新参数，这使得AutoSeqRec非常有效率。 comprehensive evaluations 表明AutoSeqRec在准确性方面高于现有方法，同时展现出了其稳定性和效率。
</details></li>
</ul>
<hr>
<h2 id="SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer"><a href="#SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer" class="headerlink" title="SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"></a>SpeechX: Neural Codec Language Model as a Versatile Speech Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06873">http://arxiv.org/abs/2308.06873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka</li>
<li>for: 这篇论文旨在探讨一种能够实现多种语音转文字和语音处理任务的新型语音生成模型。</li>
<li>methods: 该模型使用了语音-文本提示的 Audio-Text 模型，并使用多任务学习和任务取向提示来实现一体化和可扩展的模型。</li>
<li>results: 实验结果表明，该模型在多种任务中表现出色，包括零shot TTS、噪声减少、目标说话人抽取、语音除去和背景噪声下的语音编辑等，并在不同任务中与专门的模型进行比较，表现相对或超过专门的模型。<details>
<summary>Abstract</summary>
Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.
</details>
<details>
<summary>摘要</summary>
近期的生成演说模型，基于音频文本提示，已经实现了高质量的零处理文本识别。然而，现有的模型仍然面临着处理多样化的音频文本演说生成任务的限制，包括转换输入speech和处理陌生频谱条件下的音频捕获。本文介绍SpeechX，一种多功能的演说生成模型，能够零处理TTS和多种演说转换任务，处理干净和噪音信号。SpeechX结合神经编码语言模型和多任务学习，使用任务висимы的提示，实现了一个简单、扩展的模型，并提供了一个通用的方法，用于挖掘文本输入在演说增强和转换任务中的作用。实验结果表明SpeechX在不同任务中具有优秀的表现，包括零处理TTS、噪音抑制、目标说话人EXTRACTION、演说除去和演说编辑等，与专门的模型相比，在任务中表现相似或更好。可以查看https://aka.ms/speechx的示例。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition"><a href="#Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition" class="headerlink" title="Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition"></a>Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11635">http://arxiv.org/abs/2308.11635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weishan Ye, Zhiguo Zhang, Min Zhang, Fei Teng, Li Zhang, Linling Li, Gan Huang, Jianhong Wang, Dong Ni, Zhen Liang<br>for: 这篇论文是为了解决识别情绪的困难问题，具体来说是使用EEG数据进行情绪识别。methods: 该论文提出了一种半监督的双流自注意力对抗图像对比学习框架（简称DS-AGC），该框架包括两个平行的流程，一个是提取非结构的EEG特征，另一个是提取结构的EEG特征。results: 该论文的实验结果表明，在两个标准数据库（SEED和SEED-IV）上，提出的模型在不同的受测人数据下的 incomplete label 条件下表现出色，比如平均提高5.83%和6.99%。这表明该模型有效地解决了识别情绪的标签稀缺问题。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments conducted on two benchmark databases (SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out cross-validation evaluation scheme show that the proposed model outperforms existing methods under different incomplete label conditions (with an average improvement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition.
</details>
<details>
<summary>摘要</summary>
电enzephalography (EEG) 是一种客观的表征工具，具有推荐的应用前景。然而，数据标注的缺乏仍然是这个领域的主要挑战，这限制了EEG基于情感认知的广泛应用。在这篇论文中，一种半supervised dual-stream Self-Attentive Adversarial Graph Contrastive learning框架（简称DS-AGC）被提出，以解决跨个体EEG基于情感认知的数据标注稀缺的问题。DS-AGC框架包括两个平行流，用于提取非结构和结构EEG特征。非结构流利用半supervised多元适应方法，以减轻来源领域、无标注领域和目标领域之间的分布差异。结构流发展了图像异常学学习方法，以从多个EEG通道中提取有效的图像特征表示。此外，一个自注意力融合模块被开发，用于特征融合、样本选择和情感认知，强调EEG特征更加关注情感和数据样本在标注领域中的更加相似性。经过对两个参考数据库（SEED和SEED-IV）的 semi-supervised cross-subject leave-one-subject-out cross-validation评估，提出的模型在不同的未完全标注条件下（SEED上提高了5.83%，SEED-IV上提高了6.99%）表现出色，表明它有效地解决了跨个体EEG基于情感认知的数据标注稀缺问题。
</details></li>
</ul>
<hr>
<h2 id="Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks"><a href="#Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks" class="headerlink" title="Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks"></a>Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06862">http://arxiv.org/abs/2308.06862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/erfanloghmani/effect-of-loss-function-tbatching">https://github.com/erfanloghmani/effect-of-loss-function-tbatching</a></li>
<li>paper_authors: Erfan Loghmani, MohammadAmin Fazli</li>
<li>for: 这个论文旨在提出新的方法来实现动态network上的表征学习，并且利用时间信息来改善模型的精度和效率。</li>
<li>methods: 本论文使用了T-batching技术来训练动态网络模型，并提出了两种新的损失函数来解决训练损失的问题。</li>
<li>results: 实验结果显示，使用提案的两种损失函数可以超越原始的损失函数，实现更好的训练性能，并且在实际的动态网络上显示了更高的精度和效率。<details>
<summary>Abstract</summary>
Representation learning methods have revolutionized machine learning on networks by converting discrete network structures into continuous domains. However, dynamic networks that evolve over time pose new challenges. To address this, dynamic representation learning methods have gained attention, offering benefits like reduced learning time and improved accuracy by utilizing temporal information.   T-batching is a valuable technique for training dynamic network models that reduces training time while preserving vital conditions for accurate modeling. However, we have identified a limitation in the training loss function used with t-batching. Through mathematical analysis, we propose two alternative loss functions that overcome these issues, resulting in enhanced training performance.   We extensively evaluate the proposed loss functions on synthetic and real-world dynamic networks. The results consistently demonstrate superior performance compared to the original loss function. Notably, in a real-world network characterized by diverse user interaction histories, the proposed loss functions achieved more than 26.9% enhancement in Mean Reciprocal Rank (MRR) and more than 11.8% improvement in Recall@10. These findings underscore the efficacy of the proposed loss functions in dynamic network modeling.
</details>
<details>
<summary>摘要</summary>
“现代学习方法已经革命化机器学习领域中的网络结构，将离散网络结构转化为连续领域。然而，在时间演变的网络上 pose 新的挑战。为 Addressing 这些挑战，动态表示学习方法受到了关注，它们可以利用时间信息来提高模型的准确性和速度。 T-batching 是训练动态网络模型的有价值技术，它可以降低训练时间的同时保持模型的准确性。然而，我们发现了 t-batching 的训练损失函数中的一个限制。通过数学分析，我们提出了两种替代的损失函数，它们可以解决这些问题，从而提高训练性能。我们对 synthetic 和实际的动态网络进行了广泛的评估，结果 consistently 表明了我们提出的损失函数的超越性。特别是在一个实际网络中，其中用户交互历史多样化，我们的提议损失函数可以提高 Mean Reciprocal Rank (MRR) 的值超过 26.9%，并提高 Recall@10 的值超过 11.8%。这些发现讲述了我们提出的损失函数在动态网络模型中的效果。”
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning"><a href="#Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning" class="headerlink" title="Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning"></a>Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06851">http://arxiv.org/abs/2308.06851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eamon Mukhopadhyay</li>
<li>for: 这篇论文是为了检验篮球metric的有效性而写的。</li>
<li>methods: 这篇论文使用了机器学习技术来模型已有的metric，并选择了一组特有的特征来评估metric的效果。</li>
<li>results: 该论文发现ORTG指数（由Dean Oliver提出）与不同的NBA战术类型有 statistically significant的相关性，但是使用神经网络回归模型而不是线性回归模型表现更好。<details>
<summary>Abstract</summary>
Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regression model and a neural network regression model, although ultimately, a neural network worked slightly better than linear regression. Using the accuracy of the models as a justification, the next step was to optimize the output of the model with test examples, which would demonstrate the combination of features to best achieve a highly functioning offense.
</details>
<details>
<summary>摘要</summary>
在NBA analytics革命中，发展特定指标和公式为球队、教练和球员提供了一种新的视角。然而，问题出现：如何验证这些指标呢？一种方法是通过试错和错误的方式来估算，这是一种估算基于估计的和昂贵的方法。另一种方法是使用机器学习技术来模型已有的指标，并选择一组独特的特征来评估这些指标的效果。如果我们有一个准确的模型，那么它可以帮助我们确定游戏计划执行的 especifics。在这篇论文中，由Dean Oliver开发的ORTG指标（进攻评估指标）与不同的NBA游戏类型之间显示了相关性，使用线性回归模型和神经网络回归模型，其中神经网络模型在精度上略微高一些。使用模型的准确性作为正当化，接下来的步骤是使用测试例子来优化模型的输出，以示出合理的游戏计划执行。
</details></li>
</ul>
<hr>
<h2 id="When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA"><a href="#When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA" class="headerlink" title="When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA"></a>When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06849">http://arxiv.org/abs/2308.06849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/os-hxfan/bayesnn_fpga">https://github.com/os-hxfan/bayesnn_fpga</a></li>
<li>paper_authors: Hongxiang Fan, Hao Chen, Liam Castelli, Zhiqiang Que, He Li, Kenneth Long, Wayne Luk</li>
<li>for: 提高 Bayesian Neural Networks（BayesNNs）的实际应用，因为它们的算法复杂性和硬件性能妨碍了它们的应用。</li>
<li>methods: 该文提出了一种基于 Monte-Carlo Dropout（MCD）的多出口 BayesNN，可以实现准确预测，同时具有低算法复杂性。</li>
<li>results: 我们的自动生成的加速器在能效率方面高于 CPU、GPU 和其他现有硬件实现。<details>
<summary>Abstract</summary>
Bayesian Neural Networks (BayesNNs) have demonstrated their capability of providing calibrated prediction for safety-critical applications such as medical imaging and autonomous driving. However, the high algorithmic complexity and the poor hardware performance of BayesNNs hinder their deployment in real-life applications. To bridge this gap, this paper proposes a novel multi-exit Monte-Carlo Dropout (MCD)-based BayesNN that achieves well-calibrated predictions with low algorithmic complexity. To further reduce the barrier to adopting BayesNNs, we propose a transformation framework that can generate FPGA-based accelerators for multi-exit MCD-based BayesNNs. Several novel optimization techniques are introduced to improve hardware performance. Our experiments demonstrate that our auto-generated accelerator achieves higher energy efficiency than CPU, GPU, and other state-of-the-art hardware implementations.
</details>
<details>
<summary>摘要</summary>
bayesian neural networks (bayesNNs) 有能力提供准确的预测，用于安全关键应用程序，如医疗影像和自动驾驶。然而，bayesNNs 的算法复杂度和硬件性能妨碍其在实际应用中部署。为 bridge 这个差距，这篇文章提议一种新的多出口 Monte Carlo Dropout (MCD) 基于的 bayesNN，可以实现准确的预测，同时具有低的算法复杂度。此外，我们还提出了一种转换框架，可以生成 FPGA 加速器，用于multi-exit MCD 基于的 bayesNNs。我们还引入了一些新的优化技术，以提高硬件性能。我们的实验示例，我们自动生成的加速器在能耗效率方面高于 CPU、GPU 和其他现有硬件实现。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Topological-Graph-Neural-Networks-with-Paths"><a href="#Generalizing-Topological-Graph-Neural-Networks-with-Paths" class="headerlink" title="Generalizing Topological Graph Neural Networks with Paths"></a>Generalizing Topological Graph Neural Networks with Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06838">http://arxiv.org/abs/2308.06838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Truong, Peter Chin</li>
<li>for: 该论文旨在探讨图 neural network (GNN) 在多种领域中的发展，但它们受到一种理论限制，即1-Weisfeiler-Lehmann测试。</li>
<li>methods: 该论文提出了一种以路径为中心的方法，强调图中的路径结构。该方法可以建立更一般的topological视角，并与其他已知的topological领域之间建立联系。</li>
<li>results: 该论文通过对多个benchmark进行测试，发现该方法可以超越之前的技术，在不假设图的子结构的前提下，达到状态艺术性的性能。<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
</details>
<details>
<summary>摘要</summary>
While 图解网络（GNNs）在多个领域取得了 significiant progress, 它们受到一种理论限制，称为一个Weisfeiler-Lehmann测试。 latest advancements in higher-order GNNs可以突破这个boundary，但它们通常围绕某些图Component like cliques or cycles进行设计。 然而，我们的研究采取了不同的方向。 我们强调了路径，这是所有图的内在特征。 我们可以构建一个更通用的topological perspective，并与其他已知的topological domains建立联系。  Interestingly，无需任何图子结构的假设，我们的方法超越了之前在这个领域的技术，在多个标准测试上达到了state-of-the-art性能。
</details></li>
</ul>
<hr>
<h2 id="InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models"><a href="#InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models" class="headerlink" title="InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models"></a>InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08500">http://arxiv.org/abs/2308.08500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Lingyi Liu, Pablo Delgado, Prasanna Padmanabhan</li>
<li>for: 这 paper 的目的是研究深度学习基于推荐模型（DLRM）的训练方法，以及如何优化这些方法以提高效率和可扩展性。</li>
<li>methods: 这 paper 使用了人工智能的强化学习（RL）算法，以学习训练机器的 CPU 资源分配策略，以提高数据加载并行并发行为。</li>
<li>results: 这 paper 的实验结果表明，使用 InTune 可以在只需几分钟之内构建优化的数据管道配置，并且可以轻松地与现有训练工作流Integrate into existing workflows。 InTune 可以提高在线数据加载速率，从而降低模型执行时间的浪费和提高效率。<details>
<summary>Abstract</summary>
Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.   In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune.   InTune employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data loading and improve throughput. Our experiments show that InTune can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune achieves higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus state-of-the-art data pipeline optimizers while also improving both CPU & GPU utilization.
</details>
<details>
<summary>摘要</summary>
InTune 使用了强化学习（RL）代理来学习如何在 DLRM 数据管道中分配训练机器的 CPU 资源，以更有效地并行数据加载并提高吞吐量。我们的实验表明，InTune 可以在只需几分钟之内构建优化数据管道配置，并且可以轻松地与现有训练工作流 integrate。通过RL的响应和适应性，InTune 可以在现有优化器的基础上提高在线数据接收速率，从而降低模型执行时间的浪费和提高效率。我们对实际集群进行应用，发现 InTune 可以提高数据接收吞吐量达到 2.29 倍，同时提高 CPU 和 GPU 资源利用率。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM"><a href="#An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM" class="headerlink" title="An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM"></a>An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06828">http://arxiv.org/abs/2308.06828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham</li>
<li>for: 本研究旨在提出一种新的集成方法，用于问题分类，利用现代模型——Electra、GloVe和LSTM。</li>
<li>methods: 该模型使用了Electra、GloVe和LSTM三种现代模型，通过集成这些模型的优势，提供了一种robust和高效的问题分类解决方案。</li>
<li>results: 对于TREC数据集，提出的集成模型在所有评估指标上都超过了BERT、RoBERTa和DistilBERT等其他现代模型，实现了0.8的测试集准确率。这些结果表明集成方法在问题分类任务中具有显著的优势，并且鼓励进一步探索集成方法在自然语言处理领域中的应用。<details>
<summary>Abstract</summary>
This paper introduces a novel ensemble approach for question classification using state-of-the-art models -- Electra, GloVe, and LSTM. The proposed model is trained and evaluated on the TREC dataset, a well-established benchmark for question classification tasks. The ensemble model combines the strengths of Electra, a transformer-based model for language understanding, GloVe, a global vectors for word representation, and LSTM, a recurrent neural network variant, providing a robust and efficient solution for question classification. Extensive experiments were carried out to compare the performance of the proposed ensemble approach with other cutting-edge models, such as BERT, RoBERTa, and DistilBERT. Our results demonstrate that the ensemble model outperforms these models across all evaluation metrics, achieving an accuracy of 0.8 on the test set. These findings underscore the effectiveness of the ensemble approach in enhancing the performance of question classification tasks, and invite further exploration of ensemble methods in natural language processing.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的集成方法 для问题分类，使用当今最佳模型——Electra、GloVe和LSTM。该模型在TREC数据集上进行训练和评估，TREC数据集是问题分类任务的常见 benchmarck。集成模型结合了Electra、GloVe和LSTM的优势，提供了一个可靠和高效的问题分类解决方案。我们进行了广泛的实验，与其他最新的模型，如BERT、RoBERTa和DistilBERT进行比较。我们的结果表明，集成模型在所有评估指标上都超过了这些模型，在测试集上达到了0.8的准确率。这些结果证明了集成方法在问题分类任务中的效iveness，并邀请了进一步的对natural language processing领域中的集成方法进行探索。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number"><a href="#Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number" class="headerlink" title="Reinforcement Graph Clustering with Unknown Cluster Number"></a>Reinforcement Graph Clustering with Unknown Cluster Number</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06827">http://arxiv.org/abs/2308.06827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/awesome-deep-graph-clustering">https://github.com/yueliu1999/awesome-deep-graph-clustering</a></li>
<li>paper_authors: Yue Liu, Ke Liang, Jun Xia, Xihong Yang, Sihang Zhou, Meng Liu, Xinwang Liu, Stan Z. Li<br>for: 这个研究旨在提供一个不需要先知cluster number的深度图 clustering方法，并且与对图的不确定性进行适应。methods: 我们提出了一个名为Reinforcement Graph Clustering（RGC）的新方法，它通过强化学习机制让cluster number决定和无监督表现学习融合到一个整体框架中。在我们的方法中，首先learn出具有对称预测任务的描述性node表现，然后考虑 both node和cluster状态，以获得更加准确的图 clustering结果。results: 我们的方法在实验中表现出了优异的效能和效率，并且能够在对图中实现更好的适应性和稳定性。此外，我们还提供了一个包含多种深度图 clustering方法的 коллекции（paper、code和dataset），可以帮助研究人员更好地进行深度图 clustering的研究。<details>
<summary>Abstract</summary>
Deep graph clustering, which aims to group nodes into disjoint clusters by neural networks in an unsupervised manner, has attracted great attention in recent years. Although the performance has been largely improved, the excellent performance of the existing methods heavily relies on an accurately predefined cluster number, which is not always available in the real-world scenario. To enable the deep graph clustering algorithms to work without the guidance of the predefined cluster number, we propose a new deep graph clustering method termed Reinforcement Graph Clustering (RGC). In our proposed method, cluster number determination and unsupervised representation learning are unified into a uniform framework by the reinforcement learning mechanism. Concretely, the discriminative node representations are first learned with the contrastive pretext task. Then, to capture the clustering state accurately with both local and global information in the graph, both node and cluster states are considered. Subsequently, at each state, the qualities of different cluster numbers are evaluated by the quality network, and the greedy action is executed to determine the cluster number. In order to conduct feedback actions, the clustering-oriented reward function is proposed to enhance the cohesion of the same clusters and separate the different clusters. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. The source code of RGC is shared at https://github.com/yueliu1999/RGC and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github.
</details>
<details>
<summary>摘要</summary>
深度图 clustering，目标是通过神经网络在无监督情况下将节点分组到不同的分支，在过去几年内吸引了广泛的关注。although 现有的方法已经大幅提高了性能，但是它们依赖于准确预定的分支数量，这在实际场景中并不总是可用。为了使深度图 clustering 算法不受预定分支数量的限制，我们提出了一种新的深度图 clustering 方法，称为奖励图 clustering（RGC）。在我们的提议方法中，集群数量决定和无监督表示学习被统一到一个奖励学习机制中。具体来说，首先通过对比预测任务学习描述性的节点表示。然后，为了准确地捕捉图中的集群状态，包括节点状态和集群状态。在每个状态下，通过质量网络评估不同的分支数量的质量，并执行滥购行动来确定分支数量。为了进行反馈行动，我们提出了一种集群 oriented 奖励函数，以增强同一个集群之间的凝结度和不同集群之间的分离度。我们的实验证明了我们的提议方法的效果和效率。RGC 的源代码可以在 GitHub 上获取：https://github.com/yueliu1999/RGC，而深度图 clustering 相关的代码、论文和数据集可以在 GitHub 上获取：https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering。
</details></li>
</ul>
<hr>
<h2 id="Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning"><a href="#Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning" class="headerlink" title="Approximate and Weighted Data Reconstruction Attack in Federated Learning"></a>Approximate and Weighted Data Reconstruction Attack in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06822">http://arxiv.org/abs/2308.06822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wang, Yongcun Song, Enrique Zuazua</li>
<li>for: 本研究旨在攻击 Federated Learning（FL）中 horizontal Federated Averaging（FedAvg）场景中客户端的模型参数共享。</li>
<li>methods: 我们提出了一种 interpolation-based approximation 方法，可以使 fedavg 场景中客户端的模型参数攻击成为可能。此外，我们还设计了一种层Weighted loss function，可以提高数据重建质量。</li>
<li>results: 我们的 approximate and weighted attack（AWA）方法在不同评价指标中均表现出优于现有方法，特别是在图像数据重建中。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers concerning the neural network structure, with the weights tuned by Bayesian optimization. Finally, experimental results validate the superiority of our proposed approximate and weighted attack (AWA) method over the other state-of-the-art methods, as demonstrated by the substantial improvement in different evaluation metrics for image data reconstructions.
</details>
<details>
<summary>摘要</summary>
Федератированное обучение (FL) 是一种分布式学习 paradigma，允许多个客户端共同建立一个机器学习模型，无需共享其私人数据。虽然 FL 被视为隐私保护的设计，但最近的数据重建攻击表明，攻击者可以根据在 FL 中共享的参数重建客户端的训练数据。然而，现有方法大多不能攻击最常用的水平 Federated Averaging（FedAvg）场景，在这种场景下，客户端在多个本地训练步骤后共享模型参数。为解决这个问题，我们提出了一种 interpolating-based 方法，可以在 FedAvg 场景中生成客户端的本地训练过程中的中间模型更新。然后，我们设计了层wise 权重损失函数，以提高重建数据的质量。我们对模型更新在不同层中分配不同权重，并通过 Bayesian 优化调整这些权重。最后，我们对 AWA 方法进行实验 validate，并证明其在不同评价指标上具有明显的提高。
</details></li>
</ul>
<hr>
<h2 id="SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection"><a href="#SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection" class="headerlink" title="SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection"></a>SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06819">http://arxiv.org/abs/2308.06819</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Vitorino, Isabel Praça, Eva Maia</li>
<li>For: The paper is written to provide a comprehensive overview of the state-of-the-art adversarial learning approaches for realistic example generation in the context of Network Intrusion Detection (NID) using machine learning (ML) models.* Methods: The paper consolidates and summarizes various adversarial attack methods and defense strategies, specifically tailored for the NID domain and realistic network traffic flows.* Results: The paper identifies open challenges and fundamental properties required for realistic adversarial examples in NID, providing guidelines for future research to ensure adequacy for real communication networks.Here’s the same information in Simplified Chinese text:* For: 这篇论文是为了提供网络入侵检测（NID）领域中机器学习（ML）模型的现状摘要，包括最新的敌对学习方法和防御策略。* Methods: 论文总结了各种敌对攻击方法和防御策略，特别适用于NID领域和真实的网络流量。* Results: 论文描述了NID领域中敌对学习模型的开放挑战和基本要求，并提供了未来研究的指导方针，以确保实际网络通信的合理性。<details>
<summary>Abstract</summary>
Machine Learning (ML) can be incredibly valuable to automate anomaly detection and cyber-attack classification, improving the way that Network Intrusion Detection (NID) is performed. However, despite the benefits of ML models, they are highly susceptible to adversarial cyber-attack examples specifically crafted to exploit them. A wide range of adversarial attacks have been created and researchers have worked on various defense strategies to safeguard ML models, but most were not intended for the specific constraints of a communication network and its communication protocols, so they may lead to unrealistic examples in the NID domain. This Systematization of Knowledge (SoK) consolidates and summarizes the state-of-the-art adversarial learning approaches that can generate realistic examples and could be used in real ML development and deployment scenarios with real network traffic flows. This SoK also describes the open challenges regarding the use of adversarial ML in the NID domain, defines the fundamental properties that are required for an adversarial example to be realistic, and provides guidelines for researchers to ensure that their future experiments are adequate for a real communication network.
</details>
<details>
<summary>摘要</summary>
This Systematization of Knowledge (SoK) consolidates and summarizes the state-of-the-art adversarial learning approaches that can generate realistic examples and can be used in real ML development and deployment scenarios with real network traffic flows. This SoK also identifies the open challenges regarding the use of adversarial ML in the NID domain, defines the fundamental properties that are required for an adversarial example to be realistic, and provides guidelines for researchers to ensure that their future experiments are adequate for a real communication network.
</details></li>
</ul>
<hr>
<h2 id="SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning"><a href="#SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning" class="headerlink" title="SAILOR: Structural Augmentation Based Tail Node Representation Learning"></a>SAILOR: Structural Augmentation Based Tail Node Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06801">http://arxiv.org/abs/2308.06801</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jie-re/sailor">https://github.com/jie-re/sailor</a></li>
<li>paper_authors: Jie Liao, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, Zibin Zheng</li>
<li>for: 提高链接结构中tail节点的表示性</li>
<li>methods: 提出了一种基于 структур增强的tail节点表示学习框架，名为SAILOR</li>
<li>results: 对公共评估数据进行了广泛的实验，显示SAILOR可以显著提高tail节点的表示性，并超越当前的基准值<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets demonstrate that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 在最近的表示学习中达到了状态的极品性表现。然而，GNNS的效果，它们基于消息传递操作，强度取决于图结构的质量。大多数实际场景中的图follows a long-tailed distribution on node degrees, that is, most nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes due to the lack of structural information. 为了提高GNNS的表达能力 для尾节点，我们研究了尾节点表示力下降的原因和提出了一种通用的结构扩充based taIL node representation learning框架，名为SAILOR，可以同时学习扩充图结构和提取更有用的尾节点表示。我们在公共 benchmark datasets上进行了广泛的实验，显示SAILOR可以显著提高尾节点表示和超越状态的基eline。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.LG_2023_08_14/" data-id="clly3dvzi006e0988b9b75l3j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.SD_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.SD_2023_08_14/">cs.SD - 2023-08-14 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations"><a href="#Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations" class="headerlink" title="Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations"></a>Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07145">http://arxiv.org/abs/2308.07145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/w-wu/steer">https://github.com/w-wu/steer</a></li>
<li>paper_authors: Wen Wu, Chao Zhang, Philip C. Woodland</li>
<li>for: 提高自动情感识别（AER）的精度和效果，使其能够在对话系统中应用。</li>
<li>methods:  integrate AER with automatic speech recognition（ASR）和speaker diarisation（SD），共同训练一个系统，并使用分布式编码器建立不同的输出层。</li>
<li>results: 在IEMOCAP dataset上进行测试，提议的系统与两个基准系统相比，在AER、ASR和SD三个任务中均表现出色，并且在时间权重 emotions 和 speaker classification 错误上采用了两种评价指标。<details>
<summary>Abstract</summary>
Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD.
</details>
<details>
<summary>摘要</summary>
尽管自动情感识别（AER）在最近几年内受到了广泛的研究兴趣，但大多数当前AER研究使用手动分割的语音，这些语音通常不可用于对话系统。这篇论文提议将AER、自动语音识别（ASR）和 speaker分类（SD）集成为一个集成系统。该系统使用共享Encoder生成了四个子任务的特征输出层，包括AER、ASR、语音活动检测和 speaker分类。将对话的音频作为输入，该集成系统可以找到所有的语音段落，并将对应的情感类别、词序列和Speaker标识转化为文本。为评估AER性能，提出了两种指标，即基于时间权重的情感错误和Speaker错误。results表明，提议的系统在IEMOCAP dataset上比基eline两个独立的单任务系统在AER、ASR和SD领域具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera"><a href="#VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera" class="headerlink" title="VoxBlink: X-Large Speaker Verification Dataset on Camera"></a>VoxBlink: X-Large Speaker Verification Dataset on Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07056">http://arxiv.org/abs/2308.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuke Lin, Xiaoyi Qin, Ming Cheng, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 本研究做出了一个新的和广泛的语音认可数据集，包括噪音38k个标识&#x2F;1.45M次语音（VoxBlink）和相对干净的18k个标识&#x2F;1.02M次语音（VoxBlink-Clean） для训练。</li>
<li>methods: 我们首先建立了一个自动化和可扩展的数据提取管道，从YouTube上下载了60,000个用户的短视频，并从这些视频中自动提取了相关的语音和视频段落。</li>
<li>results: 我们的实验结果表明，将VoxBlink-Clean数据集用于训练，可以提高语音认可性能，比如13%-30%的提升，不同的后向架构之间。这个数据集即将公开发布。<details>
<summary>Abstract</summary>
In this paper, we contribute a novel and extensive dataset for speaker verification, which contains noisy 38k identities/1.45M utterances (VoxBlink) and relatively cleaned 18k identities/1.02M (VoxBlink-Clean) utterances for training. Firstly, we accumulate a 60K+ users' list with their avatars and download their short videos on YouTube. We then established an automatic and scalable pipeline to extract relevant speech and video segments from these videos. To our knowledge, the VoxBlink dataset is one of the largest speaker recognition datasets available. Secondly, we conduct a series of experiments based on different backbones trained on a mix of the VoxCeleb2 and the VoxBlink-Clean. Our findings highlight a notable performance improvement, ranging from 13% to 30%, across different backbone architectures upon integrating our dataset for training. The dataset will be made publicly available shortly.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一个新的和广泛的说话人验证数据集，包括噪音38k个人/1.45万个语音（VoxBlink）和相对清晰的18k个人/1.02万个语音（VoxBlink-Clean） для训练。首先，我们积累了60,000个用户的名单和他们的aviator，然后下载了YouTube上的短视频。我们然后建立了一个自动化和可扩展的管道，以提取视频和语音段落。根据我们所知，VoxBlink数据集是目前最大的说话人识别数据集之一。其次，我们进行了基于不同的后准据体系的实验，发现在将我们的数据集用于训练时，其性能提升范围为13%到30%。这些数据将在不久的将来公开。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder"><a href="#Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder" class="headerlink" title="Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder"></a>Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08488">http://arxiv.org/abs/2308.08488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mispchallenge/misp-icme-avsr">https://github.com/mispchallenge/misp-icme-avsr</a></li>
<li>paper_authors: Yusheng Dai, Hang Chen, Jun Du, Xiaofei Ding, Ning Ding, Feijun Jiang, Chin-Hui Lee</li>
<li>for: 本研究旨在提高自动语音识别系统的音视频联合识别系统（AVSR）性能，并在预训练和精度调整框架下实现这一目标。</li>
<li>methods: 本研究提出了两种新技术来提高AVSR的性能，包括利用叙述形态学生 lip shapes 和 syllable-level subword units 的相关性来确定准确的帧级句子界限，以及使用主要训练参数进行多个跨Modal的注意力层来充分利用多Modal的共轭性。</li>
<li>results: 实验结果表明，使用这两种技术可以提高AVSR系统的性能，并在MISP2021-AVSR数据集上达到比 estado-of-the-art 系统更高的性能水平，使用的训练数据量也相对较少。<details>
<summary>Abstract</summary>
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the MISP2021-AVSR data set show the effectiveness of the two proposed techniques. Together, using only a relatively small amount of training data, the final system achieves better performances than state-of-the-art systems with more complex front-ends and back-ends.
</details>
<details>
<summary>摘要</summary>
近期研究发现，自动语音识别系统到Audio-Visual语音识别系统在端到端框架下有轻微的性能提升，但是存在不匹配的协调速率和特殊的输入表示之间的问题。在这篇论文中，我们提出了两种新的技巧来提高Audio-Visual语音识别（AVSR）在预训练和精度调整训练框架下。首先，我们探索了拼音和字节水平的叙述单元之间的相关性，以确定良好的帧级叙述边界。这使得视频和音频流之间的对齐变得精准，从而提高了视频和音频流之间的混合。其次，我们提出了一种受主要训练参数 guideline的Audio-Visual混合抽象Encoder（CMFE）神经网络，以便在多个跨模态扩散层中使用主要训练参数，以便充分利用多模态的共轭性。实验表明，使用这两种技巧可以提高系统的性能，并且只需使用相对较少的训练数据。最终系统可以在与更复杂的前端和后端的系统相比，达到更好的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06981">http://arxiv.org/abs/2308.06981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了2023年 зву隔离挑战（SDX’23）的电影幂分融合（CDX）轨迹。</li>
<li>methods: 论文详细介绍了比赛的结构和使用的数据集，特别是新构建的CDXDB23隐藏数据集，以及参与者所采用的最成功的方法。</li>
<li>results: 相比干杯餐 fork基线，专门在 simulated Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而开放排行榜上的最佳系统则看到了5.7dB的显著提升。<details>
<summary>Abstract</summary>
This paper summarizes the cinematic demixing (CDX) track of the Sound Demixing Challenge 2023 (SDX'23). We provide a comprehensive summary of the challenge setup, detailing the structure of the competition and the datasets used. Especially, we detail CDXDB23, a new hidden dataset constructed from real movies that was used to rank the submissions. The paper also offers insights into the most successful approaches employed by participants. Compared to the cocktail-fork baseline, the best-performing system trained exclusively on the simulated Divide and Remaster (DnR) dataset achieved an improvement of 1.8dB in SDR whereas the top performing system on the open leaderboard, where any data could be used for training, saw a significant improvement of 5.7dB.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了2023年 зву频分离挑战（SDX'23）的电影式分离（CDX）轨迹。我们提供了竞赛设置的完整摘要，包括竞赛结构和使用的数据集。特别是，我们详细介绍了CDXDB23，一个新的隐藏数据集，从真实电影中构建而成，用于评估参赛系统的表现。文章还提供了参与者采用的最成功方法的折衔。相比干杯叉基线，专门在 simulate 的 Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而在开放排行榜上，任何数据可以用于训练的系统则得到了显著的5.7dB的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06979">http://arxiv.org/abs/2308.06979</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfturbo/mvsep-mdx23-music-separation-model">https://github.com/zfturbo/mvsep-mdx23-music-separation-model</a></li>
<li>paper_authors: Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了Sound Demixing Challenge（SDX’23）的音乐分离（MDX）轨迹。</li>
<li>methods: 论文介绍了MDX系统在训练数据中出现错误的情况下的训练方法，并提出了一种对MDX系统训练数据设计的错误形式化。</li>
<li>results: 论文描述了SDXDB23_LabelNoise和SDXDB23_Bleeding1两个新的数据集，以及在SDX’23中获得最高分的方法。此外，论文还对上一届音乐分离比赛（Music Demixing Challenge 2021）的赛果进行了直接比较，发现当用MDXDB21进行评估时，最佳实现在标准MSS形式下获得了1.6dB的信号至噪声比提高。此外，论文还进行了基于人类听觉评价的听测，并对系统的感知质量进行了报告。最后，论文提供了比赛组织方式的反思和未来版本的展望。<details>
<summary>Abstract</summary>
This paper summarizes the music demixing (MDX) track of the Sound Demixing Challenge (SDX'23). We provide a summary of the challenge setup and introduce the task of robust music source separation (MSS), i.e., training MSS models in the presence of errors in the training data. We propose a formalization of the errors that can occur in the design of a training dataset for MSS systems and introduce two new datasets that simulate such errors: SDXDB23_LabelNoise and SDXDB23_Bleeding1. We describe the methods that achieved the highest scores in the competition. Moreover, we present a direct comparison with the previous edition of the challenge (the Music Demixing Challenge 2021): the best performing system under the standard MSS formulation achieved an improvement of over 1.6dB in signal-to-distortion ratio over the winner of the previous competition, when evaluated on MDXDB21. Besides relying on the signal-to-distortion ratio as objective metric, we also performed a listening test with renowned producers/musicians to study the perceptual quality of the systems and report here the results. Finally, we provide our insights into the organization of the competition and our prospects for future editions.
</details>
<details>
<summary>摘要</summary>
In Simplified Chinese:这篇文章介绍了Sound Demixing Challenge（SDX'23）的音乐分离（MDX）轨迹，包括音乐来源分离（MSS）的Robust Training数据集的设计和两个新的数据集：SDXDB23_LabelNoise和SDXDB23_Bleeding1。文章介绍了在比赛中得分最高的方法，并对上一届音乐分离挑战（Music Demixing Challenge 2021）的赛果进行比较。结果显示，使用标准MSS形式化的最佳系统在MDXDB21上的信号至噪比高于上一届赛事的冠军的赛果。此外，文章还执行了由知名的制作人/音乐人组织的听力测试，以研究系统的主观质量。最后，文章提供了比赛组织和未来版本的前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.SD_2023_08_14/" data-id="clly3dw0y009l0988hewih7xu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/eess.IV_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/eess.IV_2023_08_14/">eess.IV - 2023-08-14 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data"></a>Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07214">http://arxiv.org/abs/2308.07214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjeewee Prasad Koirala, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这篇论文旨在利用深度学习对多Modalities MRI数据进行脑肿瘤精准分割，以优化在 SUB-SAHARAN AFRICA 患者群体中的诊断和治疗。</li>
<li>methods: 这篇论文提出了一种ensemble方法，包括eleven个不同的变种，基于三种核心架构：UNet3D、ONet3D 和 SphereNet3D，以及修改的损失函数。</li>
<li>results: 研究发现， ensemble方法可以在多Modalities MRI数据上提高脑肿瘤分割精度，特别是在 age-和 population-based 分割模型方面。 Results表明， ensemble方法的 dice分数为 0.82、0.82 和 0.87 分别用于提高脑肿瘤、脑肿瘤核心和全脑肿瘤标签。<details>
<summary>Abstract</summary>
Brain tumors, particularly glioblastoma, continue to challenge medical diagnostics and treatments globally. This paper explores the application of deep learning to multi-modality magnetic resonance imaging (MRI) data for enhanced brain tumor segmentation precision in the Sub-Saharan Africa patient population. We introduce an ensemble method that comprises eleven unique variations based on three core architectures: UNet3D, ONet3D, SphereNet3D and modified loss functions. The study emphasizes the need for both age- and population-based segmentation models, to fully account for the complexities in the brain. Our findings reveal that the ensemble approach, combining different architectures, outperforms single models, leading to improved evaluation metrics. Specifically, the results exhibit Dice scores of 0.82, 0.82, and 0.87 for enhancing tumor, tumor core, and whole tumor labels respectively. These results underline the potential of tailored deep learning techniques in precisely segmenting brain tumors and lay groundwork for future work to fine-tune models and assess performance across different brain regions.
</details>
<details>
<summary>摘要</summary>
脑肿，特别是 glioblastoma，仍然在全球医疗领域面临挑战。这篇论文探讨了深度学习在多Modal magnetic resonance imaging（MRI）数据上进行脑肿分 segmentation的精度提高。我们引入了一个ensemble方法，包括11个独特的变种，基于三个核心体系：UNet3D、ONet3D和SphereNet3D，以及修改的损失函数。该研究强调了需要根据年龄和人口进行分 segmentation模型，以全面考虑脑肿的复杂性。我们的发现表明， ensemble方法，将不同的体系结合起来，表现出了提高评价指标的效果。特别是，结果显示 dice分数为0.82、0.82和0.87，用于加强肿体、肿体核心和整个肿体标签。这些结果高亮了深度学习技术在精度地分 segmentation脑肿的潜在优势，并为未来细化模型和评价不同脑区的表现提供了基础。
</details></li>
</ul>
<hr>
<h2 id="SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation"><a href="#SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation" class="headerlink" title="SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"></a>SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07156">http://arxiv.org/abs/2308.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren</li>
<li>for: 本研究探讨了Segment Anything Model（SAM）在 роботиче外科中的 robustness和零shot泛化能力。</li>
<li>methods: 本研究使用了SAM模型，并对其进行了多种场景探讨，包括提示和无提示的情况，以及不同的提示方法。</li>
<li>results: 研究发现，SAM模型在提示情况下表现出了很好的零shot泛化能力，但在无提示情况下或者 Instrument部分重叠时，模型很难正确地分类Instrument。此外，模型在复杂的外科手术场景下也表现不佳，尤其是在血液、反射、模糊和阴影等情况下。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) serves as a fundamental model for semantic segmentation and demonstrates remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examine SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explore different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compare the performance of SAM with state-of-the-art supervised models. We conduct all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggles to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempt to fine-tune SAM using Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 是一种基本模型 для semantics segmentation，它在各种下游场景中表现出了很好的普适性。在这个实验性研究中，我们研究了SAM在 робо学手术场景中的 robustness和零shot普适性。我们全面探讨了不同的场景，包括提示和无提示的情况，以及 bounding box 和点based提示方法。此外，我们还评估了 SAM 与当前顶尖指导学习模型的性能比较。我们在 MICCAI EndoVis 2017 和 2018 挑战中获得的两个 robotic instrument segmentation 数据集进行了所有的实验。我们的广泛的评估结果表明，虽然 SAM 在 bounding box 提示下显示出了remarkable零shot普适性，但在点based提示和无提示情况下，它很难正确地分类整个工具。此外，我们的资深图示表明，当工具在同一个 bounding box 内或者点based提示情况下，模型会预测错误的部分或者完全错过certain parts of the instrument mask（例如，钩子、臂部）。实际上，SAM 在复杂的外科手术场景中，即血肉泛滥、反射、模糊和抑吸的情况下，也很难分类工具。此外，SAM 对数据损害不具备充分的Robustness，无法保持高性能。为了解决这些问题，我们尝试使用 LoRA 进行微调，并提出了 SurgicalSAM，它可以在无提示情况下进行类别 маска预测。因此，我们可以 argue ，无需进一步领域特定的微调，SAM 不够准备于下游外科任务。
</details></li>
</ul>
<hr>
<h2 id="FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving"><a href="#FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving" class="headerlink" title="FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving"></a>FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07104">http://arxiv.org/abs/2308.07104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhonghuayi/focusflow_official">https://github.com/zhonghuayi/focusflow_official</a></li>
<li>paper_authors: Zhonghua Yi, Hao Shi, Kailun Yang, Qi Jiang, Yaozu Ye, Ze Wang, Kaiwei Wang</li>
<li>for: 提高数据驱动的光流估算精度，特别是关键点方面。</li>
<li>methods: 提出点 clouds模型，并使用混合损失函数和特定点控制损失函数进行多个点精度的监督。 Condition Control Encoder (CCE) 将经典特征编码器替换为 Condition Feature Encoder (CFE)，并将帧特征编码器 (FFE) 与 CFE 进行控制相互传输。</li>
<li>results: 与普通的数据驱动光流估算方法相比，FocusFlow 在关键点方面提高了精度，并且可以与普通的特征编码器进行比较，在整个帧上也能达到类似或更高的性能。<details>
<summary>Abstract</summary>
Key-point-based scene understanding is fundamental for autonomous driving applications. At the same time, optical flow plays an important role in many vision tasks. However, due to the implicit bias of equal attention on all points, classic data-driven optical flow estimation methods yield less satisfactory performance on key points, limiting their implementations in key-point-critical safety-relevant scenarios. To address these issues, we introduce a points-based modeling method that requires the model to learn key-point-related priors explicitly. Based on the modeling method, we present FocusFlow, a framework consisting of 1) a mix loss function combined with a classic photometric loss function and our proposed Conditional Point Control Loss (CPCL) function for diverse point-wise supervision; 2) a conditioned controlling model which substitutes the conventional feature encoder by our proposed Condition Control Encoder (CCE). CCE incorporates a Frame Feature Encoder (FFE) that extracts features from frames, a Condition Feature Encoder (CFE) that learns to control the feature extraction behavior of FFE from input masks containing information of key points, and fusion modules that transfer the controlling information between FFE and CFE. Our FocusFlow framework shows outstanding performance with up to +44.5% precision improvement on various key points such as ORB, SIFT, and even learning-based SiLK, along with exceptional scalability for most existing data-driven optical flow methods like PWC-Net, RAFT, and FlowFormer. Notably, FocusFlow yields competitive or superior performances rivaling the original models on the whole frame. The source code will be available at https://github.com/ZhonghuaYi/FocusFlow_official.
</details>
<details>
<summary>摘要</summary>
“键点基本Scene理解是自动驾驶应用的基础。同时，光流扮演了许多视觉任务中重要的角色。然而，由于预设所有点都受到同等的注意力， класи型数据驱动的光流估计方法对键点的表现不如预期，从而限制它们在键点敏感的安全相关enario中的实现。为解决这些问题，我们介绍了一个点 cloud Modeling 方法，让模型Explicitly learn键点相关的先验知识。基于这个方法，我们发表了FocusFlow框架，包括以下几个部分：1) 一个mix损失函数和类别摄影损失函数以及我们提出的Conditional Point Control Loss (CPCL)函数 для多点精确指导; 2) 一个受控制的模型，将传统的Feature Encoder取代为我们提出的Condition Control Encoder (CCE)。CCE包括Frame Feature Encoder (FFE)、Condition Feature Encoder (CFE) 和融合模块，从输入mask中学习控制FFE的特征提取行为，并将控制信息转移到FFE和CFE之间。我们的FocusFlow框架在不同的键点上显示出惊人的表现，包括ORB、SIFT 和甚至学习式SiLK，并且具有卓越的扩展性，可以与现有的大多数数据驱动的光流方法相容。尤其是，FocusFlow在整幅图上表现竞争或超越原始模型。代码将在https://github.com/ZhonghuaYi/FocusFlow_official中公开。”
</details></li>
</ul>
<hr>
<h2 id="When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation"><a href="#When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation" class="headerlink" title="When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation"></a>When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07093">http://arxiv.org/abs/2308.07093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Zhiyong Wang, Yulin Huang, Junjie Wu, Haiguang Yang, Jianyu Yang</li>
<li>for: 本研究旨在提出一种基于多任务学习的Synthetic Aperture Radar（SAR）自动目标识别（ATR）方法，以实现精准的目标类别和精确的目标形态同时识别。</li>
<li>methods: 该方法基于深度学习理论，提出了一种新的多任务学习框架，包括两个主要结构：编码器和解码器。编码器用于抽取不同缩放级别的图像特征，而解码器则是一个任务特有的结构，通过使用这些抽取的特征进行适应性和优化地满足不同识别和分割任务的特征需求。</li>
<li>results: 基于Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集的实验结果表明，提出的方法在识别和分割任务中具有优越性。<details>
<summary>Abstract</summary>
With the recent advances of deep learning, automatic target recognition (ATR) of synthetic aperture radar (SAR) has achieved superior performance. By not being limited to the target category, the SAR ATR system could benefit from the simultaneous extraction of multifarious target attributes. In this paper, we propose a new multi-task learning approach for SAR ATR, which could obtain the accurate category and precise shape of the targets simultaneously. By introducing deep learning theory into multi-task learning, we first propose a novel multi-task deep learning framework with two main structures: encoder and decoder. The encoder is constructed to extract sufficient image features in different scales for the decoder, while the decoder is a tasks-specific structure which employs these extracted features adaptively and optimally to meet the different feature demands of the recognition and segmentation. Therefore, the proposed framework has the ability to achieve superior recognition and segmentation performance. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, experimental results show the superiority of the proposed framework in terms of recognition and segmentation.
</details>
<details>
<summary>摘要</summary>
Our approach is based on a deep learning framework with two main structures: an encoder and a decoder. The encoder is designed to extract comprehensive image features at multiple scales, while the decoder is a task-specific structure that adaptively and optimally utilizes these features to meet the diverse demands of recognition and segmentation. This allows our framework to achieve superior performance in both recognition and segmentation.Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset demonstrate the superiority of our proposed framework. With the ability to accurately recognize and precisely segment targets, our approach offers a significant improvement over traditional SAR ATR methods.
</details></li>
</ul>
<hr>
<h2 id="Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks"><a href="#Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks" class="headerlink" title="Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks"></a>Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07003">http://arxiv.org/abs/2308.07003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fisch, Stefan Zumdick, Carlotta Barkhau, Daniel Emden, Jan Ernsting, Ramona Leenings, Kelvin Sarink, Nils R. Winter, Benjamin Risse, Udo Dannlowski, Tim Hahn</li>
<li>for: 这个论文主要是为了提出一个高精度、快速的Magnetic Resonance Imaging（MRI）数据中的脑部分 segmentation工具，以取代传统的脑部分分类方法。</li>
<li>methods: 这个论文使用了现代的深度学习方法，包括LinkNet的现代UNet架构，在两个阶段预测过程中进行预测。这将提高了脑部分分类的性能，在测验中得到了一个新的州OF-THE-ART性能， median Dice score（DSC）为99.0%，比现有的模型高出2.2%和1.9%。</li>
<li>results: 这个论文的模型可以实现高精度的脑部分分类，Dice score（DSC）高于96.9%，并且更敏感于噪音。此外，这个模型可以将脑部分分类的时间加速到了约10倍，可以在低级硬件上处理一个数据仅需2秒钟。<details>
<summary>Abstract</summary>
Brain extraction in magnetic resonance imaging (MRI) data is an important segmentation step in many neuroimaging preprocessing pipelines. Image segmentation is one of the research fields in which deep learning had the biggest impact in recent years enabling high precision segmentation with minimal compute. Consequently, traditional brain extraction methods are now being replaced by deep learning-based methods. Here, we used a unique dataset comprising 568 T1-weighted (T1w) MR images from 191 different studies in combination with cutting edge deep learning methods to build a fast, high-precision brain extraction tool called deepbet. deepbet uses LinkNet, a modern UNet architecture, in a two stage prediction process. This increases its segmentation performance, setting a novel state-of-the-art performance during cross-validation with a median Dice score (DSC) of 99.0% on unseen datasets, outperforming current state of the art models (DSC = 97.8% and DSC = 97.9%). While current methods are more sensitive to outliers, resulting in Dice scores as low as 76.5%, deepbet manages to achieve a Dice score of > 96.9% for all samples. Finally, our model accelerates brain extraction by a factor of ~10 compared to current methods, enabling the processing of one image in ~2 seconds on low level hardware.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 数据中的脑部EXTRACTION是许多神经成像预处理管道中重要的 segmentation 步骤。图像 segmentation 是深度学习在过去几年中对神经成像领域产生了最大的影响，使得传统的脑部EXTRACTION 方法被深度学习基于的方法所取代。在本文中，我们使用了568张T1-weighted (T1w) MRI图像和 cutting-edge deep learning 方法建立了一个快速、高精度的脑部EXTRACTION 工具 called deepbet。deepbet 使用了 LinkNet，一种现代的 U-Net 架构，在两个阶段预测过程中。这使得它的 segmentation 性能得到了提高，在跨验证中 median Dice 分数 (DSC) 为 99.0%，超过当前的状态对照模型 (DSC = 97.8%和DSC = 97.9%)。而现有方法更敏感于异常值，导致 Dice 分数只有 76.5%，而 deepbet 则能够达到 > 96.9% 的 Dice 分数 для所有样本。最后，我们的模型将脑部EXTRACTION 加速了约10倍，使得一个图像只需要 ~2秒钟的处理时间。
</details></li>
</ul>
<hr>
<h2 id="How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation"><a href="#How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation" class="headerlink" title="How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation"></a>How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06964">http://arxiv.org/abs/2308.06964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parinaz Roshanzamir, Hassan Rivaz, Joshua Ahn, Hamza Mirza, Neda Naghdi, Meagan Anstruther, Michele C. Battié, Maryse Fortin, Yiming Xiao</li>
<li>for: This paper aims to explore the relationship between inter-rater variability and uncertainties in deep learning models for medical image segmentation, and to compare the performance of different label fusion strategies and DL models.</li>
<li>methods: The paper uses test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties, and compares the performance of UNet and TransUNet with two label fusion strategies.</li>
<li>results: The study reveals the interplay between inter-rater variability and uncertainties, and shows that choices of label fusion strategies and DL models can affect the resulting segmentation performance.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是探讨医学影像分割任务中间质量标注人员之间的差异对深度学习模型的不确定性的关系，以及不同的标签汇集策略和深度学习模型的性能比较。</li>
<li>methods: 该论文使用测试时数据增强（TTA）、测试时dropout（TTD）和深度ensemble来测量 aleatoric 和 epistemic 不确定性，并比较 UNet 和 TransUNet 的性能。</li>
<li>results: 研究发现，医学影像分割任务中间质量标注人员之间的差异会影响深度学习模型的不确定性，并且选择不同的标签汇集策略和深度学习模型可以affect segmentation性能。<details>
<summary>Abstract</summary>
Recent developments in deep learning (DL) techniques have led to great performance improvement in medical image segmentation tasks, especially with the latest Transformer model and its variants. While labels from fusing multi-rater manual segmentations are often employed as ideal ground truths in DL model training, inter-rater variability due to factors such as training bias, image noise, and extreme anatomical variability can still affect the performance and uncertainty of the resulting algorithms. Knowledge regarding how inter-rater variability affects the reliability of the resulting DL algorithms, a key element in clinical deployment, can help inform better training data construction and DL models, but has not been explored extensively. In this paper, we measure aleatoric and epistemic uncertainties using test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to explore their relationship with inter-rater variability. Furthermore, we compare UNet and TransUNet to study the impacts of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, affected by choices of label fusion strategies and DL models.
</details>
<details>
<summary>摘要</summary>
In this paper, we use test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties and explore their relationship with inter-rater variability. We also compare UNet and TransUNet to study the impact of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, which is influenced by choices of label fusion strategies and DL models.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Stress-Testing-in-Medical-Image-Classification"><a href="#Robustness-Stress-Testing-in-Medical-Image-Classification" class="headerlink" title="Robustness Stress Testing in Medical Image Classification"></a>Robustness Stress Testing in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06889">http://arxiv.org/abs/2308.06889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobarakol/robustness_stress_testing">https://github.com/mobarakol/robustness_stress_testing</a></li>
<li>paper_authors: Mobarakol Islam, Zeju Li, Ben Glocker</li>
<li>For: 评估医学图像疾病检测算法的 клиниче验证性能。* Methods: 使用进行挑战测试来评估模型的可靠性和不同类型和地区的表现差异。* Results: 表明了一些模型在不同的图像挑战测试中的Robustness和公平性。还发现预训练特征对下游的可靠性产生了重要的影响。<details>
<summary>Abstract</summary>
Deep neural networks have shown impressive performance for image-based disease detection. Performance is commonly evaluated through clinical validation on independent test sets to demonstrate clinically acceptable accuracy. Reporting good performance metrics on test sets, however, is not always a sufficient indication of the generalizability and robustness of an algorithm. In particular, when the test data is drawn from the same distribution as the training data, the iid test set performance can be an unreliable estimate of the accuracy on new data. In this paper, we employ stress testing to assess model robustness and subgroup performance disparities in disease detection models. We design progressive stress testing using five different bidirectional and unidirectional image perturbations with six different severity levels. As a use case, we apply stress tests to measure the robustness of disease detection models for chest X-ray and skin lesion images, and demonstrate the importance of studying class and domain-specific model behaviour. Our experiments indicate that some models may yield more robust and equitable performance than others. We also find that pretraining characteristics play an important role in downstream robustness. We conclude that progressive stress testing is a viable and important tool and should become standard practice in the clinical validation of image-based disease detection models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/eess.IV_2023_08_14/" data-id="clly3dw2i00ek0988bs15763b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.LG_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/cs.LG_2023_08_13/">cs.LG - 2023-08-13 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP"><a href="#Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP" class="headerlink" title="Faithful to Whom? Questioning Interpretability Measures in NLP"></a>Faithful to Whom? Questioning Interpretability Measures in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06795">http://arxiv.org/abs/2308.06795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz</li>
<li>for: 这篇论文主要是为了量化模型解释性的方法。</li>
<li>methods: 这篇论文使用了基于循环屏蔽输入Token的方法来计算 faithfulness 度量。</li>
<li>results: 论文发现现有的 faithfulness 度量不适合比较不同的神经网络文本分类器的解释性，因为屏蔽输入的样本频繁出现在训练时未看到的分布外。<details>
<summary>Abstract</summary>
A common approach to quantifying model interpretability is to calculate faithfulness metrics based on iteratively masking input tokens and measuring how much the predicted label changes as a result. However, we show that such metrics are generally not suitable for comparing the interpretability of different neural text classifiers as the response to masked inputs is highly model-specific. We demonstrate that iterative masking can produce large variation in faithfulness scores between comparable models, and show that masked samples are frequently outside the distribution seen during training. We further investigate the impact of adversarial attacks and adversarial training on faithfulness scores, and demonstrate the relevance of faithfulness measures for analyzing feature salience in text adversarial attacks. Our findings provide new insights into the limitations of current faithfulness metrics and key considerations to utilize them appropriately.
</details>
<details>
<summary>摘要</summary>
一种常见的方法量化模型解释性是计算基于 iteratively 掩码输入Token 的 faithfulness 度量。然而，我们显示这些度量不适合比较不同的神经网络文本分类器的解释性，因为掩码输入的响应是高度特定的。我们示出了 iterative 掩码可以导致大量的 faithfulness 分数变化，并且masked 样本 часто处于训练过程中未见过的分布之外。我们进一步研究了对 faithfulness 度量的影响，以及对文本 adversarial 攻击的分析。我们的发现为现有的 faithfulness 度量带来新的认识和关键考虑因素。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-at-a-Fraction-with-Pruned-Quaternions"><a href="#Neural-Networks-at-a-Fraction-with-Pruned-Quaternions" class="headerlink" title="Neural Networks at a Fraction with Pruned Quaternions"></a>Neural Networks at a Fraction with Pruned Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06780">http://arxiv.org/abs/2308.06780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/quartLT22">https://github.com/smlab-niser/quartLT22</a></li>
<li>paper_authors: Sahel Mohammad Iqbal, Subhankar Mishra</li>
<li>for: 这个研究旨在测试适用于具有限制性的处理器的现代神经网络，以及使用高维度数据嵌入（如复数或四元数）来实现更好的优化。</li>
<li>methods: 这个研究使用了剪除来简化神经网络中的 Parameters，并在分类任务上进行了不同架构和数据集的实验。</li>
<li>results: 研究发现，在某些架构和任务上，将神经网络转换为复数值的版本可以在具有很高简化水平的情况下提供更高的准确性。例如，在CIFAR-10中使用Conv-4架构，将 Parameters 剪除至3%以下，复数值版本可以与原始模型相比提高超过10%的准确性。<details>
<summary>Abstract</summary>
Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at $3\%$ of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than $10\%$. Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.
</details>
<details>
<summary>摘要</summary>
现代神经网络的参数数量逐渐增加，这使得具有有限计算能力的设备上进行训练和推理变得困难。剪枝技术可以将不必要的权重从神经网络中移除，以降低训练和推理的资源需求。此外，在多维输入数据的机器学习任务上，使用高维数域嵌入，如复数或四元数，可以降低参数数量而保持准确性。在这种情况下，我们对不同的架构和数据集进行了剪枝和权重融合的实验。我们发现，在某些架构上，在非常高的精灵度水平上，使用四元数模型可以提高准确性，比如在CIFAR-10上使用Conv-4，在3%的参数数量下，剪枝后的四元数模型的准确性高于剪枝后的实数模型的10%以上。各种网络架构和数据集的实验表明，在极其有限的资源环境下，一个稀疏的四元数网络可能比同样的架构的实数稀疏网络更适合进行部署。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations"><a href="#A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations" class="headerlink" title="A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations"></a>A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06767">http://arxiv.org/abs/2308.06767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrcheng1066/awesome-pruning">https://github.com/hrcheng1066/awesome-pruning</a></li>
<li>paper_authors: Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</li>
<li>for: 本文提供了一份涵盖现有研究工作的深度神经网络减少报告，以便更好地理解现有的方法和技术。</li>
<li>methods: 本文分类了现有的减少方法，包括一般&#x2F;特定速度减少、 WHEN TO PRUNE、HOW TO PRUNE 和减少与其他压缩技术的融合。</li>
<li>results: 本文进行了七对对照设定的比较分析，探讨了不同级别的监督和不同应用场景，以便更好地了解现有方法的共同点和区别。<details>
<summary>Abstract</summary>
Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络，特别是最近的大型语言模型，具有庞大的计算和存储资源需求。为实现资源有限环境中部署现代模型和加速推理时间，研究人员已经不断探索剪裁技术作为神经网络压缩的流行研究方向。然而，目前的相关评论综述缺乏。为解决这问题，在这篇评论中，我们提供了一份全面的评论综述，分为以下四个方面：1) 通用/特定加速，2) 何时剪裁，3) 如何剪裁，和4) 剪裁与其他压缩技术的融合。然后，我们对七对对比设定进行了仔细的比较分析（例如，无结构/结构），并探讨了emerging topics，如后处理剪裁、不同水平的监督剪裁和更广泛的应用（例如，防御性鲁棒性），以抛光现有方法的相似和不同，并为未来的研究铺垫基础。为便于未来的研究，我们建立了一个 curaated 的数据集、网络和评估集。最后，我们提供了一些有价值的建议，包括选择剪裁方法和未来研究方向。我们在 GitHub 上建立了一个存储库，请参考 <https://github.com/hrcheng1066/awesome-pruning>。
</details></li>
</ul>
<hr>
<h2 id="Conic-Descent-Redux-for-Memory-Efficient-Optimization"><a href="#Conic-Descent-Redux-for-Memory-Efficient-Optimization" class="headerlink" title="Conic Descent Redux for Memory-Efficient Optimization"></a>Conic Descent Redux for Memory-Efficient Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07343">http://arxiv.org/abs/2308.07343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingcong Li, Georgios B. Giannakis</li>
<li>for: 该论文旨在提高首项壳programming的效率和精度，并应用于信号处理和机器学习领域。</li>
<li>methods: 该论文提出了三个方面的改进：一是具有直观的几何解释，二是基于对偶问题的理论基础，三是一种新的批处理算法。</li>
<li>results: 研究发现，首项壳programming可以通过增加矩阵的权重来加速对偶解释，并且可以通过采用偏好的初始值来加速搜索过程。<details>
<summary>Abstract</summary>
Conic programming has well-documented merits in a gamut of signal processing and machine learning tasks. This contribution revisits a recently developed first-order conic descent (CD) solver, and advances it in three aspects: intuition, theory, and algorithmic implementation. It is found that CD can afford an intuitive geometric derivation that originates from the dual problem. This opens the door to novel algorithmic designs, with a momentum variant of CD, momentum conic descent (MOCO) exemplified. Diving deeper into the dual behavior CD and MOCO reveals: i) an analytically justified stopping criterion; and, ii) the potential to design preconditioners to speed up dual convergence. Lastly, to scale semidefinite programming (SDP) especially for low-rank solutions, a memory efficient MOCO variant is developed and numerically validated.
</details>
<details>
<summary>摘要</summary>
带有较好的记录的圆形编程在信号处理和机器学习任务中具有良好的优点。本贡献将最近开发的首览圆形下降（CD）解决方案进行三个方面的提高：直观、理论和算法实现。研究发现CD可以从对准问题的 dual 问题中得到直观的几何 derivation，这打开了新的算法设计的门户，例如旋转圆形下降（MOCO）。钻 deeper into CD和MOCO的双重行为，发现：i) 可以分析正确的停止标准；ii) 可以设计加速对准速度的预conditioners。最后，为优化semidefinite程序（SDP），尤其是低维解决方案，我们开发了内存高效的MOCO变体并 NUMERICALLY 验证了其正确性。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Class-incremental-Learning-A-Survey"><a href="#Few-shot-Class-incremental-Learning-A-Survey" class="headerlink" title="Few-shot Class-incremental Learning: A Survey"></a>Few-shot Class-incremental Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06764">http://arxiv.org/abs/2308.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghua Zhang, Li Liu, Olli Silven, Matti Pietikäinen, Dewen Hu</li>
<li>for: 这篇论文的目的是为了提供对几拟学习（Few-shot Class-Incremental Learning，FSCIL）的系统性和全面的综述。</li>
<li>methods: 这篇论文使用了多种方法来探讨FSCIL，包括问题定义、主要挑战的不可靠的实验准确风险和稳定性-多样性矛盾、通用方案和相关的增量学习和几拟学习方法。</li>
<li>results: 这篇论文提供了各种FSCIL中的分类方法和对象检测方法，包括数据基于、结构基于和优化基于的方法，以及 anchor-free和 anchor-based的对象检测方法。<details>
<summary>Abstract</summary>
Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in machine learning, as it necessitates the continuous learning of new classes from sparse labeled training samples without forgetting previous knowledge. While this field has seen recent progress, it remains an active area of exploration. This paper aims to provide a comprehensive and systematic review of FSCIL. In our in-depth examination, we delve into various facets of FSCIL, encompassing the problem definition, the discussion of primary challenges of unreliable empirical risk minimization and the stability-plasticity dilemma, general schemes, and relevant problems of incremental learning and few-shot learning. Besides, we offer an overview of benchmark datasets and evaluation metrics. Furthermore, we introduce the classification methods in FSCIL from data-based, structure-based, and optimization-based approaches and the object detection methods in FSCIL from anchor-free and anchor-based approaches. Beyond these, we illuminate several promising research directions within FSCIL that merit further investigation.
</details>
<details>
<summary>摘要</summary>
《几个shot类增长学习（FSCIL）》是机器学习中的一个特殊挑战，它需要不断学习新的类型从罕见的标签训练样本中学习，而不会忘记之前的知识。尽管这个领域已经有了一定的进步，但仍然是一个活跃的研究领域。本文的目的是提供了FSCIL的全面和系统性的综述。在我们的深入检查中，我们探讨了FSCIL的各个方面，包括问题定义、不可靠的实际风险最小化和稳定-柔软之间的矛盾、通用方案和相关的增量学习和几个shot学习问题。此外，我们介绍了FSCIL中的数据集和评价指标。此外，我们还介绍了FSCIL中的分类方法，包括数据基于、结构基于和优化基于的方法，以及对象检测方法，包括固定和无固定的方法。此外，我们还释明了FSCIL中的一些有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining"><a href="#Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining" class="headerlink" title="Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining"></a>Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06763">http://arxiv.org/abs/2308.06763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Dehghani, Zahra Yazdanparast, Mobin Mohammadi</li>
<li>for: 本研究旨在利用关联规则挖掘技术对 COVID-19 患者症状进行分析，以提供临床医生管理疾病的有用信息。</li>
<li>methods: 本研究使用 Apriori 算法进行关联规则挖掘，分析 COVID-19 患者2875条病例记录，并发现最常见的症状为呼吸困难（72%）、咳嗽（64%）、发热（59%）、衰弱（18%）、 мышьяк（14.5%）和喉咙痛（12%）。</li>
<li>results: 本研究发现，Apriori 算法可以帮助临床医生更好地理解 COVID-19 疾病的表现形式，并提供有价值的信息来帮助他们更好地诊断和治疗疾病。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 records of patient, identified the most common symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and treating it effectively.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对全球造成了毁灭性的影响，负死亡人数和社会经济秩序受到了重大的影响。为了优化决策和分配有限的资源，必须能够识别 COVID-19 的症状和每个患者的严重程度。机器学习算法在医疗领域提供了一种极其有用的工具，特别是在挖掘医疗数据中找到有用信息并导引科学决策。在本文中，我们使用 Apriori 算法来应用关联规则挖掘技术，从 COVID-19 患者的记录中挖掘症状模式。研究使用了 2875 个病人记录，并发现最常见的症状为呼吸停止（72%）、咳嗽（64%）、发热（59%）、衰竭（18%）、肌肉疼痛（14.5%）和喉咙痛（12%）。该方法为临床医生提供了有价值的病理知识，可以帮助他们更好地诊断和治疗疾病。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization"><a href="#Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization" class="headerlink" title="Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization"></a>Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06741">http://arxiv.org/abs/2308.06741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Nasiri, Mansoor Rezghi</li>
<li>for: 这个论文旨在解决多Agent Reinforcement Learning（MARL）中协同努力的挑战，其中Agent具有不同能力和个人策略。</li>
<li>methods: 该论文提出了一种基于镜像下降法的多Agent策略优化算法（HAMDPO），利用多Agent优化减少问题中的策略更新，保证总体性能提高。</li>
<li>results: 该论文通过在Multi-Agent MuJoCo和StarCraftII任务上评估HAMDPO算法，并证明其在相比之前的状态静态算法（HATRPO和HAPPO）的稳定性和性能提高。<details>
<summary>Abstract</summary>
This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文提出了一种新的方法 called Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO)，用于解决合作多代理演算学习（MARL）中的挑战，其中代理有不同的能力和个人策略。HAMDPO算法使用多代理优势分解 Lemma 来有效地更新代理策略，并保证总性的性能提高。通过迭代更新代理策略的近似解决方案，HAMDPO算法保证稳定性和性能提高。此外，该算法可以处理不同类型的动作空间，包括连续和离散动作空间，并在多种 MARL 问题中进行应用。作者们在 Multi-Agent MuJoCo 和 StarCraftII 任务上评估了 HAMDPO 算法，并证明其在当前状态的算法中具有优势，例如 HATRPO 和 HAPPO 等算法。这些结果表明，HAMDPO 是一种有前途的方法，可以解决合作 MARL 问题，并可能扩展到其他难题。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection"><a href="#Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection" class="headerlink" title="Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection"></a>Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06740">http://arxiv.org/abs/2308.06740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenwenmin/wspls">https://github.com/wenwenmin/wspls</a></li>
<li>paper_authors: Wenwen Min, Taosheng Xu, Chris Ding</li>
<li>For: The paper proposes a method for joint sample and feature selection in data fusion using sparse partial least squares (sPLS) with $\ell_\infty&#x2F;\ell_0$-norm constrained weighted sparse PLS (wsPLS) and extends it to multi-view data fusion.* Methods: The proposed method uses the $\ell_\infty&#x2F;\ell_0$-norm constrains to select a subset of samples and the Kurdyka-\L{ojasiewicz}~property to ensure global convergence of the algorithm.* Results: The proposed method is demonstrated to be efficient through numerical and biomedical data experiments, and is shown to outperform traditional sPLS methods in terms of computational efficiency and accuracy.<details>
<summary>Abstract</summary>
Sparse Partial Least Squares (sPLS) is a common dimensionality reduction technique for data fusion, which projects data samples from two views by seeking linear combinations with a small number of variables with the maximum variance. However, sPLS extracts the combinations between two data sets with all data samples so that it cannot detect latent subsets of samples. To extend the application of sPLS by identifying a specific subset of samples and remove outliers, we propose an $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS ($\ell_\infty/\ell_0$-wsPLS) method for joint sample and feature selection, where the $\ell_\infty/\ell_0$-norm constrains are used to select a subset of samples. We prove that the $\ell_\infty/\ell_0$-norm constrains have the Kurdyka-\L{ojasiewicz}~property so that a globally convergent algorithm is developed to solve it. Moreover, multi-view data with a same set of samples can be available in various real problems. To this end, we extend the $\ell_\infty/\ell_0$-wsPLS model and propose two multi-view wsPLS models for multi-view data fusion. We develop an efficient iterative algorithm for each multi-view wsPLS model and show its convergence property. As well as numerical and biomedical data experiments demonstrate the efficiency of the proposed methods.
</details>
<details>
<summary>摘要</summary>
“稀疏部分最小倍数（sPLS）是一种常见的维度减少技术 для数据融合，它通过寻找两个视图中数据样本之间的线性组合来降低维度。然而，sPLS会捕捉两个数据集中所有数据样本的组合，因此无法检测隐藏的样本subset。为了扩展sPLS的应用，我们提出了一种使用 $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS（$\ell_\infty/\ell_0$-wsPLS）方法，该方法可以同时进行样本选择和特征选择。我们证明了 $\ell_\infty/\ell_0$-norm constrains possess Kurdyka-\L{ojasiewicz} 性质，因此可以开发一个全球收敛的算法来解决它。此外，多视图数据中可以有同一个样本集。为此，我们扩展了 $\ell_\infty/\ell_0$-wsPLS 模型，并提出了两种多视图 wsPLS 模型 для多视图数据融合。我们开发了一个高效的迭代算法，并证明其收敛性。numerical和生物医学数据实验表明提出的方法的效率。”Note: Simplified Chinese is a simplified version of Chinese that is used in mainland China and is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data"><a href="#Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data" class="headerlink" title="Probabilistic Imputation for Time-series Classification with Missing Data"></a>Probabilistic Imputation for Time-series Classification with Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06738">http://arxiv.org/abs/2308.06738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout">https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout</a></li>
<li>paper_authors: SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, Juho Lee</li>
<li>for: 用于处理多变量时间序列数据中的缺失值</li>
<li>methods: 使用深度生成模型进行缺失值填充，并使用分类器进行信号分类</li>
<li>results: 通过实验表明，该方法可以有效地处理多变量时间序列数据中的缺失值，并且可以提高信号分类的准确率<details>
<summary>Abstract</summary>
Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that na\"ively combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
多变量时间序列数据在实际应用中通常含有较大的缺失值。现有的主流方法为这种缺失值是单纯地假设缺失值为零、平均值或相邻时间步的值。然而，这些简单策略并不考虑数据生成过程，更重要的是，它们不能有效地捕捉预测中的不确定性，因为缺失值有多个可能性。在这篇论文中，我们提出了一种新的概率 frameworks for classification with multivariate time series data with missing values。我们的模型包括两部分：深度生成模型和分类器。 extending the existing deep generative models to better capture the structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation。分类器部分接受时间序列数据和假设的缺失值，并将时间序列数据分类，并且是通过捕捉多个可能性的假设来捕捉预测中的不确定性。然而，我们发现在直接组合生成模型和分类器时，可能会导致生成模型不生成有用的假设值，从而影响分类的准确性。为了解决这问题，我们提出了一种新的正则化技术，可以推动模型生成有用的假设值，以便于分类。通过对实际时间序列数据进行广泛的实验，我们证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Precipitation-nowcasting-with-generative-diffusion-models"><a href="#Precipitation-nowcasting-with-generative-diffusion-models" class="headerlink" title="Precipitation nowcasting with generative diffusion models"></a>Precipitation nowcasting with generative diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06733">http://arxiv.org/abs/2308.06733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models">https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models</a></li>
<li>paper_authors: Andrea Asperti, Fabio Merizzi, Alberto Paparella, Giorgio Pedrazzi, Matteo Angelinelli, Stefano Colamonaco</li>
<li>for: 本研究使用数字天气预测方法进行气象预测，旨在提高预测的准确性和可靠性。</li>
<li>methods: 本研究使用了泛化激发模型（Diffusion Model）来处理气象预测任务，并通过将多个激发模型 ensemble起来，使用post处理网络来组合可能的天气场景，以获得最终的可能性预测结果。</li>
<li>results: 对于中欧2016-2021年的 hourly 数据，与已有的U-Net模型相比，泛化激发模型在气象预测任务中表现出了明显的优势，并且可以substantially 提高总体性能。<details>
<summary>Abstract</summary>
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction.   In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.
</details>
<details>
<summary>摘要</summary>
近年来，传统的数学方法 для准确的天气预测遭受了深度学习方法的挑战。历史数据库用于短距离和中距离天气预测通常按照一定的正方形格结构进行组织。这种设置与图像非常相似，每个天气变量都可以视为地图或者在考虑时间轴时视为视频。许多类型的生成模型，包括生成对抗网络、自适应变换器和最近的干扰扩散模型，在下一帧预测问题上都有广泛的应用，因此在天气预测标准benchmark上进行测试是自然的。干扩散模型在这个上特别有吸引力，因为天气预测的本质是 probabilistic 的：我们真正 interesseted 的是天气指标的概率分布，而这个分布的期望值是最有可能的预测。在我们的研究中，我们关注了ERA-5数据集的一个特定子集，包括2016年至2021年中欧每小时的数据。在这个上下文中，我们研究了干扩散模型在降水预测中的效果。我们的方法与文献中已有的U-Net模型相比，并使用生成ensemble扩散（GED）模型，该模型使用干扩散模型生成一系列可能的天气场景，然后通过使用后处理网络将这些场景融合成一个可能的预测。与最新的深度学习模型相比，我们的方法在总性表现上substantially outperform了它们。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables"><a href="#Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables" class="headerlink" title="Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables"></a>Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06718">http://arxiv.org/abs/2308.06718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, Kun Zhang</li>
<li>for: 学习 causal structure 中存在 latent variables 的挑战任务 (learning causal structure with latent variables)</li>
<li>methods: 提出 Generalized Independent Noise (GIN) 条件，用于 Linear non-Gaussian acyclic causal models 中包含 latent variables (propose a GIN condition for linear non-Gaussian acyclic causal models with latent variables)</li>
<li>results: 提供 necessary and sufficient graphical criteria of the GIN condition, 并可以快速 estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs) (provide necessary and sufficient graphical criteria of the GIN condition and can efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models)<details>
<summary>Abstract</summary>
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们研究一个复杂的任务：在含有隐变量的情况下学习 causal structure。包括找到隐变量的位置和它们的数量，以及确定隐变量和观测变量之间的 causal 关系。为解决这个问题，我们提议一种 Generalized Independent Noise (GIN) 条件，该条件在 linear non-Gaussian acyclic causal models 中包含隐变量，并且确定了某些观测变量的线性组合和其他观测变量的独立性。具体来说，对两个观测变量 $\mathbf{Y}$ 和 $\mathbf{Z}$，GIN 条件成立只要 $\omega^\intercal \mathbf{Y}$ 和 $\mathbf{Z}$ 独立，其中 $\omega$ 是由 $\mathbf{Y}$ 和 $\mathbf{Z}$ 之间的叠加协方差确定的非零参数向量。然后，我们给出了 linear non-Gaussian acyclic causal models 中 GIN 条件的必要和 suficient 图形 критери产。总之，GIN 条件等价于隐变量集 $\mathcal{S}$ 对于 $\mathbf{Y}$ 的父集（根据 causal 顺序）是独立的，并且 $\mathcal{S}$  separates $\mathbf{Y}$ 和 $\mathbf{Z}$。具体来说，如果没有干扰者，则 causal 关系可以看作一种特殊情况。我们还发现，GIN 条件和隐变量的 causal 结构之间存在着直接的关系。因此，我们可以通过 GIN 条件和一种合理的搜索过程来效率地估计 Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs)，其中隐变量可能也有 causal 关系，并且可能会 suivre 一种层次结构。我们证明了 LiNGLaHs 的下面 causal 结构是可Identifiable的，只要 GIN 条件 在轻量级的假设下成立。实验结果表明了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards"><a href="#Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards" class="headerlink" title="Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards"></a>Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06717">http://arxiv.org/abs/2308.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilgin Dogan, Zuo-Jun Max Shen, Anil Aswani</li>
<li>For: This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal, with the goal of addressing the challenges of information asymmetry and incentive design in real-life scenarios.* Methods: The paper introduces a novel estimator that uses the history of principal’s incentives and agent’s choices to estimate the agent’s unknown rewards, and proposes a data-driven incentive policy within a multi-armed bandit (MAB) framework.* Results: The paper proves the finite-sample consistency of the estimator and a rigorous regret bound for the principal, and provides simulations to demonstrate the applicability of the framework to green energy aggregator contracts.<details>
<summary>Abstract</summary>
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-off between consistently estimating the agent's unknown rewards and maximizing their own utility by offering adaptive incentives to lead the agent. For a non-parametric model, we introduce an estimator whose only input is the history of principal's incentives and agent's choices. We unite this estimator with a proposed data-driven incentive policy within a MAB framework. Without restricting the type of the agent's algorithm, we prove finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent. Lastly, our theoretical results are reinforced by simulations justifying applicability of our framework to green energy aggregator contracts.
</details>
<details>
<summary>摘要</summary>
在实践中，奖励提供者（即主体）通常无法观察奖励的实现，这与许多主体-代理模型不同。这种信息不对称性使得主体在 solely 观察代理人的决策后难以一直估计代理人的未知奖励。这种复杂的设定在各种实际场景中出现，如可再生能源存储合同和个性化奖励。因此，它不仅提出了有趣的理论问题，还具有广泛的实际 relevance。本文研究了一个 repeat 的反对选择游戏，在这个游戏中，一个自私的学习代理人和一个学习主体之间发生对抗。代理人面临一个多重武器bandit（MAB）问题，以最大化他们的预期奖励加上奖励。主体则在代理人学习的同时，训练一个并行的算法，面临着在估计代理人的未知奖励和自己的利用之间的负担。为了不假设代理人的算法类型，我们引入了一个仅基于奖励提供者历史和代理人选择的估计器。我们将这个估计器与一个基于 MAB 框架的数据驱动奖励策略结合。我们证明了该估计器在非 Parametric 模型下的finite-sample consistency和主体的正确偏误 bound。最后，我们通过实验证明了我们的框架在绿色能源聚合合同中的适用性。
</details></li>
</ul>
<hr>
<h2 id="CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation"><a href="#CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation" class="headerlink" title="CDR: Conservative Doubly Robust Learning for Debiased Recommendation"></a>CDR: Conservative Doubly Robust Learning for Debiased Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08461">http://arxiv.org/abs/2308.08461</a></li>
<li>repo_url: None</li>
<li>paper_authors: ZiJie Song, JiaWei Chen, Sheng Zhou, QiHao Shi, Yan Feng, Chun Chen, Can Wang</li>
<li>For: 本研究旨在解决推荐系统中的偏见问题，提高推荐系统的准确性和可靠性。* Methods: 本研究使用了双重Robust学习策略（DR），并提出了一种名为保守双重Robust策略（CDR）来解决偏见问题。* Results: 实验结果表明，CDR可以显著提高推荐系统的性能，同时减少偏见的频率。<details>
<summary>Abstract</summary>
In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.   To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
</details>
<details>
<summary>摘要</summary>
在推荐系统（RS）中，用户行为数据是观察性的而不是实验性的，导致数据中存在广泛的偏见。因此，解决偏见问题成为了推荐系统领域的主要挑战。近些年来，双重稳健学习（DR）已经受到了广泛关注，因为它在性能和稳健性方面表现出色。然而，我们的实验结果表明，现有的DR方法受到了叫做“毒补假设”的问题的影响，即假设的插入数据与事实有很大差异，导致计算结果变得对抗性很强。为解决这个问题，本文提出了保守的双重稳健策略（CDR），通过检验插入数据的均值和方差来筛选掉不符合事实的插入数据。理论分析表明，CDR可以降低方差并提高尾部上限。此外，我们的实验调查表明，CDR可以显著提高性能，并且可以减少毒补假设的频率。
</details></li>
</ul>
<hr>
<h2 id="Learning-on-Graphs-with-Out-of-Distribution-Nodes"><a href="#Learning-on-Graphs-with-Out-of-Distribution-Nodes" class="headerlink" title="Learning on Graphs with Out-of-Distribution Nodes"></a>Learning on Graphs with Out-of-Distribution Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06714">http://arxiv.org/abs/2308.06714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songyyyy/kdd22-oodgat">https://github.com/songyyyy/kdd22-oodgat</a></li>
<li>paper_authors: Yu Song, Donglin Wang</li>
<li>for: 本文旨在解决图像学中存在非标准节点的问题，特别是检测图像中的异常节点和分类其他节点为已知类别。</li>
<li>methods: 本文提出了一种基于图像注意力网络的异常检测方法，称为Out-of-Distribution Graph Attention Network (OODGAT)，它通过显式地模型不同类型节点之间的交互来分离异常节点和正常节点。</li>
<li>results: EXTENSIVE EXPERIMENTS表明，OODGAT在异常检测方面比现有方法大幅提高，而且与已知类别分类中的性能相当或更好。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model which explicitly models the interaction between different kinds of nodes and separate inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 是现代图学习模型中的状态机器。而现有的 GNN 模型在许多图学习任务中表现出色，但是对于图中存在异常节点（OOD）的情况却很少受到关注。基于 CV 和 NLP 中的概念，我们定义 OOD 节点为训练集中未见过的标签。由于现实世界中的图 oftentimes 是自动生成的，因此真实的图可能会包含来自未知分布的节点。在这工作中，我们定义了图学习中的 OOD 节点问题。特别是，我们希望完成两个任务：1）检测图中不属于已知分布的节点，2）将剩下的节点分类为已知类别之一。我们发现图中的连接模式是有用的异常检测信息，并提出了 Out-of-Distribution Graph Attention Network (OODGAT)，一种新的 GNN 模型，可以在特征传播过程中分离异常节点和正常节点。我们的实验表明，OODGAT 在异常检测方面超过现有的方法，而且在已知分布下的分类性能也不丢下。
</details></li>
</ul>
<hr>
<h2 id="The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems"><a href="#The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems" class="headerlink" title="The Hard-Constraint PINNs for Interface Optimal Control Problems"></a>The Hard-Constraint PINNs for Interface Optimal Control Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06709">http://arxiv.org/abs/2308.06709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianyouzeng/pinns-interface-optimal-control">https://github.com/tianyouzeng/pinns-interface-optimal-control</a></li>
<li>paper_authors: Ming-Chih Lai, Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</li>
<li>for: 解决部分泛函方程（PDEs）中的优化控制问题，包括界面和一些控制约束。</li>
<li>methods: 使用物理学 informed neural networks（PINNs）和最近开发的缺陷捕捉神经网络，解决 mesh-free 和可扩展的 PDEs 优化控制问题。</li>
<li>results: 提出了一种具有硬约束的 PINNs 方法，可以 garantuee 精确满足界面和边界条件，并且可以分离学习 PDEs 和约束。其效率在一些椭球和Parabolic 界面优化控制问题中得到了承诺。<details>
<summary>Abstract</summary>
We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a novel neural network architecture. The resulting hard-constraint PINNs approach guarantees that both the boundary and interface conditions can be satisfied exactly and they are decoupled from the learning of the PDEs. Its efficiency is promisingly validated by some elliptic and parabolic interface optimal control problems.
</details>
<details>
<summary>摘要</summary>
我们证明，用物理知识整合的神经网络（PINNs），可以与最近开发的阶段错误捕捉神经网络（DCNNs）一起解决受到部分数据方程式（PDEs）的最佳控制问题。这个算法是不含网点的和可扩展到不同的PDEs，并且确保控制限制。由于边界和界面条件，以及PDEs，都是转化为转量的loss函数中的软类似，因此需要同时学习它们。但是，这会导致调整这些类似的变量的问题和训练神经网络的问题。为了解决这些问题和保证数据精度，我们提议将边界和界面条件转化为硬类似的PINNs架构。这个方法可以保证边界和界面条件能够精确地满足，并且与PDEs的学习分开。我们在一些elliptic和parabolic interface最佳控制问题中调查了这个方法的效率，结果显示了承认的效果。
</details></li>
</ul>
<hr>
<h2 id="Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model"><a href="#Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model" class="headerlink" title="Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model"></a>Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06708">http://arxiv.org/abs/2308.06708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yasahi-hpc/generative-enkf">https://github.com/yasahi-hpc/generative-enkf</a></li>
<li>paper_authors: Yuuichi Asahi, Yuta Hasegawa, Naoyuki Onodera, Takashi Shimokawabe, Hayato Shiba, Yasuhiro Idomura</li>
<li>for: 本文提出了一种 ensemble data assimilation 方法，使用 pseudo ensemble 生成的推 diffusion  probabilistic model。</li>
<li>methods: 本方法使用 trained 模型生成不寻常的 ensemble，通过 variance 在 ensemble 中的差异来提高数据融合的性能。</li>
<li>results: 对比传统 ensemble data assimilation 方法，本方法在 simulation model 不完美的情况下显示更好的性能。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种ensemble数据融合方法，使用 Pseudo Ensemble 生成的随机模型。由于模型在噪声和缺失观测数据上训练，因此可以生成具有较高差异的 ensemble，使得我们的提议方法在模型不完美的情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods"><a href="#Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods" class="headerlink" title="Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods"></a>Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06703">http://arxiv.org/abs/2308.06703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avery Ma, Yangchen Pan, Amir-massoud Farahmand</li>
<li>For: 这个论文的目的是研究权重迭代法和适应性权重迭代法在训练深度神经网络时的表现。* Methods: 这个论文使用了随机梯度下降（SGD）和适应性梯度下降（Adam、RMSProp）等方法来训练深度神经网络。* Results: 研究发现，使用SGD训练的神经网络比使用适应性梯度下降方法更具robustness，并且在输入变化时表现更好。此外，研究还发现了自然 dataset 中的无关频率，并证明了使用适应性梯度下降方法可能会导致模型对这些变化敏感。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks show smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods.
</details>
<details>
<summary>摘要</summary>
Stochastic gradient descent (SGD) 和 adaptive gradient 方法，如 Adam 和 RMSProp，在深度神经网络训练中广泛应用。我们实验显示，虽然这些方法的标准化预测性表现相似，但SGD 训练的模型在输入扰动下的Robustness 表现强度远胜 adaptive 方法。尤其是，我们的调查发现自然数据集中存在无关频率，其变化不会影响模型的预测性表现。然而，使用 adaptive 方法训练的模型对这些变化具有敏感性，表明它们使用无关频率可能会导致敏感解决方案。为了更好地理解这种差异，我们研究了梯度 descend (GD) 和签名梯度 descend (signGD) 在人工数据集上的学习动态。在三维输入空间中，使用 GD 和 signGD 优化的模型具有标准风险几乎为零，但它们在 $\ell_2$-norm 约束的变化下的风险异常大。我们的结果表明，线性模型对 $\ell_2$-norm 约束的变化的Robustness 与模型参数的权重 нор呈反比关系：小权重 нор呈指示更好的Robustness。在深度学习中，我们的实验表明，SGD 训练的神经网络显示更小的 Lipschitz 常数，解释它们在输入扰动下的更好的Robustness 比 adaptive 方法训练的模型。
</details></li>
</ul>
<hr>
<h2 id="Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection"><a href="#Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection" class="headerlink" title="Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection"></a>Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06701">http://arxiv.org/abs/2308.06701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Zhang, Can Qin, Yu Yin, Yun Fu</li>
<li>for: 提高深度学习模型检测掩蔽物的能力</li>
<li>methods: 使用生成模型生成真实的掩蔽图像，用于训练现有的物体检测模型</li>
<li>results: 在三个数据集（COD10k、CAMO和CHAMELEON）上表现出色，超过当前状态的方法表现，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>隐形对象在自然场景中摒除成为深度学习模型检测和生成的挑战。隐形对象检测是计算机视觉中重要的应用领域之一，但这一研究领域受到有限的数据可用性的限制。我们提出了一种框架，用于增强自然场景中隐形对象的检测。我们的方法使用生成模型生成真实的隐形图像，这些图像可以用来训练现有的对象检测模型。我们的框架在三个数据集（COD10k、CAMO和CHAMELEON）上表现出比前一个状态的方法更高的性能，这说明了我们的方法的有效性。这种方法可以作为现有隐形对象检测任务的数据生成和扩展模块，并提供一种新的多样性和分布的引入方式，以提高当前的隐形图像数据集。
</details></li>
</ul>
<hr>
<h2 id="SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency"><a href="#SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency" class="headerlink" title="SimMatchV2: Semi-Supervised Learning with Graph Consistency"></a>SimMatchV2: Semi-Supervised Learning with Graph Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06692">http://arxiv.org/abs/2308.06692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingkai-zheng/simmatchv2">https://github.com/mingkai-zheng/simmatchv2</a></li>
<li>paper_authors: Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, Chang Xu</li>
<li>for: This paper is written for the purpose of proposing a new semi-supervised learning algorithm called SimMatchV2, which can improve the performance of image classification tasks with limited labeled data.</li>
<li>methods: The SimMatchV2 algorithm uses a graph-based approach to formulate various consistencies between labeled and unlabeled data, and utilizes a message passing mechanism to improve the performance of the algorithm.</li>
<li>results: The paper reports that SimMatchV2 achieves state-of-the-art performance on multiple semi-supervised learning benchmarks, with Top-1 Accuracy of 71.9% and 76.2% on ImageNet using 1% and 10% labeled examples, respectively.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍一种新的半监督学习算法SimMatchV2，可以在有限的标注数据的情况下提高图像分类任务的性能。</li>
<li>methods: SimMatchV2算法使用图表的方式来定义各种半监督数据之间的一致性，并利用消息传递机制来提高算法的性能。</li>
<li>results: 论文报告SimMatchV2在多个半监督学习benchmark上达到了状态的最佳性能，ImageNet上使用1%和10%标注样本时，Top-1准确率分别达到71.9%和76.2%。<details>
<summary>Abstract</summary>
Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised learning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of training, SimMatchV2 achieves 71.9\% and 76.2\% Top-1 Accuracy with 1\% and 10\% labeled examples on ImageNet, which significantly outperforms the previous methods and achieves state-of-the-art performance. Code and pre-trained models are available at \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.
</details>
<details>
<summary>摘要</summary>
“半支持学习图像分类是计算机视觉领域中最基本的问题之一，它可以大幅减少人工劳动。在这篇论文中，我们介绍了一种新的半支持学习算法——SimMatchV2，它通过图形视角来形式化各种一致性 regularization。在SimMatchV2中，我们将每个样本的扩展视图视为一个节点，该节点包含标签和其对应的表示。不同的节点之间连接起来，这些连接由节点表示的相似度来度量。受图形理论中的消息传递和节点分类的启发，我们提出了四种一致性，namely 1) 节点-节点一致性、2) 节点-边一致性、3) 边-边一致性、4) 边-节点一致性。我们还发现，一个简单的特征 нормализа可以大幅减少不同扩展视图之间的特征 нор值差距，从而显著提高 SimMatchV2 的性能。我们的 SimMatchV2 在多个半支持学习 benchmark 上进行验证，与 ResNet-50 作为 backing 和 300  epoch 训练，SimMatchV2 在 ImageNet 上 achiev 71.9% 和 76.2% Top-1 Accuracy with 1% 和 10% 标注样本，在前一些方法中显著超越，实现了状态的杰出性。代码和预训练模型可以在 \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2} 上获取。”
</details></li>
</ul>
<hr>
<h2 id="MDB-Interactively-Querying-Datasets-and-Models"><a href="#MDB-Interactively-Querying-Datasets-and-Models" class="headerlink" title="MDB: Interactively Querying Datasets and Models"></a>MDB: Interactively Querying Datasets and Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06686">http://arxiv.org/abs/2308.06686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaditya Naik, Adam Stein, Yinjun Wu, Eric Wong, Mayur Naik</li>
<li>for: 本文为了帮助开发人员系统地调试机器学习模型中出现的错误。</li>
<li>methods: 本文使用了函数编程和关系代数来构建对数据集和模型预测数据进行表达的查询框架。</li>
<li>results: 我们的实验表明，使用MDB可以比其他基elines快速40%短 queries，并且开发人员可以成功地构建复杂的查询来描述机器学习模型的错误。<details>
<summary>Abstract</summary>
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
</details>
<details>
<summary>摘要</summary>
developers 需要可以系统地调试机器学习管道中出现的错误。我们提出了MDB，一个用于交互式查询数据集和模型的调试框架。MDB将函数编程与关系代数结合起来，以建立表达性的查询数据集和模型预测中的问题。查询可重用和容易修改，允许调试者快速 iterate和细化查询，以描述和Characterize错误和模型行为。我们在对自动驾驶视频、大语言模型和医疗记录进行对象检测、偏见探测、图像分类和数据填充任务中进行了实验，结果显示MDB可以提高查询速度和查询长度，相比于其他基elines。在用户研究中，我们发现开发者可以成功地构建复杂的查询，以描述机器学习模型的错误。
</details></li>
</ul>
<hr>
<h2 id="Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations"><a href="#Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations" class="headerlink" title="Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations"></a>Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06679">http://arxiv.org/abs/2308.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Xing, Jianqiao Sun<br>for: 用于 tri-variate function approximations 和 complex geometry 函数近似methods: 使用 Separable Gaussian Neural Network (SGNN)，利用 Gaussian 函数的分离性，将输入数据拆分成多列并在平行层中逐步 feeding them into uni-variate Gaussian functionsresults: 与 GRBFNN 相比，SGNN 可以实现100倍减少计算时间，同时保持相同精度水平，并且在approximating 函数 with complex geometry 方面可以达到三个数量级更高的精度。同时，SGNN 也比 DNNs with RuLU 和 Sigmoid 函数更易于调试和优化。<details>
<summary>Abstract</summary>
The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GRBFNN on tri-variate function approximations. The SGNN also has better trainability and is more tuning-friendly than DNNs with RuLU and Sigmoid functions. For approximating functions with complex geometry, SGNN can lead to three orders of magnitude more accurate results than a RuLU-DNN with twice the number of layers and the number of neurons per layer.
</details>
<details>
<summary>摘要</summary>
Gaussian-radial-basis function neural network (GRBFNN) 是一种广泛使用的插值和分类方法。然而，当输入向量维度高时，GRBFNN 的计算复杂性会增加很多。为了解决这个问题，我们提出了一个新的前向网络 - Separable Gaussian Neural Network (SGNN)，通过利用 Gaussian 函数的分离性，将输入数据分成多列，然后将其顺序地输入到由单变量 Gaussian 函数所组成的平行层中。这样的结构可以将 GRBFNN 的neuron 数量由 O(N^d) 降至 O(dN)，从而将 computacional speed 加速到 exponentially ，并且让 SGNN 在输入维度增加时阶段性地提高。此外，SGNN 可以保留 GRBFNN 的主对角线 Hessian 矩阵的主对角线，使得在梯度下降训练中可以达到相似的精度水准。实验表明，SGNN 可以在 tri-variate 函数插值中实现 100 倍的速度提升，并且保持相似的精度水准。此外，SGNN 的训练性和适配性比 DNNs  WITH RuLU 和 Sigmoid 函数更好。当插值函数具有复杂的几何结构时，SGNN 可以实现三倍的精度提升。
</details></li>
</ul>
<hr>
<h2 id="A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks"><a href="#A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks" class="headerlink" title="A deep learning framework for multi-scale models based on physics-informed neural networks"></a>A deep learning framework for multi-scale models based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06672">http://arxiv.org/abs/2308.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Wang, Yanzhong Yao, Jiawei Guo, Zhiming Gao</li>
<li>for: 解决多Scale问题</li>
<li>methods: 修改损失函数，对不同级别的损失项应用不同数量的Power运算，使损失函数中各个损失项的级别相近</li>
<li>results: 能同时优化不同级别的损失项，扩展PINN的应用范围<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdomains. The proposed method enables loss terms with different magnitudes to be optimized simultaneously, and it advances the application of PINN for multi-scale problems.
</details>
<details>
<summary>摘要</summary>
物理学 informed neural networks (PINN) combine deep neural networks 与解决 partial differential equations (PDEs) 的解，创造了一个新的研究领域，用于数值解决 PDEs。面临多个尺度问题，其中loss function中的损失项有不同的量级，标准 PINN 方法难以获得可靠预测。在这篇论文中，我们提出了一种新的多尺度问题解决框架。该框架基于标准 PINN 方法，并对不同量级的损失项进行不同数量的幂运算，使得各个损失项组成的损失函数具有相似的量级。此外，我们还提出了一种分组常数化策略，可以有效地处理不同子区域中变化很大的问题。该方法可以同时优化不同量级的损失项，并推动 PINN 在多尺度问题上的应用。
</details></li>
</ul>
<hr>
<h2 id="Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent"><a href="#Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent" class="headerlink" title="Law of Balance and Stationary Distribution of Stochastic Gradient Descent"></a>Law of Balance and Stationary Distribution of Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06671">http://arxiv.org/abs/2308.06671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Ziyin, Hongchao Li, Masahito Ueda</li>
<li>for: 本文研究了Stochastic Gradient Descent（SGD）算法如何训练神经网络，特别是SGD在神经网络的高维和潜在不稳定的损失函数空间中如何导航。</li>
<li>methods: 本文使用了Symmetry的概念来研究SGD的训练过程，并证明了SGD在损失函数包含Symmetry时可以减轻损失函数的不稳定性。</li>
<li>results: 本文研究发现，SGD在深度和宽度具有某些特定的Symmetry时可以导致神经网络的站点分布具有复杂非线性现象，如相转化、破碎Ergodicity和强制转换。这些现象只存在于深度具有某些特定Symmetry的神经网络中，这表明了深度和浅度模型之间的基本区别。<details>
<summary>Abstract</summary>
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details>
<details>
<summary>摘要</summary>
SGD算法是我们用来训练神经网络的算法，但是它在神经网络的高度不对称和缺失散射的损失函数空间中 Navigation remains poorly understood. In this work, we prove that SGD的小批量噪声规范化解决方案带有批处理的散射过程，当损失函数具有扩大Symmetry时。由于在对称性存在时，SGD动力学与批处理的差异最大，我们的理论表明损失函数的对称性是SGD工作的重要检验。我们 then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges"><a href="#Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges" class="headerlink" title="Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges"></a>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06668">http://arxiv.org/abs/2308.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/agriculture-foundation-models">https://github.com/jiajiali04/agriculture-foundation-models</a></li>
<li>paper_authors: Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</li>
<li>for: 这项研究旨在探索基于 Machine Learning 和 Deep Learning 的智能农业领域中的Foundation Models（基础模型）的潜力。</li>
<li>methods: 我们首先回顾了最新的基础模型在通用计算机科学领域，并将它们分为四类：语言基础模型、视觉基础模型、多modal基础模型以及强化学习基础模型。然后，我们详细介绍了在农业领域开发基础模型的过程，以及它们在智能农业中的潜在应用。</li>
<li>results: 通过本研究，我们对智能农业领域中基础模型的应用提出了新的研究方向，并提供了一个概念工具和技术背景来促进理解问题空间和探索新的研究方向。此外，我们还讨论了在开发基础模型时存在的独特挑战，包括模型训练、验证和部署。通过这项研究，我们对农业 AI 系统的发展作出了贡献，并介绍了基础模型作为一种可能地减少大量标注数据的潜在解决方案。<details>
<summary>Abstract</summary>
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
</details>
<details>
<summary>摘要</summary>
过去一代，机器学习（ML）和深度学习（DL）方法在农业系统中得到了迅速发展，其中很多成果在各种农业应用中得到了证明。然而，传统的ML/DL模型具有一些局限性：它们需要大量、昂贵的标注数据进行训练，需要专业的技术人员进行开发和维护，而且主要针对特定任务，缺乏普适性。最近，基础模型（Foundation Models，FMs）在语言和视觉任务中获得了非常成功的结果，它们在多个领域和模式上训练，并且可以通过微调和少量任务特定的标注数据来完成多种任务。Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field.首先，我们回顾了最近的FMs在通用计算机科学领域中的发展，并将它们分为四类：语言FMs、视觉FMs、多模式FMs和强化学习FMs。然后，我们详细介绍了农业FMs的开发过程，并讨论了它们在智能农业中的潜在应用。我们还讨论了开发农业FMs的独特挑战，包括模型训练、验证和部署。通过本研究，我们对农业AI的发展做出了贡献，通过引入AFMs作为一种可靠的替代方案，以减少对广泛标注数据的依赖，提高农业AI系统的效率、有效性和普适性。
</details></li>
</ul>
<hr>
<h2 id="ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN"><a href="#ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN" class="headerlink" title="ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN"></a>ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06663">http://arxiv.org/abs/2308.06663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abul Bashar, Richi Nayak</li>
<li>for: 这篇论文目的是提出一个新的生成对抗网络模型（ALGAN），用于不监控的时间序列资料中的异常检测。</li>
<li>methods: 这篇论文使用的方法是基于LSTM网络的对抗网络（GAN）模型，并且对输出进行调整以提高异常检测精度。</li>
<li>results: 根据实验结果，ALGAN在46个真实世界单 Variate时间序列数据集和多个领域的大量多 Variate时间序列数据集上的异常检测精度较高，比较传统、神经网络基于的和其他GAN型方法更好。<details>
<summary>Abstract</summary>
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
</details>
<details>
<summary>摘要</summary>
《时序数据异常检测使用生成对抗网络》Introduction:时序数据异常检测是各个领域的常见问题，如制造、医疗影像和网络安全等。在这些领域中，检测时序数据中异常点的异常行为是非常重要的。Recently, Generative Adversarial Networks (GANs) have been shown to be effective in detecting anomalies in time series data. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting.Methodology:我们的方法包括以下几个部分：1. 生成对抗网络模型（GAN）的概述2. 基于LSTM网络的异常检测模型（ALGAN）的提出3. 实验设计和结果分析Results:我们对46个实际时序数据集进行了实验，并对多个领域的大量多变量时序数据进行了分析。结果表明，ALGAN在无监督的情况下，对时序数据中异常点的检测性能有显著提高。在单变量和多变量时序数据中，ALGAN都能够准确地检测异常点。Conclusion:本文提出了一种基于GAN的新方法，可以在无监督的情况下，提高时序数据中异常点的检测性能。我们的实验结果表明，ALGAN在多个领域中都能够准确地检测异常点。这种方法可以广泛应用于各个领域中的时序数据异常检测问题。
</details></li>
</ul>
<hr>
<h2 id="Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features"><a href="#Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features" class="headerlink" title="Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features"></a>Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08482">http://arxiv.org/abs/2308.08482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiiizhang/shortcutDebiasing">https://github.com/yiiizhang/shortcutDebiasing</a></li>
<li>paper_authors: Yi Zhang, Jitao Sang, Junyang Wang, Dongmei Jiang, Yaowei Wang</li>
<li>for: 降低机器学习模型中的偏见风险，特别是在社会应用中，如雇用、银行和刑事司法等领域。</li>
<li>methods: 我们提出了一种新的短路减震方法（Shortcut Debiasing），通过在训练阶段将偏见特征转换为短路特征，然后使用 causal intervention 来消除短路特征 durante la inferencia。</li>
<li>results: 我们在多个 benchmark 数据集上应用了短路减震方法，并实现了与状态前的减震方法相比的显著改善 both accuracy 和 fairness。<details>
<summary>Abstract</summary>
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate shortcut features during inference. The key idea of \emph{Shortcut Debiasing} is to design controllable shortcut features to on one hand replace bias features in contributing to the target task during the training stage, and on the other hand be easily removed by intervention during the inference stage. This guarantees the learning of the target task does not hinder the elimination of bias features. We apply \emph{Shortcut Debiasing} to several benchmark datasets, and achieve significant improvements over the state-of-the-art debiasing methods in both accuracy and fairness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks"><a href="#Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks" class="headerlink" title="Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks"></a>Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06654">http://arxiv.org/abs/2308.06654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 预测步行者的轨迹是自动驾驶汽车安全导航中的关键能力，特别是在与步行者共享空间时。步行者在共享空间中的运动受到汽车和其他步行者的影响，因此，可以准确地模拟步行者-汽车和步行者-步行者的互动，可以提高步行者轨迹预测模型的准确性。</li>
<li>methods: 我们提出了一种基于启发的方法，通过计算碰撞风险来选择互动对象。我们将关注与目标步行者之间的碰撞风险，并使用时间到碰撞和两个对象的接近方向角来编码互动效果。我们还提出了一种新的极天球碰撞网格图，以便更好地表示互动效果。</li>
<li>results: 我们的结果表明，使用我们提出的方法可以比基eline方法更加准确地预测步行者的轨迹，特别是在HBS数据集上。<details>
<summary>Abstract</summary>
Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.
</details>
<details>
<summary>摘要</summary>
预测行人轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与行人共享空间时。行人运动在共享空间中受到车辆和其他行人的影响。因此，可以准确模拟行人与其他行人和车辆之间的互动，以提高行人轨迹预测模型的准确性。尽管有庞大的文献关于使用深度学习模型来编码互动对行人预测轨迹的影响，但是有限的努力被投入到有效选择互动者方面。在大多数情况下，互动特征主要基于相对距离，而忽略了互动形式中速度和接近方向的效果。在这篇文章中，我们提出了一种基于碰撞风险计算的互动者选择规则。我们关注了可能碰撞的两个代理人之间的时间到碰撞和接近方向角的互动效果。我们通过引入一种新的极地碰撞格图来实现这一点。我们的结果显示，与基eline方法相比，我们的方法在HBS数据集上预测轨迹更加准确。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation"><a href="#Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation" class="headerlink" title="Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"></a>Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06644">http://arxiv.org/abs/2308.06644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation">https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation</a></li>
<li>paper_authors: Junwei Huang, Zhiqing Sun, Yiming Yang</li>
<li>for: 提高NP-完全 combinatorial优化问题的解决速度</li>
<li>methods: 使用进步压缩来减少推理步骤数</li>
<li>results: 在TSP-50 dataset上，提高推理速度16倍，性能下降0.019%<details>
<summary>Abstract</summary>
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
</details>
<details>
<summary>摘要</summary>
GRAPH-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition"><a href="#Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition" class="headerlink" title="Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition"></a>Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11633">http://arxiv.org/abs/2308.11633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Sheffield, Frank E. Bobe III, Bradley Marchand, Matthew S. Emigh</li>
<li>for: 本研究提出了一种基于自助学习的SAS数据处理方法，以解决SAS数据处理中缺乏标注数据的问题。</li>
<li>methods: 本研究使用了MoCo-SAS方法，即基于自助学习的SAS数据处理、分类和模式识别方法。</li>
<li>results: 实验结果表明，MoCo-SAS方法在SAS数据处理中显著超过了传统的指导学习方法，并且在F1分数方面得到了显著改善。这些发现提出了使用自助学习进行SAS数据处理的潜在可能性，并且对水下对象检测和分类提供了新的思路。<details>
<summary>Abstract</summary>
Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
</details>
<details>
<summary>摘要</summary>
这篇研究论文提出了一个名为MoCo-SAS的自动学习方法，用于对水下探索中的Synthetic Aperture Sonar（SAS）数据进行处理、分类和图像识别。这种方法利用自动学习的自我指导学习（SSL）技术，以提高SAS数据处理的精度和效率。实验结果显示，MoCo-SAS方法与传统的超级vised learning方法相比，有着明显的改善，特别是在F1分数上。这些结果显示出SSL在SAS数据处理中的应用潜力，并开启了更进一步的水下物体探测和分类技术的可能性。
</details></li>
</ul>
<hr>
<h2 id="ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss"><a href="#ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss" class="headerlink" title="ADRMX: Additive Disentanglement of Domain Features with Remix Loss"></a>ADRMX: Additive Disentanglement of Domain Features with Remix Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06624">http://arxiv.org/abs/2308.06624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkerdemirel/ADRMX">https://github.com/berkerdemirel/ADRMX</a></li>
<li>paper_authors: Berker Demirel, Erchan Aptoula, Huseyin Ozkan</li>
<li>for: 这个论文的目的是解决多个源领域中模型在新未经见过的领域中的泛化问题。</li>
<li>methods: 这个论文提出了一种新的架构 named Additive Disentanglement of Domain Features with Remix Loss (ADRMX)，它使用了添加式分解策略将域特征与域 invariants 相结合，以提高模型的泛化能力。</li>
<li>results: 经过广泛的实验，ADRMX 在 DomainBed 上实现了最佳性能。<details>
<summary>Abstract</summary>
The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.
</details>
<details>
<summary>摘要</summary>
通常假设训练集和测试集遵循类似的分布是在部署场景下不成立。给定多个源领域，领域泛化目标是创建抗辐射的模型，以便在新未看过的领域中进行泛化。然而，现有的研究通常是通过找到源领域中共同的特征来减轻交领域分布变化的影响。这可能会限制模型的泛化能力，因为它仅仅依据源领域中共同的特征来泛化。这些研究忽略了可能存在的领域特有特征，这些特征可能在一些领域中占据主导地位，并且可能包含有价值信息。在这项工作中，一种新的架构名为加法解决方案（ADRMX）被提出，它解决了这一限制，通过将领域特征和领域不变特征结合在一起使用一种原始的加法解决方案。此外，一种新的数据增强技术也被引入，以支持ADRMX的泛化能力，其中各个领域的样本在离散空间中混合。经过对DomainBed进行了广泛的实验，ADRMX被证明可以在公正的条件下实现状态的泛化性能。代码将在GitHub上公布 после修订过程。
</details></li>
</ul>
<hr>
<h2 id="Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks"><a href="#Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks" class="headerlink" title="Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?"></a>Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06619">http://arxiv.org/abs/2308.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Liao, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</li>
<li>for: 降低深度神经网络的大小 while maintaining performance</li>
<li>methods: 引入Entropy Guided Pruning（EGP）算法，优先采用含有低 entropy 的连接进行剪除</li>
<li>results: 实验结果表明，EGP可以有效地剪除深度神经网络，同时保持竞争性性能水平<details>
<summary>Abstract</summary>
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.
</details>
<details>
<summary>摘要</summary>
《剪除技术在深度神经网络中减小大小而保持性能的应用广泛。然而，这种技术，即使能够压缩深度模型，几乎不能完全从模型中移除整层（即使结构化）：是这个任务可解决吗？在这项研究中，我们介绍了EGP算法，一种基于熵指导的剪除算法，用于减小深度神经网络大小，保持性能水平。EGP的关键焦点是优先剪除层次熵低的连接，从而导致其完全移除。经过广泛的实验，我们发现EGP能够有效地减小深度神经网络，同时保持竞争性能水平。我们的结果不仅揭示了剪除技术的优势，还探讨了剪除、熵和深度学习性能之间的复杂关系。EGP算法和其理解拥有推动深度网络压缩和优化领域的前景。EGP算法的源代码已经公开发布。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness"><a href="#On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness" class="headerlink" title="On the Interplay of Convolutional Padding and Adversarial Robustness"></a>On the Interplay of Convolutional Padding and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06612">http://arxiv.org/abs/2308.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 本文旨在研究 padding 对 adversarial attack 的影响，以及不同 padding 模式对 adversarial robustness 的影响。</li>
<li>methods: 本文使用了 convolutional neural network (CNN) 和 adversarial attack 的方法，并进行了 padding 的分析和对比。</li>
<li>results: 本文发现了 perturbation anomalies 在图像边缘区域，这些区域是 padding 的应用区域。此外，本文还发现了不同 padding 模式对 adversarial robustness 的影响。<details>
<summary>Abstract</summary>
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
</details>
<details>
<summary>摘要</summary>
通常情况下，在卷积神经网络（CNN）中，会将padding应用于特征地图以保持其分辨率。虽然有很多替代方案，通常是通过添加边界上的零值来实现。在这项工作中，我们发现了对抗攻击通常会在图像边界上产生异常的扰动，这是padding使用的区域。因此，我们想进行对padding和对抗攻击之间的交互分析，并查找不同的padding模式（或其缺失）对对抗鲁棒性在不同的场景中的影响。
</details></li>
</ul>
<hr>
<h2 id="LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net"><a href="#LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net" class="headerlink" title="LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net"></a>LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06603">http://arxiv.org/abs/2308.06603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ach-1914/ladlenet">https://github.com/ach-1914/ladlenet</a></li>
<li>paper_authors: Tonghui Zou</li>
<li>for: 这篇论文的目的是为了将抛光热成像（TIR）图像转换成可见光成像（VI）图像，这个问题在各个领域都有广泛的应用，例如TIR-VI图像匹配和融合。</li>
<li>methods: 这篇论文提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net concatenation结构，并添加了跳过连接和精细特征聚合技术，从而提高了模型性能。</li>
<li>results: 在KAIST数据集上测试和分析了LadleNet和LadleNet+两种方法，结果显示，LadleNet+在图像清晰度和感知质量方面达到了当前最佳性能。<details>
<summary>Abstract</summary>
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibility by allowing the substitution of its network architecture with semantic segmentation networks, thereby establishing more abstract semantic spaces to bolster model performance. Consequently, we propose LadleNet+, which replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network, thereby endowing the model with enhanced semantic space construction capabilities. The proposed method is evaluated and tested on the KAIST dataset, accompanied by quantitative and qualitative analyses. Compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality. The source code will be made available at https://github.com/Ach-1914/LadleNet/tree/main/.
</details>
<details>
<summary>摘要</summary>
《热传 инфра红（TIR）图像到可见光（VI）图像的翻译 задача具有各种应用领域的潜在投入，如TIR-VI图像匹配和融合。利用TIR图像转换得到的补充信息可以显著提高模型性能和泛化性。然而，现有的问题包括低效图像准确性和限制模型可扩展性。在本文中，我们提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net叠加结构，并添加了跳跃连接和细化特征聚合技术，从而实现了模型性能的明显提高。LadleNet由“托”和“碗”模块组成，其中“托”模块建立了一个抽象 semantic space，而“碗”模块将这个 semantic space 转换为生成的VI图像。“托”模块具有可扩展性，可以将其网络架构替换为semantic segmentation网络，从而建立更加抽象的semantic space，进一步提高模型性能。因此，我们提出了LadleNet+，其将LadleNet的“托”模块替换为预训练的DeepLabv3+网络，从而增强模型的semantic space建构能力。我们的方法在KAIST数据集上进行评估和测试，并通过量化和质量分析进行比较。与现有方法相比，我们的方法在图像清晰度和感知质量上达到了状态 искусственный的性能。代码将在https://github.com/Ach-1914/LadleNet/tree/main/中提供。》
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.LG_2023_08_13/" data-id="clly3dvzg006609887twc29u3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/eess.IV_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/eess.IV_2023_08_13/">eess.IV - 2023-08-13 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature"><a href="#Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature" class="headerlink" title="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature"></a>Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06781">http://arxiv.org/abs/2308.06781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>For: The paper aims to generate realistic 3D segmentations of the Circle of Willis (CoW) using a conditional latent diffusion model with shape and anatomical guidance, in order to advance research on cerebrovascular diseases and refine clinical interventions.* Methods: The authors propose a novel generative approach using a conditional latent diffusion model, which incorporates shape guidance to better preserve vessel continuity and demonstrate superior performance compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE.* Results: The authors observed that their model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches, with an FID score 53% better than the best-performing GAN-based model.Here are the three points in Simplified Chinese text:* For: 本研究旨在使用 conditional latent diffusion model 生成真实的 3D CoW 分割，以提高脑血管疾病研究和临床 interven 的技术水平。* Methods: 作者们提出了一种新的生成方法，使用 conditional latent diffusion model，该模型具有形态指导，以更好地保持血管连续性。* Results: 作者们发现，他们的模型可以生成更加真实的 CoW 变体，并且与其他方法相比，有 53% 更高的视觉质量。<details>
<summary>Abstract</summary>
The Circle of Willis (CoW) is the part of cerebral vasculature responsible for delivering blood to the brain. Understanding the diverse anatomical variations and configurations of the CoW is paramount to advance research on cerebrovascular diseases and refine clinical interventions. However, comprehensive investigation of less prevalent CoW variations remains challenging because of the dominance of a few commonly occurring configurations. We propose a novel generative approach utilising a conditional latent diffusion model with shape and anatomical guidance to generate realistic 3D CoW segmentations, including different phenotypical variations. Our conditional latent diffusion model incorporates shape guidance to better preserve vessel continuity and demonstrates superior performance when compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE. We observed that our model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches with an FID score 53\% better than the best-performing GAN-based model.
</details>
<details>
<summary>摘要</summary>
圆形的威廉圈（CoW）是脑血管系统中带来脑部血液的部分。了解各种不同的CoW结构和配置是研究脑血管疾病的进步和优化临床 intervención的关键。然而，对于较少seen CoW变化的全面调查仍然是一个挑战，因为一些常见的配置占据了主导地位。我们提出了一种新的生成方法，利用决定性液态扩散模型，包括形态指导来生成真实的3D CoW分割结果，包括不同的fenotipical变化。我们的决定性液态扩散模型包含形态指导，以更好地保持血管连续性，并且与其他生成模型相比，包括 conditional GAN和3D VAE的 conditional变种，显示出更高的性能。我们观察到，我们的模型生成的CoW变化更加真实，与竞争方法的FID分数比53%高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches"><a href="#Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches" class="headerlink" title="Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches"></a>Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06776">http://arxiv.org/abs/2308.06776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxin0/scpgabnet">https://github.com/linxin0/scpgabnet</a></li>
<li>paper_authors: Xin Lin, Chao Ren, Xiao Liu, Jie Huang, Yinjie Lei</li>
<li>for: 提高无监督图像净化的性能，不需要大规模的对照数据集。</li>
<li>methods: 基于生成对抗网络的不监督方法，通过多个denoiser的级联使用，逐渐提高净化性能。</li>
<li>results: 与现有无监督方法相比，提出了新的SC策略，可以减少对GAN-based净化框架的计算复杂性，同时提高图像净化性能。实验结果表明，该方法可以在无监督下实现更高的净化性能。<details>
<summary>Abstract</summary>
Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
深度学习方法在图像除噪方面表现了非常出色，特别是在大规模对应数据集上训练的情况下。然而，在实际场景中获得这些对应数据集是一项非常困难的任务。尽管使用生成对抗网络来实现无监督的推荐方法可以解决这个问题，但是这些方法往往难以超越传统的GAN基础架构下的性能限制，而且不需要明显地修改现有结构或增加推荐器的计算复杂度。为解决这个问题，我们提出了一种SC策略。这种策略可以在GAN基础架构下实现明显的性能提升，而无需增加推荐器的推理复杂度。其基本思想是在滤波器导向噪音提取模块中，iteratively替换之前的较弱推荐器，使得当前的强大推荐器可以在下一轮中使用。这个过程生成了更好的干净清噪图像对，从而导致更强大的推荐器。这个基准保证了训练网络的稳定性和效果。实验结果表明，我们的方法在无监督方法中表现出了superiority。
</details></li>
</ul>
<hr>
<h2 id="Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes"><a href="#Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes" class="headerlink" title="Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes"></a>Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06762">http://arxiv.org/abs/2308.06762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Huang, Xukun Zhang, Zhiming Cui, He Zhang, Geng Chen, Dinggang Shen</li>
<li>for: 这个研究旨在提高胎儿脑 MR 影像中的组织分类精度，以便重建高精度的胎儿脑 MR 影像量和评估胎儿脑发展。</li>
<li>methods: 这个研究使用了领域适应技术，将高品质的胎儿脑 MR 影像作为指导，对厚层胎儿脑 MR 影像进行分类。</li>
<li>results: 研究结果显示，这个方法可以很好地改善胎儿脑 MR 影像中的组织分类精度，与现有的方法相比，表现更加出色。<details>
<summary>Abstract</summary>
Accurate tissue segmentation of thick-slice fetal brain magnetic resonance (MR) scans is crucial for both reconstruction of isotropic brain MR volumes and the quantification of fetal brain development. However, this task is challenging due to the use of thick-slice scans in clinically-acquired fetal brain data. To address this issue, we propose to leverage high-quality isotropic fetal brain MR volumes (and also their corresponding annotations) as guidance for segmentation of thick-slice scans. Due to existence of significant domain gap between high-quality isotropic volume (i.e., source data) and thick-slice scans (i.e., target data), we employ a domain adaptation technique to achieve the associated knowledge transfer (from high-quality <source> volumes to thick-slice <target> scans). Specifically, we first register the available high-quality isotropic fetal brain MR volumes across different gestational weeks to construct longitudinally-complete source data. To capture domain-invariant information, we then perform Fourier decomposition to extract image content and style codes. Finally, we propose a novel Cycle-Consistent Domain Adaptation Network (C2DA-Net) to efficiently transfer the knowledge learned from high-quality isotropic volumes for accurate tissue segmentation of thick-slice scans. Our C2DA-Net can fully utilize a small set of annotated isotropic volumes to guide tissue segmentation on unannotated thick-slice scans. Extensive experiments on a large-scale dataset of 372 clinically acquired thick-slice MR scans demonstrate that our C2DA-Net achieves much better performance than cutting-edge methods quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
通过借助高质量的ISO体积脑MR图像（以及其相应的标注），我们提议利用这些图像作为厚层扫描图像的指导。由于高质量ISO体积图像和厚层扫描图像之间存在域之间的差距，我们采用域适应技术来实现相关的知识传递。特别是，我们首先将可用的高质量ISO体积脑MR图像进行了 longitudinally-complete的注册，以构建不同 gestational 周的完整的源数据。然后，我们通过Fourier分解来提取图像内容和风格代码。最后，我们提出了一种名为C2DA-Net的循环相互适应域适应网络，以高效地传递高质量ISO体积图像中学习的知识来进行厚层扫描图像的精准组织分割。我们的C2DA-Net可以充分利用一小组标注的ISO体积图像来导引厚层扫描图像的组织分割。我们对372个临床获取的厚层扫描MR扫描图像进行了广泛的实验，并证明了我们的C2DA-Net在量和质量上都与当今的方法相比较为出色。
</details></li>
</ul>
<hr>
<h2 id="FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table"><a href="#FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table" class="headerlink" title="FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table"></a>FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06749">http://arxiv.org/abs/2308.06749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenhao-li-777/fastllve">https://github.com/wenhao-li-777/fastllve</a></li>
<li>paper_authors: Wenhao Li, Guangyang Wu, Wenyi Wang, Peiran Ren, Xiaohong Liu</li>
<li>for: 提高低光照视频质量，保持视频的时间协调性。</li>
<li>methods: 利用Look-Up-Table（LUT）技术，实现高效的低光照视频提高。设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，以适应低动态范围问题。</li>
<li>results: 实验结果表明，我们的方法在质量和时间协调性两个方面均达到了领先水平。与现有的单帧图像基于方法相比，我们的方法可以在1080p视频中实现50+帧&#x2F;秒的处理速度，并且可以保持高质量结果。<details>
<summary>Abstract</summary>
Low-Light Video Enhancement (LLVE) has received considerable attention in recent years. One of the critical requirements of LLVE is inter-frame brightness consistency, which is essential for maintaining the temporal coherence of the enhanced video. However, most existing single-image-based methods fail to address this issue, resulting in flickering effect that degrades the overall quality after enhancement. Moreover, 3D Convolution Neural Network (CNN)-based methods, which are designed for video to maintain inter-frame consistency, are computationally expensive, making them impractical for real-time applications. To address these issues, we propose an efficient pipeline named FastLLVE that leverages the Look-Up-Table (LUT) technique to maintain inter-frame brightness consistency effectively. Specifically, we design a learnable Intensity-Aware LUT (IA-LUT) module for adaptive enhancement, which addresses the low-dynamic problem in low-light scenarios. This enables FastLLVE to perform low-latency and low-complexity enhancement operations while maintaining high-quality results. Experimental results on benchmark datasets demonstrate that our method achieves the State-Of-The-Art (SOTA) performance in terms of both image quality and inter-frame brightness consistency. More importantly, our FastLLVE can process 1,080p videos at $\mathit{50+}$ Frames Per Second (FPS), which is $\mathit{2 \times}$ faster than SOTA CNN-based methods in inference time, making it a promising solution for real-time applications. The code is available at https://github.com/Wenhao-Li-777/FastLLVE.
</details>
<details>
<summary>摘要</summary>
低光照视频增强（LLVE）在过去几年内得到了广泛关注。一个重要的需求是 между帧亮度一致性，以保持视频增强后的时间一致性。然而，大多数单张图像基本方法无法解决这个问题，导致干扰效应，从而降低总质量。此外，基于3D convolutional neural network（CNN）的方法，它们是为视频维护间帧一致性而设计的，但它们计算成本高，使其在实时应用中不实际。为解决这些问题，我们提出了一个高效的排序管道，即快速LLVE，该管道利用Look-Up-Table（LUT）技术保持间帧亮度一致性。特别是，我们设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，用于自适应增强，解决低动态问题在低光照场景中。这使得快速LLVE可以在低延迟和低复杂度下进行增强操作，同时保持高质量结果。实验结果表明，我们的方法在标准测试集上达到了状态方法（SOTA）的性能，并且在帧率和亮度一致性两个指标上均有显著提高。此外，我们的快速LLVE可以处理1080P视频，并在50+帧/秒的速度下进行增强，这比SOTA CNN基本方法在推理时间上快两倍，使其成为实时应用的优秀解决方案。代码可以在https://github.com/Wenhao-Li-777/FastLLVE上获取。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising"><a href="#Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising" class="headerlink" title="Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising"></a>Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06746">http://arxiv.org/abs/2308.06746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyuan01/self-supervised-noise2noise-for-ldct">https://github.com/xyuan01/self-supervised-noise2noise-for-ldct</a></li>
<li>paper_authors: Yuting Zhu, Qiang He, Yudong Yao, Yueyang Teng</li>
<li>for: 这篇论文旨在提出一种基于单静电 Tomatoes CT（LDCT）数据的自我监督噪声降低方法，并不需要对比的噪音和清洁数据。</li>
<li>methods: 本研究使用了一种组合自我监督噪声模型和降低噪声策略，包括在LDCT图像中添加多次相似的噪音，并使用这些次生噪音做为训练数据。</li>
<li>results: 实验结果显示，提案的方法在Mayo LDCT数据集上比前一些深度学习方法更有效率。<details>
<summary>Abstract</summary>
Deep learning is a very promising technique for low-dose computed tomography (LDCT) image denoising. However, traditional deep learning methods require paired noisy and clean datasets, which are often difficult to obtain. This paper proposes a new method for performing LDCT image denoising with only LDCT data, which means that normal-dose CT (NDCT) is not needed. We adopt a combination including the self-supervised noise2noise model and the noisy-as-clean strategy. First, we add a second yet similar type of noise to LDCT images multiple times. Note that we use LDCT images based on the noisy-as-clean strategy for corruption instead of NDCT images. Then, the noise2noise model is executed with only the secondary corrupted images for training. We select a modular U-Net structure from several candidates with shared parameters to perform the task, which increases the receptive field without increasing the parameter size. The experimental results obtained on the Mayo LDCT dataset show the effectiveness of the proposed method compared with that of state-of-the-art deep learning methods. The developed code is available at https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT.
</details>
<details>
<summary>摘要</summary>
深度学习是低剂量 computed tomography（LDCT）图像减噪的非常有前途的技术。然而，传统的深度学习方法通常需要对噪声和清晰图像的对应对进行预处理，这可能具有困难。这篇论文提出了一种使用仅LDCT数据进行LDCT图像减噪的新方法。我们采用了一种组合，包括自我超级vised noise2noise模型和噪声作为清晰Strategy。首先，我们在LDCT图像中添加了多个类似的噪声。请注意，我们使用LDCT图像来实现损害代替NDCT图像。然后，我们在噪声模型中进行训练，使用只有次噪声图像。我们选择了一种模块化U-Net结构，其中共享参数来完成任务，这将增加了感知场景而不会增加参数的大小。我们在Mayo LDCT数据集上进行实验，并证明了提议方法的有效性，比对已有的深度学习方法更高。开发的代码可以在https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT中找到。
</details></li>
</ul>
<hr>
<h2 id="Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation"><a href="#Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation" class="headerlink" title="Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?"></a>Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06623">http://arxiv.org/abs/2308.06623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RisabBiswas/Polyp-SAM-PlusPlus">https://github.com/RisabBiswas/Polyp-SAM-PlusPlus</a></li>
<li>paper_authors: Risab Biswas</li>
<li>for: 本研究旨在提高肠Rectal cancer的诊断和治疗，通过使用文本提示来改进SAM模型，以提高肠脏膜膜蛋白分 segmentation的精度和稳定性。</li>
<li>methods: 本研究使用的是Segment Anything Model (SAM)，并通过文本提示来改进SAM模型，提高其对肠脏膜膜蛋白分 segmentation的能力。</li>
<li>results: 研究表明，使用文本提示的SAM模型可以提高肠脏膜膜蛋白分 segmentation的精度和稳定性，并且比未使用文本提示的SAM模型更好地处理不同的肠脏膜膜蛋白分样本。<details>
<summary>Abstract</summary>
Meta recently released SAM (Segment Anything Model) which is a general-purpose segmentation model. SAM has shown promising results in a wide variety of segmentation tasks including medical image segmentation. In the field of medical image segmentation, polyp segmentation holds a position of high importance, thus creating a model which is robust and precise is quite challenging. Polyp segmentation is a fundamental task to ensure better diagnosis and cure of colorectal cancer. As such in this study, we will see how Polyp-SAM++, a text prompt-aided SAM, can better utilize a SAM using text prompting for robust and more precise polyp segmentation. We will evaluate the performance of a text-guided SAM on the polyp segmentation task on benchmark datasets. We will also compare the results of text-guided SAM vs unprompted SAM. With this study, we hope to advance the field of polyp segmentation and inspire more, intriguing research. The code and other details will be made publically available soon at https://github.com/RisabBiswas/Polyp-SAM++.
</details>
<details>
<summary>摘要</summary>
Meta 最近发布了 SAM（Segment Anything Model），这是一种通用分割模型。SAM 在各种分割任务中表现出了扎实的成果，包括医学影像分割。在医学影像分割领域，肿瘤分割具有非常高的重要性，因此创建一个精准和Robust的模型是非常挑战性的。肿瘤分割是检测和治疗抗Rectal cancer的基本任务之一。在这项研究中，我们将看到Polyp-SAM++ 是如何使用文本提示来更好地利用 SAM 进行肿瘤分割。我们将对 Polyp-SAM++ 在标准数据集上进行评估，并与不提示 SAM 进行比较。我们希望通过这项研究，推动肿瘤分割领域的进步，并鼓励更多的有趣的研究。代码和其他细节将于 https://github.com/RisabBiswas/Polyp-SAM++ 上公开。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/eess.IV_2023_08_13/" data-id="clly3dw2h00ei0988fzqc7pc0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.LG_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.LG_2023_08_12/">cs.LG - 2023-08-12 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning"><a href="#CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning" class="headerlink" title="CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning"></a>CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06594">http://arxiv.org/abs/2308.06594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Anjan Basak, Derrik E. Asher</li>
<li>For: 本研究旨在提出一种基于深度强化学习（DRL）算法，帮助无人地面车辆在隐蔽的情况下安全地导航到预定的目的地。* Methods: 本研究使用了DRL算法，计算了地方成本图，帮助机器人选择低高度的路径，并在检测到观察者时，使用自然障碍物（如岩石、房屋、瘫痪车辆、树木等）作为隐蔽物。* Results: 研究表明，CoverNav可以在 Unity 模拟环境中保证动态可行性，并在不同高度场景下实现最大目标距离和成功率。与当前最佳方法相比，CoverNav 没有妥协精度。<details>
<summary>Abstract</summary>
Autonomous navigation in offroad environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an underexplored area. In this paper, we propose a novel Deep Reinforcement Learning (DRL) based algorithm, called CoverNav, for identifying covert and navigable trajectories with minimal cost in offroad terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. CoverNav helps robot agents to learn the low elevation terrain using a reward function while penalizing it proportionately when it experiences high elevation. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, disabled vehicles, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state of the art (SOTA) methods without compromising accuracy.
</details>
<details>
<summary>摘要</summary>
自主导航在非路面环境中已经得到了机器人学Field的广泛研究。然而，在情报人员发现自动驾驶车辆的情况下，自主导航仍然是一个未得到充分研究的领域。在这篇论文中，我们提出了一种基于深度优化学习（DRL）算法，称为CoverNav，用于在非路面环境中寻找最佳隐蔽和可行的轨迹，并且尽量降低成本。CoverNav关注于无人地面车辆在安全地 navigate到预定目的地点时，找到遮盾和避险的方法。我们提出的DRL方法计算了当地的成本地图，以帮助选择最佳隐蔽的路径，同时维护低成本轨迹。如果检测到了观察者，CoverNav允许机器人使用自然障碍物（如岩石、房屋、瘫痪车辆、树木等）作为遮盾，隐藏自己。我们使用Unity simulate环境评估CoverNav，并证明它在地形图生成自 another DRL基 Navigation algorithm时能够保证动态可行速度。此外，我们在不同高度场景下评估CoverNav的效果，并发现其与SOTA方法相比，没有妥协精度。
</details></li>
</ul>
<hr>
<h2 id="Value-Distributional-Model-Based-Reinforcement-Learning"><a href="#Value-Distributional-Model-Based-Reinforcement-Learning" class="headerlink" title="Value-Distributional Model-Based Reinforcement Learning"></a>Value-Distributional Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06590">http://arxiv.org/abs/2308.06590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters</li>
<li>for: 这个论文目的是为了解决sequential decision-making任务中的uncertainty quantification问题。</li>
<li>methods: 这个论文使用了model-based Bayesian reinforcement learning的方法，其中的目标是学习Markov决策过程中参数不确定性induced的 posterior distribution over value functions。</li>
<li>results: 论文的实验表明，EQR算法可以在 continuous-control tasks 中比Established model-based和model-free算法表现出性能优势。<details>
<summary>Abstract</summary>
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>量化政策长期表现的不确定性是解决sequential decision-making任务的重要问题。我们从model-based Bayesian reinforcement learning的视角 изуча这个问题，目标是学习Markov决策过程中参数（эпистемиче）不确定性引起的 posterior distribution over value functions。先前的工作只考虑了这些分布的一些瞬间或假设了特定的分布形式，例如 Gaussian。 inspirited by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function。 Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. 评估在多个连续控制任务上表现出与已有的model-based和model-free算法相比的性能优势。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Approximate-Answering-of-Graph-Queries"><a href="#Approximate-Answering-of-Graph-Queries" class="headerlink" title="Approximate Answering of Graph Queries"></a>Approximate Answering of Graph Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06585">http://arxiv.org/abs/2308.06585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Cochez, Dimitrios Alivanistos, Erik Arakelyan, Max Berrendorf, Daniel Daza, Mikhail Galkin, Pasquale Minervini, Mathias Niepert, Hongyu Ren</li>
<li>for: 本文旨在介绍几种方法，以帮助回答含有不完整信息的知识图（KG）中的查询。</li>
<li>methods: 本文提出了多种方法，包括基于预测、基于潜在相似性、基于证据等方法，以满足不同类型的查询需求。</li>
<li>results: 这些方法可以帮助解决各种查询问题，如答案推断、 Entity Disambiguation、 Relation extraction 等。但是，这些方法受到图数据不完整和不准确的限制。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
</details>
<details>
<summary>摘要</summary>
知识图（KG）自然而然地是不完整的，因为世界知识的不完整和输入KG中的偏见。此外，世界知识不断扩展和发展，使现有的事实过时或引入新的事实。然而，我们仍然希望能够回答问题，作为如果图完整一样。在这章中，我们将给出不同类型的查询支持的方法的概述，以及通常用于评估的数据集，以及这些方法的局限性。然后，我们将对不同的方法进行描述，包括表达力、支持的图类型和推理能力。
</details></li>
</ul>
<hr>
<h2 id="A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence"><a href="#A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence" class="headerlink" title="A new solution and concrete implementation steps for Artificial General Intelligence"></a>A new solution and concrete implementation steps for Artificial General Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09721">http://arxiv.org/abs/2308.09721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongcong Chen, Ting Zeng, Jun Zhang</li>
<li>for: 本文旨在探讨大型模型技术路径的局限性，并提出解决这些局限性的方案，以实现true AGI。</li>
<li>methods: 本文使用了现有技术和方法，包括注意机制、深度学习和补偿学习，并提出了一种新的解决方案。</li>
<li>results: 本文提出的解决方案可以解决大型模型技术路径中的缺陷，并实现true AGI。<details>
<summary>Abstract</summary>
At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. In this paper, we will reveal how to achieve true AGI step by step.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在，主流人工智能通常采用“注意机制+深度学习”+“奖励学习”的技术路径。这种方法在AIGC（人工智能生成内容）领域已经取得了 significanthistorical achievements[ 2][13 ], triggering a technological wave of big models. However, in areas that require interaction with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are costly and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, to achieve Artificial General Intelligence (AGI) that can be applied to any field, we need to leverage both existing technologies and address the limitations of existing technologies, thereby further developing the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. Through this paper, we will reveal how to achieve true AGI step by step.
</details></li>
</ul>
<hr>
<h2 id="EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction"><a href="#EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction" class="headerlink" title="EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction"></a>EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06564">http://arxiv.org/abs/2308.06564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kehua Chen, Xianda Chen, Zihan Yu, Meixin Zhu, Hai Yang</li>
<li>for: 预测自动驾驶车辆的未来轨迹是关键的，以确保安全和效率地运行。</li>
<li>methods: 我们提出了一种基于深度生成模型的轨迹预测方法，即EquiDiff。EquiDiff基于 Conditional Diffusion 模型，通过历史信息和随机抽样 Gaussian 噪声来生成未来轨迹。</li>
<li>results: 我们在 NGSIM 数据集上进行了广泛的实验，并证明了 EquiDiff 在短期预测方面的性能较高，但在长期预测方面有些较高的错误率。此外，我们还进行了一个ablation study，以调查各组件对预测精度的贡献。<details>
<summary>Abstract</summary>
Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, we conduct extensive experiments on the NGSIM dataset. Our results demonstrate that EquiDiff outperforms other baseline models in short-term prediction, but has slightly higher errors for long-term prediction. Furthermore, we conduct an ablation study to investigate the contribution of each component of EquiDiff to the prediction accuracy. Additionally, we present a visualization of the generation process of our diffusion model, providing insights into the uncertainty of the prediction.
</details>
<details>
<summary>摘要</summary>
准确预测车辆轨迹是自动驾驶车辆运行的安全和效率的关键。随着深度学习的普及，许多方法 для轨迹预测得到了开发。而深度生成模型在训练数据中学习数据分布，并考虑轨迹不确定性，因此在这种情况下变得更加受欢迎。在这项研究中，我们提出了EquiDiff，一种基于 conditional diffusion 模型的深度生成模型，用于预测未来车辆轨迹。EquiDiff 使用 SO(2)-equivariant transformer 作为底层模型，并使用循环神经网络和 Graph Attention Networks 提取历史轨迹中的社会交互。为了评估EquiDiff的性能，我们在 NGSIM 数据集上进行了广泛的实验。我们的结果表明，EquiDiff 在短期预测方面表现出色，但是在长期预测方面有些微的错误。此外，我们进行了ablation study，以investigate EquiDiff 中每个组件对预测精度的贡献。此外，我们还提供了生成过程中 diffusion 模型的视觉化，为预测不确定性提供了更多的信息。
</details></li>
</ul>
<hr>
<h2 id="Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System"><a href="#Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System" class="headerlink" title="Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System"></a>Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06549">http://arxiv.org/abs/2308.06549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Anika Rahman Joyita, Md. Golam Rabiul Alam, Mohammad Mehedi Hassan, Md. Rafiul Hassan, Raffaele Gravina</li>
<li>for: 这个研究的目的是为了提供一种基于情感计算的餐Menu建议系统，以满足用户的情感需求和营养需求。</li>
<li>methods: 这个系统使用了问卷调查和偏好认知来获取用户的餐食偏好，并使用EEG信号检测用户对不同餐食的情感反应。然后，使用一种层次ensemble方法预测餐食的情感反应，并使用TOPSIS算法生成一个基于预测结果的餐Menu。</li>
<li>results: 实验结果表明，提出的情感计算、餐Menu建议和自动菜单规划算法都能够在不同评价参数下表现良好。<details>
<summary>Abstract</summary>
The traditional dietary recommendation systems are basically nutrition or health-aware where the human feelings on food are ignored. Human affects vary when it comes to food cravings, and not all foods are appealing in all moods. A questionnaire-based and preference-aware meal recommendation system can be a solution. However, automated recognition of social affects on different foods and planning the menu considering nutritional demand and social-affect has some significant benefits of the questionnaire-based and preference-aware meal recommendations. A patient with severe illness, a person in a coma, or patients with locked-in syndrome and amyotrophic lateral sclerosis (ALS) cannot express their meal preferences. Therefore, the proposed framework includes a social-affective computing module to recognize the affects of different meals where the person's affect is detected using electroencephalography signals. EEG allows to capture the brain signals and analyze them to anticipate affective toward a food. In this study, we have used a 14-channel wireless Emotive Epoc+ to measure affectivity for different food items. A hierarchical ensemble method is applied to predict affectivity upon multiple feature extraction methods and TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) is used to generate a food list based on the predicted affectivity. In addition to the meal recommendation, an automated menu planning approach is also proposed considering a person's energy intake requirement, affectivity, and nutritional values of the different menus. The bin-packing algorithm is used for the personalized menu planning of breakfast, lunch, dinner, and snacks. The experimental findings reveal that the suggested affective computing, meal recommendation, and menu planning algorithms perform well across a variety of assessment parameters.
</details>
<details>
<summary>摘要</summary>
传统的饮食建议系统基本上是nutrition或健康意识的，忽略了人类的情感 toward food。人类的食欲情绪 varying degree，不同的情感状态下不同的食物都不能吸引人。问卷式和偏好意识的饭单推荐系统可以是一种解决方案。然而，通过自动认知不同食物的社会情感影响和根据饮食需求和社会情感规划饭单，有一些显著的优点。例如，患有严重疾病、昏迷状态或 locked-in syndrome 和 amyotrophic lateral sclerosis (ALS) 的患者无法表达他们的饭单首选。因此，我们的框架包括一个社交情感计算模块，用于识别不同饭物中的情感。我们使用了14核心无线Emotive Epoc+来测量不同食物的情感。我们使用了一种层次ensemble方法来预测情感，并使用TOPSIS (技术 дляOrder of Preference by Similarity to Ideal Solution)来生成基于预测情感的食品列表。此外，我们还提出了一种自动饭单规划方法，考虑人类的能量摄入需求、情感和不同饭单的营养价值。使用了bin-packing算法进行个性化饭单规划的早餐、午餐、晚餐和快餐。实验结果表明，我们提出的情感计算、饭单推荐和饭单规划算法在多种评估参数下表现良好。
</details></li>
</ul>
<hr>
<h2 id="Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters"><a href="#Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters" class="headerlink" title="Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters"></a>Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06545">http://arxiv.org/abs/2308.06545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chukwuma Okolie, Jon Mills, Adedayo Adeleke, Julian Smit</li>
<li>For: The paper aims to enhance the accuracy of medium-resolution digital elevation models (DEMs) in urban areas, specifically in Cape Town, South Africa, for hydrological and environmental modelling.* Methods: The authors use the extreme gradient boosting (XGBoost) ensemble algorithm to correct the DEMs, with eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, and more.* Results: The corrected DEMs achieved significant accuracy gains, with a root mean square error (RMSE) improvement of 46-53% for Copernicus DEM and 72-73% for AW3D DEM, compared to other proposed methods. These results demonstrate the potential of gradient boosted trees for enhancing DEM quality and improving hydrological modelling in urban catchments.Here is the same information in Simplified Chinese text, as requested:* For: 这个论文的目的是提高城市区域中的数字高程模型（DEM）的准确性，以便于水文和环境模型。* Methods: 作者使用极限Gradient Boosting（XGBoost）ensemble算法来修正DEM，使用的predictor变量包括高程、城市脚印、坡度、方向、表面荒凉、地形位置指数、地形荒凉指数、地形表面 текстура等 eleven个变量。* Results: 修正后的DEM实现了显著的准确性提高，比如 Copernicus DEM的RMSE提高46-53%，AW3D DEM的RMSE提高72-73%，与其他提议的方法相比。这些结果表明极限Gradient Boosting树可以提高DEM的质量，并且为城市catchments中的水文模型提供改善。<details>
<summary>Abstract</summary>
The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, forest cover and bare ground cover. The target variable (elevation error) was calculated with respect to highly accurate airborne LiDAR. After training and testing, the model was applied for correcting the DEMs at two implementation sites. The correction achieved significant accuracy gains which are competitive with other proposed methods. The root mean square error (RMSE) of Copernicus DEM improved by 46 to 53% while the RMSE of AW3D DEM improved by 72 to 73%. These results showcase the potential of gradient boosted trees for enhancing the quality of DEMs, and for improved hydrological modelling in urban catchments.
</details>
<details>
<summary>摘要</summary>
地数模型（DEM）在城市地区的准确性受到多种因素的影响，包括地表覆盖物和地形 irregularities。此外，全球 DEM 中的建筑物略导致表面流道路径的人工堵塞，从而降低其质量和适用性 для水文环境模型在城市景观中，需要精准和准确的地形信息。在这种研究中，我们采用了极限拟合搅拌（XGBoost）ensemble算法来提高两个中等分辨率 30 m DEM 的准确性，即 Copernicus GLO-30 和 ALOS World 3D（AW3D）。XGBoost 是一种可扩展、可移植和多样的拟合搅拌库，可以解决许多环境模型问题。训练数据集包括 eleven 个预测变量，包括高程、城市脚印、坡度、方向、表面粗糙度、地形坡度指数、地形表面文化、向量粗糙度度量、森林覆盖率和裸地覆盖率。target variable （高程误差）与高精度飞行 LiDAR 进行计算。之后，模型被应用于修正 DEM 的两个实施场景。修正后，DEM 的Root Mean Square Error（RMSE）提高了46%到53%，AW3D DEM 的 RMSE 提高了72%到73%。这些结果显示了拟合搅拌树的潜在可能性，以及对城市流域水文模型的改进。
</details></li>
</ul>
<hr>
<h2 id="Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models"><a href="#Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models" class="headerlink" title="Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models"></a>Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06534">http://arxiv.org/abs/2308.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wolfda95/ssl-medicalimagining-cl-mae">https://github.com/wolfda95/ssl-medicalimagining-cl-mae</a></li>
<li>paper_authors: Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Timo Ropinski, Michael Götz</li>
<li>for: 这篇论文旨在探讨deep learning在医疗影像领域中的应用，以减少诊断错误、轻量化医生工作负担，并加快诊断。</li>
<li>methods: 这篇论文使用了自动标注学习方法，包括对大量无标注影像进行自动标注。</li>
<li>results: 研究发现，使用SparK预训方法可以更好地适应小型标注数据，并且在诊断任务中表现更好。<details>
<summary>Abstract</summary>
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task. The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares state-of-the-art contrastive learning methods with the recently introduced masked autoencoder approach "SparK" for convolutional neural networks (CNNs) on medical images. Therefore we pre-train on a large unannotated CT image dataset and fine-tune on several CT classification tasks. Due to the challenge of obtaining sufficient annotated training data in medical imaging, it is of particular interest to evaluate how the self-supervised pre-training methods perform when fine-tuning on small datasets. By experimenting with gradually reducing the training dataset size for fine-tuning, we find that the reduction has different effects depending on the type of pre-training chosen. The SparK pre-training method is more robust to the training dataset size than the contrastive methods. Based on our results, we propose the SparK pre-training for medical imaging tasks with only small annotated datasets.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域可能减少诊断错误风险，减轻放射学家的工作负担，并加速诊断。深度学习模型的训练需要大量和准确的数据集，并将所有训练样本标注。然而，在医疗影像领域，特定任务的标注数据集经常很小，这可能由标注的复杂性、访问限制或疾病的罕见性引起。为解决这个挑战，可以使用自动标注学习的方法进行深度学习模型的预训练。在预训练后，只需要小量的标注数据集来精度地调整模型 для特定任务。医疗影像领域最受欢迎的自动标注预训练方法是对比学习。然而，最近的自然图像处理研究表明，遮盲 autoencoder 方法有很强的潜在性。我们的工作比较了当前状态的对比学习方法和新引入的遮盲 autoencoder 方法 "SparK" 在医疗影像中的 convolutional neural networks (CNNs) 上。因此，我们预训练在大量无注释 CT 图像数据集上，并在多个 CT 分类任务上进行精度调整。由于医疗影像领域获得足够的注释训练数据是困难的，因此特别关心自动标注预训练方法在小型注释数据集上的性能。通过逐渐减少 fine-tuning 数据集大小的实验，我们发现降低的效果与预训练方法的类型有很大的差异。SparK 预训练方法在训练数据集尺寸减少后表现更加稳定。根据我们的结果，我们建议使用 SparK 预训练方法进行医疗影像任务，只需要小量的注释训练数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices"><a href="#Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices" class="headerlink" title="Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices"></a>Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06528">http://arxiv.org/abs/2308.06528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakubkwiatkowski/abstract_compositional_transformer">https://github.com/jakubkwiatkowski/abstract_compositional_transformer</a></li>
<li>paper_authors: Jakub Kwiatkowski, Krzysztof Krawiec</li>
<li>for: 本研究旨在提高 Ravens 进步矩阵（RPM）问题的抽象逻辑能力，通过预测图像中对象的视觉特征和排序来选择答案。</li>
<li>methods: 本研究使用了一种基于 transformer 框架的深度学习模型，通过预测图像中对象的视觉特征和排序来选择答案。研究还考虑了不同的图像分割方法和自我指导学习策略。</li>
<li>results: 实验结果表明，本研究的模型不仅超越了当前最佳方法，还提供了有趣的思路和部分解释，以帮助理解问题的含义。此外，模型的设计还使其具有免疫一些已知 RPM  bencmarks 中的偏见的能力。<details>
<summary>Abstract</summary>
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details>
<details>
<summary>摘要</summary>
一个挑战在抽象逻辑学习中是，问题经常是单一任务，没有中间目标。在萨瑟进步矩阵（RPM）中，任务是根据Context选择可用的答案，Context和答案都是复杂的图像，包含多个物体在不同的空间排列。由于高级目标是唯一的指导，学习是困难的，而大多数当代解决方案都是透明的。在这种研究中，我们提出了基于变换器蓝图的深度学习架构，而不是直接选择上述高级目标，而是预测图像中物体的视觉属性和排列。 obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.Here's the translation in Traditional Chinese:一个挑战在抽象逻辑学习中是，问题经常是单一任务，没有中间目标。在萨瑟进步矩阵（RPM）中，任务是根据Context选择可用的答案，Context和答案都是复杂的图像，包含多个物体在不同的空间排列。由于高级目标是唯一的指导，学习是困难的，而大多数当代解决方案都是透明的。在这种研究中，我们提出了基于变数器蓝图的深度学习架构，而不是直接选择上述高级目标，而是预测图像中物体的视觉属性和排列。 obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details></li>
</ul>
<hr>
<h2 id="SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models"><a href="#SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models" class="headerlink" title="SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"></a>SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06522">http://arxiv.org/abs/2308.06522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, Salman Avestimehr</li>
<li>for: 这个论文主要针对的是如何使用 parameter efficient fine-tuning (PEFT) 方法在 Federated Learning (FL) 中进行语言任务的训练。</li>
<li>methods: 本文使用了 FL 技术和 PEFT 方法，并进行了实验研究以探讨在不同的数据场景下的可行性和挑战。</li>
<li>results: 实验结果表明，当用户数据变得更加多样化时，PEFT 方法与全量精度训练的性能差距逐渐增大。为了bridge这个性能差距，本文提出了一种名为 SLoRA 的方法，通过一种新的数据驱动初始化技术来超越 LoRA 在高多样数据场景下的限制。SLoRA 方法可以实现与全量精度训练相当的性能，并且可以减少训练时间，并且可以减少稀疏更新的数量。<details>
<summary>Abstract</summary>
Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data scenarios through a novel data-driven initialization technique. Our experimental results demonstrate that SLoRA achieves performance comparable to full fine-tuning, with significant sparse updates with approximately $\sim 1\%$ density while reducing training time by up to $90\%$.
</details>
<details>
<summary>摘要</summary>
通过精细调整已经训练过的变换器模型，通过中央化数据的缺失， Federated Learning (FL) 可以利用分布式和私有的 Edge 客户端数据进行 fine-tuning。然而，由于 Edge 设备的通信、计算和存储能力的限制，以及各种变换器模型的巨大大小，高效的 fine-tuning 是在 federated 训练中实现可行的。这种工作探讨了在语言任务中应用 parameter efficient fine-tuning (PEFT) 方法的机会和挑战。具体来说，我们的调查发现，当用户数据变得更加多样化时，具有完全 fine-tuning 模型和使用 PEFT 方法之间的性能差距变得更加明显。为了补做这个性能差距，我们提出了一种名为 SLoRA 的方法，通过一种新的数据驱动初始化技术，超越 LoRA 在高多样化数据场景中的关键限制。我们的实验结果表明，SLoRA 可以与完全 fine-tuning 性能相似，使用约 $\sim 1\%$ 的稀疏更新，同时降低训练时间达到 $90\%$。
</details></li>
</ul>
<hr>
<h2 id="One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training"><a href="#One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training" class="headerlink" title="One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training"></a>One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07934">http://arxiv.org/abs/2308.07934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianshuod/tba">https://github.com/jianshuod/tba</a></li>
<li>paper_authors: Jianshuo Dong, Han Qiu, Yiming Li, Tianwei Zhang, Yuanjie Li, Zeqi Lai, Chao Zhang, Shu-Tao Xia</li>
<li>for: 保持深度神经网络（DNNs）的安全性，因为它们在实际设备上广泛部署。</li>
<li>methods: 使用记忆FAULT INJECT技术，如行ammer，对量化模型进行攻击。只需要一些位置的变化，目标模型可以变成一个随机估计或者恶意功能模型。</li>
<li>results: 在基准数据集上，攻击者可以轻松地通过flipping一个关键位的变化，将高风险模型转换为恶意模型。此外，我们的攻击还能够绕过一些防御机制。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/jianshuod/TBA%7D">https://github.com/jianshuod/TBA}</a> 上复制。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipping only one critical bit} on average in the deployment stage. Moreover, our attack still poses a significant threat even when defenses are employed. The codes for reproducing main experiments are available at \url{https://github.com/jianshuod/TBA}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks"><a href="#Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks" class="headerlink" title="Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks"></a>Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06496">http://arxiv.org/abs/2308.06496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhigang Yan, Dong Li</li>
<li>for: 这个研究旨在测试和优化内存和通信参数以提高 Federated Learning（FL）的可靠性和效率。</li>
<li>methods: 这个研究使用了分布式 Federated Learning（DFL）框架，并使用了不同的通信方案（数位和分数）来分析它们的通信效率。</li>
<li>results: 研究发现，在不同的通信方案下，这个框架可以提供内存和通信参数的优化，以提高模型的训练效率和可靠性。<details>
<summary>Abstract</summary>
Federated learning (FL) can lead to significant communication overhead and reliance on a central server. To address these challenges, decentralized federated learning (DFL) has been proposed as a more resilient framework. DFL involves parameter exchange between devices through a wireless network. This study analyzes the performance of resource-constrained DFL using different communication schemes (digital and analog) over wireless networks to optimize communication efficiency. Specifically, we provide convergence bounds for both digital and analog transmission approaches, enabling analysis of the model performance trained on DFL. Furthermore, for digital transmission, we investigate and analyze resource allocation between computation and communication and convergence rates, obtaining its communication complexity and the minimum probability of correction communication required for convergence guarantee. For analog transmission, we discuss the impact of channel fading and noise on the model performance and the maximum errors accumulation with convergence guarantee over fading channels. Finally, we conduct numerical simulations to evaluate the performance and convergence rate of convolutional neural networks (CNNs) and Vision Transformer (ViT) trained in the DFL framework on fashion-MNIST and CIFAR-10 datasets. Our simulation results validate our analysis and discussion, revealing how to improve performance by optimizing system parameters under different communication conditions.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 可能会带来重要的通信负担和依赖中央服务器。为了解决这些挑战，分散式 federated learning (DFL) 已经被提议作为一个更可靠的框架。DFL 中各个设备之间的参数交换通过无线网络。本研究分析了受限制的 DFL 在无线网络上的执行效率，使用不同的通信方案（数位和分散）。特别是，我们提供了两种通信方案的整合边界值，以便分析模型在 DFL 中的表现。此外，我们还调查了在数位传输中的资源分配和计算和通信的复杂度，以及它们对模型表现的影响。另外，我们还分析了随机传输中频道折射和噪音对模型表现的影响，以及在折射频道上累累的最大错误累累。最后，我们对 fashion-MNIST 和 CIFAR-10 数据集上的 CNNs 和 ViT 在 DFL 框架中进行了数值模拟，以评估其表现和融合率。我们的模拟结果证实了我们的分析和讨论，显示了如何通过优化系统参数来改善表现，不同的通信条件下。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding"><a href="#Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding" class="headerlink" title="Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding"></a>Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06472">http://arxiv.org/abs/2308.06472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumari Nishu, Minsik Cho, Paul Dixon, Devang Naik</li>
<li>for: 这个论文的目的是提出一种高效的关键词检测方法，以便在 audio-text  embedding 空间中快速检测任意关键词。</li>
<li>methods: 该方法使用一个 audio-compliant 文本编码器，将文本转换为phonemes使用 G2P 模型，然后将phonemes转换为嵌入使用表示性强的音频编码器生成的phoneme вектор。此外，该方法还使用杂音词生成来提高audio-text embedding验证器的强度。</li>
<li>results: 实验结果表明，该方法在 Libriphrase 难 dataset 上超过了州�类-国度的Result（AUC：84.21% → 92.7%，EER：23.36% → 14.4%）， indicating that our scheme can efficiently detect arbitrary keywords in audio-text embedding space with high accuracy.<details>
<summary>Abstract</summary>
Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard dataset, increasing Area Under the ROC Curve (AUC) metric from 84.21% to 92.7% and reducing Equal-Error-Rate (EER) metric from 23.36% to 14.4%.
</details>
<details>
<summary>摘要</summary>
通常，用户定义/灵活关键词在文本中的检测使用昂贵的文本编码器进行联合分析与音频编码器在嵌入空间，这可能会导致不同类型的表达媒体表示（大匹配度差）和增加复杂性。在这种工作中，我们提出一种新的架构，可以有效地检测任意关键词基于兼容音频编码器的文本编码器，该编码器自然具有同 Audio embedding的同一个表示形式，而且比兼容的文本编码器更小。我们的文本编码器将文本转换为音频的phoneme使用图eme-to-phoneme（G2P）模型，然后将其转换为嵌入使用表示音频编码器中的可表示性phoneme вектор。我们进一步增强我们的方法，通过可混淆关键词生成来开发一个具有强 дискриминатив力的音频-文本嵌入验证器。实验结果表明，我们的方案在Libriphrase hard数据集上的Result outperform了状态的aru的结果，从84.21%提高到92.7%，并从23.36%降低到14.4%。
</details></li>
</ul>
<hr>
<h2 id="Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest"><a href="#Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest" class="headerlink" title="Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest"></a>Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06471">http://arxiv.org/abs/2308.06471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karthik R., Ramamoorthy A.</li>
<li>for: 这篇论文是为了研究森林覆盖率的预测，特别是通过VANYA模型，包括预测动物食肉动力学。</li>
<li>methods: 该论文使用了时间序列数据和人工智能技术，包括神经网络和推论算法。</li>
<li>results: 论文通过对亚马逊雨林数据进行预测，显示了VANYA模型的可靠性和准确性，并与其他预测器如LSTM、N-BEATS、RCN进行比较。<details>
<summary>Abstract</summary>
Intelligent automation supports us against cyclones, droughts, and seismic events with recent technology advancements. Algorithmic learning has advanced fields like neuroscience, genetics, and human-computer interaction. Time-series data boosts progress. Challenges persist in adopting these approaches in traditional fields. Neural networks face comprehension and bias issues. AI's expansion across scientific areas is due to adaptable descriptors and combinatorial argumentation. This article focuses on modeling Forest loss using the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest cover, demonstrated on Amazon Rainforest data against other forecasters like Long Short-Term Memory, N-BEATS, RCN.
</details>
<details>
<summary>摘要</summary>
智能自动化支持我们面对风暴、旱情和地震等自然灾害，由于最新的技术进步。算法学习在 neuroscience、遗传学和人机交互等领域得到了进步，时间序列数据也促进了进步。然而，在传统领域采用这些方法还存在挑战。神经网络具有理解和偏见问题。AI在科学领域的扩张归功于可变描述符和组合说服。本文通过使用VANYA模型，包括猎Predator Dinamics，预测森林覆盖率，并与Long Short-Term Memory、N-BEATS、RCN等预测器进行比较。
</details></li>
</ul>
<hr>
<h2 id="Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization"><a href="#Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization" class="headerlink" title="Tiny and Efficient Model for the Edge Detection Generalization"></a>Tiny and Efficient Model for the Edge Detection Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06468">http://arxiv.org/abs/2308.06468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xavysp/teed">https://github.com/xavysp/teed</a></li>
<li>paper_authors: Xavier Soria, Yachuan Li, Mohammad Rouhani, Angel D. Sappa</li>
<li>for: 本研究旨在提高图像Edge detection的简洁性、效率和通用性，对现有的State-of-the-art（SOTA）Edge detection模型进行改进。</li>
<li>methods: 本文提出了一种名为Tiny and Efficient Edge Detector（TEED）的轻量级卷积神经网络，只有58K个参数，比SOTA模型少了99.8%。该模型训练在BIPED dataset上只需30分钟左右，每个epoch只需5分钟左右。</li>
<li>results: 本文的提出的模型具有训练容易、快速收敛的特点，并且预测的边映射质量高。此外，本文还提出了一个新的边检测测试集，包括图像Edge detection和图像分割中常用的样本。代码可以在<a target="_blank" rel="noopener" href="https://github.com/xavysp/TEED%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/xavysp/TEED上下载。</a><details>
<summary>Abstract</summary>
Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which comprises samples from popular images used in edge detection and image segmentation. The source code is available in https://github.com/xavysp/TEED.
</details>
<details>
<summary>摘要</summary>
大多数高级计算机视觉任务都基于低级图像操作作为初始过程。操作如边检测、图像提高和超分解，为更高级图像分析提供基础。在这项工作中，我们考虑了三个主要目标：简单、高效和通用，因为当前状态之arte（SOTA）边检测模型在精度方面增加了复杂度。为 достичь这一目标，我们提出了小型和高效的边检测器（TEED），这是一个具有58000个参数的小 convolutional neural network，相对于状态之arte模型的0.2%。在BIPE dataset上训练TEED只需几分钟时间，每个epoch仅需5分钟或更少。我们的提议的模型容易训练，快速 converges于第一些epoch，而预测的边映射具有高质量。此外，我们还提出了一个新的边检测检验集，该集包括来自popular图像的边检测和图像分割领域的样本。代码可以在https://github.com/xavysp/TEED中下载。
</details></li>
</ul>
<hr>
<h2 id="Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks"><a href="#Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks" class="headerlink" title="Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks"></a>Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06467">http://arxiv.org/abs/2308.06467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Garaev, Bader Rasheed, Adil Khan</li>
<li>for: 挑战当代防御机制对于攻击性质变化的测试</li>
<li>methods: 使用 adversarial attacks  manipulate input data 以测试 DNN 的强项和普遍性</li>
<li>results: 发现 DNN 对于 $L_2$ 和 $L_{\infty}$ 攻击性质的差异，并通过对 DNN 表现的分析和可视化获得更深入的理解。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the impact of adversarial attack norms on DNN representations, focusing on samples subjected to $L_2$ and $L_{\infty}$ norm attacks. Further, we employ canonical correlation analysis, visualize the representations, and calculate the mean distance between these representations and various DNN decision boundaries. Our results reveal a significant difference between $L_2$ and $L_{\infty}$ norms, which could provide insights into the potential dangers posed by $L_{\infty}$ norm attacks, previously underestimated by the research community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors"><a href="#A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors" class="headerlink" title="A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors"></a>A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06464">http://arxiv.org/abs/2308.06464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Li, Minqing Zhang, Ke Niu, Yingnan Zhang, Xiaoyuan Yang</li>
<li>for: 本研究旨在提高掩埋检测性能，对高效视频编码标准（HEVC）中的动态vector域基于视频掩埋进行检测。</li>
<li>methods: 该研究提出了基于优化的动态vectorprediction（AMVP）技术的一种掩埋特征，即HEVC视频中的信息投射可能会破坏当地优化的动态vectorprediction（MVP）。然后，定义HEVC视频中的MVP优化率作为掩埋检测特征。</li>
<li>results: 通过在两个通用数据集上进行检测，研究发现，对于所有的覆盖视频，MVP优化率都为100%，而对于所有的掩埋视频，MVP优化率小于100%。因此，该掩埋方法可以准确地分辨覆盖视频和掩埋视频，并且在实际应用中具有无模型训练和低计算复杂度。<details>
<summary>Abstract</summary>
Among steganalysis techniques, detection against motion vector (MV) domain-based video steganography in High Efficiency Video Coding (HEVC) standard remains a hot and challenging issue. For the purpose of improving the detection performance, this paper proposes a steganalysis feature based on the optimality of predicted MVs with a dimension of one. Firstly, we point out that the motion vector prediction (MVP) of the prediction unit (PU) encoded using the Advanced Motion Vector Prediction (AMVP) technique satisfies the local optimality in the cover video. Secondly, we analyze that in HEVC video, message embedding either using MVP index or motion vector differences (MVD) may destroy the above optimality of MVP. And then, we define the optimal rate of MVP in HEVC video as a steganalysis feature. Finally, we conduct steganalysis detection experiments on two general datasets for three popular steganography methods and compare the performance with four state-of-the-art steganalysis methods. The experimental results show that the proposed optimal rate of MVP for all cover videos is 100\%, while the optimal rate of MVP for all stego videos is less than 100\%. Therefore, the proposed steganography scheme can accurately distinguish between cover videos and stego videos, and it is efficiently applied to practical scenarios with no model training and low computational complexity.
</details>
<details>
<summary>摘要</summary>
在隐藏分析技术中，对高效视频编码标准（HEVC）中的动态 vector domain-based 视频隐藏技术进行检测仍然是一个热点和挑战。为了提高检测性能，本文提出了基于预测动态 vector（MVP）的隐藏特征。首先，我们指出了HEVC视频中的预测单元（PU）使用高级动态 vector prediction（AMVP）技术预测的动态 vector prediction（MVP）满足了本地优化性。其次，我们分析了在HEVC视频中，使用MVP index或动态 vector differences（MVD）进行消息嵌入可能会破坏MVP的优化性。然后，我们定义HEVC视频中的MVP优化率作为隐藏特征。最后，我们对两个通用数据集上三种流行的隐藏方法进行了隐藏检测实验，并与四种现状顶尖隐藏检测方法进行比较。实验结果显示，提议的MVP优化率对所有封装视频是100%，而对所有隐藏视频是less than 100%。因此，提议的隐藏方法可以准确地分辨封装视频和隐藏视频，并且可以应用于实际场景中无模型训练和低计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Knowledge-Distillation"><a href="#Multi-Label-Knowledge-Distillation" class="headerlink" title="Multi-Label Knowledge Distillation"></a>Multi-Label Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06453">http://arxiv.org/abs/2308.06453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penghui-yang/l2d">https://github.com/penghui-yang/l2d</a></li>
<li>paper_authors: Penghui Yang, Ming-Kun Xie, Chen-Chen Zong, Lei Feng, Gang Niu, Masashi Sugiyama, Sheng-Jun Huang</li>
<li>for: 该 paper 是关于多标签学习的知识储存方法的研究，它针对现有的知识储存方法在多标签学习场景中的局限性，并提出了一种新的多标签知识储存方法。</li>
<li>methods: 该 paper 使用了分类预测器和学生网络，并将知识储存于 label-wise embeddings 中。它还利用了分类预测器的准确率来提高学生网络的分类性能。</li>
<li>results: 实验结果表明，该 paper 的方法可以避免知识冲突现象，并在多个 benchmark 数据集上达到了superior的性能。<details>
<summary>Abstract</summary>
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D
</details>
<details>
<summary>摘要</summary>
传统的知识填充方法通常是通过将教师网络的输出LOGIT或中间特征图传递给学生网络，这在多类单标学习中非常成功。然而，这些方法几乎无法扩展到多标学习场景，因为预测概率不同类别之间不相加，特征图中涉及到小类时可能被忽略。在这篇论文中，我们提出了一种新的多标知识填充方法。一方面，它利用多标学习问题的分类器的semantic知识，将问题分解成一系列的binary分类问题。另一方面，它利用标签wise嵌入结构来增强学习的特征表示的分化性。我们的实验结果表明，提案的方法可以避免标签之间的知识冲突，从而在多种 comparing方法的比较中达到更高的性能。我们的代码可以在：https://github.com/penghui-yang/L2D 查看。
</details></li>
</ul>
<hr>
<h2 id="Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More"><a href="#Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More" class="headerlink" title="Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More"></a>Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06448">http://arxiv.org/abs/2308.06448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Chanpuriya, Cameron Musco</li>
<li>for: 本 paper 是为了探讨图像 clustering 中的缺省结构，并提出了一种基于非正式矩阵分解的 probabilistic 模型，用于统一 clustering 和简化图像。</li>
<li>methods: 本 paper 使用的方法是基于非正式矩阵分解的一种 probabilistic 模型，用于模型图像的结构。该模型通过 Random Walk 过程的 фактор化来实现 clustering 和简化图像的同时进行。</li>
<li>results: 本 paper 的结果表明，使用该方法可以很好地处理具有缺省结构的图像，并且可以很好地处理一些涉及多类别的不约分类任务。<details>
<summary>Abstract</summary>
Algorithms for node clustering typically focus on finding homophilous structure in graphs. That is, they find sets of similar nodes with many edges within, rather than across, the clusters. However, graphs often also exhibit heterophilous structure, as exemplified by (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Grappling with such structure is typically left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic graph, as well as simple unsupervised learning tasks involving bipartite and tripartite clustering of orthographic and phonological data.
</details>
<details>
<summary>摘要</summary>
algorithm для clustering 通常是找到同类结点的结构。即它们在集群内部找到多个边，而不是跨集群。但是图 oftentimes 也具有异类结构，如（几乎）二分图和三分图，其中大多数边在集群之间。对这种结构的处理通常被归入图简化任务。我们提出了一种基于非负矩阵因子化的概率模型，该模型结合了 clustering 和简化，并提供了对任意图结构的模型化框架。我们的模型基于对图进行随机游走的过程的分解。它允许不受限制的参数化，通过简单的梯度下降优化。通过宽松化硬 clustering 到软 clustering，我们的算法将硬 clustering 问题转化为可解决的问题。我们在一个 sintetic 图上以及一些无监督学习任务中应用了我们的算法，包括orthographic 和 phonological 数据的二分和三分 clustering。
</details></li>
</ul>
<hr>
<h2 id="A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing"><a href="#A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing" class="headerlink" title="A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing"></a>A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06447">http://arxiv.org/abs/2308.06447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miladramzy/sequentialmetatransferpinns">https://github.com/miladramzy/sequentialmetatransferpinns</a></li>
<li>paper_authors: Milad Ramezankhani, Abbas S. Milani</li>
<li>for: 解决非线性偏微分方程（PDE）问题，即使在长时间域内。</li>
<li>methods: 使用 физи学 informed neural networks（PINNs）和sequential meta-transfer（SMT）学习框架。</li>
<li>results: 比传统PINNs更高效地解决复杂系统问题，并且具有更好的适应性。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework decomposes PDE's time domain into smaller time segments to create "easier" PDE problems for PINNs training. Then for each time interval, a meta-learner is assigned and trained to achieve an optimal initial state for rapid adaptation to a range of related tasks. Transfer learning principles are then leveraged across time intervals to further reduce the computational cost.Through a composites autoclave processing case study, it is shown that SMT is clearly able to enhance the adaptability of PINNs while significantly reducing computational cost, by a factor of 100.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经在解决非线性偏微分方程 (PDEs) 中获得了广泛应用，通过将物理法则 integrate 到 neural networks 的训练中，使其在科学和工程应用中脱颖而出。然而，传统 PINNs 仍然缺乏对复杂系统的准确描述能力，特别是在长时间领域中。此外，由于 PINNs 是设计来描述特定的 PDE 系统实现，因此缺乏能够快速适应新系统配置的一般化能力。这会导致 computationally expensive re-training from scratch 的问题。为了解决这些不足，这个研究提出了一个新的sequential meta-transfer (SMT) 学习框架，可以提供快速训练和高效适应 PINNs 的解决方案。具体来说，这个框架将 PDE 的时间领域 decomposed 为 smaller time segments，则将每个时间段赋予一个 meta-learner 进行训练，以实现快速适应一系列相关任务的能力。然后，通过将 transfer learning 原则应用到时间intervals，进一步降低 computional cost。通过一个 composite autoclave processing 案例研究，显示了 SMT 能够优化 PINNs 的适应能力，同时大幅降低 computional cost，比例为 100。
</details></li>
</ul>
<hr>
<h2 id="Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data"><a href="#Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data" class="headerlink" title="Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data"></a>Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06443">http://arxiv.org/abs/2308.06443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheol Jun Cho, Edward F. Chang, Gopala K. Anumanchipalli</li>
<li>for: 本研究旨在解决神经科学中复杂行为的神经实现问题，即找到真实表示神经数据的方法。</li>
<li>methods: 我们提出了一种新的无监督学习框架，神经幽默对应器（NLA），来找到有效、行为相关的神经表示。该方法通过对重复尝试的表示进行对应来学习交叉尝试中的共同信息。此外，我们还提出了一种完全可导时间扭曲模型（TWM）来解决尝试的时间不同问题。</li>
<li>results: 当应用于自然说话的内部电rocorticography（ECoG）数据时，我们的模型可以更好地表示行为，特别是在更低的维度空间中。TWM被验证了通过测量行为协调性 между对应的尝试。我们的框架比基线模型更好地学习了交叉尝试中的共同表示，并且当Visualized时，替换 manifold 显示了在尝试中共享的神经轨迹。<details>
<summary>Abstract</summary>
Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.
</details>
<details>
<summary>摘要</summary>
The key idea of NLA is to align representations across repeated trials to learn cross-trial consistent information. To achieve this, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space.The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.Here is the translation in Simplified Chinese:理解人类复杂行为的神经实现是神经科学的一个主要目标。为了实现这一目标，寻找真实的神经数据表示是非常困难的，因为行为的复杂性和神经信号的噪声比（SNR）都很低。在这里，我们提出了一种新的无监督学习框架——神经潜在适应器（NLA），以找到行为相关的神经表示。NLA的关键思想是将重复尝试的表示进行对齐，以学习跨试验可靠信息。为了实现这一点，我们提出了一种全部可导的时间折叠模型（TWM），以解决试验的时间不同问题。当应用于自然语言说话的电rocorticalography（ECoG）时，我们的模型可以比基eline模型更好地学习行为的表示，特别是在lower dimensional space中。TWM的实验验证了我们的模型可以更好地处理行为听起来的听起来的听起来，并且当Visualize的时候，曾经的折叠 manifold  revelas shared neural trajectories across trials。
</details></li>
</ul>
<hr>
<h2 id="A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media"><a href="#A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media" class="headerlink" title="A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media"></a>A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06436">http://arxiv.org/abs/2308.06436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Piao, Hong Gu, Aina Wang, Pan Qin</li>
<li>for: 解决Maxwell方程在不同媒质中的逆问题</li>
<li>methods: 使用Physics-informed neural networks (PINNs)和适应域训练策略</li>
<li>results: 在两个案例研究中证明了domain-adaptive PINN的有效性<details>
<summary>Abstract</summary>
Maxwell's equations are a collection of coupled partial differential equations (PDEs) that, together with the Lorentz force law, constitute the basis of classical electromagnetism and electric circuits. Effectively solving Maxwell's equations is crucial in various fields, like electromagnetic scattering and antenna design optimization. Physics-informed neural networks (PINNs) have shown powerful ability in solving PDEs. However, PINNs still struggle to solve Maxwell's equations in heterogeneous media. To this end, we propose a domain-adaptive PINN (da-PINN) to solve inverse problems of Maxwell's equations in heterogeneous media. First, we propose a location parameter of media interface to decompose the whole domain into several sub-domains. Furthermore, the electromagnetic interface conditions are incorporated into a loss function to improve the prediction performance near the interface. Then, we propose a domain-adaptive training strategy for da-PINN. Finally, the effectiveness of da-PINN is verified with two case studies.
</details>
<details>
<summary>摘要</summary>
马克斯威尔方程是一系列联动部分偏微分方程（PDEs），与 Лорен茨力学定律共同组成经典电磁学和电路Circuit。有效解决马克斯威尔方程是许多领域的关键，如电磁散射和天线设计优化。physics-informed neural networks（PINNs）已经表现出解决PDEs的强大能力。然而，PINNs仍然在不同媒体中解决马克斯威尔方程困难。为此，我们提出了域 adaptive PINN（da-PINN）解决Maxwell方程的 inverse problem在不同媒体中。首先，我们提出了媒体界面位置参数，将整个领域分解成多个子领域。然后，我们在损失函数中包含了电磁界面条件，以提高预测性能 near the interface。最后，我们提出了适应域训练策略 для da-PINN。Finally, da-PINN的效果被两个案例验证。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration"><a href="#Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration" class="headerlink" title="Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration"></a>Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06432">http://arxiv.org/abs/2308.06432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Zhang, Kun Huang, Mingchao Li, Songtao Yuan, Qiang Chen</li>
<li>for: 这 paper 的目的是预测 age-related macular degeneration (nAMD) 的发展。</li>
<li>methods: 这 paper 使用的方法包括 feature encoder、graph evolution module 和 feature decoder。具体来说，feature encoder 将输入 SD-OCT 图像转换为深度特征，然后 graph evolution module 预测了疾病发展过程在高维 latent space 中，并输出了预测的深度特征。最后，feature decoder 将预测的深度特征转换回 SD-OCT 图像。</li>
<li>results: 这 paper 的结果表明，SHENet 可以生成高质量的预测 SD-OCT 图像，同时保持疾病结构和内容的准确性。 qualitative 评估也表明，SHENet 的生成的 SD-OCT 图像比其他方法更有Visual effect。<details>
<summary>Abstract</summary>
Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement module to ensure the effectiveness of disease evolution learning and obtain realistic SD-OCT images by adversarial training. SHENet is validated on 383 SD-OCT cubes of 22 nAMD patients based on three well-designed schemes based on the quantitative and qualitative evaluations. Compared with other generative methods, the generative SD-OCT images of SHENet have the highest image quality. Besides, SHENet achieves the best structure protection and content prediction. Qualitative evaluations also demonstrate that SHENet has a better visual effect than other methods. SHENet can generate post-therapeutic SD-OCT images with both high prediction performance and good image quality, which has great potential to help ophthalmologists forecast the therapeutic effect of nAMD.
</details>
<details>
<summary>摘要</summary>
大多数现有的疾病预测方法在医学图像处理领域都属于两类，即图像到类别预测和图像到参数预测。只有少数作品强调图像到图像预测。与其他多个 horizons 预测不同，眼科医生更偏向于在单个 horizons 上展示更高的预测信任度，这是因为眼科疾病风险预测的偏好。我们提出了单个 horizon 疾病进化网络（SHENet），用于预测基于前治疗 SD-OCT 图像的后治疗 SD-OCT 图像。在 SHENet 中，一个特征编码器将输入 SD-OCT 图像转换为深度特征，然后一个图像进化模块预测疾病进化的过程在高维latent空间中，并输出预测的深度特征。最后，特征解码器将预测的深度特征恢复为 SD-OCT 图像。我们还提出了进化权威模块，以确保疾病进化学习的效果和获得实际的 SD-OCT 图像。SHENet 在 383 个 SD-OCT 立方体上基于三种良好的方案进行验证，并通过量化和质量评价来评估其效果。与其他生成方法相比，SHENet 生成的 SD-OCT 图像的生成质量最高。此外，SHENet 还实现了最好的结构保护和内容预测。质量评价还表明，SHENet 的视觉效果比其他方法更好。SHENet 可以生成具有高预测性和好的图像质量的后治疗 SD-OCT 图像，这有很大的潜在价值，可以帮助眼科医生预测 nAMD 的治疗效果。
</details></li>
</ul>
<hr>
<h2 id="Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science"><a href="#Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science" class="headerlink" title="Genetic heterogeneity analysis using genetic algorithm and network science"></a>Genetic heterogeneity analysis using genetic algorithm and network science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06429">http://arxiv.org/abs/2308.06429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhendong Sha, Yuanzhu Chen, Ting Hu</li>
<li>For: This paper is written to address the challenges of identifying disease susceptible genetic variables using genome-wide association studies (GWAS) due to genetic heterogeneity and feature interactions.* Methods: The paper introduces a novel feature selection mechanism for GWAS called Feature Co-selection Network (FCSNet), which extracts heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA) and a non-linear machine learning algorithm to detect feature interactions.* Results: The paper shows the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis, and applies the novel approach to a case-control colorectal cancer GWAS dataset, resulting in synthetic features that explain the genetic heterogeneity in an additional case-only GWAS dataset.Here’s the simplified Chinese version of the three key points:* For: 这篇论文是为了解决基因组宽协调研究（GWAS）中疾病抵触性基因变量的难题，因为基因多样性和特征交互。* Methods: 论文提出了一种新的特征选择机制，即特征共选网络（FCSNet），它从多个独立的特征选择跑程中提取了多种不同的基因变量，并使用一种进化学习算法（GA）和一种非线性机器学习算法来检测特征交互。* Results: 论文通过synthetic数据分析表明了GA基于的特征选择方法的效果，并应用了这种新方法到一个case-control大肠癌GWAS数据集中，得到了解释基因多样性的Synthetic特征。<details>
<summary>Abstract</summary>
Through genome-wide association studies (GWAS), disease susceptible genetic variables can be identified by comparing the genetic data of individuals with and without a specific disease. However, the discovery of these associations poses a significant challenge due to genetic heterogeneity and feature interactions. Genetic variables intertwined with these effects often exhibit lower effect-size, and thus can be difficult to be detected using machine learning feature selection methods. To address these challenges, this paper introduces a novel feature selection mechanism for GWAS, named Feature Co-selection Network (FCSNet). FCS-Net is designed to extract heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA), an evolutionary learning algorithm. We employ a non-linear machine learning algorithm to detect feature interaction. We introduce the Community Risk Score (CRS), a synthetic feature designed to quantify the collective disease association of each variable subset. Our experiment showcases the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis. Furthermore, we apply our novel approach to a case-control colorectal cancer GWAS dataset. The resulting synthetic features are then used to explain the genetic heterogeneity in an additional case-only GWAS dataset.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:通过全 genomic协同asso ciation研究 (GWAS)，可以通过比较患病者和无病者的基因数据来确定疾病抵触性的基因变量。然而，发现这些相互作用具有一定的挑战，因为基因多样性和特征互动。基因变量与这些效应相互作用的情况经常表现出较低的效果大小，因此可能会难以通过机器学习特征选择方法探测。为解决这些挑战，本文提出了一种新的特征选择机制，名为特征合选网络 (FCSNet)。FCS-Net 是基于多个独立的特征选择跑目的基因算法 (GA) 构建的网络，并使用一种非线性机器学习算法探测特征互动。我们还引入了一个社区风险分数 (CRS)，用于量化每个变量subset 的疾病相关度。我们的实验表明，使用我们提出的 GA 基因选择方法可以通过 sintetic 数据分析来识别特征互动。此外，我们还应用了我们的新方法到一个 case-control 大陆癌 GWAS 数据集。得到的 sintetic 特征后来用于解释一个额外的 case-only GWAS 数据集中的基因多样性。
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Learnability-Does-Not-Imply-Sample-Compression"><a href="#Multiclass-Learnability-Does-Not-Imply-Sample-Compression" class="headerlink" title="Multiclass Learnability Does Not Imply Sample Compression"></a>Multiclass Learnability Does Not Imply Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06424">http://arxiv.org/abs/2308.06424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chirag Pabbaraju</li>
<li>for: 本研究证明了一个假设集合可以 admit 一种样本压缩 schemes，即对于每个样本被标注为来自该假设集合中的某个假设，只需保留一小样本，可以推断出整个样本的标签。</li>
<li>methods: 本研究使用了learnable binary hypothesis class 和 multiclass hypothesis class，以及其们的VC dimension 和 DS dimension。</li>
<li>results: 本研究发现，learnable binary hypothesis class 总是可以 admit 一种样本压缩 schemes，但是 learnable multiclass hypothesis class 则不一定可以 admit 样本压缩 schemes，即不一定可以通过保留一小样本来推断出整个样本的标签。<details>
<summary>Abstract</summary>
A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
</details>
<details>
<summary>摘要</summary>
一个假设集合满足样本压缩方案，如果每个样本被标注为从集合中的假设，那么只需保留一小样本，使得整个样本上的标签可以被推断出。压缩方案的大小是样本上的子样本的上界。每个可学习的二进制假设集合（必然具有有限VC维度）总是具有一个只具有有限函数与样本大小无关的压缩方案。对多类假设集合，相应的VC维度的概念是DS维度。我们显示了，相应的假设集合不是真的：每个可学习的多类假设集合，必然具有有限DS维度，但并不总是具有只具有有限函数与DS维度无关的压缩方案。
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation"><a href="#Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation" class="headerlink" title="Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation"></a>Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06422">http://arxiv.org/abs/2308.06422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedarmin Azizi, Mahdi Nazemi, Arash Fayyazi, Massoud Pedram</li>
<li>for: 这个研究旨在提高深度学习模型的设计优化，以提高模型的效率。</li>
<li>methods: 本研究使用了一种新的搜寻机制，可以自动选择个别神经网络层的最佳位元和层宽。这些搜寻机制利用了希腊数-基于的删除，以确保删除不必要的参数。然后，我们使用了一种基于对应的树结构的Parzen估计器，以建立优化的对应模型。</li>
<li>results: 我们的方法在知名的数据集上进行了严谨的测试，与现有的方法相比， recording an impressive 20% decrease in model size without compromising accuracy. In addition, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available.<details>
<summary>Abstract</summary>
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 20% decrease in model size without compromising accuracy. Additionally, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available. As a result, our proposed method represents a leap forward in neural network design optimization, paving the way for quick model design and implementation in settings with limited resources, thereby propelling the potential of scalable deep learning solutions.
</details>
<details>
<summary>摘要</summary>
深度学习模型的复杂性和计算需求逐渐增长，因此选择最佳的神经网络层宽和批处理层宽成为了至关重要的一环。这项工作提出了一种新的搜索机制，可以自动选择具有最佳性能的神经网络层宽和批处理层宽。通过利用层次结构的Parzen估计器来构建封闭的搜索空间，我们可以快速地探索不同的建筑方案，并快速地找到最佳的设计。我们通过对知名数据集进行严格的测试，证明了我们的方法与现有方法相比，可以减少模型大小20%，同时保持准确性。此外，我们的方法可以减少搜索时间12倍，相比于目前最佳的搜索焦点策略。因此，我们的提议方法 represents a significant advance in neural network design optimization, paving the way for rapid model design and implementation in resource-constrained settings, and thereby accelerating the potential of scalable deep learning solutions.
</details></li>
</ul>
<hr>
<h2 id="Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review"><a href="#Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review" class="headerlink" title="Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review"></a>Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06419">http://arxiv.org/abs/2308.06419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 本研究旨在提供一种实用的行人轨迹预测算法，用于自动驾驶车辆（AV）在与行人共同使用的空间中规划路径。</li>
<li>methods: 本文系统性地查询了在文献中提出的不同方法，用于模拟行人轨迹预测在交通工具存在下。文中还详细讨论了与交通工具交互对行人未来动向的影响，以及不同变量如预测不确定性和行为差异如何在先前提出的预测模型中被考虑。</li>
<li>results: 文中提出了1260个唯一的同行评审文章，从ACM数字图书馆、IEEE Xplore和Scopus数据库中搜索到。64篇文章符合包含和排除条件，因此被包含在最终审查中。文中还提供了各种轨迹数据集的概述，包括行人和交通工具的轨迹数据。文中还讨论了未来研究中的潜在漏洞和方向，如更有效的交互代理在深度学习方法中定义，以及更多的混合交通环境中的数据采集。<details>
<summary>Abstract</summary>
Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed.
</details>
<details>
<summary>摘要</summary>
планирование пути автономного транспортного средства (АВ) в пространстве, где присутствуют пешеходы, требует рассмотрения предполагаемых траекторий пешеходов в будущем. практический алгоритм предсказания траекторий пешеходов для использования АВов в неструктурированных средах должен учитывать влияние взаимодействия автомобиля с пешеходами на будущие движения пешеходов. в этом смысле, эта статья систематически обзорывает разные методы, предложенные в литературе для моделирования предсказания траекторий пешеходов в присутствии автомобилей, которые могут быть применены в неструктурированных средах. эта статья также рассматривает конкретные аспекты взаимодействия пешехода-автомобиль (в сравнении с взаимодействием пешеходов-пешеходов) и обсуждает, как различные переменные, такие как неопределенности предсказаний и различия в поведении, учитываются в предыдущих моделях предсказания. following PRISMA guidelines, articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. a total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. an overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments, are discussed.
</details></li>
</ul>
<hr>
<h2 id="Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering"><a href="#Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering" class="headerlink" title="Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering"></a>Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06399">http://arxiv.org/abs/2308.06399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Vallegi, Marco Scutari, Federico Mattia Stefanini</li>
<li>for:  agronomic studies, handling hierarchical data with complex networks of causal relationships</li>
<li>methods:  Bayesian networks (BNs) with integrated random effects, based on linear mixed-effects models</li>
<li>results:  enhanced structural learning, discovery of new connections, improved model specification, reduction in prediction errors from 28% to 17%<details>
<summary>Abstract</summary>
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.   Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery of new connections and the improvement of improved model specification. Furthermore, we observe a reduction in prediction errors from 28\% to 17\%. By extending the applicability of BNs to complex data set structures, this approach contributes to the effective utilisation of BNs for hierarchical agronomic data. This, in turn, enhances their value as decision-support tools in the field.
</details>
<details>
<summary>摘要</summary>
研究涉及多元相关数据集，其中covariates和结果变量之间存在关系的现象，在各个领域，如农学研究，非常普遍。在这些情况下，层次模型，也称为多级模型，经常被使用，以融合不同数据集的信息，同时适应它们的特点。然而，这些结构超出了简单的不同性，因为变量经常形成复杂的 causal 关系网络。� Bayesian networks（BNs）提供了一个强大的模型化这些关系的框架，使用导向的无环图来示出变量之间的连接。本研究提出了一种新的方法，将随机效应 integrate into BN 学习。基于线性混合效应模型，这种方法特别适用于处理层次数据。实际 agronomic 试验结果表明，通过使用这种方法，可以提高结构学习，发现新的连接，并提高模型规定的精度。此外，我们发现预测错误率从28%降至17%。通过扩展 BNs 的应用范围，使其能够更好地处理复杂数据集结构，这种方法增加了 BNs 作为决策支持工具的价值。
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models"><a href="#Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models" class="headerlink" title="Detecting and Preventing Hallucinations in Large Vision Language Models"></a>Detecting and Preventing Hallucinations in Large Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06394">http://arxiv.org/abs/2308.06394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Gunjal, Jihan Yin, Erhan Bas<br>for:The paper aims to address the issue of hallucinations in instruction-tuned large vision language models (LVLMs) for visual question answering (VQA).methods:The authors introduce a new dataset called M-HalDetect, which is a multi-modal hallucination detection dataset for detailed image descriptions. They also propose a novel optimization method called Fine-grained Direct Preference Optimization (FDPO) to reduce hallucinations in LVLMs.results:The authors evaluate the effectiveness of M-HalDetect and FDPO on several state-of-the-art LVLMs, including InstructBLIP, LLaVA, and mPLUG-OWL. They find that M-HalDetect can reduce hallucination rates in InstructBLIP by 41%, and FDPO can reduce hallucination rates by 55%. Additionally, they find that their reward model generalizes well to other multi-modal models and has a strong correlation with human evaluated accuracy scores.<details>
<summary>Abstract</summary>
Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.
</details>
<details>
<summary>摘要</summary>
现代化的启示抽象语言模型（LVLM）在多模态任务上进行总结的能力已经得到了显著提高，尤其是在视觉问答（VQA）领域。然而，使模型生成具有详细Visualgrounding的回答仍然是一个挑战。我们发现，even the current state-of-the-art LVLMs（InstructBLIP）仍然包含了30%的虚假文本，包括不存在的物体、不准确的描述和关系。为解决这个问题，我们介绍了M-HalDetect，一个多模态虚假检测 dataset，可以用于训练和测试模型，以避免虚假检测。M-HalDetect包含16k细致的VQA例子，使其成为了首个多模态虚假检测 dataset。与前一代研究只考虑对象虚假，我们还注释了不准确的实体描述和关系。为证明M-HalDetect的潜在性，我们通过我们的新的精细直接偏好优化（FDPO）来优化InstructBLIP。我们还通过多模态 reward models来训练精细的多模态奖励模型，并通过best-of-n拒绝采样来评估其效果。我们对FDPO和拒绝采样进行了人工评估，并发现它们可以降低InstructBLIP中的虚假率 by 41%和55%。此外，我们发现我们的奖励模型可以泛化到其他多模态模型，降低LLaVA和mPLUG-OWL中的虚假率 by 15%和57%，并与人类评估准确率有强相关性。
</details></li>
</ul>
<hr>
<h2 id="Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion"><a href="#Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion" class="headerlink" title="Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion"></a>Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06382">http://arxiv.org/abs/2308.06382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PhonemeHallucinator/Phoneme_Hallucinator">https://github.com/PhonemeHallucinator/Phoneme_Hallucinator</a></li>
<li>paper_authors: Siyuan Shan, Yang Li, Amartya Banerjee, Junier B. Oliva</li>
<li>for: 这篇论文的目的是提出一种新的语音变换技术，以提高语音变换的Intelligibility和Speaker Similarity。</li>
<li>methods: 这篇论文使用了一种新的模型，即“Phoneme Hallucinator”，可以基于短时间内的目标说话人声音（例如3秒）生成多样化和高质量的目标说话人音频。</li>
<li>results: 对比于现有的语音变换方法，“Phoneme Hallucinator”在Intelligibility和Speaker Similarity两个指标上均表现出色，并且不需要文本标注和支持任意转换。<details>
<summary>Abstract</summary>
Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.
</details>
<details>
<summary>摘要</summary>
声音转换（VC）目标是使一个人的声音与另一个人的声音相似，同时保持语言内容的正确性。现有的方法受到一种权衡问题：即具有更高的智能可读性通常具有较低的说话人类似性，而具有更高的说话人类似性通常需要大量的目标说话人声音数据来实现高度的智能可读性。在这项工作中，我们提出了一种新的方法——《phoneme hallucinator》。这是一个一枚VC模型，它采用了一种新的模型来幻化具有多样性和高精度的目标说话人声音，基于短时间内的目标说话人声音（例如3秒）。这些幻化的声音然后被利用来进行邻居基于的声音转换。我们的模型是文本 libre，任何到任何的VC模型，不需要文本注释，并且支持转换到任何未看过的说话人。对象和主观评估表明，《phoneme hallucinator》比既有VC方法更高的智能可读性和说话人类似性。
</details></li>
</ul>
<hr>
<h2 id="DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System"><a href="#DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System" class="headerlink" title="DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System"></a>DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06378">http://arxiv.org/abs/2308.06378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Yeganejou, Kimia Honari, Ryan Kluzinski, Scott Dick, Michael Lipsett, James Miller</li>
<li>for: 提高可解释人工智能中的透明度和准确性之间的负面选择。</li>
<li>methods: 使用深度神经网络和规则引擎结合深度学习模型，设计了一种新的深度征函数逻辑决策系统（DCNFIS），并证明DCNFIS可以与现有的卷积神经网络相比，在四个常见的数据集上达到相同的准确性。</li>
<li>results: DCNFIS可以在Fashion-MNIST数据集上生成saliency map，并且对这些解释进行了进一步的研究。<details>
<summary>Abstract</summary>
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释人工智能中是论文知名的质量和可读性之间的贸易。我们报告了一种新的深度网络的设计，该网络可以提高可读性而不 sacrificing 精度。我们设计了一种深度卷积神经推理系统（DCNFIS），通过将神经网络和推理逻辑模型相结合，并证明 DCNFIS 与三种现有的卷积神经网络在四个常见数据集上的性能相同。此外，我们还证明 DCNFIS 在深度推理系统中表现更好。然后，我们利用推理逻辑的可读性，从 DCNFIS 中提取出解释，以干扰 maps 的形式。我们对 Fashion-MNIST 数据集进行更深入的调查，以explore 这些解释的性质。
</details></li>
</ul>
<hr>
<h2 id="UAMM-UBET-Automated-Market-Maker"><a href="#UAMM-UBET-Automated-Market-Maker" class="headerlink" title="UAMM: UBET Automated Market Maker"></a>UAMM: UBET Automated Market Maker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06375">http://arxiv.org/abs/2308.06375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jiwoong Im, Alexander Kondratskiy, Vincent Harvey, Hsuan-Wei Fu</li>
<li>for: 这篇论文是为了解决传统自适应市场制定价机制（AMM）的局限性，提出了一种新的价格计算方法——UBET AMM（UAMM）。</li>
<li>methods: UAMM使用外部市场价格和流动性池的不稳定损失来计算价格，并保持愿意产品曲线的定量性。关键元素是根据目标均衡来确定合适的滑动量，以避免流动性池的不稳定损失。</li>
<li>results: 我们的方法可以在有效的外部市场价格下消除投资机会。<details>
<summary>Abstract</summary>
Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.
</details>
<details>
<summary>摘要</summary>
自动化市场制造机制（AMM）是分布式交易所（DEX）中使用的价格计算机制。传统的 AMM 方法受限于基于自己的流动性池价格计算，不考虑外部市场或流动性提供者风险管理。在这篇论文中，我们提出了一种新的方法，称为 UBET AMM（UAMM），它根据外部市场价格和流动性池的不稳定损失计算价格。尽管依赖于外部市场价格，我们的方法保持了恒定的产品曲线的属性，当计算滑块时。UBET AMM 的关键元素是确定合适的滑块量，以达到目标均衡。这种办法鼓励流动性池减少不稳定损失。我们示出，当外部市场价格高效时，我们的方法可以消除投资机会。
</details></li>
</ul>
<hr>
<h2 id="Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems"><a href="#Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems" class="headerlink" title="Topic-Level Bayesian Surprise and Serendipity for Recommender Systems"></a>Topic-Level Bayesian Surprise and Serendipity for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06368">http://arxiv.org/abs/2308.06368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ton-moy/surprise-and-serendipity">https://github.com/ton-moy/surprise-and-serendipity</a></li>
<li>paper_authors: Tonmoy Hasan, Razvan Bunescu<br>for: This paper aims to mitigate the filter bubble problem in recommender systems by incorporating serendipity into the recommendation process.methods: The paper proposes a content-based formulation of serendipity that is rooted in Bayesian surprise, and uses this formulation to measure the serendipity of items after they are consumed and rated by the user. The paper also introduces a collaborative-filtering component that identifies similar users.results: The experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.<details>
<summary>Abstract</summary>
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.
</details>
<details>
<summary>摘要</summary>
一个推荐系统可以将推荐项目单独根据用户的项目点击历史进行最佳化，从而创建一个范本径（filter bubble），使用户不会获得来自新、未见类别的项目。为了解决这个问题，我们可以推荐项目具有高度的意外性，即让用户惊喜的项目，这些项目很可能会获得高度的评价。在这篇论文中，我们提出了一个基于 bayesian 的内容基式，用于衡量项目的意外性，并且与协同推荐 ком成组件相结合，从而为用户提供意外性高的项目推荐。为了促进项目级模型的surprise和serendipity的评估，我们创建了一个基于goodreads的阅读历史数据集，包含26,000名用户和1,300,000本书，并 manually annotate 449本书，其中4名用户在不同的时间点阅读这些书籍。实验结果显示，使用 bayesian  surprise 可以与距离基于的规律更好地与手动标注的题目级surprise相对较好，并且也可以获得更好的意外性项目推荐性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Distributions-via-Monte-Carlo-Marginalization"><a href="#Learning-Distributions-via-Monte-Carlo-Marginalization" class="headerlink" title="Learning Distributions via Monte-Carlo Marginalization"></a>Learning Distributions via Monte-Carlo Marginalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06352">http://arxiv.org/abs/2308.06352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenqiu Zhao, Guanfang Dong, Anup Basu</li>
<li>for: 学习难以求解的分布。</li>
<li>methods: 使用参数化分布模型（如混合型分布）来 aproximate 难以求解的分布，并使用 Monte-Carlo Marginalization 和 Kernel Density Estimation 解决计算复杂性和优化过程不可导的问题。</li>
<li>results: 提出了一种可以学习复杂分布的方法，该方法可以替代变量推理（VAE），并在标准数据集和 sintetic 数据上进行了实验，证明了该方法的效果。<details>
<summary>Abstract</summary>
We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned by the proposed approach can generate better images even based on a pre-trained VAE's decoder. Based on this point, we devise a distribution learning auto-encoder which is better than VAE under the same network architecture. Experiments on standard dataset and synthetic data demonstrate the efficiency of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于从样本中学习不可解 Distribution。主要想法是使用参数化分布模型，如 Gaussian Mixture Model（GMM），来近似不可解 Distribution，并且通过最小化KL-分布来实现。然而，存在两个挑战：首先，在分布维度增加时，KL-分布的计算复杂度过高；其次，目标分布是不可导的，因此Optimization过程中的导数不存在。我们解决了这两个问题，使用Monte-Carlo Marginalization（MCMarg）和Kernel Density Estimation（KDE）。我们的方法可以学习复杂的分布，整个过程都是导数可导的，因此可以作为VAE中的更好的替补。我们的方法可以在标准数据集和synthetic数据上实现，并且在具有相同网络架构下，我们设计了一个更好的分布学习自动编码器，比VAE更好。Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have been translated differently in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Mirror-Diffusion-Models"><a href="#Mirror-Diffusion-Models" class="headerlink" title="Mirror Diffusion Models"></a>Mirror Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06342">http://arxiv.org/abs/2308.06342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cran/DIMORA">https://github.com/cran/DIMORA</a></li>
<li>paper_authors: Jaesung Tae</li>
<li>for: 本研究旨在应用扩散模型到分类数据领域，并提出了一种基于镜像Langevin算法的受限采样问题的理论框架。</li>
<li>methods: 本文提出了一种基于镜像扩散模型的受限采样算法，并在简单的顺序扩散问题上进行了实验 validate。</li>
<li>results: 研究表明，镜像扩散模型在受限采样问题上具有良好的性能，并且可以在各种流行的领域，如图像和文本生成等，进行自然的扩展。<details>
<summary>Abstract</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
</details>
<details>
<summary>摘要</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.Here's the translation in Simplified Chinese:diffusion模型在各种连续领域中已经成功应用于生成任务。然而，将diffusion应用于分类数据仍然是一个非常困难的任务。此外，在连续领域中的生成通常需要剪辑在实践中，这引发了需要适应受限的领域的理论框架。以mirror langevin算法为 inspirations，在这份理论报告中我们提出了镜像扩散模型（MDM）。我们在简单领域中示例了MDM，并提出了自然的扩展到流行的领域，如图像和文本生成。
</details></li>
</ul>
<hr>
<h2 id="Size-Lowerbounds-for-Deep-Operator-Networks"><a href="#Size-Lowerbounds-for-Deep-Operator-Networks" class="headerlink" title="Size Lowerbounds for Deep Operator Networks"></a>Size Lowerbounds for Deep Operator Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06338">http://arxiv.org/abs/2308.06338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirbit Mukherjee, Amartya Roy</li>
<li>for: 本研究目的是为了确定深度运算网络（DeepONet）解决杂项问题所需的数据大小下限。</li>
<li>methods: 本研究使用了数据依赖的下界来证明深度运算网络需要一定的数据大小来实现低训练错误。特别是，我们证明在$n$个数据点上获得低训练错误需要通过增加分支网络和主干网络的公共输出维度的扩展。</li>
<li>results: 我们通过实验示例，表明在固定模型大小下，通过增加公共输出维度，可以实现 monotonic 下降的训练错误。此外，我们还发现，随着数据大小的增加，训练错误会 quadratic 下降。<details>
<summary>Abstract</summary>
Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
</details>
<details>
<summary>摘要</summary>
深度运算网络（DeepONet）是一种越来越受欢迎的方法，用于解决无穷维度上的回归问题，并且可以一步解决多个偏微分方程（PDE）。在这项工作中，我们想要建立一个数据依赖的下界，以确定深度运算网络的大小是否足够减少噪声数据上的实际错误。我们显示，为了在 $n$ 个数据点上获得低训练错误， THENET 的通用输出维度和树网络的输出维度必须Scaling as $\Omega \left ( \sqrt{n} \right )$.这种情况 inspirits我们对 DeepONet 解决扩散吸引反应PDE的实验，我们示出，随着模型大小不变，通过增加common output维度，在fixed模型大小下，可以实现 monotonic 下降的训练错误。这意味着，在训练数据规模增加时，可能需要随着common output维度的增加，来降低训练错误。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector"><a href="#Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector" class="headerlink" title="Foundation Model is Efficient Multimodal Multitask Model Selector"></a>Foundation Model is Efficient Multimodal Multitask Model Selector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06262">http://arxiv.org/abs/2308.06262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/multitask-model-selector">https://github.com/opengvlab/multitask-model-selector</a></li>
<li>paper_authors: Fanqing Meng, Wenqi Shao, Zhanglin Peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo<br>for:This paper addresses an under-explored problem in the field of multi-modal multi-task learning: predicting the performance of pre-trained neural networks on various tasks without fine-tuning them.methods:The proposed method, called EMMS (Efficient Multi-task Model Selector), employs large-scale foundation models to transform diverse label formats into a unified noisy label embedding, and uses a simple weighted linear regression to estimate a model’s transferability.results:EMMS achieves significant performance gains (9.0%, 26.3%, 20.1%, 54.8%, 12.2%) and speedup (5.13x, 6.29x, 3.59x, 6.19x, 5.66x) compared to the state-of-the-art method LogME, while being fast and effective for assessing the transferability of pre-trained models in a multi-task scenario.<details>
<summary>Abstract</summary>
This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0\%, 26.3\%, 20.1\%, 54.8\%, 12.2\% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose an efficient multi-task model selector (EMMS), which uses large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS estimates a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee.Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models. Compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves a 9.0%, 26.3%, 20.1%, 54.8%, and 12.2% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, respectively, while bringing a 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Resilience-with-Neural-Networks"><a href="#Predicting-Resilience-with-Neural-Networks" class="headerlink" title="Predicting Resilience with Neural Networks"></a>Predicting Resilience with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06309">http://arxiv.org/abs/2308.06309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen da Mata, Priscila Silva, Lance Fiondella</li>
<li>for: 这个论文探讨了系统能够抵抗破坏性事件的能力，并应用于多个领域。</li>
<li>methods: 该论文提出了三种人工神经网络（ANN）方法，包括 искусственный神经网络（ANN）、循环神经网络（RNN）和长短期记忆神经网络（LSTM），用于模拟和预测系统性能，包括负因素和正因素对系统抵抗力的影响。</li>
<li>results: 研究结果显示，人工神经网络模型在所有评价指标上都超过了传统模型，特别是LSTM模型的可变R平方和预测误差分别下降了34倍和60%以上。这些结果表明，人工神经网络模型可能在许多重要领域中找到实际应用。<details>
<summary>Abstract</summary>
Resilience engineering studies the ability of a system to survive and recover from disruptive events, which finds applications in several domains. Most studies emphasize resilience metrics to quantify system performance, whereas recent studies propose statistical modeling approaches to project system recovery time after degradation. Moreover, past studies are either performed on data after recovering or limited to idealized trends. Therefore, this paper proposes three alternative neural network (NN) approaches including (i) Artificial Neural Networks, (ii) Recurrent Neural Networks, and (iii) Long-Short Term Memory (LSTM) to model and predict system performance, including negative and positive factors driving resilience to quantify the impact of disruptive events and restorative activities. Goodness-of-fit measures are computed to evaluate the models and compared with a classical statistical model, including mean squared error and adjusted R squared. Our results indicate that NN models outperformed the traditional model on all goodness-of-fit measures. More specifically, LSTMs achieved an over 60\% higher adjusted R squared, and decreased predictive error by 34-fold compared to the traditional method. These results suggest that NN models to predict resilience are both feasible and accurate and may find practical use in many important domains.
</details>
<details>
<summary>摘要</summary>
这篇研究探讨了系统的恢复能力，包括系统在干扰事件后的恢复和回复。这些研究通常强调系统的恢复指标数量，而最近的研究则提出了使用统计模型估计系统的复原时间。然而，以往的研究通常是在资料复原后进行，或仅仅是对理想化趋势进行研究。因此，这篇研究提出了三种人工神经网络（ANN）方法，包括人工神经网络（ANN）、回传神经网络（RNN）和长短期内存（LSTM），以模拟和预测系统的性能，包括负面和正面因素的影响，以量化干扰事件的影响和恢复活动。我们使用了一个传统的统计模型，包括平均方差和调整乘根，来评估模型的适合度。我们的结果显示，ANN模型在所有适合度检查中表现较好，特别是LSTM模型在调整乘根上高于60%，且预测误差下降34倍。这些结果表明，ANN模型可以实际地和精确地预测系统的恢复能力，并在许多重要领域中找到实际应用。
</details></li>
</ul>
<hr>
<h2 id="FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods"><a href="#FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods" class="headerlink" title="FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods"></a>FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06248">http://arxiv.org/abs/2308.06248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visinf/funnybirds">https://github.com/visinf/funnybirds</a></li>
<li>paper_authors: Robin Hesse, Simone Schaub-Meyer, Stefan Roth</li>
<li>for: The paper is written for the field of explainable artificial intelligence (XAI), specifically to address the challenge of evaluating the effectiveness of XAI methods in a fully automatic and systematic manner.</li>
<li>methods: The paper proposes a novel synthetic vision dataset called FunnyBirds and accompanying automatic evaluation protocols to evaluate XAI methods. The dataset allows for semantically meaningful image interventions, such as removing individual object parts, which enables analyzing explanations on a part level and estimating ground-truth part importances.</li>
<li>results: The paper reports results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner. The results show that the proposed evaluation protocols are effective in identifying the strengths and weaknesses of different XAI methods.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解释人工智能（XAI）领域的一个挑战，具体来说是如何在完全自动和系统atic的方式下评估XAI方法的有效性。</li>
<li>methods: 论文提出了一个新的 sintetic vision dataset名为FunnyBirds，以及一系列的自动评估协议，用于评估XAI方法。该dataset允许进行semantic meaningful的图像干扰，例如 removing各个物体部分，这使得可以分析解释在每个部分上的含义。</li>
<li>results: 论文报告了24种不同的神经网络模型和XAI方法的结果，这些结果表明了评估方法的优劣。<details>
<summary>Abstract</summary>
The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details>
<details>
<summary>摘要</summary>
领域的解释人工智能（XAI）目标是探索复杂的深度神经网络模型的内部工作原理。虽然在安全关键领域非常重要，但XAI自然lacks ground-truth explanations，making its automatic evaluation an unsolved problem。我们解决这个挑战 by proposing a novel synthetic vision dataset named FunnyBirds， accompanied by automatic evaluation protocols。我们的数据集允许进行semantically meaningful image interventions，例如移除个体物体部分，这有三个重要的后果。首先，它允许分析解释在部件层次上进行分析，这更加接近人类理解的水平than existing methods that evaluate on a pixel level。其次，通过比较模型输出对具有移除部件的输入的比较，我们可以估算ground-truth part importances，这些importances应该被反映在解释中。最后，将各种解释映射到一个共同的部件importances空间，我们可以分析多种不同的解释类型在一个共同框架中。使用我们的工具，我们报告了24种不同的神经网络模型和XAI方法的结果，这些结果 Demonstrate the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details></li>
</ul>
<hr>
<h2 id="Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression"><a href="#Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression" class="headerlink" title="Private Distribution Learning with Public Data: The View from Sample Compression"></a>Private Distribution Learning with Public Data: The View from Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06239">http://arxiv.org/abs/2308.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shai Ben-David, Alex Bie, Clément L. Canonne, Gautam Kamath, Vikrant Singhal</li>
<li>for: 本文研究了在公共数据上进行隐私学习的问题，即公共私有学习。learner 是 Given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.</li>
<li>methods: 本文使用了 sample compression scheme for $\mathcal Q$ 和 list learning 来研究公共私有学习的可行性。</li>
<li>results: 本文的结果包括：(1) 回归 previous results on Gaussians over $\mathbb R^d$ ；(2) 适用于任意 $k$-mixtures of Gaussians over $\mathbb R^d$ 的新结果，包括学习复杂性上下文和分布转移抗性学习者的结果，以及 closure properties for public-private learnability under taking mixtures and products of distributions。此外，通过连接到 list learning，本文还证明了对 Gaussian 在 $\mathbb R^d$ 中，至少需要 $d$ 个公共样本来保证私有学习可行性，这与知道的Upper bound of $d+1$ 公共样本很接近。<details>
<summary>Abstract</summary>
We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.   We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.
</details>
<details>
<summary>摘要</summary>
我们研究了公共-私人学习问题，在这种设置下，学习者被公共和私人样本所访问，目标是输出一个未知分布$p$的估计，同时遵循隐私限制（这里是纯度ifferential privacy）只与私人样本相关。我们证明了公共-私人学习可能性与样本压缩 schemes for $\mathcal Q$ 以及 list learning 的存在有关，并且利用这种关系：1. 约束 previous results on Gaussians over $\mathbb R^d$ 的 approximately recovery;2. 导致新的结论，包括 $k$-mixtures of Gaussians over $\mathbb R^d$ 的 sample complexity upper bounds, agnostic 和 distribution-shift resistant learners,以及 distribution under taking mixtures and products of distributions. finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.
</details></li>
</ul>
<hr>
<h2 id="MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features"><a href="#MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features" class="headerlink" title="MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features"></a>MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06228">http://arxiv.org/abs/2308.06228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Chun Lee, Lipai Huang, Federico Antolini, Matthew Garcia, Andrew Juanb, Samuel D. Brody, Ali Mostafavi</li>
<li>for: 提供快速、准确和可靠的洪水信息，以支持洪水事件中的决策者、紧急管理人员和基础设施运营人员。</li>
<li>methods: 使用机器学习模型MaxFloodCast，基于物理基础的水动力学模拟，在哈里斯县进行训练，以实现高效和可解释的洪水涵顶深度预测。</li>
<li>results: MaxFloodCast模型在未见数据上达到了0.949的平均R-squared值和0.61 ft的Root Mean Square Error，表明其可靠地预测洪水涵顶深度。验证了飓风哈维和飓风伊梅拉达，MaxFloodCast模型显示出在近实时洪水管理和紧急应急管理中的潜在作用。<details>
<summary>Abstract</summary>
Timely, accurate, and reliable information is essential for decision-makers, emergency managers, and infrastructure operators during flood events. This study demonstrates a proposed machine learning model, MaxFloodCast, trained on physics-based hydrodynamic simulations in Harris County, offers efficient and interpretable flood inundation depth predictions. Achieving an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it proves reliable in forecasting peak flood inundation depths. Validated against Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in supporting near-time floodplain management and emergency operations. The model's interpretability aids decision-makers in offering critical information to inform flood mitigation strategies, to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area. The MaxFloodCast model enables accurate and interpretable inundation depth predictions while significantly reducing computational time, thereby supporting emergency response efforts and flood risk management more effectively.
</details>
<details>
<summary>摘要</summary>
时尚、准确、可靠的信息是决策者、紧急管理者和基础设施操作者在洪水事件中的基本需求。这个研究显示了一个提议的机器学习模型MaxFloodCast，在哈里斯县基于物理学 hydrodynamic 模拟中训练，可以提供高效和可解释的洪水淹没深度预测。在未见数据上，它实现了0.949的平均R-squared和0.61 ft的Root Mean Square Error，证明它在预测洪水淹没深度方面具有可靠性。验证了飓风哈维和飓风Imelda，MaxFloodCast显示了在近实时洪水平原管理和紧急作业中的潜在应用 potential。模型的可解释性帮助决策者对洪水缓和策略提供重要信息，优先级有critical facilities的区域，并考虑在一个区域中的降雨影响洪水暴露。MaxFloodCast模型可以提供高效和可解释的洪水淹没深度预测，同时大幅降低计算时间，以更好地支持紧急 Response efforts和洪水风险管理。
</details></li>
</ul>
<hr>
<h2 id="Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms"><a href="#Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms" class="headerlink" title="Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms"></a>Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06221">http://arxiv.org/abs/2308.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanishka Tyagi, Chinmay Rane, Michael Manry</li>
<li>for: 该论文旨在提出一种多步训练方法，用于设计通用线性分类器。</li>
<li>methods: 该方法包括初始化多类线性分类器，然后通过权重裁剪来降低验证错误，同时通过类似于霍-卡希普规则来改进期望输出。接着，输出推导器被拟合成为一个通用的多层感知器中的抑制器。</li>
<li>results: 该paper通过对多层感知器的搜索、验证错误的最小化和权重裁剪来提高总性能。此外，paper还提出了一种批处理算法，用于优化隐藏层大小和训练轮数。最终，paper通过对多层感知器进行权重裁剪和增长来提高其性能。<details>
<summary>Abstract</summary>
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for deep autoencoders. We investigate several problems in deep autoencoders networks including training issues, the theoretical, mathematical and experimental justification that the networks are linear, optimizing the number of hidden units in each layer and determining the depth of the deep learning model. A direct implication of the current work is the ability to construct fast deep learning models using desktop level computational resources. This, in our opinion, promotes our design philosophy of building small but powerful algorithms. Performance gains are demonstrated at each step. Using widely available datasets, the final network's ten fold testing error is shown to be less than that of several other linear, generalized linear classifiers, multi layer perceptron and deep learners reported in the literature.
</details>
<details>
<summary>摘要</summary>
我们提出了一种多步训练方法用于设计通用线性分类器。首先，通过回归获得初始多类线性分类器。然后，通过剔除不必要的输入，降低验证错误。同时，通过类似于霍-卡希普规则进行改进。接着，输出推定器被映射到通用线性分类器中的sigmoid输出单元中的核函数。我们然后开发了一种批处理训练算法，以优化隐藏层大小和训练轮次数。接着，我们结合剔除和增长策略。最后，输入单元被映射到sigmoid输出单元中的核函数，然后被输入到多层感知器中。我们提出了改进每个深度学习块的方法，从而提高整体深度学习模型的性能。我们讨论了深度学习算法的学习原理和形式化表述，并 investigate了深度学习网络中的许多问题，包括训练问题、理论、数学和实验上的正当性。我们的研究表明，通过使用桌面级计算机资源，可以快速构建高性能的深度学习模型。这与我们的设计哲学相吻合，即建立小而强大的算法。我们的实验表明，使用常用的数据集，最终网络的十倍测试错误比其他线性、通用线性分类器、多层感知器和深度学习者报道的较低。
</details></li>
</ul>
<hr>
<h2 id="Change-Point-Detection-With-Conceptors"><a href="#Change-Point-Detection-With-Conceptors" class="headerlink" title="Change Point Detection With Conceptors"></a>Change Point Detection With Conceptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06213">http://arxiv.org/abs/2308.06213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/noahgade/changepointdetectionwithconceptors">https://github.com/noahgade/changepointdetectionwithconceptors</a></li>
<li>paper_authors: Noah D. Gade, Jordan Rodu</li>
<li>for:  Identifying changes in the data generating process in time series with increasing dimension and temporal dependence.</li>
<li>methods:  Using a conceptor matrix to learn the characteristic dynamics of a specified training window, and a random recurrent neural network to featurize the data.</li>
<li>results:  A method that provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes and applied to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze.<details>
<summary>Abstract</summary>
Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes, and we evaluate performance with clustering metrics, graphical methods, and observed Type 1 error control. We apply our method to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze.
</details>
<details>
<summary>摘要</summary>
非线性变换点检测目的是检测时序序列中数据生成过程中的变化点。这个问题在独立Identical Distribution（i.i.d）数据上得到了广泛的研究，但是随着维度和时间相关性的增加，问题变得更加复杂。为了检测最多一个变换点，我们提议使用特征动力矩阵来学习指定的训练窗口中数据的特征动力。相关的随机回归神经网络作为数据的特征化器，变换点通过单variate量化的距离来确定与一个表征动力矩阵所生成的空间之间的距离。这种模型无关的方法可以提供有价值的可能性点，供进一步研究。我们证明，在某些假设下，该方法可以提供一个一致的变换点估计，并通过移动块bootstrap来生成quantile估计。我们在一些类型的过程的 simulations中测试了该方法，并根据集成度、图形方法和观察到的类型一错控制来评估性能。我们将该方法应用于公共可用的非 REM睡眠前的大鼠脑电信号。
</details></li>
</ul>
<hr>
<h2 id="Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey"><a href="#Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey" class="headerlink" title="Safety in Traffic Management Systems: A Comprehensive Survey"></a>Safety in Traffic Management Systems: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06204">http://arxiv.org/abs/2308.06204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlu Du, Ankan Dash, Jing Li, Hua Wei, Guiling Wang</li>
<li>for: 本研究提供了交通管理系统安全性的全面审查，包括交通管理系统中的安全问题、现有研究的当前状况以及提高交通管理系统安全性的技术和方法。</li>
<li>methods: 本研究审视了交通管理系统中的各种安全问题，包括技术问题、人因问题和管理问题，并评估了现有研究的当前状况和发展趋势。</li>
<li>results: 本研究结果表明，确保交通管理系统安全性是一个复杂的问题，需要考虑技术、人因和管理因素。现有的研究主要集中在技术方面，未来研究应该更加着重于人因和管理方面。<details>
<summary>Abstract</summary>
Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
</details>
<details>
<summary>摘要</summary>
交通管理系统在公路交通中扮演至关重要的角色，但是使用先进科技在交通管理系统中带来了新的安全挑战。因此，确保交通管理系统的安全性是非常重要的，以预防事故和最小化对交通路用者的影响。在本调查中，我们提供了交通管理系统安全的全面文献评审。具体来说，我们讨论了交通管理系统中不同的安全问题，现有的研究状况，以及确保交通管理系统安全的技术和方法。我们还识别了现有研究的限制，并建议未来研究的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.LG_2023_08_12/" data-id="clly3dvzi006c0988ea4ndom6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.SD_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.SD_2023_08_12/">cs.SD - 2023-08-12 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition"><a href="#Alternative-Pseudo-Labeling-for-Semi-Supervised-Automatic-Speech-Recognition" class="headerlink" title="Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition"></a>Alternative Pseudo-Labeling for Semi-Supervised Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06547">http://arxiv.org/abs/2308.06547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhu, Dongji Gao, Gaofeng Cheng, Daniel Povey, Pengyuan Zhang, Yonghong Yan</li>
<li>for: 提高自动语音识别器的性能在半监督学习中，当标注数据稀缺时</li>
<li>methods: 提出了一种名为替代pseudo-标签的框架，包括一个通用的CTC损失函数、一种自信度基于的错误检测方法和一种自动调整的阈值调整方法</li>
<li>results: 对比于传统的pseudo-标签筛选和改善pseudo-标签质量的方法，替代pseudo-标签的框架能够更好地适应噪音pseudo-标签的情况，并且不需要手动调整阈值<details>
<summary>Abstract</summary>
When labeled data is insufficient, semi-supervised learning with the pseudo-labeling technique can significantly improve the performance of automatic speech recognition. However, pseudo-labels are often noisy, containing numerous incorrect tokens. Taking noisy labels as ground-truth in the loss function results in suboptimal performance. Previous works attempted to mitigate this issue by either filtering out the nosiest pseudo-labels or improving the overall quality of pseudo-labels. While these methods are effective to some extent, it is unrealistic to entirely eliminate incorrect tokens in pseudo-labels. In this work, we propose a novel framework named alternative pseudo-labeling to tackle the issue of noisy pseudo-labels from the perspective of the training objective. The framework comprises several components. Firstly, a generalized CTC loss function is introduced to handle noisy pseudo-labels by accepting alternative tokens in the positions of incorrect tokens. Applying this loss function in pseudo-labeling requires detecting incorrect tokens in the predicted pseudo-labels. In this work, we adopt a confidence-based error detection method that identifies the incorrect tokens by comparing their confidence scores with a given threshold, thus necessitating the confidence score to be discriminative. Hence, the second proposed technique is the contrastive CTC loss function that widens the confidence gap between the correctly and incorrectly predicted tokens, thereby improving the error detection ability. Additionally, obtaining satisfactory performance with confidence-based error detection typically requires extensive threshold tuning. Instead, we propose an automatic thresholding method that uses labeled data as a proxy for determining the threshold, thus saving the pain of manual tuning.
</details>
<details>
<summary>摘要</summary>
当标注数据不充分时，半超vised学习采用 pseudo-labeling 技术可以显著提高自动语音识别的性能。然而，pseudo-标签通常含有许多错误的符号。将含有错误符号的标签作为真实标签在损失函数中使用会导致优化性不佳。先前的工作已经尝试过过滤 pseudo-标签中最含错误的符号或者提高 pseudo-标签的质量。虽然这些方法有一定的效果，但是完全消除 incorrect 符号是不现实的。在这种情况下，我们提出了一种新的框架，即 alternative pseudo-labeling，以解决 pseudo-标签中含有错误符号的问题。该框架包括以下几个组成部分：1. 一种通用的 CTC 损失函数，可以处理含有错误符号的 pseudo-标签。这种损失函数接受在 incorrect 符号的位置上的多个选项，以便在损失函数中处理多个可能的符号。2. 一种 confidence-based 错误检测方法，可以在预测 pseudo-标签时检测 incorrect 符号。这种方法比较预测 pseudo-标签中的符号 confidence 分数与一定的阈值，并将不符合阈值的符号标记为错误符号。3. 一种自动调整 threshold 的方法，可以使用标注数据作为代理，以便在不需要手动调整的情况下，自动地调整 threshold。通过这些技术，我们可以提高半超vised 学习中 pseudo-标签中含有错误符号的性能。
</details></li>
</ul>
<hr>
<h2 id="BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution"><a href="#BigWavGAN-A-Wave-To-Wave-Generative-Adversarial-Network-for-Music-Super-Resolution" class="headerlink" title="BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution"></a>BigWavGAN: A Wave-To-Wave Generative Adversarial Network for Music Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06483">http://arxiv.org/abs/2308.06483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yenan Zhang, Hiroshi Watanabe</li>
<li>for: 本研究旨在解决深度神经网络（DNNs）在音频超解像（SR）中的高性能问题，即大型DNN模型在SR中无法 producen高质量结果。</li>
<li>methods: 本研究提出了BigWavGAN模型，它通过结合大规模波形模型Demucs，SOTA的检测器和对抗训练策略来解锁大型DNN模型在SR中的潜力。我们的检测器包括多尺度检测器（MSD）和多分辨率检测器（MRD）。</li>
<li>results: 对jective和对jective评价表明，BigWavGAN在音频SR中具有显著高的感知质量，超过了基eline模型。此外，BigWavGAN在 simulated和实际场景中也超过了SOTA音频SR模型。同时，BigWavGAN还能够Address out-of-distribution数据的泛化问题。<details>
<summary>Abstract</summary>
Generally, Deep Neural Networks (DNNs) are expected to have high performance when their model size is large. However, large models failed to produce high-quality results commensurate with their scale in music Super-Resolution (SR). We attribute this to that DNNs cannot learn information commensurate with their size from standard mean square error losses. To unleash the potential of large DNN models in music SR, we propose BigWavGAN, which incorporates Demucs, a large-scale wave-to-wave model, with State-Of-The-Art (SOTA) discriminators and adversarial training strategies. Our discriminator consists of Multi-Scale Discriminator (MSD) and Multi-Resolution Discriminator (MRD). During inference, since only the generator is utilized, there are no additional parameters or computational resources required compared to the baseline model Demucs. Objective evaluation affirms the effectiveness of BigWavGAN in music SR. Subjective evaluations indicate that BigWavGAN can generate music with significantly high perceptual quality over the baseline model. Notably, BigWavGAN surpasses the SOTA music SR model in both simulated and real-world scenarios. Moreover, BigWavGAN represents its superior generalization ability to address out-of-distribution data. The conducted ablation study reveals the importance of our discriminators and training strategies. Samples are available on the demo page.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)通常情况下，深度神经网络（DNNs）预期在大型模型下表现出色。然而，大型模型在音乐超分辨率（SR）领域并未能够生成高质量结果，我们认为这是因为DNNs无法从标准的平方差损失中学习足够的信息。为了解 liberate大型DNN模型在音乐SR中的潜力，我们提出了BigWavGAN，它包括了大规模的波形模型Demucs，以及当前最佳的检测器和对抗训练策略。我们的检测器包括多尺度检测器（MSD）和多分辨率检测器（MRD）。在推理过程中，只有生成器被使用，因此无需额外的参数或计算资源，与Demucs相比。对象评估表明BigWavGAN在音乐SR中的效果极佳。主观评估表明BigWavGAN可以生成高度感知质量的音乐，超过基准模型。此外，BigWavGAN表现出了更好的总体化能力，可以 Addressing out-of-distribution data。我们进行了一项ablation Study，表明我们的检测器和训练策略的重要性。样本可以在 demo 页面上找到。
</details></li>
</ul>
<hr>
<h2 id="Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss"><a href="#Bilingual-Streaming-ASR-with-Grapheme-units-and-Auxiliary-Monolingual-Loss" class="headerlink" title="Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss"></a>Bilingual Streaming ASR with Grapheme units and Auxiliary Monolingual Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06327">http://arxiv.org/abs/2308.06327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Soleymanpour, Mahmoud Al Ismail, Fahimeh Bahmaninezhad, Kshitiz Kumar, Jian Wu</li>
<li>for: 这个论文是为了提供一种混合自动声音识别（ASR）设置下的英语作为次要地区的双语解决方案。</li>
<li>methods: 作者们提出了以下四个关键发展：（a）用图形单元代替语音单元的发音词典，（b）完全双语对应模型和随后的双语流Transformer模型，（c）并行编码结构和语言标识（LID）损失，（d）并行编码器的辅助损失 для单语jective。</li>
<li>results: 作者们通过大规模训练和测试任务来评估他们的工作，并发现他们的提出的辅助损失可以更好地特化并行编码器到各自的单语本地，从而使得双语学习更加强。特别是，双语IT模型在一个code-mix IT任务中提高了单词错误率（WER）从46.5%降低到13.8%，同时也与单语IT模型（9.5%）在IT测试任务上几乎占据了同等水平（9.6%）。<details>
<summary>Abstract</summary>
We introduce a bilingual solution to support English as secondary locale for most primary locales in hybrid automatic speech recognition (ASR) settings. Our key developments constitute: (a) pronunciation lexicon with grapheme units instead of phone units, (b) a fully bilingual alignment model and subsequently bilingual streaming transformer model, (c) a parallel encoder structure with language identification (LID) loss, (d) parallel encoder with an auxiliary loss for monolingual projections. We conclude that in comparison to LID loss, our proposed auxiliary loss is superior in specializing the parallel encoders to respective monolingual locales, and that contributes to stronger bilingual learning. We evaluate our work on large-scale training and test tasks for bilingual Spanish (ES) and bilingual Italian (IT) applications. Our bilingual models demonstrate strong English code-mixing capability. In particular, the bilingual IT model improves the word error rate (WER) for a code-mix IT task from 46.5% to 13.8%, while also achieving a close parity (9.6%) with the monolingual IT model (9.5%) over IT tests.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种双语解决方案，以支持英语为次要本地语言在混合自动语音识别（ASR）设置中。我们的关键发展包括：（a）使用字符单位 instead of 语音单位的发音词典，（b）完全双语对应模型和随后的双语流Transformer模型，（c）并行编码结构与语言标识（LID）损失，（d）并行编码器和辅助损失 для单语Project。我们 conclude that在比较LID损失的情况下，我们提posed的辅助损失能够特化并行编码器到各自的单语本地语言，从而为双语学习提供更强的支持。我们在大规模训练和测试任务上评估了我们的工作，并在双语西班牙（ES）和双语意大利（IT）应用中显示出了英语代码混合能力。特别是，双语IT模型在一个代码混合IT任务上从46.5%降至13.8%，并同时与单语IT模型（9.5%）在IT测试上达到了9.6%的相似性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.SD_2023_08_12/" data-id="clly3dw0x009j0988cq81dpj9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/eess.IV_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/eess.IV_2023_08_12/">eess.IV - 2023-08-12 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission"><a href="#Semantic-Communications-with-Explicit-Semantic-Base-for-Image-Transmission" class="headerlink" title="Semantic Communications with Explicit Semantic Base for Image Transmission"></a>Semantic Communications with Explicit Semantic Base for Image Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06599">http://arxiv.org/abs/2308.06599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zheng, Fengyu Wang, Wenjun Xu, Miao Pan, Ping Zhang</li>
<li>for: 该 paper 的目的是提出一种基于协同知识 (Seb) 的 semantic image transmission 框架，以确保信息的含义在传输过程中得到正确的理解和传输。</li>
<li>methods: 该 paper 使用了 Seb 生成器和 Seb 基于图像编码&#x2F;解码器来表示图像，并使用 E2E 训练来优化核心组件。</li>
<li>results: 对比 state-of-art 方法，该 paper 在不同 SNR 下达到了 0.5-1.5 dB 的 PSNR 提升。<details>
<summary>Abstract</summary>
Semantic communications, aiming at ensuring the successful delivery of the meaning of information, are expected to be one of the potential techniques for the next generation communications. However, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to the communication intents is still immature. In this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. To represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. To further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. The key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. Extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details>
<details>
<summary>摘要</summary>
semantic communication, aiming at ensuring the successful delivery of information meaning, is expected to be one of the potential techniques for next-generation communications. however, the knowledge forming and synchronizing mechanism that enables semantic communication systems to extract and interpret the semantics of information according to communication intents is still immature. in this paper, we propose a semantic image transmission framework with explicit semantic base (Seb), where Sebs are generated and employed as the knowledge shared between the transmitter and the receiver with flexible granularity. to represent images with Sebs, a novel Seb-based reference image generator is proposed to generate Sebs and then decompose the transmitted images. to further encode/decode the residual information for precise image reconstruction, a Seb-based image encoder/decoder is proposed. the key components of the proposed framework are optimized jointly by end-to-end (E2E) training, where the loss function is dedicated designed to tackle the problem of nondifferentiable operation in Seb-based reference image generator by introducing a gradient approximation mechanism. extensive experiments show that the proposed framework outperforms state-of-art works by 0.5 - 1.5 dB in peak signal-to-noise ratio (PSNR) w.r.t. different signal-to-noise ratio (SNR).
</details></li>
</ul>
<hr>
<h2 id="On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution"><a href="#On-Versatile-Video-Coding-at-UHD-with-Machine-Learning-Based-Super-Resolution" class="headerlink" title="On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution"></a>On Versatile Video Coding at UHD with Machine-Learning-Based Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06570">http://arxiv.org/abs/2308.06570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristian Fischer, Christian Herglotz, André Kaup</li>
<li>for: 提高4K数据的编码质量</li>
<li>methods: 使用Machine Learning基于单张超解析算法和下一代VVC编码器</li>
<li>results: 可以在低比特率场景下获得12%-18%的Bjontegaard delta rate提升，并且减少了压缩残留和扩散 artifacts。<details>
<summary>Abstract</summary>
Coding 4K data has become of vital interest in recent years, since the amount of 4K data is significantly increasing. We propose a coding chain with spatial down- and upscaling that combines the next-generation VVC codec with machine learning based single image super-resolution algorithms for 4K. The investigated coding chain, which spatially downscales the 4K data before coding, shows superior quality than the conventional VVC reference software for low bitrate scenarios. Throughout several tests, we find that up to 12 % and 18 % Bjontegaard delta rate gains can be achieved on average when coding 4K sequences with VVC and QP values above 34 and 42, respectively. Additionally, the investigated scenario with up- and downscaling helps to reduce the loss of details and compression artifacts, as it is shown in a visual example.
</details>
<details>
<summary>摘要</summary>
“ coding 4K 数据在近年变得非常重要，因为4K 数据量在增长。我们提出了一个 coding chain，其 combining 下一代 VVC 编码器和基于机器学习的单张图像超分辨算法，用于4K。我们调查的 coding chain，先将4K 数据进行空间下降scaling，然后编码，在低比特率场景下显示出超越传统 VVC 参考软件的质量。在多个测试中，我们发现，在 VVC 和 QP 值高于 34 和 42 时，可以获得12% 到 18% Bjontegaard delta Rate 增强。此外，我们发现，这种升降scaling 场景可以避免失 Details 和压缩残差的损失，如图例所示。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion"><a href="#Three-dimensional-echo-shifted-EPI-with-simultaneous-blip-up-and-blip-down-acquisitions-for-correcting-geometric-distortion" class="headerlink" title="Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion"></a>Three-dimensional echo-shifted EPI with simultaneous blip-up and blip-down acquisitions for correcting geometric distortion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06557">http://arxiv.org/abs/2308.06557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaibao Sun, Zhifeng Chen, Guangyu Dan, Qingfei Luo, Lirong Yan, Feng Liu, Xiaohong Joe Zhou</li>
<li>for: 提高echo-planar imaging（EPI）的图像质量和动态误差 corrections，使其适用于更多应用。</li>
<li>methods: 使用三维（3D）echo-shifted EPI BUDA（esEPI-BUDA）技术，通过生成两个EPI读取轨迹，以实现单shot acquiring两个blip-up和blip-down数据集。</li>
<li>results: 在phantom和人类大脑图像中， geometric distortions 得到了有效地 correction，并在人类大脑中测试了视觉活化Volume和其BOLD响应，与普通3D echo-planar图像相当。<details>
<summary>Abstract</summary>
Purpose: Echo-planar imaging (EPI) with blip-up/down acquisition (BUDA) can provide high-quality images with minimal distortions by using two readout trains with opposing phase-encoding gradients. Because of the need for two separate acquisitions, BUDA doubles the scan time and degrades the temporal resolution when compared to single-shot EPI, presenting a major challenge for many applications, particularly functional MRI (fMRI). This study aims at overcoming this challenge by developing an echo-shifted EPI BUDA (esEPI-BUDA) technique to acquire both blip-up and blip-down datasets in a single shot. Methods: A three-dimensional (3D) esEPI-BUDA pulse sequence was designed by using an echo-shifting strategy to produce two EPI readout trains. These readout trains produced a pair of k-space datasets whose k-space trajectories were interleaved with opposite phase-encoding gradient directions. The two k-space datasets were separately reconstructed using a 3D SENSE algorithm, from which time-resolved B0-field maps were derived using TOPUP in FSL and then input into a forward model of joint parallel imaging reconstruction to correct for geometric distortion. In addition, Hankel structured low-rank constraint was incorporated into the reconstruction framework to improve image quality by mitigating the phase errors between the two interleaved k-space datasets. Results: The 3D esEPI-BUDA technique was demonstrated in a phantom and an fMRI study on healthy human subjects. Geometric distortions were effectively corrected in both phantom and human brain images. In the fMRI study, the visual activation volumes and their BOLD responses were comparable to those from conventional 3D echo-planar images. Conclusion: The improved imaging efficiency and dynamic distortion correction capability afforded by 3D esEPI-BUDA are expected to benefit many EPI applications.
</details>
<details>
<summary>摘要</summary>
目的：使用双向磁场增强/减强获取（BUDA）技术可以提供高质量图像，但因为需要两次获取，BUDA将扫描时间双倍，降低时间分辨率，对许多应用程序（特别是功能磁共振成像（fMRI））提出了主要挑战。这项研究的目的是解决这一挑战，通过开发一种三维echo-shifted EPI BUDA（esEPI-BUDA）技术，在单击中获取两个磁场增强/减强数据集。方法：设计了一种三维 esEPI-BUDA脉冲序列，使用抽象阶段生成两个 EPI 读取轨迹。这两个读取轨迹生成了具有相反方向磁场增强/减强方向的两个 k-空间数据集。这两个 k-空间数据集分别使用三维 SENSE 算法重构，并从而生成了时间解析B0场图像。在FSL中使用 TOPUP 算法，将这些时间解析B0场图像输入到了一种 JOINT 平行成像重建模型中，以 corrected  geometric distortion。此外，还在重建框架中添加了具有束缚低维度的 Hankel 结构低级数据约束，以提高图像质量，减少了两个交错 k-空间数据集之间的频率错误。结果：在一个模拟器和一个人类大脑功能磁共振成像研究中，三维 esEPI-BUDA 技术得到了证明。在这些研究中，人类大脑功能磁共振图像中的形态扭曲都得到了有效地 corrections。在人类大脑功能磁共振图像中，可见功能区域的激发量和其 BOLD 响应与普通三维 EPI 图像具有相同的水平。结论：三维 esEPI-BUDA 技术的改进的扫描效率和动态扭曲纠正能力，预期将对许多 EPI 应用程序产生积极的影响。
</details></li>
</ul>
<hr>
<h2 id="The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow"><a href="#The-Color-Clifford-Hardy-Signal-Application-to-Color-Edge-Detection-and-Optical-Flow" class="headerlink" title="The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow"></a>The Color Clifford Hardy Signal: Application to Color Edge Detection and Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06485">http://arxiv.org/abs/2308.06485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Hu, Kit Ian Kou, Cuiming Zou, Dong Cheng</li>
<li>for: 该论文提出了色彩Clifford Hardy信号的想法，用于处理色彩图像。</li>
<li>methods: 该论文提出了五种方法来识别色彩图像的边缘，基于色彩Clifford Hardy信号的本地特征表示。</li>
<li>results: 该论文通过多种比较研究表明，提出的方法具有高效性和抗噪能力。例如，通过多尺度结构的色彩Clifford Hardy信号，方法可以抗衰减噪。此外，论文还提供了一种颜色动向检测方法，用于应用示例。<details>
<summary>Abstract</summary>
This paper introduces the idea of the color Clifford Hardy signal, which can be used to process color images. As a complex analytic function's high-dimensional analogue, the color Clifford Hardy signal inherits many desirable qualities of analyticity. A crucial tool for getting the color and structural data is the local feature representation of a color image in the color Clifford Hardy signal. By looking at the extended Cauchy-Riemann equations in the high-dimensional space, it is possible to see the connection between the different parts of the color Clifford Hardy signal. Based on the distinctive and important local amplitude and local phase generated by the color Clifford Hardy signal, we propose five methods to identify the edges of color images with relation to a certain color. To prove the superiority of the offered methodologies, numerous comparative studies employing image quality assessment criteria are used. Specifically by using the multi-scale structure of the color Clifford Hardy signal, the proposed approaches are resistant to a variety of noises. In addition, a color optical flow detection method with anti-noise ability is provided as an example of application.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection"><a href="#Out-of-distribution-multi-view-auto-encoders-for-prostate-cancer-lesion-detection" class="headerlink" title="Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection"></a>Out-of-distribution multi-view auto-encoders for prostate cancer lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06481">http://arxiv.org/abs/2308.06481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alvaro Fernandez-Quilez, Linas Vidziunas, Ørjan Kløvfjell Thoresen, Ketil Oppedal, Svein Reidar Kjosavik, Trygve Eftestøl</li>
<li>for: 这篇论文是为了检测抑制癌病变的方法。</li>
<li>methods: 这篇论文使用了多流程方法，以便利用不同的T2w方向来提高肿瘤检测的性能。</li>
<li>results: 这篇论文的结果显示，使用多流程方法可以提高肿瘤检测的精度，比单向方法更高（AUC&#x3D;73.1 vs 82.3）。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) approaches based on supervised learning paradigms require large amounts of annotated data that are rarely available in the medical domain. Unsupervised Out-of-distribution (OOD) detection is an alternative that requires less annotated data. Further, OOD applications exploit the class skewness commonly present in medical data. Magnetic resonance imaging (MRI) has proven to be useful for prostate cancer (PCa) diagnosis and management, but current DL approaches rely on T2w axial MRI, which suffers from low out-of-plane resolution. We propose a multi-stream approach to accommodate different T2w directions to improve the performance of PCa lesion detection in an OOD approach. We evaluate our approach on a publicly available data-set, obtaining better detection results in terms of AUC when compared to a single direction approach (73.1 vs 82.3). Our results show the potential of OOD approaches for PCa lesion detection based on MRI.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）方法基于supervised learning paradigms需要大量的标注数据，而医疗领域中这些数据很少。不需要标注数据的Out-of-distribution（OOD）检测是一个 alternativa，并且可以利用医疗数据的类偏好。核磁共振成像（MRI）已经被证明是肠癌（PCa）诊断和管理的有用工具，但现有的DL方法仅仅使用T2w极向MRI，这种MRI受到外向分辨率的限制。我们提议一种多流处理方法，以便处理不同的T2w方向，以提高PCa患部检测的性能。我们对公共可用数据集进行了评估，并 obtient了与单向方法相比的更好的检测结果（AUC=73.1 vs AUC=82.3）。我们的结果表明，基于MRI的PCa患部检测可以通过OOD方法实现更高的检测精度。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach"><a href="#Leveraging-multi-view-data-without-annotations-for-prostate-MRI-segmentation-A-contrastive-approach" class="headerlink" title="Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach"></a>Leveraging multi-view data without annotations for prostate MRI segmentation: A contrastive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06477">http://arxiv.org/abs/2308.06477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Nikolass Lindeijer, Tord Martin Ytredal, Trygve Eftestøl, Tobias Nordström, Fredrik Jäderling, Martin Eklund, Alvaro Fernandez-Quilez</li>
<li>for: 提高肾脏癌诊断的支持</li>
<li>methods: 使用对比学习方法，不需要manual注释，可以在部署时 missing views</li>
<li>results: 提高了自适应volumetric分割的精度，并且在多视图数据上实现了good external volumetric generalization<details>
<summary>Abstract</summary>
An accurate prostate delineation and volume characterization can support the clinical assessment of prostate cancer. A large amount of automatic prostate segmentation tools consider exclusively the axial MRI direction in spite of the availability as per acquisition protocols of multi-view data. Further, when multi-view data is exploited, manual annotations and availability at test time for all the views is commonly assumed. In this work, we explore a contrastive approach at training time to leverage multi-view data without annotations and provide flexibility at deployment time in the event of missing views. We propose a triplet encoder and single decoder network based on U-Net, tU-Net (triplet U-Net). Our proposed architecture is able to exploit non-annotated sagittal and coronal views via contrastive learning to improve the segmentation from a volumetric perspective. For that purpose, we introduce the concept of inter-view similarity in the latent space. To guide the training, we combine a dice score loss calculated with respect to the axial view and its manual annotations together with a multi-view contrastive loss. tU-Net shows statistical improvement in dice score coefficient (DSC) with respect to only axial view (91.25+-0.52% compared to 86.40+-1.50%,P<.001). Sensitivity analysis reveals the volumetric positive impact of the contrastive loss when paired with tU-Net (2.85+-1.34% compared to 3.81+-1.88%,P<.001). Further, our approach shows good external volumetric generalization in an in-house dataset when tested with multi-view data (2.76+-1.89% compared to 3.92+-3.31%,P=.002), showing the feasibility of exploiting non-annotated multi-view data through contrastive learning whilst providing flexibility at deployment in the event of missing views.
</details>
<details>
<summary>摘要</summary>
可以准确地定义和量化 prostata 的部分，可以支持肾癌的临床评估。许多自动 prostate 分割工具都忽略了多视图数据的可用性，即使据获取协议中有多视图数据可用。此外，当使用多视图数据时，通常需要手动标注和测试时 disponibility 的所有视图。在这种情况下，我们提出了一种对比方法，可以在训练时使用多视图数据而不需要手动标注，并在部署时提供可选的视图。我们提出了一种基于 U-Net 的 triplet 编码器和单个解码器网络，可以通过对比学习利用非标注的 sagittal 和极轴视图来提高分割。为此，我们引入了视图间的相似性在幂空间的概念。为了导航训练，我们将 dice 分数损失与 respect 到 axial 视图和其手动标注相加，并与多视图对比损失相结合。tU-Net 表示与只有 axial 视图相比（91.25+-0.52% 与 86.40+-1.50%，P<.001）显示了统计学上的改进。敏感分析表明，对于 tU-Net 来说，对比损失的volumetric 正面影响（2.85+-1.34% 与 3.81+-1.88%,P<.001）。此外，我们的方法在我们的内部数据集中进行了多视图数据的外部准确性测试（2.76+-1.89% 与 3.92+-3.31%,P=.002），表明可以通过对比学习在不具有标注的多视图数据上Exploiting 而提供可选的视图。
</details></li>
</ul>
<hr>
<h2 id="CATS-v2-Hybrid-encoders-for-robust-medical-segmentation"><a href="#CATS-v2-Hybrid-encoders-for-robust-medical-segmentation" class="headerlink" title="CATS v2: Hybrid encoders for robust medical segmentation"></a>CATS v2: Hybrid encoders for robust medical segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06377">http://arxiv.org/abs/2308.06377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoli12345/cats">https://github.com/haoli12345/cats</a></li>
<li>paper_authors: Hao Li, Han Liu, Dewei Hu, Xing Yao, Jiacheng Wang, Ipek Oguz<br>for:* 这个研究是为了提高医疗影像分类 задачі的性能，特别是在 capture 高级（本地）信息和全球信息之间的平衡。methods:* 使用 hybrid encoders，包括一个 CNN-based Encoder 路径和一个 transformer 路径，以更好地利用本地和全球信息。* 使用 skip connections 将 convolutional encoder 和 transformer 融合为最终的分类结果。results:* 在 Cross-Modality Domain Adaptation (CrossMoDA) 和 Medical Segmentation Decathlon (MSD-5) 项目中，该方法与州际先进方法相比， exhibit 高的 Dice 分数。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have exhibited strong performance in medical image segmentation tasks by capturing high-level (local) information, such as edges and textures. However, due to the limited field of view of convolution kernel, it is hard for CNNs to fully represent global information. Recently, transformers have shown good performance for medical image segmentation due to their ability to better model long-range dependencies. Nevertheless, transformers struggle to capture high-level spatial features as effectively as CNNs. A good segmentation model should learn a better representation from local and global features to be both precise and semantically accurate. In our previous work, we proposed CATS, which is a U-shaped segmentation network augmented with transformer encoder. In this work, we further extend this model and propose CATS v2 with hybrid encoders. Specifically, hybrid encoders consist of a CNN-based encoder path paralleled to a transformer path with a shifted window, which better leverage both local and global information to produce robust 3D medical image segmentation. We fuse the information from the convolutional encoder and the transformer at the skip connections of different resolutions to form the final segmentation. The proposed method is evaluated on two public challenge datasets: Cross-Modality Domain Adaptation (CrossMoDA) and task 5 of Medical Segmentation Decathlon (MSD-5), to segment vestibular schwannoma (VS) and prostate, respectively. Compared with the state-of-the-art methods, our approach demonstrates superior performance in terms of higher Dice scores.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗图像分割任务中表现出色，通过捕捉高级（本地）信息，如边缘和xture，来捕捉高级信息。然而，由于卷积核心的局部视场限制，使得CNN很难完全表示全局信息。最近，transformer在医疗图像分割中表现良好，这是因为它们可以更好地模型距离的长距离依赖关系。然而，transformer尚未能如同CNN那样有效地捕捉高级空间特征。为了实现更好的分割模型，我们需要学习更好地捕捉本地和全局特征，以确保准确和semantic地正确。在我们之前的工作中，我们提出了CATS模型，这是一种U型卷积分割网络，其中包括transformer编码器。在这个工作中，我们进一步扩展了这个模型，并提出了CATS v2模型，它包括hybrid编码器。specifically，hybrid编码器包括一个CNN基于编码器路径，并与一个偏移窗口的transformer路径并行，这样更好地利用本地和全局信息来生成robust的3D医疗图像分割。我们在不同分辨率的 skip connections中 fusions the information from the convolutional encoder and the transformer，以生成最终的分割结果。我们的方法在两个公共挑战数据集上进行评估： Cross-Modality Domain Adaptation（CrossMoDA）和Medical Segmentation Decathlon（MSD-5），用于分割vestibular schwannoma（VS）和prostate，分别。与当前状态的方法相比，我们的方法在 terms of higher Dice scores表现出色。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis"><a href="#Deep-Learning-Based-Open-Source-Toolkit-for-Eosinophil-Detection-in-Pediatric-Eosinophilic-Esophagitis" class="headerlink" title="Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis"></a>Deep Learning-Based Open Source Toolkit for Eosinophil Detection in Pediatric Eosinophilic Esophagitis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06333">http://arxiv.org/abs/2308.06333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrlblab/open-eoe">https://github.com/hrlblab/open-eoe</a></li>
<li>paper_authors: Juming Xiong, Yilin Liu, Ruining Deng, Regina N Tyree, Hernan Correa, Girish Hiremath, Yaohong Wang, Yuankai Huo<br>for: 这个研究旨在开发一个开源的工具集，用于检测食道病变中的嗜中性粒细胞（Eos）。methods: 该工具集使用三种现状最佳的深度学习基于对象检测模型，并实现了一种ensemble学习策略以提高结果的精度和可靠性。results: 实验结果表明，Open-EoE工具集可以效果地检测食道病变中的Eos，并达到了91%的准确率，与病理学家评估相符。<details>
<summary>Abstract</summary>
Eosinophilic Esophagitis (EoE) is a chronic, immune/antigen-mediated esophageal disease, characterized by symptoms related to esophageal dysfunction and histological evidence of eosinophil-dominant inflammation. Owing to the intricate microscopic representation of EoE in imaging, current methodologies which depend on manual identification are not only labor-intensive but also prone to inaccuracies. In this study, we develop an open-source toolkit, named Open-EoE, to perform end-to-end whole slide image (WSI) level eosinophil (Eos) detection using one line of command via Docker. Specifically, the toolkit supports three state-of-the-art deep learning-based object detection models. Furthermore, Open-EoE further optimizes the performance by implementing an ensemble learning strategy, and enhancing the precision and reliability of our results. The experimental results demonstrated that the Open-EoE toolkit can efficiently detect Eos on a testing set with 289 WSIs. At the widely accepted threshold of >= 15 Eos per high power field (HPF) for diagnosing EoE, the Open-EoE achieved an accuracy of 91%, showing decent consistency with pathologist evaluations. This suggests a promising avenue for integrating machine learning methodologies into the diagnostic process for EoE. The docker and source code has been made publicly available at https://github.com/hrlblab/Open-EoE.
</details>
<details>
<summary>摘要</summary>
《细胞滤镜检测诊断工具（Open-EoE）》是一个开源的检测工具，用于检测食管细胞滤镜病（EoE）的患者。该工具使用深度学习技术，可以通过一条命令在Docker环境中完成整个检测过程。工具支持三种当前顶峰的深度学习模型，并且通过 ensemble learning 策略来提高检测精度和可靠性。实验结果表明，Open-EoE 工具可以高效地检测食管细胞滤镜，在测试集上达到了91%的准确率。这表明，机器学习技术可以成功地应用于EoE 诊断过程中。工具和源代码已经在 GitHub 上公开，可以免费下载和使用。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology"><a href="#Revolutionizing-Space-Health-Swin-FSR-Advancing-Super-Resolution-of-Fundus-Images-for-SANS-Visual-Assessment-Technology" class="headerlink" title="Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology"></a>Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06332">http://arxiv.org/abs/2308.06332</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FarihaHossain/SwinFSR">https://github.com/FarihaHossain/SwinFSR</a></li>
<li>paper_authors: Khondker Fariha Hossain, Sharif Amit Kamran, Joshua Ong, Andrew G. Lee, Alireza Tavakkoli</li>
<li>for: 这 paper 是为了提高肉眼图像的分辨率，以便在不同的地方进行早期差分诊断。</li>
<li>methods: 这 paper 使用了 Swin Transformer 与空间和深度精度注意力，实现了基于肉眼图像的超分辨率。</li>
<li>results: 这 paper 在三个公共数据集上 achieved PSNR 值为 47.89、49.00 和 45.32，并在 NASA 提供的私人数据集上达到了相当的结果。<details>
<summary>Abstract</summary>
The rapid accessibility of portable and affordable retinal imaging devices has made early differential diagnosis easier. For example, color funduscopy imaging is readily available in remote villages, which can help to identify diseases like age-related macular degeneration (AMD), glaucoma, or pathological myopia (PM). On the other hand, astronauts at the International Space Station utilize this camera for identifying spaceflight-associated neuro-ocular syndrome (SANS). However, due to the unavailability of experts in these locations, the data has to be transferred to an urban healthcare facility (AMD and glaucoma) or a terrestrial station (e.g, SANS) for more precise disease identification. Moreover, due to low bandwidth limits, the imaging data has to be compressed for transfer between these two places. Different super-resolution algorithms have been proposed throughout the years to address this. Furthermore, with the advent of deep learning, the field has advanced so much that x2 and x4 compressed images can be decompressed to their original form without losing spatial information. In this paper, we introduce a novel model called Swin-FSR that utilizes Swin Transformer with spatial and depth-wise attention for fundus image super-resolution. Our architecture achieves Peak signal-to-noise-ratio (PSNR) of 47.89, 49.00 and 45.32 on three public datasets, namely iChallenge-AMD, iChallenge-PM, and G1020. Additionally, we tested the model's effectiveness on a privately held dataset for SANS provided by NASA and achieved comparable results against previous architectures.
</details>
<details>
<summary>摘要</summary>
“现代化的眼科医学技术已经使得早期的医学诊断变得更加容易。例如，彩色基准摄影是在偏远村庄中可以提供的，可以帮助诊断年龄相关 macular degeneration（AMD）、 glaucoma 或 PATHOLOGICAL MYOPIA（PM）等疾病。然而，由于这些地点缺乏专家，因此需要将数据传输到城市医疗机构（AMD和 glaucoma）或地面站（例如，SANS）进行更加准确的疾病诊断。此外，由于带宽限制，摄影数据需要压缩传输。过去数年，有许多超解像算法的提案，以解决这个问题。另外，随着深度学习的发展，场景有了很大的进步，可以使用 x2 和 x4 压缩图像重新恢复到原始形态，无需失去空间信息。在本文中，我们介绍了一种新的模型called Swin-FSR，该模型利用SwinTransformer的空间和深度宽分注意力来进行基准图像超解像。我们的架构实现了 PSNR 的值为 47.89、49.00 和 45.32 在三个公共数据集上，namely iChallenge-AMD、iChallenge-PM 和 G1020。此外，我们对 NASA 提供的一个私有数据集进行了测试，并与之前的建筑物实现了相似的结果。”
</details></li>
</ul>
<hr>
<h2 id="A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies"><a href="#A-Hierarchical-Descriptor-Framework-for-On-the-Fly-Anatomical-Location-Matching-between-Longitudinal-Studies" class="headerlink" title="A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies"></a>A Hierarchical Descriptor Framework for On-the-Fly Anatomical Location Matching between Longitudinal Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07337">http://arxiv.org/abs/2308.07337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Halid Ziya Yerebakan, Yoshihisa Shinagawa, Mahesh Ranganath, Simon Allen-Raffl, Gerardo Hermosillo Valadez</li>
<li>for: 医疗影像比较 longitudinal 比较中医学影像之间的 анатомиче位置匹配</li>
<li>methods: 使用 hierarchical sparse sampling of image intensities 计算查询点在源图像中的描述子，然后使用 hierarchical search 操作找到目标图像中最相似的描述子。</li>
<li>results: 实现了减少计算时间到毫秒级别的易行医学影像比较，无需额外建筑、存储或训练步骤。在 Deep Lesion Tracking 数据集注释中观察到更高准确的匹配结果，并且比最精确的报告algorithm 24 倍 faster。<details>
<summary>Abstract</summary>
We propose a method to match anatomical locations between pairs of medical images in longitudinal comparisons. The matching is made possible by computing a descriptor of the query point in a source image based on a hierarchical sparse sampling of image intensities that encode the location information. Then, a hierarchical search operation finds the corresponding point with the most similar descriptor in the target image. This simple yet powerful strategy reduces the computational time of mapping points to a millisecond scale on a single CPU. Thus, radiologists can compare similar anatomical locations in near real-time without requiring extra architectural costs for precomputing or storing deformation fields from registrations. Our algorithm does not require prior training, resampling, segmentation, or affine transformation steps. We have tested our algorithm on the recently published Deep Lesion Tracking dataset annotations. We observed more accurate matching compared to Deep Lesion Tracker while being 24 times faster than the most precise algorithm reported therein. We also investigated the matching accuracy on CT and MR modalities and compared the proposed algorithm's accuracy against ground truth consolidated from multiple radiologists.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于在医疗图像对比中匹配生物学位置。该方法基于源图像中点Query的层次稀疏抽象来计算描述符，以便在目标图像中找到最相似的点。这种简单 yet powerful的策略可以在单个CPU上减少计算时间至毫秒级，因此，辐射学家可以在实时比较相似的生物学位置，无需额外的建筑成本或存储扭转场景的预计算或存储。我们的算法不需要先期训练、重新采样、分割或 afine 变换步骤。我们在最近发布的 Deep Lesion Tracking 数据集注释中进行了测试，并观察到比 Deep Lesion Tracker 更准确的匹配，而且比最精确的报告算法更快24倍。我们还研究了该方法在 CT 和 MR 模式下的匹配精度，并与多个 radiologists 共同组织的ground truth进行了比较。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/eess.IV_2023_08_12/" data-id="clly3dw2h00eg0988fp4u4rze" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.LG_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.LG_2023_08_11/">cs.LG - 2023-08-11 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks"><a href="#Towards-a-Causal-Probabilistic-Framework-for-Prediction-Action-Selection-Explanations-for-Robot-Block-Stacking-Tasks" class="headerlink" title="Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks"></a>Towards a Causal Probabilistic Framework for Prediction, Action-Selection &amp; Explanations for Robot Block-Stacking Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06203">http://arxiv.org/abs/2308.06203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Cannizzaro, Jonathan Routley, Lars Kunze</li>
<li>for: 本研究旨在提供一种基于 causal probabilistic 框架的自主 робоット，以便让 robot 能够更好地理解和描述它所处环境。</li>
<li>methods: 本研究使用 causal inference 和 physics simulation 技术，将 causal models 与 probabilistic representations 结合起来，以便 robot 能够更好地理解和描述它所处环境。</li>
<li>results: 研究提出了一种新的 causal probabilistic 框架，可以帮助 robot 更好地完成块排序任务，并提供了一些 exemplar 的下一步行动选择结果。 I hope this helps! Let me know if you have any further questions or if there’s anything else I can help with.<details>
<summary>Abstract</summary>
Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability into a structural causal model to permit robots to perceive and assess the current state of a block-stacking task, reason about the next-best action from placement candidates, and generate post-hoc counterfactual explanations. We provide exemplar next-best action selection results and outline planned experimentation in simulated and real-world robot block-stacking tasks.
</details>
<details>
<summary>摘要</summary>
世界中的不确定性使得系统设计者无法预期和明确地设计机器人可能遇到的所有场景。因此，基于这种设计方式的机器人在控制环境外会失败。 causal模型提供了一个原则性的框架，用于编码机器人与环境之间的 causal 关系，以及通常遇到的实际世界机器人中的抽象概率和不确定性。这些模型与 causal 推理相结合，允许自主机器人理解、推理和解释其环境。在这项工作中，我们关注了机器人块堆垫任务，因为它涉及到机器人的基本感知和操作能力，这些能力是许多应用程序，如仓库自动化和家庭服务机器人所必需的。我们提出了一种新的 causal 概率 frameworks，用于在机器人块堆垫任务中嵌入物理模拟能力，让机器人可以识别和评估当前块堆垫任务的状态，从选择候选位置中选择下一步行动，并生成后续 counterfactual 解释。我们提供了示例下一步行动选择结果，并详细描述计划在模拟和实际机器人块堆垫任务中进行实验。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions"><a href="#Exploring-Predicate-Visual-Context-in-Detecting-of-Human-Object-Interactions" class="headerlink" title="Exploring Predicate Visual Context in Detecting of Human-Object Interactions"></a>Exploring Predicate Visual Context in Detecting of Human-Object Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06202">http://arxiv.org/abs/2308.06202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredzzhang/pvic">https://github.com/fredzzhang/pvic</a></li>
<li>paper_authors: Frederic Z. Zhang, Yuhui Yuan, Dylan Campbell, Zhuoyao Zhong, Stephen Gould</li>
<li>for: 本研究旨在解决人–物交互（HOI）领域中现有的问题，即使用两stage transformer-based HOI检测器，但这些检测器常常基于物体特征，而忽略了姿态和方向信息，导致复杂或抽象的交互检测受到阻碍。</li>
<li>methods: 本研究使用了视觉化和优化的查询设计、广泛的键和值搜索以及盒对位嵌入作为空间引导，以提高 predicate visual context（PViC）模型的表现，并与当前状态顶峰方法在 HICO-DET 和 V-COCO 测试集上进行比较。</li>
<li>results: 根据测试结果，我们的 PViC 模型在 HICO-DET 和 V-COCO 测试集上具有更高的表现，同时保持了训练成本的低。<details>
<summary>Abstract</summary>
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaining low training cost.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:Recently, DETR 框架在人-物互动（HOI）研究中成为主流方法。特别是两阶段转换器基于 HOI 检测器在性能和训练效率方面表现出色。然而，这些通常基于缺乏细化上下文信息的对象特征，忽略对象的姿势和方向信息，而仅仅依靠对象的视觉特征来确定对象的标识和边框极限。这会导致复杂或抽象的互动无法正确识别。在这项工作中，我们通过视觉化和仔细设计的实验来研究这些问题。我们尝试通过跨注意力来重新引入图像特征，并通过改进的查询设计、广泛探索键和值、以及对象对的位域嵌入来提高 predicate 视觉上下文（PViC）模型的性能。我们的 PViC 模型在 HICO-DET 和 V-COCO 测试数据集上的表现比 state-of-the-art 方法更高，而且训练成本仍然很低。
</details></li>
</ul>
<hr>
<h2 id="Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features"><a href="#Complex-Facial-Expression-Recognition-Using-Deep-Knowledge-Distillation-of-Basic-Features" class="headerlink" title="Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features"></a>Complex Facial Expression Recognition Using Deep Knowledge Distillation of Basic Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06197">http://arxiv.org/abs/2308.06197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/angusmaiden/complex-fer">https://github.com/angusmaiden/complex-fer</a></li>
<li>paper_authors: Angus Maiden, Bahareh Nakisa</li>
<li>for: 该论文的目的是提出一种基于人类认知和学习的新型不间断学习方法，以便准确地识别复杂的人脸表达。</li>
<li>methods: 该方法基于人类认知和学习，包括知识储存、知识总结和预测排序记忆等技术。它还使用 GradCAM 视觉化来表明基本和复杂表达之间的关系。</li>
<li>results: 该方法可以准确地识别新的复杂表达类型，使用少量示例来学习新类别，并且在新类别上达到了74.28% 的总准确率。此外，该方法还证明了不间断学习方法在复杂表达识别中的优越性，比非不间断学习方法高出13.95%。此外，该方法还是首次应用了几shot学习到复杂表达识别中，达到了100% 的准确率。<details>
<summary>Abstract</summary>
Complex emotion recognition is a cognitive task that has so far eluded the same excellent performance of other tasks that are at or above the level of human cognition. Emotion recognition through facial expressions is particularly difficult due to the complexity of emotions expressed by the human face. For a machine to approach the same level of performance in this domain as a human, it may need to synthesise knowledge and understand new concepts in real-time as humans do. Humans are able to learn new concepts using only few examples, by distilling the important information from memories and discarding the rest. Similarly, continual learning methods learn new classes whilst retaining the knowledge of known classes, whilst few-shot learning methods are able to learn new classes using very few training examples. We propose a novel continual learning method inspired by human cognition and learning that can accurately recognise new compound expression classes using few training samples, by building on and retaining its knowledge of basic expression classes. Using GradCAM visualisations, we demonstrate the relationship between basic and compound facial expressions, which our method leverages through knowledge distillation and a novel Predictive Sorting Memory Replay. Our method achieves the current state-of-the-art in continual learning for complex facial expression recognition with 74.28% Overall Accuracy on new classes. We also demonstrate that using continual learning for complex facial expression recognition achieves far better performance than non-continual learning methods, improving on state-of-the-art non-continual learning methods by 13.95%. To the best of our knowledge, our work is also the first to apply few-shot learning to complex facial expression recognition, achieving the state-of-the-art with 100% accuracy using a single training sample for each expression class.
</details>
<details>
<summary>摘要</summary>
人工智能在复杂情绪认知方面的表现，虽然已经达到了其他一些任务的水平，但是情绪认知仍然是一个挑战。人脸表达的情绪认知特别Difficult，因为人脸上可以表达出的情感复杂多样。为了让机器达到人类水平，它可能需要合并知识并理解新概念，就像人类一样。人类可以通过几个示例学习新概念，从记忆中提炼出重要信息，并丢弃其他信息。我们提出了一种基于人类认知和学习的新型连续学习方法，可以准确地识别新的复杂表达类型，使用很少的训练样本。我们使用GradCAM视觉化来描述基本和复杂表达之间的关系，然后通过知识储存和一种新的预测排序记忆回放来利用这种关系。我们的方法实现了当前领域内连续学习的最佳性，新类别上的总准确率为74.28%。我们还证明了使用连续学习进行复杂表达认知比非连续学习方法更好，提高了领域内最佳非连续学习方法的13.95%。此外，我们是第一个将几个样本学习应用于复杂表达认知，并达到了领域内最佳性，每个表达类型的准确率为100%。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Guest-Nationality-Composition-from-Hotel-Reviews"><a href="#Assessing-Guest-Nationality-Composition-from-Hotel-Reviews" class="headerlink" title="Assessing Guest Nationality Composition from Hotel Reviews"></a>Assessing Guest Nationality Composition from Hotel Reviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06175">http://arxiv.org/abs/2308.06175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Gröger, Marc Pouly, Flavia Tinner, Leif Brandes</li>
<li>for: 这篇论文旨在为企业提供方法，以优化客户端的客户分布。</li>
<li>methods: 该论文使用机器学习技术，从不结构化文本评论中提取出客户国籍信息，以动态评估和监测具体业务客户分布的变化。</li>
<li>results: 研究表明，使用简单的预训练embeddings和堆式LSTM层可以提供更好的性能-运行时间平衡，比较复杂的语言模型。<details>
<summary>Abstract</summary>
Many hotels target guest acquisition efforts to specific markets in order to best anticipate individual preferences and needs of their guests. Likewise, such strategic positioning is a prerequisite for efficient marketing budget allocation. Official statistics report on the number of visitors from different countries, but no fine-grained information on the guest composition of individual businesses exists. There is, however, growing interest in such data from competitors, suppliers, researchers and the general public. We demonstrate how machine learning can be leveraged to extract references to guest nationalities from unstructured text reviews in order to dynamically assess and monitor the dynamics of guest composition of individual businesses. In particular, we show that a rather simple architecture of pre-trained embeddings and stacked LSTM layers provides a better performance-runtime tradeoff than more complex state-of-the-art language models.
</details>
<details>
<summary>摘要</summary>
许多酒店会向特定市场进行客户营销努力，以最好地预测客人偏好和需求。这种策略也是市场营销预算的必要前提。官方统计数据会报告不同国家的游客数量，但没有细化的信息对具体的企业客户组成进行了报告。然而，有越来越多的竞争对手、供应商、研究人员和公众对这些数据感兴趣。我们示例如如何使用机器学习来从无结构文本评论中提取客人国籍信息，以动态评估和监测个体企业客户组成的动态变化。具体来说，我们发现使用简单的预训练 embedding 和堆叠 LSTM 层可以提供更好的性能-运行时间质量比例，比较复杂的现状语言模型。
</details></li>
</ul>
<hr>
<h2 id="Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook"><a href="#Physical-Adversarial-Attacks-For-Camera-based-Smart-Systems-Current-Trends-Categorization-Applications-Research-Challenges-and-Future-Outlook" class="headerlink" title="Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook"></a>Physical Adversarial Attacks For Camera-based Smart Systems: Current Trends, Categorization, Applications, Research Challenges, and Future Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06173">http://arxiv.org/abs/2308.06173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amira Guesmi, Muhammad Abdullah Hanif, Bassem Ouni, Muhammed Shafique</li>
<li>for: 这篇论文旨在为研究人员、实践者和政策制定者提供关于物理抗击攻击的全面评估和概述，以便开发强健和安全的深度学习系统。</li>
<li>methods: 论文分析了物理抗击攻击的主要特征和特点，并描述了不同应用领域中的物理抗击攻击方法，包括分类、检测、人脸识别、 semantic segmentation 和深度估计。</li>
<li>results: 论文评估了这些攻击方法的效果、隐蔽性和可靠性，并探讨了如何在实际世界中执行攻击，以及如何提高防御机制。<details>
<summary>Abstract</summary>
In this paper, we present a comprehensive survey of the current trends focusing specifically on physical adversarial attacks. We aim to provide a thorough understanding of the concept of physical adversarial attacks, analyzing their key characteristics and distinguishing features. Furthermore, we explore the specific requirements and challenges associated with executing attacks in the physical world. Our article delves into various physical adversarial attack methods, categorized according to their target tasks in different applications, including classification, detection, face recognition, semantic segmentation and depth estimation. We assess the performance of these attack methods in terms of their effectiveness, stealthiness, and robustness. We examine how each technique strives to ensure the successful manipulation of DNNs while mitigating the risk of detection and withstanding real-world distortions. Lastly, we discuss the current challenges and outline potential future research directions in the field of physical adversarial attacks. We highlight the need for enhanced defense mechanisms, the exploration of novel attack strategies, the evaluation of attacks in different application domains, and the establishment of standardized benchmarks and evaluation criteria for physical adversarial attacks. Through this comprehensive survey, we aim to provide a valuable resource for researchers, practitioners, and policymakers to gain a holistic understanding of physical adversarial attacks in computer vision and facilitate the development of robust and secure DNN-based systems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了物理 adversarial 攻击的现代趋势的全面检查。我们的目的是为您提供物理 adversarial 攻击的深入理解，包括其关键特征和区别特征。此外，我们还探讨了在物理世界中执行攻击的具体要求和挑战。我们的文章探讨了不同应用领域中的物理 adversarial 攻击方法，分为不同的目标任务，如分类、检测、识别、 semantic segmentation 和深度估计。我们评估了这些攻击方法的效果、隐蔽性和Robustness。我们探究每种技术如何在 DNN 上成功 manipulate 而 minimizing 检测和快速应对实际扭曲。最后，我们讨论了物理 adversarial 攻击领域的当前挑战和未来研究方向，包括增强防御机制、探索新的攻击策略、在不同应用领域中评估攻击、以及建立 DNN 领域的标准化评估标准和评估方法。通过这篇全面的检查，我们希望为研究人员、实践人员和政策制定者提供一份有价值的资源，以便更好地理解物理 adversarial 攻击，并促进 DNN 基于系统的开发。
</details></li>
</ul>
<hr>
<h2 id="Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction"><a href="#Phased-Deep-Spatio-temporal-Learning-for-Highway-Traffic-Volume-Prediction" class="headerlink" title="Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction"></a>Phased Deep Spatio-temporal Learning for Highway Traffic Volume Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06155">http://arxiv.org/abs/2308.06155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilong Ding, Tianpu Zhang, Zhe Wang</li>
<li>for: 预测高速公路日常交通量</li>
<li>methods: employs a hybrid model combining fully convolution network (FCN) and long short-term memory (LSTM), considering time, space, meteorology, and calendar from heterogeneous data</li>
<li>results: 实际使用一个中国省级高速公路的实际数据，对比traditional models，our method has distinct improvement for predictive accuracy, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.<details>
<summary>Abstract</summary>
Inter-city highway transportation is significant for citizens' modern urban life and generates heterogeneous sensory data with spatio-temporal characteristics. As a routine analysis in transportation domain, daily traffic volume estimation faces challenges for highway toll stations including lacking of exploration of correlative spatio-temporal features from a long-term perspective and effective means to deal with data imbalance which always deteriorates the predictive performance. In this paper, a deep spatio-temporal learning method is proposed to predict daily traffic volume in three phases. In feature pre-processing phase, data is normalized elaborately according to latent long-tail distribution. In spatio-temporal learning phase, a hybrid model is employed combining fully convolution network (FCN) and long short-term memory (LSTM), which considers time, space, meteorology, and calendar from heterogeneous data. In decision phase, traffic volumes on a coming day at network-wide toll stations would be achieved effectively, which is especially calibrated for vital few highway stations. Using real-world data from one Chinese provincial highway, extensive experiments show our method has distinct improvement for predictive accuracy than various traditional models, reaching 5.269 and 0.997 in MPAE and R-squre metrics, respectively.
</details>
<details>
<summary>摘要</summary>
市区间高速公路交通是现代城市居民日常生活中的重要一环，生成了多样化的感知数据，具有空间时间特征。为了解决高速公路收费站的日常交通量预测问题，我们面临着缺乏探索长期征特的相关空间时间特征以及有效地处理数据不均衡问题的挑战。在本文中，我们提出了一种深度空间时间学习方法，可以预测高速公路收费站的日常交通量。在特征预处理阶段，我们对数据进行了细心的Normal化，根据潜在的长尾分布。在空间时间学习阶段，我们采用了一种混合模型，结合了全连接网络（FCN）和长短期记忆（LSTM），考虑了时间、空间、气象和历法等多种不同数据。在决策阶段，我们可以准确预测高速公路收费站的未来一天内的交通量，特别是对于重要的一些高速公路站点进行了精准补做。使用了一个中国省道高速公路的实际数据，我们进行了广泛的实验，结果表明，我们的方法在预测精度方面与传统模型相比，具有明显的改善，分别达到了5.269和0.997的MPAE和R-squre指标。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Process-Regression-for-Maximum-Entropy-Distribution"><a href="#Gaussian-Process-Regression-for-Maximum-Entropy-Distribution" class="headerlink" title="Gaussian Process Regression for Maximum Entropy Distribution"></a>Gaussian Process Regression for Maximum Entropy Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06149">http://arxiv.org/abs/2308.06149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Sadr, Manuel Torrilhon, M. Hossein Gorji</li>
<li>for: 用于闭合问题的最大熵分布</li>
<li>methods: 使用加aussian prior逼近Lagrange多个性数</li>
<li>results: 对不同的kernel函数和Hyperparameter进行优化，实现数据驱动的最大熵闭合，并应用于非平衡分布的relaxation和Bhatnagar-Gross-Krook等方程的解。<details>
<summary>Abstract</summary>
Maximum-Entropy Distributions offer an attractive family of probability densities suitable for moment closure problems. Yet finding the Lagrange multipliers which parametrize these distributions, turns out to be a computational bottleneck for practical closure settings. Motivated by recent success of Gaussian processes, we investigate the suitability of Gaussian priors to approximate the Lagrange multipliers as a map of a given set of moments. Examining various kernel functions, the hyperparameters are optimized by maximizing the log-likelihood. The performance of the devised data-driven Maximum-Entropy closure is studied for couple of test cases including relaxation of non-equilibrium distributions governed by Bhatnagar-Gross-Krook and Boltzmann kinetic equations.
</details>
<details>
<summary>摘要</summary>
最大熵分布可以提供一个有优点的可能性密度，适用于矩阵闭合问题。然而，计算这些分布的拉格朗日 Parameters是实际应用中的计算瓶颈。受最近 Gaussian 过程的成功启发，我们研究将 Gaussian 假设用于 Approximate 拉格朗日 Parameters 的Map 的可行性。对各种核函数进行优化，我们使用最大 log-likelihood 来优化 гипер参数。我们研究了这种数据驱动的最大熵闭合的性能，并在几个测试案例中，包括适应不平衡分布的Relaxation 和 Boltzmann 动力学方程的闭合。
</details></li>
</ul>
<hr>
<h2 id="A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning"><a href="#A-New-Approach-to-Overcoming-Zero-Trade-in-Gravity-Models-to-Avoid-Indefinite-Values-in-Linear-Logarithmic-Equations-and-Parameter-Verification-Using-Machine-Learning" class="headerlink" title="A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning"></a>A New Approach to Overcoming Zero Trade in Gravity Models to Avoid Indefinite Values in Linear Logarithmic Equations and Parameter Verification Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06303">http://arxiv.org/abs/2308.06303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikrajuddin Abdullah</li>
<li>For: The paper aims to solve the challenge of identifying gravity parameters in the gravity model to explain international trade, specifically when there are zero flow trades.* Methods: The authors propose a two-step technique that involves performing linear regression locally to establish a dummy value for zero flow trades, and then estimating the gravity parameters using iterative techniques. Machine learning is also used to test the estimated parameters by analyzing their position in the cluster.* Results: The authors calculate international trade figures for 2004, 2009, 2014, and 2019 and find that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented in the paper can be used to solve other problems involving log-linear regression.Here’s the simplified Chinese text for the three information points:* 用途：文章解决国际贸易预测模型中零流通问题，即预测零流通时的重力参数。* 方法：文章提出了一种两步技术，先本地线性回归以确定零流通的假值，然后使用迭代法估算重力参数。同时，文章使用机器学习测试估算参数的位置在集群中。* 结果：文章计算了2004年、2009年、2014年和2019年的国际贸易数据，发现GDP的势和距离的势都处在同一个集群，均值约为1。文章的策略可以解决其他带有封零的log-线性回归问题。<details>
<summary>Abstract</summary>
The presence of a high number of zero flow trades continues to provide a challenge in identifying gravity parameters to explain international trade using the gravity model. Linear regression with a logarithmic linear equation encounters an indefinite value on the logarithmic trade. Although several approaches to solving this problem have been proposed, the majority of them are no longer based on linear regression, making the process of finding solutions more complex. In this work, we suggest a two-step technique for determining the gravity parameters: first, perform linear regression locally to establish a dummy value to substitute trade flow zero, and then estimating the gravity parameters. Iterative techniques are used to determine the optimum parameters. Machine learning is used to test the estimated parameters by analyzing their position in the cluster. We calculated international trade figures for 2004, 2009, 2014, and 2019. We just examine the classic gravity equation and discover that the powers of GDP and distance are in the same cluster and are both worth roughly one. The strategy presented here can be used to solve other problems involving log-linear regression.
</details>
<details>
<summary>摘要</summary>
高数量的零流贸易仍然成为国际贸易使用重力模型确定重力参数的挑战。线性回归的对数几何方程遇到了对数贸易的不定值。虽然有几种解决方案被提议，但大多数都不再基于线性回归，使得解决问题的过程变得更加复杂。在这项工作中，我们提议一种两步技巧来确定重力参数：首先，在本地使用线性回归来设置一个占位符来替代零流贸易，然后估算重力参数。使用迭代技术来确定优化参数。机器学习被用来测试估算的参数，分析它们的位置在群集中。我们计算了2004、2009、2014和2019年的国际贸易数据。我们只考虑 классический重力方程，发现GDP的势和距离的势都在同一个群集中，它们的值约为1。这种策略可以用于解决其他带有对数几何方程的问题。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models"><a href="#Identification-of-the-Relevance-of-Comments-in-Codes-Using-Bag-of-Words-and-Transformer-Based-Models" class="headerlink" title="Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models"></a>Identification of the Relevance of Comments in Codes Using Bag of Words and Transformer Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06144">http://arxiv.org/abs/2308.06144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sruthisudheer/comment-classification-of-c-code">https://github.com/sruthisudheer/comment-classification-of-c-code</a></li>
<li>paper_authors: Sruthi S, Tanmay Basu</li>
<li>for: 本研究的目的是为了对不同代码段的注释进行分类。</li>
<li>methods: 本研究使用了不同的特征工程方案和文本分类技术，包括经典的袋包模型和基于变换器的模型。</li>
<li>results: 研究发现，使用袋包模型在训练集上表现最佳，但模型在训练和测试集上的表现不理想。研究还总结了模型的局限性和进一步改进的可能性。<details>
<summary>Abstract</summary>
The Forum for Information Retrieval (FIRE) started a shared task this year for classification of comments of different code segments. This is binary text classification task where the objective is to identify whether comments given for certain code segments are relevant or not. The BioNLP-IISERB group at the Indian Institute of Science Education and Research Bhopal (IISERB) participated in this task and submitted five runs for five different models. The paper presents the overview of the models and other significant findings on the training corpus. The methods involve different feature engineering schemes and text classification techniques. The performance of the classical bag of words model and transformer-based models were explored to identify significant features from the given training corpus. We have explored different classifiers viz., random forest, support vector machine and logistic regression using the bag of words model. Furthermore, the pre-trained transformer based models like BERT, RoBERT and ALBERT were also used by fine-tuning them on the given training corpus. The performance of different such models over the training corpus were reported and the best five models were implemented on the given test corpus. The empirical results show that the bag of words model outperforms the transformer based models, however, the performance of our runs are not reasonably well in both training and test corpus. This paper also addresses the limitations of the models and scope for further improvement.
</details>
<details>
<summary>摘要</summary>
forum for information retrieval (FIRE) 这年开始了代码段评注分类的共同任务。这是一个二进制文本分类任务，目标是判断给定的代码段评注是否相关。bioNLP-IISERB 组在印度科学教育研究所 Bhopal（IISERB）参加了这个任务，并提交了五个运行，每个运行使用了不同的模型。本文介绍了模型和其他有关训练集的发现。方法包括不同的特征工程方案和文本分类技术。我们研究了传统的袋子模型和基于 transformer 的模型，并使用了不同的分类器，如Random Forest、支持向量机和логисти准则回归。此外，我们还使用了预训练的 transformer 模型，如 BERT、RoBERT 和 ALBERT，并在给定的训练集上进行了微调。对于给定的训练集和测试集，我们Reported 不同模型的性能。 Results show that the bag of words model outperforms the transformer-based models, but the performance of our runs is not satisfactory in both the training and test corpora. This paper also discusses the limitations of the models and the scope for further improvement.
</details></li>
</ul>
<hr>
<h2 id="CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients"><a href="#CompTLL-UNet-Compressed-Domain-Text-Line-Localization-in-Challenging-Handwritten-Documents-using-Deep-Feature-Learning-from-JPEG-Coefficients" class="headerlink" title="CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients"></a>CompTLL-UNet: Compressed Domain Text-Line Localization in Challenging Handwritten Documents using Deep Feature Learning from JPEG Coefficients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06142">http://arxiv.org/abs/2308.06142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bulla Rajesh, Sk Mahafuz Zaman, Mohammed Javed, P. Nagabhushan</li>
<li>for: 本研究旨在提出一种能够直接在JPEG压缩表示中进行文本线Localization的方法，以避免 decompression 和重新压缩的过程，从而降低存储和计算成本。</li>
<li>methods: 提出了一种基于深度特征学习的Modified U-Net架构，称为Compressed Text-Line Localization Network (CompTLL-UNet)，用于实现文本线Localization。</li>
<li>results: 通过对ICDAR2017（cBAD）和ICDAR2019（cBAD）测试集进行训练和测试，实现了在JPEG压缩Domain中的文本线Localization，并reported state-of-the-art perfomance。<details>
<summary>Abstract</summary>
Automatic localization of text-lines in handwritten documents is still an open and challenging research problem. Various writing issues such as uneven spacing between the lines, oscillating and touching text, and the presence of skew become much more challenging when the case of complex handwritten document images are considered for segmentation directly in their respective compressed representation. This is because, the conventional way of processing compressed documents is through decompression, but here in this paper, we propose an idea that employs deep feature learning directly from the JPEG compressed coefficients without full decompression to accomplish text-line localization in the JPEG compressed domain. A modified U-Net architecture known as Compressed Text-Line Localization Network (CompTLL-UNet) is designed to accomplish it. The model is trained and tested with JPEG compressed version of benchmark datasets including ICDAR2017 (cBAD) and ICDAR2019 (cBAD), reporting the state-of-the-art performance with reduced storage and computational costs in the JPEG compressed domain.
</details>
<details>
<summary>摘要</summary>
自动化手写文档中文行的本地化仍然是一个打开的和挑战性的研究问题。不同的写作问题，如文本行间距不均匀、文本抖动和触摸、扭曲等问题，在考虑复杂手写文档图像时变得更加挑战。这是因为，传统的文档处理方法是通过解压缩来处理压缩文档，但在这篇论文中，我们提出了一个想法，即使用深度特征学习直接从JPEG压缩率中提取特征来实现文本行的本地化。我们称之为Compressed Text-Line Localization Network（CompTLL-UNet）。我们设计了一种修改后的U-Net架构，并对其进行训练和测试，使用JPEG压缩版本的标准评测数据集，包括ICDAR2017（cBAD）和ICDAR2019（cBAD）。我们的模型在JPEG压缩领域实现了状态之巅性表现，同时具有减少存储和计算成本的优点。
</details></li>
</ul>
<hr>
<h2 id="Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling"><a href="#Application-of-Artificial-Neural-Networks-for-Investigation-of-Pressure-Filtration-Performance-a-Zinc-Leaching-Filter-Cake-Moisture-Modeling" class="headerlink" title="Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling"></a>Application of Artificial Neural Networks for Investigation of Pressure Filtration Performance, a Zinc Leaching Filter Cake Moisture Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06138">http://arxiv.org/abs/2308.06138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoume Kazemi, Davood Moradkhani, Alireza A. Alipour</li>
<li>for: 这项研究旨在开发一个人工神经网络模型，用于预测压filtering proces中的蛋白湿度。</li>
<li>methods: 该研究使用了人工神经网络技术，并在288次测试中采用了两种不同的 Filter Fabric（S1和S2）。</li>
<li>results: 研究结果显示，人工神经网络模型可以高度准确地预测压filtering proces中的蛋白湿度，R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。<details>
<summary>Abstract</summary>
Machine Learning (ML) is a powerful tool for material science applications. Artificial Neural Network (ANN) is a machine learning technique that can provide high prediction accuracy. This study aimed to develop an ANN model to predict the cake moisture of the pressure filtration process of zinc production. The cake moisture was influenced by seven parameters: temperature (35 and 65 Celsius), solid concentration (0.2 and 0.38 g/L), pH (2, 3.5, and 5), air-blow time (2, 10, and 15 min), cake thickness (14, 20, 26, and 34 mm), pressure, and filtration time. The study conducted 288 tests using two types of fabrics: polypropylene (S1) and polyester (S2). The ANN model was evaluated by the Coefficient of determination (R2), the Mean Square Error (MSE), and the Mean Absolute Error (MAE) metrics for both datasets. The results showed R2 values of 0.88 and 0.83, MSE values of 6.243x10-07 and 1.086x10-06, and MAE values of 0.00056 and 0.00088 for S1 and S2, respectively. These results indicated that the ANN model could predict the cake moisture of pressure filtration in the zinc leaching process with high accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）是一种强大的工具，可以应用于材料科学领域。人工神经网络（ANN）是一种机器学习技术，可以提供高精度预测。本研究目的是开发一个ANN模型，以预测压 filtering过程中锻生产中的蛋白湿度。蛋白湿度受到七个参数的影响：温度（35和65摄氏度）、固体浓度（0.2和0.38g/L）、pH（2、3.5和5）、空气喷流时间（2、10和15分）、蛋白厚度（14、20、26和34mm）、压力和过滤时间。研究通过288次测试，使用两种不同的 fabrics： polypropylene（S1）和 polyester（S2）。ANN模型被评估于 Coefficient of determination（R2）、Mean Square Error（MSE）和 Mean Absolute Error（MAE）三个指标，其中R2值分别为0.88和0.83，MSE值分别为6.243x10-07和1.086x10-06，MAE值分别为0.00056和0.00088。这些结果表明，ANN模型可以高精度预测压 filtering过程中的蛋白湿度。
</details></li>
</ul>
<hr>
<h2 id="PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion"><a href="#PDE-Discovery-for-Soft-Sensors-Using-Coupled-Physics-Informed-Neural-Network-with-Akaike’s-Information-Criterion" class="headerlink" title="PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion"></a>PDE Discovery for Soft Sensors Using Coupled Physics-Informed Neural Network with Akaike’s Information Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06132">http://arxiv.org/abs/2308.06132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aina Wang, Pan Qin, Xi-Ming Sun</li>
<li>For: 这篇论文旨在探讨一种基于物理学习的软传感器PDE发现方法，以便更好地监测工业过程中的关键变量。* Methods: 该方法基于物理学习的软传感器PDE模型，并使用了Akaike的准则信息来找到合适的偏微分方程结构。* Results: 实验结果表明，CPINN-AIC方法可以准确地找到合适的偏微分方程结构，并且可以用来预测工业过程中的变量。<details>
<summary>Abstract</summary>
Soft sensors have been extensively used to monitor key variables using easy-to-measure variables and mathematical models. Partial differential equations (PDEs) are model candidates for soft sensors in industrial processes with spatiotemporal dependence. However, gaps often exist between idealized PDEs and practical situations. Discovering proper structures of PDEs, including the differential operators and source terms, can remedy the gaps. To this end, a coupled physics-informed neural network with Akaike's criterion information (CPINN-AIC) is proposed for PDE discovery of soft sensors. First, CPINN is adopted for obtaining solutions and source terms satisfying PDEs. Then, we propose a data-physics-hybrid loss function for training CPINN, in which undetermined combinations of differential operators are involved. Consequently, AIC is used to discover the proper combination of differential operators. Finally, the artificial and practical datasets are used to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.
</details>
<details>
<summary>摘要</summary>
Soft sensors 通过使用容易测量的变量和数学模型来监测关键变量。但是，实际情况中存在idealized PDEs和实际情况之间的差距。发现合适的 PDE 结构，包括偏微分运算和源项，可以解决这些差距。为此，我们提出了物理学习混合数据损失函数（CPINN-AIC），用于PDE发现。首先，我们采用CPINN来获取满足 PDE 的解和源项。然后，我们提出了一种数据物理混合损失函数，其中包含未知的偏微分运算。最后，我们使用AIC来发现合适的偏微分运算结构。 Finally, we use artificial and practical datasets to verify the feasibility and effectiveness of CPINN-AIC for soft sensors. The proposed CPINN-AIC is a data-driven method to discover proper PDE structures and neural network-based solutions for soft sensors.Note: The translation is done using the Simplified Chinese grammar and vocabulary, which is commonly used in mainland China. However, it should be noted that there are different dialects and variations of Chinese, and the translation may vary depending on the specific region or dialect.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities"><a href="#Uncertainty-Quantification-for-Image-based-Traffic-Prediction-across-Cities" class="headerlink" title="Uncertainty Quantification for Image-based Traffic Prediction across Cities"></a>Uncertainty Quantification for Image-based Traffic Prediction across Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06129">http://arxiv.org/abs/2308.06129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alextimans/traffic4cast-uncertainty">https://github.com/alextimans/traffic4cast-uncertainty</a></li>
<li>paper_authors: Alexander Timans, Nina Wiedemann, Nishant Kumar, Ye Hong, Martin Raubal</li>
<li>for: 这个论文的目的是提高深度学习模型在智能交通系统中的可解释性，以便更好地做出决策和提高模型的部署潜力。</li>
<li>methods: 这个论文使用了两种 эпистемиче和两种 aleatoric 不确定性评估方法来评估深度学习模型的不确定性，并对多个城市和时间段进行比较。</li>
<li>results: 研究发现，可以通过使用不确定性评估方法来获得有意义的不确定性估计，并且可以用这些估计来检测城市交通动力学的异常情况。在一个 Moscow 的示例研究中，我们发现了时间和空间效应的影响于城市交通行为。<details>
<summary>Abstract</summary>
Despite the strong predictive performance of deep learning models for traffic prediction, their widespread deployment in real-world intelligent transportation systems has been restrained by a lack of interpretability. Uncertainty quantification (UQ) methods provide an approach to induce probabilistic reasoning, improve decision-making and enhance model deployment potential. To gain a comprehensive picture of the usefulness of existing UQ methods for traffic prediction and the relation between obtained uncertainties and city-wide traffic dynamics, we investigate their application to a large-scale image-based traffic dataset spanning multiple cities and time periods. We compare two epistemic and two aleatoric UQ methods on both temporal and spatio-temporal transfer tasks, and find that meaningful uncertainty estimates can be recovered. We further demonstrate how uncertainty estimates can be employed for unsupervised outlier detection on changes in city traffic dynamics. We find that our approach can capture both temporal and spatial effects on traffic behaviour in a representative case study for the city of Moscow. Our work presents a further step towards boosting uncertainty awareness in traffic prediction tasks, and aims to highlight the value contribution of UQ methods to a better understanding of city traffic dynamics.
</details>
<details>
<summary>摘要</summary>
启示深度学习模型对交通预测的强大预测能力，实际应用中的广泛部署却受到了不可预测性的限制。不确定性评估（UQ）方法可以带来 probabilistic reasoning，改善决策和提高模型部署的潜力。为了了解现有UQ方法对交通预测的用途和获得的不确定性与城市范围内交通动力学的关系，我们对多座城市和多个时间段的大规模图像基本交通数据进行了 investigate。我们比较了两种 эпистеمic和两种 aleatoric UQ方法的性能在时间和空间转移任务上，并发现了有意义的不确定性估计可以被恢复。我们还示出了如何使用不确定性估计进行无supervised outlier检测，检测城市交通动力学的变化。我们在 Moskva 城市的示例研究中发现，我们的方法可以捕捉到时间和空间效应的交通行为。我们的工作是 uncertainty awareness 在交通预测任务中的一个进一步步骤，旨在高亮不确定性评估对城市交通动力学的理解的重要性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data"><a href="#Learning-Control-Policies-for-Variable-Objectives-from-Offline-Data" class="headerlink" title="Learning Control Policies for Variable Objectives from Offline Data"></a>Learning Control Policies for Variable Objectives from Offline Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06127">http://arxiv.org/abs/2308.06127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Weber, Phillip Swazinna, Daniel Hein, Steffen Udluft, Volkmar Sterzing</li>
<li>for: 本研究旨在提供一种可靠的机器学习方法，用于控制动态系统，特别是当直接与环境交互不可用时。</li>
<li>methods: 本研究使用了模型基于政策搜索方法的扩展，即变量目标策略（VOP）。这种方法使得政策可以高效地泛化到多种目标，即在奖励函数中的参数。</li>
<li>results: 通过对目标函数中的参数进行调整，用户可以在运行时调整政策的行为或重新平衡优化目标，无需收集更多的观察批量或重新训练。<details>
<summary>Abstract</summary>
Offline reinforcement learning provides a viable approach to obtain advanced control strategies for dynamical systems, in particular when direct interaction with the environment is not available. In this paper, we introduce a conceptual extension for model-based policy search methods, called variable objective policy (VOP). With this approach, policies are trained to generalize efficiently over a variety of objectives, which parameterize the reward function. We demonstrate that by altering the objectives passed as input to the policy, users gain the freedom to adjust its behavior or re-balance optimization targets at runtime, without need for collecting additional observation batches or re-training.
</details>
<details>
<summary>摘要</summary>
转换文本为简化中文：</SYS>在线上学习不可靠的方法可以提供先进的控制策略 для动力系统，特别是当直接与环境进行交互不可用时。本文提出了基于模型的政策搜索方法的概念扩展，称为变量目标策略（VOP）。通过这种方法，政策被训练以通用化效率地处理多种目标，这些目标参数化奖励函数。我们示例显示，通过在运行时更改目标，用户可以在不需要收集更多观察批处或重新训练的情况下，通过变化目标来调整行为或重新平衡优化目标。
</details></li>
</ul>
<hr>
<h2 id="Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic"><a href="#Learning-Deductive-Reasoning-from-Synthetic-Corpus-based-on-Formal-Logic" class="headerlink" title="Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic"></a>Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07336">http://arxiv.org/abs/2308.07336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitachi-nlp/fld">https://github.com/hitachi-nlp/fld</a></li>
<li>paper_authors: Terufumi Morishita, Gaku Morio, Atsuki Yamaguchi, Yasuhiro Sogawa</li>
<li>for: 本研究旨在使语言模型（LM）学习逻辑推理能力。 previous studies使用特定的推理规则生成推理例子，但这些规则有限或else arbitrary，这限制了获得的逻辑推理能力的一致性。</li>
<li>methods: 我们采用基于正式逻辑理论的准确的推理规则集，可以 derivation 任何其他推理规则。我们名为这种推理规则集为 $\textbf{FLD}$（正式逻辑推理）。</li>
<li>results: 我们实验表明，使用 $\textbf{FLD}$ 推理规则集训练LMs，LMs可以获得更一致的逻辑推理能力。此外，我们还识别了逻辑推理能力中哪些方面可以通过推理 corpora 增强LMs，那些方面无法增强。最后，基于这些结果，我们讨论未来如何使用推理 corpora 或其他方法来解决每个方面的问题。我们发布了代码、数据和模型。<details>
<summary>Abstract</summary>
We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each aspect. We release the code, data, and models.
</details>
<details>
<summary>摘要</summary>
我们研究了一种基于合成语料库的方法，用于语言模型（LM）学习逻辑推理能力。之前的研究通过特定的推理规则生成了推理示例，但这些规则是有限的或else是arbitrary的。这会限制学习得到的逻辑推理能力的通用性。我们重新思考了这一点，采用基于形式逻辑理论的固定的推理规则，可以在多步骤中组合以 derivation任何其他的推理规则。我们employmaterially verify that LMs trained on our proposed corpora，which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability。此外，我们还确定了推理能力中哪些方面可以通过推理 corpora进行增强，以及哪些方面无法进行增强。最后，基于这些结果，我们讨论了将来采用推理 corpora或其他方法对每个方面的应用。我们释放了代码、数据和模型。
</details></li>
</ul>
<hr>
<h2 id="Hawkes-Processes-with-Delayed-Granger-Causality"><a href="#Hawkes-Processes-with-Delayed-Granger-Causality" class="headerlink" title="Hawkes Processes with Delayed Granger Causality"></a>Hawkes Processes with Delayed Granger Causality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06106">http://arxiv.org/abs/2308.06106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Hengyuan Miao, Shuang Li</li>
<li>for: 本研究旨在Explicitly Modeling delayed Granger causal effects based on multivariate Hawkes processes, 即 causal event usually takes some time to exert an effect, 研究这个时间延迟自身的科学意义。</li>
<li>methods: 我们首先证明了延迟参数的可 identificability under mild conditions, 然后 investigate a model estimation method under complex setting, 即 want to infer the posterior distribution of time lags and understand how this distribution varies across different scenarios, 我们将时延 treated as latent variables, 并使用Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of time lags.</li>
<li>results: 我们 empirically evaluate our model’s event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.<details>
<summary>Abstract</summary>
We aim to explicitly model the delayed Granger causal effects based on multivariate Hawkes processes. The idea is inspired by the fact that a causal event usually takes some time to exert an effect. Studying this time lag itself is of interest. Given the proposed model, we first prove the identifiability of the delay parameter under mild conditions. We further investigate a model estimation method under a complex setting, where we want to infer the posterior distribution of the time lags and understand how this distribution varies across different scenarios. We treat the time lags as latent variables and formulate a Variational Auto-Encoder (VAE) algorithm to approximate the posterior distribution of the time lags. By explicitly modeling the time lags in Hawkes processes, we add flexibility to the model. The inferred time-lag posterior distributions are of scientific meaning and help trace the original causal time that supports the root cause analysis. We empirically evaluate our model's event prediction and time-lag inference accuracy on synthetic and real data, achieving promising results.
</details>
<details>
<summary>摘要</summary>
我们目标是显式地模型延迟的格兰格 causal 效应基于多变量 Hawkes 过程。这个想法源于事件引起效应通常需要一些时间。研究这个时间延迟本身很有趣。给出的模型，我们首先证明延迟参数的可识别性于轻量级 услови下。我们进一步调查了一种复杂的设定下的模型估计方法，我们想要从多个enario中推断时延参数的 posterior 分布，并理解这个分布在不同enario下如何变化。我们将时延参数作为隐藏变量，并采用Variational Auto-Encoder（VAE）算法来近似 posterior 分布。通过显式地模型 Hawkes 过程中的时延参数，我们增加了模型的灵活性。经验证明我们的模型在实验数据上的事件预测和时延参数推断精度都很高。
</details></li>
</ul>
<hr>
<h2 id="Composable-Function-preserving-Expansions-for-Transformer-Architectures"><a href="#Composable-Function-preserving-Expansions-for-Transformer-Architectures" class="headerlink" title="Composable Function-preserving Expansions for Transformer Architectures"></a>Composable Function-preserving Expansions for Transformer Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06103">http://arxiv.org/abs/2308.06103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Gesmundo, Kaitlin Maile</li>
<li>for: 提高现代神经网络的训练成本，特别是计算和时间成本。</li>
<li>methods: 提出六种可 композиitely 的变换，用于逐步增加 transformer 类神经网络的大小，保持功能完整性。</li>
<li>results: 证明每种变换都可以保持函数完整性，并且可以有效地升级模型规模。<details>
<summary>Abstract</summary>
Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.
</details>
<details>
<summary>摘要</summary>
培训现代神经网络需要高效计算和时间成本。模型缩放被认为是提高现状的关键因素。在增加模型缩放时，通常需要从scratch开始，随机初始化整个模型的参数，因为这会导致模型结构中参数的变化，不允许小型模型知识的直接传递。在这项工作中，我们提出六种可组合的变换来逐步增加基于转换器的神经网络缩放，保持功能完整性，以便在训练过程中逐步扩展模型的容量。我们提供了准确功能保持的证明，并且在 minimal initialization constraints 下进行证明。这些方法可能会帮助建立更大更强的模型，并通过逐步扩展模型结构来实现高效的训练管道。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation"><a href="#Diffusion-based-Visual-Counterfactual-Explanations-–-Towards-Systematic-Quantitative-Evaluation" class="headerlink" title="Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation"></a>Diffusion-based Visual Counterfactual Explanations – Towards Systematic Quantitative Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06100">http://arxiv.org/abs/2308.06100</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cairo-thws/dbvce_eval">https://github.com/cairo-thws/dbvce_eval</a></li>
<li>paper_authors: Philipp Vaeth, Alexander M. Fruehwald, Benjamin Paassen, Magda Gregorova</li>
<li>For: This paper aims to provide a systematic and quantitative evaluation framework for visual counterfactual explanations (VCE) methods, and to explore the effects of crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet).* Methods: The paper proposes a framework for evaluating VCE methods using a minimal set of metrics, and conducts a battery of ablation-like experiments generating thousands of VCEs for a suite of classifiers of various complexity, accuracy, and robustness.* Results: The paper finds multiple directions for future advancements and improvements of VCE methods, and provides a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是提供一个系统的和量化的评估框架，以帮助评估视觉对称解释（VCE）方法，并探索最新的扩散基于生成模型中的关键设计选择对自然图像分类（ImageNet）VCE的影响。* Methods: 论文提出一个评估VCE方法的最小集合的度量，并通过大量的拟合实验生成了不同复杂性、准确率和稳定性的多个分类器的VCE。* Results: 论文发现了未来的进步和改进的方向，并提供了一个有价值的指南，以便在评估对称解释中增加一致性和透明度。<details>
<summary>Abstract</summary>
Latest methods for visual counterfactual explanations (VCE) harness the power of deep generative models to synthesize new examples of high-dimensional images of impressive quality. However, it is currently difficult to compare the performance of these VCE methods as the evaluation procedures largely vary and often boil down to visual inspection of individual examples and small scale user studies. In this work, we propose a framework for systematic, quantitative evaluation of the VCE methods and a minimal set of metrics to be used. We use this framework to explore the effects of certain crucial design choices in the latest diffusion-based generative models for VCEs of natural image classification (ImageNet). We conduct a battery of ablation-like experiments, generating thousands of VCEs for a suite of classifiers of various complexity, accuracy and robustness. Our findings suggest multiple directions for future advancements and improvements of VCE methods. By sharing our methodology and our approach to tackle the computational challenges of such a study on a limited hardware setup (including the complete code base), we offer a valuable guidance for researchers in the field fostering consistency and transparency in the assessment of counterfactual explanations.
</details>
<details>
<summary>摘要</summary>
最新的视觉对比解释方法（VCE）利用深度生成模型Synthesize高维像素图像的新示例，质量非常高。然而，目前很难比较这些VCE方法的性能，因为评估方法大多不同，经常降到视觉检查具体示例和小规模用户研究。在这项工作中，我们提出了一个系统性评估VCE方法的框架和最小的 metric集，并使用这些框架来探索 diffusion-based生成模型在自然图像分类（ImageNet）中VCE的效果。我们进行了一系列减少-like实验，生成了数千个VCE，用于一组不同的分类器，包括不同的复杂度、准确率和鲁棒性。我们的发现建议了未来VCE方法的进一步改进。通过分享我们的方法和我们对限制硬件设置（包括完整的代码库）的处理方式，我们提供了行业研究人员的有价值指南，促进了透明度和一致性在对对比解释的评估中。
</details></li>
</ul>
<hr>
<h2 id="Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes"><a href="#Neural-Conversation-Models-and-How-to-Rein-Them-in-A-Survey-of-Failures-and-Fixes" class="headerlink" title="Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes"></a>Neural Conversation Models and How to Rein Them in: A Survey of Failures and Fixes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06095">http://arxiv.org/abs/2308.06095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Galetzka, Anne Beyer, David Schlangen</li>
<li>for: 本研究探讨了基于强大语言模型的开放领域对话系统，以做出合适的对话贡献。</li>
<li>methods: 研究人员使用了不同的截止点和训练策略来控制语言模型，以确保贡献的优质。</li>
<li>results: 研究人员发现了一些有前途的方法，并建议了未来研究的新方向。<details>
<summary>Abstract</summary>
Recent conditional language models are able to continue any kind of text source in an often seemingly fluent way. This fact encouraged research in the area of open-domain conversational systems that are based on powerful language models and aim to imitate an interlocutor by generating appropriate contributions to a written dialogue. From a linguistic perspective, however, the complexity of contributing to a conversation is high. In this survey, we interpret Grice's maxims of cooperative conversation from the perspective of this specific research area and systematize the literature under the aspect of what makes a contribution appropriate: A neural conversation model has to be fluent, informative, consistent, coherent, and follow social norms. In order to ensure these qualities, recent approaches try to tame the underlying language models at various intervention points, such as data, training regime or decoding. Sorted by these categories and intervention points, we discuss promising attempts and suggest novel ways for future research.
</details>
<details>
<summary>摘要</summary>
现代条件语言模型能够继续任何类型的文本源，并且在看起来很流畅地进行交流。这一事实激发了基于强大语言模型的开放领域对话系统的研究，旨在通过生成相应的贡献来模拟对话伙伴。从语言学角度来看，参与对话的复杂度很高。在这种情况下，我们根据这个特定的研究领域来解释格雷斯的协作对话原则，并将文献分为合适贡献的几个方面：一个神经网络对话模型需要流畅、有用、一致、 coherent 和遵循社会规范。为确保这些质量，当前的方法在不同的 intervening point 上尝试控制基础语言模型，例如数据、训练方法或解码。按照这些类别和 intervening point 排序，我们讨论了有前途的尝试，并建议未来研究的新方法。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes"><a href="#Reinforcement-Logic-Rule-Learning-for-Temporal-Point-Processes" class="headerlink" title="Reinforcement Logic Rule Learning for Temporal Point Processes"></a>Reinforcement Logic Rule Learning for Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06094">http://arxiv.org/abs/2308.06094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Yang, Lu Wang, Kun Gao, Shuang Li</li>
<li>for: 用于解释 temporal events 的发生</li>
<li>methods: 使用 temporal point process modeling and learning framework，逐渐优化规则集和其重要性，并通过 neural search policy 生成新规则</li>
<li>results: 在 synthetic 和实际医疗数据上获得了promising的结果<details>
<summary>Abstract</summary>
We propose a framework that can incrementally expand the explanatory temporal logic rule set to explain the occurrence of temporal events. Leveraging the temporal point process modeling and learning framework, the rule content and weights will be gradually optimized until the likelihood of the observational event sequences is optimal. The proposed algorithm alternates between a master problem, where the current rule set weights are updated, and a subproblem, where a new rule is searched and included to best increase the likelihood. The formulated master problem is convex and relatively easy to solve using continuous optimization, whereas the subproblem requires searching the huge combinatorial rule predicate and relationship space. To tackle this challenge, we propose a neural search policy to learn to generate the new rule content as a sequence of actions. The policy parameters will be trained end-to-end using the reinforcement learning framework, where the reward signals can be efficiently queried by evaluating the subproblem objective. The trained policy can be used to generate new rules in a controllable way. We evaluate our methods on both synthetic and real healthcare datasets, obtaining promising results.
</details>
<details>
<summary>摘要</summary>
我们提出了一个框架，可以逐步扩展解释时间事件的发生。利用时间点处理模型和学习框架，规则内容和权重将被逐步优化，直到观测事件序列的可能性最高。我们的算法会 alternate между主问题和子问题。主问题中，当前规则集权重将被更新；而子问题中，一个新的规则将被搜索并添加到最大化可能性。我们形式ulated主问题是凸型的，可以使用连续优化来解决；而子问题则需要搜索庞大的 combinatorial 规则 predicate 和关系空间。为了解决这个挑战，我们提出了一种神经搜索策略，可以学习生成新规则的内容作为一个序列动作。这个策略的参数将通过可行学习框架进行培养，其中的奖励信号可以快速地查询由辅助问题的目标函数来提供。已经训练的策略可以用于生成新规则的控制方式。我们对具有 sintetic 和实际医疗数据的方法进行了评估，获得了有前途的结果。
</details></li>
</ul>
<hr>
<h2 id="Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers"><a href="#Experts-Weights-Averaging-A-New-General-Training-Scheme-for-Vision-Transformers" class="headerlink" title="Experts Weights Averaging: A New General Training Scheme for Vision Transformers"></a>Experts Weights Averaging: A New General Training Scheme for Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06093">http://arxiv.org/abs/2308.06093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqi Huang, Peng Ye, Xiaoshui Huang, Sheng Li, Tao Chen, Wanli Ouyang</li>
<li>for: 提高 ViT 模型的性能而不增加推理成本</li>
<li>methods: 使用 Mixture-of-Experts (MoE) 实现对 ViT 模型的训练，并在训练和推理阶段之间进行分解</li>
<li>results: 对多个 2D 和 3D 视觉任务、ViT 架构和数据集进行了全面的实验 validate 提议的训练方法的效果和普适性，同时还可以应用于细化 ViT 模型的 fine-tuning 过程中提高性能。<details>
<summary>Abstract</summary>
Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the ViT with specially designed, more efficient MoEs that assign tokens to experts by random uniform partition, and perform Experts Weights Averaging (EWA) on these MoEs at the end of each iteration. After training, we convert each MoE into an FFN by averaging the experts, transforming the model back into original ViT for inference. We further provide a theoretical analysis to show why and how it works. Comprehensive experiments across various 2D and 3D visual tasks, ViT architectures, and datasets validate the effectiveness and generalizability of the proposed training scheme. Besides, our training scheme can also be applied to improve performance when fine-tuning ViTs. Lastly, but equally important, the proposed EWA technique can significantly improve the effectiveness of naive MoE in various 2D visual small datasets and 3D visual tasks.
</details>
<details>
<summary>摘要</summary>
《Structural re-parameterization是一种通用训练方案 для Convolutional Neural Networks (CNNs),它可以提高性能而不增加推理成本。在Vision Transformers (ViTs)逐渐超越CNNs的视觉任务中，有人可能会提问：如果存在专门 дляViTs的训练方案，可以提高性能而不增加推理成本？Recently, Mixture-of-Experts (MoE)has attracted increasing attention,因为它可以高效地扩展Transformers的容量在固定成本下。考虑到MoE可以被视为多支分支结构，那么我们可以使用MoE来实现ViTs的训练方案类似于structural re-parameterization。在这篇论文中，我们答于这些问题，并提出了一种新的通用训练策略 дляViTs。 Specifically,我们在训练阶段将ViTs中的一些Feed-Forward Networks (FFNs)替换为特制的、更高效的MoEs，并在每个迭代结束后进行Experts Weights Averaging (EWA)。之后，我们将MoEs转换成FFNs，并将模型转换回原始的ViTs模型进行推理。我们还提供了一种理论分析，以证明这种训练方案的有效性和如何工作。我们在多种2D和3D视觉任务、ViT结构和数据集上进行了广泛的实验，证明了提议的训练策略的效果和通用性。此外，我们的训练策略还可以用于改进ViTs的性能when fine-tuning。最后，但也非常重要的是，我们提出的EWA技术可以在多种2D视觉小数据集和3D视觉任务中显著提高MoE的效果。》
</details></li>
</ul>
<hr>
<h2 id="Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering"><a href="#Toward-a-Better-Understanding-of-Loss-Functions-for-Collaborative-Filtering" class="headerlink" title="Toward a Better Understanding of Loss Functions for Collaborative Filtering"></a>Toward a Better Understanding of Loss Functions for Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06091">http://arxiv.org/abs/2308.06091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/psm1206/mawu">https://github.com/psm1206/mawu</a></li>
<li>paper_authors: Seongmin Park, Mincheol Yoon, Jae-woong Lee, Hogun Park, Jongwuk Lee</li>
<li>for: 这篇论文主要研究了相互推荐系统中的协同推荐技术，具体来说是分析现有的损失函数之间的关系，并提出了一种新的损失函数来改进现有的协同推荐模型。</li>
<li>methods: 该论文使用了数学分析来探究现有损失函数的关系，并在这基础上提出了一种新的损失函数called Margin-aware Alignment and Weighted Uniformity (MAWU)，它通过（i）margin-aware alignment（MA）和（ii）weighted uniformity（WU）来改进协同推荐模型的设计。</li>
<li>results: 实验结果表明，当 equiped with MAWU，MF和LightGCN相比现有的协同推荐模型，在三个公共数据集上具有相当或更高的性能。<details>
<summary>Abstract</summary>
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU is two-fold: (i) margin-aware alignment (MA) mitigates user/item-specific popularity biases, and (ii) weighted uniformity (WU) adjusts the significance between user and item uniformities to reflect the inherent characteristics of datasets. Extensive experimental results show that MF and LightGCN equipped with MAWU are comparable or superior to state-of-the-art CF models with various loss functions on three public datasets.
</details>
<details>
<summary>摘要</summary>
合作 filtering (CF) 是现代推荐系统中的关键技术。 CF 模型的学习过程通常包括三个组成部分：交互编码器、损失函数和负样本。虽然现有的研究已经提出了许多不同的 CF 模型，但是最近的研究表明，只是修改损失函数的设计可以获得显著性能提升。本文分析了现有损失函数之间的关系。我们的数学分析表明，前一些损失函数可以被解释为对用户和项目表示的对齐和分布均匀函数：（i）对用户和项目表示进行对齐（ii）对用户和项目分布进行均匀化。根据这一分析，我们提出了一种新的损失函数，称为 Margin-aware Alignment and Weighted Uniformity (MAWU)。MAWU 的关键创新有两个方面：（i）对用户/项目特有的流行偏好进行缓和（ii）根据数据集的特点进行加权均匀化。我们进行了广泛的实验研究，发现 MF 和 LightGCN 搭配 MAWU 与 state-of-the-art CF 模型相比，在三个公共数据集上具有相似或更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications"><a href="#Safeguarding-Learning-based-Control-for-Smart-Energy-Systems-with-Sampling-Specifications" class="headerlink" title="Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications"></a>Safeguarding Learning-based Control for Smart Energy Systems with Sampling Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06069">http://arxiv.org/abs/2308.06069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Venkatesh Prasad Venkataramanan, Pragya Kirti Gupta, Yun-Fei Hsu, Simon Burton</li>
<li>for: 这篇论文是关于使用强化学习控制能源系统中的挑战，特别是在保证性和安全性两个方面的。</li>
<li>methods: 论文详细介绍了在实时逻辑中强化学习安全要求的方法，包括将实时逻辑转换为线性逻辑（LTL），以便利用高级工程技术，如安全学习盾和正式验证。</li>
<li>results: 论文表明，通过将实时逻辑转换为LTL，可以在强化学习过程中提供更高的安全性保证，并且可以通过统计模型检查来获得更高的满意度。<details>
<summary>Abstract</summary>
We study challenges using reinforcement learning in controlling energy systems, where apart from performance requirements, one has additional safety requirements such as avoiding blackouts. We detail how these safety requirements in real-time temporal logic can be strengthened via discretization into linear temporal logic (LTL), such that the satisfaction of the LTL formulae implies the satisfaction of the original safety requirements. The discretization enables advanced engineering methods such as synthesizing shields for safe reinforcement learning as well as formal verification, where for statistical model checking, the probabilistic guarantee acquired by LTL model checking forms a lower bound for the satisfaction of the original real-time safety requirements.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:我们研究和开发用控制能源系统的强化学习，同时考虑性能要求和安全要求，如避免黑OUT。我们详细说明如何通过离散到线性时间逻辑（LTL）来加强安全要求，使得满足LTL公式的满足性意味着满足原始的安全要求。这种离散Enabled advanced工程技术，如生成安全屏障和正式验证，以及统计模型检查中的概率保证，这个保证是原始时间安全要求满足的下界。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-flow-disaggregation-for-hydropower-plant-management"><a href="#Deep-learning-based-flow-disaggregation-for-hydropower-plant-management" class="headerlink" title="Deep learning-based flow disaggregation for hydropower plant management"></a>Deep learning-based flow disaggregation for hydropower plant management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11631">http://arxiv.org/abs/2308.11631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Zhang</li>
<li>for:  Norwegian hydropower plant management</li>
<li>methods:  deep learning-based time series disaggregation model</li>
<li>results:  promising results for disaggregating daily flow into hourly flow<details>
<summary>Abstract</summary>
High temporal resolution data is a vital resource for hydropower plant management. Currently, only daily resolution data are available for most of Norwegian hydropower plant, however, to achieve more accurate management, sub-daily resolution data are often required. To deal with the wide absence of sub-daily data, time series disaggregation is a potential tool. In this study, we proposed a time series disaggregation model based on deep learning, the model is tested using flow data from a Norwegian flow station, to disaggregate the daily flow into hourly flow. Preliminary results show some promising aspects for the proposed model.
</details>
<details>
<summary>摘要</summary>
高时间分辨率数据是 Norway 水力发电厂的重要资源。目前，大多数 Norwegian 水力发电厂的数据只有每天的分辨率，但是为更准确的管理， often 需要更高的时间分辨率数据。为了解决宽泛的无法获得 sub-daily 数据的问题，时间序列分解是一种可能的工具。本研究提出了基于深度学习的时间序列分解模型，在使用挪威流站的流量数据进行测试，以分解每天的流量为每小时的流量。初步结果显示该模型具有一些有前途的特点。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction"><a href="#Adaptive-SGD-with-Polyak-stepsize-and-Line-search-Robust-Convergence-and-Variance-Reduction" class="headerlink" title="Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction"></a>Adaptive SGD with Polyak stepsize and Line-search: Robust Convergence and Variance Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06058">http://arxiv.org/abs/2308.06058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaowen Jiang, Sebastian U. Stich</li>
<li>for: 这个论文目的是提出两种新的随机波兰梯（AdaSPS和AdaSLS），以确保在非 interpolative 设定下进行训练，并且在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率。</li>
<li>methods: 这两种新算法使用了随机波兰梯和随机搜索，并且使用了一种新的减少偏差的技术来提高速度。</li>
<li>results: 这些新算法可以在非 interpolative 设定下进行训练，并且可以在对 convex 和强 convex 函数进行训练时维持下线性和线性的 converges 速率，并且可以和 AdaSVRG 的速率匹配，但是不需要内部外部循环结构。<details>
<summary>Abstract</summary>
The recently proposed stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for SGD have shown remarkable effectiveness when training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution which may result in a worse output than the initial guess. While artificially decreasing the adaptive stepsize has been proposed to address this issue (Orvieto et al. [2022]), this approach results in slower convergence rates for convex and over-parameterized models. In this work, we make two contributions: Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS requires no knowledge of problem-dependent parameters, and AdaSPS requires only a lower bound of the optimal function value as input. Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze. Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details>
<details>
<summary>摘要</summary>
Recently, the stochastic Polyak stepsize (SPS) and stochastic line-search (SLS) for stochastic gradient descent (SGD) have been proposed and have shown great effectiveness in training over-parameterized models. However, in non-interpolation settings, both algorithms only guarantee convergence to a neighborhood of a solution, which may result in a worse output than the initial guess. To address this issue, artificially decreasing the adaptive stepsize has been proposed (Orvieto et al., 2022), but this approach leads to slower convergence rates for convex and over-parameterized models.In this work, we make two contributions:Firstly, we propose two new variants of SPS and SLS, called AdaSPS and AdaSLS, which guarantee convergence in non-interpolation settings and maintain sub-linear and linear convergence rates for convex and strongly convex functions when training over-parameterized models. AdaSLS does not require knowledge of problem-dependent parameters, and AdaSPS only requires a lower bound of the optimal function value as input.Secondly, we equip AdaSPS and AdaSLS with a novel variance reduction technique, and obtain algorithms that require $\smash{\widetilde{\mathcal{O}}}(n+1/\epsilon)$ gradient evaluations to achieve an $\mathcal{O}(\epsilon)$-suboptimality for convex functions, which improves upon the slower $\mathcal{O}(1/\epsilon^2)$ rates of AdaSPS and AdaSLS without variance reduction in the non-interpolation regimes. Moreover, our result matches the fast rates of AdaSVRG but removes the inner-outer-loop structure, which is easier to implement and analyze.Finally, numerical experiments on synthetic and real datasets validate our theory and demonstrate the effectiveness and robustness of our algorithms.
</details></li>
</ul>
<hr>
<h2 id="Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro"><a href="#Cost-effective-On-device-Continual-Learning-over-Memory-Hierarchy-with-Miro" class="headerlink" title="Cost-effective On-device Continual Learning over Memory Hierarchy with Miro"></a>Cost-effective On-device Continual Learning over Memory Hierarchy with Miro</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06053">http://arxiv.org/abs/2308.06053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Ma, Suyeon Jeong, Minjia Zhang, Di Wang, Jonghyun Choi, Myeongjae Jeon</li>
<li>for: 本研究旨在实现Edge设备上的持续学习（Continual Learning，CL）系统，以提高数据隐私和能源效率。</li>
<li>methods: 本研究使用层次memory replay的CL方法，并开发了一个名为Miro的系统运行时，用于在Edge设备上动态配置CL系统以实现最佳成本效率。</li>
<li>results: 对baseline系统进行比较，Miro显示了显著的成本效率提升。<details>
<summary>Abstract</summary>
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperforms baseline systems we build for comparison, consistently achieving higher cost-effectiveness.
</details>
<details>
<summary>摘要</summary>
Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by dynamically configuring the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead.Extensive evaluations show that Miro significantly outperforms baseline systems we built for comparison, consistently achieving higher cost-effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Towards-Instance-adaptive-Inference-for-Federated-Learning"><a href="#Towards-Instance-adaptive-Inference-for-Federated-Learning" class="headerlink" title="Towards Instance-adaptive Inference for Federated Learning"></a>Towards Instance-adaptive Inference for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06051">http://arxiv.org/abs/2308.06051</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chunmeifeng/fedins">https://github.com/chunmeifeng/fedins</a></li>
<li>paper_authors: Chun-Mei Feng, Kai Yu, Nian Liu, Xinxing Xu, Salman Khan, Wangmeng Zuo</li>
<li>for: 这个论文的目的是提出一种基于联合学习（Federated Learning，FL）框架的实例适应性推理方法，以提高FL在复杂实际数据上的性能。</li>
<li>methods: 这个论文使用了一种基于缩放和偏移（scale and shift）的深度特征方法（SSF），以及一种客户端启发式推理方法（instance-adaptive inference），以适应实际数据上的实例差异性。</li>
<li>results: 对于Tiny-ImageNet dataset，这个方法比顶峰性能的方法提高6.64%，而且通信成本低于15%。<details>
<summary>Abstract</summary>
Federated learning (FL) is a distributed learning paradigm that enables multiple clients to learn a powerful global model by aggregating local training. However, the performance of the global model is often hampered by non-i.i.d. distribution among the clients, requiring extensive efforts to mitigate inter-client data heterogeneity. Going beyond inter-client data heterogeneity, we note that intra-client heterogeneity can also be observed on complex real-world data and seriously deteriorate FL performance. In this paper, we present a novel FL algorithm, i.e., FedIns, to handle intra-client data heterogeneity by enabling instance-adaptive inference in the FL framework. Instead of huge instance-adaptive models, we resort to a parameter-efficient fine-tuning method, i.e., scale and shift deep features (SSF), upon a pre-trained model. Specifically, we first train an SSF pool for each client, and aggregate these SSF pools on the server side, thus still maintaining a low communication cost. To enable instance-adaptive inference, for a given instance, we dynamically find the best-matched SSF subsets from the pool and aggregate them to generate an adaptive SSF specified for the instance, thereby reducing the intra-client as well as the inter-client heterogeneity. Extensive experiments show that our FedIns outperforms state-of-the-art FL algorithms, e.g., a 6.64\% improvement against the top-performing method with less than 15\% communication cost on Tiny-ImageNet. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种分布式学习 paradigm，允许多个客户端学习一个强大的全球模型，通过Client中的本地训练数据进行汇总。然而，全球模型的性能经常受到客户端数据之间的非同质化的影响，需要广泛的减少客户端数据之间的不同性。此外，我们注意到了复杂的实际数据中的内部客户端数据不同性，也会严重降低 FL 性能。在这篇论文中，我们提出了一种新的 FL 算法，即 FedIns，以处理内部客户端数据不同性。我们在 FL 框架中实现了实例适应的推理，而不需要巨大的实例适应模型。我们首先在每个客户端上训练一个可缩放和调整的深度特征池（SSF），并在服务器端将这些 SSF 池进行汇总，以保持低的通信成本。为实现实例适应推理，对于一个给定的实例，我们在实例级别 dynamically 找到最佳适应的 SSF 子集，并将这些子集进行汇总，以生成适应该实例的 adaptive SSF。这有助于降低内部客户端数据不同性以及客户端数据之间的不同性。我们的 FedIns 在 Tiny-ImageNet 上比顶尖方法提供了6.64%的提升，并且与之前的最好方法在 less than 15% 的通信成本下。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors"><a href="#AI-Assisted-Investigation-of-On-Chain-Parameters-Risky-Cryptocurrencies-and-Price-Factors" class="headerlink" title="AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors"></a>AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies and Price Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08554">http://arxiv.org/abs/2308.08554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdulrezzak Zekiye, Semih Utku, Fadi Amroush, Oznur Ozkasap<br>for:This paper aims to analyze historical data and use artificial intelligence algorithms to identify the factors affecting a cryptocurrency’s price and to find risky cryptocurrencies.methods:The paper uses on-chain parameters to analyze historical cryptocurrency data and employs clustering and classification techniques to group cryptocurrencies based on their on-chain characteristics. The paper also uses multiple classifiers to predict whether a cryptocurrency is risky or not.results:The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. The paper also found a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Additionally, the paper clustered cryptocurrencies into five distinct groups based on their on-chain parameters, and obtained the best f1-score of 76% using K-Nearest Neighbor for predicting risky cryptocurrencies.<details>
<summary>Abstract</summary>
Cryptocurrencies have become a popular and widely researched topic of interest in recent years for investors and scholars. In order to make informed investment decisions, it is essential to comprehend the factors that impact cryptocurrency prices and to identify risky cryptocurrencies. This paper focuses on analyzing historical data and using artificial intelligence algorithms on on-chain parameters to identify the factors affecting a cryptocurrency's price and to find risky cryptocurrencies. We conducted an analysis of historical cryptocurrencies' on-chain data and measured the correlation between the price and other parameters. In addition, we used clustering and classification in order to get a better understanding of a cryptocurrency and classify it as risky or not. The analysis revealed that a significant proportion of cryptocurrencies (39%) disappeared from the market, while only a small fraction (10%) survived for more than 1000 days. Our analysis revealed a significant negative correlation between cryptocurrency price and maximum and total supply, as well as a weak positive correlation between price and 24-hour trading volume. Moreover, we clustered cryptocurrencies into five distinct groups using their on-chain parameters, which provides investors with a more comprehensive understanding of a cryptocurrency when compared to those clustered with it. Finally, by implementing multiple classifiers to predict whether a cryptocurrency is risky or not, we obtained the best f1-score of 76% using K-Nearest Neighbor.
</details>
<details>
<summary>摘要</summary>
digital currencies 在最近几年内已经成为投资者和学者关注的热点话题。为了做出 Informed 投资决策，需要了解 криптовалюencies 价格的影响因素并确定风险较高的 криптовалюencies。这篇论文通过分析历史数据和使用人工智能算法对 chain 参数进行分析，以确定 криптовалюencies 价格的影响因素和风险评估。我们对历史 криптовалюencies 的 chain 数据进行分析，并测量价格和其他参数之间的相关性。此外，我们还使用聚类和分类来更好地理解 криптовалюencies，并将其分为五个不同类别。最后，我们通过应用多种分类器来预测 криптовалюencies 是否为风险的，并获得了最佳的 f1 分数为 76%。
</details></li>
</ul>
<hr>
<h2 id="Controlling-Character-Motions-without-Observable-Driving-Source"><a href="#Controlling-Character-Motions-without-Observable-Driving-Source" class="headerlink" title="Controlling Character Motions without Observable Driving Source"></a>Controlling Character Motions without Observable Driving Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06025">http://arxiv.org/abs/2308.06025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyuan Li, Bin Dai, Ziyi Zhou, Qi Yao, Baoyuan Wang</li>
<li>for: 生成无驱动源的多样化、自然和无限长的头部&#x2F;身体序列</li>
<li>methods: 提议一个系统性框架，结合VQ-VAE和一种新的токен级控制策略，使用返回学习算法和经过设计的奖励函数来生成无限长的多样化和自然的头部&#x2F;身体序列</li>
<li>results: 通过全面的评估，发现提议的框架可以解决无驱动源生成中的各种挑战，并与其他强基线相比表现出众。<details>
<summary>Abstract</summary>
How to generate diverse, life-like, and unlimited long head/body sequences without any driving source? We argue that this under-investigated research problem is non-trivial at all, and has unique technical challenges behind it. Without semantic constraints from the driving sources, using the standard autoregressive model to generate infinitely long sequences would easily result in 1) out-of-distribution (OOD) issue due to the accumulated error, 2) insufficient diversity to produce natural and life-like motion sequences and 3) undesired periodic patterns along the time. To tackle the above challenges, we propose a systematic framework that marries the benefits of VQ-VAE and a novel token-level control policy trained with reinforcement learning using carefully designed reward functions. A high-level prior model can be easily injected on top to generate unlimited long and diverse sequences. Although we focus on no driving sources now, our framework can be generalized for controlled synthesis with explicit driving sources. Through comprehensive evaluations, we conclude that our proposed framework can address all the above-mentioned challenges and outperform other strong baselines very significantly.
</details>
<details>
<summary>摘要</summary>
如何生成无驱动源的多样化、生命般自然的头部/身体序列？我们认为这是一个未受抨拿的研究问题，具有独特的技术挑战。不受 semantics 驱动源的限制，使用标准的自然语言模型来生成无限长序列会导致1) OOD 问题 Due to the accumulated error, 2) 不够多样性来生成自然和生命般的动作序列和 3) 不想要的时间 periodic patterns.为了解决以上挑战，我们提议一个系统性的框架，该框架结合 VQ-VAE 的优点和一种基于 reinforcement learning 的新的 токен级控制策略。高级 prior model 可以轻松地注入到该框架中，以生成无限长和多样化的序列。虽然我们现在没有驱动源，但我们的框架可以通过 Carefully designed reward functions 来扩展到控制的 synthesis 中Explicit driving sources。通过全面的评估，我们结议了我们提议的框架可以解决所有以上挑战，并与其他强大的基准模型相比，表现非常出色。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment"><a href="#Evaluating-Picture-Description-Speech-for-Dementia-Detection-using-Image-text-Alignment" class="headerlink" title="Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment"></a>Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07933">http://arxiv.org/abs/2308.07933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youxiang Zhu, Nana Lin, Xiaohui Liang, John A. Batsis, Robert M. Roth, Brian MacWhinney</li>
<li>for: 本研究旨在提高诊断老人痴呆症的精度，通过利用图像描述文本对应关系来提高检测精度。</li>
<li>methods: 本研究提出了首个将图像和描述文本作为输入，并利用大规模预训练图像文本对应模型的知识来进行诊断的模型。我们发现了健康和痴呆样本之间的文本与图像之间的差异，并使用文本与图像之间的相关性来排序和筛选样本。此外，我们还将图像分解成不同主题，并将文本分类为每个主题中的不同话题。</li>
<li>results: 我们的三种进阶模型，通过对样本进行预处理，使用图像与文本之间的相关性和图像分解、文本分类等技术，实现了诊断精度的提高。我们的最佳模型在83.44%的检测精度上得到了状元表现，高于文本只基线模型的79.91%。此外，我们还可视化样本和图像结果，以便解释我们的模型的优势。<details>
<summary>Abstract</summary>
Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-processed the samples based on their relevance to the picture, sub-image, and focused areas. The evaluation results show that our advanced models, with knowledge of the picture and large image-text alignment models, achieve state-of-the-art performance with the best detection accuracy at 83.44%, which is higher than the text-only baseline model at 79.91%. Lastly, we visualize the sample and picture results to explain the advantages of our models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry"><a href="#Large-Language-Models-for-Telecom-Forthcoming-Impact-on-the-Industry" class="headerlink" title="Large Language Models for Telecom: Forthcoming Impact on the Industry"></a>Large Language Models for Telecom: Forthcoming Impact on the Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06013">http://arxiv.org/abs/2308.06013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Maatouk, Nicola Piovesan, Fadhel Ayed, Antonio De Domenico, Merouane Debbah</li>
<li>for: 本研究旨在探讨LLM技术在电信领域的应用和影响，以及如何在这些领域中充分利用LLM的潜力。</li>
<li>methods: 本研究采用了LLM技术的内部结构和应用场景的分析，以及在电信领域中可以立即实施的用例的探讨。</li>
<li>results: 研究发现了LLM技术在电信领域的现有能力和局限性，以及需要进一步研究的领域和挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as a transformative force, revolutionizing numerous fields well beyond the conventional domain of Natural Language Processing (NLP) and garnering unprecedented attention. As LLM technology continues to progress, the telecom industry is facing the prospect of its potential impact on its landscape. To elucidate these implications, we delve into the inner workings of LLMs, providing insights into their current capabilities and limitations. We also examine the use cases that can be readily implemented in the telecom industry, streamlining numerous tasks that currently hinder operational efficiency and demand significant manpower and engineering expertise. Furthermore, we uncover essential research directions that deal with the distinctive challenges of utilizing the LLMs within the telecom domain. Addressing these challenges represents a significant stride towards fully harnessing the potential of LLMs and unlocking their capabilities to the fullest extent within the telecom domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields"><a href="#Does-AI-for-science-need-another-ImageNet-Or-totally-different-benchmarks-A-case-study-of-machine-learning-force-fields" class="headerlink" title="Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields"></a>Does AI for science need another ImageNet Or totally different benchmarks? A case study of machine learning force fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05999">http://arxiv.org/abs/2308.05999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yatao Li, Wanling Gao, Lei Wang, Lixin Sun, Zun Wang, Jianfeng Zhan</li>
<li>for: 这 paper 的目的是探讨 AI for science 领域中的模型性能评估方法，以便更好地适应科学计算任务中的特殊挑战。</li>
<li>methods: 该 paper 使用 machine learning force field (MLFF) 作为一个案例研究，检查了现有的 AI benchmarking 方法是否能够有效地评估 AI for science 模型的性能。它还提出了一些解决方案来评估 MLFF 模型，包括样本效率、时间域敏感性和交叉数据集泛化能力等方面。</li>
<li>results: 该 paper 通过设置问题实例类似于实际科学应用，提出了一些更加科学意义的性能指标，以评估 AI for science 模型的性能。这些指标在实际应用中表现出更高的泛化能力和更好的时间域敏感性，与传统的 AI 评估方法相比。<details>
<summary>Abstract</summary>
AI for science (AI4S) is an emerging research field that aims to enhance the accuracy and speed of scientific computing tasks using machine learning methods. Traditional AI benchmarking methods struggle to adapt to the unique challenges posed by AI4S because they assume data in training, testing, and future real-world queries are independent and identically distributed, while AI4S workloads anticipate out-of-distribution problem instances. This paper investigates the need for a novel approach to effectively benchmark AI for science, using the machine learning force field (MLFF) as a case study. MLFF is a method to accelerate molecular dynamics (MD) simulation with low computational cost and high accuracy. We identify various missed opportunities in scientifically meaningful benchmarking and propose solutions to evaluate MLFF models, specifically in the aspects of sample efficiency, time domain sensitivity, and cross-dataset generalization capabilities. By setting up the problem instantiation similar to the actual scientific applications, more meaningful performance metrics from the benchmark can be achieved. This suite of metrics has demonstrated a better ability to assess a model's performance in real-world scientific applications, in contrast to traditional AI benchmarking methodologies. This work is a component of the SAIBench project, an AI4S benchmarking suite. The project homepage is https://www.computercouncil.org/SAIBench.
</details>
<details>
<summary>摘要</summary>
人工智能 для科学（AI4S）是一个emerging研究领域，旨在使用机器学习方法提高科学计算任务的准确率和速度。传统的AI测试方法困难适应AI4S的特殊挑战，因为它们假设训练、测试和未来实际世界中的数据都是独立并且相同分布的，而AI4S工作负荷预期的问题实例将出现在不同的分布上。这篇论文研究了AI4S测试方法的需要，使用机器学习力场（MLFF）作为一个案例研究。MLFF是一种加速分子动力学（MD）仿真的方法，可以减少计算成本并保持高度准确。我们认为存在多种科学上有意义的测试机会被遗弃，并提出了一些解决方案来评估MLFF模型，包括样本效率、时间域敏感和cross-dataset泛化能力。通过设置问题实例类似于实际科学应用，可以更 meaningful的性能指标从测试中获得。这组指标已经表明可以更好地评估模型在实际科学应用中的性能，与传统的AI测试方法不同。这是SAIBench项目的一部分，SAIBench是一个AI4S测试集。项目主页在https://www.computercouncil.org/SAIBench。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network"><a href="#Automatic-Classification-of-Blood-Cell-Images-Using-Convolutional-Neural-Network" class="headerlink" title="Automatic Classification of Blood Cell Images Using Convolutional Neural Network"></a>Automatic Classification of Blood Cell Images Using Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06300">http://arxiv.org/abs/2308.06300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Abeera Mahfooz</li>
<li>For: The paper aims to automatically classify ten types of blood cells with increased accuracy using a convolutional neural network (CNN) model.* Methods: The authors use transfer learning with pre-trained CNN models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20, on the PBC dataset’s normal DIB. They also propose a novel CNN-based framework to improve accuracy.* Results: The authors achieve an accuracy of 99.91% on the PBC dataset with their proposed CNN model, outperforming earlier results reported in the literature.<details>
<summary>Abstract</summary>
Human blood primarily comprises plasma, red blood cells, white blood cells, and platelets. It plays a vital role in transporting nutrients to different organs, where it stores essential health-related data about the human body. Blood cells are utilized to defend the body against diverse infections, including fungi, viruses, and bacteria. Hence, blood analysis can help physicians assess an individual's physiological condition. Blood cells have been sub-classified into eight groups: Neutrophils, eosinophils, basophils, lymphocytes, monocytes, immature granulocytes (promyelocytes, myelocytes, and metamyelocytes), erythroblasts, and platelets or thrombocytes on the basis of their nucleus, shape, and cytoplasm. Traditionally, pathologists and hematologists in laboratories have examined these blood cells using a microscope before manually classifying them. The manual approach is slower and more prone to human error. Therefore, it is essential to automate this process. In our paper, transfer learning with CNN pre-trained models. VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3, MobileNetV2, and DenseNet-20 applied to the PBC dataset's normal DIB. The overall accuracy achieved with these models lies between 91.375 and 94.72%. Hence, inspired by these pre-trained architectures, a model has been proposed to automatically classify the ten types of blood cells with increased accuracy. A novel CNN-based framework has been presented to improve accuracy. The proposed CNN model has been tested on the PBC dataset normal DIB. The outcomes of the experiments demonstrate that our CNN-based framework designed for blood cell classification attains an accuracy of 99.91% on the PBC dataset. Our proposed convolutional neural network model performs competitively when compared to earlier results reported in the literature.
</details>
<details>
<summary>摘要</summary>
人体血液主要由血液溶解、红细胞、白细胞和板块组成。它扮演着将营养物质传递到不同器官的重要角色，同时也存储了人体重要的生物学信息。血液细胞可以用于防御体内各种感染，包括病毒、真菌和细菌。因此，血液分析可以帮助医生评估个体的生理状况。血液细胞被分为八种类型：neutrophils、eosinophils、basophils、lymphocytes、monocytes、immature granulocytes（promyelocytes、myelocytes和metamyelocytes）、erythroblasts和板块或血液板块。传统上，pathologists和hematologists在实验室中使用显微镜进行血液细胞的识别，这是一个慢速且容易出错的手动过程。因此，自动化这个过程是非常重要。在我们的论文中，我们采用了转移学习与CNN预训练模型。VGG16、VGG19、ResNet-50、ResNet-101、ResNet-152、InceptionV3、MobileNetV2和DenseNet-20在PBC数据集的正常DIB上应用了CNN预训练模型。这些模型的总准确率在91.375%到94.72%之间。因此，我们被这些预训练模型所 inspirited，并提出了一种自动化血液细胞类型分类的模型。我们提出了一种基于CNN的框架来提高准确率。我们的提议的CNN模型在PBC数据集的正常DIB上进行测试，实验结果表明，我们的CNN模型在血液细胞类型分类方面实现了99.91%的准确率。我们的提议的 convolutional neural network模型与文献中已经报道的结果相比，表现竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance"><a href="#Fast-and-Accurate-Transferability-Measurement-by-Evaluating-Intra-class-Feature-Variance" class="headerlink" title="Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance"></a>Fast and Accurate Transferability Measurement by Evaluating Intra-class Feature Variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05986">http://arxiv.org/abs/2308.05986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snudatalab/TMI">https://github.com/snudatalab/TMI</a></li>
<li>paper_authors: Huiwen Xu, U Kang</li>
<li>for: 这个论文的目的是如何快速和准确地找到下游任务中最有用的预训练模型。</li>
<li>methods: 这个论文提出了一种名为TMI（转移性评估器）的算法，用于评估预训练模型的转移性。TMI视转移性为预训练模型在目标任务上的总体化，并通过评估模型内类差异来评估模型的适应性。</li>
<li>results: 对于多个实际数据集，TMI表现出了较好的选择性，可以快速和准确地选择预训练模型。与之前的研究相比，TMI在13个案例中展现出了更高的相关性。<details>
<summary>Abstract</summary>
Given a set of pre-trained models, how can we quickly and accurately find the most useful pre-trained model for a downstream task? Transferability measurement is to quantify how transferable is a pre-trained model learned on a source task to a target task. It is used for quickly ranking pre-trained models for a given task and thus becomes a crucial step for transfer learning. Existing methods measure transferability as the discrimination ability of a source model for a target data before transfer learning, which cannot accurately estimate the fine-tuning performance. Some of them restrict the application of transferability measurement in selecting the best supervised pre-trained models that have classifiers. It is important to have a general method for measuring transferability that can be applied in a variety of situations, such as selecting the best self-supervised pre-trained models that do not have classifiers, and selecting the best transferring layer for a target task. In this work, we propose TMI (TRANSFERABILITY MEASUREMENT WITH INTRA-CLASS FEATURE VARIANCE), a fast and accurate algorithm to measure transferability. We view transferability as the generalization of a pre-trained model on a target task by measuring intra-class feature variance. Intra-class variance evaluates the adaptability of the model to a new task, which measures how transferable the model is. Compared to previous studies that estimate how discriminative the models are, intra-class variance is more accurate than those as it does not require an optimal feature extractor and classifier. Extensive experiments on real-world datasets show that TMI outperforms competitors for selecting the top-5 best models, and exhibits consistently better correlation in 13 out of 17 cases.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:给定一个集合先进模型，如何快速和准确地找到下游任务中最有用的先进模型？转移可量度是用于衡量先进模型在源任务上学习后，转移到目标任务上的抽象能力。现有的方法通常是通过计算源模型对目标数据的分类能力来衡量转移可量度，这不能准确地估计微调性能。一些方法还限制了转移可量度的测量在选择最佳监督式先进模型中应用。因此，有一个通用的方法可以在多种情况下测量转移可量度，如选择最佳无监督式先进模型和选择最佳转移层。在这项工作中，我们提出了TMI（转移可量度测量与内类特征异常）算法，它是一种快速和准确的转移可量度测量方法。我们视转移可量度为将先进模型在目标任务上通过测量内类特征异常来衡量。内类异常评估模型在新任务上适应度，这也衡量了模型的转移可量度。与之前的研究所计算的模型掌握性相比，内类异常更准确，因为它不需要优化特征提取器和分类器。我们在实际世界数据集上进行了广泛的实验，显示TMI在选择top-5最佳模型时高效，并在13个 случа中展现了更高的相关性。
</details></li>
</ul>
<hr>
<h2 id="Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment"><a href="#Defensive-Perception-Estimation-and-Monitoring-of-Neural-Network-Performance-under-Deployment" class="headerlink" title="Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment"></a>Defensive Perception: Estimation and Monitoring of Neural Network Performance under Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06299">http://arxiv.org/abs/2308.06299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Vogt, Stefan Buehler, Mark Schutera</li>
<li>for: addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving</li>
<li>methods: encapsulating the neural network under deployment within an uncertainty estimation envelope based on Monte Carlo Dropout, without modifying the deployed neural network</li>
<li>results: demonstrating the applicability of the method for multiple different potential deployment shifts relevant to autonomous driving, including transitions into the night, rainy, or snowy domain, and enabling operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.Here’s the same information in Simplified Chinese:</li>
<li>for: 解决神经网络 semantic segmentation 自动驾驶中的不可预测性和域shift问题</li>
<li>methods: 基于 Monte Carlo Dropout 的 epistemic uncertainty 估计方法，不需要修改部署 neural network</li>
<li>results: 对各种自动驾驶中可能的部署变化进行了演示，包括夜晚、雨天和雪天等域shift，并实现了运行设计域认知via uncertainty，允许DEFENSIVE PERCEPTION、安全状态触发、警告通知和测试或开发和适应性改进的感知栈反馈。<details>
<summary>Abstract</summary>
In this paper, we propose a method for addressing the issue of unnoticed catastrophic deployment and domain shift in neural networks for semantic segmentation in autonomous driving. Our approach is based on the idea that deep learning-based perception for autonomous driving is uncertain and best represented as a probability distribution. As autonomous vehicles' safety is paramount, it is crucial for perception systems to recognize when the vehicle is leaving its operational design domain, anticipate hazardous uncertainty, and reduce the performance of the perception system. To address this, we propose to encapsulate the neural network under deployment within an uncertainty estimation envelope that is based on the epistemic uncertainty estimation through the Monte Carlo Dropout approach. This approach does not require modification of the deployed neural network and guarantees expected model performance. Our defensive perception envelope has the capability to estimate a neural network's performance, enabling monitoring and notification of entering domains of reduced neural network performance under deployment. Furthermore, our envelope is extended by novel methods to improve the application in deployment settings, including reducing compute expenses and confining estimation noise. Finally, we demonstrate the applicability of our method for multiple different potential deployment shifts relevant to autonomous driving, such as transitions into the night, rainy, or snowy domain. Overall, our approach shows great potential for application in deployment settings and enables operational design domain recognition via uncertainty, which allows for defensive perception, safe state triggers, warning notifications, and feedback for testing or development and adaptation of the perception stack.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法来解决自适应驾驶 neural network 中的不注意性投入和领域转换问题。我们的方法基于深度学习基于自适应驾驶的感知系统是不确定的，最好表示为一个概率分布。自驾驶车辆的安全性 Paramount，因此感知系统必须能够识别车辆离开操作设计领域，预测危险不确定性，并降低感知系统的性能。为此，我们提议将投入 neural network 内部的深度学习模型包装在一个不确定性估计膜中，该膜基于 Monte Carlo Dropout 方法来估计模型的 epistemic 不确定性。这种方法不需要修改已经部署的 neural network，并且保证模型的预期性能。我们的防御感知膜可以估计 neural network 的性能，并且可以监测和通知车辆进入性能下降的领域。此外，我们还提出了一些新的方法来改进在部署Setting中的应用，包括减少计算成本和限制估计噪声。最后，我们示出了我们方法在多种不同的部署转换中的应用可能性，例如在夜晚、雨天或雪天等领域。总之，我们的方法在部署Setting中表现出了很好的应用潜力，并允许操作设计领域的认知，以及发出警告通知、测试或开发和适应感知堆。
</details></li>
</ul>
<hr>
<h2 id="An-Encoder-Decoder-Approach-for-Packing-Circles"><a href="#An-Encoder-Decoder-Approach-for-Packing-Circles" class="headerlink" title="An Encoder-Decoder Approach for Packing Circles"></a>An Encoder-Decoder Approach for Packing Circles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07335">http://arxiv.org/abs/2308.07335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshay Kiran Jose, Gangadhar Karevvanavar, Rajshekhar V Bhat</li>
<li>for: 本文关于如何封装小对象在大对象中，以实现不 overlap 和 minimum overlap 的目标。</li>
<li>methods: 本文提出了一种基于encoder-decoder架构的方法，包括encoder块、perturbation块和decoder块。encoder块通过normalization层输出中心点，perturbation块添加控制的偏移，确保中心点不超过小对象的半径，decoder块使用偏移中心点来估计 intend circle index。</li>
<li>results: 该方法可以 Parametrize encoder和decoder使用神经网络，并通过优化减少 decoder 估计的误差和实际输入 encoder 中心点的差异，从而实现不 overlap 和 minimum overlap 的目标。该方法可以对高维度和不同形状的对象进行扩展。<details>
<summary>Abstract</summary>
The problem of packing smaller objects within a larger object has been of interest since decades. In these problems, in addition to the requirement that the smaller objects must lie completely inside the larger objects, they are expected to not overlap or have minimum overlap with each other. Due to this, the problem of packing turns out to be a non-convex problem, obtaining whose optimal solution is challenging. As such, several heuristic approaches have been used for obtaining sub-optimal solutions in general, and provably optimal solutions for some special instances. In this paper, we propose a novel encoder-decoder architecture consisting of an encoder block, a perturbation block and a decoder block, for packing identical circles within a larger circle. In our approach, the encoder takes the index of a circle to be packed as an input and outputs its center through a normalization layer, the perturbation layer adds controlled perturbations to the center, ensuring that it does not deviate beyond the radius of the smaller circle to be packed, and the decoder takes the perturbed center as input and estimates the index of the intended circle for packing. We parameterize the encoder and decoder by a neural network and optimize it to reduce an error between the decoder's estimated index and the actual index of the circle provided as input to the encoder. The proposed approach can be generalized to pack objects of higher dimensions and different shapes by carefully choosing normalization and perturbation layers. The approach gives a sub-optimal solution and is able to pack smaller objects within a larger object with competitive performance with respect to classical methods.
</details>
<details>
<summary>摘要</summary>
这个问题已经引起关注了几十年。在这些问题中，除了要求小对象完全 locate 在大对象中之外，还要求小对象之间不会 overlap 或者最小化 overlap。由于这个原因，packing 问题变成了非凸问题，获得优化解决方案是困难的。为此，许多启发性方法被用来获得不优化解决方案，以及对特殊情况下的可证优化解决方案。在这篇论文中，我们提出了一种新的编码器-解码器架构，包括编码器块、抖动块和解码器块，用于将同形圆包含在大圆中。在我们的方法中，编码器接受圆的索引作为输入，并通过正规化层输出圆心，抖动层添加控制的偏移，使圆心不会超过小圆的半径，而解码器接受偏移后的圆心作为输入，并估算圆的索引。我们使用神经网络参数化编码器和解码器，并优化它们以降低由解码器估算的圆索引与实际输入圆索引之间的错误。我们的方法可以通过选择正规化和抖动层来扩展到包含高维度和不同形状的对象。该方法可以提供竞争性的非优化解决方案，并将小对象包含在大对象中。
</details></li>
</ul>
<hr>
<h2 id="Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC"><a href="#Learning-nonparametric-DAGs-with-incremental-information-via-high-order-HSIC" class="headerlink" title="Learning nonparametric DAGs with incremental information via high-order HSIC"></a>Learning nonparametric DAGs with incremental information via high-order HSIC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05969">http://arxiv.org/abs/2308.05969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Jianguo Liu</li>
<li>for: 本文 targets at learning Bayesian networks (BN) by maximizing global score functions, but it addresses the issue of local variables having direct and indirect dependence simultaneously, which can lead to missed edges in the global optimization.</li>
<li>methods: 本文提出了一个可 identificability condition based on a determined subset of parents，并开发了一个两阶段算法（OT algorithm）来解决本问题。在第一阶段，使用 first-order Hilbert-Schmidt independence criterion (HSIC) 得到一个初始确定的父集。在第二阶段，使用 theoretically proved incremental properties of high-order HSIC 进行了本地调整。</li>
<li>results:  numrical experiments on different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods, especially in Sigmoid Mix model with the size of the graph being d&#x3D;40, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, indicating that the graph estimated by the OT algorithm misses fewer edges compared with CAM.<details>
<summary>Abstract</summary>
Score-based methods for learning Bayesain networks(BN) aim to maximizing the global score functions. However, if local variables have direct and indirect dependence simultaneously, the global optimization on score functions misses edges between variables with indirect dependent relationship, of which scores are smaller than those with direct dependent relationship. In this paper, we present an identifiability condition based on a determined subset of parents to identify the underlying DAG. By the identifiability condition, we develop a two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the global optimization. In the optimal phase, an optimization problem based on first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated skeleton as the initial determined parents subset. In the tuning phase, the skeleton is locally tuned by deletion, addition and DAG-formalization strategies using the theoretically proved incremental properties of high-order HSIC. Numerical experiments for different synthetic datasets and real-world datasets show that the OT algorithm outperforms existing methods. Especially in Sigmoid Mix model with the size of the graph being ${\rm\bf d=40}$, the structure intervention distance (SID) of the OT algorithm is 329.7 smaller than the one obtained by CAM, which indicates that the graph estimated by the OT algorithm misses fewer edges compared with CAM.
</details>
<details>
<summary>摘要</summary>
Score-based方法学习 bayesian网络（BN）目的是最大化全局分数函数。然而，如果本地变量同时具有直接和间接依赖关系，全局优化分数函数会忽略变量之间的间接依赖关系中的边，其分数较直接依赖关系中的边小。在这篇论文中，我们提出了一个可识别条件，基于确定的父集来识别下面的DAG。通过可识别条件，我们开发了一个两阶段算法，称为最优调整（OT）算法。在优化阶段，基于第一阶段希尔伯特- Schmidt独立性标准（HSIC）的优化问题提供了一个初始确定父集的skeleton。在调整阶段，skeleton通过删除、添加和DAG-形式化策略进行了本地调整，使用了理论上证明的高阶HSIC的增量性质。numerical experiments表明，OT算法在不同的synthetic数据集和实际数据集上的性能都高于现有方法。特别是在sigmoid mix模型中，OT算法的结构间断距（SID）为329.7，与CAM所获得的结构间断距相比，表示OT算法估算的图 missed fewer edges。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review"><a href="#Classification-of-White-Blood-Cells-Using-Machine-and-Deep-Learning-Models-A-Systematic-Review" class="headerlink" title="Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review"></a>Classification of White Blood Cells Using Machine and Deep Learning Models: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06296">http://arxiv.org/abs/2308.06296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Asghar, Sanjay Kumar, Paul Hynds, Arslan Shaukat</li>
<li>for: 这篇论文的目的是对医疗影像分析中的白血球分类进行深入分析，并评估现代技术在这个领域的应用。</li>
<li>methods: 这篇论文使用了许多现代技术，包括机器学习（ML）和深度学习（DL），以提高医疗影像分析的准确性和分类精度。</li>
<li>results: 这篇论文发现，过去的17年间，医疗影像分析中的白血球分类方法有所进步，并且使用了许多不同的技术和数据来进行分析。但是，还有一些挑战需要解决，例如获得适当的数据集和增强医疗培训。<details>
<summary>Abstract</summary>
Machine learning (ML) and deep learning (DL) models have been employed to significantly improve analyses of medical imagery, with these approaches used to enhance the accuracy of prediction and classification. Model predictions and classifications assist diagnoses of various cancers and tumors. This review presents an in-depth analysis of modern techniques applied within the domain of medical image analysis for white blood cell classification. The methodologies that use blood smear images, magnetic resonance imaging (MRI), X-rays, and similar medical imaging domains are identified and discussed, with a detailed analysis of ML/DL techniques applied to the classification of white blood cells (WBCs) representing the primary focus of the review. The data utilized in this research has been extracted from a collection of 136 primary papers that were published between the years 2006 and 2023. The most widely used techniques and best-performing white blood cell classification methods are identified. While the use of ML and DL for white blood cell classification has concurrently increased and improved in recent year, significant challenges remain - 1) Availability of appropriate datasets remain the primary challenge, and may be resolved using data augmentation techniques. 2) Medical training of researchers is recommended to improve current understanding of white blood cell structure and subsequent selection of appropriate classification models. 3) Advanced DL networks including Generative Adversarial Networks, R-CNN, Fast R-CNN, and faster R-CNN will likely be increasingly employed to supplement or replace current techniques.
</details>
<details>
<summary>摘要</summary>
医学影像分析（ML）和深度学习（DL）模型已经被应用到医疗影像分析中，以提高预测和分类的准确性。这些方法可以帮助诊断多种恶性肿瘤和癌症。本文总结了现代医学影像分析领域中使用ML/DL技术进行白血球类型分类的方法。这些方法包括血液滴血图像、核磁共振成像（MRI）、X射线成像等医学影像领域，并进行了详细的ML/DL技术应用于白血球类型分类的分析。研究使用的数据来自于2006年至2023年发表的136篇原始论文。最常用的技术和最佳白血球类型分类方法被识别出来。虽然在过去几年内，用ML和DL进行白血球类型分类的使用和提高在不断增长，但还存在一些挑战，包括：1）获得适当数据集的可用性问题，可以通过数据扩展技术解决。2）医学研究人员的培训，以提高白血球结构的理解，并选择合适的分类模型。3）将来，高级的深度学习网络，如生成对抗网络、R-CNN、快速R-CNN和更快的R-CNN将被广泛应用，以补充或取代当前的方法。
</details></li>
</ul>
<hr>
<h2 id="Learned-Point-Cloud-Compression-for-Classification"><a href="#Learned-Point-Cloud-Compression-for-Classification" class="headerlink" title="Learned Point Cloud Compression for Classification"></a>Learned Point Cloud Compression for Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05959">http://arxiv.org/abs/2308.05959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification">https://github.com/multimedialabsfu/learned-point-cloud-compression-for-classification</a></li>
<li>paper_authors: Mateen Ulhaq, Ivan V. Bajić</li>
<li>for: 本研究旨在提出一种特种的点云编码器，用于在服务器端进行机器分析 tasks 的点云数据传输。</li>
<li>methods: 我们基于 PointNet 提出了一种特种的点云编码器，实现了与非特种编码器相比significantly better的Rate-Accuracy 质量比。</li>
<li>results: 我们的编码器在 ModelNet40 数据集上实现了94%的BD-比特率减少，而且对于低资源的终端设备，我们还提出了两种轻量级的编码器配置，可以实现相似的BD-比特率减少（93%和92%），同时只消耗0.470和0.048 encoder-side kMACs&#x2F;点。<details>
<summary>Abstract</summary>
Deep learning is increasingly being used to perform machine vision tasks such as classification, object detection, and segmentation on 3D point cloud data. However, deep learning inference is computationally expensive. The limited computational capabilities of end devices thus necessitate a codec for transmitting point cloud data over the network for server-side processing. Such a codec must be lightweight and capable of achieving high compression ratios without sacrificing accuracy. Motivated by this, we present a novel point cloud codec that is highly specialized for the machine task of classification. Our codec, based on PointNet, achieves a significantly better rate-accuracy trade-off in comparison to alternative methods. In particular, it achieves a 94% reduction in BD-bitrate over non-specialized codecs on the ModelNet40 dataset. For low-resource end devices, we also propose two lightweight configurations of our encoder that achieve similar BD-bitrate reductions of 93% and 92% with 3% and 5% drops in top-1 accuracy, while consuming only 0.470 and 0.048 encoder-side kMACs/point, respectively. Our codec demonstrates the potential of specialized codecs for machine analysis of point clouds, and provides a basis for extension to more complex tasks and datasets in the future.
</details>
<details>
<summary>摘要</summary>
深度学习在处理3D点云数据上进行机器视觉任务，如分类、物体检测和分割，日益受到欢迎。然而，深度学习推理过程具有计算成本高的问题，因此在终端设备上进行处理时需要一个点云编码器。这个编码器应该轻量级，能够实现高度压缩比，而无需牺牲准确性。为了解决这个问题，我们提出了一种特种的点云编码器，基于PointNet，可以在机器分类任务中实现显著更好的比例-准确性质量。具体来说，我们的编码器在ModelNet40数据集上实现了94%的BD-比特率减少，相比非特种编码器。而为了适应低资源的终端设备，我们还提出了两种轻量级的编码器配置，它们可以实现类似的BD-比特率减少，分别为93%和92%，但是消耗了0.470和0.048个encoder-side kMACs/点。我们的编码器表明特种编码器在机器分析点云数据时具有潜在的优势，并为未来扩展到更复杂的任务和数据集提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights"><a href="#Node-Embedding-for-Homophilous-Graphs-with-ARGEW-Augmentation-of-Random-walks-by-Graph-Edge-Weights" class="headerlink" title="Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights"></a>Node Embedding for Homophilous Graphs with ARGEW: Augmentation of Random walks by Graph Edge Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05957">http://arxiv.org/abs/2308.05957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ncsoft/argew">https://github.com/ncsoft/argew</a></li>
<li>paper_authors: Jun Hee Kim, Jaeman Son, Hyunsoo Kim, Eunjo Lee</li>
<li>for: 本文是针对 dense vector  represent nodes in network 的研究，尤其是Weighted homophilous graphs中 node pairs with stronger edges weights 应该有更加相近的 embedding。</li>
<li>methods: 本文提出了 ARGEW（Augmentation of Random walks by Graph Edge Weights），一种基于随机漫步的增强方法，可以使得 node embeddings 更加准确地反映 edge weights。</li>
<li>results: 在多个实际网络上，ARGEW 可以使得 node pairs with larger edge weights 有更加相近的 embedding，并且在 node classification 任务中，ARGEW 可以提高 node2vec 的性能，并且不受 hyperparameters 的影响。<details>
<summary>Abstract</summary>
Representing nodes in a network as dense vectors node embeddings is important for understanding a given network and solving many downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ do work for weighted networks via including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details>
<details>
<summary>摘要</summary>
importance of representing nodes in a network as dense vectors (node embeddings) for understanding the network and solving downstream tasks. In particular, for weighted homophilous graphs where similar nodes are connected with larger edge weights, we desire node embeddings where node pairs with strong weights have closer embeddings. Although random walk based node embedding methods like node2vec and node2vec+ can work for weighted networks by including edge weights in the walk transition probabilities, our experiments show that the embedding result does not adequately reflect edge weights. In this paper, we propose ARGEW (Augmentation of Random walks by Graph Edge Weights), a novel augmentation method for random walks that expands the corpus in such a way that nodes with larger edge weights end up with closer embeddings. ARGEW can work with any random walk based node embedding method, because it is independent of the random sampling strategy itself and works on top of the already-performed walks. With several real-world networks, we demonstrate that with ARGEW, compared to not using it, the desired pattern that node pairs with larger edge weights have closer embeddings is much clearer. We also examine ARGEW's performance in node classification: node2vec with ARGEW outperforms pure node2vec and is not sensitive to hyperparameters (i.e. consistently good). In fact, it achieves similarly good results as supervised GCN, even without any node feature or label information during training. Finally, we explain why ARGEW works consistently well by exploring the coappearance distributions using a synthetic graph with clear structural roles.
</details></li>
</ul>
<hr>
<h2 id="INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing"><a href="#INR-Arch-A-Dataflow-Architecture-and-Compiler-for-Arbitrary-Order-Gradient-Computations-in-Implicit-Neural-Representation-Processing" class="headerlink" title="INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing"></a>INR-Arch: A Dataflow Architecture and Compiler for Arbitrary-Order Gradient Computations in Implicit Neural Representation Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05930">http://arxiv.org/abs/2308.05930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Abi-Karam, Rishov Sarkar, Dejia Xu, Zhiwen Fan, Zhangyang Wang, Cong Hao</li>
<li>for: 这个论文主要用于探讨nth-order gradient计算在图形学、元学习（MAML）、科学计算和最近的隐藏神经表示（INR）中的应用。</li>
<li>methods: 这个论文使用了一种叫做INR-Arch的框架，它可以将计算图的nth-order gradient转换成一个硬件优化的数据流体系结构。这个框架包括两个阶段：首先，设计了一个高效的数据流体系结构，其中使用了FIFO流和优化的计算kernels库，以确保高效的内存利用和并行计算。其次，提出了一种编译器，它可以自动从计算图中提取和优化计算，并配置硬件参数 such as 延迟和流深度以优化吞吐量，保证不会出现死锁现象，并生成高级合成（HLS）代码 дляFPGA实现。</li>
<li>results: 这个论文通过对INR编辑作为测试样本，实现了对CPU和GPU基线的1.8-4.8倍和1.5-3.6倍的速度提升，同时也实现了对内存使用的3.1-8.9倍和1.7-4.3倍的减少，以及对能效率的1.7-11.3倍和5.5-32.8倍的下降。<details>
<summary>Abstract</summary>
An increasing number of researchers are finding use for nth-order gradient computations for a wide variety of applications, including graphics, meta-learning (MAML), scientific computing, and most recently, implicit neural representations (INRs). Recent work shows that the gradient of an INR can be used to edit the data it represents directly without needing to convert it back to a discrete representation. However, given a function represented as a computation graph, traditional architectures face challenges in efficiently computing its nth-order gradient due to the higher demand for computing power and higher complexity in data movement. This makes it a promising target for FPGA acceleration. In this work, we introduce INR-Arch, a framework that transforms the computation graph of an nth-order gradient into a hardware-optimized dataflow architecture. We address this problem in two phases. First, we design a dataflow architecture that uses FIFO streams and an optimized computation kernel library, ensuring high memory efficiency and parallel computation. Second, we propose a compiler that extracts and optimizes computation graphs, automatically configures hardware parameters such as latency and stream depths to optimize throughput, while ensuring deadlock-free operation, and outputs High-Level Synthesis (HLS) code for FPGA implementation. We utilize INR editing as our benchmark, presenting results that demonstrate 1.8-4.8x and 1.5-3.6x speedup compared to CPU and GPU baselines respectively. Furthermore, we obtain 3.1-8.9x and 1.7-4.3x lower memory usage, and 1.7-11.3x and 5.5-32.8x lower energy-delay product. Our framework will be made open-source and available on GitHub.
</details>
<details>
<summary>摘要</summary>
更多研究人员正在发现使用 nth-order Gradient 计算在各种应用程序中，包括图形学、多学习（MAML）、科学计算和最近的隐藏神经表示（INRs）。最新的研究表明，INR 的 Gradient 可以直接编辑所表示的数据，而无需将其转换回分割表示。然而，传统架构在计算 Graph 中的 nth-order Gradient 计算时面临着更高的计算能力和数据移动复杂性的挑战，这使得它成为了可 acceleration 的目标。在这项工作中，我们介绍 INR-Arch，一个将计算 Graph 转换为硬件优化数据流架构的框架。我们解决这个问题在两个阶段。首先，我们设计了一个数据流架构，使用 FIFO 流和优化的计算内核库，保证高内存效率和并行计算。其次，我们提出了一个编译器，可以提取和优化计算 Graph，自动配置硬件参数 such as 延迟和流深度，以优化通过put，并确保不会出现堵塞的操作。我们使用 INR 编辑作为我们的标准，发表了结果，表明在 CPU 和 GPU 基线相比，得到了 1.8-4.8 倍和 1.5-3.6 倍的速度提升。此外，我们获得了 3.1-8.9 倍和 1.7-4.3 倍的内存使用量减少，以及 1.7-11.3 倍和 5.5-32.8 倍的能量延迟产品。我们的框架即将被开源，并在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="On-the-equivalence-of-Occam-algorithms"><a href="#On-the-equivalence-of-Occam-algorithms" class="headerlink" title="On the equivalence of Occam algorithms"></a>On the equivalence of Occam algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05906">http://arxiv.org/abs/2308.05906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaman Keinath-Esmail</li>
<li>for: 本文提供了一个后验正则化的基础，即任何可学习的概念类都可以由Occam算法学习。</li>
<li>methods: 本文使用了Board和Pitt（1990）提出的一种受到例外列表的影响的Occam算法，并证明了这种算法可以学习任何可学习的概念类。</li>
<li>results: 本文证明了Board和Pitt（1990）的一种受到例外列表的影响的Occam算法可以学习任何可学习的概念类，并且这种算法的复杂度是$\delta$-无关的。<details>
<summary>Abstract</summary>
Blumer et al. (1987, 1989) showed that any concept class that is learnable by Occam algorithms is PAC learnable. Board and Pitt (1990) showed a partial converse of this theorem: for concept classes that are closed under exception lists, any class that is PAC learnable is learnable by an Occam algorithm. However, their Occam algorithm outputs a hypothesis whose complexity is $\delta$-dependent, which is an important limitation. In this paper, we show that their partial converse applies to Occam algorithms with $\delta$-independent complexities as well. Thus, we provide a posteriori justification of various theoretical results and algorithm design methods which use the partial converse as a basis for their work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems"><a href="#Comparing-the-quality-of-neural-network-uncertainty-estimates-for-classification-problems" class="headerlink" title="Comparing the quality of neural network uncertainty estimates for classification problems"></a>Comparing the quality of neural network uncertainty estimates for classification problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05903">http://arxiv.org/abs/2308.05903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Joshua Michalenko, Tyler Ganter, Rashad Imad-Fayez Baiyasi, Jason Adams<br>for:这个论文的目的是评估深度学习模型中的不确定性评估方法的质量。methods:这个论文使用了统计方法来评估信任区间的覆盖率和信任间隔，以及预测分类预测信任的均值误差来评估深度学习模型的不确定性评估方法。results:研究发现，不同的不确定性评估方法在同一个数据集上可以生成不同的结果，而且这些结果之间存在差异。此外，研究还发现，使用MCMC和VI方法可以获得更好的不确定性评估结果，而使用DE和MC dropout方法的结果则更为不稳定。<details>
<summary>Abstract</summary>
Traditional deep learning (DL) models are powerful classifiers, but many approaches do not provide uncertainties for their estimates. Uncertainty quantification (UQ) methods for DL models have received increased attention in the literature due to their usefulness in decision making, particularly for high-consequence decisions. However, there has been little research done on how to evaluate the quality of such methods. We use statistical methods of frequentist interval coverage and interval width to evaluate the quality of credible intervals, and expected calibration error to evaluate classification predicted confidence. These metrics are evaluated on Bayesian neural networks (BNN) fit using Markov Chain Monte Carlo (MCMC) and variational inference (VI), bootstrapped neural networks (NN), Deep Ensembles (DE), and Monte Carlo (MC) dropout. We apply these different UQ for DL methods to a hyperspectral image target detection problem and show the inconsistency of the different methods' results and the necessity of a UQ quality metric. To reconcile these differences and choose a UQ method that appropriately quantifies the uncertainty, we create a simulated data set with fully parameterized probability distribution for a two-class classification problem. The gold standard MCMC performs the best overall, and the bootstrapped NN is a close second, requiring the same computational expense as DE. Through this comparison, we demonstrate that, for a given data set, different models can produce uncertainty estimates of markedly different quality. This in turn points to a great need for principled assessment methods of UQ quality in DL applications.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）模型是强大的分类器，但许多方法不提供不确定性的估计。不确定性量化（UQ）方法 для DL 模型在文献中收到了更多的关注，因为它们在决策中非常有用，特别是对高 conseqüência 的决策。然而，对 UQ 方法评价的研究相对较少。我们使用统计方法的频率interval coverage和interval width来评价credible interval的质量，以及预期准确性error来评价分类预测的自信度。这些指标在bayesian neural network（BNN）适用markov chain Monte Carlo（MCMC）和variational inference（VI）、bootstrapped neural network（NN）、deep ensembles（DE）和Monte Carlo（MC）dropout中被评价。我们对这些不同的 UQ 方法应用到一个 hyperspectral image target detection问题，并显示了不同方法的结果之间的不一致，以及需要一个 UQ 质量指标。为了解决这些不一致并选择一个正确地量化不确定性的 UQ 方法，我们创建了一个完全参数化的概率分布的 simulated data set，用于两类分类问题。金标准 MCMC 表现最佳，而 bootstrapped NN 紧随其后，需要与 DE 相同的计算成本。通过这种比较，我们证明了，对于给定的数据集，不同的模型可以生成不同质量的不确定性估计。这一点点到了深度学习应用中需要原则性评价 UQ 质量的强需求。
</details></li>
</ul>
<hr>
<h2 id="Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks"><a href="#Target-Detection-on-Hyperspectral-Images-Using-MCMC-and-VI-Trained-Bayesian-Neural-Networks" class="headerlink" title="Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks"></a>Target Detection on Hyperspectral Images Using MCMC and VI Trained Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06293">http://arxiv.org/abs/2308.06293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Ries, Jason Adams, Joshua Zollweg</li>
<li>for: 这篇论文的目的是提出了一种 bayesian neural network (BNN) 的应用，以便在图像分类 tasks 中提供 uncertainty quantification (UQ)。</li>
<li>methods: 这篇论文使用了 MCMC 和 VI 两种不同的训练方法，以评估 BNN 的训练效果。</li>
<li>results: 研究发现，MCMC 和 VI 两种训练方法都可以达到良好的检测效果，但是 VI 方法的计算速度较快。此外，研究还发现，不同的训练方法可能会导致不同的模型结果，特别是在高风险应用中。<details>
<summary>Abstract</summary>
Neural networks (NN) have become almost ubiquitous with image classification, but in their standard form produce point estimates, with no measure of confidence. Bayesian neural networks (BNN) provide uncertainty quantification (UQ) for NN predictions and estimates through the posterior distribution. As NN are applied in more high-consequence applications, UQ is becoming a requirement. BNN provide a solution to this problem by not only giving accurate predictions and estimates, but also an interval that includes reasonable values within a desired probability. Despite their positive attributes, BNN are notoriously difficult and time consuming to train. Traditional Bayesian methods use Markov Chain Monte Carlo (MCMC), but this is often brushed aside as being too slow. The most common method is variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy. We apply and compare MCMC- and VI-trained BNN in the context of target detection in hyperspectral imagery (HSI), where materials of interest can be identified by their unique spectral signature. This is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra. Both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene. Both MCMC- and VI-trained BNN perform well overall at target detection on a simulated HSI scene. This paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model. If sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems.
</details>
<details>
<summary>摘要</summary>
neural networks (NN) 已经在图像分类中变得极其普遍，但在标准形式下产生点估计，无法提供信度量。 bayesian neural networks (BNN) 提供图像分类预测和估计的不确定性评估（UQ），通过 posterior distribution。 随着 NN 在高重要性应用中使用，UQ 变得必须。 BNN 不仅提供准确的预测和估计，还提供一个包含合理值的时间间隔，在所需的概率范围内。 despite their positive attributes, BNN 很难和时间consuming 进行训练。 traditional Bayesian methods 使用 markov chain Monte Carlo (MCMC)，但这经常被认为是太慢。 the most common method 是 variational inference (VI) due to its fast computation, but there are multiple concerns with its efficacy。 我们在 target detection 中应用和比较 MCMC- 和 VI-trained BNN 在 hyperspectral imagery (HSI) 中，where materials of interest can be identified by their unique spectral signature。 this is a challenging field, due to the numerous permuting effects practical collection of HSI has on measured spectra。 both models are trained using out-of-the-box tools on a high fidelity HSI target detection scene。 both MCMC- 和 VI-trained BNN perform well overall at target detection on a simulated HSI scene。 this paper provides an example of how to utilize the benefits of UQ, but also to increase awareness that different training methods can give different results for the same model。 if sufficient computational resources are available, the best approach rather than the fastest or most efficient should be used, especially for high consequence problems。
</details></li>
</ul>
<hr>
<h2 id="The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences"><a href="#The-divergence-time-of-protein-structures-modelled-by-Markov-matrices-and-its-relation-to-the-divergence-of-sequences" class="headerlink" title="The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences"></a>The divergence time of protein structures modelled by Markov matrices and its relation to the divergence of sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06292">http://arxiv.org/abs/2308.06292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandun Rajapaksa, Lloyd Allison, Peter J. Stuckey, Maria Garcia de la Banda, Arun S. Konagurthu</li>
<li>for: 这种研究是为了开发一种基于蛋白质结构的时间参数化统计模型，以量化蛋白质结构在演化过程中的差异进化。</li>
<li>methods: 该研究使用了一个大量的蛋白质三维结构对比来推断一种时间参数化的统计模型，并使用了 bayesian 和信息理论的框架来推断时间参数化的随机矩阵和 Dirichlet 模型。</li>
<li>results: 研究发现，使用这种时间参数化统计模型可以更准确地估计蛋白质结构之间的分化时间，并且可以与序列相关性的时间参数化模型进行比较。此外，该模型还可以在次结构预测中与常见的神经网络架构竞争。<details>
<summary>Abstract</summary>
A complete time-parameterized statistical model quantifying the divergent evolution of protein structures in terms of the patterns of conservation of their secondary structures is inferred from a large collection of protein 3D structure alignments. This provides a better alternative to time-parameterized sequence-based models of protein relatedness, that have clear limitations dealing with twilight and midnight zones of sequence relationships. Since protein structures are far more conserved due to the selection pressure directly placed on their function, divergence time estimates can be more accurate when inferred from structures. We use the Bayesian and information-theoretic framework of Minimum Message Length to infer a time-parameterized stochastic matrix (accounting for perturbed structural states of related residues) and associated Dirichlet models (accounting for insertions and deletions during the evolution of protein domains). These are used in concert to estimate the Markov time of divergence of tertiary structures, a task previously only possible using proxies (like RMSD). By analyzing one million pairs of homologous structures, we yield a relationship between the Markov divergence time of structures and of sequences. Using these inferred models and the relationship between the divergence of sequences and structures, we demonstrate a competitive performance in secondary structure prediction against neural network architectures commonly employed for this task. The source code and supplementary information are downloadable from \url{http://lcb.infotech.monash.edu.au/sstsum}.
</details>
<details>
<summary>摘要</summary>
一个完整的时间参数化统计模型，用于描述蛋白质结构的不同演化的 Patterns of conservation of secondary structures，从一个大量的蛋白质三维结构对Alignment中得到了推断。这提供了一个更好的代替时间参数化序列基于模型，该模型在处理晚上和午夜时区的序列关系时存在显著的限制。由于蛋白质结构受直接选择压力的影响，因此从结构来进行演化时间估计的准确性比序列基于模型更高。我们使用 bayesian 和信息理论的框架，Minimum Message Length 来推断时间参数化随机矩阵（考虑相关的结构态态）和 Dirichlet 模型（考虑插入和删除 durante protein domains 的演化）。这些模型在一起使用，以估计蛋白质结构的马克夫时间异同。在分析一百万对同源结构的情况下，我们发现了结构异同时间和序列异同时间之间的关系。使用这些推断出的模型和序列异同时间之间的关系，我们展示了在二级结构预测中与通用的神经网络架构相比，我们的表现是竞争性的。源代码和补充信息可以从 \url{http://lcb.infotech.monash.edu.au/sstsum} 下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding"><a href="#Learning-to-Team-Based-Navigation-A-Review-of-Deep-Reinforcement-Learning-Techniques-for-Multi-Agent-Pathfinding" class="headerlink" title="Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding"></a>Learning to Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05893">http://arxiv.org/abs/2308.05893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaehoon Chung, Jamil Fayyad, Younes Al Younes, Homayoun Najjaran</li>
<li>for: 本研究旨在探讨 Deep Reinforcement Learning（DRL）在多体系统中的应用，尤其是在多体路径找索中。</li>
<li>methods: 本文使用了多种 Deep Reinforcement Learning（DRL）方法，包括价值函数法和策略法，以解决多体系统中的路径找索问题。</li>
<li>results: 本文提供了一个统一的评价指标集，以便比较不同的多体路径找索算法的性能。此外，本文还发现了模型基于的DRL在多体系统中的潜在应用潜力，并提供了相关的基础知识。<details>
<summary>Abstract</summary>
Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified metrics for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.
</details>
<details>
<summary>摘要</summary>
多智能路径找索（MAPF）是许多大规模 роботи库应用中的关键领域，经常作为多智能系统的基础步骤。然而，随着环境的增加复杂性，MAPF的现有解决方案的效果逐渐减退。与其他研究不同，我们的工作在这篇评论文中不仅提供了近期MAPF的进展概述，还广泛评论了深度强化学习（DRL）在多智能系统设置中的应用。此外，我们的工作还强调了评价MAPF解决方案的缺乏统一评价指标，并提供了全面的解释。最后，我们的文章还讨论了基于模型的DRL作为未来方向的潜在发展，并提供了相应的基础理解，以解决当前MAPF中的挑战。我们的目标是帮助读者更深入了解当前的研究方向，提供统一的评价指标，以及扩展他们对基于模型的DRL的知识，以Addressing the existing challenges in MAPF。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DF2-Distribution-Free-Decision-Focused-Learning"><a href="#DF2-Distribution-Free-Decision-Focused-Learning" class="headerlink" title="DF2: Distribution-Free Decision-Focused Learning"></a>DF2: Distribution-Free Decision-Focused Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05889">http://arxiv.org/abs/2308.05889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingkai Kong, Wenhao Mu, Jiaming Cui, Yuchen Zhuang, B. Aditya Prakash, Bo Dai, Chao Zhang</li>
<li>for: 这篇论文是针对predict-then-optimize问题的decision-focused learning（DFL）方法，对于exististing end-to-end DFL方法的三个瓶颈：模型误差错误、抽象描述错误和梯度近似错误。</li>
<li>methods: 我们的DF2方法是第一个不需要任务特定的预测器，直接在训练过程中学习预期优化目标函数。我们创造了一个注意力基于分布的模型架构，以便对预期目标函数进行有效的学习。</li>
<li>results: 我们在一个实验中，用DF2方法解决了一个 sintetic问题、一个风力发电问题和一个非凸疫苗分布问题，得到了DF2方法的有效性。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending on a task-specific forecaster that requires precise model assumptions, our method directly learns the expected optimization function during training. To efficiently learn the function in a data-driven manner, we devise an attention-based model architecture inspired by the distribution-based parameterization of the expected objective. Our method is, to the best of our knowledge, the first to address all three bottlenecks within a single model. We evaluate DF2 on a synthetic problem, a wind power bidding problem, and a non-convex vaccine distribution problem, demonstrating the effectiveness of DF2.
</details>
<details>
<summary>摘要</summary>
决策关注学习（DFL）是一种有力的方法，用于解决预测后优化问题，通过自适应一个预测模型来满足下游优化任务。然而，现有的端到端DFL方法受到三种主要瓶颈的限制：模型匹配错误、样本平均化错误和梯度估计错误。模型匹配错误来自预测模型中参数化的预测分布与真实概率分布之间的不一致。样本平均化错误发生在使用有限样本来估计优化目标函数的期望值时。梯度估计错误则是因为DFL通过KKT条件来计算梯度，而大多数方法在非拟合目标函数中使用较差的梯度估计进行反向传播。在这篇论文中，我们提出了DF2方法——首先的无模型匹配的决策关注学习方法，用于解决这三种瓶颈。而不是基于任务特定的预测器，我们的方法直接在训练过程中学习预测函数。为效率地学习函数，我们设计了一种注意力基于分布的模型架构，得益于分布基于参数化的预测目标函数。我们的方法是，到目前为止所知道的第一个能够同时解决这三种瓶颈的单一模型。我们在一个 sintetic问题、一个风力发电拍卖问题和一个非拟合疫苗分布问题上进行了评估，并证明了DF2的效果。
</details></li>
</ul>
<hr>
<h2 id="GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder"><a href="#GPLaSDI-Gaussian-Process-based-Interpretable-Latent-Space-Dynamics-Identification-through-Deep-Autoencoder" class="headerlink" title="GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder"></a>GPLaSDI: Gaussian Process-based Interpretable Latent Space Dynamics Identification through Deep Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05882">http://arxiv.org/abs/2308.05882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llnl/gplasdi">https://github.com/llnl/gplasdi</a></li>
<li>paper_authors: Christophe Bonneville, Youngsoo Choi, Debojyoti Ghosh, Jonathan L. Belof</li>
<li>for:  This paper aims to provide a novel reduced-order modeling (ROM) framework that leverages machine learning to solve partial differential equations (PDEs) efficiently and accurately.</li>
<li>methods:  The proposed method, called GPLaSDI, utilizes Gaussian processes (GPs) for latent space ODE interpolations, which enables the quantification of uncertainty over the ROM predictions and allows for efficient adaptive training.</li>
<li>results:  The proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error, on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem.<details>
<summary>Abstract</summary>
Numerically solving partial differential equations (PDEs) can be challenging and computationally expensive. This has led to the development of reduced-order models (ROMs) that are accurate but faster than full order models (FOMs). Recently, machine learning advances have enabled the creation of non-linear projection methods, such as Latent Space Dynamics Identification (LaSDI). LaSDI maps full-order PDE solutions to a latent space using autoencoders and learns the system of ODEs governing the latent space dynamics. By interpolating and solving the ODE system in the reduced latent space, fast and accurate ROM predictions can be made by feeding the predicted latent space dynamics into the decoder. In this paper, we introduce GPLaSDI, a novel LaSDI-based framework that relies on Gaussian process (GP) for latent space ODE interpolations. Using GPs offers two significant advantages. First, it enables the quantification of uncertainty over the ROM predictions. Second, leveraging this prediction uncertainty allows for efficient adaptive training through a greedy selection of additional training data points. This approach does not require prior knowledge of the underlying PDEs. Consequently, GPLaSDI is inherently non-intrusive and can be applied to problems without a known PDE or its residual. We demonstrate the effectiveness of our approach on the Burgers equation, Vlasov equation for plasma physics, and a rising thermal bubble problem. Our proposed method achieves between 200 and 100,000 times speed-up, with up to 7% relative error.
</details>
<details>
<summary>摘要</summary>
解决部分泛函方程（PDE）数学问题可以是困难的并且 computationally expensive。这导致了减少顺序模型（ROM）的发展，这些模型具有准确性，但速度比整个顺序模型（FOM）更快。近些年，机器学习的进步使得非线性投影方法，如潜在空间动力学标识（LaSDI）的创造。LaSDI将全序PDE解析到一个潜在空间使用自适应神经网络，并学习潜在空间动力学系统的ODE。通过在减少的潜在空间中预测和解决ODE系统，可以快速并准确地预测ROM。在这篇论文中，我们介绍了GPLaSDI，一种基于 Gaussian process（GP）的LaSDI框架。使用GP提供了两点优势。首先，它允许量化ROM预测中的uncertainty。其次，通过利用这种预测uncertainty，可以高效地进行适应性训练，通过滥见训练数据点。这种方法不需要先知道下辖PDE或其剩余。因此，GPLaSDI是非侵入的，可以应用于没有known PDE或其剩余的问题。我们在Burgers方程、Vlasov方程 для плазма物理和热气囊问题中展示了我们的方法的效果。我们的提posed方法可以实现200到100,000倍的速度增加，相对误差在7%之间。
</details></li>
</ul>
<hr>
<h2 id="Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models"><a href="#Aphid-Cluster-Recognition-and-Detection-in-the-Wild-Using-Deep-Learning-Models" class="headerlink" title="Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models"></a>Aphid Cluster Recognition and Detection in the Wild Using Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05881">http://arxiv.org/abs/2308.05881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Cuncong Zhong, Bo Luo, Ivan Grijalva, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang</li>
<li>for: 本研究旨在使用深度学习模型检测螟蛾群体，以实现Targeted pesticide application。</li>
<li>methods: 我们使用了大规模的实验数据和深度学习模型来检测螟蛾群体，并对数据进行了处理和分割以便机器学习模型的使用。</li>
<li>results: 我们的实验结果表明，使用深度学习模型可以准确地检测螟蛾群体，并且可以通过合并邻近的群体和移除小 clusters来进一步提高性能。<details>
<summary>Abstract</summary>
Aphid infestation poses a significant threat to crop production, rural communities, and global food security. While chemical pest control is crucial for maximizing yields, applying chemicals across entire fields is both environmentally unsustainable and costly. Hence, precise localization and management of aphids are essential for targeted pesticide application. The paper primarily focuses on using deep learning models for detecting aphid clusters. We propose a novel approach for estimating infection levels by detecting aphid clusters. To facilitate this research, we have captured a large-scale dataset from sorghum fields, manually selected 5,447 images containing aphids, and annotated each individual aphid cluster within these images. To facilitate the use of machine learning models, we further process the images by cropping them into patches, resulting in a labeled dataset comprising 151,380 image patches. Then, we implemented and compared the performance of four state-of-the-art object detection models (VFNet, GFLV2, PAA, and ATSS) on the aphid dataset. Extensive experimental results show that all models yield stable similar performance in terms of average precision and recall. We then propose to merge close neighboring clusters and remove tiny clusters caused by cropping, and the performance is further boosted by around 17%. The study demonstrates the feasibility of automatically detecting and managing insects using machine learning models. The labeled dataset will be made openly available to the research community.
</details>
<details>
<summary>摘要</summary>
螟蛀感染 pose 对农业生产、农村社区和全球食品安全构成了严重的威胁。虽然化学防治是提高产量的重要手段，但是在整个场景中应用化学品是环境不可持续和昂贵的。因此，准确地Localization和管理螟蛀是必要的。本文主要关注使用深度学习模型 для检测螟蛀群。我们提出了一种新的方法，通过检测螟蛀群来估算感染水平。为了进行这项研究，我们在高粮田中采集了大规模数据集，手动选择了5,447张图像中包含螟蛀的图像，并对每个个体螟蛀群进行了标注。为了使机器学习模型可以使用，我们进一步处理了图像，将其分割成 patches，得到了151,380个标注图像 patches。然后，我们实现了和比较了四种当前最佳 объек detection 模型（VFNet、GFLV2、PAA 和 ATSS）在螟蛀数据集上的性能。广泛的实验结果表明，所有模型在精度和准确性方面具有稳定的性能。我们 then propose 将邻近的螟蛀群合并并 removes 小于patches 的螟蛀群，性能得到了约17%的提高。该研究表明了使用机器学习模型自动检测和管理昆虫的可能性。我们将标注数据集公开提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams"><a href="#Composable-Core-sets-for-Diversity-Approximation-on-Multi-Dataset-Streams" class="headerlink" title="Composable Core-sets for Diversity Approximation on Multi-Dataset Streams"></a>Composable Core-sets for Diversity Approximation on Multi-Dataset Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05878">http://arxiv.org/abs/2308.05878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie Wang, Michael Flynn, Fangyu Luo</li>
<li>for: 这篇论文旨在描述如何使用核心集来简化流处理数据，以便在实时训练机器学习模型时提高效率。</li>
<li>methods: 该论文提出了一种基于核心集的核心集建构算法，用于简化流处理数据并在活动学习环境中使用。此外，论文还提出了一种使用核心集和CRAIG等技术来加速建构速度。</li>
<li>results: 论文通过对推送数据进行预测分析，证明了这种方法可以在实时训练机器学习模型时提高效率。此外，论文还提出了一些改进建构速度的策略和技术。<details>
<summary>Abstract</summary>
Core-sets refer to subsets of data that maximize some function that is commonly a diversity or group requirement. These subsets are used in place of the original data to accomplish a given task with comparable or even enhanced performance if biases are removed. Composable core-sets are core-sets with the property that subsets of the core set can be unioned together to obtain an approximation for the original data; lending themselves to be used for streamed or distributed data. Recent work has focused on the use of core-sets for training machine learning models. Preceding solutions such as CRAIG have been proven to approximate gradient descent while providing a reduced training time. In this paper, we introduce a core-set construction algorithm for constructing composable core-sets to summarize streamed data for use in active learning environments. If combined with techniques such as CRAIG and heuristics to enhance construction speed, composable core-sets could be used for real time training of models when the amount of sensor data is large. We provide empirical analysis by considering extrapolated data for the runtime of such a brute force algorithm. This algorithm is then analyzed for efficiency through averaged empirical regression and key results and improvements are suggested for further research on the topic.
</details>
<details>
<summary>摘要</summary>
核心集（core-set）指的是一 subset of data 可以最大化某种函数，通常是多样性或组合要求。这些子集用于取代原始数据来完成一个给定任务，并且可以保持或提高性能，即使存在偏见。可搅 core-sets 是指可以将核心集中的子集 union 起来 obtaint 原始数据的一个近似。这些核心集具有可搅性，可以用于流处理或分布式数据。现有研究集中焦点在 core-sets 的使用，特别是用于训练机器学习模型。之前的解决方案，如 CRAIG，已经证明可以近似梯度下降，同时提供减少的训练时间。在这篇论文中，我们介绍一种用于构建可搅 core-sets 的算法，用于概要流处理数据，以便在活动学习环境中使用。如果与 CRAIG 和其他优化技术相结合，可搅 core-sets 可以用于实时训练模型，当感知数据量很大时。我们对此进行了实验分析，考虑了扩展数据的运行时间。这种算法的效率被分析了，并且提出了一些关键结果和改进建议。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-N-CNN-for-Clinical-Practice"><a href="#Revisiting-N-CNN-for-Clinical-Practice" class="headerlink" title="Revisiting N-CNN for Clinical Practice"></a>Revisiting N-CNN for Clinical Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05877">http://arxiv.org/abs/2308.05877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Antunes Ferreira, Lucas Pereira Carlini, Gabriel de Almeida Sá Coutrin, Tatiany Marcondes Heideirich, Marina Carvalho de Moraes Barros, Ruth Guinsburg, Carlos Eduardo Thomaz</li>
<li>for: 这篇论文旨在优化Neonatal Convolutional Neural Network（N-CNN）的超参数，并评估这些超参数对类别指标、可解释性和可靠性的影响，以及它们在临床实践中的潜在影响。</li>
<li>methods: 我们选择了不改变原始N-CNN架构的超参数，主要是修改学习率和训练正则化。我们通过评估每个超参数的改进情况来选择最佳超参数，并创建了调整后的Tuned N-CNN。此外，我们还应用了基于新生脸部编码系统的软标签，提出了一种新的训练 expresión facial类型分类模型的方法，用于评估新生痛的评估。</li>
<li>results: 结果表明，调整后的Tuned N-CNN显示出了类别指标和可解释性的改进，但这些改进直接不对准确性表现出来。我们认为这些发现可能有助于开发更可靠的痛评估工具 для新生，帮助医疗专业人员提供适当的 intervención和改善病人结果。<details>
<summary>Abstract</summary>
This paper revisits the Neonatal Convolutional Neural Network (N-CNN) by optimizing its hyperparameters and evaluating how they affect its classification metrics, explainability and reliability, discussing their potential impact in clinical practice. We have chosen hyperparameters that do not modify the original N-CNN architecture, but mainly modify its learning rate and training regularization. The optimization was done by evaluating the improvement in F1 Score for each hyperparameter individually, and the best hyperparameters were chosen to create a Tuned N-CNN. We also applied soft labels derived from the Neonatal Facial Coding System, proposing a novel approach for training facial expression classification models for neonatal pain assessment. Interestingly, while the Tuned N-CNN results point towards improvements in classification metrics and explainability, these improvements did not directly translate to calibration performance. We believe that such insights might have the potential to contribute to the development of more reliable pain evaluation tools for newborns, aiding healthcare professionals in delivering appropriate interventions and improving patient outcomes.
</details>
<details>
<summary>摘要</summary>
（本文重新审查了新生儿 convolutional neural network（N-CNN）的超参数，并评估它们如何影响其分类指标、可解释性和可靠性，并讨论它们在临床实践中的潜在影响。我们选择了不改变原始 N-CNN 架构的超参数，主要是 modify 学习率和训练正则化。优化是通过评估每个超参数的改进情况来进行，并选择最佳超参数来创建一个优化后的 Tuned N-CNN。我们还应用了来自新生儿表情编码系统的软标签，提出了一种新的训练 facial expression 分类模型的方法，用于新生儿疼痛评估。有趣的是，改进后的 Tuned N-CNN 结果表明，对于分类指标和可解释性来说，有所改进，但这些改进并不直接对准报表性能产生影响。我们认为，这些发现可能对新生儿疼痛评估工具的开发产生影响，帮助医疗专业人员提供适当的 intervención和改善病人结果。）
</details></li>
</ul>
<hr>
<h2 id="UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data"><a href="#UFed-GAN-A-Secure-Federated-Learning-Framework-with-Constrained-Computation-and-Unlabeled-Data" class="headerlink" title="UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data"></a>UFed-GAN: A Secure Federated Learning Framework with Constrained Computation and Unlabeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05870">http://arxiv.org/abs/2308.05870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achintha Wijesinghe, Songyang Zhang, Siyu Qi, Zhi Ding</li>
<li>for: 这篇论文是为了解决在云端环境中部署低延迟多媒体数据分类和数据隐私问题而提出的一种学习平衡，尤其是在有限的计算资源和没有标签数据的情况下。</li>
<li>methods: 这篇论文提出了一个名为UFed-GAN的无监督联邦学习框架，可以在用户端数据分布下进行学习，不需要进行本地分类训练。此外，论文还进行了对UFed-GAN的参数分析和隐私分析。</li>
<li>results: 实验结果显示，UFed-GAN在有限的计算资源和没有标签数据的情况下可以实现高效的数据分类和隐私保护。<details>
<summary>Abstract</summary>
To satisfy the broad applications and insatiable hunger for deploying low latency multimedia data classification and data privacy in a cloud-based setting, federated learning (FL) has emerged as an important learning paradigm. For the practical cases involving limited computational power and only unlabeled data in many wireless communications applications, this work investigates FL paradigm in a resource-constrained and label-missing environment. Specifically, we propose a novel framework of UFed-GAN: Unsupervised Federated Generative Adversarial Network, which can capture user-side data distribution without local classification training. We also analyze the convergence and privacy of the proposed UFed-GAN. Our experimental results demonstrate the strong potential of UFed-GAN in addressing limited computational resources and unlabeled data while preserving privacy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为满足云端数据分类和隐私的广泛应用和不满足的需求，联邦学习（FL）已经成为一种重要的学习模式。在许多无线通信应用中，由于限制的计算资源和只有无标签数据，这项工作研究了在资源限制和标签缺失环境中的FL模式。我们提出了一种新的框架——无监督联邦生成敌战网络（UFed-GAN），可以在用户端 cattcapture数据分布without local classification training。我们还分析了UFed-GAN的 converges和隐私性。我们的实验结果表明，UFed-GAN在限制计算资源和无标签数据的情况下具有强大的潜在性，能够解决数据隐私和安全问题。
</details></li>
</ul>
<hr>
<h2 id="Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment"><a href="#Using-Twitter-Data-to-Determine-Hurricane-Category-An-Experiment" class="headerlink" title="Using Twitter Data to Determine Hurricane Category: An Experiment"></a>Using Twitter Data to Determine Hurricane Category: An Experiment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05866">http://arxiv.org/abs/2308.05866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songhui Yue, Jyothsna Kondari, Aibek Musaev, Randy K. Smith, Songqing Yue</li>
<li>for: 本研究旨在找到社交媒体数据和自然灾害的严重程度之间的映射关系。</li>
<li>methods: 本研究使用数据挖掘技术来分析Twitter数据，并对各地区的Twitter数据和飓风等级之间进行相关性分析。</li>
<li>results: 实验结果表明，Twitter数据和飓风等级之间存在积极的相关性，并提出了一种使用Twitter数据预测飓风等级的方法。<details>
<summary>Abstract</summary>
Social media posts contain an abundant amount of information about public opinion on major events, especially natural disasters such as hurricanes. Posts related to an event, are usually published by the users who live near the place of the event at the time of the event. Special correlation between the social media data and the events can be obtained using data mining approaches. This paper presents research work to find the mappings between social media data and the severity level of a disaster. Specifically, we have investigated the Twitter data posted during hurricanes Harvey and Irma, and attempted to find the correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details>
<details>
<summary>摘要</summary>
社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。 Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.Here's the word-for-word translation:社交媒体帖子中含有很多关于大事件的公众意见信息，尤其是自然灾害such as 飓风。posts相关的事件通常由用户们在事件发生时在附近的地方发布。我们的研究旨在找到社交媒体数据和灾害严重程度之间的映射。Specifically, we investigated Twitter data posted during hurricanes Harvey and Irma and attempted to find a correlation between the Twitter data of a specific area and the hurricane level in that area. Our experimental results indicate a positive correlation between them. We also present a method to predict the hurricane category for a specific area using relevant Twitter data.
</details></li>
</ul>
<hr>
<h2 id="The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions"><a href="#The-Multi-modality-Cell-Segmentation-Challenge-Towards-Universal-Solutions" class="headerlink" title="The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions"></a>The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05864">http://arxiv.org/abs/2308.05864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric Upschulte, Timo Dickscheid, José Guilherme de Almeida, Yixin Wang, Lin Han, Xin Yang, Marco Labagnara, Sahand Jamal Rahi, Carly Kempster, Alice Pollitt, Leon Espinosa, Tâm Mignot, Jan Moritz Middeke, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe Bai, Noah F. Greenwald, David Van Valen, Erin Weisbart, Beth A. Cimini, Zhuoshi Li, Chao Zuo, Oscar Brück, Gary D. Bader, Bo Wang</li>
<li>for: 单细胞分析中的细胞分割步骤是 kritical。</li>
<li>methods: 这些方法通常适应特定的modalities或需要手动参数调整来适应不同的实验设置。</li>
<li>results: 这个benchmark和改进的算法可以无需手动参数调整地应用于多种微显技术和组织类型的细胞图像。<details>
<summary>Abstract</summary>
Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyperparameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.
</details>
<details>
<summary>摘要</summary>
cell 分 segmentation 是单细胞分析中的关键步骤，exist 的 cell 分 segmentation 方法 oftentailored 到特定Modalities 或需要手动 intervene  specify  hyperparameters  in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods, but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.Note that the word "Transformer" in the original text was translated as "Transformer-based" in Simplified Chinese, as there is no direct equivalent of the word "Transformer" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Propagation-over-Conditional-Independence-Graphs"><a href="#Knowledge-Propagation-over-Conditional-Independence-Graphs" class="headerlink" title="Knowledge Propagation over Conditional Independence Graphs"></a>Knowledge Propagation over Conditional Independence Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05857">http://arxiv.org/abs/2308.05857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: 本研究旨在提出ci图表知识传播算法，用于从domain topology中提取有用信息。</li>
<li>methods: 本研究使用ci图表模型，并提出了一种基于ci图表的知识传播算法。</li>
<li>results: 实验结果表明，该算法在公共 disponibles的cora和pubmed datasets上有所提高，与现有技术相比。<details>
<summary>Abstract</summary>
Conditional Independence (CI) graph is a special type of a Probabilistic Graphical Model (PGM) where the feature connections are modeled using an undirected graph and the edge weights show the partial correlation strength between the features. Since the CI graphs capture direct dependence between features, they have been garnering increasing interest within the research community for gaining insights into the systems from various domains, in particular discovering the domain topology. In this work, we propose algorithms for performing knowledge propagation over the CI graphs. Our experiments demonstrate that our techniques improve upon the state-of-the-art on the publicly available Cora and PubMed datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>conditional independence (CI) 图是一种特殊的概率图模型 (PGM)，其中特征连接使用无向图表示，边重量表示特征之间的半相关度。由于 CI 图表示直接相互关联的特征，因此在不同领域中的系统研究中备受关注，特别是发现领域 то波动。在这项工作中，我们提出了在 CI 图上进行知识传播的算法。我们的实验表明，我们的技术在公共可用的 Cora 和 PubMed 数据集上超过了当前最佳的状况。
</details></li>
</ul>
<hr>
<h2 id="CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services"><a href="#CSPM-A-Contrastive-Spatiotemporal-Preference-Model-for-CTR-Prediction-in-On-Demand-Food-Delivery-Services" class="headerlink" title="CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services"></a>CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08446">http://arxiv.org/abs/2308.08446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guyu Jiang, Xiaoyun Li, Rongrong Jing, Ruoqi Zhao, Xingliang Ni, Guodong Cao, Ning Hu</li>
<li>for: 这篇论文的目的是为了提高在在线快递食品平台上的点击率预测（CTR）。</li>
<li>methods: 这篇论文使用了三个模块：对比性空间时间表示学习（CSRL）、空间时间喜好EXTRACTOR（StPE）和空间时间信息过滤器（StIF）。CSRL使用了对比学习框架来生成搜索行为中的空间时间活动表示（SAR）。StPE使用了SAR来活化用户的不同的位置和时间相关的喜好，使用多头注意机制。StIF将SARintegrated into a gating network来自动捕捉重要的隐藏空间时间效果。</li>
<li>results: 在两个大规模的工业数据集上进行了广泛的实验，并证明了CSPM的状态之前性表现。尤其是，CSPM已经成功部署在阿里巴巴的在线快递食品平台Ele.me上，导致了显著的0.88%提升Click-through rate，这有重要的商业意义。<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction is a crucial task in the context of an online on-demand food delivery (OFD) platform for precisely estimating the probability of a user clicking on food items. Unlike universal e-commerce platforms such as Taobao and Amazon, user behaviors and interests on the OFD platform are more location and time-sensitive due to limited delivery ranges and regional commodity supplies. However, existing CTR prediction algorithms in OFD scenarios concentrate on capturing interest from historical behavior sequences, which fails to effectively model the complex spatiotemporal information within features, leading to poor performance. To address this challenge, this paper introduces the Contrastive Sres under different search states using three modules: contrastive spatiotemporal representation learning (CSRL), spatiotemporal preference extractor (StPE), and spatiotemporal information filter (StIF). CSRL utilizes a contrastive learning framework to generate a spatiotemporal activation representation (SAR) for the search action. StPE employs SAR to activate users' diverse preferences related to location and time from the historical behavior sequence field, using a multi-head attention mechanism. StIF incorporates SAR into a gating network to automatically capture important features with latent spatiotemporal effects. Extensive experiments conducted on two large-scale industrial datasets demonstrate the state-of-the-art performance of CSPM. Notably, CSPM has been successfully deployed in Alibaba's online OFD platform Ele.me, resulting in a significant 0.88% lift in CTR, which has substantial business implications.
</details>
<details>
<summary>摘要</summary>
Click-through rate (CTR) 预测是在在线快递食品平台上关键的任务，准确地估计用户会点击食品项。不同于通用电商平台如淘宝和amazon，用户在食品平台上的行为和兴趣更加地受到地域和时间影响，因为交通范围和地域商品供应有限。然而，现有的 CTRL 预测算法在食品平台场景中集中在捕捉历史行为序列中的兴趣，而不能有效地模型特有的空间时间信息，导致表现不佳。为解决这个挑战，本文引入了不同搜索状态下的 Contrastive Sres，使用三个模块：对比空间时间表示学习（CSRL）、空间时间偏好提取器（StPE）和空间时间信息筛选器（StIF）。CSRL 利用对比学习框架生成一个空间时间活动表示（SAR） для搜索行为。StPE 使用 SAR 来激活用户的不同地域时间上的偏好，使用多头注意机制。StIF 将 SAR integrated into a gating network 自动捕捉特有的空间时间效应。经验表明，CSPM 在两个大规模的业务数据集上达到了领先的性能，并在阿里巴巴在线食品平台 Ele.me 上部署成功，导致了显著的0.88%增加 Click-through rate，这有substantial商业意义。
</details></li>
</ul>
<hr>
<h2 id="GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks"><a href="#GaborPINN-Efficient-physics-informed-neural-networks-using-multiplicative-filtered-networks" class="headerlink" title="GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks"></a>GaborPINN: Efficient physics informed neural networks using multiplicative filtered networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05843">http://arxiv.org/abs/2308.05843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinquan Huang, Tariq Alkhalifah<br>for: 这篇论文的目的是提出一种改进的物理学 Informed Neural Network（PINN）方法，以加速冲击波场的计算。methods: 这篇论文使用了多个技术，包括physics-informed neural networks（PINNs）和Gabor基函数。它们在训练过程中嵌入了一些已知的冲击波场特征，如频率，以提高快速度。results: 论文的实验结果表明，使用GaborPINN方法可以大大提高冲击波场的计算速度，比传统的PINN方法快两个数量级。<details>
<summary>Abstract</summary>
The computation of the seismic wavefield by solving the Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. Physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. To address this problem, we propose a modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of the wavefield in training, e.g., frequency, to achieve much faster convergence. Specifically, we use the Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. Meanwhile, we incorporate prior information on the frequency of the wavefield into the design of the method to mitigate the influence of the discontinuity of the represented wavefield by GaborPINN. The proposed method achieves up to a two-magnitude increase in the speed of convergence as compared with conventional PINNs.
</details>
<details>
<summary>摘要</summary>
computations of seismic wavefield by solving Helmholtz equation is crucial to many practical applications, e.g., full waveform inversion. physics-informed neural networks (PINNs) provide functional wavefield solutions represented by neural networks (NNs), but their convergence is slow. to address this problem, we propose modified PINN using multiplicative filtered networks, which embeds some of the known characteristics of wavefield in training, e.g., frequency, to achieve much faster convergence. specifically, we use Gabor basis function due to its proven ability to represent wavefields accurately and refer to the implementation as GaborPINN. meanwhile, we incorporate prior information on frequency of wavefield into the design of method to mitigate influence of discontinuity of represented wavefield by GaborPINN. proposed method achieves up to two-magnitude increase in speed of convergence as compared with conventional PINNs.
</details></li>
</ul>
<hr>
<h2 id="FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks"><a href="#FLShield-A-Validation-Based-Federated-Learning-Framework-to-Defend-Against-Poisoning-Attacks" class="headerlink" title="FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks"></a>FLShield: A Validation Based Federated Learning Framework to Defend Against Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05832">http://arxiv.org/abs/2308.05832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsanul Kabir, Zeyu Song, Md Rafi Ur Rashid, Shagufta Mehnaz</li>
<li>for: 本研究旨在提出一种新的联合学习（Federated Learning，FL）框架，以保障FL系统的安全性和可靠性。</li>
<li>methods: 本研究使用了本地模型的有益数据来验证本地模型的可靠性，而不是依赖服务器访问spotless数据集，这种做法和FL的基本原则不匹配。</li>
<li>results: 研究人员通过对不同情况下的FLShield框架进行了广泛的实验，并证明了FLShield框架能够有效地防范各种毒素和后门攻击，同时保护本地数据的隐私。<details>
<summary>Abstract</summary>
Federated learning (FL) is revolutionizing how we learn from data. With its growing popularity, it is now being used in many safety-critical domains such as autonomous vehicles and healthcare. Since thousands of participants can contribute in this collaborative setting, it is, however, challenging to ensure security and reliability of such systems. This highlights the need to design FL systems that are secure and robust against malicious participants' actions while also ensuring high utility, privacy of local data, and efficiency. In this paper, we propose a novel FL framework dubbed as FLShield that utilizes benign data from FL participants to validate the local models before taking them into account for generating the global model. This is in stark contrast with existing defenses relying on server's access to clean datasets -- an assumption often impractical in real-life scenarios and conflicting with the fundamentals of FL. We conduct extensive experiments to evaluate our FLShield framework in different settings and demonstrate its effectiveness in thwarting various types of poisoning and backdoor attacks including a defense-aware one. FLShield also preserves privacy of local data against gradient inversion attacks.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是如何改变我们如何从数据中学习的革命。随着其 Popularity 的增长，它现在在许多安全关键领域，如自动驾驶和医疗，中使用。由于千余名参与者可以在这种合作环境中贡献，因此保持安全性和可靠性的系统是挑战。这 Heightens 需要设计安全可靠的 FL 系统，能够抵御恶意参与者的行为，同时保持本地数据隐私和效率。在这篇论文中，我们提出了一种新的 FL 框架，名为 FLShield，它利用 FL 参与者的善意数据来验证本地模型，然后将其作为全球模型生成。这与现有防御方法，它们基于服务器访问干净的数据集的假设，不同。我们进行了广泛的实验来评估我们的 FLShield 框架在不同的设置下的效果，并证明它在不同类型的毒素和后门攻击中具有有效性。FLShield 还保持本地数据隐私性免受Gradient Inversion攻击。
</details></li>
</ul>
<hr>
<h2 id="Neural-Progressive-Meshes"><a href="#Neural-Progressive-Meshes" class="headerlink" title="Neural Progressive Meshes"></a>Neural Progressive Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05741">http://arxiv.org/abs/2308.05741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Chun Chen, Vladimir G. Kim, Noam Aigerman, Alec Jacobson</li>
<li>for:  efficiently transmit large geometric data (e.g., 3D meshes) over the Internet</li>
<li>methods: subdivision-based encoder-decoder architecture trained on a large collection of surfaces, with progressive transmission of residual features</li>
<li>results: outperforms baselines in terms of compression ratio and reconstruction quality<details>
<summary>Abstract</summary>
The recent proliferation of 3D content that can be consumed on hand-held devices necessitates efficient tools for transmitting large geometric data, e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a challenge to storage as well as transmission bandwidth, and level-of-detail techniques are often used to transmit an asset using an appropriate bandwidth budget. It is especially desirable for these methods to transmit data progressively, improving the quality of the geometry with more data. Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space. We learn this space using a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces. We further observe that additional residual features can be transmitted progressively between intermediate levels of subdivision that enable the client to control the tradeoff between bandwidth cost and quality of reconstruction, providing a neural progressive mesh representation. We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details>
<details>
<summary>摘要</summary>
Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns across different shapes, and can be effectively represented with a shared learned generative space. We use a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces to learn this space. Additionally, we observe that residual features can be transmitted progressively between intermediate levels of subdivision, enabling the client to control the tradeoff between bandwidth cost and quality of reconstruction. This provides a neural progressive mesh representation.We evaluate our method on a diverse set of complex 3D shapes and demonstrate that it outperforms baselines in terms of compression ratio and reconstruction quality.
</details></li>
</ul>
<hr>
<h2 id="Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics"><a href="#Zero-Grads-Ever-Given-Learning-Local-Surrogate-Losses-for-Non-Differentiable-Graphics" class="headerlink" title="Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics"></a>Zero Grads Ever Given: Learning Local Surrogate Losses for Non-Differentiable Graphics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05739">http://arxiv.org/abs/2308.05739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Fischer, Tobias Ritschel</li>
<li>for: 解决 Graphics 中的Gradient-based优化问题，因为现有的搜索方法无法处理undefined或zero gradients。</li>
<li>methods: 提出了一种自动化替换损失函数的框架，即ZeroGrads，通过学习一个神经网络来 aproximate objective function，并使用这个神经网络来 differentiate through arbitrary black-box graphics pipelines。</li>
<li>results: 实现了在online和self-supervised的情况下，使用 actively smoothed version of the objective 进行训练，并且可以在 tractable run-times 和competitive performance下解决多个非对称、非 differentiable black-box problem in Graphics，如视力rendering、排序参数空间和物理驱动动画优化等。<details>
<summary>Abstract</summary>
Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a "surrogate" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, the surrogate, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to more traditional algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables.
</details>
<details>
<summary>摘要</summary>
“梯度基本优化现在在图形处理中广泛应用，但它无法应用于无定义或 zeros 梯度的问题。为了缺 Bibliography 这个问题，我们可以手动替换损失函数，使其成为可导的。我们的提议的框架，ZeroGrads，可以自动实现这个过程，它通过学习一个神经网络来模拟目标函数，并使用这个模拟来分子 Graphics pipeline 中的任意黑盒问题。我们在训练过程中使用在目标函数上实时缓和的版本，并且强调本集成性，使得模拟的能量集中在当前训练集中。我们的整个训练过程是在线进行的，并且是自动的，不需要预计算数据或预训练模型。由于评估目标函数的成本 relativity 高（它需要一个完整的渲染或 simulator 运行），我们开发了一种有效的采样方案，使得我们可以在可耗时间和竞争性的情况下实现高性能。我们在不同的非等式、非导数的黑盒问题上进行了优化，例如渲染中的可见性、procedural 模型中的分配空间和物理驱动的动画中的优化问题。与传统算法相比，我们的方法可以很好地扩展到更高的维度，我们在问题中的35k个相互关联变量上进行了示例。”
</details></li>
</ul>
<hr>
<h2 id="Follow-Anything-Open-set-detection-tracking-and-following-in-real-time"><a href="#Follow-Anything-Open-set-detection-tracking-and-following-in-real-time" class="headerlink" title="Follow Anything: Open-set detection, tracking, and following in real-time"></a>Follow Anything: Open-set detection, tracking, and following in real-time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05737">http://arxiv.org/abs/2308.05737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alaamaalouf/followanything">https://github.com/alaamaalouf/followanything</a></li>
<li>paper_authors: Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallabhula, Makram Chahine, Daniel M. Vogt, Robert J. Wood, Antonio Torralba, Daniela Rus</li>
<li>for: 本研究旨在开发一种可以在实时控制循环中跟踪任何对象的机器人系统。</li>
<li>methods: 该系统基于大规模预训练模型（基础模型），可以在实时控制循环中检测、分割和跟踪对象，并且可以考虑 occlusion 和对象重新出现。</li>
<li>results: 研究人员在一架微型飞行器上部署了该系统，并成功地跟踪了各种对象。系统可以在一个简单的 laptop 上运行，并且可以达到 6-20 帧每秒的throughput。<details>
<summary>Abstract</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at https://github.com/alaamaalouf/FollowAnything . We also encourage the reader the watch our 5-minutes explainer video in this https://www.youtube.com/watch?v=6Mgt3EPytrw .
</details>
<details>
<summary>摘要</summary>
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed “follow anything” (FAn), is an open-vocabulary and multimodal model — it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of interest in a real-time control loop. FAn can be deployed on a laptop with a lightweight (6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To enable rapid adoption, deployment, and extensibility, we open-source all our code on our project webpage at <https://github.com/alaamaalouf/FollowAnything>. We also encourage the reader to watch our 5-minutes explainer video in this <https://www.youtube.com/watch?v=6Mgt3EPytrw>.
</details></li>
</ul>
<hr>
<h2 id="PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers"><a href="#PDE-Refiner-Achieving-Accurate-Long-Rollouts-with-Neural-PDE-Solvers" class="headerlink" title="PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers"></a>PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05732">http://arxiv.org/abs/2308.05732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Lippe, Bastiaan S. Veeling, Paris Perdikaris, Richard E. Turner, Johannes Brandstetter<br>for:This paper aims to improve the accuracy and stability of deep neural network-based solution techniques for partial differential equations (PDEs) by addressing the neglect of non-dominant spatial frequency information.methods:The authors use a large-scale analysis of common temporal rollout strategies and draw inspiration from recent advances in diffusion models to introduce a novel model class called PDE-Refiner, which uses a multistep refinement process to accurately model all frequency components of PDE solutions.results:The authors validate PDE-Refiner on challenging benchmarks of complex fluid dynamics and demonstrate stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. Additionally, PDE-Refiner is shown to greatly enhance data efficiency by implicitly inducing a novel form of spectral data augmentation, and the authors demonstrate an accurate and efficient assessment of the model’s predictive uncertainty.Here is the simplified Chinese text:for: 这篇论文目的是提高深度神经网络基于 partial differential equation (PDE) 的解决方法的精度和稳定性。methods: 作者使用大规模的时间滚动策略分析，并启发自latest advances in diffusion models ，引入了一种新的模型类叫 PDE-Refiner，该模型使用多步增强过程来准确地模型 PDE 解的所有频率组成部分。results: 作者验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，并显示了稳定和准确的滚动性能，常常超越当前的模型，包括神经网络、数学、和混合神经网络-数学模型。此外， PDE-Refiner 能够大幅提高数据效率，因为净化目标意味着在数据增强过程中隐式地引入了一种新的频率数据增强。最后， PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计模型在滚动过程中的不确定性。<details>
<summary>Abstract</summary>
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of complex fluid dynamics, demonstrating stable and accurate rollouts that consistently outperform state-of-the-art models, including neural, numerical, and hybrid neural-numerical architectures. We further demonstrate that PDE-Refiner greatly enhances data efficiency, since the denoising objective implicitly induces a novel form of spectral data augmentation. Finally, PDE-Refiner's connection to diffusion models enables an accurate and efficient assessment of the model's predictive uncertainty, allowing us to estimate when the surrogate becomes inaccurate.
</details>
<details>
<summary>摘要</summary>
时间依赖的 partial differential equations (PDEs) 在科学和工程中具有广泛的应用。最近，主要由于传统解决方案的计算成本高，深度神经网络基于的 surrogate 获得了更多的关注。然而，实际应用中，这些神经网络 PDE 解决器的实用性取决于它们能够提供稳定、准确的预测，这是一个非常困难的问题。在这种情况下，我们提出了一项大规模分析 temporal rollout 策略，发现忽略非主要空间频率信息，通常与 PDE 解的高频成分相关，是主要的障碍物，限制稳定、准确的 rollout 性能。基于这些发现，我们启发自 recent advances in diffusion models，提出了 PDE-Refiner; 一种新的模型类，可以更好地模拟所有频率组成部分。我们验证 PDE-Refiner 在复杂的流体动力学 benchmark 上，可以实现稳定和准确的 rollouts，常常超越当前模型，包括神经网络、数值和混合神经网络-数值模型。此外，PDE-Refiner 可以大幅提高数据效率，因为净化目标隐式地导入了一种新的 spectral data augmentation。最后，PDE-Refiner 的连接到 diffusion models 使得可以准确地评估模型的预测不确定性，从而估计 surrogate 是否准确。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review"><a href="#Rethinking-Integration-of-Prediction-and-Planning-in-Deep-Learning-Based-Automated-Driving-Systems-A-Review" class="headerlink" title="Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review"></a>Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05731">http://arxiv.org/abs/2308.05731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steffen Hagedorn, Marcel Hallgarten, Martin Stoll, Alexandru Condurache</li>
<li>for: 这篇论文主要写于自动驾驶技术的发展，具体来说是关于 Prediction、Planning 和 Integrated Prediction and Planning 模型的系统性评估。</li>
<li>methods: 论文使用了深度学习技术来实现 Prediction 和 Planning 模型，并对不同的集成方法进行了系统性的比较和分析。</li>
<li>results: 论文发现了不同集成方法在自动驾驶中的优缺点，并指出了未来研究的方向和挑战。<details>
<summary>Abstract</summary>
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details>
<details>
<summary>摘要</summary>
（简化中文）自动驾驶有可能改变个人、公共和货物运输方式。除了巨大的感知挑战以外，自动驾驶还包括规划安全、舒适和有效的运动轨迹。为促进安全和进步，许多工作依赖于周围交通的未来运动预测。现有的自动驾驶系统通常将预测和规划作为独立的两个任务进行处理。这种方法虽然考虑了周围交通对ego汽车的影响，但是忽略了ego汽车行为对交通参与者的反应。 latest works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. We systematically review state-of-the-art deep learning-based prediction, planning, and integrated prediction and planning models. Different facets of the integration ranging from model architecture and model design to behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration methods. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.
</details></li>
</ul>
<hr>
<h2 id="EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis"><a href="#EXPRESSO-A-Benchmark-and-Analysis-of-Discrete-Expressive-Speech-Resynthesis" class="headerlink" title="EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis"></a>EXPRESSO: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05725">http://arxiv.org/abs/2308.05725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tu Anh Nguyen, Wei-Ning Hsu, Antony D’Avirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, Emmanuel Dupoux</li>
<li>for: 本研究旨在开发一个高质量的自然语言表达 speech 数据集，用于无文本 speech 生成。</li>
<li>methods: 研究人员使用了自我超级vised学习的方法，将低比特率的精度单元学习到 speech 中，以捕捉expressive aspect of speech。</li>
<li>results: 研究人员在 Expresso 数据集上进行了一系列的evalution，并结果表明，这种方法可以实现高质量的 expressive speech 生成。Here’s a more detailed explanation of each point:</li>
<li>for: The paper is aimed at developing a high-quality expressive speech dataset for textless speech synthesis.</li>
<li>methods: The researchers use self-supervised learning methods to learn low-bitrate discrete units that can capture expressive aspects of speech, such as prosody and voice styles.</li>
<li>results: The researchers evaluate the effectiveness of their approach on the Expresso dataset, which includes both read speech and improvised dialogues in 26 spontaneous expressive styles. The results show that the method can achieve high-quality expressive speech synthesis.<details>
<summary>Abstract</summary>
Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe (prosody, voice styles, non-verbal vocalization). The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce Expresso, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 spontaneous expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete encoders, and explore tradeoffs between quality, bitrate and invariance to speaker and style. All the dataset, evaluation metrics and baseline models are open source
</details>
<details>
<summary>摘要</summary>
近期研究表明，可以使用低比特率不同单元进行自我指导的高质量语音重建。这些单元可以捕捉到表达方面的声音特征，如声音态度、声音风格和非语言 vocalization，这些特征Difficult to transcribe。然而，目前这些方法的应用仍然受限于大多数语音重建数据集是阅读的，因此减少了自由和表达力。在这篇文章中，我们介绍了一个高质量表达语音数据集，即Expresso，该数据集包括了阅读语音和自由对话，并且在26种自然表达风格下进行了渲染。我们介绍了这个数据集的挑战和潜在性，并在表达重建中进行了一种表达编码和重建任务，以测试不同的自我指导精度单元的质量。我们使用了自动化 метри来评估重建质量，并 explore了不同精度单元的平衡点，包括质量、比特率和对 speaker和风格的不变性。所有的数据集、评估指标和基础模型都是开源的。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions"><a href="#Optimizing-Performance-of-Feedforward-and-Convolutional-Neural-Networks-through-Dynamic-Activation-Functions" class="headerlink" title="Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions"></a>Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05724">http://arxiv.org/abs/2308.05724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chinmay Rane, Kanishka Tyagi, Michael Manry</li>
<li>for: 本研究旨在探讨 activation functions 在 convolutional neural networks (CNNs) 中的影响，并提出了一种复杂的 Piece-wise Linear (PWL) 活动函数来取代 Relu 活动函数。</li>
<li>methods: 本研究使用了 PyTorch 框架进行实验，并对 shallow CNNs 和深度 CNNs 进行比较。</li>
<li>results: 研究发现，使用 PWL 活动函数可以大大提高 CNNs 的性能，并且在深度 CNNs 中提供了更多的可能性空间。<details>
<summary>Abstract</summary>
Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.
</details>
<details>
<summary>摘要</summary>
深度学习训练算法在最近几年内在多个领域取得了巨大成功，包括语音、文本、图像和视频等。随着更深层的提议，深度学习模型在152层以上的ResNet结构中得到了巨大成功。 however， shallow convolutional neural networks（CNN）仍然是一个活跃的研究领域，一些现象仍未得到解释。 activation functions在网络中具有重要的作用，它们提供了非线性性，使网络更加复杂。 ReLU activation function是最常用的activation function。我们在隐藏层使用复杂的 piece-wise linear（PWL）activation，并证明这些PWL activation在我们的网络中工作得更好than ReLU activation。我们还在PyTorch中对 shallow和深度CNN进行比较，以更加强化我们的论据。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control"><a href="#A-Comparison-of-Classical-and-Deep-Reinforcement-Learning-Methods-for-HVAC-Control" class="headerlink" title="A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control"></a>A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05711">http://arxiv.org/abs/2308.05711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marshall Wang, John Willes, Thomas Jiralerspong, Matin Moezzi</li>
<li>for: 这个论文旨在使用强化学习（RL）方法优化冷却空调系统的控制，提高系统性能，降低能源消耗，提高成本效益。</li>
<li>methods: 这篇论文使用了两种流行的 класси型和深度RL方法（Q-学习和深度Q-网络），在多个冷却空调环境下进行了比较。</li>
<li>results: 研究发现，RL方法可以在冷却空调系统中提高性能，降低能源消耗，且模型参数选择和奖励调整是RL agents配置的关键因素。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a promising approach for optimizing HVAC control. RL offers a framework for improving system performance, reducing energy consumption, and enhancing cost efficiency. We benchmark two popular classical and deep RL methods (Q-Learning and Deep-Q-Networks) across multiple HVAC environments and explore the practical consideration of model hyper-parameter selection and reward tuning. The findings provide insight for configuring RL agents in HVAC systems, promoting energy-efficient and cost-effective operation.
</details>
<details>
<summary>摘要</summary>
现代控制技术（Reinforcement Learning，RL）可以有效地优化冷暖空调系统的控制。RL提供了一个框架，可以提高系统性能，降低能源消耗，提高成本效益。我们在多个冷暖空调环境中对两种流行的古典RL和深度RL方法（Q-学习和深度Q网络）进行了比较，并探讨RL代理人在冷暖空调系统中的实用考虑和奖励调整。发现提供了RL代理人配置的指导，推动了能源减少和成本效益的操作。
</details></li>
</ul>
<hr>
<h2 id="Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning"><a href="#Shadow-Datasets-New-challenging-datasets-for-Causal-Representation-Learning" class="headerlink" title="Shadow Datasets, New challenging datasets for Causal Representation Learning"></a>Shadow Datasets, New challenging datasets for Causal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05707">http://arxiv.org/abs/2308.05707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jiagengzhu/Shadow-dataset-for-crl">https://github.com/Jiagengzhu/Shadow-dataset-for-crl</a></li>
<li>paper_authors: Jiageng Zhu, Hanchen Xie, Jianhua Wu, Jiazhi Li, Mahyar Khayatkhoei, Mohamed E. Hussein, Wael AbdAlmageed</li>
<li>for: 本研究的目的是探索语义因素之间的 causal 关系，以便进行更好的表征学习。</li>
<li>methods: 该研究使用了weakly supervised causal representation learning（CRL）方法，以解决高成本的标注问题。</li>
<li>results: 研究提出了两个新的数据集，以及对现有数据集的修改，以满足更复杂的 causal 图和更多的生成因素的需求。<details>
<summary>Abstract</summary>
Discovering causal relations among semantic factors is an emergent topic in representation learning. Most causal representation learning (CRL) methods are fully supervised, which is impractical due to costly labeling. To resolve this restriction, weakly supervised CRL methods were introduced. To evaluate CRL performance, four existing datasets, Pendulum, Flow, CelebA(BEARD) and CelebA(SMILE), are utilized. However, existing CRL datasets are limited to simple graphs with few generative factors. Thus we propose two new datasets with a larger number of diverse generative factors and more sophisticated causal graphs. In addition, current real datasets, CelebA(BEARD) and CelebA(SMILE), the originally proposed causal graphs are not aligned with the dataset distributions. Thus, we propose modifications to them.
</details>
<details>
<summary>摘要</summary>
发现 semantic 因素之间的 causal 关系是 representation learning 中一个emerging topic。大多数 causal representation learning（CRL）方法是完全supervised，这是因为标注成本太高。为解决这种限制，我们提出了弱标注 CRL 方法。为评估 CRL 性能，我们使用了四个现有的数据集：Pendulum、Flow、CelebA（BEARD）和 CelebA（SMILE）。然而，现有的 CRL 数据集受限于简单的图与少量生成因素。因此，我们提出了两个新的数据集，它们具有更多的多样化生成因素和更复杂的 causal 图。此外，原始的现实数据集 CelebA（BEARD）和 CelebA（SMILE）的 causal 图与数据分布不一致。因此，我们提出了修改。
</details></li>
</ul>
<hr>
<h2 id="Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient"><a href="#Hard-No-Box-Adversarial-Attack-on-Skeleton-Based-Human-Action-Recognition-with-Skeleton-Motion-Informed-Gradient" class="headerlink" title="Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient"></a>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05681">http://arxiv.org/abs/2308.05681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luyg45/hardnoboxattack">https://github.com/luyg45/hardnoboxattack</a></li>
<li>paper_authors: Zhengzhi Lu, He Wang, Ziyi Chang, Guoan Yang, Hubert P. H. Shum</li>
<li>for: 这 paper 探讨了 skeleton-based 人体活动识别系统 的攻击性评估问题。</li>
<li>methods: 该 paper 使用了一种新的攻击任务，即攻击者没有访问受害者模型或训练数据或标签。具体来说，它们首先学习了一个动作抽象空间，然后定义了一种对抗损失来计算一个新的攻击方向，称为skeleton-motion-informed（SMI）梯度。这个梯度包含了动作动态信息，与现有的梯度基于攻击方法不同。</li>
<li>results: 该 paper 的实验和比较结果表明，SMI 梯度可以在无框杆和转移基于黑盒 Setting 中提高攻击性和透明度。<details>
<summary>Abstract</summary>
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.
</details>
<details>
<summary>摘要</summary>
近期，基于骨架的人体活动识别方法已经被证明容易受到敌意攻击。然而，这些攻击方法都需要受害者（白盒攻击）、训练数据（传输基于攻击）或模型查询（黑盒攻击）的访问权限。这些需求都是非常限制的，这引发了对攻击性的评估。在这篇论文中，我们证明了这种攻击性确实存在。为此，我们提出了一个新的攻击任务：攻击者无法访问受害者的模型或训练数据或标签。我们称之为“困难无框攻击”（hard no-box attack）。我们首先学习了一个运动拟合，并定义了一种对抗损失来计算一个新的攻击Gradient，称之为skeleton-motion-informed（SMI）梯度。我们的梯度包含运动动力学信息，与现有的梯度基本攻击方法不同，它们计算损失梯度，假设每个数据维度独立。SMI梯度可以增强许多梯度基本攻击方法，导致一个新的无框攻击家族。我们的评估和比较表明，我们的方法对现有分类器 pose a real threat。它们还表明了SMI梯度的传播性和隐蔽性在无框和传输基于黑盒 Setting 中得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning"><a href="#Finding-Already-Debunked-Narratives-via-Multistage-Retrieval-Enabling-Cross-Lingual-Cross-Dataset-and-Zero-Shot-Learning" class="headerlink" title="Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning"></a>Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05680">http://arxiv.org/abs/2308.05680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iknoor Singh, Carolina Scarton, Xingyi Song, Kalina Bontcheva</li>
<li>for: 本研究的目的是探讨跨语言验证答案已经证明为假的故事的检测，以减少专业验证者的手动努力并为防止谣言的传播做出贡献。</li>
<li>methods: 本研究使用了一个新的数据集，该数据集包含了用于验证的检查答案和各种语言的社交媒体帖子，以便进行跨语言验证答案已经证明为假的故事的检测。</li>
<li>results: 研究发现，跨语言验证答案已经证明为假的故事的检测是一项具有挑战性的任务，而一些常用的跨语言预处理 transformer 模型也未能超越一个强的基于词语的基线（BM25）。然而，我们的多Stage检索框架在大多数情况下能够超越 BM25，并具有跨频率和零扩展学习的能力。<details>
<summary>Abstract</summary>
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk retrieval task into refinement and re-ranking stages. Results show that the task of cross-lingual retrieval of already debunked narratives is challenging and off-the-shelf Transformer models fail to outperform a strong lexical-based baseline (BM25). Nevertheless, our multistage retrieval framework is robust, outperforming BM25 in most scenarios and enabling cross-domain and zero-shot learning, without significantly harming the model's performance.
</details>
<details>
<summary>摘要</summary>
这个任务是检索已经证伪的故事，目的是检测已经被ifact-checked的故事。成功检测已经证伪的故事不仅可以减少专业ifact-checker的手动努力，还可以减速谣言的传播。但因为数据不足，这个问题尚未得到充分研究，特别是跨语言任务，即在不同语言的 онлайн帖子被检查时，检索ifact-checking文章的跨语言任务。这篇论文填补这一漏洞，通过以下三个方法：1. 创建一个新的数据集，用于启动研究跨语言检索已经证伪的故事。2. 进行了广泛的实验，以benchmark fine-tuned和off-the-shelf多语言预训练Transformer模型。3. 提出了一个新的多阶段框架，将跨语言检索已经证伪的故事任务分为两个阶段：精度阶段和重新排序阶段。结果表明，跨语言检索已经证伪的故事是一个具有挑战性的任务，off-the-shelf Transformer模型无法超过一个强的基于词语的基准值（BM25）。然而，我们的多阶段检索框架具有坚固性，在大多数情况下超过BM25，并且允许跨频域和零shot学习，无需对模型性能产生重要的影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.LG_2023_08_11/" data-id="clly3dvzh006a098882x51shi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_11" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/11/cs.SD_2023_08_11/" class="article-date">
  <time datetime="2023-08-10T16:00:00.000Z" itemprop="datePublished">2023-08-11</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/11/cs.SD_2023_08_11/">cs.SD - 2023-08-11 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-Joint-Speech-Text-Representations-Without-Alignment"><a href="#Improving-Joint-Speech-Text-Representations-Without-Alignment" class="headerlink" title="Improving Joint Speech-Text Representations Without Alignment"></a>Improving Joint Speech-Text Representations Without Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06125">http://arxiv.org/abs/2308.06125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho</li>
<li>for: 这个论文旨在提出一种基于modal space的文本生成方法，用于处理 speech和text两种不同的模式。</li>
<li>methods: 该方法使用joint speech-text encoder，通过在modal space中对文本和语音进行共同表示，以减少模型的参数量。</li>
<li>results: 该方法可以自动解决模式长度不同的问题，并且在下游WER测试中显示了良好的表现，包括单语言和多语言系统。<details>
<summary>Abstract</summary>
The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.
</details>
<details>
<summary>摘要</summary>
最近一年内，文本承词生成技术呈现了宏まScale的进步，基于跨Modal表示空间的想法，在这个空间中，文本和图像领域都被合并表示。在ASR中，这个想法得到应用，通过将语音和文本域合并编码，可以让模型 Parameters scale 到非常大的规模。虽然这些方法显示了承诺，但它们需要特殊地处理语音和文本序列长度的差异，通过上映或者显式对齐模型。在这种工作中，我们提供证据，表明Join speech-text编码器可以自然地实现多Modal的一致表示，而不需要注意序列长度。我们还 argues that consistency损失可以宽容序列长度差异，并且可以假设最佳对齐。我们示出，这种损失可以提高下游 WER 在大参数 monolingual 和 multilingual 系统中。
</details></li>
</ul>
<hr>
<h2 id="Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping"><a href="#Lip2Vec-Efficient-and-Robust-Visual-Speech-Recognition-via-Latent-to-Latent-Visual-to-Audio-Representation-Mapping" class="headerlink" title="Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping"></a>Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06112">http://arxiv.org/abs/2308.06112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Haithem Boussaid, Ebtessam Almazrouei, Merouane Debbah</li>
<li>For: 这个论文的目的是提出一种简单的方法，以便在视频序列中进行舌头语音识别（VSR）任务。这种方法可以在训练集外的挑战性enario中表现出色，而不需要大量的标注数据。* Methods: 这个论文使用了一种名为Lip2Vec的简单方法，该方法基于学习一个先验模型。该网络将视频序列中的舌头编码后的干扰表示与其对应的音频对的干扰表示进行映射。然后，使用一个市场上的音频识别模型来解码音频，并将其转化为文本。* Results: 根据LRS3数据集的测试结果，这种方法可以与完全监督学习方法相比，达到26个WER的水平。与State-of-the-Art（SoTA）方法不同，我们的模型在VoxCeleb测试集上保持了合理的性能。<details>
<summary>Abstract</summary>
Visual Speech Recognition (VSR) differs from the common perception tasks as it requires deeper reasoning over the video sequence, even by human experts. Despite the recent advances in VSR, current approaches rely on labeled data to fully train or finetune their models predicting the target speech. This hinders their ability to generalize well beyond the training set and leads to performance degeneration under out-of-distribution challenging scenarios. Unlike previous works that involve auxiliary losses or complex training procedures and architectures, we propose a simple approach, named Lip2Vec that is based on learning a prior model. Given a robust visual speech encoder, this network maps the encoded latent representations of the lip sequence to their corresponding latents from the audio pair, which are sufficiently invariant for effective text decoding. The generated audio representation is then decoded to text using an off-the-shelf Audio Speech Recognition (ASR) model. The proposed model compares favorably with fully-supervised learning methods on the LRS3 dataset achieving 26 WER. Unlike SoTA approaches, our model keeps a reasonable performance on the VoxCeleb test set. We believe that reprogramming the VSR as an ASR task narrows the performance gap between the two and paves the way for more flexible formulations of lip reading.
</details>
<details>
<summary>摘要</summary>
视觉语音识别（VSR）与常见的观察任务不同，因为它需要对视频序列进行更深入的理解，即使人类专家也需要这样做。尽管最近有大量的进步在VSR方面，但现在的方法仍然依赖于标注数据来完全训练或微调其模型，以预测目标语音。这会导致其在不同于训练集的情况下表现不佳，并且会导致性能下降。与之前的工作不同，我们提出了一种简单的方法，即Lip2Vec，它基于学习一个先验模型。给定一个强大的视觉语音编码器，这个网络将编码的舌唇序列的 latent 表示与它们对应的 audio 对应的 latent 表示进行映射，这些表示够具有效果的文本解码。生成的音频表示然后被解码成文本使用一个可用的 Audio Speech Recognition（ASR）模型。我们提出的模型与完全监督学习方法在 LRS3 数据集上比较 favorably，实现 26 WER。与 SoTA 方法不同，我们的模型在 VoxCeleb 测试集上保持了合理的性能。我们认为将 VSR 转换为 ASR 任务，将两者之间的性能差减少，并且开创了更 flexible 的舌唇读取形式。
</details></li>
</ul>
<hr>
<h2 id="An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition"><a href="#An-Autoethnographic-Exploration-of-XAI-in-Algorithmic-Composition" class="headerlink" title="An Autoethnographic Exploration of XAI in Algorithmic Composition"></a>An Autoethnographic Exploration of XAI in Algorithmic Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06089">http://arxiv.org/abs/2308.06089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashley Noel-Hirst, Nick Bryan-Kinns</li>
<li>for: 这篇论文旨在探讨如何使用可解释的人工智能（XAI）生成模型来创作传统音乐。</li>
<li>methods: 本研究使用MeasureVAE生成模型，该模型具有可解释的秘密维度，并在爱尔兰传统音乐上进行训练。</li>
<li>results: 研究发现，在音乐创作过程中，探索性的音乐创作 workflow 会强调音乐训练数据中的音乐特征，而不是生成模型本身的特征。这种应用XAI模型在创作过程中的可能性可能会扩展到更复杂和多样化的工作流程。<details>
<summary>Abstract</summary>
Machine Learning models are capable of generating complex music across a range of genres from folk to classical music. However, current generative music AI models are typically difficult to understand and control in meaningful ways. Whilst research has started to explore how explainable AI (XAI) generative models might be created for music, no generative XAI models have been studied in music making practice. This paper introduces an autoethnographic study of the use of the MeasureVAE generative music XAI model with interpretable latent dimensions trained on Irish folk music. Findings suggest that the exploratory nature of the music-making workflow foregrounds musical features of the training dataset rather than features of the generative model itself. The appropriation of an XAI model within an iterative workflow highlights the potential of XAI models to form part of a richer and more complex workflow than they were initially designed for.
</details>
<details>
<summary>摘要</summary>
machine learning模型可以生成复杂的音乐，从民族音乐到古典音乐。但当前的生成音乐AI模型通常难以理解和控制有意义的方式。研究已经开始探索如何创建音乐XAI生成模型，但没有任何生成XAI模型在音乐创作实践中被研究。本文介绍了一个自传式研究，使用MeasureVAE生成音乐XAI模型，具有可解释的幂等维度，在爱尔兰传统音乐上进行训练。发现结果表明，音乐创作工作流程的探索性强调了训练集音乐特征而不是生成模型本身的特征。将XAI模型包含在迭代工作流程中，表明XAI模型可以成为更加丰富和复杂的工作流程的一部分。
</details></li>
</ul>
<hr>
<h2 id="Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model"><a href="#Audio-is-all-in-one-speech-driven-gesture-synthetics-using-WavLM-pre-trained-model" class="headerlink" title="Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model"></a>Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05995">http://arxiv.org/abs/2308.05995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li<br>for:* 这篇论文旨在创造数字人类的合作语言姿势，以解决现有的挑战，包括复杂的语音、语义和人性等因素。methods:* 该论文提出了一种基于扩散的DiffMotion-v2模型，利用RawSpeech音频 directly生成个性化和风格化的全身语姿，不需要复杂的多Modal处理和手动标注。results:* 经验证明，DiffMotion-v2模型可以生成自然的语姿，并且可以适应不同的风格和人性特征。<details>
<summary>Abstract</summary>
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acoustic and semantic features but also conveys personality traits, emotions, and more subtle information related to accompanying gestures, we pioneer the adaptation of WavLM, a large-scale pre-trained model, to extract low-level and high-level audio information. Secondly, we introduce an adaptive layer norm architecture in the transformer-based layer to learn the relationship between speech information and accompanying gestures. Extensive subjective evaluation experiments are conducted on the Trinity, ZEGGS, and BEAT datasets to confirm the WavLM and the model's ability to synthesize natural co-speech gestures with various styles.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的虚拟人物创造领域中的协调姿势生成技术是一个emerging领域。先前的研究使用了音响和语义信息作为输入，采用分类方法来确定人的ID和情绪，以驱动协调姿势生成。然而，这一领域仍面临着重大挑战。这些挑战不仅包括协调姿势、音响和语义之间的细微互动，还包括人性、情绪和其他一些重要而不那么明确的因素。本文介绍了“diffmotion-v2”，一种基于 transformer 架构的 speech-conditional 协同扩散型生成模型，使用 WavLM 预训练模型。该模型可以通过 Raw speech 音频alone 生成具有个性化和风格化特点的全身协调姿势，无需进行复杂的多Modal 处理和手动标注。首先，我们认为 speech 音频不仅包含了音响和语义特征，还拥有人性特征、情绪特征和更为细微的协调姿势相关信息。因此，我们采用 WavLM 预训练模型来提取低级和高级 audio 信息。其次，我们引入了 transformer 架构中的 adaptive layer norm 层，以学习 speech 信息和协调姿势之间的关系。我们对 Trinity、ZEGGS 和 BEAT 等三个 dataset 进行了许多主观评估实验，以确认 WavLM 和模型的能力以生成自然的协调姿势。
</details></li>
</ul>
<hr>
<h2 id="Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection"><a href="#Advancing-the-study-of-Large-Scale-Learning-in-Overlapped-Speech-Detection" class="headerlink" title="Advancing the study of Large-Scale Learning in Overlapped Speech Detection"></a>Advancing the study of Large-Scale Learning in Overlapped Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05987">http://arxiv.org/abs/2308.05987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaohui Yin, Jingguang Tian, Xinhui Hu, Xinkang Xu</li>
<li>for: 多个党人对话分析中的干扰语音检测（OSD）是一个重要的应用领域，但现有的大多数OSD模型都是基于特定的数据集进行训练和评估，限制了这些模型的应用场景。</li>
<li>methods: 我们提出了大规模学习（LSL）在OSD任务中的应用，并设计了一种16K单annelOSD模型。我们使用了522小时不同语言和风格的标注音频作为大规模数据集，并进行了严格的比较实验来评估LSL在OSD任务中的效果和不同深度神经网络基于OSD模型的性能。</li>
<li>results: 我们的实验结果表明，LSL可以显著提高OSD模型的性能和鲁棒性，并且CF-OSD与LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别达到了80.8%和52.0%，创造了当前最佳的16K单annelOSD模型。<details>
<summary>Abstract</summary>
Overlapped Speech Detection (OSD) is an important part of speech applications involving analysis of multi-party conversations. However, Most of the existing OSD models are trained and evaluated on specific dataset, which limits the application scenarios of these models. In order to solve this problem, we conduct a study of large-scale learning (LSL) in OSD and propose a more general 16K single-channel OSD model. In our study, 522 hours of labeled audio in different languages and styles are collected and used as the large-scale dataset. Rigorous comparative experiments are designed and used to evaluate the effectiveness of LSL in OSD task and the performance of OSD models based on different deep neural networks. The results show that LSL can significantly improve the performance and robustness of OSD models, and the OSD model based on Conformer (CF-OSD) with LSL is currently the best 16K single-channel OSD model. Moreover, the CF-OSD with LSL establishes a state-of-the-art performance with a F1-score of 80.8% and 52.0% on the Alimeeting test set and DIHARD II evaluation set, respectively.
</details>
<details>
<summary>摘要</summary>
大量学习（LSL）在对话分析中的另 overlap speech detection（OSD）是一个重要的部分，但大多数现有的OSD模型都是基于特定的数据集进行训练和评估，这限制了这些模型的应用场景。为解决这个问题，我们在OSD领域进行了大规模学习的研究，并提出了一个16K单annelOSD模型。在我们的研究中，我们收集了522小时的不同语言和风格的标注音频数据，并使用这些数据作为大规模数据集进行训练和测试。我们设计了严格的比较实验，以评估LSL在OSD任务中的效果和不同深度神经网络基于的OSD模型的性能。结果显示LSL可以显著提高OSD模型的性能和可靠性，并且CF-OSD WITH LSL目前是最佳的16K单annelOSD模型。此外，CF-OSD WITH LSL在Alimeeting测试集和DIHARD II评估集上的F1分数分别为80.8%和52.0%，创造了当前最佳的州态。
</details></li>
</ul>
<hr>
<h2 id="AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining"><a href="#AudioLDM-2-Learning-Holistic-Audio-Generation-with-Self-supervised-Pretraining" class="headerlink" title="AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining"></a>AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05734">http://arxiv.org/abs/2308.05734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoheliu/AudioLDM2">https://github.com/haoheliu/AudioLDM2</a></li>
<li>paper_authors: Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</li>
<li>for: 这 paper 的目的是提出一种框架，用于将 speech、music 和 sound effect 等不同类型的声音生成模型共同拟合。</li>
<li>methods: 该框架使用同一种学习方法，通过 AudioMAE 自然适应学习模型和 latent diffusion 模型来翻译不同类型的声音，并在生成过程中进行自我监督学习。</li>
<li>results: 经验表明，该框架可以在主要的 benchmark 上达到新的 state-of-the-art 或竞争性的性能，并且具有很好的培化学习能力和可重用的自然适应学习模型。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE and latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech demonstrate new state-of-the-art or competitive performance to previous approaches. Our demo and code are available at https://audioldm.github.io/audioldm2.
</details>
<details>
<summary>摘要</summary>
尽管各种听音都有共同之处，如speech、音乐和声音效果，但设计模型时需要仔细考虑每种类型的特定目标和偏见，这些偏见可能与其他类型异常大。为了带领我们更近到一个统一的听音生成视角，这篇论文提出了一个框架，该框架利用同一种学习方法来生成speech、音乐和声音效果。我们的框架引入了一个通用的听音表示，称为语言听音（LOA）。任何听音都可以根据AudioMAE自我超vised学习表示学习模型翻译为LOA。在生成过程中，我们使用GPT-2模型将任何Modalities翻译为LOA，然后使用一个conditional on LOA的隐藏噪声模型进行自我超vised听音生成学习。我们的提议的框架自然带来了在上下文学习能力和可重用的自我超vised Pre-trained AudioMAE和隐藏噪声模型的优点。我们的实验在主要的文本到听音、文本到音乐和文本到语音的标准 benchmarcks 上达到了新的状态码或竞争性的性能。我们的 demo 和代码可以在https://audioldm.github.io/audioldm2 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/11/cs.SD_2023_08_11/" data-id="clly3dw0y009n09885zqy7128" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/6/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/8/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">104</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">165</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
