
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/7/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_08_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/25/cs.CV_2023_08_25/" class="article-date">
  <time datetime="2023-08-24T16:00:00.000Z" itemprop="datePublished">2023-08-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/25/cs.CV_2023_08_25/">cs.CV - 2023-08-25 21:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ROAM-Robust-and-Object-aware-Motion-Generation-using-Neural-Pose-Descriptors"><a href="#ROAM-Robust-and-Object-aware-Motion-Generation-using-Neural-Pose-Descriptors" class="headerlink" title="ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors"></a>ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12969">http://arxiv.org/abs/2308.12969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyue Zhang, Rishabh Dabral, Thomas Leimkühler, Vladislav Golyanik, Marc Habermann, Christian Theobalt</li>
<li>for: 本研究旨在解决现有自动方法对新物体的抗性和泛化问题，提高3D虚拟人物运动合成中对新物体的适应性和自然性。</li>
<li>methods: 本研究使用一个受过参数化的动作模型，并通过对物体只 datasets上学习的半定态特征表示来增强模型对新物体的抗性和泛化能力。</li>
<li>results: 通过对比当前状态的方法和用户研究，本研究得到了较好的3D虚拟人物运动和互动质量和稳定性，并且可以在未看过物体的情况下进行高质量的动作生成。<details>
<summary>Abstract</summary>
Existing automatic approaches for 3D virtual character motion synthesis supporting scene interactions do not generalise well to new objects outside training distributions, even when trained on extensive motion capture datasets with diverse objects and annotated interactions. This paper addresses this limitation and shows that robustness and generalisation to novel scene objects in 3D object-aware character synthesis can be achieved by training a motion model with as few as one reference object. We leverage an implicit feature representation trained on object-only datasets, which encodes an SE(3)-equivariant descriptor field around the object. Given an unseen object and a reference pose-object pair, we optimise for the object-aware pose that is closest in the feature space to the reference pose. Finally, we use l-NSM, i.e., our motion generation model that is trained to seamlessly transition from locomotion to object interaction with the proposed bidirectional pose blending scheme. Through comprehensive numerical comparisons to state-of-the-art methods and in a user study, we demonstrate substantial improvements in 3D virtual character motion and interaction quality and robustness to scenarios with unseen objects. Our project page is available at https://vcai.mpi-inf.mpg.de/projects/ROAM/.
</details>
<details>
<summary>摘要</summary>
现有自动化方法 для3D虚拟人物运动合成，不能很好地泛化到新物体外部训练分布，即使训练在含有多种物体和注释交互的大规模运动捕捉数据集上。这篇论文解决了这一问题，并显示了在含有新物体的场景中的3D物体意识Character Synthesis中的稳定性和泛化性可以通过训练一个运动模型，只需要一个参考物体。我们利用了一种基于物体专门采集的隐藏特征表示，这种表示在物体周围的SE(3)-等变换equivariant描述器场中编码了物体。给定一个未看过的物体和一个参考姿态-物体对，我们优化了 closest在特征空间的物体意识姿态。最后，我们使用l-NSM，即我们训练的运动生成模型，通过我们的拟合bidirectional姿态混合方案来协调转换从步行到物体交互。通过对现有方法的数字比较和用户研究，我们展示了3D虚拟人物运动和交互质量和稳定性在未看过物体场景中得到了显著改善。我们的项目页面可以在https://vcai.mpi-inf.mpg.de/projects/ROAM/上找到。
</details></li>
</ul>
<hr>
<h2 id="Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation"><a href="#Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation" class="headerlink" title="Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation"></a>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12968">http://arxiv.org/abs/2308.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxinn-j/scenimefy">https://github.com/yuxinn-j/scenimefy</a></li>
<li>paper_authors: Yuxin Jiang, Liming Jiang, Shuai Yang, Chen Change Loy</li>
<li>for: 这种研究的目的是提高动漫场景的自动高质量渲染，以解决现有的镜像匹配问题，提高图像的semantic preserve和精细特征。</li>
<li>methods: 这种方法使用了semi-supervised image-to-image翻译框架，使用了Structure-consistent pseudo paired data，并使用了segementation-guided data selection和patch-wise contrastive style loss来提高风格化和精细特征。</li>
<li>results: 对比 estado-of-the-art 基eline，这种方法在 both perceptual quality和量化性能方面表现出色，得到了更高的质量和更好的结果。<details>
<summary>Abstract</summary>
Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.Translated by Google Translate.
</details></li>
</ul>
<hr>
<h2 id="POCO-3D-Pose-and-Shape-Estimation-with-Confidence"><a href="#POCO-3D-Pose-and-Shape-Estimation-with-Confidence" class="headerlink" title="POCO: 3D Pose and Shape Estimation with Confidence"></a>POCO: 3D Pose and Shape Estimation with Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12965">http://arxiv.org/abs/2308.12965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Kumar Dwivedi, Cordelia Schmid, Hongwei Yi, Michael J. Black, Dimitrios Tzionas<br>for: The paper is written for improving the accuracy of 3D human pose and shape estimation from images, and providing uncertainty estimates for downstream tasks.methods: The paper proposes a novel framework called POCO, which uses a Dual Conditioning Strategy (DCS) to estimate both the 3D body pose and the per-sample variance in a single feed-forward pass.results: The paper shows that training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose, and demonstrates the effectiveness of the proposed method by applying it to three state-of-the-art HPS regressors and showing improvement in accuracy. Additionally, the paper demonstrates the usefulness of the uncertainty estimates for downstream tasks such as bootstrap HPS training and video pose estimation.Here’s the Chinese translation of the three information:for: 本文是为了提高图像中人体三维姿态和形状估计的准确性, 并为下游任务提供不确定性估计。methods: 本文提出了一种名为POCO的新框架，该框架使用双conditioning策略（DCS）来在单一的前向传播中估计3D人体姿态和每个样本的方差。results: 本文显示了训练网络理解不确定性可以帮助其更加准确地估计3D姿态，并通过应用到三个state-of-the-art HPS regressors上显示了改进准确性。此外，本文还示出了不确定性估计的实用性，例如通过自动划分不确定样本来进行HPS训练、视频人体姿态估计等。<details>
<summary>Abstract</summary>
The regression of 3D Human Pose and Shape (HPS) from an image is becoming increasingly accurate. This makes the results useful for downstream tasks like human action recognition or 3D graphics. Yet, no regressor is perfect, and accuracy can be affected by ambiguous image evidence or by poses and appearance that are unseen during training. Most current HPS regressors, however, do not report the confidence of their outputs, meaning that downstream tasks cannot differentiate accurate estimates from inaccurate ones. To address this, we develop POCO, a novel framework for training HPS regressors to estimate not only a 3D human body, but also their confidence, in a single feed-forward pass. Specifically, POCO estimates both the 3D body pose and a per-sample variance. The key idea is to introduce a Dual Conditioning Strategy (DCS) for regressing uncertainty that is highly correlated to pose reconstruction quality. The POCO framework can be applied to any HPS regressor and here we evaluate it by modifying HMR, PARE, and CLIFF. In all cases, training the network to reason about uncertainty helps it learn to more accurately estimate 3D pose. While this was not our goal, the improvement is modest but consistent. Our main motivation is to provide uncertainty estimates for downstream tasks; we demonstrate this in two ways: (1) We use the confidence estimates to bootstrap HPS training. Given unlabelled image data, we take the confident estimates of a POCO-trained regressor as pseudo ground truth. Retraining with this automatically-curated data improves accuracy. (2) We exploit uncertainty in video pose estimation by automatically identifying uncertain frames (e.g. due to occlusion) and inpainting these from confident frames. Code and models will be available for research at https://poco.is.tue.mpg.de.
</details>
<details>
<summary>摘要</summary>
“三维人体姿态和形状（HPS）从图像回推的进步越来越精确，使得结果可以用于人做动作识别或3Dgraphics。然而，没有一个优秀的回推器，因为图像证据不明确或者人做动作和外表都没有在训练过程中出现过。现今大多数HPS回推器都不会报告其出力的可信度，因此下游任务无法分辨实际的估计和错误的估计。为了解决这个问题，我们开发了POCO，一个新的框架，可以在单一的从前进推 pass中预测HPS和其可信度。具体来说，POCO会预测3D人体姿态和每个样本的条件方差。我们的关键思想是通过引入双条件策略（DCS）来预测不确定性，这和姿态重建质量高度相关。POCO框架可以应用于任何HPS回推器，我们在这里评估了修改HMR、PARE和CLIFF等回推器。在所有情况下，将network培训来理解不确定性，使其更好地估计3D姿态。这并不是我们的主要目标，但是改善是微不足道，但是一致的。我们的主要动机是提供不确定性估计，我们在两种方式中示出了这个：（1）我们使用POCO-trained回推器的自信估计作为自动生成的pseudo陌生标本。将这些自信估计作为训练标本，然后重训，可以提高准确性。（2）我们利用不确定性在动作捕捉中自动识别 uncertain frames（例如由遮蔽所致），并从自信frames中填充这些frame。”
</details></li>
</ul>
<hr>
<h2 id="Dense-Text-to-Image-Generation-with-Attention-Modulation"><a href="#Dense-Text-to-Image-Generation-with-Attention-Modulation" class="headerlink" title="Dense Text-to-Image Generation with Attention Modulation"></a>Dense Text-to-Image Generation with Attention Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12964">http://arxiv.org/abs/2308.12964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/densediffusion">https://github.com/naver-ai/densediffusion</a></li>
<li>paper_authors: Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, Jun-Yan Zhu</li>
<li>for: 实现文本描述中的具体图像Synthesize realistic images from dense captions, where each text prompt provides a detailed description for a specific image region.</li>
<li>methods: 使用预训条件为文本描述中的具体图像构成，并通过控制图像的构成来实现具体图像的生成。</li>
<li>results: 以无需训练和数据集，提高文本描述中的具体图像生成效果，并与特定构成条件下的图像生成效果相似。<details>
<summary>Abstract</summary>
Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.
</details>
<details>
<summary>摘要</summary>
existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.Here's the translation in Traditional Chinese:现有的文本至图扩散模型对于细节描述的文本提示则做不好，这些文本提示每个图像区域的详细描述。为解决这个问题，我们提出了DenseDiffusion，一种不需要训练的方法，可以将预训练的文本至图模型调整以应对这些细节描述。我们首先分析生成图像的布局和预训练模型的中间注意力图。接着，我们开发了一种注意力调节方法，可以根据布局指导物品出现在特定的区域中。不需要进一步的调整或数据，我们提高了对细节描述的图像生成性能，并且在自动和人类评估上都有改善。此外，我们可以使用特定布局条件进行训练，以获得相似的视觉效果。
</details></li>
</ul>
<hr>
<h2 id="MapPrior-Bird’s-Eye-View-Map-Layout-Estimation-with-Generative-Models"><a href="#MapPrior-Bird’s-Eye-View-Map-Layout-Estimation-with-Generative-Models" class="headerlink" title="MapPrior: Bird’s-Eye View Map Layout Estimation with Generative Models"></a>MapPrior: Bird’s-Eye View Map Layout Estimation with Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12963">http://arxiv.org/abs/2308.12963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiyue Zhu, Vlas Zyrianov, Zhijian Liu, Shenlong Wang</li>
<li>for: 提高 bird’s-eye view (BEV) 识别模型的准确性和生成性的 semantic map 布局</li>
<li>methods:  combine 传统的探测型 BEV 识别模型和学习的生成模型 для semantic map 布局</li>
<li>results: 在 nuScenes benchmark 上，MapPrior 比最强竞争对手提高 MMD 和 ECE  scores 的 camera-和 LiDAR-based BEV 识别任务中表现出色，得到了显著改善的结果。<details>
<summary>Abstract</summary>
Despite tremendous advancements in bird's-eye view (BEV) perception, existing models fall short in generating realistic and coherent semantic map layouts, and they fail to account for uncertainties arising from partial sensor information (such as occlusion or limited coverage). In this work, we introduce MapPrior, a novel BEV perception framework that combines a traditional discriminative BEV perception model with a learned generative model for semantic map layouts. Our MapPrior delivers predictions with better accuracy, realism, and uncertainty awareness. We evaluate our model on the large-scale nuScenes benchmark. At the time of submission, MapPrior outperforms the strongest competing method, with significantly improved MMD and ECE scores in camera- and LiDAR-based BEV perception.
</details>
<details>
<summary>摘要</summary>
尽管存在巨大的进步，现有的鸟瞰视（BEV）感知模型仍未能生成真实、凝重的 semantic map 布局，并且无法考虑部分感知器（如遮挡或有限覆盖）中的不确定性。在这项工作中，我们引入 MapPrior，一种新的 BEV 感知框架，该框架将传统的推理 BEV 感知模型与学习的生成模型结合在一起。我们的 MapPrior 能够提供更加准确、真实和不确定性意识的预测。我们在 nuScenes benchmark 上进行了评估，当时提交的 MapPrior 已经超过了最强竞争对手，在摄像头和 LiDAR 基于的 BEV 感知方面具有显著提高的 MMD 和 ECE 分数。
</details></li>
</ul>
<hr>
<h2 id="Motion-Guided-Masking-for-Spatiotemporal-Representation-Learning"><a href="#Motion-Guided-Masking-for-Spatiotemporal-Representation-Learning" class="headerlink" title="Motion-Guided Masking for Spatiotemporal Representation Learning"></a>Motion-Guided Masking for Spatiotemporal Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12962">http://arxiv.org/abs/2308.12962</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Fan, Jue Wang, Shuai Liao, Yi Zhu, Vimal Bhat, Hector Santos-Villalobos, Rohith MV, Xinyu Li</li>
<li>for: 这个论文主要是为了提高视频理解性，并且使用随机遮盲法来提高视频 autoencoder 的性能。</li>
<li>methods: 这个论文提出了一种新的推寄算法，即动态推寄法（Motion-guided masking，MGM），该算法利用运动向量来引导遮盲器在时间上的位置。</li>
<li>results: 在两个复杂的大规模视频测试集（Kinetics-400和Something-Something V2）上，这个方法可以与之前的状态OF-THE-ART方法相比，在视频 autoencoder 中获得最大 $1.3%$ 的提高。此外，这个方法还可以在训练EPoch数量相同的情况下，与之前的方法相比，在视频 autoencoder 中获得最大 $66%$ 的提高。最后，这个方法在下游传输学习和领域适应任务中表现出色，在 UCF101、HMDB51 和 Diving48  datasets上获得最大 $4.9%$ 的提高。<details>
<summary>Abstract</summary>
Several recent works have directly extended the image masked autoencoder (MAE) with random masking into video domain, achieving promising results. However, unlike images, both spatial and temporal information are important for video understanding. This suggests that the random masking strategy that is inherited from the image MAE is less effective for video MAE. This motivates the design of a novel masking algorithm that can more efficiently make use of video saliency. Specifically, we propose a motion-guided masking algorithm (MGM) which leverages motion vectors to guide the position of each mask over time. Crucially, these motion-based correspondences can be directly obtained from information stored in the compressed format of the video, which makes our method efficient and scalable. On two challenging large-scale video benchmarks (Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and achieve up to +$1.3\%$ improvement compared to previous state-of-the-art methods. Additionally, our MGM achieves equivalent performance to previous video MAE using up to $66\%$ fewer training epochs. Lastly, we show that MGM generalizes better to downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\%$ improvement compared to baseline methods.
</details>
<details>
<summary>摘要</summary>
On two challenging large-scale video benchmarks (Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and achieve up to +$1.3\%$ improvement compared to previous state-of-the-art methods. Additionally, our MGM achieves equivalent performance to previous video MAE using up to $66\%$ fewer training epochs. Our MGM also generalizes better to downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\%$ improvement compared to baseline methods.
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-Towards-Efficient-Few-shot-3D-Semantic-Segmentation-via-Training-free-Networks"><a href="#Less-is-More-Towards-Efficient-Few-shot-3D-Semantic-Segmentation-via-Training-free-Networks" class="headerlink" title="Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks"></a>Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12961">http://arxiv.org/abs/2308.12961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangyangyang127/tfs3d">https://github.com/yangyangyang127/tfs3d</a></li>
<li>paper_authors: Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Hao Dong, Peng Gao</li>
<li>for: 提高3D分割任务中的几个shot学习效果，减少大规模数据的依赖。</li>
<li>methods: 提出了一种没有学习参数的培训自由3D分割网络（TFS3D）和其进一步改进版本TFS3D-T。TFS3D使用三角函数坐标编码提取密集表示，与之前的培训方法相比具有相似的性能。TFS3D-T通过增强几个shot查询和支持数据之间的交互，提高了前期培训的效果。</li>
<li>results: 对S3DIS和ScanNet数据集进行实验，TFS3D-T在mIoU方面提高了+6.93%和+17.96%，同时减少了培训时间 by -90%，表明TFS3D-T具有更高的效果和效率。<details>
<summary>Abstract</summary>
To reduce the reliance on large-scale datasets, recent works in 3D segmentation resort to few-shot learning. Current 3D few-shot semantic segmentation methods first pre-train the models on `seen' classes, and then evaluate their generalization performance on `unseen' classes. However, the prior pre-training stage not only introduces excessive time overhead, but also incurs a significant domain gap on `unseen' classes. To tackle these issues, we propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and a further training-based variant, TFS3D-T. Without any learnable parameters, TFS3D extracts dense representations by trigonometric positional encodings, and achieves comparable performance to previous training-based methods. Due to the elimination of pre-training, TFS3D can alleviate the domain gap issue and save a substantial amount of time. Building upon TFS3D, TFS3D-T only requires to train a lightweight query-support transferring attention (QUEST), which enhances the interaction between the few-shot query and support data. Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by +6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the training time by -90%, indicating superior effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
Recent works in 3D segmentation have resorted to few-shot learning to reduce reliance on large-scale datasets. Current 3D few-shot semantic segmentation methods first pre-train the models on "seen" classes and then evaluate their generalization performance on "unseen" classes. However, the prior pre-training stage not only introduces excessive time overhead but also incurs a significant domain gap on "unseen" classes. To address these issues, we propose an efficient Training-free Few-shot 3D Segmentation network (TFS3D) and a further training-based variant (TFS3D-T). Without any learnable parameters, TFS3D extracts dense representations by trigonometric positional encodings and achieves comparable performance to previous training-based methods. Due to the elimination of pre-training, TFS3D can alleviate the domain gap issue and save a substantial amount of time. Building upon TFS3D, TFS3D-T only requires training a lightweight query-support transferring attention (QUEST), which enhances the interaction between the few-shot query and support data. Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by +6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the training time by -90%, indicating superior effectiveness and efficiency.
</details></li>
</ul>
<hr>
<h2 id="Towards-Realistic-Zero-Shot-Classification-via-Self-Structural-Semantic-Alignment"><a href="#Towards-Realistic-Zero-Shot-Classification-via-Self-Structural-Semantic-Alignment" class="headerlink" title="Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment"></a>Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12960">http://arxiv.org/abs/2308.12960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheng-eatamath/S3A">https://github.com/sheng-eatamath/S3A</a></li>
<li>paper_authors: Sheng Zhang, Muzammal Naseer, Guangyi Chen, Zhiqiang Shen, Salman Khan, Kun Zhang, Fahad Khan</li>
<li>for: 本研究的目的是解决零 shot 分类中的开放世界问题，即没有注释但具有广泛的词汇。</li>
<li>methods: 本研究提出了 Self Structural Semantic Alignment (S^3A) 框架，它可以从无注释数据中提取结构性 semantics，并同时进行自我学习。S^3A 框架包括一种唯一的 Cluster-Vote-Prompt-Realign (CVPR) 算法，它通过轮循图像集成、选择每个集合中的图像，通过大语言模型生成权威提示，以及将图像和词汇进行结构性 semantic alignment，来提取结构性 semantics。</li>
<li>results: 对多种通用和细化的 benchmarcks 进行了广泛的实验，结果表明，S^3A 方法可以在零 shot 分类中提供较高的精度改进，相比 CLIP 的平均改进率高于 15%。<details>
<summary>Abstract</summary>
Large-scale pre-trained Vision Language Models (VLMs) have proven effective for zero-shot classification. Despite the success, most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal vocabularies, which rarely satisfy the open-world scenario. In this paper, we aim at a more challenging setting, Realistic Zero-Shot Classification, which assumes no annotation but instead a broad vocabulary. To address this challenge, we propose the Self Structural Semantic Alignment (S^3A) framework, which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S^3A framework adopts a unique Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR process includes iterative clustering on images, voting within each cluster to identify initial class candidates from the vocabulary, generating discriminative prompts with large language models to discern confusing candidates, and realigning images and the vocabulary as structural semantic alignment. Finally, we propose to self-learn the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S^3A method offers substantial improvements over existing VLMs-based approaches, achieving a more than 15% accuracy improvement over CLIP on average. Our codes, models, and prompts are publicly released at https://github.com/sheng-eatamath/S3A.
</details>
<details>
<summary>摘要</summary>
大规模预训练视觉语言模型（VLM）已经证明有效于零shot分类。 despite the success， most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal vocabularies，which rarely satisfy the open-world scenario. In this paper，we aim at a more challenging setting，Realistic Zero-Shot Classification，which assumes no annotation but instead a broad vocabulary. To address this challenge，we propose the Self Structural Semantic Alignment（S^3A）framework，which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S^3A framework adopts a unique Cluster-Vote-Prompt-Realign（CVPR）algorithm，which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR process includes iterative clustering on images，voting within each cluster to identify initial class candidates from the vocabulary，generating discriminative prompts with large language models to discern confusing candidates，and realigning images and the vocabulary as structural semantic alignment. Finally，we propose to self-learn the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S^3A method offers substantial improvements over existing VLMs-based approaches，achieving a more than 15% accuracy improvement over CLIP on average. Our codes，models，and prompts are publicly released at https://github.com/sheng-eatamath/S3A.
</details></li>
</ul>
<hr>
<h2 id="Label-Budget-Allocation-in-Multi-Task-Learning"><a href="#Label-Budget-Allocation-in-Multi-Task-Learning" class="headerlink" title="Label Budget Allocation in Multi-Task Learning"></a>Label Budget Allocation in Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12949">http://arxiv.org/abs/2308.12949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ximeng Sun, Kihyuk Sohn, Kate Saenko, Clayton Mellina, Xiao Bian</li>
<li>for: 提高机器学习系统的性能，解决标签数据的成本问题。</li>
<li>methods: 提出了标签预算分配问题，并提出了一种适应任务的预算分配算法来解决这个问题。</li>
<li>results: 通过实验证明了我们的方法可以比其他各种常用的标签策略提高多任务学习的性能。<details>
<summary>Abstract</summary>
The cost of labeling data often limits the performance of machine learning systems. In multi-task learning, related tasks provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over other widely used heuristic labeling strategies.
</details>
<details>
<summary>摘要</summary>
machine learning系统的性能受数据标注成本的限制。在多任务学习中，相关任务之间交换信息，提高总性能，但标注成本可能因任务而异。如何在多任务学习中合理分配标注预算（即用于标注的费用）以达到最佳性能？我们是第一个提出并正式定义多任务学习标注预算分配问题，并通过实验证明不同预算分配策略对性能产生了很大影响。我们提出了适应任务的预算分配算法，可以在不同的多任务学习设置下生成最佳的预算分配策略。specifically，我们估算并最大化从分配预算中获得的新信息的总量，作为多任务学习性能的代理。 Pascal VOC和Taskonomy的实验表明我们的方法比其他常见的标注策略更有效。
</details></li>
</ul>
<hr>
<h2 id="Perspective-aware-Convolution-for-Monocular-3D-Object-Detection"><a href="#Perspective-aware-Convolution-for-Monocular-3D-Object-Detection" class="headerlink" title="Perspective-aware Convolution for Monocular 3D Object Detection"></a>Perspective-aware Convolution for Monocular 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12938">http://arxiv.org/abs/2308.12938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KenYu910645/perspective-aware-convolution">https://github.com/KenYu910645/perspective-aware-convolution</a></li>
<li>paper_authors: Jia-Quan Yu, Soo-Chang Pei</li>
<li>for: 提高自动驾驶车辆中的单摄像头三维物体检测精度。</li>
<li>methods: 提出了一种新的视角意识核心层，该层可以在图像中提取长距离依赖关系，以捕捉场景的视角信息。</li>
<li>results: 在KITTI3D数据集上测试，该方法可以提高3D物体检测精度，达到了23.9%的准确率。这些结果表明了场景信息的重要性，以及网络设计中场景结构的潜在优势。<details>
<summary>Abstract</summary>
Monocular 3D object detection is a crucial and challenging task for autonomous driving vehicle, while it uses only a single camera image to infer 3D objects in the scene. To address the difficulty of predicting depth using only pictorial clue, we propose a novel perspective-aware convolutional layer that captures long-range dependencies in images. By enforcing convolutional kernels to extract features along the depth axis of every image pixel, we incorporates perspective information into network architecture. We integrate our perspective-aware convolutional layer into a 3D object detector and demonstrate improved performance on the KITTI3D dataset, achieving a 23.9\% average precision in the easy benchmark. These results underscore the importance of modeling scene clues for accurate depth inference and highlight the benefits of incorporating scene structure in network design. Our perspective-aware convolutional layer has the potential to enhance object detection accuracy by providing more precise and context-aware feature extraction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>>单目3D对象检测是自动驾驶车辆中的一项关键和挑战性任务，它使用单个摄像头图像来推断Scene中的3D对象。为了解决基于图像的深度预测困难，我们提出了一种新的视角意识核心层。我们要求核心层在每个图像像素上提取深度轴方向的特征，从而将视角信息integrated into网络 architecture。我们将这种视角意识核心层与3D对象检测器结合，并在KITTI3D数据集上进行了评估，实现了23.9%的准确率在易 benchmark。这些结果证明了场景 clue的重要性，并高亮了网络设计中场景结构的 incorporation 的好处。我们的视角意识核心层有可能提高对象检测精度，通过提供更加准确和上下文感知的特征提取。
</details></li>
</ul>
<hr>
<h2 id="Panoptic-Depth-Color-Map-for-Combination-of-Depth-and-Image-Segmentation"><a href="#Panoptic-Depth-Color-Map-for-Combination-of-Depth-and-Image-Segmentation" class="headerlink" title="Panoptic-Depth Color Map for Combination of Depth and Image Segmentation"></a>Panoptic-Depth Color Map for Combination of Depth and Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12937">http://arxiv.org/abs/2308.12937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Quan Yu, Soo-Chang Pei</li>
<li>for: 这篇论文旨在提出一种将图像分割和深度估计结合在一起的新方法，以提高自动驾驶场景中图像识别的精度和安全性。</li>
<li>methods: 该方法具有一个额外的深度估计分支，用于在分割网络中预测每个实例段的深度。</li>
<li>results: 在Cityscape数据集上测试，该方法能够实现高质量的分割结果，同时包含深度信息，并通过色彩地图可视化。这种方法开拓了将不同任务和网络结合起来生成更全面的图像识别结果，以提高自动驾驶车辆的安全性。<details>
<summary>Abstract</summary>
Image segmentation and depth estimation are crucial tasks in computer vision, especially in autonomous driving scenarios. Although these tasks are typically addressed separately, we propose an innovative approach to combine them in our novel deep learning network, Panoptic-DepthLab. By incorporating an additional depth estimation branch into the segmentation network, it can predict the depth of each instance segment. Evaluating on Cityscape dataset, we demonstrate the effectiveness of our method in achieving high-quality segmentation results with depth and visualize it with a color map. Our proposed method demonstrates a new possibility of combining different tasks and networks to generate a more comprehensive image recognition result to facilitate the safety of autonomous driving vehicles.
</details>
<details>
<summary>摘要</summary>
Image segmentation和深度估计是计算机视觉中关键任务，尤其在自动驾驶场景下。虽然这两个任务通常被视为独立的，但我们提出了一种创新的方法，将它们结合在一起。我们的新型深度学习网络Panoptic-DepthLab中添加了一个深度估计分支，可以预测每个实例分割结果中的深度。在Cityscape数据集上评估，我们示出了我们的方法可以实现高质量的分割结果，并通过色彩地图进行可见化。我们的提议的方法开 up了将不同任务和网络结合起来以生成更全面的图像认知结果，以便促进自动驾驶车辆的安全。
</details></li>
</ul>
<hr>
<h2 id="Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP"><a href="#Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP" class="headerlink" title="Towards Realistic Unsupervised Fine-tuning with CLIP"></a>Towards Realistic Unsupervised Fine-tuning with CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12919">http://arxiv.org/abs/2308.12919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Liang, Lijun Sheng, Zhengbo Wang, Ran He, Tieniu Tan</li>
<li>for: 这个研究旨在应用CLIPvision-language模型进行下游有监督学习任务，并在无监督下精致化CLIP。</li>
<li>methods: 本研究提出了一个简单、高效的精致化方法，名为Universal Entropy Optimization（UEO），它利用amples的信任程度来减少信任度高的例子的 conditional entropy，并将不信任度高的例子的margin entropy提高。 UEO还包括了对CLIP的视觉分支中的通道对称变换进行优化。</li>
<li>results: 经过了15个领域和4种不同的专门知识的广泛实验，结果显示UEO的表现比基eline方法更好，具有更高的普遍化和外部调整检测能力。<details>
<summary>Abstract</summary>
The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.   To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.
</details>
<details>
<summary>摘要</summary>
“视觉语言模型（VLM）的出现，如CLIP，已经引发了大量关于其应用于下游有监督学习任务的研究effort。 although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Robot-Pose-Nowcasting-Forecast-the-Future-to-Improve-the-Present"><a href="#Robot-Pose-Nowcasting-Forecast-the-Future-to-Improve-the-Present" class="headerlink" title="Robot Pose Nowcasting: Forecast the Future to Improve the Present"></a>Robot Pose Nowcasting: Forecast the Future to Improve the Present</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12914">http://arxiv.org/abs/2308.12914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Simoni, Francesco Marchetti, Guido Borghi, Federico Becattini, Lorenzo Seidenari, Roberto Vezzani, Alberto Del Bimbo</li>
<li>for: 本研究旨在实现人机合作的安全和有效实现，尤其在工业4.0enario中。</li>
<li>methods: 该研究提出了一种基于视觉数据的系统，用于准确地确定机器人的3D位势。</li>
<li>results: 实验结果表明，该系统可以通过同时学习预测未来位势来提高当前位势估计精度。<details>
<summary>Abstract</summary>
In recent years, the effective and safe collaboration between humans and machines has gained significant importance, particularly in the Industry 4.0 scenario. A critical prerequisite for realizing this collaborative paradigm is precisely understanding the robot's 3D pose within its environment. Therefore, in this paper, we introduce a novel vision-based system leveraging depth data to accurately establish the 3D locations of robotic joints. Specifically, we prove the ability of the proposed system to enhance its current pose estimation accuracy by jointly learning to forecast future poses. Indeed, we introduce the concept of Pose Nowcasting, denoting the capability of a system to exploit the learned knowledge of the future to improve the estimation of the present. The experimental evaluation is conducted on two different datasets, providing state-of-the-art and real-time performance and confirming the validity of the proposed method on both the robotic and human scenarios.
</details>
<details>
<summary>摘要</summary>
Recently, the collaboration between humans and machines has become increasingly important, especially in the Industry 4.0 scenario. To realize this collaborative paradigm, it is crucial to accurately understand the robot's 3D pose within its environment. In this paper, we propose a novel vision-based system that leverages depth data to establish the 3D locations of robotic joints. Our system can enhance its current pose estimation accuracy by jointly learning to forecast future poses, which we call "Pose Nowcasting." We evaluate our system on two datasets and achieve state-of-the-art and real-time performance, demonstrating its validity in both robotic and human scenarios.Here's the word-for-word translation of the text into Simplified Chinese:近年来，人机合作的有效和安全合作在四点差enario中具有重要意义。为实现这种合作模式，精准理解机器人的3D姿态在环境中是关键。在这篇论文中，我们提出了一种基于视觉的系统，利用深度数据来准确地确定机器人关节的3D位置。我们的系统可以通过同时学习预测未来姿态来提高当前姿态估计精度，我们称之为“姿态预测”。我们在两个不同的数据集上进行了实验，并在机器人和人类场景中实现了状态最佳和实时性能，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="SCoRD-Subject-Conditional-Relation-Detection-with-Text-Augmented-Data"><a href="#SCoRD-Subject-Conditional-Relation-Detection-with-Text-Augmented-Data" class="headerlink" title="SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data"></a>SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12910">http://arxiv.org/abs/2308.12910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyan Yang, Kushal Kafle, Zhe Lin, Scott Cohen, Zhihong Ding, Vicente Ordonez</li>
<li>for: 本研究的目的是提出一种基于subject conditional关系检测（SCoRD）的方法，可以 Conditioned on an input subject, predict all its relations to other objects in a scene along with their locations.</li>
<li>methods: 我们提出了一种自动循环模型，可以给定一个主题，预测其关系、物体和物体位置。我们将这个输出视为一个序列的Token，并使用自动生成的关系-物体对来进行训练。</li>
<li>results: 我们显示了前一代场景图生成器方法在本benchmark上不能生成相同的对象-关系对。我们的方法可以在relation-物体和物体-框位置预测中提高泛化性。 particualrly, 我们在没有对象框注释时可以获得82.59%的recall@3 для关系-物体对和32.27%的box位置预测。<details>
<summary>Abstract</summary>
We propose Subject-Conditional Relation Detection SCoRD, where conditioned on an input subject, the goal is to predict all its relations to other objects in a scene along with their locations. Based on the Open Images dataset, we propose a challenging OIv6-SCoRD benchmark such that the training and testing splits have a distribution shift in terms of the occurrence statistics of $\langle$subject, relation, object$\rangle$ triplets. To solve this problem, we propose an auto-regressive model that given a subject, it predicts its relations, objects, and object locations by casting this output as a sequence of tokens. First, we show that previous scene-graph prediction methods fail to produce as exhaustive an enumeration of relation-object pairs when conditioned on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for our relation-object predictions compared to the 49.75% obtained by a recent scene graph detector. Then, we show improved generalization on both relation-object and object-box predictions by leveraging during training relation-object pairs obtained automatically from textual captions and for which no object-box annotations are available. Particularly, for $\langle$subject, relation, object$\rangle$ triplets for which no object locations are available during training, we are able to obtain a recall@3 of 42.59% for relation-object pairs and 32.27% for their box locations.
</details>
<details>
<summary>摘要</summary>
我们提出了主题 conditional relation detection（SCoRD），其中，基于输入主题，目标是预测它与其他对象之间的关系以及它们的位置。使用Open Images dataset，我们提出了一个具有分布转移的OIv6-SCoRD benchmark，以便在训练和测试分布中有 statistically significant distribution shift in terms of $\langle$主题, 关系, 对象$\rangle$ triplets的发生频率。为解决这个问题，我们提出了一种自动生成的模型，它在给定一个主题后，预测其关系、对象和对象位置，通过将输出映射为一个序列的token。首先，我们证明了以前的场景图预测方法在这个benchmark上无法生成主题 conditional的完整的关系对象对。特别是，我们在relation-object预测中获得了83.8%的Recall@3，而一个最近的场景图探测器只得49.75%。然后，我们表明了通过在训练过程中使用自动获得的文本描述中的关系对象对来改进场景图预测的一致性。特别是，对于没有输入对象位置的主题 conditional $\langle$主题, 关系, 对象$\rangle$ triplets，我们在relation-object预测中获得了42.59%的Recall@3，而在object-box预测中获得了32.27%。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Semantic-Segmentation-from-the-Perspective-of-Explicit-Class-Embeddings"><a href="#Boosting-Semantic-Segmentation-from-the-Perspective-of-Explicit-Class-Embeddings" class="headerlink" title="Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings"></a>Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12894">http://arxiv.org/abs/2308.12894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhe Liu, Chuanjian Liu, Kai Han, Quan Tang, Zengchang Qin</li>
<li>for: 本研究旨在提高 semantic segmentation 任务中类 embedding 的生成和利用，以及探索类 embedding 的机制。</li>
<li>methods: 本 paper 提出了 ECENet，一种新的 segmentation  парадиг，其中类 embedding 在多Stage 图像特征交互中被明确地生成和提高。此外，作者还提出了一种 Feature Reconstruction 模块，用于保证特征的多样性和重复性。</li>
<li>results: 实验表明，ECENet 在 ADE20K 数据集上比其他方法具有更好的性能，并在 PASCAL-Context 数据集上达到了新的顶峰性能。<details>
<summary>Abstract</summary>
Semantic segmentation is a computer vision task that associates a label with each pixel in an image. Modern approaches tend to introduce class embeddings into semantic segmentation for deeply utilizing category semantics, and regard supervised class masks as final predictions. In this paper, we explore the mechanism of class embeddings and have an insight that more explicit and meaningful class embeddings can be generated based on class masks purposely. Following this observation, we propose ECENet, a new segmentation paradigm, in which class embeddings are obtained and enhanced explicitly during interacting with multi-stage image features. Based on this, we revisit the traditional decoding process and explore inverted information flow between segmentation masks and class embeddings. Furthermore, to ensure the discriminability and informativity of features from backbone, we propose a Feature Reconstruction module, which combines intrinsic and diverse branches together to ensure the concurrence of diversity and redundancy in features. Experiments show that our ECENet outperforms its counterparts on the ADE20K dataset with much less computational cost and achieves new state-of-the-art results on PASCAL-Context dataset. The code will be released at https://gitee.com/mindspore/models and https://github.com/Carol-lyh/ECENet.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation 是一种计算机视觉任务，它将每个图像像素绑定一个标签。现代方法通常会将类嵌入引入到 semantic segmentation 中，以深入利用类 semantics，并视 supervised class masks 为最终预测。在这篇论文中，我们研究了类嵌入的机制，并发现可以根据 deliberately 设计的 class masks 生成更直观和有意义的类嵌入。从这一点出发，我们提出了 ECENet，一种新的分割 paradigm，其中类嵌入在多个阶段图像特征的交互中获得和提高。基于这一点，我们重新评估了传统的解码过程，并探索了分割masks 和类嵌入之间的反向信息流。此外，为保证特征的推断能力和信息含量，我们提出了一种 Feature Reconstruction 模块，它将内在和多样的分支结合起来，以保证特征的多样性和重复性。实验显示，我们的 ECENet 在 ADE20K 数据集上比其他方法具有更少的计算成本，并在 PASCAL-Context 数据集上达到了新的状态码记录。代码将在 <https://gitee.com/mindspore/models> 和 <https://github.com/Carol-lyh/ECENet> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Multi-stage-feature-decorrelation-constraints-for-improving-CNN-classification-performance"><a href="#Multi-stage-feature-decorrelation-constraints-for-improving-CNN-classification-performance" class="headerlink" title="Multi-stage feature decorrelation constraints for improving CNN classification performance"></a>Multi-stage feature decorrelation constraints for improving CNN classification performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12880">http://arxiv.org/abs/2308.12880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuyu Zhu, Xuewen Zu, Chengfei Liu</li>
<li>for: 这个论文主要针对的是如何使用多stage feature decorrelation loss (MFD Loss) 提高 convolutional Neural Network (CNN) 的类别精度。</li>
<li>methods: 这个论文提出了一种基于 MFD Loss 的多stage feature decorrelation loss 方法，通过在不同层次上对特征进行约束，以消除特征之间的信息重复，提高 CNN 的类别精度。</li>
<li>results: 实验表明，与单个 Softmax Loss 超参比较，Softmax Loss + MFD Loss 的组合可以提高 CNN 的类别精度，而且与其他一些常见的损失函数结合后也可以获得更好的性能。<details>
<summary>Abstract</summary>
For the convolutional neural network (CNN) used for pattern classification, the training loss function is usually applied to the final output of the network, except for some regularization constraints on the network parameters. However, with the increasing of the number of network layers, the influence of the loss function on the network front layers gradually decreases, and the network parameters tend to fall into local optimization. At the same time, it is found that the trained network has significant information redundancy at all stages of features, which reduces the effectiveness of feature mapping at all stages and is not conducive to the change of the subsequent parameters of the network in the direction of optimality. Therefore, it is possible to obtain a more optimized solution of the network and further improve the classification accuracy of the network by designing a loss function for restraining the front stage features and eliminating the information redundancy of the front stage features .For CNN, this article proposes a multi-stage feature decorrelation loss (MFD Loss), which refines effective features and eliminates information redundancy by constraining the correlation of features at all stages. Considering that there are many layers in CNN, through experimental comparison and analysis, MFD Loss acts on multiple front layers of CNN, constrains the output features of each layer and each channel, and performs supervision training jointly with classification loss function during network training. Compared with the single Softmax Loss supervised learning, the experiments on several commonly used datasets on several typical CNNs prove that the classification performance of Softmax Loss+MFD Loss is significantly better. Meanwhile, the comparison experiments before and after the combination of MFD Loss and some other typical loss functions verify its good universality.
</details>
<details>
<summary>摘要</summary>
für die konvolutionelle neurale network (CNN) verwendet für musterclassifizierung, wird die trainingsverlustfunktion usually auf das finale output des networks angewendet, mit Ausnahme von einigen regularisierungsbeschränkungen auf die netzzustände. jedoch mit zunahme des umsatzes von netzzuständen increases, decrease the influence of the loss function on the network front layers, and the network parameters tend to fall into local optimization. at the same time, it is found that the trained network has significant information redundancy at all stages of features, which reduces the effectiveness of feature mapping at all stages and is not conducive to the change of the subsequent parameters of the network in the direction of optimality. therefore, it is possible to obtain a more optimized solution of the network and further improve the classification accuracy of the network by designing a loss function for restraining the front stage features and eliminating the information redundancy of the front stage features .für CNN, this article proposes a multi-stage feature decorrelation loss (MFD Loss), which refines effective features and eliminates information redundancy by constraining the correlation of features at all stages. considering that there are many layers in CNN, through experimental comparison and analysis, MFD Loss acts on multiple front layers of CNN, constrains the output features of each layer and each channel, and performs supervision training jointly with classification loss function during network training. compared with the single Softmax Loss supervised learning, the experiments on several commonly used datasets on several typical CNNs prove that the classification performance of Softmax Loss+MFD Loss is significantly better. meanwhile, the comparison experiments before and after the combination of MFD Loss and some other typical loss functions verify its good universality.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/25/cs.CV_2023_08_25/" data-id="clmjn91kh00430j888z9a3ska" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/25/cs.LG_2023_08_25/" class="article-date">
  <time datetime="2023-08-24T16:00:00.000Z" itemprop="datePublished">2023-08-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/25/cs.LG_2023_08_25/">cs.LG - 2023-08-25 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NeuralClothSim-Neural-Deformation-Fields-Meet-the-Kirchhoff-Love-Thin-Shell-Theory"><a href="#NeuralClothSim-Neural-Deformation-Fields-Meet-the-Kirchhoff-Love-Thin-Shell-Theory" class="headerlink" title="NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory"></a>NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12970">http://arxiv.org/abs/2308.12970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navami Kairanda, Marc Habermann, Christian Theobalt, Vladislav Golyanik</li>
<li>For: 本研究旨在提出一种基于神经网络的精炼裤子模拟方法，以解决现有的精炼裤子模拟器具有限制的问题。* Methods: 该方法使用薄shell理论来编码表面演化，并使用神经网络来学习表面的演化规律。* Results: 实验结果表明，该方法可以准确地模拟精炼裤子的演化，并且具有高效的存储和计算能力。<details>
<summary>Abstract</summary>
Cloth simulation is an extensively studied problem, with a plethora of solutions available in computer graphics literature. Existing cloth simulators produce realistic cloth deformations that obey different types of boundary conditions. Nevertheless, their operational principle remains limited in several ways: They operate on explicit surface representations with a fixed spatial resolution, perform a series of discretised updates (which bounds their temporal resolution), and require comparably large amounts of storage. Moreover, back-propagating gradients through the existing solvers is often not straightforward, which poses additional challenges when integrating them into modern neural architectures. In response to the limitations mentioned above, this paper takes a fundamentally different perspective on physically-plausible cloth simulation and re-thinks this long-standing problem: We propose NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in which surface evolution is encoded in neural network weights. Our memory-efficient and differentiable solver operates on a new continuous coordinate-based representation of dynamic surfaces, i.e., neural deformation fields (NDFs); it supervises NDF evolution with the rules of the non-linear Kirchhoff-Love shell theory. NDFs are adaptive in the sense that they 1) allocate their capacity to the deformation details as the latter arise during the cloth evolution and 2) allow surface state queries at arbitrary spatial and temporal resolutions without retraining. We show how to train our NeuralClothSim solver while imposing hard boundary conditions and demonstrate multiple applications, such as material interpolation and simulation editing. The experimental results highlight the effectiveness of our formulation and its potential impact.
</details>
<details>
<summary>摘要</summary>
cloth simulation是一个已经非常广泛研究的问题，计算机图形文献中有很多解决方案。现有的布料模拟器都可以生成真实的布料变形，但它们的运作原理受到一些限制：它们在固定的空间分辨率上操作，执行一系列精度化的更新（这限制了它们的时间分辨率），同时需要相对较大的存储空间。此外，在现有的解决方案中，将梯度反推到现代神经网络中是不直接的，这会增加集成的困难。为了解决这些限制，本文采用了一种完全不同的方法来实现物理可能的布料模拟：我们提出了一种基于薄层的新布料模拟方法，即NeuralClothSim。我们的方法使用辐射表示法来编码布料的表面演化，并且通过神经网络的权重来控制布料的演化。我们的方法是有内存效率和可微分的，可以在不需要大量存储空间和精度化更新的情况下进行模拟。我们还展示了如何在NeuralClothSim中训练模型，并在布料演化过程中遵循非线性的 Kirchhoff-Love 封闭理论来监督NDF的演化。NDF是可变的，它们可以根据布料演化的细节来分配其容量，同时允许在任何空间和时间分辨率下进行表面状态的查询无需重新训练。我们的实验结果表明，我们的方法可以具有高效性和可能的影响。
</details></li>
</ul>
<hr>
<h2 id="NeO-360-Neural-Fields-for-Sparse-View-Synthesis-of-Outdoor-Scenes"><a href="#NeO-360-Neural-Fields-for-Sparse-View-Synthesis-of-Outdoor-Scenes" class="headerlink" title="NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes"></a>NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12967">http://arxiv.org/abs/2308.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zubair-irshad/NeO-360">https://github.com/zubair-irshad/NeO-360</a></li>
<li>paper_authors: Muhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Vitor Guizilini, Thomas Kollar, Adrien Gaidon, Zsolt Kira, Rares Ambrus</li>
<li>for: 本研究旨在开发一种能够从单个或几个RGB图像中生成360度场景的神经网络方法，以解决现有方法需要费时优化多个视图的问题，从而限制其在实际世界无限范围内的应用。</li>
<li>methods: 我们提出了一种名为NeO 360的新方法，它使用神经场的方法来描述复杂的自然场景，并使用混合的图像条件三平面表示来从任何世界点进行查询。这种表示结合了精灵体的 voxel-based 表示和鸟瞰视图（BEV）表示的优点，并且在描述和推理中更有效和表达力。</li>
<li>results: 我们在提出的挑战性360度无限集成 dataset（NeRDS 360）上进行了实验，并证明了NeO 360在比较复杂的场景下能够具有更高的渲染质量和更好的普适性，而且还可以进行编辑和组合操作。<details>
<summary>Abstract</summary>
Recent implicit neural representations have shown great results for novel view synthesis. However, existing methods require expensive per-scene optimization from many views hence limiting their application to real-world unbounded urban settings where the objects of interest or backgrounds are observed from very few views. To mitigate this challenge, we introduce a new approach called NeO 360, Neural fields for sparse view synthesis of outdoor scenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenes from a single or a few posed RGB images. The essence of our approach is in capturing the distribution of complex real-world outdoor 3D scenes and using a hybrid image-conditional triplanar representation that can be queried from any world point. Our representation combines the best of both voxel-based and bird's-eye-view (BEV) representations and is more effective and expressive than each. NeO 360's representation allows us to learn from a large collection of unbounded 3D scenes while offering generalizability to new views and novel scenes from as few as a single image during inference. We demonstrate our approach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS 360, and show that NeO 360 outperforms state-of-the-art generalizable methods for novel view synthesis while also offering editing and composition capabilities. Project page: https://zubair-irshad.github.io/projects/neo360.html
</details>
<details>
<summary>摘要</summary>
近期的隐式神经表示法已经实现了出色的新视图合成效果。然而，现有的方法需要每个场景进行贵重的多视图优化，因此在真实世界中无限无Bound的城市场景中应用起来有限。为解决这个挑战，我们介绍了一种新的方法called NeO 360，它是一种普适的方法，可以从单个或几个姿态的 RGB 图像中重construct 360度场景。我们的方法的核心思想是捕捉复杂的实际室外3D场景的分布，并使用一种混合图像条件的三平面表示，可以在任何世界点上进行查询。我们的表示结合了 voxel-based 和 bird's-eye-view（BEV）表示的优点，并且比每个表示更有效和表达力强。NeO 360 的表示允许我们从大量的无限室外3D场景中学习，并在新视图和新场景中进行推理，只需要从单个图像中提取信息。我们在 propose 的挑战性 360度无限数据集（NeRDS 360）上展示了 NeO 360 的效果，并证明它在比较最佳的通用方法之上。项目页面：https://zubair-irshad.github.io/projects/neo360.html
</details></li>
</ul>
<hr>
<h2 id="Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation"><a href="#Scenimefy-Learning-to-Craft-Anime-Scene-via-Semi-Supervised-Image-to-Image-Translation" class="headerlink" title="Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation"></a>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12968">http://arxiv.org/abs/2308.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxinn-j/scenimefy">https://github.com/yuxinn-j/scenimefy</a></li>
<li>paper_authors: Yuxin Jiang, Liming Jiang, Shuai Yang, Chen Change Loy</li>
<li>for: 高质量动漫场景自实际图像转换</li>
<li>methods: 结构一致的pseudo对数据驱动学习，采用semantic-constrained StyleGAN生成质量Model priors，并应用分割指导数据选择来获得高质量 pseudo 超级vision。</li>
<li>results: 比基eline性能和 perceived quality 都有显著提高，并且可以保持具有Semantic consistency、精细 Detail和 evident Stylization 的结果。<details>
<summary>Abstract</summary>
Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.
</details>
<details>
<summary>摘要</summary>
自动高质量渲染动漫场景从复杂实际图像是有重要实用价值的。这个任务的挑战在于场景的复杂性、动漫风格的独特特征以及域之间的差距。虽然之前有一些有前途的尝试，但这些尝试仍然无法取得满意的结果，保持 semantics 的保持、明显的风格化和细节。在这个研究中，我们提出了 Scenimefy，一种新的 semi-supervised 图像-to-图像翻译框架。我们的方法利用具有结构含义的 pseudo 对数据进行引导学习，从而简化了纯无监督的设置。 pseudo 数据由具有 CLIP 的semantic-constrained StyleGAN 生成，并且通过 segmentation-guided 数据选择来获得高质量 pseudo 监督。此外，我们还引入了一种 patch-wise 对比性风格损失，以提高风格化和细节。除此之外，我们还为未来研究提供了一个高分辨率动漫场景数据集。我们的广泛的实验表明我们的方法在比较现有基线上的both perceptual quality 和量化性能方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="Dense-Text-to-Image-Generation-with-Attention-Modulation"><a href="#Dense-Text-to-Image-Generation-with-Attention-Modulation" class="headerlink" title="Dense Text-to-Image Generation with Attention Modulation"></a>Dense Text-to-Image Generation with Attention Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12964">http://arxiv.org/abs/2308.12964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naver-ai/densediffusion">https://github.com/naver-ai/densediffusion</a></li>
<li>paper_authors: Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, Jun-Yan Zhu</li>
<li>for: 将 dense captions 转化为真实的图像</li>
<li>methods: 使用 pre-trained text-to-image 模型，并通过 analyze 图像的布局与模型的中间注意力映射关系，以及开发一种注意力调整方法，使对象在指定的区域出现。</li>
<li>results: 无需进一步的 fine-tuning 或数据集，对 dense captions 进行图像生成，自动评估和人工评估分数都得到了改进，并且与特定布局条件训练的模型达到了相似的视觉效果。<details>
<summary>Abstract</summary>
Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images' layouts and the pre-trained model's intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.
</details>
<details>
<summary>摘要</summary>
原文：现有的文本到图像扩散模型很难生成真实的图像，当每个文本提示给出了具体的图像区域的详细描述时。为解决这个问题，我们提出了DenseDiffusion，一种没有培训的方法，可以使用先前训练的文本到图像模型来处理这些紧凑的文本提示，同时提供场景布局的控制。我们首先分析生成的图像布局和先前训练模型的中间注意力地图之间的关系。然后，我们开发了一种注意力调节方法，可以根据场景布局指导物体出现在特定的区域中。无需进行额外的微调或数据集，我们在给出紧凑的文本提示时改进了图像生成性能， Both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.中文翻译：现有的文本到图像扩散模型在给出紧凑的文本提示时很难生成真实的图像，例如每个文本提示给出了具体的图像区域的详细描述。为解决这个问题，我们提出了DenseDiffusion，一种没有培训的方法，可以使用先前训练的文本到图像模型来处理这些紧凑的文本提示，同时提供场景布局的控制。我们首先分析生成的图像布局和先前训练模型的中间注意力地图之间的关系。然后，我们开发了一种注意力调节方法，可以根据场景布局指导物体出现在特定的区域中。无需进行额外的微调或数据集，我们在给出紧凑的文本提示时改进了图像生成性能， Both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.
</details></li>
</ul>
<hr>
<h2 id="DLIP-Distilling-Language-Image-Pre-training"><a href="#DLIP-Distilling-Language-Image-Pre-training" class="headerlink" title="DLIP: Distilling Language-Image Pre-training"></a>DLIP: Distilling Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12956">http://arxiv.org/abs/2308.12956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huafeng Kuang, Jie Wu, Xiawu Zheng, Ming Li, Xuefeng Xiao, Rui Wang, Min Zheng, Rongrong Ji</li>
<li>For: The paper aims to explore how to distill a light Vision-Language Pre-training (VLP) model, specifically investigating the model distillation from multiple dimensions such as architecture characteristics and information transfer.* Methods: The proposed method, called DLIP, dissects the model distillation from multiple dimensions and conducts comprehensive experiments to provide insights on distilling a light but performant VLP model.* Results: The experimental results show that DLIP can achieve a state-of-the-art accuracy&#x2F;efficiency trade-off across diverse cross-modal tasks, with a 1.9x compression ratio compared to the teacher model, while retaining more than 95% of the performance and accelerating inference speed by 2.7x.Here’s the simplified Chinese version:* For: 这篇论文目标是探索如何压缩轻量级的视听预训练模型（VLP），具体来说是从多个维度进行模型压缩，包括模型结构特点和不同模式之间的信息传递。* Methods: 提出的方法是DLIP，它对模型压缩进行多维度的分析和实验，以提供适用于VLP的压缩模型。* Results: 实验结果显示，DLIP可以在多modal任务上实现状态之 искусственный智能&#x2F;效率的平衡，比如图文检索、图文描述和视觉问答等，并且可以压缩BLIP模型2.9亿个参数下来1.9倍，保持与教师模型相当或更好的性能。<details>
<summary>Abstract</summary>
Vision-Language Pre-training (VLP) shows remarkable progress with the assistance of extremely heavy parameters, which challenges deployment in real applications. Knowledge distillation is well recognized as the essential procedure in model compression. However, existing knowledge distillation techniques lack an in-depth investigation and analysis of VLP, and practical guidelines for VLP-oriented distillation are still not yet explored. In this paper, we present DLIP, a simple yet efficient Distilling Language-Image Pre-training framework, through which we investigate how to distill a light VLP model. Specifically, we dissect the model distillation from multiple dimensions, such as the architecture characteristics of different modules and the information transfer of different modalities. We conduct comprehensive experiments and provide insights on distilling a light but performant VLP model. Experimental results reveal that DLIP can achieve a state-of-the-art accuracy/efficiency trade-off across diverse cross-modal tasks, e.g., image-text retrieval, image captioning and visual question answering. For example, DLIP compresses BLIP by 1.9x, from 213M to 108M parameters, while achieving comparable or better performance. Furthermore, DLIP succeeds in retaining more than 95% of the performance with 22.4% parameters and 24.8% FLOPs compared to the teacher model and accelerates inference speed by 2.7x.
</details>
<details>
<summary>摘要</summary>
美化语言预训练（VLP）显示了惊人的进步，却因为极重的参数而困难应用于实际应用场景。知识填充是通用的程序压缩技术，但现有的知识填充技术尚未对VLP进行了深入的研究和分析，也没有提供VLP-指导的压缩指南。在这篇论文中，我们提出了DILP，一个简单又高效的语言图像预训练框架，通过该框架，我们进行了多维度模型填充的研究。具体来说，我们分析了不同模块的建筑特点和不同模式之间的信息传递。我们进行了广泛的实验，并提供了压缩轻量级VLP模型的惊人的成果。实验结果表明，DILP可以在多种跨模态任务中实现状态之最的精度/效率质量平衡，例如图像文本检索、图像captioning和视觉问答。例如，DILP可以将BLIP压缩至1.9倍，从213M Parameters下降至108M Parameters，同时保持与教师模型相同或更好的性能。此外，DILP成功地保留了95%以上的性能，使用22.4%的参数和24.8%的FLOPs，相比教师模型快速执行2.7倍。
</details></li>
</ul>
<hr>
<h2 id="BridgeData-V2-A-Dataset-for-Robot-Learning-at-Scale"><a href="#BridgeData-V2-A-Dataset-for-Robot-Learning-at-Scale" class="headerlink" title="BridgeData V2: A Dataset for Robot Learning at Scale"></a>BridgeData V2: A Dataset for Robot Learning at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12952">http://arxiv.org/abs/2308.12952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rail-berkeley/BridgeData-V2">https://github.com/rail-berkeley/BridgeData-V2</a></li>
<li>paper_authors: Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, Sergey Levine</li>
<li>For: This paper introduces BridgeData V2, a large and diverse dataset of robotic manipulation behaviors for research on scalable robot learning.* Methods: The dataset contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot, and is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions.* Results: The authors train 6 state-of-the-art imitation learning and offline reinforcement learning methods on the dataset and demonstrate that they succeed on a suite of tasks requiring varying amounts of generalization. Additionally, they show that the performance of these methods improves with more data and higher capacity models, and that training on a greater variety of skills leads to improved generalization.<details>
<summary>Abstract</summary>
We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that training on a greater variety of skills leads to improved generalization. By publicly sharing BridgeData V2 and our pre-trained models, we aim to accelerate research in scalable robot learning methods. Project page at https://rail-berkeley.github.io/bridgedata
</details>
<details>
<summary>摘要</summary>
我们介绍 BridgeData V2，一个大型和多样化的机器人 manipulate 行为数据集，用于促进可扩展的机器人学习研究。 BridgeData V2 包含 60,096 轨迹，在 24 个环境中采集，并且可以用于各种开放词汇、多任务学习方法，如图像目标或自然语言指令。在我们的实验中，我们训练了 6 种 state-of-the-art 仿制学习和离线奖励学习方法，并发现它们在一系列需要不同程度的泛化的任务上取得了成功。我们还证明了这些方法的性能随着更多的数据和更高的模型容量而提高，以及训练更多的技能会提高泛化性能。我们公共分享 BridgeData V2 和我们预训练的模型，以促进可扩展机器人学习方法的研究。项目页面位于 <https://rail-berkeley.github.io/bridgedata>
</details></li>
</ul>
<hr>
<h2 id="Label-Budget-Allocation-in-Multi-Task-Learning"><a href="#Label-Budget-Allocation-in-Multi-Task-Learning" class="headerlink" title="Label Budget Allocation in Multi-Task Learning"></a>Label Budget Allocation in Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12949">http://arxiv.org/abs/2308.12949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ximeng Sun, Kihyuk Sohn, Kate Saenko, Clayton Mellina, Xiao Bian</li>
<li>for: 提高机器学习系统的性能，解决标签成本问题</li>
<li>methods: 提出了标签预算分配问题，并提出了一种适应任务的预算分配算法</li>
<li>results: 经验表明，该算法可以在多任务学习中提高性能，并且比其他常用的标签策略更有效<details>
<summary>Abstract</summary>
The cost of labeling data often limits the performance of machine learning systems. In multi-task learning, related tasks provide information to each other and improve overall performance, but the label cost can vary among tasks. How should the label budget (i.e. the amount of money spent on labeling) be allocated among different tasks to achieve optimal multi-task performance? We are the first to propose and formally define the label budget allocation problem in multi-task learning and to empirically show that different budget allocation strategies make a big difference to its performance. We propose a Task-Adaptive Budget Allocation algorithm to robustly generate the optimal budget allocation adaptive to different multi-task learning settings. Specifically, we estimate and then maximize the extent of new information obtained from the allocated budget as a proxy for multi-task learning performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy of our approach over other widely used heuristic labeling strategies.
</details>
<details>
<summary>摘要</summary>
机器学习系统的标签成本常常限制其性能。在多任务学习中，相关任务之间共享信息，提高总体性能，但标签成本可能因任务而异。如何将标签预算（即用于标签的金钱）分配到不同任务以实现最佳多任务性能？我们是第一个提出和正式定义多任务学习标签预算分配问题，并通过实验证明不同预算分配策略对性能产生很大影响。我们提出了适应任务的预算分配算法，以适应不同的多任务学习设置。具体来说，我们估算并最大化分配的预算中新信息的总量，作为多任务学习性能的代理。 Pascal VOC和Taskonomy的实验表明，我们的方法在其他广泛使用的习惯标签策略的基础上具有较高的效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Only-On-Boundaries-a-Physics-Informed-Neural-operator-for-Solving-Parametric-Partial-Differential-Equations-in-Complex-Geometries"><a href="#Learning-Only-On-Boundaries-a-Physics-Informed-Neural-operator-for-Solving-Parametric-Partial-Differential-Equations-in-Complex-Geometries" class="headerlink" title="Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries"></a>Learning Only On Boundaries: a Physics-Informed Neural operator for Solving Parametric Partial Differential Equations in Complex Geometries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12939">http://arxiv.org/abs/2308.12939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Fang, Sifan Wang, Paris Perdikaris</li>
<li>for: 解决 parametrized boundary value problems without labeled data</li>
<li>methods: 使用 physics-informed neural operator 方法，将 PDE 转化为 boundary integral equations (BIEs)，并通过训练网络来解决</li>
<li>results: 可以快速训练网络，并可以处理复杂的形状和无界问题，实验表明方法的效果<details>
<summary>Abstract</summary>
Recently deep learning surrogates and neural operators have shown promise in solving partial differential equations (PDEs). However, they often require a large amount of training data and are limited to bounded domains. In this work, we present a novel physics-informed neural operator method to solve parametrized boundary value problems without labeled data. By reformulating the PDEs into boundary integral equations (BIEs), we can train the operator network solely on the boundary of the domain. This approach reduces the number of required sample points from $O(N^d)$ to $O(N^{d-1})$, where $d$ is the domain's dimension, leading to a significant acceleration of the training process. Additionally, our method can handle unbounded problems, which are unattainable for existing physics-informed neural networks (PINNs) and neural operators. Our numerical experiments show the effectiveness of parametrized complex geometries and unbounded problems.
</details>
<details>
<summary>摘要</summary>
最近，深度学习代理和神经操作已经显示出解决部分 diferencial equations (PDEs) 的承诺。然而，它们经常需要大量的训练数据并且受到 bounded domains 的限制。在这种工作中，我们提出了一种新的物理学习神经操作方法，可以解决没有标注数据的参数化边值问题。我们通过将 PDEs 转换为边 интегра方程 (BIEs)，可以在边界上训练操作网络，从而减少了训练数据的数量从 $O(N^d)$ 降低到 $O(N^{d-1})$，其中 $d$ 是域的维度，这对训练过程带来了显著的加速。此外，我们的方法可以处理无限制的问题，这些问题是现有的物理学习神经网络 (PINNs) 和神经操作无法解决的。我们的数学实验表明，可以有效地处理参数化复杂的几何和无限制的问题。
</details></li>
</ul>
<hr>
<h2 id="Low-count-Time-Series-Anomaly-Detection"><a href="#Low-count-Time-Series-Anomaly-Detection" class="headerlink" title="Low-count Time Series Anomaly Detection"></a>Low-count Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12925">http://arxiv.org/abs/2308.12925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Renz, Kurt Cutajar, Niall Twomey, Gavin K. C. Cheung, Hanting Xie</li>
<li>for: 本研究旨在为低频时间序列预测和检测异常点提供新的方法和工具。</li>
<li>methods: 本研究使用了一种新的生成过程，可以创建含有异常段的低频时间序列的标准数据集。此外，我们还进行了 тео리тиче和实验分析，以解释一些常用算法在低频时间序列中的缺陷。</li>
<li>results: 我们的研究发现，使用异常分数平滑可以有效地提高检测异常点的性能。此外，我们还 Validated our analysis and recommendation on a real-world dataset containing sales data for retail stores, demonstrating its practical utility.<details>
<summary>Abstract</summary>
Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly score smoothing consistently improves performance. The practical utility of our analysis and recommendation is validated on a real-world dataset containing sales data for retail stores.
</details>
<details>
<summary>摘要</summary>
低计数时序系列描述稀疏或间歇性事件，这些事件在大规模在线平台上采集和监测多种数据类型中很普遍。在模型低计数时序系列时，存在一些独特的挑战，包括低信号噪声比（畸变特征在确切检测时是不可识别的）和非均匀性（平均指标不是地方行为的代表）。现有的时序异常检测社区缺乏专门的工具和过程来模型和可靠地检测异常情况。我们填补这个空白，引入了一种新的生成过程，用于创建含有低计数时序系列异常段的 referential 数据集。通过理论和实验分析，我们解释了广泛使用的算法在正常和异常段之间的分布重叠问题。为了解决这个缺陷，我们then 利用我们的发现，示出了如何使用异常分数平滑来提高性能。我们的分析和建议在实际的零售业务数据中得到了验证。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Distributed-Multi-Agent-Reinforcement-Learning-for-EV-Charging-Network-Control"><a href="#An-Efficient-Distributed-Multi-Agent-Reinforcement-Learning-for-EV-Charging-Network-Control" class="headerlink" title="An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control"></a>An Efficient Distributed Multi-Agent Reinforcement Learning for EV Charging Network Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12921">http://arxiv.org/abs/2308.12921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amin Shojaeighadikolaei, Morteza Hashemi</li>
<li>for: 这篇论文是为了开发一个具有隐私保护的电动车充电控制器。</li>
<li>methods: 这篇论文使用多智能体循环 reinforcement learning（MARL）的架构，并将训练和执行分离，以保持隐私。</li>
<li>results: 研究结果显示，CTDE框架可以提高充电网络的性能，并降低总需求的峰值变化率（PAR），从而降低发电transformer的风险。<details>
<summary>Abstract</summary>
The increasing trend in adopting electric vehicles (EVs) will significantly impact the residential electricity demand, which results in an increased risk of transformer overload in the distribution grid. To mitigate such risks, there are urgent needs to develop effective EV charging controllers. Currently, the majority of the EV charge controllers are based on a centralized approach for managing individual EVs or a group of EVs. In this paper, we introduce a decentralized Multi-agent Reinforcement Learning (MARL) charging framework that prioritizes the preservation of privacy for EV owners. We employ the Centralized Training Decentralized Execution-Deep Deterministic Policy Gradient (CTDE-DDPG) scheme, which provides valuable information to users during training while maintaining privacy during execution. Our results demonstrate that the CTDE framework improves the performance of the charging network by reducing the network costs. Moreover, we show that the Peak-to-Average Ratio (PAR) of the total demand is reduced, which, in turn, reduces the risk of transformer overload during the peak hours.
</details>
<details>
<summary>摘要</summary>
随着电动车（EV）的普及趋势，室内电力需求将增加，从而增加分布网络中变压器的负担风险。为了解决这一问题，需要开发有效的电动车充电控制器。目前，大多数电动车充电控制器采用中央化的方法来管理个体电动车或一群电动车。在这篇论文中，我们介绍了一种分布式多代理人学习（MARL）充电框架，这种框架强调避免电动车所有者的隐私泄露。我们采用了中央训练分布执行-深度决策优化（CTDE-DDPG）方案，该方案在训练时提供有价值的信息，而执行时保持隐私。我们的结果显示，CTDE框架可以提高充电网络的性能，同时降低峰值负荷的风险。此外，我们还发现，峰值至平均值比（PAR）的总需求下降，从而降低了变压器过载的风险。
</details></li>
</ul>
<hr>
<h2 id="Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP"><a href="#Towards-Realistic-Unsupervised-Fine-tuning-with-CLIP" class="headerlink" title="Towards Realistic Unsupervised Fine-tuning with CLIP"></a>Towards Realistic Unsupervised Fine-tuning with CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12919">http://arxiv.org/abs/2308.12919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Liang, Lijun Sheng, Zhengbo Wang, Ran He, Tieniu Tan</li>
<li>for: 这篇论文的目的是探讨CLIP模型的不监控下精确推导，并且考虑到测验数据可能包含未知的类别。</li>
<li>methods: 这篇论文提出了一个简单、高效、有效的精确推导方法，称为全局Entropy优化（UEO），它利用样本级别的信任度来最小化可信度的条件 entropy 和最大化不可信度的条件 entropy。</li>
<li>results: 这篇论文通过对15个领域和4种不同的内部知识进行广泛的实验，展示了 UEO 在精确推导和未知类别检测方面的表现都较好。<details>
<summary>Abstract</summary>
The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.   To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.
</details>
<details>
<summary>摘要</summary>
“随着感知语言模型（VLM）的出现，如CLIP，研究人员对其应用于下游指导学习任务进行了大量的研究努力。 although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. 在这篇论文中，我们进行了一个实际的无超级 fine-tuningenario，assuming that the unlabeled data may contain out-of-distribution samples from unknown classes. 此外，我们强调了同时增强不同类别的检测能力和预先定义的类别标签相关的instance检测。  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entropy of less confident instances. Apart from optimizing the textual prompts, UEO also incorporates optimization of channel-wise affine transformations within the visual branch of CLIP. Through extensive experiments conducted across 15 domains and 4 different types of prior knowledge, we demonstrate that UEO surpasses baseline methods in terms of both generalization and out-of-distribution detection.”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Vulnerabilities-in-ML-systems-in-terms-of-adversarial-attacks"><a href="#Evaluating-the-Vulnerabilities-in-ML-systems-in-terms-of-adversarial-attacks" class="headerlink" title="Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks"></a>Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12918">http://arxiv.org/abs/2308.12918</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Harshith, Mantej Singh Gill, Madhan Jothimani</li>
<li>for: 本研究探讨了现代深度学习系统中的攻击漏洞问题，尤其是新型的敏感攻击方法对现有防御系统的挑战。</li>
<li>methods: 作者采用了许多不同的方法来研究攻击漏洞，包括讲解攻击漏洞的起源、随机例子和敏感例子的区别，以及攻击漏洞的伦理问题。</li>
<li>results: 研究发现，攻击漏洞可能对现有防御系统 pose 挑战，并且需要采取适当的训练措施来准备AI系统进行更广泛的应用。<details>
<summary>Abstract</summary>
There have been recent adversarial attacks that are difficult to find. These new adversarial attacks methods may pose challenges to current deep learning cyber defense systems and could influence the future defense of cyberattacks. The authors focus on this domain in this research paper. They explore the consequences of vulnerabilities in AI systems. This includes discussing how they might arise, differences between randomized and adversarial examples and also potential ethical implications of vulnerabilities. Moreover, it is important to train the AI systems appropriately when they are in testing phase and getting them ready for broader use.
</details>
<details>
<summary>摘要</summary>
有些最新的敌对攻击方法已经变得很难找到。这些新的敌对攻击方法可能会对当前的深度学习网络防御系统 pose 挑战，并可能影响未来的网络攻击防御。作者在这篇研究报告中关注这个领域。他们探讨了人工智能系统披露漏洞的后果，包括披露漏洞如何产生，Randomized和敌对示例之间的差异，以及披露漏洞的伦理性。此外，在测试阶段，我们需要适当地训练人工智能系统，以便在更广泛的应用中使用。
</details></li>
</ul>
<hr>
<h2 id="POLCA-Power-Oversubscription-in-LLM-Cloud-Providers"><a href="#POLCA-Power-Oversubscription-in-LLM-Cloud-Providers" class="headerlink" title="POLCA: Power Oversubscription in LLM Cloud Providers"></a>POLCA: Power Oversubscription in LLM Cloud Providers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12908">http://arxiv.org/abs/2308.12908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, Ricardo Bianchini</li>
<li>for:  This paper is written to address the issue of power oversubscription in large language model (LLM) clusters, and to propose a framework for power oversubscription that is robust, reliable, and readily deployable.</li>
<li>methods:  The paper uses extensive characterization of power consumption patterns of various LLMs and their configurations, as well as simulations using open-source models to replicate the power patterns observed in production.</li>
<li>results:  The paper shows that there is a significant opportunity to oversubscribe power in LLM clusters, and that the average and peak power utilization in LLM clusters for inference should not be very high. The proposed framework, POLCA, is demonstrated to be able to deploy 30% more servers in the same GPU cluster for inference with minimal performance loss.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解决大语言模型（LLM）集群中的电力过载问题，并提出一个可靠、可靠、ready to deploy的电力过载框架。</li>
<li>methods: 这篇论文使用了大量的LLM的电力消耗特征和配置的描述，以及使用开源模型来复制生产环境中观察到的电力特征来进行模拟。</li>
<li>results: 论文显示了LLM集群中的电力过载潜在性，并证明了average和peak电力使用率在LLM集群中的推理任务应该不太高。提议的框架POLCA可以在同一个GPU集群中部署30%更多的服务器来进行推理，而无需 sacrify性能。<details>
<summary>Abstract</summary>
Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow.   We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the average and peak power utilization in LLM clusters for inference should not be very high. Our deductions align with the data from production LLM clusters, revealing that inference workloads offer substantial headroom for power oversubscription. However, the stringent set of telemetry and controls that GPUs offer in a virtualized environment, makes it challenging to have a reliable and robust power oversubscription mechanism.   We propose POLCA, our framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters. Using open-source models to replicate the power patterns observed in production, we simulate POLCA and demonstrate that we can deploy 30% more servers in the same GPU cluster for inference, with minimal performance loss
</details>
<details>
<summary>摘要</summary>
Recent innovations in large language models (LLMs) have led to a surge in demand for datacenter GPUs, driving up compute capacity needs. Several cloud providers and enterprises have made significant plans for growth in their datacenters to support these new workloads. However, power remains a key bottleneck resource in datacenters, and as LLMs continue to increase in size, they are becoming increasingly power-intensive. In this paper, we explore the opportunity to oversubscribe power in LLM clusters, which can improve power efficiency, allow for more deployable servers per datacenter, and reduce deployment time.We analyze the power consumption patterns of various LLMs and their configurations, identifying differences between inference and training power consumption patterns. Based on our findings, we claim that the average and peak power utilization in LLM clusters for inference should not be very high. Our conclusions align with data from production LLM clusters, indicating that inference workloads offer substantial headroom for power oversubscription.However, the strict telemetry and controls offered by GPUs in a virtualized environment make it challenging to implement a reliable and robust power oversubscription mechanism. To address this challenge, we propose POLCA, our framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters.Using open-source models to replicate the power patterns observed in production, we simulate POLCA and demonstrate that we can deploy 30% more servers in the same GPU cluster for inference with minimal performance loss.
</details></li>
</ul>
<hr>
<h2 id="CDAN-Convolutional-Dense-Attention-guided-Network-for-Low-light-Image-Enhancement"><a href="#CDAN-Convolutional-Dense-Attention-guided-Network-for-Low-light-Image-Enhancement" class="headerlink" title="CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement"></a>CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12902">http://arxiv.org/abs/2308.12902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shakibania, Sina Raoufi, Hassan Khotanlou</li>
<li>for: 提高低光照图像的品质和明暗分布，以便更好地进行图像分析和 интерпретация。</li>
<li>methods: 基于 autoencoder 架构，具有卷积和权重块，并且具有注意力机制和跳过连接。</li>
<li>results: 在低光照图像增强任务中表现出色，与现有技术相比显著提高了图像的品质和明暗分布，并且在多种低光照场景中表现稳定和可靠。<details>
<summary>Abstract</summary>
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs remarkably on benchmark datasets, effectively mitigating under-exposure and proficiently restoring textures and colors in diverse low-light scenarios. This achievement underscores CDAN's potential for diverse computer vision tasks, notably enabling robust object detection and recognition in challenging low-light conditions.
</details>
<details>
<summary>摘要</summary>
低光照图像，受到不足照明的限制，会呈现出降低的清晰度、淡化的颜色和减少的细节。低光照图像增强是计算机视觉中的一项重要任务，旨在通过提高亮度、对比度和总体观察质量来促进准确的分析和解释。本文介绍了一种新的低光照图像增强方法——卷积神经网络加注意机制（CDAN）。CDAN结合了自适应网络架构、卷积块和稠密块，并且添加了注意机制和跳过连接。这种架构使得信息传递得到有效地进行，并且Feature学习得到了保障。此外，专门的后处理阶段可以进一步调整颜色均衡和对比度。我们的方法在低光照图像增强任务中表现出了明显的进步，在多种复杂的场景中都能够达到州前的结果。我们的模型在标准测试集上表现出了remarkable的表现，可以有效地抑制下 exposure 和修充细节和颜色在多种低光照场景中。这一成就表明CDAN在计算机视觉任务中具有广泛的潜力，特别是在挑战性的低光照条件下进行Robust对象检测和识别。
</details></li>
</ul>
<hr>
<h2 id="Unified-Data-Management-and-Comprehensive-Performance-Evaluation-for-Urban-Spatial-Temporal-Prediction-Experiment-Analysis-Benchmark"><a href="#Unified-Data-Management-and-Comprehensive-Performance-Evaluation-for-Urban-Spatial-Temporal-Prediction-Experiment-Analysis-Benchmark" class="headerlink" title="Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]"></a>Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis &amp; Benchmark]</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12899">http://arxiv.org/abs/2308.12899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/libcity/bigscity-libcity">https://github.com/libcity/bigscity-libcity</a></li>
<li>paper_authors: Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang</li>
<li>for: 这个论文的目的是提高城市空间时间预测的精度和效率，以及为城市规划和管理带来长期的贡献。</li>
<li>methods: 该论文使用了“原子文件”的统一存储格式，以及对城市空间时间预测模型的全面概述和实验研究，以解决城市空间时间数据的访问和利用问题。</li>
<li>results: 该论文的实验结果显示，使用“原子文件”可以简化城市空间时间数据的管理，而且对40个多样化的数据集进行验证，并提供了一个性能排名，以帮助未来的研究。<details>
<summary>Abstract</summary>
The field of urban spatial-temporal prediction is advancing rapidly with the development of deep learning techniques and the availability of large-scale datasets. However, challenges persist in accessing and utilizing diverse urban spatial-temporal datasets from different sources and stored in different formats, as well as determining effective model structures and components with the proliferation of deep learning models. This work addresses these challenges and provides three significant contributions. Firstly, we introduce "atomic files", a unified storage format designed for urban spatial-temporal big data, and validate its effectiveness on 40 diverse datasets, simplifying data management. Secondly, we present a comprehensive overview of technological advances in urban spatial-temporal prediction models, guiding the development of robust models. Thirdly, we conduct extensive experiments using diverse models and datasets, establishing a performance leaderboard and identifying promising research directions. Overall, this work effectively manages urban spatial-temporal data, guides future efforts, and facilitates the development of accurate and efficient urban spatial-temporal prediction models. It can potentially make long-term contributions to urban spatial-temporal data management and prediction, ultimately leading to improved urban living standards.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术的发展和大规模数据的可用性，城市空间时间预测领域在迅速进步。然而，获取和利用多种城市空间时间数据的挑战仍然存在，这些数据来源于不同的地方，格式也不同。此外，深度学习模型的迅速增加也使得确定有效的模型结构和组件变得更加复杂。本文解决这些挑战，并为城市空间时间预测领域提供三大贡献。首先，我们引入“原子文件”，一种适合城市空间时间大数据的统一存储格式，并在40个多样化数据集上验证其效果，从而简化数据管理。其次，我们提供城市空间时间预测模型技术的全面概述，指导未来的发展。最后，我们通过多种模型和数据集进行广泛的实验，建立了性能排名和有前途的研究方向。总之，本文有效地管理城市空间时间数据，引导未来的努力，并促进了城市空间时间预测模型的准确和高效。这可能在长期内对城市空间时间数据管理和预测产生深远的影响，从而提高城市生活标准。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Document-Page-Classification-Design-Datasets-and-Challenges"><a href="#Beyond-Document-Page-Classification-Design-Datasets-and-Challenges" class="headerlink" title="Beyond Document Page Classification: Design, Datasets, and Challenges"></a>Beyond Document Page Classification: Design, Datasets, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12896">http://arxiv.org/abs/2308.12896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordy Van Landeghem, Sanket Biswas, Matthew B. Blaschko, Marie-Francine Moens</li>
<li>for: 这篇论文提出了将文档分类评测更加接近实际应用的需求，包括数据的性质（多通道、多页、多产业）和分类任务的类型（多页文档、页流、文档包）。</li>
<li>methods: 论文指出了公共多页文档分类数据集缺乏，正式了不同应用场景中的分类任务，并强调了完整文档表示的重要性。</li>
<li>results: 实验研究表明，现有的分类指标已经成为了无关的，需要更新以评测实际中的完整文档。这也需要更加成熟的评价方法，涵盖calibration评估、计算复杂性（时间-内存）和多种实际分布变换（例如，生成vs扫描噪声、页码顺序变换）。<details>
<summary>Abstract</summary>
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distribution shifts (e.g., born-digital vs. scanning noise, shifting page order). Our study ends on a hopeful note by recommending concrete avenues for future improvements.}
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/25/cs.LG_2023_08_25/" data-id="clmjn91mq007j0j882iejfk5i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.AI_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.AI_2023_08_24/">cs.AI - 2023-08-24 20:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FaceTouch-Detecting-hand-to-face-touch-with-supervised-contrastive-learning-to-assist-in-tracing-infectious-disease"><a href="#FaceTouch-Detecting-hand-to-face-touch-with-supervised-contrastive-learning-to-assist-in-tracing-infectious-disease" class="headerlink" title="FaceTouch: Detecting hand-to-face touch with supervised contrastive learning to assist in tracing infectious disease"></a>FaceTouch: Detecting hand-to-face touch with supervised contrastive learning to assist in tracing infectious disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12840">http://arxiv.org/abs/2308.12840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed R. Ibrahim, Terry Lyons</li>
<li>for: 本研究旨在提出一种基于深度学习的计算机视觉框架，以探索在复杂的城市场景中自动检测人员之间的手带面接触。</li>
<li>methods: 该框架基于深度学习的两个子模型，一个用于检测人员，另一个用于分析人员的动作。 FaceTouch 使用RGB图像来检测手带面接触，并利用人体姿势such as arm movement来减少部分遮挡。</li>
<li>results: 研究表明，FaceTouch 在 Complex urban scenes 中能够准确检测手带面接触，并在未经过其他数据集训练的情况下显示了强验证能力。<details>
<summary>Abstract</summary>
Through our respiratory system, many viruses and diseases frequently spread and pass from one person to another. Covid-19 served as an example of how crucial it is to track down and cut back on contacts to stop its spread. There is a clear gap in finding automatic methods that can detect hand-to-face contact in complex urban scenes or indoors. In this paper, we introduce a computer vision framework, called FaceTouch, based on deep learning. It comprises deep sub-models to detect humans and analyse their actions. FaceTouch seeks to detect hand-to-face touches in the wild, such as through video chats, bus footage, or CCTV feeds. Despite partial occlusion of faces, the introduced system learns to detect face touches from the RGB representation of a given scene by utilising the representation of the body gestures such as arm movement. This has been demonstrated to be useful in complex urban scenarios beyond simply identifying hand movement and its closeness to faces. Relying on Supervised Contrastive Learning, the introduced model is trained on our collected dataset, given the absence of other benchmark datasets. The framework shows a strong validation in unseen datasets which opens the door for potential deployment.
</details>
<details>
<summary>摘要</summary>
我们的呼吸系统中有许多病毒和疾病经常传播和传递 від一个人到另一个。COVID-19 作为一个例子，说明了如何重要地追踪和降低联系以阻据其传播。然而，在复杂的城市场景或室内环境中找到自动方法检测手部触摸是一个明显的潜在难点。在这篇文章中，我们介绍了一个基于深度学习的计算机视觉框架，called FaceTouch。这个框架包括深度子模型以检测人类和分析他们的动作。FaceTouch 目标在野兽中检测手部触摸，例如透过视频聊天、公共汽车录影或 CCTV 输入。即使人脸部分被遮蔽，引入的系统可以从 RGB 表示中检测手部触摸，利用人体姿势的变化，如臂部运动。这已经在复杂的城市场景中显示出了实用性，超出了单纯检测手部运动和距离面部的能力。我们靠Supervised Contrastive Learning 进行训练，使用我们收集的数据集，因为没有其他参考数据集。引入的模型在未见 datasets 中显示了强大的验证，这开启了潜在的应用之门。
</details></li>
</ul>
<hr>
<h2 id="Short-Run-Transit-Route-Planning-Decision-Support-System-Using-a-Deep-Learning-Based-Weighted-Graph"><a href="#Short-Run-Transit-Route-Planning-Decision-Support-System-Using-a-Deep-Learning-Based-Weighted-Graph" class="headerlink" title="Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph"></a>Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12828">http://arxiv.org/abs/2308.12828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadav Shalit, Michael Fire, Dima Kagan, Eran Ben-Elia</li>
<li>for: 提高公共交通服务的效率和可靠性，帮助公共交通观察者快速地找到更好的路线。</li>
<li>methods: 使用深度学习技术建立一个决策支持系统，将多种数据来源（如GTFS和智能卡数据）处理和模型，并使用自我超vision进行训练，以预测路段延迟值。这些延迟值被用作交通Graph的边重量，以便高效地寻找路径。</li>
<li>results: 在 tel aviv 进行评估中，我们能够 reducemore than 9% of the routes, including both intraurban and suburban routes, highlighting the model’s versatility and effectiveness in improving public transport services.<details>
<summary>Abstract</summary>
Public transport routing plays a crucial role in transit network design, ensuring a satisfactory level of service for passengers. However, current routing solutions rely on traditional operational research heuristics, which can be time-consuming to implement and lack the ability to provide quick solutions. Here, we propose a novel deep learning-based methodology for a decision support system that enables public transport (PT) planners to identify short-term route improvements rapidly. By seamlessly adjusting specific sections of routes between two stops during specific times of the day, our method effectively reduces times and enhances PT services. Leveraging diverse data sources such as GTFS and smart card data, we extract features and model the transportation network as a directed graph. Using self-supervision, we train a deep learning model for predicting lateness values for road segments.   These lateness values are then utilized as edge weights in the transportation graph, enabling efficient path searching. Through evaluating the method on Tel Aviv, we are able to reduce times on more than 9\% of the routes. The improved routes included both intraurban and suburban routes showcasing a fact highlighting the model's versatility. The findings emphasize the potential of our data-driven decision support system to enhance public transport and city logistics, promoting greater efficiency and reliability in PT services.
</details>
<details>
<summary>摘要</summary>
公共交通路径规划在公共交通网络设计中发挥重要作用，确保乘客获得满意的服务水平。然而，当前的路径解决方案通常基于传统的操作研究策略，可能需要较长时间来实现并且缺乏快速解决方案。在这里，我们提出了一种基于深度学习的决策支持系统，可以帮助公共交通（PT）规划人员在短时间内迅速地提高路径。通过在两个停站之间的特定路段进行轻量级调整，我们的方法可以减少时间并提高PT服务质量。我们利用了多种数据源，如GTFS和智能卡数据，提取特征并将交通网络模型为指定图。使用无监督学习，我们训练了一个深度学习模型，可以预测路段延迟值。这些延迟值然后被用作路径搜索的边重量，使得路径搜索更加高效。通过对特拉维夫进行评估，我们可以减少路线的时间超过9％。改进的路线包括城市内和郊区路线，这一结果表明模型的 universality。这些发现强调了我们数据驱动的决策支持系统的潜在能力，推动公共交通和城市物流的更高效和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Job-Shop-Scheduling-Benchmark-Environments-and-Instances-for-Learning-and-Non-learning-Methods"><a href="#Job-Shop-Scheduling-Benchmark-Environments-and-Instances-for-Learning-and-Non-learning-Methods" class="headerlink" title="Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods"></a>Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12794">http://arxiv.org/abs/2308.12794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-for-decision-making-tue/job_shop_scheduling_benchmark">https://github.com/ai-for-decision-making-tue/job_shop_scheduling_benchmark</a></li>
<li>paper_authors: Robbert Reijnen, Kjell van Straaten, Zaharah Bukhsh, Yingqian Zhang</li>
<li>for: 提供一个中央化的测试平台供研究人员、实践者和热门追求者在机器平面管理中解决问题。</li>
<li>methods: 使用 GitHub 开源存储平台提供了丰富的测试库，涵盖了各种机器平面管理问题，包括 Job Shop Scheduling (JSP)、Flow Shop Scheduling (FSP)、Flexible Job Shop Scheduling (FJSP)、FJSP with Assembly constraints (FAJSP)、FJSP with Sequence-Dependent Setup Times (FJSP-SDST) 和在线 FJSP。</li>
<li>results: 提供了一个中央化的测试平台，以便研究人员、实践者和热门追求者可以在一个集中化的位置上解决机器平面管理的挑战。<details>
<summary>Abstract</summary>
We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
</details>
<details>
<summary>摘要</summary>
我们介绍一个开源的GitHub存储库，包含了各种机器调度问题的完整的benchmark，包括作业shop调度（JSP）、流shop调度（FSP）、可变作业shop调度（FJSP）、FJSP具有组装约束（FAJSP）、FJSP具有时间序列依赖的设置（FJSP-SDST）以及在线FJSP。我们的主要目标是为研究人员、实践者和爱好者提供一个中心化的平台，以便他们可以解决机器调度挑战。
</details></li>
</ul>
<hr>
<h2 id="Acquiring-Qualitative-Explainable-Graphs-for-Automated-Driving-Scene-Interpretation"><a href="#Acquiring-Qualitative-Explainable-Graphs-for-Automated-Driving-Scene-Interpretation" class="headerlink" title="Acquiring Qualitative Explainable Graphs for Automated Driving Scene Interpretation"></a>Acquiring Qualitative Explainable Graphs for Automated Driving Scene Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12755">http://arxiv.org/abs/2308.12755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nassim Belmecheri, Arnaud Gotlieb, Nadjib Lazaar, Helge Spieker</li>
<li>for: 这篇论文旨在提出一种新的自动驾驶场景表示方法，以便更好地解释自动驾驶的决策。</li>
<li>methods: 该方法基于 Qualitative Constraint Acquisition  paradigm，可以快速计算出自动驾驶场景的Qualitative eXplainable Graph。</li>
<li>results: 实验结果表明，这种方法可以在实时计算和快速存储的情况下构建自动驾驶场景的Qualitative eXplainable Graph，这使得它成为可能有用的工具 для提高自动驾驶的识别和控制过程。<details>
<summary>Abstract</summary>
The future of automated driving (AD) is rooted in the development of robust, fair and explainable artificial intelligence methods. Upon request, automated vehicles must be able to explain their decisions to the driver and the car passengers, to the pedestrians and other vulnerable road users and potentially to external auditors in case of accidents. However, nowadays, most explainable methods still rely on quantitative analysis of the AD scene representations captured by multiple sensors. This paper proposes a novel representation of AD scenes, called Qualitative eXplainable Graph (QXG), dedicated to qualitative spatiotemporal reasoning of long-term scenes. The construction of this graph exploits the recent Qualitative Constraint Acquisition paradigm. Our experimental results on NuScenes, an open real-world multi-modal dataset, show that the qualitative eXplainable graph of an AD scene composed of 40 frames can be computed in real-time and light in space storage which makes it a potentially interesting tool for improved and more trustworthy perception and control processes in AD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Motion-In-Betweening-with-Phase-Manifolds"><a href="#Motion-In-Betweening-with-Phase-Manifolds" class="headerlink" title="Motion In-Betweening with Phase Manifolds"></a>Motion In-Betweening with Phase Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12751">http://arxiv.org/abs/2308.12751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pauzii/phasebetweener">https://github.com/pauzii/phasebetweener</a></li>
<li>paper_authors: Paul Starke, Sebastian Starke, Taku Komura, Frank Steinicke</li>
<li>for:  This paper introduces a novel data-driven motion in-betweening system to reach target poses of characters.</li>
<li>methods: The paper uses a mixture-of-experts neural network model, a Periodic Autoencoder, and a learned bi-directional control scheme to generate smooth and realistic character movements.</li>
<li>results: The proposed framework can compete with popular state-of-the-art methods for motion in-betweening in terms of motion quality and generalization, especially in the existence of long transition durations, and can also synthesize more challenging movements beyond locomotion behaviors. Additionally, style control is enabled between given target keyframes.<details>
<summary>Abstract</summary>
This paper introduces a novel data-driven motion in-betweening system to reach target poses of characters by making use of phases variables learned by a Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network model, in which the phases cluster movements in both space and time with different expert weights. Each generated set of weights then produces a sequence of poses in an autoregressive manner between the current and target state of the character. In addition, to satisfy poses which are manually modified by the animators or where certain end effectors serve as constraints to be reached by the animation, a learned bi-directional control scheme is implemented to satisfy such constraints. The results demonstrate that using phases for motion in-betweening tasks sharpen the interpolated movements, and furthermore stabilizes the learning process. Moreover, using phases for motion in-betweening tasks can also synthesize more challenging movements beyond locomotion behaviors. Additionally, style control is enabled between given target keyframes. Our proposed framework can compete with popular state-of-the-art methods for motion in-betweening in terms of motion quality and generalization, especially in the existence of long transition durations. Our framework contributes to faster prototyping workflows for creating animated character sequences, which is of enormous interest for the game and film industry.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Separating-the-Human-Touch-from-AI-Generated-Text-using-Higher-Criticism-An-Information-Theoretic-Approach"><a href="#Separating-the-Human-Touch-from-AI-Generated-Text-using-Higher-Criticism-An-Information-Theoretic-Approach" class="headerlink" title="Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach"></a>Separating the Human Touch from AI-Generated Text using Higher Criticism: An Information-Theoretic Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12747">http://arxiv.org/abs/2308.12747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alon Kipnis</li>
<li>for: 本研究旨在判断一篇文章是否完全由生成语言模型编写，或者另一种情况下，文章包含了一些重要的人工编辑。</li>
<li>methods: 本研究使用多种困惑测试来评估文章中各句的起源，并将这些测试结果组合使用高等批判（HC）。这种方法可以同时判断各句的起源是否为生成语言模型所致，以及哪些句子可能有人工编辑。</li>
<li>results: 研究使用实际数据进行了证明，并分析了影响方法效果的因素。这种分析提出了一些有趣的开放挑战，解决这些挑战可能会提高方法的效果。<details>
<summary>Abstract</summary>
We propose a method to determine whether a given article was entirely written by a generative language model versus an alternative situation in which the article includes some significant edits by a different author, possibly a human. Our process involves many perplexity tests for the origin of individual sentences or other text atoms, combining these multiple tests using Higher Criticism (HC). As a by-product, the method identifies parts suspected to be edited. The method is motivated by the convergence of the log-perplexity to the cross-entropy rate and by a statistical model for edited text saying that sentences are mostly generated by the language model, except perhaps for a few sentences that might have originated via a different mechanism. We demonstrate the effectiveness of our method using real data and analyze the factors affecting its success. This analysis raises several interesting open challenges whose resolution may improve the method's effectiveness.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于判断一篇文章是否完全由生成语言模型写成，或者是一种另外的情况，文章包含了一些重要的人工修改。我们的过程包括多种plexity测试，对各个句子或其他文本元素的起源进行组合使用高等批判（HC）。这种方法可以标识可能有人工修改的部分。我们的方法受到了log-plexity converging到cross-entropy rate的统计模型，以及一种编辑文本的统计模型，即句子主要由语言模型生成，除了一些句子可能通过不同的机制生成。我们使用实际数据进行示例，并分析了这种方法的成功因素。这种分析提出了一些有趣的开放挑战，解决这些挑战可能会提高方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Human-Comprehensible-Active-Learning-of-Genome-Scale-Metabolic-Networks"><a href="#Human-Comprehensible-Active-Learning-of-Genome-Scale-Metabolic-Networks" class="headerlink" title="Human Comprehensible Active Learning of Genome-Scale Metabolic Networks"></a>Human Comprehensible Active Learning of Genome-Scale Metabolic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12740">http://arxiv.org/abs/2308.12740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lun Ai, Shi-Shun Liang, Wang-Zhou Dai, Liam Hallett, Stephen H. Muggleton, Geoff S. Baldwin</li>
<li>for: 这个论文主要用于提高干预生物学中的设计、建造、测试和学习（DBTL）循环中的实验设计和成本降低。</li>
<li>methods: 该论文提出了一种基于卷积逻辑编程（ILP）的新机器学习框架ILP-iML1515，该框架通过逻辑推理和学习新的逻辑结构从auxotrophic异常变种试验中更新模型。</li>
<li>results: ILP-iML1515可以高速进行模拟和活动地选择实验，并且可以在测试新功能蛋白时降低实验成本。<details>
<summary>Abstract</summary>
An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that reduce the experimental cost of learning gene functions in comparison to randomly selected experiments.
</details>
<details>
<summary>摘要</summary>
синтетиче生物的重要应用之一是通过引入主机细胞系统来生产有用产品。然而，随着主机系统的扩大，设计空间的增加导致了庞大的实验成本和高效性的需求。我们需要一种可靠的机器学习方法，能够有效地探索假设空间，并 guid experimental design 进行 DBTL 循环。我们提出了一种基于推理学习的机器学习框架 ILP-iML1515，通过推理逻辑学习来协助主机细胞系统的设计和验证。与数值模型不同，ILP-iML1515 基于可读性的逻辑表示，可以通过学习新的逻辑结构来更新模型，并且能够高效地进行大规模的 simulations。此外，ILP-iML1515 框架还可以活动地选择实验，以降低学习基因功能的实验成本，相比于随机选择的实验。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Co-Training-with-Explainable-Cell-Graph-Ensembling-for-Histopathological-Image-Classification"><a href="#Asymmetric-Co-Training-with-Explainable-Cell-Graph-Ensembling-for-Histopathological-Image-Classification" class="headerlink" title="Asymmetric Co-Training with Explainable Cell Graph Ensembling for Histopathological Image Classification"></a>Asymmetric Co-Training with Explainable Cell Graph Ensembling for Histopathological Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12737">http://arxiv.org/abs/2308.12737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Yang, Zhongyu Li, Chen Liu, Xiangde Luo, Xingguang Wang, Dou Xu, Chaoqun Li, Xiaoying Qin, Meng Yang, Long Jin</li>
<li>for: This paper focuses on multi-class histopathological image classification, with the goal of improving explainability and performance.</li>
<li>methods: The proposed method combines a deep graph convolutional network and a convolutional neural network, with an asymmetric co-training framework to dynamically integrate pixel-level and cell-level information.</li>
<li>results: The proposed method achieves superior performance, explainability, and generalizability in multi-class histopathological image classification, as demonstrated on private and public datasets.Here is the full text in Simplified Chinese:</li>
<li>for: 本研究旨在提高多类组织肿瘤图像分类的可解释性和性能。</li>
<li>methods: 提议的方法结合深度图像神经网络和图像神经网络，采用不对称共训框架，动态地集成像素级和细胞级信息。</li>
<li>results: 提议的方法在多类组织肿瘤图像分类中具有优秀的性能、可解释性和普适性，如 demonstrated 在私有和公共数据集上。<details>
<summary>Abstract</summary>
Convolutional neural networks excel in histopathological image classification, yet their pixel-level focus hampers explainability. Conversely, emerging graph convolutional networks spotlight cell-level features and medical implications. However, limited by their shallowness and suboptimal use of high-dimensional pixel data, GCNs underperform in multi-class histopathological image classification. To make full use of pixel-level and cell-level features dynamically, we propose an asymmetric co-training framework combining a deep graph convolutional network and a convolutional neural network for multi-class histopathological image classification. To improve the explainability of the entire framework by embedding morphological and topological distribution of cells, we build a 14-layer deep graph convolutional network to handle cell graph data. For the further utilization and dynamic interactions between pixel-level and cell-level information, we also design a co-training strategy to integrate the two asymmetric branches. Notably, we collect a private clinically acquired dataset termed LUAD7C, including seven subtypes of lung adenocarcinoma, which is rare and more challenging. We evaluated our approach on the private LUAD7C and public colorectal cancer datasets, showcasing its superior performance, explainability, and generalizability in multi-class histopathological image classification.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks 在 Histopathological 图像分类中表现出色，但是它们的像素级别关注使得解释性受限。相反，出现在的图像 convolutional neural networks 注重 cell 级别特征和医学意义。然而，由于它们的浅度和高维像素数据的不佳使用，GCNs在多类 Histopathological 图像分类中表现不佳。为了在动态地使用像素级别和 cell 级别特征，我们提议一种不对称 co-training 框架， combining a deep graph convolutional network 和一个 convolutional neural network  для多类 Histopathological 图像分类。为了提高整个框架的解释性，我们建立了一个 14 层深的 graph convolutional network 来处理 cell graph 数据。此外，我们还设计了一种 co-training 策略，以实现像素级别和 cell 级别信息之间的动态交互。值得一提的是，我们收集了一个私有的临床获得的数据集 termed LUAD7C，包括七种肺adenocarcinoma 的亚型，这种数据集是罕见且更加挑战。我们对私人 LUAD7C 和公共的 colorectal cancer 数据集进行评估，展示了我们的方法在多类 Histopathological 图像分类中的优秀表现、解释性和普适性。
</details></li>
</ul>
<hr>
<h2 id="DeepLOC-Deep-Learning-based-Bone-Pathology-Localization-and-Classification-in-Wrist-X-ray-Images"><a href="#DeepLOC-Deep-Learning-based-Bone-Pathology-Localization-and-Classification-in-Wrist-X-ray-Images" class="headerlink" title="DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images"></a>DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12727">http://arxiv.org/abs/2308.12727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/olegrgv/DeepLOC">https://github.com/olegrgv/DeepLOC</a></li>
<li>paper_authors: Razan Dibo, Andrey Galichin, Pavel Astashev, Dmitry V. Dylov, Oleg Y. Rogov</li>
<li>for: 该论文旨在帮助放射学家更加准确和高效地分析骨病变图像。</li>
<li>methods: 该方法结合了YOLO和Shifted Window Transformer（Swin），并提出了一个新的块来解决骨病变图像分类和定位的两大挑战。YOLO框架用于检测和定位骨病变，利用其实时物体检测功能；而Swin则用于从定位区域中提取 Contextual information，以准确地分类骨病变。</li>
<li>results: 该方法可以准确地定位和分类骨病变，并且可以提高放射学家的分析效率和准确率。<details>
<summary>Abstract</summary>
In recent years, computer-aided diagnosis systems have shown great potential in assisting radiologists with accurate and efficient medical image analysis. This paper presents a novel approach for bone pathology localization and classification in wrist X-ray images using a combination of YOLO (You Only Look Once) and the Shifted Window Transformer (Swin) with a newly proposed block. The proposed methodology addresses two critical challenges in wrist X-ray analysis: accurate localization of bone pathologies and precise classification of abnormalities. The YOLO framework is employed to detect and localize bone pathologies, leveraging its real-time object detection capabilities. Additionally, the Swin, a transformer-based module, is utilized to extract contextual information from the localized regions of interest (ROIs) for accurate classification.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Continuous-Reinforcement-Learning-based-Dynamic-Difficulty-Adjustment-in-a-Visual-Working-Memory-Game"><a href="#Continuous-Reinforcement-Learning-based-Dynamic-Difficulty-Adjustment-in-a-Visual-Working-Memory-Game" class="headerlink" title="Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game"></a>Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12726">http://arxiv.org/abs/2308.12726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoud Rahimi, Hadi Moradi, Abdol-hossein Vahabie, Hamed Kebriaei</li>
<li>for: 这个论文目的是提出一种基于强化学习的游戏难度调整方法，以提高玩家的游戏体验。</li>
<li>methods: 该方法使用了连续控制器学习（RL）方法，并使用了视觉工作记忆（VWM）游戏来处理复杂的搜索空间。</li>
<li>results: 该方法在52名参与者的在人体实验中表现出了显著更好的游戏体验，包括积极、紧张、负面情绪和正面情绪。同时，玩家的得分和胜率也有所提高，并且难度调整方法导致20次试Session中的得分下降减少了。<details>
<summary>Abstract</summary>
Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a player's experience in video games. Recently, Reinforcement Learning (RL) methods have been employed for DDA in non-competitive games; nevertheless, they rely solely on discrete state-action space with a small search space. In this paper, we propose a continuous RL-based DDA methodology for a visual working memory (VWM) game to handle the complex search space for the difficulty of memorization. The proposed RL-based DDA tailors game difficulty based on the player's score and game difficulty in the last trial. We defined a continuous metric for the difficulty of memorization. Then, we consider the task difficulty and the vector of difficulty-score as the RL's action and state, respectively. We evaluated the proposed method through a within-subject experiment involving 52 subjects. The proposed approach was compared with two rule-based difficulty adjustment methods in terms of player's score and game experience measured by a questionnaire. The proposed RL-based approach resulted in a significantly better game experience in terms of competence, tension, and negative and positive affect. Players also achieved higher scores and win rates. Furthermore, the proposed RL-based DDA led to a significantly less decline in the score in a 20-trial session.
</details>
<details>
<summary>摘要</summary>
dynamical difficulty adjustment (DDA) 是一种有效的方法来提高玩家在电子游戏中的体验。最近，人工智能学习（RL）方法已经在非竞争性游戏中应用于 DDA;然而，它们仅仅基于精确的状态动作空间和小搜索空间。在这篇论文中，我们提议了一种基于连续RL的 DDA方法ology for a visual working memory (VWM) game to handle the complex search space for the difficulty of memorization. 我们定义了一个连续的听力难度度量，然后考虑了任务难度和游戏难度的向量作为RL的动作和状态，分别。我们通过一个在subject experiment中，涉及52名参与者进行评估我们的方法。我们的方法与两种规则基于的难度调整方法进行比较，以获得玩家的得分和游戏体验的评估，通过问卷调查。我们的RL基于的方法在玩家的得分和游戏体验方面表现出了显著更好的效果，并且玩家的得分和胜率也更高。此外，我们的RL基于的 DDA 方法还导致了20次Session中的得分下降减少为显著水平。
</details></li>
</ul>
<hr>
<h2 id="VIGC-Visual-Instruction-Generation-and-Correction"><a href="#VIGC-Visual-Instruction-Generation-and-Correction" class="headerlink" title="VIGC: Visual Instruction Generation and Correction"></a>VIGC: Visual Instruction Generation and Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12714">http://arxiv.org/abs/2308.12714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang, Conghui He</li>
<li>For: The paper aims to address the challenge of obtaining high-quality instruction-tuning data for vision-language tasks, specifically by utilizing multimodal large language models (MLLMs) to generate such data.* Methods: The proposed framework, called Visual Instruction Generation and Correction (VIGC), consists of two main components: Visual Instruction Generation (VIG) and Visual Instruction Correction (VIC). VIG guides the vision-language model to generate diverse instruction-tuning data, while VIC corrects any inaccuracies in the generated data through an iterative update mechanism.* Results: The proposed VIGC framework effectively enhances the quality of instruction-tuning data, as demonstrated by experimental results that show improved benchmark performance compared to language-only data generation methods.Here’s the simplified Chinese version of the three key points:* For: 解决视语言任务中获得高质量指令调试数据的挑战，通过使用多Modal大语言模型（MLLMs）生成相关数据。* Methods: 提出的框架为Visual Instruction Generation and Correction（VIGC），包括Visual Instruction Generation（VIG）和Visual Instruction Correction（VIC）两个主要组成部分。VIG使得视语言模型生成多样的指令调试数据，而VIC通过迭代更新机制，确保生成数据的准确性。* Results: VIGC框架能够有效提高指令调试数据的质量，经实验证明，比语言只数据生成方法得到更好的 benchMark性能。<details>
<summary>Abstract</summary>
The integration of visual encoders and large language models (LLMs) has driven recent progress in multimodal large language models (MLLMs). However, the scarcity of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, suffering from understanding image details. A practical solution to this problem would be to utilize the available multimodal large language models (MLLMs) to generate instruction data for vision-language tasks. However, it's worth noting that the currently accessible MLLMs are not as powerful as their LLM counterparts, as they tend to produce inadequate responses and generate false information. As a solution for addressing the current issue, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework that enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. Leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
Recent progress in multimodal large language models (MLLMs) has been driven by the integration of visual encoders and large language models (LLMs). However, the lack of high-quality instruction-tuning data for vision-language tasks remains a challenge. The current leading paradigm, such as LLaVA, relies on language-only GPT-4 to generate data, which requires pre-annotated image captions and detection bounding boxes, and suffers from a lack of understanding of image details. To address this problem, this paper proposes the Visual Instruction Generation and Correction (VIGC) framework, which enables multimodal large language models to generate instruction-tuning data and progressively enhance its quality on-the-fly. Specifically, Visual Instruction Generation (VIG) guides the vision-language model to generate diverse instruction-tuning data. To ensure generation quality, Visual Instruction Correction (VIC) adopts an iterative update mechanism to correct any inaccuracies in data produced by VIG, effectively reducing the risk of hallucination. By leveraging the diverse, high-quality data generated by VIGC, we finetune mainstream models and validate data quality based on various evaluations. Experimental results demonstrate that VIGC not only compensates for the shortcomings of language-only data generation methods, but also effectively enhances the benchmark performance. The models, datasets, and code will be made publicly available.
</details></li>
</ul>
<hr>
<h2 id="SayCanPay-Heuristic-Planning-with-Large-Language-Models-using-Learnable-Domain-Knowledge"><a href="#SayCanPay-Heuristic-Planning-with-Large-Language-Models-using-Learnable-Domain-Knowledge" class="headerlink" title="SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge"></a>SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12682">http://arxiv.org/abs/2308.12682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De Raedt</li>
<li>for: 这个论文旨在使用语言模型（LLM）和规划方法（heuristic planning）结合以产生可行和成本效益的计划。</li>
<li>methods: 本文提出的方法是使用LLM生成动作（Say），根据学习的域知识进行评估动作的可行性（Can）和长期奖励（Pay），并使用规划搜索选择最佳动作序列。</li>
<li>results: 对比其他LLM规划方法，本文的模型在评估中表现出色，可以生成更加可行和成本效益的计划。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast "world knowledge". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>将 LLMS 规划问题框入规划搜索的Context。2. 在生成的计划中 integrate 可行和cost-effective的元素。3. 使用规划搜索 sobre 动作。我们的广泛评估表明，我们的模型超过了其他 LLMS 规划方法。</details></li>
</ol>
<hr>
<h2 id="LR-XFL-Logical-Reasoning-based-Explainable-Federated-Learning"><a href="#LR-XFL-Logical-Reasoning-based-Explainable-Federated-Learning" class="headerlink" title="LR-XFL: Logical Reasoning-based Explainable Federated Learning"></a>LR-XFL: Logical Reasoning-based Explainable Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12681">http://arxiv.org/abs/2308.12681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanci Zhang, Han Yu</li>
<li>for:  This paper aims to improve the transparency and explainability of federated learning (FL) models by incorporating logic-based explanations into the FL framework.</li>
<li>methods:  The proposed Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach involves FL clients creating local logic rules based on their local data and sending them to the FL server, which connects the local logic rules through a proper logical connector without requiring access to the raw data. The server aggregates the local model updates with weight values determined by the quality of the clients’ local data as reflected by their uploaded logic rules.</li>
<li>results:  The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively. The explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model’s robustness to errors.<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively. The explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model's robustness to errors. It has the potential to enhance the transparency of FL models for areas like healthcare and finance where both data privacy and explainability are important.
</details>
<details>
<summary>摘要</summary>
Federated learning（FL）是一种emergingapproach для协同训练机器学习模型，保持数据隐私。由于需要隐私保护，FL模型具有限制global transparency和解释性。为Address这些Limitations,我们在FL中嵌入逻辑基于的解释by proposing the Logical Reasoning-based eXplainable Federated Learning（LR-XFL）approach。在LR-XFL中，FL客户端创建基于本地数据的本地逻辑规则，并将其发送到FL服务器。FL服务器通过基于客户端数据的逻辑连接器连接客户端的本地逻辑规则，而无需访问原始数据。此外，服务器还将客户端上传的模型更新与基于客户端数据质量的重量值相结合。结果表明，LR-XFL比最相关的基eline提高了1.19%、5.81%和5.41%的分类精度、逻辑规则精度和逻辑权重精度，分别。explicit Rule evaluation和表达在LR-XFL中允许人工专家在服务器端验证和更正规则，因此提高了全局FL模型的Robustness。它可以提高FL模型在医疗和金融等领域的透明度，这些领域都是数据隐私和解释性重要。
</details></li>
</ul>
<hr>
<h2 id="Improving-Translation-Faithfulness-of-Large-Language-Models-via-Augmenting-Instructions"><a href="#Improving-Translation-Faithfulness-of-Large-Language-Models-via-Augmenting-Instructions" class="headerlink" title="Improving Translation Faithfulness of Large Language Models via Augmenting Instructions"></a>Improving Translation Faithfulness of Large Language Models via Augmenting Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12674">http://arxiv.org/abs/2308.12674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pppa2019/swie_overmiss_llm4mt">https://github.com/pppa2019/swie_overmiss_llm4mt</a></li>
<li>paper_authors: Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou</li>
<li>for: 提高大型自然语言模型（LLM）的特殊能力，如机器翻译，通过低成本的指令调整。</li>
<li>methods: 提出了 Segment-Weighted Instruction Embedding（SWIE）和 OVERMISS  instrucion-following 数据集，以解决 LLM 的注意力机制受限，容易忘记指令的问题。</li>
<li>results: 对两种主流开源 LLM（BLOOM 和 LLaMA）进行应用，实验结果表明，SWIE 可以改善翻译性能，特别是零shot 和长文本翻译，而 OVERMISS 可以提高翻译性能和对word alignment的忠实度。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS. SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two main-stream open-source LLMs, BLOOM and LLaMA. The experimental results demonstrate significant improvements in translation performance with SWIE based on BLOOMZ-3b, particularly in zero-shot and long text translations due to reduced instruction forgetting risk. Additionally, OVERMISS outperforms the baseline in translation performance (e.g. an increase in BLEU scores from 0.69 to 3.12 and an average improvement of 0.48 percentage comet scores for LLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE (e.g. the BLUE scores increase up to 0.56 from English to German across three different backbones), and both exhibit improvements in the faithfulness metric based on word alignment.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有强大的通用能力，现在的挑战是鼓励它们的特殊能力，例如机器翻译，通过低成本的指令调整。标准的指令跟踪数据是逐一 concatenate 一个指令、输入和回应。由于 LL 的注意机制有局部关注的限制， LL 倾向于在每个位置上更多地关注词句或句子。这会导致翻译过程中的指令忘记风险增加。为了解决上述问题，我们提出了 SWIE（Segment-Weighted Instruction Embedding）和 OVERMISS  instruction-following 数据集。SWIE 改善了模型对指令的理解，将全球指令表现添加到下一个输入和回应表现中。OVERMISS 则提高了模型的忠实度，通过比较翻译结果和正确翻译结果之间的差异。我们将我们的方法应用到两个主流的开源 LL 中，namely BLOOM 和 LLaMA。实验结果显示，SWIE 在 BLOOMZ-3b 中具有优化翻译性能，特别是在零shot 和长文翻译中具有削减指令忘记风险的效果。此外，OVERMISS 在翻译性能方面表现出色，例如对于 LLMA-7b 的 BLEU 分数从 0.69 提高到 3.12，平均提高 0.48 百分比统计分数。此外，在组合 OVERMISS 和 SWIE 时，模型具有进一步的改善，例如对于英文到德文的翻译中，BLUE 分数从 0.56 提高到 0.64。此外，SWIE 和 OVERMISS 都具有改善的忠实度，基于词汇对Alignment。
</details></li>
</ul>
<hr>
<h2 id="Don’t-Look-into-the-Sun-Adversarial-Solarization-Attacks-on-Image-Classifiers"><a href="#Don’t-Look-into-the-Sun-Adversarial-Solarization-Attacks-on-Image-Classifiers" class="headerlink" title="Don’t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers"></a>Don’t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12661">http://arxiv.org/abs/2308.12661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paulgavrikov/adversarial_solarization">https://github.com/paulgavrikov/adversarial_solarization</a></li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 这篇论文的目的是测试深度神经网络对于不同类型的输入进行抗性测试，特别是在自动驾驶和安全系统中，以防止黑客利用数位修改输入来逃脱安全检查。</li>
<li>methods: 这篇论文提出了一种基于图像阳光化的攻击方法，这是一种概念简单但不会干扰自然图像的结构的攻击方法。研究者透过对多个ImageNet模型进行了全面的评估，证明了这种攻击方法可以对精度造成严重的下降，但是不包括在训练增强中。</li>
<li>results: 研究者发现这种攻击方法可以对精度造成严重的下降，并且不包括在训练增强中。此外，这种攻击方法可以转化为黑盒攻击，并且不需要了解特定的模型内部细节。这些结果表明，对于深度神经网络的抗性测试仍然是一个复杂的和需要进一步研究的领域。<details>
<summary>Abstract</summary>
Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into the training augmentations. Interestingly, even then, no full immunity to accuracy deterioration is achieved. In other settings, the attack can often be simplified into a black-box attack with model-independent parameters. Defenses against other corruptions do not consistently extend to be effective against our specific attack.   Project website: https://github.com/paulgavrikov/adversarial_solarization
</details>
<details>
<summary>摘要</summary>
评估深度神经网络对非标型输入的Robustness是非常重要的，尤其在自动驾驶和安全系统中， где可能有恶意actor会通过数字修改输入来逃脱安全护垫。然而，设计全面的非标型测试方法，涵盖所有可能的场景，而且保持准确的标签信息是一项具有挑战性的任务。现有的方法ologies oft en compromise on variety and constraint levels for attacks, and sometimes even both.为了更好地评估图像分类模型的Robustness，我们提出了基于图像折射的攻击方法，这是一种简单的概念，但是不会损害自然图像的全球结构，无论输入的强度如何。通过对多个ImageNet模型进行全面的评估，我们示示了这种攻击的能力可以让准确性降低显著，即使不包括在训练增强中。另外，这种攻击可以在其他设置下简化为黑盒攻击，并且可以独立地设置模型参数。防御其他损害的方法不一定能够对我们的特定攻击提供有效防御。更多信息可以查看我们的项目网站：<https://github.com/paulgavrikov/adversarial_solarization>
</details></li>
</ul>
<hr>
<h2 id="kTrans-Knowledge-Aware-Transformer-for-Binary-Code-Embedding"><a href="#kTrans-Knowledge-Aware-Transformer-for-Binary-Code-Embedding" class="headerlink" title="kTrans: Knowledge-Aware Transformer for Binary Code Embedding"></a>kTrans: Knowledge-Aware Transformer for Binary Code Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12659">http://arxiv.org/abs/2308.12659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/learner0x5a/ktrans-release">https://github.com/learner0x5a/ktrans-release</a></li>
<li>paper_authors: Wenyu Zhu, Hao Wang, Yuchen Zhou, Jiaming Wang, Zihan Sha, Zeyu Gao, Chao Zhang</li>
<li>for: 本文旨在提出一种基于Transformer模型的知识塑化二进制代码嵌入（kTrans），以提高下游任务的性能。</li>
<li>methods: 本文使用Transformer模型，并FeedExplicit知识作为额外输入，同时使用一种新的预训练任务来融合Implicit知识。</li>
<li>results: 对于3个下游任务（二进制代码相似检测、函数类型恢复和间接调用识别），kTrans可以生成高质量的二进制代码嵌入，并与现有最佳方法相比，提高了5.2%, 6.8%和12.6%的性能。<details>
<summary>Abstract</summary>
Binary Code Embedding (BCE) has important applications in various reverse engineering tasks such as binary code similarity detection, type recovery, control-flow recovery and data-flow analysis. Recent studies have shown that the Transformer model can comprehend the semantics of binary code to support downstream tasks. However, existing models overlooked the prior knowledge of assembly language. In this paper, we propose a novel Transformer-based approach, namely kTrans, to generate knowledge-aware binary code embedding. By feeding explicit knowledge as additional inputs to the Transformer, and fusing implicit knowledge with a novel pre-training task, kTrans provides a new perspective to incorporating domain knowledge into a Transformer framework. We inspect the generated embeddings with outlier detection and visualization, and also apply kTrans to 3 downstream tasks: Binary Code Similarity Detection (BCSD), Function Type Recovery (FTR) and Indirect Call Recognition (ICR). Evaluation results show that kTrans can generate high-quality binary code embeddings, and outperforms state-of-the-art (SOTA) approaches on downstream tasks by 5.2%, 6.8%, and 12.6% respectively. kTrans is publicly available at: https://github.com/Learner0x5a/kTrans-release
</details>
<details>
<summary>摘要</summary>
“二进制代码嵌入”（BCE）在各种反工程任务中扮演着重要角色，如二进制代码相似检测、类型恢复、控制流恢复和数据流分析。现有研究表明，Transformer模型可以理解二进制代码的 semantics，以支持下游任务。然而，现有模型忽略了 Assembly 语言的先前知识。在本文中，我们提出了一种新的 Transformer 基于的方法，即 kTrans，用于生成知识感知的二进制代码嵌入。通过将显式知识作为 Transformer 的输入，并将隐式知识与一种新的预训练任务结合，kTrans 提供了一种新的方式来 incorporate 领域知识 into Transformer 框架。我们通过检测异常值和可视化，以及应用 kTrans 于 3 个下游任务：二进制代码相似检测（BCSD）、函数类型恢复（FTR）和间接调用认知（ICR）。评估结果表明，kTrans 可以生成高质量的二进制代码嵌入，并在下游任务上过去 SOTA 方法 by 5.2%, 6.8%, 和 12.6% 分别。kTrans 可以在：https://github.com/Learner0x5a/kTrans-release 上获取。
</details></li>
</ul>
<hr>
<h2 id="APART-Diverse-Skill-Discovery-using-All-Pairs-with-Ascending-Reward-and-DropouT"><a href="#APART-Diverse-Skill-Discovery-using-All-Pairs-with-Ascending-Reward-and-DropouT" class="headerlink" title="APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT"></a>APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12649">http://arxiv.org/abs/2308.12649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadar Schreiber Galler, Tom Zahavy, Guillaume Desjardins, Alon Cohen</li>
<li>for: 本研究旨在在奖励环境中发现多样化技能，目的是在简单的格子世界中发现所有可能的技能，而前一些方法在这些环境中很难 succeed.</li>
<li>methods: 我们使用了一种名为APART的方法，即多对多推论器和一种新的内在奖励函数，以及一种Dropout regularization技术。</li>
<li>results: 我们的实验表明，APART可以在格子世界中发现所有可能的技能，比前一些方法需要更少的样本数据。此外，我们还提出了一种简化版的算法，通过修改VIC、涨奖励和软max推论器的温度来实现最大技能数。我们认为我们的发现对于激励学习中技能发现算法的成功因素提供了灵感。<details>
<summary>Abstract</summary>
We study diverse skill discovery in reward-free environments, aiming to discover all possible skills in simple grid-world environments where prior methods have struggled to succeed. This problem is formulated as mutual training of skills using an intrinsic reward and a discriminator trained to predict a skill given its trajectory. Our initial solution replaces the standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs) discriminator and combines it with a novel intrinsic reward function and a dropout regularization technique. The combined approach is named APART: Diverse Skill Discovery using All Pairs with Ascending Reward and Dropout. We demonstrate that APART discovers all the possible skills in grid worlds with remarkably fewer samples than previous works. Motivated by the empirical success of APART, we further investigate an even simpler algorithm that achieves maximum skills by altering VIC, rescaling its intrinsic reward, and tuning the temperature of its softmax discriminator. We believe our findings shed light on the crucial factors underlying success of skill discovery algorithms in reinforcement learning.
</details>
<details>
<summary>摘要</summary>
我们研究了不带奖励的环境中多样化技能发现，目标是在简单的网格世界中发现所有可能的技能。这个问题被формализова为通过内在奖励和一个用于预测技能的权重来训练技能。我们的初始解决方案是将标准的一对一（所有对）权重交换掉了一个对一（softmax）权重，并将其与一种新的内在奖励函数和掉量 regularization 技术结合使用。这种结合的方法被称为APART：多样化技能发现使用所有对与升奖和掉量。我们示出了APART在网格世界中发现所有可能的技能，只需要非常少的样本数据，远远少于前一作。受APART的实际成功的激励，我们进一步调查了一种更简单的算法，通过修改VIC，调整其内在奖励，并调整其softmax权重的温度来实现最大技能数。我们认为我们的发现可能把 reinforcement learning 中技能发现算法的成功因素 shed light on 。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Hungarian-Text-Processing-with-HuSpaCy-Efficient-and-Accurate-NLP-Pipelines"><a href="#Advancing-Hungarian-Text-Processing-with-HuSpaCy-Efficient-and-Accurate-NLP-Pipelines" class="headerlink" title="Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines"></a>Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12635">http://arxiv.org/abs/2308.12635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huspacy/huspacy">https://github.com/huspacy/huspacy</a></li>
<li>paper_authors: György Orosz, Gergő Szabó, Péter Berkecz, Zsolt Szántó, Richárd Farkas</li>
<li>for: 这篇论文旨在提供一个高效且资源减少的工业级文本处理模型，以达到near state-of-the-art性能水平。</li>
<li>methods: 该论文使用了spaCy框架，并对 HuSpaCy 工具集进行了多个改进，包括Tokenization、句子分界检测、parts-of-speech 标注、 morphological feature 标注、lemmatization、依赖分析和命名实体识别等基本文本处理步骤。</li>
<li>results: 论文对提议的改进进行了全面评估，并与现有的 NLP 工具进行了比较，并证明了新的模型在所有文本预处理步骤中具有竞争性的性能。<details>
<summary>Abstract</summary>
This paper presents a set of industrial-grade text processing models for Hungarian that achieve near state-of-the-art performance while balancing resource efficiency and accuracy. Models have been implemented in the spaCy framework, extending the HuSpaCy toolkit with several improvements to its architecture. Compared to existing NLP tools for Hungarian, all of our pipelines feature all basic text processing steps including tokenization, sentence-boundary detection, part-of-speech tagging, morphological feature tagging, lemmatization, dependency parsing and named entity recognition with high accuracy and throughput. We thoroughly evaluated the proposed enhancements, compared the pipelines with state-of-the-art tools and demonstrated the competitive performance of the new models in all text preprocessing steps. All experiments are reproducible and the pipelines are freely available under a permissive license.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Hierarchical-Regional-Transformer-based-Multiple-Instance-Learning"><a href="#Towards-Hierarchical-Regional-Transformer-based-Multiple-Instance-Learning" class="headerlink" title="Towards Hierarchical Regional Transformer-based Multiple Instance Learning"></a>Towards Hierarchical Regional Transformer-based Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12634">http://arxiv.org/abs/2308.12634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josef Cersovsky, Sadegh Mohammadi, Dagmar Kainmueller, Johannes Hoehne</li>
<li>for: 这篇论文主要针对高分辨率 histopathology 图像的分类问题进行研究，以实现数位patology 和精确医疗的需求。</li>
<li>methods: 本研究使用 transformer 基础的多个实例学习模型，并取代传统的学习注意力机制使用区域 Vision Transformer 灵活自注意力机制。文章还提出了一种方法，将区域贴图信息融合以 derive 标本水平预测，并示出了在不同距离水平上堆叠Feature Processing的方法。</li>
<li>results: 本研究在两个 histopathology 数据集上进行评估，比基eline表现出色，尤其是 для datasets 中小的本地 morphological 特征。研究还引入了一种方法，在推断过程中专注于高注意区域，以提高预测精度。<details>
<summary>Abstract</summary>
The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对巨像病理图像的分类使用深度多例学习模型已成为数字病理学和精度医学中的关键任务。在这种工作中，我们提议使用Transformer基于自我注意机制来替代传统学习的注意力机制。我们介绍了一种将区域补做信息融合以获得滤波器级别预测的方法，并显示了如何在不同的距离水平上堆叠特征进行处理。为了提高预测精度，特别是 для datasets中的小本地 morphological features，我们引入了一种在推理时对高注意区域进行图像处理的方法。我们的方法可以在两个病理图像集上显著提高性能，并指向了未来研究的可能的方向。>>>
</details></li>
</ul>
<hr>
<h2 id="A-Greedy-Approach-for-Offering-to-Telecom-Subscribers"><a href="#A-Greedy-Approach-for-Offering-to-Telecom-Subscribers" class="headerlink" title="A Greedy Approach for Offering to Telecom Subscribers"></a>A Greedy Approach for Offering to Telecom Subscribers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12606">http://arxiv.org/abs/2308.12606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piyush Kanti Bhunre, Tanmay Sen, Arijit Sarkar</li>
<li>for:  This paper is written for telecom operators to optimize offer campaigns for customer retention and churn prevention.</li>
<li>methods:  The paper proposes a novel combinatorial algorithm for solving offer optimization under heterogeneous offers by maximizing expected revenue under the scenario of subscriber churn.</li>
<li>results:  The proposed algorithm is efficient and accurate even for a very large subscriber-base.Here’s the Chinese translation of the three key information points:</li>
<li>for: 这篇论文是为telecom运营商优化奖励计划以防止用户流失。</li>
<li>methods: 论文提出了一种新的 combinatorial 算法，用于在不同的奖励下进行奖励优化，以达到用户流失情况下的预期收入最大化。</li>
<li>results: 提出的算法能够具有高效率和准确性，甚至对很大的用户基数进行处理。<details>
<summary>Abstract</summary>
Customer retention or churn prevention is a challenging task of a telecom operator. One of the effective approaches is to offer some attractive incentive or additional services or money to the subscribers for keeping them engaged and make sure they stay in the operator's network for longer time. Often, operators allocate certain amount of monetary budget to carry out the offer campaign. The difficult part of this campaign is the selection of a set of customers from a large subscriber-base and deciding the amount that should be offered to an individual so that operator's objective is achieved. There may be multiple objectives (e.g., maximizing revenue, minimizing number of churns) for selection of subscriber and selection of an offer to the selected subscriber. Apart from monetary benefit, offers may include additional data, SMS, hots-spot tethering, and many more. This problem is known as offer optimization. In this paper, we propose a novel combinatorial algorithm for solving offer optimization under heterogeneous offers by maximizing expected revenue under the scenario of subscriber churn, which is, in general, seen in telecom domain. The proposed algorithm is efficient and accurate even for a very large subscriber-base.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a novel combinatorial algorithm to solve offer optimization under heterogeneous offers by maximizing expected revenue under the scenario of subscriber churn, which is common in the telecom domain. The proposed algorithm is efficient and accurate, even for a very large subscriber base.
</details></li>
</ul>
<hr>
<h2 id="APLA-Additional-Perturbation-for-Latent-Noise-with-Adversarial-Training-Enables-Consistency"><a href="#APLA-Additional-Perturbation-for-Latent-Noise-with-Adversarial-Training-Enables-Consistency" class="headerlink" title="APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency"></a>APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12605">http://arxiv.org/abs/2308.12605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yupu Yao, Shangqi Deng, Zihan Cao, Harry Zhang, Liang-Jian Deng</li>
<li>for: 这个论文旨在提出一种基于噪声扩散模型的文本到视频（T2V）生成网络结构，以解决传统噪声扩散模型在视频生成中缺乏地方具有一致性的问题。</li>
<li>methods: 该论文提出了一种基于噪声扩散模型的T2V生成网络结构，其中引入了一个额外的干扰分布网络（VGT），以EXTRACT perturbances from the inherent information contained within the input, 并且通过混合 transformers 和卷积神经来补做时间细节，从而提高视频生成中的一致性。</li>
<li>results: 实验表明，该论文提出的T2V生成网络结构可以显著提高视频生成中的一致性， both qualitatively 和 quantitatively。<details>
<summary>Abstract</summary>
Diffusion models have exhibited promising progress in video generation. However, they often struggle to retain consistent details within local regions across frames. One underlying cause is that traditional diffusion models approximate Gaussian noise distribution by utilizing predictive noise, without fully accounting for the impact of inherent information within the input itself. Additionally, these models emphasize the distinction between predictions and references, neglecting information intrinsic to the videos. To address this limitation, inspired by the self-attention mechanism, we propose a novel text-to-video (T2V) generation network structure based on diffusion models, dubbed Additional Perturbation for Latent noise with Adversarial training (APLA). Our approach only necessitates a single video as input and builds upon pre-trained stable diffusion networks. Notably, we introduce an additional compact network, known as the Video Generation Transformer (VGT). This auxiliary component is designed to extract perturbations from the inherent information contained within the input, thereby refining inconsistent pixels during temporal predictions. We leverage a hybrid architecture of transformers and convolutions to compensate for temporal intricacies, enhancing consistency between different frames within the video. Experiments demonstrate a noticeable improvement in the consistency of the generated videos both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
文本转化为简化中文：传播模型在视频生成方面已经取得了可观的进步。然而，它们经常在不同帧之间保持相同的细节存在问题。这一问题的一个根本原因是传播模型通常通过预测噪声来估算 Gaussian 噪声分布，不充分考虑输入中的内在信息的影响。此外，这些模型强调预测和参照之间的差异，忽视视频中的内在信息。为了解决这些局限性，我们提出了一种基于传播模型的文本转化（T2V）生成网络结构，称之为附加噪声扰动with Adversarial training（APLA）。我们的方法只需要一个输入视频，并基于预训练稳定的传播网络。另外，我们引入了一个附加的小型网络，称之为视频生成变换器（VGT）。这个辅助组件是用于提取输入中的内在信息，并在时间预测中进行细节调整。我们利用一种混合的扩展和卷积网络架构，以资料 temporal 细节，提高不同帧之间的视频生成的一致性。实验表明，我们的方法可以在视频生成中提高一致性， tanto qualitatively 还是 quantitatively。</SYS>Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SICNN-Soft-Interference-Cancellation-Inspired-Neural-Network-Equalizers"><a href="#SICNN-Soft-Interference-Cancellation-Inspired-Neural-Network-Equalizers" class="headerlink" title="SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers"></a>SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12591">http://arxiv.org/abs/2308.12591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Baumgartner, Oliver Lang, Mario Huemer</li>
<li>for: 这种论文主要是为了提出一种基于神经网络的平衡方法，以解决模型基于平衡方法的高计算复杂度和性能下降问题。</li>
<li>methods: 该论文提出了两种不同的神经网络平衡方法，即SICNNv1和SICNNv2，其中SICNNv1是专门针对单载频域平衡系统，而SICNNv2是更通用的，适用于任何块式数据传输系统。</li>
<li>results: 论文表明，SICNNv1在比较其他方法时表现出色，并且可以在不同的训练集大小下达到优秀的性能。此外，论文还进行了计算复杂度分析，并证明了神经网络平衡方法的可行性。<details>
<summary>Abstract</summary>
Equalization is an important task at the receiver side of a digital wireless communication system, which is traditionally conducted with model-based estimation methods. Among the numerous options for model-based equalization, iterative soft interference cancellation (SIC) is a well-performing approach since error propagation caused by hard decision data symbol estimation during the iterative estimation procedure is avoided. However, the model-based method suffers from high computational complexity and performance degradation due to required approximations. In this work, we propose a novel neural network (NN-)based equalization approach, referred to as SICNN, which is designed by deep unfolding of a model-based iterative SIC method, eliminating the main disadvantages of its model-based counterpart. We present different variants of SICNN. SICNNv1 is very similar to the model-based method, and is specifically tailored for single carrier frequency domain equalization systems, which is the communication system we regard in this work. The second variant, SICNNv2, is more universal, and is applicable as an equalizer in any communication system with a block-based data transmission scheme. We highlight the pros and cons of both variants. Moreover, for both SICNNv1 and SICNNv2 we present a version with a highly reduced number of learnable parameters. We compare the achieved bit error ratio performance of the proposed NN-based equalizers with state-of-the-art model-based and NN-based approaches, highlighting the superiority of SICNNv1 over all other methods. Also, we present a thorough complexity analysis of the proposed NN-based equalization approaches, and we investigate the influence of the training set size on the performance of NN-based equalizers.
</details>
<details>
<summary>摘要</summary>
Equalization是收发器端数字无线通信系统中的重要任务，传统上采用模型基于估计方法进行实现。iterative soft interference cancellation（SIC）是一种 performs well的方法，因为它可以避免由硬判据数据符号估计过程中的错误卷积。然而，模型基于方法受到高计算复杂性和性能下降的限制。在这项工作中，我们提出了一种基于神经网络（NN）的平衡方法， referred to as SICNN，这种方法通过深度 unfolding 的方式消除了模型基于方法的主要缺陷。我们提出了不同的 SICNN 变种。SICNNv1 和 SICNNv2。SICNNv1 特性类似于模型基于方法，并且特意适用于单 carriers frequency domain equalization 系统，这是我们在这项工作中考虑的系统。第二个变种，SICNNv2，更加通用，可以作为任何通信系统的平衡器。我们比较了这两种变种的优缺点。此外，我们还提出了具有很少学习参数的版本。我们比较了我们提出的 NN 基于平衡器与现有的模型基于和 NN 基于方法的 bit error ratio 性能，并证明了 SICNNv1 在所有其他方法之上表现出色。此外，我们还进行了 NN 基于平衡器的复杂度分析，并investigated 训练集大小对 NN 基于平衡器的性能的影响。
</details></li>
</ul>
<hr>
<h2 id="A-Huber-Loss-Minimization-Approach-to-Byzantine-Robust-Federated-Learning"><a href="#A-Huber-Loss-Minimization-Approach-to-Byzantine-Robust-Federated-Learning" class="headerlink" title="A Huber Loss Minimization Approach to Byzantine Robust Federated Learning"></a>A Huber Loss Minimization Approach to Byzantine Robust Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12581">http://arxiv.org/abs/2308.12581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Fei Yu, Zhiguo Wan</li>
<li>for: 防止 Federated Learning 系统受到攻击，提出一种基于捷径函数损失最小化的新集成器，并进行了全面的理论分析。</li>
<li>methods: 使用捷径函数损失最小化来防止 Federated Learning 系统受到攻击，并且不需要精确知道攻击客户端的比率（epsilon）。</li>
<li>results: 在独立同分布（i.i.d）假设下，我们的方法具有优化的 $\epsilon$ 依赖性，允许客户端有不同的数据大小，并且可以扩展到非 i.i.d 数据。<details>
<summary>Abstract</summary>
Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
</details>
<details>
<summary>摘要</summary>
联合学习系统容易受到敌意攻击。为此，我们提出了基于捷径损函数优化的新的聚合器，并进行了全面的理论分析。在独立同分布（i.i.d）假设下，我们的方法有以下优势：首先，它具有优化的 $\epsilon$ 依赖性，其中 $\epsilon$ 表示攻击客户端的比率。其次，我们的方法不需要准确知道 $\epsilon$。最后，它允许客户端有不同的数据大小。然后，我们扩展了我们的分析至包括非i.i.d数据，例如客户端的数据分布略有不同。>>>Note: The text is translated into Simplified Chinese, which is the standard form of Chinese used in mainland China. The Traditional Chinese form is also commonly used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Mind-vs-Mouth-On-Measuring-Re-judge-Inconsistency-of-Social-Bias-in-Large-Language-Models"><a href="#Mind-vs-Mouth-On-Measuring-Re-judge-Inconsistency-of-Social-Bias-in-Large-Language-Models" class="headerlink" title="Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models"></a>Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12578">http://arxiv.org/abs/2308.12578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yachao Zhao, Bo Wang, Dongming Zhao, Kun Huang, Yan Wang, Ruifang He, Yuexian Hou</li>
<li>for: 本研究探讨了大型自然语言模型（LLMs）中的认知特征，特别是人类认知constructs的相似性。</li>
<li>methods: 本研究采用了两个阶段的方法，包括自动完成句子和后续重新评价句子。</li>
<li>results: 研究发现，LLMs中存在一种同人类认知constructs相似的”重新评价不一致”现象，即自动完成句子后，LLMs会重新评价并 contradicts 自己生成的句子。这种现象可能与人类的不意识的社会偏见和自我意识的社会偏见之间的不一致有关。<details>
<summary>Abstract</summary>
Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as "re-judge inconsistency" in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware implicit social bias and their aware explicit social bias. Experimental investigations on ChatGPT and GPT-4 concerning common gender biases examined in psychology corroborate the highly stable nature of the re-judge inconsistency. This finding may suggest that diverse cognitive constructs emerge as LLMs' capabilities strengthen. Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="REB-Reducing-Biases-in-Representation-for-Industrial-Anomaly-Detection"><a href="#REB-Reducing-Biases-in-Representation-for-Industrial-Anomaly-Detection" class="headerlink" title="REB: Reducing Biases in Representation for Industrial Anomaly Detection"></a>REB: Reducing Biases in Representation for Industrial Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12577">http://arxiv.org/abs/2308.12577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuailyu/reb">https://github.com/shuailyu/reb</a></li>
<li>paper_authors: Shuai Lyu, Dongmei Mo, Waikeung Wong</li>
<li>for: 提高工业异常检测的性能，减少域名偏见和特定区域密度偏见。</li>
<li>methods: 提出了减少域名偏见的 Representing Reducing Biases (REB) 方法，以及基于自我超视的学习任务和异常生成策略（DefectMaker）来更好地适应域名偏见。同时，提出了一种基于Local Density的KNN（LDKNN）方法，以减少本地密度偏见。</li>
<li>results: 在广泛使用的 MVTec AD  bencmark 上达到了 99.5% AUROC 的优秀成绩，并在挑战性的 MVTec LOCO AD dataset 上达到了 88.0% AUROC，超过了州对比的最佳成绩。此外，使用较小的后向网络（Vgg11 和 Resnet18）获得了更好的效果和效率，表明 REB 方法在实际工业应用中具有效果和经济性。<details>
<summary>Abstract</summary>
Existing K-nearest neighbor (KNN) retrieval-based methods usually conduct industrial anomaly detection in two stages: obtain feature representations with a pre-trained CNN model and perform distance measures for defect detection. However, the features are not fully exploited as they ignore domain bias and the difference of local density in feature space, which limits the detection performance. In this paper, we propose Reducing Biases (REB) in representation by considering the domain bias of the pre-trained model and building a self-supervised learning task for better domain adaption with a defect generation strategy (DefectMaker) imitating the natural defects. Additionally, we propose a local density KNN (LDKNN) to reduce the local density bias and obtain effective anomaly detection. We achieve a promising result of 99.5\% AUROC on the widely used MVTec AD benchmark. We also achieve 88.0\% AUROC on the challenging MVTec LOCO AD dataset and bring an improvement of 4.7\% AUROC to the state-of-the-art result. All results are obtained with smaller backbone networks such as Vgg11 and Resnet18, which indicates the effectiveness and efficiency of REB for practical industrial applications.
</details>
<details>
<summary>摘要</summary>
现有的K-最近邻（KNN）检索基于方法通常在两个阶段进行工业异常检测：首先使用预训练的CNN模型获取特征表示，然后进行距离度量以检测异常。然而，这些特征不完全利用，因为它们忽略预测模型的领域偏见和特征空间中的地方浓度差异，这限制了检测性能。在这篇论文中，我们提出了减少偏见（REB）在表示中的技术，通过考虑预测模型的领域偏见来减少偏见。此外，我们还提出了一种本地浓度KNN（LDKNN），以减少本地浓度偏见并获得有效的异常检测。我们在广泛使用的MVTec AD数据集上实现了99.5%的AUROC报告，并在挑战性的MVTec LOCO AD数据集上实现了88.0%的AUROC报告，超过了状态 искусственный界的result。这些结果都是使用较小的背部网络，如Vgg11和Resnet18，这表明REB的效iveness和可行性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-Strategies-of-Retriever-and-Large-Language-Models"><a href="#Exploring-the-Integration-Strategies-of-Retriever-and-Large-Language-Models" class="headerlink" title="Exploring the Integration Strategies of Retriever and Large Language Models"></a>Exploring the Integration Strategies of Retriever and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12574">http://arxiv.org/abs/2308.12574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye Liu, Semih Yavuz, Rui Meng, Meghana Moorthy, Shafiq Joty, Caiming Xiong, Yingbo Zhou</li>
<li>for: 该论文目的是提高开放预测问答的能力， investigate different methods of combining retrieved passages with LLMs to enhance answer generation.</li>
<li>methods: 论文使用了四种策略来结合检索结果和LLMs，包括两种单回路方法和两种多回路方法，以实现更好的答案生成。</li>
<li>results: 经过广泛的分析和实验，论文提供了有用的见解，以帮助更好地利用检索结果来提高LLMs的答案生成能力。<details>
<summary>Abstract</summary>
The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating "unknown" outputs, even when the correct document is among the top-k retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, we provide insightful observations on how to effectively leverage retrieved passages to enhance the answer generation capability of LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("The integration of retrieved passages and large language models (LLMs), such as ChatGPTs, has significantly contributed to improving open-domain question answering. However, there is still a lack of exploration regarding the optimal approach for incorporating retrieved passages into the answer generation process. This paper aims to fill this gap by investigating different methods of combining retrieved passages with LLMs to enhance answer generation. We begin by examining the limitations of a commonly-used concatenation approach. Surprisingly, this approach often results in generating "unknown" outputs, even when the correct document is among the top-k retrieved passages. To address this issue, we explore four alternative strategies for integrating the retrieved passages with the LLMs. These strategies include two single-round methods that utilize chain-of-thought reasoning and two multi-round strategies that incorporate feedback loops. Through comprehensive analyses and experiments, we provide insightful observations on how to effectively leverage retrieved passages to enhance the answer generation capability of LLMs.")]以下是文本的Simplified Chinese翻译：<<SYS>>大语模型（LLMs）和检索段落的集成已经有效提高了开放领域问答。然而，在将检索段落与LLMs结合的优化方法方面，还存在一些不足。这篇论文的目标是填补这一空白，通过不同的方法来融合检索段落和LLMs来提高答案生成能力。我们首先检查了常用的 concatenation 方法的局限性。尝试意外地发现，这种方法经常生成 "未知" 输出，即使正确的文档在前top-k检索段落中。为解决这一问题，我们探索了四种不同的方法，包括两种单回合方法和两种多回合方法，以便更好地利用检索段落来增强 LLMS 的答案生成能力。通过对这些方法的全面分析和实验，我们提供了有价值的观察和建议，以帮助更好地利用检索段落来提高 LLMS 的答案生成能力。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Kernel-Imitation-Learning-for-Continuous-State-Environments"><a href="#Conditional-Kernel-Imitation-Learning-for-Continuous-State-Environments" class="headerlink" title="Conditional Kernel Imitation Learning for Continuous State Environments"></a>Conditional Kernel Imitation Learning for Continuous State Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12573">http://arxiv.org/abs/2308.12573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Agrawal, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar</li>
<li>for: 这 paper 的目的是解决基于观察行为的 reinforcement learning 问题，无需transition dynamics信息、奖励结构或其他环境交互数据。</li>
<li>methods: 这 paper 使用Markov balance equation和 conditional kernel density estimation 来建立一种基于观察行为的 imitation learning 框架，并证明其 asymptotic consistency 性。</li>
<li>results: 通过在连续状态环境上进行numerical experiments， authors 发现该方法在 empirical performance 上表现出 beat many state-of-the-art IL algorithms 的特点。<details>
<summary>Abstract</summary>
Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our approach is based on the Markov balance equation and introduces a novel conditional kernel density estimation-based imitation learning framework. It involves estimating the environment's transition dynamics using conditional kernel density estimators and seeks to satisfy the probabilistic balance equations for the environment. We establish that our estimators satisfy basic asymptotic consistency requirements. Through a series of numerical experiments on continuous state benchmark environments, we show consistently superior empirical performance over many state-of-the-art IL algorithms.
</details>
<details>
<summary>摘要</summary>
模仿学习（IL）是激励学习（RL）方法中的一个重要分支，不同于大多数RL方法，它不假设环境提供奖励反馈。奖励推断和形成是知识到达难度和错误感知的方法，特别是当示例数据来自人类专家时。经典方法 such as 行为做抄和反射学习是高度敏感于估计错误，特别是在连续状态空间问题上。在这篇论文中，我们考虑了基于观察行为的激励学习问题，无需环境过程动力学信息、奖励结构或任何其他与环境交互的数据。我们的方法基于Markov平衡方程，并提出了一种基于Conditional Kernel Density Estimation的新型模仿学习框架。它通过估计环境的过程动力学使用Conditional Kernel Density Estimator，并寻求满足环境的 probabilistic balance equation。我们证明了我们的估计符合基本的极限consistency要求。通过对连续状态标准环境进行数值实验，我们显示了在许多状态艺术IL算法的实际性表现上的一致性。
</details></li>
</ul>
<hr>
<h2 id="A-Co-training-Approach-for-Noisy-Time-Series-Learning"><a href="#A-Co-training-Approach-for-Noisy-Time-Series-Learning" class="headerlink" title="A Co-training Approach for Noisy Time Series Learning"></a>A Co-training Approach for Noisy Time Series Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12551">http://arxiv.org/abs/2308.12551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiqi Zhang, Jianfeng Zhang, Jia Li, Fugee Tsung</li>
<li>for: 本研究强调Robust时间序列表示学习，假设真实世界的时间序列受到噪声和杂音的影响，同时不同视图的时间序列信息具有重要的作用。</li>
<li>methods: 我们采用了两个不同的编码器来创建两个视图，然后通过训练协同对照学习来学习这两个编码器。我们的实验表明，这种协同对照学习方法可以显著提高表达的性能。</li>
<li>results: 我们在四个时间序列 benchmark上进行了无监督和半监督的实验，结果显示，TS-CoT 方法可以减轻数据噪声和杂音的影响，并且表达可以 Transfer 到下游任务 through fine-tuning。<details>
<summary>Abstract</summary>
In this work, we focus on robust time series representation learning. Our assumption is that real-world time series is noisy and complementary information from different views of the same time series plays an important role while analyzing noisy input. Based on this, we create two views for the input time series through two different encoders. We conduct co-training based contrastive learning iteratively to learn the encoders. Our experiments demonstrate that this co-training approach leads to a significant improvement in performance. Especially, by leveraging the complementary information from different views, our proposed TS-CoT method can mitigate the impact of data noise and corruption. Empirical evaluations on four time series benchmarks in unsupervised and semi-supervised settings reveal that TS-CoT outperforms existing methods. Furthermore, the representations learned by TS-CoT can transfer well to downstream tasks through fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们关注于鲁棒时间序列表示学习。我们假设真实世界中的时间序列是噪音的，并且不同视图中的时间序列信息具有重要的作用。基于这个假设，我们创建了两个视图来表示输入时间序列，并通过对这两个视图进行互训练的对照学习来学习这两个视图的编码器。我们的实验表明，这种合作学习方法可以提高表达性能。尤其是通过利用不同视图之间的补做信息，我们的提案的TS-CoT方法可以减轻数据噪音和损害的影响。我们在四个时间序列标准 bencmarks 上进行了无监督和半监督的实验，并证明了TS-CoT方法在表达性能方面的优异性。此外，TS-CoT方法学习的表示可以通过细化进行下游任务的调整，以便在不同任务上进行应用。
</details></li>
</ul>
<hr>
<h2 id="Synchronize-Feature-Extracting-and-Matching-A-Single-Branch-Framework-for-3D-Object-Tracking"><a href="#Synchronize-Feature-Extracting-and-Matching-A-Single-Branch-Framework-for-3D-Object-Tracking" class="headerlink" title="Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking"></a>Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12549">http://arxiv.org/abs/2308.12549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teli Ma, Mengmeng Wang, Jimin Xiao, Huifeng Wu, Yong Liu</li>
<li>for: 本研究旨在提出一种新的单支 Framework，即SyncTrack，用于3D LiDAR对象跟踪。</li>
<li>methods: SyncTrack弃备传统的Siamese方式，改用一个单支Encoder，同时在模型中引入了一种新的同步机制，以避免在模型中两次使用Encoder，并且引入了一种新的批处理策略。</li>
<li>results: 实验结果表明，SyncTrack在两个标准数据集（KITTI和NuScenes）上达到了实时跟踪的状态态ridge性表现。<details>
<summary>Abstract</summary>
Siamese network has been a de facto benchmark framework for 3D LiDAR object tracking with a shared-parametric encoder extracting features from template and search region, respectively. This paradigm relies heavily on an additional matching network to model the cross-correlation/similarity of the template and search region. In this paper, we forsake the conventional Siamese paradigm and propose a novel single-branch framework, SyncTrack, synchronizing the feature extracting and matching to avoid forwarding encoder twice for template and search region as well as introducing extra parameters of matching network. The synchronization mechanism is based on the dynamic affinity of the Transformer, and an in-depth analysis of the relevance is provided theoretically. Moreover, based on the synchronization, we introduce a novel Attentive Points-Sampling strategy into the Transformer layers (APST), replacing the random/Farthest Points Sampling (FPS) method with sampling under the supervision of attentive relations between the template and search region. It implies connecting point-wise sampling with the feature learning, beneficial to aggregating more distinctive and geometric features for tracking with sparse points. Extensive experiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack achieves state-of-the-art performance in real-time tracking.
</details>
<details>
<summary>摘要</summary>
三元网络在3D LiDAR物体跟踪中 serves as a de facto benchmark framework, with a shared-parametric encoder extracting features from the template and search region, respectively. This paradigm relies heavily on an additional matching network to model the cross-correlation/similarity of the template and search region. In this paper, we abandon the conventional Siamese paradigm and propose a novel single-branch framework, SyncTrack, which synchronizes the feature extraction and matching to avoid forwarding the encoder twice for the template and search region, as well as introducing extra parameters of the matching network. The synchronization mechanism is based on the dynamic affinity of the Transformer, and an in-depth analysis of the relevance is provided theoretically. Moreover, based on the synchronization, we introduce a novel Attentive Points-Sampling strategy into the Transformer layers (APST), replacing the random/Farthest Points Sampling (FPS) method with sampling under the supervision of attentive relations between the template and search region. This implies connecting point-wise sampling with the feature learning, beneficial to aggregating more distinctive and geometric features for tracking with sparse points. Extensive experiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack achieves state-of-the-art performance in real-time tracking.
</details></li>
</ul>
<hr>
<h2 id="CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias"><a href="#CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias" class="headerlink" title="CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias"></a>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12539">http://arxiv.org/abs/2308.12539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vipulgupta1011/calm">https://github.com/vipulgupta1011/calm</a></li>
<li>paper_authors: Vipul Gupta, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, Rebecca J. Passonneau</li>
<li>for: 评估语言模型（LM）的社会经济偏见，以避免可能导致伤害的潜在危害。</li>
<li>methods: 引入了Comprehensive Assessment of Language Model bias（CALM），一个可靠的评估语言模型偏见的benchmarkdataset，通过Integrating16个不同领域的 dataset，并对224个模板进行筛选，构建了一 dataset of 78,400例。</li>
<li>results: 比较了CALM dataset的多样性与之前的dataset，并测试了小幅度的变化的敏感性，发现CALM dataset更加多样和可靠，能够更好地评估语言模型的偏见。此外，对20个大型语言模型进行了评估，发现大型模型更加偏向于某些群体，而T0系列的模型最少偏见。<details>
<summary>Abstract</summary>
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language models including six prominent families of LMs such as Llama-2. In two LM series, OPT and Bloom, we found that larger parameter models are more biased than lower parameter models. We found the T0 series of models to be the least biased. Furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. The code is available at https://github.com/vipulgupta1011/CALM.
</details>
<details>
<summary>摘要</summary>
为了评估语言模型（LM）的社会经济偏见，需要量化和比较它们。但现有的偏见测试数据集有很多缺陷，例如 manually designed templates 的修改会导致测试结果不可靠。为了解决这问题，我们引入了 Comprehensive Assessment of Language Model Bias（CALM），一个可靠的偏见测试数据集，用于评估 LM 的偏见。我们将16个不同领域的数据集集成，包括 Wikipedia 和新闻文章，并从这些模板中过滤出224个模板，然后构建了一个包含 78,400 个示例的数据集。我们比较了 CALM 数据集的多样性和先前数据集的多样性，并测试了小改动的敏感性。我们发现 CALM 数据集更加多样和可靠，因此更能够正确地评估 LM 的偏见。我们测试了 20 个大型语言模型，包括六种主要的 LM 家族，例如 Llama-2。在一些模型系列中，我们发现大型模型更加偏见，而小型模型更加不偏见。此外，我们发现一些模型系列中， gender 和种族偏见之间存在负相关性。模型代码可以在 GitHub 上找到：https://github.com/vipulgupta1011/CALM。
</details></li>
</ul>
<hr>
<h2 id="FedSoL-Bridging-Global-Alignment-and-Local-Generality-in-Federated-Learning"><a href="#FedSoL-Bridging-Global-Alignment-and-Local-Generality-in-Federated-Learning" class="headerlink" title="FedSoL: Bridging Global Alignment and Local Generality in Federated Learning"></a>FedSoL: Bridging Global Alignment and Local Generality in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12532">http://arxiv.org/abs/2308.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gihun Lee, Minchan Jeong, Sangmook Kim, Jaehoon Oh, Se-Young Yun</li>
<li>for: 提高 Federated Learning（FL）的性能，解决 Client 数据分布不均的问题。</li>
<li>methods:  combinest both the concepts of global alignment and local generality，使用 parameter region robust against proximal perturbations 来做 Local Learning。</li>
<li>results:  experiments show that FedSoL consistently achieves state-of-the-art performance on various setups。<details>
<summary>Abstract</summary>
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (FedSoL), which combines both the concepts of global alignment and local generality. In FedSoL, the local learning seeks a parameter region robust against proximal perturbations. This strategy introduces an implicit proximal restriction effect in local learning while maintaining the original local objective for parameter update. Our experiments show that FedSoL consistently achieves state-of-the-art performance on various setups.
</details>
<details>
<summary>摘要</summary>
federa 学习（FL）将本地训练的模型从客户端集成成全局模型。而FL可以保持数据隐私，但它经常受到客户端数据分布不均的影响，导致性能下降。许多前一代FL算法已经解决这个问题，通过引入不同的距离约束。这些约束 стремятся强制全局对应，但它们会限制本地学习。在最近几年，一种新的方法出现了，以提高本地学习的通用性。通过在本地学习中获得一个平滑的损失函数，这种方法减少了不同客户端的本地目标之间的冲突。然而，这种方法并不保证全局稳定性，因为本地学习并不考虑全局目标。在本研究中，我们提出了联邦稳定学习（FedSoL），它结合了全局对应和本地通用性两个概念。在FedSoL中，本地学习寻找一个对于质量变化具有鲁棒性的参数空间。这种策略引入了一种含义质量变化的隐藏约束效果，同时保持原始本地目标进行参数更新。我们的实验显示，FedSoL能够在不同的设置下保持状态革命性的性能。
</details></li>
</ul>
<hr>
<h2 id="Not-Only-Rewards-But-Also-Constraints-Applications-on-Legged-Robot-Locomotion"><a href="#Not-Only-Rewards-But-Also-Constraints-Applications-on-Legged-Robot-Locomotion" class="headerlink" title="Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion"></a>Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12517">http://arxiv.org/abs/2308.12517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunho Kim, Hyunsik Oh, Jeonghyun Lee, Jinhyeok Choi, Gwanghyeon Ji, Moonkyu Jung, Donghoon Youm, Jemin Hwangbo<br>for: 本研究旨在开发一种基于强化学习的控制器训练框架，以便在复杂的机器人系统中实现自然的运动风格和高任务性能。methods: 该框架使用了两种约束类型和一种高效的政策优化算法，以便让工程师可以合理地反映他们的意图并处理约束，而不需要进行大量的计算开销。results: 在大量的simulation和实际实验中，这种学习框架可以让performant的控制器在不需要大量的奖励工程的情况下被训练，只需要调整一个奖励系数即可。此外，由于约束的可读性和普适性，可以更好地利用这些约束来实现更直观和intuitive的工程过程。<details>
<summary>Abstract</summary>
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attributes to traverse challenging terrains. Extensive simulation and real-world experiments demonstrate that performant controllers can be trained with significantly less reward engineering, by tuning only a single reward coefficient. Furthermore, a more straightforward and intuitive engineering process can be utilized, thanks to the interpretability and generalizability of constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</details>
<details>
<summary>摘要</summary>
几乎早前的研究已经表明，通过使用神经网络和无模型学习来设计控制器，可以实现复杂的机器人系统中的出色的控制性能。然而，这些出色的控制器具有自然的运动风格和高任务性能，通过广泛的奖励工程来实现，这是一项高度劳动密集和时间耗费的过程，涉及到设计很多奖励项和确定适当的奖励系数。在这种情况下，我们提出了一种新的学习框架，用于训练神经网络控制器，以便在复杂的机器人系统中实现高性能。为了让工程师能够正确地反映他们的意图，并减少计算开销，我们建议了两种约束类型和一种高效的政策优化算法。我们的学习框架在许多跑动和实际实验中被应用，并证明了可以通过微不足的奖励工程，训练出高性能的控制器。此外，由于约束的可读性和普遍性，工程师可以使用更加直观和直接的工程过程。有关视频可以在以下链接中找到：https://youtu.be/KAlm3yskhvM。
</details></li>
</ul>
<hr>
<h2 id="I3DOD-Towards-Incremental-3D-Object-Detection-via-Prompting"><a href="#I3DOD-Towards-Incremental-3D-Object-Detection-via-Prompting" class="headerlink" title="I3DOD: Towards Incremental 3D Object Detection via Prompting"></a>I3DOD: Towards Incremental 3D Object Detection via Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12512">http://arxiv.org/abs/2308.12512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqi Liang, Gan Sun, Chenxi Liu, Jiahua Dong, Kangru Wang</li>
<li>for: 本研究的目的是提出一种基于指导的增量3D对象检测框架（I3DOD），以解决现有的类增量3D对象检测方法会导致致命忘记旧类问题。</li>
<li>methods: 该方法提出了一种任务共享提示机制，通过学习对象位置信息和类别 semantic 信息之间的匹配关系，以及一种可靠的维持知识传递策略，包括一种可靠的动态维持策略和一种关系特征来捕捉响应特征空间中的相互关系。</li>
<li>results: 通过对两个 benchmark 数据集进行了广泛的实验， authors 发现，与现有对象检测方法相比，该方法在 <a href="mailto:&#109;&#65;&#80;&#x40;&#x30;&#x2e;&#x32;&#x35;">&#109;&#65;&#80;&#x40;&#x30;&#x2e;&#x32;&#x35;</a> 中提高了0.6% - 2.7%。<details>
<summary>Abstract</summary>
3D object detection has achieved significant performance in many fields, e.g., robotics system, autonomous driving, and augmented reality. However, most existing methods could cause catastrophic forgetting of old classes when performing on the class-incremental scenarios. Meanwhile, the current class-incremental 3D object detection methods neglect the relationships between the object localization information and category semantic information and assume all the knowledge of old model is reliable. To address the above challenge, we present a novel Incremental 3D Object Detection framework with the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared prompts mechanism to learn the matching relationships between the object localization information and category semantic information. After training on the current task, these prompts will be stored in our prompt pool, and perform the relationship of old classes in the next task. Moreover, we design a reliable distillation strategy to transfer knowledge from two aspects: a reliable dynamic distillation is developed to filter out the negative knowledge and transfer the reliable 3D knowledge to new detection model; the relation feature is proposed to capture the responses relation in feature space and protect plasticity of the model when learning novel 3D classes. To the end, we conduct comprehensive experiments on two benchmark datasets and our method outperforms the state-of-the-art object detection methods by 0.6% - 2.7% in terms of mAP@0.25.
</details>
<details>
<summary>摘要</summary>
三维物体检测已经在许多领域取得了显著性能，如 робо扮系统、自动驾驶和增强现实。然而，大多数现有方法在类增量enario中会导致致命的忘记老类。同时，当前的类增量三维物体检测方法忽视了物体Localization信息和类别Semantic信息之间的关系，并将所有古老模型的知识视为可靠。为了解决这些挑战，我们提出了一种新的增量三维物体检测框架，即I3DOD。我们特点是提出了一种任务分享提示机制，用于学习物体Localization信息和类别Semantic信息之间的匹配关系。在训练当前任务后，这些提示将被存储在我们的提示Pool中，并在下一个任务中进行相应的关系学习。此外，我们设计了一种可靠的搅拌策略，用于从两个方面传递知识：一种可靠的动态搅拌策略可以过滤出负知识，并将可靠的三维知识传递到新的检测模型；另一种特征relation被提出，用于在特征空间中捕捉响应关系，并保护模型在学习新的三维类时的塑性。最后，我们进行了对两个benchmark数据集的全面实验，并发现我们的方法在mAP@0.25上比前state-of-the-art object detection方法高出0.6% - 2.7%。
</details></li>
</ul>
<hr>
<h2 id="Masked-Autoencoders-are-Efficient-Class-Incremental-Learners"><a href="#Masked-Autoencoders-are-Efficient-Class-Incremental-Learners" class="headerlink" title="Masked Autoencoders are Efficient Class Incremental Learners"></a>Masked Autoencoders are Efficient Class Incremental Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12510">http://arxiv.org/abs/2308.12510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scok30/mae-cil">https://github.com/scok30/mae-cil</a></li>
<li>paper_authors: Jiang-Tian Zhai, Xialei Liu, Andrew D. Bagdanov, Ke Li, Ming-Ming Cheng</li>
<li>for: 这篇论文旨在Sequential Learning中学习新的类别，并避免过去知识的抹除。</li>
<li>methods: 我们提出使用Masked Autoencoders（MAEs）作为有效的学习器，MAEs可以透过复原式无监督学习得到有用的表现，并且可以轻松地与类别标签整合。我们还提出了两边MAE框架，从图像水平和嵌入水平的融合中学习。</li>
<li>results: 我们的方法在CIFAR-100、ImageNet-Subset和ImageNet-Full上比顶对比方法更好，实验证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified ChineseClass Incremental Learning (CIL) targets Sequential Learning of new classes while avoiding catastrophic forgetting of previous knowledge. We propose using Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. Code available at https://github.com/scok30/MAE-CIL .Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese languages. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CGMI-Configurable-General-Multi-Agent-Interaction-Framework"><a href="#CGMI-Configurable-General-Multi-Agent-Interaction-Framework" class="headerlink" title="CGMI: Configurable General Multi-Agent Interaction Framework"></a>CGMI: Configurable General Multi-Agent Interaction Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12503">http://arxiv.org/abs/2308.12503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi Jinxin, Zhao Jiabao, Wang Yilei, Wu Xingjiao, Li Jiawen, He Liang</li>
<li>for: 这个论文旨在提供一个基于大语言模型的多代理人系统，用于模拟人类交互和解决域专任务。</li>
<li>methods: 该系统使用树结构的方法来分配、检测和维护代理人性格，同时采用基于ACT*模型的认知建筑，包括记忆、反思和规划模块。</li>
<li>results: 通过在虚拟环境中模拟教师与学生之间的互动，该系统实现了许多与实际教室情况相似的方面，如教学方法、课程和学生表现。<details>
<summary>Abstract</summary>
Benefiting from the powerful capabilities of large language models (LLMs), agents based on LLMs have shown the potential to address domain-specific tasks and emulate human behaviors. However, the content generated by these agents remains somewhat superficial, owing to their limited domain expertise and the absence of an effective cognitive architecture. To address this, we present the Configurable General Multi-Agent Interaction (CGMI) framework, designed to replicate human interactions in real-world scenarios. Specifically, we propose a tree-structured methodology for the assignment, detection, and maintenance of agent personality. Additionally, we designed a cognitive architecture equipped with a skill library based on the ACT* model, which contains memory, reflection, and planning modules. We have also integrated general agents to augment the virtual environment's realism. Using the CGMI framework, we simulated numerous classroom interactions between teacher and students. The experiments indicate that aspects such as the teaching methodology, curriculum, and student performance closely mirror real classroom settings. We will open source our work.
</details>
<details>
<summary>摘要</summary>
利用大型自然语言模型（LLM）的强大能力，基于LLM的代理人已经展示了解决域特定任务和模拟人类行为的潜力。然而，由这些代理人生成的内容仍然有所 superficious，主要是因为它们的域专业知识有限和缺乏有效的认知架构。为了解决这个问题，我们提出了可 configurable通用多代理人交互（CGMI）框架，用于复制真实世界中的人类互动。具体来说，我们提出了一种树状的方法ологи？ для代理人性分配、检测和维护。此外，我们还设计了一个基于ACT*模型的认知架构，包括记忆、反思和规划模块。此外，我们还将通用代理人集成到虚拟环境中，以增加真实性。使用CGMI框架，我们模拟了许多教室互动 между教师和学生。实验结果表明，教学方法、课程和学生表现均具有真实教室情况的特点。我们将将我们的工作开源。
</details></li>
</ul>
<hr>
<h2 id="Source-Free-Collaborative-Domain-Adaptation-via-Multi-Perspective-Feature-Enrichment-for-Functional-MRI-Analysis"><a href="#Source-Free-Collaborative-Domain-Adaptation-via-Multi-Perspective-Feature-Enrichment-for-Functional-MRI-Analysis" class="headerlink" title="Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis"></a>Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12495">http://arxiv.org/abs/2308.12495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yqfang9199/scda">https://github.com/yqfang9199/scda</a></li>
<li>paper_authors: Yuqi Fang, Jinjian Wu, Qianqian Wang, Shijun Qiu, Andrea Bozoki, Huaicheng Yan, Mingxia Liu</li>
<li>for: 这项研究旨在提供一种无源数据的域适应方法，以帮助解决Functional MRI（fMRI）数据的差异问题，从而提高脑功能研究的精度和可重复性。</li>
<li>methods: 该方法基于多个视角特征增强方法（MFE），包括多个协作分支，每个分支都有数据供应模块、空间时间特征编码器和分类预测器。此外，我们还提出了一种互相一致约束，以便在不同域的数据上学习坚实的特征表示。</li>
<li>results: 我们的方法在三个公共数据集和一个私有数据集上进行了实验，并达到了跨扫描仪和跨研究任务的预测任务中的高效性。此外，我们还提供了一个基于大规模rs-fMRI数据的预训练模型，并将其公开发布。<details>
<summary>Abstract</summary>
Resting-state functional MRI (rs-fMRI) is increasingly employed in multi-site research to aid neurological disorder analysis. Existing studies usually suffer from significant cross-site/domain data heterogeneity caused by site effects such as differences in scanners/protocols. Many methods have been proposed to reduce fMRI heterogeneity between source and target domains, heavily relying on the availability of source data. But acquiring source data is challenging due to privacy concerns and/or data storage burdens in multi-site studies. To this end, we design a source-free collaborative domain adaptation (SCDA) framework for fMRI analysis, where only a pretrained source model and unlabeled target data are accessible. Specifically, a multi-perspective feature enrichment method (MFE) is developed for target fMRI analysis, consisting of multiple collaborative branches to dynamically capture fMRI features of unlabeled target data from multiple views. Each branch has a data-feeding module, a spatiotemporal feature encoder, and a class predictor. A mutual-consistency constraint is designed to encourage pair-wise consistency of latent features of the same input generated from these branches for robust representation learning. To facilitate efficient cross-domain knowledge transfer without source data, we initialize MFE using parameters of a pretrained source model. We also introduce an unsupervised pretraining strategy using 3,806 unlabeled fMRIs from three large-scale auxiliary databases, aiming to obtain a general feature encoder. Experimental results on three public datasets and one private dataset demonstrate the efficacy of our method in cross-scanner and cross-study prediction tasks. The model pretrained on large-scale rs-fMRI data has been released to the public.
</details>
<details>
<summary>摘要</summary>
休息态功能磁共振成像（rs-fMRI）在多站研究中越来越广泛应用，以帮助诊断神经系统疾病。现有的研究通常受到数据不同站点/领域的差异所致的重大跨站/领域数据不一致性问题，而且许多方法已经被提出来减少rs-fMRI数据的不一致性。然而，获取源数据是困难的，这主要是因为隐私问题和数据存储压力在多站研究中。为此，我们设计了一个无源数据的协同领域适应（SCDA）框架，只有预训练的源模型和无标签目标数据可用。具体来说，我们开发了一种多视角特征增强方法（MFE），用于目标rs-fMRI分析，该方法包括多个协同分支，用于动态捕捉目标rs-fMRI数据的不同视角特征。每个分支有数据供应模块、空间时间特征编码器和类别预测器。我们设计了一种互相一致性约束，以便在这些分支中对同一个输入生成的约束实现Robust特征学习。为了快速无源数据进行跨站知识传递，我们使用预训练源模型的参数初始化MFE。此外，我们还提出了一种无监督预训练策略，使用3806个无标签rs-fMRI数据集，以获得一个通用的特征编码器。实验结果表明，我们的方法在三个公共数据集和一个私有数据集上表现出色，在跨扫描和跨研究预测任务中。我们已经将预训练于大规模rs-fMRI数据的模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="GPTEval-A-Survey-on-Assessments-of-ChatGPT-and-GPT-4"><a href="#GPTEval-A-Survey-on-Assessments-of-ChatGPT-and-GPT-4" class="headerlink" title="GPTEval: A Survey on Assessments of ChatGPT and GPT-4"></a>GPTEval: A Survey on Assessments of ChatGPT and GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12488">http://arxiv.org/abs/2308.12488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, Erik Cambria</li>
<li>for: This survey aims to comprehensively review and analyze the collective assessment findings of ChatGPT and GPT-4 in various tasks and disciplines, focusing on their language and reasoning abilities, scientific knowledge, and ethical considerations.</li>
<li>methods: The survey examines prior evaluations of ChatGPT and GPT-4, including their language and reasoning abilities, scientific knowledge, and ethical considerations.</li>
<li>results: The survey provides a comprehensive assessment of the collective findings of prior evaluations, offering several recommendations for future research in evaluating large language models.<details>
<summary>Abstract</summary>
The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research in evaluating large language models.
</details>
<details>
<summary>摘要</summary>
chatgpt的出现引发了媒体的很多 спекуляция，关于其可能性影响社会和经济系统。它的语言能力引起了学者们强烈的好奇，关于它在不同领域的表现。已有许多研究评估chatgpt和gpt-4的能力，但没有一篇全面的评估报告。本调查的目标是对先前评估chatgpt和gpt-4的结果进行全面分析，强调其语言和理智能力、科学知识和伦理考虑。此外，还进行了现有评估方法的检查，提出了未来研究评估大语言模型的建议。
</details></li>
</ul>
<hr>
<h2 id="A-Model-of-Sequential-Learning-based-on-Non-Axiomatic-Logic"><a href="#A-Model-of-Sequential-Learning-based-on-Non-Axiomatic-Logic" class="headerlink" title="A Model of Sequential Learning based on Non-Axiomatic Logic"></a>A Model of Sequential Learning based on Non-Axiomatic Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12486">http://arxiv.org/abs/2308.12486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Xu</li>
<li>for: 本论文是关于智能代理人的sequential learning功能的研究报告。</li>
<li>methods: 本论文使用非AXIomial逻辑来解释学习过程，包括三步：假设、修订和回收。学习过程可以在不充分的知识和资源下进行。</li>
<li>results: 虽然当前设计有限制，但模型在一些简单的情况下已经证明有效。<details>
<summary>Abstract</summary>
Sequential learning is a fundamental function of an intelligent agent. This technical report introduces a model of sequential learning, which is interpretable through Non-Axiomatic Logic. The learning procedure includes three steps, hypothesizing, revising, and recycling, and can work under the Assumption of Insufficient Knowledge and Resources. Although there are limitations for the current design, the model has been proven effective in some simple cases.
</details>
<details>
<summary>摘要</summary>
Sequential learning是智能代理的基本功能之一。本技报介绍了一种可解释的续学学习模型，通过非AXIом逻辑进行解释。学习过程包括三步：假设、修订和回收，可以在不充分的知识和资源下进行。虽然当前设计有限制，但模型已在一些简单的情况下证明有效。Note: "Sequential learning" in Chinese is usually translated as "续学学习" (xùxué xuéxí), but in this context, the word "sequential" is used to emphasize the order of the learning steps, so I used "续学" (xùxué) instead.
</details></li>
</ul>
<hr>
<h2 id="Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection"><a href="#Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection" class="headerlink" title="Attention-Based Acoustic Feature Fusion Network for Depression Detection"></a>Attention-Based Acoustic Feature Fusion Network for Depression Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12478">http://arxiv.org/abs/2308.12478</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuxiaoooo/abafnet">https://github.com/xuxiaoooo/abafnet</a></li>
<li>paper_authors: Xiao Xu, Yang Wang, Xinru Wei, Fei Wang, Xizhe Zhang</li>
<li>for: 这个论文旨在提出一种新的听音数据检测方法，用于早期发现抑郁症。</li>
<li>methods: 该方法利用了高级机器学习理论，并将四种不同的听音特征 fusion 到一起，以提高抑郁症检测的准确率。</li>
<li>results: 对两个临床听音数据库进行了广泛验证，并比前期方法提高了抑郁症检测和亚型分类的性能。 I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Depression, a common mental disorder, significantly influences individuals and imposes considerable societal impacts. The complexity and heterogeneity of the disorder necessitate prompt and effective detection, which nonetheless, poses a difficult challenge. This situation highlights an urgent requirement for improved detection methods. Exploiting auditory data through advanced machine learning paradigms presents promising research directions. Yet, existing techniques mainly rely on single-dimensional feature models, potentially neglecting the abundance of information hidden in various speech characteristics. To rectify this, we present the novel Attention-Based Acoustic Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines four different acoustic features into a comprehensive deep learning model, thereby effectively integrating and blending multi-tiered features. We present a novel weight adjustment module for late fusion that boosts performance by efficaciously synthesizing these features. The effectiveness of our approach is confirmed via extensive validation on two clinical speech databases, CNRAC and CS-NRAC, thereby outperforming previous methods in depression detection and subtype classification. Further in-depth analysis confirms the key role of each feature and highlights the importance of MFCCrelated features in speech-based depression detection.
</details>
<details>
<summary>摘要</summary>
抑郁症是一种常见的心理疾病，对个人和社会产生了重要的影响。由于抑郁症的复杂性和多样性，早期检测成为了一项紧迫的需求。然而，现有的检测方法多数仅仅利用单一的特征模型，可能会损失大量的信息。为了解决这问题，我们提出了一种新的注意力基于的听音特征融合网络（ABAFnet），用于抑郁检测。ABAFnet结合了四种不同的听音特征，通过深度学习模型进行集成和融合。我们还提出了一种新的权重调整模块，用于late fusion，可以有效地合并这些特征。我们通过对两个临床听音数据库（CNRAC和CS-NRAC）进行广泛验证，证明了我们的方法在抑郁检测和分型分类方面的表现比前方法更高。进一步的深入分析表明，每种特征都扮演着重要的角色，而MFCC相关的特征在听音基于的抑郁检测中具有重要性。
</details></li>
</ul>
<hr>
<h2 id="Are-ChatGPT-and-GPT-4-Good-Poker-Players-–-A-Pre-Flop-Analysis"><a href="#Are-ChatGPT-and-GPT-4-Good-Poker-Players-–-A-Pre-Flop-Analysis" class="headerlink" title="Are ChatGPT and GPT-4 Good Poker Players? – A Pre-Flop Analysis"></a>Are ChatGPT and GPT-4 Good Poker Players? – A Pre-Flop Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12466">http://arxiv.org/abs/2308.12466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshat Gupta</li>
<li>for: 这个论文旨在评估 chatGPT 和 GPT-4 在牛牛游戏中的表现。</li>
<li>methods: 作者使用 chatGPT 和 GPT-4 进行了一系列实验，以评估这两种模型在牛牛游戏中的表现。</li>
<li>results: 研究发现，虽然 chatGPT 和 GPT-4 都具备了牛牛游戏的基本理解，但两者都不是 game theory optimal (GTO) 牛牛 player。GPT-4 比 chatGPT 更为进攻性，但两者的策略都不是 GTO。<details>
<summary>Abstract</summary>
Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players.   Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This exploration then sheds light on the divergent poker tactics of the two models: ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker vernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which means that it has a propensity to only engage with premium hands and folds a majority of hands. When subjected to the same directive, GPT-4 plays like a maniac, showcasing a loose and aggressive style of play. Both strategies, although relatively advanced, are not game theory optimal.
</details>
<details>
<summary>摘要</summary>
Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their aptitude across domains is evident, but their ability in playing games and specifically their ability in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions, and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players.  Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This exploration then sheds light on the divergent poker tactics of the two models: ChatGPT's conservativeness juxtaposed against GPT-4's aggression. In poker vernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which means that it has a propensity to only engage with premium hands and folds a majority of hands. When subjected to the same directive, GPT-4 plays like a maniac, showcasing a loose and aggressive style of play. Both strategies, although relatively advanced, are not game theory optimal.
</details></li>
</ul>
<hr>
<h2 id="PFL-GAN-When-Client-Heterogeneity-Meets-Generative-Models-in-Personalized-Federated-Learning"><a href="#PFL-GAN-When-Client-Heterogeneity-Meets-Generative-Models-in-Personalized-Federated-Learning" class="headerlink" title="PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning"></a>PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12454">http://arxiv.org/abs/2308.12454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achintha Wijesinghe, Songyang Zhang, Zhi Ding</li>
<li>for: 提高个人化 federated learning (PFL) 的效果，处理客户端数据不同的场景。</li>
<li>methods: 提出一种基于生成器对抗网络 (GAN) 的 PFL 模型，通过学习客户端之间的相似性，实现Weighted collaborative data aggregation。</li>
<li>results: 通过对几个常见的数据集进行严谨的实验，证明 PFL-GAN 的效果。<details>
<summary>Abstract</summary>
Recent advances of generative learning models are accompanied by the growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure, and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) sometimes can be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To cope with client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in different scenarios. More specially, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results through the rigorous experimentation on several well-known datasets demonstrate the effectiveness of PFL-GAN.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:现有的生成学模型技术的进步，使得联合学习（FL）基于生成对抗网络（GAN）模型的兴趣在提高。在FL中，GAN可以捕捉客户端数据结构，并生成类似原始数据分布的样本，无需披露私有的原始数据。然而，大多数现有的GAN-based FL工作都是通过全局模型进行训练，而Personalized FL（PFL）可能更有效，尤其是在客户端数据多样性方面，包括不同的数据样本分布、特征空间和标签。为了 Addressing client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses client heterogeneity in different scenarios by first learning the similarity among clients and then developing an weighted collaborative data aggregation. Empirical results from rigorous experimentation on several well-known datasets demonstrate the effectiveness of PFL-GAN.
</details></li>
</ul>
<hr>
<h2 id="Augmenting-medical-image-classifiers-with-synthetic-data-from-latent-diffusion-models"><a href="#Augmenting-medical-image-classifiers-with-synthetic-data-from-latent-diffusion-models" class="headerlink" title="Augmenting medical image classifiers with synthetic data from latent diffusion models"></a>Augmenting medical image classifiers with synthetic data from latent diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12453">http://arxiv.org/abs/2308.12453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke W. Sagers, James A. Diao, Luke Melas-Kyriazi, Matthew Groh, Pranav Rajpurkar, Adewole S. Adamson, Veronica Rotemberg, Roxana Daneshjou, Arjun K. Manrai</li>
<li>for: 这篇论文的目的是为了探讨人工智能（AI）算法在医疗领域中的应用，特别是在处理皮肤病的情况下。</li>
<li>methods: 这篇论文使用了潜在扩散模型来生成皮肤病的synthetic图像，并评估了这些数据是否能够提高医疗AI算法的表现。</li>
<li>results: 研究发现，使用synthetic图像进行模型训练可以提高模型的表现，但是这些表现增强随着实际图像的数量增加。 Specifically, the performance gains saturate at a synthetic-to-real image ratio of 10:1, and are substantially smaller than the gains obtained from adding real images.<details>
<summary>Abstract</summary>
While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA), many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations. Some have proposed that generative AI could reduce the need for real data, but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images. As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using several generation strategies. Our results suggest that synthetic data could serve as a force-multiplier for model development, but the collection of diverse real-world data remains the most important step to improve medical AI algorithms.
</details>
<details>
<summary>摘要</summary>
美国食品和药品管理局（FDA）已批准或批准了数百个人工智能（AI）算法，但许多研究表明这些算法在不同人群中存在不一致的泛化或隐藏偏见，特别是对于受保护的人群。一些人提议使用生成AI可以减少实际数据的需求，但它在模型开发中的使用效果仍然不清楚。皮肤病 serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. 在这里，我们表明了液态增殖模型可以可扩展地生成皮肤病图像，并且在数据有限的情况下，通过这些数据进行模型训练可以提高表现。这些表现提升随synthetic-to-real图像比率增加，并且比于使用实际图像添加表现更小。在我们的分析中，我们生成了458,920个synthetic图像，并进行了分析。我们的结果表明，生成数据可以成为模型开发中的力multiplier，但收集多样化的实际数据仍然是改进医疗AI算法的最重要步骤。
</details></li>
</ul>
<hr>
<h2 id="An-Intentional-Forgetting-Driven-Self-Healing-Method-For-Deep-Reinforcement-Learning-Systems"><a href="#An-Intentional-Forgetting-Driven-Self-Healing-Method-For-Deep-Reinforcement-Learning-Systems" class="headerlink" title="An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"></a>An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12445">http://arxiv.org/abs/2308.12445</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedhajyahmed/drdrl">https://github.com/ahmedhajyahmed/drdrl</a></li>
<li>paper_authors: Ahmed Haj Yahmed, Rached Bouchoucha, Houssem Ben Braiek, Foutse Khomh</li>
<li>for: 本研究旨在提出一种有效的自适应方法，以解决 Deep Reinforcement Learning (DRL) 系统在大规模生产环境中遇到的环境风险。</li>
<li>methods: 本研究使用了 vanilla Continual Learning (CL) 方法，并增加了一种归yk intentional forgetting 机制，以解决 CL 中的主要问题，如 catastrophic forgetting、warm-starting failure 和 slow convergence。</li>
<li>results: 比较 vanilla CL 和 Dr. DRL 两种方法，Dr. DRL 能够在不同的演变环境中减少平均的恢复时间和精度调整集数量，并在19.63% 的演变环境中成功适应。同时，Dr. DRL 能够保持和提高在演变环境中解决的奖励 Water 到 45%。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment's conditions shifts. However, successive shifts of considerable magnitude may cause the production environment to drift from its original state. Recent studies have shown that these environmental drifts tend to drive CL into long, or even unsuccessful, healing cycles, which arise from inefficiencies such as catastrophic forgetting, warm-starting failure, and slow convergence. In this paper, we propose Dr. DRL, an effective self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately erases the DRL system's minor behaviors to systematically prioritize the adaptation of the key problem-solving skills. Using well-established DRL algorithms, Dr. DRL is compared with vanilla CL on various drifted environments. Dr. DRL is able to reduce, on average, the healing time and fine-tuning episodes by, respectively, 18.74% and 17.72%. Dr. DRL successfully helps agents to adapt to 19.63% of drifted environments left unsolved by vanilla CL while maintaining and even enhancing by up to 45% the obtained rewards for drifted environments that are resolved by both approaches.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）在大规模生产中越来越普遍应用，如Netflix和Facebook。然而，与大多数数据驱动系统一样，DRL系统可能会表现出不良行为，即因环境变化而导致的环境漂移。循环学习（CL）是DRLAgent的自适应方法，可以响应环境的变化。然而，连续的大规模变化可能会使生产环境偏离原来的状态。现有研究表明，这些环境变化通常会让CL进入长期或无法恢复的循环征化，这是因为它们可能会导致忘记、暖启缺陷和慢 converges。在这篇论文中，我们提出了Dr. DRL，一种有效的自适应方法，用于解决DRL系统中的环境漂移问题。Dr. DRL通过novel的意图忘记机制，系统地优先级掌握DRL系统的关键问题解决技能。使用已知的DRL算法，Dr. DRL与vanilla CL进行了比较。Dr. DRL能够将循环时间和微调集数降低，减少了18.74%和17.72%。Dr. DRL成功地帮助代理人适应了19.63%的漂移环境，并保持了或提高了对漂移环境的解决率。
</details></li>
</ul>
<hr>
<h2 id="BaDExpert-Extracting-Backdoor-Functionality-for-Accurate-Backdoor-Input-Detection"><a href="#BaDExpert-Extracting-Backdoor-Functionality-for-Accurate-Backdoor-Input-Detection" class="headerlink" title="BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection"></a>BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12439">http://arxiv.org/abs/2308.12439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal<br>for:这则研究旨在防止深度神经网络（DNNs）上的后门攻击，其中敌友将附加了阴性的行为（backdoor）到DNNs中。methods:我们的防御方法属于post-development防御， Meaning it operates independently of how the model was generated。我们的防御方法基于一种新的反工程approach，可以直接将backdoor功能从一个附加了backdoor的模型中提取出来，并转换为一个名为backdoor expert model的模型。results:我们的防御方法可以高度有效地排除backdoor输入，并对于清洁的使用者享有轻微的影响。我们在多个数据集（CIFAR10、GTSRB和ImageNet）上验证了我们的防御方法，并在不同的模型架构（ResNet、VGG、MobileNetV2和Vision Transformer）上进行了实验。<details>
<summary>Abstract</summary>
We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet) across various model architectures (ResNet, VGG, MobileNetV2 and Vision Transformer).
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的防御机制，以防止深度神经网络（DNN）上的后门攻击。在这种攻击中，敌人将附加了阴性的行为（backdoor）到DNN中。我们的防御方法属于后期开发防御，可以独立于模型的生成方式运作。我们的防御方法基于一种新的反引擎方法，可以直接将backdoor功能从一个附加了backdoor的模型中提取出来，并将其转换为一个专门的backdoor专家模型。这个方法简单明了：通过在一小量故意错abeled的清洁样本上调整附加了backdoor的模型，使其忘记正常功能，但仍保留backdoor功能，从而将模型转换为可以只识别backdoor输入的模型（称为backdoor专家模型）。基于提取的backdoor专家模型，我们显示了可以创建高精度的backdoor输入检测器，以筛出backdoor输入在模型测试过程中。另外，我们还将这个防御方法与一个调整的副标的模型进行拓展，创建了一个名为BaDExpert（深度神经网络后门专家）的防御系统。BaDExpert有效地抵销了16个SOTA后门攻击，而且对于清洁的功能影响轻微。我们在CIFAR10、GTSRB和ImageNet等多个数据集上验证了BaDExpert的可靠性，并在不同的模型架构（ResNet、VGG、MobileNetV2和Vision Transformer）上进行了实验。
</details></li>
</ul>
<hr>
<h2 id="Deploying-Deep-Reinforcement-Learning-Systems-A-Taxonomy-of-Challenges"><a href="#Deploying-Deep-Reinforcement-Learning-Systems-A-Taxonomy-of-Challenges" class="headerlink" title="Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges"></a>Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12438">http://arxiv.org/abs/2308.12438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drldeploymentchallenges-icsme2023/replicationpackage">https://github.com/drldeploymentchallenges-icsme2023/replicationpackage</a></li>
<li>paper_authors: Ahmed Haj Yahmed, Altaf Allah Abbassi, Amin Nikanjam, Heng Li, Foutse Khomh</li>
<li>For: This paper aims to understand the challenges that practitioners face when deploying deep reinforcement learning (DRL) systems, and to identify the most common and difficult challenges in deploying DRL to different platforms.* Methods: The paper uses an empirical study on Stack Overflow (SO), the most popular Q&amp;A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. The study categorizes relevant SO posts by deployment platforms and investigates the current state and challenges related to deploying DRL systems.* Results: The study finds that the general interest in DRL deployment is growing, confirming the study’s relevance and importance. The study also finds that DRL deployment is more difficult than other DRL issues, and that RL environment-related challenges are the most popular, while communication-related challenges are the most difficult among practitioners. The study identifies a taxonomy of 31 unique challenges in deploying DRL to different platforms.<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in reinforcement learning, has shown significant potential in achieving human-level autonomy in a wide range of domains, including robotics, computer vision, and computer games. This potential justifies the enthusiasm and growing interest in DRL in both academia and industry. However, the community currently focuses mostly on the development phase of DRL systems, with little attention devoted to DRL deployment. In this paper, we propose an empirical study on Stack Overflow (SO), the most popular Q&A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. Then, we investigate the prevalence and difficulty of these challenges. Results show that the general interest in DRL deployment is growing, confirming the study's relevance and importance. Results also show that DRL deployment is more difficult than other DRL issues. Additionally, we built a taxonomy of 31 unique challenges in deploying DRL to different platforms. On all platforms, RL environment-related challenges are the most popular, and communication-related challenges are the most difficult among practitioners. We hope our study inspires future research and helps the community overcome the most common and difficult challenges practitioners face when deploying DRL systems.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL），利用深度学习（DL）在强化学习中，已经表现出了人类水平自主性的潜力，在 робо技术、计算机视觉和电子游戏等领域取得了 significante 的成果。这种潜力正ifies和业界的兴趣，但现在大多数研究者在DRL系统的开发阶段所投入的时间较多，对DRL部署的关注相对较少。在这篇论文中，我们通过Stack Overflow（SO），最popular的开发者问答社区，进行了实证研究，探索和理解实践者在部署DRL系统时遇到的挑战。 Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. Then, we investigate the prevalence and difficulty of these challenges. Results show that the general interest in DRL deployment is growing, confirming the study's relevance and importance. Results also show that DRL deployment is more difficult than other DRL issues. Additionally, we built a taxonomy of 31 unique challenges in deploying DRL to different platforms. On all platforms, RL environment-related challenges are the most popular, and communication-related challenges are the most difficult among practitioners. We hope our study inspires future research and helps the community overcome the most common and difficult challenges practitioners face when deploying DRL systems.
</details></li>
</ul>
<hr>
<h2 id="Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach"><a href="#Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach" class="headerlink" title="Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach"></a>Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12416">http://arxiv.org/abs/2308.12416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Gianchandani, Mahsa Dibaji, Mariana Bento, Ethan MacDonald, Roberto Souza</li>
<li>for: 这篇论文旨在用深度学习模型从核磁共振图像中预测大脑年龄，并提供更加可读的解释方式。</li>
<li>methods: 这篇论文使用了图像到图像回归模型，以估计大脑每个脑窍ixel的年龄。它们比较了全像年龄预测模型和相应的精密度地图，并证明了 voxel-wise 预测模型更加可读，因为它们提供了脑部年龄变化的空间信息，并且具有量化的优势。</li>
<li>results: 研究结果表明，voxel-wise 预测模型在提供脑部年龄变化的空间信息方面比较出色，而且具有量化的优势。<details>
<summary>Abstract</summary>
Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comparable across different samples. In this work, we reframe the age prediction problem from MR images to an image-to-image regression problem where we estimate the brain age for each brain voxel in MR images. We compare voxel-wise age prediction models against global age prediction models and their corresponding saliency maps. The results indicate that voxel-wise age prediction models are more interpretable, since they provide spatial information about the brain aging process, and they benefit from being quantitative.
</details>
<details>
<summary>摘要</summary>
In this work, we reframe the age prediction problem from MR images to an image-to-image regression problem, where we estimate the brain age for each brain voxel in MR images. We compare voxel-wise age prediction models against global age prediction models and their corresponding saliency maps. The results show that voxel-wise age prediction models are more interpretable, as they provide spatial information about the brain aging process, and they benefit from being quantitative.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Causal-Study-to-Interpret-Large-Language-Models-for-Source-Code"><a href="#Benchmarking-Causal-Study-to-Interpret-Large-Language-Models-for-Source-Code" class="headerlink" title="Benchmarking Causal Study to Interpret Large Language Models for Source Code"></a>Benchmarking Causal Study to Interpret Large Language Models for Source Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12415">http://arxiv.org/abs/2308.12415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Rodriguez-Cardenas, David N. Palacio, Dipin Khati, Henry Burke, Denys Poshyvanyk</li>
<li>for: 本研究旨在提供一种基于 causal inference 的生成代码评价策略，以帮助研究人员更好地理解 LLMS 的性能。</li>
<li>methods: 本研究使用了 Galeras  benchmarking strategy，包括三个 SE 任务（代码完成、代码摘要和提交生成）的测试床，以帮助解释 LLMS 的性能。</li>
<li>results: 对 ChatGPT 的表现进行了一个案例研究，发现prompt semantics 对 ChatGPT 的生成性能具有正向 causal 影响（均值效应约为 3%），并发现 prompt size 和 accuracy metrics 之间存在高度相关性（约为 0.412%）。这些结果表明，通过使用 causal inference 评价策略，可以减少偏见干扰，提供更可靠的 LLMS 性能评价。<details>
<summary>Abstract</summary>
One of the most common solutions adopted by software researchers to address code generation is by training Large Language Models (LLMs) on massive amounts of source code. Although a number of studies have shown that LLMs have been effectively evaluated on popular accuracy metrics (e.g., BLEU, CodeBleu), previous research has largely overlooked the role of Causal Inference as a fundamental component of the interpretability of LLMs' performance. Existing benchmarks and datasets are meant to highlight the difference between the expected and the generated outcome, but do not take into account confounding variables (e.g., lines of code, prompt size) that equally influence the accuracy metrics. The fact remains that, when dealing with generative software tasks by LLMs, no benchmark is available to tell researchers how to quantify neither the causal effect of SE-based treatments nor the correlation of confounders to the model's performance. In an effort to bring statistical rigor to the evaluation of LLMs, this paper introduces a benchmarking strategy named Galeras comprised of curated testbeds for three SE tasks (i.e., code completion, code summarization, and commit generation) to help aid the interpretation of LLMs' performance. We illustrate the insights of our benchmarking strategy by conducting a case study on the performance of ChatGPT under distinct prompt engineering methods. The results of the case study demonstrate the positive causal influence of prompt semantics on ChatGPT's generative performance by an average treatment effect of $\approx 3\%$. Moreover, it was found that confounders such as prompt size are highly correlated with accuracy metrics ($\approx 0.412\%$). The end result of our case study is to showcase causal inference evaluations, in practice, to reduce confounding bias. By reducing the bias, we offer an interpretable solution for the accuracy metric under analysis.
</details>
<details>
<summary>摘要</summary>
一种非常常见的解决方案是训练大型自然语言模型（LLM）以生成代码。虽然许多研究表明了LLM的评价精度（例如BLEU、CodeBleu），但previous research largely overlooked causal inference的作用。现有的benchmark和数据集只能highlight生成结果与预期结果的差异，而不考虑混合变量（例如代码行数、提示大小），这些变量也影响精度指标。因此，在LLM执行生成代码任务时，无法用benchmark来衡量SE-基于治疗的 causal效应，也无法衡量混合变量与模型性能的相关性。为了带来统计学的正规性，本文提出了一种名为Galerascurrency的 benchmarking策略，包括了三个SE任务（代码完成、代码摘要、提交生成）的curated testbed，以帮助解释LLM的性能。我们通过对ChatGPT的表现进行case study，ILLUSTRATE了我们的benchmarking策略的启示。结果表明，ChatGPT的生成性能受提示 semantics 的average treatment effect（ATT）的Positive causal影响，其中ATT约为3%。此外，我们发现提示大小和精度指标之间存在高度的相关性（约0.412%）。最终，我们的case study表明，通过使用causal inference评价，可以减少混合偏见，从而提供可解释的精度指标。
</details></li>
</ul>
<hr>
<h2 id="A-Theory-of-Intelligences-Concepts-Models-Implications"><a href="#A-Theory-of-Intelligences-Concepts-Models-Implications" class="headerlink" title="A Theory of Intelligences: Concepts, Models, Implications"></a>A Theory of Intelligences: Concepts, Models, Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12411">http://arxiv.org/abs/2308.12411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Michael E. Hochberg</li>
<li>for: The paper is written to propose a theory of intelligence based on first principles, with the goal of understanding intelligence in humans and machines, and to explain endeavors that do not necessarily affect Darwinian fitness.</li>
<li>methods: The paper uses a variety of methods, including discussing key features of intelligence, presenting a framework for a first principles Theory of Intelligence, and proposing a compact mathematical form of surprisal and difficulty.</li>
<li>results: The paper presents several conceptual advances, including the prediction that paths to a goal not only function to accurately achieve goals, but also lead to higher probabilities for future attainable goals and increased breadth to enter new goal spaces.Here is the information in Simplified Chinese text:</li>
<li>for: 本文提出了一种基于初等原理的智能理论，以便更好地理解人类和机器智能，以及不直接影响达尔沃因果适应的尝试。</li>
<li>methods: 本文使用了多种方法，包括讨论智能的关键特征、提出基于初等原理的智能理论框架，以及提出一种简洁的数学表达方式。</li>
<li>results: 本文提出了多个概念进步，包括Path efficiency和目标准确率的预测，以及路径可能导致更高的未来可达目标的可能性和新目标空间的扩展。<details>
<summary>Abstract</summary>
Intelligence is a human construct to represent the ability to achieve goals. Given this wide berth, intelligence has been defined countless times, studied in a variety of ways and quantified using numerous measures. Understanding intelligence ultimately requires theory and quantification, both of which are elusive. My main objectives are to identify some of the central elements in and surrounding intelligence, discuss some of its challenges and propose a theory based on first principles. I focus on intelligence as defined by and for humans, frequently in comparison to machines, with the intention of setting the stage for more general characterizations in life, collectives, human designs such as AI and in non-designed physical and chemical systems. I discuss key features of intelligence, including path efficiency and goal accuracy, intelligence as a Black Box, environmental influences, flexibility to deal with surprisal, the regress of intelligence, the relativistic nature of intelligence and difficulty, and temporal changes in intelligence including its evolution. I present a framework for a first principles Theory of IntelligenceS (TIS), based on the quantifiable macro-scale system features of difficulty, surprisal and goal resolution accuracy. The proposed partitioning of uncertainty/solving and accuracy/understanding is particularly novel since it predicts that paths to a goal not only function to accurately achieve goals, but as experimentations leading to higher probabilities for future attainable goals and increased breadth to enter new goal spaces. TIS can therefore explain endeavors that do not necessarily affect Darwinian fitness, such as leisure, politics, games and art. I conclude with several conceptual advances of TIS including a compact mathematical form of surprisal and difficulty, the theoretical basis of TIS, and open questions.
</details>
<details>
<summary>摘要</summary>
人类创造出的智能概念表示能够实现目标的能力。由此广泛定义、研究和量化，智能的理解最终需要理论和量化，它们都是悬而不稳的。我的主要目标是确定智能的中心元素，讨论智能的挑战，并提出基于初始原则的理论（TIS）。我将智能定义为人类定义的，并与机器进行比较，以设置更广泛的生命、集体、人工智能和非设计的物理和化学系统的舞台。我将讨论智能的关键特征，包括路径效率和目标准确率，智能的黑盒特性，环境影响、意外处理能力、智能的征途回归、智能的相对性和困难度，以及时间变化，包括智能的演化。我将提出一种基于量化大规模系统特征的TIS理论框架，该框架包括难度、意外和目标解决准确率三个量化特征。这种分解不确定性/解决和准确性/理解的分 partitioning是特别有趣，因为它预测了路径不仅用于准确实现目标，还用于产生更高的未来可能目标的可能性和扩大进入新的目标空间。TIS可以解释不必要影响达尔沃尼遗传fitness的活动，如休闲、政治、游戏和艺术。我的概念进步包括量化难度和意外的mathematical表达、TIS理论基础和开Question。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Endoscopic-Video-Analysis"><a href="#Self-Supervised-Learning-for-Endoscopic-Video-Analysis" class="headerlink" title="Self-Supervised Learning for Endoscopic Video Analysis"></a>Self-Supervised Learning for Endoscopic Video Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12394">http://arxiv.org/abs/2308.12394</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/royhirsch/endossl">https://github.com/royhirsch/endossl</a></li>
<li>paper_authors: Roy Hirsch, Mathilde Caron, Regev Cohen, Amir Livne, Ron Shapiro, Tomer Golany, Roman Goldenberg, Daniel Freedman, Ehud Rivlin</li>
<li>for: 这篇论文的目的是探讨自主学习（SSL）在医疗领域中的应用，特别是在镜头结构探查和腹腔镜检查等领域。</li>
<li>methods: 这篇论文使用了Masked Siamese Networks（MSNs）作为SSL框架，并使用大量的无标注影像资料进行训练。</li>
<li>results: 这篇论文在医疗领域的测试中获得了state-of-the-art的表现，包括胃腔镜检查和colonoscopy的手术阶段识别和肿瘤特征分类等。此外，这篇论文还获得了50%的标注数据量减少，未对表现造成影响。因此，这篇论文提供了SSL可以对医疗领域中的标注数据量做出具体的减少。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) has led to important breakthroughs in computer vision by allowing learning from large amounts of unlabeled data. As such, it might have a pivotal role to play in biomedicine where annotating data requires a highly specialized expertise. Yet, there are many healthcare domains for which SSL has not been extensively explored. One such domain is endoscopy, minimally invasive procedures which are commonly used to detect and treat infections, chronic inflammatory diseases or cancer. In this work, we study the use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit the power of SSL, we create sizable unlabeled endoscopic video datasets for training MSNs. These strong image representations serve as a foundation for secondary training with limited annotated datasets, resulting in state-of-the-art performance in endoscopic benchmarks like surgical phase recognition during laparoscopy and colonoscopic polyp characterization. Additionally, we achieve a 50% reduction in annotated data size without sacrificing performance. Thus, our work provides evidence that SSL can dramatically reduce the need of annotated data in endoscopy.
</details>
<details>
<summary>摘要</summary>
自我监督学习（SSL）已经在计算机视觉领域取得了重要突破，允许学习大量无标注数据。因此，它可能在生物医学领域扮演重要的角色，因为标注数据的获得需要特殊的专业知识。然而，医疗领域中有许多尚未得到广泛探索的领域。我们在这种领域中研究了一种主流SSL框架，即假设网络（MSNs），用于endoскопи视频分析，如colonoscopy和laparoscopy。为了充分利用SSL的力量，我们创建了大量无标注endoскопи视频数据集用于MSNs的训练。这些强大的图像表示serve as a foundation for secondary training with limited annotated datasets， resulting in state-of-the-art performance in endoscopic benchmarks like surgical phase recognition during laparoscopy and colonoscopic polyp characterization。此外，我们实现了标注数据大小的50%减少，无需牺牲性能。因此，我们的工作提供了证据，表明SSL可以在endoscopy中减少标注数据的需求。
</details></li>
</ul>
<hr>
<h2 id="With-a-Little-Help-from-your-own-Past-Prototypical-Memory-Networks-for-Image-Captioning"><a href="#With-a-Little-Help-from-your-own-Past-Prototypical-Memory-Networks-for-Image-Captioning" class="headerlink" title="With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning"></a>With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12383">http://arxiv.org/abs/2308.12383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aimagelab/pma-net">https://github.com/aimagelab/pma-net</a></li>
<li>paper_authors: Manuele Barraco, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara</li>
<li>for: 本研究旨在提高图像描述task中Transformer建模器的表现，特别是利用其他训练样本的信息来提高图像描述的准确率。</li>
<li>methods: 本研究提出了一种基于prototypical memory模型的注意力机制，可以在图像描述任务中共享知识，提高模型的表现。</li>
<li>results: 实验结果表明，与基elines和state-of-the-art方法进行比较，提出的方案可以在COCO数据集上提高Encoder-Decoder Transformer模型的表现，增加CIDEr点3.7个。<details>
<summary>Abstract</summary>
Image captioning, like many tasks involving vision and language, currently relies on Transformer-based architectures for extracting the semantics in an image and translating it into linguistically coherent descriptions. Although successful, the attention operator only considers a weighted summation of projections of the current input sample, therefore ignoring the relevant semantic information which can come from the joint observation of other samples. In this paper, we devise a network which can perform attention over activations obtained while processing other training samples, through a prototypical memory model. Our memory models the distribution of past keys and values through the definition of prototype vectors which are both discriminative and compact. Experimentally, we assess the performance of the proposed model on the COCO dataset, in comparison with carefully designed baselines and state-of-the-art approaches, and by investigating the role of each of the proposed components. We demonstrate that our proposal can increase the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when training in cross-entropy only and when fine-tuning with self-critical sequence training. Source code and trained models are available at: https://github.com/aimagelab/PMA-Net.
</details>
<details>
<summary>摘要</summary>
图像描述、如多种视觉语言任务一样，目前都是基于Transformer架构来提取图像中的 semantics并将其转换成语言上的准确描述。虽然成功，但只考虑当前输入样本的权重汇集的注意操作，ignore了与其他样本的共同观察可能提供的相关 semantic信息。在这篇论文中，我们设计了一个网络，可以在处理其他训练样本的活动中进行注意力，通过一种prototype模型。我们的记忆模型将过去的键和值分布模型为prototype вектор，这些 вектор都是 Both discriminative和compact。实验中，我们对COCO dataset进行评估，与注意点设计和现有approaches进行比较，并investigate每个提案的作用。我们发现，我们的提议可以在encoder-decoder Transformer中提高性能，即使只在cross-entropy中训练或者自我批判序列训练。源代码和训练模型可以在：https://github.com/aimagelab/PMA-Net 中找到。
</details></li>
</ul>
<hr>
<h2 id="Open-set-Face-Recognition-with-Neural-Ensemble-Maximal-Entropy-Loss-and-Feature-Augmentation"><a href="#Open-set-Face-Recognition-with-Neural-Ensemble-Maximal-Entropy-Loss-and-Feature-Augmentation" class="headerlink" title="Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation"></a>Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12371">http://arxiv.org/abs/2308.12371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, Manuel Günther, William Robson Schwartz</li>
<li>for: 本研究旨在提高面Recognition系统的准确率和安全性，特别是在面识别任务中处理无法识别的人脸场景。</li>
<li>methods: 本研究提出了一种新的方法，通过结合紧凑型神经网络 ensemble和margin-based cost function来提高面识别精度和鲁棒性。在训练时间内，通过新的混合特征增强技术，可以从外部数据库或者生成synthetically获取补充的负样本。</li>
<li>results: 在LFW和IJB-C datasets上进行了实验，结果显示，该方法可以提高closed和open-set identification率。<details>
<summary>Abstract</summary>
Open-set face recognition refers to a scenario in which biometric systems have incomplete knowledge of all existing subjects. Therefore, they are expected to prevent face samples of unregistered subjects from being identified as previously enrolled identities. This watchlist context adds an arduous requirement that calls for the dismissal of irrelevant faces by focusing mainly on subjects of interest. As a response, this work introduces a novel method that associates an ensemble of compact neural networks with a margin-based cost function that explores additional samples. Supplementary negative samples can be obtained from external databases or synthetically built at the representation level in training time with a new mix-up feature augmentation approach. Deep neural networks pre-trained on large face datasets serve as the preliminary feature extraction module. We carry out experiments on well-known LFW and IJB-C datasets where results show that the approach is able to boost closed and open-set identification rates.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SafeAR-Towards-Safer-Algorithmic-Recourse-by-Risk-Aware-Policies"><a href="#SafeAR-Towards-Safer-Algorithmic-Recourse-by-Risk-Aware-Policies" class="headerlink" title="SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies"></a>SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12367">http://arxiv.org/abs/2308.12367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Wu, Shubham Sharma, Sunandita Patra, Sriram Gopalakrishnan<br>for:This paper focuses on providing recourse for individuals adversely affected by machine learning (ML) models in critical domains like finance and healthcare. The goal is to empower people to choose a recourse based on their risk tolerance, considering the risk of higher costs.methods:The paper proposes a method called Safer Algorithmic Recourse (SafeAR) that computes recourse policies with risk considerations. It connects algorithmic recourse literature with risk-sensitive reinforcement learning and adopts financial measures like Value at Risk and Conditional Value at Risk to summarize risk concisely.results:The paper compares policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity) on two real-world datasets. The results show that SafeAR can provide more risk-sensitive recourse recommendations than existing methods, enabling individuals to make more informed decisions based on their risk tolerance.<details>
<summary>Abstract</summary>
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures ``Value at Risk'' and ``Conditional Value at Risk'' from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity).
</details>
<details>
<summary>摘要</summary>
In this work, we discuss how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures like "Value at Risk" and "Conditional Value at Risk" from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity).
</details></li>
</ul>
<hr>
<h2 id="CHORUS-Learning-Canonicalized-3D-Human-Object-Spatial-Relations-from-Unbounded-Synthesized-Images"><a href="#CHORUS-Learning-Canonicalized-3D-Human-Object-Spatial-Relations-from-Unbounded-Synthesized-Images" class="headerlink" title="CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images"></a>CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12288">http://arxiv.org/abs/2308.12288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sookwan Han, Hanbyul Joo</li>
<li>for: 这项研究的目的是教育机器人理解和模型人类和物体之间的3D空间共同谐谑。</li>
<li>methods: 这种方法利用一种生成模型，该模型可以生成高质量的2D图像从不同视角 capture human-object互动的多个图像。</li>
<li>results: 研究人员表明，使用这种方法可以从不同视角的2D图像中学习人类和物体之间的3D空间共同谐谑。此外，他们还提出了多种方法，包括利用生成图像模型来学习3D人类-物体空间关系，以及使用3D占用理解和pose canonicalization来解释不一致的2D图像。<details>
<summary>Abstract</summary>
We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way. This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions. Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a supervised way. One way of learning the 3D spatial relationship between humans and objects during interaction is by showing multiple 2D images captured from different viewpoints when humans interact with the same type of objects. The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an "unbounded" data generator with effective controllability and view diversity. Despite its imperfection of the image quality over real images, we demonstrate that the synthesized images are sufficient to learn the 3D human-object spatial relations. We present multiple strategies to leverage the synthesized images, including (1) the first method to leverage a generative image model for 3D human-object spatial relation learning; (2) a framework to reason about the 3D spatial relations from inconsistent 2D cues in a self-supervised manner via 3D occupancy reasoning with pose canonicalization; (3) semantic clustering to disambiguate different types of interactions with the same object types; and (4) a novel metric to assess the quality of 3D spatial learning of interaction. Project Page: https://jellyheadandrew.github.io/projects/chorus
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，用于教学机器人理解和模型人类和物体之间的底层空间共识，在3D空间中自主监督式地学习。这是一项挑战性的任务，因为人类的姿势和物体的几何结构可以在相似的交互中存在差异，这导致了标注3D交互的困难和缺乏扩展性，限制了我们可以在监督方式下理解的可能性。一种方法是通过显示多个视角 capture的2D图像，以便在人类和物体之间的交互中学习3D空间关系。我们的核心想法是利用一种生成模型，可以生成高质量的2D图像，从不同的视角捕捉到人类和物体之间的交互。尽管生成的图像质量不如实际图像，但我们示出了这些生成的图像足够以学习人类和物体之间的3D空间关系。我们提出了多种使用生成的图像进行3D人物空间关系学习的策略，包括：1. 首次利用生成图像模型来学习3D人物空间关系。2. 通过自主监督的方式，从不一致的2Dcue中理解3D空间关系，并使用pose canonicalization进行姿势标准化。3. 使用语义归一化来综合分类不同的交互方式。4. 提出了一种新的评价指标，用于评估3D人物空间关系的学习质量。更多细节请参考我们的项目页面：https://jellyheadandrew.github.io/projects/chorus
</details></li>
</ul>
<hr>
<h2 id="D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification"><a href="#D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification" class="headerlink" title="D4: Improving LLM Pretraining via Document De-Duplication and Diversification"></a>D4: Improving LLM Pretraining via Document De-Duplication and Diversification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12284">http://arxiv.org/abs/2308.12284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos</li>
<li>for: 这paper主要针对大型自然语言模型（LLM）的预训练和下游任务的性能提升。</li>
<li>methods: 这paper使用了预训练模型的嵌入来进行精心的数据选择，以提高预训练和下游任务的性能。</li>
<li>results: 这paper的实验结果表明，采用精心的数据选择可以提高LLM的预训练速度（20%的效率提升），并在16种NLP任务中提高下游任务的平均性能（最多2%的提升），特别是在6.7B模型 scales。此外，paper还表明，通过智能重复数据可以超过基eline训练，而随机重复数据则 perfoms worse than baseline training。<details>
<summary>Abstract</summary>
Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Simple-is-Better-and-Large-is-Not-Enough-Towards-Ensembling-of-Foundational-Language-Models"><a href="#Simple-is-Better-and-Large-is-Not-Enough-Towards-Ensembling-of-Foundational-Language-Models" class="headerlink" title="Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models"></a>Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12272">http://arxiv.org/abs/2308.12272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nancy Tyagi, Aidin Shiri, Surjodeep Sarkar, Abhishek Kumar Umrawal, Manas Gaur</li>
<li>for:  This paper aims to explore the potential of smaller foundational language models (FLMs) and their ensembling on benchmark and real-world datasets, and to investigate the influence of ensemble on the individualistic attention of FLMs.</li>
<li>methods:  The authors use three ensemble techniques: Shallow, Semi, and Deep, and introduce a knowledge-guided reinforcement learning approach in the Deep-Ensemble.</li>
<li>results:  The suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by a factor of many times using datasets that show the usefulness of NLP in sensitive fields, such as mental health.Here’s the summary in traditional Chinese characters:</li>
<li>for: 本研究旨在探讨小型基础语言模型 (FLMs) 的可能性和其ensemble在标准和实际数据集上的表现，并 investigate ensemble对FLMs的个性注意力的影响。</li>
<li>methods: 作者使用三种ensemble技术：浅层、半深和深，并引入知识导向资源学习approach。</li>
<li>results: 建议的深度ensembleBERT exceeds其大型变体BERTlarge，使用敏感领域中NLP的有用性的数据集，例如心理健康。<details>
<summary>Abstract</summary>
Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guided reinforcement learning approach. We discovered that the suggested Deep-Ensemble BERT outperforms its large variation i.e. BERTlarge, by a factor of many times using datasets that show the usefulness of NLP in sensitive fields, such as mental health.
</details>
<details>
<summary>摘要</summary>
基础语言模型（FLM）在自然语言处理（NLP）研究中进步了大量。当前研究人员在开发更大的FLM（如XLNet、T5）以实现语言表示、分类和生成上的上下文化化能力。虽然开发更大的FLM有了显著的优势，但也存在幻觉和预测不确定性的问题。基本上，更大的FLM都是基于小型FLM（如BERT）的基础上建立的，因此需要认可小型FLM的潜在能力，并通过集成来实现。在当前的研究中，我们对FLM和其集成的现实检查，并对标准和实际数据集上进行评估。我们假设 ensemble FLM 可以影响它们的个人注意力，并探索不同 FLM 之间的协作和合作的强度。我们使用 BERT 定义三种集成技术：{浅、半、深}，其中深度集成还使用了知识导向的强化学习方法。我们发现，我们提议的深度集成 BERT 在使用敏感领域中的 NLP 数据集上，比其大版本 BERTlarge 高得多。
</details></li>
</ul>
<hr>
<h2 id="Language-Reward-Modulation-for-Pretraining-Reinforcement-Learning"><a href="#Language-Reward-Modulation-for-Pretraining-Reinforcement-Learning" class="headerlink" title="Language Reward Modulation for Pretraining Reinforcement Learning"></a>Language Reward Modulation for Pretraining Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12270">http://arxiv.org/abs/2308.12270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ademiadeniji/lamp">https://github.com/ademiadeniji/lamp</a></li>
<li>paper_authors: Ademi Adeniji, Amber Xie, Carmelo Sferrazza, Younggyo Seo, Stephen James, Pieter Abbeel</li>
<li>For: 本研究考虑了使用学习奖励函数（LRF）来解决稀缺奖励学习（RL）任务，并取得了一些任务复杂度的进步。但我们提问是否今天的LRF适用于直接替换任务奖励。相反，我们提议利用LRF作为RL前期训练的能力。* Methods: 我们提出了$\textbf{LA}$nguage Reward $\textbf{M}$odulated $\textbf{P}$retraining（LAMP）方法，它利用预训练的视觉语言模型（VLM）来生成随机的语言指令和图像观察的对比准则，并用这些准则作为RL前期训练的预训练资源。* Results: 我们的LAMP方法可以在RLBench中使用 sample-efficient 的方式快速学习爬行任务。<details>
<summary>Abstract</summary>
Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\textbf{LA}$nguage Reward $\textbf{M}$odulated $\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench.
</details>
<details>
<summary>摘要</summary>
LAMP leverages a frozen, pretrained VLM to generate noisy, yet shaped exploration rewards by computing the contrastive alignment between a diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which departs from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench.
</details></li>
</ul>
<hr>
<h2 id="FECoM-A-Step-towards-Fine-Grained-Energy-Measurement-for-Deep-Learning"><a href="#FECoM-A-Step-towards-Fine-Grained-Energy-Measurement-for-Deep-Learning" class="headerlink" title="FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning"></a>FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12264">http://arxiv.org/abs/2308.12264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabhsingh Rajput, Tim Widmayer, Ziyuan Shang, Maria Kechagia, Federica Sarro, Tushar Sharma</li>
<li>for: 本研究旨在提高深度学习（DL）模型的能源消耗量，并且提出了一种精细化能源消耗量测量的框架（FECoM），以便帮助研究人员和开发者更好地了解DL系统的能源消耗量。</li>
<li>methods: FECoM使用静态实rumentation技术，考虑了多种因素，包括计算负荷和温度稳定性，以减少测量精细化能源消耗量的挑战。</li>
<li>results: 通过使用FECoM，我们对TensorFlow框架的能源消耗量进行了精细化测量，并研究了参数大小和执行时间对能源消耗量的影响。这些结果可以帮助我们更好地理解TensorFlow API的能源资源。此外，我们还讨论了设计和实现精细化能源消耗量测量工具的一些考虑因素和挑战。<details>
<summary>Abstract</summary>
With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, namely TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles. Furthermore, we elaborate on the considerations, issues, and challenges that one needs to consider while designing and implementing a fine-grained energy consumption measurement tool. We hope this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems.
</details>
<details>
<summary>摘要</summary>
In this paper, we introduce Fine-grained Energy Consumption Meter (FECoM), a framework for measuring DL energy consumption at a fine granularity. FECoM provides researchers and developers with a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at a fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability.We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles. Furthermore, we discuss the considerations, issues, and challenges that need to be considered when designing and implementing a fine-grained energy consumption measurement tool.We hope that this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems.
</details></li>
</ul>
<hr>
<h2 id="Multi-Objective-Optimization-for-Sparse-Deep-Neural-Network-Training"><a href="#Multi-Objective-Optimization-for-Sparse-Deep-Neural-Network-Training" class="headerlink" title="Multi-Objective Optimization for Sparse Deep Neural Network Training"></a>Multi-Objective Optimization for Sparse Deep Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12243">http://arxiv.org/abs/2308.12243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salomonhotegni/mdmtn">https://github.com/salomonhotegni/mdmtn</a></li>
<li>paper_authors: S. S. Hotegni, S. Peitz, M. Berkemeier</li>
<li>for: 这篇论文目的是为了训练深度学习网络（DNNs），并使其能够同时完成多个任务（Multi-Task Learning）。</li>
<li>methods: 这篇论文使用了一种 modificated Weighted Chebyshev scalarization 技术，将多个任务转换为一个单一的问题，并使用 Augmented Lagrangian 方法来解决。</li>
<li>results: 这篇论文的实验结果显示，这种方法可以在训练 DNNs 时，逐步简化网络结构，并且可以适应不同任务的适应率，无需对网络结构进行大量修改。<details>
<summary>Abstract</summary>
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economical and also ecological) sustainability issue of DNN models, with a particular focus on Deep Multi-Task models, which are typically designed with a very large number of weights to perform equally well on multiple tasks. Through experiments conducted on two Machine Learning datasets, we demonstrate the possibility of adaptively sparsifying the model during training without significantly impacting its performance, if we are willing to apply task-specific adaptations to the network weights. Code is available at https://github.com/salomonhotegni/MDMTN.
</details>
<details>
<summary>摘要</summary>
不同的冲突优化标准在深度学习场景中自然出现。这些标准可以用于不同的主任务（例如在多任务学习设置中），也可以用于主任务和次任务之间的冲突，如损失最小化和稀疏化。通常的方法是将这些标准简单地权衡，但这只能在凸Setting中有效。在这篇论文中，我们提出了一种多目标优化算法，使用修改后的Weighted ChebyshevScalarization来训练深度神经网络（DNNs）对多个任务进行训练。通过使用这种Scalarization技术，算法可以找到原始问题的所有优化解决方案，并将其减少到一个序列中的单个目标问题。这些简化后的问题然后可以使用 Augmented Lagrangian 方法解决，使用流行的优化技术such as Adam和Stochastic Gradient Descent，同时有效地处理约束。我们的工作旨在解决深度神经网络模型的（经济和生态）可持续性问题，尤其是深度多任务模型，这些模型通常具有很多权重，以便在多个任务上具有相同的性能。通过对两个机器学习数据集进行实验，我们示出了在训练过程中适应性减少模型的可能性，只要愿意在任务特定的网络权重上应用适应。代码可以在 https://github.com/salomonhotegni/MDMTN 上获取。
</details></li>
</ul>
<hr>
<h2 id="LLMRec-Benchmarking-Large-Language-Models-on-Recommendation-Task"><a href="#LLMRec-Benchmarking-Large-Language-Models-on-Recommendation-Task" class="headerlink" title="LLMRec: Benchmarking Large Language Models on Recommendation Task"></a>LLMRec: Benchmarking Large Language Models on Recommendation Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12241">http://arxiv.org/abs/2308.12241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/williamliujl/llmrec">https://github.com/williamliujl/llmrec</a></li>
<li>paper_authors: Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, Philip S. Yu<br>for:本研究旨在对大型自然语言模型（LLM）在推荐领域的应用进行 investigate。methods:本研究使用了多种常用的Off-the-shelf LLM，如ChatGPT、LLaMA、ChatGLM，对五种推荐任务进行了 benchmark，包括评分预测、sequential recommendation、直接推荐、解释生成和评论概要。此外，我们还 investigate了精度 fine-tuning 的效iveness以提高 LLM 的指令遵从能力。results:结果表明 LLM 在准确性基于任务中只 display moderate 的能力，但在可解释性基于任务中与现状方法相当。此外，我们还进行了质量评估，发现 LLM 可以真正理解提供的信息，并生成 clearer 和更合理的结果。<details>
<summary>Abstract</summary>
Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.
</details>
<details>
<summary>摘要</summary>
近些时候，大型语言模型（LLM）如ChatGPT的快速发展已经对自然语言处理（NLP）任务提供了显著改进。然而，LLM在推荐领域的应用还未得到了全面的探索。为了填补这一空白，我们提出了LLMRec，一个基于LLM的推荐系统，用于对不同的推荐任务进行比较。具体来说，我们对几种流行的准备好的LLM，如ChatGPT、LLaMA、ChatGLM进行了多种推荐任务的评估，包括评分预测、顺序推荐、直接推荐、解释生成和评论概要。此外，我们还 investigate了LLM的指导遵从能力是否可以通过监督微调来改进。 benchmark结果显示，LLM在精度基于任务中只示 moderate 的能力，但在可解释性基于任务中表现和当前领先方法相当。我们还进行了质量评估，以评估不同模型生成的内容质量，结果表明LLM可以真正理解提供的信息，并生成更清晰和合理的结果。我们希望这个 benchmark 能够激励研究人员更深入研究LLM在提高推荐性能方面的潜在力量。我们的代码、处理数据和 benchmark 结果可以在 <https://github.com/williamliujl/LLMRec> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-cardiovascular-risk-prediction-through-AI-enabled-calcium-omics"><a href="#Enhancing-cardiovascular-risk-prediction-through-AI-enabled-calcium-omics" class="headerlink" title="Enhancing cardiovascular risk prediction through AI-enabled calcium-omics"></a>Enhancing cardiovascular risk prediction through AI-enabled calcium-omics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12224">http://arxiv.org/abs/2308.12224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ammar Hoori, Sadeer Al-Kindi, Tao Hu, Yingnan Song, Hao Wu, Juhwan Lee, Nour Tashtish, Pingfu Fu, Robert Gilkeson, Sanjay Rajagopalan, David L. Wilson<br>for:The paper aims to determine if AI methods using detailed calcification features can improve the prediction of major adverse cardiovascular events (MACE).methods:The study uses a Cox model with elastic-net regularization on 2457 CT calcium score (CTCS) enriched for MACE events, and employs sampling techniques to enhance model training. The study also investigates Cox models with selected features to identify explainable high-risk characteristics.results:The proposed calcium-omics model with modified synthetic down sampling and up sampling gave higher C-index and two-year AUC compared to the Agatston score. The study found that numbers of calcifications, LAD mass, and diffusivity were important determinants of increased risk, and dense calcification was associated with lower risk. The calcium-omics model reclassified 63% of MACE patients to the high-risk group in a held-out test, with a categorical net-reclassification index of NRI&#x3D;0.153.Here’s the information in Simplified Chinese text:for: 这项研究的目的是判断使用细化calcification特征的人工智能方法是否可以提高主要不良心血管事件预测。methods: 这项研究使用了Cox模型与杂化正则化，对2457个CT calcification score（CTCS）中的MACE事件进行了增强模型训练。研究还调查了Cox模型中选择的特征，以确定高风险特征的解释。results: 提案的calcification-omics模型使用了修改的同步下采样和上采样，对80:20的训练&#x2F;测试集进行了评估。与Agatston分数相比，calcification-omics模型的C-index和两年AUC得分较高（80.5%&#x2F;71.6% vs 71.3%&#x2F;70.3%）。研究发现，calcification数量、LAD质量和扩散率（一种度量calcification的空间分布）是增加风险的重要因素，而高浓度calcification（&gt;1000HU）则与降低风险相关。calcification-omics模型在一个储存测试集中重新分类了63%的MACE患者为高风险组。<details>
<summary>Abstract</summary>
Background. Coronary artery calcium (CAC) is a powerful predictor of major adverse cardiovascular events (MACE). Traditional Agatston score simply sums the calcium, albeit in a non-linear way, leaving room for improved calcification assessments that will more fully capture the extent of disease.   Objective. To determine if AI methods using detailed calcification features (i.e., calcium-omics) can improve MACE prediction.   Methods. We investigated additional features of calcification including assessment of mass, volume, density, spatial distribution, territory, etc. We used a Cox model with elastic-net regularization on 2457 CT calcium score (CTCS) enriched for MACE events obtained from a large no-cost CLARIFY program (ClinicalTri-als.gov Identifier: NCT04075162). We employed sampling techniques to enhance model training. We also investigated Cox models with selected features to identify explainable high-risk characteristics.   Results. Our proposed calcium-omics model with modified synthetic down sampling and up sampling gave C-index (80.5%/71.6%) and two-year AUC (82.4%/74.8%) for (80:20, training/testing), respectively (sampling was applied to the training set only). Results compared favorably to Agatston which gave C-index (71.3%/70.3%) and AUC (71.8%/68.8%), respectively. Among calcium-omics features, numbers of calcifications, LAD mass, and diffusivity (a measure of spatial distribution) were important determinants of increased risk, with dense calcification (>1000HU) associated with lower risk. The calcium-omics model reclassified 63% of MACE patients to the high risk group in a held-out test. The categorical net-reclassification index was NRI=0.153.   Conclusions. AI analysis of coronary calcification can lead to improved results as compared to Agatston scoring. Our findings suggest the utility of calcium-omics in improved prediction of risk.
</details>
<details>
<summary>摘要</summary>
背景：肺动脉 calcification（CAC）是一个强大的预测主要冠状疾病事件（MACE）的预测因素。传统的阿加顿分数简单地总计calcification，却不是线性的，留下了更好的calcification评估方法来更全面地捕捉疾病的程度。目标：确定AI方法使用细节calcification特征（i.e., calcium-omics）可以提高MACE预测。方法：我们调查了calcification的更多特征，包括质量、体积、密度、空间分布、领域等。我们使用了一个Cox模型与栅格 regularization，对于2457个CT calcification score（CTCS）中的MACE事件进行了大规模的no-cost CLARIFY计划（ClinicalTrials.gov Identifier: NCT04075162）。我们使用了采样技术来增强模型训练。我们还使用了Cox模型选择特征来 Identify可解释的高风险特征。结果：我们的提议的calcification-omics模型与修改后的同步下采样和上采样给C-index（80.5%/71.6%）和两年AUC（82.4%/74.8%），分别在（80:20，训练/测试）。结果与阿加顿相比，给C-index（71.3%/70.3%）和AUC（71.8%/68.8%）。 Among calcification-omics features，numbers of calcifications、LAD mass和diffusivity（一种度量空间分布）是风险增加的重要决定因素，而dense calcification（>1000HU）与lower risk相关。calcification-omics模型在一个保留测试中重新分类了63%的MACE患者。NRI=0.153。结论：AI对肺动脉calcification的分析可以导致更好的结果，相比阿加顿分数。我们的发现表明calcification-omics的使用可以提高预测风险的准确性。
</details></li>
</ul>
<hr>
<h2 id="Critical-Learning-Periods-Emerge-Even-in-Deep-Linear-Networks"><a href="#Critical-Learning-Periods-Emerge-Even-in-Deep-Linear-Networks" class="headerlink" title="Critical Learning Periods Emerge Even in Deep Linear Networks"></a>Critical Learning Periods Emerge Even in Deep Linear Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12221">http://arxiv.org/abs/2308.12221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kleinman, Alessandro Achille, Stefano Soatto</li>
<li>for: 本研究探讨了深度学习网络中的批处理期，即在学习过程中，某些特定的批处理可以对后续学习产生深刻的影响。</li>
<li>methods: 本研究使用了深度线性网络模型，并通过分析和实验表明了批处理期的依存关系。</li>
<li>results: 研究发现，批处理期在深度网络中存在，并且与模型的深度和数据分布结构有关。此外，研究还发现了在多任务学习中，预训练的影响，并提出了一种基于竞争的解释模型。<details>
<summary>Abstract</summary>
Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show analytically and in simulations that the learning of features is tied to competition between sources. Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage. To the best of our knowledge, our work provides the first analytically tractable model that sheds light into why critical learning periods emerge in biological and artificial networks.
</details>
<details>
<summary>摘要</summary>
“重要学习期是在发育早期的一些时期，这些时期的暂时感知缺陷可能会导致永久性的行为和学习表征的改变。尽管生物和人工网络之间有很大差异，但critical periods仍然在两种系统中被观察到。这表明critical periods可能是学习的基本特征，而不是生物学意外现象。然而，critical periods在哪里emerge仍然是一个开放的问题，尤其是不知道critical periods在两种系统中是否受到特定的建筑或优化细节的影响。为了孤立关键因素，我们将注重深度Linear Network模型，并显示了这些网络在生物和人工网络中显示了大量的行为，同时可以进行分析处理。我们发现critical periods与模型的深度和数据分布结构有关，并且在分析和 simulations中表明了学习特征的吸引是与多源竞争相关。最后，我们扩展我们的分析到多任务学习，并显示了在某些任务上进行预训练可能会对新任务的传输性能产生负面影响，并且如何这种影响与任务之间的关系和预训练阶段的长度有关。到目前为止，我们的工作提供了第一个可分析的模型，可以解释critical learning periods在生物和人工网络中的出现。”
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning"><a href="#Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning" class="headerlink" title="Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning"></a>Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12219">http://arxiv.org/abs/2308.12219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yegcjs/diffusionllm">https://github.com/yegcjs/diffusionllm</a></li>
<li>paper_authors: Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu</li>
<li>for: 这篇论文旨在探讨 diffusion 语言模型是否可以解决通用语言任务，并证明可以通过扩大数据、大小和任务来使 diffusion 语言模型成为强大的语言学习模型。</li>
<li>methods: 作者首先通过大量数据的隐藏语言模型预训练获得知识，然后通过填充式适应来转化预训练的masked语言模型为 diffusion 语言模型，并在不同任务上进行任务特定的训练和指令特化训练来解锁其多样性。</li>
<li>results: 实验显示，随着 diffusion 语言模型的扩大，其表现在下游语言任务中逐渐提高，并且在不同任务上具有zero-shot和几少shot在场景学习能力，可以根据自然语言指令来解决许多未看过任务。<details>
<summary>Abstract</summary>
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning
</details>
<details>
<summary>摘要</summary>
最近的生成AI冲击浪潮得以归功于扩散概率模型的生成能力和大语言模型的可扩展性。 despite their potential, it remains unclear whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models with respect to data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining, thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning.Here's a word-for-word translation of the text into Simplified Chinese:最近的生成AI冲击浪潮得以归功于扩散概率模型的生成能力和大语言模型的可扩展性。despite their potential, it remains unclear whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models with respect to data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining, thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.AI_2023_08_24/" data-id="clmjn91j900110j88c8v880ri" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.CL_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.CL_2023_08_24/">cs.CL - 2023-08-24 19:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Text-Similarity-from-Image-Contents-using-Statistical-and-Semantic-Analysis-Techniques"><a href="#Text-Similarity-from-Image-Contents-using-Statistical-and-Semantic-Analysis-Techniques" class="headerlink" title="Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques"></a>Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12842">http://arxiv.org/abs/2308.12842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagar Kulkarni, Sharvari Govilkar, Dhiraj Amin</li>
<li>for: 这种论文主要targets the problem of plagiarism in image content, such as figures, graphs, and tables, and proposes a system to detect plagiarism in these contents.</li>
<li>methods: The proposed system uses a combination of statistical algorithms, including Jaccard and Cosine, and semantic algorithms, such as LSA, BERT, and WordNet, to detect plagiarism in image content.</li>
<li>results: The system outperformed in detecting efficient and accurate plagiarism in image content, demonstrating its effectiveness in addressing the challenge of plagiarism in this area.<details>
<summary>Abstract</summary>
Plagiarism detection is one of the most researched areas among the Natural Language Processing(NLP) community. A good plagiarism detection covers all the NLP methods including semantics, named entities, paraphrases etc. and produces detailed plagiarism reports. Detection of Cross Lingual Plagiarism requires deep knowledge of various advanced methods and algorithms to perform effective text similarity checking. Nowadays the plagiarists are also advancing themselves from hiding the identity from being catch in such offense. The plagiarists are bypassed from being detected with techniques like paraphrasing, synonym replacement, mismatching citations, translating one language to another. Image Content Plagiarism Detection (ICPD) has gained importance, utilizing advanced image content processing to identify instances of plagiarism to ensure the integrity of image content. The issue of plagiarism extends beyond textual content, as images such as figures, graphs, and tables also have the potential to be plagiarized. However, image content plagiarism detection remains an unaddressed challenge. Therefore, there is a critical need to develop methods and systems for detecting plagiarism in image content. In this paper, the system has been implemented to detect plagiarism form contents of Images such as Figures, Graphs, Tables etc. Along with statistical algorithms such as Jaccard and Cosine, introducing semantic algorithms such as LSA, BERT, WordNet outperformed in detecting efficient and accurate plagiarism.
</details>
<details>
<summary>摘要</summary>
“抄袭探测是自然语言处理（NLP）社区中最受欢迎的研究领域之一。一个好的抄袭探测系统应包括所有NLP方法，包括语意、名称实体、重复文本等，并生成详细的抄袭报告。跨语言抄袭探测需要深厚的多种高级方法和算法，以进行有效的文本相似性检查。现在，抄袭者也在不断地提高自己的隐身技巧，以避免被检测。抄袭者会使用技巧如重写、词汇替换、不一致的引用、翻译语言等。图像内容抄袭探测（ICPD）已经获得了重要性，通过进阶的图像内容处理技术来确保图像内容的完整性。但是，图像内容抄袭探测仍然是一个未解决的挑战。因此，有一个急需开发方法和系统来检测图像内容中的抄袭。在这篇文章中，我们已经实现了对图像内容中的内容进行抄袭探测，包括 figura、图表、グラフ等。我们还使用了统计算法如Jaccard和Cosine，以及语义算法如LSA、BERT和WordNet，它们在检测效率和准确性方面表现出色。”
</details></li>
</ul>
<hr>
<h2 id="Use-of-LLMs-for-Illicit-Purposes-Threats-Prevention-Measures-and-Vulnerabilities"><a href="#Use-of-LLMs-for-Illicit-Purposes-Threats-Prevention-Measures-and-Vulnerabilities" class="headerlink" title="Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities"></a>Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12833">http://arxiv.org/abs/2308.12833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Mozes, Xuanli He, Bennett Kleinberg, Lewis D. Griffin</li>
<li>for: 本研究旨在提高大语言模型（LLM）的安全性和安全性问题的认识，包括恶意使用、人工欺诈和生成恶意软件等问题。</li>
<li>methods: 本研究使用了现有的科学研究来识别和解决LLM所存在的威胁和漏洞。我们还提出了一个概念体系，描述了威胁、预防措施和预防措施的漏洞之间的关系。</li>
<li>results: 本研究希望通过提高开发者和实践者对LLM安全性问题的认识，提高LLM在安全性方面的可靠性和可靠性。<details>
<summary>Abstract</summary>
Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures. With our work, we hope to raise awareness of the limitations of LLMs in light of such security concerns, among both experienced developers and novel users of such technologies.
</details>
<details>
<summary>摘要</summary>
受到大语言模型（LLM）的快速发展和散布的影响，近期的许多研究都集中在LLM的安全和安全性问题上，特别是在涉及到可犯罪活动的情况下。据显示，LLM可以被滥用于诈骗、人身伪造和生成恶意软件等; 而其他作者则考虑了更一般的人工智能对齐问题。在这篇论文中，我们提供了现有的主要科学努力，以识别和解决由LLM引起的威胁和漏洞。我们提出了一种分类方案，描述了由LLM生成能力引起的威胁、防范措施和不完全防范措施所导致的漏洞之间的关系。我们希望通过这种工作，让开发者和实践者都意识到LLM的安全限制，以及相关的安全问题。
</details></li>
</ul>
<hr>
<h2 id="WavMark-Watermarking-for-Audio-Generation"><a href="#WavMark-Watermarking-for-Audio-Generation" class="headerlink" title="WavMark: Watermarking for Audio Generation"></a>WavMark: Watermarking for Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12770">http://arxiv.org/abs/2308.12770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei</li>
<li>for: 这篇论文旨在提出一种新的声音水印框架，以增强声音水印的鲁棒性和可靠性。</li>
<li>methods: 该框架使用了1秒钟的音频抽象，并在不可见的情况下编码了32位水印。它还可以组合多个水印段以实现更高的鲁棒性和容量。</li>
<li>results: 该框架在10-20秒的音频上实现了0.48%的比特错误率，比现有的水印工具减少了超过2800%的比特错误率。<details>
<summary>Abstract</summary>
Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. Utilizing 10 to 20-second audio as the host, our approach demonstrates an average Bit Error Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over 2800\% in BER compared to the state-of-the-art watermarking tool. See https://aka.ms/wavmark for demos of our work.
</details>
<details>
<summary>摘要</summary>
最近的零频讲话突破口已经使得可以通过几秒钟的录音来模仿说话者的声音，同时保持高度的真实感。然而，这项强大技术也带来了明显的风险，包括声音骗财和说话者模仿。不同于传统的仅仅依靠静止方法来检测合成数据，水印技术提供了一种积极和坚强的防御机制。这篇论文介绍了一种创新的音频水印框架，可以在1秒钟的音频截取中编码Up to 32位的水印，人类感知不到。这个水印具有强大的抗击攻击特性，可以作为合成声音的标识符，并且有广泛的应用前途在音频版权保护方面。此外，该框架具有高灵活性，可以将多个水印段组合以实现更高的坚强性和扩展性。使用10-20秒的音频作为主机，我们的方法在十种常见攻击中显示了平均的比特错误率（BER）为0.48%，相比之下，当前的水印工具的BER下降了超过2800%。请参考https://aka.ms/wavmark了解我们的工作。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion"><a href="#Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion" class="headerlink" title="Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion"></a>Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12734">http://arxiv.org/abs/2308.12734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan J. Bird, Ahmad Lotfi</li>
<li>for: 这个研究旨在探讨深度复制声音技术，以探索它们对于声音识别和伪造的潜在影响。</li>
<li>methods: 这个研究使用了Retrieval-based Voice Conversion技术生成了DEEP-VOICE数据集，包含了八位知名人士的真实声音，并将其转换为对方的声音。研究以binary分类问题的形式进行分析，使用了时间 audio 特征的 Statistical Analysis，发现真实声音和AI生成声音之间存在显著的不同分布。</li>
<li>results: 这个研究发现，使用Extreme Gradient Boosting模型可以实现平均分类精度为99.3%，并且可以在0.004毫秒运行，即一秒钟的声音。所有的数据都公开发布，以便未来的研究人员对于AI声音检测进行更多的研究。<details>
<summary>Abstract</summary>
There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learning models over 10-fold cross validation, it is found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection.
</details>
<details>
<summary>摘要</summary>
“有着增长的对话AI生成技术的含义，可以让语音变为别人的语音。这种技术可能会导致隐私泄露和误导，因此需要实时检测AI生成的语音。为解决这些问题，本研究中提出了DEEP-VOICE数据集，包括8名知名人士的真实语音和使用检索式语音转换后转换为别人的语音。这被视为一个二分类问题，即语音是真实的或者是AI生成的。通过统计Audio特征的时间分布，通过t检测发现了不同的分布。对于机器学习模型来源的标识，进行了参数优化。经过208个个人机器学习模型的10次横向分割训练，发现了使用极限梯度提升模型，可以在0.004毫秒内将语音分类为真实或AI生成，并且在1秒钟语音时，平均分类精度达99.3%。所有用于本研究的数据都公开发布，以便未来关于AI语音检测的研究。”
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-David-against-Goliath-Exploring-Instruction-Data-Generation-without-Using-Closed-Source-Models"><a href="#Harnessing-the-Power-of-David-against-Goliath-Exploring-Instruction-Data-Generation-without-Using-Closed-Source-Models" class="headerlink" title="Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models"></a>Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12711">http://arxiv.org/abs/2308.12711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Wang, Xinrui Wang, Juntao Li, Jinxiong Chang, Qishen Zhang, Zhongyi Liu, Guannan Zhang, Min Zhang</li>
<li>for: This paper aims to explore alternative methods for generating high-quality instruction data for training large language models, without relying on closed-source models.</li>
<li>methods: The authors investigate various existing instruction generation methods and integrate the most efficient variant with two novel strategies to enhance quality.</li>
<li>results: The generated instruction data outperforms Alpaca, a method reliant on closed-source models, as demonstrated by evaluation results from two benchmarks and the GPT-4 model.Here’s the text in Simplified Chinese:</li>
<li>for: 本研究目的是探讨不使用关闭源模型的方法生成高质量的指令数据，用于训练大语言模型。</li>
<li>methods: 作者们 investigate了多种现有的指令生成方法，并将最高效的变体与两种新策略相结合，以进一步提高质量。</li>
<li>results: 生成的指令数据超过了基于关闭源模型的Alpaca方法，如果从两个 benchmark 和 GPT-4 模型的评估结果来看。<details>
<summary>Abstract</summary>
Instruction tuning is instrumental in enabling Large Language Models~(LLMs) to follow user instructions to complete various open-domain tasks. The success of instruction tuning depends on the availability of high-quality instruction data. Owing to the exorbitant cost and substandard quality of human annotation, recent works have been deeply engaged in the exploration of the utilization of powerful closed-source models to generate instruction data automatically. However, these methods carry potential risks arising from the usage requirements of powerful closed-source models, which strictly forbid the utilization of their outputs to develop machine learning models. To deal with this problem, in this work, we explore alternative approaches to generate high-quality instruction data that do not rely on closed-source models. Our exploration includes an investigation of various existing instruction generation methods, culminating in the integration of the most efficient variant with two novel strategies to enhance the quality further. Evaluation results from two benchmarks and the GPT-4 model demonstrate the effectiveness of our generated instruction data, which can outperform Alpaca, a method reliant on closed-source models. We hope that more progress can be achieved in generating high-quality instruction data without using closed-source models.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的 instrucion tuning 是实现用户指令完成各种开放领域任务的关键因素。 instrucion tuning 的成功取决于高质量的 instrucion 数据的可用性。由于人工标注的成本高昂且质量不高，现有的研究专注于自动生成 instrucion 数据的方法。但这些方法存在受到强大关闭源模型的使用需求的风险。为了解决这个问题，这个工作寻找不靠强大关闭源模型的替代方法来生成高质量的 instrucion 数据。我们的探索包括评估多种现有的 instrucion 生成方法，并将最高效的variant与两个新的策略相互融合，以进一步提高 instrucion 数据的质量。实验结果显示，我们所生成的 instrucion 数据能够超越 Alpaca，一种基于关闭源模型的方法。我们希望这个领域能够获得更多的进步，以生成高质量的 instrucion 数据，不靠强大关闭源模型。
</details></li>
</ul>
<hr>
<h2 id="From-Chatter-to-Matter-Addressing-Critical-Steps-of-Emotion-Recognition-Learning-in-Task-oriented-Dialogue"><a href="#From-Chatter-to-Matter-Addressing-Critical-Steps-of-Emotion-Recognition-Learning-in-Task-oriented-Dialogue" class="headerlink" title="From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue"></a>From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12648">http://arxiv.org/abs/2308.12648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shutong Feng, Nurul Lubis, Benjamin Ruppik, Christian Geishauser, Michael Heck, Hsien-chin Lin, Carel van Niekerk, Renato Vukovic, Milica Gašić</li>
<li>for: 提高人类对话机器人的听众性能（Emotion Recognition in Conversations，ERC），特别是在任务导向对话（Task-Oriented Dialogues，ToDs）中。</li>
<li>methods: 将适合协议对话（Chit-Chat Dialogues）的ERC模型转化为任务导向的模型，通过三个关键方面：数据、特征和目标。首先，我们提出了两种增强罕见情感的方法。其次，我们使用对话状态作为辅助特征，以包含用户的目标信息。最后，我们采用多方面情感定义和多任务学习目标，以及一种情感距离权重损失函数。</li>
<li>results: 在Emowoz大规模数据集上，我们的框架可以提高许多适合协议对话的ERC模型的性能。此外，我们还进行了不同ToD数据集上的满意度预测研究，并与超参数比较，显示了我们的框架在各种场景中的可 reuse性。<details>
<summary>Abstract</summary>
Emotion recognition in conversations (ERC) is a crucial task for building human-like conversational agents. While substantial efforts have been devoted to ERC for chit-chat dialogues, the task-oriented counterpart is largely left unattended. Directly applying chit-chat ERC models to task-oriented dialogues (ToDs) results in suboptimal performance as these models overlook key features such as the correlation between emotions and task completion in ToDs. In this paper, we propose a framework that turns a chit-chat ERC model into a task-oriented one, addressing three critical aspects: data, features and objective. First, we devise two ways of augmenting rare emotions to improve ERC performance. Second, we use dialogue states as auxiliary features to incorporate key information from the goal of the user. Lastly, we leverage a multi-aspect emotion definition in ToDs to devise a multi-task learning objective and a novel emotion-distance weighted loss function. Our framework yields significant improvements for a range of chit-chat ERC models on EmoWOZ, a large-scale dataset for user emotion in ToDs. We further investigate the generalisability of the best resulting model to predict user satisfaction in different ToD datasets. A comparison with supervised baselines shows a strong zero-shot capability, highlighting the potential usage of our framework in wider scenarios.
</details>
<details>
<summary>摘要</summary>
情感认知在对话中（ERC）是创建人类化对话代理的关键任务。虽然有大量努力投入到了ERC的普通对话（Chit-chat）中，但相对的任务导向对话（ToD）尚未得到了足够的注意。直接将Chit-chat ERC模型应用到ToD中会导致性能下降，因为这些模型忽略了对话完成任务的关键特征。在这篇论文中，我们提出了一个框架，把Chit-chat ERC模型转换成任务导向的模型，解决了三个关键方面：数据、特征和目标。首先，我们提出了两种增强罕见情感的方法，以提高ERC性能。其次，我们使用对话状态作为助记特征，以包含用户的目标信息。最后，我们采用了多方面情感定义和多任务学习目标，并提出了一种新的情感距离权重损失函数。我们的框架在EmoWOZ大规模数据集上实现了显著的改进，并且我们进一步调查了最佳模型在不同的ToD数据集中预测用户满意度的能力。与超级vised基准相比，我们的模型在零上shot情况下具有强大的能力， highlighting the potential usage of our framework in wider scenarios.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Method-of-Measuring-Linguistic-Productivity"><a href="#Probabilistic-Method-of-Measuring-Linguistic-Productivity" class="headerlink" title="Probabilistic Method of Measuring Linguistic Productivity"></a>Probabilistic Method of Measuring Linguistic Productivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12643">http://arxiv.org/abs/2308.12643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergei Monakhov</li>
<li>for: 这个论文旨在提出一种新的语言产率测量方法，以评估词根在拓展新词汇时的能力，而不是直接依赖于token频率。</li>
<li>methods: 该方法认为语言产率可以视为词根与随机基本单词的概率相互结合。这些优点包括：首先，token频率不会直接影响产率测量；其次，我们不仅是计数已知词类具有词根，而是通过模拟词类的构造并检查它们是否存在于词库中来评估词根的产率。最后，基于词库和随机设计，新词和旧词都有平等的机会被选择。</li>
<li>results: 在英语和俄语数据上测试了该算法，结果显示，语言产率与词类数量和token频率之间存在一定的关系。具体来说，语言产率首先增加高频项目，然后才增加低频项目。<details>
<summary>Abstract</summary>
In this paper I propose a new way of measuring linguistic productivity that objectively assesses the ability of an affix to be used to coin new complex words and, unlike other popular measures, is not directly dependent upon token frequency. Specifically, I suggest that linguistic productivity may be viewed as the probability of an affix to combine with a random base. The advantages of this approach include the following. First, token frequency does not dominate the productivity measure but naturally influences the sampling of bases. Second, we are not just counting attested word types with an affix but rather simulating the construction of these types and then checking whether they are attested in the corpus. Third, a corpus-based approach and randomised design assure that true neologisms and words coined long ago have equal chances to be selected. The proposed algorithm is evaluated both on English and Russian data. The obtained results provide some valuable insights into the relation of linguistic productivity to the number of types and tokens. It looks like burgeoning linguistic productivity manifests itself in an increasing number of types. However, this process unfolds in two stages: first comes the increase in high-frequency items, and only then follows the increase in low-frequency items.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我提出了一种新的语言产率测量方法，该方法对象ively评估一个词缀的使用能力，并不直接受到各种各样的token频率的影响。 Specifically, 我建议将语言产率视为一个词缀与随机基础的概率。这些方法的优点包括以下几点：首先，token频率不会控制产率测量，而是自然地影响采样的基础。其次，我们不仅是统计已知的单词类型，而是通过模拟这些类型的构建，然后检查它们是否存在于词库中。最后，基于词库和随机设计，所有新词和昔日的词都有平等的机会被选择。我们使用的算法在英语和俄语数据上进行了评估，得到的结果提供了一些有价值的发现，关于语言产率与单元和token之间的关系。看来，语言产率的增长 manifested itself in an increasing number of types，但是这个过程发展在两个阶段：首先是高频项的增长，然后才是低频项的增长。
</details></li>
</ul>
<hr>
<h2 id="PromptMRG-Diagnosis-Driven-Prompts-for-Medical-Report-Generation"><a href="#PromptMRG-Diagnosis-Driven-Prompts-for-Medical-Report-Generation" class="headerlink" title="PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation"></a>PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12604">http://arxiv.org/abs/2308.12604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haibo Jin, Haoxuan Che, Yi Lin, Hao Chen<br>for:The paper aims to improve the accuracy of automatic medical report generation (MRG) by proposing a novel framework called PromptMRG.methods:PromptMRG uses an encoder-decoder architecture with an extra disease classification branch, and incorporates cross-modal feature enhancement and adaptive logit-adjusted loss to address the challenges of disease imbalance and precise clinical understanding.results:The proposed method achieves state-of-the-art clinical efficacy performance on two MRG benchmarks, demonstrating its effectiveness in improving the accuracy of MRG.<details>
<summary>Abstract</summary>
Automatic medical report generation (MRG) is of great research value as it has the potential to relieve radiologists from the heavy burden of report writing. Despite recent advancements, accurate MRG remains challenging due to the need for precise clinical understanding and the identification of clinical findings. Moreover, the imbalanced distribution of diseases makes the challenge even more pronounced, as rare diseases are underrepresented in training data, making their diagnostic performance unreliable. To address these challenges, we propose diagnosis-driven prompts for medical report generation (PromptMRG), a novel framework that aims to improve the diagnostic accuracy of MRG with the guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on encoder-decoder architecture with an extra disease classification branch. When generating reports, the diagnostic results from the classification branch are converted into token prompts to explicitly guide the generation process. To further improve the diagnostic accuracy, we design cross-modal feature enhancement, which retrieves similar reports from the database to assist the diagnosis of a query image by leveraging the knowledge from a pre-trained CLIP. Moreover, the disease imbalanced issue is addressed by applying an adaptive logit-adjusted loss to the classification branch based on the individual learning status of each disease, which overcomes the barrier of text decoder's inability to manipulate disease distributions. Experiments on two MRG benchmarks show the effectiveness of the proposed method, where it obtains state-of-the-art clinical efficacy performance on both datasets.
</details>
<details>
<summary>摘要</summary>
自动医疗报告生成（MRG）具有很大的研究价值，因为它有可能减轻医生对报告写作的重荷。尽管最近有所进步，但准确的MRG仍然是一项挑战，因为需要精准的临床理解和病理发现的识别。此外，疾病的分布不均，使得报告生成的性能变得更加困难，因为罕见疾病在训练数据中的表现不充分，导致报告生成的准确性受到影响。为 Address these challenges, we propose 诊断驱动的报告生成PromptMRG，一种新的框架，用于提高MRG的诊断准确性，通过诊断意识的指导来Explicitly guide the generation process。具体来说，PromptMRG采用encoder-decoder架构，并添加了疾病分类分支。当生成报告时，从分支中获取的诊断结果会被转换为token提示，以直接导引生成过程。为了进一步提高诊断准确性，我们设计了跨模态特征增强，该方法可以通过在数据库中检索相似报告，帮助诊断查询图像的疾病诊断，并利用预训练的CLIP来增强报告生成的准确性。此外，我们还解决了疾病分布不均的问题，通过应用适应的逻辑调整损失，根据每种疾病的学习状态来调整分支的损失。实验结果表明，提案的方法在两个MRG标准测试集上表现出色，在两个 dataset上都达到了状态的诊断效果。
</details></li>
</ul>
<hr>
<h2 id="A-Small-and-Fast-BERT-for-Chinese-Medical-Punctuation-Restoration"><a href="#A-Small-and-Fast-BERT-for-Chinese-Medical-Punctuation-Restoration" class="headerlink" title="A Small and Fast BERT for Chinese Medical Punctuation Restoration"></a>A Small and Fast BERT for Chinese Medical Punctuation Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12568">http://arxiv.org/abs/2308.12568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongtao Ling, Chen Liao, Zhipeng Yu, Lei Chen, Shilei Huang, Yi Liu</li>
<li>for: 提高Automatic Speech Recognition（ASR）输出文本的精度和可读性，使得医疗报告能够更加precise和可理解。</li>
<li>methods: 基于’预训练和精度调整’ парадигм，提出了一种快速和轻量级的预训练模型，并通过练习对比学习和一种新的辅助预训练任务（括号标记预测）来适应医疗报告的括号 restauration。</li>
<li>results: 我们的实验表明，我们的模型可以达到95%的性能水平，而且与state-of-the-art中的Chinese RoBERTa模型相比，其模型体积只占10%。<details>
<summary>Abstract</summary>
In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.
</details>
<details>
<summary>摘要</summary>
在临床词汇录制中，无法显式标点符号的utterances可能会导致译音报告的歧义。为了提供准确可理解的临床报告，自动标点 restauration 是必要的。在实践场景中，我们提议一种快速轻量级的预训练模型，基于 '预训练和精度调整' 模型。在这种情况下，我们通过将预训练模型进行精度调整，并在新的辅助预训练任务（标点符号预测）中进行精度调整。我们的实验表明，我们的模型可以达到 95% 的性能，而且与 state-of-the-art 中文 RoBERTa 模型相比，只有 10% 的模型大小。
</details></li>
</ul>
<hr>
<h2 id="CARE-Co-Attention-Network-for-Joint-Entity-and-Relation-Extraction"><a href="#CARE-Co-Attention-Network-for-Joint-Entity-and-Relation-Extraction" class="headerlink" title="CARE: Co-Attention Network for Joint Entity and Relation Extraction"></a>CARE: Co-Attention Network for Joint Entity and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12531">http://arxiv.org/abs/2308.12531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjun Kong, Yamei Xia</li>
<li>for: 本文旨在提高 JOINT 信息抽取的性能，即同时提取实体和关系信息。</li>
<li>methods: 该方法使用 Co-Attention 网络，分别学习实体和关系信息的表示，以避免特征混淆。主要 componenet 是两个任务之间的协作模块，使模型可以利用实体信息来预测关系，并 vice versa。</li>
<li>results: EXTENSIVE 实验表明，提出的模型在三个 JOINT 实体-关系抽取 benchmark 数据集（NYT、WebNLG 和 SciERC）上表现出色，超过现有基eline模型。<details>
<summary>Abstract</summary>
Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. Most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between two subtasks. In this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach involves learning separate representations for each subtask, aiming to avoid feature overlap. At the core of our approach is the co-attention module that captures two-way interaction between two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Extensive experiments on three joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC) show that our proposed model achieves superior performance, surpassing existing baseline models.
</details>
<details>
<summary>摘要</summary>
共同实体和关系抽取是信息抽取的基本任务，包括两个子任务：命名实体识别和关系抽取。现有的大多数共同抽取方法受到特征混乱或两个子任务之间不足的互动的问题。在这种情况下，我们提出了一种共同注意力网络 для共同实体和关系抽取（CARE）。我们的方法是学习每个子任务的独立表示，以避免特征重叠。我们的核心方法是两个子任务之间的共同注意力模块，允许模型利用实体信息来预测关系，并 vice versa，从而促进互助。我们在三个共同实体-关系抽取 benchmark 数据集（NYT、WebNLG 和 SciERC）进行了广泛的实验，结果显示，我们提出的模型在比较存在的基准模型上表现出色，超越了现有的基准模型。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-as-Autonomous-Decision-Maker"><a href="#Large-Language-Model-as-Autonomous-Decision-Maker" class="headerlink" title="Large Language Model as Autonomous Decision Maker"></a>Large Language Model as Autonomous Decision Maker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12519">http://arxiv.org/abs/2308.12519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yining Ye, Xin Cong, Yujia Qin, Yankai Lin, Zhiyuan Liu, Maosong Sun</li>
<li>For: This paper aims to enable large language models (LLMs) to make autonomous decisions by endowing them with self-judgment ability, allowing them to explore and judge decision steps based on their values and utilities.* Methods: The proposed approach, called JuDec, uses an Elo-based Self-Judgment Mechanism to assign Elo scores to decision steps and guide the decision-searching process toward the optimal solution.* Results: Experimental results on the ToolBench dataset show that JuDec achieves over 10% improvement in Pass Rate on diverse tasks, offering higher-quality solutions and reducing costs (ChatGPT API calls), demonstrating its effectiveness and efficiency.Here is the summary in Traditional Chinese:* For: 这篇论文目的是将大型语言模型（LLMs）变成自主的决策者，通过将自己的判断能力授与 LLMs。* Methods: 提案的方法称为 JuDec，采用 Elo 分数自我评价机制，将 Elo 分数 assign 给决策步骤，以judge 其值和利益via 对两个解决方案的对比，导引决策搜寻过程向优化解决方案。* Results: 实验结果显示，JuDec 在 ToolBench 数据集上超过 10% 的提升率，在多个任务上表现出色，提供更高质量的解决方案，降低 ChatGPT API 调用成本，强调其效率和有效性。<details>
<summary>Abstract</summary>
While large language models (LLMs) exhibit impressive language understanding and in-context learning abilities, their decision-making ability still heavily relies on the guidance of task-specific expert knowledge when solving real-world tasks. To unleash the potential of LLMs as autonomous decision makers, this paper presents an approach JuDec to endow LLMs with the self-judgment ability, enabling LLMs to achieve autonomous judgment and exploration for decision making. Specifically, in JuDec, Elo-based Self-Judgment Mechanism is designed to assign Elo scores to decision steps to judge their values and utilities via pairwise comparisons between two solutions and then guide the decision-searching process toward the optimal solution accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's superiority over baselines, achieving over 10% improvement in Pass Rate on diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT API calls), highlighting its effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有吸引人的语言理解和Contextual learning能力，但它们的决策能力仍然受到专业知识的导引。为了让 LLM 成为独立决策者，这篇论文提出了 JuDec approach，旨在赋予 LLM 自我评价能力，使其能够达到自主评估和探索决策。具体来说，JuDec 使用 Elo 分数机制来评估决策步骤的价值和用于导引决策搜索过程。实验结果表明 JuDec 在 ToolBench 数据集上表现出优于基eline，实现了多达 10% 的提升率，并且可以提供更高质量的解决方案，降低 ChatGPT API 调用成本， highlighting 其效率和可行性。
</details></li>
</ul>
<hr>
<h2 id="MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario"><a href="#MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario" class="headerlink" title="MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario"></a>MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12490">http://arxiv.org/abs/2308.12490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Zhou Yu, Julia Hirschberg</li>
<li>for: 这个研究旨在开发一种多任务语音发音评估系统，以提供更加精准和全面的发音技巧评估。</li>
<li>methods: 这个系统使用多任务学习方法，包括卷积神经网络和长期循环神经网络，以实现在关闭和开放响应场景下的发音评估。</li>
<li>results: 实验结果表明，这个系统在关闭响应场景下的性能相对较高，而且在开放响应场景下的性能更加稳定。<details>
<summary>Abstract</summary>
The design of automatic speech pronunciation assessment can be categorized into closed and open response scenarios, each with strengths and limitations. A system with the ability to function in both scenarios can cater to diverse learning needs and provide a more precise and holistic assessment of pronunciation skills. In this study, we propose a Multi-task Pronunciation Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based systems in that it has simpler format requirements and better compatibility with other neural network models. Compared with previous open response systems, MultiPA provides a wider range of evaluations, encompassing assessments at both the sentence and word-level. Our experimental results show that MultiPA achieves comparable performance when working in closed response scenarios and maintains more robust performance when directly used for open responses.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="American-Stories-A-Large-Scale-Structured-Text-Dataset-of-Historical-U-S-Newspapers"><a href="#American-Stories-A-Large-Scale-Structured-Text-Dataset-of-Historical-U-S-Newspapers" class="headerlink" title="American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers"></a>American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12477">http://arxiv.org/abs/2308.12477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dell-research-harvard/americanstories">https://github.com/dell-research-harvard/americanstories</a></li>
<li>paper_authors: Melissa Dell, Jacob Carlson, Tom Bryan, Emily Silcock, Abhishek Arora, Zejiang Shen, Luca D’Amico-Wong, Quan Le, Pablo Querubin, Leander Heldring</li>
<li>for: The paper is written to extract full article texts from newspaper images in the Library of Congress’s public domain Chronicling America collection, with the goal of providing high-quality data for pre-training a large language model and improving historical information accessibility.</li>
<li>methods: The paper develops a novel, deep learning pipeline for extracting full article texts from newspaper images, including layout detection, legibility classification, custom OCR, and association of article texts spanning multiple bounding boxes. The pipeline is designed for mobile phones to achieve high scalability.</li>
<li>results: The resulting American Stories dataset provides high-quality, structured article texts that can be used for pre-training a large language model, topic classification, detection of reproduced content, and news story clustering. The dataset also facilitates innovation in multimodal layout analysis models and other multimodal applications.<details>
<summary>Abstract</summary>
Existing full text datasets of U.S. public domain newspapers do not recognize the often complex layouts of newspaper scans, and as a result the digitized content scrambles texts from articles, headlines, captions, advertisements, and other layout regions. OCR quality can also be low. This study develops a novel, deep learning pipeline for extracting full article texts from newspaper images and applies it to the nearly 20 million scans in Library of Congress's public domain Chronicling America collection. The pipeline includes layout detection, legibility classification, custom OCR, and association of article texts spanning multiple bounding boxes. To achieve high scalability, it is built with efficient architectures designed for mobile phones. The resulting American Stories dataset provides high quality data that could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge. The dataset could also be added to the external database of a retrieval-augmented language model to make historical information - ranging from interpretations of political events to minutiae about the lives of people's ancestors - more widely accessible. Furthermore, structured article texts facilitate using transformer-based methods for popular social science applications like topic classification, detection of reproduced content, and news story clustering. Finally, American Stories provides a massive silver quality dataset for innovating multimodal layout analysis models and other multimodal applications.
</details>
<details>
<summary>摘要</summary>
现有的美国公共领域报纸全文数据集不能识别报纸扫描图像中的复杂布局，因此扫描的内容会混乱，包括文章、标题、标签、广告和其他布局区域。此研究开发了一个新的深度学习管道，用于从报纸图像中提取全文，并应用到美国国会图书馆的公共领域Chronicling America收藏中的 nearly 20 万扫描。该管道包括布局检测、可读性分类、自定义 OCR 和文章тексты横跨多个 bounding box 的关联。为实现高可扩展性，它采用了高效的建筑设计 для移动电话。结果的美国故事数据集提供了高质量的数据，可以用于预训练大型自然语言模型，以达到更好地理解历史英语和历史世界知识。此数据集还可以添加到一个外部数据库中，以便通过检索增强的语言模型来访问历史信息，从 interpretations of political events 到人们祖先的生活细节。此外，结构化的文章文本可以使用 transformer 类型的方法进行流行的社会科学应用，如新闻故事归一化、检测复制内容和主题分类。最后，美国故事提供了一个庞大的高质量银屑数据集，用于创新多模态布局分析模型和其他多模态应用。
</details></li>
</ul>
<hr>
<h2 id="Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature"><a href="#Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature" class="headerlink" title="Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature"></a>Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12420">http://arxiv.org/abs/2308.12420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walter Hernandez, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, Jiahua Xu</li>
<li>for: 本研究旨在提供一种机器学习驱动的系统atic literature review方法，用于 investigate Distributed Ledger Technologies (DLTs) 的多种组成部分。</li>
<li>methods: 研究采用了 transformer-based 语言模型，使用自己标注的数据集进行Named Entity Recognition (NER) 任务的 fine-tuning，并使用 Temporal Graph Analysis (TGA) 进行文献综述。</li>
<li>results: 研究发现了一个包含 505 个关键论文的核心论文集，这些论文具有关于 DLT 的 Environmental, Sustainability, and Governance (ESG) 方面的内容。同时，研究还提供了一个包含 54,808 个名实体的 NER 数据集，可供 DLT 和 ESG-相关的探索。<details>
<summary>Abstract</summary>
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our contributions are a methodology to conduct a machine learning-driven systematic literature review in the DLT field, placing a special emphasis on ESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed of 54,808 named entities, designed for DLT and ESG-related explorations.
</details>
<details>
<summary>摘要</summary>
分布式笔记技术（DLT）在发展中，需要全面的掌握其多种组成部分。然而，一篇系统性的文献评议，强调环境、可持续发展和管理（ESG）方面的分析，仍然缺失。为了填补这一空白，我们选择了107个种子论文，建立了63,083个参考文献的公共网络，并从中缩放到24,539篇文献进行分析。然后，我们将46篇论文中的名称实体标注为12个顶级类别，根据已有的技术分类标准，并将DLT的ESG元素细化。通过使用基于转换器的自然语言模型，我们精细了一个预训练的语言模型，以进行名称实体识别（NER）任务。我们使用我们精细化的语言模型，对文献库进行筛选，得到505份关键论文，可以进行基于名称实体和时间图分析，对DLT在ESG方面的进化。我们的贡献包括在DLT领域进行机器学习驱动的系统性文献评议方法，以及一个特有的NER数据集，包含54,808个名称实体，适用于DLT和ESG相关的探索。
</details></li>
</ul>
<hr>
<h2 id="Toward-American-Sign-Language-Processing-in-the-Real-World-Data-Tasks-and-Methods"><a href="#Toward-American-Sign-Language-Processing-in-the-Real-World-Data-Tasks-and-Methods" class="headerlink" title="Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods"></a>Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12419">http://arxiv.org/abs/2308.12419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bowen Shi<br>for: 本论文的目的是研究自然环境中的自动手语处理，使用来自互联网的签语视频。methods: 本论文使用了新的大规模 ASL  datasets，以及一些新的任务和方法。其中大部分章节都关注了手语中的指文字识别，这是手语中重要的一部分，尚未受到过去的研究。results: 本论文提出了一种基于迭代注意力的 end-to-end 方法，可以从 raw video 直接识别指文字。此外，使用 Conformer 网络同时模型手势和 lip  mouthing 可以达到人类水平的性能。此外，本论文还提出了一些用于实际应用程序的两个任务：指文字检测和搜索。<details>
<summary>Abstract</summary>
Sign language, which conveys meaning through gestures, is the chief means of communication among deaf people. Recognizing sign language in natural settings presents significant challenges due to factors such as lighting, background clutter, and variations in signer characteristics. In this thesis, I study automatic sign language processing in the wild, using signing videos collected from the Internet. This thesis contributes new datasets, tasks, and methods. Most chapters of this thesis address tasks related to fingerspelling, an important component of sign language and yet has not been studied widely by prior work. I present three new large-scale ASL datasets in the wild: ChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and ChicagoFSWild+, I address fingerspelling recognition, which consists of transcribing fingerspelling sequences into text. I propose an end-to-end approach based on iterative attention that allows recognition from a raw video without explicit hand detection. I further show that using a Conformer-based network jointly modeling handshape and mouthing can bring performance close to that of humans. Next, I propose two tasks for building real-world fingerspelling-based applications: fingerspelling detection and search. For fingerspelling detection, I introduce a suite of evaluation metrics and a new detection model via multi-task training. To address the problem of searching for fingerspelled keywords in raw sign language videos, we propose a novel method that jointly localizes and matches fingerspelling segments to text. Finally, I will describe a benchmark for large-vocabulary open-domain sign language translation based on OpenASL. To address the challenges of sign language translation in realistic settings, we propose a set of techniques including sign search as a pretext task for pre-training and fusion of mouthing and handshape features.
</details>
<details>
<summary>摘要</summary>
sign language，通过手势表达意义，是聋人之主要沟通方式。在自然环境中识别手语具有许多因素的挑战，如照明、背景干扰和手语表达者的变化。在这个论文中，我研究了在野外自动处理手语，使用互联网上收集的手语视频。这个论文的贡献包括新的数据集、任务和方法。大多数本论文的章节关注手语 fingerspelling，尚未得到了前期研究的广泛关注。我提供了三个大规模的ASL数据集在野外：ChicagoFSWild、ChicagoFSWild+和OpenASL。使用ChicagoFSWild和ChicagoFSWild+，我解决了手语 fingerspelling 识别问题，即将手语 fingerspelling 序列转换成文本。我提出了一种综合注意力的端到端方法，可以从原始视频中识别手语 fingerspelling 无需显式手势检测。此外，我还证明了使用基于 Conformer 网络同时模型手势和嘴形可以达到人类水平。接下来，我提出了两个用于实际应用程序开发的任务：手语 fingerspelling 检测和搜索。为手语 fingerspelling 检测，我引入了一系列评估指标和一种新的检测模型，通过多任务训练来实现。为了在原始手语视频中搜索手语 fingerspelling 关键字，我们提出了一种新的方法，即同时地Localize和匹配手语 fingerspelling 段落到文本。最后，我将介绍一个基于 OpenASL 的大词汇开放语言翻译 benchmark，用于Addressing the challenges of sign language translation in realistic settings, we propose a set of techniques including sign search as a pretext task for pre-training and fusion of mouthing and handshape features.
</details></li>
</ul>
<hr>
<h2 id="Vision-Transformer-Adapters-for-Generalizable-Multitask-Learning"><a href="#Vision-Transformer-Adapters-for-Generalizable-Multitask-Learning" class="headerlink" title="Vision Transformer Adapters for Generalizable Multitask Learning"></a>Vision Transformer Adapters for Generalizable Multitask Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12372">http://arxiv.org/abs/2308.12372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IVRL/VTAGML">https://github.com/IVRL/VTAGML</a></li>
<li>paper_authors: Deblina Bhattacharjee, Sabine Süsstrunk, Mathieu Salzmann</li>
<li>for: 这篇论文旨在提出一种基于视Transformer的多任务适应器，可以将任务相似性学习到新任务和领域中，无需重新训练或微调。</li>
<li>methods: 该方法基于视Transformer的底层结构，并将多个稠密视图任务集成到一起，通过一种任务相似性学习机制来学习通用任务相似性。</li>
<li>results: 作者表明，对于多个稠密视图任务，该方法可以在参数效率的情况下同时解决多个任务，并且在零shot任务转移、无监督领域适应和不需要微调到新领域中达到更高的性能。<details>
<summary>Abstract</summary>
We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains. Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive. In contrast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generalization without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-based ones. Our project page is at \url{https://ivrl.github.io/VTAGML}.
</details>
<details>
<summary>摘要</summary>
我们介绍了首个多任务视觉变换器适应器，这些适应器可以学习通用的任务相似性，并可以应用于新任务和领域。它们integrated into an off-the-shelf vision transformer backbone，可以同时解决多个密集视觉任务，而不需要 Parametrically expensive。与现有的多任务变换器不同，我们不需要在新任务或领域添加时进行重新训练或微调。我们引入了任务适应的注意力机制，该机制 combining gradient-based task similarities with attention-based ones。学习的任务相似性可以通过以下设置进行推广：零学习任务转移、不监督领域适应和无需微调来新领域的普适化。我们 demonstrably that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-based ones。我们的项目页面可以在 \url{https://ivrl.github.io/VTAGML} 中找到。
</details></li>
</ul>
<hr>
<h2 id="Prompt2Model-Generating-Deployable-Models-from-Natural-Language-Instructions"><a href="#Prompt2Model-Generating-Deployable-Models-from-Natural-Language-Instructions" class="headerlink" title="Prompt2Model: Generating Deployable Models from Natural Language Instructions"></a>Prompt2Model: Generating Deployable Models from Natural Language Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12261">http://arxiv.org/abs/2308.12261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neulab/prompt2model">https://github.com/neulab/prompt2model</a></li>
<li>paper_authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig</li>
<li>for: 这 paper 是为了探讨如何使用 prompt 来训练特殊用途 NLP 模型，以提高模型的性能和可用性。</li>
<li>methods: 这 paper 使用了一种多步骤的方法，包括数据 retrieve 和 pretrained 模型，数据生成 using LLMs，以及supervised fine-tuning 这些获取和生成的数据。</li>
<li>results: 在三个任务上，这 paper 展示了Prompt2Model 可以使用同样的几个示例prompt来训练模型，并取得比 gpt-3.5-turbo 强的平均提升20%，而且模型的大小可以减少到700倍。此外，这 paper 还表明这些数据可以用于获取可靠的模型性能估计，帮助模型开发者在部署前评估模型可靠性。<details>
<summary>Abstract</summary>
Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.
</details>
<details>
<summary>摘要</summary>
Prompt2Model 通过多个步骤来实现这一目标：首先，检索现有的数据集和预训练模型；其次，使用 LLM 生成新的数据集；最后，使用这些检索和生成的数据集进行supervised fine-tuning。在三个任务上，我们证明了，给定相同的几个示例提示，Prompt2Model 可以训练比 gpt-3.5-turbo 强的模型，而且模型的大小可以减少到 700 倍。此外，我们还示出了这些数据可以用于获得可靠的模型性能估计，从而帮助模型开发者在部署之前评估模型可靠性。Prompt2Model 已经开源在 GitHub 上，可以在 <https://github.com/neulab/prompt2model> 中下载。
</details></li>
</ul>
<hr>
<h2 id="How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models"><a href="#How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models" class="headerlink" title="How to Protect Copyright Data in Optimization of Large Language Models?"></a>How to Protect Copyright Data in Optimization of Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12247">http://arxiv.org/abs/2308.12247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy Chu, Zhao Song, Chiwun Yang</li>
<li>for: 本研究旨在防止大语言模型（LLMs）生成版权数据。</li>
<li>methods: 本研究使用了一种新的方法，即视为softmax回归问题进行大语言模型训练和优化。</li>
<li>results: 研究表明，通过这种方法可以有效避免生成版权数据，从而实现了训练大语言模型的理论基础。<details>
<summary>Abstract</summary>
Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和生成AI在计算机研究和应用中扮演了transformative的角色。但是，有 controvery arose as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function. 在这篇论文中，我们显示了大型语言模型的训练和优化可以被看作softmax regression问题。我们然后建立了一种有效地进行softmax regression的方法，以避免生成版权 Daten。这个方法可以帮助train大型语言模型，以避免生成版权 Daten。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.CL_2023_08_24/" data-id="clmjn91jv002m0j88dlaqbbdf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.CV_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.CV_2023_08_24/">cs.CV - 2023-08-24 21:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="VNI-Net-Vector-Neurons-based-Rotation-Invariant-Descriptor-for-LiDAR-Place-Recognition"><a href="#VNI-Net-Vector-Neurons-based-Rotation-Invariant-Descriptor-for-LiDAR-Place-Recognition" class="headerlink" title="VNI-Net: Vector Neurons-based Rotation-Invariant Descriptor for LiDAR Place Recognition"></a>VNI-Net: Vector Neurons-based Rotation-Invariant Descriptor for LiDAR Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12870">http://arxiv.org/abs/2308.12870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gengxuan Tian, Junqiao Zhao, Yingfeng Cai, Fenglin Zhang, Wenjie Mu, Chen Ye</li>
<li>for: 提高 LiDAR 场景认知中的旋转不敏感性</li>
<li>methods: 使用 Vector Neurons Network (VNN) 实现 SO(3) 旋转不变性，提取邻近点的旋转等价特征，将低维特征映射到高维空间</li>
<li>results: 对公共数据集进行实验，与其他基准方法相比，提高了旋转不敏感性，与当前状态艺术场景认知方法几乎匹配Here’s a brief explanation of each point:* “for”: The paper aims to improve the rotation-invariance of LiDAR scene recognition.* “methods”: The proposed method uses Vector Neurons Network (VNN) to achieve SO(3) rotation invariance, and extracts rotation-equivalent features from neighboring points.* “results”: The proposed method significantly outperforms other baseline methods that consider rotation invariance, and achieves comparable results with current state-of-the-art place recognition methods that do not consider rotation issues.<details>
<summary>Abstract</summary>
LiDAR-based place recognition plays a crucial role in Simultaneous Localization and Mapping (SLAM) and LiDAR localization.   Despite the emergence of various deep learning-based and hand-crafting-based methods, rotation-induced place recognition failure remains a critical challenge.   Existing studies address this limitation through specific training strategies or network structures.   However, the former does not produce satisfactory results, while the latter focuses mainly on the reduced problem of SO(2) rotation invariance. Methods targeting SO(3) rotation invariance suffer from limitations in discrimination capability.   In this paper, we propose a new method that employs Vector Neurons Network (VNN) to achieve SO(3) rotation invariance.   We first extract rotation-equivariant features from neighboring points and map low-dimensional features to a high-dimensional space through VNN.   Afterwards, we calculate the Euclidean and Cosine distance in the rotation-equivariant feature space as rotation-invariant feature descriptors.   Finally, we aggregate the features using GeM pooling to obtain global descriptors.   To address the significant information loss when formulating rotation-invariant descriptors, we propose computing distances between features at different layers within the Euclidean space neighborhood.   This greatly improves the discriminability of the point cloud descriptors while ensuring computational efficiency.   Experimental results on public datasets show that our approach significantly outperforms other baseline methods implementing rotation invariance, while achieving comparable results with current state-of-the-art place recognition methods that do not consider rotation issues.
</details>
<details>
<summary>摘要</summary>
利用LiDAR技术实现地点识别在同时地图和地点位置确定（SLAM）中扮演关键角色。  despite the emergence of various深度学习基于和手动设计基于方法， rotate induced place recognition failure remains a critical challenge。 existing studies address this limitation through specific training strategies or network structures。  However, the former does not produce satisfactory results, while the latter focuses mainly on the reduced problem of SO(2) rotation invariance。 methods targeting SO(3) rotation invariance suffer from limitations in discrimination capability。  In this paper, we propose a new method that employs Vector Neurons Network (VNN) to achieve SO(3) rotation invariance。  we first extract rotation-equivariant features from neighboring points and map low-dimensional features to a high-dimensional space through VNN。  Afterwards, we calculate the Euclidean and Cosine distance in the rotation-equivariant feature space as rotation-invariant feature descriptors。  Finally, we aggregate the features using GeM pooling to obtain global descriptors。  To address the significant information loss when formulating rotation-invariant descriptors, we propose computing distances between features at different layers within the Euclidean space neighborhood。  This greatly improves the discriminability of the point cloud descriptors while ensuring computational efficiency。  Experimental results on public datasets show that our approach significantly outperforms other baseline methods implementing rotation invariance, while achieving comparable results with current state-of-the-art place recognition methods that do not consider rotation issues。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="ToonTalker-Cross-Domain-Face-Reenactment"><a href="#ToonTalker-Cross-Domain-Face-Reenactment" class="headerlink" title="ToonTalker: Cross-Domain Face Reenactment"></a>ToonTalker: Cross-Domain Face Reenactment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12866">http://arxiv.org/abs/2308.12866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Gong, Yong Zhang, Xiaodong Cun, Fei Yin, Yanbo Fan, Xuan Wang, Baoyuan Wu, Yujiu Yang</li>
<li>for: 本研究旨在实现跨域人脸reenactment，即将真实视频转换为动漫图像和 vice versa。</li>
<li>methods: 我们提出了一种基于 transformer 框架的新方法，包括两个域特定的动作编码器和两个可学习的动作基准存储。我们还使用了源查询 transformer 和驱动 transformer 来将域特定动作 proyect 到共同幂 space，然后在该空间中进行动作传输。</li>
<li>results: 我们的方法在评估中表现出色，与竞争方法相比有所超越。此外，我们还提供了一个 Disney 风格的动漫数据集，以便进一步验证和应用我们的方法。<details>
<summary>Abstract</summary>
We target cross-domain face reenactment in this paper, i.e., driving a cartoon image with the video of a real person and vice versa. Recently, many works have focused on one-shot talking face generation to drive a portrait with a real video, i.e., within-domain reenactment. Straightforwardly applying those methods to cross-domain animation will cause inaccurate expression transfer, blur effects, and even apparent artifacts due to the domain shift between cartoon and real faces. Only a few works attempt to settle cross-domain face reenactment. The most related work AnimeCeleb requires constructing a dataset with pose vector and cartoon image pairs by animating 3D characters, which makes it inapplicable anymore if no paired data is available. In this paper, we propose a novel method for cross-domain reenactment without paired data. Specifically, we propose a transformer-based framework to align the motions from different domains into a common latent space where motion transfer is conducted via latent code addition. Two domain-specific motion encoders and two learnable motion base memories are used to capture domain properties. A source query transformer and a driving one are exploited to project domain-specific motion to the canonical space. The edited motion is projected back to the domain of the source with a transformer. Moreover, since no paired data is provided, we propose a novel cross-domain training scheme using data from two domains with the designed analogy constraint. Besides, we contribute a cartoon dataset in Disney style. Extensive evaluations demonstrate the superiority of our method over competing methods.
</details>
<details>
<summary>摘要</summary>
我们在这篇论文中targetcross-domain face reenactment，即将动漫图像驱动真实视频和真实视频驱动动漫图像。在最近的许多工作中，人们主要关注在一个shot中生成真实人脸，即在同一个频谱中reenactment。如果直接应用这些方法到cross-domain animation，会导致不准确的表达传递、模糊效果和甚至显式的artefacts，这是因为cartoon和真实人脸之间的频谱差异。只有一些工作尝试了cross-domain face reenactment。最相关的工作是AnimeCeleb，它需要构建一个数据集，其中包含pose vector和动漫图像对的Pair，并通过动画3D人物来生成这些对。这使得它在没有对应数据时无法应用。在这篇论文中，我们提出了一种新的方法，即使没有对应数据也可以实现cross-domain reenactment。具体来说，我们提出了一个基于transformer的框架，用于将不同频谱中的动作都尝试到一个共同的幂space中，然后通过幂码加法进行动作传递。我们使用了两个域特定的动作编码器和两个可学习的动作基准记忆来捕捉域属性。 sources query transformer和驱动一个是用于将域特定的动作 проек到共同空间，而编辑的动作则是通过transformer将其 projet回到源域。此外，由于没有提供对应数据，我们提出了一种新的跨域训练方案，使用了两个域的数据，并通过设计的相似性约束。此外，我们还贡献了一个以Disney风格为主的动漫数据集。我们的方法在多个评估中表现出色，超过了竞争方法的性能。
</details></li>
</ul>
<hr>
<h2 id="SkipcrossNets-Adaptive-Skip-cross-Fusion-for-Road-Detection"><a href="#SkipcrossNets-Adaptive-Skip-cross-Fusion-for-Road-Detection" class="headerlink" title="SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection"></a>SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12863">http://arxiv.org/abs/2308.12863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Yan Gong, Zhiwei Li, Xin Gao, Dafeng Jin, Jun Li, Huaping Liu</li>
<li>For: This paper proposes a novel fusion architecture called SkipcrossNets for multi-modal fusion of LiDAR point clouds and camera images in autonomous driving tasks.* Methods: The SkipcrossNets architecture uses skip-cross connections to adaptively combine features from both modalities at each layer, without being bound to a specific fusion epoch. The network is divided into several blocks to reduce the complexity of feature fusion and the number of model parameters.* Results: The proposed SkipcrossNets architecture achieved a MaxF score of 96.85% on the KITTI dataset and an F1 score of 84.84% on the A2D2 dataset, with a memory requirement of only 2.33 MB and a speed of 68.24 FPS, making it viable for mobile terminals and embedded devices.<details>
<summary>Abstract</summary>
Multi-modal fusion is increasingly being used for autonomous driving tasks, as images from different modalities provide unique information for feature extraction. However, the existing two-stream networks are only fused at a specific network layer, which requires a lot of manual attempts to set up. As the CNN goes deeper, the two modal features become more and more advanced and abstract, and the fusion occurs at the feature level with a large gap, which can easily hurt the performance. In this study, we propose a novel fusion architecture called skip-cross networks (SkipcrossNets), which combines adaptively LiDAR point clouds and camera images without being bound to a certain fusion epoch. Specifically, skip-cross connects each layer to each layer in a feed-forward manner, and for each layer, the feature maps of all previous layers are used as input and its own feature maps are used as input to all subsequent layers for the other modality, enhancing feature propagation and multi-modal features fusion. This strategy facilitates selection of the most similar feature layers from two data pipelines, providing a complementary effect for sparse point cloud features during fusion processes. The network is also divided into several blocks to reduce the complexity of feature fusion and the number of model parameters. The advantages of skip-cross fusion were demonstrated through application to the KITTI and A2D2 datasets, achieving a MaxF score of 96.85% on KITTI and an F1 score of 84.84% on A2D2. The model parameters required only 2.33 MB of memory at a speed of 68.24 FPS, which could be viable for mobile terminals and embedded devices.
</details>
<details>
<summary>摘要</summary>
多Modal融合在自动驾驶任务中日益普遍应用，因为不同模式的图像提供了独特的特征提取信息。然而，现有的两派网络只是在特定网络层进行融合，需要大量的手动尝试设置。随着CNN深入，两种模式的特征变得越来越先进和抽象，融合发生在特征层级，这可能会产生性能下降。在这种研究中，我们提出了一种新的融合架构，即跳过网络（SkipcrossNets），它将雷达点云和摄像头图像在不同的层次进行 Adaptive 融合。具体来说，跳过连接每层与每层进行Feed-forward 的连接，并且为每层的特征图使用所有前一层的特征图作为输入，并将每层的特征图作为输入给后续层的另一个模式。这种策略使得在融合过程中选择最相似的特征层，提供了质点云特征的补做效果。此外，网络还被分解成多个块，以降低特征融合的复杂度和模型参数的数量。Skipcross融合的优点在应用于 KITTI 和 A2D2 数据集上得到了证明，最大评分达 96.85% 和 F1 分数达 84.84%。模型参数只需 2.33 MB 的内存和 68.24 FPS 的速度，这可能是蜂窝Terminal和嵌入式设备的可行选择。
</details></li>
</ul>
<hr>
<h2 id="Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations"><a href="#Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations" class="headerlink" title="Learned Local Attention Maps for Synthesising Vessel Segmentations"></a>Learned Local Attention Maps for Synthesising Vessel Segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12861">http://arxiv.org/abs/2308.12861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou Wei, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>for: 这个论文是为了制作血管分割图像的IMAGING模式，以便在诊断和评估血液管的风险方面提供更好的工具。</li>
<li>methods: 这篇论文使用了一种encoder-decoder模型，通过将T1和T2磁共振图像进行编码，生成基于T2磁共振图像的血管分割图像。该模型采用了两个阶段多目标学习方法，以捕捉全局和局部特征。它使用学习的本地注意力地图，从T2磁共振图像中提取与生成圆束血管相关的信息。</li>
<li>results: 测试中，这个模型的生成的血管分割图像的 dice分数为$0.79\pm0.03$，比state-of-the-art的分割网络（如转换器U-Net和nnU-net）更高，同时使用的参数数量只是这些网络的一部分。主要的 Qualitative difference between our synthetic vessel segmentations and the comparative models was in the sharper resolution of the CoW vessel segments, especially in the posterior circulation.<details>
<summary>Abstract</summary>
Magnetic resonance angiography (MRA) is an imaging modality for visualising blood vessels. It is useful for several diagnostic applications and for assessing the risk of adverse events such as haemorrhagic stroke (resulting from the rupture of aneurysms in blood vessels). However, MRAs are not acquired routinely, hence, an approach to synthesise blood vessel segmentations from more routinely acquired MR contrasts such as T1 and T2, would be useful. We present an encoder-decoder model for synthesising segmentations of the main cerebral arteries in the circle of Willis (CoW) from only T2 MRI. We propose a two-phase multi-objective learning approach, which captures both global and local features. It uses learned local attention maps generated by dilating the segmentation labels, which forces the network to only extract information from the T2 MRI relevant to synthesising the CoW. Our synthetic vessel segmentations generated from only T2 MRI achieved a mean Dice score of $0.79 \pm 0.03$ in testing, compared to state-of-the-art segmentation networks such as transformer U-Net ($0.71 \pm 0.04$) and nnU-net($0.68 \pm 0.05$), while using only a fraction of the parameters. The main qualitative difference between our synthetic vessel segmentations and the comparative models was in the sharper resolution of the CoW vessel segments, especially in the posterior circulation.
</details>
<details>
<summary>摘要</summary>
磁共振成像（MRA）是一种成像血管的技术，可以用于诊断和评估风险，如血栓roke（由血管壁崩溃引起的血栓roke）。然而，MRA不是常见的成像方式，因此一种能够从常见的MR增强像素，如T1和T2，synthesize血管分 segmentation的方法会很有用。我们提出了一种编码器-解码器模型，可以从T2 MRI中synthesize主要脑血管的圆桌（CoW）分 segmentation。我们采用了两个阶段多目标学习方法，可以捕捉全局和局部特征。它使用学习的本地注意力图，从T2 MRI中提取与synthesize CoW相关的信息，使网络只提取T2 MRI中与synthesize CoW相关的信息。我们使用只有T2 MRI Synthesize的血管分 segmentation在测试中 achieve了 mean dice score为$0.79 \pm 0.03$，比 estado-of-the-art segmentation网络如transformer U-Net ($0.71 \pm 0.04$)和nnU-net ($0.68 \pm 0.05$) 的segmentation网络，而且只用了一小部分的参数。主要的qualitative difference между我们的synthetic vessel segmentation和相关的模型在CoW血管段的高分辨率，尤其是 posterior circulation。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Obstacle-Map-driven-Indoor-Navigation-Model-for-Robust-Obstacle-Avoidance"><a href="#Implicit-Obstacle-Map-driven-Indoor-Navigation-Model-for-Robust-Obstacle-Avoidance" class="headerlink" title="Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance"></a>Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12845">http://arxiv.org/abs/2308.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwaiyy123/object-navigation">https://github.com/xwaiyy123/object-navigation</a></li>
<li>paper_authors: Wei Xie, Haobo Jiang, Shuo Gu, Jin Xie</li>
<li>for: 本研究旨在提高室内导航任务中的目标避免碰撞率，尤其是在视觉图像中缺失障碍物和可能的检测错误问题下。</li>
<li>methods: 本研究提出了一种基于历史尝试和错误经验学习的隐式障碍地图驱动的室内导航框架，以提高避免碰撞的Robustness。同时，一种基于非本地网络的目标念 памя库聚合模块是设计来利用非本地网络来描述导航过程中target semantic和target方向准确的相关性，以便在导航过程中挖掘最相关的物品准确准确。</li>
<li>results: 对于AI2-Thor和RoboTHOR的测试数据集，我们的提出方法得到了优秀的避免碰撞和导航效率。<details>
<summary>Abstract</summary>
Robust obstacle avoidance is one of the critical steps for successful goal-driven indoor navigation tasks.Due to the obstacle missing in the visual image and the possible missed detection issue, visual image-based obstacle avoidance techniques still suffer from unsatisfactory robustness. To mitigate it, in this paper, we propose a novel implicit obstacle map-driven indoor navigation framework for robust obstacle avoidance, where an implicit obstacle map is learned based on the historical trial-and-error experience rather than the visual image. In order to further improve the navigation efficiency, a non-local target memory aggregation module is designed to leverage a non-local network to model the intrinsic relationship between the target semantic and the target orientation clues during the navigation process so as to mine the most target-correlated object clues for the navigation decision. Extensive experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent obstacle avoidance and navigation efficiency of our proposed method. The core source code is available at https://github.com/xwaiyy123/object-navigation.
</details>
<details>
<summary>摘要</summary>
Robust obstacle avoidance 是indoor navigation任务中的一个关键步骤。由于视觉图像中缺失障碍物和可能的检测问题，视觉图像基于的障碍物避免技术仍然具有不满足的Robustness。为了解决这个问题，在这篇论文中，我们提出了一种基于历史尝试和错误经验学习的隐式障碍地图驱动的indoor navigation框架，以便更好地避免障碍物。为了进一步提高导航效率，我们还设计了一个非本地目标记忆聚合模块，通过非本地网络模型target semantic和导航过程中的target orientation clue之间的内在关系，以便在导航过程中挖掘最相关的目标对象指示。经验结果表明，我们提出的方法具有优秀的障碍物避免和导航效率。主要代码可以在https://github.com/xwaiyy123/object-navigation中获取。
</details></li>
</ul>
<hr>
<h2 id="EFormer-Enhanced-Transformer-towards-Semantic-Contour-Features-of-Foreground-for-Portraits-Matting"><a href="#EFormer-Enhanced-Transformer-towards-Semantic-Contour-Features-of-Foreground-for-Portraits-Matting" class="headerlink" title="EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting"></a>EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12831">http://arxiv.org/abs/2308.12831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zitao Wang, Qiguang Miao, Yue Xi</li>
<li>for: 提取完整的 semantics 和细腻的 outline</li>
<li>methods: 使用 transformers 自动注意 Mechanism，具有更大的接受场景，能够更好地捕捉人脸的长距离依赖关系和低频semantic 信息</li>
<li>results: 提高模型对人脸 outline 的准确性和完整性，并且不需要trimap<details>
<summary>Abstract</summary>
The portrait matting task aims to extract an alpha matte with complete semantics and finely-detailed contours. In comparison to CNN-based approaches, transformers with self-attention allow a larger receptive field, enabling it to better capture long-range dependencies and low-frequency semantic information of a portrait. However, the recent research shows that self-attention mechanism struggle with modeling high-frequency information and capturing fine contour details, which can lead to bias while predicting the portrait's contours. To address the problem, we propose EFormer to enhance the model's attention towards semantic and contour features. Especially the latter, which is surrounded by a large amount of high-frequency details. We build a semantic and contour detector (SCD) to accurately capture the distribution of semantic and contour features. And we further design contour-edge extraction branch and semantic extraction branch for refining contour features and complete semantic information. Finally, we fuse the two kinds of features and leverage the segmentation head to generate the predicted portrait matte. Remarkably, EFormer is an end-to-end trimap-free method and boasts a simple structure. Experiments conducted on VideoMatte240K-JPEGSD and AIM datasets demonstrate that EFormer outperforms previous portrait matte methods.
</details>
<details>
<summary>摘要</summary>
PORTRAIT MATTING TASK的目标是提取一个完整的α抑制矩阵，以捕捉人脸的完整 semantics和细腻的边缘信息。相比CNN基于的方法， transformer自注意力 Mechanism 具有更大的接受场，可以更好地捕捉人脸的长距离依赖关系和低频semantic信息。然而， latest research 表明 that self-attention mechanism 在模型高频信息和细腻边缘特征的处理方面存在困难，可能导致预测人脸的边缘偏倚。为了解决问题，我们提出 EFormer，用于增强模型的注意力 towards semantic和contour特征。特别是后者，它围绕着大量高频细节。我们建立了 semantic和contour探测器 (SCD)，以准确捕捉人脸的semantic和contour特征的分布。此外，我们还设计了 contour-edge extraction branch 和 semantic extraction branch，用于精细化contour特征和完整semantic信息。最后，我们融合两种特征，并利用 segmentation head 生成预测的人脸抑制矩阵。值得注意的是， EFormer 是一种端到端的trimap-free方法，具有简单的结构。在 VideoMatte240K-JPEGSD 和 AIM 数据集上进行的实验表明，EFormer 在人脸抑制矩阵预测方面高效。
</details></li>
</ul>
<hr>
<h2 id="Robotic-Scene-Segmentation-with-Memory-Network-for-Runtime-Surgical-Context-Inference"><a href="#Robotic-Scene-Segmentation-with-Memory-Network-for-Runtime-Surgical-Context-Inference" class="headerlink" title="Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference"></a>Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12789">http://arxiv.org/abs/2308.12789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uva-dsa/runtime_robscene_seg_2context">https://github.com/uva-dsa/runtime_robscene_seg_2context</a></li>
<li>paper_authors: Zongyu Li, Ian Reyes, Homa Alemzadeh</li>
<li>for: 这 paper 是为了解决runtime context inference在机器助手手术中的挑战，以及提高视频数据的分割精度和时间一致性。</li>
<li>methods: 这 paper 使用了 Space Time Correspondence Network (STCN)，这是一种记忆网络，它可以进行二分分割并减少类别偏见的影响。STCN 使用了记忆银行，以使用过去的图像和分割信息，以确保分割掩模的一致性。</li>
<li>results: 实验表明，STCN 在公共可用的 JIGSAWS 数据集上表现出色，对于难以分割的对象，如针和织物，可以提高分割精度和上下文推断。此外，这 paper 还证明了在 runtime 无需妥协性能的情况下，可以同时进行分割和上下文推断。<details>
<summary>Abstract</summary>
Surgical context inference has recently garnered significant attention in robot-assisted surgery as it can facilitate workflow analysis, skill assessment, and error detection. However, runtime context inference is challenging since it requires timely and accurate detection of the interactions among the tools and objects in the surgical scene based on the segmentation of video data. On the other hand, existing state-of-the-art video segmentation methods are often biased against infrequent classes and fail to provide temporal consistency for segmented masks. This can negatively impact the context inference and accurate detection of critical states. In this study, we propose a solution to these challenges using a Space Time Correspondence Network (STCN). STCN is a memory network that performs binary segmentation and minimizes the effects of class imbalance. The use of a memory bank in STCN allows for the utilization of past image and segmentation information, thereby ensuring consistency of the masks. Our experiments using the publicly available JIGSAWS dataset demonstrate that STCN achieves superior segmentation performance for objects that are difficult to segment, such as needle and thread, and improves context inference compared to the state-of-the-art. We also demonstrate that segmentation and context inference can be performed at runtime without compromising performance.
</details>
<details>
<summary>摘要</summary>
医疗机器人助手中的手术上下文推断在最近几年内受到了广泛关注，因为它可以帮助分析工作流程、评估技能和检测错误。然而，运行时上下文推断具有挑战性，因为它需要在视频数据中检测工具和物品之间的互动，并在实时上提供准确的上下文推断。然而，现有的状态 искусственный智能方法通常对不常见的类型存在偏见，并且无法提供时间上的一致性 для分类mask。这可能会对上下文推断产生负面影响，并妨碍精准检测关键状态。在本研究中，我们提出了一种解决这些挑战的解决方案，即使用Space Time Correspondence Network（STCN）。STCN是一种记忆网络，它可以实现二分 segmentation，并最小化类别不均衡的影响。记忆银行在STCN中的使用，使得可以利用过去的图像和分类信息，以确保mask的一致性。我们使用公共可用的JIGSAWS数据集进行实验，并证明STCN可以在难以分类的对象，如针和织物，中提供superior的分类性能，并提高上下文推断的精度。此外，我们还证明可以在运行时进行分类和上下文推断，不会影响性能。
</details></li>
</ul>
<hr>
<h2 id="On-Offline-Evaluation-of-3D-Object-Detection-for-Autonomous-Driving"><a href="#On-Offline-Evaluation-of-3D-Object-Detection-for-Autonomous-Driving" class="headerlink" title="On Offline Evaluation of 3D Object Detection for Autonomous Driving"></a>On Offline Evaluation of 3D Object Detection for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12779">http://arxiv.org/abs/2308.12779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Schreier, Katrin Renz, Andreas Geiger, Kashyap Chitta</li>
<li>for: 这个论文是为了评估3D对象检测模型在自动驾驶核心任务中的性能而写的。</li>
<li>methods: 这篇论文使用了16种对象检测模型，并在CARLA simulate器上进行了广泛的实验，以评估不同检测精度指标如何影响自动驾驶性能。</li>
<li>results: 研究发现，nuScenes检测得分更高地相关于驾驶性能，而且警告了对&#96; плаanner-centric’指标的封闭依赖。<details>
<summary>Abstract</summary>
Prior work in 3D object detection evaluates models using offline metrics like average precision since closed-loop online evaluation on the downstream driving task is costly. However, it is unclear how indicative offline results are of driving performance. In this work, we perform the first empirical evaluation measuring how predictive different detection metrics are of driving performance when detectors are integrated into a full self-driving stack. We conduct extensive experiments on urban driving in the CARLA simulator using 16 object detection models. We find that the nuScenes Detection Score has a higher correlation to driving performance than the widely used average precision metric. In addition, our results call for caution on the exclusive reliance on the emerging class of `planner-centric' metrics.
</details>
<details>
<summary>摘要</summary>
先前的工作在3D对象检测中通常使用离线指标如平均准确率来评估模型。然而，不清楚这些离线结果对驱动性能的指导性。在本工作中，我们实施了首次employmetric evaluation的研究，measure如何不同的检测指标对自动驱动栈中检测器的驱动性能具有预测性。我们在CARLA simulator上进行了大规模的城市驱动实验，使用16个对象检测模型。我们发现，nuScenes Detection Score与驱动性能之间存在更高的相关性，而且我们的结果表明，应该小心对新般的` плаanner-centric'指标的归类依赖。
</details></li>
</ul>
<hr>
<h2 id="LISTER-Neighbor-Decoding-for-Length-Insensitive-Scene-Text-Recognition"><a href="#LISTER-Neighbor-Decoding-for-Length-Insensitive-Scene-Text-Recognition" class="headerlink" title="LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition"></a>LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12774">http://arxiv.org/abs/2308.12774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changxu Cheng, Peng Wang, Cheng Da, Qi Zheng, Cong Yao</li>
<li>for: 提高Scene Text Recognition（STR）的长文本识别能力和长文本推断能力。</li>
<li>methods: 提出Length-Insensitive Scene TExt Recognizer（LISTER）算法，包括Neighbor Decoder和Feature Enhancement Module两部分。Neighbor Decoder使用帮助器矩阵获取准确的字符注意力地图，不受文本长度影响。Feature Enhancement Module通过低计算成本模型长距离依赖关系，可以逐步增强特征地图进行迭代处理。</li>
<li>results: 实验表明，提出的LISTER算法在长文本识别和长文本推断方面具有显著优势，并且与之前的STATE-OF-THE-ART方法在标准STR测试集（主要是短文本）上表现相当。<details>
<summary>Abstract</summary>
The diversity in length constitutes a significant characteristic of text. Due to the long-tail distribution of text lengths, most existing methods for scene text recognition (STR) only work well on short or seen-length text, lacking the capability of recognizing longer text or performing length extrapolation. This is a crucial issue, since the lengths of the text to be recognized are usually not given in advance in real-world applications, but it has not been adequately investigated in previous works. Therefore, we propose in this paper a method called Length-Insensitive Scene TExt Recognizer (LISTER), which remedies the limitation regarding the robustness to various text lengths. Specifically, a Neighbor Decoder is proposed to obtain accurate character attention maps with the assistance of a novel neighbor matrix regardless of the text lengths. Besides, a Feature Enhancement Module is devised to model the long-range dependency with low computation cost, which is able to perform iterations with the neighbor decoder to enhance the feature map progressively. To the best of our knowledge, we are the first to achieve effective length-insensitive scene text recognition. Extensive experiments demonstrate that the proposed LISTER algorithm exhibits obvious superiority on long text recognition and the ability for length extrapolation, while comparing favourably with the previous state-of-the-art methods on standard benchmarks for STR (mainly short text).
</details>
<details>
<summary>摘要</summary>
Text 的多样性在长度方面是一个重要特征。由于文本长度的长尾分布，大多数现有的场景文本识别（STR）方法只能在短文本上工作良好，缺乏长文本或者长文本 extrapolation 的能力。这是一个关键问题，因为实际应用中文本的长度通常不会提前给出，但这一问题在前一未得到充分调查。因此，我们在这篇论文中提出了一种名为Length-Insensitive Scene TExt Recognizer（LISTER）的方法，以解决这种限制。具体来说，我们提出了一种邻居解码器，可以通过一个新的邻居矩阵获得准确的字符注意地图，不管文本的长度。此外，我们还设计了一种特征增强模块，可以通过低计算成本模elling 长距离关系，并且可以与邻居解码器进行多次迭代来进一步增强特征图。根据我们所知，我们的提出的 LISTER 算法是首次实现了长度不敏感的场景文本识别。我们的实验结果表明，LISTER 算法在长文本识别和长度推算方面具有明显的优势，同时与之前的状态lava 方法在标准 STR 标准库（主要是短文本）上比较了常。
</details></li>
</ul>
<hr>
<h2 id="IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation"><a href="#IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation" class="headerlink" title="IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation"></a>IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12761">http://arxiv.org/abs/2308.12761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nyothiri Aung, Tahar Kechadi, Liming Chen, Sahraoui Dhelim<br>for:  automatic breast calcification detectionmethods:  IP-UNet model, which performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data, and uses limited memory capability for training without losing the original 3D image resolution.results:  IP-UNet achieves similar segmentation accuracy as 3D-UNet but with much better performance, reducing training time by 70% and memory consumption by 92%.<details>
<summary>Abstract</summary>
CNNs have been widely applied for medical image analysis. However, limited memory capacity is one of the most common drawbacks of processing high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized first before processing, which can result in a loss of resolution, increase class imbalance, and affect the performance of the segmentation algorithms. In this paper, we propose an end-to-end deep learning approach called IP-UNet. IP-UNet is a UNet-based model that performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming 3D volumes. IP-UNet uses limited memory capability for training without losing the original 3D image resolution. We compare the performance of three models in terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D segmentation of the CT scan images using a conventional 2D UNet model. 2) IP-UNet that operates on data obtained by merging the extracted Maximum Intensity Projection (MIP), Closest Vessel Projection (CVP), and Average Intensity Projection (AvgIP) representations of the source 3D volumes, then applying the UNet model on the output IP images. 3) 3D-UNet model directly reads the 3D volumes constructed from a series of CT scan images and outputs the 3D volume of the predicted segmentation. We test the performance of these methods on 3D volumetric images for automatic breast calcification detection. Experimental results show that IP-Unet can achieve similar segmentation accuracy with 3D-Unet but with much better performance. It reduces the training time by 70\% and memory consumption by 92\%.
</details>
<details>
<summary>摘要</summary>
对于医疗影像分析，广泛应用了深度学习网络（CNN）。然而，处理高分辨率3D数据时的内存容量问题是最常见的问题。通常会将3Dvolume裁剪或缩小以便处理，这会导致解析损失、增加分布不均和影像分析表现下降。在这篇论文中，我们提出了一个端到端的深度学习方法，即IP-UNet。IP-UNet是基于UNet模型，用于多类分类INTENSITY PROJECTION（IP）3D数据，而不需要内存昂贵的3Dvolume训练。我们将比较三种模型的表现，包括：1）对CT扫描影像的单面2D分类使用传统2D UNet模型。2）IP-UNet，它在提取Maximum Intensity Projection（MIP）、Closest Vessel Projection（CVP）和Average Intensity Projection（AvgIP）表现后，将UNet模型应用于输出IP图像。3）直接从CT扫描影像构建3D数据，并将预测分类结果传回3D数据。我们将这些方法评估在3D数据中自动胸腔癌斑准确性检测上。实验结果显示，IP-UNet可以与3D-UNet实现相似的分类精度，但是具有训练时间快速和内存消耗几乎减半的优点。
</details></li>
</ul>
<hr>
<h2 id="PartSeg-Few-shot-Part-Segmentation-via-Part-aware-Prompt-Learning"><a href="#PartSeg-Few-shot-Part-Segmentation-via-Part-aware-Prompt-Learning" class="headerlink" title="PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning"></a>PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12757">http://arxiv.org/abs/2308.12757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengya Han, Heliang Zheng, Chaoyue Wang, Yong Luo, Han Hu, Jing Zhang, Yonggang Wen</li>
<li>for: 本研究旨在实现少数示例部分 segmentation，即使用非常少的标注示例来分割未经见过的物体中的不同部分。</li>
<li>methods: 我们提出了一种基于多Modal学习的新方法，称为PartSeg，用于实现少数示例部分 segmentation。我们特别设计了一种可以让CLIP模型更好地理解“部分”概念的部分掌握学习方法。此外，我们在提问学习过程中建立了不同物体类别中同一部分之间的关系。</li>
<li>results: 我们在PartImageNet和Pascal$_$Part datasets上进行了广泛的实验，结果表明，我们提出的方法可以达到状态 искусственный智能的表现。<details>
<summary>Abstract</summary>
In this work, we address the task of few-shot part segmentation, which aims to segment the different parts of an unseen object using very few labeled examples. It is found that leveraging the textual space of a powerful pre-trained image-language model (such as CLIP) can be beneficial in learning visual features. Therefore, we develop a novel method termed PartSeg for few-shot part segmentation based on multimodal learning. Specifically, we design a part-aware prompt learning method to generate part-specific prompts that enable the CLIP model to better understand the concept of ``part'' and fully utilize its textual space. Furthermore, since the concept of the same part under different object categories is general, we establish relationships between these parts during the prompt learning process. We conduct extensive experiments on the PartImageNet and Pascal$\_$Part datasets, and the experimental results demonstrated that our proposed method achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑了几个shot部分 segmentation任务，该任务的目标是使用非常少的标注例进行不同对象的部分分类。我们发现可以利用一个强大预训练的图像语言模型（如CLIP）的文本空间，可以有利于学习视觉特征。因此，我们开发了一种基于多Modal学习的新方法，称为PartSeg。具体来说，我们设计了一种部分意识的提问学习方法，以便CLIP模型更好地理解“部分”的概念，并充分利用其文本空间。此外，我们在提问学习过程中建立了这些部分之间的关系。我们在PartImageNet和Pascal$\_$Part datasets上进行了广泛的实验，并发现我们的提议方法可以达到状态的表现。
</details></li>
</ul>
<hr>
<h2 id="Learning-Heavily-Degraded-Prior-for-Underwater-Object-Detection"><a href="#Learning-Heavily-Degraded-Prior-for-Underwater-Object-Detection" class="headerlink" title="Learning Heavily-Degraded Prior for Underwater Object Detection"></a>Learning Heavily-Degraded Prior for Underwater Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12738">http://arxiv.org/abs/2308.12738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaodetection/learning-heavily-degraed-prior">https://github.com/xiaodetection/learning-heavily-degraed-prior</a></li>
<li>paper_authors: Chenping Fu, Xin Fan, Jiewen Xiao, Wanqi Yuan, Risheng Liu, Zhongxuan Luo</li>
<li>for: 解决水下物体检测中的质量下降问题，通过利用受损图像中的特征分布偏移来提高检测性能。</li>
<li>methods: 基于受损图像的统计观察，提出了差异特征传递模块（RFTM），通过学习受损图像和水下图像之间的映射，提高水下物体检测性能。</li>
<li>results: 对URPC2020和UODD数据集进行评估，显示 compared to CNN基于的检测器，本方法可以大幅提高水下物体检测性能，并且具有更高的速度和更少的参数。<details>
<summary>Abstract</summary>
Underwater object detection suffers from low detection performance because the distance and wavelength dependent imaging process yield evident image quality degradations such as haze-like effects, low visibility, and color distortions. Therefore, we commit to resolving the issue of underwater object detection with compounded environmental degradations. Typical approaches attempt to develop sophisticated deep architecture to generate high-quality images or features. However, these methods are only work for limited ranges because imaging factors are either unstable, too sensitive, or compounded. Unlike these approaches catering for high-quality images or features, this paper seeks transferable prior knowledge from detector-friendly images. The prior guides detectors removing degradations that interfere with detection. It is based on statistical observations that, the heavily degraded regions of detector-friendly (DFUI) and underwater images have evident feature distribution gaps while the lightly degraded regions of them overlap each other. Therefore, we propose a residual feature transference module (RFTM) to learn a mapping between deep representations of the heavily degraded patches of DFUI- and underwater- images, and make the mapping as a heavily degraded prior (HDP) for underwater detection. Since the statistical properties are independent to image content, HDP can be learned without the supervision of semantic labels and plugged into popular CNNbased feature extraction networks to improve their performance on underwater object detection. Without bells and whistles, evaluations on URPC2020 and UODD show that our methods outperform CNN-based detectors by a large margin. Our method with higher speeds and less parameters still performs better than transformer-based detectors. Our code and DFUI dataset can be found in https://github.com/xiaoDetection/Learning-Heavily-Degraed-Prior.
</details>
<details>
<summary>摘要</summary>
水下对象检测受到质量低下的影响，因为图像处理过程中的距离和波长的效果会导致明显的图像质量下降，如霾效、低可见度和颜色扭曲。因此，我们决心解决水下对象检测中的环境降低效应。常见的方法是开发复杂的深度架构来生成高质量图像或特征。然而，这些方法只能在有限的范围内工作，因为图像因素是不稳定、太敏感或复杂的。与这些方法不同，这篇论文寻求从检测友好的图像（DFUI）中提取知识。这种知识指导检测器去除影响检测的降低因素。这基于统计观察，水下和DFUI图像中的严重降低区域有明显的特征分布差异，而轻度降低区域之间重叠。因此，我们提出了差异特征传递模块（RFTM），以学习将深度表示DFUI图像中的严重降低区域和水下图像之间建立一个映射，并将这个映射作为强制质量（HDP）来进行水下检测。由于统计性质独立于图像内容，HDP可以在无监督的情况下学习，并可以插入流行的CNN基于特征提取网络来提高水下对象检测性能。我们的方法在URPC2020和UODD上进行评估，与CNN基于检测器相比，我们的方法在大幅度下表现出较好的性能。我们的方法还具有更高的速度和更少的参数，仍然可以在 transformer 基于检测器之上进行改进。我们的代码和 DFUI 数据集可以在 GitHub 上找到：https://github.com/xiaoDetection/Learning-Heavily-Degraed-Prior。
</details></li>
</ul>
<hr>
<h2 id="FastSurfer-HypVINN-Automated-sub-segmentation-of-the-hypothalamus-and-adjacent-structures-on-high-resolutional-brain-MRI"><a href="#FastSurfer-HypVINN-Automated-sub-segmentation-of-the-hypothalamus-and-adjacent-structures-on-high-resolutional-brain-MRI" class="headerlink" title="FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI"></a>FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12736">http://arxiv.org/abs/2308.12736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Estrada, David Kügler, Emad Bahrami, Peng Xu, Dilshad Mousa, Monique M. B. Breteler, N. Ahmad Aziz, Martin Reuter</li>
<li>for: 这个论文的目的是提供一种自动分割哈米尼肌肉的方法，以便更好地研究哈米尼肌肉的功能和结构。</li>
<li>methods: 这个方法使用了深度学习算法，并且可以处理0.8mm的是otropic T1w和T2w MR图像。</li>
<li>results: 这个方法可以具有高度的分割精度和可靠性，并且可以在多个数据集上进行扩展验证。<details>
<summary>Abstract</summary>
The hypothalamus plays a crucial role in the regulation of a broad range of physiological, behavioural, and cognitive functions. However, despite its importance, only a few small-scale neuroimaging studies have investigated its substructures, likely due to the lack of fully automated segmentation tools to address scalability and reproducibility issues of manual segmentation. While the only previous attempt to automatically sub-segment the hypothalamus with a neural network showed promise for 1.0 mm isotropic T1-weighted (T1w) MRI, there is a need for an automated tool to sub-segment also high-resolutional (HiRes) MR scans, as they are becoming widely available, and include structural detail also from multi-modal MRI. We, therefore, introduce a novel, fast, and fully automated deep learning method named HypVINN for sub-segmentation of the hypothalamus and adjacent structures on 0.8 mm isotropic T1w and T2w brain MR images that is robust to missing modalities. We extensively validate our model with respect to segmentation accuracy, generalizability, in-session test-retest reliability, and sensitivity to replicate hypothalamic volume effects (e.g. sex-differences). The proposed method exhibits high segmentation performance both for standalone T1w images as well as for T1w/T2w image pairs. Even with the additional capability to accept flexible inputs, our model matches or exceeds the performance of state-of-the-art methods with fixed inputs. We, further, demonstrate the generalizability of our method in experiments with 1.0 mm MR scans from both the Rhineland Study and the UK Biobank. Finally, HypVINN can perform the segmentation in less than a minute (GPU) and will be available in the open source FastSurfer neuroimaging software suite, offering a validated, efficient, and scalable solution for evaluating imaging-derived phenotypes of the hypothalamus.
</details>
<details>
<summary>摘要</summary>
《响应腔室中的肥厚腔室功能》的研究具有重要的意义，但由于缺乏可靠的自动分割工具，因此只有一些小规模的 нейро成像研究对其下部结构进行了调查。尽管之前一个使用神经网络自动分割肥厚腔室的尝试显示了对1.0 mm是otropic T1束成像（T1w）的承诺，但是有必要为高分辨率（HiRes）MR扫描图像提供自动分割工具，因为它们在广泛使用并包含多modal MRI结构细节。我们因此介绍了一种新的快速、自动化深度学习方法，名为 HypVINN，用于肥厚腔室和相邻结构的0.8 mm是otropic T1w和T2w大脑MR扫描图像的分割，具有对缺失模式的Robust性。我们对模型进行了广泛验证，包括分割精度、普适性、在SESSION中的重复测试可靠性和性别差异的敏感性。我们的模型在单独的T1w图像上以及T1w/T2w图像对上都 exhibits高度的分割性能。即使可以接受 flexible inputs，我们的模型与已有的方法相当或超过性能。我们进一步证明了我们的方法在1.0 mm MR扫描图像上的普适性，并在 Rheinland Study和UK Biobank中进行了实验。最后，HypVINN可以在 less than a minute（GPU）内完成分割，并将被包含在开源的 FastSurfer neuroscience imaging software suite中，提供一个验证、高效、扩展的解决方案，用于评估基于成像的肥厚腔室相关性。
</details></li>
</ul>
<hr>
<h2 id="Ground-to-Aerial-Person-Search-Benchmark-Dataset-and-Approach"><a href="#Ground-to-Aerial-Person-Search-Benchmark-Dataset-and-Approach" class="headerlink" title="Ground-to-Aerial Person Search: Benchmark Dataset and Approach"></a>Ground-to-Aerial Person Search: Benchmark Dataset and Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12712">http://arxiv.org/abs/2308.12712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yqc123456/hkd_for_person_search">https://github.com/yqc123456/hkd_for_person_search</a></li>
<li>paper_authors: Shizhou Zhang, Qingchun Yang, De Cheng, Yinghui Xing, Guoqiang Liang, Peng Wang, Yanning Zhang</li>
<li>for: 这个论文旨在构建一个大规模的人体搜索数据集，以便进行跨平台智能监测应用程序开发。</li>
<li>methods: 该论文使用了两步人体搜索方法和终端到终端人体搜索方法，并提出了一种简单 yet effective的知识储存方法，用于提高人体搜索性能。</li>
<li>results: 该论文通过对G2APS数据集和两个公共的人体搜索数据集进行分析，并提出了一种基于知识储存的人体搜索方法，实现了状态畅的性能。<details>
<summary>Abstract</summary>
In this work, we construct a large-scale dataset for Ground-to-Aerial Person Search, named G2APS, which contains 31,770 images of 260,559 annotated bounding boxes for 2,644 identities appearing in both of the UAVs and ground surveillance cameras. To our knowledge, this is the first dataset for cross-platform intelligent surveillance applications, where the UAVs could work as a powerful complement for the ground surveillance cameras. To more realistically simulate the actual cross-platform Ground-to-Aerial surveillance scenarios, the surveillance cameras are fixed about 2 meters above the ground, while the UAVs capture videos of persons at different location, with a variety of view-angles, flight attitudes and flight modes. Therefore, the dataset has the following unique characteristics: 1) drastic view-angle changes between query and gallery person images from cross-platform cameras; 2) diverse resolutions, poses and views of the person images under 9 rich real-world scenarios. On basis of the G2APS benchmark dataset, we demonstrate detailed analysis about current two-step and end-to-end person search methods, and further propose a simple yet effective knowledge distillation scheme on the head of the ReID network, which achieves state-of-the-art performances on both of the G2APS and the previous two public person search datasets, i.e., PRW and CUHK-SYSU. The dataset and source code available on \url{https://github.com/yqc123456/HKD_for_person_search}.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们构建了一个大规模的人earch数据集，名为G2APS，其包含31,770张图像和260,559个注解的矩形框，其中每个矩形框都包含2,644个人肖象出现在UAV和地面监测摄像头中。我们知道，这是首个跨平台智能监测应用程序的数据集，UAV可以作为地面监测摄像头的强力补充。为更真实地模拟实际跨平台地面-空中监测场景，地面监测摄像头 fixes在2米上，而UAV拍摄了不同位置的人肖象，并且有多种视角、飞行姿态和飞行模式。因此，该数据集具有以下独特特点：1）跨平台摄像头之间人肖象的极大视角变化；2）人肖象的多种分辨率、姿势和视野下的9种实际场景。基于G2APS标准数据集，我们对现有的两步人earch方法和端到端人earch方法进行详细分析，并提出了一种简单 yet有效的知识储存 scheme，用于在ReID网络的头部进行人earch，该方法在G2APS和以前两个公共人earch数据集上实现了状态当前性。数据集和源代码可以在 \url{https://github.com/yqc123456/HKD_for_person_search} 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Parse-Then-Place-Approach-for-Generating-Graphic-Layouts-from-Textual-Descriptions"><a href="#A-Parse-Then-Place-Approach-for-Generating-Graphic-Layouts-from-Textual-Descriptions" class="headerlink" title="A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions"></a>A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12700">http://arxiv.org/abs/2308.12700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Lin, Jiaqi Guo, Shizhao Sun, Weijiang Xu, Ting Liu, Jian-Guang Lou, Dongmei Zhang</li>
<li>for: 这个论文的目的是提出一种基于文本指导的图形设计方法，以低下设计难度。</li>
<li>methods: 这种方法包括两个阶段：解析阶段和放置阶段。解析阶段通过将文本描述转换为一种中间表示（IR），来模拟文本中的隐式约束。放置阶段使用Transformer模型生成图形。为了处理组合和不完整的约束，我们使用Transformer模型并且 специаль地设计了约束和图形的表示方式。</li>
<li>results: 我们在两个 Text-to-Layout 数据集上进行了实验，并取得了优秀的成绩。量化结果、质量分析和用户研究都证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Creating layouts is a fundamental step in graphic design. In this work, we propose to use text as the guidance to create graphic layouts, i.e., Text-to-Layout, aiming to lower the design barriers. Text-to-Layout is a challenging task, because it needs to consider the implicit, combined, and incomplete layout constraints from text, each of which has not been studied in previous work. To address this, we present a two-stage approach, named parse-then-place. The approach introduces an intermediate representation (IR) between text and layout to represent diverse layout constraints. With IR, Text-to-Layout is decomposed into a parse stage and a place stage. The parse stage takes a textual description as input and generates an IR, in which the implicit constraints from the text are transformed into explicit ones. The place stage generates layouts based on the IR. To model combined and incomplete constraints, we use a Transformer-based layout generation model and carefully design a way to represent constraints and layouts as sequences. Besides, we adopt the pretrain-then-finetune strategy to boost the performance of the layout generation model with large-scale unlabeled layouts. To evaluate our approach, we construct two Text-to-Layout datasets and conduct experiments on them. Quantitative results, qualitative analysis, and user studies demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
创建布局是图形设计的基本步骤。在这项工作中，我们提议使用文本作为布局创建的指导，即文本到布局（Text-to-Layout），以降低设计障碍。文本到布局是一项复杂的任务，因为它需要考虑文本中的隐式、共同和部分缺失的布局约束，每一种都没有在前期工作中研究过。为解决这个问题，我们提出了两个阶段方法，称之为parse-then-place。这种方法引入了一个中间表示（IR），用于将文本中的布局约束转换为Explicit的约束。在IR中，我们使用Transformer模型来生成布局。此外，我们还设计了一种方法来表示约束和布局为序列，以便处理共同和缺失的约束。此外，我们采用了预训练后finetune策略，以提高布局生成模型的性能。为评估我们的方法，我们构建了两个文本到布局数据集，并在其中进行了实验。量化结果、质量分析和用户研究都证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification"><a href="#A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification" class="headerlink" title="A Continual Learning Approach for Cross-Domain White Blood Cell Classification"></a>A Continual Learning Approach for Cross-Domain White Blood Cell Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12679">http://arxiv.org/abs/2308.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Raheleh Salehi, Armin Gruber, Sayedali Shetab Boushehri, Pascal Giehr, Nassir Navab, Carsten Marr</li>
<li>for: 这个研究旨在提高白血球类别的准确性，以便诊断血液疾病。</li>
<li>methods: 本研究使用了一种叫做复习式专有学习的方法，可以逐步学习来自新数据流的知识，而不会忘记之前学习的知识。</li>
<li>results: 研究结果显示，使用复习式专有学习方法可以在不同的颜色、分辨率和类别结构下，实现白血球类别的准确分类。此外，在长期演进学习中，本方法也可以优于现有的iCaRL和EWC方法。<details>
<summary>Abstract</summary>
Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We thoroughly evaluated our proposed approach on three white blood cell classification datasets that differ in color, resolution, and class composition, including scenarios where new domains or new classes are introduced to the model with every task. We also test a long class incremental experiment with both new domains and new classes. Our results demonstrate that our approach outperforms established baselines in continual learning, including existing iCaRL and EWC methods for classifying white blood cells in cross-domain environments.
</details>
<details>
<summary>摘要</summary>
Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We thoroughly evaluated our proposed approach on three white blood cell classification datasets that differ in color, resolution, and class composition, including scenarios where new domains or new classes are introduced to the model with every task. We also test a long class incremental experiment with both new domains and new classes. Our results demonstrate that our approach outperforms established baselines in continual learning, including existing iCaRL and EWC methods for classifying white blood cells in cross-domain environments.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes"><a href="#A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes" class="headerlink" title="A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes"></a>A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12675">http://arxiv.org/abs/2308.12675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Matthias Hehr, Nassir Navab, Carsten Marr<br>for: 这份研究旨在探讨急性白血病（AML）分型的精确分类是否受到年龄和性别偏好的影响，以提高临床决策和患者照顾。methods: 这份研究使用多例学习（MIL）架构，训练多个MIL模型，并评估它们在不同的性别偏好和年龄偏好下的表现。results: 研究发现，AML分型分类中受到性别和年龄偏好的影响，特别是女性患者更容易受到性别偏好的影响，而 certain age groups，例如72-86岁的患者，则受到年龄偏好的影响。确保训练数据的多元性是关键，以确保AML分型分类的可靠性和公平性，最终帮助多元化的患者人口。<details>
<summary>Abstract</summary>
Accurate classification of Acute Myeloid Leukemia (AML) subtypes is crucial for clinical decision-making and patient care. In this study, we investigate the potential presence of age and sex bias in AML subtype classification using Multiple Instance Learning (MIL) architectures. To that end, we train multiple MIL models using different levels of sex imbalance in the training set and excluding certain age groups. To assess the sex bias, we evaluate the performance of the models on male and female test sets. For age bias, models are tested against underrepresented age groups in the training data. We find a significant effect of sex and age bias on the performance of the model for AML subtype classification. Specifically, we observe that females are more likely to be affected by sex imbalance dataset and certain age groups, such as patients with 72 to 86 years of age with the RUNX1::RUNX1T1 genetic subtype, are significantly affected by an age bias present in the training data. Ensuring inclusivity in the training data is thus essential for generating reliable and equitable outcomes in AML genetic subtype classification, ultimately benefiting diverse patient populations.
</details>
<details>
<summary>摘要</summary>
《急性白细胞病（AML）分型准确分类是临床决策和患者护理中非常重要。本研究探讨AML分型准确分类中年龄和性别偏见的可能性，使用多例学习（MIL）架构。为此，我们在不同的性别占比水平和年龄组中训练多个MIL模型，并在测试集上评估模型的性别偏见和年龄偏见。结果显示，女性患者更容易受到数据集中的性别偏见的影响，而72-86岁的年龄组患者则受到训练数据中的年龄偏见的影响。因此，在训练数据中保证包容性是AML分型准确分类中不可或缺的。这有助于促进多样化患者群体的可靠和公平的结果，最终总是有利于患者的护理。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition"><a href="#Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition" class="headerlink" title="Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition"></a>Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12673">http://arxiv.org/abs/2308.12673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Daskalakis, Nikolaos Gkalelis, Vasileios Mezaris</li>
<li>for: 本研究旨在提出一种未监督预训练方法，用于提高视频事件识别模型的起点和总体性能。</li>
<li>methods: 本研究使用了一种已经预训练的视觉Tokenizer来重建视频中对象的遮盖特征，然后将预训练的GAT块 integrate到现有的视频事件识别架构中，以提高模型的起点和总体性能。</li>
<li>results: 实验结果表明，使用Masked Feature Modelling（MFM）方法可以提高视频事件识别性能。<details>
<summary>Abstract</summary>
In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的无监督预训练方法，即Masked Feature Modelling（MFM），用于提高视频事件认知性能。MFM使用一个预训练的视觉分词器来重建视频中对象的遮盲特征，利用MiniKinetics dataset。然后，我们将预训练的GAT块 integrate到了一个现有的 bottom-up 超级视频事件识别架构ViGAT中，以提高模型的起点和总体准确率。实验评估在YLI-MED数据集上，表明MFM有效地提高事件识别性能。
</details></li>
</ul>
<hr>
<h2 id="An-All-Deep-System-for-Badminton-Game-Analysis"><a href="#An-All-Deep-System-for-Badminton-Game-Analysis" class="headerlink" title="An All Deep System for Badminton Game Analysis"></a>An All Deep System for Badminton Game Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12645">http://arxiv.org/abs/2308.12645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Yung Chou, Yu-Chun Lo, Bo-Zheng Xie, Cheng-Hung Lin, Yu-Yung Kao</li>
<li>for:  automatic detection of events within badminton match videos, especially the shuttlecock</li>
<li>methods:  modified TrackNet model and diverse data types to improve precision</li>
<li>results:  score of 0.78 out of 1.0 in the challenge<details>
<summary>Abstract</summary>
The CoachAI Badminton 2023 Track1 initiative aim to automatically detect events within badminton match videos. Detecting small objects, especially the shuttlecock, is of quite importance and demands high precision within the challenge. Such detection is crucial for tasks like hit count, hitting time, and hitting location. However, even after revising the well-regarded shuttlecock detecting model, TrackNet, our object detection models still fall short of the desired accuracy. To address this issue, we've implemented various deep learning methods to tackle the problems arising from noisy detectied data, leveraging diverse data types to improve precision. In this report, we detail the detection model modifications we've made and our approach to the 11 tasks. Notably, our system garnered a score of 0.78 out of 1.0 in the challenge.
</details>
<details>
<summary>摘要</summary>
coachai 羽毛球 2023 跟踪1 INITIATIVE 目标是自动探测羽毛球赛事视频中的事件。 特别是小 object，如羽毛球，需要高精度的探测，这是因为这些探测对于hit count、 hitting time 和 hitting location 等任务非常重要。 不过，即使修改了 widely recognized 的羽毛球探测模型 TrackNet，我们的 object detection 模型仍然没有达到所需的准确性。 为了解决这个问题，我们采用了多种深度学习方法，以提高不同数据类型的精度。 在这份报告中，我们详细介绍了我们对模型的修改和我们对11个任务的方法。 值得注意的是，我们的系统在挑战中得到了 0.78 分的成绩。
</details></li>
</ul>
<hr>
<h2 id="Tag-Based-Annotation-for-Avatar-Face-Creation"><a href="#Tag-Based-Annotation-for-Avatar-Face-Creation" class="headerlink" title="Tag-Based Annotation for Avatar Face Creation"></a>Tag-Based Annotation for Avatar Face Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12642">http://arxiv.org/abs/2308.12642</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Ngo, Daniel Phelps, Derrick Lai, Thanyared Wong, Lucas Mathias, Anish Shivamurthy, Mustafa Ajmal, Minghao Liu, James Davis</li>
<li>for: 这篇论文的目的是如何自动生成数字人物图像。</li>
<li>methods: 这篇论文使用了标签基于注释的方法来训练模型生成人物图像。</li>
<li>results: 这篇论文的结果是通过标签基于注释的方法来提高模型的预测质量和降低噪音水平。<details>
<summary>Abstract</summary>
Currently, digital avatars can be created manually using human images as reference. Systems such as Bitmoji are excellent producers of detailed avatar designs, with hundreds of choices for customization. A supervised learning model could be trained to generate avatars automatically, but the hundreds of possible options create difficulty in securing non-noisy data to train a model. As a solution, we train a model to produce avatars from human images using tag-based annotations. This method provides better annotator agreement, leading to less noisy data and higher quality model predictions. Our contribution is an application of tag-based annotation to train a model for avatar face creation. We design tags for 3 different facial facial features offered by Bitmoji, and train a model using tag-based annotation to predict the nose.
</details>
<details>
<summary>摘要</summary>
当前，数字化人物可以通过人像作为参考来手动创建。系统如Bitmoji可以生成细节rich的人物设计，具有数百个个性化选项。一个监督学习模型可以自动生成人物，但是数百个可能的选项带来难度，困难于获得不含噪声数据来训练模型。为解决这个问题，我们使用标签基本注解来训练模型生成人物脸。这种方法可以提供更好的注释协议，从而减少噪声数据和提高模型预测质量。我们的贡献是通过标签基本注解来训练模型，以生成人物脸。我们设计了Bitmoji提供的三种不同的 facial 特征标签，并使用标签基本注解来预测脸的鼻子。
</details></li>
</ul>
<hr>
<h2 id="Cross-Video-Contextual-Knowledge-Exploration-and-Exploitation-for-Ambiguity-Reduction-in-Weakly-Supervised-Temporal-Action-Localization"><a href="#Cross-Video-Contextual-Knowledge-Exploration-and-Exploitation-for-Ambiguity-Reduction-in-Weakly-Supervised-Temporal-Action-Localization" class="headerlink" title="Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization"></a>Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12609">http://arxiv.org/abs/2308.12609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songchun Zhang, Chunhui Zhao</li>
<li>for: 本研究旨在提高无rimmed视频中动作地址的准确性和效率，使用视频级标签进行weakly supervised temporal action localization。</li>
<li>methods: 我们提出了一个综合的框架，包括Robust Memory-Guided Contrastive Learning（RMGCL）模块和Global Knowledge Summarization and Aggregation（GKSA）模块，以挖掘和利用跨视频动作特征的相似性和一致性，从而提高动作特征的结构化编码，并降低分类学习中的ambiguity。</li>
<li>results: 我们的方法在THUMOS14、ActivityNet1.3和FineAction等三个 datasets上进行了广泛的实验，结果显示，我们的方法可以高效地提高无rimmed视频中动作地址的准确性和效率，并且可以与其他WSTAL方法结合使用。<details>
<summary>Abstract</summary>
Weakly supervised temporal action localization (WSTAL) aims to localize actions in untrimmed videos using video-level labels. Despite recent advances, existing approaches mainly follow a localization-by-classification pipeline, generally processing each segment individually, thereby exploiting only limited contextual information. As a result, the model will lack a comprehensive understanding (e.g. appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this from a novel perspective, by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, an end-to-end framework is proposed, including a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods, and can be easily plugged into other WSTAL methods.
</details>
<details>
<summary>摘要</summary>
弱类超级视频动作地标（WSTAL）目标是使用视频级标签来地标视频中的动作。 DESPITE recent advances, existing methods mainly follow a localization-by-classification pipeline, which only utilizes limited contextual information. As a result, the model may lack a comprehensive understanding (e.g., appearance and temporal structure) of various action patterns, leading to ambiguity in classification learning and temporal localization. Our work addresses this issue from a novel perspective by exploring and exploiting the cross-video contextual knowledge within the dataset to recover the dataset-level semantic structure of action instances via weak labels only, thereby indirectly improving the holistic understanding of fine-grained action patterns and alleviating the aforementioned ambiguities. Specifically, we propose an end-to-end framework that includes a Robust Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge Summarization and Aggregation (GKSA) module. First, the RMGCL module explores the contrast and consistency of cross-video action features, assisting in learning more structured and compact embedding space, thus reducing ambiguity in classification learning. Further, the GKSA module is used to efficiently summarize and propagate the cross-video representative action knowledge in a learnable manner to promote holistic action patterns understanding, which in turn allows the generation of high-confidence pseudo-labels for self-learning, thus alleviating ambiguity in temporal localization. Extensive experiments on THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method outperforms the state-of-the-art methods and can be easily plugged into other WSTAL methods.
</details></li>
</ul>
<hr>
<h2 id="HR-Pro-Point-supervised-Temporal-Action-Localization-via-Hierarchical-Reliability-Propagation"><a href="#HR-Pro-Point-supervised-Temporal-Action-Localization-via-Hierarchical-Reliability-Propagation" class="headerlink" title="HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation"></a>HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12608">http://arxiv.org/abs/2308.12608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pipixin321/hr-pro">https://github.com/pipixin321/hr-pro</a></li>
<li>paper_authors: Huaxin Zhang, Xiang Wang, Xiaohao Xu, Zhiwu Qing, Changxin Gao, Nong Sang</li>
<li>for: 本研究目的是提出一种基于可靠性协议的 temporal action localization 方法，以提高 label-efficient learning 的性能。</li>
<li>methods: 本方法包括两个可靠性感知阶段：短 clip 级可靠性学习和实例级可靠性学习，两个阶段都会利用高 confidence 的点签注入进行可靠性传播。</li>
<li>results: 通过多级可靠性感知学习，我们得到了更可靠的 confidence 分布和更准确的 temporal 边界。我们的 HR-Pro 在多个挑战性 benchmark 上达到了状态的最佳性能，包括 THUMOS14 的平均 mAP 60.3%。<details>
<summary>Abstract</summary>
Point-supervised Temporal Action Localization (PSTAL) is an emerging research direction for label-efficient learning. However, current methods mainly focus on optimizing the network either at the snippet-level or the instance-level, neglecting the inherent reliability of point annotations at both levels. In this paper, we propose a Hierarchical Reliability Propagation (HR-Pro) framework, which consists of two reliability-aware stages: Snippet-level Discrimination Learning and Instance-level Completeness Learning, both stages explore the efficient propagation of high-confidence cues in point annotations. For snippet-level learning, we introduce an online-updated memory to store reliable snippet prototypes for each class. We then employ a Reliability-aware Attention Block to capture both intra-video and inter-video dependencies of snippets, resulting in more discriminative and robust snippet representation. For instance-level learning, we propose a point-based proposal generation approach as a means of connecting snippets and instances, which produces high-confidence proposals for further optimization at the instance level. Through multi-level reliability-aware learning, we obtain more reliable confidence scores and more accurate temporal boundaries of predicted proposals. Our HR-Pro achieves state-of-the-art performance on multiple challenging benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably, our HR-Pro largely surpasses all previous point-supervised methods, and even outperforms several competitive fully supervised methods. Code will be available at https://github.com/pipixin321/HR-Pro.
</details>
<details>
<summary>摘要</summary>
《点指导时间动作Localization（PSTAL）是一个emerging研究方向，它的目标是实现标签效率学习。然而，当前方法主要集中于网络优化，忽略了点级和实例级的可靠性。在这篇论文中，我们提出了一个层次可靠性传播（HR-Pro）框架，它包括两个可靠性感知阶段：幂级可靠性学习和实例级可靠性学习。两个阶段都是利用高信任点级别的缓存来进行可靠性传播。为幂级学习，我们引入了一个在线更新的内存，用于存储每个类型的可靠性缓存。然后，我们使用一个可靠性感知块来捕捉内视频和 между视频依赖关系，从而生成更加准确和稳定的幂级表示。为实例级学习，我们提出了一种基于点的提议生成方法，用于将幂级与实例相连接，从而生成高信任度的提议。通过多级可靠性感知学习，我们得到了更加可靠的信任分数和更加准确的时间边界。我们的HR-Pro在多个挑战性的benchmark上实现了state-of-the-art性能，其中包括THUMOS14的很出色的平均精度（mAP）60.3%。值得注意的是，我们的HR-Pro大大超过了所有前期点指导方法，并且甚至超过了一些高效的完全监督方法。代码将在https://github.com/pipixin321/HR-Pro中提供。
</details></li>
</ul>
<hr>
<h2 id="PoseSync-Robust-pose-based-video-synchronization"><a href="#PoseSync-Robust-pose-based-video-synchronization" class="headerlink" title="PoseSync: Robust pose based video synchronization"></a>PoseSync: Robust pose based video synchronization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12600">http://arxiv.org/abs/2308.12600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishit Javia, Falak Shah, Shivam Dave</li>
<li>for: 这篇论文是用于提出一个端到端管道，用于基于姿势进行视频同步。</li>
<li>methods: 该管道包括对图像中人体部分进行剪辑，然后使用姿势检测器对剪辑的图像进行姿势检测，最后使用动态时间扭曲（DTW）算法对姿势关键点之间的角度&#x2F;距离度量进行比较，从而实现一个可比静态图像的姿势匹配管道。</li>
<li>results: 该管道可以帮助在多个领域，如游戏表现评估、编舞或导引运动员等，进行比较和评估人体动作。<details>
<summary>Abstract</summary>
Pose based video sychronization can have applications in multiple domains such as gameplay performance evaluation, choreography or guiding athletes. The subject's actions could be compared and evaluated against those performed by professionals side by side. In this paper, we propose an end to end pipeline for synchronizing videos based on pose. The first step crops the region where the person present in the image followed by pose detection on the cropped image. This is followed by application of Dynamic Time Warping(DTW) on angle/ distance measures between the pose keypoints leading to a scale and shift invariant pose matching pipeline.
</details>
<details>
<summary>摘要</summary>
pose基于视频同步可以在多个领域有应用，如游戏性能评估、编舞或引导运动员。将主体的动作与专业人员的动作进行比较和评估。在这篇论文中，我们提出了基于pose的视频同步管道的终端到终点解决方案。首先，将图像中人物的区域裁剪，然后进行pose检测。接着，对于裁剪后的图像，应用动态时间扩展(DTW)来计算pose关键点之间的角度/距离度量，从而实现一个可以快速匹配pose的缩放和平移不敏感管道。
</details></li>
</ul>
<hr>
<h2 id="Logic-induced-Diagnostic-Reasoning-for-Semi-supervised-Semantic-Segmentation"><a href="#Logic-induced-Diagnostic-Reasoning-for-Semi-supervised-Semantic-Segmentation" class="headerlink" title="Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation"></a>Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12595">http://arxiv.org/abs/2308.12595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liang, Wenguan Wang, Jiaxu Miao, Yi Yang</li>
<li>for: 提高 semi-supervised semantic segmentation 的精度，使用 pseudo labeling 补做了有限的标注数据，忽略了 semantic concept 之间的关系知识。</li>
<li>methods: 提出了 LogicDiag，一种基于神经逻辑学习框架的新方法，利用 pseudo label 中的冲突，通过逻辑检查和诊断，纠正 pseudo label，从而缓解 error accumulation 问题。</li>
<li>results: 在三个标准 semi-supervised semantic segmentation 测试集上进行了广泛的实验，证明了 LogicDiag 的有效性和通用性。此外，LogicDiag 还探讨了将符号逻辑reasoning integrate 到 prevailing 的统计学、神经网络学习方法中的可能性。<details>
<summary>Abstract</summary>
Recent advances in semi-supervised semantic segmentation have been heavily reliant on pseudo labeling to compensate for limited labeled data, disregarding the valuable relational knowledge among semantic concepts. To bridge this gap, we devise LogicDiag, a brand new neural-logic semi-supervised learning framework. Our key insight is that conflicts within pseudo labels, identified through symbolic knowledge, can serve as strong yet commonly ignored learning signals. LogicDiag resolves such conflicts via reasoning with logic-induced diagnoses, enabling the recovery of (potentially) erroneous pseudo labels, ultimately alleviating the notorious error accumulation problem. We showcase the practical application of LogicDiag in the data-hungry segmentation scenario, where we formalize the structured abstraction of semantic concepts as a set of logic rules. Extensive experiments on three standard semi-supervised semantic segmentation benchmarks demonstrate the effectiveness and generality of LogicDiag. Moreover, LogicDiag highlights the promising opportunities arising from the systematic integration of symbolic reasoning into the prevalent statistical, neural learning approaches.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近期 semi-supervised semantic segmentation 领域的进步都受到了 pseudo labeling 的限制，忽视了 semantic concepts 之间的 valuabe relational knowledge。为了bridging这个 gap，我们提出 LogicDiag，一种全新的 neural-logic  semi-supervised learning 框架。我们的关键发现是，在 pseudo labels 中的 conflicts，可以作为强大 yet 常被忽略的学习信号。LogicDiag 通过逻辑检查，解决这些 conflicts，使得可以恢复 (可能) 错误的 pseudo labels，从而缓解 error accumulation 问题。我们在数据充沛的 segmentation enario 中实现了 LogicDiag，并形式化 semantic concepts 的抽象结构为逻辑规则。我们在三个标准 semi-supervised semantic segmentation benchmark 上进行了广泛的实验，证明 LogicDiag 的效果和通用性。此外，LogicDiag 还展示了将逻辑推理系统化到 prevailing statistical, neural learning approaches 中的推动力。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Learning-of-Implicit-Shape-Representation-with-Dense-Correspondence-for-Deformable-Objects"><a href="#Self-supervised-Learning-of-Implicit-Shape-Representation-with-Dense-Correspondence-for-Deformable-Objects" class="headerlink" title="Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects"></a>Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12590">http://arxiv.org/abs/2308.12590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baowen Zhang, Jiahe Li, Xiaoming Deng, Yinda Zhang, Cuixia Ma, Hongan Wang</li>
<li>for: 学习3D形状表示的精细对应方法，用于扭形物体。</li>
<li>methods: 提出了一种新的自动标注方法，使用签名距离场来学习神经隐式形状表示，不需要骨架和皮肤纹理的假设。</li>
<li>results: 实验表明，该方法可以表示大幅扭形的形状，并且可以支持文本传输和形状编辑等应用，性能竞争力强。Here is the summary in English for reference:</li>
<li>for: Learning 3D shape representation with dense correspondence for deformable objects.</li>
<li>methods: Propose a novel self-supervised approach to learn neural implicit shape representation, which does not require prior knowledge of skeleton and skinning weight.</li>
<li>results: Experimental results show that the method can represent shapes with large deformations and support applications such as texture transfer and shape editing with competitive performance.<details>
<summary>Abstract</summary>
Learning 3D shape representation with dense correspondence for deformable objects is a fundamental problem in computer vision. Existing approaches often need additional annotations of specific semantic domain, e.g., skeleton poses for human bodies or animals, which require extra annotation effort and suffer from error accumulation, and they are limited to specific domain. In this paper, we propose a novel self-supervised approach to learn neural implicit shape representation for deformable objects, which can represent shapes with a template shape and dense correspondence in 3D. Our method does not require the priors of skeleton and skinning weight, and only requires a collection of shapes represented in signed distance fields. To handle the large deformation, we constrain the learned template shape in the same latent space with the training shapes, design a new formulation of local rigid constraint that enforces rigid transformation in local region and addresses local reflection issue, and present a new hierarchical rigid constraint to reduce the ambiguity due to the joint learning of template shape and correspondences. Extensive experiments show that our model can represent shapes with large deformations. We also show that our shape representation can support two typical applications, such as texture transfer and shape editing, with competitive performance. The code and models are available at https://iscas3dv.github.io/deformshape
</details>
<details>
<summary>摘要</summary>
学习3D形状表示方法中的密集匹配问题是计算机视觉的基本问题。现有的方法经常需要特定的 semantic 领域的更多注释，例如人体或动物的skeleton 姿势，这会增加注释努力并受到错误堆积的限制，同时它们只适用于特定的领域。在这篇论文中，我们提出了一种新的自助学习方法，用于学习神经凝聚形状表示方法，可以在3D中表示形状。我们的方法不需要预先知道skeleton和皮肤粘性的积分，只需要一个包含形状的signed distance fields。为了处理大幅度的变形，我们将学习的模板形状固定在同一个隐藏空间中，并设计了一种新的本地刚性约束，以便在本地区域中强制刚性变换，解决本地反射问题。此外，我们还提出了一种新的层次刚性约束，以减少由模板形状和匹配的共同学习所导致的模糊性。广泛的实验表明我们的模型可以表示大幅度的变形。此外，我们还证明了我们的形状表示可以支持两种典型的应用，例如纹理传输和形状编辑，并且与竞争性表现。代码和模型可以在https://iscas3dv.github.io/deformshape 上获取。
</details></li>
</ul>
<hr>
<h2 id="Grounded-Entity-Landmark-Adaptive-Pre-training-for-Vision-and-Language-Navigation"><a href="#Grounded-Entity-Landmark-Adaptive-Pre-training-for-Vision-and-Language-Navigation" class="headerlink" title="Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation"></a>Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12587">http://arxiv.org/abs/2308.12587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csir1996/vln-gela">https://github.com/csir1996/vln-gela</a></li>
<li>paper_authors: Yibo Cui, Liang Xie, Yakun Zhang, Meishan Zhang, Ye Yan, Erwei Yin</li>
<li>for: 本研究的目的是解决视觉语言导航（VLN）中的跨模态Alignment问题。</li>
<li>methods: 我们提出了一种新的Grounded Entity-Landmark Adaptive（GELA）预训练方法，通过引入基于实体和Landmark的 annotated数据（GEL-R2R），并采用三种基于实体和Landmark的适应预训练目标来强制学习细致的跨模态Alignment。</li>
<li>results: 我们的GELA模型在两个下游任务上（R2R和CVDN）得到了状态级 результа们，证明了其效果和普适性。<details>
<summary>Abstract</summary>
Cross-modal alignment is one key challenge for Vision-and-Language Navigation (VLN). Most existing studies concentrate on mapping the global instruction or single sub-instruction to the corresponding trajectory. However, another critical problem of achieving fine-grained alignment at the entity level is seldom considered. To address this problem, we propose a novel Grounded Entity-Landmark Adaptive (GELA) pre-training paradigm for VLN tasks. To achieve the adaptive pre-training paradigm, we first introduce grounded entity-landmark human annotations into the Room-to-Room (R2R) dataset, named GEL-R2R. Additionally, we adopt three grounded entity-landmark adaptive pre-training objectives: 1) entity phrase prediction, 2) landmark bounding box prediction, and 3) entity-landmark semantic alignment, which explicitly supervise the learning of fine-grained cross-modal alignment between entity phrases and environment landmarks. Finally, we validate our model on two downstream benchmarks: VLN with descriptive instructions (R2R) and dialogue instructions (CVDN). The comprehensive experiments show that our GELA model achieves state-of-the-art results on both tasks, demonstrating its effectiveness and generalizability.
</details>
<details>
<summary>摘要</summary>
cross-modalAlignment是Vision-and-Language Navigation（VLN）中一个关键挑战。大多数现有研究专注于将全球指令或单一子指令映射到相应的路径上。然而，另一个重要的问题是实现细部对齐，即在实体水平上进行精确的对齐。为了解决这个问题，我们提出了一个新的Grounded Entity-Landmark Adaptive（GELA）预训方法 дляVLN任务。为了实现这个预训方法，我们首先将固有的实体-Landmark人工注释添加到Room-to-Room（R2R） dataset中，名为GEL-R2R。其次，我们采用三种固有的实体-Landmark适应预训练目标：1）实体短语预测，2）Landmark bounding box预测，和3）实体-Landmark语义对齐，这些目标直接监督学习跨模态的精确对齐。最后，我们 validate our model on two downstream benchmarks：VLN with descriptive instructions (R2R)和 dialogue instructions (CVDN)。弹性实验结果显示，我们的GELA模型在两个任务上均 achieve state-of-the-art results，证明其有效性和普遍性。
</details></li>
</ul>
<hr>
<h2 id="LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data"><a href="#LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data" class="headerlink" title="LORD: Leveraging Open-Set Recognition with Unknown Data"></a>LORD: Leveraging Open-Set Recognition with Unknown Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12584">http://arxiv.org/abs/2308.12584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Koch, Christian Riess, Thomas Köhler</li>
<li>for: 这个论文的目的是如何处理未知数据，以便在部署过程中更好地进行分类。</li>
<li>methods: 这篇论文使用了一种名为LORD的框架，该框架在分类器训练过程中直接模型了开放空间，并提供了一系列可靠的评估方法。</li>
<li>results: 经过对多种评估协议的测试，这篇论文表明了在未知数据中进行分类时的改进表现，并且通过使用mixup作为数据生成技术，减轻了依赖于大量和昂贵的背景数据的问题。<details>
<summary>Abstract</summary>
Handling entirely unknown data is a challenge for any deployed classifier. Classification models are typically trained on a static pre-defined dataset and are kept in the dark for the open unassigned feature space. As a result, they struggle to deal with out-of-distribution data during inference. Addressing this task on the class-level is termed open-set recognition (OSR). However, most OSR methods are inherently limited, as they train closed-set classifiers and only adapt the downstream predictions to OSR. This work presents LORD, a framework to Leverage Open-set Recognition by exploiting unknown Data. LORD explicitly models open space during classifier training and provides a systematic evaluation for such approaches. We identify three model-agnostic training strategies that exploit background data and applied them to well-established classifiers. Due to LORD's extensive evaluation protocol, we consistently demonstrate improved recognition of unknown data. The benchmarks facilitate in-depth analysis across various requirement levels. To mitigate dependency on extensive and costly background datasets, we explore mixup as an off-the-shelf data generation technique. Our experiments highlight mixup's effectiveness as a substitute for background datasets. Lightweight constraints on mixup synthesis further improve OSR performance.
</details>
<details>
<summary>摘要</summary>
处理完全未知数据是任何部署类фика器的挑战。类фика器通常是在静态预先定义的数据集上训练，因此在推理过程中难以处理外部不确定数据。为解决这个问题，我们提出了开放集 recognition（OSR）技术。然而，大多数OSR方法都受限于它们只是将关闭集类фика器 retrained，并且只是在推理过程中适应OSR。本文介绍了LORD框架，它可以利用未知数据进行开放集 recognition。LORD在类ifica器训练过程中直接模型开放空间，并提供了系统的评估方法。我们identified三种模型无关的训练策略，并应用这些策略到了已知的类ifica器上。由于LORD的广泛的评估协议，我们在不同的需求水平上 consistently 示出了未知数据的更好的识别。这些标准化的协议为我们进行了深入的分析。为了减少依赖于广泛和昂贵的背景数据集，我们探索了mixup作为一种可用的数据生成技术。我们的实验表明，mixup是一种有效的替代方案。在进一步提高OSR性能的同时，我们还提出了一些轻量级的约束来限制mixup的生成。
</details></li>
</ul>
<hr>
<h2 id="StreamMapNet-Streaming-Mapping-Network-for-Vectorized-Online-HD-Map-Construction"><a href="#StreamMapNet-Streaming-Mapping-Network-for-Vectorized-Online-HD-Map-Construction" class="headerlink" title="StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction"></a>StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12570">http://arxiv.org/abs/2308.12570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyuan Yuan, Yicheng Liu, Yue Wang, Yilun Wang, Hang Zhao</li>
<li>for: 高清晰地图是自动驾驶系统的关键 Component， StreamMapNet 提供了一种新的在线地图生成管线，可以处理长串 temporal 信息，提高了稳定性和性能。</li>
<li>methods: StreamMapNet 使用多点注意力和时间信息来建立大范围的本地高清晰地图，并且可以处理复杂的场景，如 occlusion。</li>
<li>results: StreamMapNet 在所有设置下都与现有方法进行比较，表现出色，并且可以在 $14.2$ FPS 的在线推理速度下保持稳定性和高性能。<details>
<summary>Abstract</summary>
High-Definition (HD) maps are essential for the safety of autonomous driving systems. While existing techniques employ camera images and onboard sensors to generate vectorized high-precision maps, they are constrained by their reliance on single-frame input. This approach limits their stability and performance in complex scenarios such as occlusions, largely due to the absence of temporal information. Moreover, their performance diminishes when applied to broader perception ranges. In this paper, we present StreamMapNet, a novel online mapping pipeline adept at long-sequence temporal modeling of videos. StreamMapNet employs multi-point attention and temporal information which empowers the construction of large-range local HD maps with high stability and further addresses the limitations of existing methods. Furthermore, we critically examine widely used online HD Map construction benchmark and datasets, Argoverse2 and nuScenes, revealing significant bias in the existing evaluation protocols. We propose to resplit the benchmarks according to geographical spans, promoting fair and precise evaluations. Experimental results validate that StreamMapNet significantly outperforms existing methods across all settings while maintaining an online inference speed of $14.2$ FPS.
</details>
<details>
<summary>摘要</summary>
高清定义（HD）地图是自动驾驶系统的关键。现有技术使用摄像头图像和车辆上的感知器来生成 вектор化高精度地图，但这些技术受到单帧输入的限制，导致它们在复杂的情况下表现不稳定，主要是因为缺乏时间信息。此外，它们在扩大观察范围时表现下降。在这篇论文中，我们提出了StreamMapNet，一种新的在线地图生成管道，可以长时间序列模型视频。StreamMapNet使用多点注意力和时间信息，使得在大范围本地高清定义地图的建构中具有高稳定性，并解决了现有方法的局限性。此外，我们严格检查了 Argoverse2 和 nuScenes 等在线 HD 地图建构标准和数据集，发现这些标准存在偏见。我们提议将标准按地理范围重新分割，以便更公正和精确的评估。实验结果表明，StreamMapNet 在所有设置下与现有方法进行比较，并且保持在线推理速度为14.2帧/秒。
</details></li>
</ul>
<hr>
<h2 id="NOVA-NOvel-View-Augmentation-for-Neural-Composition-of-Dynamic-Objects"><a href="#NOVA-NOvel-View-Augmentation-for-Neural-Composition-of-Dynamic-Objects" class="headerlink" title="NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects"></a>NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12560">http://arxiv.org/abs/2308.12560</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dakshitagrawal/nova">https://github.com/dakshitagrawal/nova</a></li>
<li>paper_authors: Dakshit Agrawal, Jiajie Xu, Siva Karthik Mustikovela, Ioannis Gkioulekas, Ashish Shrivastava, Yuning Chai</li>
<li>for:  trains NeRFs for photo-realistic 3D composition of dynamic objects in a static scene</li>
<li>methods:  uses a novel-view augmentation (NOVA) strategy</li>
<li>results:  reduces blending artifacts, achieves comparable PSNR without additional ground truth modalities, and provides ease, flexibility, and scalability in neural composition.<details>
<summary>Abstract</summary>
We propose a novel-view augmentation (NOVA) strategy to train NeRFs for photo-realistic 3D composition of dynamic objects in a static scene. Compared to prior work, our framework significantly reduces blending artifacts when inserting multiple dynamic objects into a 3D scene at novel views and times; achieves comparable PSNR without the need for additional ground truth modalities like optical flow; and overall provides ease, flexibility, and scalability in neural composition. Our codebase is on GitHub.
</details>
<details>
<summary>摘要</summary>
我们提出一种新视图增强策略（NOVA），用于在静止场景中使用神经网络组合动态对象的3D组合。相比之前的工作，我们的框架可以在新视图和时间插入多个动态对象时减少融合 artifacts，达到相同的PSNR，而不需要额外的真实流动模式 like 光学流体；同时提供了更容易、灵活和可扩展的神经组合。我们的代码库在 GitHub 上。
</details></li>
</ul>
<hr>
<h2 id="Hyperbolic-Audio-visual-Zero-shot-Learning"><a href="#Hyperbolic-Audio-visual-Zero-shot-Learning" class="headerlink" title="Hyperbolic Audio-visual Zero-shot Learning"></a>Hyperbolic Audio-visual Zero-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12558">http://arxiv.org/abs/2308.12558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Hong, Zeeshan Hayder, Junlin Han, Pengfei Fang, Mehrtash Harandi, Lars Petersson<br>for: 这个论文的目的是探讨采用几何学变换来实现零shot学习，以便更好地处理具有复杂层次结构的数据。methods: 该方法使用了一种新的损失函数，该损失函数将视频和音频特征在几何空间进行交叉模块对齐。此外，该方法还 explore了使用多个自适应几何 curvature来进行几何投影。results: 实验结果表明，我们的提议的几何方法在三个数据集上（VGGSound-GZSL、UCF-GZSL和ActivityNet-GZSL）实现了预测值的大约3.0%、7.0%和5.3%的提升，相对于现有的最佳方法。<details>
<summary>Abstract</summary>
Audio-visual zero-shot learning aims to classify samples consisting of a pair of corresponding audio and video sequences from classes that are not present during training. An analysis of the audio-visual data reveals a large degree of hyperbolicity, indicating the potential benefit of using a hyperbolic transformation to achieve curvature-aware geometric learning, with the aim of exploring more complex hierarchical data structures for this task. The proposed approach employs a novel loss function that incorporates cross-modality alignment between video and audio features in the hyperbolic space. Additionally, we explore the use of multiple adaptive curvatures for hyperbolic projections. The experimental results on this very challenging task demonstrate that our proposed hyperbolic approach for zero-shot learning outperforms the SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%, respectively.
</details>
<details>
<summary>摘要</summary>
audio-visual zero-shot learning 目标是将相对应的音频和视频序列分类为在训练中不存在的类。 数据分析显示 audio-visual 数据具有大量的抽象性，表明可能通过使用抽象变换来实现曲线意识的几何学学习，以探索更复杂的层次数据结构。 我们提议的方法使用一种新的损失函数，该函数包含视频和音频特征在抽象空间的交叉模块Alignment。 此外，我们还探讨了多个自适应曲线的投影。 实验结果表明，我们的提议的抽象方法在三个数据集上（VGGSound-GZSL、UCF-GZSL 和 ActivityNet-GZSL）实现了harmonic mean（HM）提高约3.0%, 7.0%, 5.3%，分别。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Models-for-Facial-Emotion-Recognition-in-Children"><a href="#Hybrid-Models-for-Facial-Emotion-Recognition-in-Children" class="headerlink" title="Hybrid Models for Facial Emotion Recognition in Children"></a>Hybrid Models for Facial Emotion Recognition in Children</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12547">http://arxiv.org/abs/2308.12547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Zimmer, Marcos Sobral, Helio Azevedo</li>
<li>for: 这个研究旨在使用情绪识别技术来帮助儿童心理师在远程 робо扮SESSION中进行儿童治疗。</li>
<li>methods: 该研究使用了Embodied Conversational Agents（ECA）作为中间工具，以帮助专业人员与儿童进行交流，特别是对于患有注意力不足过动症（ADHD）、自闭症 спектル（ASD）或者因为战争、自然灾害或其他原因而无法进行面对面会议的儿童。 情绪识别技术作为反馈工具，能够帮助心理师更好地了解儿童的情绪状态。</li>
<li>results: 该研究首先对儿童情绪识别领域进行了文献综述，并对当前社区广泛使用的算法和数据集进行了初步的检视。然后，通过使用 dense optical flow features 技术，提高了儿童情绪识别的精度。 HybridCNNFusion 模型由一个 Convolutional Neural Network 和两个中间特征 fusion 组成，可以更好地识别儿童的情绪。最终，该研究使用了巴西儿童的数据集，并取得了初步的情绪识别结果。<details>
<summary>Abstract</summary>
This paper focuses on the use of emotion recognition techniques to assist psychologists in performing children's therapy through remotely robot operated sessions. In the field of psychology, the use of agent-mediated therapy is growing increasingly given recent advances in robotics and computer science. Specifically, the use of Embodied Conversational Agents (ECA) as an intermediary tool can help professionals connect with children who face social challenges such as Attention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorder (ASD) or even who are physically unavailable due to being in regions of armed conflict, natural disasters, or other circumstances. In this context, emotion recognition represents an important feedback for the psychotherapist. In this article, we initially present the result of a bibliographical research associated with emotion recognition in children. This research revealed an initial overview on algorithms and datasets widely used by the community. Then, based on the analysis carried out on the results of the bibliographical research, we used the technique of dense optical flow features to improve the ability of identifying emotions in children in uncontrolled environments. From the output of a hybrid model of Convolutional Neural Network, two intermediary features are fused before being processed by a final classifier. The proposed architecture was called HybridCNNFusion. Finally, we present the initial results achieved in the recognition of children's emotions using a dataset of Brazilian children.
</details>
<details>
<summary>摘要</summary>
In this article, we first present the results of a bibliographical research on emotion recognition in children. This research provided an initial overview of the algorithms and datasets commonly used by the community. Based on the analysis of the results, we improved the ability to identify emotions in children in uncontrolled environments using the technique of dense optical flow features. A hybrid model of Convolutional Neural Network (CNN) was used, which fused two intermediary features before being processed by a final classifier. The proposed architecture was called HybridCNNFusion.Finally, we present the initial results achieved in recognizing children's emotions using a dataset of Brazilian children.
</details></li>
</ul>
<hr>
<h2 id="Mutual-Guided-Dynamic-Network-for-Image-Fusion"><a href="#Mutual-Guided-Dynamic-Network-for-Image-Fusion" class="headerlink" title="Mutual-Guided Dynamic Network for Image Fusion"></a>Mutual-Guided Dynamic Network for Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12538">http://arxiv.org/abs/2308.12538</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanys-dar/mgdn">https://github.com/guanys-dar/mgdn</a></li>
<li>paper_authors: Yuanshen Guan, Ruikang Xu, Mingde Yao, Lizhi Wang, Zhiwei Xiong<br>for:This paper proposes a novel mutual-guided dynamic network (MGDN) for image fusion, which aims to generate high-quality images from multiple inputs captured under varying conditions.methods:The proposed MGDN method utilizes a mutual-guided dynamic filter (MGDF) for adaptive feature extraction, which incorporates additional guidance from different inputs and generates spatial-variant kernels for different locations. Additionally, a parallel feature fusion (PFF) module is introduced to effectively fuse local and global information of the extracted features.results:Experimental results on five benchmark datasets demonstrate that the proposed MGDN method outperforms existing methods on four image fusion tasks, showcasing its effectiveness in preserving complementary information while filtering out irrelevant information for the fused result.<details>
<summary>Abstract</summary>
Image fusion aims to generate a high-quality image from multiple images captured under varying conditions. The key problem of this task is to preserve complementary information while filtering out irrelevant information for the fused result. However, existing methods address this problem by leveraging static convolutional neural networks (CNNs), suffering two inherent limitations during feature extraction, i.e., being unable to handle spatial-variant contents and lacking guidance from multiple inputs. In this paper, we propose a novel mutual-guided dynamic network (MGDN) for image fusion, which allows for effective information utilization across different locations and inputs. Specifically, we design a mutual-guided dynamic filter (MGDF) for adaptive feature extraction, composed of a mutual-guided cross-attention (MGCA) module and a dynamic filter predictor, where the former incorporates additional guidance from different inputs and the latter generates spatial-variant kernels for different locations. In addition, we introduce a parallel feature fusion (PFF) module to effectively fuse local and global information of the extracted features. To further reduce the redundancy among the extracted features while simultaneously preserving their shared structural information, we devise a novel loss function that combines the minimization of normalized mutual information (NMI) with an estimated gradient mask. Experimental results on five benchmark datasets demonstrate that our proposed method outperforms existing methods on four image fusion tasks. The code and model are publicly available at: https://github.com/Guanys-dar/MGDN.
</details>
<details>
<summary>摘要</summary>
图像融合目标是生成多个图像下 varying 条件下的高质量图像。该任务的关键问题是保留 complementary information 而过滤 irrelevant information 以生成融合结果。然而，现有方法通过利用静态 convolutional neural networks (CNNs) 解决这个问题，具有两个内在的限制：无法处理空间 variant 内容和缺乏多输入的指导。在这篇论文中，我们提出了一种新的 mutual-guided dynamic network (MGDN)  для图像融合，允许有效地利用不同的位置和输入中的信息。具体来说，我们设计了一种 mutual-guided cross-attention (MGCA) 模块和一种动态滤波预测器，其中前者包含不同输入的额外指导，而后者生成不同位置的空间variant 滤波器。此外，我们引入了一种 parallel feature fusion (PFF) 模块，以有效地融合本地和全局的特征信息。为了进一步减少提取的特征信息之间的重复，我们设计了一种新的损失函数，它将 normalized mutual information (NMI) 的最小化与一个估计的梯度掩码相结合。实验结果表明，我们的提出的方法在五个 benchmark 数据集上比现有方法在四个图像融合任务上表现出色。代码和模型可以在：https://github.com/Guanys-dar/MGDN 上获取。
</details></li>
</ul>
<hr>
<h2 id="HuBo-VLM-Unified-Vision-Language-Model-designed-for-HUman-roBOt-interaction-tasks"><a href="#HuBo-VLM-Unified-Vision-Language-Model-designed-for-HUman-roBOt-interaction-tasks" class="headerlink" title="HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks"></a>HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12537">http://arxiv.org/abs/2308.12537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dzcgaara/HuBo-VLM">https://github.com/dzcgaara/HuBo-VLM</a></li>
<li>paper_authors: Zichao Dong, Weikun Zhang, Xufeng Huang, Hang Ji, Xin Zhan, Junbo Chen</li>
<li>for: 这个论文旨在提出一种基于 трансформа器视觉语言模型的人机交互模型，以便帮助机器人理解人类的自然语言指令并完成相关任务。</li>
<li>methods: 该论文提出了一种基于 transformer 视觉语言模型的人机交互模型，包括对象检测和视觉定位。</li>
<li>results: EXTENSIVE EXPERIMENTS ON THE TALK2CAR BENCHMARK DEMONSTRATE THE EFFECTIVENESS OF THE PROPOSED APPROACH。<details>
<summary>Abstract</summary>
Human robot interaction is an exciting task, which aimed to guide robots following instructions from human. Since huge gap lies between human natural language and machine codes, end to end human robot interaction models is fair challenging. Further, visual information receiving from sensors of robot is also a hard language for robot to perceive. In this work, HuBo-VLM is proposed to tackle perception tasks associated with human robot interaction including object detection and visual grounding by a unified transformer based vision language model. Extensive experiments on the Talk2Car benchmark demonstrate the effectiveness of our approach. Code would be publicly available in https://github.com/dzcgaara/HuBo-VLM.
</details>
<details>
<summary>摘要</summary>
人机交互是一项有趣的任务，旨在使 робоッツ按照人类的指令行动。由于人类自然语言与机器代码之间存在巨大的差距，结束到端人机交互模型是非常困难的。此外，机器人的感知器也是一种困难的语言，对机器人来说很难理解。在这项工作中，我们提出了一种解决人机交互相关的感知任务，包括物体检测和视觉定位，的方法。我们使用了一种基于转换器的视Language模型，并进行了广泛的实验，证明了我们的方法的有效性。代码将在https://github.com/dzcgaara/HuBo-VLM上公开。
</details></li>
</ul>
<hr>
<h2 id="SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression"><a href="#SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression" class="headerlink" title="SCP: Spherical-Coordinate-based Learned Point Cloud Compression"></a>SCP: Spherical-Coordinate-based Learned Point Cloud Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12535">http://arxiv.org/abs/2308.12535</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoao-kddi/SCP">https://github.com/luoao-kddi/SCP</a></li>
<li>paper_authors: Ao Luo, Linxin Song, Keisuke Nonaka, Kyohei Unno, Heming Sun, Masayuki Goto, Jiro Katto</li>
<li>for: 本研究targets learned point cloud compression, particularly for spinning LiDAR point clouds with circular shapes and azimuthal angle invariance features.</li>
<li>methods: 该方法基于Spherical-Coordinate-based learned Point cloud compression (SCP)，利用了上述特征，并提出了多级Octree来降低远区域重建误差。</li>
<li>results: 实验结果显示，SCP比前一代方法提高了29.14%的点到点PSNR BD-Rate。<details>
<summary>Abstract</summary>
In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, the spinning LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud compression (SCP), designed to leverage the aforementioned features fully. Additionally, we propose a multi-level Octree for SCP to mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree. SCP exhibits excellent universality, making it applicable to various learned point cloud compression techniques. Experimental results demonstrate that SCP surpasses previous state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.
</details>
<details>
<summary>摘要</summary>
近年来，学习点云压缩任务得到了更多的关注。一种重要的点云类型是旋转雷达点云，由旋转雷达在车辆上生成。这个过程会生成很多圆形和方位角协variance特征，但这些特征在前一代方法中受到了忽略。在这篇论文中，我们介绍了一种模型无关的方法called Spherical-Coordinate-based learned Point cloud compression (SCP)，旨在利用上述特征。此外，我们提议了一种多级 Octree 来 mitigate SCP 的重建误差。SCP 具有优秀的通用性，可以应用于多种学习点云压缩技术。实验结果表明，SCP 可以比前一代方法提高点-到-点 PSNR BD-Rate 的最高提升率达29.14%。
</details></li>
</ul>
<hr>
<h2 id="Channel-and-Spatial-Relation-Propagation-Network-for-RGB-Thermal-Semantic-Segmentation"><a href="#Channel-and-Spatial-Relation-Propagation-Network-for-RGB-Thermal-Semantic-Segmentation" class="headerlink" title="Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation"></a>Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12534">http://arxiv.org/abs/2308.12534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikun Zhou, Shukun Wu, Guoqing Zhu, Hongpeng Wang, Zhenyu He</li>
<li>for: 这个论文的目的是提出一个 Channel and Spatial Relation-Propagation Network (CSRPNet)，用于RGB-T semantic segmentation，以利用两 modalities 之间的共同特征来提高 semantic segmentation 的精度。</li>
<li>methods: 这个论文使用了一个叫做 Channel and Spatial Relation-Propagation Network (CSRPNet) 的网络，它首先在通道和空间 dimension 进行了关系传递，以捕捉两 modalities 之间的共同特征。然后，它将一 modalities 的特征与另一 modalities 的入力特征进行了融合，以提高入力特征不受污染的问题。</li>
<li>results: 实验结果显示，CSRPNet 可以与现有的方法相比，在RGB-T semantic segmentation 中表现出色。<details>
<summary>Abstract</summary>
RGB-Thermal (RGB-T) semantic segmentation has shown great potential in handling low-light conditions where RGB-based segmentation is hindered by poor RGB imaging quality. The key to RGB-T semantic segmentation is to effectively leverage the complementarity nature of RGB and thermal images. Most existing algorithms fuse RGB and thermal information in feature space via concatenation, element-wise summation, or attention operations in either unidirectional enhancement or bidirectional aggregation manners. However, they usually overlook the modality gap between RGB and thermal images during feature fusion, resulting in modality-specific information from one modality contaminating the other. In this paper, we propose a Channel and Spatial Relation-Propagation Network (CSRPNet) for RGB-T semantic segmentation, which propagates only modality-shared information across different modalities and alleviates the modality-specific information contamination issue. Our CSRPNet first performs relation-propagation in channel and spatial dimensions to capture the modality-shared features from the RGB and thermal features. CSRPNet then aggregates the modality-shared features captured from one modality with the input feature from the other modality to enhance the input feature without the contamination issue. While being fused together, the enhanced RGB and thermal features will be also fed into the subsequent RGB or thermal feature extraction layers for interactive feature fusion, respectively. We also introduce a dual-path cascaded feature refinement module that aggregates multi-layer features to produce two refined features for semantic and boundary prediction. Extensive experimental results demonstrate that CSRPNet performs favorably against state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks"><a href="#SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks" class="headerlink" title="SieveNet: Selecting Point-Based Features for Mesh Networks"></a>SieveNet: Selecting Point-Based Features for Mesh Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12530">http://arxiv.org/abs/2308.12530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sievenet/sievenet.github.io">https://github.com/sievenet/sievenet.github.io</a></li>
<li>paper_authors: Shengchao Yuan, Yishun Dou, Rui Shi, Bingbing Ni, Zhong Zheng</li>
<li>for: 提高3D计算机视觉和图形领域中的网格使用，解决网格的不规则结构限制现有神经网络体系中的应用。</li>
<li>methods: 提出了一种新的思路，即使用结构化网格结构和精度地理信息，从原始网格表面进行误差意识抽取点批量检测，从而兼顾规则结构和准确地理信息。</li>
<li>results: 经过广泛的实验研究，在分类和 segmentation 任务中，提出的 Sievenet 方法能够具有较高的效果和优势，不需要手动设计特征工程。<details>
<summary>Abstract</summary>
Meshes are widely used in 3D computer vision and graphics, but their irregular topology poses challenges in applying them to existing neural network architectures. Recent advances in mesh neural networks turn to remeshing and push the boundary of pioneer methods that solely take the raw meshes as input. Although the remeshing offers a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer. Comprehensive experimental results on classification and segmentation tasks well demonstrate the effectiveness and superiority of our method.
</details>
<details>
<summary>摘要</summary>
mesh 广泛应用于3D计算机视觉和图形领域，但它们的不规则结构会对现有神经网络架构的应用带来挑战。 recent advances in mesh neural networks have turned to remeshing and pushed the boundaries of pioneering methods that only take raw meshes as input. Although remeshing provides a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer. Comprehensive experimental results on classification and segmentation tasks well demonstrate the effectiveness and superiority of our method.Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China. The traditional Chinese form of the text would be slightly different.
</details></li>
</ul>
<hr>
<h2 id="Uniformly-Distributed-Category-Prototype-Guided-Vision-Language-Framework-for-Long-Tail-Recognition"><a href="#Uniformly-Distributed-Category-Prototype-Guided-Vision-Language-Framework-for-Long-Tail-Recognition" class="headerlink" title="Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition"></a>Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12522">http://arxiv.org/abs/2308.12522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siming Fu, Xiaoxuan He, Xinpeng Ding, Yuchen Cao, Hualiang Wang</li>
<li>for: 这个研究是为了解决长尾认知 task 中的类别偏度问题，特别是当训练数据具有类别差异时，模型会受到特定类别的扭曲。</li>
<li>methods: 我们提出了一个uniformly category prototype-guided vision-language框架，通过生成一些均匀分布在球体上的类别原型，将不同类别的特征扩展到这些原型上，使得特征空间内的分布变得均匀。此外，我们还提出了一个无关文本筛选和特征增强模组，让模型忽略无关的噪音文本，更加重视关键特征信息。</li>
<li>results: 我们的方法比前一代的视觉语言方法更好地适应长尾认知任务，并且实现了类别偏度问题的解决。具体来说，我们的方法在认知精度上比前一代的方法提高了大约20%，并且在长尾类别上保持了高度的稳定性。<details>
<summary>Abstract</summary>
Recently, large-scale pre-trained vision-language models have presented benefits for alleviating class imbalance in long-tailed recognition. However, the long-tailed data distribution can corrupt the representation space, where the distance between head and tail categories is much larger than the distance between two tail categories. This uneven feature space distribution causes the model to exhibit unclear and inseparable decision boundaries on the uniformly distributed test set, which lowers its performance. To address these challenges, we propose the uniformly category prototype-guided vision-language framework to effectively mitigate feature space bias caused by data imbalance. Especially, we generate a set of category prototypes uniformly distributed on a hypersphere. Category prototype-guided mechanism for image-text matching makes the features of different classes converge to these distinct and uniformly distributed category prototypes, which maintain a uniform distribution in the feature space, and improve class boundaries. Additionally, our proposed irrelevant text filtering and attribute enhancement module allows the model to ignore irrelevant noisy text and focus more on key attribute information, thereby enhancing the robustness of our framework. In the image recognition fine-tuning stage, to address the positive bias problem of the learnable classifier, we design the class feature prototype-guided classifier, which compensates for the performance of tail classes while maintaining the performance of head classes. Our method outperforms previous vision-language methods for long-tailed learning work by a large margin and achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
近期，大规模预训练视觉语言模型已经显示出了对长尾识别问题的缓解效果。然而，长尾数据分布可以损害模型的表征空间，导致模型在uniform测试集上展示不明确和不分化的决策边界，从而降低其性能。为解决这些挑战，我们提议使用 uniformly分布的类prototype来引导视觉语言框架，以有效地消除数据不均分带来的表径空间偏见。具体来说，我们生成了一组 uniformly分布在 hypersphere 上的类prototype。这些类prototype在图像文本匹配中 acted as a guide, making the features of different classes converge to these distinct and uniformly distributed category prototypes, thereby maintaining a uniform distribution in the feature space and improving class boundaries.此外，我们还提出了不相关文本过滤和特征增强模块，使模型忽略不相关的噪音文本，更加注重关键特征信息，从而提高了我们的框架的Robustness。在图像识别细化阶段，为了解决learnable classifier的正面偏好问题，我们设计了类feature prototype-guided类ifier，这种方法可以补偿尾类的性能，同时保持头类的性能。根据我们的实验结果，我们的方法在长尾学习任务上比前一代视觉语言方法出performanced by a large margin，达到了状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Transfer-Learning-for-Remote-Sensing-Image-Text-Retrieval"><a href="#Parameter-Efficient-Transfer-Learning-for-Remote-Sensing-Image-Text-Retrieval" class="headerlink" title="Parameter-Efficient Transfer Learning for Remote Sensing Image-Text Retrieval"></a>Parameter-Efficient Transfer Learning for Remote Sensing Image-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12509">http://arxiv.org/abs/2308.12509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yuan, Yang Zhan, Zhitong Xiong</li>
<li>for: 这研究旨在提出一种高效高可用的视言语传播学习方法，以便在实际应用中处理大量的远程感知数据。</li>
<li>methods: 该研究使用了CLIP模型作为预训练模型，并设计了一种多模态远程感知适配器以及一种混合多模态对比学习目标。此外，我们还提出了一种简单 yet有效的HMMC损失函数来解决高内模态相似性问题。</li>
<li>results: 我们的研究表明，使用PETL方法可以有效地传播视言语知识从自然领域到远程感知领域，并且可以大幅降低训练成本和环境影响。我们的模型只包含0.16M个训练参数，可以实现98.9%的参数减少，并且在 Retrieval 性能方面超过传统方法7-13%，与全 Fine-tuning 的性能相当或更好。<details>
<summary>Abstract</summary>
Vision-and-language pre-training (VLP) models have experienced a surge in popularity recently. By fine-tuning them on specific datasets, significant performance improvements have been observed in various tasks. However, full fine-tuning of VLP models not only consumes a significant amount of computational resources but also has a significant environmental impact. Moreover, as remote sensing (RS) data is constantly being updated, full fine-tuning may not be practical for real-world applications. To address this issue, in this work, we investigate the parameter-efficient transfer learning (PETL) method to effectively and efficiently transfer visual-language knowledge from the natural domain to the RS domain on the image-text retrieval task. To this end, we make the following contributions. 1) We construct a novel and sophisticated PETL framework for the RS image-text retrieval (RSITR) task, which includes the pretrained CLIP model, a multimodal remote sensing adapter, and a hybrid multi-modal contrastive (HMMC) learning objective; 2) To deal with the problem of high intra-modal similarity in RS data, we design a simple yet effective HMMC loss; 3) We provide comprehensive empirical studies for PETL-based RS image-text retrieval. Our results demonstrate that the proposed method is promising and of great potential for practical applications. 4) We benchmark extensive state-of-the-art PETL methods on the RSITR task. Our proposed model only contains 0.16M training parameters, which can achieve a parameter reduction of 98.9% compared to full fine-tuning, resulting in substantial savings in training costs. Our retrieval performance exceeds traditional methods by 7-13% and achieves comparable or better performance than full fine-tuning. This work can provide new ideas and useful insights for RS vision-language tasks.
</details>
<details>
<summary>摘要</summary>
Recently, vision-and-language pre-training (VLP) 模型在不同领域中得到了广泛的应用。通过特定数据集的精细调整，VLP模型在各种任务中表现出了显著的性能提升。然而，全量调整VLP模型不仅需要巨量的计算资源，还会对环境产生巨大的影响。此外，随着Remote Sensing（RS）数据不断更新，全量调整可能无法适应实际应用中的需求。为此，本文提出了参数有效传播学习（PETL）方法，以有效地和高效地将视觉语言知识从自然领域传播到RS领域中的图文检索任务上。为此，我们做了以下贡献：1. 我们建立了一个新的和复杂的PETL框架 дляRS图文检索任务，包括预训练的CLIP模型、多模态RS适配器和混合多模态对比（HMMC）学习目标；2. 为RS数据中高内模态相似性问题而设计了一个简单 yet effective的HMMC损失函数；3. 我们提供了RS图文检索的广泛的实验研究。我们的结果表明，我们提出的方法具有扎实的推荐和实际应用的潜在价值。4. 我们对现有的PETL方法进行了广泛的比较研究，并发现我们的提出的模型只需0.16M参数进行训练，相比涵盖所有模型，可以实现参数减少98.9%，减少训练成本。我们的检索性能高于传统方法7-13%，并且与全量调整的性能相当或更好。本文可以提供新的想法和有用的意见 дляRS视觉语言任务。
</details></li>
</ul>
<hr>
<h2 id="FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution"><a href="#FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution" class="headerlink" title="FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution"></a>FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12508">http://arxiv.org/abs/2308.12508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyue Jiao, Chongke Bi, Lu Yang</li>
<li>for: 提高流体动力学数据的空间和时间分辨率</li>
<li>methods: 基于嵌入型神经网络（FFEINR）和特征增强技术</li>
<li>results: 比较比特里利内插法更好的结果<details>
<summary>Abstract</summary>
Large-scale numerical simulations are capable of generating data up to terabytes or even petabytes. As a promising method of data reduction, super-resolution (SR) has been widely studied in the scientific visualization community. However, most of them are based on deep convolutional neural networks (CNNs) or generative adversarial networks (GANs) and the scale factor needs to be determined before constructing the network. As a result, a single training session only supports a fixed factor and has poor generalization ability. To address these problems, this paper proposes a Feature-Enhanced Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of flow field data. It can take full advantage of the implicit neural representation in terms of model structure and sampling resolution. The neural representation is based on a fully connected network with periodic activation functions, which enables us to obtain lightweight models. The learned continuous representation can decode the low-resolution flow field input data to arbitrary spatial and temporal resolutions, allowing for flexible upsampling. The training process of FFEINR is facilitated by introducing feature enhancements for the input layer, which complements the contextual information of the flow field.To demonstrate the effectiveness of the proposed method, a series of experiments are conducted on different datasets by setting different hyperparameters. The results show that FFEINR achieves significantly better results than the trilinear interpolation method.
</details>
<details>
<summary>摘要</summary>
大规模数值计算可以生成数据达到tera bytes甚至petabytes级别。作为数据压缩的承诺方法，超分辨率（SR）在科学视觉社区得到了广泛的研究。然而，大多数都基于深度卷积神经网络（CNN）或生成敌对网络（GAN），并且需要确定缩放因子之前建立网络。这意味着单个训练会话只支持固定因子，并且具有较差的泛化能力。为解决这些问题，本文提出了基于几何卷积神经网络的特征增强隐藏表示（FFEINR），用于空间时间超分辨率的流场数据。它可以完全利用隐藏表示的几何结构和采样分辨率来获得轻量级模型。学习的连续表示可以将低分辨率流场输入数据解码到任意空间和时间分辨率，以便灵活增加。为便于FFEINR的训练，我们引入了输入层的特征增强，以增强流场的上下文信息。为证明提案的效iveness，我们在不同的数据集上进行了一系列实验，并通过设置不同的超参数来评估结果。结果显示，FFEINR在比较方法中表现出了显著的优势。
</details></li>
</ul>
<hr>
<h2 id="DD-GCN-Directed-Diffusion-Graph-Convolutional-Network-for-Skeleton-based-Human-Action-Recognition"><a href="#DD-GCN-Directed-Diffusion-Graph-Convolutional-Network-for-Skeleton-based-Human-Action-Recognition" class="headerlink" title="DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition"></a>DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12501">http://arxiv.org/abs/2308.12501</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shiyin-lc/dd-gcn">https://github.com/shiyin-lc/dd-gcn</a></li>
<li>paper_authors: Chang Li, Qian Huang, Yingchi Mao</li>
<li>for: 这篇论文是为了提高skeleton-based human action recognition中的Graph Convolutional Networks（GCNs）性能而写的。</li>
<li>methods: 该论文使用了导向协同分布图（DD-GCN），它利用了建立导向分布图以实现动作模型化，并引入了活动分区策略来优化图 convolution kernels 的加权共享机制。此外，它还提出了空间时间同步编码器来嵌入同步空间时间 semantics。</li>
<li>results: 实验结果表明，该方法在三个公共数据集（NTU-RGB+D、NTU-RGB+D 120、NW-UCLA）上达到了当前最佳性能。<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) have been widely used in skeleton-based human action recognition. In GCN-based methods, the spatio-temporal graph is fundamental for capturing motion patterns. However, existing approaches ignore the physical dependency and synchronized spatio-temporal correlations between joints, which limits the representation capability of GCNs. To solve these problems, we construct the directed diffusion graph for action modeling and introduce the activity partition strategy to optimize the weight sharing mechanism of graph convolution kernels. In addition, we present the spatio-temporal synchronization encoder to embed synchronized spatio-temporal semantics. Finally, we propose Directed Diffusion Graph Convolutional Network (DD-GCN) for action recognition, and the experiments on three public datasets: NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art performance of our method.
</details>
<details>
<summary>摘要</summary>
格点图 neural network (GCN) 在人体动作识别中广泛应用。在 GCN 基本方法中，空间时间图是关键 для捕捉运动模式。然而，现有方法忽略了物理依赖性和同步空间时间相关性 между 关节，这限制了 GCN 的表示能力。为解决这些问题，我们构建了导向干扰图 для动作模型化，并引入活动分区策略来优化图像卷积核的加权共享机制。此外，我们提出了空间时间同步编码器，以嵌入同步空间时间 semantics。最后，我们提出了导向干扰图 convolutional neural network (DD-GCN)  для动作识别，并在三个公共数据集（NTU-RGB+D、NTU-RGB+D 120、NW-UCLA）上进行实验，其中表现出了当今最佳性能。
</details></li>
</ul>
<hr>
<h2 id="MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices"><a href="#MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices" class="headerlink" title="MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices"></a>MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12494">http://arxiv.org/abs/2308.12494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Chen, Ruiwen Zhen, Shuai Li, Xiaotian Li, Guanghui Wang</li>
<li>for:  restore high-quality images from degraded counterparts and improve the efficiency of image restoration models on mobile devices.</li>
<li>methods:  add more parameters to partial convolutions on FLOPs non-sensitive layers, apply partial depthwise convolution coupled with decoupling upsampling&#x2F;downsampling layers.</li>
<li>results:  decrease runtime by up to 13%, reduce the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets.Here is the text in Simplified Chinese:</li>
<li>for:  restore高品质的图像从损坏版本中，并提高移动设备上图像恢复模型的效率。</li>
<li>methods: 增加FLOPs非敏感层中的参数，应用部分深度卷积并与解解锁升降样例层。</li>
<li>results: 减少运行时间，减少参数数量，同时提高PSNR和SSIM在多个图像恢复数据集上。<details>
<summary>Abstract</summary>
Image restoration aims to restore high-quality images from degraded counterparts and has seen significant advancements through deep learning techniques. The technique has been widely applied to mobile devices for tasks such as mobile photography. Given the resource limitations on mobile devices, such as memory constraints and runtime requirements, the efficiency of models during deployment becomes paramount. Nevertheless, most previous works have primarily concentrated on analyzing the efficiency of single modules and improving them individually. This paper examines the efficiency across different layers. We propose a roadmap that can be applied to further accelerate image restoration models prior to deployment while simultaneously increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The roadmap first increases the model capacity by adding more parameters to partial convolutions on FLOPs non-sensitive layers. Then, it applies partial depthwise convolution coupled with decoupling upsampling/downsampling layers to accelerate the model speed. Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets. Source Code of our method is available at \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.
</details>
<details>
<summary>摘要</summary>
Image restoration aims to restore high-quality images from degraded counterparts and has seen significant advancements through deep learning techniques. The technique has been widely applied to mobile devices for tasks such as mobile photography. Given the resource limitations on mobile devices, such as memory constraints and runtime requirements, the efficiency of models during deployment becomes paramount. Nevertheless, most previous works have primarily concentrated on analyzing the efficiency of single modules and improving them individually. This paper examines the efficiency across different layers. We propose a roadmap that can be applied to further accelerate image restoration models prior to deployment while simultaneously increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The roadmap first increases the model capacity by adding more parameters to partial convolutions on FLOPs non-sensitive layers. Then, it applies partial depthwise convolution coupled with decoupling upsampling/downsampling layers to accelerate the model speed. Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets. 源代码我们的方法可以在 \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA} 上 obtain.
</details></li>
</ul>
<hr>
<h2 id="Diffuse-Attend-and-Segment-Unsupervised-Zero-Shot-Segmentation-using-Stable-Diffusion"><a href="#Diffuse-Attend-and-Segment-Unsupervised-Zero-Shot-Segmentation-using-Stable-Diffusion" class="headerlink" title="Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion"></a>Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12469">http://arxiv.org/abs/2308.12469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, Mar Gonzalez-Franco</li>
<li>for: 实现零shot Segmentation的高质量分割mask，解决了计算机视觉领域中的基本问题。</li>
<li>methods: 利用稳定扩散模型的自我注意层，通过衡量KL差值 среди注意图来 merge them into valid segmentation masks。</li>
<li>results: 在COCO-Stuff-27上，我们的方法超过了先前的无supervised zero-shot SOTA方法，净误率提高26%， Mean IoU提高17%。<details>
<summary>Abstract</summary>
Producing quality segmentation masks for images is a fundamental problem in computer vision. Recent research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU.
</details>
<details>
<summary>摘要</summary>
生成高质量的图像分割 маSK是计算机视觉的基本问题。 latest research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU.Here's the text with traditional Chinese characters:生成高质量的图像分割mask是计算机视觉的基本问题。 latest research has explored large-scale supervised training to enable zero-shot segmentation on virtually any image style and unsupervised training to enable segmentation without dense annotations. However, constructing a model capable of segmenting anything in a zero-shot manner without any annotations is still challenging. In this paper, we propose to utilize the self-attention layers in stable diffusion models to achieve this goal because the pre-trained stable diffusion model has learned inherent concepts of objects within its attention layers. Specifically, we introduce a simple yet effective iterative merging process based on measuring KL divergence among attention maps to merge them into valid segmentation masks. The proposed method does not require any training or language dependency to extract quality segmentation for any images. On COCO-Stuff-27, our method surpasses the prior unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17% in mean IoU.
</details></li>
</ul>
<hr>
<h2 id="InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model"><a href="#InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model" class="headerlink" title="InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model"></a>InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12465">http://arxiv.org/abs/2308.12465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biomedai-ucsc/inversesr">https://github.com/biomedai-ucsc/inversesr</a></li>
<li>paper_authors: Jueqi Wang, Jacob Levman, Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, M. Jorge Cardoso, Razvan Marinescu</li>
<li>for: 这个论文的目的是提出一种基于深度学习的MRI超分辨（SR）方法，以提高临床MRI扫描的分辨率。</li>
<li>methods: 该方法利用一个 estado-of-the-art 3D脑生成模型（LDM），通过在 UK BioBank 上训练该模型，来提高临床MRI扫描的分辨率。</li>
<li>results: 该方法可以在多种不同的MRI SR问题中提高分辨率，并且可以在不同的设置下选择合适的方法。Here are the three points in Simplified Chinese text:</li>
<li>for: 这个论文的目的是提出一种基于深度学习的MRI超分辨（SR）方法，以提高临床MRI扫描的分辨率。</li>
<li>methods: 该方法利用一个 estado-of-the-art 3D脑生成模型（LDM），通过在 UK BioBank 上训练该模型，来提高临床MRI扫描的分辨率。</li>
<li>results: 该方法可以在多种不同的MRI SR问题中提高分辨率，并且可以在不同的设置下选择合适的方法。<details>
<summary>Abstract</summary>
High-resolution (HR) MRI scans obtained from research-grade medical centers provide precise information about imaged tissues. However, routine clinical MRI scans are typically in low-resolution (LR) and vary greatly in contrast and spatial resolution due to the adjustments of the scanning parameters to the local needs of the medical center. End-to-end deep learning methods for MRI super-resolution (SR) have been proposed, but they require re-training each time there is a shift in the input distribution. To address this issue, we propose a novel approach that leverages a state-of-the-art 3D brain generative model, the latent diffusion model (LDM) trained on UK BioBank, to increase the resolution of clinical MRI scans. The LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details>
<details>
<summary>摘要</summary>
高解度（HR）MRI扫描从研究级医疗机构获取的信息非常精确。然而，日常临床MRI扫描通常是低解度（LR）的，并且因为扫描参数的调整而具有不同的对比度和空间分辨率。为解决这个问题，我们提出了一种新的方法，利用UK BioBank上训练的状态态流模型（LDM）来提高临床MRI扫描的解度。LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI。基于LDM的架构，我们发现不同的方法适用于不同的MRI SR设置，因此我们提出了两种新的策略：1）为SR with more sparsity，我们通过LDM的解码器和deterministic Denoising Diffusion Implicit Models（DDIM）进行逆变换，一种我们将称为InverseSR(LDM)；2）为SR with less sparsity，我们只通过LDM的解码器进行逆变换，一种我们将称为InverseSR(Decoder)。这两种方法在LDM模型中寻找不同的秘密空间，以找到将LR MRI映射到HR的最佳秘密代码。我们的训练过程不依赖MRI下抽样过程，因此我们的方法可以通用许多MRI SR问题。我们验证了我们的方法在IXI数据集上的100多个脑T1w MRI中。我们的方法可以证明LDM可以提供强大的PRIOR，用于MRI重建。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-General-Knowledge-Loss-with-Selective-Parameter-Finetuning"><a href="#Overcoming-General-Knowledge-Loss-with-Selective-Parameter-Finetuning" class="headerlink" title="Overcoming General Knowledge Loss with Selective Parameter Finetuning"></a>Overcoming General Knowledge Loss with Selective Parameter Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12462">http://arxiv.org/abs/2308.12462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Zhang, Paul Janson, Rahaf Aljundi, Mohamed Elhoseiny</li>
<li>for: 提高基础模型的更新能力，以适应新的信息和维护原有知识。</li>
<li>methods: 本文提出了一种新的方法，通过对小部分参数进行本地修改来实现基础模型的不断更新。这种方法基于先前分析基础模型的经验，首先局部化特定层进行模型精度，然后引入重要性分数机制，以更新关键参数。</li>
<li>results: 对基础视觉语言模型进行了广泛评估，证明该方法可以在不同的持续学习任务上提高现有的持续学习方法，并将预先学习的知识减少到0.97%。<details>
<summary>Abstract</summary>
Foundation models encompass an extensive knowledge base and offer remarkable transferability. However, this knowledge becomes outdated or insufficient over time. The challenge lies in updating foundation models to accommodate novel information while retaining their original ability. In this paper, we present a novel approach to achieving continual model updates by effecting localized modifications to a small subset of parameters. Guided by insights gleaned from prior analyses of foundational models, we first localize a specific layer for model refinement and then introduce an importance scoring mechanism designed to update only the most crucial weights. Our method is exhaustively evaluated on foundational vision-language models, measuring its efficacy in both learning new information and preserving pre-established knowledge across a diverse spectrum of continual learning tasks, including Aircraft, Birdsnap CIFAR-100, CUB, Cars, and GTSRB. The results show that our method improves the existing continual learning methods by 0.5\% - 10\% on average, and reduces the loss of pre-trained knowledge from around 5\% to 0.97\%. Comprehensive ablation studies substantiate our method design, shedding light on the contributions of each component to controllably learning new knowledge and mitigating the forgetting of pre-trained knowledge.
</details>
<details>
<summary>摘要</summary>
基础模型包含广泛的知识库和卓越的跨 Transferability。然而，这些知识随着时间的推移会变得过时或不足。挑战在于更新基础模型以容纳新的信息，而不会失去原有的知识。在这篇论文中，我们提出了一种新的方法来实现不断模型更新，通过对一小部分参数进行本地化修改。以先前分析基础模型所获得的知识为指导，我们首先本地化特定层，然后引入一种重要性分配机制，以更新最重要的权重。我们的方法在基础视觉语言模型上进行了完整的评估，并测试其在多种不断学习任务上的效果，包括飞机、鸟卷CIFAR-100、CUB、汽车和GTSRB。结果表明，我们的方法与现有的不断学习方法相比，平均提高了0.5%-10%，并将先前学习的知识损失从约5%降低至0.97%。我们还进行了广泛的减少分析，以证明我们的方法设计的每一部分对于控制新知识学习和减少先前知识损失做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="ARF-Plus-Controlling-Perceptual-Factors-in-Artistic-Radiance-Fields-for-3D-Scene-Stylization"><a href="#ARF-Plus-Controlling-Perceptual-Factors-in-Artistic-Radiance-Fields-for-3D-Scene-Stylization" class="headerlink" title="ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization"></a>ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for 3D Scene Stylization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12452">http://arxiv.org/abs/2308.12452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhao Li, Tianhao Wu, Fangcheng Zhong, Cengiz Oztireli</li>
<li>for: 用于三维场景样式传递</li>
<li>methods: 使用3D神经辐射场进行样式传递，并提供四种控制方法：色彩保持控制、纹理尺度控制、空间选择性风格控制和深度增强控制</li>
<li>results: 通过实际数据集的量化和质量评估，表明ARF-Plus框架在三维场景样式传递中提供了有效的控制功能，并且可以同时应用多种样式效果，创造出独特和引人注目的风格效果。<details>
<summary>Abstract</summary>
The radiance fields style transfer is an emerging field that has recently gained popularity as a means of 3D scene stylization, thanks to the outstanding performance of neural radiance fields in 3D reconstruction and view synthesis. We highlight a research gap in radiance fields style transfer, the lack of sufficient perceptual controllability, motivated by the existing concept in the 2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style transfer framework offering manageable control over perceptual factors, to systematically explore the perceptual controllability in 3D scene stylization. Four distinct types of controls - color preservation control, (style pattern) scale control, spatial (selective stylization area) control, and depth enhancement control - are proposed and integrated into this framework. Results from real-world datasets, both quantitative and qualitative, show that the four types of controls in our ARF-Plus framework successfully accomplish their corresponding perceptual controls when stylizing 3D scenes. These techniques work well for individual style inputs as well as for the simultaneous application of multiple styles within a scene. This unlocks a realm of limitless possibilities, allowing customized modifications of stylization effects and flexible merging of the strengths of different styles, ultimately enabling the creation of novel and eye-catching stylistic effects on 3D scenes.
</details>
<details>
<summary>摘要</summary>
《几何场景风格传输》是一个刚刚崛起的领域，感谢神经透辉场景的出色表现在3D重建和视觉合成中。我们指出了几何场景风格传输的研究漏洞，即无 suficient perceptual控制，这是基于2D图像风格传输的现有概念。在这篇论文中，我们提出了ARF-Plus，一个3D神经风格传输框架，可以系统地探索3D场景风格传输中的perceptual控制。我们提出了四种控制类型：颜色保持控制、样式模式比例控制、空间（选择性风格着色）控制和深度强化控制。这些控制被纳入到这个框架中，并在实际世界数据集上进行了评估。结果表明，ARF-Plus框架中的四种控制类型能够成功地实现对perceptual控制的管理，并且这些控制可以单独应用于个体风格输入或同时应用于场景中的多种风格。这些技术在创建个性化 modify 3D场景风格效果和自由混合不同风格的优点时，都工作良好。
</details></li>
</ul>
<hr>
<h2 id="MOFO-MOtion-FOcused-Self-Supervision-for-Video-Understanding"><a href="#MOFO-MOtion-FOcused-Self-Supervision-for-Video-Understanding" class="headerlink" title="MOFO: MOtion FOcused Self-Supervision for Video Understanding"></a>MOFO: MOtion FOcused Self-Supervision for Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12447">http://arxiv.org/abs/2308.12447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mona Ahmadian, Frank Guerin, Andrew Gilbert<br>for: 本研究的目的是提高视频中动作识别的性能，通过对视频中动作区域进行自我监督学习，以提高视频中动作的表征学习。methods: 我们提出了一种新的自我监督学习方法，称为MOFO（动作区域关注），它可以自动检测视频中动作区域，并使用这些区域来引导自我监督学习任务。我们使用了一种帮助器隐藏一定比例的输入序列中的掩码，并强制掩码在动作区域内部的一定比例被隐藏，而其余部分来自外部。此外，我们还在下游任务中进行了动作信息的加入，以强调动作的表征。results: 我们的研究表明，我们的动作区域关注技术可以明显提高当前最佳的自我监督学习方法（VideoMAE）的动作识别性能。我们在Epic-Kitchens verb、noun和动作分类任务上提高了2.6%、2.1%和1.3%的精度，并在Something-Something V2动作分类任务上提高了4.7%的精度。这表明，在自我监督学习中显式地编码动作是非常重要的。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) techniques have recently produced outstanding results in learning visual representations from unlabeled videos. Despite the importance of motion in supervised learning techniques for action recognition, SSL methods often do not explicitly consider motion information in videos. To address this issue, we propose MOFO (MOtion FOcused), a novel SSL method for focusing representation learning on the motion area of a video, for action recognition. MOFO automatically detects motion areas in videos and uses these to guide the self-supervision task. We use a masked autoencoder which randomly masks out a high proportion of the input sequence; we force a specified percentage of the inside of the motion area to be masked and the remainder from outside. We further incorporate motion information into the finetuning step to emphasise motion in the downstream task. We demonstrate that our motion-focused innovations can significantly boost the performance of the currently leading SSL method (VideoMAE) for action recognition. Our method improves the recent self-supervised Vision Transformer (ViT), VideoMAE, by achieving +2.6%, +2.1%, +1.3% accuracy on Epic-Kitchens verb, noun and action classification, respectively, and +4.7% accuracy on Something-Something V2 action classification. Our proposed approach significantly improves the performance of the current SSL method for action recognition, indicating the importance of explicitly encoding motion in SSL.
</details>
<details>
<summary>摘要</summary>
自顾学（SSL）技术在无标注视频中学习视觉表示方面最近取得了出色的结果。尽管动作认知中的运动信息在指导学习过程中非常重要，但SSL方法通常不直接考虑视频中的运动信息。为解决这个问题，我们提议MOFO（运动区域关注）方法，它是一种新的SSL方法，用于在视频中注意力集中在运动区域上，以提高动作认知。MOFO方法自动检测视频中的运动区域，并使用这些区域来引导自我超vision任务。我们使用一个随机屏蔽输入序列的masked autoencoder，其中高比例的输入序列会被随机屏蔽，而在运动区域内部则强制屏蔽一定比例。此外，我们还在下游任务中注入运动信息，以强调运动在下游任务中的作用。我们示出，我们的运动关注创新可以显著提高现有的SSL方法（VideoMAE）对动作认知的性能。我们的方法可以在Epic-Kitchens动词、名词和动作分类中提高VideoMAE的性能，分别提高+2.6%、+2.1%和+1.3%的精度。此外，我们还在Something-Something V2动作分类中提高了+4.7%的精度。这表明，在SSL中显式编码运动的重要性。
</details></li>
</ul>
<hr>
<h2 id="TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction"><a href="#TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction" class="headerlink" title="TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction"></a>TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12443">http://arxiv.org/abs/2308.12443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gxq1998/tai-gan">https://github.com/gxq1998/tai-gan</a></li>
<li>paper_authors: Xueqi Guo, Luyao Shi, Xiongchao Chen, Bo Zhou, Qiong Liu, Huidong Xie, Yi-Hwa Liu, Richard Palyo, Edward J. Miller, Albert J. Sinusas, Bruce Spottiswoode, Chi Liu, Nicha C. Dvornek</li>
<li>for: 这篇论文主要关注的是动脉心PET图像中的快速追踪器动力学和各帧分布的高变化，以及这些变化对插入动作 corrections 的影响。</li>
<li>methods: 该论文提出了一种使用生成方法处理 tracer 分布变化以帮助现有的注册方法。具体来说，我们提出了一种 Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN)，用于在早期帧中将 tracer 分布变化转换为late reference frame中的图像。</li>
<li>results: 我们在临床 $^{82}$Rb PET数据集上验证了我们的提议方法，并发现我们的 TAI-GAN 可以生成高质量的转换图像，与参照帧图像相似。 после TAI-GAN 转换，运动估计精度和临床血液流量（MBF）的量化得到了改善。<details>
<summary>Abstract</summary>
The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. Our code is published at https://github.com/gxq1998/TAI-GAN.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. Our code is published at https://github.com/gxq1998/TAI-GAN." into Simplified Chinese.Here's the translation:<<SYS>>rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) pose significant challenges for inter-frame motion correction, especially for early frames where conventional intensity-based image registration techniques are not applicable. alternatively, a promising approach utilizes generative methods to handle tracer distribution changes to assist existing registration methods. to improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform early frames into the late reference frame using an all-to-one mapping. specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as anatomical information. we validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to real reference frames. after TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. our code is published at https://github.com/gxq1998/TAI-GAN.Note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration"><a href="#HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration" class="headerlink" title="HNAS-reg: hierarchical neural architecture search for deformable medical image registration"></a>HNAS-reg: hierarchical neural architecture search for deformable medical image registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12440">http://arxiv.org/abs/2308.12440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiong Wu, Yong Fan</li>
<li>for: 这篇论文是为了找出最佳的深度学习模型，用于医疗影像注册。</li>
<li>methods: 这篇论文使用了一个内在的 NAS 框架 (HNAS-Reg)，包括了扩散操作搜索和网络架构搜索，以找到最佳的网络架构。具体来说，这个框架使用了一种参数化的搜索方法，以找到最佳的扩散操作和网络架构。</li>
<li>results: 实验结果显示，提议的方法可以建立一个具有更高影像注册精度和较小的模型大小的深度学习模型，比过去的影像注册方法更好。具体来说，在三个数据集上（包括 636 个 T1-调试磁共振成像（MRI）），提议的方法可以建立一个深度学习模型，并且与其他两个Unsupervised Learning-based方法相比，具有更高的影像注册精度和较小的模型大小。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been widely used to build deep learning models for medical image registration, but manually designed network architectures are not necessarily optimal. This paper presents a hierarchical NAS framework (HNAS-Reg), consisting of both convolutional operation search and network topology search, to identify the optimal network architecture for deformable medical image registration. To mitigate the computational overhead and memory constraints, a partial channel strategy is utilized without losing optimization quality. Experiments on three datasets, consisting of 636 T1-weighted magnetic resonance images (MRIs), have demonstrated that the proposal method can build a deep learning model with improved image registration accuracy and reduced model size, compared with state-of-the-art image registration approaches, including one representative traditional approach and two unsupervised learning-based approaches.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）已经广泛用于深度学习模型的医学图像注册，但是手动设计的网络架构可能不是最佳的。这篇论文提出了一种层次 NAS 框架（HNAS-Reg），包括卷积操作搜索和网络架构搜索，以确定最佳的医学图像注册网络架构。为了减少计算负担和内存限制，该方法使用了部分通道策略，而不失去优化质量。在三个数据集上，包括 636 个 T1 束缚磁共振成像（MRI），实验表明，提议方法可以建立一个具有提高图像注册精度和减少模型大小的深度学习模型，相比之下一个代表性的传统方法和两个无监督学习方法。
</details></li>
</ul>
<hr>
<h2 id="Characterising-representation-dynamics-in-recurrent-neural-networks-for-object-recognition"><a href="#Characterising-representation-dynamics-in-recurrent-neural-networks-for-object-recognition" class="headerlink" title="Characterising representation dynamics in recurrent neural networks for object recognition"></a>Characterising representation dynamics in recurrent neural networks for object recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12435">http://arxiv.org/abs/2308.12435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sushrut Thorat, Adrien Doerig, Tim C. Kietzmann</li>
<li>for: 这种研究旨在理解Recurrent Neural Networks (RNNs) 在复杂视觉任务中的表征动态，特别是大规模视觉模型中的计算。</li>
<li>methods: 研究者使用了MiniEcoset，一个新的子集，来训练 RNNs 进行物体分类。他们还使用了“读取区”来描述计算轨迹的活动排序。</li>
<li>results: 研究者发现，在推断时，表示 continuted to evolve  после正确的分类，这表明 RNNs 没有“完成分类”的概念。此外，研究者发现，在“读取区”中，错误的表示具有较低的L2范数活动排序，并位于更加外围的位置。这种排序可以帮助错误的表示逐渐移动到正确的区域。这些发现可以普通到其他类型的 RNNs，包括理解Primates 视觉中的表征动态。<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) have yielded promising results for both recognizing objects in challenging conditions and modeling aspects of primate vision. However, the representational dynamics of recurrent computations remain poorly understood, especially in large-scale visual models. Here, we studied such dynamics in RNNs trained for object classification on MiniEcoset, a novel subset of ecoset. We report two main insights. First, upon inference, representations continued to evolve after correct classification, suggesting a lack of the notion of being ``done with classification''. Second, focusing on ``readout zones'' as a way to characterize the activation trajectories, we observe that misclassified representations exhibit activation patterns with lower L2 norm, and are positioned more peripherally in the readout zones. Such arrangements help the misclassified representations move into the correct zones as time progresses. Our findings generalize to networks with lateral and top-down connections, and include both additive and multiplicative interactions with the bottom-up sweep. The results therefore contribute to a general understanding of RNN dynamics in naturalistic tasks. We hope that the analysis framework will aid future investigations of other types of RNNs, including understanding of representational dynamics in primate vision.
</details>
<details>
<summary>摘要</summary>
recurrent neural networks (RNNs) 已经在具有挑战性的条件下识别对象以及模型Primates的视觉方面显示了promising的结果。然而，RNNs中的表达动力学 Dynamics 仍未得到了充分的理解，特别是在大规模的视觉模型中。在这里，我们对RNNs在MiniEcoset上进行了对象分类训练。我们发现了两个主要的发现：首先，在推理时，表达还在继续进行改变，表明没有“完成分类”的概念。第二，我们将“读取区”作为表达轨迹的特征进行分析，发现了在读取区中的表达方式具有更低的L2范数，并且位于读取区的更外围位置。这种排列可以帮助错误的表达移动到正确的区域，并在时间的推移中进行改变。我们的发现涵盖了具有 Lateral 和上下 Connection 的网络，并包括了加法和乘法交互。这些结果因此对 RNN 动力学在自然任务中的一般理解做出了贡献，并且可以帮助未来对其他类型的 RNN 进行更深入的研究，包括理解primates 视觉中的表达动力学。
</details></li>
</ul>
<hr>
<h2 id="A-Spatiotemporal-Correspondence-Approach-to-Unsupervised-LiDAR-Segmentation-with-Traffic-Applications"><a href="#A-Spatiotemporal-Correspondence-Approach-to-Unsupervised-LiDAR-Segmentation-with-Traffic-Applications" class="headerlink" title="A Spatiotemporal Correspondence Approach to Unsupervised LiDAR Segmentation with Traffic Applications"></a>A Spatiotemporal Correspondence Approach to Unsupervised LiDAR Segmentation with Traffic Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12433">http://arxiv.org/abs/2308.12433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Pan He, Aotian Wu, Sanjay Ranka, Anand Rangarajan</li>
<li>for: 这个研究旨在解决室外LiDAR点云Sequence中的无监督Semantic Segmentation问题，尤其是在自动驾驶和交叉基建中的多种交通情况下。</li>
<li>methods: 本研究利用Point cloud sequence的空间时间特性，并在多帧框架之间建立强大的对应关系，以提高Semantic Segmentation的精度。研究将 clustering和pseudo-label学习结合，将点 cloud分组成Semantic groups，并使用点 clouds的pseudo-spatiotemporal标签进行模型优化。</li>
<li>results: 研究在Semantic-KITTI、SemanticPOSS和FLORIDAbenchmark dataset上得到了竞争性的Semantic Segmentation性能，与许多现有的对照学习方法相比。这个通用框架可以带来LiDAR点云Sequence中的统一表现学习方法，并结合对领域知识的导入。<details>
<summary>Abstract</summary>
We address the problem of unsupervised semantic segmentation of outdoor LiDAR point clouds in diverse traffic scenarios. The key idea is to leverage the spatiotemporal nature of a dynamic point cloud sequence and introduce drastically stronger augmentation by establishing spatiotemporal correspondences across multiple frames. We dovetail clustering and pseudo-label learning in this work. Essentially, we alternate between clustering points into semantic groups and optimizing models using point-wise pseudo-spatiotemporal labels with a simple learning objective. Therefore, our method can learn discriminative features in an unsupervised learning fashion. We show promising segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark datasets covering scenarios in autonomous vehicle and intersection infrastructure, which is competitive when compared against many existing fully supervised learning methods. This general framework can lead to a unified representation learning approach for LiDAR point clouds incorporating domain knowledge.
</details>
<details>
<summary>摘要</summary>
我们 Addressing the problem of unsupervised semantic segmentation of outdoor LiDAR point clouds in diverse traffic scenarios. The key idea is to leverage the spatiotemporal nature of a dynamic point cloud sequence and introduce drastically stronger augmentation by establishing spatiotemporal correspondences across multiple frames. We dovetail clustering and pseudo-label learning in this work. Essentially, we alternate between clustering points into semantic groups and optimizing models using point-wise pseudo-spatiotemporal labels with a simple learning objective. Therefore, our method can learn discriminative features in an unsupervised learning fashion. We show promising segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark datasets covering scenarios in autonomous vehicle and intersection infrastructure, which is competitive when compared against many existing fully supervised learning methods. This general framework can lead to a unified representation learning approach for LiDAR point clouds incorporating domain knowledge.Here's the word-for-word translation:我们 Addressing outdoor LiDAR point cloud semantic segmentation problem in diverse traffic scenarios. 针对多个 traffic scenarios 中的 outdoor LiDAR point cloud semantic segmentation problem. The key idea is to leverage point cloud sequence's spatiotemporal nature and introduce stronger augmentation by establishing spatiotemporal correspondences across multiple frames. 利用 point cloud sequence 的 spatiotemporal nature 和多幅 frames 之间的匹配，提高 semantic segmentation 的精度。 We dovetail clustering and pseudo-label learning in this work. 在这个工作中，我们将 clustering 和 pseudo-label learning 相互协调使用。 Essentially, we alternate between clustering points into semantic groups and optimizing models using point-wise pseudo-spatiotemporal labels with a simple learning objective. 我们将 alternate between clustering points into semantic groups 和使用 point-wise pseudo-spatiotemporal labels 来优化模型，使用简单的 learning objective。 Therefore, our method can learn discriminative features in an unsupervised learning fashion. 因此，我们的方法可以在无监督学习中学习出 distinguishing 特征。 We show promising segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark datasets covering scenarios in autonomous vehicle and intersection infrastructure. 我们在 Semantic-KITTI, SemanticPOSS, 和 FLORIDA benchmark datasets 上显示出了优秀的 segmentation 性能，这些 datasets 涵盖了自动驾驶车和交叉道路基础设施的场景。 These datasets are competitive when compared against many existing fully supervised learning methods. 这些 datasets 与许多现有的完全监督学习方法相比，显示出了竞争力。 This general framework can lead to a unified representation learning approach for LiDAR point clouds incorporating domain knowledge. 这个通用的框架可以导致一种 incorporating domain knowledge 的 LiDAR point clouds 的表示学习方法。
</details></li>
</ul>
<hr>
<h2 id="An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video"><a href="#An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video" class="headerlink" title="An Initial Exploration: Learning to Generate Realistic Audio for Silent Video"></a>An Initial Exploration: Learning to Generate Realistic Audio for Silent Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12408">http://arxiv.org/abs/2308.12408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Martel, Jackson Wagner</li>
<li>for: 这个论文的目的是开发一种基于深度学习的框架，用于生成电影和其他媒体中的真实的音效。</li>
<li>methods: 这个论文使用了多种不同的模型建立，包括深度融合CNN、扩展Wavenet CNN以及Transformer结构。这些模型都将视频上下文和先前生成的音频融合在一起，以生成真实的音效。</li>
<li>results: 研究发现，使用Transformer结构可以匹配视频中的低频信号，但是无法生成更加复杂的波形。<details>
<summary>Abstract</summary>
Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it's natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.
</details>
<details>
<summary>摘要</summary>
generate realistic audio effects for movies and other media is a challenging task that is primarily accomplished today through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in its natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.Here's the text with some notes on the translation:* "generate realistic audio effects" is translated as "生成真实的音效" (shēngjīn zhēnshí de yīngxìng), which is a more literal translation of the original English phrase.* "Foley art" is translated as "FOLEY艺术" (fōlēi yìshù), which is a direct translation of the original English phrase.* "captivating audio tracks" is translated as "吸引人的音乐轨迹" (xīhuī rén de yīngyuè guītà), which is a more idiomatic translation that conveys the idea of audio that is engaging and immersive.* "deep-learning based framework" is translated as "基于深度学习的框架" (jīyù shēngrán de kuàihù), which is a more literal translation of the original English phrase.* "low-frequencies" is translated as "低频谱" (dīfreqè), which is a more technical term that is commonly used in audio engineering.* "visual patterns" is translated as "视觉模式" (wèishì móxìng), which is a more idiomatic translation that conveys the idea of patterns that are visible and can be perceived through sight.I hope this helps! Let me know if you have any further questions or if you would like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="FG-Net-Facial-Action-Unit-Detection-with-Generalizable-Pyramidal-Features"><a href="#FG-Net-Facial-Action-Unit-Detection-with-Generalizable-Pyramidal-Features" class="headerlink" title="FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features"></a>FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12380">http://arxiv.org/abs/2308.12380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ihp-lab/fg-net">https://github.com/ihp-lab/fg-net</a></li>
<li>paper_authors: Yufeng Yin, Di Chang, Guoxian Song, Shen Sang, Tiancheng Zhi, Jing Liu, Linjie Luo, Mohammad Soleymani</li>
<li>for: 该文章目的是提出一种通用的表情动作单元检测方法，以优化对 facial expression 的 объектив分析。</li>
<li>methods: 该方法使用 StyleGAN2 模型预训练在大型和多样化的面孔图像集上，然后使用 Pyramid CNN Interpreter 检测表情动作单元。</li>
<li>results: 对于 DISFA 和 BP4D  datasets，提出的方法在跨域和同域检测中均达到了优于预先的状态艺术，同时在1000个样本上进行训练并且可以达到竞争性的性能。<details>
<summary>Abstract</summary>
Automatic detection of facial Action Units (AUs) allows for objective facial expression analysis. Due to the high cost of AU labeling and the limited size of existing benchmarks, previous AU detection methods tend to overfit the dataset, resulting in a significant performance loss when evaluated across corpora. To address this problem, we propose FG-Net for generalizable facial action unit detection. Specifically, FG-Net extracts feature maps from a StyleGAN2 model pre-trained on a large and diverse face image dataset. Then, these features are used to detect AUs with a Pyramid CNN Interpreter, making the training efficient and capturing essential local features. The proposed FG-Net achieves a strong generalization ability for heatmap-based AU detection thanks to the generalizable and semantic-rich features extracted from the pre-trained generative model. Extensive experiments are conducted to evaluate within- and cross-corpus AU detection with the widely-used DISFA and BP4D datasets. Compared with the state-of-the-art, the proposed method achieves superior cross-domain performance while maintaining competitive within-domain performance. In addition, FG-Net is data-efficient and achieves competitive performance even when trained on 1000 samples. Our code will be released at \url{https://github.com/ihp-lab/FG-Net}
</details>
<details>
<summary>摘要</summary>
自动检测人脸动作单元（AU）可以实现 объектив的人脸表达分析。由于AU标注的高成本和现有 benchmark 的有限大小，前一代AU检测方法往往会适应数据集，导致在 corpora 中表现不佳。为解决这个问题，我们提出了 FG-Net，一种通用的人脸动作单元检测方法。具体来说，FG-Net 从 StyleGAN2 模型在大量和多样的人脸图像数据集上预训练后的特征图进行检测AU。然后，这些特征图被 Pyramid CNN Interpreter 使用，以实现高效的训练和捕捉本地特征。我们提出的 FG-Net 在热图基于 AU 检测中实现了强大的总结能力，因为它可以从预训练的生成模型中提取通用和含义 Rich 的特征。我们进行了广泛的实验，以评估在 DISFA 和 BP4D 数据集上的在 corpora 和 across-corpus 中的 AU 检测性能。与当前状态的方法相比，我们的方法在跨频谱上实现了superior 的横跨频谱性能，同时保持竞争的在频谱内性能。此外，FG-Net 是数据效率的，可以在1000个样本上实现竞争性的表现。我们的代码将在 \url{https://github.com/ihp-lab/FG-Net} 上发布。
</details></li>
</ul>
<hr>
<h2 id="AdVerb-Visually-Guided-Audio-Dereverberation"><a href="#AdVerb-Visually-Guided-Audio-Dereverberation" class="headerlink" title="AdVerb: Visually Guided Audio Dereverberation"></a>AdVerb: Visually Guided Audio Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12370">http://arxiv.org/abs/2308.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjoy Chowdhury, Sreyan Ghosh, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha</li>
<li>for: 提高听起来的声音质量，使其更加清晰和可识别。</li>
<li>methods: 利用视觉特征和听起来的声音，通过一种新的几何学感知架构，捕捉场景几何和听起来的跨Modal关系，生成复杂的理想比例幕，以提高听起来的声音质量。</li>
<li>results: 比较传统的听起来只和听起来+视觉两个基elines，实现了18%-82%的提升，在LibriSpeech测试集上。同时，在AVSpeech数据集上也实现了非常满意的RT60错误分数。<details>
<summary>Abstract</summary>
We present AdVerb, a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. Although audio-only dereverberation is a well-studied problem, our approach incorporates the complementary visual modality to perform audio dereverberation. Given an image of the environment where the reverberated sound signal has been recorded, AdVerb employs a novel geometry-aware cross-modal transformer architecture that captures scene geometry and audio-visual cross-modal relationship to generate a complex ideal ratio mask, which, when applied to the reverberant audio predicts the clean sound. The effectiveness of our method is demonstrated through extensive quantitative and qualitative evaluations. Our approach significantly outperforms traditional audio-only and audio-visual baselines on three downstream tasks: speech enhancement, speech recognition, and speaker verification, with relative improvements in the range of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly satisfactory RT60 error scores on the AVSpeech dataset.
</details>
<details>
<summary>摘要</summary>
我们介绍了AdVerb，一种新的音频-视觉减振框架，该框架利用视觉信号以及干扰音频来估算清晰音频。虽然音频只的减振框架已经广泛研究过，但我们的方法具有较好的场景准确性和音频视觉跨模态关系，可以更好地进行音频减振。给出了环境中录制的干扰音频的图像，AdVerb使用了一种新的场景意识的cross-modal transformer架构，捕捉场景准确性和音频视觉跨模态关系，生成复杂的理想比例面积，当应用于干扰音频时，可以预测清晰音频。我们的方法的效果得到了广泛的量化和质量评估。与传统的音频只和音频视觉基线相比，我们的方法在三个下游任务中表现出了显著的改善，即speech enhancement、speech recognition和speaker verification，改善比例在0.18-0.82之间。此外，我们在AVSpeech dataset上也实现了高度满意的RT60错误分布。
</details></li>
</ul>
<hr>
<h2 id="Continual-Zero-Shot-Learning-through-Semantically-Guided-Generative-Random-Walks"><a href="#Continual-Zero-Shot-Learning-through-Semantically-Guided-Generative-Random-Walks" class="headerlink" title="Continual Zero-Shot Learning through Semantically Guided Generative Random Walks"></a>Continual Zero-Shot Learning through Semantically Guided Generative Random Walks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12366">http://arxiv.org/abs/2308.12366</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wx-zhang/igczsl">https://github.com/wx-zhang/igczsl</a></li>
<li>paper_authors: Wenxuan Zhang, Paul Janson, Kai Yi, Ivan Skorokhodov, Mohamed Elhoseiny</li>
<li>for: 本研究旨在模型人类在生活中不断学习和应用新知识，以及将之应用于未来任务中。</li>
<li>methods: 本研究使用生成模型，通过学习seen类的质量表示来提高对未经训练的视觉空间的生成理解。</li>
<li>results: 提出了一种基于生成模型的 continual zero-shot learning 算法，在 AWA1、AWA2、CUB 和 SUN 数据集上达到了状态之 arts 性能，比现有的 CZSL 方法高出 3-7%。<details>
<summary>Abstract</summary>
Learning novel concepts, remembering previous knowledge, and adapting it to future tasks occur simultaneously throughout a human's lifetime. To model such comprehensive abilities, continual zero-shot learning (CZSL) has recently been introduced. However, most existing methods overused unseen semantic information that may not be continually accessible in realistic settings. In this paper, we address the challenge of continual zero-shot learning where unseen information is not provided during training, by leveraging generative modeling. The heart of the generative-based methods is to learn quality representations from seen classes to improve the generative understanding of the unseen visual space. Motivated by this, we introduce generalization-bound tools and provide the first theoretical explanation for the benefits of generative modeling to CZSL tasks. Guided by the theoretical analysis, we then propose our learning algorithm that employs a novel semantically guided Generative Random Walk (GRW) loss. The GRW loss augments the training by continually encouraging the model to generate realistic and characterized samples to represent the unseen space. Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN datasets, surpassing existing CZSL methods by 3-7\%. The code has been made available here \url{https://github.com/wx-zhang/IGCZSL}
</details>
<details>
<summary>摘要</summary>
人类生命中，同时学习新概念，记忆过去知识，并将其应用到未来任务中发生。为模型这种全面能力，最近才提出了无限 zero-shot learning（CZSL）。然而，现有的方法往往过度利用无法在实际场景中 continually 获得的无序 semantic information。在这篇论文中，我们解决了 CZSL 任务中无法在训练中提供无序信息的挑战，通过使用生成模型。生成模型的核心是学习seen类型的高质量表示，以改善对未seen visual空间的生成理解。这些基于的概念工具，我们提供了第一个理论解释，描述了生成模型对 CZSL 任务的优势。受理论分析的指导，我们然后提出了我们的学习算法，该算法使用了一种新的semantically guided Generative Random Walk（GRW）损失函数。GRW损失函数在训练中不断地鼓励模型生成真实、特征化的样本，以表示未seen空间。我们的算法在 AWA1、AWA2、CUB 和 SUN 数据集上达到了状态机器人的性能，超过了现有的 CZSL 方法3-7\%。我们的代码已经在 GitHub 上公开，访问地址为 \url{https://github.com/wx-zhang/IGCZSL}。
</details></li>
</ul>
<hr>
<h2 id="Saliency-based-Video-Summarization-for-Face-Anti-spoofing"><a href="#Saliency-based-Video-Summarization-for-Face-Anti-spoofing" class="headerlink" title="Saliency-based Video Summarization for Face Anti-spoofing"></a>Saliency-based Video Summarization for Face Anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12364">http://arxiv.org/abs/2308.12364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Usman1021/Saliency">https://github.com/Usman1021/Saliency</a></li>
<li>paper_authors: Usman Muhammad, Mourad Oussalah, Md Ziaul Hoque, Jorma Laaksonen</li>
<li>for: 提高面部骗取检测器的性能和效率，使用视觉吸引力理论来增强深度学习模型的表现。</li>
<li>methods: 提出了一种视频概要方法，通过提取源图像的视觉吸引力信息，对每帧图像进行分解，并使用重要性映射来线性组合源图像，创建一个代表整个视频的单一图像。</li>
<li>results: 实验结果表明，该方法可以在五个面部骗取检测 datasets 上达到状态 искусственный智能的性能，并且比 tradicional 方法有更好的性能和效率。<details>
<summary>Abstract</summary>
Due to the growing availability of face anti-spoofing databases, researchers are increasingly focusing on video-based methods that use hundreds to thousands of images to assess their impact on performance. However, there is no clear consensus on the exact number of frames in a video required to improve the performance of face anti-spoofing tasks. Inspired by the visual saliency theory, we present a video summarization method for face anti-spoofing tasks that aims to enhance the performance and efficiency of deep learning models by leveraging visual saliency. In particular, saliency information is extracted from the differences between the Laplacian and Wiener filter outputs of the source images, enabling identification of the most visually salient regions within each frame. Subsequently, the source images are decomposed into base and detail layers, enhancing representation of important information. The weighting maps are then computed based on the saliency information, indicating the importance of each pixel in the image. By linearly combining the base and detail layers using the weighting maps, the method fuses the source images to create a single representative image that summarizes the entire video. The key contribution of our proposed method lies in demonstrating how visual saliency can be used as a data-centric approach to improve the performance and efficiency of face presentation attack detection models. By focusing on the most salient images or regions within the images, a more representative and diverse training set can be created, potentially leading to more effective models. To validate the method's effectiveness, a simple deep learning architecture (CNN-RNN) was used, and the experimental results showcased state-of-the-art performance on five challenging face anti-spoofing datasets.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:由于面对面骗降库的可用性不断增长，研究人员正在更加关注视频基于方法，使用数百到千个图像来评估其影响性。然而，没有明确的共识，关于视频中帧数所需要提高面对面骗降模型的性能。我们根据视觉吸引力理论，提出了一种面对面骗降视频 summarization方法，以提高深度学习模型的性能和效率。具体来说，该方法使用源图像的差分 Laplacian 和 Wiener 滤波器输出来提取视觉吸引力信息，并在每帧中标识最有吸引力的区域。然后，源图像被分解成基层和详细层，从而增强图像的重要信息表示。最后，根据视觉吸引力信息，计算weighting map，以指示每个像素的重要性。通过线性组合基层和详细层，方法将源图像总结为整个视频的代表图像。我们的提案的关键在于，通过使用视觉吸引力来为面对面骗降模型提高性能和效率。通过关注图像中最有吸引力的部分或区域，可以创建更加代表和多样的训练集，可能导致更有效的模型。为验证方法的效果，我们使用了一种简单的深度学习架构（CNN-RNN），并在五个面对面骗降数据集上获得了状态艺术性的实验结果。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Image-Translation-with-Label-Guidance-for-Domain-Adaptive-Semantic-Segmentation"><a href="#Diffusion-based-Image-Translation-with-Label-Guidance-for-Domain-Adaptive-Semantic-Segmentation" class="headerlink" title="Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation"></a>Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12350">http://arxiv.org/abs/2308.12350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duo Peng, Ping Hu, Qiuhong Ke, Jun Liu</li>
<li>for: 提高频率领域转换图像的semantic consistency</li>
<li>methods: 使用源域标签作为Explicit导航 during image translation</li>
<li>results: 比对前方法有superiority<details>
<summary>Abstract</summary>
Translating images from a source domain to a target domain for learning target models is one of the most common strategies in domain adaptive semantic segmentation (DASS). However, existing methods still struggle to preserve semantically-consistent local details between the original and translated images. In this work, we present an innovative approach that addresses this challenge by using source-domain labels as explicit guidance during image translation. Concretely, we formulate cross-domain image translation as a denoising diffusion process and utilize a novel Semantic Gradient Guidance (SGG) method to constrain the translation process, conditioning it on the pixel-wise source labels. Additionally, a Progressive Translation Learning (PTL) strategy is devised to enable the SGG method to work reliably across domains with large gaps. Extensive experiments demonstrate the superiority of our approach over state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
通常，域适应semantic segmentation（DASS）中将源域图像翻译到目标域图像是一种常见的策略。然而，现有方法仍然困难保持 semantic consistency的本地细节 между原始图像和翻译图像。在这种情况下，我们提出了一种创新的方法，通过在翻译过程中使用源域标签作为直接导航来解决这个挑战。具体来说，我们将cross-domain image translation表示为干扰扩散过程，并使用一种新的Semantic Gradient Guidance（SGG）方法来约束翻译过程，将其受到像素级source标签的控制。此外，我们还提出了一种Progressive Translation Learning（PTL）策略，以确保 SGG 方法在不同域的大差下可靠地工作。广泛的实验证明了我们的方法在现有方法之上表现出了superiority。
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Approach-for-Image-Registration-of-Visible-Thermal-VT-Cancer-Faces"><a href="#A-Generative-Approach-for-Image-Registration-of-Visible-Thermal-VT-Cancer-Faces" class="headerlink" title="A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces"></a>A Generative Approach for Image Registration of Visible-Thermal (VT) Cancer Faces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12271">http://arxiv.org/abs/2308.12271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catherine Ordun, Alexandra Cha, Edward Raff, Sanjay Purushotham, Karen Kwok, Mason Rule, James Gulley</li>
<li>for: 这项研究旨在提高人工智能下的疼痛研究，使用可见光和热成像图像进行对比。</li>
<li>methods: 该研究使用了生成对应算法进行图像 регистра，以解决可见光和热成像图像之间的偏移问题。</li>
<li>results: 研究发现，通过对可见光和热成像图像进行REGISTERING，可以提高热成像图像质量，提高疼痛研究的效果，最高提高率达52.5%。<details>
<summary>Abstract</summary>
Since thermal imagery offers a unique modality to investigate pain, the U.S. National Institutes of Health (NIH) has collected a large and diverse set of cancer patient facial thermograms for AI-based pain research. However, differing angles from camera capture between thermal and visible sensors has led to misalignment between Visible-Thermal (VT) images. We modernize the classic computer vision task of image registration by applying and modifying a generative alignment algorithm to register VT cancer faces, without the need for a reference or alignment parameters. By registering VT faces, we demonstrate that the quality of thermal images produced in the generative AI downstream task of Visible-to-Thermal (V2T) image translation significantly improves up to 52.5\%, than without registration. Images in this paper have been approved by the NIH NCI for public dissemination.
</details>
<details>
<summary>摘要</summary>
由于热影像可以提供一种独特的方式来研究疼痛，美国国家医学研究院（NIH）已经收集了大量和多样化的癌症患者脸部热影像，用于人工智能基于痛症研究。然而，相机捕捉的角度差异导致热影像和可见感器拍摄的图像不一致，这导致了可见热图像的注册问题。我们使用和修改生成对齐算法，以无需参考或对齐参数，对热照相机拍摄的癌症脸部进行注册。通过注册热照相机拍摄，我们证明了在生成AI下渠道任务中，将可见图像翻译成热图像的质量显著提高，比无注册情况提高至52.5%。图像在本文中已经获得了NIH NCI的批准，可以公开发布。
</details></li>
</ul>
<hr>
<h2 id="MolGrapher-Graph-based-Visual-Recognition-of-Chemical-Structures"><a href="#MolGrapher-Graph-based-Visual-Recognition-of-Chemical-Structures" class="headerlink" title="MolGrapher: Graph-based Visual Recognition of Chemical Structures"></a>MolGrapher: Graph-based Visual Recognition of Chemical Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12234">http://arxiv.org/abs/2308.12234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ds4sd/molgrapher">https://github.com/ds4sd/molgrapher</a></li>
<li>paper_authors: Lucas Morin, Martin Danelljan, Maria Isabel Agea, Ahmed Nassar, Valery Weber, Ingmar Meijer, Peter Staar, Fisher Yu</li>
<li>for: 本研究旨在提高化学文献自动分析的效率，以促进新材料和药物的发现。</li>
<li>methods: 本研究使用了深度键点检测器和图学神经网络来自动识别化学结构。</li>
<li>results: 对五个数据集进行了广泛的实验，结果表明，我们的方法在大多数情况下与经典和学习基于方法相比，有显著的优异表现。<details>
<summary>Abstract</summary>
The automatic analysis of chemical literature has immense potential to accelerate the discovery of new materials and drugs. Much of the critical information in patent documents and scientific articles is contained in figures, depicting the molecule structures. However, automatically parsing the exact chemical structure is a formidable challenge, due to the amount of detailed information, the diversity of drawing styles, and the need for training data. In this work, we introduce MolGrapher to recognize chemical structures visually. First, a deep keypoint detector detects the atoms. Second, we treat all candidate atoms and bonds as nodes and put them in a graph. This construct allows a natural graph representation of the molecule. Last, we classify atom and bond nodes in the graph with a Graph Neural Network. To address the lack of real training data, we propose a synthetic data generation pipeline producing diverse and realistic results. In addition, we introduce a large-scale benchmark of annotated real molecule images, USPTO-30K, to spur research on this critical topic. Extensive experiments on five datasets show that our approach significantly outperforms classical and learning-based methods in most settings. Code, models, and datasets are available.
</details>
<details>
<summary>摘要</summary>
自动分析化学文献的潜在可能性非常大，可以加速发现新材料和药物。文献中大量关键信息都集中在图像中，其中包括分子结构。然而，自动解析图像中的具体化学结构是一项具有挑战性的任务，原因在于图像中的信息量、绘制风格的多样性以及需要训练数据。在这项工作中，我们介绍了MolGrapher，一种可视化化学结构的识别算法。首先，我们使用深度关键点检测器检测原子。其次，我们将所有候选原子和键视为图像中的节点，并将它们建立成一个图。这种构建方式允许自然地表示分子的图像。最后，我们使用图 neural network 来分类原子和键节点。因为缺乏真实的训练数据，我们提出了一个生成 sintetic 数据的管道，以生成多样化和真实的结果。此外，我们还介绍了一个大规模的注释实验室， USPTO-30K，以促进这一重要领域的研究。我们在五个数据集上进行了广泛的实验，结果显示，我们的方法在大多数情况下与 класси方法和学习型方法相比，表现出了显著的优势。代码、模型和数据集都可以获得。
</details></li>
</ul>
<hr>
<h2 id="SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation"><a href="#SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation" class="headerlink" title="SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation"></a>SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12231">http://arxiv.org/abs/2308.12231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xq141839/sppnet">https://github.com/xq141839/sppnet</a></li>
<li>paper_authors: Qing Xu, Wenwei Kuang, Zeyu Zhang, Xueyao Bao, Haoran Chen, Wenting Duan</li>
<li>for: 这个研究旨在提出一个单点提示网络（SPPNet），用于核仁像分类，以解决目前的模型存在大量参数和训练成本的问题。</li>
<li>methods: 这个模型使用了轻量级的投影 транс福曼（ViT）来取代原始的图像编码器，并添加了一个有效的混合层来提高图像中低层次的Semantic信息EXTRACTION。</li>
<li>results: 这个研究显示了 SPPNet 比现有的 U-shape 架构更好地运行，并且在训练过程中更快地训练。相比于目前的模型，SPPNet 的测试速度大约是 20 倍 faster，仅需要 1&#x2F;70 参数和computational cost。此外，这个模型只需要在训练和测试阶段点击一次，更适合临床应用。<details>
<summary>Abstract</summary>
Image segmentation plays an essential role in nuclei image analysis. Recently, the segment anything model has made a significant breakthrough in such tasks. However, the current model exists two major issues for cell segmentation: (1) the image encoder of the segment anything model involves a large number of parameters. Retraining or even fine-tuning the model still requires expensive computational resources. (2) in point prompt mode, points are sampled from the center of the ground truth and more than one set of points is expected to achieve reliable performance, which is not efficient for practical applications. In this paper, a single-point prompt network is proposed for nuclei image segmentation, called SPPNet. We replace the original image encoder with a lightweight vision transformer. Also, an effective convolutional block is added in parallel to extract the low-level semantic information from the image and compensate for the performance degradation due to the small image encoder. We propose a new point-sampling method based on the Gaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset. The result demonstrated that SPPNet outperforms existing U-shape architectures and shows faster convergence in training. Compared to the segment anything model, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and computational cost. Particularly, only one set of points is required in both the training and inference phases, which is more reasonable for clinical applications. The code for our work and more technical details can be found at https://github.com/xq141839/SPPNet.
</details>
<details>
<summary>摘要</summary>
Image segmentation plays an essential role in nuclei image analysis. Recently, the segment anything model has made a significant breakthrough in such tasks. However, the current model exists two major issues for cell segmentation: (1) the image encoder of the segment anything model involves a large number of parameters. Retraining or even fine-tuning the model still requires expensive computational resources. (2) in point prompt mode, points are sampled from the center of the ground truth and more than one set of points is expected to achieve reliable performance, which is not efficient for practical applications.In this paper, a single-point prompt network is proposed for nuclei image segmentation, called SPPNet. We replace the original image encoder with a lightweight vision transformer. Also, an effective convolutional block is added in parallel to extract the low-level semantic information from the image and compensate for the performance degradation due to the small image encoder. We propose a new point-sampling method based on the Gaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset. The result demonstrated that SPPNet outperforms existing U-shape architectures and shows faster convergence in training. Compared to the segment anything model, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and computational cost. Particularly, only one set of points is required in both the training and inference phases, which is more reasonable for clinical applications.The code for our work and more technical details can be found at <https://github.com/xq141839/SPPNet>.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.CV_2023_08_24/" data-id="clmjn91kl004d0j885vrtf49t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.LG_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.LG_2023_08_24/">cs.LG - 2023-08-24 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Easy-attention-A-simple-self-attention-mechanism-for-Transformers"><a href="#Easy-attention-A-simple-self-attention-mechanism-for-Transformers" class="headerlink" title="Easy attention: A simple self-attention mechanism for Transformers"></a>Easy attention: A simple self-attention mechanism for Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12874">http://arxiv.org/abs/2308.12874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcial Sanchis-Agudo, Yuning Wang, Karthik Duraisamy, Ricardo Vinuesa</li>
<li>for: 提高逻辑神经网络对时间动态预测的稳定性，我们提出了一种新的注意力机制called easy attention。</li>
<li>methods: 我们的方法通过对softmax注意力分数进行特征值分解（SVD），从而发现self注意力在 span 空间中压缩了查询和KEYS的贡献。因此，我们直接将注意力分数作为学习参数来处理。</li>
<li>results: 我们的方法在重建和预测时间动态系统中表现出色，比self注意力或广泛使用的长短期记忆（LSTM）网络更加稳定、更加简单。我们的结果表明这种方法在更复杂的高维动力系统中具有潜在的应用前景。<details>
<summary>Abstract</summary>
To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-term memory (LSTM) network. Our results show great potential for applications in more complex high-dimensional dynamical systems.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:为了提高转换器神经网络在时间序列预测中的稳定性，我们提出了一种新的注意力机制，即“容易注意”机制。这种机制不需要使用自注意的内积、 queries 和 softmax，它们通常用于捕捉时间序列中的长期依赖关系。而我们通过对 softmax 注意score 的 singular-value decomposition (SVD)，发现自注意对 queries 和 keys 在扩展空间中的贡献被压缩。因此，我们直接将注意分数视为学习参数，这种方法在预测和重建混沌系统的时间动力学中表现出色，比自注意或通用的长期记忆网络 (LSTM) 更加稳定和简单。我们的结果表明，这种方法在更复杂的高维动力系统中具有潜在的应用前景。
</details></li>
</ul>
<hr>
<h2 id="IPA-Inference-Pipeline-Adaptation-to-Achieve-High-Accuracy-and-Cost-Efficiency"><a href="#IPA-Inference-Pipeline-Adaptation-to-Achieve-High-Accuracy-and-Cost-Efficiency" class="headerlink" title="IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency"></a>IPA: Inference Pipeline Adaptation to Achieve High Accuracy and Cost-Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12871">http://arxiv.org/abs/2308.12871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid Ghafouri, Kamran Razavi, Mehran Salmani, Alireza Sanaee, Tania Lorido-Botran, Lin Wang, Joseph Doyle, Pooyan Jamshidi</li>
<li>for: 提高 ML 生产系统中的快速、准确和成本效果的推理管线优化，以满足紧张的终端延迟要求。</li>
<li>methods: 使用 Integer Programming 方法 dynamically 配置批处理大小、复制和模型变体，以优化准确率、降低成本并遵循用户定义的延迟 SLA。</li>
<li>results: 对实际推理管线中的五个实际应用进行了广泛的实验，结果表明，IPA 可以提高 норма化准确率达到 35%，而成本增加仅为 less than 5%。<details>
<summary>Abstract</summary>
Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in ML production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of accuracy and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling accuracy and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep-learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency SLAs using Integer Programming. It supports multi-objective settings for achieving different trade-offs between accuracy and cost objectives while remaining adaptable to varying workloads and dynamic traffic patterns. Extensive experiments on a Kubernetes implementation with five real-world inference pipelines demonstrate that IPA improves normalized accuracy by up to 35% with a minimal cost increase of less than 5%.
</details>
<details>
<summary>摘要</summary>
实现快速、准确且成本效益的多模型推论管线协调是机器学习生产系统中的挑战，因为它们具有紧密的终端径准确性要求。为了简化对准确和成本费用之间的复杂贸易空间的探索，提供者通常会选择忽略其中一个。但是，这个挑战在整合准确和成本费用之间的调整上还是一个挑战。为了解决这个挑战并提出一个解决方案，我们提出了IPA，一个基于深度学习的推论管线自适应系统。IPA通过动态配置批次大小、重复和模型Variant来优化准确性、降低成本和遵循用户定义的径准确性SLAs，使用整数程式设计。它支持多目标设定，以实现不同的准确和成本目标之间的变化，同时保持可靠的运算和变化的工作负载模式。实验结果显示，IPA可以将Normalized准确性提高至35%，并且仅增加成本少于5%。
</details></li>
</ul>
<hr>
<h2 id="Auto-weighted-Bayesian-Physics-Informed-Neural-Networks-and-robust-estimations-for-multitask-inverse-problems-in-pore-scale-imaging-of-dissolution"><a href="#Auto-weighted-Bayesian-Physics-Informed-Neural-Networks-and-robust-estimations-for-multitask-inverse-problems-in-pore-scale-imaging-of-dissolution" class="headerlink" title="Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution"></a>Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12864">http://arxiv.org/abs/2308.12864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Perez, Philippe Poncet<br>for:The paper presents a novel data assimilation strategy for pore-scale imaging to address reactive inverse problems incorporating Uncertainty Quantification (UQ).methods:The method combines data-driven and physics-informed techniques, including sequential reinforcement, Bayesian Physics-Informed Neural Networks (BPINNs), and adaptive weighting to ensure robust and unbiased uncertainty quantification.results:The method is demonstrated to be successful in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT images, providing reliable micro-porosity changes during geochemical transformations and quantifying morphological uncertainties on the porosity field.<details>
<summary>Abstract</summary>
In this article, we present a novel data assimilation strategy in pore-scale imaging and demonstrate that this makes it possible to robustly address reactive inverse problems incorporating Uncertainty Quantification (UQ). Pore-scale modeling of reactive flow offers a valuable opportunity to investigate the evolution of macro-scale properties subject to dynamic processes. Yet, they suffer from imaging limitations arising from the associated X-ray microtomography (X-ray microCT) process, which induces discrepancies in the properties estimates. Assessment of the kinetic parameters also raises challenges, as reactive coefficients are critical parameters that can cover a wide range of values. We account for these two issues and ensure reliable calibration of pore-scale modeling, based on dynamical microCT images, by integrating uncertainty quantification in the workflow.   The present method is based on a multitasking formulation of reactive inverse problems combining data-driven and physics-informed techniques in calcite dissolution. This allows quantifying morphological uncertainties on the porosity field and estimating reactive parameter ranges through prescribed PDE models with a latent concentration field and dynamical microCT. The data assimilation strategy relies on sequential reinforcement incorporating successively additional PDE constraints. We guarantee robust and unbiased uncertainty quantification by straightforward adaptive weighting of Bayesian Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity changes during geochemical transformations. We demonstrate successful Bayesian Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT images with meaningful posterior distribution on the reactive parameters and dimensionless numbers.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们提出了一种新的数据融合策略，用于 Addressing Reactive Inverse Problems  incorporating Uncertainty Quantification (UQ)。孔径级模拟涉及的激发过程可以帮助我们研究宏观质量特性的演化。然而，X射微 Tomatoes 的制图过程会导致属性估计偏差，而且评估激发系数也存在挑战，因为激发系数范围很广。我们解决了这两个问题，并确保了精确的孔径级模拟calibration，基于动态微 Tomatoes 图像，通过结合不确定性评估和物理学 inform 技术。我们的方法基于多任务形式的激发反问题，结合数据驱动和物理学 inform 技术，用于评估孔径级模拟中的不确定性。我们采用了顺序强化法，包括逐渐添加 PDE 约束。我们 garantía 不偏不倚的不确定性评估，通过简单的自适应权重 bayesian physics-informed neural networks (BPINNs)，以确保可靠的微порosity 变化 durante geochemical transformations。我们在1D+Time和2D+Time calcite dissolution中成功进行 Bayesian Inference，并获得了有意义的 posterior distribution on the reactive parameters and dimensionless numbers。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automated-Animal-Density-Estimation-with-Acoustic-Spatial-Capture-Recapture"><a href="#Towards-Automated-Animal-Density-Estimation-with-Acoustic-Spatial-Capture-Recapture" class="headerlink" title="Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture"></a>Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12859">http://arxiv.org/abs/2308.12859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuheng Wang, Juan Ye, David L. Borchers</li>
<li>for: 监测野生动物人口，特别是难以视见的动物，可以通过投影式监测获得大量数据，但是需要采用机器学习（ML）方法进行识别。</li>
<li>methods: 我们提出了三种方法，即混合模型、隐藏状态模型和隐藏变量模型，这些方法可以将机器学习的不确定性integrated into inference。</li>
<li>results: 我们通过模拟测试发现，在基于海南小猩猩的声学数据中，忽略假阳性会导致17%的正确率偏高，而我们的方法则具有几乎零偏高和95%的正确率。<details>
<summary>Abstract</summary>
Passive acoustic monitoring can be an effective way of monitoring wildlife populations that are acoustically active but difficult to survey visually. Digital recorders allow surveyors to gather large volumes of data at low cost, but identifying target species vocalisations in these data is non-trivial. Machine learning (ML) methods are often used to do the identification. They can process large volumes of data quickly, but they do not detect all vocalisations and they do generate some false positives (vocalisations that are not from the target species). Existing wildlife abundance survey methods have been designed specifically to deal with the first of these mistakes, but current methods of dealing with false positives are not well-developed. They do not take account of features of individual vocalisations, some of which are more likely to be false positives than others. We propose three methods for acoustic spatial capture-recapture inference that integrate individual-level measures of confidence from ML vocalisation identification into the likelihood and hence integrate ML uncertainty into inference. The methods include a mixture model in which species identity is a latent variable. We test the methods by simulation and find that in a scenario based on acoustic data from Hainan gibbons, in which ignoring false positives results in 17% positive bias, our methods give negligible bias and coverage probabilities that are close to the nominal 95% level.
</details>
<details>
<summary>摘要</summary>
（注：以下是简化中文版本）通过激光监测，可以有效地监测难以视见的野生动物 populations，但是 Machine Learning（ML）方法可能会导致一些假阳性结果（不是目标种的叫声）。现有的野生动物数量评估方法已经特地处理了第一种错误，但是处理假阳性结果的方法并不完善。它们不会考虑具体的叫声特征，一些叫声更容易被误分为假阳性。我们提出了三种基于 ML 声音特征置信度的声音空间捕捉-再次捕捉方法，并将这些方法与现有的方法进行比较。这些方法包括一种混合模型，其中种类标识是隐藏变量。我们通过模拟测试，发现在基于海南 Gibbon 的声音数据场景中，忽略假阳性结果会导致正确率为 17% 的偏好，而我们的方法则具有几乎零偏好和95% 的覆盖 probabilities。
</details></li>
</ul>
<hr>
<h2 id="Fast-Adversarial-Training-with-Smooth-Convergence"><a href="#Fast-Adversarial-Training-with-Smooth-Convergence" class="headerlink" title="Fast Adversarial Training with Smooth Convergence"></a>Fast Adversarial Training with Smooth Convergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12857">http://arxiv.org/abs/2308.12857</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fat-cs/convergesmooth">https://github.com/fat-cs/convergesmooth</a></li>
<li>paper_authors: Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin</li>
<li>for: 提高神经网络的攻击鲁棒性。</li>
<li>methods: 提出了一种新的振荡约束（ConvergeSmooth），用于限制前一个级别的损失差和当前级别的损失差之间的差异，以保证损失的涨幂平滑。同时，提出了一种新的权重中心化策略，不需要添加额外的参数。</li>
<li>results: 在各种流行的数据集上进行了广泛的实验，发现提出的方法可以减少极端欠拟合问题，并比前一代FAT技术表现更好。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/FAT-CS/ConvergeSmooth%7D">https://github.com/FAT-CS/ConvergeSmooth}</a> 上获取。<details>
<summary>Abstract</summary>
Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, \ie the adversarial robustness of models declines to near zero during training.   To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers.   Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting.   To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the loss balance coefficient.   Our proposed methods are attack-agnostic and thus can improve the training stability of various FAT techniques.   Extensive experiments on popular datasets show that the proposed methods efficiently avoid catastrophic overfitting and outperform all previous FAT methods. Code is available at \url{https://github.com/FAT-CS/ConvergeSmooth}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Probabilistic-load-forecasting-with-Reservoir-Computing"><a href="#Probabilistic-load-forecasting-with-Reservoir-Computing" class="headerlink" title="Probabilistic load forecasting with Reservoir Computing"></a>Probabilistic load forecasting with Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12844">http://arxiv.org/abs/2308.12844</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MicheleUIT/Probabilistic-load-forecasting-with-Reservoir-Computing">https://github.com/MicheleUIT/Probabilistic-load-forecasting-with-Reservoir-Computing</a></li>
<li>paper_authors: Michele Guerra, Simone Scardapane, Filippo Maria Bianchi</li>
<li>for: 预测电力负荷的准确性和不确定性评估</li>
<li>methods: 使用托管 computing 作为时间序列预测方法，并评估不确定性评估方法的可行性、计算资源效率和可靠性</li>
<li>results: 研究发现，使用托管 computing 和不确定性评估方法可以提高预测结果的准确性和可靠性，同时减少计算资源的消耗<details>
<summary>Abstract</summary>
Some applications of deep learning require not only to provide accurate results but also to quantify the amount of confidence in their prediction. The management of an electric power grid is one of these cases: to avoid risky scenarios, decision-makers need both precise and reliable forecasts of, for example, power loads. For this reason, point forecasts are not enough hence it is necessary to adopt methods that provide an uncertainty quantification.   This work focuses on reservoir computing as the core time series forecasting method, due to its computational efficiency and effectiveness in predicting time series. While the RC literature mostly focused on point forecasting, this work explores the compatibility of some popular uncertainty quantification methods with the reservoir setting. Both Bayesian and deterministic approaches to uncertainty assessment are evaluated and compared in terms of their prediction accuracy, computational resource efficiency and reliability of the estimated uncertainty, based on a set of carefully chosen performance metrics.
</details>
<details>
<summary>摘要</summary>
某些深度学习应用需要不仅提供准确的结果，还需要评估预测结果的信度。电力系统管理是这些情况之一：以避免危险场景，决策者需要精准、可靠地预测电力负荷。为此，点预测不足，需要采用能够评估预测结果的不确定性方法。本工作选择了储池计算作为核心时间序列预测方法，因为它的计算效率高并能有效预测时间序列。而 RC 文献主要关注点预测，这里的工作探讨了储池设置下的不确定性评估方法的Compatibility。本文对 uncertainty quantification 方法进行了评估和比较，包括 Bayesian 和deterministic 方法，并根据选择的性能指标进行了对比。
</details></li>
</ul>
<hr>
<h2 id="Actuator-Trajectory-Planning-for-UAVs-with-Overhead-Manipulator-using-Reinforcement-Learning"><a href="#Actuator-Trajectory-Planning-for-UAVs-with-Overhead-Manipulator-using-Reinforcement-Learning" class="headerlink" title="Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning"></a>Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12843">http://arxiv.org/abs/2308.12843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazim Alzorgan, Abolfazl Razi, Ata Jahangir Moshayedi</li>
<li>for: 这种 aerial manipulator system 是用于实现 actuation tasks 在飞行中的，例如高空焊接、结构监测和维护、电池换装、排障清理、高层建筑清洁和维护等。</li>
<li>methods: 这种系统使用 Q-learning 方法控制末端机械的 trajectory，并使用 Time To Collision (TTC) 模型来让 quadrotor UAV 环境中绕过障碍物，保证 manipulate 器的可达性。</li>
<li>results: 这种控制方法可以在各种具有飞行和抓取功能的环境中实现 actuation tasks，并且可以快速学习和适应不确定的 UAV 运动。在 15,000 集 episodes 中，Q-learning 方法可以达到 92% 的准确率（即目标轨迹和实际轨迹之间的平均距离）。<details>
<summary>Abstract</summary>
In this paper, we investigate the operation of an aerial manipulator system, namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with two degrees of freedom to carry out actuation tasks on the fly. Our solution is based on employing a Q-learning method to control the trajectory of the tip of the arm, also called \textit{end-effector}. More specifically, we develop a motion planning model based on Time To Collision (TTC), which enables a quadrotor UAV to navigate around obstacles while ensuring the manipulator's reachability. Additionally, we utilize a model-based Q-learning model to independently track and control the desired trajectory of the manipulator's end-effector, given an arbitrary baseline trajectory for the UAV platform. Such a combination enables a variety of actuation tasks such as high-altitude welding, structural monitoring and repair, battery replacement, gutter cleaning, sky scrapper cleaning, and power line maintenance in hard-to-reach and risky environments while retaining compatibility with flight control firmware. Our RL-based control mechanism results in a robust control strategy that can handle uncertainties in the motion of the UAV, offering promising performance. Specifically, our method achieves 92\% accuracy in terms of average displacement error (i.e. the mean distance between the target and obtained trajectory points) using Q-learning with 15,000 episodes
</details>
<details>
<summary>摘要</summary>
本文研究了一种天空 manipulate系统，即一架有控制的机械臂的无人飞行器（UAV），用于实现飞行中 actuation 任务。我们的解决方案基于 employing Q-learning 方法控制 manipulate 系统的 trajectory，具体来说是控制 manipulate 系统的端效器（end-effector）的运动轨迹。我们开发了基于 Time To Collision（TTC）的动态规划模型，使得四旋翼 UAV 可以在避免碰撞的情况下环绕障碍物移动。此外，我们利用模型基于 Q-learning 模型独立跟踪和控制 manipulate 系统的端效器的desired trajectory，给出了一个arbitrary baseline trajectory для UAV 平台。这种组合使得 manipulate 系统可以完成高空焊接、结构监测和修理、电池更换、尖顶扫除、高层建筑清洁、电缆维护等多种 actuation 任务，而且保持与飞行控制 firmware 的兼容性。我们的RL-based control mechanism 实现了一种可靠的控制策略，可以处理 UAV 运动中的不确定性，提供了promising performance。具体来说，我们的方法在15,000个episode中达到了92%的均方差误差率（即target和实际轨迹点之间的平均距离）使用 Q-learning。
</details></li>
</ul>
<hr>
<h2 id="Short-Run-Transit-Route-Planning-Decision-Support-System-Using-a-Deep-Learning-Based-Weighted-Graph"><a href="#Short-Run-Transit-Route-Planning-Decision-Support-System-Using-a-Deep-Learning-Based-Weighted-Graph" class="headerlink" title="Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph"></a>Short Run Transit Route Planning Decision Support System Using a Deep Learning-Based Weighted Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12828">http://arxiv.org/abs/2308.12828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadav Shalit, Michael Fire, Dima Kagan, Eran Ben-Elia</li>
<li>for: 提高公共交通服务的效率和可靠性，协助公共交通观察员快速地找到短程改善。</li>
<li>methods: 使用深度学习方法，利用多种数据来源（如GTFS和智能卡数据）提取特征，建立交通网络 graphs，并通过自我监督进行预测。</li>
<li>results: 透过评估在 Tel Aviv 的应用，能够降低更多于 9% 的路线时间，包括城市内部和郊区路线，实现更高效的公共交通服务。<details>
<summary>Abstract</summary>
Public transport routing plays a crucial role in transit network design, ensuring a satisfactory level of service for passengers. However, current routing solutions rely on traditional operational research heuristics, which can be time-consuming to implement and lack the ability to provide quick solutions. Here, we propose a novel deep learning-based methodology for a decision support system that enables public transport (PT) planners to identify short-term route improvements rapidly. By seamlessly adjusting specific sections of routes between two stops during specific times of the day, our method effectively reduces times and enhances PT services. Leveraging diverse data sources such as GTFS and smart card data, we extract features and model the transportation network as a directed graph. Using self-supervision, we train a deep learning model for predicting lateness values for road segments.   These lateness values are then utilized as edge weights in the transportation graph, enabling efficient path searching. Through evaluating the method on Tel Aviv, we are able to reduce times on more than 9\% of the routes. The improved routes included both intraurban and suburban routes showcasing a fact highlighting the model's versatility. The findings emphasize the potential of our data-driven decision support system to enhance public transport and city logistics, promoting greater efficiency and reliability in PT services.
</details>
<details>
<summary>摘要</summary>
公共交通路径规划在城市交通网络设计中发挥关键作用，为乘客提供满意的服务水平。然而，当前的路径解决方案通常基于传统的运筹学方法，可能需要很长时间来实施并且无法提供快速的解决方案。在这里，我们提出了一种基于深度学习的决策支持系统，可以帮助公共交通（PT）规划员在短时间内迅速地改善路径。通过在特定时间段和特定路段上轻松调整路径，我们的方法可以减少时间并提高PT服务质量。通过利用不同的数据源，such as GTFS和智能卡数据，我们提取特征并模型了交通网络为指向图。使用无监督学习，我们训练了深度学习模型，以预测路段延迟值。这些延迟值然后被用作交通图的边权重，使得 PATH 搜索更加高效。通过对特拉维夫进行评估，我们可以减少路径时间超过 9%。改进的路径包括了城市内部和郊区路径，这一结果表明模型的 universality。发现表明了我们数据驱动的决策支持系统的潜力，以提高公共交通和城市物流的效率和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Prediction-without-Preclusion-Recourse-Verification-with-Reachable-Sets"><a href="#Prediction-without-Preclusion-Recourse-Verification-with-Reachable-Sets" class="headerlink" title="Prediction without Preclusion: Recourse Verification with Reachable Sets"></a>Prediction without Preclusion: Recourse Verification with Reachable Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12820">http://arxiv.org/abs/2308.12820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avni Kothari, Bogdan Kulynych, Tsui-Wei Weng, Berk Ustun</li>
<li>for: 这篇论文是关于机器学习模型如何决定提供借款、面试或公共援助的。</li>
<li>methods: 这篇论文使用了一种正式的测试过程来检测模型是否可以提供“回退”（recourse），并开发了一种机制来确定模型是否可以在用户指定的行动可行性约束下提供回退。</li>
<li>results: 研究人员使用了这种方法来检测真实世界数据集中的模型是否可以提供回退，并发现了一些模型可能会不可逆地分配预测，从而永久地排除用户访问借款、面试或援助的机会。<details>
<summary>Abstract</summary>
Machine learning models are often used to decide who will receive a loan, a job interview, or a public benefit. Standard techniques to build these models use features about people but overlook their actionability. In turn, models can assign predictions that are fixed, meaning that consumers who are denied loans, interviews, or benefits may be permanently locked out from access to credit, employment, or assistance. In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar access, and we provide tools to design algorithms that account for actionability when developing models.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce a formal testing procedure to flag models that assign fixed predictions that we call recourse verification. We develop machinery to reliably determine if a given model can provide recourse to its decision subjects from a set of user-specified actionability constraints. We demonstrate how our tools can ensure recourse and adversarial robustness in real-world datasets and use them to study the infeasibility of recourse in real-world lending datasets. Our results highlight how models can inadvertently assign fixed predictions that permanently bar access, and we provide tools to design algorithms that account for actionability when developing models.Translated into Simplified Chinese:机器学习模型经常用于决定谁会获得贷款、面试或公共援助。标准的建模技术会忽略人们的行动能力。因此，模型可能会分配 fixes 的预测，这意味着被拒绝的消费者可能会被永久排除出访问借贷、就业或援助的机会。在这项工作中，我们引入了一种正式的测试过程，用于检测模型是否可以提供回退（recourse），我们称之为回退验证。我们开发了一套可靠地确定给定模型是否可以通过用户指定的行动能力约束来提供回退。我们示例了我们的工具可以在实际数据集中确保回退和对抗强度，并使用它们来研究实际借贷数据集中回退的不可能性。我们的结果表明模型可能会不慎地分配 fixes 的预测，永久排除访问权限，我们提供了工具来设计考虑行动能力的算法。
</details></li>
</ul>
<hr>
<h2 id="Job-Shop-Scheduling-Benchmark-Environments-and-Instances-for-Learning-and-Non-learning-Methods"><a href="#Job-Shop-Scheduling-Benchmark-Environments-and-Instances-for-Learning-and-Non-learning-Methods" class="headerlink" title="Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods"></a>Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12794">http://arxiv.org/abs/2308.12794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-for-decision-making-tue/job_shop_scheduling_benchmark">https://github.com/ai-for-decision-making-tue/job_shop_scheduling_benchmark</a></li>
<li>paper_authors: Robbert Reijnen, Kjell van Straaten, Zaharah Bukhsh, Yingqian Zhang</li>
<li>for: 提供一个开源的GitHub存储库，包含各种机器调度问题的完整 benchmark，包括Job Shop Scheduling (JSP)、Flow Shop Scheduling (FSP)、Flexible Job Shop Scheduling (FJSP)、FAJSP with Assembly constraints (FAJSP)、FJSP with Sequence-Dependent Setup Times (FJSP-SDST)、在线 FJSP (with online job arrivals)。</li>
<li>methods: 提供一个中心化的平台，供研究者、实践者和爱好者一起解决机器调度问题。</li>
<li>results: 提供一个完整的 benchmark，可以帮助研究者和实践者更好地了解不同机器调度问题的特点和挑战。<details>
<summary>Abstract</summary>
We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
</details>
<details>
<summary>摘要</summary>
我们介绍一个开源的GitHub存储库，包含了广泛的机器安排问题的测试，包括作业shop scheduling (JSP)、流行shop scheduling (FSP)、可动作job shop scheduling (FJSP)、FJSP具有组装约束 (FAJSP)、FJSP具有时间相依的组装 (FJSP-SDST)，以及在线的FJSP。我们的主要目标是提供一个中央集中地 для研究人员、实践者和热爱者，来解决机器安排挑战。
</details></li>
</ul>
<hr>
<h2 id="Single-shot-Bayesian-approximation-for-neural-networks"><a href="#Single-shot-Bayesian-approximation-for-neural-networks" class="headerlink" title="Single-shot Bayesian approximation for neural networks"></a>Single-shot Bayesian approximation for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12785">http://arxiv.org/abs/2308.12785</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaibrach/Moment-Propagation">https://github.com/kaibrach/Moment-Propagation</a></li>
<li>paper_authors: Kai Brach, Beate Sick, Oliver Dürr</li>
<li>for: 提高深度神经网络（NN）的预测性能和不确定性评估。</li>
<li>methods: 使用蒙те Carlo（MC）抽取和粒子抽取来提供不确定性度量，并同时提高预测性能。</li>
<li>results: 提出了一种单次MC抽取近似方法，可以快速地对常用的NN层（如卷积层、最大池化层、稠密层、softmax层和抽取层）进行预测，并且可以Analytically approximate MC抽取信号的期望值和方差。这种方法可以将标准抽取采用的NN转换成BNN，无需重新训练。在不同的 benchmark 数据集和一个模拟的玩具示例中进行了评估，并证明了这种单次MC抽取近似方法可以快速地提供预测分布的点估计和不确定度评估，而且可以在实时部署BNN时使用。此外，我们还证明了将部分时间用于将MP方法与深度ensemble技术相结合可以进一步改善不确定度评估。<details>
<summary>Abstract</summary>
Deep neural networks (NNs) are known for their high-prediction performances. However, NNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of NNs (BNNs), such as Monte Carlo (MC) dropout BNNs, do provide uncertainty measures and simultaneously increase the prediction performance. The only disadvantage of BNNs is their higher computation time during test time because they rely on a sampling approach. Here we present a single-shot MC dropout approximation that preserves the advantages of BNNs while being as fast as NNs. Our approach is based on moment propagation (MP) and allows to analytically approximate the expected value and the variance of the MC dropout signal for commonly used layers in NNs, i.e. convolution, max pooling, dense, softmax, and dropout layers. The MP approach can convert an NN into a BNN without re-training given the NN has been trained with standard dropout. We evaluate our approach on different benchmark datasets and a simulated toy example in a classification and regression setting. We demonstrate that our single-shot MC dropout approximation resembles the point estimate and the uncertainty estimate of the predictive distribution that is achieved with an MC approach, while being fast enough for real-time deployments of BNNs. We show that using part of the saved time to combine our MP approach with deep ensemble techniques does further improve the uncertainty measures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Intentionally-underestimated-Value-Function-at-Terminal-State-for-Temporal-difference-Learning-with-Mis-designed-Reward"><a href="#Intentionally-underestimated-Value-Function-at-Terminal-State-for-Temporal-difference-Learning-with-Mis-designed-Reward" class="headerlink" title="Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward"></a>Intentionally-underestimated Value Function at Terminal State for Temporal-difference Learning with Mis-designed Reward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12772">http://arxiv.org/abs/2308.12772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taisuke Kobayashi</li>
<li>for: 解决TD学习在终端时的问题，即在终端时强制设置值为零，导致不INTENTIONAL的过高估计或者下降估计，具体来说是在任务失败时导致高估计，Acquire wrong policy。</li>
<li>methods: 提出一种方法，INTENTIONALLY underestimate the value after termination to avoid learning failures due to unintentional overestimation。同时，根据终端状态的度适应性来调整度量下降的程度，以避免过度探索。</li>
<li>results: 通过实验和实际机器人实验，显示该方法可以稳定地获得多种任务和奖励设计的优化策略。<details>
<summary>Abstract</summary>
Robot control using reinforcement learning has become popular, but its learning process generally terminates halfway through an episode for safety and time-saving reasons. This study addresses the problem of the most popular exception handling that temporal-difference (TD) learning performs at such termination. That is, by forcibly assuming zero value after termination, unintentionally implicit underestimation or overestimation occurs, depending on the reward design in the normal states. When the episode is terminated due to task failure, the failure may be highly valued with the unintentional overestimation, and the wrong policy may be acquired. Although this problem can be avoided by paying attention to the reward design, it is essential in practical use of TD learning to review the exception handling at termination. This paper therefore proposes a method to intentionally underestimate the value after termination to avoid learning failures due to the unintentional overestimation. In addition, the degree of underestimation is adjusted according to the degree of stationarity at termination, thereby preventing excessive exploration due to the intentional underestimation. Simulations and real robot experiments showed that the proposed method can stably obtain the optimal policies for various tasks and reward designs. https://youtu.be/AxXr8uFOe7M
</details>
<details>
<summary>摘要</summary>
机器人控制使用反做学习已经广泛应用，但其学习过程通常在一个 episoden 中中止，以保证安全和时间效率。这个研究解决了反做学习在中止时的最常见问题，即TD学习在中止时对未完成任务的执行进行强制设置为零值，导致不INTENDED 的下降或上涨。当 episoden 中止由任务失败引起时，失败可能具有不INTENDED 的高估，并且可能获得错误的策略。虽然这个问题可以通过奖励设计的注意来避免，但在TD学习的实际应用中，需要 periodic 检查中止时的例外处理。这篇论文因此提出了一种将中止时强制设置为零值，以避免由不INTENDED 的高估导致的学习失败。此外，根据中止时的稳定性度进行调整，以避免由意外估计导致的过度探索。实验和实际机器人实验表明，提议的方法可以稳定地获得多种任务和奖励设计的优化策略。
</details></li>
</ul>
<hr>
<h2 id="On-the-Consistency-of-Average-Embeddings-for-Item-Recommendation"><a href="#On-the-Consistency-of-Average-Embeddings-for-Item-Recommendation" class="headerlink" title="On the Consistency of Average Embeddings for Item Recommendation"></a>On the Consistency of Average Embeddings for Item Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12767">http://arxiv.org/abs/2308.12767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deezer/consistency">https://github.com/deezer/consistency</a></li>
<li>paper_authors: Walid Bendada, Guillaume Salha-Galvan, Romain Hennequin, Thomas Bouabça, Tristan Cazenave</li>
<li>for: 这篇论文 investigate了权重平均法的合理性，它是一种常见的推荐系统方法。</li>
<li>methods: 作者提出了一个预期准确度分数，用于衡量权重平均的一致性。然后，他们分析了这个分数的数学表达，以及其在实际数据上的实验行为。</li>
<li>results: 研究发现，实际的权重平均不太一致于对应的理论设定，这提供了未来研究的可能性。<details>
<summary>Abstract</summary>
A prevalent practice in recommender systems consists of averaging item embeddings to represent users or higher-level concepts in the same embedding space. This paper investigates the relevance of such a practice. For this purpose, we propose an expected precision score, designed to measure the consistency of an average embedding relative to the items used for its construction. We subsequently analyze the mathematical expression of this score in a theoretical setting with specific assumptions, as well as its empirical behavior on real-world data from music streaming services. Our results emphasize that real-world averages are less consistent for recommendation, which paves the way for future research to better align real-world embeddings with assumptions from our theoretical setting.
</details>
<details>
<summary>摘要</summary>
一种常见的做法在推荐系统中是将item embedding平均化以表示用户或更高层次的概念在同一个嵌入空间中。这篇论文检查了这种做法的相关性。为此，我们提出了一个预期精度分数，用于衡量average embedding的一致性 relative to the items used for its construction。我们随后对这个分数的数学表达在具体的假设下进行了分析，以及其在实际数据上的实际行为。我们的结果表明，实际中的均值更不一致，这为未来的研究提供了更好地将实际嵌入与假设中的嵌入相一致的可能性。
</details></li>
</ul>
<hr>
<h2 id="IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation"><a href="#IP-UNet-Intensity-Projection-UNet-Architecture-for-3D-Medical-Volume-Segmentation" class="headerlink" title="IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation"></a>IP-UNet: Intensity Projection UNet Architecture for 3D Medical Volume Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12761">http://arxiv.org/abs/2308.12761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nyothiri Aung, Tahar Kechadi, Liming Chen, Sahraoui Dhelim</li>
<li>For: The paper is written for proposing an end-to-end deep learning approach called IP-UNet for multi-class segmentation of 3D volumetric data, specifically for automatic breast calcification detection.* Methods: The paper uses a UNet-based model that operates on Intensity Projection (IP) of 3D volumetric data, which reduces the memory consumption and maintains the original image resolution. The model is compared with two other methods: 2D UNet model for slice-by-slice segmentation and 3D-UNet model for direct segmentation of 3D volumes.* Results: The experimental results show that IP-UNet achieves similar segmentation accuracy as 3D-UNet but with much better performance in terms of training time and memory consumption. Specifically, IP-UNet reduces the training time by 70% and memory consumption by 92%.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了提出一种终端深度学习方法，即IP-UNet，用于多类分割三维数据集，特别是自动乳腺细胞检测。* Methods: 该论文使用基于UNet的模型，将三维数据集转化为强度投影(IP)，从而降低内存使用量并保持原始图像分辨率。模型与两种其他方法进行比较：2D UNet模型进行 slice-by-slice分割，以及直接使用3D-UNet模型进行3D分割。* Results: 实验结果表明，IP-UNet可以与3D-UNet准确率相似，但性能更好，具体来说，它降低了训练时间70%，内存使用量92%。<details>
<summary>Abstract</summary>
CNNs have been widely applied for medical image analysis. However, limited memory capacity is one of the most common drawbacks of processing high-resolution 3D volumetric data. 3D volumes are usually cropped or downsized first before processing, which can result in a loss of resolution, increase class imbalance, and affect the performance of the segmentation algorithms. In this paper, we propose an end-to-end deep learning approach called IP-UNet. IP-UNet is a UNet-based model that performs multi-class segmentation on Intensity Projection (IP) of 3D volumetric data instead of the memory-consuming 3D volumes. IP-UNet uses limited memory capability for training without losing the original 3D image resolution. We compare the performance of three models in terms of segmentation accuracy and computational cost: 1) Slice-by-slice 2D segmentation of the CT scan images using a conventional 2D UNet model. 2) IP-UNet that operates on data obtained by merging the extracted Maximum Intensity Projection (MIP), Closest Vessel Projection (CVP), and Average Intensity Projection (AvgIP) representations of the source 3D volumes, then applying the UNet model on the output IP images. 3) 3D-UNet model directly reads the 3D volumes constructed from a series of CT scan images and outputs the 3D volume of the predicted segmentation. We test the performance of these methods on 3D volumetric images for automatic breast calcification detection. Experimental results show that IP-Unet can achieve similar segmentation accuracy with 3D-Unet but with much better performance. It reduces the training time by 70\% and memory consumption by 92\%.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗图像分析中广泛应用。然而，处理高分辨率三维数据的内存容量有限是最常见的缺点。通常会将三维数据进行裁剪或压缩，从而导致分辨率下降、类别不均衡加大和分割算法性能下降。在这篇论文中，我们提出了一种终端深度学习方法，即IP-UNet。IP-UNet是基于UNet模型的终端深度学习方法，对三维图像的强化投影（IP）进行多类分割，而不是耗费内存的三维数据。IP-UNet在训练中不失去原始三维图像的分辨率，并且具有有限的内存 capacidad。我们将三种模型进行比较，分别是：1）通过切割CT扫描图像的层次进行二维分割，使用传统的二维UNet模型进行分割。2）IP-UNet，它在提取的最大强化投影（MIP）、最近血管投影（CVP）和均衡强化投影（AvgIP）表示中提取数据，然后应用UNet模型对输出的IP图像进行分割。3）直接使用CT扫描图像序列构建的三维体volume，使用3D-UNet模型进行分割。我们对这些方法在三维体积图像自动乳腺病变检测中的性能进行测试。实验结果表明，IP-UNet可以与3D-UNet具有相同的分割精度，但是具有训练时间减少70%和内存消耗减少92%的优势。
</details></li>
</ul>
<hr>
<h2 id="Motion-In-Betweening-with-Phase-Manifolds"><a href="#Motion-In-Betweening-with-Phase-Manifolds" class="headerlink" title="Motion In-Betweening with Phase Manifolds"></a>Motion In-Betweening with Phase Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12751">http://arxiv.org/abs/2308.12751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pauzii/phasebetweener">https://github.com/pauzii/phasebetweener</a></li>
<li>paper_authors: Paul Starke, Sebastian Starke, Taku Komura, Frank Steinicke</li>
<li>for: 这种研究旨在开发一种基于数据驱动的人物动作 interpolating 系统，以达到目标pose的character。</li>
<li>methods: 这种方法使用Periodic Autoencoder学习的阶段变量，并使用权重的混合 neural network 模型，以生成在当前和目标状态之间的动作序列。在满足特定的制定pose或特定的终端器的约束下，还实现了学习控制方案。</li>
<li>results: 结果表明，使用阶段变量进行动作 interpolating 可以增强 interpolated 运动的精度，并且可以在长transition duration 下稳定学习过程。此外，这种方法还可以synthesize 更加复杂的运动，以及在给定的目标帧之间实现样式控制。与当前状态艺技相比，这种方法可以具有类似的运动质量和泛化能力，尤其是在存在长transition duration 时。<details>
<summary>Abstract</summary>
This paper introduces a novel data-driven motion in-betweening system to reach target poses of characters by making use of phases variables learned by a Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network model, in which the phases cluster movements in both space and time with different expert weights. Each generated set of weights then produces a sequence of poses in an autoregressive manner between the current and target state of the character. In addition, to satisfy poses which are manually modified by the animators or where certain end effectors serve as constraints to be reached by the animation, a learned bi-directional control scheme is implemented to satisfy such constraints. The results demonstrate that using phases for motion in-betweening tasks sharpen the interpolated movements, and furthermore stabilizes the learning process. Moreover, using phases for motion in-betweening tasks can also synthesize more challenging movements beyond locomotion behaviors. Additionally, style control is enabled between given target keyframes. Our proposed framework can compete with popular state-of-the-art methods for motion in-betweening in terms of motion quality and generalization, especially in the existence of long transition durations. Our framework contributes to faster prototyping workflows for creating animated character sequences, which is of enormous interest for the game and film industry.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Human-Comprehensible-Active-Learning-of-Genome-Scale-Metabolic-Networks"><a href="#Human-Comprehensible-Active-Learning-of-Genome-Scale-Metabolic-Networks" class="headerlink" title="Human Comprehensible Active Learning of Genome-Scale Metabolic Networks"></a>Human Comprehensible Active Learning of Genome-Scale Metabolic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12740">http://arxiv.org/abs/2308.12740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lun Ai, Shi-Shun Liang, Wang-Zhou Dai, Liam Hallett, Stephen H. Muggleton, Geoff S. Baldwin</li>
<li>for: 这项研究旨在提高生物工程的HOST Cell系统设计、建立和测试过程中的效率和成本。</li>
<li>methods: 该研究使用了一种新的机器学习框架ILP-iML1515，基于推理逻辑编程（ILP），实现了推理逻辑推理和从训练示例学习。</li>
<li>results: ILP-iML1515能够高效地进行大规模 simulations，并可以通过学习新的逻辑结构来更新模型，从而降低了学习基因功能的实验成本。<details>
<summary>Abstract</summary>
An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that reduce the experimental cost of learning gene functions in comparison to randomly selected experiments.
</details>
<details>
<summary>摘要</summary>
Important applications of synthetic biology include engineering host cell systems to produce useful products. However, as the scale of the host system increases, the design space grows exponentially, requiring a large number of validation trials with high experimental costs. To address this challenge, we need a machine learning approach that efficiently explores the hypothesis space and guides experimental design.We propose a novel machine learning framework called ILP-iML1515 based on inductive logic programming (ILP). This framework performs abductive logical reasoning and actively learns from training examples. Unlike numerical models, ILP-iML1515 uses comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials.The ILP-iML1515 framework offers two key advantages:1. High-throughput simulations: The framework allows for high-throughput simulations, enabling the rapid exploration of the design space.2. Experiment selection: The framework actively selects experiments that reduce the experimental cost of learning gene functions, compared to randomly selected experiments.In summary, ILP-iML1515 is a novel machine learning framework that efficiently explores the hypothesis space and guides experimental design for the design-build-test-learn (DBTL) cycle of host cell systems, reducing the experimental cost of learning gene functions.
</details></li>
</ul>
<hr>
<h2 id="Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion"><a href="#Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion" class="headerlink" title="Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion"></a>Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12734">http://arxiv.org/abs/2308.12734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan J. Bird, Ahmad Lotfi</li>
<li>for: 本研究旨在 Addressing the growing ethical concerns surrounding generative AI in the speech domain, particularly DeepFake Voice Conversion.</li>
<li>methods: 研究使用了 Retrieval-based Voice Conversion 技术，生成了 DEEP-VOICE 数据集，包含 eight well-known figures 的真实人类语音和他们之间的转换语音。将 speech 分类为真实语音和 AI-generated 语音，并通过 Statistical analysis of temporal audio features 进行分析，发现了两者之间存在显著不同的分布。</li>
<li>results: 研究人员使用 Hyperparameter optimisation 进行机器学习模型的训练，并实现了一个 Extreme Gradient Boosting 模型，可以在10-fold cross validation中达到平均分类精度为99.3%，并在约0.004毫秒内分类语音。所有数据生成于本研究都公开发布，供未来研究人员进行 AI speech detection 的研究。<details>
<summary>Abstract</summary>
There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learning models over 10-fold cross validation, it is found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection.
</details>
<details>
<summary>摘要</summary>
“Currently, there are growing implications surrounding generative AI in the speech domain, which enables voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation. Therefore, there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address these emerging issues, the DEEP-VOICE dataset was generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. The problem is presented as a binary classification issue, with statistical analysis of temporal audio features through t-testing revealing significantly different distributions. Hyperparameter optimization is implemented for machine learning models to identify the source of speech. After training 208 individual machine learning models over 10-fold cross validation, it was found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection.”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Out-of-the-Box-Thinking-Improving-Customer-Lifetime-Value-Modelling-via-Expert-Routing-and-Game-Whale-Detection"><a href="#Out-of-the-Box-Thinking-Improving-Customer-Lifetime-Value-Modelling-via-Expert-Routing-and-Game-Whale-Detection" class="headerlink" title="Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection"></a>Out of the Box Thinking: Improving Customer Lifetime Value Modelling via Expert Routing and Game Whale Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12729">http://arxiv.org/abs/2308.12729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Zhang, Xin Yan, Xuejiao Yang, Binfeng Jia, Shuangyang Wang</li>
<li>For: 这种研究旨在提高手机游戏发布者的用户生命周期预测（LTV）和游戏鲨鱼检测（Game Whale Detection）的准确性。* Methods: 该研究提出了一种名为ExpLTV的多任务框架，通过将游戏鲨鱼检测作为门控网络，可以融合共享信息和场景特定信息，以提高LTV预测的准确性。* Results: 经过广泛的实验 validate，ExpLTV在三个industrial dataset上显示出了明显的超越性。<details>
<summary>Abstract</summary>
Customer lifetime value (LTV) prediction is essential for mobile game publishers trying to optimize the advertising investment for each user acquisition based on the estimated worth. In mobile games, deploying microtransactions is a simple yet effective monetization strategy, which attracts a tiny group of game whales who splurge on in-game purchases. The presence of such game whales may impede the practicality of existing LTV prediction models, since game whales' purchase behaviours always exhibit varied distribution from general users. Consequently, identifying game whales can open up new opportunities to improve the accuracy of LTV prediction models. However, little attention has been paid to applying game whale detection in LTV prediction, and existing works are mainly specialized for the long-term LTV prediction with the assumption that the high-quality user features are available, which is not applicable in the UA stage. In this paper, we propose ExpLTV, a novel multi-task framework to perform LTV prediction and game whale detection in a unified way. In ExpLTV, we first innovatively design a deep neural network-based game whale detector that can not only infer the intrinsic order in accordance with monetary value, but also precisely identify high spenders (i.e., game whales) and low spenders. Then, by treating the game whale detector as a gating network to decide the different mixture patterns of LTV experts assembling, we can thoroughly leverage the shared information and scenario-specific information (i.e., game whales modelling and low spenders modelling). Finally, instead of separately designing a purchase rate estimator for two tasks, we design a shared estimator that can preserve the inner task relationships. The superiority of ExpLTV is further validated via extensive experiments on three industrial datasets.
</details>
<details>
<summary>摘要</summary>
顾客全生值预测（LTV）是移动游戏发布商必须优化每个用户获取的广告投资的关键，根据估计的值来进行优化。在移动游戏中，实施微交易是一种简单而有效的营收化渠道，吸引了一些游戏巨鲸，这些巨鲸在游戏中购买各种内购。巨鲸的购买行为可能会妨碍现有的LTV预测模型的实用性，因为巨鲸的购买行为总是与普通用户的购买行为存在差异。因此，识别巨鲸可以开启新的机会，以提高LTV预测模型的准确性。然而，目前对游戏巨鲸检测的应用在LTV预测中受到了少量的关注，现有的工作主要是长期LTV预测，假设高质量的用户特征可以获取，这不适用于UA阶段。在本文中，我们提出了ExpLTV，一种新的多任务框架，用于同时进行LTV预测和游戏巨鲸检测。在ExpLTV中，我们首先创新地设计了一个深度神经网络基于的游戏巨鲸检测器，可以不只是根据财务价值来INFER内在顺序，还可以精准地识别高支付者（即游戏巨鲸）和低支付者。然后，我们将游戏巨鲸检测器作为LTV预测器的闭合网络，以便全面利用共享信息和场景特定信息（即游戏巨鲸模型和低支付者模型）。最后，相反于分别设计两个任务的购买率估计器，我们设计了共享的估计器，以保持内任务之间的关系。ExpLTV的优势得到了EXTENSIVE的实验 validate，在三个industrial dataset上。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Reinforcement-Learning-based-Dynamic-Difficulty-Adjustment-in-a-Visual-Working-Memory-Game"><a href="#Continuous-Reinforcement-Learning-based-Dynamic-Difficulty-Adjustment-in-a-Visual-Working-Memory-Game" class="headerlink" title="Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game"></a>Continuous Reinforcement Learning-based Dynamic Difficulty Adjustment in a Visual Working Memory Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12726">http://arxiv.org/abs/2308.12726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masoud Rahimi, Hadi Moradi, Abdol-hossein Vahabie, Hamed Kebriaei</li>
<li>for: 提高玩家的游戏体验 (enhance the player’s experience in video games)</li>
<li>methods: 使用可观察学习 (RL) 方法，并在非竞争性游戏中应用连续状态动作空间 (continuous state-action space)</li>
<li>results: 提出了一种基于RL的可观察学习改进游戏难度调整方法，该方法可以根据玩家的得分和上一次游戏难度来调整游戏难度，从而提高玩家的游戏体验和得分。通过对52名参与者进行 Within-subject experiment 进行评估，该方法与两种基于规则的难度调整方法进行比较，并表明该方法可以提供更好的游戏体验、更高的得分和更高的胜率，同时也可以避免在20个尝试中的得分下降。<details>
<summary>Abstract</summary>
Dynamic Difficulty Adjustment (DDA) is a viable approach to enhance a player's experience in video games. Recently, Reinforcement Learning (RL) methods have been employed for DDA in non-competitive games; nevertheless, they rely solely on discrete state-action space with a small search space. In this paper, we propose a continuous RL-based DDA methodology for a visual working memory (VWM) game to handle the complex search space for the difficulty of memorization. The proposed RL-based DDA tailors game difficulty based on the player's score and game difficulty in the last trial. We defined a continuous metric for the difficulty of memorization. Then, we consider the task difficulty and the vector of difficulty-score as the RL's action and state, respectively. We evaluated the proposed method through a within-subject experiment involving 52 subjects. The proposed approach was compared with two rule-based difficulty adjustment methods in terms of player's score and game experience measured by a questionnaire. The proposed RL-based approach resulted in a significantly better game experience in terms of competence, tension, and negative and positive affect. Players also achieved higher scores and win rates. Furthermore, the proposed RL-based DDA led to a significantly less decline in the score in a 20-trial session.
</details>
<details>
<summary>摘要</summary>
“智能困难调整”（DDA）是一种可以增强玩家体验的游戏技术。近期，人工智能学习（RL）方法已经应用于非竞争性游戏中的 DDA，但是它们仅将状态动作空间划分为简单的类别。在这篇论文中，我们提出了一个基于RL的绘谱记忆游戏中的连续RL-DDA方法。这个方法根据玩家的得分和上一次游戏的难度来调整游戏的困难度。我们定义了一个连续的记忆难度度量，然后将任务难度和难度得分作为RL的动作和状态。我们透过对52名参与者进行在内体验研究来评估这个方法。该方法与两种基于规则的困难调整方法进行比较，并且根据玩家的得分和游戏体验（Questionnaire）进行评估。结果显示，提案的RL-DDA方法可以提供更好的游戏体验，包括能力感、紧张感、正面和负面情感。玩家也取得了更高的得分和胜率。此外，RL-DDA还导致游戏20次的得分下降较少。”
</details></li>
</ul>
<hr>
<h2 id="Solving-Forward-and-Inverse-Problems-of-Contact-Mechanics-using-Physics-Informed-Neural-Networks"><a href="#Solving-Forward-and-Inverse-Problems-of-Contact-Mechanics-using-Physics-Informed-Neural-Networks" class="headerlink" title="Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks"></a>Solving Forward and Inverse Problems of Contact Mechanics using Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12716">http://arxiv.org/abs/2308.12716</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. Sahin, M. von Danwitz, A. Popp</li>
<li>for: 本文使用物理学 Informed Neural Networks (PINNs) 解决了小塑性弹性理论中的前向和反向问题。</li>
<li>methods: 本文使用了混合变量的 PINNs 形式，通过输出变换来强制执行 Dirichlet 和 Neumann 边界条件作为硬件约束。 具有 KKT 类型条件的不等约束被 incorporated 到网络训练中的损失函数中，以便在训练过程中作为软约束。</li>
<li>results: 本文表明了 PINNs 可以作为纯 PDE 解决器，数据增强的前向模型，参数透析的 inverse 解决方案，以及快速评估的代理模型。 此外，本文还证明了选择合适的 hyperparameter，如损失权重，以及组合 Adam 和 L-BFGS-B 优化器可以提高准确性和训练时间。<details>
<summary>Abstract</summary>
This paper explores the ability of physics-informed neural networks (PINNs) to solve forward and inverse problems of contact mechanics for small deformation elasticity. We deploy PINNs in a mixed-variable formulation enhanced by output transformation to enforce Dirichlet and Neumann boundary conditions as hard constraints. Inequality constraints of contact problems, namely Karush-Kuhn-Tucker (KKT) type conditions, are enforced as soft constraints by incorporating them into the loss function during network training. To formulate the loss function contribution of KKT constraints, existing approaches applied to elastoplasticity problems are investigated and we explore a nonlinear complementarity problem (NCP) function, namely Fischer-Burmeister, which possesses advantageous characteristics in terms of optimization. Based on the Hertzian contact problem, we show that PINNs can serve as pure partial differential equation (PDE) solver, as data-enhanced forward model, as inverse solver for parameter identification, and as fast-to-evaluate surrogate model. Furthermore, we demonstrate the importance of choosing proper hyperparameters, e.g. loss weights, and a combination of Adam and L-BFGS-B optimizers aiming for better results in terms of accuracy and training time.
</details>
<details>
<summary>摘要</summary>
To formulate the loss function contribution of KKT constraints, we investigate existing approaches applied to elastoplasticity problems and explore a nonlinear complementarity problem (NCP) function, namely Fischer-Burmeister, which has advantageous characteristics in terms of optimization.Based on the Hertzian contact problem, we show that PINNs can serve as pure partial differential equation (PDE) solvers, as data-enhanced forward models, as inverse solvers for parameter identification, and as fast-to-evaluate surrogate models. Furthermore, we demonstrate the importance of choosing proper hyperparameters, such as loss weights, and a combination of Adam and L-BFGS-B optimizers to achieve better results in terms of accuracy and training time.
</details></li>
</ul>
<hr>
<h2 id="Disentanglement-Learning-via-Topology"><a href="#Disentanglement-Learning-via-Topology" class="headerlink" title="Disentanglement Learning via Topology"></a>Disentanglement Learning via Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12696">http://arxiv.org/abs/2308.12696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Balabin, Daria Voronkova, Ilya Trofimov, Evgeny Burnaev, Serguei Barannikov</li>
<li>for: 本研究的目的是提出一种基于多尺度拓扑特征的分解表示学习方法，以实现数据的解释性和深度学习模型的稳定性。</li>
<li>methods: 本方法基于VAE的架构，通过添加多尺度拓扑损失项来实现分解表示。我们从数据集的拓扑特征角度分析数据集的特征，并且通过优化拓扑相似性来优化分解表示。我们是第一个提出了可导的拓扑损失的论文。</li>
<li>results: 我们的实验表明，提出的拓扑损失可以提高分解表示的评价指标，如MIG、FactorVAE分数、SAP分数和DCI分解分数，与现有的最佳Result比较。此外，我们还示出了如何使用提出的拓扑损失来找到已经训练的GAN中的分解方向。<details>
<summary>Abstract</summary>
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised manner, permitting to apply it for problems without labeled factors of variation. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN.
</details>
<details>
<summary>摘要</summary>
我们提出了TopDis（拓扑分解）方法，通过添加多尺度拓扑损失项来学习分解表示。分解是深度学习模型的解释性和鲁棒性的重要属性，也是高级认知的一步。现状的方法是基于VAE（变量Autoencoder）的总 correlate损失。我们从数据集的拓扑性质来分析数据集的分解性。特别是，我们优化了数据集的拓扑相似性。到目前为止，我们的文章是第一篇提出了可导的拓扑损失的分解方法。我们的实验表明，我们的方法可以在不带标签因素的情况下提高分解分数，包括MIG、FactorVAE分数、SAP分数和DCI分解分数。我们的方法是无监督的，可以应用于无标签因素的问题。此外，我们还展示了如何使用我们的拓扑损失来找分解方向在训练过的GAN中。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Data-Analysis-Method-for-Big-Data-using-Multiple-Model-Linear-Regression"><a href="#An-Efficient-Data-Analysis-Method-for-Big-Data-using-Multiple-Model-Linear-Regression" class="headerlink" title="An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression"></a>An Efficient Data Analysis Method for Big Data using Multiple-Model Linear Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12691">http://arxiv.org/abs/2308.12691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Bohan Lyu, Jianzhong Li</li>
<li>for: 这篇论文提出了一种新的大数据分析方法，使用新定义的多模型线性回归（MMLR）模型，可以将输入数据集分解成子集并建立本地线性回归模型。</li>
<li>methods: 该论文提出了一种新的approx算法，基于($\epsilon$, $\delta$)-估计器，用于构建MMLR模型。论文还提供了对MMLR算法的数学证明，证明其时间复杂度为输入数据集的线性。</li>
<li>results: 该论文通过实验表明，MMLR算法在许多情况下与现有回归方法相当，而且它的运行时间相对较短。<details>
<summary>Abstract</summary>
This paper introduces a new data analysis method for big data using a newly defined regression model named multiple model linear regression(MMLR), which separates input datasets into subsets and construct local linear regression models of them. The proposed data analysis method is shown to be more efficient and flexible than other regression based methods. This paper also proposes an approximate algorithm to construct MMLR models based on $(\epsilon,\delta)$-estimator, and gives mathematical proofs of the correctness and efficiency of MMLR algorithm, of which the time complexity is linear with respect to the size of input datasets. This paper also empirically implements the method on both synthetic and real-world datasets, the algorithm shows to have comparable performance to existing regression methods in many cases, while it takes almost the shortest time to provide a high prediction accuracy.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的大数据分析方法，使用新定义的多模型线性回归（MMLR）模型，将输入数据集分解成子集并建立当地线性回归模型。提议的数据分析方法比其他回归基于方法更高效和灵活。这篇论文还提出了一种近似算法来构建MMLR模型，基于($\epsilon$, $\delta$)-估计器，并提供了数学证明MMLR算法的正确性和效率。时间复杂度为输入数据集的线性。此外，论文还通过实验证明了方法在 sintetic 和实际数据上的表现，其性能与现有回归方法相当，而时间几乎最短提供高预测精度。Note: "MMLR" stands for "Multiple Model Linear Regression" in English.
</details></li>
</ul>
<hr>
<h2 id="Match-And-Deform-Time-Series-Domain-Adaptation-through-Optimal-Transport-and-Temporal-Alignment"><a href="#Match-And-Deform-Time-Series-Domain-Adaptation-through-Optimal-Transport-and-Temporal-Alignment" class="headerlink" title="Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment"></a>Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12686">http://arxiv.org/abs/2308.12686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rtavenar/MatchAndDeform">https://github.com/rtavenar/MatchAndDeform</a></li>
<li>paper_authors: François Painblanc, Laetitia Chapel, Nicolas Courty, Chloé Friguet, Charlotte Pelletier, Romain Tavenard</li>
<li>for: 这篇论文旨在适应没有标签的大量数据的情况，通过源领域中的标签来分类目标领域的数据。</li>
<li>methods: 这篇论文提出了匹配和变形（Match-And-Deform，MAD）方法，该方法在源和目标时间序中寻找匹配，同时允许时间偏移。该方法通过优化transport损失和时间扭曲来同时对时间序进行对齐。</li>
<li>results: 实验结果表明，MAD可以做到有效地对时间序进行对齐和时间偏移估计，并且可以在深度神经网络中学习新的时间序表示，同时将领域对齐和分类性能提高。<details>
<summary>Abstract</summary>
While large volumes of unlabeled data are usually available, associated labels are often scarce. The unsupervised domain adaptation problem aims at exploiting labels from a source domain to classify data from a related, yet different, target domain. When time series are at stake, new difficulties arise as temporal shifts may appear in addition to the standard feature distribution shift. In this paper, we introduce the Match-And-Deform (MAD) approach that aims at finding correspondences between the source and target time series while allowing temporal distortions. The associated optimization problem simultaneously aligns the series thanks to an optimal transport loss and the time stamps through dynamic time warping. When embedded into a deep neural network, MAD helps learning new representations of time series that both align the domains and maximize the discriminative power of the network. Empirical studies on benchmark datasets and remote sensing data demonstrate that MAD makes meaningful sample-to-sample pairing and time shift estimation, reaching similar or better classification performance than state-of-the-art deep time series domain adaptation strategies.
</details>
<details>
<summary>摘要</summary>
大量未标注数据通常可available,但相关的标签却罕见。不supervised domain adaptation问题 aimsto exploit source domain中的标签来类别target domain中的数据。当时序序列是问题时，新的困难出现，因为特征分布shift可能会出现，同时还需要解决时间推移问题。在这篇论文中，我们引入了Match-And-Deform（MAD）方法，该方法 aimsto在source和target时序序列之间找到匹配，并允许时间扭曲。相关的优化问题同时使用了最优运输损失和时间戳Dynamic Time Warping来协调时序序列。当 embed into a deep neural network时，MAD可以帮助学习新的时序序列表示，同时同时对域和网络的泛化能力做出贡献。empirical studies on benchmark datasets and remote sensing data表明，MAD可以实现meaningful sample-to-sample pairing和时间偏移估计，达到或更好的深度时序领域域 adaptation表现。
</details></li>
</ul>
<hr>
<h2 id="LR-XFL-Logical-Reasoning-based-Explainable-Federated-Learning"><a href="#LR-XFL-Logical-Reasoning-based-Explainable-Federated-Learning" class="headerlink" title="LR-XFL: Logical Reasoning-based Explainable Federated Learning"></a>LR-XFL: Logical Reasoning-based Explainable Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12681">http://arxiv.org/abs/2308.12681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanci Zhang, Han Yu</li>
<li>for: 提高 Federated Learning（FL）模型的透明度和解释性，保护数据隐私。</li>
<li>methods:  incorporating logic-based explanations into FL， clients create local logic rules based on their local data and send them to the FL server，server aggregates local model updates with weight values determined by the quality of clients’ local data as reflected by their uploaded logic rules.</li>
<li>results: LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively，explicit rule evaluation and expression enable human experts to validate and correct the rules on the server side.<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging approach for training machine learning models collaboratively while preserving data privacy. The need for privacy protection makes it difficult for FL models to achieve global transparency and explainability. To address this limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach. Under LR-XFL, FL clients create local logic rules based on their local data and send them, along with model updates, to the FL server. The FL server connects the local logic rules through a proper logical connector that is derived based on properties of client data, without requiring access to the raw data. In addition, the server also aggregates the local model updates with weight values determined by the quality of the clients' local data as reflected by their uploaded logic rules. The results show that LR-XFL outperforms the most relevant baseline by 1.19%, 5.81% and 5.41% in terms of classification accuracy, rule accuracy and rule fidelity, respectively. The explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model's robustness to errors. It has the potential to enhance the transparency of FL models for areas like healthcare and finance where both data privacy and explainability are important.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种emergingapproach дляtrain machine learning model collaboratively while preserving data privacy. 由于需要隐私保护，FL模型难以 achieve global transparency和 explainability. 为了解决这些limitation, we incorporate logic-based explanations into FL by proposing the Logical Reasoning-based eXplainable Federated Learning (LR-XFL) approach.在LR-XFL中, FL客户端创建本地逻辑规则 based on their local data,并将其发送到FL服务器 alongside model updates. FL服务器通过基于客户端数据的属性 derive proper logical connector,而无需访问原始数据。此外，服务器还将本地模型更新与客户端数据质量所决定的权重值相结合。结果显示，LR-XFL比最相关的基准方案高1.19%、5.81%和5.41% in terms of classification accuracy, rule accuracy和 rule fidelity, respectively. explicit rule evaluation and expression under LR-XFL enable human experts to validate and correct the rules on the server side, hence improving the global FL model's robustness to errors. it has the potential to enhance the transparency of FL models for areas like healthcare and finance where both data privacy and explainability are important.
</details></li>
</ul>
<hr>
<h2 id="Master-slave-Deep-Architecture-for-Top-K-Multi-armed-Bandits-with-Non-linear-Bandit-Feedback-and-Diversity-Constraints"><a href="#Master-slave-Deep-Architecture-for-Top-K-Multi-armed-Bandits-with-Non-linear-Bandit-Feedback-and-Diversity-Constraints" class="headerlink" title="Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints"></a>Master-slave Deep Architecture for Top-K Multi-armed Bandits with Non-linear Bandit Feedback and Diversity Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12680">http://arxiv.org/abs/2308.12680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanghanchi/master-slave-algorithm-for-top-k-bandits">https://github.com/huanghanchi/master-slave-algorithm-for-top-k-bandits</a></li>
<li>paper_authors: Hanchi Huang, Li Shen, Deheng Ye, Wei Liu</li>
<li>for:  solving the top-$K$ combinatorial multi-armed bandits problem with non-linear bandit feedback and diversity constraints in recommendation tasks</li>
<li>methods:  introducing six slave models with distinguished merits to generate diversified samples balancing rewards and constraints, and using teacher learning based optimization and policy co-training technique to boost the performance of multiple slave models</li>
<li>results:  significantly surpassing existing state-of-the-art algorithms in both synthetic and real datasets for recommendation tasks.Here is the Chinese translation of the three key information points:</li>
<li>for: 解决 combinatorial multi-armed bandits problem 中的 top-$K$ 问题，具有非线性的飞行反馈和多样性约束，在推荐任务中。</li>
<li>methods: 引入 six 个具有独特优势的附属模型，以生成具有奖励和约束的多样化样本，并使用教师学习基于优化和策略共训技术来提高多个附属模型的性能。</li>
<li>results: 在 synthetic 和实际数据集中，与现有状态的算法相比，显著超越。<details>
<summary>Abstract</summary>
We propose a novel master-slave architecture to solve the top-$K$ combinatorial multi-armed bandits problem with non-linear bandit feedback and diversity constraints, which, to the best of our knowledge, is the first combinatorial bandits setting considering diversity constraints under bandit feedback. Specifically, to efficiently explore the combinatorial and constrained action space, we introduce six slave models with distinguished merits to generate diversified samples well balancing rewards and constraints as well as efficiency. Moreover, we propose teacher learning based optimization and the policy co-training technique to boost the performance of the multiple slave models. The master model then collects the elite samples provided by the slave models and selects the best sample estimated by a neural contextual UCB-based network to make a decision with a trade-off between exploration and exploitation. Thanks to the elaborate design of slave models, the co-training mechanism among slave models, and the novel interactions between the master and slave models, our approach significantly surpasses existing state-of-the-art algorithms in both synthetic and real datasets for recommendation tasks. The code is available at: \url{https://github.com/huanghanchi/Master-slave-Algorithm-for-Top-K-Bandits}.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的主奴隶架构，用于解决排名前-$K$  combinatorial多臂炮兵问题，具有非线性炮兵反馈和多样性约束。到目前为止，这是首次对 combinatorial bandits 设置中考虑多样性约束的研究。specifically，我们引入了六个奴隶模型，每个模型具有特殊优势，以生成多样化的样本，既保证奖励也保证约束。此外，我们提出了教师学习基于优化和策略共训技术，以提高多个奴隶模型的性能。master模型然后收集奴隶模型提供的精英样本，并通过神经网络基于上下文ual UCB 算法选择最佳样本，以实现 Explorer 和 exploiter 之间的平衡。由于奴隶模型的精心设计、奴隶模型之间的合作机制以及主奴隶模型与奴隶模型之间的新型互动，我们的方法在 synthetic 和实际数据集上对于推荐任务显著超越了现状的算法。代码可以在：\url{https://github.com/huanghanchi/Master-slave-Algorithm-for-Top-K-Bandits} 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification"><a href="#A-Continual-Learning-Approach-for-Cross-Domain-White-Blood-Cell-Classification" class="headerlink" title="A Continual Learning Approach for Cross-Domain White Blood Cell Classification"></a>A Continual Learning Approach for Cross-Domain White Blood Cell Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12679">http://arxiv.org/abs/2308.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Raheleh Salehi, Armin Gruber, Sayedali Shetab Boushehri, Pascal Giehr, Nassir Navab, Carsten Marr</li>
<li>for: 这篇论文旨在提出一种继续学习方法来实现白血球分类 tasks 中的持续学习，以应对在临床设定中不断改变的数据来源和疾病分类。</li>
<li>methods: 本文提出的方法是使用练习集选择法来选择 previous tasks 中最有代表性的数据，以解决继续学习时的忘却问题。</li>
<li>results: 本文的结果显示，该方法在三个不同的白血球分类 dataset 上均表现出色，并在 Cross-domain 环境中进行了持续学习。特别是在长期增量学习和跨领域学习 scenario 中，该方法均以最佳成绩超越了现有的 iCaRL 和 EWC 方法。<details>
<summary>Abstract</summary>
Accurate classification of white blood cells in peripheral blood is essential for diagnosing hematological diseases. Due to constantly evolving clinical settings, data sources, and disease classifications, it is necessary to update machine learning classification models regularly for practical real-world use. Such models significantly benefit from sequentially learning from incoming data streams without forgetting previously acquired knowledge. However, models can suffer from catastrophic forgetting, causing a drop in performance on previous tasks when fine-tuned on new data. Here, we propose a rehearsal-based continual learning approach for class incremental and domain incremental scenarios in white blood cell classification. To choose representative samples from previous tasks, we employ exemplar set selection based on the model's predictions. This involves selecting the most confident samples and the most challenging samples identified through uncertainty estimation of the model. We thoroughly evaluated our proposed approach on three white blood cell classification datasets that differ in color, resolution, and class composition, including scenarios where new domains or new classes are introduced to the model with every task. We also test a long class incremental experiment with both new domains and new classes. Our results demonstrate that our approach outperforms established baselines in continual learning, including existing iCaRL and EWC methods for classifying white blood cells in cross-domain environments.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将白血球分类在 péripheral 血液中的准确性是诊断血液疾病的关键。由于临床设置不断改变，数据源不断增加，疾病分类不断演化，因此需要定期更新机器学习分类模型以适应实际世界中的应用。这些模型受益于随着新数据流入而不断学习，而不会忘记之前学习的知识。然而，模型可能会出现慢速忘记，导致对先前任务的性能下降。在这种情况下，我们提出了一种基于熵的连续学习方法，适用于白血球分类的类增量和领域增量场景。我们使用模型预测结果来选择先前任务的表示样本。这里选择最有信心的样本和通过模型的不确定性测量得到的最具挑战性的样本。我们对三个不同的白血球分类数据集进行了严格的评估，包括分类任务中新增的领域和新类。我们还进行了长期类增量实验，其中模型需要处理新的领域和新类。我们的结果表明，我们的方法在跨领域环境中的 continual learning 中表现出色，比较了 EXISTS iCaRL 和 EWC 方法。
</details></li>
</ul>
<hr>
<h2 id="Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition"><a href="#Masked-Feature-Modelling-Feature-Masking-for-the-Unsupervised-Pre-training-of-a-Graph-Attention-Network-Block-for-Bottom-up-Video-Event-Recognition" class="headerlink" title="Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition"></a>Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12673">http://arxiv.org/abs/2308.12673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Daskalakis, Nikolaos Gkalelis, Vasileios Mezaris</li>
<li>for: 这 paper 是为了提高视频事件识别性能而设计的。</li>
<li>methods: 这 paper 使用了一种名为 Masked Feature Modelling (MFM) 的新方法，该方法利用一个预训练的视觉化トークниザー来重建视频中 объек 的遮盲特征，然后将这些特征与一个已有的底层超参 Video-Event Recognition 架构（ViGAT）结合使用，以提高模型的起点和总准确率。</li>
<li>results:  эксперименталь评估表明，MFM 可以有效地提高事件识别性能。<details>
<summary>Abstract</summary>
In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了一种新的无监督预训练方法，即Masked Feature Modelling（MFM），用于提高视频事件识别模型的起点和总体准确率。MFM使用预训练的视觉化 токен化器来重建视频中对象的遮盲特征，利用了MiniKinetics dataset。然后，我们将预训练的GAT块integrated到了state-of-the-art底层supervised视频事件识别架构ViGAT中，以提高模型的起点和总体准确率。实验评估在YLI-MED dataset上，表明MFM可以提高事件识别性能。
</details></li>
</ul>
<hr>
<h2 id="Optimal-data-pooling-for-shared-learning-in-maintenance-operations"><a href="#Optimal-data-pooling-for-shared-learning-in-maintenance-operations" class="headerlink" title="Optimal data pooling for shared learning in maintenance operations"></a>Optimal data pooling for shared learning in maintenance operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12670">http://arxiv.org/abs/2308.12670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Collin Drent, Melvin Drent, Geert-Jan van Houtum</li>
<li>for: 本研究探讨了维护操作中数据共享学习的好处。我们考虑了一个由波尔tz分布降解的系统集合，这些系统之间有一个未知的概率coupling。</li>
<li>methods: 我们提出了一种分解结果，将高维Markov决策过程（MDP）分解成两个维度的MDP。我们利用这种分解，证明数据共享可以导致成本减少，比不共享数据更便宜。</li>
<li>results: 我们的结果表明，共享数据可以减少成本，比不共享数据更便宜。这种减少的成本可以达到10%以上。<details>
<summary>Abstract</summary>
This paper addresses the benefits of pooling data for shared learning in maintenance operations. We consider a set of systems subject to Poisson degradation that are coupled through an a-priori unknown rate. Decision problems involving these systems are high-dimensional Markov decision processes (MDPs). We present a decomposition result that reduces such an MDP to two-dimensional MDPs, enabling structural analyses and computations. We leverage this decomposition to demonstrate that pooling data can lead to significant cost reductions compared to not pooling.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文研究了维护操作中数据集合的好处。我们考虑了一个系统集合，这些系统都是受到Poisson衰减的，并且通过一个未知的速率相互连接。决策问题 involving这些系统是高维Markov决策过程（MDP）。我们提出了一个分解结果，将这个MDP分解成两个维度的MDP，使得结构分析和计算变得可能。我们利用这个分解，示出了将数据集合起来可以比不集合更大的成本减少。
</details></li>
</ul>
<hr>
<h2 id="Geodesic-Mode-Connectivity"><a href="#Geodesic-Mode-Connectivity" class="headerlink" title="Geodesic Mode Connectivity"></a>Geodesic Mode Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12666">http://arxiv.org/abs/2308.12666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/char-tan/geodesic-mode-connectivity">https://github.com/char-tan/geodesic-mode-connectivity</a></li>
<li>paper_authors: Charlie Tan, Theodore Long, Sarah Zhao, Rudolf Laine</li>
<li>for: 研究模型连接性，即训练模型之间可以通过低损失连接。</li>
<li>methods: 使用信息几何学方法研究神经网络 Parametric 分布空间的弯曲结构，并提出使用最短路径（geodesics）来实现模型连接性。</li>
<li>results: 提出了一种算法来近似地odesics，并证明其可以实现模型连接性。<details>
<summary>Abstract</summary>
Mode connectivity is a phenomenon where trained models are connected by a path of low loss. We reframe this in the context of Information Geometry, where neural networks are studied as spaces of parameterized distributions with curved geometry. We hypothesize that shortest paths in these spaces, known as geodesics, correspond to mode-connecting paths in the loss landscape. We propose an algorithm to approximate geodesics and demonstrate that they achieve mode connectivity.
</details>
<details>
<summary>摘要</summary>
模式连接性是一种现象，训练过的模型被连接成一条低损的路径。我们将这种现象重新划分为信息几何学的视角，将神经网络看作参数化分布空间的弯曲geometry。我们假设低损路径在这些空间中对应于模式连接的路径，并提出了一种算法来近似低损路径。我们证明了这种算法可以实现模式连接。
</details></li>
</ul>
<hr>
<h2 id="Don’t-Look-into-the-Sun-Adversarial-Solarization-Attacks-on-Image-Classifiers"><a href="#Don’t-Look-into-the-Sun-Adversarial-Solarization-Attacks-on-Image-Classifiers" class="headerlink" title="Don’t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers"></a>Don’t Look into the Sun: Adversarial Solarization Attacks on Image Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12661">http://arxiv.org/abs/2308.12661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paulgavrikov/adversarial_solarization">https://github.com/paulgavrikov/adversarial_solarization</a></li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 评估深度神经网络对于不同输入的鲁棒性是非常重要，特别在自动驾驶和安全系统中，以避免安全隐患和黑客digital alter输入。</li>
<li>methods: 我们引入了基于图像曝光的攻击方法，该方法简单易行，但它不会破坏自然图像的全局结构独立于强度。我们通过对多个ImageNet模型进行全面评估，示出了该攻击的准确率下降，但是它并不能保证完全免疫于准确下降。在其他设置下，该攻击可以简化为黑盒子攻击，并且可以使用模型独立的参数。</li>
<li>results: 我们的实验结果表明，对于不同的图像分类模型，该攻击可以导致准确率下降，尤其是当攻击不包括在训练增强中时。此外，我们发现了一些防御机制不能延伸到对我们的特定攻击的鲁棒性。<details>
<summary>Abstract</summary>
Assessing the robustness of deep neural networks against out-of-distribution inputs is crucial, especially in safety-critical domains like autonomous driving, but also in safety systems where malicious actors can digitally alter inputs to circumvent safety guards. However, designing effective out-of-distribution tests that encompass all possible scenarios while preserving accurate label information is a challenging task. Existing methodologies often entail a compromise between variety and constraint levels for attacks and sometimes even both. In a first step towards a more holistic robustness evaluation of image classification models, we introduce an attack method based on image solarization that is conceptually straightforward yet avoids jeopardizing the global structure of natural images independent of the intensity. Through comprehensive evaluations of multiple ImageNet models, we demonstrate the attack's capacity to degrade accuracy significantly, provided it is not integrated into the training augmentations. Interestingly, even then, no full immunity to accuracy deterioration is achieved. In other settings, the attack can often be simplified into a black-box attack with model-independent parameters. Defenses against other corruptions do not consistently extend to be effective against our specific attack.   Project website: https://github.com/paulgavrikov/adversarial_solarization
</details>
<details>
<summary>摘要</summary>
评估深度神经网络对于不同类型的输入的Robustness是非常重要，特别是在自动驾驶和安全系统中，因为这些系统可能会遭受到恶意的攻击。然而，设计有效的不同类型输入测试是一项具有挑战性的任务，因为需要涵盖所有可能的场景，同时保持准确的标签信息。现有的方法ologies often involve a compromise between variety and constraint levels for attacks, and sometimes even both.在图像分类模型的Robustness评估中，我们引入了基于图像折衣的攻击方法，该方法是概念简单且不会损害自然图像的全球结构，无论输入的折衣水平。通过对多个ImageNet模型进行全面的评估，我们示出了该攻击的能力可以导致准确性下降，只要它不包括在训练增强中。意外地，甚至在这种情况下，也没有完全免疫于准确性下降。在其他设置下，该攻击可以简化为黑obox攻击，并且模型独立的参数。防御其他损害的方法不一定能够对我们的特定攻击延伸出效果。项目网站：https://github.com/paulgavrikov/adversarial_solarization
</details></li>
</ul>
<hr>
<h2 id="APART-Diverse-Skill-Discovery-using-All-Pairs-with-Ascending-Reward-and-DropouT"><a href="#APART-Diverse-Skill-Discovery-using-All-Pairs-with-Ascending-Reward-and-DropouT" class="headerlink" title="APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT"></a>APART: Diverse Skill Discovery using All Pairs with Ascending Reward and DropouT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12649">http://arxiv.org/abs/2308.12649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadar Schreiber Galler, Tom Zahavy, Guillaume Desjardins, Alon Cohen</li>
<li>for: 本研究旨在在奖励环境中发现多样化技能，目标是在简单的格子世界环境中发现所有可能的技能，而先前的方法在这些环境中很难成功。</li>
<li>methods: 我们的初始解决方案是替换标准的一对一（所有对）推识器与一对一推识器，并将其与一种新的内在奖励函数和dropout regularization技术结合使用。这种结合的方法被称为APART：多样化技能发现使用所有对with ascending reward和dropout。我们证明了APART在格子世界中发现所有可能的技能，使用了非常少的样本，比先前的工作更高效。</li>
<li>results: 我们的实验结果表明，APART在格子世界中发现所有可能的技能，使用了非常少的样本，比先前的工作更高效。此外，我们还提出了一种更简单的算法，通过修改VIC、重新规定内在奖励和软max推识器温度来实现最大技能数。我们认为我们的发现可能 shed light on 抽象学习中成功的关键因素。<details>
<summary>Abstract</summary>
We study diverse skill discovery in reward-free environments, aiming to discover all possible skills in simple grid-world environments where prior methods have struggled to succeed. This problem is formulated as mutual training of skills using an intrinsic reward and a discriminator trained to predict a skill given its trajectory. Our initial solution replaces the standard one-vs-all (softmax) discriminator with a one-vs-one (all pairs) discriminator and combines it with a novel intrinsic reward function and a dropout regularization technique. The combined approach is named APART: Diverse Skill Discovery using All Pairs with Ascending Reward and Dropout. We demonstrate that APART discovers all the possible skills in grid worlds with remarkably fewer samples than previous works. Motivated by the empirical success of APART, we further investigate an even simpler algorithm that achieves maximum skills by altering VIC, rescaling its intrinsic reward, and tuning the temperature of its softmax discriminator. We believe our findings shed light on the crucial factors underlying success of skill discovery algorithms in reinforcement learning.
</details>
<details>
<summary>摘要</summary>
我们研究了不带奖励的环境中多样化技能发现，目的是发现所有可能的技能在简单的格子世界环境中。这个问题被формализова为用内在奖励和用来预测技能的抽象器进行相互训练。我们的初始解决方案是将标准的一对一（所有对）抽象器取代一个一对多（softmax）抽象器，并将其与一种新的内在奖励函数和抽象器 Dropout 技术相结合。这种结合方法被命名为 APART：多样化技能发现使用所有对的升降奖励和Dropout。我们示出了 APART 在格子世界中发现所有可能的技能，并且使用 remarkably  fewer samples  than previous works。受 APART 的实验成功的激发，我们进一步调查了一种更简单的算法，通过修改 VIC，增加其内在奖励，并调整其 softmax 抽象器的温度来实现最大技能数。我们认为我们的发现可以透视到抽象学习中成功技能发现算法的关键因素。
</details></li>
</ul>
<hr>
<h2 id="The-GENEA-Challenge-2023-A-large-scale-evaluation-of-gesture-generation-models-in-monadic-and-dyadic-settings"><a href="#The-GENEA-Challenge-2023-A-large-scale-evaluation-of-gesture-generation-models-in-monadic-and-dyadic-settings" class="headerlink" title="The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings"></a>The GENEA Challenge 2023: A large scale evaluation of gesture generation models in monadic and dyadic settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12646">http://arxiv.org/abs/2308.12646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taras Kucherenko, Rajmund Nagy, Youngwoo Yoon, Jieyeon Woo, Teodor Nikolov, Mihail Tsakov, Gustav Eje Henter</li>
<li>for: 这个研究的目的是为了评估参与者们在 Speech-driven Gesture-generation 领域的系统，以及这些系统在实际对话中的表现。</li>
<li>methods: 这个研究使用了同一个语音和动作数据集，并采用了共同评估方式。参与者们需要基于语音和动作来生成全身动作。</li>
<li>results: 研究发现了一个很大的人类化范围，其中一些系统评分非常接近人类捕捉数据。然而，适应性问题仍然很大，大多数提交系统在一定范围内表现在随机上方。此外，与对话伙伴的效果也很微妙，最好的提交系统只能在随机上方表现。更多资料可以通过项目网站 <a target="_blank" rel="noopener" href="https://svito-zar.github.io/GENEAchallenge2023/">https://svito-zar.github.io/GENEAchallenge2023/</a> 获取。<details>
<summary>Abstract</summary>
This paper reports on the GENEA Challenge 2023, in which participating teams built speech-driven gesture-generation systems using the same speech and motion dataset, followed by a joint evaluation. This year's challenge provided data on both sides of a dyadic interaction, allowing teams to generate full-body motion for an agent given its speech (text and audio) and the speech and motion of the interlocutor. We evaluated 12 submissions and 2 baselines together with held-out motion-capture data in several large-scale user studies. The studies focused on three aspects: 1) the human-likeness of the motion, 2) the appropriateness of the motion for the agent's own speech whilst controlling for the human-likeness of the motion, and 3) the appropriateness of the motion for the behaviour of the interlocutor in the interaction, using a setup that controls for both the human-likeness of the motion and the agent's own speech. We found a large span in human-likeness between challenge submissions, with a few systems rated close to human mocap. Appropriateness seems far from being solved, with most submissions performing in a narrow range slightly above chance, far behind natural motion. The effect of the interlocutor is even more subtle, with submitted systems at best performing barely above chance. Interestingly, a dyadic system being highly appropriate for agent speech does not necessarily imply high appropriateness for the interlocutor. Additional material is available via the project website at https://svito-zar.github.io/GENEAchallenge2023/ .
</details>
<details>
<summary>摘要</summary>
We found a large range in human-likeness among challenge submissions, with a few systems rated close to human motion capture. However, appropriateness was not well-solved, with most submissions performing in a narrow range slightly above chance, far behind natural motion. The effect of the interlocutor was even more subtle, with submitted systems at best performing barely above chance. Interestingly, a dyadic system being highly appropriate for agent speech does not necessarily imply high appropriateness for the interlocutor. Additional information can be found on the project website at <https://svito-zar.github.io/GENEAchallenge2023/>.
</details></li>
</ul>
<hr>
<h2 id="Towards-Hierarchical-Regional-Transformer-based-Multiple-Instance-Learning"><a href="#Towards-Hierarchical-Regional-Transformer-based-Multiple-Instance-Learning" class="headerlink" title="Towards Hierarchical Regional Transformer-based Multiple Instance Learning"></a>Towards Hierarchical Regional Transformer-based Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12634">http://arxiv.org/abs/2308.12634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josef Cersovsky, Sadegh Mohammadi, Dagmar Kainmueller, Johannes Hoehne</li>
<li>for: 这个研究旨在提高大幅 histopathology 图像的分类效果，使用深度多实例学习模型。</li>
<li>methods: 该方法使用 Transformer 核心，替代传统的学习注意力机制，使用区域 Vision Transformer 自注意力机制。另外，该方法还利用区域补做来 derive 板块级别预测，并可以堆叠来进行不同距离水平的特征处理。</li>
<li>results: 该方法在两个 histopathology 数据集上显著提高了性能，特别是对于具有小、地方形态特征的数据集。这些结果预示了该方法在精细医学领域的潜在应用。<details>
<summary>Abstract</summary>
The classification of gigapixel histopathology images with deep multiple instance learning models has become a critical task in digital pathology and precision medicine. In this work, we propose a Transformer-based multiple instance learning approach that replaces the traditional learned attention mechanism with a regional, Vision Transformer inspired self-attention mechanism. We present a method that fuses regional patch information to derive slide-level predictions and show how this regional aggregation can be stacked to hierarchically process features on different distance levels. To increase predictive accuracy, especially for datasets with small, local morphological features, we introduce a method to focus the image processing on high attention regions during inference. Our approach is able to significantly improve performance over the baseline on two histopathology datasets and points towards promising directions for further research.
</details>
<details>
<summary>摘要</summary>
“ digitization pathology 和精度医学中， gigapixel  histopathology 图像的分类已成为一个关键任务。在这种工作中，我们提议使用 transformer 基于多实例学习模型，取代传统的学习注意力机制。我们提出一种基于区域的 Vision Transformer 自注意力机制，并将地域补丁信息融合以 derive 扫描级别预测。我们还介绍了一种用于在推理过程中专注高注意区域进行图像处理，以提高预测精度，特别是对于具有小、本地形态特征的数据集。我们的方法能够显著超越基线性能，在两个 histopathology 数据集上，并指向了未来研究的可能性。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-and-Explainable-Analysis-of-Machine-Learning-Model-for-Reconstruction-of-Sonic-Slowness-Logs"><a href="#Uncertainty-and-Explainable-Analysis-of-Machine-Learning-Model-for-Reconstruction-of-Sonic-Slowness-Logs" class="headerlink" title="Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs"></a>Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12625">http://arxiv.org/abs/2308.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hua Wang, Yuqiong Wu, Yushun Zhang, Fuqiang Lai, Zhou Feng, Bing Xie, Ailin Zhao<br>for:This paper aims to predict missing compressional wave slowness and shear wave slowness logs in horizontal or old wells using other logs in the same borehole.methods:The authors use the NGBoost algorithm to construct an Ensemble Learning model that can predict the results as well as their uncertainty. They also use the SHAP method to investigate the interpretability of the machine learning model.results:The NGBoost model performs well in the testing set and provides a probability distribution for the prediction results. The variance of the probability distribution can be used to justify the quality of the constructed log. The machine learning model captures the influence of the changing borehole caliper on slowness, which is consistent with the physical principle of borehole acoustics.Here’s the simplified Chinese text:for: 这篇论文目的是预测垂直或老井中缺失的压缩波慢速和剪切波慢速记录。methods: 作者使用NGBoost算法构建了一个ensemble学习模型，可以预测结果以及其不确定性。他们还使用SHAP方法来调查机器学习模型的可读性。results: NGBoost模型在测试集中表现良好，可以提供预测结果的概率分布。预测结果的方差可以用来评估构造的日志质量。机器学习模型捕捉了改变的孔隙矿物带产生的影响，这是physical principle of borehole acoustics的一部分。<details>
<summary>Abstract</summary>
Logs are valuable information for oil and gas fields as they help to determine the lithology of the formations surrounding the borehole and the location and reserves of subsurface oil and gas reservoirs. However, important logs are often missing in horizontal or old wells, which poses a challenge in field applications. In this paper, we utilize data from the 2020 machine learning competition of the SPWLA, which aims to predict the missing compressional wave slowness and shear wave slowness logs using other logs in the same borehole. We employ the NGBoost algorithm to construct an Ensemble Learning model that can predicate the results as well as their uncertainty. Furthermore, we combine the SHAP method to investigate the interpretability of the machine learning model. We compare the performance of the NGBosst model with four other commonly used Ensemble Learning methods, including Random Forest, GBDT, XGBoost, LightGBM. The results show that the NGBoost model performs well in the testing set and can provide a probability distribution for the prediction results. In addition, the variance of the probability distribution of the predicted log can be used to justify the quality of the constructed log. Using the SHAP explainable machine learning model, we calculate the importance of each input log to the predicted results as well as the coupling relationship among input logs. Our findings reveal that the NGBoost model tends to provide greater slowness prediction results when the neutron porosity and gamma ray are large, which is consistent with the cognition of petrophysical models. Furthermore, the machine learning model can capture the influence of the changing borehole caliper on slowness, where the influence of borehole caliper on slowness is complex and not easy to establish a direct relationship. These findings are in line with the physical principle of borehole acoustics.
</details>
<details>
<summary>摘要</summary>
Logs 是钻井油气田中的重要信息，它们可以帮助确定附近钻井的地层学特性和潜在的油气储量。然而，在水平或老井中，重要的 logs 经常缺失，这会对钻井实际应用带来挑战。在这篇文章中，我们使用2020年机器学习竞赛的SPWLA数据，以预测缺失的压缩波慢速度和剪切波慢速度 logs。我们使用NGBoost算法建立了一个 Ensemble Learning 模型，可以预测结果以及其不确定性。此外，我们使用 SHAP 方法来调查机器学习模型的可解释性。我们将 NGBoost 模型与四种常用的 Ensemble Learning 方法进行比较，包括Random Forest、GBDT、XGBoost 和 LightGBM。结果表明，NGBoost 模型在测试集中表现良好，可以提供预测结果的概率分布。此外，预测结果的方差可以用来评估构造的 logs 质量。使用 SHAP 可解释机器学习模型，我们计算输入 logs 对预测结果的重要性以及输入 logs 之间的相互作用。我们发现，NGBoost 模型在 neutron  pórosity 和γ射辐射大的情况下提供更高的慢速度预测结果，这与石油物理模型的认知一致。此外，机器学习模型可以捕捉随着钻井尺寸的变化而影响慢速度的复杂关系，这与物理原理相一致。
</details></li>
</ul>
<hr>
<h2 id="Try-with-Simpler-–-An-Evaluation-of-Improved-Principal-Component-Analysis-in-Log-based-Anomaly-Detection"><a href="#Try-with-Simpler-–-An-Evaluation-of-Improved-Principal-Component-Analysis-in-Log-based-Anomaly-Detection" class="headerlink" title="Try with Simpler – An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection"></a>Try with Simpler – An Evaluation of Improved Principal Component Analysis in Log-based Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12612">http://arxiv.org/abs/2308.12612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Yang, Junjie Chen, Zhihao Gong, Shutao Gao, Hongyu Zhang, Yue Kang, Huaan Li<br>for: 这个研究的目的是将传统机器学习和数据探勘技术与深度学习结合，以实现对log事件的独特检测。methods: 这个研究使用了七种log事件检测方法，包括四种深度学习方法、两种传统方法和优化的PCA技术。results: 结果显示，优化的PCA技术与高级的对照方法相比，具有更高的稳定性和资源效率，并且在训练数据和资源方面具有更好的适应力。<details>
<summary>Abstract</summary>
The rapid growth of deep learning (DL) has spurred interest in enhancing log-based anomaly detection. This approach aims to extract meaning from log events (log message templates) and develop advanced DL models for anomaly detection. However, these DL methods face challenges like heavy reliance on training data, labels, and computational resources due to model complexity. In contrast, traditional machine learning and data mining techniques are less data-dependent and more efficient but less effective than DL. To make log-based anomaly detection more practical, the goal is to enhance traditional techniques to match DL's effectiveness. Previous research in a different domain (linking questions on Stack Overflow) suggests that optimized traditional techniques can rival state-of-the-art DL methods. Drawing inspiration from this concept, we conducted an empirical study. We optimized the unsupervised PCA (Principal Component Analysis), a traditional technique, by incorporating lightweight semantic-based log representation. This addresses the issue of unseen log events in training data, enhancing log representation. Our study compared seven log-based anomaly detection methods, including four DL-based, two traditional, and the optimized PCA technique, using public and industrial datasets. Results indicate that the optimized unsupervised PCA technique achieves similar effectiveness to advanced supervised/semi-supervised DL methods while being more stable with limited training data and resource-efficient. This demonstrates the adaptability and strength of traditional techniques through small yet impactful adaptations.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）的快速发展促使异常检测方法的改进。这种方法目的在于从日志事件模板中提取意义并开发高级DL模型进行异常检测。然而，这些DL方法面临着大量训练数据、标签和计算资源的压力，尤其是模型的复杂性。相比之下，传统机器学习和数据挖掘技术更加不依赖于训练数据和计算资源，但效果较差。为了让日志异常检测更加实用，目标是提高传统技术，使其与DL的效果相匹配。前一项研究（在Stack Overflow上的问题连接）表明，优化传统技术可以与当前DL方法相比。以此为起点，我们进行了一项实验研究。我们对传统的无监督PCA（主成分分析）进行了优化，通过 integrate lightweight semantic-based日志表示。这解决了训练数据中未见日志事件的问题，提高日志表示。我们对公共和工业 dataset 进行了七种日志异常检测方法的比较，包括四种DL基于方法、两种传统方法和优化PCA技术。结果显示，优化无监督PCA技术与高级监督/半监督DL方法相比，在有限的训练数据和资源下具有更高的稳定性和效率。这表明传统技术通过小 yet 有力的改进，可以具备DL方法的灵活性和效果。
</details></li>
</ul>
<hr>
<h2 id="A-Greedy-Approach-for-Offering-to-Telecom-Subscribers"><a href="#A-Greedy-Approach-for-Offering-to-Telecom-Subscribers" class="headerlink" title="A Greedy Approach for Offering to Telecom Subscribers"></a>A Greedy Approach for Offering to Telecom Subscribers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12606">http://arxiv.org/abs/2308.12606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piyush Kanti Bhunre, Tanmay Sen, Arijit Sarkar</li>
<li>for: 这个论文是为了解决电信运营商面临的客户销售或落户预防问题，提出了一种新的组合算法来优化电信运营商的奖励策略，以最大化预期收入。</li>
<li>methods: 该论文提出了一种新的组合算法，使用了各种策略来选择目标客户和奖励，包括考虑多个目标函数，例如最大化收入和最小化落户率。</li>
<li>results: 该论文的实验结果表明，该算法可以准确地选择目标客户并提供合适的奖励，从而最大化预期收入。 当前的算法可以处理大规模的客户基数，并且具有高效和准确的特点。<details>
<summary>Abstract</summary>
Customer retention or churn prevention is a challenging task of a telecom operator. One of the effective approaches is to offer some attractive incentive or additional services or money to the subscribers for keeping them engaged and make sure they stay in the operator's network for longer time. Often, operators allocate certain amount of monetary budget to carry out the offer campaign. The difficult part of this campaign is the selection of a set of customers from a large subscriber-base and deciding the amount that should be offered to an individual so that operator's objective is achieved. There may be multiple objectives (e.g., maximizing revenue, minimizing number of churns) for selection of subscriber and selection of an offer to the selected subscriber. Apart from monetary benefit, offers may include additional data, SMS, hots-spot tethering, and many more. This problem is known as offer optimization. In this paper, we propose a novel combinatorial algorithm for solving offer optimization under heterogeneous offers by maximizing expected revenue under the scenario of subscriber churn, which is, in general, seen in telecom domain. The proposed algorithm is efficient and accurate even for a very large subscriber-base.
</details>
<details>
<summary>摘要</summary>
客户退订或防止落叶是电信运营商面临的挑战。一种有效的方法是向用户提供一些吸引力强的奖励或额外服务，以保持用户的兴趣和使他们尽快留在运营商的网络中。运营商通常将一定的财务预算用于实施这些奖励。选择一个从大量用户基础中选择用户并决定每个用户所需的奖励金额是这个campaign的困难之处。运营商可能有多个目标（例如，最大化收入、最小化落叶数），这会影响选择用户和向选择用户提供的奖励。此外，奖励可能包括额外的数据、短信、快速热点连接等。这个问题被称为奖励优化。在这篇论文中，我们提出了一种新的 combinatorial 算法，用于在不同的奖励下进行奖励优化，以 maximize 用户退订后的预期收入。提议的算法是高效和准确， même pour une base de souscripteurs très large。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Time-Frequency-Conformers-for-Music-Audio-Enhancement"><a href="#Exploiting-Time-Frequency-Conformers-for-Music-Audio-Enhancement" class="headerlink" title="Exploiting Time-Frequency Conformers for Music Audio Enhancement"></a>Exploiting Time-Frequency Conformers for Music Audio Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12599">http://arxiv.org/abs/2308.12599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunkee Chae, Junghyun Koo, Sungho Lee, Kyogu Lee</li>
<li>for: 提高网络视频平台上的音乐表演录音质量，改善听众体验。</li>
<li>methods: 基于Conformer架构，利用注意力机制进行音乐提升。</li>
<li>results: 实验结果显示，提出的模型在单音轨音乐提升任务中达到了状态元表现，并且可以进行通用音乐提升，处理多轨混合音频。<details>
<summary>Abstract</summary>
With the proliferation of video platforms on the internet, recording musical performances by mobile devices has become commonplace. However, these recordings often suffer from degradation such as noise and reverberation, which negatively impact the listening experience. Consequently, the necessity for music audio enhancement (referred to as music enhancement from this point onward), involving the transformation of degraded audio recordings into pristine high-quality music, has surged to augment the auditory experience. To address this issue, we propose a music enhancement system based on the Conformer architecture that has demonstrated outstanding performance in speech enhancement tasks. Our approach explores the attention mechanisms of the Conformer and examines their performance to discover the best approach for the music enhancement task. Our experimental results show that our proposed model achieves state-of-the-art performance on single-stem music enhancement. Furthermore, our system can perform general music enhancement with multi-track mixtures, which has not been examined in previous work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data"><a href="#LORD-Leveraging-Open-Set-Recognition-with-Unknown-Data" class="headerlink" title="LORD: Leveraging Open-Set Recognition with Unknown Data"></a>LORD: Leveraging Open-Set Recognition with Unknown Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12584">http://arxiv.org/abs/2308.12584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Koch, Christian Riess, Thomas Köhler</li>
<li>for: 本研究旨在解决部署过程中遇到的完全未知数据问题，即 Classification 模型在推理过程中遇到的 откры集数据。</li>
<li>methods: 本研究提出了一种名为 LORD 的框架，即 Leverage Open-set Recognition by exploiting unknown Data。LORD 在类ifier 训练过程中直接模型开放空间，并提供了一种系统atic evaluation 方法来评估这些方法。研究人员还提出了三种模型无关的训练策略，这些策略可以应用于常见的类ifier 中。</li>
<li>results: 研究人员通过广泛的实验和分析表明，LORD 可以更好地识别未知数据。此外，研究人员还发现了一种名为 mixup 的数据生成技术，可以作为背景数据的替代品。并且通过对 mixup 的约束进行优化，可以进一步提高 OSR 性能。<details>
<summary>Abstract</summary>
Handling entirely unknown data is a challenge for any deployed classifier. Classification models are typically trained on a static pre-defined dataset and are kept in the dark for the open unassigned feature space. As a result, they struggle to deal with out-of-distribution data during inference. Addressing this task on the class-level is termed open-set recognition (OSR). However, most OSR methods are inherently limited, as they train closed-set classifiers and only adapt the downstream predictions to OSR. This work presents LORD, a framework to Leverage Open-set Recognition by exploiting unknown Data. LORD explicitly models open space during classifier training and provides a systematic evaluation for such approaches. We identify three model-agnostic training strategies that exploit background data and applied them to well-established classifiers. Due to LORD's extensive evaluation protocol, we consistently demonstrate improved recognition of unknown data. The benchmarks facilitate in-depth analysis across various requirement levels. To mitigate dependency on extensive and costly background datasets, we explore mixup as an off-the-shelf data generation technique. Our experiments highlight mixup's effectiveness as a substitute for background datasets. Lightweight constraints on mixup synthesis further improve OSR performance.
</details>
<details>
<summary>摘要</summary>
处理完全未知数据是任何部署 классифика器的挑战。分类模型通常是静态预定的数据集上训练的，因此在推理时遇到不同类型的数据时会陷入困难。为解决这个问题，我们称之为开放集 recognition（OSR）。然而，大多数 OSR 方法都有限制，因为它们只是在预定的类别上进行了适应。这项工作提出了一个框架，称之为 LORD，可以利用未知数据进行开放集认识。LORD 在分类器训练时显式地模型开放空间，并提供了系统的评估方法。我们认为有三种模型无关的训练策略可以利用背景数据，并应用到了成熟的分类器上。由于 LORD 的广泛的评估协议，我们在不同的需求水平上 consistently 表现出了未知数据的更好的识别。这些标准化的宽泛可以进行深入的分析。为了减少依赖于大量和昂贵的背景数据，我们探索了 mixup 作为一种可用的数据生成技术。我们的实验表明，mixup 是一种有效的替代方案。另外，对 mixup 的 sintesis 进行轻量级的限制可以进一步提高 OSR 性能。
</details></li>
</ul>
<hr>
<h2 id="Persistent-learning-signals-and-working-memory-without-continuous-attractors"><a href="#Persistent-learning-signals-and-working-memory-without-continuous-attractors" class="headerlink" title="Persistent learning signals and working memory without continuous attractors"></a>Persistent learning signals and working memory without continuous attractors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12585">http://arxiv.org/abs/2308.12585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Il Memming Park, Ábel Ságodi, Piotr Aleksander Sokół</li>
<li>for: 这篇论文探讨了神经动力系统中稳定吸引结构，如点吸引器和连续吸引器，如何支持有用的时间行为，需要工作记忆。</li>
<li>methods: 论文使用了 periodic和 quasi-periodic吸引器来支持学习，而不 LIKE continuous吸引器，这些吸引器可以学习无限长的时间关系。</li>
<li>results: 论文的理论有广泛的应用于人工学习系统的设计，以及对生物神经动力系统中的时间依赖学习和工作记忆的预测。同时，论文还提出了一种新的初始化方案，可以超过标准方法在需要学习时间动力学任务上表现出色。此外，论文还提出了一种Robust的回忆机制，可以维护和集成方向memory。<details>
<summary>Abstract</summary>
Neural dynamical systems with stable attractor structures, such as point attractors and continuous attractors, are hypothesized to underlie meaningful temporal behavior that requires working memory. However, working memory may not support useful learning signals necessary to adapt to changes in the temporal structure of the environment. We show that in addition to the continuous attractors that are widely implicated, periodic and quasi-periodic attractors can also support learning arbitrarily long temporal relationships. Unlike the continuous attractors that suffer from the fine-tuning problem, the less explored quasi-periodic attractors are uniquely qualified for learning to produce temporally structured behavior. Our theory has broad implications for the design of artificial learning systems and makes predictions about observable signatures of biological neural dynamics that can support temporal dependence learning and working memory. Based on our theory, we developed a new initialization scheme for artificial recurrent neural networks that outperforms standard methods for tasks that require learning temporal dynamics. Moreover, we propose a robust recurrent memory mechanism for integrating and maintaining head direction without a ring attractor.
</details>
<details>
<summary>摘要</summary>
神经动力系统 WITH stable attractor structure, such as point attractors and continuous attractors, are hypothesized to underlie meaningful temporal behavior that requires working memory. However, working memory may not support useful learning signals necessary to adapt to changes in the temporal structure of the environment. We show that in addition to the continuous attractors that are widely implicated, periodic and quasi-periodic attractors can also support learning arbitrarily long temporal relationships. Unlike the continuous attractors that suffer from the fine-tuning problem, the less explored quasi-periodic attractors are uniquely qualified for learning to produce temporally structured behavior. Our theory has broad implications for the design of artificial learning systems and makes predictions about observable signatures of biological neural dynamics that can support temporal dependence learning and working memory. Based on our theory, we developed a new initialization scheme for artificial recurrent neural networks that outperforms standard methods for tasks that require learning temporal dynamics. Moreover, we propose a robust recurrent memory mechanism for integrating and maintaining head direction without a ring attractor.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other countries. The translation is written in Simplified Chinese, but the text remains the same in both Simplified and Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Huber-Loss-Minimization-Approach-to-Byzantine-Robust-Federated-Learning"><a href="#A-Huber-Loss-Minimization-Approach-to-Byzantine-Robust-Federated-Learning" class="headerlink" title="A Huber Loss Minimization Approach to Byzantine Robust Federated Learning"></a>A Huber Loss Minimization Approach to Byzantine Robust Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12581">http://arxiv.org/abs/2308.12581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Fei Yu, Zhiguo Wan</li>
<li>for: 防止 Federated Learning 系统受到攻击，我们提出一种基于ubyte loss函数的聚合器，并提供了全面的理论分析。</li>
<li>methods: 我们的方法基于ubyte loss函数进行聚合，具有优化 $\epsilon$ 的依赖性，不需要准确知道 $\epsilon$，并允许客户端数据大小不同。</li>
<li>results: 我们的方法在独立同分布（i.i.d）假设下有多个优势，包括优化 $\epsilon$ 的依赖性和不需要准确知道 $\epsilon$。此外，我们还扩展了分析至非i.i.d数据，包括客户端数据有轻度不同的情况。<details>
<summary>Abstract</summary>
Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
</details>
<details>
<summary>摘要</summary>
联合学习系统容易受到敌意攻击。为了解决这问题，我们提出了基于捷径函数损失的新的聚合器，并进行了全面的理论分析。在独立同分布（i.i.d）假设下，我们的方法有以下优点：首先，它对epsillon（攻击客户端的比率）具有优化的依赖度。其次，我们的方法不需要准确地知道epsillon。最后，它允许客户端有不同的数据大小。然后，我们扩展了我们的分析范围，包括非i.i.d数据，例如客户端的数据具有略微不同的分布。
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Convolutional-Networks-for-Fine-grained-ICU-Patient-Similarity-Analysis-and-Risk-Prediction"><a href="#Hypergraph-Convolutional-Networks-for-Fine-grained-ICU-Patient-Similarity-Analysis-and-Risk-Prediction" class="headerlink" title="Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction"></a>Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12575">http://arxiv.org/abs/2308.12575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes, Jun Shen</li>
<li>for: 预测 critically ill 患者的死亡风险</li>
<li>methods: 使用 Hypergraph Convolutional Network capture 非对应关系，以计算细致的患者相似性</li>
<li>results: 在 eICU Collaborative Research Database 上，方法比前一代模型提高了死亡风险预测性能，并在几个实际案例中展现了良好的透明度和可靠性。Here’s a breakdown of each point:</li>
<li>for: The paper is written to predict the mortality risk of critically ill patients.</li>
<li>methods: The paper proposes using a Hypergraph Convolutional Network (HGCN) to capture non-pairwise relationships among diagnosis codes in a hypergraph, allowing for the calculation of fine-grained patient similarity and personalized mortality risk prediction.</li>
<li>results: The proposed method achieves superior performance over state-of-the-art models on mortality risk prediction, as demonstrated by evaluation on a publicly available dataset (eICU Collaborative Research Database). Additionally, the results of several case studies show that the method provides good transparency and robustness in decision-making.<details>
<summary>Abstract</summary>
The Intensive Care Unit (ICU) is one of the most important parts of a hospital, which admits critically ill patients and provides continuous monitoring and treatment. Various patient outcome prediction methods have been attempted to assist healthcare professionals in clinical decision-making. Existing methods focus on measuring the similarity between patients using deep neural networks to capture the hidden feature structures. However, the higher-order relationships are ignored, such as patient characteristics (e.g., diagnosis codes) and their causal effects on downstream clinical predictions.   In this paper, we propose a novel Hypergraph Convolutional Network that allows the representation of non-pairwise relationships among diagnosis codes in a hypergraph to capture the hidden feature structures so that fine-grained patient similarity can be calculated for personalized mortality risk prediction. Evaluation using a publicly available eICU Collaborative Research Database indicates that our method achieves superior performance over the state-of-the-art models on mortality risk prediction. Moreover, the results of several case studies demonstrated the effectiveness of constructing graph networks in providing good transparency and robustness in decision-making.
</details>
<details>
<summary>摘要</summary>
医院重症监护部（ICU）是医院中最重要的部分之一，患有严重疾病的患者可以在这里接受无间断监测和治疗。不同的患者结果预测方法已经被尝试以帮助医疗专业人员进行临床决策。现有方法主要是使用深度神经网络来捕捉患者特征结构的隐藏关系。然而，高阶关系（如诊断代码）和其对下游临床预测的影响却被忽略了。在本文中，我们提出了一种新的超graph卷积神经网络，允许表示诊断代码之间的非对比关系在超graph中表示隐藏特征结构，从而可以计算出细化的患者相似性，以进行个性化死亡风险预测。使用公共可用的eICU合作研究数据库，我们的方法比现有的模型在死亡风险预测中表现出色。此外，多个案例研究表明，建立图网络可以提供良好的透明度和稳定性在决策中。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Kernel-Imitation-Learning-for-Continuous-State-Environments"><a href="#Conditional-Kernel-Imitation-Learning-for-Continuous-State-Environments" class="headerlink" title="Conditional Kernel Imitation Learning for Continuous State Environments"></a>Conditional Kernel Imitation Learning for Continuous State Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12573">http://arxiv.org/abs/2308.12573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Agrawal, Nathan Dahlin, Rahul Jain, Ashutosh Nayyar</li>
<li>for: 本研究旨在 solving the imitation learning problem in continuous state space environments without access to transition dynamics information, reward structure, or additional interactions with the environment.</li>
<li>methods: 我们的方法基于Markov balance equation，并使用conditional kernel density estimator来估算环境的转移动力学。</li>
<li>results: 我们的实验结果表明，我们的方法在 continuous state benchmark environments 上表现出了 consistently superior empirical performance compared to many state-of-the-art IL algorithms。<details>
<summary>Abstract</summary>
Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Reward inference and shaping are known to be difficult and error-prone methods particularly when the demonstration data comes from human experts. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, a problem that is particularly acute in continuous state space problems. Meanwhile, state-of-the-art IL algorithms convert behavioral policy learning problems into distribution-matching problems which often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our approach is based on the Markov balance equation and introduces a novel conditional kernel density estimation-based imitation learning framework. It involves estimating the environment's transition dynamics using conditional kernel density estimators and seeks to satisfy the probabilistic balance equations for the environment. We establish that our estimators satisfy basic asymptotic consistency requirements. Through a series of numerical experiments on continuous state benchmark environments, we show consistently superior empirical performance over many state-of-the-art IL algorithms.
</details>
<details>
<summary>摘要</summary>
imitational learning（IL）是RL方法olo的重要分支，不同于大多数RL，它不假设有奖励反馈。奖励推断和形成是difficult和error-prone的方法，特别是当示例数据来自人类专家时。古典方法 such as behavioral cloning和inverse reinforcement learning是高度敏感于估计错误，这是特别突出的在连续状态空间问题上。在这篇论文中，我们考虑了基于观察行为的imitational learning在连续状态空间环境中的问题，没有访问过程动力学信息、奖励结构或任何其他与环境进行交互的数据。我们的方法基于Markov平衡方程，并提出了一种基于 conditional kernel density estimation的imitational learning框架。它通过估计环境的过程动力学使用conditional kernel density estimator，并寻求满足环境的 probabilistic balance equation。我们证明了我们的估计符合基本的极限consistency要求。通过对连续状态benchmark环境进行numerical experiment，我们展示了与许多现状征的IL算法相比，我们的方法具有consistent superior empirical performance。
</details></li>
</ul>
<hr>
<h2 id="Multivariate-Time-Series-Anomaly-Detection-with-Contaminated-Data-Application-to-Physiological-Signals"><a href="#Multivariate-Time-Series-Anomaly-Detection-with-Contaminated-Data-Application-to-Physiological-Signals" class="headerlink" title="Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals"></a>Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12563">http://arxiv.org/abs/2308.12563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thi Kieu Khanh Ho, Narges Armanfard</li>
<li>for: 本研究旨在 Addressing the challenge of training with noise in practical anomaly detection, and presenting a novel and practical end-to-end unsupervised time-series anomaly detection (TSAD) approach when the training data are contaminated with anomalies.</li>
<li>methods: 本方法包括 three modules: a Decontaminator to rectify the abnormalities present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data, and an Anomaly Scoring module to detect anomalies.</li>
<li>results: 经过广泛的实验 validate that our approach surpasses existing methodologies, thus establishing a new state-of-the-art performance in the field.<details>
<summary>Abstract</summary>
Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate of the pure normal data, and an Anomaly Scoring module to detect anomalies. Our extensive experiments conducted on three widely used physiological datasets conclusively demonstrate that our approach surpasses existing methodologies, thus establishing a new state-of-the-art performance in the field.
</details>
<details>
<summary>摘要</summary>
主流无监督异常检测算法经常在学术数据集上表现出色，然而在实际应用中，它们的性能受到干扰的限制，主要是因为干扰的训练数据。寻找训练数据中干扰的挑战是实际异常检测中的一个普遍存在的问题，然而这种问题在学术研究中得到了忽视。本研究在感知时序异常检测（TSAD）领域进行了先锋性的尝试，推出了一种新的实用无监督TSAD方法，称为TSAD-C。TSAD-C方法在干扰了异常数据的训练数据上进行了三个模块的组合：一个Rectifier模块用于修正训练数据中的异常（即噪声），一个Variable Dependency Modeling模块用于捕捉修正后的数据中的长期内部和间部变量相互关系，以及一个异常检测模块用于检测异常。我们对三个常用的生理数据集进行了广泛的实验，结果表明，我们的方法在现有方法之上升级了新的状态态�arte，这有力地证明了我们的方法在实际应用中的优势。
</details></li>
</ul>
<hr>
<h2 id="Variational-Information-Pursuit-with-Large-Language-and-Multimodal-Models-for-Interpretable-Predictions"><a href="#Variational-Information-Pursuit-with-Large-Language-and-Multimodal-Models-for-Interpretable-Predictions" class="headerlink" title="Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions"></a>Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12562">http://arxiv.org/abs/2308.12562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kwan Ho Ryan Chan, Aditya Chattopadhyay, Benjamin David Haeffele, Rene Vidal</li>
<li>for: 这个论文的目的是提出一种基于变换信息抽取（V-IP）框架的可解释预测方法，以便在预测 задании中实现内在可解释性。</li>
<li>methods: 该方法使用了两个步骤，首先使用大语言模型（LLM）生成一个具有足够大容量的任务相关可解释概念集，然后使用大量多模态模型对每个数据样本进行semantic similarity的标注。</li>
<li>results:  comparative experiments show that the proposed FM+V-IP method can achieve better test performance than V-IP with human-annotated concepts, and can also achieve competitive test performance using fewer number of concepts&#x2F;queries compared to other interpretable-by-design frameworks such as CBMs.<details>
<summary>Abstract</summary>
Variational Information Pursuit (V-IP) is a framework for making interpretable predictions by design by sequentially selecting a short chain of task-relevant, user-defined and interpretable queries about the data that are most informative for the task. While this allows for built-in interpretability in predictive models, applying V-IP to any task requires data samples with dense concept-labeling by domain experts, limiting the application of V-IP to small-scale tasks where manual data annotation is feasible. In this work, we extend the V-IP framework with Foundational Models (FMs) to address this limitation. More specifically, we use a two-step process, by first leveraging Large Language Models (LLMs) to generate a sufficiently large candidate set of task-relevant interpretable concepts, then using Large Multimodal Models to annotate each data sample by semantic similarity with each concept in the generated concept set. While other interpretable-by-design frameworks such as Concept Bottleneck Models (CBMs) require an additional step of removing repetitive and non-discriminative concepts to have good interpretability and test performance, we mathematically and empirically justify that, with a sufficiently informative and task-relevant query (concept) set, the proposed FM+V-IP method does not require any type of concept filtering. In addition, we show that FM+V-IP with LLM generated concepts can achieve better test performance than V-IP with human annotated concepts, demonstrating the effectiveness of LLMs at generating efficient query sets. Finally, when compared to other interpretable-by-design frameworks such as CBMs, FM+V-IP can achieve competitive test performance using fewer number of concepts/queries in both cases with filtered or unfiltered concept sets.
</details>
<details>
<summary>摘要</summary>
各位：Variational Information Pursuit（V-IP）是一个框架，用于将预测结果变得更加解释性，通过逐步选择任务相关、用户定义且可解释性的问题，以实现预测模型内置的解释性。然而，实施V-IP需要具有对应的数据样本，且需要专家 manually 进行标签，因此仅能应用于小规模任务。在这个工作中，我们将V-IP框架与基础模型（FM）融合，以解决这个限制。具体来说，我们运用两步过程：首先，通过大语言模型（LLM）产生足够多的任务相关且可解释性的概念集，然后，使用大多媒体模型进行标签，将每个数据样本与每个概念进行semantic similarity标识。相比于其他可解释性设计框架，如概念瓶颈模型（CBM），我们不需要进行拒绝重复且无用数据的概念 filtering。此外，我们还证明了，只要概念集足够信息内容和任务相关，则FM+V-IP方法不需要进行任何型数据排除。最后，我们显示了FM+V-IP这个方法可以使用LLM生成的概念集，在test数据上取得更好的表现，并且在对照数据上也能够取得更好的表现。此外，相比于其他可解释性设计框架，FM+V-IP可以使用 fewer 数量的概念来取得相同或更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-driven-Cross-Community-Energy-Interaction-Optimal-Scheduling"><a href="#Deep-Reinforcement-Learning-driven-Cross-Community-Energy-Interaction-Optimal-Scheduling" class="headerlink" title="Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling"></a>Deep Reinforcement Learning-driven Cross-Community Energy Interaction Optimal Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12554">http://arxiv.org/abs/2308.12554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Fanjin Bu, Zhen Yang, Bin Wang, Meng Han</li>
<li>for: 该论文 targets 多个社区和多种能源互相协同、共享资源，以实现多元社区综合能源系统的优化和调度。</li>
<li>methods: 该论文提出了一种基于多智能深度学习算法的综合调度模型，通过学习不同社区的负荷特点来做出决策。</li>
<li>results:  simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate reasonable energy interactions among them, leading to a reduction in wind curtailment rate from 16.3% to 0% and lowering the overall operating cost by 5445.6 Yuan.<details>
<summary>Abstract</summary>
In order to coordinate energy interactions among various communities and energy conversions among multi-energy subsystems within the multi-community integrated energy system under uncertain conditions, and achieve overall optimization and scheduling of the comprehensive energy system, this paper proposes a comprehensive scheduling model that utilizes a multi-agent deep reinforcement learning algorithm to learn load characteristics of different communities and make decisions based on this knowledge. In this model, the scheduling problem of the integrated energy system is transformed into a Markov decision process and solved using a data-driven deep reinforcement learning algorithm, which avoids the need for modeling complex energy coupling relationships between multi-communities and multi-energy subsystems. The simulation results show that the proposed method effectively captures the load characteristics of different communities and utilizes their complementary features to coordinate reasonable energy interactions among them. This leads to a reduction in wind curtailment rate from 16.3% to 0% and lowers the overall operating cost by 5445.6 Yuan, demonstrating significant economic and environmental benefits.
</details>
<details>
<summary>摘要</summary>
为了协调不同社区之间的能源互动和多种能源子系统内部的能源转换，并在不确定条件下实现整体能源系统的优化和调度，这篇论文提出了一种综合调度模型，利用多代理深度学习算法来学习不同社区的荷载特点，并根据这些知识来做出决策。在这个模型中，集成能源系统的调度问题被转化为了马尔可夫决策过程，并使用数据驱动的深度学习算法来解决，这有效避免了复杂的能源协同关系的建模。实验结果表明，提议的方法能够有效捕捉不同社区的荷载特点，并利用它们的共同特点来协调合理的能源互动。这导致风力废弃率从16.3%降至0%，并且全局运行成本下降5445.6元，表明了明显的经济和环保效益。
</details></li>
</ul>
<hr>
<h2 id="Don’t-blame-Dataset-Shift-Shortcut-Learning-due-to-Gradients-and-Cross-Entropy"><a href="#Don’t-blame-Dataset-Shift-Shortcut-Learning-due-to-Gradients-and-Cross-Entropy" class="headerlink" title="Don’t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy"></a>Don’t blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12553">http://arxiv.org/abs/2308.12553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aahlad Puli, Lily Zhang, Yoav Wald, Rajesh Ranganath</li>
<li>for: 这篇论文旨在解释短cut学习的问题，即模型在训练分布下采用短cut来预测结果，但在测试分布下可能不适用。</li>
<li>methods: 这篇论文使用了一种名为default-ERM的方法，即通过梯度下降优化十字积分loss函数来训练模型。然而，研究发现，even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, default-ERM still exhibits shortcut learning。</li>
<li>results: 研究发现，default-ERM的 preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization。这种情况存在于感知任务中，并且可以通过开发一种偏见（induction bias）toward uniform margins来解决这个问题。这种偏见使得模型仅仅依赖于稳定的feature，而不是短cut。<details>
<summary>Abstract</summary>
Common explanations for shortcut learning assume that the shortcut improves prediction under the training distribution but not in the test distribution. Thus, models trained via the typical gradient-based optimization of cross-entropy, which we call default-ERM, utilize the shortcut. However, even when the stable feature determines the label in the training distribution and the shortcut does not provide any additional information, like in perception tasks, default-ERM still exhibits shortcut learning. Why are such solutions preferred when the loss for default-ERM can be driven to zero using the stable feature alone? By studying a linear perception task, we show that default-ERM's preference for maximizing the margin leads to models that depend more on the shortcut than the stable feature, even without overparameterization. This insight suggests that default-ERM's implicit inductive bias towards max-margin is unsuitable for perception tasks. Instead, we develop an inductive bias toward uniform margins and show that this bias guarantees dependence only on the perfect stable feature in the linear perception task. We develop loss functions that encourage uniform-margin solutions, called margin control (MARG-CTRL). MARG-CTRL mitigates shortcut learning on a variety of vision and language tasks, showing that better inductive biases can remove the need for expensive two-stage shortcut-mitigating methods in perception tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>传统的短cut learning解释假设，短cut提高训练分布下预测的准确率，但在测试分布下不提供任何帮助。因此，通过常见的梯度下降优化方法来训练模型，我们称之为默认ERM（default-ERM），会使用短cut。然而，即使稳定特征可以决定训练分布下的标签，而短cut不提供任何信息，如感知任务中，默认ERM仍然会展现短cut learning。为何这些解决方案被选择，即使损失函数可以通过稳定特征alone驱动到零？通过研究一个线性感知任务，我们显示了默认ERM的偏好是通过最大化margin来适应模型，这会导致模型更加依赖于短cut，而不是稳定特征，即使没有过参数。这一见解表明，默认ERM的隐式偏好向max-margin是不适合感知任务的。相反，我们开发了一种偏好向 uniform margins，并证明这种偏好使得模型只依赖于稳定特征。我们开发了一种损失函数，called MARG-CTRL，以便鼓励uniform-margin解决方案。MARG-CTRL可以 Mitigating短cut learning在视觉和语言任务中，证明更好的偏好可以消除过两个阶段的短cut-mitigating方法。
</details></li>
</ul>
<hr>
<h2 id="A-Co-training-Approach-for-Noisy-Time-Series-Learning"><a href="#A-Co-training-Approach-for-Noisy-Time-Series-Learning" class="headerlink" title="A Co-training Approach for Noisy Time Series Learning"></a>A Co-training Approach for Noisy Time Series Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12551">http://arxiv.org/abs/2308.12551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiqi Zhang, Jianfeng Zhang, Jia Li, Fugee Tsung</li>
<li>for: 本文关注 robust 时间序列表示学习，假设实际世界时间序列具有噪音和不同视图的信息对分析噪音输入时发挥重要作用。</li>
<li>methods: 我们创建了两个视图的输入时间序列通过两个不同的编码器，然后通过协同对比学习来学习这两个编码器。</li>
<li>results: 我们的TS-CoT方法在四个时间序列标准 benchmark 上的无监督和半监督设置下表现出色，特别是通过利用不同视图的补做信息来mitigate数据噪音和损害。此外，TS-CoT 学习的表示可以通过细化来转移到下游任务中。<details>
<summary>Abstract</summary>
In this work, we focus on robust time series representation learning. Our assumption is that real-world time series is noisy and complementary information from different views of the same time series plays an important role while analyzing noisy input. Based on this, we create two views for the input time series through two different encoders. We conduct co-training based contrastive learning iteratively to learn the encoders. Our experiments demonstrate that this co-training approach leads to a significant improvement in performance. Especially, by leveraging the complementary information from different views, our proposed TS-CoT method can mitigate the impact of data noise and corruption. Empirical evaluations on four time series benchmarks in unsupervised and semi-supervised settings reveal that TS-CoT outperforms existing methods. Furthermore, the representations learned by TS-CoT can transfer well to downstream tasks through fine-tuning.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们关注于鲁棒的时间序列表示学习。我们假设现实世界中的时间序列是噪音的，而不同视图中的相同时间序列信息在分析噪音输入时扮演着重要的角色。基于这个假设，我们创建了两个视图来对输入时间序列进行编码。我们通过训练彩色学习来学习这两个编码器。我们的实验表明，这种合作学习方法可以提高表达性。特别是通过不同视图之间的补充信息，我们的提案的TS-CoT方法可以减轻数据噪音的影响。我们在四个时间序列标准曲线上进行了empirical评估，并在无监督和半监督设置下证明TS-CoT方法在现有方法之上具有显著的提升。此外，TS-CoT方法学习的表示可以通过细化来转移到下游任务中。
</details></li>
</ul>
<hr>
<h2 id="CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias"><a href="#CALM-A-Multi-task-Benchmark-for-Comprehensive-Assessment-of-Language-Model-Bias" class="headerlink" title="CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias"></a>CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12539">http://arxiv.org/abs/2308.12539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vipulgupta1011/calm">https://github.com/vipulgupta1011/calm</a></li>
<li>paper_authors: Vipul Gupta, Pranav Narayanan Venkit, Hugo Laurençon, Shomir Wilson, Rebecca J. Passonneau</li>
<li>for: 这个论文的目的是为了量化和比较语言模型（LM）中的社会经济阶层偏见，以及这些偏见可能对人们造成伤害的可能性。</li>
<li>methods: 这篇论文使用了16个不同领域的数据集，包括Wikipedia和新闻文章，并将224个模板筛选成78,400个例子的 dataset。它们还比较了不同的数据集的多样性，并测试了小型变化的敏感性。</li>
<li>results: 研究发现，新的数据集（CALM）比之前的数据集更加多样和可靠，能够更好地评估语言模型的偏见。研究还发现，某些LM系列中的大型参数模型比小型参数模型更加偏见，而T0系列的模型最少偏见。此外，研究发现一些LM系列中的 gender 和种族偏见与模型大小成正相关。<details>
<summary>Abstract</summary>
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language models including six prominent families of LMs such as Llama-2. In two LM series, OPT and Bloom, we found that larger parameter models are more biased than lower parameter models. We found the T0 series of models to be the least biased. Furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. The code is available at https://github.com/vipulgupta1011/CALM.
</details>
<details>
<summary>摘要</summary>
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language models including six prominent families of LMs such as Llama-2. In two LM series, OPT and Bloom, we found that larger parameter models are more biased than lower parameter models. We found the T0 series of models to be the least biased. Furthermore, we noticed a tradeoff between gender and racial bias with increasing model size in some model series. The code is available at https://github.com/vipulgupta1011/CALM.Here's the translation in Traditional Chinese:当语言模型（LM）的能力不断增强时，这些模型的社会经济特征偏见的评估也变得非常重要。现有的偏见评估数据集在手动设计的模板上有敏感性，因此不可靠。为了获得可靠性，我们引入了语言模型偏见全面评估（CALM）数据集，用于评估语言模型在三个任务上的偏见。我们将16个不同领域的数据集融合，包括Wikipedia和新闻文章，以筛选出224个模板，从而建立一个78,400个例子的数据集。我们与先前的数据集进行比较，包括语义相似性和模板长度的多样性，并测试小量的敏感性。我们发现，我们的数据集比先前的数据集更加多样和可靠，因此更好地捕捉了需要评估模型偏见的语言多样性。我们评估了20个大型语言模型，包括Llama-2等6个家族的模型。在两个LM系列中，OPT和Bloom中，我们发现大型模型比较偏见，而且模型大小增加时，男女和种族偏见之间存在贸易。相关的代码可以在https://github.com/vipulgupta1011/CALM上获取。
</details></li>
</ul>
<hr>
<h2 id="FedSoL-Bridging-Global-Alignment-and-Local-Generality-in-Federated-Learning"><a href="#FedSoL-Bridging-Global-Alignment-and-Local-Generality-in-Federated-Learning" class="headerlink" title="FedSoL: Bridging Global Alignment and Local Generality in Federated Learning"></a>FedSoL: Bridging Global Alignment and Local Generality in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12532">http://arxiv.org/abs/2308.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gihun Lee, Minchan Jeong, Sangmook Kim, Jaehoon Oh, Se-Young Yun</li>
<li>for: 这篇论文的目的是提出一种名为 Federated Stability on Learning (FedSoL) 的联合学习方法，以提高联合学习的稳定性和本地学习的通用性。</li>
<li>methods: 这篇论文使用了一种称为“偏好损失曲线”的概念，将本地学习变得更加稳定，并且不会干扰原本的本地目标。它还使用了一种名为“偏好损失”的概念，帮助实现本地学习的通用性。</li>
<li>results: 这篇论文的实验结果显示，FedSoL 可以在不同的设置下实现州际学习的稳定性和本地学习的通用性，并且在不同的资料分布下实现比较好的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (FedSoL), which combines both the concepts of global alignment and local generality. In FedSoL, the local learning seeks a parameter region robust against proximal perturbations. This strategy introduces an implicit proximal restriction effect in local learning while maintaining the original local objective for parameter update. Our experiments show that FedSoL consistently achieves state-of-the-art performance on various setups.
</details>
<details>
<summary>摘要</summary>
federa 学习（FL）通过将客户端上进行本地训练的模型集成而构建全球模型。 FL 允许在数据隐私的前提下学习模型，但它经常因客户端数据分布不均而表现较差。 多种前一代 FL 算法已经解决了这个问题，通过引入不同的距离约束来激励全球准确性。 然而，这些约束会限制本地学习，因为它们会干扰本地目标。 最近，一种新的方法出现了，以提高本地学习通用性。 通过在本地学习中获得一个平滑的损失曲线，这种方法可以减轻客户端之间的冲突。 然而，这种方法不能保证全球稳定性，因为本地学习没有考虑全球目标。 在这项研究中，我们提出了联邦稳定学习（FedSoL），它结合了全球准确性和本地通用性的两个概念。 在 FedSoL 中，本地学习寻找一个鲁棒对抗辐射的参数区域。 这种策略将在本地学习中引入隐形的距离约束效果，同时保持原始本地目标 для参数更新。 我们的实验表明，FedSoL 可靠地实现多种设置的状态前例性表现。
</details></li>
</ul>
<hr>
<h2 id="SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks"><a href="#SieveNet-Selecting-Point-Based-Features-for-Mesh-Networks" class="headerlink" title="SieveNet: Selecting Point-Based Features for Mesh Networks"></a>SieveNet: Selecting Point-Based Features for Mesh Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12530">http://arxiv.org/abs/2308.12530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sievenet/sievenet.github.io">https://github.com/sievenet/sievenet.github.io</a></li>
<li>paper_authors: Shengchao Yuan, Yishun Dou, Rui Shi, Bingbing Ni, Zhong Zheng</li>
<li>for:  This paper aims to address the limitations of using remeshed proxies in mesh neural networks, and proposes a novel paradigm called SieveNet that combines the benefits of regular topology and exact geometry.</li>
<li>methods:  The proposed method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. It also eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer.</li>
<li>results:  The comprehensive experimental results on classification and segmentation tasks demonstrate the effectiveness and superiority of the proposed method, showing that it can retain the underlying geometry of the original mesh while maintaining the benefits of regular topology.<details>
<summary>Abstract</summary>
Meshes are widely used in 3D computer vision and graphics, but their irregular topology poses challenges in applying them to existing neural network architectures. Recent advances in mesh neural networks turn to remeshing and push the boundary of pioneer methods that solely take the raw meshes as input. Although the remeshing offers a regular topology that significantly facilitates the design of mesh network architectures, features extracted from such remeshed proxies may struggle to retain the underlying geometry faithfully, limiting the subsequent neural network's capacity. To address this issue, we propose SieveNet, a novel paradigm that takes into account both the regular topology and the exact geometry. Specifically, this method utilizes structured mesh topology from remeshing and accurate geometric information from distortion-aware point sampling on the surface of the original mesh. Furthermore, our method eliminates the need for hand-crafted feature engineering and can leverage off-the-shelf network architectures such as the vision transformer. Comprehensive experimental results on classification and segmentation tasks well demonstrate the effectiveness and superiority of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>meshes 广泛应用于三维计算机视觉和图形处理中，但它们的不规则结构会使得现有神经网络架构中应用困难。最近的 mesh 神经网络技术转而使用重新拟合，将边缘推导法的限制 pushed 到最前面。虽然重新拟合提供了规则的结构，但从such 拟合代理中提取的特征可能难以准确保持下方geometry，这会限制后续神经网络的能力。为解决这个问题，我们提出了 Sievenet，一种新的思路，该思路考虑了结构化的 mesh 结构和原始 mesh 上的精度信息。此外，我们的方法不需要手动设计特征工程，可以利用现有的网络架构，如视Transformer。 comprehensive 实验结果表明，我们的方法在分类和 segmentation 任务中表现出色，superior 于 существу方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The traditional Chinese writing system is also widely used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="UNISOUND-System-for-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#UNISOUND-System-for-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023"></a>UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12526">http://arxiv.org/abs/2308.12526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Zheng, Yajun Zhang, Chuanying Niu, Yibin Zhan, Yanhua Long, Dongxing Xu</li>
<li>for: 这份报告介绍了在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC 2023）的Track1和Track2上提交的UNISOUND系统。我们对Track 1和Track 2使用了同一个系统，它是基于VoxCeleb2-dev进行训练的。</li>
<li>methods: 我们采用了大规模ResNet和RepVGG架构来开发系统，并提出了一种具有稳定性的相似性分数权重因子（CMF）来改善系统的性能。</li>
<li>results: 我们的最终系统是六个模型的 fusions，在VoxSRC 2023的Track 1中获得了第一名，在Track 2中获得了第二名。我们的最终提交的minDCF为0.0855，EER为1.5880%。<details>
<summary>Abstract</summary>
This report describes the UNISOUND submission for Track1 and Track2 of VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC 2023). We submit the same system on Track 1 and Track 2, which is trained with only VoxCeleb2-dev. Large-scale ResNet and RepVGG architectures are developed for the challenge. We propose a consistency-aware score calibration method, which leverages the stability of audio voiceprints in similarity score by a Consistency Measure Factor (CMF). CMF brings a huge performance boost in this challenge. Our final system is a fusion of six models and achieves the first place in Track 1 and second place in Track 2 of VoxSRC 2023. The minDCF of our submission is 0.0855 and the EER is 1.5880%.
</details>
<details>
<summary>摘要</summary>
这份报告介绍了我们在VoxCeleb Speaker Recognition Challenge 2023（VoxSRC 2023）的UNISOUND提交，包括Track1和Track2。我们使用了VoxCeleb2-dev进行训练，并开发了大规模ResNet和RepVGG架构。我们提出了一种具有稳定性的相似性分数准则，通过一个稳定度指数（CMF）来优化分数。CMF带来了很大的性能提升。我们的最终系统是六个模型的拟合，在Track 1中获得第一名，在Track 2中获得第二名。我们的最小dCF是0.0855，EER为1.5880%。
</details></li>
</ul>
<hr>
<h2 id="Not-Only-Rewards-But-Also-Constraints-Applications-on-Legged-Robot-Locomotion"><a href="#Not-Only-Rewards-But-Also-Constraints-Applications-on-Legged-Robot-Locomotion" class="headerlink" title="Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion"></a>Not Only Rewards But Also Constraints: Applications on Legged Robot Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12517">http://arxiv.org/abs/2308.12517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunho Kim, Hyunsik Oh, Jeonghyun Lee, Jinhyeok Choi, Gwanghyeon Ji, Moonkyu Jung, Donghoon Youm, Jemin Hwangbo</li>
<li>for: 这个论文的目的是提出一种基于奖励学习的控制器训练方法，以便用于复杂的机械系统。</li>
<li>methods: 该方法使用了一种基于神经网络的奖励学习算法，并具有两种约束类型和高效的政策优化算法。</li>
<li>results: 该方法可以在许多不同的四肢动物机器人上培养出高性能的控制器，并且可以避免大量的奖励工程化。<details>
<summary>Abstract</summary>
Several earlier studies have shown impressive control performance in complex robotic systems by designing the controller using a neural network and training it with model-free reinforcement learning. However, these outstanding controllers with natural motion style and high task performance are developed through extensive reward engineering, which is a highly laborious and time-consuming process of designing numerous reward terms and determining suitable reward coefficients. In this work, we propose a novel reinforcement learning framework for training neural network controllers for complex robotic systems consisting of both rewards and constraints. To let the engineers appropriately reflect their intent to constraints and handle them with minimal computation overhead, two constraint types and an efficient policy optimization algorithm are suggested. The learning framework is applied to train locomotion controllers for several legged robots with different morphology and physical attributes to traverse challenging terrains. Extensive simulation and real-world experiments demonstrate that performant controllers can be trained with significantly less reward engineering, by tuning only a single reward coefficient. Furthermore, a more straightforward and intuitive engineering process can be utilized, thanks to the interpretability and generalizability of constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</details>
<details>
<summary>摘要</summary>
多个以前的研究已经表现出了复杂机器人系统中使用神经网络控制器和无模型奖励学习实现出色的控制性能。然而，这些独特的控制器通过大量的奖励工程来实现自然的运动风格和高任务性能，这是一个非常劳动 INTENSITY 和时间消耗的过程。在这种工作中，我们提出了一种基于奖励学习的控制器训练框架，用于训练神经网络控制器。为了让工程师明确表达他们的意图并快速地处理约束，我们建议了两种约束类型和一种高效的政策优化算法。该学习框架在许多脚趾机器人的不同形态和物理属性下训练步行控制器，并在实际实验和模拟中得到了证明。结果表明，可以通过较少的奖励工程，只需调整单个奖励系数，可以训练出高性能的控制器。此外，通过约束的解释和普适性，工程师可以使用更直观和直接的工程过程。相关视频可以在https://youtu.be/KAlm3yskhvM中找到。
</details></li>
</ul>
<hr>
<h2 id="Masked-Autoencoders-are-Efficient-Class-Incremental-Learners"><a href="#Masked-Autoencoders-are-Efficient-Class-Incremental-Learners" class="headerlink" title="Masked Autoencoders are Efficient Class Incremental Learners"></a>Masked Autoencoders are Efficient Class Incremental Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12510">http://arxiv.org/abs/2308.12510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scok30/mae-cil">https://github.com/scok30/mae-cil</a></li>
<li>paper_authors: Jiang-Tian Zhai, Xialei Liu, Andrew D. Bagdanov, Ke Li, Ming-Ming Cheng</li>
<li>for: Sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge.</li>
<li>methods: 使用Masked Autoencoders (MAEs) 为Class Incremental Learning (CIL) 提供有效的学习方法，并通过捆绑supervised loss来实现分类。</li>
<li>results: 实验表明，我们的方法在CIFAR-100、ImageNet-Subset和ImageNet-Full上比前一个状态的方法表现更好，并且可以更好地保持过去任务的知识。<details>
<summary>Abstract</summary>
Class Incremental Learning (CIL) aims to sequentially learn new classes while avoiding catastrophic forgetting of previous knowledge. We propose to use Masked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originally designed to learn useful representations through reconstructive unsupervised learning, and they can be easily integrated with a supervised loss for classification. Moreover, MAEs can reliably reconstruct original input images from randomly selected patches, which we use to store exemplars from past tasks more efficiently for CIL. We also propose a bilateral MAE framework to learn from image-level and embedding-level fusion, which produces better-quality reconstructed images and more stable representations. Our experiments confirm that our approach performs better than the state-of-the-art on CIFAR-100, ImageNet-Subset, and ImageNet-Full. The code is available at https://github.com/scok30/MAE-CIL .
</details>
<details>
<summary>摘要</summary>
类增量学习（CIL）目标是逐渐学习新类，而不导致之前的知识忘记。我们提议使用Masked Autoencoders（MAEs）作为CIL的有效学习器。MAEs原本是用于通过无监督学习获得有用表示，并可以轻松地与分类损失结合使用。此外，MAEs可靠地重建原始输入图像从随机选择的块中，我们用此来更高效地存储过去任务的示例。我们还提出了双向MAE框架，以学习图像水平和表示水平的混合，该框架可生成更高质量的重建图像和更稳定的表示。我们的实验表明，我们的方法在CIFAR-100、ImageNet-Subset和ImageNet-Full上比现状态的更好。代码可以在https://github.com/scok30/MAE-CIL上获取。
</details></li>
</ul>
<hr>
<h2 id="False-Information-Bots-and-Malicious-Campaigns-Demystifying-Elements-of-Social-Media-Manipulations"><a href="#False-Information-Bots-and-Malicious-Campaigns-Demystifying-Elements-of-Social-Media-Manipulations" class="headerlink" title="False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations"></a>False Information, Bots and Malicious Campaigns: Demystifying Elements of Social Media Manipulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12497">http://arxiv.org/abs/2308.12497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Majid Akhtar, Rahat Masood, Muhammad Ikram, Salil S. Kanhere</li>
<li>for: 本研究旨在提供关于社交媒体欺诈（SMM）元素的全面分析，以满足现有研究中缺乏这些元素的涵盖。</li>
<li>methods: 本研究通过系统性的文献调查，挖掘社交媒体欺诈元素的相互关系，并提出了现有研究的共同点、缺点和有价值的发现。</li>
<li>results: 研究发现，社交媒体欺诈的元素包括假信息、机器人和恶意运动，这些元素之间存在着密切的关系。此外，研究还发现了现有研究中缺乏用户心理学、假信息检测和恶意软件的研究，这些方面的研究具有重要的意义。<details>
<summary>Abstract</summary>
The rapid spread of false information and persistent manipulation attacks on online social networks (OSNs), often for political, ideological, or financial gain, has affected the openness of OSNs. While researchers from various disciplines have investigated different manipulation-triggering elements of OSNs (such as understanding information diffusion on OSNs or detecting automated behavior of accounts), these works have not been consolidated to present a comprehensive overview of the interconnections among these elements. Notably, user psychology, the prevalence of bots, and their tactics in relation to false information detection have been overlooked in previous research. To address this research gap, this paper synthesizes insights from various disciplines to provide a comprehensive analysis of the manipulation landscape. By integrating the primary elements of social media manipulation (SMM), including false information, bots, and malicious campaigns, we extensively examine each SMM element. Through a systematic investigation of prior research, we identify commonalities, highlight existing gaps, and extract valuable insights in the field. Our findings underscore the urgent need for interdisciplinary research to effectively combat social media manipulations, and our systematization can guide future research efforts and assist OSN providers in ensuring the safety and integrity of their platforms.
</details>
<details>
<summary>摘要</summary>
随着社交媒体 manipulate（SMM）的迅速扩散和持续的欺诈攻击，在在线社交网络（OSN）上的开放性受到了影响。虽然不同领域的研究人员已经对OSN上的不同欺诈触发元素进行了研究（如了解信息传播在OSN上或检测帐户的自动化行为），但这些研究没有被集成起来，不能提供全面的报告。特别是用户心理学、bot的普遍性和他们在假信息检测方面的策略尚未被前期研究所涵盖。为了填补这个研究漏洞，本文通过将不同领域的知识和技术集成起来，进行了全面的欺诈领域的分析。我们通过系统性的调查和分析，找到了各种SMM元素之间的相互关联，并揭示了现有的研究漏洞和未解决的问题。我们的发现表明，需要跨学科研究，以有效地对抗社交媒体欺诈，而我们的系统化分析可以导引未来的研究努力，并帮助OSN提供者保持平台的安全和完整性。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Neural-Network-Scale-for-ECG-Classification"><a href="#Optimizing-Neural-Network-Scale-for-ECG-Classification" class="headerlink" title="Optimizing Neural Network Scale for ECG Classification"></a>Optimizing Neural Network Scale for ECG Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12492">http://arxiv.org/abs/2308.12492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeong Tak Lee, Yong-Yeon Jo, Joon-Myoung Kwon</li>
<li>for: 这个论文主要是为了分析电cardiogram（ECG）信号，使用卷积神经网络（CNN）模型。</li>
<li>methods: 这个论文使用了残差神经网络（ResNet）模型，并对其进行了优化。</li>
<li>results: 研究发现，采用更 shallow 的网络、更多的通道数和更小的卷积kernel size可以提高 ECG 分类的性能。<details>
<summary>Abstract</summary>
We study scaling convolutional neural networks (CNNs), specifically targeting Residual neural networks (ResNet), for analyzing electrocardiograms (ECGs). Although ECG signals are time-series data, CNN-based models have been shown to outperform other neural networks with different architectures in ECG analysis. However, most previous studies in ECG analysis have overlooked the importance of network scaling optimization, which significantly improves performance. We explored and demonstrated an efficient approach to scale ResNet by examining the effects of crucial parameters, including layer depth, the number of channels, and the convolution kernel size. Through extensive experiments, we found that a shallower network, a larger number of channels, and smaller kernel sizes result in better performance for ECG classifications. The optimal network scale might differ depending on the target task, but our findings provide insight into obtaining more efficient and accurate models with fewer computing resources or less time. In practice, we demonstrate that a narrower search space based on our findings leads to higher performance.
</details>
<details>
<summary>摘要</summary>
我们研究了归一化 convolutional neural networks (CNNs)，特指 Residual neural networks (ResNet)，用于分析电子心脏图像 (ECG)。虽然 ECG 信号是时间序列数据，但 CNN 基本结构已经在 ECG 分析中显示出比其他神经网络更高的性能。然而，大多数前一 Studies 在 ECG 分析中忽略了网络缩放优化的重要性，这对性能有着重要的提高作用。我们探索了和证明了一种有效的方法来缩放 ResNet，包括层数、通道数和滤波器大小等关键参数的影响。通过广泛的实验，我们发现了一个更浅的网络结构、更多的通道数和更小的滤波器大小会对 ECG 分类表现更好。但是，优化网络缩放的最佳方法可能因目标任务而异，但我们的发现可以帮助您更快地获得更高效和更准确的模型，使用更少的计算资源或时间。在实践中，我们示出了基于我们发现的更窄的搜索空间可以 достичь更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Fall-Detection-using-Knowledge-Distillation-Based-Long-short-term-memory-for-Offline-Embedded-and-Low-Power-Devices"><a href="#Fall-Detection-using-Knowledge-Distillation-Based-Long-short-term-memory-for-Offline-Embedded-and-Low-Power-Devices" class="headerlink" title="Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices"></a>Fall Detection using Knowledge Distillation Based Long short-term memory for Offline Embedded and Low Power Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12481">http://arxiv.org/abs/2308.12481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Zhou, Allison Chen, Celine Buer, Emily Chen, Kayleen Tang, Lauryn Gong, Zhiqi Liu, Jianbin Tang</li>
<li>for: 本研究旨在提出一种可靠、低功耗的滥落检测方法，以提高检测精度。</li>
<li>methods: 本研究使用知识储备学习（Knowledge Distillation）基于LSTM（Long Short-Term Memory）模型，以提高检测精度。它主要分析各种感知器的时序数据，并提供实时检测功能，以确保提前、可靠地识别滥落。</li>
<li>results: 研究表明，通过使用知识储备学习技术，可以提高检测精度，同时降低能耗。因此，该提议的解决方案可能成为未来关键领域中能效的滥落检测系统的开发之道。<details>
<summary>Abstract</summary>
This paper presents a cost-effective, low-power approach to unintentional fall detection using knowledge distillation-based LSTM (Long Short-Term Memory) models to significantly improve accuracy. With a primary focus on analyzing time-series data collected from various sensors, the solution offers real-time detection capabilities, ensuring prompt and reliable identification of falls. The authors investigate fall detection models that are based on different sensors, comparing their accuracy rates and performance. Furthermore, they employ the technique of knowledge distillation to enhance the models' precision, resulting in refined accurate configurations that consume lower power. As a result, this proposed solution presents a compelling avenue for the development of energy-efficient fall detection systems for future advancements in this critical domain.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字" or "简化字".Translation in Traditional Chinese:本研究提出了一种经济高效、低功耗的滥落检测方法，使用知识储 transmit 基于 LSTM (Long Short-Term Memory) 模型，以提高准确率。该解决方案主要是分析不同传感器收集的时间序列数据，并在实时检测中提供可靠的滥落标识。作者们比较不同传感器基于的滥落检测模型的准确率和性能，并使用知识储 transmit 技术进一步提高模型的精度，从而实现更加精准的配置，同时减少功耗。因此，该提出的解决方案为未来在这一重要领域的能量减少型滥落检测系统的开发提供了一条可行的道路。
</details></li>
</ul>
<hr>
<h2 id="Zero-delay-Consistent-Signal-Reconstruction-from-Streamed-Multivariate-Time-Series"><a href="#Zero-delay-Consistent-Signal-Reconstruction-from-Streamed-Multivariate-Time-Series" class="headerlink" title="Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series"></a>Zero-delay Consistent Signal Reconstruction from Streamed Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12459">http://arxiv.org/abs/2308.12459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emilio Ruiz-Moreno, Luis Miguel López-Ramos, Baltasar Beferull-Lozano</li>
<li>for: 这篇论文是为了提出一种可靠地从数据流中重建多变量时间序列的方法。</li>
<li>methods: 该方法使用了一种基于循环神经网络的方法，通过学习时空相关性来减少数据流中的信号拟合粗糙。</li>
<li>results: 对于测试数据，该方法可以实现与采样率相对的较低的错误率下降，并且保证了信号重建的一致性。<details>
<summary>Abstract</summary>
Digitalizing real-world analog signals typically involves sampling in time and discretizing in amplitude. Subsequent signal reconstructions inevitably incur an error that depends on the amplitude resolution and the temporal density of the acquired samples. From an implementation viewpoint, consistent signal reconstruction methods have proven a profitable error-rate decay as the sampling rate increases. Despite that, these results are obtained under offline settings. Therefore, a research gap exists regarding methods for consistent signal reconstruction from data streams. This paper presents a method that consistently reconstructs streamed multivariate time series of quantization intervals under a zero-delay response requirement. On the other hand, previous work has shown that the temporal dependencies within univariate time series can be exploited to reduce the roughness of zero-delay signal reconstructions. This work shows that the spatiotemporal dependencies within multivariate time series can also be exploited to achieve improved results. Specifically, the spatiotemporal dependencies of the multivariate time series are learned, with the assistance of a recurrent neural network, to reduce the roughness of the signal reconstruction on average while ensuring consistency. Our experiments show that our proposed method achieves a favorable error-rate decay with the sampling rate compared to a similar but non-consistent reconstruction.
</details>
<details>
<summary>摘要</summary>
通常情况下，将实际世界上的扩散信号数字化都需要采样时间和采样幅度。接下来的信号重建都会产生错误，这些错误取决于采样幅度和时间分布的获取频率。尽管在实现视图下，一些信号重建方法已经证明了随采样率增加的负责任逻辑下的错误率下降。然而，这些结果是在线上获取的。因此，一个研究漏洞存在，即如何在数据流中一致地重建流体时间序列。这篇论文提出了一种方法，可以在零延迟响应下一致地重建流体多变量时间序列。在前一些研究中，已经证明了在单变量时间序列中可以利用时间相关性来减少零延迟重建的粗糙性。这篇论文则表明，在多变量时间序列中也可以利用空间时间相关性来实现改进的结果。具体来说，通过一种循环神经网络的帮助，学习多变量时间序列中的空间时间相关性，以减少重建中的粗糙性，同时保证一致性。我们的实验表明，我们提出的方法可以与采样率相比，实现更好的错误率下降。
</details></li>
</ul>
<hr>
<h2 id="PFL-GAN-When-Client-Heterogeneity-Meets-Generative-Models-in-Personalized-Federated-Learning"><a href="#PFL-GAN-When-Client-Heterogeneity-Meets-Generative-Models-in-Personalized-Federated-Learning" class="headerlink" title="PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning"></a>PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12454">http://arxiv.org/abs/2308.12454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achintha Wijesinghe, Songyang Zhang, Zhi Ding</li>
<li>for: 这篇论文的目的是提出一种基于对各客户数据的聚合和共享的GAN模型，以应对客户数据不同的情况。</li>
<li>methods: 本论文使用的方法是基于GAN模型，并且提出了一种Weighted Collaborative Data Aggregation（WCDA）策略，以应对客户数据不同的情况。</li>
<li>results: 经过严谨的实验证明，PFL-GAN可以有效地应对客户数据不同的情况，并且可以提高模型的准确性和稳定性。<details>
<summary>Abstract</summary>
Recent advances of generative learning models are accompanied by the growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure, and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) sometimes can be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To cope with client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in different scenarios. More specially, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results through the rigorous experimentation on several well-known datasets demonstrate the effectiveness of PFL-GAN.
</details>
<details>
<summary>摘要</summary>
Recent advances in generative learning models have led to growing interest in federated learning (FL) based on generative adversarial network (GAN) models. In the context of FL, GAN can capture the underlying client data structure and regenerate samples resembling the original data distribution without compromising the private raw data. Although most existing GAN-based FL works focus on training a global model, Personalized FL (PFL) can sometimes be more effective in view of client data heterogeneity in terms of distinct data sample distributions, feature spaces, and labels. To address client heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation strategy for PFL. The proposed PFL-GAN addresses client heterogeneity in different scenarios. Specifically, we first learn the similarity among clients and then develop an weighted collaborative data aggregation. The empirical results from rigorous experimentation on several well-known datasets demonstrate the effectiveness of PFL-GAN.Here is the word-for-word translation of the text into Simplified Chinese:现代生成学模型的进步使得联合学习（FL）基于生成对抗网络（GAN）模型的兴趣不断增长。FL中的GAN可以捕捉客户端数据结构，并生成符合原始数据分布的样本，无需泄露私有原始数据。大多数现有的GAN基于FL工作集中在全球模型的训练上，而个性化联合学习（PFL）在客户端数据多样性方面可以更有效。为了Address客户端多样性在GAN基于FL中，我们提出了一种新的GAN共享和汇集策略，称为PFL-GAN。PFL-GAN在不同的场景下可以处理客户端多样性。具体来说，我们首先学习客户端之间的相似性，然后开发一种权重合作数据汇集。经验结果表明，PFL-GAN在多个知名数据集上得到了有效的result。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-medical-image-classifiers-with-synthetic-data-from-latent-diffusion-models"><a href="#Augmenting-medical-image-classifiers-with-synthetic-data-from-latent-diffusion-models" class="headerlink" title="Augmenting medical image classifiers with synthetic data from latent diffusion models"></a>Augmenting medical image classifiers with synthetic data from latent diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12453">http://arxiv.org/abs/2308.12453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke W. Sagers, James A. Diao, Luke Melas-Kyriazi, Matthew Groh, Pranav Rajpurkar, Adewole S. Adamson, Veronica Rotemberg, Roxana Daneshjou, Arjun K. Manrai</li>
<li>for: 这个论文的目的是提高医疗人工智能算法的性能，尤其是在数据有限的情况下。</li>
<li>methods: 该论文使用了潜在扩散模型来生成皮肤病图像，并在模型训练中添加这些数据以提高性能。</li>
<li>results: 研究发现，使用生成的皮肤病图像可以提高模型的性能，但是这种提高的效果随着真实图像的比例而减少。研究还生成了458,920张synthetic图像，并对其进行分析。<details>
<summary>Abstract</summary>
While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA), many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations. Some have proposed that generative AI could reduce the need for real data, but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images. As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using several generation strategies. Our results suggest that synthetic data could serve as a force-multiplier for model development, but the collection of diverse real-world data remains the most important step to improve medical AI algorithms.
</details>
<details>
<summary>摘要</summary>
美国食品和药品管理局（FDA）已批准或批准了数百种人工智能（AI）算法，但许多研究表明这些算法在不同人口群体中存在不一致的泛化或隐藏偏见。一些人提议使用生成AI可以减少实际数据的需求，但这些模型在模型开发中的用途仍然不清楚。皮肤病提供了一个有用的案例研究，因为皮肤病的表现iversity，特别是根据保护特征（skin tone）。我们显示了潜在扩散模型可以在数据有限情况下扫描性地生成皮肤病图像，并且在模型训练时添加这些数据可以提高性能。这些性能提升在真实图像与生成图像的比例大于10:1时达到饱和，与真实图像添加的提升相比较小。在分析中，我们生成了458,920个synthetic图像，并进行了分析。我们的结果表明，生成数据可以成为模型开发的力multiplier，但收集真实世界数据仍然是改进医疗AI算法的最重要步骤。
</details></li>
</ul>
<hr>
<h2 id="An-Intentional-Forgetting-Driven-Self-Healing-Method-For-Deep-Reinforcement-Learning-Systems"><a href="#An-Intentional-Forgetting-Driven-Self-Healing-Method-For-Deep-Reinforcement-Learning-Systems" class="headerlink" title="An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"></a>An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12445">http://arxiv.org/abs/2308.12445</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahmedhajyahmed/drdrl">https://github.com/ahmedhajyahmed/drdrl</a></li>
<li>paper_authors: Ahmed Haj Yahmed, Rached Bouchoucha, Houssem Ben Braiek, Foutse Khomh</li>
<li>for: 本研究旨在解决深度强化学习（DRL）系统在大规模生产环境中遇到的环境风险问题。</li>
<li>methods: 本研究提出了一种名为“Dr. DRL”的自适应approach，它将在DRL系统中 интеGRATE一种新的忘记机制，以解决环境风险问题。</li>
<li>results:  compared withvanilla CL，Dr. DRL能够降低平均维护时间和精制化集数量，并在19.63%的风险环境中保持或提高获得的奖励。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment's conditions shifts. However, successive shifts of considerable magnitude may cause the production environment to drift from its original state. Recent studies have shown that these environmental drifts tend to drive CL into long, or even unsuccessful, healing cycles, which arise from inefficiencies such as catastrophic forgetting, warm-starting failure, and slow convergence. In this paper, we propose Dr. DRL, an effective self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL to overcome its main issues. Dr. DRL deliberately erases the DRL system's minor behaviors to systematically prioritize the adaptation of the key problem-solving skills. Using well-established DRL algorithms, Dr. DRL is compared with vanilla CL on various drifted environments. Dr. DRL is able to reduce, on average, the healing time and fine-tuning episodes by, respectively, 18.74% and 17.72%. Dr. DRL successfully helps agents to adapt to 19.63% of drifted environments left unsolved by vanilla CL while maintaining and even enhancing by up to 45% the obtained rewards for drifted environments that are resolved by both approaches.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）在大规模生产环境中应用越来越普遍，如Netflix和Facebook。就像大多数数据驱动系统一样，DRL系统可能会出现不жела的行为，这些行为经常在生产环境中逐渐发生变化。逐渐学习（CL）是DRLAgent的自适应方法，但是连续的环境变化可能会使生产环境偏离原始状态。现在的研究表明，这些环境变化可能会驱使CL进入长期或无法恢复的治疗循环，这些循环由于不可预测的干扰、热启动失败和慢 converges而起来。在这篇论文中，我们提出了Dr. DRL，一种有效的自适应方法，用于解决DRL系统中的主要问题。Dr. DRL通过在vanilla CL中添加一种新的忘记机制，系统地忘记DRL系统的次要行为，以优先级刻意适应关键问题的解决技能。使用了已知的DRL算法，Dr. DRL与vanilla CL进行比较，Dr. DRL能够在不同的逐渐变化环境中减少，平均退还时间和精度调整集数量为18.74%和17.72%。Dr. DRL成功地帮助代理人适应19.63%的逐渐变化环境，同时保持和提高达45%的得到的奖励。
</details></li>
</ul>
<hr>
<h2 id="TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction"><a href="#TAI-GAN-Temporally-and-Anatomically-Informed-GAN-for-early-to-late-frame-conversion-in-dynamic-cardiac-PET-motion-correction" class="headerlink" title="TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction"></a>TAI-GAN: Temporally and Anatomically Informed GAN for early-to-late frame conversion in dynamic cardiac PET motion correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12443">http://arxiv.org/abs/2308.12443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gxq1998/tai-gan">https://github.com/gxq1998/tai-gan</a></li>
<li>paper_authors: Xueqi Guo, Luyao Shi, Xiongchao Chen, Bo Zhou, Qiong Liu, Huidong Xie, Yi-Hwa Liu, Richard Palyo, Edward J. Miller, Albert J. Sinusas, Bruce Spottiswoode, Chi Liu, Nicha C. Dvornek</li>
<li>for: 这个论文是为了解决快速追踪和动态心脏PET中的帧间运动偏移问题，特别是在早期帧中，传统的INTENSITY-based图像 регистраción技术无法应用。</li>
<li>methods: 该论文提出了一种使用生成方法来处理追踪分布的变化，以帮助现有的图像 регистраción方法。具体来说，我们提出了一种名为TAI-GAN的模型，通过一个all-to-one映射，将早期帧转换成参照帧中的图像。</li>
<li>results: 我们在一个临床 $^{82}$Rb PET数据集上验证了我们的TAI-GAN模型，并发现它可以生成高质量的转换图像，与参照帧的真实图像相似。另外，我们还发现，在使用TAI-GAN转换后，运动估计精度和临床血液流量（MBF）的量化也得到了改善。<details>
<summary>Abstract</summary>
The rapid tracer kinetics of rubidium-82 ($^{82}$Rb) and high variation of cross-frame distribution in dynamic cardiac positron emission tomography (PET) raise significant challenges for inter-frame motion correction, particularly for the early frames where conventional intensity-based image registration techniques are not applicable. Alternatively, a promising approach utilizes generative methods to handle the tracer distribution changes to assist existing registration methods. To improve frame-wise registration and parametric quantification, we propose a Temporally and Anatomically Informed Generative Adversarial Network (TAI-GAN) to transform the early frames into the late reference frame using an all-to-one mapping. Specifically, a feature-wise linear modulation layer encodes channel-wise parameters generated from temporal tracer kinetics information, and rough cardiac segmentations with local shifts serve as the anatomical information. We validated our proposed method on a clinical $^{82}$Rb PET dataset and found that our TAI-GAN can produce converted early frames with high image quality, comparable to the real reference frames. After TAI-GAN conversion, motion estimation accuracy and clinical myocardial blood flow (MBF) quantification were improved compared to using the original frames. Our code is published at https://github.com/gxq1998/TAI-GAN.
</details>
<details>
<summary>摘要</summary>
《快速追踪器kinetics和动态心脏PET中的差异性框架分布降低了插入动作 corrections的极大挑战，特别是在早期帧中，传统的Intensity-based图像registration技术无法适用。 alternatively，我们提出了利用生成方法来处理追踪器分布变化，以帮助现有的registration方法。为了提高帧内registration和参数量化，我们提出了一种Temporally and Anatomically Informed Generative Adversarial Network（TAI-GAN），用于将早期帧转换成参照帧中的late frame。specifically，一个特征wise linear modulation层编码了由时间追踪器动力学信息生成的通道wise参数，而粗略的cardiac segmentation with local shifts serves as the anatomical information。我们在临床$^{82}$Rb PET数据集上验证了我们的提议方法，并发现了TAI-GAN可以生成高品质的转换帧，与参照帧相似。 после TAI-GAN转换，运动估计精度和临床血液流量（MBF）量化得到了改进。我们的代码已经在https://github.com/gxq1998/TAI-GAN上发布。》Note: Simplified Chinese is used here, which is a more casual and informal version of Chinese. If you prefer Traditional Chinese or a more formal version, please let me know and I can translate it accordingly.
</details></li>
</ul>
<hr>
<h2 id="BaDExpert-Extracting-Backdoor-Functionality-for-Accurate-Backdoor-Input-Detection"><a href="#BaDExpert-Extracting-Backdoor-Functionality-for-Accurate-Backdoor-Input-Detection" class="headerlink" title="BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection"></a>BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12439">http://arxiv.org/abs/2308.12439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal</li>
<li>for: 防止深度神经网络（DNN）中的后门攻击（backdoor attacks）。</li>
<li>methods: 基于反工程approach，直接提取backdoored模型中的后门功能，并将其转换为一个名为backdoor expert模型。</li>
<li>results: 能够高度准确地检测backdoor输入，并且可以在模型推理过程中过滤掉backdoor输入。<details>
<summary>Abstract</summary>
We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet) across various model architectures (ResNet, VGG, MobileNetV2 and Vision Transformer).
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的防御机制，用于对深度神经网络（DNN）中的后门攻击。在这种攻击中，敌对者会隐藏式地嵌入恶意行为（后门）到DNN中。我们的防御方式属于post-开发防御，可以独立于模型的生成方式操作。我们的防御方式基于一种新的反工程approach，可以直接将backdoored模型中的后门功能提取出来，并将其转换为一个名为backdoor expert模型。这种approach straightforward，只需要使用一些意外地标注的清洁样本进行微调，使得模型忘记了正常的功能，仍然保留后门功能，从而生成一个只能识别后门输入的模型。基于提取出的backdoor expert模型，我们证明了可以设计高精度的后门输入检测器，以避免在模型推断过程中发现后门输入。此外，我们还使用了一种ensemble策略和微调的auxiliary模型，我们的防御机制（BaDExpert）可以高效地抵御16种SOTA后门攻击，而且尽量减少了干扰量。我们的防御机制在多个 dataset（CIFAR10、GTSRB和ImageNet）和多种模型架构（ResNet、VGG、MobileNetV2和Vision Transformer）上进行了验证。
</details></li>
</ul>
<hr>
<h2 id="Deploying-Deep-Reinforcement-Learning-Systems-A-Taxonomy-of-Challenges"><a href="#Deploying-Deep-Reinforcement-Learning-Systems-A-Taxonomy-of-Challenges" class="headerlink" title="Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges"></a>Deploying Deep Reinforcement Learning Systems: A Taxonomy of Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12438">http://arxiv.org/abs/2308.12438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/drldeploymentchallenges-icsme2023/replicationpackage">https://github.com/drldeploymentchallenges-icsme2023/replicationpackage</a></li>
<li>paper_authors: Ahmed Haj Yahmed, Altaf Allah Abbassi, Amin Nikanjam, Heng Li, Foutse Khomh</li>
<li>For: The paper aims to identify and understand the challenges that practitioners face when deploying deep reinforcement learning (DRL) systems, and to provide a taxonomy of these challenges.* Methods: The paper uses an empirical study on Stack Overflow (SO), a popular Q&amp;A forum for developers, to identify and analyze the challenges related to DRL deployment. The study involves categorizing relevant SO posts by deployment platforms, filtering and manually analyzing the posts, and investigating the prevalence and difficulty of the challenges.* Results: The study finds that the general interest in DRL deployment is growing, and that DRL deployment is more difficult than other DRL issues. The paper also identifies a taxonomy of 31 unique challenges in deploying DRL to different platforms, with RL environment-related challenges being the most popular and communication-related challenges being the most difficult among practitioners.Here are the three key points in Simplified Chinese text:* For: 本研究目标是找到和理解实践者在深度强化学习（DRL）系统部署中遇到的挑战，并提供一个DRL部署挑战的分类。* Methods: 本研究使用Stack Overflow（SO），一个最受欢迎的开发者问答论坛，来找到和分析DRL部署中遇到的挑战。研究包括对SO帖子的分类、筛选和手动分析，以及挑战的流行度和困难程度的调查。* Results: 研究发现DRL部署的一般兴趣在增长，而DRL部署比其他DRL问题更加困难。研究还发现了31个不同的DRL部署挑战，其中RL环境相关的挑战最多，而通信相关的挑战最Difficult。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL), leveraging Deep Learning (DL) in reinforcement learning, has shown significant potential in achieving human-level autonomy in a wide range of domains, including robotics, computer vision, and computer games. This potential justifies the enthusiasm and growing interest in DRL in both academia and industry. However, the community currently focuses mostly on the development phase of DRL systems, with little attention devoted to DRL deployment. In this paper, we propose an empirical study on Stack Overflow (SO), the most popular Q&A forum for developers, to uncover and understand the challenges practitioners faced when deploying DRL systems. Specifically, we categorized relevant SO posts by deployment platforms: server/cloud, mobile/embedded system, browser, and game engine. After filtering and manual analysis, we examined 357 SO posts about DRL deployment, investigated the current state, and identified the challenges related to deploying DRL systems. Then, we investigate the prevalence and difficulty of these challenges. Results show that the general interest in DRL deployment is growing, confirming the study's relevance and importance. Results also show that DRL deployment is more difficult than other DRL issues. Additionally, we built a taxonomy of 31 unique challenges in deploying DRL to different platforms. On all platforms, RL environment-related challenges are the most popular, and communication-related challenges are the most difficult among practitioners. We hope our study inspires future research and helps the community overcome the most common and difficult challenges practitioners face when deploying DRL systems.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL），利用深度学习（DL）在强化学习中，已经显示出可以达到人类水平自主的潜力，在Robotics、计算机视觉和计算机游戏等领域都有广泛的应用前景。这种潜力正在学术界和业界中受到广泛关注和探索。然而，社区目前主要关注DRL系统的开发阶段，却忽略了DRL系统的部署。在这篇论文中，我们通过Stack Overflow（SO），最受欢迎的开发人员问答论坛，进行了一项实证研究，探索和理解开发者在部署DRL系统时遇到的挑战。特别是，我们将相关的SO帖子分为不同的部署平台：服务器/云端、移动/嵌入式系统、浏览器和游戏引擎。经过筛选和手动分析，我们分析了357个SO帖子关于DRL部署，了解了当前状况，并识别了部署DRL系统时遇到的挑战。然后，我们研究了这些挑战的普遍性和Difficulty。结果显示，DRL部署的总兴趣正在增长，证明了这项研究的重要性和实用性。结果还表明DRL部署比其他DRL问题更加困难。此外，我们还建立了一个31个唯一挑战的分类标准，这些挑战分别发生在不同的平台上。在所有平台上，RL环境相关的挑战是最受欢迎的，而与通信相关的挑战则是开发者最难以解决的。我们希望这项研究能够激励未来的研究，并帮助社区解决最常见和最难以解决的开发者在部署DRL系统时遇到的挑战。
</details></li>
</ul>
<hr>
<h2 id="Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature"><a href="#Evolution-of-ESG-focused-DLT-Research-An-NLP-Analysis-of-the-Literature" class="headerlink" title="Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature"></a>Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12420">http://arxiv.org/abs/2308.12420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Walter Hernandez, Kamil Tylinski, Alastair Moore, Niall Roche, Nikhil Vadgama, Horst Treiblmaier, Jiangbo Shangguan, Paolo Tasca, Jiahua Xu</li>
<li>for: 本研究旨在为分布LEDGER技术（DLT）的发展提供全面的认知，特别是关注环境、可持续性和治理（ESG）方面的多个组成部分。</li>
<li>methods: 本研究采用机器学习技术进行系统性的文献评估，首先选择107个种子论文，建立了63,083个参考文献的公共网络，然后缩放到24,539份文献进行分析。其中，对46份论文进行命名实体标注，并将DLT的ESG元素归类为12个顶级类别。使用变换器基于语言模型，进行Named Entity Recognition（NER）任务的精度调整。</li>
<li>results: 本研究提供了一种机器学习驱动的系统性文献评估方法，特别是在DLT领域中强调ESG方面的研究。此外，我们还提供了一个具有54,808个命名实体的NER数据集，用于DLT和ESG相关的探索。<details>
<summary>Abstract</summary>
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our contributions are a methodology to conduct a machine learning-driven systematic literature review in the DLT field, placing a special emphasis on ESG aspects. Furthermore, we present a first-of-its-kind NER dataset, composed of 54,808 named entities, designed for DLT and ESG-related explorations.
</details>
<details>
<summary>摘要</summary>
区块链技术(DLT)在快速演化，需要全面的报告其多种组件。然而，一篇强调环境、可持续发展和治理(ESG)元素的系统性文献评议仍然缺失。为了填补这个差距，我们选择了107个种子论文，建立了63,083个引用的公共网络和提高了其中的24,539篇文献进行分析。然后，我们将46篇论文中的命名实体标注为12个顶级类别，基于已确立的技术分类法，并将DLT的ESG元素突出。通过使用基于转换器的自然语言模型，我们精细了一个预训练的语言模型，以NER任务进行特정。我们使用我们精细的语言模型来缩减 corpus 到505个关键论文，以便通过命名实体和时间图分析来对DLT的演化进行文献评议。我们的贡献是一种机器学习驱动的系统性文献评议方法，特别是在DLT领域的ESG方面。此外，我们还提供了一个具有54,808个命名实体的NER数据集，用于DLT和ESG相关的探索。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-in-parameter-estimation-of-nonlinear-systems"><a href="#Machine-learning-in-parameter-estimation-of-nonlinear-systems" class="headerlink" title="Machine learning in parameter estimation of nonlinear systems"></a>Machine learning in parameter estimation of nonlinear systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12393">http://arxiv.org/abs/2308.12393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaushal Kumar</li>
<li>for: 用于估计复杂非线性系统中参数的精度是科学和工程领域的关键。</li>
<li>methods: 我们提出了一种使用神经网络和惊异损失函数来进行参数估计的新方法。这种方法可以利用深度学习来探索非线性方程中的参数。我们使用synthetic数据和预定的函数来验证我们的方法。通过训练神经网络，它可以根据噪声时间序列数据来细化惊异损失函数，以达到准确的参数。</li>
<li>results: 我们在振荡体、Van der Pol振荡器、Lotka-Volterra系统和lorenz系统中使用这种方法进行参数估计，并得到了高精度的结果。训练神经网络可以准确地估计参数，可以从相似的latent dynamics中看到。在比较真实和估计的轨迹时，我们可以视觉地证明我们的方法的精度和可靠性。这种方法可以在实际挑战中 navigates 噪声和不确定性，表明其适应性。<details>
<summary>Abstract</summary>
Accurately estimating parameters in complex nonlinear systems is crucial across scientific and engineering fields. We present a novel approach for parameter estimation using a neural network with the Huber loss function. This method taps into deep learning's abilities to uncover parameters governing intricate behaviors in nonlinear equations. We validate our approach using synthetic data and predefined functions that model system dynamics. By training the neural network with noisy time series data, it fine-tunes the Huber loss function to converge to accurate parameters. We apply our method to damped oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz systems under multiplicative noise. The trained neural network accurately estimates parameters, evident from closely matching latent dynamics. Comparing true and estimated trajectories visually reinforces our method's precision and robustness. Our study underscores the Huber loss-guided neural network as a versatile tool for parameter estimation, effectively uncovering complex relationships in nonlinear systems. The method navigates noise and uncertainty adeptly, showcasing its adaptability to real-world challenges.
</details>
<details>
<summary>摘要</summary>
估计复杂非线性系统中参数的精度是科学和工程领域中关键的。我们提出了一种使用神经网络和哈伯损失函数的新方法来估计参数。这种方法可以利用深度学习的能力来探索非线性方程中的参数。我们使用生成的数据和预定义的函数来验证我们的方法。通过训练神经网络，它可以根据噪声时间序列数据来细化哈伯损失函数，以达到准确的参数。我们在振荡体、范德洛 oscillators、洛兹几何和多个随机变量下进行了实验，并证明了神经网络可以准确地估计参数。 Comparing true and estimated trajectories visually reinforces our method's precision and robustness. 我们的研究表明，哈伯损失函数引导的神经网络是一种 versatile 的参数估计工具，可以有效地揭示非线性系统中复杂的关系。这种方法在噪声和不确定性中 Navigation 的能力，表明它适用于实际问题。
</details></li>
</ul>
<hr>
<h2 id="FOSA-Full-Information-Maximum-Likelihood-FIML-Optimized-Self-Attention-Imputation-for-Missing-Data"><a href="#FOSA-Full-Information-Maximum-Likelihood-FIML-Optimized-Self-Attention-Imputation-for-Missing-Data" class="headerlink" title="FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data"></a>FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12388">http://arxiv.org/abs/2308.12388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oudeng/fosa">https://github.com/oudeng/fosa</a></li>
<li>paper_authors: Ou Deng, Qun Jin</li>
<li>for: 这篇论文旨在提出一种新的数据填充方法，即 FIML Optimized Self-attention (FOSA) 框架，用于有效地 Addressing missing values 在复杂的数据集中。</li>
<li>methods: 该方法首先使用 FIML 估计初始化缺失值，然后使用自注意力神经网络进行精细调整。</li>
<li>results: 对于 simulate 和实际数据集的实验表明，FOSA 方法在精度、计算效率和数据结构适应性等方面表现出优于传统 FIML 技术，并且能够在 SEM 模型被误差报告的情况下仍然提供优秀的预测结果。<details>
<summary>Abstract</summary>
In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputation outcomes. Our empirical tests reveal that FOSA consistently delivers commendable predictions, even in the face of up to 40% random missingness, highlighting its robustness and potential for wide-scale applications in data imputation.
</details>
<details>
<summary>摘要</summary>
在数据补充中，有效地处理缺失值是关键，特别是在复杂的数据集中。这篇论文探讨了FIML优化自注意（FOSA）框架，这是一种将FIML估计的优点与自注意神经网络的能力相结合的创新方法。我们的方法开始于使用FIML初步估计缺失值，然后通过自注意机制来纠正这些估计。我们在实验中发现，FOSA在真实世界数据集和模拟数据集上具有明显的优势，包括精度、计算效率和对多种数据结构的适应性。尤其是在SEM可能是错误的情况下，FOSA的自注意结构可以干涉和优化投入结果，从而提高数据补充的准确性。我们的实验结果表明，FOSA可以在40%的随机缺失下提供优秀的预测结果，这说明它的稳定性和广泛应用的潜力。
</details></li>
</ul>
<hr>
<h2 id="Open-set-Face-Recognition-with-Neural-Ensemble-Maximal-Entropy-Loss-and-Feature-Augmentation"><a href="#Open-set-Face-Recognition-with-Neural-Ensemble-Maximal-Entropy-Loss-and-Feature-Augmentation" class="headerlink" title="Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation"></a>Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12371">http://arxiv.org/abs/2308.12371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, Manuel Günther, William Robson Schwartz</li>
<li>for: 针对无法全面知ledge的面部识别场景，旨在防止未注册人脸样本被误认为已经注册的身份。</li>
<li>methods: 该方法采用一种新的组合方法，结合紧凑型神经网络和边缘基于成本函数的margin化技术，以及新的混合特征增强技术，从外部数据库或生成于表示层次上 Synthetically obtain supplementary negative samples。</li>
<li>results: 对知名的LFW和IJB-C datasets进行实验，结果表明该方法能够提高关闭和开放集face识别率。<details>
<summary>Abstract</summary>
Open-set face recognition refers to a scenario in which biometric systems have incomplete knowledge of all existing subjects. Therefore, they are expected to prevent face samples of unregistered subjects from being identified as previously enrolled identities. This watchlist context adds an arduous requirement that calls for the dismissal of irrelevant faces by focusing mainly on subjects of interest. As a response, this work introduces a novel method that associates an ensemble of compact neural networks with a margin-based cost function that explores additional samples. Supplementary negative samples can be obtained from external databases or synthetically built at the representation level in training time with a new mix-up feature augmentation approach. Deep neural networks pre-trained on large face datasets serve as the preliminary feature extraction module. We carry out experiments on well-known LFW and IJB-C datasets where results show that the approach is able to boost closed and open-set identification rates.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SafeAR-Towards-Safer-Algorithmic-Recourse-by-Risk-Aware-Policies"><a href="#SafeAR-Towards-Safer-Algorithmic-Recourse-by-Risk-Aware-Policies" class="headerlink" title="SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies"></a>SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12367">http://arxiv.org/abs/2308.12367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Wu, Shubham Sharma, Sunandita Patra, Sriram Gopalakrishnan</li>
<li>For: This paper focuses on providing recourse for individuals adversely affected by machine learning (ML) models in critical domains such as finance and healthcare. The goal is to empower people to choose a recourse based on their risk tolerance.* Methods: The paper proposes a method called Safer Algorithmic Recourse (SafeAR) that considers risk uncertainties and variability in cost when computing and evaluating recourse. The method connects algorithmic recourse literature with risk-sensitive reinforcement learning and uses measures such as Value at Risk and Conditional Value at Risk to summarize risk concisely.* Results: The paper applies SafeAR to two real-world datasets and compares policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity). The results show that SafeAR can provide more risk-sensitive and personalized recourse recommendations compared to existing methods.<details>
<summary>Abstract</summary>
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The objective is to empower people to choose a recourse based on their risk tolerance. In this work, we discuss and show how existing recourse desiderata can fail to capture the risk of higher costs. We present a method to compute recourse policies that consider variability in cost and connect algorithmic recourse literature with risk-sensitive reinforcement learning. We also adopt measures ``Value at Risk'' and ``Conditional Value at Risk'' from the financial literature to summarize risk concisely. We apply our method to two real-world datasets and compare policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity).
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose Safer Algorithmic Recourse (SafeAR), which incorporates risk considerations when computing and evaluating recourse. Our objective is to empower individuals to choose a recourse based on their risk tolerance. We show that existing recourse desiderata can fail to capture the risk of higher costs and present a method to compute recourse policies that consider variability in cost. We also connect algorithmic recourse literature with risk-sensitive reinforcement learning and adopt measures like "Value at Risk" and "Conditional Value at Risk" from the financial literature to summarize risk concisely.We apply our method to two real-world datasets and compare policies with different levels of risk-aversion using risk measures and recourse desiderata (sparsity and proximity). Our results show that SafeAR can provide more risk-aware recourse recommendations, which can help individuals make more informed decisions and reduce the risk of higher costs.
</details></li>
</ul>
<hr>
<h2 id="Renormalizing-Diffusion-Models"><a href="#Renormalizing-Diffusion-Models" class="headerlink" title="Renormalizing Diffusion Models"></a>Renormalizing Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12355">http://arxiv.org/abs/2308.12355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Cotler, Semon Rezchikov</li>
<li>for: The paper is written for studying field theories using machine learning, specifically using diffusion models to learn inverse renormalization group flows.</li>
<li>methods: The paper uses diffusion models, which are a class of machine learning models that can generate samples from complex distributions, to learn the inverse process of a diffusion process that adds noise to the data until the distribution is pure noise. The paper also combines these observations with nonperturbative renormalization group schemes to build ML-based models for studying field theories.</li>
<li>results: The paper details how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory, and provides explicit prescriptions for comparing results derived from models associated with different renormalization group schemes of interest. The paper also applies some of these methods to numerically find RG flows of interacting statistical field theories.<details>
<summary>Abstract</summary>
We explain how to use diffusion models to learn inverse renormalization group flows of statistical and quantum field theories. Diffusion models are a class of machine learning models which have been used to generate samples from complex distributions, such as the distribution of natural images, by learning the inverse process to a diffusion process which adds noise to the data until the distribution of the data is pure noise. Nonperturbative renormalization group schemes can naturally be written as diffusion processes in the space of fields. We combine these observations in a concrete framework for building ML-based models for studying field theories, in which the models learn the inverse process to an explicitly-specified renormalization group scheme. We detail how these models define a class of adaptive bridge (or parallel tempering) samplers for lattice field theory. Because renormalization group schemes have a physical meaning, we provide explicit prescriptions for how to compare results derived from models associated to several different renormalization group schemes of interest. We also explain how to use diffusion models in a variational method to find ground states of quantum systems. We apply some of our methods to numerically find RG flows of interacting statistical field theories. From the perspective of machine learning, our work provides an interpretation of multiscale diffusion models, and gives physically-inspired suggestions for diffusion models which should have novel properties.
</details>
<details>
<summary>摘要</summary>
我们解释如何使用扩散模型来学习逆减杂化群流程的统计和量子场论。扩散模型是一种机器学习模型，可以生成复杂分布的样本，如自然图像的分布，通过学习逆 процесс，将资料中的噪声添加到样本中，直到样本的分布是纯粹的噪声。非perturbative renormalization group scheme可以自然地写作扩散程序在场论空间中。我们将这些观察结合在一个实际框架中，以建立基于机器学习的场论模型，这些模型学习逆 процесс，以跟踪一个明确地Specified renormalization group scheme。我们详细说明这些模型是一种适应桥（或平行摄氏）抽样器，用于确定点网场论。因为renormalization group scheme有物理意义，我们提供了明确的比较方法，以评估从各个renormalization group scheme中获得的结果之间的关联。我们还说明如何使用扩散模型在统计方法中进行测试，以找到量子系统的基点。从机器学习的角度来看，我们的工作提供了扩散模型的解释，并将提供物理灵感的扩散模型建议，这些建议应有新的性质。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generative-Model-based-Unfolding-with-Schrodinger-Bridges"><a href="#Improving-Generative-Model-based-Unfolding-with-Schrodinger-Bridges" class="headerlink" title="Improving Generative Model-based Unfolding with Schrödinger Bridges"></a>Improving Generative Model-based Unfolding with Schrödinger Bridges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12351">http://arxiv.org/abs/2308.12351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sascha Diefenbacher, Guan-Horng Liu, Vinicius Mikuni, Benjamin Nachman, Weili Nie</li>
<li>for: 这篇论文旨在探讨基于机器学习的对折推算法，以实现不归一化和高维对折推算法的应用。</li>
<li>methods: 这篇论文提出了基于描述性模型和生成模型的两种主要方法，其中描述性模型可以学习一个小的修正，而生成模型可以更好地扩展到数据少的区域。</li>
<li>results: 作者提出了一种基于施罗丁格 bridges 和扩散模型的 unfolding 方法，称为 SBUnfold，该方法可以结合描述性和生成模型的优点，并且可以在一个 synthetic Z+jets 数据集上实现出色的性能。<details>
<summary>Abstract</summary>
Machine learning-based unfolding has enabled unbinned and high-dimensional differential cross section measurements. Two main approaches have emerged in this research area: one based on discriminative models and one based on generative models. The main advantage of discriminative models is that they learn a small correction to a starting simulation while generative models scale better to regions of phase space with little data. We propose to use Schroedinger Bridges and diffusion models to create SBUnfold, an unfolding approach that combines the strengths of both discriminative and generative models. The key feature of SBUnfold is that its generative model maps one set of events into another without having to go through a known probability density as is the case for normalizing flows and standard diffusion models. We show that SBUnfold achieves excellent performance compared to state of the art methods on a synthetic Z+jets dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification"><a href="#D4-Improving-LLM-Pretraining-via-Document-De-Duplication-and-Diversification" class="headerlink" title="D4: Improving LLM Pretraining via Document De-Duplication and Diversification"></a>D4: Improving LLM Pretraining via Document De-Duplication and Diversification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12284">http://arxiv.org/abs/2308.12284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kushal Tirumala, Daniel Simig, Armen Aghajanyan, Ari S. Morcos</li>
<li>for: 这篇论文是为了研究大语言模型（LLM）的预训练和下游性能而写的。</li>
<li>methods: 这篇论文使用了预训练模型 embedding 进行精心的数据选择，以提高预训练和下游性能。</li>
<li>results: 这篇论文的结果表明，通过精心的数据选择可以提高 LLM 的预训练和下游性能，并且可以获得20%的效率提升和2%的下游准确率提升。此外， repeating 数据也可以提高下游性能，而不同的数据重复可以超过基eline训练。<details>
<summary>Abstract</summary>
Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.
</details>
<details>
<summary>摘要</summary>
在过去几年，一些大型语言模型（LLM）的训练已经减少了大量计算和数据，通常通过一次学习在大规模网络corpus中随机选择的单词进行训练。虽然训练在无限大规模网络上 führt zu consistent performance improvements，但这些改进的大小随着规模减少，并且有少量的工作研究了预训练和下游性能以外的数据选择的影响。在这里，我们表明了针对预训练模型embeddings进行精心的数据选择（在除掉了重复的数据之后）可以提高训练效率（20%的效率提高）和16种NLP任务的平均下游准确率（最高达2%），并且我们表明了重复数据智能的方法可以连续超过基eline训练（而重复随机数据的训练则比基eline训练差）。我们的结果表明，智能的数据选择可以在LLM预训练中提高性能，质疑了训练一次性地使用最多数据的常见做法，并且表明了可以继续改进我们的模型，超过随机抽样网络数据的限制。
</details></li>
</ul>
<hr>
<h2 id="Extended-Linear-Regression-A-Kalman-Filter-Approach-for-Minimizing-Loss-via-Area-Under-the-Curve"><a href="#Extended-Linear-Regression-A-Kalman-Filter-Approach-for-Minimizing-Loss-via-Area-Under-the-Curve" class="headerlink" title="Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve"></a>Extended Linear Regression: A Kalman Filter Approach for Minimizing Loss via Area Under the Curve</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12280">http://arxiv.org/abs/2308.12280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokulprasath R</li>
<li>for: 提高线性回归模型的精度，使其能够更好地预测数据。</li>
<li>methods: 将 kalman 筛引入到线性回归模型中，分析曲线面积来减少损失。</li>
<li>results: 提出一种基于杂志梯逐步进程的优化线性回归方程，使用梯逐进程来更新参数，并使用 kalman 筛来预测下一个融合参数。该方法可以避免不断更新参数的困境，并且可以处理部分数据集，不同于需要整个数据集来进行预测。但是，计算复杂性应该被考虑。<details>
<summary>Abstract</summary>
This research enhances linear regression models by integrating a Kalman filter and analysing curve areas to minimize loss. The goal is to develop an optimal linear regression equation using stochastic gradient descent (SGD) for weight updating. Our approach involves a stepwise process, starting with user-defined parameters. The linear regression model is trained using SGD, tracking weights and loss separately and zipping them finally. A Kalman filter is then trained based on weight and loss arrays to predict the next consolidated weights. Predictions result from multiplying input averages with weights, evaluated for loss to form a weight-versus-loss curve. The curve's equation is derived using the two-point formula, and area under the curve is calculated via integration. The linear regression equation with minimum area becomes the optimal curve for prediction. Benefits include avoiding constant weight updates via gradient descent and working with partial datasets, unlike methods needing the entire set. However, computational complexity should be considered. The Kalman filter's accuracy might diminish beyond a certain prediction range.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种基于 Kalman 筛和直线回归模型的优化方法，以降低损失。目标是通过随机梯度下降（SGD）来更新参数。我们的方法包括以下步骤：首先定义用户参数，然后使用 SGD 训练直线回归模型，并跟踪参数和损失的变化。接着，我们使用 Kalman 筛来预测下一个融合参数。预测结果是通过输入均值与参数进行乘法，并评估损失来形成一个参数对损失曲线。这个曲线的方程可以通过两点方程得出，而曲线下面积可以通过积分来计算。最佳曲线的方程是Minimum Area的曲线，它可以用于预测。这种方法的优点包括避免了不断地更新参数 via 梯度下降，同时可以处理部分数据集，而不是需要整个数据集。然而，计算复杂度应该被考虑。Kalman 筛的准确性可能在预测范围内逐渐减退。
</details></li>
</ul>
<hr>
<h2 id="On-Manifold-Projected-Gradient-Descent"><a href="#On-Manifold-Projected-Gradient-Descent" class="headerlink" title="On-Manifold Projected Gradient Descent"></a>On-Manifold Projected Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12279">http://arxiv.org/abs/2308.12279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JonasGrabbe/GradientDecentOnManifolds">https://github.com/JonasGrabbe/GradientDecentOnManifolds</a></li>
<li>paper_authors: Aaron Mahler, Tyrus Berry, Tom Stephens, Harbir Antil, Michael Merritt, Jeanie Schreiber, Ioannis Kevrekidis</li>
<li>for: This paper is written for researchers and practitioners in the field of machine learning, particularly those interested in adversarial training and the geometry of high-dimensional data.</li>
<li>methods: The paper uses conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and the Nystr&quot;{o}m projection to project novel points onto class manifolds in this setting. The paper also employs the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold.</li>
<li>results: The paper provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, and demonstrates the effectiveness of the approach by generating novel, on-manifold data samples and implementing a projected gradient descent algorithm for on-manifold adversarial training. The paper also shows that the proposed method can obtain adversarial examples that reside on a class manifold, yet fool a classifier, and provides a human-understandable explanation of the manipulations within the data that lead to misclassifications.Here is the answer in Simplified Chinese:</li>
<li>for: 这篇论文是为了各种机器学习领域的研究者和实践者，特别是关注防御训练和高维数据的geometry。</li>
<li>methods: 这篇论文使用均匀扩散映射(CIDM)来 aproximate类 manifold在扩散坐标中，并使用尼斯特罗姆投影来在这种设定下 проек novel点 onto class manifold。论文还使用spectral exterior calculus (SEC)来 determinegeometric量 such as tangent vector of the manifold。</li>
<li>results: 这篇论文提供了可计算、直接、数学上正确的高维数据类 manifold的difficult geometryapproximation，并证明了该方法的有效性。论文还表明了该方法可以生成onto-manifold数据amples，并实现了 onto-manifold防御训练中的梯度下降算法。论文还显示了该方法可以获得onto-manifold上的骗例，却能让分类器进行误分类，并提供了人类可理解的数据中的扭曲 manipulate的解释。<details>
<summary>Abstract</summary>
This work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds. The tools are applied to the setting of neural network image classifiers, where we generate novel, on-manifold data samples, and implement a projected gradient descent algorithm for on-manifold adversarial training. The susceptibility of neural networks (NNs) to adversarial attack highlights the brittle nature of NN decision boundaries in input space. Introducing adversarial examples during training has been shown to reduce the susceptibility of NNs to adversarial attack; however, it has also been shown to reduce the accuracy of the classifier if the examples are not valid examples for that class. Realistic "on-manifold" examples have been previously generated from class manifolds in the latent of an autoencoder. Our work explores these phenomena in a geometric and computational setting that is much closer to the raw, high-dimensional input space than can be provided by VAE or other black box dimensionality reductions. We employ conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and develop the Nystr\"{o}m projection to project novel points onto class manifolds in this setting. On top of the manifold approximation, we leverage the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold. We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier. These misclassifications then become explainable in terms of human-understandable manipulations within the data, by expressing the on-manifold adversary in the semantic basis on the manifold.
</details>
<details>
<summary>摘要</summary>
Traditional adversarial training methods introduce adversarial examples into the training process to increase the robustness of neural networks (NNs) to attacks. However, these methods can also reduce the accuracy of the classifier if the examples are not valid examples for that class. Our work explores the use of realistic "on-manifold" examples generated from class manifolds in the latent space of an autoencoder to improve the robustness of NNs. We employ conformally invariant diffusion maps (CIDM) to approximate class manifolds in diffusion coordinates, and develop the Nystr\"{o}m projection to project novel points onto class manifolds in this setting. On top of the manifold approximation, we leverage the spectral exterior calculus (SEC) to determine geometric quantities such as tangent vectors of the manifold. We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier. These misclassifications can be explained in terms of human-understandable manipulations within the data, by expressing the on-manifold adversary in the semantic basis on the manifold. Our approach is more effective than traditional adversarial training methods because it uses realistic, on-manifold examples that are closer to the raw, high-dimensional input space than can be provided by VAE or other black box dimensionality reductions. This makes our approach more robust and more accurate than traditional methods. In summary, our work provides a computable, direct, and mathematically rigorous approximation to the differential geometry of class manifolds for high-dimensional data, along with nonlinear projections from input space onto these class manifolds. We use these tools to obtain adversarial examples that reside on a class manifold, yet fool a classifier, and to explain the misclassifications in terms of human-understandable manipulations within the data. Our approach is more effective and more robust than traditional adversarial training methods.
</details></li>
</ul>
<hr>
<h2 id="LCANets-Robust-Audio-Classification-using-Multi-layer-Neural-Networks-with-Lateral-Competition"><a href="#LCANets-Robust-Audio-Classification-using-Multi-layer-Neural-Networks-with-Lateral-Competition" class="headerlink" title="LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition"></a>LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12882">http://arxiv.org/abs/2308.12882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti</li>
<li>for: 实现音频分类任务中的敏捷识别和防御性能。</li>
<li>methods: 使用类神经网络（CNNs），并在第一层使用本地竞争算法（LCA）进行简短编码，以减少依赖标签数据。</li>
<li>results: 比较标准CNNs和LCANets，LCANets++ 具有更高的防御性和敏捷性，可以对噪声和攻击（如误导和快速梯度签名攻击）进行更好的识别和抵抗。<details>
<summary>Abstract</summary>
Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion and fast gradient sign (FGSM) attacks.
</details>
<details>
<summary>摘要</summary>
Audio 分类目标是识别音频信号，包括语音命令或声音事件。然而，现有的音频分类器受到干扰和敌意攻击的影响。此外，实际世界中的音频分类任务经常面临有限的标注数据的问题。为了bridging这些差距，过去的工作开发了基于神经元的启发式卷积神经网络（CNN），使用本地竞争算法（LCA）在第一层进行稀疏编码，称为LCANets。LCANets在超过supervised和unsupervised学习方法下学习，减少了依赖于标注样本。驱动了听觉皮层也是稀疏的事实，我们扩展LCANets到音频识别任务，并引入LCANets++，它是多层使用LCA进行稀疏编码的CNN。我们示示了LCANets++对干扰、背景噪音、黑盒和白盒攻击等方面更加强大，比如逃避和快速梯度签名（FGSM）攻击。
</details></li>
</ul>
<hr>
<h2 id="Language-Reward-Modulation-for-Pretraining-Reinforcement-Learning"><a href="#Language-Reward-Modulation-for-Pretraining-Reinforcement-Learning" class="headerlink" title="Language Reward Modulation for Pretraining Reinforcement Learning"></a>Language Reward Modulation for Pretraining Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12270">http://arxiv.org/abs/2308.12270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ademiadeniji/lamp">https://github.com/ademiadeniji/lamp</a></li>
<li>paper_authors: Ademi Adeniji, Amber Xie, Carmelo Sferrazza, Younggyo Seo, Stephen James, Pieter Abbeel</li>
<li>for: 本研究问题是否现代learned reward functions（LRFs）适用于直接替换任务奖励？相反，我们提议利用LRFs作为RL的预训练信号。</li>
<li>methods: 我们提出了一种名为LAMP的方法，它利用预先冻结的视觉语言模型（VLMs）作为RL的预训练资源，通过计算语言指令和机器人观察图像之间的对比性Alignment来生成噪音的探索奖励。</li>
<li>results: 我们的LAMP方法可以在RLBench上具有较高的效率，通过与标准的探索奖励结合LRFs来学习语言conditioned的预训练策略。<details>
<summary>Abstract</summary>
Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\textbf{LA}$nguage Reward $\textbf{M}$odulated $\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with reinforcement learning to acquire a language-conditioned, pretrained policy. Our VLM pretraining approach, which is a departure from previous attempts to use LRFs, can warmstart sample-efficient learning on robot manipulation tasks in RLBench.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FECoM-A-Step-towards-Fine-Grained-Energy-Measurement-for-Deep-Learning"><a href="#FECoM-A-Step-towards-Fine-Grained-Energy-Measurement-for-Deep-Learning" class="headerlink" title="FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning"></a>FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12264">http://arxiv.org/abs/2308.12264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabhsingh Rajput, Tim Widmayer, Ziyuan Shang, Maria Kechagia, Federica Sarro, Tushar Sharma</li>
<li>for:  This paper aims to provide a fine-grained energy consumption measurement framework for deep learning (DL) systems, which can help promote green development and energy awareness in the field.</li>
<li>methods: The paper introduces FECoM (Fine-grained Energy Consumption Meter), a framework that uses static instrumentation to measure energy consumption at a fine granularity (e.g., at method level) for DL systems. FECoM addresses various challenges, such as computational load and temperature stability, to provide accurate energy consumption measurements.</li>
<li>results: The paper assesses FECoM’s capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, TensorFlow. The results show that FECoM can effectively measure the energy consumption of TensorFlow APIs and provide insights into the impact of parameter size and execution time on energy consumption.<details>
<summary>Abstract</summary>
With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, namely TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles. Furthermore, we elaborate on the considerations, issues, and challenges that one needs to consider while designing and implementing a fine-grained energy consumption measurement tool. We hope this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems.
</details>
<details>
<summary>摘要</summary>
In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers with a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at a fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability.We assess FECoM's capability to measure fine-grained energy consumption for one of the most popular open-source DL frameworks, namely TensorFlow. Using FECoM, we also investigate the impact of parameter size and execution time on energy consumption, enriching our understanding of TensorFlow APIs' energy profiles. Furthermore, we elaborate on the considerations, issues, and challenges that one needs to consider while designing and implementing a fine-grained energy consumption measurement tool.We hope this work will facilitate further advances in DL energy measurement and the development of energy-aware practices for DL systems.
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Negative-User-Feedback-and-Measuring-Responsiveness-for-Sequential-Recommenders"><a href="#Learning-from-Negative-User-Feedback-and-Measuring-Responsiveness-for-Sequential-Recommenders" class="headerlink" title="Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders"></a>Learning from Negative User Feedback and Measuring Responsiveness for Sequential Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12256">http://arxiv.org/abs/2308.12256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueqi Wang, Yoni Halpern, Shuo Chang, Jingchen Feng, Elaine Ya Le, Longfei Li, Xujian Liang, Min-Cheng Huang, Shane Li, Alex Beutel, Yaping Zhang, Shuchao Bi</li>
<li>for:  This paper aims to improve the performance of sequential recommender systems by incorporating negative user feedback into the training objective.</li>
<li>methods: The authors use a “not-to-recommend” loss function to optimize for the log-likelihood of not recommending items with negative feedback in the retrieval stage.</li>
<li>results: The authors demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system, and also develop a counterfactual simulation framework to compare recommender responses between different user actions, showing improved responsiveness from the modeling change.<details>
<summary>Abstract</summary>
Sequential recommenders have been widely used in industry due to their strength in modeling user preferences. While these models excel at learning a user's positive interests, less attention has been paid to learning from negative user feedback. Negative user feedback is an important lever of user control, and comes with an expectation that recommenders should respond quickly and reduce similar recommendations to the user. However, negative feedback signals are often ignored in the training objective of sequential retrieval models, which primarily aim at predicting positive user interactions. In this work, we incorporate explicit and implicit negative user feedback into the training objective of sequential recommenders in the retrieval stage using a "not-to-recommend" loss function that optimizes for the log-likelihood of not recommending items with negative feedback. We demonstrate the effectiveness of this approach using live experiments on a large-scale industrial recommender system. Furthermore, we address a challenge in measuring recommender responsiveness to negative feedback by developing a counterfactual simulation framework to compare recommender responses between different user actions, showing improved responsiveness from the modeling change.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:Sequential recommender systems 已经广泛应用于实际场景，这是因为它们可以很好地模型用户喜好。然而，这些模型往往忽略了用户的负反馈，这是用户控制和期望的重要依据。在这种情况下，我们将负反馈 incorporated 到sequential retrieval模型的训练目标中，使用“不推荐”损失函数，以优化不推荐item的log-likelihood。我们通过实际实验表明了这种方法的效果，并且通过对不同用户行为进行对照分析，显示了改进的响应性。
</details></li>
</ul>
<hr>
<h2 id="How-Safe-Am-I-Given-What-I-See-Calibrated-Prediction-of-Safety-Chances-for-Image-Controlled-Autonomy"><a href="#How-Safe-Am-I-Given-What-I-See-Calibrated-Prediction-of-Safety-Chances-for-Image-Controlled-Autonomy" class="headerlink" title="How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy"></a>How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12252">http://arxiv.org/abs/2308.12252</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maozj6/hsai-predictor">https://github.com/maozj6/hsai-predictor</a></li>
<li>paper_authors: Zhenjiang Mao, Carson Sobolewski, Ivan Ruchkin</li>
<li>for: 该论文主要用于提出一种可配置的家族式学习管道，以提高自动化系统的安全性验证。</li>
<li>methods: 该论文使用生成世界模型，并解决了在预测induced分布转移下学习安全知识的问题，以及在缺少安全标签的情况下实现安全预测。</li>
<li>results: 该论文通过对两个图像控制系统的评估，证明了该方法的可靠性和有效性。<details>
<summary>Abstract</summary>
End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a cartpole.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models"><a href="#How-to-Protect-Copyright-Data-in-Optimization-of-Large-Language-Models" class="headerlink" title="How to Protect Copyright Data in Optimization of Large Language Models?"></a>How to Protect Copyright Data in Optimization of Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12247">http://arxiv.org/abs/2308.12247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy Chu, Zhao Song, Chiwun Yang</li>
<li>for: 研究大型自然语言模型（LLM）和生成AI的法律问题</li>
<li>methods: 使用softmax函数进行回归问题来训练和优化大型语言模型</li>
<li>results: 提出一种有效地进行回归问题的方法，以避免生成版权数据<details>
<summary>Abstract</summary>
Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.   In this paper, we show that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和生成AI在计算机研究和应用中发挥了转变性的作用。但是，有争议是这些模型输出的数据是否受版权保护，这可能会发生如果模型训练数据是受版权保护的。LLMs是基于转换神经网络架构，该架构则依赖于一种数学计算 called Attention，它使用软饶函数。在这篇论文中，我们表明了大型语言模型训练和优化可以被看作为软饶 regression问题。然后，我们建立了一种有效地进行软饶 regression的方法，以避免生成版权数据。这种方法可以在训练大型语言模型时避免生成版权数据。
</details></li>
</ul>
<hr>
<h2 id="Multi-Objective-Optimization-for-Sparse-Deep-Neural-Network-Training"><a href="#Multi-Objective-Optimization-for-Sparse-Deep-Neural-Network-Training" class="headerlink" title="Multi-Objective Optimization for Sparse Deep Neural Network Training"></a>Multi-Objective Optimization for Sparse Deep Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12243">http://arxiv.org/abs/2308.12243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salomonhotegni/mdmtn">https://github.com/salomonhotegni/mdmtn</a></li>
<li>paper_authors: S. S. Hotegni, S. Peitz, M. Berkemeier</li>
<li>for: 这篇论文旨在提出一种多目标优化算法，用于训练深度学习网络（DNNs），以满足多个任务的需求。</li>
<li>methods: 这篇论文使用了一种 modificated Weighted Chebyshev scalarization 技术，将多个任务转换为一个单一目标函数，并使用 Augmented Lagrangian 方法来解决这个问题。</li>
<li>results: 这篇论文的实验结果显示，这种多目标优化算法可以在训练 Deep Multi-Task 网络时，自动将网络簇化为更简单的网络，而不会影响网络的性能。<details>
<summary>Abstract</summary>
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economical and also ecological) sustainability issue of DNN models, with a particular focus on Deep Multi-Task models, which are typically designed with a very large number of weights to perform equally well on multiple tasks. Through experiments conducted on two Machine Learning datasets, we demonstrate the possibility of adaptively sparsifying the model during training without significantly impacting its performance, if we are willing to apply task-specific adaptations to the network weights. Code is available at https://github.com/salomonhotegni/MDMTN.
</details>
<details>
<summary>摘要</summary>
不同的冲突优化标准在深度学习场景中自然出现。这些标准可以处理不同的主任务（例如在多任务学习设置中），也可以处理主任务和次任务，如损失最小化和稀疏化。通常的方法是简单地Weighting这些标准，这只有在几何设定下才能正确工作。在这篇论文中，我们提出了一种多目标优化算法，使用修改后的Weighted ChebyshevScalarization来训练深度神经网络（DNNs）与多个任务之间的关系。通过使用这种Scalarization技术，算法可以找到原始问题的所有优秀解决方案，同时减少问题的复杂度到一个序列单个目标问题。这些简化后的问题可以使用Augmented Lagrangian方法解决，这使得可以使用流行的优化技术，如Adam和Stochastic Gradient Descent，同时有效地处理约束。我们的工作旨在解决深度神经网络模型的（经济和生态）可持续性问题，尤其是深度多任务模型，这些模型通常具有大量的参数，以便在多个任务上 equally well 表现。通过在两个机器学习数据集上进行实验，我们表明了在训练过程中适应性减少模型的可能性，只要愿意对任务特定的网络参数进行修改。代码可以在https://github.com/salomonhotegni/MDMTN 上获取。
</details></li>
</ul>
<hr>
<h2 id="Critical-Learning-Periods-Emerge-Even-in-Deep-Linear-Networks"><a href="#Critical-Learning-Periods-Emerge-Even-in-Deep-Linear-Networks" class="headerlink" title="Critical Learning Periods Emerge Even in Deep Linear Networks"></a>Critical Learning Periods Emerge Even in Deep Linear Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12221">http://arxiv.org/abs/2308.12221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kleinman, Alessandro Achille, Stefano Soatto</li>
<li>For: 这个论文研究了深度学习网络中的批处理期，以及这些期间对学习行为和学习表征的影响。* Methods: 该论文使用了深度线性网络模型，并通过分析和实验来研究批处理期的特点和影响因素。* Results: 研究发现，批处理期取决于网络的深度和数据分布结构，并且学习特征的形成与数据源之间的竞争有关。此外，研究还发现在多任务学习中，预训练过程的持续时间可以影响新任务的转移性能。<details>
<summary>Abstract</summary>
Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show analytically and in simulations that the learning of features is tied to competition between sources. Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage. To the best of our knowledge, our work provides the first analytically tractable model that sheds light into why critical learning periods emerge in biological and artificial networks.
</details>
<details>
<summary>摘要</summary>
“重要的学习时期是在发育初期的一段时间，在这段时间内，暂时的感知缺陷可能会导致永久性的行为和学习的表征。尽管生物和人工网络之间存在巨大的不同，但critical learning periods在两种系统中都有证据存在。这表明critical periods可能是学习的基本特征，而不是生物学的巧合。然而， precisely why critical periods emerge in deep networks is still an open question, and in particular, it is unclear whether the critical periods observed in both systems depend on specific architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and the structure of the data distribution. We also show analytically and in simulations that the learning of features is tied to competition between sources. Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage. To the best of our knowledge, our work provides the first analytically tractable model that sheds light on why critical learning periods emerge in biological and artificial networks.”Please note that the translation is in Simplified Chinese, and the sentence structure and wording may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning"><a href="#Diffusion-Language-Models-Can-Perform-Many-Tasks-with-Scaling-and-Instruction-Finetuning" class="headerlink" title="Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning"></a>Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12219">http://arxiv.org/abs/2308.12219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yegcjs/diffusionllm">https://github.com/yegcjs/diffusionllm</a></li>
<li>paper_authors: Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Quanquan Gu</li>
<li>for: 这篇论文目的是探讨扩展扩散概率模型的可能性，以解决通用语言任务。</li>
<li>methods: 该论文使用扩散语言模型，并通过数据、大小和任务的扩展来提高其语言学习能力。</li>
<li>results: 实验显示，扩展扩散语言模型可以在下游语言任务中提高表现，并且发现了在线上学习和受 instruktion 的适应能力。<details>
<summary>Abstract</summary>
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning
</details>
<details>
<summary>摘要</summary>
Recent years have seen a surge in the development of generative AI, fueled by the power of diffusion probabilistic models and the scalability of large language models. However, it remains to be seen whether diffusion language models can perform general language tasks as well as their autoregressive counterparts. This paper shows that scaling diffusion models in terms of data, size, and tasks can make them strong language learners. We build competent diffusion language models by first pretraining them on massive data using masked language modeling, and then adapting them to specific tasks through a process called diffusive adaptation. This involves task-specific finetuning and instruction finetuning, which enables the models to learn a wide range of language tasks with few examples. Our experiments show that scaling diffusion language models consistently improves performance across downstream language tasks, and we also discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that allow the models to tackle unseen tasks by following natural language instructions. This has promising applications in advanced and challenging abilities such as reasoning.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.LG_2023_08_24/" data-id="clmjn91mp007h0j885kww5qko" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/cs.SD_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/cs.SD_2023_08_24/">cs.SD - 2023-08-24 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sparks-of-Large-Audio-Models-A-Survey-and-Outlook"><a href="#Sparks-of-Large-Audio-Models-A-Survey-and-Outlook" class="headerlink" title="Sparks of Large Audio Models: A Survey and Outlook"></a>Sparks of Large Audio Models: A Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12792">http://arxiv.org/abs/2308.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto Cuayáhuitl, Björn W. Schuller</li>
<li>for: 这份论文提供了最近的进展和挑战在应用大型语言模型于音频处理领域。</li>
<li>methods: 这份论文探讨了现代大型语言模型的应用方法，包括基于传播器架构的模型，以及它们在不同的音频任务上的表现。</li>
<li>results: 这份论文总结了现有的大型语言模型在音频处理领域的表现，包括自动语音识别、文本读取和音乐生成等多个任务。此外，这些模型还在不同的语言上展示了多语言支持和可转换性。<details>
<summary>Abstract</summary>
This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.
</details>
<details>
<summary>摘要</summary>
这份调查论文提供了大语音模型在听音信号处理领域的最新进展和挑战。听音信号处理领域的多样化信号表示和广泛的来源，从人声到乐器和环境声音，与传统自然语言处理场景不同，但是大语音模型，如基于转换器的架构，在这个领域表现出了突出的能力。通过吞吐大量数据，这些模型在各种听音任务中表现出了多样化的能力，从自动语音识别和文本转语音到音乐生成等。尤其是最近，这些基础Audio模型，如SeamlessM4T，已经开始展示了多语言支持的能力，无需单独的任务特定系统。这篇论文对现状的方法ologies进行了深入分析，包括基础大语音模型的性能标准和其在实际场景中的应用性。我们还提出了当前的限制和未来研究方向，以便在下一代听音处理系统中促进更多的创新。此外，为了应对这一领域的快速发展，我们将在https://github.com/EmulationAI/awesome-large-audio-models中不断更新相关的文章和其开源实现。
</details></li>
</ul>
<hr>
<h2 id="WavMark-Watermarking-for-Audio-Generation"><a href="#WavMark-Watermarking-for-Audio-Generation" class="headerlink" title="WavMark: Watermarking for Audio Generation"></a>WavMark: Watermarking for Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12770">http://arxiv.org/abs/2308.12770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei</li>
<li>for: 这篇论文旨在探讨一种新的声音杀除技术，可以在几秒钟的音频记录上模仿说话者的声音，同时保持高度的真实性。</li>
<li>methods: 该技术使用了一种新的音频杀除框架，可以在1秒钟的音频记录上编码32位的水印，并且这个水印是不可见的，具有强大的鲁棒性，可以抵御多种攻击。</li>
<li>results: 该技术可以具有高度的鲁棒性和扩展性，可以在10-20秒的音频上实现0.48%的比特错误率，与现有技术相比，可以提高2800%的比特错误率。<details>
<summary>Abstract</summary>
Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. Utilizing 10 to 20-second audio as the host, our approach demonstrates an average Bit Error Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over 2800\% in BER compared to the state-of-the-art watermarking tool. See https://aka.ms/wavmark for demos of our work.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)最近的零批训练语音合成技术突破有了可以使用只需几秒钟的录音来实现高度真实的语音imitating的能力。这些技术具有可观的可能性，但同时也 introduce了一些风险，如语音fraud和speaker impersonation。与传统方法所取得的只通过静止方法检测合成数据的approach不同， watermarking 提供了一种积极和强大的防御机制。这篇论文介绍了一种创新的音频 watermarking 框架，可以在1秒钟的音频片断中编码 Up to 32 bits的水印。这个水印是人类感知不到的，并且在不同的攻击下显示出强大的抗性。它可以作为合成voice的标识符，并且有可能在更广泛的音频版权保护领域得到应用。此外，这个框架具有高度的灵活性，可以将多个水印段组合以实现更高的强度和扩展性。使用10-20秒的音频作为主体，我们的方法在10种常见的攻击下demonstrate了 average Bit Error Rate（BER）为0.48%，相比之下，现有的 watermarking 工具的BER下降了2800%以上。请参考https://aka.ms/wavmark 查看我们的工作示例。
</details></li>
</ul>
<hr>
<h2 id="Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics"><a href="#Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics" class="headerlink" title="Whombat: An open-source annotation tool for machine learning development in bioacoustics"></a>Whombat: An open-source annotation tool for machine learning development in bioacoustics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12688">http://arxiv.org/abs/2308.12688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Martinez Balvanera, Oisin Mac Aodha, Matthew J. Weldy, Holly Pringle, Ella Browning, Kate E. Jones</li>
<li>for: 这个论文旨在提高生物声学记录的自动分析，使用机器学习方法可以大幅提高生物多样性监测的规模。</li>
<li>methods: 这篇论文使用了一种名为Whombat的用户友好的浏览器基本的界面，用于管理音频记录和注释项目，并提供了许多视觉化、探索和注释工具。</li>
<li>results: 作者通过使用Whombat工具，成功地解决了音频记录注释的一些挑战，如管理大量的记录和相关metadata，开发 flexible的注释工具，并解决专家注释员的缺乏问题。<details>
<summary>Abstract</summary></li>
</ul>
<ol>
<li>Automated analysis of bioacoustic recordings using machine learning (ML) methods has the potential to greatly scale biodiversity monitoring efforts. The use of ML for high-stakes applications, such as conservation research, demands a data-centric approach with a focus on utilizing carefully annotated and curated evaluation and training data that is relevant and representative. Creating annotated datasets of sound recordings presents a number of challenges, such as managing large collections of recordings with associated metadata, developing flexible annotation tools that can accommodate the diverse range of vocalization profiles of different organisms, and addressing the scarcity of expert annotators.   2. We present Whombat a user-friendly, browser-based interface for managing audio recordings and annotation projects, with several visualization, exploration, and annotation tools. It enables users to quickly annotate, review, and share annotations, as well as visualize and evaluate a set of machine learning predictions on a dataset. The tool facilitates an iterative workflow where user annotations and machine learning predictions feedback to enhance model performance and annotation quality.   3. We demonstrate the flexibility of Whombat by showcasing two distinct use cases: an project aimed at enhancing automated UK bat call identification at the Bat Conservation Trust (BCT), and a collaborative effort among the USDA Forest Service and Oregon State University researchers exploring bioacoustic applications and extending automated avian classification models in the Pacific Northwest, USA.   4. Whombat is a flexible tool that can effectively address the challenges of annotation for bioacoustic research. It can be used for individual and collaborative work, hosted on a shared server or accessed remotely, or run on a personal computer without the need for coding skills.</details>
<details>
<summary>摘要</summary></li>
<li>机器学习（ML）技术可以帮助自动分析生物声音记录，大大提高生物多样性监测的规模。在保护研究等高度重要应用领域使用ML时，需要一种数据驱动的方法，强调使用仔细标注和整理的评估和培训数据，该数据是有 relevance 和 representativeness 的。创建声音记录的标注数据集存在许多挑战，如管理大量记录和相关 metadata，开发可扩展的标注工具，以便 Accommodate 不同生物体种的声音profile。2. 我们提供了一个用户友好的浏览器基本的界面，用于管理声音记录和标注项目，包括视觉化、探索和标注工具。它允许用户快速标注、复审和分享标注，以及可视化和评估一个数据集上的机器学习预测结果。工具支持迭代式的工作流程，在用户标注和机器学习预测之间进行反馈，以提高标注质量和模型性能。3. 我们在两个不同的应用场景中展示了 Whombat 的灵活性：一个是英国蝙蝠保护协会（BCT）的自动蝙蝠叫出识别项目，另一个是美国农业部和奥REGON州立大学合作的生物声音应用研究，探索生物声音应用和自动鸟类分类模型在太平洋北西部的扩展。4. Whombat 是一种灵活的工具，可以有效地解决生物声音研究中的标注挑战。它可以用于个人和团队的工作，可以在共享服务器上hosts或远程访问，或者在个人电脑上运行，无需编程技能。</details></li>
</ol>
<hr>
<h2 id="Naaloss-Rethinking-the-objective-of-speech-enhancement"><a href="#Naaloss-Rethinking-the-objective-of-speech-enhancement" class="headerlink" title="Naaloss: Rethinking the objective of speech enhancement"></a>Naaloss: Rethinking the objective of speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12615">http://arxiv.org/abs/2308.12615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan-Hsun Ho, En-Lun Yu, Jeih-weih Hung, Berlin Chen</li>
<li>for: 提高自动语音识别（ASR）系统在实际场景中的性能，减少干扰声音的影响。</li>
<li>methods: 提出了一种针对干扰声音的损失函数NAaLoss，通过考虑损失估计、去噪和干扰声音无关的三个方面，使SE模型能够准确地模型干扰声音、降低误差。</li>
<li>results: 对两种SE模型（简单&#x2F;高级）在不同的输入enario（干扰&#x2F;不干扰）和ASR系统的两种配置（含&#x2F;不含噪音鲁棒性）进行了测试，结果表明NAaLoss可以显著提高ASR性能，同时保持SE模型的质量。此外，通过波形和spectrogram的视觉化，解释了干扰声音对ASR的影响。<details>
<summary>Abstract</summary>
Reducing noise interference is crucial for automatic speech recognition (ASR) in a real-world scenario. However, most single-channel speech enhancement (SE) generates "processing artifacts" that negatively affect ASR performance. Hence, in this study, we suggest a Noise- and Artifacts-aware loss function, NAaLoss, to ameliorate the influence of artifacts from a novel perspective. NAaLoss considers the loss of estimation, de-artifact, and noise ignorance, enabling the learned SE to individually model speech, artifacts, and noise. We examine two SE models (simple/advanced) learned with NAaLoss under various input scenarios (clean/noisy) using two configurations of the ASR system (with/without noise robustness). Experiments reveal that NAaLoss significantly improves the ASR performance of most setups while preserving the quality of SE toward perception and intelligibility. Furthermore, we visualize artifacts through waveforms and spectrograms, and explain their impact on ASR.
</details>
<details>
<summary>摘要</summary>
红外干扰减少是自动语音识别（ASR）实际场景中的关键因素。然而，大多数单通道speech增强（SE）生成“处理 artifacts”，这些artifacts negatively affect ASR性能。因此，在本研究中，我们提出了一种听力和 artifacts 意识的损失函数（NAaLoss），从新的角度来缓解 artifacts 的影响。NAaLoss 考虑了损失估计、去artifact、和干扰无关的损失，使得学习的 SE 可以单独模型speech、artifacts 和干扰。我们在不同的输入场景（干净/噪音）和 ASR 系统的两种配置（带/没有噪音鲁�能）中，使用两种 SE 模型（简单/高级），并对其进行了实验。实验结果表明，NAaLoss 可以明显提高 ASR 性能的大多数设置，同时保持 SE 的质量。此外，我们通过波形和spectrogram来可视化artefacts，并解释它们对 ASR 的影响。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music"><a href="#Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music" class="headerlink" title="Emotion-Aligned Contrastive Learning Between Images and Music"></a>Emotion-Aligned Contrastive Learning Between Images and Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12610">http://arxiv.org/abs/2308.12610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanti Stewart, Tiantian Feng, Kleanthis Avramidis, Shrikanth Narayanan</li>
<li>for: 这篇论文主要针对的是如何通过图像查询检索具有情感特征的音乐。</li>
<li>methods: 本文提出了一种基于感受协调的图像-音乐 JOINT embedding 框架，通过情感调教的对照学习来学习图像和音乐之间的共同embedding空间。</li>
<li>results: 实验结果表明，该方法可以成功对图像和音乐进行对应的匹配，并且学习出的 embedding 空间有效地应用于跨模态检索任务。<details>
<summary>Abstract</summary>
Traditional music search engines rely on retrieval methods that match natural language queries with music metadata. There have been increasing efforts to expand retrieval methods to consider the audio characteristics of music itself, using queries of various modalities including text, video, and speech. Most approaches aim to match general music semantics to the input queries, while only a few focus on affective qualities. We address the task of retrieving emotionally-relevant music from image queries by proposing a framework for learning an affective alignment between images and music audio. Our approach focuses on learning an emotion-aligned joint embedding space between images and music. This joint embedding space is learned via emotion-supervised contrastive learning, using an adapted cross-modal version of the SupCon loss. We directly evaluate the joint embeddings with cross-modal retrieval tasks (image-to-music and music-to-image) based on emotion labels. In addition, we investigate the generalizability of the learned music embeddings with automatic music tagging as a downstream task. Our experiments show that our approach successfully aligns images and music, and that the learned embedding space is effective for cross-modal retrieval applications.
</details>
<details>
<summary>摘要</summary>
传统音乐搜索引擎通常采用匹配自然语言查询与音乐元数据的方法。随着扩展到考虑音乐自身特征的搜索方法的努力，包括文本、视频和speech等多种模式的查询。大多数方法尝试匹配通用音乐 semantics 与输入查询，只有一些关注情感质量。我们解决通过图像查询获取情感相关的音乐的任务，我们提议一种学习图像和音乐audio之间的情感匹配。我们的方法是学习图像和音乐之间的情感对齐的共同嵌入空间。我们使用基于情感标签的cross-modal SupCon损失函数进行情感激活的contrastive学习，以学习图像和音乐之间的情感匹配。我们直接测试共同嵌入的表现，使用图像到音乐和音乐到图像的跨模态检索任务，并评估下游自动音乐标签的可行性。我们的实验表明，我们的方法成功地将图像和音乐进行了对齐，并且学习的嵌入空间对跨模态检索应用有效。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window"><a href="#Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window" class="headerlink" title="Hybrid noise shaping for audio coding using perfectly overlapped window"></a>Hybrid noise shaping for audio coding using perfectly overlapped window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12566">http://arxiv.org/abs/2308.12566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongho Jo, Seungkwon Beack</li>
<li>for: 低比特率音频编码</li>
<li>methods: 模拟复复数扩展 transform-based coding框架、transformcoded excitation（TCX）和复复数 Laplace transform-based noise shaping（CTNS）</li>
<li>results: 提高编码效率和聆听评价表现<details>
<summary>Abstract</summary>
In recent years, audio coding technology has been standardized based on several frameworks that incorporate linear predictive coding (LPC). However, coding the transient signal using frequency-domain LP residual signals remains a challenge. To address this, temporal noise shaping (TNS) can be adapted, although it cannot be effectively operated since the estimated temporal envelope in the modified discrete cosine transform (MDCT) domain is accompanied by the time-domain aliasing (TDA) terms. In this study, we propose the modulated complex lapped transform-based coding framework integrated with transform coded excitation (TCX) and complex LPC-based TNS (CTNS). Our approach uses a 50\% overlap window and switching scheme for the CTNS to improve the coding efficiency. Additionally, an adaptive calculation of the target bits for the sub-bands using the frequency envelope information based on the quantized LPC coefficients is proposed. To minimize the quantization mismatch between both modes, an integrated quantization for real and complex values and a TDA augmentation method that compensates for the artificially generated TDA components during switching operations are proposed. The proposed coding framework shows a superior performance in both objective metrics and subjective listening tests, thereby demonstrating its low bit-rate audio coding.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, which is the standardized form of Chinese used in mainland China. However, the translation may not be perfect and may not capture all the nuances of the original text.)
</details></li>
</ul>
<hr>
<h2 id="MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario"><a href="#MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario" class="headerlink" title="MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario"></a>MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12490">http://arxiv.org/abs/2308.12490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Zhou Yu, Julia Hirschberg</li>
<li>for: 这个研究旨在开发一种多任务语音发音评估模型，以提供更加准确和全面的发音技巧评估。</li>
<li>methods: 该模型使用了多任务学习方法，并与其他神经网络模型相结合，以提高发音评估的准确性和可靠性。</li>
<li>results: 实验结果表明，该模型在关闭响应场景下的性能与以往的Kaldi-based系统相当，而在开放响应场景下的性能更为稳定和可靠。<details>
<summary>Abstract</summary>
The design of automatic speech pronunciation assessment can be categorized into closed and open response scenarios, each with strengths and limitations. A system with the ability to function in both scenarios can cater to diverse learning needs and provide a more precise and holistic assessment of pronunciation skills. In this study, we propose a Multi-task Pronunciation Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based systems in that it has simpler format requirements and better compatibility with other neural network models. Compared with previous open response systems, MultiPA provides a wider range of evaluations, encompassing assessments at both the sentence and word-level. Our experimental results show that MultiPA achieves comparable performance when working in closed response scenarios and maintains more robust performance when directly used for open responses.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection"><a href="#Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection" class="headerlink" title="Attention-Based Acoustic Feature Fusion Network for Depression Detection"></a>Attention-Based Acoustic Feature Fusion Network for Depression Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12478">http://arxiv.org/abs/2308.12478</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuxiaoooo/abafnet">https://github.com/xuxiaoooo/abafnet</a></li>
<li>paper_authors: Xiao Xu, Yang Wang, Xinru Wei, Fei Wang, Xizhe Zhang</li>
<li>for: 这篇论文的目的是提出一个新的语音特征融合网络，以提高对抑郁症的检测。</li>
<li>methods: 这篇论文使用了一种新的注意力网络，将四种不同的语音特征融合到一个深度学习模型中，以实现多维度特征的有效融合。</li>
<li>results: 这篇论文的实验结果显示，这个新的方法可以在两个临床语音数据库中进行优化，并在抑郁症检测和亚型分类中表现出色，比前一些方法更好。<details>
<summary>Abstract</summary>
Depression, a common mental disorder, significantly influences individuals and imposes considerable societal impacts. The complexity and heterogeneity of the disorder necessitate prompt and effective detection, which nonetheless, poses a difficult challenge. This situation highlights an urgent requirement for improved detection methods. Exploiting auditory data through advanced machine learning paradigms presents promising research directions. Yet, existing techniques mainly rely on single-dimensional feature models, potentially neglecting the abundance of information hidden in various speech characteristics. To rectify this, we present the novel Attention-Based Acoustic Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines four different acoustic features into a comprehensive deep learning model, thereby effectively integrating and blending multi-tiered features. We present a novel weight adjustment module for late fusion that boosts performance by efficaciously synthesizing these features. The effectiveness of our approach is confirmed via extensive validation on two clinical speech databases, CNRAC and CS-NRAC, thereby outperforming previous methods in depression detection and subtype classification. Further in-depth analysis confirms the key role of each feature and highlights the importance of MFCCrelated features in speech-based depression detection.
</details>
<details>
<summary>摘要</summary>
抑郁症，一种常见的心理疾病，对个人和社会产生了深远的影响。由于这种疾病的复杂性和多样性，早期检测成为了一项紧迫的需求。然而，现有的方法主要基于单一的特征模型，可能会忽略潜在的语音特征信息。为了解决这个问题，我们提出了一种新的注意力基于的听音特征融合网络（ABAFnet），用于抑郁检测。ABAFnet组合了四种不同的听音特征，并将其集成到深度学习模型中，以有效地融合多维特征。我们还提出了一种新的权重调整模块，用于补做末级融合，从而提高表现。我们的方法在两个临床speech数据库中进行了广泛验证，并在抑郁检测和亚型分类方面表现出了超过前方法的优异性。进一步的深入分析表明，每种特征在抑郁检测中扮演着重要的角色，并且MFCC相关特征在语音基于的抑郁检测中具有重要性。
</details></li>
</ul>
<hr>
<h2 id="An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video"><a href="#An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video" class="headerlink" title="An Initial Exploration: Learning to Generate Realistic Audio for Silent Video"></a>An Initial Exploration: Learning to Generate Realistic Audio for Silent Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12408">http://arxiv.org/abs/2308.12408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Martel, Jackson Wagner</li>
<li>for: 这篇论文旨在开发一个基于深度学习的框架，用于创建电影和其他媒体中的吸引人声音效。</li>
<li>methods: 这篇论文使用了深度学习技术，将影像和声音融合为一体，并使用了多种模型架构，包括深度融合CNN、增压Wavenet CNN和transformer架构。</li>
<li>results: 研究发现，使用transformer架构可以将低频声音与视觉模式匹配得非常好，但是尚未能够生成更加细腻的波形。<details>
<summary>Abstract</summary>
Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it's natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.
</details>
<details>
<summary>摘要</summary>
生成电影和其他媒体中的真实音效是一项复杂的任务，主要通过物理方法实现，称为FOLEY艺术。FOLEY艺术家使用常见物品（例如拳击手套、碎玻璃）与视频同步生成吸引人的音轨。在这种工作中，我们希望通过深度学习框架来实现类似的目标——观察视频的自然序列，并生成真实的音效。我们认为这是可能的，因为现有的真实音效生成技术（例如文本conditioned Wavenet）的进步。我们检 explore了多种不同的模型架构来完成这个任务，包括深度融合CNN、扩展Wavenet CNN与视觉上下文、转换器基 Architecture。我们发现Transformer-based architecture最有前途，能够准确地匹配视觉模式下的低频谱，但是无法生成更复杂的波形。
</details></li>
</ul>
<hr>
<h2 id="AdVerb-Visually-Guided-Audio-Dereverberation"><a href="#AdVerb-Visually-Guided-Audio-Dereverberation" class="headerlink" title="AdVerb: Visually Guided Audio Dereverberation"></a>AdVerb: Visually Guided Audio Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12370">http://arxiv.org/abs/2308.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjoy Chowdhury, Sreyan Ghosh, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha</li>
<li>for: 增强杂音音频识别和人脸识别</li>
<li>methods: 利用视觉特征和听频特征共同学习计算清晰音频</li>
<li>results: 在三个下游任务中，包括语音提升、语音识别和人脸识别，与传统音频只和音频视频基线相比，提高了18%-82%的相对提升率，并在AVSpeech数据集上达到了非常满意的RT60错误值。<details>
<summary>Abstract</summary>
We present AdVerb, a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. Although audio-only dereverberation is a well-studied problem, our approach incorporates the complementary visual modality to perform audio dereverberation. Given an image of the environment where the reverberated sound signal has been recorded, AdVerb employs a novel geometry-aware cross-modal transformer architecture that captures scene geometry and audio-visual cross-modal relationship to generate a complex ideal ratio mask, which, when applied to the reverberant audio predicts the clean sound. The effectiveness of our method is demonstrated through extensive quantitative and qualitative evaluations. Our approach significantly outperforms traditional audio-only and audio-visual baselines on three downstream tasks: speech enhancement, speech recognition, and speaker verification, with relative improvements in the range of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly satisfactory RT60 error scores on the AVSpeech dataset.
</details>
<details>
<summary>摘要</summary>
我们介绍了 AdVerb，一种新的听视频去扩散框架，该框架使用视觉准确信息以及扩散声音来估算清晰声音。虽然听音只的去扩散是已经广泛研究的问题，但我们的方法将视觉modalitat incorporated into the framework，以实现听视频去扩散。给出了 recording environment中的图像，AdVerb使用了一种新的场景意识geometry-aware cross-modal transformer架构，该架构能够捕捉场景准确性和听视频cross-modal关系，生成一个复杂的理想比率幕，当应用于扩散声音时，可以预测清晰声音。我们的方法的效果被通过广泛的量化和质量评估证明。我们的方法在三个下游任务中表现出了显著的改善，即speech enhancement、speech recognition和speaker verification，相对于传统的听音只和听视频基线，在LibriSpeech测试集上的改善范围为18%-82%。我们还在AVSpeech数据集上获得了非常满意的RT60错误分数。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/cs.SD_2023_08_24/" data-id="clmjn91o800bh0j88220jecw5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/eess.AS_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/eess.AS_2023_08_24/">eess.AS - 2023-08-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Automated-Animal-Density-Estimation-with-Acoustic-Spatial-Capture-Recapture"><a href="#Towards-Automated-Animal-Density-Estimation-with-Acoustic-Spatial-Capture-Recapture" class="headerlink" title="Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture"></a>Towards Automated Animal Density Estimation with Acoustic Spatial Capture-Recapture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12859">http://arxiv.org/abs/2308.12859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuheng Wang, Juan Ye, David L. Borchers</li>
</ul>
<p>Abstract:<br>Passive acoustic monitoring can be an effective way of monitoring wildlife populations that are acoustically active but difficult to survey visually. Digital recorders allow surveyors to gather large volumes of data at low cost, but identifying target species vocalisations in these data is non-trivial. Machine learning (ML) methods are often used to do the identification. They can process large volumes of data quickly, but they do not detect all vocalisations and they do generate some false positives (vocalisations that are not from the target species). Existing wildlife abundance survey methods have been designed specifically to deal with the first of these mistakes, but current methods of dealing with false positives are not well-developed. They do not take account of features of individual vocalisations, some of which are more likely to be false positives than others. We propose three methods for acoustic spatial capture-recapture inference that integrate individual-level measures of confidence from ML vocalisation identification into the likelihood and hence integrate ML uncertainty into inference. The methods include a mixture model in which species identity is a latent variable. We test the methods by simulation and find that in a scenario based on acoustic data from Hainan gibbons, in which ignoring false positives results in 17% positive bias, our methods give negligible bias and coverage probabilities that are close to the nominal 95% level.</p>
<hr>
<h2 id="Sparks-of-Large-Audio-Models-A-Survey-and-Outlook"><a href="#Sparks-of-Large-Audio-Models-A-Survey-and-Outlook" class="headerlink" title="Sparks of Large Audio Models: A Survey and Outlook"></a>Sparks of Large Audio Models: A Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12792">http://arxiv.org/abs/2308.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Heriberto Cuayáhuitl, Björn W. Schuller</li>
</ul>
<p>Abstract:<br>This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources–from human voices to musical instruments and environmental sounds–poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at <a target="_blank" rel="noopener" href="https://github.com/EmulationAI/awesome-large-audio-models">https://github.com/EmulationAI/awesome-large-audio-models</a>.</p>
<hr>
<h2 id="WavMark-Watermarking-for-Audio-Generation"><a href="#WavMark-Watermarking-for-Audio-Generation" class="headerlink" title="WavMark: Watermarking for Audio Generation"></a>WavMark: Watermarking for Audio Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12770">http://arxiv.org/abs/2308.12770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei</li>
</ul>
<p>Abstract:<br>Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker’s voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. Utilizing 10 to 20-second audio as the host, our approach demonstrates an average Bit Error Rate (BER) of 0.48% across ten common attacks, a remarkable reduction of over 2800% in BER compared to the state-of-the-art watermarking tool. See <a target="_blank" rel="noopener" href="https://aka.ms/wavmark">https://aka.ms/wavmark</a> for demos of our work.</p>
<hr>
<h2 id="Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion"><a href="#Real-time-Detection-of-AI-Generated-Speech-for-DeepFake-Voice-Conversion" class="headerlink" title="Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion"></a>Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12734">http://arxiv.org/abs/2308.12734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan J. Bird, Ahmad Lotfi</li>
</ul>
<p>Abstract:<br>There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learning models over 10-fold cross validation, it is found that the Extreme Gradient Boosting model can achieve an average classification accuracy of 99.3% and can classify speech in real-time, at around 0.004 milliseconds given one second of speech. All data generated for this study is released publicly for future research on AI speech detection.</p>
<hr>
<h2 id="Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics"><a href="#Whombat-An-open-source-annotation-tool-for-machine-learning-development-in-bioacoustics" class="headerlink" title="Whombat: An open-source annotation tool for machine learning development in bioacoustics"></a>Whombat: An open-source annotation tool for machine learning development in bioacoustics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12688">http://arxiv.org/abs/2308.12688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santiago Martinez Balvanera, Oisin Mac Aodha, Matthew J. Weldy, Holly Pringle, Ella Browning, Kate E. Jones</li>
</ul>
<p>Abstract:</p>
<ol>
<li>Automated analysis of bioacoustic recordings using machine learning (ML) methods has the potential to greatly scale biodiversity monitoring efforts. The use of ML for high-stakes applications, such as conservation research, demands a data-centric approach with a focus on utilizing carefully annotated and curated evaluation and training data that is relevant and representative. Creating annotated datasets of sound recordings presents a number of challenges, such as managing large collections of recordings with associated metadata, developing flexible annotation tools that can accommodate the diverse range of vocalization profiles of different organisms, and addressing the scarcity of expert annotators.   2. We present Whombat a user-friendly, browser-based interface for managing audio recordings and annotation projects, with several visualization, exploration, and annotation tools. It enables users to quickly annotate, review, and share annotations, as well as visualize and evaluate a set of machine learning predictions on a dataset. The tool facilitates an iterative workflow where user annotations and machine learning predictions feedback to enhance model performance and annotation quality.   3. We demonstrate the flexibility of Whombat by showcasing two distinct use cases: an project aimed at enhancing automated UK bat call identification at the Bat Conservation Trust (BCT), and a collaborative effort among the USDA Forest Service and Oregon State University researchers exploring bioacoustic applications and extending automated avian classification models in the Pacific Northwest, USA.   4. Whombat is a flexible tool that can effectively address the challenges of annotation for bioacoustic research. It can be used for individual and collaborative work, hosted on a shared server or accessed remotely, or run on a personal computer without the need for coding skills.</li>
</ol>
<hr>
<h2 id="Naaloss-Rethinking-the-objective-of-speech-enhancement"><a href="#Naaloss-Rethinking-the-objective-of-speech-enhancement" class="headerlink" title="Naaloss: Rethinking the objective of speech enhancement"></a>Naaloss: Rethinking the objective of speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12615">http://arxiv.org/abs/2308.12615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan-Hsun Ho, En-Lun Yu, Jeih-weih Hung, Berlin Chen</li>
</ul>
<p>Abstract:<br>Reducing noise interference is crucial for automatic speech recognition (ASR) in a real-world scenario. However, most single-channel speech enhancement (SE) generates “processing artifacts” that negatively affect ASR performance. Hence, in this study, we suggest a Noise- and Artifacts-aware loss function, NAaLoss, to ameliorate the influence of artifacts from a novel perspective. NAaLoss considers the loss of estimation, de-artifact, and noise ignorance, enabling the learned SE to individually model speech, artifacts, and noise. We examine two SE models (simple&#x2F;advanced) learned with NAaLoss under various input scenarios (clean&#x2F;noisy) using two configurations of the ASR system (with&#x2F;without noise robustness). Experiments reveal that NAaLoss significantly improves the ASR performance of most setups while preserving the quality of SE toward perception and intelligibility. Furthermore, we visualize artifacts through waveforms and spectrograms, and explain their impact on ASR.</p>
<hr>
<h2 id="Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music"><a href="#Emotion-Aligned-Contrastive-Learning-Between-Images-and-Music" class="headerlink" title="Emotion-Aligned Contrastive Learning Between Images and Music"></a>Emotion-Aligned Contrastive Learning Between Images and Music</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12610">http://arxiv.org/abs/2308.12610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanti Stewart, Tiantian Feng, Kleanthis Avramidis, Shrikanth Narayanan</li>
</ul>
<p>Abstract:<br>Traditional music search engines rely on retrieval methods that match natural language queries with music metadata. There have been increasing efforts to expand retrieval methods to consider the audio characteristics of music itself, using queries of various modalities including text, video, and speech. Most approaches aim to match general music semantics to the input queries, while only a few focus on affective qualities. We address the task of retrieving emotionally-relevant music from image queries by proposing a framework for learning an affective alignment between images and music audio. Our approach focuses on learning an emotion-aligned joint embedding space between images and music. This joint embedding space is learned via emotion-supervised contrastive learning, using an adapted cross-modal version of the SupCon loss. We directly evaluate the joint embeddings with cross-modal retrieval tasks (image-to-music and music-to-image) based on emotion labels. In addition, we investigate the generalizability of the learned music embeddings with automatic music tagging as a downstream task. Our experiments show that our approach successfully aligns images and music, and that the learned embedding space is effective for cross-modal retrieval applications.</p>
<hr>
<h2 id="Exploiting-Time-Frequency-Conformers-for-Music-Audio-Enhancement"><a href="#Exploiting-Time-Frequency-Conformers-for-Music-Audio-Enhancement" class="headerlink" title="Exploiting Time-Frequency Conformers for Music Audio Enhancement"></a>Exploiting Time-Frequency Conformers for Music Audio Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12599">http://arxiv.org/abs/2308.12599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunkee Chae, Junghyun Koo, Sungho Lee, Kyogu Lee</li>
</ul>
<p>Abstract:<br>With the proliferation of video platforms on the internet, recording musical performances by mobile devices has become commonplace. However, these recordings often suffer from degradation such as noise and reverberation, which negatively impact the listening experience. Consequently, the necessity for music audio enhancement (referred to as music enhancement from this point onward), involving the transformation of degraded audio recordings into pristine high-quality music, has surged to augment the auditory experience. To address this issue, we propose a music enhancement system based on the Conformer architecture that has demonstrated outstanding performance in speech enhancement tasks. Our approach explores the attention mechanisms of the Conformer and examines their performance to discover the best approach for the music enhancement task. Our experimental results show that our proposed model achieves state-of-the-art performance on single-stem music enhancement. Furthermore, our system can perform general music enhancement with multi-track mixtures, which has not been examined in previous work.</p>
<hr>
<h2 id="Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window"><a href="#Hybrid-noise-shaping-for-audio-coding-using-perfectly-overlapped-window" class="headerlink" title="Hybrid noise shaping for audio coding using perfectly overlapped window"></a>Hybrid noise shaping for audio coding using perfectly overlapped window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12566">http://arxiv.org/abs/2308.12566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeongho Jo, Seungkwon Beack</li>
</ul>
<p>Abstract:<br>In recent years, audio coding technology has been standardized based on several frameworks that incorporate linear predictive coding (LPC). However, coding the transient signal using frequency-domain LP residual signals remains a challenge. To address this, temporal noise shaping (TNS) can be adapted, although it cannot be effectively operated since the estimated temporal envelope in the modified discrete cosine transform (MDCT) domain is accompanied by the time-domain aliasing (TDA) terms. In this study, we propose the modulated complex lapped transform-based coding framework integrated with transform coded excitation (TCX) and complex LPC-based TNS (CTNS). Our approach uses a 50% overlap window and switching scheme for the CTNS to improve the coding efficiency. Additionally, an adaptive calculation of the target bits for the sub-bands using the frequency envelope information based on the quantized LPC coefficients is proposed. To minimize the quantization mismatch between both modes, an integrated quantization for real and complex values and a TDA augmentation method that compensates for the artificially generated TDA components during switching operations are proposed. The proposed coding framework shows a superior performance in both objective metrics and subjective listening tests, thereby demonstrating its low bit-rate audio coding.</p>
<hr>
<h2 id="UNISOUND-System-for-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#UNISOUND-System-for-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023"></a>UNISOUND System for VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12526">http://arxiv.org/abs/2308.12526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Zheng, Yajun Zhang, Chuanying Niu, Yibin Zhan, Yanhua Long, Dongxing Xu</li>
</ul>
<p>Abstract:<br>This report describes the UNISOUND submission for Track1 and Track2 of VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC 2023). We submit the same system on Track 1 and Track 2, which is trained with only VoxCeleb2-dev. Large-scale ResNet and RepVGG architectures are developed for the challenge. We propose a consistency-aware score calibration method, which leverages the stability of audio voiceprints in similarity score by a Consistency Measure Factor (CMF). CMF brings a huge performance boost in this challenge. Our final system is a fusion of six models and achieves the first place in Track 1 and second place in Track 2 of VoxSRC 2023. The minDCF of our submission is 0.0855 and the EER is 1.5880%.</p>
<hr>
<h2 id="MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario"><a href="#MultiPA-a-multi-task-speech-pronunciation-assessment-system-for-a-closed-and-open-response-scenario" class="headerlink" title="MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario"></a>MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12490">http://arxiv.org/abs/2308.12490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Zhou Yu, Julia Hirschberg</li>
</ul>
<p>Abstract:<br>The design of automatic speech pronunciation assessment can be categorized into closed and open response scenarios, each with strengths and limitations. A system with the ability to function in both scenarios can cater to diverse learning needs and provide a more precise and holistic assessment of pronunciation skills. In this study, we propose a Multi-task Pronunciation Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based systems in that it has simpler format requirements and better compatibility with other neural network models. Compared with previous open response systems, MultiPA provides a wider range of evaluations, encompassing assessments at both the sentence and word-level. Our experimental results show that MultiPA achieves comparable performance when working in closed response scenarios and maintains more robust performance when directly used for open responses.</p>
<hr>
<h2 id="Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection"><a href="#Attention-Based-Acoustic-Feature-Fusion-Network-for-Depression-Detection" class="headerlink" title="Attention-Based Acoustic Feature Fusion Network for Depression Detection"></a>Attention-Based Acoustic Feature Fusion Network for Depression Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12478">http://arxiv.org/abs/2308.12478</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuxiaoooo/abafnet">https://github.com/xuxiaoooo/abafnet</a></li>
<li>paper_authors: Xiao Xu, Yang Wang, Xinru Wei, Fei Wang, Xizhe Zhang</li>
</ul>
<p>Abstract:<br>Depression, a common mental disorder, significantly influences individuals and imposes considerable societal impacts. The complexity and heterogeneity of the disorder necessitate prompt and effective detection, which nonetheless, poses a difficult challenge. This situation highlights an urgent requirement for improved detection methods. Exploiting auditory data through advanced machine learning paradigms presents promising research directions. Yet, existing techniques mainly rely on single-dimensional feature models, potentially neglecting the abundance of information hidden in various speech characteristics. To rectify this, we present the novel Attention-Based Acoustic Feature Fusion Network (ABAFnet) for depression detection. ABAFnet combines four different acoustic features into a comprehensive deep learning model, thereby effectively integrating and blending multi-tiered features. We present a novel weight adjustment module for late fusion that boosts performance by efficaciously synthesizing these features. The effectiveness of our approach is confirmed via extensive validation on two clinical speech databases, CNRAC and CS-NRAC, thereby outperforming previous methods in depression detection and subtype classification. Further in-depth analysis confirms the key role of each feature and highlights the importance of MFCCrelated features in speech-based depression detection.</p>
<hr>
<h2 id="An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video"><a href="#An-Initial-Exploration-Learning-to-Generate-Realistic-Audio-for-Silent-Video" class="headerlink" title="An Initial Exploration: Learning to Generate Realistic Audio for Silent Video"></a>An Initial Exploration: Learning to Generate Realistic Audio for Silent Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12408">http://arxiv.org/abs/2308.12408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Martel, Jackson Wagner</li>
</ul>
<p>Abstract:<br>Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it’s natural sequence and generates realistic audio to accompany it. Notably, we have reason to believe this is achievable due to advancements in realistic audio generation techniques conditioned on other inputs (e.g., Wavenet conditioned on text). We explore several different model architectures to accomplish this task that process both previously-generated audio and video context. These include deep-fusion CNN, dilated Wavenet CNN with visual context, and transformer-based architectures. We find that the transformer-based architecture yields the most promising results, matching low-frequencies to visual patterns effectively, but failing to generate more nuanced waveforms.</p>
<hr>
<h2 id="AdVerb-Visually-Guided-Audio-Dereverberation"><a href="#AdVerb-Visually-Guided-Audio-Dereverberation" class="headerlink" title="AdVerb: Visually Guided Audio Dereverberation"></a>AdVerb: Visually Guided Audio Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12370">http://arxiv.org/abs/2308.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjoy Chowdhury, Sreyan Ghosh, Subhrajyoti Dasgupta, Anton Ratnarajah, Utkarsh Tyagi, Dinesh Manocha</li>
</ul>
<p>Abstract:<br>We present AdVerb, a novel audio-visual dereverberation framework that uses visual cues in addition to the reverberant sound to estimate clean audio. Although audio-only dereverberation is a well-studied problem, our approach incorporates the complementary visual modality to perform audio dereverberation. Given an image of the environment where the reverberated sound signal has been recorded, AdVerb employs a novel geometry-aware cross-modal transformer architecture that captures scene geometry and audio-visual cross-modal relationship to generate a complex ideal ratio mask, which, when applied to the reverberant audio predicts the clean sound. The effectiveness of our method is demonstrated through extensive quantitative and qualitative evaluations. Our approach significantly outperforms traditional audio-only and audio-visual baselines on three downstream tasks: speech enhancement, speech recognition, and speaker verification, with relative improvements in the range of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly satisfactory RT60 error scores on the AVSpeech dataset.</p>
<hr>
<h2 id="LCANets-Robust-Audio-Classification-using-Multi-layer-Neural-Networks-with-Lateral-Competition"><a href="#LCANets-Robust-Audio-Classification-using-Multi-layer-Neural-Networks-with-Lateral-Competition" class="headerlink" title="LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition"></a>LCANets++: Robust Audio Classification using Multi-layer Neural Networks with Lateral Competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12882">http://arxiv.org/abs/2308.12882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti</li>
</ul>
<p>Abstract:<br>Audio classification aims at recognizing audio signals, including speech commands or sound events. However, current audio classifiers are susceptible to perturbations and adversarial attacks. In addition, real-world audio classification tasks often suffer from limited labeled data. To help bridge these gaps, previous work developed neuro-inspired convolutional neural networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA) in the first layer (i.e., LCANets) for computer vision. LCANets learn in a combination of supervised and unsupervised learning, reducing dependency on labeled samples. Motivated by the fact that auditory cortex is also sparse, we extend LCANets to audio recognition tasks and introduce LCANets++, which are CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that LCANets++ are more robust than standard CNNs and LCANets against perturbations, e.g., background noise, as well as black-box and white-box attacks, e.g., evasion and fast gradient sign (FGSM) attacks.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/eess.AS_2023_08_24/" data-id="clmjn91ph00dz0j88ec3pe7fs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/24/eess.IV_2023_08_24/" class="article-date">
  <time datetime="2023-08-23T16:00:00.000Z" itemprop="datePublished">2023-08-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/24/eess.IV_2023_08_24/">eess.IV - 2023-08-24 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations"><a href="#Learned-Local-Attention-Maps-for-Synthesising-Vessel-Segmentations" class="headerlink" title="Learned Local Attention Maps for Synthesising Vessel Segmentations"></a>Learned Local Attention Maps for Synthesising Vessel Segmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12861">http://arxiv.org/abs/2308.12861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Rodrigo Bonazzola, Haoran Dou, Yan Xia, Tianyou Wei, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>for: 这个论文的目的是用MRA进行血管视化，但MRA不是常见的成像方法，因此需要一种方法来合成血管分割结果。</li>
<li>methods: 这个论文使用了一种编码器-解码器模型，用于从T2 MRI中提取血管分割结果。该模型使用了两个阶段的多目标学习方法，以捕捉全局和本地特征。它还使用了学习的本地注意力图，以便只从T2 MRI中提取与合成CoW血管相关的信息。</li>
<li>results: 在测试中，这个模型可以从T2 MRI中生成高质量的血管分割结果，其中的Dice分数为0.79±0.03，高于现有的分割网络，如转换器U-Net（0.71±0.04）和nnU-net（0.68±0.05）。主要区别在于生成的CoW血管段的分辨率更高，特别是后circulation。<details>
<summary>Abstract</summary>
Magnetic resonance angiography (MRA) is an imaging modality for visualising blood vessels. It is useful for several diagnostic applications and for assessing the risk of adverse events such as haemorrhagic stroke (resulting from the rupture of aneurysms in blood vessels). However, MRAs are not acquired routinely, hence, an approach to synthesise blood vessel segmentations from more routinely acquired MR contrasts such as T1 and T2, would be useful. We present an encoder-decoder model for synthesising segmentations of the main cerebral arteries in the circle of Willis (CoW) from only T2 MRI. We propose a two-phase multi-objective learning approach, which captures both global and local features. It uses learned local attention maps generated by dilating the segmentation labels, which forces the network to only extract information from the T2 MRI relevant to synthesising the CoW. Our synthetic vessel segmentations generated from only T2 MRI achieved a mean Dice score of $0.79 \pm 0.03$ in testing, compared to state-of-the-art segmentation networks such as transformer U-Net ($0.71 \pm 0.04$) and nnU-net($0.68 \pm 0.05$), while using only a fraction of the parameters. The main qualitative difference between our synthetic vessel segmentations and the comparative models was in the sharper resolution of the CoW vessel segments, especially in the posterior circulation.
</details>
<details>
<summary>摘要</summary>
磁共振成像（MRA）是一种成像血管的方法，可以用于诊断和评估血栓roke的风险。然而，MRA不是Routine获得的，因此一种能够从T1和T2磁共振图像中合成血管分割的方法会很有用。我们提出了一种encoder-decoder模型，可以从T2 MRI中生成主要脑动脉的分割。我们使用了两个阶段的多目标学习方法，包括全局和局部特征。它使用学习的本地注意力地图，从T2 MRI中提取与合成CoW的信息。我们的合成血管分割从T2 MRI中获得的Mean Dice分数为$0.79\pm0.03$,比如state-of-the-art segmentation网络（如transformer U-Net和nnU-net）高出了一些。主要的区别在于CoW血管段的分辨率更加高，特别是后 circulation。
</details></li>
</ul>
<hr>
<h2 id="Achromatic-imaging-systems-with-flat-lenses-enabled-by-deep-learning"><a href="#Achromatic-imaging-systems-with-flat-lenses-enabled-by-deep-learning" class="headerlink" title="Achromatic imaging systems with flat lenses enabled by deep learning"></a>Achromatic imaging systems with flat lenses enabled by deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12776">http://arxiv.org/abs/2308.12776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roy Maman, Eitan Mualem, Noa Mazurski, Jacob Engelberg, Uriel Levy</li>
<li>for: 这篇论文旨在探讨使用凹型镜代替传统凸型镜元件的现代光学系统中，使用凹型镜带来的颜色偏振问题的解决方案。</li>
<li>methods: 作者使用了一种基于深度学习的方法，使用凹型镜拍摄的颜色户外图像数据集来对凹型镜的颜色偏振进行修正。</li>
<li>results: 作者的实验结果表明，使用这种方法可以在整个可见光谱中实现高质量的图像捕捉，并且在量化方面也有显著的提升，PSNR和SSIM分别达到了45.5dB和0.93。这些结果开启了使用凹型镜在高级多色光学捕捉系统中的应用前景。<details>
<summary>Abstract</summary>
Motivated by their great potential to reduce the size, cost and weight, flat lenses, a category that includes diffractive lenses and metalenses, are rapidly emerging as key components with the potential to replace the traditional refractive optical elements in modern optical systems. Yet, the inherently strong chromatic aberration of these flat lenses is significantly impairing their performance in systems based on polychromatic illumination or passive ambient light illumination, stalling their widespread implementation. Hereby, we provide a promising solution and demonstrate high quality imaging based on flat lenses over the entire visible spectrum. Our approach is based on creating a novel dataset of color outdoor images taken with our flat lens and using this dataset to train a deep-learning model for chromatic aberrations correction. Based on this approach we show unprecedented imaging results not only in terms of qualitative measures but also in the quantitative terms of the PSNR and SSIM scores of the reconstructed images. The results pave the way for the implementation of flat lenses in advanced polychromatic imaging systems.
</details>
<details>
<summary>摘要</summary>
驱动了它们的巨大潜力来减少大小、成本和重量，扁镜，包括扁Diffractive镜和金属镜，在现代光学系统中逐渐emerge为关键组件，替代传统的反射光学元件。然而，扁镜的自然强烈多色偏振问题在基于多色照明或普通 ambient light照明的系统中，对其性能产生了很大的障碍，使其广泛实施受阻。我们提供了一个有优势的解决方案，通过创建一个新的彩色户外图像数据集，使用这个数据集来训练深度学习模型来修正扁镜的多色偏振。根据这种方法，我们展示了具有很高质量的图像成像结果，不仅在质量上有显著的提升，还在量化上通过PSNR和SSIM分数来评估图像重建结果，得到了前所未有的成果。这些结果为扁镜在高级多色成像系统中的实施铺平了道路。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes"><a href="#A-Study-of-Age-and-Sex-Bias-in-Multiple-Instance-Learning-based-Classification-of-Acute-Myeloid-Leukemia-Subtypes" class="headerlink" title="A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes"></a>A Study of Age and Sex Bias in Multiple Instance Learning based Classification of Acute Myeloid Leukemia Subtypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12675">http://arxiv.org/abs/2308.12675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ario Sadafi, Matthias Hehr, Nassir Navab, Carsten Marr</li>
<li>for: 这个研究旨在探讨急性白血病（AML）分型中是否存在年龄和性别偏见，以实现临床决策和患者照顾中的可靠性和公平性。</li>
<li>methods: 这个研究使用多例学习（MIL）架构，对不同的性别差异和年龄偏见进行训练。以评估性别偏见的影响，我们评估了男女试料集的表现。为了评估年龄偏见，我们对训练资料中缺乏的年龄组进行测试。</li>
<li>results: 我们发现，女性患者更容易受到性别偏见的影响，而certain age groups，例如72-86岁的患者，受到年龄偏见的影响。确保训练资料的多元性是生成可靠和公平的AML分型结果的关键。<details>
<summary>Abstract</summary>
Accurate classification of Acute Myeloid Leukemia (AML) subtypes is crucial for clinical decision-making and patient care. In this study, we investigate the potential presence of age and sex bias in AML subtype classification using Multiple Instance Learning (MIL) architectures. To that end, we train multiple MIL models using different levels of sex imbalance in the training set and excluding certain age groups. To assess the sex bias, we evaluate the performance of the models on male and female test sets. For age bias, models are tested against underrepresented age groups in the training data. We find a significant effect of sex and age bias on the performance of the model for AML subtype classification. Specifically, we observe that females are more likely to be affected by sex imbalance dataset and certain age groups, such as patients with 72 to 86 years of age with the RUNX1::RUNX1T1 genetic subtype, are significantly affected by an age bias present in the training data. Ensuring inclusivity in the training data is thus essential for generating reliable and equitable outcomes in AML genetic subtype classification, ultimately benefiting diverse patient populations.
</details>
<details>
<summary>摘要</summary>
精准分类AML分型是至关重要的临床决策和患者护理。本研究探讨AML分型分类中可能存在年龄和性别偏见的问题，使用多个实例学习（MIL）架构。为此，我们在培育不同性别倾向和不同年龄组的模型时进行训练多个MIL模型。以评估性别偏见，我们对男女测试集进行评估模型的性能。为年龄偏见，我们对培育数据中下 representations of age groups进行测试。我们发现，女性更容易受到数据性别偏见的影响，而certain age groups，如72-86岁的患者，受到培育数据中的年龄偏见的影响。因此，保证培育数据的多样性是AML分型分类中的关键，以实现多元患者群体的可靠和公平结果。
</details></li>
</ul>
<hr>
<h2 id="SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression"><a href="#SCP-Spherical-Coordinate-based-Learned-Point-Cloud-Compression" class="headerlink" title="SCP: Spherical-Coordinate-based Learned Point Cloud Compression"></a>SCP: Spherical-Coordinate-based Learned Point Cloud Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12535">http://arxiv.org/abs/2308.12535</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luoao-kddi/SCP">https://github.com/luoao-kddi/SCP</a></li>
<li>paper_authors: Ao Luo, Linxin Song, Keisuke Nonaka, Kyohei Unno, Heming Sun, Masayuki Goto, Jiro Katto</li>
<li>for: 该论文主要探讨了一种基于圆柱坐标系的学习点云压缩方法（SCP），以便全面利用点云中各种圆形特征和方向强相关性，提高压缩率和重建质量。</li>
<li>methods: 该方法基于点云中的圆柱坐标系，并采用多层 Octree 结构来降低远区域重建误差。具有universal性，可应用于多种学习点云压缩技术。</li>
<li>results: 实验结果显示，SCP 方法可以与前期state-of-the-art方法比肩，并且在点对点 PSNR BD-Rate 指标上达到29.14%的提高。<details>
<summary>Abstract</summary>
In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, the spinning LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud compression (SCP), designed to leverage the aforementioned features fully. Additionally, we propose a multi-level Octree for SCP to mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree. SCP exhibits excellent universality, making it applicable to various learned point cloud compression techniques. Experimental results demonstrate that SCP surpasses previous state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.
</details>
<details>
<summary>摘要</summary>
近年来，学习点云压缩任务已经受到重视。一种重要的点云数据类型是旋转 LiDAR 点云，通常由旋转 LiDAR 设备在车辆上生成。这个过程会生成很多圆形和方位角度不变特征，但这些特征在之前的方法中受到了忽略。在本文中，我们提出了一种无模型的方法called Spherical-Coordinate-based learned Point cloud compression (SCP)，旨在完全利用上述特征。此外，我们还提议了一种多级 Octree 来 Mitigate the reconstruction error for distant areas within the Spherical-coordinate-based Octree。SCP 具有优秀的通用性，可以应用于多种学习点云压缩技术。实验结果表明，SCP 可以与前一代方法相比提高点-to-点 PSNR BD-Rate 值达29.14%。
</details></li>
</ul>
<hr>
<h2 id="FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution"><a href="#FFEINR-Flow-Feature-Enhanced-Implicit-Neural-Representation-for-Spatio-temporal-Super-Resolution" class="headerlink" title="FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution"></a>FFEINR: Flow Feature-Enhanced Implicit Neural Representation for Spatio-temporal Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12508">http://arxiv.org/abs/2308.12508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyue Jiao, Chongke Bi, Lu Yang</li>
<li>for: 本研究旨在提出一种Feature-Enhanced Implicit Neural Representation（FFEINR）方法，用于精度高的流体场数据超分辨。</li>
<li>methods: 该方法基于卷积神经网络（CNN）和生成敌对网络（GAN），并通过增强输入层的特征来支持流体场数据的精度高超分辨。</li>
<li>results: 实验结果表明，FFEINR方法比较常用的三次 interpolate方法得到更好的结果。<details>
<summary>Abstract</summary>
Large-scale numerical simulations are capable of generating data up to terabytes or even petabytes. As a promising method of data reduction, super-resolution (SR) has been widely studied in the scientific visualization community. However, most of them are based on deep convolutional neural networks (CNNs) or generative adversarial networks (GANs) and the scale factor needs to be determined before constructing the network. As a result, a single training session only supports a fixed factor and has poor generalization ability. To address these problems, this paper proposes a Feature-Enhanced Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of flow field data. It can take full advantage of the implicit neural representation in terms of model structure and sampling resolution. The neural representation is based on a fully connected network with periodic activation functions, which enables us to obtain lightweight models. The learned continuous representation can decode the low-resolution flow field input data to arbitrary spatial and temporal resolutions, allowing for flexible upsampling. The training process of FFEINR is facilitated by introducing feature enhancements for the input layer, which complements the contextual information of the flow field.To demonstrate the effectiveness of the proposed method, a series of experiments are conducted on different datasets by setting different hyperparameters. The results show that FFEINR achieves significantly better results than the trilinear interpolation method.
</details>
<details>
<summary>摘要</summary>
大规模数值 simulations 可以生成数据 hasta terabytes 或者 Even petabytes。作为数据压缩的承诺方法，超分辨率（SR）在科学视觉社区中得到了广泛的研究。然而，大多数都是基于深度卷积神经网络（CNN）或生成对抗网络（GAN），并且扩大因子需要在建立网络之前确定。因此，单一训练会话只支持固定因子，并且generalization能力不好。为了解决这些问题，本文提出了基于含义增强的偏微分神经表示（FFEINR），用于空间时间超分辨率流场数据。它可以在模型结构和采样分辨率上取得全面利用。神经表示基于完全连接网络，使得模型变得轻量级。学习的连续表示可以将低分辨率输入数据解码到任意空间和时间分辨率，以便灵活的上扩。训练过程中，我们引入了输入层的特征增强，以便补充流场的Contextual信息。为了证明提案的有效性，我们在不同的数据集上进行了一系列实验，并设置了不同的超参数。结果表明，FFEINR可以与三元 interpolate 方法相比，获得显著更好的结果。
</details></li>
</ul>
<hr>
<h2 id="MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices"><a href="#MOFA-A-Model-Simplification-Roadmap-for-Image-Restoration-on-Mobile-Devices" class="headerlink" title="MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices"></a>MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12494">http://arxiv.org/abs/2308.12494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Chen, Ruiwen Zhen, Shuai Li, Xiaotian Li, Guanghui Wang<br>for:这篇论文主要针对的是提高图像恢复模型在移动设备上的效率，以便在资源有限的情况下实现高质量的图像恢复。methods:该论文提出了一种方法，包括在不敏感层添加更多参数，然后使用部分深度卷积和分离卷积&#x2F;下采样层来加速模型速度。results:对多个图像恢复 datasets 进行了广泛的实验，发现该方法可以降低运行时间，同时提高 PSNR 和 SSIM。具体来说，运行时间可以降低到最多 13%，参数数量可以减少到最多 23%，而 PSNR 和 SSIM 则可以提高。代码源代码可以在 \href{<a target="_blank" rel="noopener" href="https://github.com/xiangyu8/MOFA%7D%7Bhttps://github.com/xiangyu8/MOFA%7D">https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}</a> 上找到。<details>
<summary>Abstract</summary>
Image restoration aims to restore high-quality images from degraded counterparts and has seen significant advancements through deep learning techniques. The technique has been widely applied to mobile devices for tasks such as mobile photography. Given the resource limitations on mobile devices, such as memory constraints and runtime requirements, the efficiency of models during deployment becomes paramount. Nevertheless, most previous works have primarily concentrated on analyzing the efficiency of single modules and improving them individually. This paper examines the efficiency across different layers. We propose a roadmap that can be applied to further accelerate image restoration models prior to deployment while simultaneously increasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index). The roadmap first increases the model capacity by adding more parameters to partial convolutions on FLOPs non-sensitive layers. Then, it applies partial depthwise convolution coupled with decoupling upsampling/downsampling layers to accelerate the model speed. Extensive experiments demonstrate that our approach decreases runtime by up to 13% and reduces the number of parameters by up to 23%, while increasing PSNR and SSIM on several image restoration datasets. Source Code of our method is available at \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model"><a href="#InverseSR-3D-Brain-MRI-Super-Resolution-Using-a-Latent-Diffusion-Model" class="headerlink" title="InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model"></a>InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12465">http://arxiv.org/abs/2308.12465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biomedai-ucsc/inversesr">https://github.com/biomedai-ucsc/inversesr</a></li>
<li>paper_authors: Jueqi Wang, Jacob Levman, Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, M. Jorge Cardoso, Razvan Marinescu</li>
<li>for: 提高低分辨率（LR）磁共振成像（MRI）图像的分辨率。</li>
<li>methods: 利用一个状态最佳的3D脑生成模型——潜在扩散模型（LDM）训练在UK BioBank上，以提高低分辨率磁共振成像图像的分辨率。</li>
<li>results: 验证了将LDM作为生成模型，可以帮助提高低分辨率磁共振成像图像的分辨率。<details>
<summary>Abstract</summary>
High-resolution (HR) MRI scans obtained from research-grade medical centers provide precise information about imaged tissues. However, routine clinical MRI scans are typically in low-resolution (LR) and vary greatly in contrast and spatial resolution due to the adjustments of the scanning parameters to the local needs of the medical center. End-to-end deep learning methods for MRI super-resolution (SR) have been proposed, but they require re-training each time there is a shift in the input distribution. To address this issue, we propose a novel approach that leverages a state-of-the-art 3D brain generative model, the latent diffusion model (LDM) trained on UK BioBank, to increase the resolution of clinical MRI scans. The LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details>
<details>
<summary>摘要</summary>
高分辨率（HR）MRI扫描从研究级医疗机构获得的数据提供了精确的组织信息。然而，日常临床MRI扫描通常是低分辨率（LR），并且因为扫描参数的调整而具有不同的对比度和空间分辨率。为解决这个问题，我们提议了一种新的方法，利用了UK BioBank上训练的状态艺术3D脑生成模型（LDM），以提高临床MRI扫描的分辨率。LDM acts as a generative prior, which has the ability to capture the prior distribution of 3D T1-weighted brain MRI. Based on the architecture of the brain LDM, we find that different methods are suitable for different settings of MRI SR, and thus propose two novel strategies: 1) for SR with more sparsity, we invert through both the decoder of the LDM and also through a deterministic Denoising Diffusion Implicit Models (DDIM), an approach we will call InverseSR(LDM); 2) for SR with less sparsity, we invert only through the LDM decoder, an approach we will call InverseSR(Decoder). These two approaches search different latent spaces in the LDM model to find the optimal latent code to map the given LR MRI into HR. The training process of the generative model is independent of the MRI under-sampling process, ensuring the generalization of our method to many MRI SR problems with different input measurements. We validate our method on over 100 brain T1w MRIs from the IXI dataset. Our method can demonstrate that powerful priors given by LDM can be used for MRI reconstruction.
</details></li>
</ul>
<hr>
<h2 id="HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration"><a href="#HNAS-reg-hierarchical-neural-architecture-search-for-deformable-medical-image-registration" class="headerlink" title="HNAS-reg: hierarchical neural architecture search for deformable medical image registration"></a>HNAS-reg: hierarchical neural architecture search for deformable medical image registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12440">http://arxiv.org/abs/2308.12440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiong Wu, Yong Fan</li>
<li>for: 这篇论文旨在找到最佳的深度学习模型，用于静止医疗影像注册。</li>
<li>methods: 这篇论文使用了一个层次 NAS 框架（HNAS-Reg），包括了 convolutional 操作搜索和网络架构搜索，以找到最佳的网络架构。实际上，这篇论文还使用了一个 partial channel 策略，以减少计算负担和内存限制。</li>
<li>results: 实验结果显示，提案方法可以建立一个具有改善医疗影像注册精度和减少模型大小的深度学习模型，较比一个传统方法和两个无监督学习方法来得好。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been widely used to build deep learning models for medical image registration, but manually designed network architectures are not necessarily optimal. This paper presents a hierarchical NAS framework (HNAS-Reg), consisting of both convolutional operation search and network topology search, to identify the optimal network architecture for deformable medical image registration. To mitigate the computational overhead and memory constraints, a partial channel strategy is utilized without losing optimization quality. Experiments on three datasets, consisting of 636 T1-weighted magnetic resonance images (MRIs), have demonstrated that the proposal method can build a deep learning model with improved image registration accuracy and reduced model size, compared with state-of-the-art image registration approaches, including one representative traditional approach and two unsupervised learning-based approaches.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在医疗影像注册领域广泛使用深度学习模型，但是人工设计的网络架构可能并不是最佳的。这篇论文提出了一种层次 NAS 框架（HNAS-Reg），包括卷积操作搜索和网络架构搜索，以便确定最佳的网络架构 для 可变的医疗影像注册。为了减少计算负担和内存限制，该方法使用了部分通道策略而不失去优化质量。实验在三个数据集上，包括 636 个 T1 束缚 magnetic resonance imaging（MRI）图像，表明该方法可以建立一个具有改进的图像注册精度和减少的模型大小的深度学习模型，比于现有的图像注册方法，包括一种传统方法和两种无监督学习方法。
</details></li>
</ul>
<hr>
<h2 id="Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach"><a href="#Reframing-the-Brain-Age-Prediction-Problem-to-a-More-Interpretable-and-Quantitative-Approach" class="headerlink" title="Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach"></a>Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12416">http://arxiv.org/abs/2308.12416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Gianchandani, Mahsa Dibaji, Mariana Bento, Ethan MacDonald, Roberto Souza</li>
<li>for: 这研究的目的是使用深度学习模型从核磁共振成像图像中预测大脑年龄，并提供更加可解的结果。</li>
<li>methods: 该研究使用了一种image-to-image regression方法，将大脑年龄预测问题转化为每个大脑细胞voxel的预测问题，并与全局预测模型和相关的涉及度地图进行比较。</li>
<li>results: 结果表明，voxel-wise预测模型比全局预测模型更加可解，因为它们提供了脑部年龄变化的空间信息，并且具有量化的优点。<details>
<summary>Abstract</summary>
Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comparable across different samples. In this work, we reframe the age prediction problem from MR images to an image-to-image regression problem where we estimate the brain age for each brain voxel in MR images. We compare voxel-wise age prediction models against global age prediction models and their corresponding saliency maps. The results indicate that voxel-wise age prediction models are more interpretable, since they provide spatial information about the brain aging process, and they benefit from being quantitative.
</details>
<details>
<summary>摘要</summary>
深度学习模型已经达到了评估大脑年龄的国际标准Result，这是重要的大脑健康指标，从核磁共振（MR）图像中获取。然而，大多数这些模型只提供全局年龄预测，并且使用技术，如吸引力地图来解释其结果。这些吸引力地图会高亮输入图像中对模型预测有影响的区域，但它们很难被解释，而且吸引力地图值不可比较。在这项工作中，我们将MR图像中大脑年龄预测问题重新定义为图像到图像回归问题，我们预测每个大脑磁共振 voxel 的年龄。我们比较了 voxel-wise 年龄预测模型和全局年龄预测模型以及其相应的吸引力地图。结果表明，voxel-wise 年龄预测模型更加可读性高，因为它们提供了空间信息关于大脑年龄过程，并且它们受益于量化。
</details></li>
</ul>
<hr>
<h2 id="SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation"><a href="#SPPNet-A-Single-Point-Prompt-Network-for-Nuclei-Image-Segmentation" class="headerlink" title="SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation"></a>SPPNet: A Single-Point Prompt Network for Nuclei Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12231">http://arxiv.org/abs/2308.12231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xq141839/sppnet">https://github.com/xq141839/sppnet</a></li>
<li>paper_authors: Qing Xu, Wenwei Kuang, Zeyu Zhang, Xueyao Bao, Haoran Chen, Wenting Duan</li>
<li>for: 本研究旨在提出一种单点提示网络（SPPNet），用于核体图像分割。</li>
<li>methods: 我们将原始图像Encoder被替换为轻量级视transformer，并添加了一个有效的 convolutional block，以提高图像下降的semantic信息。我们还提出了基于 Gaussian kernel的新的点抽象方法。</li>
<li>results: 我们在MoNuSeg-2018 dataset上评估了SPPNet，结果表明它在比较较快的训练速度和较低的计算成本下，可以达到比较高的性能水平。相比之下，SPPNet比segment anything模型快了约20倍，仅需要一个点集，这更加合理 для临床应用。<details>
<summary>Abstract</summary>
Image segmentation plays an essential role in nuclei image analysis. Recently, the segment anything model has made a significant breakthrough in such tasks. However, the current model exists two major issues for cell segmentation: (1) the image encoder of the segment anything model involves a large number of parameters. Retraining or even fine-tuning the model still requires expensive computational resources. (2) in point prompt mode, points are sampled from the center of the ground truth and more than one set of points is expected to achieve reliable performance, which is not efficient for practical applications. In this paper, a single-point prompt network is proposed for nuclei image segmentation, called SPPNet. We replace the original image encoder with a lightweight vision transformer. Also, an effective convolutional block is added in parallel to extract the low-level semantic information from the image and compensate for the performance degradation due to the small image encoder. We propose a new point-sampling method based on the Gaussian kernel. The proposed model is evaluated on the MoNuSeg-2018 dataset. The result demonstrated that SPPNet outperforms existing U-shape architectures and shows faster convergence in training. Compared to the segment anything model, SPPNet shows roughly 20 times faster inference, with 1/70 parameters and computational cost. Particularly, only one set of points is required in both the training and inference phases, which is more reasonable for clinical applications. The code for our work and more technical details can be found at https://github.com/xq141839/SPPNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/24/eess.IV_2023_08_24/" data-id="clmjn91qr00hb0j887pot9ffe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.AI_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.AI_2023_08_23/">cs.AI - 2023-08-23 20:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No"><a href="#CLIPN-for-Zero-Shot-OOD-Detection-Teaching-CLIP-to-Say-No" class="headerlink" title="CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No"></a>CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12213">http://arxiv.org/abs/2308.12213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/clipn">https://github.com/xmed-lab/clipn</a></li>
<li>paper_authors: Hualiang Wang, Yi Li, Huifeng Yao, Xiaomeng Li</li>
<li>For: This paper focuses on developing a novel method for zero-shot out-of-distribution (OOD) detection using CLIP, a text-to-image model. The goal is to equip CLIP with the ability to distinguish between in-distribution (ID) and OOD samples using positive-semantic prompts and negation-semantic prompts.* Methods: The proposed method, called CLIP saying no (CLIPN), utilizes a novel learnable no prompt and a no text encoder to capture negation semantics within images. Two loss functions are introduced to teach CLIPN to associate images with no prompts, enabling it to identify unknown samples. Additionally, two threshold-free inference algorithms are proposed for OOD detection.* Results: The proposed CLIPN method, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. The code is available on GitHub.<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying no (CLIPN), which empowers the logic of saying no within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable no prompt and a no text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with no prompts, thereby enabling it to identify unknown samples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection by utilizing negation semantics from no prompts and the text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD detection task demonstrate that CLIPN, based on ViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% in terms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPN can serve as a solid foundation for effectively leveraging CLIP in downstream OOD tasks. The code is available on https://github.com/xmed-lab/CLIPN.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION (OOD) 检测指的是在 IN-DISTRIBUTION (ID) 数据集上训练模型，以判断输入图像来自未知类。针对这问题，各种 OOD 检测方法已经得到了广泛的投入，其中一些基于卷积神经网络，一些基于 transformers。然而，驱动 CLIP 的零shot OOD 检测方法却受到了更少的关注。本文提出了一种新的方法，即 CLIP 说不 (CLIPN)，该方法通过帮助 CLIP 内部的逻辑分别 ID 和 OOD 样本。我们的关键动机是让 CLIP 能够通过正面 semantics 和否定 semantics 来分辨 ID 和 OOD 样本。具体来说，我们设计了一个可学习的 no 提示和一个 no 文本编码器，以捕捉图像中的否定 semantics。然后，我们引入了两个损失函数：图像文本二进制对立损失和文本 semantics 对立损失，以教 CLIPN 将图像与 no 提示相关联，从而让它能够识别未知样本。此外，我们提出了两种无阈值的推理算法，以利用 no 提示和文本编码器来进行 OOD 检测。实验结果表明，基于 ViT-B-16 的 CLIPN 在 9 个标准数据集（3 ID 数据集和 6 OOD 数据集）上的 OOD 检测任务中，与 7 种常用算法相比，至少提高了 2.34% 和 11.64% 的 AUROC 和 FPR95。我们的 CLIPN 可以作为一个可靠的基础，用于有效地利用 CLIP 在下游 OOD 任务中。代码可以在 https://github.com/xmed-lab/CLIPN 上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies"><a href="#Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies" class="headerlink" title="Learning to Learn Financial Networks for Optimising Momentum Strategies"></a>Learning to Learn Financial Networks for Optimising Momentum Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12212">http://arxiv.org/abs/2308.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong</li>
<li>for: 这篇论文旨在提供一种新型的风险豁免，利用金融网络中资产之间的连接来预测未来的回报。</li>
<li>methods: 该论文提出了一种名为L2GMOM的机器学习框架，该框架同时学习金融网络和股票投资策略，以提高股票投资的性能和风险控制。</li>
<li>results: 根据64个连续Future合约的回报测试，L2GMOM模型在20年时间段内能够显著提高股票投资的盈利率和风险控制，Sharpe比率为1.74。<details>
<summary>Abstract</summary>
Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 64 continuous future contracts demonstrates a significant improvement in portfolio profitability and risk control, with a Sharpe ratio of 1.74 across a 20-year period.
</details>
<details>
<summary>摘要</summary>
网络势头提供了一种新型的风险偏好，利用财务网络中资产之间的关系预测未来的回报。然而，现有的金融网络建构过程受到高优质数据库和金融专业知识的限制，导致小型和学术机构的访问受到限制。此外，传统方法将网络建构和投资策略优化视为两个独立的任务，可能会降低投资策略的优化性。为解决这些挑战，我们提出了L2GMOM，一种结束到终点的机器学习框架，同时学习金融网络和优化交易信号。L2GMOM的模型是一种高度可解释的前进卷积神经网络，由算法抽象而来。L2GMOM是灵活的，可以使用多种损失函数来优化股票表现，例如负方均值系数。在64个连续未来合约的回测中，L2GMOM显示出了 significiant提高投资收益和风险控制，负方均值系数为20年期间1.74。
</details></li>
</ul>
<hr>
<h2 id="Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques"><a href="#Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques" class="headerlink" title="Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques"></a>Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie A. Neubauer, Radu Grosu</li>
<li>for: 这个论文旨在统一地present deterministic和统计 lagrange 验证技术，以量化时间连续过程中行为的稳定性。</li>
<li>methods: 这个论文使用了 LRT-NG、SLR 和 GoTube 算法来构建紧距盒，即在给定时间范围内可达的状态的上下文。这些算法提供了确定性和统计性的保证。</li>
<li>results: 实验表明，lagrange 技术在比较于 LRT、Flow* 和 CAPD 的情况下表现更优异，并用于不同的时间连续模型的稳定性分析。<details>
<summary>Abstract</summary>
This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.
</details>
<details>
<summary>摘要</summary>
In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation to compute the radius of an ellipsoid that over-approximates the set of reachable states. In SLR and GoTube, the Lipschitz constants are used to compute local balls around samples, which are needed to calculate the probability of finding an upper bound of the true maximum perturbation at each timestep. The authors demonstrate the superior performance of Lagrangian techniques compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models through experiments.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning"><a href="#Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning" class="headerlink" title="Unsupervised anomalies detection in IIoT edge devices networks using federated learning"></a>Unsupervised anomalies detection in IIoT edge devices networks using federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niyomukiza Thamar, Hossam Samy Elsaid Sharara</li>
<li>for:  solves the privacy problem for IoT&#x2F; IIoT devices that held sensitive data for the owners.</li>
<li>methods:  Federated learning(FL) as a distributed machine learning approach, specifically the Fedavg algorithm.</li>
<li>results:  Almost the same as the centralized machine learning approach, but with the added benefit of addressing privacy concerns.Here’s the simplified Chinese text for the three points:</li>
<li>for: 解决 IoT&#x2F; IIoT 设备上的敏感数据所有者隐私问题。</li>
<li>methods: 分布式机器学习方法（Federated Learning，FL），特别是 Fedavg 算法。</li>
<li>results: 与中央机器学习方法相似，但具有隐私保护的优点。<details>
<summary>Abstract</summary>
In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.
</details>
<details>
<summary>摘要</summary>
在许多物联网设备之间的连接中，通常需要将数据传输到中央服务器进行机器学习模型的训练，但有些 propietarios 对于数据安全问题感到担忧。 Federated learning（FL）作为分布式机器学习方法，在设备上进行机器学习模型的训练，不需要将数据传输到服务器。 Fedavg 是 FL 算法之一，允许在训练过程中将模型复制到参与设备上。这些设备可以随机选择，并且可以在训练过程中被终止。获得的模型将被发送到协调服务器，并与其他完成训练的设备的模型进行平均值。这种方法可以解决物联网/IIoT 设备持有敏感数据的所有者隐私问题。在这篇论文中，我们利用 FL 的优点，并在最新的数据集上实现 Fedavg 算法。结果与中央机器学习方法的结果几乎相同。我们还评估了 Fedavg 的一些缺点，如训练过程中不参与的设备会导致不公平性。这可能导致 IoT/IIoT 设备上开发的投入检测系统中出现高比例的假警示。因此，我们在未来工作中将提出和实现公平的 Fedavg 算法。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence"><a href="#Evaluation-of-Faithfulness-Using-the-Longest-Supported-Subsequence" class="headerlink" title="Evaluation of Faithfulness Using the Longest Supported Subsequence"></a>Evaluation of Faithfulness Using the Longest Supported Subsequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12157">http://arxiv.org/abs/2308.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Mittal, Timo Schick, Mikel Artetxe, Jane Dwivedi-Yu</li>
<li>for: evaluating the trustworthiness of machine-generated text, specifically in tasks such as summarization and question-answering</li>
<li>methods: introducing a novel approach called the Longest Supported Subsequence (LSS) to compute the faithfulness of machine-generated text, and finetuning a model to generate LSS using a new human-annotated dataset</li>
<li>results: demonstrating that the proposed metric correlates better with human ratings than prevailing state-of-the-art metrics, with an 18% enhancement in faithfulness on the dataset, and consistently outperforming other metrics on a summarization dataset across six different models, as well as comparing several popular Large Language Models (LLMs) for faithfulness using this metric.<details>
<summary>Abstract</summary>
As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarization dataset across six different models. Finally, we compare several popular Large Language Models (LLMs) for faithfulness using this metric. We release the human-annotated dataset built for predicting LSS and our fine-tuned model for evaluating faithfulness.
</details>
<details>
<summary>摘要</summary>
“随着越来越进步的语言模型出现，它们的可靠性成为一个关键的问题，特别是在摘要和问答中。确保它们的回答是基于上下文的，并不是单纯地根据语言模型的假设，是一个具有挑战性的任务。在这篇论文中，我们提出了一种新的方法来评估机器生成的文本的可靠性，通过计算文本中最长的不连续子串，我们称之为“最长支持子串”（LSS）。我们使用了一个新的人类验证数据集，调整了一个模型以生成LSS，并导入了一个新的评估方法。我们示示了这些指标与人类评分更加相似，而且在摘要数据集上，我们的提案的指标与现有的指标相比，有18%的提升。我们的指标在六个不同的模型上的表现都与其他指标相比较高。最后，我们使用这个指标评估了一些流行的大型语言模型的可靠性。我们发布了我们建立的人类验证数据集和调整后的模型，以便用于评估可靠性。”
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals"><a href="#Multimodal-Latent-Emotion-Recognition-from-Micro-expression-and-Physiological-Signals" class="headerlink" title="Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals"></a>Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12156">http://arxiv.org/abs/2308.12156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangfei Zhang, Yifei Qian, Ognjen Arandjelovic, Anthony Zhu</li>
<li>for: 提高隐藏情感识别精度</li>
<li>methods:  combining微表情(ME)和生理信号(PS)，使用1D可分和混合深度卷积网络，标准化分布预测权重混合法，以及深度&#x2F;生理指导注意模块</li>
<li>results: 提高比较方法的表现<details>
<summary>Abstract</summary>
This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth/physiology guided attention modules for multimodal learning. Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了通过多模式数据的汇入来提高潜在情绪识别精度，特点在微表情（ME）和生理信号（PS）之间。提议的方法框架组合了ME和PS，包括一个可分离的深度wise嵌入网络，一种标准化正态分布权重Feature合并方法，以及深度/生理学引导注意模块 для多模式学习。实验结果显示，提议的方法在比较方法上表现出色，权重合并方法和引导注意模块都对精度提高做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
<li>for: 本研究探讨了基于生成模型的会员推测攻击（MIA），并提出了一种基于概率波动的会员推测方法（PFAMI）。</li>
<li>methods: PFAMI 基于生成模型中的记忆效应，通过分析生成记录的概率波动来推断会员性。</li>
<li>results: 对多种生成模型和数据集进行了广泛的实验，显示 PFAMI 可以提高攻击成功率（ASR）约27.9%  comparing with 基准值。<details>
<summary>Abstract</summary>
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.
</details>
<details>
<summary>摘要</summary>
机制成员攻击（MIA）可以决定一个记录是否在机器学习模型的训练集中，通过询问模型。过往的研究主要集中在传统的分类模型上，而现在的研究则开始对生成模型进行应用。我们的研究显示，现有的生成模型MIA主要依赖目标模型的过滤。然而，过滤可以使用多种正规化技术来避免，而现有的MIA实际上却表现不佳。不同的过滤，记忆是深度学习模型所需的一种基本现象，它会使模型在实际应用中表现更好。记忆在生成模型中导致生成记录的概率分布增加，因此我们提出了一个概率波动评估机制成员攻击（PFAMI），这是一种黑盒子MIA，可以通过分析givens record的概率波动来决定成员。我们进行了多种生成模型和数据集的广泛实验，结果显示，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基eline。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Change-Detection-for-the-Romanian-Language"><a href="#Semantic-Change-Detection-for-the-Romanian-Language" class="headerlink" title="Semantic Change Detection for the Romanian Language"></a>Semantic Change Detection for the Romanian Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12131">http://arxiv.org/abs/2308.12131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ds4ai-upb/semanticchange-ro">https://github.com/ds4ai-upb/semanticchange-ro</a></li>
<li>paper_authors: Ciprian-Octavian Truică, Victor Tudose, Elena-Simona Apostol</li>
<li>for: 本研究旨在分析语言变化的自动Semantic Change Methods，以及在实际英语和罗马尼亚语 Corporas中的应用。</li>
<li>methods: 本研究使用Word2Vec和ELMo两种静态和 контекстual word embedding模型，并对这两种模型在英语dataset上进行评估。然后，对罗马尼亚语 dataset进行实验，并强调不同的semantic change aspect，如意义获得和丢失。</li>
<li>results: 实验结果显示，取决于 corpus，模型选择和评估距离是检测semantic change的重要因素。<details>
<summary>Abstract</summary>
Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.
</details>
<details>
<summary>摘要</summary>
自动 semantic change 方法试图通过分析在时间上的使用情况来识别词语的意义变化。在这篇论文中，我们分析了不同的策略来创建静态和 контекст word embedding 模型，即 Word2Vec 和 ELMo，在实际的英语和罗马尼亚数据集上。为了测试我们的管道和确定模型的表现，我们首先评估了这两种 word embedding 模型在英语数据集（SEMEVAL-CCOHA）上。接着，我们将注意力集中在罗马尼亚数据集上，并强调不同的 semantics 变化方面，如 meaning acquisition 和 loss。实验结果表明，具体取决于 corpus，最重要的因素是选择模型和计算分数的距离。
</details></li>
</ul>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
<li>for: 这种研究旨在探讨细化图像分类任务中背景引起的偏见问题，以及如何使用masking策略来 Mitigate这种偏见。</li>
<li>methods: 这些研究使用了标准的Convolutional Neural Network (CNN)和Vision Transformers (ViT)模型，并评估了两种masking策略来解决背景引起的偏见问题。</li>
<li>results: 研究发现，使用这两种masking策略可以提高模型对不同背景的抗干扰性能，特别是在使用GAP-Pooled Patch token-based classification和 early masking的情况下。<details>
<summary>Abstract</summary>
Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.
</details>
<details>
<summary>摘要</summary>
模型 для细化图像分类任务，其中一些类别之间的差别可能很小，而每个类别的样本数也很少，容易受到背景相关的偏见。为了更深入地理解这个重要问题，我们的研究探讨了背景引起的偏见对细化图像分类的影响，并评估了标准的背景模型，如卷积神经网络（CNN）和视Transformers（ViT）。我们研究了两种遮盾策略来减轻背景引起的偏见：早期遮盾，即在输入图像水平上移除背景信息，以及晚期遮盾，即在高级空间特征水平上选择性地遮盾背景相关的特征。我们进行了广泛的实验，评估不同遮盾策略对CNN和ViT模型的影响，尤其是对于不同的背景。结果显示，我们所提出的两种遮盾策略都能提高对于不同背景的性能，而早期遮盾一直保持最好的OOD性能。另外，一种基于GAP-Pooled Patch token的ViT变体，结合早期遮盾，达到了最高的OOD Robustness。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient"><a href="#Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient" class="headerlink" title="Quantifying degeneracy in singular models via the learning coefficient"></a>Quantifying degeneracy in singular models via the learning coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12108">http://arxiv.org/abs/2308.12108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edmundlth/scalable_learning_coefficient_with_sgld">https://github.com/edmundlth/scalable_learning_coefficient_with_sgld</a></li>
<li>paper_authors: Edmund Lau, Daniel Murfet, Susan Wei</li>
<li>for: This paper is written to explore the concept of degeneracy in deep neural networks (DNN) and to develop a method for quantifying the degree of degeneracy using a quantity called the “learning coefficient”.</li>
<li>methods: The paper uses singular learning theory and stochastic gradient Langevin dynamics to develop a computationally scalable approximation of the localized learning coefficient.</li>
<li>results: The paper demonstrates the accuracy of the proposed approach in low-dimensional models with known theoretical values, and shows that the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. Additionally, the paper demonstrates the ability of the local learning coefficient to reveal the inductive bias of stochastic optimizers for more or less degenerate critical points using an experiment on the MNIST dataset.<details>
<summary>Abstract</summary>
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments"><a href="#Out-of-the-Cage-How-Stochastic-Parrots-Win-in-Cyber-Security-Environments" class="headerlink" title="Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments"></a>Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12086">http://arxiv.org/abs/2308.12086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Rigaki, Ondřej Lukáš, Carlos A. Catania, Sebastian Garcia</li>
<li>for: This paper focuses on using pre-trained language models (LLMs) as agents in cybersecurity network environments for sequential decision-making processes.</li>
<li>methods: The authors propose using pre-trained LLMs as attacking agents in two reinforcement learning environments and compare their performance to state-of-the-art agents and human testers.</li>
<li>results: The LLM agents demonstrate similar or better performance than state-of-the-art agents in most scenarios and configurations, and the best LLM agents perform similarly to human testers without any additional training. This suggests that LLMs have the potential to efficiently address complex decision-making tasks within cybersecurity.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文探讨了使用预训练语言模型（LLM）作为网络安全环境中的决策代理。</li>
<li>methods: 作者们提议使用预训练LLM作为两个强化学习环境中的攻击者，并与当前最佳代理进行比较。</li>
<li>results: LLM代理在大多数情况下和配置下表现相当或更好于当前最佳代理，并且最佳LLM代理在没有任何额外训练的情况下与人工测试人员表现相当。这表明LLM有可能高效地解决网络安全中的复杂决策问题。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have gained widespread popularity across diverse domains involving text generation, summarization, and various natural language processing tasks. Despite their inherent limitations, LLM-based designs have shown promising capabilities in planning and navigating open-world scenarios. This paper introduces a novel application of pre-trained LLMs as agents within cybersecurity network environments, focusing on their utility for sequential decision-making processes.   We present an approach wherein pre-trained LLMs are leveraged as attacking agents in two reinforcement learning environments. Our proposed agents demonstrate similar or better performance against state-of-the-art agents trained for thousands of episodes in most scenarios and configurations. In addition, the best LLM agents perform similarly to human testers of the environment without any additional training process. This design highlights the potential of LLMs to efficiently address complex decision-making tasks within cybersecurity.   Furthermore, we introduce a new network security environment named NetSecGame. The environment is designed to eventually support complex multi-agent scenarios within the network security domain. The proposed environment mimics real network attacks and is designed to be highly modular and adaptable for various scenarios.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）在多种自然语言处理任务中得到了广泛的推广，包括文本生成、摘要和各种自然语言处理任务。尽管它们有自然的限制，但LLM基本设计在开放世界enario中的规划和导航方面表现了扎实的能力。本文介绍了一种使用预训练LLM作为网络安全环境中的代理人，关注它们在顺序决策过程中的使用。我们提出了一种方法，其中预训练LLM被用作攻击者在两个循环学习环境中。我们的提议代理人在大多数情况下和现有EPisode数千个话的代理人之间表现相似或更好。此外，我们的最佳LLM代理人在没有任何额外训练过程的情况下与人类测试者的性能相似。这种设计高亮了LLM在网络安全中的潜在能力。此外，我们介绍了一个新的网络安全环境名为NetSecGame。该环境旨在最终支持复杂多代理人场景在网络安全领域。我们的设计模仿了实际网络攻击，并设计为高度可组合和可调整的多种场景。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-RNN-Gradients-through-Pre-training"><a href="#Stabilizing-RNN-Gradients-through-Pre-training" class="headerlink" title="Stabilizing RNN Gradients through Pre-training"></a>Stabilizing RNN Gradients through Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12075">http://arxiv.org/abs/2308.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Herranz-Celotti, Jean Rouat<br>for: This paper aims to improve the stability of deep neural networks during training, particularly for complex networks that are difficult to analyze analytically.methods: The authors propose a new approach called the Local Stability Condition (LSC) to stabilize deep neural networks. They extend known stability theories to encompass a broader family of deep recurrent networks and propose a new initialization scheme that gives a weight of a half to the time and depth contributions to the gradient.results: The authors confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. Their approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.<details>
<summary>Abstract</summary>
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.
</details>
<details>
<summary>摘要</summary>
多种学习理论建议防止梯度变异的束缚增长，以稳定和改进训练。通常，这些分析是在具有数学 tractability 的批量化神经网络或单层循环神经网络上进行的。然而，这项研究表明，在神经网络太复杂以至于无法进行分析初始化时，可以预训练网络到地方稳定性。此外，我们扩展了已知稳定性理论，以覆盖更广泛的深度循环神经网络家族，不需要对数据和参数分布做出过多的假设。我们称之为地方稳定条件（LSC）。我们的调查表明，经典的格洛罗特、和合理初始化方案满足 LSC 当应用于批量化神经网络。然而，对深度循环神经网络进行分析，我们发现了一种新的加法式爆炸源，来自于计算梯度路径在深度和时间方向的矩阵中的计数。我们提出一种新的方法来缓解这个问题，即在计算梯度时，将时间和深度的贡献权重设为 0.5，而不是经典的 1.0。我们的实验结果表明，在 feed-forward 和循环神经网络中预训练满足 LSC 后，可以获得改进的最终性能。这项研究对深度学习领域的稳定性做出了贡献，并提供了一种可以稳定任何复杂性的神经网络的方法。我们的方法可以作为训练之前的额外步骤，或者作为在大量增强数据集上进行分析初始化的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning"><a href="#Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning" class="headerlink" title="Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning"></a>Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12069">http://arxiv.org/abs/2308.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Dang, Tao Shi, Zengjie Zhang, Wanxin Jin, Marion Leibold, Martin Buss</li>
<li>for: 本研究旨在提供一种基于最大熵逆激励学习（ME-IRL）方法的自动驾驶车辆（AV）驾驶风格识别方法，以便在多辆AV交通系统中评估风险和做出更合理的驾驶决策。</li>
<li>methods: 本研究使用ME-IRL方法来定义AV驾驶风格，并设计了一系列新的特征来捕捉AV对附近AV的反应。</li>
<li>results: 经过验证Using MATLAB实验和一个Off-the-shelf experiment，提出的方法可以准确地识别AV的驾驶风格，并且可以在多辆AV交通系统中提高安全性。<details>
<summary>Abstract</summary>
The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles from the demonstration trajectories generated by the Stochastic Model Predictive Control (SMPC) using a modified ME-IRL method with our newly proposed features. The proposed method is validated using MATLAB simulation and an off-the-shelf experiment.
</details>
<details>
<summary>摘要</summary>
自动驾驶车（AV）的驾驶方式指的是它如何行驶和与其他AV交互。在多辆自动驾驶车系统中，一个能够识别附近AV的驾驶方式的AV可以更加可靠地评估碰撞风险并做出更加合理的驾驶决策。然而，在文献中没有一个共识的自动驾驶车驾驶方式定义。尽管认为驾驶方式是在AV的轨迹中嵌入的，可以使用最大 entropy inverse reinforcement learning（ME-IRL）方法来识别它。然而，驾驶方式中一个重要指标，即AV如何 реаги于附近AV，并没有被完全包含在先前的ME-IRL方法中。在这篇论文中，我们定义了自动驾驶车的驾驶方式为一系列加权特征的成本函数。我们还设计了一些新的反应感知特征，以 capture AV的响应特性。然后，我们使用修改后的ME-IRL方法和我们新提出的特征来识别驾驶方式。我们的方法在MATLAB simulations和一个商业实验中得到了验证。
</details></li>
</ul>
<hr>
<h2 id="RemovalNet-DNN-Fingerprint-Removal-Attacks"><a href="#RemovalNet-DNN-Fingerprint-Removal-Attacks" class="headerlink" title="RemovalNet: DNN Fingerprint Removal Attacks"></a>RemovalNet: DNN Fingerprint Removal Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12319">http://arxiv.org/abs/2308.12319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grasses/removalnet">https://github.com/grasses/removalnet</a></li>
<li>paper_authors: Hongwei Yao, Zheng Li, Kunzhe Huang, Jian Lou, Zhan Qin, Kui Ren<br>for: 这个论文主要是研究DNNS的知识抽取和模型权利保护问题。methods: 作者提出了一种基于最小最大二重优化的DNNS模型抽取攻击方法，以逃脱模型权利验证。在下面优化中，作者将攻击者模型的特定指纹知识除掉，而在上面优化中，作者通过液化模型的总semantic知识来保持代理模型的性能。results: 作者通过对四种高级防御方法进行了广泛的实验，证明了RemovalNet的效果、效率和精度。特别是，与基准攻击方法相比，RemovalNet使用的计算资源减少了约85%。同时，创造的代理模型保持了高精度 послеDNNS模型抽取过程。<details>
<summary>Abstract</summary>
With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance. We conduct extensive experiments to evaluate the fidelity, effectiveness, and efficiency of the RemovalNet against four advanced defense methods on six metrics. The empirical results demonstrate that (1) the RemovalNet is effective. After our DNN fingerprint removal attack, the model distance between the target and surrogate models is x100 times higher than that of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2% (400 samples) of the substitute dataset and 1,000 iterations to conduct our attack. Besides, compared with advanced model stealing attacks, the RemovalNet saves nearly 85% of computational resources at most, (3) the RemovalNet achieves high fidelity that the created surrogate model maintains high accuracy after the DNN fingerprint removal process. Our code is available at: https://github.com/grasses/RemovalNet.
</details>
<details>
<summary>摘要</summary>
WITH 深度神经网络（DNN）性能显著提高，DNN已广泛应用于多个领域。因此，DNN模型成为了重要的财产，其知识产权得到了保护。然而，DNN指纹移除攻击的可能性和影响仍然是一个开放的问题。在这篇论文中，我们进行了首次全面的DNN指纹移除攻击调查。通常，DNN模型中的知识可以分为总Semantic和指纹特定知识。为此，我们提出了一种基于最小最大二级优化的DNN指纹移除攻击方法，名为RemovalNet，以避免模型所有权验证。lower-level优化设计移除指纹特定知识。而在upper-level优化中，我们通过液态热塑化将受害者模型的总Semantic知识萃取出来，以保持代理模型的性能。我们对四种高级防御方法进行了广泛的实验，并评估了RemovalNet的准确性、有效性和效率。实验结果显示了以下三点：1. RemovalNet是有效的。在我们的DNN指纹移除攻击后，模型之间的距离增加了100倍，比基eline攻击更高。2. RemovalNet是高效的。它只需使用400个样本和1000次迭代来进行攻击，而基eline攻击需要2000个样本和5000次迭代。此外，与高级模型盗取攻击相比，RemovalNet可以释放大约85%的计算资源。3. RemovalNet实现了高准确性，创建的代理模型在指纹移除过程后仍然保持高度准确。我们的代码可以在https://github.com/grasses/RemovalNet上下载。
</details></li>
</ul>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
<li>for: 这个论文主要用于探讨大语言模型在多模态场景中遵循指令的能力是如何强化的。</li>
<li>methods: 论文使用了两个阶段的训练方法：首先在图片和文本对的情况下进行预训练，然后在超参数数据上进行精度调整。</li>
<li>results: 论文通过提出一些metric来评估多模态指令数据的质量，并使用这些metric来自动选择高质量的视力语言数据，从而使用InstructionGPT-4超越了原始的MiniGPT-4在多种评估（如视觉问答、GPT-4首选）中的表现。<details>
<summary>Abstract</summary>
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.
</details>
<details>
<summary>摘要</summary>
多模态大语言模型通过两stage训练过程获得指令遵循能力：先于插入图像文本对的预训练，然后在指导视语言数据上进行精度调整。现有研究表明，大语言模型可以通过有限量高质量指令遵循数据来达到满意的结果。在本文中，我们介绍InstructionGPT-4，它是基于只有200个例子，相当于MiniGPT-4的整合数据中的6%的指令遵循数据进行精度调整。我们首先提出了评估多模态指令数据质量的多种指标，然后基于这些指标，我们提出了一种简单有效的数据选择器，可以自动将低质量的视语言数据滤除。通过使用这种方法，InstructionGPT-4在多种评估中（如视觉问答、GPT-4偏好）都超过了原始MiniGPT-4。总之，我们的发现表明，虽然只有少量但高质量的指令循数据，可以使多模态大语言模型生成更好的输出。
</details></li>
</ul>
<hr>
<h2 id="Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference"><a href="#Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference" class="headerlink" title="Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"></a>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, Minsoo Rhu</li>
<li>for: 大型自然语言模型（LLM）基于变换器的实现，以实现高性能。</li>
<li>methods: 使用 Mixture-of-Experts（MoE）架构，以适应大规模 LLM 的计算和存储需求。</li>
<li>results: 提出了 Pre-gated MoE 系统，可以有效地解决 conventional MoE 架构中的计算和存储挑战，同时保持高性能和减少 GPU 内存占用量。<details>
<summary>Abstract</summary>
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.
</details>
<details>
<summary>摘要</summary>
Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs a novel pre-gating function that alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE can improve performance, reduce GPU memory consumption, and maintain the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.In simplified Chinese, the text would be:大型语言模型（LLM） based on transformers  recent years  achieved significant progress, success driven by scaling up model size. However, the high computational and memory requirements of LLMs present unprecedented challenges. To address these challenges, Mixture-of-Experts（MoE） architecture was introduced, which can scale its model size without proportionally increasing its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts limit its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead.我们的 Pre-gated MoE 系统使用我们的算法-系统合理设计，有效地解决了传统 MoE 架构中的计算和内存挑战。Pre-gated MoE 使用我们的新的预 Gate 函数，解决了 sparse expert 动态 activation 的问题，使我们的提议的系统可以Addressing the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE can improve performance, reduce GPU memory consumption, and maintain the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers"><a href="#Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers" class="headerlink" title="Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers"></a>Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12065">http://arxiv.org/abs/2308.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli</li>
<li>for: 本研究提出了一种安全包装（SPROUT），用于检测和防止机器学习（ML）算法的错误分类。</li>
<li>methods: 该方法使用多个不确定度测量来检测输入和输出的不确定性，并在检测到错误分类时阻止输出的传播。</li>
<li>results: 实验表明，SPROUT可以准确地检测大量的错误分类，并在特定情况下检测所有错误分类。 SPROUT适用于 binary 和多类分类问题，包括图像和表格数据集。<details>
<summary>Abstract</summary>
Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）算法可能会预测错误的类别，导致错误分类。这是已知的一点，错误分类可能会带来整体系统的崩溃。这篇文章提议了“护皮”（SPROUT），它是通过多个不确定度测量来怀疑错误分类的一种安全包装。如果检测到错误分类，SPROUT会阻止分类器的输出传递到包含系统。这会使安全性受到改善，因为SPROUT将异常输入（错误分类）转化为数据漏洞失败，这可以轻松地在系统层面进行管理。SPROUT适用于二分类和多分类，包括图像和表格数据集。我们实验表明，SPROUT总能够检测大量超级vised分类器中的错误分类，并且在某些情况下可以检测所有错误分类。SPROUT的实现包括预训练包装，它公共可用，ready to deploy 需要最小的努力。
</details></li>
</ul>
<hr>
<h2 id="FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering"><a href="#FlexKBQA-A-Flexible-LLM-Powered-Framework-for-Few-Shot-Knowledge-Base-Question-Answering" class="headerlink" title="FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering"></a>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12060">http://arxiv.org/abs/2308.12060</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leezythu/flexkbqa">https://github.com/leezythu/flexkbqa</a></li>
<li>paper_authors: Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong, Ning Liu, Jianyong Wang</li>
<li>for: 提高KBQA模型在实际应用中的性能，尤其是在缺乏高质量annotated数据的情况下。</li>
<li>methods: 利用自动生成的程序，如SPARQL查询，和大型自然语言模型（LLMs）来address问题。采用自动生成的程序可以减少人工标注的努力，而LLMs可以将程序转换成自然语言问题。</li>
<li>results: 在GrailQA、WebQSP和KQA Pro等 benchmark上进行了广泛的实验，发现在几个shot和零shot情况下，FlexKBQA可以达到很高的性能，比超过所有基eline和even approaching supervised模型的性能，达到93%相对于彻底supervised模型的性能。<details>
<summary>Abstract</summary>
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of LLMs to enhance the entire framework. Consequently, FlexKBQA delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, FlexKBQA achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93% performance relative to the fully-supervised models. We posit that FlexKBQA represents a significant advancement towards exploring better integration of large and lightweight models. The code is open-sourced.
</details>
<details>
<summary>摘要</summary>
知识库问答（KBQA）是一项关键性的 yet 挑战性的任务，由于知识库中的维度多样性和用户提交的自然语言问题的多样性。尽管大多数 KBQA 模型在实际场景中表现不佳，这主要归结于缺乏高质量标注数据的问题。为了解决这个问题，我们引入 FlexKBQA，利用大型自然语言模型（LLMs）作为知识库程序翻译器，以解决几何shot KBQA 任务中的挑战。Specifically, FlexKBQA 使用自动生成算法来采样知识库中的多样程序，例如 SPARQL 查询，并将其转化为自然语言问题。这些人工生成的数据可以用来训练特殊的轻量级模型。此外，为了减少实际问题和人工标注数据之间的分布差异，FlexKBQA 引入执行引导自动训练方法，以便逐步利用无标注的用户问题进行自动训练。此外，我们还考虑了利用 LLMs 的内在逻辑能力来增强整个框架。通过广泛的实验在 GrailQA、WebQSP 和 KQA Pro 等平台上，我们发现在几何shot 和零shot enario下，FlexKBQA 可以很好地表现，与完全监督模型相当，达到了93% 的性能相对于完全监督模型。我们认为 FlexKBQA 代表了大量和轻量级模型更好的 интеграción的一个重要进展。代码开源。
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-Feedback-Propagation"><a href="#Layer-wise-Feedback-Propagation" class="headerlink" title="Layer-wise Feedback Propagation"></a>Layer-wise Feedback Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leander Weber, Jim Berend, Alexander Binder, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</li>
<li>for: 这篇论文旨在提出层 wise feedback propagation（LFP），一种基于解释的训练方法，通过分层扩散反馈来评估神经网络中每个连接的贡献，从而实现比 tradicional Gradient Descent 更高效的训练。</li>
<li>methods: LFP 使用层 wise relevance propagation（LRP）来分层扩散反馈，不需要计算梯度，从而避免了一些基于梯度的限制。LFP 可以在不同的模型和数据集上实现相似的性能。</li>
<li>results: 在论文中， authors 提供了 LFP 的理论和实验证明，并证明了它在不同的模型和数据集上的效果。LFP 可以在不同的应用中提高模型的训练效率，例如在 Step-function activated Spiking Neural Networks（SNNs）中进行训练，或者进行知识传递学习。<details>
<summary>Abstract</summary>
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出层 wise Feedback Propagation（LFP），一种基于解释的训练方法，使用层 wise Relevance Propagation（LRP）来为解决特定任务中的每个连接分配奖励。这与传统的梯度下降不同，梯度下降更新参数向估计损失最小值。LFP在模型中分配奖励信号，不需要梯度计算。它然后强化收到正面反馈的结构，而减少收到负面反馈的影响。我们 theoretically 和 empirically 证明 LFP 的 converges，并在不同模型和数据集上证明其效果。值得注意的是，LFP 可以超越一些相关的梯度基本方法的限制，如依赖于意义 derivatives。我们还 investigate 如何 extend LRP-rules 到 LFP，它们在训练中的效果，以及潜在应用，如训练无意义 derivatives 的模型，例如步函数激活的神经网络（SNNs），或者用于传输学习，以高效地利用现有的知识。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback"><a href="#Aligning-Language-Models-with-Offline-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Aligning Language Models with Offline Reinforcement Learning from Human Feedback"></a>Aligning Language Models with Offline Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12050">http://arxiv.org/abs/2308.12050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian Hu, Li Tao, June Yang, Chandler Zhou</li>
<li>For: This paper aims to align language models with human preferences using offline reinforcement learning from human feedback (RLHF) frameworks, without relying on online reinforcement learning techniques like Proximal Policy Optimization (PPO) that can be unstable and challenging to tune.* Methods: The authors propose using maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. They employ a loss function similar to supervised fine-tuning to ensure stable model training, and compare their methods with PPO and other Offline RLHF methods.* Results: The experimental results show that the DT alignment outperforms other Offline RLHF methods and is better than PPO, with a much lower computing resource requirement (around 12.3%) and a simpler machine learning system.<details>
<summary>Abstract</summary>
Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our methods ensure more stable model training than PPO with a simple machine learning system~(MLSys) and much fewer (around 12.3\%) computing resources. Experimental results demonstrate the DT alignment outperforms other Offline RLHF methods and is better than PPO.
</details>
<details>
<summary>摘要</summary>
学习人类偏好是语言模型（LM）效果服务的关键。过去的研究已经做出了可观的进步，通过使用人类反馈来跟进 instruction。然而，这些方法主要依赖于在线强化学习（RL）技术，如 proximal policy optimization（PPO），这些技术有unstable和难于调整的问题。另外，PPO需要复杂的分布式系统实现，这会阻碍大规模分布式训练的效率。在这种情况下，我们提出了一个偏好RLHF框架，用于不需要与RL环境交互的情况下，使语言模型与人类偏好相匹配。具体来说，我们explore maximum likelihood estimation（MLE）with filtering、reward-weighted regression（RWR）和Decision Transformer（DT）来对语言模型进行偏好调整。我们的方法使用一个类似于超vised fine-tuning的损失函数，以确保更稳定的模型训练，并且只需要相对较少的计算资源（约12.3%）。实验结果表明，DT调整超过其他Offline RLHF方法，并且比PPO更好。
</details></li>
</ul>
<hr>
<h2 id="Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation"><a href="#Towards-Privacy-Supporting-Fall-Detection-via-Deep-Unsupervised-RGB2Depth-Adaptation" class="headerlink" title="Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation"></a>Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12049">http://arxiv.org/abs/2308.12049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/1015206533/privacy_supporting_fall_detection">https://github.com/1015206533/privacy_supporting_fall_detection</a></li>
<li>paper_authors: Hejun Xiao, Kunyu Peng, Xiangsheng Huang, Alina Roitberg1, Hao Li, Zhaohui Wang, Rainer Stiefelhagen</li>
<li>for: 预防跌倒，提高健康监测的效果</li>
<li>methods: 利用深度感知器和RGB视频数据，通过域 adaptation进行跌倒检测</li>
<li>results: 实现了在不需要细致图像数据的情况下，使用RGB视频数据进行跌倒检测，并达到了最佳效果<details>
<summary>Abstract</summary>
Fall detection is a vital task in health monitoring, as it allows the system to trigger an alert and therefore enabling faster interventions when a person experiences a fall. Although most previous approaches rely on standard RGB video data, such detailed appearance-aware monitoring poses significant privacy concerns. Depth sensors, on the other hand, are better at preserving privacy as they merely capture the distance of objects from the sensor or camera, omitting color and texture information. In this paper, we introduce a privacy-supporting solution that makes the RGB-trained model applicable in depth domain and utilizes depth data at test time for fall detection. To achieve cross-modal fall detection, we present an unsupervised RGB to Depth (RGB2Depth) cross-modal domain adaptation approach that leverages labelled RGB data and unlabelled depth data during training. Our proposed pipeline incorporates an intermediate domain module for feature bridging, modality adversarial loss for modality discrimination, classification loss for pseudo-labeled depth data and labeled source data, triplet loss that considers both source and target domains, and a novel adaptive loss weight adjustment method for improved coordination among various losses. Our approach achieves state-of-the-art results in the unsupervised RGB2Depth domain adaptation task for fall detection. Code is available at https://github.com/1015206533/privacy_supporting_fall_detection.
</details>
<details>
<summary>摘要</summary>
“fall detection是健康监控中的重要任务，可以让系统发送警示，从而更快地对人员坠落时进行应对。然而，大多数先前的方法仅使用标准的RGB影像数据，这种细节意识敏感的监控具有重要的隐私问题。深度感知器，则可以更好地保持隐私，因为它们仅capture物体对感知器或相机的距离，排除颜色和 texture信息。在本文中，我们介绍了一个关于隐私支持的解决方案，让RGB模型在深度领域中可用并在试用时使用深度数据进行坠落探测。”“实现跨模式的坠落探测，我们提出了一个不需要 labels的RGB to Depth（RGB2Depth）跨模式领域适应方法。我们的提案包括一个中继领域模组，用于Feature Bridging，模组挑战数据的类型和大小，以及一个对于模组的挑战数据的多对多挑战数据。我们还使用了一个对于source和target领域的多对多挑战数据，以及一个新的适应式损失调整方法，以改善不同损失函数之间的协调。”“我们的方法在RGB2Depth领域适应任务中得到了state-of-the-art的结果。我们的代码可以在https://github.com/1015206533/privacy_supporting_fall_detection中找到。”
</details></li>
</ul>
<hr>
<h2 id="CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning"><a href="#CgT-GAN-CLIP-guided-Text-GAN-for-Image-Captioning" class="headerlink" title="CgT-GAN: CLIP-guided Text GAN for Image Captioning"></a>CgT-GAN: CLIP-guided Text GAN for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12045">http://arxiv.org/abs/2308.12045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lihr747/cgtgan">https://github.com/lihr747/cgtgan</a></li>
<li>paper_authors: Jiarui Yu, Haoran Li, Yanbin Hao, Bin Zhu, Tong Xu, Xiangnan He</li>
<li>for: The paper is written for improving image captioning without human-annotated image-caption pairs, using a text-only training paradigm and incorporating images into the training process.</li>
<li>methods: The paper proposes a CLIP-guided text GAN (CgT-GAN) that uses adversarial training and a CLIP-based reward to provide semantic guidance, and introduces a novel semantic guidance reward called CLIP-agg that aligns the generated caption with a weighted text embedding.</li>
<li>results: The paper shows that CgT-GAN outperforms state-of-the-art methods significantly across all metrics on three subtasks (ZS-IC, In-UIC, and Cross-UIC).Here’s the simplified Chinese text version of the three key information points:</li>
<li>for: 文章是为了提高无人注意图像描述的image captioning，使用文本单独训练 paradigm，并在训练过程中包含图像。</li>
<li>methods: 文章提出了一种基于CLIP的文本GAN（CgT-GAN），使用对抗训练和基于CLIP的奖励来提供语义指导，并引入了一种新的语义指导奖励called CLIP-agg。</li>
<li>results: 文章表明，CgT-GAN在三个任务（ZS-IC、In-UIC和Cross-UIC）上比州前方法显著出众，包括所有指标。<details>
<summary>Abstract</summary>
The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to "see" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at https://github.com/Lihr747/CgtGAN.
</details>
<details>
<summary>摘要</summary>
大规模的视觉语言预训练模型CLIP（Contrastive Language-Image Pre-training）在没有人类标注的场景下提高了图像描述。最新的CLIP基于的图像描述方法采用文本只训练 paradigm，即在共享 embedding 空间中重建文本。然而，这些方法受到训练/推断差距或巨大的存储要求的限制。因为在实际世界中可以轻松地获得图像，我们提出了CLIP引导的文本GAN（CgT-GAN），它将图像 inclusion 到训练过程中，使模型可以"看到"实际的视觉Modal。特别是，我们使用对抗训练来教育CgT-GAN模仿外部文本聚合体和CLIP基于的奖励来提供语义指导。描述生成器被同时激励基于描述自然度计算从GAN的探测器和CLIP基于的奖励模块计算的语义指导奖励。此外，我们还引入了一种新的语义指导奖励called CLIP-agg，它将生成的描述与权重文本embedding进行协调，通过对整个聚合体进行注意力聚集来实现。实验结果在三个SUB Task（ZS-IC、In-UIC和Cross-UIC）中显示，CgT-GAN具有与状态艺术方法相比明显的优势，在所有指标上出现显著提升。代码可以在https://github.com/Lihr747/CgtGAN 上找到。
</details></li>
</ul>
<hr>
<h2 id="A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks"><a href="#A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks" class="headerlink" title="A multiobjective continuation method to compute the regularization path of deep neural networks"></a>A multiobjective continuation method to compute the regularization path of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aamakor/continuation-method">https://github.com/aamakor/continuation-method</a></li>
<li>paper_authors: Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</li>
<li>for: 本文旨在提出一种高效的方法，以优化深度神经网络（DNN）的稀疏性和损失函数之间的衔接。</li>
<li>methods: 本文使用了一种基于多目标优化的算法，以 aproximate Pareto front 上的整个衔接。</li>
<li>results: 数据示出了该算法的高效性和通用性，并且可以在不同的梯度下进行数据的验证。此外，本文还证明了知道衔接路径可以帮助网络 Parametrization 得到更好的泛化性。<details>
<summary>Abstract</summary>
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）中的稀畴性是一个非常强地需求的特性，因为它确保了数学效率、提高模型解释性（由于更少的相关特征），并且提高了模型的稳定性。在线性机器学习方法基于的模型中，已经知道存在一个连接到最稀 Solution 的梯度路径，这个梯度路径被称为规regularization path。很近期，有一个首次尝试将这个概念扩展到 DNN 中，通过对 empirical loss 和稀畴性（$\ell^1$ 范数）作为两个矛盾的目标，解决 resulting 多目标优化问题。然而，由于 $\ell^1$ 范数的非滑坡性和参数的高数量，这种方法并不很有效从计算机科学的角度。为了解决这个限制，我们提出了一个算法，可以高效地 aproximate 整个 Pareto front 上的目标。我们通过 deterministic 和 Stochastic 梯度来进行数值示例。此外，我们还证明了知道规regularization path 可以提供一个良好的网络参数化。
</details></li>
</ul>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
<li>for: 这篇论文主要针对于大型预训语言模型（PLMs）的精致化训练进行优化，以减少训练和储存成本，特别是在大量下游任务时。</li>
<li>methods: 这篇论文提出了一种增量化对应（IncreLoRA）方法，将预训模组中的参数转换为可变的权重矩阵，以提高模组之间的通信。此外，这篇论文还提出了一些对LoRA的修正方法，以提高其效能。</li>
<li>results: 在GLUE测试集上，这篇论文的方法与基eline相比，具有更高的参数效率，特别是在资源不足的情况下。另外，这篇论文还展示了对LoRA的修正方法可以对模组之间的通信进行更好的控制。<details>
<summary>Abstract</summary>
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.
</details>
<details>
<summary>摘要</summary>
随着预训语言模型（PLM）的大小的增加，精细调整所有模型参数不是efficient，特别是当有大量下游任务时，会导致显著的训练和存储成本。许多参数精细调整（PEFT）approach已经提出，其中LoRA是一个代表性的方法，它在每个目标模块中注入可学习的排序矩阵。然而，LoRA忽略了参数在不同模块中的重要性。为解决这个问题，许多工作已经提出了对LoRA的剪枝。然而，在限制的训练条件下，剪枝后的参数矩阵的rankUpperBound仍然受到先前设置的值的影响。因此，我们提出了IncreLoRA，一种逐步分配参数的方法，它在训练过程中基于每个模块的重要性分数进行逐步添加可学习参数。这种方法与剪枝方法不同，它不受限于初始训练参数的数量，每个参数矩阵的rankUpperBound都高于同样的训练负担。我们在GLUE上进行了广泛的实验，结果表明我们的方法具有更高的参数效率，特别是在低资源设置下，我们的方法显著超过了基eline。我们的代码公开 disponibles。
</details></li>
</ul>
<hr>
<h2 id="PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine"><a href="#PREFER-Prompt-Ensemble-Learning-via-Feedback-Reflect-Refine" class="headerlink" title="PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine"></a>PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12033">http://arxiv.org/abs/2308.12033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zcrwind/prefer">https://github.com/zcrwind/prefer</a></li>
<li>paper_authors: Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang, Mingchen Cai</li>
<li>for: 提高 Large Language Model (LLM) 的表现，增强其能力。</li>
<li>methods: 提出了一种简单、通用、自动化的方法 named PREFER，通过反馈机制和迭代优化来提高 LLM 的表现。</li>
<li>results: 经过广泛的实验，我们的 PREFER 方法在多种任务上达到了 state-of-the-art 水平，超过了现有方法的表现。<details>
<summary>Abstract</summary>
As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.
</details>
<details>
<summary>摘要</summary>
为了更好地利用大语言模型（LLM）的能力，提问最近在多种复杂任务中表现出了无 precedent 的能力。为了进一步提高性能，提问ensemble 已经吸引了很多关注，以解决 LLM 的幻觉和不稳定性。然而，现有的方法通常采用两个阶段 paradigm，需要大量的手动努力来预先准备提问集，并且无法 direktly 优化不同的弱学习者。在这篇论文中，我们提出了一种简单、通用和自动的方法 named PREFER (提问组合学习 via 反馈反思改进)，以解决所提到的限制。具体来说，我们知道弱学习者在扩大时会关注困难的示例，PREFER 建立了反馈机制，以反思现有弱学习者的不足。基于这，LLM 需要自动生成新的提问，进行迭代改进。此外，为了增强提问效果评估的稳定性，我们提出了一种新的提问袋裹法，其包括前向和后向思考，比较有利于提问评估和权重计算在扩大中。我们的 EXPERIMENT 表明，我们的 PREFER 可以在多种任务中达到 estado 的表现，与当前最佳方法相比，差距非常大。我们的代码已经公开发布。
</details></li>
</ul>
<hr>
<h2 id="CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures"><a href="#CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures" class="headerlink" title="CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures"></a>CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Gherardini, Varun Ravi Varma, Karol Capala, Roger Woods, Jose Sousa</li>
<li>for: 本研究旨在提高安全分析的解释能力，为现代人工智能发展提供帮助。</li>
<li>methods: 本研究使用了CACTUS，一种可解释的人工智能工具，以提高安全分析的效果。CACTUS支持分类特征，保持特征的原始含义，提高内存使用率，并通过并行计算加速计算速度。</li>
<li>results: 本研究在应用于美洲矿业癌症和甲状腺癌0387数据集中展现出色的表现，并且可以显示每个类别中特征的频率和排名。<details>
<summary>Abstract</summary>
The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
</details>
<details>
<summary>摘要</summary>
大量数据的可用性正为现代人工智能发展提供了推动力。然而，对小数据集的解决方案存在实用和成本效益的挑战，尤其是深度学习模型的透明性问题。本文提出了一种名为“CACTUS”的全面抽象分类工具，用于提高安全分析。它能够有效地使用可解释人工智能，并且支持 categorical 特征，保持原始含义，优化内存使用情况，并通过并行计算加速计算。它可以在用户看到每个类别 attribute 的频率和排名它们的抑制力。它的性能被评估通过应用于美国威斯康星诊断乳腺癌和 thyroid0387 数据集。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 提高 GPT 类模型的准确性和效率，以便更好地满足不同场景中的需求。</li>
<li>methods: 采用了反馈学习，通过训练或使用规则来定义奖励模型，以便控制 GPT 类模型的生成长度。</li>
<li>results: 在 популяр的数据集 CNNDM 和 NYT 上实现了更高的描述精度和准确性。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose a prompt-based length control method using reinforcement learning with a trainable or rule-based reward model. Our method aims to achieve length-controlled generation in GPT-style LLMs, and experiments show that it significantly improves the accuracy of prompt-based length control for summarization tasks on popular datasets like CNNDM and NYT. We believe that this length-controllable ability has great potential in the era of LLMs.
</details></li>
</ul>
<hr>
<h2 id="A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning"><a href="#A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning" class="headerlink" title="A Scale-Invariant Task Balancing Approach for Multi-Task Learning"></a>A Scale-Invariant Task Balancing Approach for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12029">http://arxiv.org/abs/2308.12029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu</li>
<li>for: 提高多任务学习（MTL）中任务均衡的问题，以便同时学习多个相关任务并实现优秀表现。</li>
<li>methods: 提出了一种具有整数归一化特性的多任务学习方法（SI-MTL），通过对所有任务损失进行对数变换来保证损失水平的均衡，并通过SI-G方法对所有任务导数进行归一化，使所有任务导数具有同一个 максималь去向量范围。</li>
<li>results: 经过广泛的实验表明，SI-G方法能够有效地约束任务导数，而SI-MTL方法能够在多个 benchmark 数据集上达到领先的性能水平。<details>
<summary>Abstract</summary>
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL），一种同时学习多个相关任务的学习方法，在各个领域取得了很大成功。然而，任务均衡仍然是MTL中的主要挑战，因为任务损失/梯度的尺度差异常常导致性能下降。在这篇论文中，我们提出了一种减小任务均衡问题的扩展MTL方法（SI-MTL）。具体来说，SI-MTL包括一种对所有任务损失进行对数变换，以保证损失水平上的减小，以及一种梯度均衡方法SI-G，该方法将所有任务梯度 норmalizes到最大梯度 норма的同一个范围内。我们在多个标准数据集上进行了广泛的实验，并经常证明了SI-G的有效性和SI-MTL的状态之最性。
</details></li>
</ul>
<hr>
<h2 id="LKPNR-LLM-and-KG-for-Personalized-News-Recommendation-Framework"><a href="#LKPNR-LLM-and-KG-for-Personalized-News-Recommendation-Framework" class="headerlink" title="LKPNR: LLM and KG for Personalized News Recommendation Framework"></a>LKPNR: LLM and KG for Personalized News Recommendation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12028">http://arxiv.org/abs/2308.12028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuan-zw/lkpnr">https://github.com/xuan-zw/lkpnr</a></li>
<li>paper_authors: Chen hao, Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai</li>
<li>for: 提高新闻推荐系统的准确率，解决传统方法对复杂新闻文本的理解困难和长尾问题。</li>
<li>methods:  combining Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods, using LLMs’ powerful text understanding ability to generate news representations containing rich semantic information, and combining information about news entities and mining high-order structural information through multiple hops in KG.</li>
<li>results:  compared with various traditional models, the framework significantly improves the recommendation effect, and the successful integration of LLM and KG in the framework has established a feasible path for achieving more accurate personalized recommendations in the news field.<details>
<summary>Abstract</summary>
Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the "long tail problem" of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex news texts, we use LLMs' powerful text understanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities and mines high-order structural information through multiple hops in KG, thus alleviating the challenge of long tail distribution. Experimental results demonstrate that compared with various traditional models, the framework significantly improves the recommendation effect. The successful integration of LLM and KG in our framework has established a feasible path for achieving more accurate personalized recommendations in the news field. Our code is available at https://github.com/Xuan-ZW/LKPNR.
</details>
<details>
<summary>摘要</summary>
基于大语言模型和知识图的新闻个性化推荐系统是一个基本挑战。传统方法通常难以捕捉新闻文本中复杂的 semantic information，导致推荐结果不 satisfactory。另外，这些传统方法更适合有活跃用户行为的活跃用户。然而，它们无法有效解决“长尾问题”，即不活跃用户。为解决这些问题，本研究提出了一种新的通用框架，combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods。为了提高新闻文本中的semantic理解，我们使用 LLMs的强大文本理解能力生成新闻表示形式，具有丰富的semantic信息。此外，我们的方法结合新闻实体信息，通过多个层次结构信息在 KG 中挖掘高阶结构信息，从而缓解长尾分布的挑战。实验结果表明，与各种传统模型相比，我们的框架显著提高了推荐效果。我们成功地将 LLM 和 KG 集成到我们的框架中，建立了实现更高精度的个性化推荐在新闻领域的可行道路。我们的代码可以在 <https://github.com/Xuan-ZW/LKPNR> 中找到。
</details></li>
</ul>
<hr>
<h2 id="From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models"><a href="#From-Instructions-to-Intrinsic-Human-Values-–-A-Survey-of-Alignment-Goals-for-Big-Models" class="headerlink" title="From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models"></a>From Instructions to Intrinsic Human Values – A Survey of Alignment Goals for Big Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12014">http://arxiv.org/abs/2308.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie</li>
<li>for: 本研究旨在探讨现有工作中的各种Alignment Goals，以帮助确定最重要的目标。</li>
<li>methods: 本研究从两个角度 investigate了现有工作：一是对Alignment Goals的定义，二是对Alignment evaluation的研究。</li>
<li>results: 研究发现了三级别的Alignment Goals，并发现了目标转化从基本能力到价值观，这表明了可以利用内在人类价值作为Enhanced LLMs的Alignment goal。<details>
<summary>Abstract</summary>
Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.
</details>
<details>
<summary>摘要</summary>
大型模型，如大语言模型（LLMs），是通常在庞大数据上预训练的模型，不仅在多种任务上显示出较好的性能，而且具有emergent功能，与更小的模型不同。然而，大型模型与人类生活的日益相互 penetration可能会带来潜在的风险，可能会对社会造成严重的危害。因此，许多努力已经被做出，以使LMMs与人类更好地配合，使其更好地遵从用户的指令和满足人类的偏好。然而，`与何进行对齐'的问题尚未得到了完全的讨论，不当的对齐目标可能会倒退。在这篇论文中，我们进行了完整的对齐目标的检查，并跟踪它们的演化路径，以帮助identify最重要的目标。特别是，我们从两个视角 investigate existing work：对齐目标的定义和对齐评估。我们的分析覆盖了三级别的对齐目标，并显示了对齐目标的变化从基本能力到价值观，这表明了内置人类价值的可能性作为LLMs的对齐目标，以提高它们的性能。基于这些结果，我们进一步讨论了实现这种内置价值对齐的挑战，并提供了未来对big models的对齐研究的可用资源。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Noise-driven-Generative-Diffusion-Models"><a href="#Quantum-Noise-driven-Generative-Diffusion-Models" class="headerlink" title="Quantum-Noise-driven Generative Diffusion Models"></a>Quantum-Noise-driven Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12013">http://arxiv.org/abs/2308.12013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Parigi, Stefano Martina, Filippo Caruso</li>
<li>for: 这个论文旨在提出和讨论量子扩散模型的量子扩散模型，用于生成复杂的数据分布。</li>
<li>methods: 该论文使用机器学习技术实现生成模型，并利用量子随机过程中的偶极性、Entanglement和噪声来超越经典扩散模型的计算困难。</li>
<li>results: 该论文预计可以开拓新的量子感知或量子基于的生成扩散算法，用于解决经典任务，如数据生成&#x2F;预测，并具有广泛的实际应用，如气候预测、神经科学、交通流量分析和财务预测。<details>
<summary>Abstract</summary>
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation/prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting.
</details>
<details>
<summary>摘要</summary>
通过机器学习技术实现的生成模型是一种 poderoso工具，可以从 finite 数据样本中推断出复杂而未知的数据分布，生成新的合成数据。扩散模型是一种emerging框架，最近已经超越了生成对抗网络在创造合成文本和高质量图像方面的性能。在这里，我们提出并讨论了量子扩散模型的普适化，即利用量子噪声驱动的三种量子扩散生成模型，可以在真正的量子系统上进行实验。我们的想法是利用量子特有的非rivial相互作用，即准确性、耦合和噪声，以超越经典扩散模型的主要计算危机。因此，我们建议利用量子噪声不作为问题，而是作为非常有利的重要组分，以生成更复杂的概率分布，这些分布可能是经典计算不能表达，而量子处理器可能可以更高效地采样这些分布。因此，我们的结果预计将为新的量子激发或量子基于的生成扩散算法开拓出新的应用领域，从气候预测到神经科学，从交通流量分析到金融预测。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Representation-Learning-Across-Domains"><a href="#Trustworthy-Representation-Learning-Across-Domains" class="headerlink" title="Trustworthy Representation Learning Across Domains"></a>Trustworthy Representation Learning Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li</li>
<li>for: 这个论文的目的是提出一个可靠的表示学习框架，以适应实际应用场景中的跨domain问题。</li>
<li>methods: 该论文使用了四个概念，即Robustness、Privacy、Fairness和Explainability，以提供一个全面的文献复盘。</li>
<li>results: 该论文提出了一个基于这四个概念的信任worthy表示学习框架，并对现有方法进行了概括和分析。<details>
<summary>Abstract</summary>
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details>
<details>
<summary>摘要</summary>
Inspired by the principles of trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four key concepts: robustness, privacy, fairness, and explainability. This comprehensive literature review provides an overview of this research direction, including the details of the proposed trustworthy framework for representation learning across domains, a summary of existing methods that align with the four concepts, and insights and discussions on future research directions. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. We then provide a comprehensive overview of existing methods that align with the four concepts, including robustness, privacy, fairness, and explainability. Finally, we conclude this survey with insights and discussions on future research directions.The proposed trustworthy framework for representation learning across domains includes four key concepts:1. Robustness: The ability of the model to perform well in the presence of noise, outliers, or distributional shifts.2. Privacy: The protection of sensitive information and the prevention of unauthorized access or misuse.3. Fairness: The avoidance of bias and discrimination in the model's predictions, ensuring that all individuals or groups are treated equally and without prejudice.4. Explainability: The ability to provide clear and understandable explanations for the model's predictions, allowing users to understand the reasoning behind the model's decisions.Existing methods for the trustworthy framework from these four concepts include:1. Robustness: Techniques such as data augmentation, adversarial training, and ensemble methods can improve the model's robustness to noise and distributional shifts.2. Privacy: Methods such as differential privacy, secure multi-party computation, and homomorphic encryption can protect sensitive information and prevent unauthorized access.3. Fairness: Techniques such as fair batch normalization, fair representation learning, and fair evaluation metrics can help to mitigate bias and discrimination in the model's predictions.4. Explainability: Approaches such as feature importance, saliency maps, and model interpretability techniques can provide clear explanations for the model's predictions.In conclusion, this survey provides a comprehensive overview of the trustworthy representation learning across domains framework, including the proposed framework and existing methods that align with the four key concepts. We also discuss insights and future research directions in this field, highlighting the importance of trustworthy AI systems in our daily lives and human society.
</details></li>
</ul>
<hr>
<h2 id="Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations"><a href="#Topical-Chat-Towards-Knowledge-Grounded-Open-Domain-Conversations" class="headerlink" title="Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations"></a>Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11995">http://arxiv.org/abs/2308.11995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexa/Topical-Chat">https://github.com/alexa/Topical-Chat</a></li>
<li>paper_authors: Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tur</li>
<li>for: 这个论文的目的是提供一个基于知识的人机对话集，帮助开发更加深入、有趣的人机对话AI。</li>
<li>methods: 论文使用了知识基础的人机对话集，并在这个集合中采用了无显式角色的对话方式。</li>
<li>results: 论文通过对这个知识基础的人机对话集进行自动和人工评价，提出了一些state-of-the-art的对话模型。<details>
<summary>Abstract</summary>
Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.
</details>
<details>
<summary>摘要</summary>
建立社交机器人，能够与人类进行深入有趣的开放领域对话，是人工智能（AI）的极大挑战之一。为此，机器人需要能够有效地利用多个领域的世界知识进行对话。现有的知识基础对话数据集主要是通过显式角色定义对话伙伴进行预设。这些数据集还不探讨对话的深度或广度，也没有探讨对话的转变。我们介绍Topical-Chat，一个基于知识的人类对话数据集，其下面知识覆盖8个广泛的主题，对话伙伴没有显式定义角色，以便进一步推动开放领域对话AI的研究。我们还在Topical-Chat上训练了多种当今最佳encoder-decoder对话模型，并进行自动和人类评估，以便作为参考。
</details></li>
</ul>
<hr>
<h2 id="Critical-Evaluation-of-Artificial-Intelligence-as-Digital-Twin-of-Pathologist-for-Prostate-Cancer-Pathology"><a href="#Critical-Evaluation-of-Artificial-Intelligence-as-Digital-Twin-of-Pathologist-for-Prostate-Cancer-Pathology" class="headerlink" title="Critical Evaluation of Artificial Intelligence as Digital Twin of Pathologist for Prostate Cancer Pathology"></a>Critical Evaluation of Artificial Intelligence as Digital Twin of Pathologist for Prostate Cancer Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11992">http://arxiv.org/abs/2308.11992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Okyaz Eminaga, Mahmoud Abbas, Christian Kunder, Yuri Tolkach, Ryan Han, James D. Brooks, Rosalie Nolley, Axel Semjonow, Martin Boegemann, Robert West, Jin Long, Richard Fan, Olaf Bettendorf</li>
<li>for: 这项研究旨在测试一种基于人工智能的 Digitaltwin 技术，用于检测 próstate cancer 和分类。</li>
<li>methods: 研究使用了 2,603 个 histology 图像，由 Hematoxylin 和 Eosin 染色。使用了多种因素对 prostate cancer 的诊断和分类进行了分析。</li>
<li>results: 研究发现，vPatho 可以与人类Pathologist 相比，在 prostate cancer 的检测和卷积量测量方面具有相当的表现。但是，在 tumor grading 方面，vPatho 和人类Pathologist 之间存在一定的不一致。此外，研究还发现了一些可能导致 grade 不一致的因素，如 tumor 的垂直扩展和抽屉含量。<details>
<summary>Abstract</summary>
Prostate cancer pathology plays a crucial role in clinical management but is time-consuming. Artificial intelligence (AI) shows promise in detecting prostate cancer and grading patterns. We tested an AI-based digital twin of a pathologist, vPatho, on 2,603 histology images of prostate tissue stained with hematoxylin and eosin. We analyzed various factors influencing tumor-grade disagreement between vPatho and six human pathologists. Our results demonstrated that vPatho achieved comparable performance in prostate cancer detection and tumor volume estimation, as reported in the literature. Concordance levels between vPatho and human pathologists were examined. Notably, moderate to substantial agreement was observed in identifying complementary histological features such as ductal, cribriform, nerve, blood vessels, and lymph cell infiltrations. However, concordance in tumor grading showed a decline when applied to prostatectomy specimens (kappa = 0.44) compared to biopsy cores (kappa = 0.70). Adjusting the decision threshold for the secondary Gleason pattern from 5% to 10% improved the concordance level between pathologists and vPatho for tumor grading on prostatectomy specimens (kappa from 0.44 to 0.64). Potential causes of grade discordance included the vertical extent of tumors toward the prostate boundary and the proportions of slides with prostate cancer. Gleason pattern 4 was particularly associated with discordance. Notably, grade discordance with vPatho was not specific to any of the six pathologists involved in routine clinical grading. In conclusion, our study highlights the potential utility of AI in developing a digital twin of a pathologist. This approach can help uncover limitations in AI adoption and the current grading system for prostate cancer pathology.
</details>
<details>
<summary>摘要</summary>
prostata cancer 的生理学pathology 在临床管理中发挥关键作用，但是它很时间消耗。人工智能（AI）表示可能用于检测 prostata cancer 和分化模式。我们使用了一个基于 AI 的pathologist 数字 близнеvPatho 测试了 2,603 个 prostata组织片中的 Hematoxylin 和 Eosin 染色图像。我们分析了不同因素 influencing tumor-grade 的不一致性。结果表明，vPatho 在检测 prostata cancer 和组织体积方面达到了文献报告的性能。我们对 vPatho 和六名人类病理学家之间的一致性进行了分析。注意，在识别 complementary 的 histological 特征方面，such as ductal、cribriform、nerve、血管和lymphocyte infiltration 中，moderate to substantial 的一致性被观察到。然而，在评估 tumor grading 方面，一致性下降到 prostatectomy specimens （kappa = 0.44），比 biopsy cores （kappa = 0.70）更低。通过调整 secondary Gleason 模式的决策阈值从 5% 到 10%，提高了 pathologists 和 vPatho 之间的一致性水平（kappa from 0.44 to 0.64）。可能导致 grade discordance 的原因包括 tumor 的 vertical 分布向 prostata 边界以及检测到的肿瘤组织片的比例。Gleason 模式 4 特别与不一致相关。需要注意的是，grade discordance 与 vPatho 不特别任何一名病理学家的 routine clinical grading 相关。在结论中，我们的研究表明了 AI 可能在开发一个 pathologist 数字 близне的方面具有潜在的用途。这种方法可以帮助揭露 AI 的采用 limitation 和当前的 prostata cancer 生理学pathology 评估系统的限制。
</details></li>
</ul>
<hr>
<h2 id="Relational-Concept-Based-Models"><a href="#Relational-Concept-Based-Models" class="headerlink" title="Relational Concept Based Models"></a>Relational Concept Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11991">http://arxiv.org/abs/2308.11991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aghoreshwar/Awesome-Customer-Analytics">https://github.com/Aghoreshwar/Awesome-Customer-Analytics</a></li>
<li>paper_authors: Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra</li>
<li>for: 这个论文的目的是解决关系领域中的深度学习模型可读性问题，这些模型不是专门设计来解决关系问题，而且关系模型不如概念基础模型（CBMs）那样可读性。</li>
<li>methods: 作者提议了一种名为关系概念基础模型（Relational CBMs）的家族关系深度学习方法，这些方法可以在关系领域中提供可读性的任务预测。</li>
<li>results: 作者的实验表明，关系CBMs可以与现有的关系黑obox（黑obox）相比，在图像分类和知识图表链接预测等问题上达到同等的泛化性能，同时支持生成量化的概念基础解释，并能够应对测试时间 intervención，在有限的训练数据 régime和罕见概念监督下也能够保持稳定性。<details>
<summary>Abstract</summary>
The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
</details>
<details>
<summary>摘要</summary>
<SYS>translate("The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.")</SYS>Here's the translation in Simplified Chinese:“relational deep learning模型的设计问题对开放式挑战：可解释深度学习方法，如基于概念的模型（CBMs），不适合解决关系问题，而关系模型不如CBMs可解释。为解决这个问题，我们提议了关系基于概念模型（Relational Concept-Based Models），这是一种可解释的关系深度学习方法。我们的实验，从图像分类到知识图的链接预测，显示了关系CBMs（i）与现有关系黑盒（as opposed to non-relational CBMs）的一致性表现，（ii）支持生成量化的概念基于解释，（iii）在测试时干预有效，（iv）在具有异常场景、有限训练数据 régime和罕见概念监督的情况下坚持。”
</details></li>
</ul>
<hr>
<h2 id="Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks"><a href="#Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks" class="headerlink" title="Will More Expressive Graph Neural Networks do Better on Generative Tasks?"></a>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiandong Zou, Xiangyu Zhao, Pietro Liò, Yiren Zhao</li>
<li>for: 本研究的目的是探讨 Graph Neural Network (GNN) 在分子图生成任务中的表达能力，并将 GNN 应用于两种不同的生成框架（GCPN 和 GraphAF）中。</li>
<li>methods: 本研究使用了六种不同的 GNN，包括 GCN、GAT、GGN、GraphSAGE、Graph Isomorphism Network (GIN) 和 Graph Attention Network (GAT)，并对这些 GNN 进行了比较。</li>
<li>results: 研究发现，使用更高级的 GNN 可以提高 GCPN 和 GraphAF 在分子图生成任务中的表现，但 GNN 表现不是必需的 condition  для一个好的 GNN-based 生成模型。此外，研究还发现，使用更高级的 GNN 可以使 GCPN 和 GraphAF 在17种非 GNN-based 图生成方法（如变量 autoencoders 和 Bayesian 优化模型）中 achieve state-of-the-art 结果。<details>
<summary>Abstract</summary>
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZINC-250k dataset. Through our extensive experiments, we demonstrate that advanced GNNs can indeed improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results across 17 other non-GNN-based graph generative approaches, such as variational autoencoders and Bayesian optimisation models, on the proposed molecular generative objectives (DRD2, Median1, Median2), which are important metrics for de-novo molecular design.
</details>
<details>
<summary>摘要</summary>
“图生成具有重要挑战，因为它需要预测一个完整的图像，包括多个节点和边，基于只提供的标签。这个任务对于许多实际应用都具有重要性，如新药和分子设计。在过去几年，图生成领域内出现了许多成功的方法。然而，这些方法受到两大缺点的影响：（1）用于这些方法的基本图神经网络（GNN）架构经常未得到充分的探索；（2）这些方法通常只被评估在有限的约束下。为了填补这个差距，我们在图生成任务中研究GNN的表达能力，通过将基本GNN替换为更表达能力的GNN来进行分析。我们在ZINC-250k数据集上进行了广泛的实验，并证明了高级GNN可以提高GCPN和GraphAF在分子生成任务中的表现，但GNN表达能力不是必要的condition。此外，我们还示出了GCPN和GraphAF与高级GNN的组合可以在17种非GNN基于的图生成方法（如变量自动编码器和搜索优化模型）中实现州际级结果，这些对于新药设计是重要的度量。”
</details></li>
</ul>
<hr>
<h2 id="Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression"><a href="#Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression" class="headerlink" title="Approximating Score-based Explanation Techniques Using Conformal Regression"></a>Approximating Score-based Explanation Techniques Using Conformal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11975">http://arxiv.org/abs/2308.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Henrik Boström, Sofiane Ennadir, Ulf Johansson</li>
<li>for: 这些 papers 是为了解释黑obox 模型的逻辑而写的。</li>
<li>methods: 这些 papers 使用了 computationally costly 的 explanation techniques, such as SHAP, 并提出了一种使用 computationally less costly  regression models 来近似 score-based explanation techniques 的方法。</li>
<li>results: 这些 papers 提出了一些 non-conformity measures 来考虑 approximating explanations 的困难度，并在大规模的 empirical investigation 中证明了其效果。 Results 表明，提出的方法可以significantly improve execution time compared to fast version of SHAP, TreeSHAP, 并且可以生成紧凑的 interval。<details>
<summary>Abstract</summary>
Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time compared to the fast version of SHAP, TreeSHAP. The results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. Moreover, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.
</details>
<details>
<summary>摘要</summary>
黑obox模型的解释技术 oftentimes 使用分数基因 explainable machine-learning 技术。然而，这些解释技术通常 computationally expensive，这限制了它们在时间敏感上下文中的应用。因此，我们提出并 investigate 使用 computationally less costly 回归模型来近似 score-based explanation techniques, such as SHAP。此外，我们提供了雇佣 inductive conformal prediction 框架来提供有效性保证。我们还提出了一些非准确度度量，用于考虑近似解释的困难性，同时保持计算成本低。我们在大规模的实验中发现，我们提posed方法可以在执行时间方面取得显著改进，比如 TreeSHAP 的快速版本。此外，我们的结果还表明，我们的方法可以生成紧凑的间隔，同时提供有效性保证。此外，我们的方法允许比较不同的近似方法的解释，并选择一个基于解释 intervals 的紧凑程度（tightness）。
</details></li>
</ul>
<hr>
<h2 id="Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields"><a href="#Blending-NeRF-Text-Driven-Localized-Editing-in-Neural-Radiance-Fields" class="headerlink" title="Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields"></a>Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11974">http://arxiv.org/abs/2308.11974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, Taehyeong Kim</li>
<li>for: 本研究旨在提出一种基于NeRF的模型，用于文本驱动地地方化编辑3D对象，以实现在文本提示中指定的本地修改。</li>
<li>methods: 该模型包含两个NeRF网络：预训练NeRF和可编辑NeRF，以及新的混合操作。使用CLIP模型进行视觉语言对Alignment，引导Blending-NeRF模型在文本提示中添加新物体、修改 текстуры和 removing部分原对象。</li>
<li>results: 我们的广泛实验表明，Blending-NeRF模型能够自然地和地方化地编辑3D对象，从多种文本提示中生成修改后的结果。<details>
<summary>Abstract</summary>
Text-driven localized editing of 3D objects is particularly difficult as locally mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.
</details>
<details>
<summary>摘要</summary>
文本驱动的3D对象编辑 particullary difficult, because mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.Here's the translation in Traditional Chinese:文本驱动的3D对象编译 particullary difficult, because mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.
</details></li>
</ul>
<hr>
<h2 id="Value-of-Assistance-for-Mobile-Agents"><a href="#Value-of-Assistance-for-Mobile-Agents" class="headerlink" title="Value of Assistance for Mobile Agents"></a>Value of Assistance for Mobile Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11961">http://arxiv.org/abs/2308.11961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clair-lab-technion/voa">https://github.com/clair-lab-technion/voa</a></li>
<li>paper_authors: Adi Amuzig, David Dovrat, Sarah Keren</li>
<li>for: 这篇论文是为了解决移动机器人agent的地理位置uncertainty问题，通过增加协助行为来减少uncertainty。</li>
<li>methods: 该论文提出了一种基于Gaussian process的Value of Assistance（VOA）计算方法，用于评估协助行为的效果。</li>
<li>results: 研究人员通过实验和实际应用 validate了VOA计算方法，并证明了VOA可以准确预测机器人的成本减少效果。<details>
<summary>Abstract</summary>
Mobile robotic agents often suffer from localization uncertainty which grows with time and with the agents' movement. This can hinder their ability to accomplish their task. In some settings, it may be possible to perform assistive actions that reduce uncertainty about a robot's location. For example, in a collaborative multi-robot system, a wheeled robot can request assistance from a drone that can fly to its estimated location and reveal its exact location on the map or accompany it to its intended location. Since assistance may be costly and limited, and may be requested by different members of a team, there is a need for principled ways to support the decision of which assistance to provide to an agent and when, as well as to decide which agent to help within a team. For this purpose, we propose Value of Assistance (VOA) to represent the expected cost reduction that assistance will yield at a given point of execution. We offer ways to compute VOA based on estimations of the robot's future uncertainty, modeled as a Gaussian process. We specify conditions under which our VOA measures are valid and empirically demonstrate the ability of our measures to predict the agent's average cost reduction when receiving assistance in both simulated and real-world robotic settings.
</details>
<details>
<summary>摘要</summary>
Mobile robotic agents often suffer from localization uncertainty, which increases over time and with the agents' movement. This can hinder their ability to complete tasks. In some cases, it may be possible to perform assistive actions that reduce uncertainty about a robot's location. For example, in a collaborative multi-robot system, a wheeled robot can request assistance from a drone that can fly to its estimated location and reveal its exact location on the map or accompany it to its intended location. Since assistance may be costly and limited, and may be requested by different team members, there is a need for principled ways to support the decision of which assistance to provide to an agent and when, as well as to decide which agent to help within a team. To address this need, we propose the Value of Assistance (VOA) to represent the expected cost reduction that assistance will yield at a given point of execution. We provide methods to compute VOA based on estimations of the robot's future uncertainty, modeled as a Gaussian process. We specify conditions under which our VOA measures are valid and empirically demonstrate the ability of our measures to predict the agent's average cost reduction when receiving assistance in both simulated and real-world robotic settings.
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Neural-Networks-applied-to-the-description-of-wave-particle-resonance-in-kinetic-simulations-of-fusion-plasmas"><a href="#Physics-informed-Neural-Networks-applied-to-the-description-of-wave-particle-resonance-in-kinetic-simulations-of-fusion-plasmas" class="headerlink" title="Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas"></a>Physics informed Neural Networks applied to the description of wave-particle resonance in kinetic simulations of fusion plasmas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12312">http://arxiv.org/abs/2308.12312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jai Kumar, David Zarzoso, Virginie Grandgirard, Jan Ebert, Stefan Kesselheim</li>
<li>for: 这篇论文使用了哈曼-普朗托纳系统的减少形式版本（1D1V）作为物理信息学神经网络（PINN）的应用测试平台，以解决气体振荡和杯尖不稳定性问题。</li>
<li>methods: 这篇论文首先使用了PINN作为压缩方法来解决哈曼-普朗托纳系统的解，并与标准神经网络进行比较。其次，文章还应用了PINN来解决哈曼-普朗托纳系统，并强调了对部分权重的特殊强调，导致了一种基于自动导数和自动积分的PINN变体，称为可integrable PINN（I-PINN）。</li>
<li>results: 文章的结果表明，PINN可以成功地解决哈曼-普朗托纳系统的问题，并且可以提供更高精度的解决方案。此外，I-PINN还能够更好地处理部分权重的问题，提高了解决速度和精度。<details>
<summary>Abstract</summary>
The Vlasov-Poisson system is employed in its reduced form version (1D1V) as a test bed for the applicability of Physics Informed Neural Network (PINN) to the wave-particle resonance. Two examples are explored: the Landau damping and the bump-on-tail instability. PINN is first tested as a compression method for the solution of the Vlasov-Poisson system and compared to the standard neural networks. Second, the application of PINN to solving the Vlasov-Poisson system is also presented with the special emphasis on the integral part, which motivates the implementation of a PINN variant, called Integrable PINN (I-PINN), based on the automatic-differentiation to solve the partial differential equation and on the automatic-integration to solve the integral equation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用减 simplify 的 Vlasov-Poisson 系统作为测试床，以检验物理学 Informed Neural Network (PINN) 在波动-粒子共振中的可应用性。两个例子被探讨：兰道抑压和块在尾部不稳定。首先，PINN 作为 Vlasov-Poisson 系统解的压缩方法，与标准神经网络进行比较。其次，通过特别强调完 integral part，实现了一种基于自动极点 differentiable 和自动极点 integrate 的 PINN 变体，称为可 integrate PINN（I-PINN），以解决 partial differential equation 和 integral equation。[/INST0]  Here's the text in Traditional Chinese:<<SYS>>使用减 simplify 的 Vlasov-Poisson 系统作为测试床，以检验物理学 Informed Neural Network (PINN) 在波动-粒子共振中的可应用性。两个例子被探讨：兰道抑压和块在尾部不稳定。首先，PINN 作为 Vlasov-Poisson 系统解的压缩方法，与标准神经网络进行比较。其次，通过特别强调完 integral part，实现了一种基于自动极点 differentiable 和自动极点 integrate 的 PINN 变体，称为可 integrate PINN（I-PINN），以解决 partial differential equation 和 integral equation。
</details></li>
</ul>
<hr>
<h2 id="Maintaining-Plasticity-via-Regenerative-Regularization"><a href="#Maintaining-Plasticity-via-Regenerative-Regularization" class="headerlink" title="Maintaining Plasticity via Regenerative Regularization"></a>Maintaining Plasticity via Regenerative Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy</li>
<li>for: 维护权重的柔软性（plasticity）在处理非站点数据流时降低。</li>
<li>methods: 提出了L2Init方法，即在损失函数中添加L2正则项，以保持初始参数的柔软性。</li>
<li>results: 在不同类型的非站点性问题上，L2Init可以均衡权重的大小和柔软性，并在处理非站点数据流时提高模型的性能。<details>
<summary>Abstract</summary>
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates plasticity loss. We additionally find that our regularization term reduces parameter magnitudes and maintains a high effective feature rank.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation"><a href="#When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation" class="headerlink" title="When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation"></a>When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Geng Tian, Ming Tang</li>
<li>for: 这个论文的目的是提出一种名为MiniBatch-SFL的新的分布式学习方法，以解决在分布式学习中发生的“客户端漂移”问题。</li>
<li>methods: 这个方法利用了MiniBatch SGD和分布式学习的概念，在客户端和服务器之间分成了两部分的模型，让客户端只需要训练部分模型，以减少 computation workload。</li>
<li>results: 这个方法可以提高分布式学习的精度，尤其是在非同一的数据时。在实验中，MiniBatch-SFL比传统的分布式学习和Federated learning方法提高了精度，具体来说，可以提高24.1%和17.1%。<details>
<summary>Abstract</summary>
Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-side updates do not depend on the non-IID degree of the clients' datasets and can potentially mitigate client drift. However, the client-side model relies on the non-IID degree and can be optimized by properly choosing the cut layer. Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance. Moreover, numerical results show that MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The accuracy improvement can be up to 24.1\% and 17.1\% with highly non-IID data, respectively.
</details>
<details>
<summary>摘要</summary>
分布式学习（FL）可以在分布式客户端（例如边缘设备）上进行模型训练，而不需要将原始数据共享。然而，FL可能会很 computationally expensive，因为客户端需要训练整个模型多次。SplitFed learning（SFL）是一种最近的分布式方法，它可以减轻客户端设备上的计算工作负担，通过在一层截分模型两部分，其中客户端只需要训练模型的一部分。然而，SFL仍然会遭受客户端数据高度异步的问题，称为“客户端漂移”问题。为解决这个问题，我们提出了MiniBatch-SFL。这个算法将MiniBatch SGD integrate到SFL中，客户端在FL的方式上训练客户端模型，服务器则在MiniBatch SGD的方式上训练服务器模型。我们分析MiniBatch-SFL的整合和融合，并证明了预期的损失下界可以通过分析服务器和客户端模型更新的预期值来获得。服务器端的更新不виси于客户端数据的异步度，可能减轻客户端漂移问题。然而，客户端模型取决于异步度，可以通过合适地选择截分层来优化。奇怪的是，我们的实验结果表明，将截分层放在后者位置可以减少平均梯度差异和提高算法性能。此外，我们的数值结果表明，MiniBatch-SFL可以在异步数据上达到高度的准确率，比 conventinal SFL和FL高达24.1%和17.1%。
</details></li>
</ul>
<hr>
<h2 id="Pose-Modulated-Avatars-from-Video"><a href="#Pose-Modulated-Avatars-from-Video" class="headerlink" title="Pose Modulated Avatars from Video"></a>Pose Modulated Avatars from Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11951">http://arxiv.org/abs/2308.11951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunjin Song, Bastian Wandt, Helge Rhodin</li>
<li>for: 用于重建动态人体运动和形态，并模型人体的衣物和皮肤塑形。</li>
<li>methods: 使用神经辐射场（NeRF）驱动下方skeleton，并开发了一个两极分支神经网络，以adaptive和explcit方式在频率域中模型人体部件之间的相互关系。</li>
<li>results: 对比州方法，该方法能够更好地保留细节和总体化能力。<details>
<summary>Abstract</summary>
It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state-of-the-art methods in terms of preserving details and generalization capabilities.
</details>
<details>
<summary>摘要</summary>
现在可以使用神经辐射场（NeRF）和下面的骨架来重建动态人体运动和形状。然而，模拟人体皮肤和衣服的塑形仍然是一个挑战。现有的人物模型通常是通过隐藏的方式学习或者通过代理表面来实现。我们的方法受到不同姿势需要唯一频谱分配的观察所启发。忽略这种分配会导致缺陷的纹理和形状细节。我们开发了一个两极分支神经网络，其中第一极是一个图像神经网络，地方地模型体部之间的相关性，带入骨架姿势作为输入。第二极将这些相关特征与一组全局频率相结合，然后修饰特征编码。我们的实验表明，我们的网络在保持细节和泛化能力方面超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="High-quality-Image-Dehazing-with-Diffusion-Model"><a href="#High-quality-Image-Dehazing-with-Diffusion-Model" class="headerlink" title="High-quality Image Dehazing with Diffusion Model"></a>High-quality Image Dehazing with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11949">http://arxiv.org/abs/2308.11949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Yu, Jie Huang, Kaiwen Zheng, Man Zhou, Feng Zhao</li>
<li>for: 解压缩雾化图像，即在浓雾场景下还原原始图像的信息。</li>
<li>methods: 本文提出了一种基于DDPM的物理学习框架，即DehazeDDPM，它首先使用物理模型（ASM）模拟雾化任务，然后使用DDPM进行补偿，以恢复雾化induced的信息损失。</li>
<li>results: 对比实验表明，DehazeDDPM在 sintetic和实际雾化数据集上达到了领先的表现。<details>
<summary>Abstract</summary>
Image dehazing is quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Though previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-induced huge information loss, by working in conjunction with the physical modelling. Extensive experiments demonstrate that our method attains state-of-the-art performance on both synthetic and real-world hazy datasets.
</details>
<details>
<summary>摘要</summary>
Image 降霾 quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Although previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-induced huge information loss, by working in conjunction with the physical modelling. Extensive experiments demonstrate that our method attains state-of-the-art performance on both synthetic and real-world hazy datasets.
</details></li>
</ul>
<hr>
<h2 id="LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model"><a href="#LongDanceDiff-Long-term-Dance-Generation-with-Conditional-Diffusion-Model" class="headerlink" title="LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model"></a>LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11945">http://arxiv.org/abs/2308.11945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqi Yang, Zejun Yang, Zhisheng Wang</li>
<li>for: 这个研究旨在解决长期三维真实舞蹈生成中的静止问题，以提高舞蹈生成的可调和自然性。</li>
<li>methods: 我们运用了一个条件扩散模型，长舞蹈扩散（LongDanceDiff），并将输入组合了音乐、过去的动作和随机化的未来动作。我们还引入了一个共同信息最小化目标，以优化生成的舞蹈动作的多样性和自然性。</li>
<li>results: 我们的方法与现有的方法相比，实现了重大的改善，包括增加了舞蹈生成的可调和自然性。我们计划将我们的代码和模型发布给社区。<details>
<summary>Abstract</summary>
Dancing with music is always an essential human art form to express emotion. Due to the high temporal-spacial complexity, long-term 3D realist dance generation synchronized with music is challenging. Existing methods suffer from the freezing problem when generating long-term dances due to error accumulation and training-inference discrepancy. To address this, we design a conditional diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance generation, addressing the challenges of temporal coherency and spatial constraint. LongDanceDiff contains a transformer-based diffusion model, where the input is a concatenation of music, past motions, and noised future motions. This partial noising strategy leverages the full-attention mechanism and learns the dependencies among music and past motions. To enhance the diversity of generated dance motions and mitigate the freezing problem, we introduce a mutual information minimization objective that regularizes the dependency between past and future motions. We also address common visual quality issues in dance generation, such as foot sliding and unsmooth motion, by incorporating spatial constraints through a Global-Trajectory Modulation (GTM) layer and motion perceptual losses, thereby improving the smoothness and naturalness of motion generation. Extensive experiments demonstrate a significant improvement in our approach over the existing state-of-the-art methods. We plan to release our codes and models soon.
</details>
<details>
<summary>摘要</summary>
人类常用舞蹈作为表达情感的重要艺术形式。由于高度时空复杂性，长期3D真实舞蹈生成同音乐同步是一项挑战。现有方法受到预测-实际差异和错误积累的问题。为解决这问题，我们设计了一种 conditional diffusion 模型，长 dance diff（LongDanceDiff），用于这种序列到序列长期舞蹈生成任务，解决时间准确性和空间约束的挑战。LongDanceDiff 包括一个基于 transformer 的扩散模型，输入是音乐、过去动作和噪音未来动作的 concatenation。这种 partial noising 策略利用了全程注意机制，学习音乐和过去动作之间的依赖关系。为提高生成舞蹈动作的多样性和减少冻结问题，我们引入了一个 mutual information minimization 目标，规范过去和未来动作之间的依赖关系。我们还通过 incorporating 全球轨迹修饰（GTM）层和运动观察损失，提高生成动作的平滑性和自然性。广泛的实验表明我们的方法在现有状态的方法上显著提高了性能。我们计划 soon 发布我们的代码和模型。
</details></li>
</ul>
<hr>
<h2 id="RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching"><a href="#RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching" class="headerlink" title="RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching"></a>RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11943">http://arxiv.org/abs/2308.11943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Vott, Adam M. Lehavi</li>
<li>for: 本 paper 探讨了使用最佳先搜索算法和强化学习（RL）技术来找到特定 Ramsey 数字的反例。</li>
<li>methods: 本 paper 使用了图vectorization和深度神经网络（DNN）基于的优化和搜索算法，以评估图是否为反例。</li>
<li>results: 本 paper 提出了一种搜索框架，可以支持 Ramsey 反例探索使用其他heelures。<details>
<summary>Abstract</summary>
The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
</details>
<details>
<summary>摘要</summary>
“拉姆齐数”是最小的节点数量，$n = R(s, t)$, 使得所有无向简单图的顺序为$n$，必然包含一个 clique 的顺序为$s$，或一个独立集的顺序为$t$。这篇论文探索使用最佳先搜索算法和强化学习（RL）技术来找到特定拉姆齐数的反例。我们通过引入图像化和深度神经网络（DNN）基于的优化来提高先前的搜索方法，如随机搜索。 paper 还提出了算法优化，以确保搜索时间 polynomial。这篇论文不是想要发现新的反例，而是介绍和评估一个支持拉姆齐反例探索的框架，使用其他规则。代码和方法通过 PyPI 包和 GitHub 存储库提供。
</details></li>
</ul>
<hr>
<h2 id="Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series"><a href="#Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series" class="headerlink" title="Retail Demand Forecasting: A Comparative Study for Multivariate Time Series"></a>Retail Demand Forecasting: A Comparative Study for Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11939">http://arxiv.org/abs/2308.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah</li>
<li>for: 预测零售需求的精度是零售业的金融性和供应链效率的关键因素。在全球市场变得越来越连接起来，企业们正在寻找更高级别的预测模型，以获得竞争优势。</li>
<li>methods: 本研究使用时间系列数据中的顾客需求和macro经济变量（如Consumer Price Index（CPI）、Index of Consumer Sentiment（ICS）和失业率）进行拓展。我们采用了不同的回归和机器学习模型，以准确预测零售需求。</li>
<li>results: 我们的研究发现，通过拓展时间系列数据中的顾客需求和macro经济变量，可以提高预测零售需求的准确度。不同的回归和机器学习模型在预测零售需求方面具有不同的表现，但是综合评价下，机器学习模型的表现较好。<details>
<summary>Abstract</summary>
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
</details>
<details>
<summary>摘要</summary>
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.Here's the translation in Traditional Chinese:精准的预测是商业领域中的一个关键因素，对于财务性能和供应链效率都是决定性的。随着全球市场变得越来越联系，企业们正在转向更进步的预测模型，以获得竞争优势。然而，现有的文献主要集中在历史销售数据上，忽略了消费者支出行为中的重要影响因素。在这项研究中，我们将把历史销售数据丰富化，加入 macroeconomic 变量，例如消费者物价指数 (CPI)、消费者信心指数 (ICS) 和失业率。利用这个完整的数据集，我们将开发和比较不同的回归和机器学习模型，以精准预测零售需求。
</details></li>
</ul>
<hr>
<h2 id="Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification"><a href="#Learning-Bottleneck-Transformer-for-Event-Image-Voxel-Feature-Fusion-based-Classification" class="headerlink" title="Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification"></a>Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11937">http://arxiv.org/abs/2308.11937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/event-ahu/efv_event_classification">https://github.com/event-ahu/efv_event_classification</a></li>
<li>paper_authors: Chengguo Yuan, Yu Jin, Zongzhen Wu, Fanting Wei, Yangzirui Wang, Lan Chen, Xiao Wang</li>
<li>for: 本文提出了一种新的双流框架，用于事件表示、提取和融合，以解决现有方法的缺点，包括单一模式表达和网络结构设计。</li>
<li>methods: 本文使用了Transformer和结构化图 neural network（GNN）架构，同时学习事件图像和事件立方体信息。在这个框架中，用瓶颈Transformer来实现双流信息融合。</li>
<li>results: 经过广泛的实验表明，我们的提议的框架可以在两个常用的事件基本分类数据集上达到最新的性能水平。代码可以在：\url{<a target="_blank" rel="noopener" href="https://github.com/Event-AHU/EFV_event_classification%7D">https://github.com/Event-AHU/EFV_event_classification}</a> 中找到。<details>
<summary>Abstract</summary>
Recognizing target objects using an event-based camera draws more and more attention in recent years. Existing works usually represent the event streams into point-cloud, voxel, image, etc, and learn the feature representations using various deep neural networks. Their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. To address the aforementioned challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. This framework simultaneously models two common representations: event images and event voxels. By utilizing Transformer and Structured Graph Neural Network (GNN) architectures, spatial information and three-dimensional stereo information can be learned separately. Additionally, a bottleneck Transformer is introduced to facilitate the fusion of the dual-stream information. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on two widely used event-based classification datasets. The source code of this work is available at: \url{https://github.com/Event-AHU/EFV_event_classification}
</details>
<details>
<summary>摘要</summary>
recognizing target objects using event-based cameras has attracted increasing attention in recent years. existing works usually convert event streams into point clouds, voxels, images, etc., and learn feature representations using various deep neural networks. however, their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. to address these challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. this framework simultaneously models two common representations: event images and event voxels. by utilizing transformer and structured graph neural network (gnn) architectures, spatial information and three-dimensional stereo information can be learned separately. additionally, a bottleneck transformer is introduced to facilitate the fusion of the dual-stream information. extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance on two widely used event-based classification datasets. the source code of this work is available at: \url{https://github.com/Event-AHU/EFV_event_classification}Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe"><a href="#Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe" class="headerlink" title="Diverse Policies Converge in Reward-free Markov Decision Processe"></a>Diverse Policies Converge in Reward-free Markov Decision Processe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11924">http://arxiv.org/abs/2308.11924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrl-lab/diversepolicies">https://github.com/openrl-lab/diversepolicies</a></li>
<li>paper_authors: Fanqi Lin, Shiyu Huang, Weiwei Tu</li>
<li>for: 本文旨在提供一个统一的多种策略学习框架，并调查多种策略学习算法的训练是如何 converges 和效率如何。</li>
<li>methods: 本文提出了一种可证明高效的多种策略学习算法，并通过数学实验证明了其效果。</li>
<li>results: 经过数学实验，本文发现了多种策略学习算法的训练可以高效地 converge 到优化策略，并且可以提高策略的多样性和鲁棒性。<details>
<summary>Abstract</summary>
Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
</details>
<details>
<summary>摘要</summary>
“强化学习在很多决策任务中取得了很大成功，但传统的强化学习算法主要是为了获得单一的优化解决方案。然而，latest works表明了多种策略的重要性，使得这成为一个emerging研究话题。虽然多种多样性强化学习算法已经出现，但没有任何一个能回答强化学习算法如何 converges和效率如何。在这篇论文中，我们提出了一个统一的多样性强化学习框架，并investigate了训练多种策略的聚合。根据这种框架，我们还提出了可证明有效的多样性强化学习算法。最后，我们通过数值实验验证了我们的方法的有效性。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification"><a href="#Concept-Bottleneck-with-Visual-Concept-Filtering-for-Explainable-Medical-Image-Classification" class="headerlink" title="Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification"></a>Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11920">http://arxiv.org/abs/2308.11920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Injae Kim, Jongha Kim, Joonmyung Choi, Hyunwoo J. Kim</li>
<li>for: 提高医疗应用中模型可靠性的一个关键因素是可读性。概念瓶颈模型（CBM）可以使用人类理解的概念作为中间目标进行可读性图像分类。</li>
<li>methods: 在使用大型自然语言模型（LLM）生成概念的现有方法中，不考虑概念是否具有视觉特征，这是计算有意义的概念分数的重要因素。因此，我们提议使用视觉活动分数来衡量概念是否含有视觉cue，可以使用无标注图像数据来计算。</li>
<li>results: 我们的实验结果表明，采用我们提议的视觉活动分数来筛选概念可以consistently提高性能，相比基线。此外，qualitative analyses还证明了视觉相关概念被选择。<details>
<summary>Abstract</summary>
Interpretability is a crucial factor in building reliable models for various medical applications. Concept Bottleneck Models (CBMs) enable interpretable image classification by utilizing human-understandable concepts as intermediate targets. Unlike conventional methods that require extensive human labor to construct the concept set, recent works leveraging Large Language Models (LLMs) for generating concepts made automatic concept generation possible. However, those methods do not consider whether a concept is visually relevant or not, which is an important factor in computing meaningful concept scores. Therefore, we propose a visual activation score that measures whether the concept contains visual cues or not, which can be easily computed with unlabeled image data. Computed visual activation scores are then used to filter out the less visible concepts, thus resulting in a final concept set with visually meaningful concepts. Our experimental results show that adopting the proposed visual activation score for concept filtering consistently boosts performance compared to the baseline. Moreover, qualitative analyses also validate that visually relevant concepts are successfully selected with the visual activation score.
</details>
<details>
<summary>摘要</summary>
“可读性”是医疗应用中建立可靠模型的重要因素。概念瓶颈模型（CBM）可以实现可读性检查，通过使用人类可理解的概念作为中间目标。与传统方法不同的是，这些方法不需要大量的人工劳动来建立概念集。最近的工作则是利用大型自然语言模型（LLM）生成概念，并使用这些概念来生成可读性检查。但是，这些方法并不考虑概念是否具有视觉相关性，这是 Computing meaningful concept scores 中的重要因素。因此，我们提出了视觉活动 scores，它可以衡量概念是否包含视觉提示，并且可以轻松地使用无标注图像资料来计算。我们的实验结果显示，运用我们提出的视觉活动 scores 进行概念筛选可以与基准相比，实现更高的性能。此外，实验分析也显示，这些视觉相关的概念被成功选择。
</details></li>
</ul>
<hr>
<h2 id="LFS-GAN-Lifelong-Few-Shot-Image-Generation"><a href="#LFS-GAN-Lifelong-Few-Shot-Image-Generation" class="headerlink" title="LFS-GAN: Lifelong Few-Shot Image Generation"></a>LFS-GAN: Lifelong Few-Shot Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11917">http://arxiv.org/abs/2308.11917</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jjuon/lfs-gan">https://github.com/jjuon/lfs-gan</a></li>
<li>paper_authors: Juwon Seo, Ji-Su Kang, Gyeong-Moon Park</li>
<li>For: The paper addresses the challenging task of lifelong few-shot image generation, where a generative model learns a sequence of tasks using only a few samples per task, and prevents catastrophic forgetting and overfitting.* Methods: The proposed framework, called Lifelong Few-Shot GAN (LFS-GAN), uses an efficient task-specific modulator called Learnable Factorized Tensor (LeFT) to learn each task, and a novel mode seeking loss to improve diversity in low-data circumstances.* Results: The proposed LFS-GAN can generate high-quality and diverse images in various domains without any forgetting and mode collapse, achieving state-of-the-art in lifelong few-shot image generation task, and even outperforming existing few-shot GANs in the few-shot image generation task.Here is the simplified Chinese text for the three key points:* For: 这篇论文首次解决了难度较高的生命周期几个shot图像生成任务，其中一个生成模型需要使用只有几个样本来学习每个任务。* Methods: 该提议的框架called Lifelong Few-Shot GAN (LFS-GAN)使用高效的任务特定修饰器called Learnable Factorized Tensor (LeFT)来学习每个任务，并使用一种新的模式寻找损失来提高模型在低数据情况下的多样性。* Results: 提议的LFS-GAN可以在不同领域中生成高质量和多样的图像，无论是在几个shot图像生成任务中还是在生命周期中，并且可以超越现有的几个shot GANs在几个shot图像生成任务中的性能。<details>
<summary>Abstract</summary>
We address a challenging lifelong few-shot image generation task for the first time. In this situation, a generative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encounters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments demonstrate that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available at Github.
</details>
<details>
<summary>摘要</summary>
我们首次Addressing a challenging lifelong few-shot image generation task. In this situation, a generative model learns a sequence of tasks using only a few samples per task. As a result, the learned model encounters both catastrophic forgetting and overfitting problems at the same time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting, but these methods require additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To address these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in the lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator called Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments show that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in the lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available on Github.Here's the translation in Traditional Chinese:我们首次Addressing a challenging lifelong few-shot image generation task. 在这个情况下, a generative model learns a sequence of tasks using only a few samples per task. 因此, the learned model encounters both catastrophic forgetting and overfitting problems at the same time.  existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting, but these methods require additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To address these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in the lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator called Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich representation ability due to its unique reconstruction technique. Furthermore, we propose a novel mode seeking loss to improve the diversity of our model in low-data circumstances. Extensive experiments show that the proposed LFS-GAN can generate high-fidelity and diverse images without any forgetting and mode collapse in various domains, achieving state-of-the-art in the lifelong few-shot image generation task. Surprisingly, we find that our LFS-GAN even outperforms the existing few-shot GANs in the few-shot image generation task. The code is available on Github.
</details></li>
</ul>
<hr>
<h2 id="Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs"><a href="#Towards-CausalGPT-A-Multi-Agent-Approach-for-Faithful-Knowledge-Reasoning-via-Promoting-Causal-Consistency-in-LLMs" class="headerlink" title="Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs"></a>Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11914">http://arxiv.org/abs/2308.11914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen, Liang Lin</li>
<li>for: 提高知识基于reasoning的 faithfulness和 causality</li>
<li>methods: 多智能体协作 reasoning-and-consensus 框架</li>
<li>results: 在多种知识reasoning任务（如科学问答和常识reasoning）中，我们的框架比所有比较方法都高得多<details>
<summary>Abstract</summary>
Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoner and causal evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the causal evaluator agent scrutinizes if the answer in a solution is causally deducible from the question and vice versa, with a counterfactual answer replacing the original. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.
</details>
<details>
<summary>摘要</summary>
尽管LLM技术得到了进步，知识基于的理解仍然是一个长期的问题，因为知识回忆和推理的 fragility。现有方法主要是让LLM自动规划和解决问题，或者广泛采样推理链而未能解决概念和推理错误。借鉴多智能代理（i.e., 理解者和 causal评估器）的合作，我们提出了增强知识基于的理解 faithfulness 的框架。 Specifically, 我们提议使用多个智能代理（i.e., 理解者和 causal评估器）在一种理解和共识 paradigm中合作，以提高理解的准确性。理解者专注于提供人类化的 causality 来解决开放领域问题，而 causal评估器代理则检查问题和答案之间的 causal 关系是否正确，并将对应的 counterfactual 答案替换原答案。根据对多种知识理解任务（如科学问答和常识理解）的广泛和全面评估，我们的框架在比较的state-of-the-art方法之上出现大幅提升。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Admissible-Bounds-for-Heuristic-Learning"><a href="#Utilizing-Admissible-Bounds-for-Heuristic-Learning" class="headerlink" title="Utilizing Admissible Bounds for Heuristic Learning"></a>Utilizing Admissible Bounds for Heuristic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11905">http://arxiv.org/abs/2308.11905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Masataro Asai</li>
<li>for: 本研究的目的是提高前向搜索算法中的modern机器学习技术应用，并提供更好的理论基础 для这种应用。</li>
<li>methods: 本研究使用的方法包括使用Truncated Gaussian distribution作为参数，以及在训练过程中考虑扩展的admissible heuristics。</li>
<li>results: 研究发现，使用admissible heuristics作为参数，Truncated Gaussian distribution可以更好地适应实际问题，并且在训练过程中更快 converges。<details>
<summary>Abstract</summary>
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and empirically show that, as a result, it yields more accurate heuristics and converges faster during training.
</details>
<details>
<summary>摘要</summary>
Recently, there has been growing interest in using modern machine learning techniques to learn heuristic functions for forward search algorithms. However, there has been little theoretical understanding of what these functions should learn, how to train them, and why we do so. This lack of understanding has led to various literature selecting datasets and optimization metrics on an ad-hoc basis, and little attention has been paid to the role of admissibility during learning.This paper focuses on the role of admissible heuristics in supervised heuristic learning, using Truncated Gaussian distributions as parameters. This approach tightens the hypothesis space compared to ordinary Gaussian distributions, and faithfully follows the principle of maximum entropy. Empirical results show that this approach yields more accurate heuristics and converges faster during training.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection"><a href="#Exploring-the-Optimization-Objective-of-One-Class-Classification-for-Anomaly-Detection" class="headerlink" title="Exploring the Optimization Objective of One-Class Classification for Anomaly Detection"></a>Exploring the Optimization Objective of One-Class Classification for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11898">http://arxiv.org/abs/2308.11898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Gao, Huiyuan Luo, Fei Shen, Zhengtao Zhang<br>for:This paper focuses on the optimization objective space within one-class classification (OCC) methods and its impact on performance.methods:The paper proposes a novel, data-agnostic deep one-class classification method that utilizes a single 1x1 convolutional layer as a trainable projector and any space with a suitable norm as the optimization objective.results:The proposed method achieves state-of-the-art performance in both one-class classification and industrial vision anomaly detection and segmentation tasks, validating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
One-class classification (OCC) is a longstanding method for anomaly detection. With the powerful representation capability of the pre-trained backbone, OCC methods have witnessed significant performance improvements. Typically, most of these OCC methods employ transfer learning to enhance the discriminative nature of the pre-trained backbone's features, thus achieving remarkable efficacy. While most current approaches emphasize feature transfer strategies, we argue that the optimization objective space within OCC methods could also be an underlying critical factor influencing performance. In this work, we conducted a thorough investigation into the optimization objective of OCC. Through rigorous theoretical analysis and derivation, we unveil a key insights: any space with the suitable norm can serve as an equivalent substitute for the hypersphere center, without relying on the distribution assumption of training samples. Further, we provide guidelines for determining the feasible domain of norms for the OCC optimization objective. This novel insight sparks a simple and data-agnostic deep one-class classification method. Our method is straightforward, with a single 1x1 convolutional layer as a trainable projector and any space with suitable norm as the optimization objective. Extensive experiments validate the reliability and efficacy of our findings and the corresponding methodology, resulting in state-of-the-art performance in both one-class classification and industrial vision anomaly detection and segmentation tasks.
</details>
<details>
<summary>摘要</summary>
一类分类（OCC）是一种长期使用的异常检测方法。随着预训练后处理的特征表示能力的提高，OCC方法已经经历了显著性能提高。通常，大多数这些OCC方法使用传输学来强化预训练后处理的特征，从而实现了很好的效果。而我们认为，OCC方法的优化目标空间也是影响性能的关键因素。在这项工作中，我们进行了一项全面的OCC优化目标对象的调查。通过严格的理论分析和逻辑推导，我们揭示出一个关键发现：任何具有适当 нор 的空间都可以作为异常中心的等价substitute，不需要基于训练样本的分布假设。此外，我们还提供了确定OCC优化目标空间的可行范围的指南。这一新发现引出了一种简单、数据非依的深度一类分类方法。我们的方法包括一个单一的1x1卷积层作为可训练的投影器，以及任何具有适当 norm的空间作为优化目标。我们的实验证明了我们的发现和相应的方法ология的可靠性和效果，在一类分类和工业视觉异常检测和分割任务中实现了状态的末点性能。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model"><a href="#Bridging-the-Gap-Deciphering-Tabular-Data-Using-Large-Language-Model" class="headerlink" title="Bridging the Gap: Deciphering Tabular Data Using Large Language Model"></a>Bridging the Gap: Deciphering Tabular Data Using Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11891">http://arxiv.org/abs/2308.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengyuan Zhang, Peng Chang, Zongcheng Ji</li>
<li>for: 本研究旨在探讨大语言模型如何用于表格问答 tasks，以提高表格结构和内容的理解。</li>
<li>methods: 我们提出了一种特有的模块，用于将表格 serialized 到可以与大语言模型集成的格式。此外，我们还实施了一种纠正机制，以检查和修正模型的可能错误。</li>
<li>results: 我们的提议方法在总体指标中落后 SOTA 约 11.7%，但在特定数据集上测试时，超过 SOTA 约 1.2%。这些结果表明我们的方法可以增强大语言模型对表格结构和内容的理解。<details>
<summary>Abstract</summary>
In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by approximately 11.7% in overall metrics, it surpasses the SOTA by about 1.2% in tests on specific datasets. This research marks the first application of large language models to table-based question answering tasks, enhancing the model's comprehension of both table structures and content.
</details>
<details>
<summary>摘要</summary>
在自然语言处理领域中，表格数据的理解一直是学术研究的焦点。现在，大型语言模型的出现，如ChatGPT，使得研究人员尝试使用这些模型来解决表格问题。我们的探索的核心在于发展一种能够增强大型语言模型对表格结构和内容的理解，以便它们能够准确回答相关的问题。为此，我们设计了一个专门用于将表格序列化的模块，并在模型中实施了纠正机制以消除可能的错误。实验结果表明，虽然我们的提议方法相对于最佳实践（SOTA）落后约11.7%的总指标，但在特定数据集上测试时超过了SOTA约1.2%。这项研究是大型语言模型在表格问题回答上的首次应用，提高了模型对表格结构和内容的理解。
</details></li>
</ul>
<hr>
<h2 id="Integrating-the-Wikidata-Taxonomy-into-YAGO"><a href="#Integrating-the-Wikidata-Taxonomy-into-YAGO" class="headerlink" title="Integrating the Wikidata Taxonomy into YAGO"></a>Integrating the Wikidata Taxonomy into YAGO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11884">http://arxiv.org/abs/2308.11884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yago-naga/yago-4.5">https://github.com/yago-naga/yago-4.5</a></li>
<li>paper_authors: Fabian Suchanek, Mehwish Alam, Thomas Bonald, Pierre-Henri Paris, Jules Soria</li>
<li>for: The paper aims to merge the entire Wikidata taxonomy into the YAGO KB as much as possible, while maintaining logical consistency.</li>
<li>methods: The authors combine Wikidata with the ontology from Schema.org to reduce and clean up the taxonomy, and use automated reasoners to ensure logical consistency.</li>
<li>results: The authors create YAGO 4.5, which adds a rich layer of informative classes to YAGO while keeping the KB logically consistent.<details>
<summary>Abstract</summary>
Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our effort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.
</details>
<details>
<summary>摘要</summary>
wikidata是一个非常大的公共通用知识库（kb）。然而由于其协作性，其架构和分类已经变得混乱。为了构建yaogo4kb，我们将wikidata与schema.org的ontology结合了起来，这有效地减少了和约束，并使得数据可以通过自动推理。但是，这也剪辑了大量wikidata分类。在这篇论文中，我们报告了我们将wikidata分类完全 merged into yaogoKB中的努力。我们特别注重逻辑约束和精心分类和实例的区分。我们的工作创造了yaogo4.5，它添加了一层有用的类到yaogoKB中，同时保持kb的逻辑一致。
</details></li>
</ul>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
<li>for: 提高特定语言或领域上表现，以及有效地进行分词。</li>
<li>methods: 使用自scratch训练模型，并开发了一种名为Cabrita的方法ологи。</li>
<li>results: 在评估少量学习任务中，与传统连续预训练方法和7B英语预训练模型的结果相似，并且减少了分词的数量。<details>
<summary>Abstract</summary>
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.
</details>
<details>
<summary>摘要</summary>
strategy 训练模型从零开始在特定语言或领域上服务两个重要目的：一是提高特定语言或领域上表现，二是确保有效的分词。但这种方法存在一个主要的限制，即成本，可以达到6到7位数字的值，具体取决于模型大小和参数数量。为了缓解这个问题，可以依靠可用的预训练模型，尽管最近的进步，如LLaMA和LLaMA-2模型，仍然在特定领域问题上不具有效果，因为需要大量的字符表示文本。为了解决这个问题，我们提出了一种方法ологи，名为Cabrita，我们的研究表明，该方法能够成功地解决表现和有效的分词问题，并且具有可Affordable的成本。我们认为该方法可以应用于任何 transformer-like 架构模型。为了验证这种方法，我们进行了继续预训练，专门使用葡萄牙语文本，在一个3亿参数的模型OpenLLaMA上进行了不断预训练，从而获得了一个名为openCabrita 3B的模型。openCabrita 3B还使用了一种新的分词器，从而导致文本表示的字符数量减少了许多。在我们的评估中，对于几个shot学习任务，我们使用这个3B模型和传统预训练方法以及7B英语预训练模型进行比较，得到了类似的结果。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach"><a href="#Integrated-Image-and-Location-Analysis-for-Wound-Classification-A-Deep-Learning-Approach" class="headerlink" title="Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach"></a>Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11877">http://arxiv.org/abs/2308.11877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Patel, Tirth Shah, Mrinal Kanti Dhar, Taiyu Zhang, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Zeyun Yu</li>
<li>for: 提高伤口分类精度，以便更好地诊断和治疗伤口。</li>
<li>methods: 基于深度卷积神经网络的多Modal网络，使用伤口图像和其相应的体部位置进行更加精确的分类。</li>
<li>results: 比 tradicional方法高，达到了74.79%到100%的ROI（区域 интерес）无位置分类精度，73.98%到100%的ROIwith位置分类精度，和78.10%到100%的全图分类精度。<details>
<summary>Abstract</summary>
The global burden of acute and chronic wounds presents a compelling case for enhancing wound classification methods, a vital step in diagnosing and determining optimal treatments. Recognizing this need, we introduce an innovative multi-modal network based on a deep convolutional neural network for categorizing wounds into four categories: diabetic, pressure, surgical, and venous ulcers. Our multi-modal network uses wound images and their corresponding body locations for more precise classification. A unique aspect of our methodology is incorporating a body map system that facilitates accurate wound location tagging, improving upon traditional wound image classification techniques. A distinctive feature of our approach is the integration of models such as VGG16, ResNet152, and EfficientNet within a novel architecture. This architecture includes elements like spatial and channel-wise Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated Multi-Layer Perceptron, providing a robust foundation for classification. Our multi-modal network was trained and evaluated on two distinct datasets comprising relevant images and corresponding location information. Notably, our proposed network outperformed traditional methods, reaching an accuracy range of 74.79% to 100% for Region of Interest (ROI) without location classifications, 73.98% to 100% for ROI with location classifications, and 78.10% to 100% for whole image classifications. This marks a significant enhancement over previously reported performance metrics in the literature. Our results indicate the potential of our multi-modal network as an effective decision-support tool for wound image classification, paving the way for its application in various clinical contexts.
</details>
<details>
<summary>摘要</summary>
全球各类伤口的扩大问题，提出了加强伤口分类方法的需求，这是诊断和治疗伤口的重要一步。为此，我们介绍了一种创新的多模态网络，基于深度卷积神经网络，用于分类伤口为四类：糖尿病、压力、手术和血液溢出损伤。我们的多模态网络使用伤口图像和其相应的身体位置信息进行更加精确的分类。我们的方法的一个独特特点是通过身体地图系统，实现了更加准确的伤口位置标记，从传统伤口图像分类技术中的改进。我们的方法还integrates了多种模型，如VGG16、ResNet152和EfficientNet，并在一种新的架构中进行组合。这种架构包括空间和通道方向的压缩和激活模块、轴向注意力和适应阀控多层感知机制，为分类提供了坚实的基础。我们的多模态网络在两个不同的数据集上进行训练和评估，其中一个包含了相关的图像和身体位置信息，另一个只包含图像。我们的方法在这两个数据集上达到了74.79%到100%的ROI（区域 интереса）无地址分类精度范围，73.98%到100%的ROI与地址分类精度范围，以及78.10%到100%的整个图像分类精度范围。这表明我们的多模态网络在文献中已经报道的性能指标中达到了显著的提高。我们的结果表明，我们的多模态网络可以作为伤口图像分类的有效决策支持工具，为各种临床上下文应用。
</details></li>
</ul>
<hr>
<h2 id="Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0"><a href="#Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0" class="headerlink" title="Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0"></a>Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11854">http://arxiv.org/abs/2308.11854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chaure, Ashok Kumar Behera, Sudip Bhattacharya</li>
<li>for: 本研究使用数据驱动机器学习模型作为气候模拟器，以便政策制定者能够做出有知识基础的决策。</li>
<li>methods: 本研究使用机器学习模型作为计算昂贵的GCM模拟器的代理，从而降低时间和碳脚印。特别是，使用核函数特性，回归模型可以捕捉复杂关系，提高预测能力。</li>
<li>results: 在使用 клима本chmark 数据集进行评估时，我们发现， amongst three non-linear regression models, Gaussian Process Regressor 表现最佳，在标准评估指标上对气候场的模拟表现出色。然而， Gaussian Process Regression 具有空间和时间复杂度的问题。相比之下， Support Vector 和 Kernel Ridge 模型也能够达到竞争力水平，但是有一定的交易offs。此外，我们正在 актив地调查 composite kernels 和变量抽象等技术，以提高回归模型的性能，更好地模拟复杂非线性 patrerns，包括降水现象。<details>
<summary>Abstract</summary>
Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computational resource hungry in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results and but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.
</details>
<details>
<summary>摘要</summary>
政策制定者可以通过使用数据驱动机器学模型作为模拟器，来做出了 informed 的决策。通过使用机器学模型作为计算成本高GCM模拟器的代理，可以降低时间和碳脚印。在这个方向下，ClimateBench [1] 是最近筹集的气候模拟数据集，用于评估机器学模型的性能。据研究，尽管被视为基本的，但是回归模型在气候模拟方面具有许多优势。具体来说，通过核心技术，回归模型可以捕捉复杂的关系，提高预测能力。本研究将对非线性回归模型进行评估，并比较其表现。Specifically，我们将 comparing the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computationally resource-intensive in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results, but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.
</details></li>
</ul>
<hr>
<h2 id="A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data"><a href="#A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data" class="headerlink" title="A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data"></a>A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11849">http://arxiv.org/abs/2308.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Liu, Zhiyuan Lin, Judith Y. T. Wang, Hong Chen</li>
<li>For: This paper aims to provide a demand-responsive approach for real-time railway rescheduling during severe emergency events such as natural disasters, with a focus on a heavy-demand station upstream of the disrupted area.* Methods: The paper proposes using mobile data (MD) to infer real-world passenger mobility and avoid overcrowding at the target station, and a deep reinforcement learning (DRL) framework to determine the optimal reschededuled timetable, route stops, and rolling stock allocation.* Results: The paper addresses challenges such as station overcrowding, rolling stock shortage, open-ended disruption duration, and delays due to detours, and aims to improve the efficiency and safety of real-time railway rescheduling during emergency events.<details>
<summary>Abstract</summary>
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accumulation of overcrowded passengers at this station, to prevent additional accidents arising from overcrowding. This research addresses the challenges associated with this scenario, including the dynamics of arriving and leaving of passengers, station overcrowding, rolling stock shortage, open-ended disruption duration, integrated rescheduling on multiple routes, and delays due to detours. A deep reinforcement learning (DRL) framework is proposed to determine the optimal rescheduled timetable, route stops, and rolling stock allocation, while considering real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.
</details>
<details>
<summary>摘要</summary>
现实时铁路重新规划是一种时间变化的和灵活的技术，可以自动修改运营计划应对时间变化的条件。现有研究缺乏基于实时旅客流动数据的数据驱动方法，而是主要依赖于 Origin-Destination（OD）数据和模型基本方法来估算列车需求。同时，长期干扰的调度原则忽略了旅客需求的不均分布。为了填补这一漏洞，本文提出了一种需求响应的方法，通过推理实际旅客流动数据（MD）来促进实时重新规划。与传统网络水平方法不同，本文将注重一个重要的快速车站，该站位于干扰区域之上游。目标是重新规划通过该站的多个路线的列车，这些列车受到严重紧急事件（如自然灾害）的影响。特别是要避免该站堵塞的乘客堆积，以避免由过度堆积而导致的一次性事故。本研究解决了这种情况中的挑战，包括到站拥堵、车厢短缺、开放式干扰持续时间、多路线集成重新规划和延迟。一种深度鼓励学习（DRL）框架被提议，以确定最佳重新规划时间表、站点停留、车厢分配，同时考虑实时需求满足、站点拥堵、车厢容量利用和距离安全。
</details></li>
</ul>
<hr>
<h2 id="rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning"></a>${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11842">http://arxiv.org/abs/2308.11842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dchen48/e3ac">https://github.com/dchen48/e3ac</a></li>
<li>paper_authors: Dingyang Chen, Qi Zhang</li>
<li>for: 这个论文旨在利用生物世界中的对称Pattern进行多智能体强化学习（MARL）问题的研究，以提高其在多种应用中的性能。</li>
<li>methods: 该论文使用了Euclidean symmetries作为多智能体强化学习问题的一种限制，并采用了具有对称约束的神经网络架构。</li>
<li>results: 该研究发现，通过对称约束的适应，神经网络架构在多种合作MARL benchmark中表现出色，并且具有很好的泛化能力，如零shot学习和转移学习。<details>
<summary>Abstract</summary>
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: https://github.com/dchen48/E3AC.
</details>
<details>
<summary>摘要</summary>
Identification和分析自然界中的对称征象导致了各科学领域的重要发现，如物理学中的重力法律的制定和化学结构的研究的进步。在这篇论文中，我们关注利用多智能体强化学习（MARL）问题中的欧几何对称的特性，这些特性在许多应用中很普遍。我们首先正式定义了一类马尔可夫游戏，其具有一般对称性，这使得存在对称优质值和策略。这些属性激发我们在多智能体actor-critic方法中嵌入对称约束，这种约束导致在各种合作MARLbenchmark中表现出色，并且具有很好的泛化能力，如零shot学习和转移学习在未看到的对称 patrern中。代码可以在以下链接中找到：https://github.com/dchen48/E3AC。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-Study-on-Calibration"><a href="#A-Benchmark-Study-on-Calibration" class="headerlink" title="A Benchmark Study on Calibration"></a>A Benchmark Study on Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/history1">https://github.com/Aryia-Behroziuan/history1</a></li>
<li>paper_authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu</li>
<li>for: 这种研究的目的是为了探讨神经网络模型的准确性和稳定性之间的关系，以及如何在神经网络模型中提高准确性和稳定性的方法。</li>
<li>methods: 这种研究使用了Neural Architecture Search（NAS）搜索空间，并创建了一个神经网络模型准确性评估集（Model Calibration Dataset），以探讨神经网络模型的准确性和稳定性问题。</li>
<li>results: 研究发现，模型准确性可以在不同任务之间进行泛化，而且可以使用Robustness作为准确性评估指标。另外，研究还发现了一些常见的准确性评估指标的不可靠性，以及在不同的搜索空间中，post-hoc准确性调整方法对所有模型是否具有同样的影响。此外，研究还发现了准确性和精度之间的关系，以及带区大小对准确性评估指标的影响。最后，研究发现了一些特定的建筑设计可以提高神经网络模型的准确性。<details>
<summary>Abstract</summary>
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.
</details>
<details>
<summary>摘要</summary>
深度神经网络在不同的机器学习任务中日益普及，但是随着模型复杂度的增加，它们经常面临调整问题，即使Predictive accuracy得到了提高。许多研究尝试通过数据预处理、特定的损失函数和训练框架来改进调整性能。然而，调整性质的研究受到了一定的忽视。我们的研究利用Neural Architecture Search（NAS）搜索空间，提供了详细的模型建筑空间，以便对调整性质进行全面的探索。我们专门创建了一个模型调整数据集。这个数据集评估了90个分割值和12个附加的调整测量，在117,702个Unique Neural Networks中进行了广泛的 NATS-Bench 搜索空间中进行了测试。我们的分析旨在回答一些在领域中长期存在的问题，使用我们提出的数据集：（i）可否将模型调整 generalized 到不同任务？（ii）可否使用Robustness作为调整测量？（iii）如何判定调整指标的可靠性？（iv）post-hoc calibration方法对所有模型是否具有相同的影响？（v）调整与准确度之间是否存在相互关系？（vi）分割值如何影响调整测量？（vii）哪些建筑设计对调整有利？我们的研究填补了现有的空白，通过调整在NAS中进行exploration。我们的研究表明，调整性质在NAS中存在一定的问题，并且我们的研究是这类研究中的第一个大规模调整性质的研究。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity"><a href="#Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity" class="headerlink" title="Characterizing normal perinatal development of the human brain structural connectivity"></a>Characterizing normal perinatal development of the human brain structural connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11836">http://arxiv.org/abs/2308.11836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Lana Vasung, Camilo Calixto, Ali Gholipour, Davood Karimi<br>for:这个研究的目的是为了研究新生儿期的脑发育中Structural connectome的发展趋势。methods:这个研究使用了基于时空平均的计算框架，以确定新生儿期的Structural connectivity的 normative baselines。results:研究发现，在33-44postmenstrual weeks期间，Structural connectivity发展了明显的趋势，包括全脑和局部效率的增加，特征路径长度的减少，以及脑叶和脑半球之间的连接强化。此外，研究还发现了一些偏好性特征，这些特征在不同的连接评估方法中都有一致性。<details>
<summary>Abstract</summary>
Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details>
<details>
<summary>摘要</summary>
早期大脑发展 caracterized by the formation of a highly organized structural connectome. 这个 connectome的交互性是大脑的认知能力的基础，也影响了它对疾病和环境因素的应对。因此，在早期生长阶段的量化评估结构连接性是研究正常和异常神经发展的有用工具。然而，从Diffusion MRI数据中计算structural connectivity的估计具有复杂的计算。在早期生长阶段，这些计算受到迅速发展的大脑和成像困难的挑战。此外，高 между个体变化性和不同年龄的数据也使得 Charting the normal development of the structural connectome 是困难的。因此，我们缺乏可靠的正常发展基线的结构连接度度量。在这项研究中，我们开发了一种基于时空平均的计算框架，以确定这些基线。我们使用这个框架分析33-44周孕期的结构连接度，使用166个主要数据。我们的结果表明，在早期生长阶段，结构连接度呈现了明确和强的趋势。基于分数方差和神经纤维密度的连接重量Produced the most consistent results。我们发现全球和局部效率增加，特征路径长度减少，并且广泛加强连接在大脑叶和半球之间和之间。我们还发现了相互 symmetries 的偏好，这些偏好在不同的连接重量方法之间呈现一致。这些新的计算方法和结果有用于评估早期生长阶段的正常和异常结构连接度发展。
</details></li>
</ul>
<hr>
<h2 id="Algorithm-assisted-discovery-of-an-intrinsic-order-among-mathematical-constants"><a href="#Algorithm-assisted-discovery-of-an-intrinsic-order-among-mathematical-constants" class="headerlink" title="Algorithm-assisted discovery of an intrinsic order among mathematical constants"></a>Algorithm-assisted discovery of an intrinsic order among mathematical constants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11829">http://arxiv.org/abs/2308.11829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rotem Elimelech, Ofir David, Carlos De la Cruz Mengual, Rotem Kalisch, Wolfgang Berndt, Michael Shalyt, Mark Silberstein, Yaron Hadad, Ido Kaminer</li>
<li>for: 这个论文的目的是探索数学领域中的新概念和关系，利用计算机算法和人类直觉的结合来发现新的数学常量。</li>
<li>methods: 这篇论文使用了大规模并行计算机算法，探索了巨量的参数空间，并发现了一种新的数学结构——保守矩阵场。</li>
<li>results: 这篇论文发现了一系列新的约束数学常量表达式，包括ζ(3)的多个整数值，并通过新的数学证明，证明了这些常量的不可数性。这些结果表明了计算机支持的数学研究策略的力量，并开启了新的可能性 для解决长期开放的数学问题。<details>
<summary>Abstract</summary>
In recent decades, a growing number of discoveries in fields of mathematics have been assisted by computer algorithms, primarily for exploring large parameter spaces that humans would take too long to investigate. As computers and algorithms become more powerful, an intriguing possibility arises - the interplay between human intuition and computer algorithms can lead to discoveries of novel mathematical concepts that would otherwise remain elusive. To realize this perspective, we have developed a massively parallel computer algorithm that discovers an unprecedented number of continued fraction formulas for fundamental mathematical constants. The sheer number of formulas discovered by the algorithm unveils a novel mathematical structure that we call the conservative matrix field. Such matrix fields (1) unify thousands of existing formulas, (2) generate infinitely many new formulas, and most importantly, (3) lead to unexpected relations between different mathematical constants, including multiple integer values of the Riemann zeta function. Conservative matrix fields also enable new mathematical proofs of irrationality. In particular, we can use them to generalize the celebrated proof by Ap\'ery for the irrationality of $\zeta(3)$. Utilizing thousands of personal computers worldwide, our computer-supported research strategy demonstrates the power of experimental mathematics, highlighting the prospects of large-scale computational approaches to tackle longstanding open problems and discover unexpected connections across diverse fields of science.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
<li>for: 本研究旨在使用Context不在GPT模型训练数据中的信息来解决GPT模型无法回答最新发展或非公共文档中的问题。</li>
<li>methods: 该方法包括对Context信息进行处理、将Context和问题embedding，通过Context embedding的集成构建提问、使用GPT模型回答问题。</li>
<li>results: 在控制测试enario中，使用加州driver’s Handbook作为信息源，GPT-3模型在50个样本驾驶知识测试题上达到了96%的通过率，而无Context情况下的通过率为82%。然而，Model仍然无法正确回答一些问题，反映出还有改进空间。研究还研究了提问长度和Context格式对模型性能的影响。<details>
<summary>Abstract</summary>
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model's performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型如Open AI的生成预训练变换器（GPT）模型在回答问题方面表现出色，但它们的知识受训数据的限制。这个限制使得它们无法回答最新的发展或者非公共文档中的问题。我们的研究提出了一种方法，使得GPT模型可以通过使用不包含在它们训练数据中的信息源来回答问题。该方法包括Contextual information的处理、查询和Context的编码、通过 integrate context embedding和构建提问的推荐、使用GPT模型回答问题。我们在控制测试enario中应用了这种方法，使用加利福尼亚驾驶手册作为信息源。GPT-3模型在50个示例驾驶知识测试问题中取得96%的通过率，而无Context的情况下，模型的通过率下降到82%。然而，即使提供了库存Context，模型仍然无法回答一些问题正确，这 highlights 进一步的改进空间。研究还检查了提问长度和Context格式对模型性能的影响。总的来说，这项研究提供了GPT模型在问题回答任务中的局限性和可能的改进方向。
</details></li>
</ul>
<hr>
<h2 id="Expressive-probabilistic-sampling-in-recurrent-neural-networks"><a href="#Expressive-probabilistic-sampling-in-recurrent-neural-networks" class="headerlink" title="Expressive probabilistic sampling in recurrent neural networks"></a>Expressive probabilistic sampling in recurrent neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11809">http://arxiv.org/abs/2308.11809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirui Chen, Linxin Preston Jiang, Rajesh P. N. Rao, Eric Shea-Brown</li>
<li>for: 这篇论文的目的是解决 sampling-based Bayesian 模型中神经活动的问题，即神经活动被视为是 probablistic 计算中的样本。</li>
<li>methods: 这篇论文使用了函数分析和随机差分方程来探讨 recurrent 神经网络是如何从复杂分布中采样的。</li>
<li>results: 论文表明，使用分立输出单元的 recurrent 神经网络可以采样到任意分布，并提出了一种有效的训练方法基于减噪得分匹配。Empirical 试验表明，该模型可以采样到一些复杂数据分布。<details>
<summary>Abstract</summary>
In sampling-based Bayesian models of brain function, neural activities are assumed to be samples from probability distributions that the brain uses for probabilistic computation. However, a comprehensive understanding of how mechanistic models of neural dynamics can sample from arbitrary distributions is still lacking. We use tools from functional analysis and stochastic differential equations to explore the minimum architectural requirements for $\textit{recurrent}$ neural circuits to sample from complex distributions. We first consider the traditional sampling model consisting of a network of neurons whose outputs directly represent the samples (sampler-only network). We argue that synaptic current and firing-rate dynamics in the traditional model have limited capacity to sample from a complex probability distribution. We show that the firing rate dynamics of a recurrent neural circuit with a separate set of output units can sample from an arbitrary probability distribution. We call such circuits reservoir-sampler networks (RSNs). We propose an efficient training procedure based on denoising score matching that finds recurrent and output weights such that the RSN implements Langevin sampling. We empirically demonstrate our model's ability to sample from several complex data distributions using the proposed neural dynamics and discuss its applicability to developing the next generation of sampling-based brain models.
</details>
<details>
<summary>摘要</summary>
在基于抽样的贝叶斯模型中，神经活动被假设为抽样来自神经网络中的概率分布。然而，完整理解如何使机制模型的神经动力学可以从任意分布中抽样仍然缺乏。我们使用函数分析和随机差分方程来探索神经网络的最小建筑要求，以便它们可以从复杂的分布中抽样。我们首先考虑传统抽样模型，即一个由神经元输出直接表示抽样的网络（抽样器只网络）。我们 argue that synaptic current和神经元发射速率动力学在传统模型中有限的抽样能力。我们显示，一个具有分离输出单元的循环神经网络可以从任意概率分布中抽样。我们称之为储备抽样网络（RSN）。我们提出了一种高效的训练方法，基于排除掉噪声的对准得分，以找到循环和输出参数，使得 RSN 实现朗凡 sampling。我们employmontricate了我们的模型，并证明其可以从多种复杂数据分布中抽样，并讨论了其在开发下一代抽样基于脑模型方面的应用。
</details></li>
</ul>
<hr>
<h2 id="Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings"><a href="#Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings" class="headerlink" title="Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"></a>Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Bagdasaryan, Vitaly Shmatikov</li>
<li>for: 这种研究用于检测多modal embeddings中的攻击点，以及这些攻击点如何影响下游任务。</li>
<li>methods: 研究人员使用了多modal embeddings，并通过对这些 embeddings 进行攻击来证明它们的易攻击性。</li>
<li>results: 研究发现，使用这种攻击方法可以让恶意者将任意输入与其他模式的输入相关联，从而影响多modal embeddings 的性能。<details>
<summary>Abstract</summary>
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.   Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
</details>
<details>
<summary>摘要</summary>
多模态编码器将图像、声音、文本、视频等转换到单一的嵌入空间中，使表示之间匹配（例如，将一张狗图像与一个叫声相对应）。我们表明，多模态嵌入可能敏感于我们称为“ adversarial 幻觉”的攻击。给定任意模式的输入，敌方可以对其进行扰动，使其嵌入接近另一种选择的敌方输入的嵌入。幻觉如此能让敌方将任意图像与任意文本、任意声音等相对应。这些幻觉利用嵌入空间的近似性，因此不受下游任务的限制。使用 ImageBind 嵌入，我们示例了如何使用不知道下游任务的情况，通过生成 adversarially 对齐的输入，使图像生成、文本生成和零学习分类发生幻觉。
</details></li>
</ul>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
<li>for: 这个论文的目的是建立一个自我超vised的深度和 egocentric 运动估计系统，以便在外科视频中提取深度图和摄像头参数。</li>
<li>methods: 该论文使用了一种基于 cost-volume 的超vision 方法来为系统提供辅助supervision，以便预测摄像头参数。</li>
<li>results: 实验结果表明，提议的方法可以改善摄像头参数、 egocentric 运动和深度估计的准确性。<details>
<summary>Abstract</summary>
Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.
</details>
<details>
<summary>摘要</summary>
depth 估算在手术视频中发挥重要作用，但创建深度地图真实数据集在手术视频中具有不一致的亮度和噪声，因此建立精准和可靠的自我超视方法在计算机视觉社区中受到更多的关注。虽然一些自我超视方法可以减少需要深度地图和姿态的真实数据，但它们仍然需要已知的摄像头内参数，这些参数通常缺失或者不记录。此外，现有的摄像头内参数预测方法依赖于数据质量的改进。在这项工作中，我们希望建立一个可以预测不仅准确的深度地图和摄像头姿态，还可以预测摄像头内参数的自我超视系统。我们提出了基于Cost Volume的超视方式，以供系统进行摄像头参数预测的auxiliary超视。实验结果表明，我们的方法可以提高摄像头参数、ego-动作和深度估算的准确性。
</details></li>
</ul>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
<li>for: 这 paper 是为了开发一种基于 Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) 模型，用于Event-based eye tracking，这是下一代可穿戴医疗技术，如 AR&#x2F;VR 头盔。</li>
<li>methods: 这 paper 使用了 retina-inspired event cameras，它们具有低延迟响应和稀疏输出事件流，而不是传统的帧基本摄像头。CB-ConvLSTM 架构 efficiently 提取了 spatial-temporal 特征，用于 pupil tracking，并且比 convential CNN 结构更高效。</li>
<li>results: 这 paper 使用 delta-encoded recurrent path 提高了 activation sparsity，从而减少了数学运算量约 4.7 $\times$，不会失去准确性。这使得它成为实时眼动跟踪的理想选择，特别是在资源有限的设备上。项目代码和数据集都公开可用于 \url{<a target="_blank" rel="noopener" href="https://github.com/qinche106/cb-convlstm-eyetracking%7D">https://github.com/qinche106/cb-convlstm-eyetracking}</a>.<details>
<summary>Abstract</summary>
This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种稀疏变化基于卷积长短期记忆遮盾（CB-ConvLSTM）模型，用于事件基于眼动跟踪，这是下一代可穿戴医疗技术如AR/VR头戴式设备的关键。我们利用了眼睛引发的事件摄像头的优点，即快速响应和稀疏输出事件流，而不是传统的帧基本摄像头。我们的CB-ConvLSTM架构有效地提取了眼动跟踪的空间时间特征，超过了传统的CNN结构。通过使用delta编码的回归路增强活动稀疏，CB-ConvLSTM可以减少笔算操作数约4.7倍，无损loss性能，使其适用于实时眼动跟踪 resource-constrained设备。项目代码和数据集在GitHub上公开可用，请参考\url{https://github.com/qinche106/cb-convlstm-eyetracking}.
</details></li>
</ul>
<hr>
<h2 id="Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models"><a href="#Halo-Estimation-and-Reduction-of-Hallucinations-in-Open-Source-Weak-Large-Language-Models" class="headerlink" title="Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models"></a>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11764">http://arxiv.org/abs/2308.11764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/engsalem/halo">https://github.com/engsalem/halo</a></li>
<li>paper_authors: Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu</li>
<li>for: 这 paper 的目的是量化和缓解强大语言模型（LLMs）中的幻觉现象。</li>
<li>methods: 这 paper 使用了一种名为 HaloCheck 的轻量级黑盒无知框架，以量化 LLMS 中幻觉的严重程度。此外，paper 还探讨了知识注入和教师学生方法来缓解 LLMS 中幻觉现象。</li>
<li>results:  experiments 表明，使用 HaloCheck 和其他技术可以有效缓解 LLMS 中幻觉现象，特别是在复杂的领域。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型自然语言处理模型（LLM）已经革命化了自然语言处理（NLP）领域。虽然对研究和实际应用来说非常方便，但公共源 LLM 的 fewer parameters  often suffer from severe hallucinations compared to their larger counterparts. 这篇论文关注测量和减少 LLM 中的 hallucinations，特别是公共源 LLM 中的 BLOOM 7B。我们介绍 HaloCheck，一种轻量级黑盒无知框架，用于评估 LLM 中 hallucinations 的严重程度。此外，我们还探讨了如何通过知识注入和教师-学生方法来缓解低参数 LLM 中的 hallucinations。我们的实验效果地示了在这些 LLM 中的挑战领域中减少 hallucinations。
</details></li>
</ul>
<hr>
<h2 id="VBMO-Voting-Based-Multi-Objective-Path-Planning"><a href="#VBMO-Voting-Based-Multi-Objective-Path-Planning" class="headerlink" title="VBMO: Voting-Based Multi-Objective Path Planning"></a>VBMO: Voting-Based Multi-Objective Path Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11755">http://arxiv.org/abs/2308.11755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raj Korpan</li>
<li>for: 本研究开发了一种VBMO算法，用于生成优化单个目标计划，并对每个目标进行评估，使用投票机制来选择最佳计划。</li>
<li>methods: VBMO算法不使用手动调整的权重，不是基于进化算法，而是根据一个计划在一个目标方面的优化程度来评估其在其他目标方面的表现。VBMO使用三种投票机制：范围、波达和组合批准。</li>
<li>results: 对于多种和复杂的环境，VBMO算法能够高效生成满足多个目标的计划。<details>
<summary>Abstract</summary>
This paper presents VBMO, the Voting-Based Multi-Objective path planning algorithm, that generates optimal single-objective plans, evaluates each of them with respect to the other objectives, and selects one with a voting mechanism. VBMO does not use hand-tuned weights, consider the multiple objectives at every step of search, or use an evolutionary algorithm. Instead, it considers how a plan that is optimal in one objective may perform well with respect to others. VBMO incorporates three voting mechanisms: range, Borda, and combined approval. Extensive evaluation in diverse and complex environments demonstrates the algorithm's ability to efficiently produce plans that satisfy multiple objectives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Instance-Adversarial-Attack-on-GNN-Based-Malicious-Domain-Detection"><a href="#Multi-Instance-Adversarial-Attack-on-GNN-Based-Malicious-Domain-Detection" class="headerlink" title="Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection"></a>Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11754">http://arxiv.org/abs/2308.11754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Nazzal, Issa Khalil, Abdallah Khreishah, NhatHai Phan, Yao Ma</li>
<li>for: 这种研究的目的是检测互联网域名是否与网络攻击相关。</li>
<li>methods: 该方法使用图神经网络（GNN）来推断互联网域名的危险程度，并使用DNS日志来构建域名图（DMG）。</li>
<li>results: 该研究发现，现有的单个攻击者节点 manipulation 技术不具备防止多个节点同时 manipulate 的能力，并且提出了一种基于黑盒模型的多实例攻击方法（MintA），可以在无法访问模型的情况下进行攻击。该方法可以在实际数据上实现攻击成功率超过 80%。<details>
<summary>Abstract</summary>
Malicious domain detection (MDD) is an open security challenge that aims to detect if an Internet domain is associated with cyber-attacks. Among many approaches to this problem, graph neural networks (GNNs) are deemed highly effective. GNN-based MDD uses DNS logs to represent Internet domains as nodes in a maliciousness graph (DMG) and trains a GNN to infer their maliciousness by leveraging identified malicious domains. Since this method relies on accessible DNS logs to construct DMGs, it exposes a vulnerability for adversaries to manipulate their domain nodes' features and connections within DMGs. Existing research mainly concentrates on threat models that manipulate individual attacker nodes. However, adversaries commonly generate multiple domains to achieve their goals economically and avoid detection. Their objective is to evade discovery across as many domains as feasible. In this work, we call the attack that manipulates several nodes in the DMG concurrently a multi-instance evasion attack. We present theoretical and empirical evidence that the existing single-instance evasion techniques for are inadequate to launch multi-instance evasion attacks against GNN-based MDDs. Therefore, we introduce MintA, an inference-time multi-instance adversarial attack on GNN-based MDDs. MintA enhances node and neighborhood evasiveness through optimized perturbations and operates successfully with only black-box access to the target model, eliminating the need for knowledge about the model's specifics or non-adversary nodes. We formulate an optimization challenge for MintA, achieving an approximate solution. Evaluating MintA on a leading GNN-based MDD technique with real-world data showcases an attack success rate exceeding 80%. These findings act as a warning for security experts, underscoring GNN-based MDDs' susceptibility to practical attacks that can undermine their effectiveness and benefits.
</details>
<details>
<summary>摘要</summary>
“恶意域名检测（MDD）是一个开放的安全挑战，旨在检测互联网域名是否与网络攻击相关。许多方法来解决这个问题，Graph Neural Networks（GNN）被视为非常有效。GNN基于的 MDD 使用 DNS 日志来表示互联网域名为节点在恶意域名图（DMG）中，并使用 GNN 来推断它们的恶意程度，利用已知的恶意域名。由于这种方法依赖于可 accessible DNS 日志来构建 DMG，因此暴露了一个攻击者可以 manipulate 其域名节点的特征和连接在 DMG 中的漏洞。现有的研究主要集中在单个攻击者节点的威胁模型上。然而，攻击者通常会生成多个域名来实现他们的目标，以避免检测。我们称这种攻击为多实例逃脱攻击。我们提供了证明和实验证据，证明现有的单实例逃脱技术无法对 GNN 基于的 MDD 进行多实例逃脱攻击。因此，我们引入 MintA，一种在检测时进行多实例逃脱攻击的含糊逻辑攻击。MintA 通过优化的杂化和只有黑盒访问目标模型的能力，实现节点和邻居隐蔽性的提高。我们提出了 MintA 的优化挑战，并实现了一个近似解决方案。对一种主流 GNN 基于 MDD 技术进行实验，MintA 的攻击成功率超过 80%。这些发现作为一个警告，强调 GNN 基于 MDD 的抵御力和优势受到了实际攻击的威胁。”
</details></li>
</ul>
<hr>
<h2 id="Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data"><a href="#Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data" class="headerlink" title="Patient Clustering via Integrated Profiling of Clinical and Digital Data"></a>Patient Clustering via Integrated Profiling of Clinical and Digital Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11748">http://arxiv.org/abs/2308.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</li>
<li>for: 这个研究是为了开发一个基于Profile的患者划分模型，用于医疗数据分析。</li>
<li>methods: 这个模型使用基于受限制低维度approximation的方法，利用患者的临床数据和数字互动数据（包括浏览和搜索）构建患者profile。这个方法生成了非负嵌入 вектор，作为患者的低维度表示。</li>
<li>results: 对于使用真实世界患者数据集进行评估，这个方法在划分准确性和推荐精度方面表现出优于其他基线方法。<details>
<summary>Abstract</summary>
We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种基于专业 patient clustering 模型，适用于医疗数据。我们的模型通过使用受限制的低级数据方法，使用病人的临床数据和数字互动数据（包括浏览和搜索）构建病人 профи。这样，我们可以生成非负嵌入 вектор，作为病人的低维度表示。我们的模型在使用实际的病人数据from a healthcare web portal 进行评估，并通过了一种全面的评估方法，考虑 clustering 和推荐能力。与其他基准相比，我们的方法在 clustering 准确性和推荐精度方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Lifted-Inference-beyond-First-Order-Logic"><a href="#Lifted-Inference-beyond-First-Order-Logic" class="headerlink" title="Lifted Inference beyond First-Order Logic"></a>Lifted Inference beyond First-Order Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11738">http://arxiv.org/abs/2308.11738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagar Malhotra, Davide Bizzaro, Luciano Serafini</li>
<li>for: 这 paper 的目的是探讨Weighted First Order Model Counting (WFOMC) 的基本性和可行性，以及可以在 probabilistic inference 中使用的逻辑 фрагментов。</li>
<li>methods: 这 paper 使用了一种新的方法 called “counting by splitting”，用于解决 WFOMC 的难题。这种方法可以应用于多种不同的逻辑结构，如directed acyclic graphs, connected graphs, trees, etc。</li>
<li>results: 这 paper 的结果表明，使用 “counting by splitting” 方法可以将多种逻辑结构转化为 domain-liftable 的形式，从而实现 probabilistic inference。此外，这 paper 还推广了许多之前的结果，如 directed acyclic graphs, phylogenetic networks, etc。<details>
<summary>Abstract</summary>
Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($\#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\mathrm{C^2}$ with multiple such properties. We show that any $\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results rely on a novel and general methodology of "counting by splitting". Besides their application to probabilistic inference, our results provide a general framework for counting combinatorial structures. We expand a vast array of previous results in discrete mathematics literature on directed acyclic graphs, phylogenetic networks, etc.
</details>
<details>
<summary>摘要</summary>
Weighted First Order Model Counting (WFOMC) 是统计关系学习模型的基本概念。由于 WFOMC 在总体来说是NP完全的（#P-完全），因此可以在有限时间内完成的逻辑 фрагменты具有重要的科学意义。这些 фрагменты被称为域 liftable。 recent works 表明，两变量 fragments 的第一阶alogic 加上计数量词（C^2）可以域 liftable。然而，许多实际数据的特性，如社交网络中的环状图和 citations 网络中的连接性，无法在 C^2 或首阶alogic 中表示。在这种情况下，我们扩展了 C^2 的域 liftability，使其能够模型这些特性。我们证明，任何 C^2 句子都可以域 liftable，当其中一个关系被限制为表示直接径向图、连接图、树（resp. 直接树）或森林（resp. 直接森林）时。我们的结果基于一种新的通用方法，称为“计数分解”。 beside 其应用于 probabilistic inference，我们的结果提供了一个总体的计数结构框架。我们扩展了许多过去的结果 в 直接径向图、phylogenetic 网络等数学文献中。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr<br>for: 这种方法可以帮助大语言模型在多文档问答 задании中提高表现，特别是在需要深刻理解不同文档之间的逻辑关系时。methods: 这种方法包括建立知识图和LM帮助图 traversal模块，用于导航多文档之间的Semantic&#x2F;语言相似性和结构关系。results: 广泛的实验表明，这种方法可以提高大语言模型在多文档问答 задании中的表现，并且可以减少检索延迟。<details>
<summary>Abstract</summary>
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at https://github.com/YuWVandy/KG-LLM-MDQA.
</details>
<details>
<summary>摘要</summary>
“对多篇文档问题回答（MD-QA）任务，大型语言模型（LLM）的“预训、提示、预测”模式已经实现了杰出的成功。然而，现有的研究几乎没有探讨这种模式在MD-QA任务中的应用。为了填补这个重要的空白，我们提出了知识图表示法（KGP），用于将适当的 контекст提示 LLM 进行 MD-QA，这包括对多篇文档建立知识图和对图中的node和edge进行遍历。 для知识图建立，我们创建了多篇文档之间的知识图，其中node表示文档或文档中的章节/表格，而edge表示文档之间的semantic/lexical相似性或内部结构相关。 для图中的遍历，我们设计了LM-导向的图游击者，可以在图中穿梭，寻找支持文档，以助LLM进行MD-QA。建立的图 acted as a global ruler，对文档之间的转换空间实现了统一的规范，同时LM-导向的游击者 acted as a local navigator，寻找适当的上下文，以逐渐进行问题回答和提高检索质量。实验结果证明了KGP的可行性和有效性，这表明了可以通过图在提高LLM的预训设计中应用。我们的代码可以在https://github.com/YuWVandy/KG-LLM-MDQA中找到。”
</details></li>
</ul>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sumankrsh/Sentiment-Analysis.ipynb">https://github.com/sumankrsh/Sentiment-Analysis.ipynb</a></li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
<li>for: 本研究旨在提高语言模型（LM）评估 benchmark的效率，不会 compromising 可靠性。</li>
<li>methods: 该研究使用 HELM  benchmark 作为测试例子， investigate 不同 benchmark 设计选择对计算成本-可靠性贸易的影响。提出一种新的度量方法 Decision Impact on Reliability（DIoR）来评估决策对可靠性的影响。</li>
<li>results: 研究发现，现有领先者在 HELM 上可能会改变，只需要移除一些低排名的模型即可；同时，一些不同的 HELM 场景选择可以导致成本计算的变化。基于这些发现，提出了一些具体的建议，可以带来计算成本的减少，而无需牺牲 benchmark 的可靠性。例如，可以通过 x100 或更多的计算成本减少来实现这一目标。<details>
<summary>Abstract</summary>
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.
</details>
<details>
<summary>摘要</summary>
LM模型的多样化化使得新一代的测试标准出现了，这些标准可以全面评估LM模型的各种能力。然而，这些评估努力的效率却得到了少量的讨论。在这篇文章中，我们提出了一个问题，即如何智能减少LM评估计算成本而不失去可靠性。使用HELM测试标准作为示例，我们研究了不同的测试标准设计选择对计算vs可靠性的负担交互的影响。我们提出了一个新的度量指标，即决策影响可靠性（DIoR），以评估这些决策的可靠性。我们发现，例如，当 removing一个低排名模型时，可以改变当前领先者的位置，并且只需几个例子即可获得正确的排名。然而，不同的HELM场景选择会导致排名差异很大。根据我们的发现，我们提出了一些具体的建议，以提高LM评估设计和使用做法，从而实现计算成本的减少，通常是x100或更多的减少，而且失去可靠性的可能性很低。
</details></li>
</ul>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这个研究是为了提出一个适应环境的路由系统，Tryage，以便自动选择适当的语言模型库中的专家模型，以满足用户的多元化工作流程和数据领域的需求，同时解决了计算、安全性和新鲜度等考虑。</li>
<li>methods: Tryage使用了一个语言模型路由器来预测下游模型的表现，然后使用一个目标函数集成表现预测、用户目标和限制（例如模型大小、模型新鲜度、安全性、 verbosity 和可读性）来做路由决策。</li>
<li>results: Tryage在多元的数据集中，包括代码、文本、医疗资料和专利，超过 Gorilla 和 GPT3.5 Turbo 在动态模型选择中，可以实现50.9% 的准确率，比 GPT3.5 Turbo 的23.6% 和 Gorilla 的10.8% 高得多。<details>
<summary>Abstract</summary>
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details>
<details>
<summary>摘要</summary>
<sys><trans-type>text</trans-type><text><![CDATA[Introduction of transformer architecture and self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200,000 models in the Hugging Face ecosystem, users struggle with selecting and optimizing models for multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. We propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection, identifying the optimal model with an accuracy of 50.9%, compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.]]></text></sys>Note that Simplified Chinese is the standard form of Chinese used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge"><a href="#Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge" class="headerlink" title="Practical Insights on Incremental Learning of New Human Physical Activity on the Edge"></a>Practical Insights on Incremental Learning of New Human Physical Activity on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11691">http://arxiv.org/abs/2308.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Arvanitakis, Jingwei Zuo, Mthandazo Ndhlovu, Hakim Hacid</li>
<li>for: This paper explores the challenges of Edge-based learning, particularly in the context of limited data storage, computing power, and the number of learning classes.</li>
<li>methods: The paper uses the MAGNETO system to conduct experiments and demonstrate the challenges of Edge ML, using data collected from mobile sensors to learn human activities.</li>
<li>results: The paper highlights the challenges of Edge ML and offers valuable perspectives on how to address them.<details>
<summary>Abstract</summary>
Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
</details>
<details>
<summary>摘要</summary>
《边缘机器学习（边缘ML）》，它将计算智能从云端系统传递到边缘设备，目前吸引了广泛关注，因为它的明显优势包括降低延迟、提高数据隐私和减少连接依赖。虽然这些优势吸引人，但它们也引入了传统云端方法中缺失的挑战。本文将探讨边缘学习的细节，探讨（i）边缘设备的受限数据存储、（ii）训练计算能力的限制和（iii）学习类数。通过我们的MAGNETO系统的实验，我们探讨了这些挑战，并提供了边缘ML的有价值见解。
</details></li>
</ul>
<hr>
<h2 id="Handling-the-inconsistency-of-systems-of-min-rightarrow-fuzzy-relational-equations"><a href="#Handling-the-inconsistency-of-systems-of-min-rightarrow-fuzzy-relational-equations" class="headerlink" title="Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations"></a>Handling the inconsistency of systems of $\min\rightarrow$ fuzzy relational equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12385">http://arxiv.org/abs/2308.12385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismaïl Baaj</li>
<li>for: 这篇论文研究了系统$\min-\rightarrow$抽象关系方程的不一致性。</li>
<li>methods: 该论文使用了分析方法，计算了基于系统$\min-\rightarrow$抽象关系方程的Chebysev距离$\nabla &#x3D; \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$。</li>
<li>results: 该论文显示了$\nabla$的下界是一个vector不等式的解，无论使用了哪种剩下推导器（G&quot;odel、Goguen或Lukasiewicz）。此外，在$\min-\rightarrow_{G}$系统中，$\nabla$可能是下界，而在$\min-\rightarrow_{GG}$和$\min-\rightarrow_{L}$系统中，$\nabla$总是最小值。<details>
<summary>Abstract</summary>
In this article, we study the inconsistency of systems of $\min-\rightarrow$ fuzzy relational equations. We give analytical formulas for computing the Chebyshev distances $\nabla = \inf_{d \in \mathcal{D}} \Vert \beta - d \Vert$ associated to systems of $\min-\rightarrow$ fuzzy relational equations of the form $\Gamma \Box_{\rightarrow}^{\min} x = \beta$, where $\rightarrow$ is a residual implicator among the G\"odel implication $\rightarrow_G$, the Goguen implication $\rightarrow_{GG}$ or Lukasiewicz's implication $\rightarrow_L$ and $\mathcal{D}$ is the set of second members of consistent systems defined with the same matrix $\Gamma$. The main preliminary result that allows us to obtain these formulas is that the Chebyshev distance $\nabla$ is the lower bound of the solutions of a vector inequality, whatever the residual implicator used. Finally, we show that, in the case of the $\min-\rightarrow_{G}$ system, the Chebyshev distance $\nabla$ may be an infimum, while it is always a minimum for $\min-\rightarrow_{GG}$ and $\min-\rightarrow_{L}$ systems.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们研究了系统$\min-\rightarrow$uzzifiable relational equation的不一致性。我们给出了计算$\nabla$的分解式，其相关于系统$\Gamma \Box_{\rightarrow}^{\min} x = \beta$，其中$\rightarrow$是G\"odel逻辑$\rightarrow_G$, Goguen逻辑$\rightarrow_{GG}$或Lukasiewicz逻辑$\rightarrow_L$中的剩余逻辑，而$\mathcal{D}$是定义同一个矩阵$\Gamma$的一致系统中的第二个成员的集合。主要的前提结果是$\nabla$是一个准确的下界，无论使用哪种剩余逻辑。最后，我们表明，在$\min-\rightarrow_{G}$系统中，$\nabla$可能是下界，而在$\min-\rightarrow_{GG}$和$\min-\rightarrow_{L}$系统中，$\nabla$总是最小值。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.AI_2023_08_23/" data-id="clmjn91j8000z0j88bwa13ksz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/6/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/8/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
