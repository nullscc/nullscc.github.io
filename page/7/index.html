
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/7/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.AI_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T12:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.AI_2023_08_15/">cs.AI - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science"><a href="#REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science" class="headerlink" title="REFORMS: Reporting Standards for Machine Learning Based Science"></a>REFORMS: Reporting Standards for Machine Learning Based Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07832">http://arxiv.org/abs/2308.07832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayash Kapoor, Emily Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail, Odd Erik Gundersen, Jake M. Hofman, Jessica Hullman, Michael A. Lones, Momin M. Malik, Priyanka Nanayakkara, Russell A. Poldrack, Inioluwa Deborah Raji, Michael Roberts, Matthew J. Salganik, Marta Serra-Garcia, Brandon M. Stewart, Gilles Vandewiele, Arvind Narayanan</li>
<li>for: 这篇论文的目的是提供机器学习（ML）基于科学研究的清晰报告标准。</li>
<li>methods: 这篇论文使用了一份名为REFORMS（Reporting Standards For Machine Learning Based Science）的检查列表，该列表包含32个问题和一对拥有的指南。REFORMS是基于19名研究者来自计算机科学、数据科学、数学、社会科学和医学等领域的共识而开发的。</li>
<li>results: 这篇论文提供了一个资源 для研究者在设计和实施研究时使用，以及为评审人员在审查论文时使用，以确保透明度和可重复性。<details>
<summary>Abstract</summary>
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Tightest-Admissible-Shortest-Path"><a href="#Tightest-Admissible-Shortest-Path" class="headerlink" title="Tightest Admissible Shortest Path"></a>Tightest Admissible Shortest Path</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08453">http://arxiv.org/abs/2308.08453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyal Weiss, Ariel Felner, Gal A. Kaminka</li>
<li>for: 解决Weighted Directed Graphs中的短est path问题，考虑edge-weight计算时间和不确定性的影响。</li>
<li>methods: 基于 generalized framework的提议，引入紧跟最优路径问题（TASP），解决在 bounded uncertainty 下的短est path问题，通过质量保证来提供解决方案。</li>
<li>results: 提出了一种完整的算法，并提供了解决方案的质量保证，验证结果表明该方法的有效性。<details>
<summary>Abstract</summary>
The shortest path problem in graphs is fundamental to AI. Nearly all variants of the problem and relevant algorithms that solve them ignore edge-weight computation time and its common relation to weight uncertainty. This implies that taking these factors into consideration can potentially lead to a performance boost in relevant applications. Recently, a generalized framework for weighted directed graphs was suggested, where edge-weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. We build on this framework to introduce the problem of finding the tightest admissible shortest path (TASP); a path with the tightest suboptimality bound on the optimal cost. This is a generalization of the shortest path problem to bounded uncertainty, where edge-weight uncertainty can be traded for computational cost. We present a complete algorithm for solving TASP, with guarantees on solution quality. Empirical evaluation supports the effectiveness of this approach.
</details>
<details>
<summary>摘要</summary>
<SYS>将文本翻译成简化字符串。</SYS>图形中的最短路径问题是人工智能的基础问题之一。大多数变体的问题和解决方案忽略了边Weight计算时间和Weight不确定性之间的通常关系。这意味着考虑这些因素可能会导致应用中的性能提升。最近，一种总结框架 для权重有向图被建议，其中边Weight可以在不同的精度和计算成本下重复计算。我们在这个框架之上引入了找到最紧张的可接受路径（TASP）问题，这是一种对于不确定性 bounded 的扩展，可以通过计算成本来贸易边Weight uncertainty。我们提出了一个完整的解决TASP问题的算法，并提供了解决方案质量的保证。实验证明了这种方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Identify-Critical-States-for-Reinforcement-Learning-from-Videos"><a href="#Learning-to-Identify-Critical-States-for-Reinforcement-Learning-from-Videos" class="headerlink" title="Learning to Identify Critical States for Reinforcement Learning from Videos"></a>Learning to Identify Critical States for Reinforcement Learning from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07795">http://arxiv.org/abs/2308.07795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-initiative-kaust/videorlcs">https://github.com/ai-initiative-kaust/videorlcs</a></li>
<li>paper_authors: Haozhe Liu, Mingchen Zhuge, Bing Li, Yuhui Wang, Francesco Faccio, Bernard Ghanem, Jürgen Schmidhuber</li>
<li>for: 本研究的目的是利用视频数据提取深度强化学习中的有用策略信息，不需要明确的动作信息。</li>
<li>methods: 该方法使用视频编码的集集数据，通过深度学习预测回报，然后使用面积基于的敏感分析提取重要的关键状态。</li>
<li>results: 广泛的实验显示，该方法可以理解和改进代理行为。代码和生成的数据集可以在 GitHub 上找到。<details>
<summary>Abstract</summary>
Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good policies can be extracted from offline data which lack explicit information about executed actions. For example, videos of humans or robots may convey a lot of implicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards. Without relying on ground-truth annotations, our new method called Deep State Identifier learns to predict returns from episodes encoded as videos. Then it uses a kind of mask-based sensitivity analysis to extract/identify important critical states. Extensive experiments showcase our method's potential for understanding and improving agent behavior. The source code and the generated datasets are available at https://github.com/AI-Initiative-KAUST/VideoRLCS.
</details>
<details>
<summary>摘要</summary>
近期深度强化学习（DRL）的研究表明，可以从没有明确行动信息的线上数据中提取良好策略的算法信息。例如，人类或机器人视频可以传递大量的隐式信息关于奖励行动序列，但一个DRL机器人想要从这些视频中获益，首先必须自己学习 identificifying和识别相关的状态/行动/奖励。无需基于真实标注，我们的新方法called Deep State Identifier可以预测episode编码为视频中的返回。然后使用一种mask-based敏感分析来提取/识别重要的关键状态。广泛的实验表明了我们方法的可能性 для理解和改进代理行为。源代码和生成的数据集可以在https://github.com/AI-Initiative-KAUST/VideoRLCS中下载。
</details></li>
</ul>
<hr>
<h2 id="Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance"><a href="#Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance" class="headerlink" title="Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance"></a>Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08448">http://arxiv.org/abs/2308.08448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santanu Ganguly</li>
<li>for: 本研究探讨了应用量子机器学习（QML）在金融领域的新热点研究领域，包括股票价格预测、资产风险管理和评估等。</li>
<li>methods: 本研究使用了真实的金融数据集和模拟环境，对量子机器学习（QML）模型进行比较，包括qGAN（量子生成对抗网络）和QCBM（量子环境生成机器）等模型。</li>
<li>results: 研究发现，量子机器学习（QML）在金融领域可以提供未来的量子优势，特别是在股票价格预测和资产风险管理等领域。<details>
<summary>Abstract</summary>
Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material & molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
</details>
<details>
<summary>摘要</summary>
量子机器学习（QML）是两个最有前途的研究领域之间的跨学科领域：量子计算和经典机器学习（ML）， Machine learning和人工智能（AI）被predict为第一个受到量子机器的影响的领域。量子计算机在今天的药物发现、物质和分子模型以及金融领域中使用。在这项工作中，我们讨论了在金融领域中应用量子机器学习（QML）的未来活跃研究领域。我们讨论了一些在金融世界中受到关注的QML模型，并使用实际的金融数据进行比较。我们使用 simulated environments 来评估 qGAN 和 QCBM 等模型，并显示了未来量子优势的推荐。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models"><a href="#Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models" class="headerlink" title="Informed Named Entity Recognition Decoding for Generative Language Models"></a>Informed Named Entity Recognition Decoding for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07791">http://arxiv.org/abs/2308.07791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Deußer, Lars Hillebrand, Christian Bauckhage, Rafet Sifa</li>
<li>for: 这个论文主要是为了提高命名实体识别（Named Entity Recognition，NER）的性能。</li>
<li>methods: 这篇论文提出了一种简单 yet effective的方法，即 Informed Named Entity Recognition Decoding（iNERD），它将命名实体识别视为一种生成过程，利用了最新的生成模型的语言理解能力，并采用了了一种有知识的解码方案，以便更好地处理有限的信息抽取任务。</li>
<li>results: 论文在使用五种生成语言模型，测试在八个命名实体识别 datasets 上，得到了很出色的结果，特别是在未知实体类型集合的环境下，这说明了该方法的适应性。<details>
<summary>Abstract</summary>
Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, especially in an environment with an unknown entity class set, demonstrating the adaptability of the approach.
</details>
<details>
<summary>摘要</summary>
现代语言模型在功能上不断提高，成为文本处理工具的标准配置。可是，信息提取任务，如命名实体识别，仍然受到这些进步的影响很少，因为它们主要基于上一代encoder-only transformer模型。在这里，我们提出了一种简单 yet有效的方法，命名实体识别生成（iNERD），它将命名实体识别视为生成过程。它利用最新的生成模型对语言理解能力的提高，并采用了有知识的编码方案，将开放式文本生成和信息提取的限制纳入考虑，从而提高性能并消除所有的幻觉。我们在合并的命名实体 корпу斯上粗略调整我们的模型，以强化其表现，并评估了五种生成语言模型在八个命名实体识别 datasets 上的表现，取得了非常出色的结果，特别是在未知实体类集的环境中，这表明了方法的适应性。
</details></li>
</ul>
<hr>
<h2 id="Do-We-Fully-Understand-Students’-Knowledge-States-Identifying-and-Mitigating-Answer-Bias-in-Knowledge-Tracing"><a href="#Do-We-Fully-Understand-Students’-Knowledge-States-Identifying-and-Mitigating-Answer-Bias-in-Knowledge-Tracing" class="headerlink" title="Do We Fully Understand Students’ Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing"></a>Do We Fully Understand Students’ Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07779">http://arxiv.org/abs/2308.07779</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucky7-code/core">https://github.com/lucky7-code/core</a></li>
<li>paper_authors: Chaoran Cui, Hebo Ma, Chen Zhang, Chunyun Zhang, Yumo Yao, Meng Chen, Yuling Ma</li>
<li>for: 这 paper 的目的是解决知识追踪 (KT) 中存在的答案偏见问题，以便更好地理解学生们的知识状态。</li>
<li>methods: 这 paper 使用了 causality 理论来解决 KT 中的答案偏见问题，并提出了一种 COunterfactual REasoning (CORE) 框架来减少答案偏见的影响。</li>
<li>results: 这 paper 的实验结果表明，CORE 框架可以减少 KT 中答案偏见的影响，并且可以与现有的多种 KT 模型结合使用。<details>
<summary>Abstract</summary>
Knowledge tracing (KT) aims to monitor students' evolving knowledge states through their learning interactions with concept-related questions, and can be indirectly evaluated by predicting how students will perform on future questions. In this paper, we observe that there is a common phenomenon of answer bias, i.e., a highly unbalanced distribution of correct and incorrect answers for each question. Existing models tend to memorize the answer bias as a shortcut for achieving high prediction performance in KT, thereby failing to fully understand students' knowledge states. To address this issue, we approach the KT task from a causality perspective. A causal graph of KT is first established, from which we identify that the impact of answer bias lies in the direct causal effect of questions on students' responses. A novel COunterfactual REasoning (CORE) framework for KT is further proposed, which separately captures the total causal effect and direct causal effect during training, and mitigates answer bias by subtracting the latter from the former in testing. The CORE framework is applicable to various existing KT models, and we implement it based on the prevailing DKT, DKVMN, and AKT models, respectively. Extensive experiments on three benchmark datasets demonstrate the effectiveness of CORE in making the debiased inference for KT.
</details>
<details>
<summary>摘要</summary>
知识跟踪（KT）目的是监测学生在学习过程中知识状态的变化，通过问题相关的问题来评估学生的知识水平，并可以通过预测未来问题的回答来间接评估。在这篇论文中，我们发现了一种常见的答案偏见现象，即每个问题的答案准确率和错误率存在极大的偏见。现有的模型通常会借助答案偏见作为短cut来实现高度预测性能，从而忽略了学生的知识状态。为解决这问题，我们从 causality 角度对 KT 进行了研究。首先，我们从 KT 问题中构建了一个 causal 图，并发现了答案偏见对学生回答的直接 causal 效应。基于这个 causal 图，我们提出了一种新的 COunterfactual REasoning（CORE）框架，它在训练时分别捕捉总 causal 效应和直接 causal 效应，并在测试时对答案偏见进行补做，以确保debias 的推理。CORE 框架可以应用于多种现有 KT 模型，我们在 DKT、DKVMN 和 AKT 模型上实现了它。我们在三个 benchmark 数据集上进行了广泛的实验，并证明了 CORE 在 KT 中的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-generative-modelling-for-autonomous-robots"><a href="#Hierarchical-generative-modelling-for-autonomous-robots" class="headerlink" title="Hierarchical generative modelling for autonomous robots"></a>Hierarchical generative modelling for autonomous robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07775">http://arxiv.org/abs/2308.07775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yuan, Noor Sajid, Karl Friston, Zhibin Li</li>
<li>for: 这个论文旨在研究人类在与环境交互时如何生成复杂全身运动，以便在自主机器人操作中实现高效的目标完成。</li>
<li>methods: 这篇论文使用了层次生成模型，包括多级规划和自动控制，来模拟人类动作控制的深度时间架构。</li>
<li>results: 通过数字和物理实验，这篇论文证明了使用人类动作控制算法可以实现自主机器人完成复杂任务，例如抓取和运输箱子、穿过门户、踢足球等，并在身体损伤和地面不平的情况下保持稳定性。<details>
<summary>Abstract</summary>
Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct experiments and to establish the efficacy of this formulation. Using a hierarchical generative model, we show how a humanoid robot can autonomously complete a complex task that necessitates a holistic use of locomotion, manipulation, and grasping. Specifically, we demonstrate the ability of a humanoid robot that can retrieve and transport a box, open and walk through a door to reach the destination, approach and kick a football, while showing robust performance in presence of body damage and ground irregularities. Our findings demonstrated the effectiveness of using human-inspired motor control algorithms, and our method provides a viable hierarchical architecture for the autonomous completion of challenging goal-directed tasks.
</details>
<details>
<summary>摘要</summary>
人类可以生成复杂全身运动when interacting with其 surroundings，通过规划、执行和组合各个肢体运动。我们在自主 роботизирован操作的设置下调查了这一基本的 дви作控制问题。我们采用层次生成模型，带有多级规划，以模仿人类 дви作控制的深度时间建筑。在这里，时间深度指的是成功级别模型在不同时间层次上进行的嵌套执行，例如，为了交付物品，需要一个全局规划，以Contextualize the rapid coordination of multiple local limb movements。这种时间层次分离也驱动了机器人和控制。特别是，为了实现多元感知motor控制，是在层次结构的规划和低级motor控制中进行分离。我们使用数字和物理 simulate experiments to verify the effectiveness of this approach.使用层次生成模型，我们展示了一个人工智能机器人可以自主完成一个复杂任务，需要整体使用 locomotion、抓取和抓取。例如，我们示出了一个人工智能机器人可以拾取和运送一个盒子，通过门way，然后踢过一个足球，并在存在身体损伤和地面不平的情况下表现稳定。我们的发现表明了使用人类 inspirational motor control算法的有效性，而我们的方法提供了一个可靠的层次建筑，用于自主完成具有挑战性的目标导向任务。
</details></li>
</ul>
<hr>
<h2 id="A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection"><a href="#A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection" class="headerlink" title="A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection"></a>A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07774">http://arxiv.org/abs/2308.07774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Mesgaran, A. Ben Hamza</li>
<li>for: 检测图像中异常节点</li>
<li>methods: 使用无监督图像编码器-解码器模型，学习异常分数函数对节点进行排序，并使用本地性受限的线性编码方法来找到异常分数矩阵</li>
<li>results: 在六个基准数据集上使用多种评价指标进行实验，结果显示该方法在异常检测方面具有优异性，比之前的方法更高效和可靠。<details>
<summary>Abstract</summary>
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free from learnable parameters, capable of efficiently handling large graphs, and can effectively generate a coarser graph representation while retaining the most significant structural characteristics of the graph. In the decoding stage, we propose an unpooling operation, called LCUnpool, to reconstruct both the structure and nodal features of the original graph. We conduct empirical evaluations of our method on six benchmark datasets using several evaluation metrics, and the results demonstrate its superiority over state-of-the-art anomaly detection approaches.
</details>
<details>
<summary>摘要</summary>
Many graph neural networks (GNNs) 的关键组件是聚合操作，该操作的目标是将图的大小减小，保留重要的结构信息。然而，大多数现有的图聚合策略都是基于使用 GNN 层获得的分配矩阵，这些矩阵通常具有可学习参数，导致计算复杂性很高并且解释性差。在这篇论文中，我们提出了一种无监督的图编码器-解码器模型，用于从图中检测异常节点。在编码阶段，我们设计了一种新的聚合机制，名为 LCPool，它利用了本地化的线性编码来找到一个归一化矩阵，通过解决一个最小二乘优化问题来实现。通过在编码过程中强制实施本地化约束，LCPool 设计为无学习参数，能够高效处理大图，并能够生成一个粗略的图表示，保留原图的最重要的结构特征。在解码阶段，我们提出了一种解聚机制，名为 LCUnpool，用于重建原始图的结构和节点特征。我们对六个标准数据集进行了实验评估，并通过多个评价指标证明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization"><a href="#MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization" class="headerlink" title="MOLE: MOdular Learning FramEwork via Mutual Information Maximization"></a>MOLE: MOdular Learning FramEwork via Mutual Information Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07772">http://arxiv.org/abs/2308.07772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchao Li, Yulong Pei</li>
<li>for: 这个论文旨在介绍一种异步本地学习框架，即Modular Learning Framework (MOLE)，用于神经网络。</li>
<li>methods: 这个框架通过层次归一化神经网络，定义每个模块的训练目标为相互信息增大，然后逐次训练每个模块以增大相互信息。</li>
<li>results: 实验表明，MOLE可以解决不同类型的数据，包括向量、网格和图数据。此外，MOLE还可以解决图数据上的节点级和图级任务。因此，MOLE已经在实验上证明是对不同类型数据的通用解决方案。<details>
<summary>Abstract</summary>
This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.
</details>
<details>
<summary>摘要</summary>
这份论文旨在介绍一种异步本地学习框架，名为模块学习框架（MOLE）。这个框架将神经网络归一化为层，通过互信息定义每个模块的训练目标，并逐渐训练每个模块以互信息最大化。MOLE使得训练变成了本地优化，梯度归一化在模块之间，这种方法更加生物学可靠性高于bp。我们在向量-, 网格-和图型数据上进行了实验，并证明MOLE可以解决图型数据上的图级和节点级任务。因此，MOLE已经实验证明对不同类型的数据都是通用的。
</details></li>
</ul>
<hr>
<h2 id="NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients"><a href="#NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients" class="headerlink" title="NeFL: Nested Federated Learning for Heterogeneous Clients"></a>NeFL: Nested Federated Learning for Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07761">http://arxiv.org/abs/2308.07761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Honggu Kang, Seohyeon Cha, Jinwoo Shin, Jongmyeong Lee, Joonhyuk Kang</li>
<li>For: The paper is written for discussing the issue of slow or incapable clients in federated learning (FL) and proposing a new framework called nested federated learning (NeFL) to address this issue.* Methods: The paper uses a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling, and interprets models as solving ordinary differential equations (ODEs) with adaptive step sizes.* Results: The paper demonstrates that NeFL leads to significant gains, especially for the worst-case submodel, and aligns with recent studies in FL. Specifically, the paper shows an improvement of 8.33 on CIFAR-10.<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.33 improvement on CIFAR-10). Furthermore, we demonstrate NeFL aligns with recent studies in FL.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种有前途的方法，它可以保持隐私性而在分布式学习中进行训练。然而，在 FL 的训练管道中，慢速或无力的客户端（即延迟者）会降低总训练时间和性能。系统不同性，包括不同的计算和网络带宽，已经得到了 Mitigate 的注意。先前的研究把模型分成了解决这个问题，但是它们具有较少的度量自由度，即模型体系结构。我们提出了嵌入式联邦学习（NeFL），一种总体化的框架，它可以高效地将模型分成子模型使用深度和宽度的扩展。NeFL 通过解释模型为解决常微分方程（ODEs）的解释，并使用适应步长来实现。为了解决多个子模型不同体系结构时出现的不一致性，我们划分了一些参数。NeFL 让资源受限的客户端可以有效地参与 FL 管道，并让模型在更大的数据量上进行训练。通过一系列实验，我们表明了 NeFL 带来了显著的改善，特别是最差的子模型（例如， CIFAR-10 上的 8.33 提高）。此外，我们还证明了 NeFL 与最近的 FL 研究相一致。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Embedding-Size-Search-with-Minimum-Regret-for-Streaming-Recommender-System"><a href="#Dynamic-Embedding-Size-Search-with-Minimum-Regret-for-Streaming-Recommender-System" class="headerlink" title="Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System"></a>Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07760">http://arxiv.org/abs/2308.07760</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hebowei2000/DESS">https://github.com/hebowei2000/DESS</a></li>
<li>paper_authors: Bowei He, Xu He, Renrui Zhang, Yingxue Zhang, Ruiming Tang, Chen Ma</li>
<li>for: 寻找适合不断增长的用户和项目的流动推荐系统，以适应 dynamically changing environments。</li>
<li>methods: 模型更新过程中采用了流动模型更新策略，并将 embedding layer 的大小设置为动态变量，以提高推荐性能和减少内存成本。</li>
<li>results: 对两个推荐任务中的四个公共数据集进行了实验，证明了我们的方法可以在流动环境下提供更好的推荐性能，同时具有更低的内存成本和更高的时间效率。<details>
<summary>Abstract</summary>
With the continuous increase of users and items, conventional recommender systems trained on static datasets can hardly adapt to changing environments. The high-throughput data requires the model to be updated in a timely manner for capturing the user interest dynamics, which leads to the emergence of streaming recommender systems. Due to the prevalence of deep learning-based recommender systems, the embedding layer is widely adopted to represent the characteristics of users, items, and other features in low-dimensional vectors. However, it has been proved that setting an identical and static embedding size is sub-optimal in terms of recommendation performance and memory cost, especially for streaming recommendations. To tackle this problem, we first rethink the streaming model update process and model the dynamic embedding size search as a bandit problem. Then, we analyze and quantify the factors that influence the optimal embedding sizes from the statistics perspective. Based on this, we propose the \textbf{D}ynamic \textbf{E}mbedding \textbf{S}ize \textbf{S}earch (\textbf{DESS}) method to minimize the embedding size selection regret on both user and item sides in a non-stationary manner. Theoretically, we obtain a sublinear regret upper bound superior to previous methods. Empirical results across two recommendation tasks on four public datasets also demonstrate that our approach can achieve better streaming recommendation performance with lower memory cost and higher time efficiency.
</details>
<details>
<summary>摘要</summary>
随着用户和项目的增加，传统的推荐系统通常采用静态数据集训练，但这些系统在变化的环境中难以适应。高 Throughput 数据需要模型在有效时间内进行更新，以捕捉用户兴趣动态，这导致了流处理推荐系统的出现。由于深度学习基本推荐系统的普遍性，嵌入层广泛采用低维度向量表示用户、项目和其他特征的特征。但是，已经证明将嵌入层大小设置为静态和共同的是优化推荐性和内存成本的不佳选择，特别是在流处理推荐中。为解决这个问题，我们首先重新思考流处理模型更新过程，并将动态嵌入大小搜索视为一个bandit问题。然后，我们分析和量化影响优化嵌入大小的因素，并基于这些因素提出了\textbf{D}ynamic \textbf{E}mbedding \textbf{S}ize \textbf{S}earch (\textbf{DESS})方法，以最小化嵌入大小选择 regret 在用户和项目两个方面。理论上，我们获得了superior于之前方法的下线 regret upper bound。实验结果在四个公共数据集上的两个推荐任务中也表明，我们的方法可以在不同的环境下实现更好的流处理推荐性，同时具有较低的内存成本和更高的时间效率。
</details></li>
</ul>
<hr>
<h2 id="Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification"><a href="#Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification" class="headerlink" title="Forward-Backward Reasoning in Large Language Models for Verification"></a>Forward-Backward Reasoning in Large Language Models for Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07758">http://arxiv.org/abs/2308.07758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T. Kwok</li>
<li>for: 本研究旨在提高开端问题 answering 的能力，提出了一种基于 Self-Consistency 和 backwards reasoning 的方法。</li>
<li>methods: 本方法使用了 Self-Consistency  sampling 一些可能的 reasoning chains，并使用了 backwards reasoning 来验证 candidate answers。</li>
<li>results: 实验结果表明，FOBAR 可以在六个数据集和三个 LLMS 上达到开端问题 answering 的state-of-the-art性能。<details>
<summary>Abstract</summary>
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., "\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}" Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.
</details>
<details>
<summary>摘要</summary>
Chain-of-Though (CoT) 提示法在不同的理解任务中表现出色。自Consistency \citep{wang2023selfconsistency} 提议采样多种不同的理解链，以便通过不同的答案来选择最佳答案。在这篇论文中，我们提出了一种使用反向理解的新方法，用于验证候选答案。我们将问题中的一个token用{\bf x}来mask，然后问LLM predict这个masked token，当提供了一个简单的模板，即 "\textit{\textbf{如果我们知道上面的问题的答案是\{一个候选答案\},则unknown变量{\bf x}的值是什么？}"。Intuitively，LLM是预计能够成功预测masked token，如果提供的候选答案是正确的。我们还提出了FOBAR来组合前向和反向理解来估计候选答案的概率。我们在六个数据集和三个LLM上进行了广泛的实验，实验结果表明，FOBAR在多种理解 bencmarks 上达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Dancing-Avatar-Pose-and-Text-Guided-Human-Motion-Videos-Synthesis-with-Image-Diffusion-Model"><a href="#Dancing-Avatar-Pose-and-Text-Guided-Human-Motion-Videos-Synthesis-with-Image-Diffusion-Model" class="headerlink" title="Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model"></a>Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07749">http://arxiv.org/abs/2308.07749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, Yueting Zhuang</li>
<li>for: 生成高质量人物动画视频，用于应用于游戏、电影等领域。</li>
<li>methods: 使用预训练的T2I扩散模型，通过权重学习模型来生成每帧视频，并使用文本引导和人物姿势来控制人物的动作。</li>
<li>results: 与现有state-of-the-art方法相比，Dancing Avatar可以生成高质量的人物动画视频，保持人物和背景的一致性，同时具有更高的时间协调性。<details>
<summary>Abstract</summary>
The rising demand for creating lifelike avatars in the digital realm has led to an increased need for generating high-quality human videos guided by textual descriptions and poses. We propose Dancing Avatar, designed to fabricate human motion videos driven by poses and textual cues. Our approach employs a pretrained T2I diffusion model to generate each video frame in an autoregressive fashion. The crux of innovation lies in our adept utilization of the T2I diffusion model for producing video frames successively while preserving contextual relevance. We surmount the hurdles posed by maintaining human character and clothing consistency across varying poses, along with upholding the background's continuity amidst diverse human movements. To ensure consistent human appearances across the entire video, we devise an intra-frame alignment module. This module assimilates text-guided synthesized human character knowledge into the pretrained T2I diffusion model, synergizing insights from ChatGPT. For preserving background continuity, we put forth a background alignment pipeline, amalgamating insights from segment anything and image inpainting techniques. Furthermore, we propose an inter-frame alignment module that draws inspiration from an auto-regressive pipeline to augment temporal consistency between adjacent frames, where the preceding frame guides the synthesis process of the current frame. Comparisons with state-of-the-art methods demonstrate that Dancing Avatar exhibits the capacity to generate human videos with markedly superior quality, both in terms of human and background fidelity, as well as temporal coherence compared to existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
“因应数字世界中创造生命如实的人物需求的增加，我们提出了舞动人物（Dancing Avatar），一个基于文本描述和姿势驱动的人工动画生成器。我们的方法使用预训T2I散射模型来生成每帧影像，透过autoregressive的方式实现每帧影像的生成。我们的创新在于，通过将文本描述和姿势知识融合到预训T2I散射模型中，以确保人物和背景的一致性。为保持人物的一致性，我们提出了一个内部对焦模块，将文本描述驱动的人物知识融合到预训T2I散射模型中。此外，我们还提出了一个间隔对焦模块，将预训T2I散射模型与另一个自动推理管线结合，以增强动画中人物的一致性。 Comparisons with state-of-the-art methods show that Dancing Avatar can generate high-quality human videos with superior fidelity, both in terms of human and background, as well as temporal coherence compared to existing state-of-the-art approaches.”
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks"><a href="#Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks" class="headerlink" title="Exploiting Sparsity in Automotive Radar Object Detection Networks"></a>Exploiting Sparsity in Automotive Radar Object Detection Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07748">http://arxiv.org/abs/2308.07748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Lippke, Maurice Quach, Sascha Braun, Daniel Köhler, Michael Ulrich, Bastian Bischoff, Wei Yap Tan</li>
<li>for: 本研究旨在提高自动驾驶系统中的环境识别精度，以确保系统的安全和可靠运行。</li>
<li>methods: 本文使用 sparse convolutional object detection networks，这种网络结合了高效的网格式检测和低计算资源。 authors 还提出了适应 радиар特有挑战的 sparse kernel point pillars (SKPP) 和 dual voxel point convolutions (DVPC)，以解决网格渲染和稀疏基础架构的问题。</li>
<li>results: 在 nuScenes 上测试的 SKPP-DPVCN 架构，与基eline 相比提高了4.19%，并且与之前的状态分析提高了5.89%的Car AP4.0。此外，SKPP-DPVCN 还将平均扩散错误 (ASE) 降低了21.41%。<details>
<summary>Abstract</summary>
Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
</details>
<details>
<summary>摘要</summary>
“精确的环境认知是自动驾驶系统的安全和可靠运行所必备的。这篇论文探讨了具有强大的格子基础的对象探测网络，它们可以在自动驾驶系统中提供高性能，但是它们需要大量的计算资源。本文提出了稀疑几何点柱（SKPP）和双对称点核心（DVPC）来解决格式化和稀疑网络架构的挑战。我们评估了基于SKPP-DVPC的SKPP-DPVCN架构在nuScenes上的表现，该架构比基准点出5.89%的提升和前一个状态的实验出4.19%的提升。此外，SKPP-DPVCN还 redues了平均规模错误（ASE）的值比基准点下降21.41%。”
</details></li>
</ul>
<hr>
<h2 id="Formally-Sharp-DAgger-for-MCTS-Lower-Latency-Monte-Carlo-Tree-Search-using-Data-Aggregation-with-Formal-Methods"><a href="#Formally-Sharp-DAgger-for-MCTS-Lower-Latency-Monte-Carlo-Tree-Search-using-Data-Aggregation-with-Formal-Methods" class="headerlink" title="Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods"></a>Formally-Sharp DAgger for MCTS: Lower-Latency Monte Carlo Tree Search using Data Aggregation with Formal Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07738">http://arxiv.org/abs/2308.07738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debraj Chakraborty, Damien Busatto-Gaston, Jean-François Raskin, Guillermo A. Pérez</li>
<li>for: 这 paper 的目的是提出一种高效的组合 formal methods、Monte Carlo Tree Search (MCTS) 和 deep learning 来生成大 Markov Decision processes (MDPs) 中的高质量递减时间策略。</li>
<li>methods: 这 paper 使用 model-checking 技术来引导 MCTS 算法，生成 MDP 中的高质量决策样本，并用这些样本来训练一个模仿策略。这个模仿策略可以用作在线 MCTS 搜索的导向，或者作为最低延迟时间的策略。</li>
<li>results: 这 paper 使用 statistical model checking 来检测需要更多样本的情况，并将更多样本集中在 MDP 中的不同配置下，以便训练模仿策略。并在 Frozen Lake 和 Pac-Man 环境中进行了实验，证明了该方法的有效性。<details>
<summary>Abstract</summary>
We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs). In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP. Those samples can then be used to train a neural network that imitates the policy used to generate them. This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required. We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy. We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments -- two popular benchmarks to evaluate reinforcement-learning algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究如何有效地结合正式方法、Monte Carlo Tree Search（MCTS）和深度学习，以生成高质量的回溯时间政策在大Markov决策过程（MDP）中。特别是，我们使用模型检查技术来引导MCTS算法，以生成 offline 样本高质量决策在 MDP 的表示集中。这些样本可以用来训练一个模仿政策的神经网络，这个神经网络可以在更低的延迟下在线搜索中作为引导，或者作为尽可能快的全功能政策。我们使用统计模型检查来检测需要更多的样本，并将这些样本集中在计算机严重的 offline 政策与学习的神经网络政策之间的差异。我们在 Frozen Lake 和 Pac-Man 环境中使用我们的方法进行示例。
</details></li>
</ul>
<hr>
<h2 id="Flashpoints-Signal-Hidden-Inherent-Instabilities-in-Land-Use-Planning"><a href="#Flashpoints-Signal-Hidden-Inherent-Instabilities-in-Land-Use-Planning" class="headerlink" title="Flashpoints Signal Hidden Inherent Instabilities in Land-Use Planning"></a>Flashpoints Signal Hidden Inherent Instabilities in Land-Use Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07714">http://arxiv.org/abs/2308.07714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazhir Aliahmadi, Maeve Beckett, Sam Connolly, Dongmei Chen, Greg van Anders</li>
<li>For: The paper aims to improve the objectivity and transparency of land-use decision-making processes by using optimization-based planning approaches, such as Multi-Objective Land Allocation (MOLA).* Methods: The paper uses quantitative methods to evaluate planning priorities and generate a series of unstable “flashpoints” where small changes in planning priorities lead to large-scale changes in land use.* Results: The paper shows that quantitative methods can reduce the combinatorially large space of possible land-use patterns to a small, characteristic set that can engage stakeholders to arrive at more efficient and just outcomes. Additionally, the paper identifies “gray areas” in land-use type that arise due to instabilities in the planning process.<details>
<summary>Abstract</summary>
Land-use decision-making processes have a long history of producing globally pervasive systemic equity and sustainability concerns. Quantitative, optimization-based planning approaches, e.g. Multi-Objective Land Allocation (MOLA), seemingly open the possibility to improve objectivity and transparency by explicitly evaluating planning priorities by the type, amount, and location of land uses. Here, we show that optimization-based planning approaches with generic planning criteria generate a series of unstable "flashpoints" whereby tiny changes in planning priorities produce large-scale changes in the amount of land use by type. We give quantitative arguments that the flashpoints we uncover in MOLA models are examples of a more general family of instabilities that occur whenever planning accounts for factors that coordinate use on- and between-sites, regardless of whether these planning factors are formulated explicitly or implicitly. We show that instabilities lead to regions of ambiguity in land-use type that we term "gray areas". By directly mapping gray areas between flashpoints, we show that quantitative methods retain utility by reducing combinatorially large spaces of possible land-use patterns to a small, characteristic set that can engage stakeholders to arrive at more efficient and just outcomes.
</details>
<details>
<summary>摘要</summary>
农用决策过程具有历史悠久的生产全球性平等和可持续发展问题。量化优化规划方法，例如多目标农用分配（MOLA），似乎可以提高 объекivity和透明度，由明确规划优先级来评估农用类型、量和位置。在这里，我们表明了量化规划方法中的“闪点”现象，即小 Change in 规划优先级可能导致大规模的农用类型占用量变化。我们提供了量化的证明，表明这些闪点在MOLA模型中是一种更通用的不稳定性现象，无论规划因素是否明确或暗示地表达。我们还显示了这些不稳定性导致农用类型之间的“灰色区”，即不同规划优先级下的农用类型占用量的变化范围。通过直接映射灰色区，我们表明了量化方法仍然保留了实用性，可以将可能的农用模式空间减少到一个小、特征集，以便更有效和公正的决策结果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models"><a href="#Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models" class="headerlink" title="Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models"></a>Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07706">http://arxiv.org/abs/2308.07706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchan Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal</li>
<li>for: 医疗图像分割是医疗领域中重要的应用之一，但是将文本指导integrated到图像分割模型中仍然是一个有限的进展。</li>
<li>methods: 我们提议使用多Modal vision-language模型来捕捉图像描述和图像的semantic信息，以便进行多种医疗图像的分割。</li>
<li>results: 我们的研究发现，将open-domain图像的视觉语言模型直接应用于医疗图像分割 tasks是不可靠的，但是通过微调可以提高其性能。我们在11个医疗dataset上使用4种VLMs和9种提示来评估其零基eline和微调性能。<details>
<summary>Abstract</summary>
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.   To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations in model performance based on the generated prompts.   Our findings highlight the distribution shift between the open-domain images and the medical domain and show that the segmentation models trained on open-domain images are not directly transferrable to the medical field. But their performance can be increased by finetuning them in the medical datasets. We report the zero-shot and finetuned segmentation performance of 4 Vision Language Models (VLMs) on 11 medical datasets using 9 types of prompts derived from 14 attributes.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗领域中不同临床应用中的关键。虽然当前的分割模型有效，但将文本指导 integrate into 图像特征以提高分割效果是一个有限的进展。现有的分割模型，它们主要是在开放领域图像上训练的，这引发了对其直接适用性在医疗领域的担忧。为了解决这些挑战，我们提议使用多模态视语言模型，以便从图像描述和图像中提取 semantic information，以便分割多种医疗图像。本研究对多个数据集进行了全面的评估，以评估现有的视语言模型在医疗领域是否可以进行转移。此外，我们还引入了图像描述中的变化，并评估模型的性能差异。我们的发现表明，开放领域图像和医疗领域之间存在分布差异，而且训练在开放领域图像上的模型不能直接应用于医疗领域。但是，通过训练这些模型在医疗数据集上，可以提高其性能。我们在11个医疗数据集上使用4种视语言模型进行零基础和训练性能测试，使用9种Prompt derived from 14个特征。
</details></li>
</ul>
<hr>
<h2 id="DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models"><a href="#DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models" class="headerlink" title="DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models"></a>DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07687">http://arxiv.org/abs/2308.07687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cure-lab/diffguard">https://github.com/cure-lab/diffguard</a></li>
<li>paper_authors: Ruiyuan Gao, Chenchen Zhao, Lanqing Hong, Qiang Xu</li>
<li>for: 这个研究旨在提出一种基于 semantic mismatch 的 Out-of-Distribution (OOD) 检测方法，并使用 pre-trained diffusion models 来实现。</li>
<li>methods: 本研究使用了 conditional Generative Adversarial Network (cGAN) 来增加 semantic mismatch 在图像空间中，并使用 pre-trained diffusion models 来直接进行 semantic mismatch-guided OOD 检测。</li>
<li>results: 实验结果显示 DiffGuard 能够在 Cifar-10 和 ImageNet 上达到州际级的 OOD 检测效果，并且可以与现有的 OOD 检测技术结合以 дости持续获得最佳 OOD 检测结果。<details>
<summary>Abstract</summary>
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and the original input image. We also present several test-time techniques to further strengthen such differences. Experimental results show that DiffGuard is effective on both Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily combined with existing OOD detection techniques to achieve state-of-the-art OOD detection results.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)给定一个分类器，外围样本的内在特性是其 contenuto 与所有合法类型的 semantics 不同，即 semantics mismatch。有一项最近的工作直接应用于 OOD 检测，使用 conditional Generative Adversarial Network (cGAN) 来增大图像空间中的 semantic mismatch。虽然在小 dataset 上达到了惊人的 OOD 检测性能，但是在 ImageNet  scale 上 dataset 上不可能进行训练 cGAN 因为 condition 的困难。由于 diffusion models 训练更加容易，并且可以适应多种 condition，因此在这里我们提议直接使用预训练的 diffusion models 进行 semantics mismatch 导向的 OOD 检测，名为 DiffGuard。Specifically，给定一个 OOD 输入图像和分类器预测的标签，我们尝试通过增大这些 condition 下重建 OOD 图像的 semantic difference 和原始输入图像之间的差异来增大 semantic mismatch。我们还提供了多种测试时技术来进一步强化这些差异。实验结果表明，DiffGuard 效果良好于 Cifar-10 和 ImageNet 中的困难情况，并且可以与现有 OOD 检测技术相结合以 достиieving state-of-the-art OOD 检测结果。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Multi-modal-Model-Performance-with-Adaptive-Gradient-Modulation"><a href="#Boosting-Multi-modal-Model-Performance-with-Adaptive-Gradient-Modulation" class="headerlink" title="Boosting Multi-modal Model Performance with Adaptive Gradient Modulation"></a>Boosting Multi-modal Model Performance with Adaptive Gradient Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07686">http://arxiv.org/abs/2308.07686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lihong2303/agm_iccv2023">https://github.com/lihong2303/agm_iccv2023</a></li>
<li>paper_authors: Hong Li, Xingyu Li, Pengbo Hu, Yinuo Lei, Chunxiao Li, Yi Zhou</li>
<li>for: 提高多模态学习模型的性能，解决现有的模态竞争问题。</li>
<li>methods: 提出了一种适应性Gradient Modulation方法，可以提高多模态模型的表现，并且可以应用于不同的融合策略。</li>
<li>results: 经验表明，我们的方法可以超越现有的模ulation方法，并且通过引入一种新的竞争强度度量，得到了对模态竞争的量化理解。<details>
<summary>Abstract</summary>
While the field of multi-modal learning keeps growing fast, the deficiency of the standard joint training paradigm has become clear through recent studies. They attribute the sub-optimal performance of the jointly trained model to the modality competition phenomenon. Existing works attempt to improve the jointly trained model by modulating the training process. Despite their effectiveness, those methods can only apply to late fusion models. More importantly, the mechanism of the modality competition remains unexplored. In this paper, we first propose an adaptive gradient modulation method that can boost the performance of multi-modal models with various fusion strategies. Extensive experiments show that our method surpasses all existing modulation methods. Furthermore, to have a quantitative understanding of the modality competition and the mechanism behind the effectiveness of our modulation method, we introduce a novel metric to measure the competition strength. This metric is built on the mono-modal concept, a function that is designed to represent the competition-less state of a modality. Through systematic investigation, our results confirm the intuition that the modulation encourages the model to rely on the more informative modality. In addition, we find that the jointly trained model typically has a preferred modality on which the competition is weaker than other modalities. However, this preferred modality need not dominate others. Our code will be available at https://github.com/lihong2303/AGM_ICCV2023.
</details>
<details>
<summary>摘要</summary>
而 field of multi-modal learning 的发展速度不断增长，标准的联合训练方法的缺点也日益明显。这些研究表明，联合训练模型的性能下降归结于modal competition现象。现有的方法可以通过修改训练过程来改善联合训练模型，但这些方法只适用于late fusion模型。更重要的是，modal competition的机制还没有得到解释。在这篇论文中，我们首先提出一种适应性的梯度修正方法，可以提高不同拟合策略的多Modal模型性能。广泛的实验表明，我们的方法超过了所有现有的修正方法。此外，为了有一个准确的理解modal competition的机制，我们引入了一种新的竞争力度量，它基于单模态概念，这是一种用于表示没有竞争的状态的函数。通过系统性的调查，我们的结果证明了我们的修正方法能够鼓励模型依赖于更有用的感知模式。此外，我们发现，联合训练模型通常有一个具有较弱竞争力的首选模式，但这并不意味着这个模式会完全控制其他模式。我们的代码将在https://github.com/lihong2303/AGM_ICCV2023上发布。
</details></li>
</ul>
<hr>
<h2 id="EQ-Net-Elastic-Quantization-Neural-Networks"><a href="#EQ-Net-Elastic-Quantization-Neural-Networks" class="headerlink" title="EQ-Net: Elastic Quantization Neural Networks"></a>EQ-Net: Elastic Quantization Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07650">http://arxiv.org/abs/2308.07650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuke225/eq-net">https://github.com/xuke225/eq-net</a></li>
<li>paper_authors: Ke Xu, Lei Han, Ye Tian, Shangshang Yang, Xingyi Zhang</li>
<li>for: 该 paper 目的是提出一种一键网络量化 regime， named Elastic Quantization Neural Networks (EQ-Net)，用于训练一个可重用的量化超网。</li>
<li>methods: 该 paper 使用了一种弹性量化空间 (包括弹性比特宽、粒子大小和对称) 适应不同的主流量化形式。其次，提出了 Weight Distribution Regularization Loss (WDR-Loss) 和 Group Progressive Guidance Loss (GPG-Loss) 两种损失函数来减少量化空间中 weights 和输出 logits 的分布不一致。最后，使用了遗传算法和提出的 Conditional Quantization-Aware Accuracy Predictor (CQAP) 作为估计器快速搜索混合精度量化神经网络在超网中。</li>
<li>results: 广泛的实验表明，我们的 EQ-Net 与其静态对应物以及当前最佳稳定量化方法几乎相当或更好。代码可以在 \href{<a target="_blank" rel="noopener" href="https://github.com/xuke225/EQ-Net.git%7D%7Bhttps://github.com/xuke225/EQ-Net%7D">https://github.com/xuke225/EQ-Net.git}{https://github.com/xuke225/EQ-Net}</a> 上获得。<details>
<summary>Abstract</summary>
Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to various mainstream quantitative forms. Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we incorporate genetic algorithms and the proposed Conditional Quantization-Aware Accuracy Predictor (CQAP) as an estimator to quickly search mixed-precision quantized neural networks in supernet. Extensive experiments demonstrate that our EQ-Net is close to or even better than its static counterparts as well as state-of-the-art robust bit-width methods. Code can be available at \href{https://github.com/xuke225/EQ-Net.git}{https://github.com/xuke225/EQ-Net}.
</details>
<details>
<summary>摘要</summary>
当前的模型量化方法已经表现出了减少存储空间和计算复杂度的承诺。然而，由于不同硬件支持的量化形式的多样性，现有的解决方案通常需要重复优化不同的场景。在这篇论文中，我们探索了一种一键网络量化方式，名为弹性量化神经网络（EQ-Net），旨在训练一个可以共享量化超网。首先，我们提出了弹性量化空间（包括弹性位数、粒度和对称），以适应不同主流量化形式。其次，我们提出了Weight Distribution Regularization Loss（WDR-Loss）和Group Progressive Guidance Loss（GPG-Loss）来bridging弹性量化空间中 weights和输出logits的分布不一致性。最后，我们将遗传算法和提出的Conditional Quantization-Aware Accuracy Predictor（CQAP）作为估计器，快速查找混合精度量化神经网络在超网中。广泛的实验证明了我们的EQ-Net与其静态对手以及State-of-the-art Robust Bit-Width Methods相当或甚至更好。代码可以在 \href{https://github.com/xuke225/EQ-Net.git}{https://github.com/xuke225/EQ-Net} 中获取。
</details></li>
</ul>
<hr>
<h2 id="Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping"><a href="#Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping" class="headerlink" title="Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping"></a>Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07641">http://arxiv.org/abs/2308.07641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyu Chen, Hanxuan Chen, Jiao He, Fengyu Sun, Shangling Jui</li>
<li>For: 本文提出了一种简单 yet novel的参数化线性映射方法，以实现杰出的网络压缩性能。* Methods: 该方法基于pseudo SVD（Ternary SVD，TSVD），与标准SVD不同的是，TSVD限制$U$和$V$矩阵为ternary矩阵（${\pm 1, 0}$）。这意味着在计算$U(\cdot)$和$V(\cdot)$时，只需要进行加法运算。* Results: 在各种网络和任务中，TSVD可以实现现状顶峰的网络压缩性能，包括当前基线模型如ConvNext、Swim、BERT以及大型语言模型如OPT。<details>
<summary>Abstract</summary>
We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).   Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.   We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.   In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单 yet novel的参数化线性映射方法，可以夺得惊人的网络压缩性能：一种 pseudo SVD called Ternary SVD (TSVD)。 不同于普通的 SVD，TSVD 限制 $U$ 和 $V$ 矩阵在 SVD 中到了三元矩阵形式 ($\{\pm 1, 0\}$)。这意味着在计算 $U(\cdot)$ 和 $V(\cdot)$ 时，TSVD 只需要使用加法指令，而不需要昂贵的乘法指令。我们提供了直接转移算法和训练转移算法 для TSVD，如 Post Training Quantization 和 Quantization Aware Training 等。此外，我们也 theoretically 分析了直接转移算法的整合性。在实验中，我们证明了 TSVD 可以在不同类型的网络和任务上夺得当今基线模型如 ConvNext、Swim、BERT 和大语言模型 OPT 的状态级网络压缩性能。
</details></li>
</ul>
<hr>
<h2 id="LLM-Mini-CEX-Automatic-Evaluation-of-Large-Language-Model-for-Diagnostic-Conversation"><a href="#LLM-Mini-CEX-Automatic-Evaluation-of-Large-Language-Model-for-Diagnostic-Conversation" class="headerlink" title="LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation"></a>LLM-Mini-CEX: Automatic Evaluation of Large Language Model for Diagnostic Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07635">http://arxiv.org/abs/2308.07635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoming Shi, Jie Xu, Jinru Ding, Jiali Pang, Sichen Liu, Shuqing Luo, Xingwei Peng, Lu Lu, Haihong Yang, Mingtao Hu, Tong Ruan, Shaoting Zhang</li>
<li>for: 这研究旨在提供一个统一的评估标准，以评估医疗语言模型（LLM）的诊断能力。</li>
<li>methods: 该研究首先建立了一个特有的评估标准，称为LLM特有的Mini-CEX，以评估医疗LLM的诊断能力。此外，研究者还开发了一个patient simulator，用于自动与LLM进行对话，并使用ChatGPT来自动评估诊断对话的质量。</li>
<li>results: 实验结果表明，LLM特有的Mini-CEX是一个有效和必需的评估标准，可以评估医疗LLM的诊断对话质量。此外，ChatGPT也可以自动评估诊断对话的人文特质，并提供可重复和自动比较不同LLM的能力。<details>
<summary>Abstract</summary>
There is an increasing interest in developing LLMs for medical diagnosis to improve diagnosis efficiency. Despite their alluring technological potential, there is no unified and comprehensive evaluation criterion, leading to the inability to evaluate the quality and potential risks of medical LLMs, further hindering the application of LLMs in medical treatment scenarios. Besides, current evaluations heavily rely on labor-intensive interactions with LLMs to obtain diagnostic dialogues and human evaluation on the quality of diagnosis dialogue. To tackle the lack of unified and comprehensive evaluation criterion, we first initially establish an evaluation criterion, termed LLM-specific Mini-CEX to assess the diagnostic capabilities of LLMs effectively, based on original Mini-CEX. To address the labor-intensive interaction problem, we develop a patient simulator to engage in automatic conversations with LLMs, and utilize ChatGPT for evaluating diagnosis dialogues automatically. Experimental results show that the LLM-specific Mini-CEX is adequate and necessary to evaluate medical diagnosis dialogue. Besides, ChatGPT can replace manual evaluation on the metrics of humanistic qualities and provides reproducible and automated comparisons between different LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>随着医疗推荐系统的发展，有越来越多的研究者关注开发医疗推荐系统，以提高诊断效率。然而，这些系统的评价标准尚未统一，导致诊断系统的质量和风险难以评估，从而限制了医疗推荐系统的应用场景。此外，当前的评价方法仍然依赖于人工干预，通过与医疗推荐系统进行劳动密集的对话来获取诊断对话，以及人工评估诊断对话的质量。为了解决统一评价标准的缺失，我们首先建立了一个特定于医疗推荐系统的评价标准，称为LLM特定的Mini-CEX，以评估医疗推荐系统的诊断能力。为了解决人工干预的问题，我们开发了一个模拟病人的模拟器，可以自动与医疗推荐系统进行对话，并使用ChatGPT来自动评估诊断对话的质量。实验结果表明，LLM特定的Mini-CEX是有效和必要的评价医疗诊断对话的标准，而ChatGPT可以替代人工评估，并提供可重复和自动化的对比。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Model-Compression-for-Large-Language-Models"><a href="#A-Survey-on-Model-Compression-for-Large-Language-Models" class="headerlink" title="A Survey on Model Compression for Large Language Models"></a>A Survey on Model Compression for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07633">http://arxiv.org/abs/2308.07633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang</li>
<li>for: 本文旨在概述大自然语言处理任务中的语言模型压缩技术，尤其是针对资源有限的环境下进行实用部署。</li>
<li>methods: 本文介绍了各种压缩方法，包括量化、剪辑、知识传承和更多的技术，并讲述了每种方法的最新发展和创新应用。</li>
<li>results: 本文提供了评估压缩后模型效果的方法和指标，并探讨了这些方法在实际应用中的实用性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have revolutionized natural language processing tasks with remarkable success. However, their formidable size and computational demands present significant challenges for practical deployment, especially in resource-constrained environments. As these challenges become increasingly pertinent, the field of model compression has emerged as a pivotal research area to alleviate these limitations. This paper presents a comprehensive survey that navigates the landscape of model compression techniques tailored specifically for LLMs. Addressing the imperative need for efficient deployment, we delve into various methodologies, encompassing quantization, pruning, knowledge distillation, and more. Within each of these techniques, we highlight recent advancements and innovative approaches that contribute to the evolving landscape of LLM research. Furthermore, we explore benchmarking strategies and evaluation metrics that are essential for assessing the effectiveness of compressed LLMs. By providing insights into the latest developments and practical implications, this survey serves as an invaluable resource for both researchers and practitioners. As LLMs continue to evolve, this survey aims to facilitate enhanced efficiency and real-world applicability, establishing a foundation for future advancements in the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Vision-based-Semantic-Communications-for-Metaverse-Services-A-Contest-Theoretic-Approach"><a href="#Vision-based-Semantic-Communications-for-Metaverse-Services-A-Contest-Theoretic-Approach" class="headerlink" title="Vision-based Semantic Communications for Metaverse Services: A Contest Theoretic Approach"></a>Vision-based Semantic Communications for Metaverse Services: A Contest Theoretic Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07618">http://arxiv.org/abs/2308.07618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Boon Hee Soong</li>
<li>for: 提供一个对Metaverse中人员与服务提供商之间的对话和资源分配的Semantic Communication框架，以提高用户在虚拟世界中的体验。</li>
<li>methods: 使用Contest Theory来模型用户和服务提供商之间的互动，并根据每个用户的需求进行资源分配。使用Semantic Communication技术将数据量降至51字节，从而减少网络资源的消耗。使用深度Q学网来优化优先级，以最大化性能和资源分配效率。</li>
<li>results: 比较传统平均分配方法，透过优化优先级，将下调损失率降至66.076%。提供一个为Metaverse中人员与服务提供商之间的资源分配解决方案，以提高用户在虚拟世界中的体验。<details>
<summary>Abstract</summary>
The popularity of Metaverse as an entertainment, social, and work platform has led to a great need for seamless avatar integration in the virtual world. In Metaverse, avatars must be updated and rendered to reflect users' behaviour. Achieving real-time synchronization between the virtual bilocation and the user is complex, placing high demands on the Metaverse Service Provider (MSP)'s rendering resource allocation scheme. To tackle this issue, we propose a semantic communication framework that leverages contest theory to model the interactions between users and MSPs and determine optimal resource allocation for each user. To reduce the consumption of network resources in wireless transmission, we use the semantic communication technique to reduce the amount of data to be transmitted. Under our simulation settings, the encoded semantic data only contains 51 bytes of skeleton coordinates instead of the image size of 8.243 megabytes. Moreover, we implement Deep Q-Network to optimize reward settings for maximum performance and efficient resource allocation. With the optimal reward setting, users are incentivized to select their respective suitable uploading frequency, reducing down-sampling loss due to rendering resource constraints by 66.076\% compared with the traditional average distribution method. The framework provides a novel solution to resource allocation for avatar association in VR environments, ensuring a smooth and immersive experience for all users.
</details>
<details>
<summary>摘要</summary>
“Metaverse的受欢迎程度使得虚拟世界中的人物集成变得非常重要。在Metaverse中，人物需要实时更新和渲染，以反映用户的行为。实现实时同步是复杂的，对Metaverse服务提供商（MSP）的渲染资源分配方案带来高要求。为解决这个问题，我们提议一种基于 semantics 的通信框架，利用对用户和 MSP 之间的交互进行模型化，并确定每个用户的优化资源分配策略。使用semantic通信技术可以减少无线传输中的网络资源消耗，并且我们使用 Deep Q-Network 优化奖励设置，以实现最佳性和有效的资源分配。根据优化奖励设置，用户可以选择适合自己的上传频率，从而减少由渲染资源限制引起的下采样损失，比传统均值分布方法减少了66.076%。该框架为虚拟世界中人物协调资源分配提供了一种新的解决方案，以保证所有用户都能获得平滑和充满感的体验。”
</details></li>
</ul>
<hr>
<h2 id="ERA-Enhanced-Relaxed-A-algorithm-for-Solving-the-Shortest-Path-Problem-in-Regular-Grid-Maps"><a href="#ERA-Enhanced-Relaxed-A-algorithm-for-Solving-the-Shortest-Path-Problem-in-Regular-Grid-Maps" class="headerlink" title="ERA*: Enhanced Relaxed A* algorithm for Solving the Shortest Path Problem in Regular Grid Maps"></a>ERA*: Enhanced Relaxed A* algorithm for Solving the Shortest Path Problem in Regular Grid Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10988">http://arxiv.org/abs/2308.10988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Ammar</li>
<li>for: 解决点到点最短路径问题在静态Regular 8- neighbbor connectivity（G8）格网中。</li>
<li>methods: 使用一种新的算法，可以看作是 Hadlock 算法的普遍化，并且与 relaxed $A^*$（$RA^*）算法相等于，但具有不同的计算策略，基于定义lookup矩阵。</li>
<li>results: 通过对不同类型和大小的格图（1290个运行在43个地图上）进行实验，证明该算法比 $RA^*$ 快2.25倍，比原始 $A^*$ 快17倍，具有更好的内存利用率，不需要存储 G Score 矩阵。<details>
<summary>Abstract</summary>
This paper introduces a novel algorithm for solving the point-to-point shortest path problem in a static regular 8-neighbor connectivity (G8) grid. This algorithm can be seen as a generalization of Hadlock algorithm to G8 grids, and is shown to be theoretically equivalent to the relaxed $A^*$ ($RA^*$) algorithm in terms of the provided solution's path length, but with substantial time and memory savings, due to a completely different computation strategy, based on defining a set of lookup matrices. Through an experimental study on grid maps of various types and sizes (1290 runs on 43 maps), it is proven to be 2.25 times faster than $RA^*$ and 17 times faster than the original $A^*$, in average. Moreover, it is more memory-efficient, since it does not need to store a G score matrix.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的算法，用于解决在静态正方形8邻连接（G8）网格上的点到点最短路径问题。这种算法可以看作是 Hadlock 算法的总线式扩展，并且与 $RA^*$ 算法在解提供的路径长度方面是等价的，但具有不同的计算策略，基于定义一组查找表。通过对不同类型和大小的网格图（1290 个运行在 43 个图）进行实验研究，这种算法被证明为 $RA^*$ 的 2.25 倍快，并且比原始 $A^*$ 快了 17 倍，平均而言。此外，它还更加具有内存效率，因为它不需要存储 G 分数矩阵。
</details></li>
</ul>
<hr>
<h2 id="SGDiff-A-Style-Guided-Diffusion-Model-for-Fashion-Synthesis"><a href="#SGDiff-A-Style-Guided-Diffusion-Model-for-Fashion-Synthesis" class="headerlink" title="SGDiff: A Style Guided Diffusion Model for Fashion Synthesis"></a>SGDiff: A Style Guided Diffusion Model for Fashion Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07605">http://arxiv.org/abs/2308.07605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taited/sgdiff">https://github.com/taited/sgdiff</a></li>
<li>paper_authors: Zhengwentai Sun, Yanghong Zhou, Honghong He, P. Y. Mok</li>
<li>For: 本研究旨在开发一种新的样式指导扩散模型（SGDiff），以解决现有图像生成模型存在的一些缺陷。* Methods: 该模型结合图像modal和预训练的文本到图像扩散模型，以实现创新的时尚图像生成。它利用补充性的样式指导，降低训练成本，并在文本输入只能控制生成的样式方面解决了一些问题。* Results: 本研究新引入了SG-Fashion数据集，该数据集专门用于时尚图像生成应用，具有高分辨率图像和广泛的衣物类别。通过了全面的缺失学习研究，我们证明了提议的模型可以生成符合类别、产品特性和风格的时尚图像。<details>
<summary>Abstract</summary>
This paper reports on the development of \textbf{a novel style guided diffusion model (SGDiff)} which overcomes certain weaknesses inherent in existing models for image synthesis. The proposed SGDiff combines image modality with a pretrained text-to-image diffusion model to facilitate creative fashion image synthesis. It addresses the limitations of text-to-image diffusion models by incorporating supplementary style guidance, substantially reducing training costs, and overcoming the difficulties of controlling synthesized styles with text-only inputs. This paper also introduces a new dataset -- SG-Fashion, specifically designed for fashion image synthesis applications, offering high-resolution images and an extensive range of garment categories. By means of comprehensive ablation study, we examine the application of classifier-free guidance to a variety of conditions and validate the effectiveness of the proposed model for generating fashion images of the desired categories, product attributes, and styles. The contributions of this paper include a novel classifier-free guidance method for multi-modal feature fusion, a comprehensive dataset for fashion image synthesis application, a thorough investigation on conditioned text-to-image synthesis, and valuable insights for future research in the text-to-image synthesis domain. The code and dataset are available at: \url{https://github.com/taited/SGDiff}.
</details>
<details>
<summary>摘要</summary>
The paper also introduces a new dataset called SG-Fashion, which is specifically designed for fashion image synthesis and includes high-resolution images and a wide range of garment categories. The authors conduct a comprehensive ablation study to examine the effectiveness of the proposed method in various conditions and demonstrate its ability to generate fashion images with the desired categories, attributes, and styles.The main contributions of this paper include a novel classifier-free guidance method for multi-modal feature fusion, a comprehensive dataset for fashion image synthesis, a thorough investigation of conditioned text-to-image synthesis, and valuable insights for future research in the text-to-image synthesis domain. The code and dataset are available online at: <https://github.com/taited/SGDiff>.
</details></li>
</ul>
<hr>
<h2 id="Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning"><a href="#Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning" class="headerlink" title="Generating Personas for Games with Multimodal Adversarial Imitation Learning"></a>Generating Personas for Games with Multimodal Adversarial Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07598">http://arxiv.org/abs/2308.07598</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Ahlberg, Alessandro Sestini, Konrad Tollmar, Linus Gisslén</li>
<li>for: 这篇论文目标是生成多个人工智能机器人可以模仿人类游戏玩家的多种玩法。</li>
<li>methods: 该论文提出了一种基于多模式生成对抗学习（MultiGAIL）的新方法，使用辅助输入参数来学习不同的人工智能玩家模式，并使用多个批评器作为奖励模型。</li>
<li>results: 实验结果表明，该方法在两个环境中（连续和离散动作空间）都有效，可以生成多个不同的人工智能玩家模式。<details>
<summary>Abstract</summary>
Reinforcement learning has been widely successful in producing agents capable of playing games at a human level. However, this requires complex reward engineering, and the agent's resulting policy is often unpredictable. Going beyond reinforcement learning is necessary to model a wide range of human playstyles, which can be difficult to represent with a reward function. This paper presents a novel imitation learning approach to generate multiple persona policies for playtesting. Multimodal Generative Adversarial Imitation Learning (MultiGAIL) uses an auxiliary input parameter to learn distinct personas using a single-agent model. MultiGAIL is based on generative adversarial imitation learning and uses multiple discriminators as reward models, inferring the environment reward by comparing the agent and distinct expert policies. The reward from each discriminator is weighted according to the auxiliary input. Our experimental analysis demonstrates the effectiveness of our technique in two environments with continuous and discrete action spaces.
</details>
<details>
<summary>摘要</summary>
现在的人工智能技术中，强化学习已经广泛应用于生成人类水平的游戏机器人。然而，这需要复杂的奖励工程，并且机器人的结果策略可能很难预测。为了模型人类多种游戏风格，超过强化学习是必要的，但这可以很难以表示为奖励函数。本文提出了一种新的模仿学习方法，可以生成多个人格策略用于游戏测试。我们称之为多modal生成对抗学习（MultiGAIL）。MultiGAIL使用了一个辅助输入参数，通过单个机器人模型来学习不同的人格。我们使用多个判据器作为奖励模型，通过比较机器人和各个专家策略来推断环境奖励。每个判据器的奖励得分被Weighted According to辅助输入。我们的实验分析表明，我们的技术在 kontinuous 和 discrete 动作空间中的两个环境中具有效果。
</details></li>
</ul>
<hr>
<h2 id="AutoLTS-Automating-Cycling-Stress-Assessment-via-Contrastive-Learning-and-Spatial-Post-processing"><a href="#AutoLTS-Automating-Cycling-Stress-Assessment-via-Contrastive-Learning-and-Spatial-Post-processing" class="headerlink" title="AutoLTS: Automating Cycling Stress Assessment via Contrastive Learning and Spatial Post-processing"></a>AutoLTS: Automating Cycling Stress Assessment via Contrastive Learning and Spatial Post-processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07580">http://arxiv.org/abs/2308.07580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Lin, Shoshanna Saxe, Timothy C. Y. Chan</li>
<li>for: 这个论文是为了提供一种快速、精准和大规模的自行车压力评估方法，以便在城市道路网中规划自行车设施和路线建议。</li>
<li>methods: 这个论文使用了深度学习框架，利用街景图像来支持快速、精准和大规模的自行车压力评估。具体来说，这个框架包括一种对比学习方法，利用自行车压力标签的顺序关系，以及一种后处理技术，以保证预测结果的空间稳定性。</li>
<li>results: 在使用了39,153条道路段的 datasets 上，我们的结果表明，我们的深度学习框架可以快速、精准地进行自行车压力评估，并且可以使用街景图像来评估自行车压力，即使没有高质量的道路几何和机动车数据。<details>
<summary>Abstract</summary>
Cycling stress assessment, which quantifies cyclists' perceived stress imposed by the built environment and motor traffics, increasingly informs cycling infrastructure planning and cycling route recommendation. However, currently calculating cycling stress is slow and data-intensive, which hinders its broader application. In this paper, We propose a deep learning framework to support accurate, fast, and large-scale cycling stress assessments for urban road networks based on street-view images. Our framework features i) a contrastive learning approach that leverages the ordinal relationship among cycling stress labels, and ii) a post-processing technique that enforces spatial smoothness into our predictions. On a dataset of 39,153 road segments collected in Toronto, Canada, our results demonstrate the effectiveness of our deep learning framework and the value of using image data for cycling stress assessment in the absence of high-quality road geometry and motor traffic data.
</details>
<details>
<summary>摘要</summary>
《单车压力评估》，它衡量单车者对建筑环境和机动车流的感知压力，逐渐成为单车基础设施规划和单车路径建议的重要指标。但目前计算单车压力却是慢且资料密集的，这限制了它的广泛应用。在这篇论文中，我们提出了一个深度学习框架，用于支持精确、快速、大规模的单车压力评估，基于街景影像。我们的框架包括：①一种对比学习方法，利用单车压力标签之间的顺序关系，并②一种后处理技术，对我们的预测进行空间稳定化。在加拿大多伦多的39,153条道路段的数据集上，我们的结果显示了我们的深度学习框架的有效性，以及使用影像数据进行单车压力评估在缺乏高品质道路几何和机动车流数据的情况下的价值。
</details></li>
</ul>
<hr>
<h2 id="IoT-Data-Trust-Evaluation-via-Machine-Learning"><a href="#IoT-Data-Trust-Evaluation-via-Machine-Learning" class="headerlink" title="IoT Data Trust Evaluation via Machine Learning"></a>IoT Data Trust Evaluation via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11638">http://arxiv.org/abs/2308.11638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Timothy Tadj, Reza Arablouei, Volkan Dedeoglu</li>
<li>for: 评估互联网器件（IoT）数据的信任性。</li>
<li>methods: 使用随机散步填充（RWI）方法生成具有不可靠性的数据，并从感知器件数据中提取有效地捕捉自适应性和它们与邻居传感器数据的相关性的新特征。</li>
<li>results: 通过对多种基于机器学习（ML）的 IoT 数据信任评估方法进行广泛的实验，发现常见的 ML 基于方法表现不佳，这可以归因于不可靠的假设，即归一化提供可靠的标签 для数据信任。同时，通过使用RWI生成的数据和提取的特征，ML模型在未看到的数据上进行了好的普适性和超越性。此外， semi-supervised ML 方法，只需要约 10% 的数据标注，可以提供竞争力强的表现，并且在实际应用中更加具有实用性。<details>
<summary>Abstract</summary>
Various approaches based on supervised or unsupervised machine learning (ML) have been proposed for evaluating IoT data trust. However, assessing their real-world efficacy is hard mainly due to the lack of related publicly-available datasets that can be used for benchmarking. Since obtaining such datasets is challenging, we propose a data synthesis method, called random walk infilling (RWI), to augment IoT time-series datasets by synthesizing untrustworthy data from existing trustworthy data. Thus, RWI enables us to create labeled datasets that can be used to develop and validate ML models for IoT data trust evaluation. We also extract new features from IoT time-series sensor data that effectively capture its auto-correlation as well as its cross-correlation with the data of the neighboring (peer) sensors. These features can be used to learn ML models for recognizing the trustworthiness of IoT sensor data. Equipped with our synthesized ground-truth-labeled datasets and informative correlation-based feature, we conduct extensive experiments to critically examine various approaches to evaluating IoT data trust via ML. The results reveal that commonly used ML-based approaches to IoT data trust evaluation, which rely on unsupervised cluster analysis to assign trust labels to unlabeled data, perform poorly. This poor performance can be attributed to the underlying unsubstantiated assumption that clustering provides reliable labels for data trust, a premise that is found to be untenable. The results also show that the ML models learned from datasets augmented via RWI while using the proposed features generalize well to unseen data and outperform existing related approaches. Moreover, we observe that a semi-supervised ML approach that requires only about 10% of the data labeled offers competitive performance while being practically more appealing compared to the fully-supervised approaches.
</details>
<details>
<summary>摘要</summary>
各种基于监督或无监督机器学习（ML）方法已经提出来评估互联网器件（IoT）数据的信任性。然而，在实际世界中评估这些方法的效果很难，主要因为缺乏相关的公共可用数据集，可以用于比较。为了解决这个问题，我们提议一种数据生成方法，即随机游走填充（RWI），用于增强IoT时序数据集。这种方法可以生成可信worthy数据，并将其与现有的可信worthy数据相结合，以生成可靠的标注数据集。这些标注数据集可以用于开发和验证ML模型，以评估IoT数据的信任性。此外，我们还提取了IoT时序感知器数据中有效地捕捉自身的自相关性以及与邻居感知器数据的相关性。这些特征可以用于学习ML模型，以识别IoT感知器数据的信任性。利用我们生成的标注数据集和有用的相关特征，我们进行了广泛的实验，用以检验不同的IoT数据信任评估方法。结果显示，通常用于IoT数据信任评估的ML基于方法，即基于归一化分析进行无监督标注，表现不佳。这种不佳表现可以归因于下面的前提，即归一化分析提供了可靠的标注数据，这是一个不可靠的假设。另外，我们发现，使用RWI生成的数据集和相关特征来学习ML模型，可以在未看过数据时达到比较好的表现，并且与现有相关方法相比，具有更好的普适性。此外，我们还发现，一种半监督的ML方法，只需要标注约10%的数据，可以达到相对较高的表现，而且在实际应用中更加实际。
</details></li>
</ul>
<hr>
<h2 id="Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory"><a href="#Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory" class="headerlink" title="Story Visualization by Online Text Augmentation with Context Memory"></a>Story Visualization by Online Text Augmentation with Context Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07575">http://arxiv.org/abs/2308.07575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonseivnl/cmota">https://github.com/yonseivnl/cmota</a></li>
<li>paper_authors: Daechul Ahn, Daneul Kim, Gwangmo Song, Seung Hwan Kim, Honglak Lee, Dongyeop Kang, Jonghyun Choi</li>
<li>for: 提高 Story Visualization  task 的效果，使得模型能够更好地从文本描述中提取视觉细节并在多句文本中保持上下文。</li>
<li>methods: 提出了一种基于 Bi-directional Transformer 框架的新内存体系结构，并在训练过程中使用在线文本增强来生成多个 Pseudo-descriptions 作为补做性的超级vision 。</li>
<li>results: 在 Pororo-SV 和 Flintstones-SV 两个常用的 Story Visualization 测试集上，提出的方法significantly 超过了现有的状态天地，包括 FID、character F1、frame accuracy、BLEU-2&#x2F;3 和 R-precision 等多个维度的metric ，同时 computation complexity 相对较低。<details>
<summary>Abstract</summary>
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformer framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.
</details>
<details>
<summary>摘要</summary>
Story visualization (SV) 是一个具有挑战性的文本到图像生成任务，因为不仅需要从文本描述中提取视觉细节，还需要编码长期上下文 Across multiple sentences。 而且，在给出的段落中生成语言上下文感的图像（例如，正确的人物或场景背景）仍然是一个挑战。为此，我们提议一种新的储存架构，用于 Bi-directional Transformer 框架中的在线文本增强，在训练期间生成多个 pseudo-descriptions 作为补充性超级视图，以提高语言变化的泛化性。在两个流行的 SV 测试基准上，即 Pororo-SV 和 Flintstones-SV，我们的方法在多个纪录metric中显著超越了现状的术语，包括 FID、人物 F1、帧精度、BLEU-2/3 和 R-精度，同时具有相似或更低的计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Action-Class-Relation-Detection-and-Classification-Across-Multiple-Video-Datasets"><a href="#Action-Class-Relation-Detection-and-Classification-Across-Multiple-Video-Datasets" class="headerlink" title="Action Class Relation Detection and Classification Across Multiple Video Datasets"></a>Action Class Relation Detection and Classification Across Multiple Video Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07558">http://arxiv.org/abs/2308.07558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Yoshikawa, Yutaro Shigeto, Masashi Shimbo, Akikazu Takeuchi</li>
<li>for: 提高视频人体动作识别的数据集 augmenteion</li>
<li>methods: 使用语言和视觉信息关联类别进行类别关系探测和分类</li>
<li>results: 使用预训练的最新神经网络模型对文本和视频进行预测，可以获得高度预测性能，并且文本标签预测性能高于视频预测，可以将多模态融合以提高预测性能。<details>
<summary>Abstract</summary>
The Meta Video Dataset (MetaVD) provides annotated relations between action classes in major datasets for human action recognition in videos. Although these annotated relations enable dataset augmentation, it is only applicable to those covered by MetaVD. For an external dataset to enjoy the same benefit, the relations between its action classes and those in MetaVD need to be determined. To address this issue, we consider two new machine learning tasks: action class relation detection and classification. We propose a unified model to predict relations between action classes, using language and visual information associated with classes. Experimental results show that (i) pre-trained recent neural network models for texts and videos contribute to high predictive performance, (ii) the relation prediction based on action label texts is more accurate than based on videos, and (iii) a blending approach that combines predictions by both modalities can further improve the predictive performance in some cases.
</details>
<details>
<summary>摘要</summary>
meta 视频集（MetaVD）提供了动作类别之间的注解关系，这些注解关系可以用于视频人体动作识别的数据集进行数据增强。然而，这些注解关系只适用于MetaVD覆盖的数据集。为了解决这个问题，我们考虑了两个新的机器学习任务：动作类别关系检测和分类。我们提议一种统一的模型，可以预测动作类别之间的关系，使用类别相关的语言和视觉信息。实验结果表明：（i）使用最近的预训练神经网络模型对文本和视频进行预测可以获得高度的预测性能，（ii）基于动作标签文本的关系预测比基于视频的预测更准确，（iii）将两种模态的预测结果融合使用可以在一些情况下提高预测性能。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-RL-Augmented-Cold-Start-Frequency-Reduction-in-Serverless-Computing"><a href="#Reinforcement-Learning-RL-Augmented-Cold-Start-Frequency-Reduction-in-Serverless-Computing" class="headerlink" title="Reinforcement Learning (RL) Augmented Cold Start Frequency Reduction in Serverless Computing"></a>Reinforcement Learning (RL) Augmented Cold Start Frequency Reduction in Serverless Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07541">http://arxiv.org/abs/2308.07541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Agarwal, Maria A. Rodriguez, Rajkumar Buyya</li>
<li>for: 本研究旨在减少 Function-as-a-Service（FaaS）平台上的冷启动频率，通过使用人工智能学习算法来预先启动函数。</li>
<li>methods: 本研究使用了Q学习算法，考虑了函数CPU使用率、现有函数实例和响应失败率等指标，以实现在预期需求的基础上进行前置启动函数。</li>
<li>results: 对比 Kubeless 默认策略和函数保持活动策略，RL 算法能够提高吞吐量达到 8.81%，降低计算负担和资源浪费达到 55% 和 37%，这直接归结于减少冷启动。<details>
<summary>Abstract</summary>
Function-as-a-Service is a cloud computing paradigm offering an event-driven execution model to applications. It features serverless attributes by eliminating resource management responsibilities from developers and offers transparent and on-demand scalability of applications. Typical serverless applications have stringent response time and scalability requirements and therefore rely on deployed services to provide quick and fault-tolerant feedback to clients. However, the FaaS paradigm suffers from cold starts as there is a non-negligible delay associated with on-demand function initialization. This work focuses on reducing the frequency of cold starts on the platform by using Reinforcement Learning. Our approach uses Q-learning and considers metrics such as function CPU utilization, existing function instances, and response failure rate to proactively initialize functions in advance based on the expected demand. The proposed solution was implemented on Kubeless and was evaluated using a normalised real-world function demand trace with matrix multiplication as the workload. The results demonstrate a favourable performance of the RL-based agent when compared to Kubeless' default policy and function keep-alive policy by improving throughput by up to 8.81% and reducing computation load and resource wastage by up to 55% and 37%, respectively, which is a direct outcome of reduced cold starts.
</details>
<details>
<summary>摘要</summary>
Function-as-a-Service 是一种云计算 paradigm，它提供了事件驱动的执行模型，让应用程序在无需管理资源的情况下进行执行。它具有无Serverless特性，从开发者那里消除了资源管理责任，同时提供了透明的升级和缩放应用程序的功能。通常的Serverless应用程序具有严格的响应时间和可扩展性要求，因此它们通常依赖于部署的服务来提供快速的和可靠的反馈给客户端。然而，FaaS paradigm 受到冷启动的限制，即在需求时启动函数时存在非致命的延迟。这种工作将focuses on reducing the frequency of cold starts on the platform by using Reinforcement Learning。我们的方法使用Q-learning，考虑了函数 CPU 利用率、现有函数实例和响应失败率，以进行预先 initialize 函数基于预计的需求。我们的解决方案在 Kubeless 上实现，并通过使用一个 нормализован的实际函数需求轨迹进行评估。结果表明，我们的RL-based 代理比 Kubeless 的默认策略和函数保持活动策略提高了吞吐量，同时降低了计算负担和资源浪费，减少了冷启动的频率，从而提高了系统的性能。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts"><a href="#Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts" class="headerlink" title="Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts"></a>Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07538">http://arxiv.org/abs/2308.07538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Cabrera-Vives, César Bolivar, Francisco Förster, Alejandra M. Muñoz Arancibia, Manuel Pérez-Carrasco, Esteban Reyes</li>
<li>for: 这个论文是为了研究域域适应（Domain Adaptation）在天文物理数据分析中的应用，以提高天文物理数据分类的准确率。</li>
<li>methods: 该论文使用了四个不同的数据集：HiTS、DES、ATLAS和ZTF，并研究了这些数据集之间的域shift。它使用了一种简单的深度学习分类模型，并通过微调和半supervised深度域适应（MME）来改进模型。</li>
<li>results: 研究发现，只要在目标数据集上有一个或 fewer 的标注项目，就可以使得基本模型得到显著提高。此外，MME模型不会对源数据集的性能产生负面影响。<details>
<summary>Abstract</summary>
Time domain astronomy is advancing towards the analysis of multiple massive datasets in real time, prompting the development of multi-stream machine learning models. In this work, we study Domain Adaptation (DA) for real/bogus classification of astronomical alerts using four different datasets: HiTS, DES, ATLAS, and ZTF. We study the domain shift between these datasets, and improve a naive deep learning classification model by using a fine tuning approach and semi-supervised deep DA via Minimax Entropy (MME). We compare the balanced accuracy of these models for different source-target scenarios. We find that both the fine tuning and MME models improve significantly the base model with as few as one labeled item per class coming from the target dataset, but that the MME does not compromise its performance on the source dataset.
</details>
<details>
<summary>摘要</summary>
时域天文学在实时处理多个大规模数据集方面取得了进展，导致多流机器学习模型的开发。在这个工作中，我们研究天文知讯报警的域 adapted（DA）技术，用于实时/假报警分类。我们使用四个不同的数据集进行研究：HiTS、DES、ATLAS和ZTF。我们研究这些数据集之间的域转换，并通过微调和半supervised深度DA来提高基本模型的性能。我们对不同的源目标场景进行比较，发现两种方法都可以大幅提高基本模型的性能，但MME方法不会优化源数据集的性能。
</details></li>
</ul>
<hr>
<h2 id="KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification"><a href="#KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification" class="headerlink" title="KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification"></a>KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08563">http://arxiv.org/abs/2308.08563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Likang Wu, Junji Jiang, Hongke Zhao, Hao Wang, Defu Lian, Mengdi Zhang, Enhong Chen</li>
<li>for:  Zero-Shot Node Classification (ZNC) task in graph data analysis, to predict nodes from unseen classes.</li>
<li>methods:  Knowledge-Aware Multi-Faceted (KMF) framework that enhances label semantics via extracted KG-based topics, and reconstructs node content to a topic-level representation.</li>
<li>results:  Extensive experiments on several public graph datasets, with an application of zero-shot cross-domain recommendation, demonstrating the effectiveness and generalization of KMF compared to state-of-the-art baselines.Here is the same information in Simplified Chinese:</li>
<li>for:  Zero-Shot Node Classification (ZNC) 任务在图数据分析中，预测从训练过程中未经见过的节点。</li>
<li>methods:  Knowledge-Aware Multi-Faceted (KMF) 框架，通过提取的知识图(KG)来增强标签 semantics，并将节点内容重建到一个话题级别表示。</li>
<li>results: 在多个公共图据集上进行了广泛的实验，并设计了跨领域零shot推荐应用，比较了状态的基elines。<details>
<summary>Abstract</summary>
Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level representation that offers multi-faceted and fine-grained semantic relevancy to different labels. Due to the particularity of the graph's instance (i.e., node) representation, a novel geometric constraint is developed to alleviate the problem of prototype drift caused by node information aggregation. Finally, we conduct extensive experiments on several public graph datasets and design an application of zero-shot cross-domain recommendation. The quantitative results demonstrate both the effectiveness and generalization of KMF with the comparison of state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
近期，零shot节点分类（ZNC）在图数据分析中变得越来越重要。这个任务的目标是从训练过程中未见过的类型中预测节点。现有的工作主要利用图神经网络（GNNs）将特征抽象和标签含义相关联，从而实现知识传递从见到未见类型。然而，先前的工作忽略了多面性 semantic orientation的问题，即节点的内容通常涉及多个相关的标签 semantics。为了提高模型的通用性，我们提出了知识注意力多面性框架（KMF），该框架通过提取的知识图（KG）基于主题来增强标签 semantics的 ricness。然后，每个节点的内容被重建为主题级别的表示，以提供多面性和细化的semantic relevancy。由于图的实例（即节点）表示的特殊性，我们开发了一种新的 геометрических约束，以解决由节点信息汇集引起的prototype drift问题。最后，我们进行了多个公共图据集的广泛实验，并设计了零shot跨领域推荐应用。实验结果表明，KMF具有与状态 искусственныйbaseline的效果和通用性。
</details></li>
</ul>
<hr>
<h2 id="Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning"><a href="#Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning" class="headerlink" title="Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning"></a>Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07520">http://arxiv.org/abs/2308.07520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyan Wang</li>
<li>for: 这个论文的目的是找到自动搜索方法，以便从观察数据中学习 causal structure。</li>
<li>methods: 这个论文使用的方法包括提出一种弱 faithfulness 定义，以及一种修改后的 causal discovery 算法，以relaxing Various simplification assumptions，使其适用于更广泛的 causal mechanism 和统计现象。</li>
<li>results: 这个论文的结果表明，使用修改后的 causal discovery 算法，可以在不同的 distributive 下学习 causal structure，并且可以找到 latent variables 的 causal connections。<details>
<summary>Abstract</summary>
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfulness when applied to the Gaussian family of distributions, (ii) can be applied to non-Gaussian families of distributions, and (iii) under the assumption that the modified version of Strong Faithfulness holds, can be used to show the uniform consistency of a modified causal discovery algorithm; relaxing the sufficiency assumption to learn causal structures with latent variables. Given the importance of inferring cause-and-effect relationships for understanding and forecasting complex systems, the work in this thesis of relaxing various simplification assumptions is expected to extend the causal discovery method to be applicable in a wider range with diversified causal mechanism and statistical phenomena.
</details>
<details>
<summary>摘要</summary>
目标是找到自动搜寻方法，以学习 causal 结构从观察数据中。在某些情况下，所有变量的 interested  causal mechanism 都被测量，需要预测一个测量到的变量对另一个变量的效应。相反，有时变量的首选变量不直接可观察，而是从数据中推导出来的。这些变量被称为 latent 变量。一个常见的例子是心理学中的智商，无法直接测量，因此研究人员会通过不同的指标，如 IQ 测试，来评估。在这种情况下， causal discovery 算法可以揭示下面的 causal 连接和 latent 变量与观察变量之间的连接。本论文关注两个问题在 causal discovery：提供一个 alternative 定义，可以用于 Gaussian 家族的分布，并且可以应用于非 Gaussian 分布家族，以及在 modified 版本的 Strong Faithfulness 假设下，可以用来证明一种修改后的 causal discovery 算法的均匀一致性。减少 sufficiency 假设，以学习包含 latent 变量的 causal 结构。由于推导 causal 关系的重要性，以上工作的扩展 causal discovery 方法的应用范围，预计将能够扩展到更多的 causal 机制和统计现象。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Semi-Supervised-Learning-by-bridging-high-and-low-confidence-predictions"><a href="#Boosting-Semi-Supervised-Learning-by-bridging-high-and-low-confidence-predictions" class="headerlink" title="Boosting Semi-Supervised Learning by bridging high and low-confidence predictions"></a>Boosting Semi-Supervised Learning by bridging high and low-confidence predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07509">http://arxiv.org/abs/2308.07509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khanh-Binh Nguyen, Joon-Sung Yang</li>
<li>for: 本研究旨在解决 Pseudo-labeling 方法中的三大问题，提高 semi-supervised learning 的性能和泛化能力。</li>
<li>methods: 本研究提出了一种新的 ReFixMatch 方法，通过全面利用无标示数据来提高模型的泛化能力和性能。</li>
<li>results: 对于 ImageNet 图像集，ReFixMatch 方法实现了 41.05% 的 top-1 准确率，超过 FixMatch 和当前状态的方法。<details>
<summary>Abstract</summary>
Pseudo-labeling is a crucial technique in semi-supervised learning (SSL), where artificial labels are generated for unlabeled data by a trained model, allowing for the simultaneous training of labeled and unlabeled data in a supervised setting. However, several studies have identified three main issues with pseudo-labeling-based approaches. Firstly, these methods heavily rely on predictions from the trained model, which may not always be accurate, leading to a confirmation bias problem. Secondly, the trained model may be overfitted to easy-to-learn examples, ignoring hard-to-learn ones, resulting in the \textit{"Matthew effect"} where the already strong become stronger and the weak weaker. Thirdly, most of the low-confidence predictions of unlabeled data are discarded due to the use of a high threshold, leading to an underutilization of unlabeled data during training. To address these issues, we propose a new method called ReFixMatch, which aims to utilize all of the unlabeled data during training, thus improving the generalizability of the model and performance on SSL benchmarks. Notably, ReFixMatch achieves 41.05\% top-1 accuracy with 100k labeled examples on ImageNet, outperforming the baseline FixMatch and current state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
假标注是SSL中的一种重要技术，其中一个训练过的模型将生成对未标注数据的人工标注，允许同时在指导下进行 Label 和无标注数据的同时训练。然而，一些研究发现了假标注基于方法的三大问题。首先，这些方法强调训练过的模型的预测结果，可能并不准确，导致确认偏见问题。其次，训练过的模型可能会过拟合易学习的示例，忽略困难学习的示例，从而导致"马太效应"，即已经强的变得更强，弱的变得更弱。最后，大多数无标注数据的低信度预测被抛弃，因为使用高reshold，导致在训练中未充分利用无标注数据。为了解决这些问题，我们提出了一种新的方法 called ReFixMatch，它计划在训练中利用所有的无标注数据，从而提高模型的一般化性和SSL Benchmark中的性能。各种方法的实验结果表明，ReFixMatch可以与 FixMatch 和当前状态的方法相比，在 ImageNet 上达到41.05% 的 top-1 准确率，使用 100k 标注示例。
</details></li>
</ul>
<hr>
<h2 id="Detecting-The-Corruption-Of-Online-Questionnaires-By-Artificial-Intelligence"><a href="#Detecting-The-Corruption-Of-Online-Questionnaires-By-Artificial-Intelligence" class="headerlink" title="Detecting The Corruption Of Online Questionnaires By Artificial Intelligence"></a>Detecting The Corruption Of Online Questionnaires By Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07499">http://arxiv.org/abs/2308.07499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lebrun, Sharon Temtsin, Andrew Vonasch, Christoph Bartneck</li>
<li>for: 这个研究是为了检测在在线问卷中使用人工智能生成的文本是否可以被识别出来。</li>
<li>methods: 这个研究使用了人类和自动AI检测系统来检测文本的作者性。</li>
<li>results: 人类参与者的识别率为76%，但是这还不够保证数据质量。自动AI检测系统完全无用。如果AI变得太普遍，那么检测伪造提交的成本将超过在线问卷的 beneficial。这问题只能由群组平台系统上解决。<details>
<summary>Abstract</summary>
Online questionnaires that use crowd-sourcing platforms to recruit participants have become commonplace, due to their ease of use and low costs. Artificial Intelligence (AI) based Large Language Models (LLM) have made it easy for bad actors to automatically fill in online forms, including generating meaningful text for open-ended tasks. These technological advances threaten the data quality for studies that use online questionnaires. This study tested if text generated by an AI for the purpose of an online study can be detected by both humans and automatic AI detection systems. While humans were able to correctly identify authorship of text above chance level (76 percent accuracy), their performance was still below what would be required to ensure satisfactory data quality. Researchers currently have to rely on the disinterest of bad actors to successfully use open-ended responses as a useful tool for ensuring data quality. Automatic AI detection systems are currently completely unusable. If AIs become too prevalent in submitting responses then the costs associated with detecting fraudulent submissions will outweigh the benefits of online questionnaires. Individual attention checks will no longer be a sufficient tool to ensure good data quality. This problem can only be systematically addressed by crowd-sourcing platforms. They cannot rely on automatic AI detection systems and it is unclear how they can ensure data quality for their paying clients.
</details>
<details>
<summary>摘要</summary>
在线问卷使用人群投票平台Recruit participants变得普遍，因为它们的使用容易和成本低。人工智能（AI）基于大语言模型（LLM）使得坏actor可以自动填充在线表单，包括生成有意义的文本 для开放式任务。这些技术进步威胁在线问卷中的数据质量。这个研究测试了一I生成的文本是否可以由人类和自动AI检测系统检测出来。人类参与者的准确率为76%，但是这还不够保证数据质量的满意度。研究人员目前需要坏actor的无自身利益来成功使用开放式回答。自动AI检测系统目前不可用。如果AI变得太普遍，则提交假答案的成本将超过在线问卷的利益。人类注意性检查不再是一个有效的数据质量保证工具。这个问题只能通过人群投票平台系统地解决。它们不能依靠自动AI检测系统，而且不清楚如何保证付出客户的数据质量。
</details></li>
</ul>
<hr>
<h2 id="DREAMWALKER-Mental-Planning-for-Continuous-Vision-Language-Navigation"><a href="#DREAMWALKER-Mental-Planning-for-Continuous-Vision-Language-Navigation" class="headerlink" title="DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation"></a>DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07498">http://arxiv.org/abs/2308.07498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HanqingWangAI/Dreamwalker">https://github.com/HanqingWangAI/Dreamwalker</a></li>
<li>paper_authors: Hanqing Wang, Wei Liang, Luc Van Gool, Wenguan Wang<br>for:* DREAMWALKER is a world model-based VLN-CE agent that leverages mental experiments to plan and make strategic decisions in a freely traversable environment.methods:* The world model is built to summarize the visual, topological, and dynamic properties of the environment into a discrete, structured, and compact representation.* DREAMWALKER simulates and evaluates possible plans entirely in the internal abstract world before executing costly actions.results:* Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.Here’s the Chinese translation:for:* DREAMWALKER 是一个基于世界模型的 VLN-CE 代理，通过MENTAL EXPERIMENTS 进行规划和策略决策在一个自由可行的环境中。methods:* 世界模型将环境的视觉、拓扑和动态特性总结为一个简化、结构化和压缩的表示。* DREAMWALKER 在内部抽象世界中 simulate 和评估可能的计划，以避免在真实世界中的浪费。results:* 对 VLN-CE 数据集的广泛实验和减少研究表明，提议的方法有效，并提供了未来工作的有价值导向。<details>
<summary>Abstract</summary>
VLN-CE is a recently released embodied task, where AI agents need to navigate a freely traversable environment to reach a distant target location, given language instructions. It poses great challenges due to the huge space of possible strategies. Driven by the belief that the ability to anticipate the consequences of future actions is crucial for the emergence of intelligent and interpretable planning behavior, we propose DREAMWALKER -- a world model based VLN-CE agent. The world model is built to summarize the visual, topological, and dynamic properties of the complicated continuous environment into a discrete, structured, and compact representation. DREAMWALKER can simulate and evaluate possible plans entirely in such internal abstract world, before executing costly actions. As opposed to existing model-free VLN-CE agents simply making greedy decisions in the real world, which easily results in shortsighted behaviors, DREAMWALKER is able to make strategic planning through large amounts of ``mental experiments.'' Moreover, the imagined future scenarios reflect our agent's intention, making its decision-making process more transparent. Extensive experiments and ablation studies on VLN-CE dataset confirm the effectiveness of the proposed approach and outline fruitful directions for future work.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting"><a href="#ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting" class="headerlink" title="ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting"></a>ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07496">http://arxiv.org/abs/2308.07496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zepu Wang, Yuqi Nie, Peng Sun, Nam H. Nguyen, John Mulvey, H. Vincent Poor</li>
<li>for: 预测交通流量管理在智能交通系统（ITS）中的优化</li>
<li>methods: 使用简单的多层感知机制（MLP）模组和线性层，同时考虑时间资讯、空间资讯和预先定义的路径结构</li>
<li>results: 与现有的STGNNs和其他模型相比，ST-MLP表现出更高的准确性和computational efficiency<details>
<summary>Abstract</summary>
The criticality of prompt and precise traffic forecasting in optimizing traffic flow management in Intelligent Transportation Systems (ITS) has drawn substantial scholarly focus. Spatio-Temporal Graph Neural Networks (STGNNs) have been lauded for their adaptability to road graph structures. Yet, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model solely based on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encourages further exploration of more concise and effective neural network architectures in the field of traffic forecasting.
</details>
<details>
<summary>摘要</summary>
“智能交通系统（ITS）中的实时流量预测 Criticality has drawn substantial scholarly focus, and Spatio-Temporal Graph Neural Networks (STGNNs) have been praised for their adaptability to road graph structures. However, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model based solely on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information, and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encourages further exploration of more concise and effective neural network architectures in the field of traffic forecasting.”Here's a word-for-word translation of the text into Simplified Chinese:“智能交通系统（ITS）中的实时流量预测 Criticality 吸引了大量的学术关注，而 Spatio-Temporal Graph Neural Networks (STGNNs) 被赞誉为路径 граosph 结构的适应性。然而，目前 STGNNs 架构设计中经常优先过往复杂的设计，导致计算成本增加，仅对准确性有小量改善。为解决这个问题，我们提出 ST-MLP，一个简洁的 spatio-temporal 模型，基于弹性 Multi-Layer Perceptron (MLP) 模组和线性层。具体而言，我们将时间信息、空间信息和预设的路径结构融合在一起，并成功地实现了通道独立策略 - 一种有效的时间序列预测技术。实验结果显示，ST-MLP 在准确性和计算效率方面都超过了现有的 STGNNs 和其他模型。我们的发现鼓励我们继续探索更简洁和有效的神经网络架构在交通预测领域。”
</details></li>
</ul>
<hr>
<h2 id="Omega-Regular-Reward-Machines"><a href="#Omega-Regular-Reward-Machines" class="headerlink" title="Omega-Regular Reward Machines"></a>Omega-Regular Reward Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07469">http://arxiv.org/abs/2308.07469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak</li>
<li>for: 这篇论文旨在探讨在强化学习中设计合适的奖励机制是如何实现更高效的行为学习。</li>
<li>methods: 这篇论文使用了奖励机器和ωRegular语言来表达非马歇维奖励，以满足更复杂的学习目标。</li>
<li>results: 研究人员通过设计了一种基于模型自由RL算法的ε-优化策略来评估奖励机器的效果，并通过实验证明了该方法的可行性和有效性。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a powerful approach for training agents to perform tasks, but designing an appropriate reward mechanism is critical to its success. However, in many cases, the complexity of the learning objectives goes beyond the capabilities of the Markovian assumption, necessitating a more sophisticated reward mechanism. Reward machines and omega-regular languages are two formalisms used to express non-Markovian rewards for quantitative and qualitative objectives, respectively. This paper introduces omega-regular reward machines, which integrate reward machines with omega-regular languages to enable an expressive and effective reward mechanism for RL. We present a model-free RL algorithm to compute epsilon-optimal strategies against omega-egular reward machines and evaluate the effectiveness of the proposed algorithm through experiments.
</details>
<details>
<summary>摘要</summary>
利用强化学习（RL）训练代理人完成任务，但设计合适的奖励机制是关键。然而，在许多情况下，学习目标的复杂性超出了Markov预测的能力，需要更加复杂的奖励机制。奖励机器和ωRegular语言是两种用于表达非Markov奖励的形式，分别用于量化和质量目标。本文提出了ωRegular奖励机器，可以将奖励机器与ωRegular语言集成，以实现RL中的表达力和有效性。我们提出了一种无模型RL算法，可以计算ε优策略对ωRegular奖励机器，并通过实验评估提案的效果。
</details></li>
</ul>
<hr>
<h2 id="Playing-with-Words-Comparing-the-Vocabulary-and-Lexical-Richness-of-ChatGPT-and-Humans"><a href="#Playing-with-Words-Comparing-the-Vocabulary-and-Lexical-Richness-of-ChatGPT-and-Humans" class="headerlink" title="Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans"></a>Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07462">http://arxiv.org/abs/2308.07462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Reviriego, Javier Conde, Elena Merino-Gómez, Gonzalo Martínez, José Alberto Hernández</li>
<li>for: This study aims to compare the vocabulary and lexical richness of ChatGPT and human responses when performing the same tasks.</li>
<li>methods: The study uses two datasets containing answers to different types of questions answered by ChatGPT and humans, and analyzes the number of distinct words and lexical richness of each.</li>
<li>results: Preliminary results show that ChatGPT tends to use fewer distinct words and lower lexical richness than humans.Here’s the same information in Simplified Chinese:</li>
<li>for: 这项研究目的是对 chatGPT 和人类回答相同任务的 vocabulary 和语言丰富度进行比较。</li>
<li>methods: 研究使用两个数据集，每个数据集包含不同类型的问题，chatGPT 和人类回答的答案，并对每个数据集进行 vocabulary 和语言丰富度的分析。</li>
<li>results: 初步结果显示，chatGPT 使用的单词数量和语言丰富度比人类低。<details>
<summary>Abstract</summary>
The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of ChatGPT and humans when performing the same tasks. In more detail, two datasets containing the answers to different types of questions answered by ChatGPT and humans are used, and the analysis shows that ChatGPT tends to use fewer distinct words and lower lexical richness than humans. These results are very preliminary and additional datasets and ChatGPT configurations have to be evaluated to extract more general conclusions. Therefore, further research is needed to understand how the use of ChatGPT and more broadly generative AI tools will affect the vocabulary and lexical richness in different types of text and languages.
</details>
<details>
<summary>摘要</summary>
人工智能语言生成模型如GPT（生成预训练变换器）和ChatGPT等工具的出现已经引发了一场革命，这将对文本生成方式产生深远的影响。这有很多意义，例如，如果人工智能生成的文本在多个领域中占据了一定的比例，会对读者的语言能力和 newer AI工具的训练产生影响吗？会对语言演化产生影响？对于语言中的一个方面来说，使用工具如ChatGPT会增加或减少在写作文本时使用的词汇数量和语言 ricness（理解为在书面或口语中使用的不同词汇数量）？这有关词汇的影响，那些不包括在人工智能生成内容中将变得更加少用，并最终可能产生失传。在这项工作中，我们对ChatGPT和人类的答案集进行了初步比较，结果显示ChatGPT使用的词汇数量和语言 ricness较低。这些结果是非常初步的，需要更多的数据和ChatGPT配置来提取更广泛的结论。因此，进一步的研究是必要的，以了解人工智能生成工具在不同类型的文本和语言中的词汇和语言 ricness的影响。
</details></li>
</ul>
<hr>
<h2 id="Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis"><a href="#Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis" class="headerlink" title="Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis"></a>Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07942">http://arxiv.org/abs/2308.07942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anilakash/indkgc">https://github.com/anilakash/indkgc</a></li>
<li>paper_authors: Akash Anil, Víctor Gutiérrez-Basulto, Yazmín Ibañéz-García, Steven Schockaert</li>
<li>for: 这个论文的目的是研究 inductive knowledge graph completion 任务，即从训练图像上学习推理规则，并将其应用于独立的测试图像上进行预测。</li>
<li>methods: 这个论文使用了规则基于的方法，但是在实践中，这些方法表现非常差。作者认为这是因为两个因素：（i）不可能的实体没有被评估，（ii）只考虑最有用的路径来确定链接预测答案的信任程度。作者们为了解决这些问题，研究了一些变种方法，包括一些专门 Addressing 这些问题。</li>
<li>results: 研究发现，这些变种方法可以几乎与 NBFNet 相当的性能，而且它们只使用了 NBFNet 使用的一小部分证据。此外，作者们还发现，一个further variant，即考虑整个知识图，可以一直高于 NBFNet 的性能。<details>
<summary>Abstract</summary>
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which means that they largely keep the interpretability advantage of rule-based methods. Moreover, we show that a further variant, which does look at the full KG, consistently outperforms NBFNet.
</details>
<details>
<summary>摘要</summary>
任务是 inductive 知识图完成需要模型学习从训练图中学习推理模式，然后用于测试图上预测。 规则式方法看起来很自然地适合这个任务，但在实践中它们实际上显著地下表现。我们认为这是因为两个因素：（i）不可能的实体没有被排序，（ii）只考虑测试图中最有用的路径来确定链接预测答案的信任度。为了分析这些因素的影响，我们研究了一些 variants 的规则式方法，它们专门解决这些问题。我们发现这些模型可以达到与 NBFNet 相似的性能，而且它们只使用了 NBFNet 使用的一小部分证据，这意味着它们保持了解释性的优势。此外，我们还显示了一种 further 的变体，它会在全知识图上进行预测，并一直表现出perform better 于 NBFNet。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-for-Smart-Transportation"><a href="#Artificial-Intelligence-for-Smart-Transportation" class="headerlink" title="Artificial Intelligence for Smart Transportation"></a>Artificial Intelligence for Smart Transportation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07457">http://arxiv.org/abs/2308.07457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SarmisthaDutta/application-of-artificial-Intelligence-for-future-sustainable-smart-city">https://github.com/SarmisthaDutta/application-of-artificial-Intelligence-for-future-sustainable-smart-city</a></li>
<li>paper_authors: Michael Wilbur, Amutheezan Sivagnanam, Afiya Ayman, Samitha Samaranayeke, Abhishek Dubey, Aron Laszka</li>
<li>for: 提高公共交通系统的效率和使用率，以满足社会发展和人类价值创造的需求。</li>
<li>methods: 利用人工智能技术，提供数据驱动的智能交通系统，包括数据收集、人工智能决策支持和计算机科学问题解决方案。</li>
<li>results: 通过对交通系统的数据分析和人工智能技术应用，提高交通系统的效率和使用率，为社会发展和人类价值创造提供可能性。<details>
<summary>Abstract</summary>
There are more than 7,000 public transit agencies in the U.S. (and many more private agencies), and together, they are responsible for serving 60 billion passenger miles each year. A well-functioning transit system fosters the growth and expansion of businesses, distributes social and economic benefits, and links the capabilities of community members, thereby enhancing what they can accomplish as a society. Since affordable public transit services are the backbones of many communities, this work investigates ways in which Artificial Intelligence (AI) can improve efficiency and increase utilization from the perspective of transit agencies. This book chapter discusses the primary requirements, objectives, and challenges related to the design of AI-driven smart transportation systems. We focus on three major topics. First, we discuss data sources and data. Second, we provide an overview of how AI can aid decision-making with a focus on transportation. Lastly, we discuss computational problems in the transportation domain and AI approaches to these problems.
</details>
<details>
<summary>摘要</summary>
美国有超过7,000个公共交通机构，以及许多私人机构，每年共运送600亿公里的乘客。一个健全的公共交通系统会促进企业增长和扩张、分配社会和经济的利益，并将社区成员的能力相互连接，从而提高社会的可能性。由于公共交通服务是许多社区的基础设施，这项工作研究了使用人工智能（AI）提高公共交通系统的效率和使用率。本章讨论了智能交通系统设计的主要需求、目标和挑战。我们主要讨论以下三个主题：第一，讨论数据来源和数据；第二，介绍transportation领域中AI助于决策的方法；第三，讨论交通领域的计算问题和AI应用于这些问题。
</details></li>
</ul>
<hr>
<h2 id="GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction"><a href="#GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction" class="headerlink" title="GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction"></a>GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07452">http://arxiv.org/abs/2308.07452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Ruan, Liwei Wang, Charat Thongprayoon, Wisit Cheungpasitporn, Hongfang Liu</li>
<li>for: 这份研究的目的是发展一个新的方法，即GRU-D-Weibull，用于预测个人水平的终点和时间至终点。这种方法可以实现实时个人化终点预测和人口水平的风险管理。</li>
<li>methods: 这份研究使用了一个新的方法，即GRU-D-Weibull，它结合了闸道运算和衰减（GRU-D）来模型Weibull分布。这种方法可以实现实时个人化终点预测和人口水平的风险管理。</li>
<li>results: 这份研究发现，GRU-D-Weibull方法在终点预测中表现出色，C-指数约为0.7，并在4.3年的追踪期间持续提高到约0.77。这与随机生存树的表现相似。GRU-D-Weibull方法在L1损失上显示出了优秀的表现，与其他方法相比，具有较低的损失值。此外，GRU-D-Weibull方法还能够对终点预测进行时间轴上的精确调整。<details>
<summary>Abstract</summary>
Accurate prediction models for individual-level endpoints and time-to-endpoints are crucial in clinical practice. In this study, we propose a novel approach, GRU-D-Weibull, which combines gated recurrent units with decay (GRU-D) to model the Weibull distribution. Our method enables real-time individualized endpoint prediction and population-level risk management. Using a cohort of 6,879 patients with stage 4 chronic kidney disease (CKD4), we evaluated the performance of GRU-D-Weibull in endpoint prediction. The C-index of GRU-D-Weibull was ~0.7 at the index date and increased to ~0.77 after 4.3 years of follow-up, similar to random survival forest. Our approach achieved an absolute L1-loss of ~1.1 years (SD 0.95) at the CKD4 index date and a minimum of ~0.45 years (SD0.3) at 4 years of follow-up, outperforming competing methods significantly. GRU-D-Weibull consistently constrained the predicted survival probability at the time of an event within a smaller and more fixed range compared to other models throughout the follow-up period. We observed significant correlations between the error in point estimates and missing proportions of input features at the index date (correlations from ~0.1 to ~0.3), which diminished within 1 year as more data became available. By post-training recalibration, we successfully aligned the predicted and observed survival probabilities across multiple prediction horizons at different time points during follow-up. Our findings demonstrate the considerable potential of GRU-D-Weibull as the next-generation architecture for endpoint risk management, capable of generating various endpoint estimates for real-time monitoring using clinical data.
</details>
<details>
<summary>摘要</summary>
准确的预测模型对个体级别终点和时间至终点是临床实践中非常重要。在这项研究中，我们提出了一种新的方法，即GRU-D-Weibull，它将闭合杂列单元（GRU-D）与减速分布（Weibull distribution）结合在一起。我们的方法可以实现实时个体化终点预测和人口级别风险管理。使用6,879名CKD4阶段4慢性肾病患者的 cohort，我们评估了GRU-D-Weibull在终点预测方面的性能。GRU-D-Weibull的C-指数在指定日期为 aproximadamente 0.7，而在4.3年后跟踪中，它提高至 aproximadamente 0.77，与随机生存森林类似。我们的方法实现了终点预测的L1损失约为1.1年（SD 0.95）在CKD4指定日期，并在4年后跟踪中达到了约0.45年（SD0.3）的最小值，与其他方法相比显著性能更高。GRU-D-Weibull通常在终点预测过程中压缩预测生存概率的范围，与其他模型在跟踪期间保持相对更小和更固定的范围相比，表现更稳定。我们发现在指定日期的输入特征批量缺失率和预测点估值的误差之间存在 statistically significant 的相关性（从 approximately 0.1到 approximately 0.3），这种相关性随着时间的推移而减少。通过后期重新训练，我们成功地将预测和观察到的生存概率相互对应，并在不同的跟踪时间点 durante el seguimiento。我们的发现表明GRU-D-Weibull可能是下一代结构，它可以使用临床数据生成多种终点估计，用于实时监测终点风险。
</details></li>
</ul>
<hr>
<h2 id="Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data"><a href="#Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data" class="headerlink" title="Open-set Face Recognition using Ensembles trained on Clustered Data"></a>Open-set Face Recognition using Ensembles trained on Clustered Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07445">http://arxiv.org/abs/2308.07445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, William Robson Schwartz</li>
<li>for: 该论文目的是开发一种可扩展的开放集面Recognition方法，能够处理大量未知人脸。</li>
<li>methods: 该方法使用 clustering 和多个二进制学习算法，对查询人脸样本进行分类，并使用 ensemble 提高预测性能。</li>
<li>results: 实验结果表明，该方法可以在大量人脸库中实现竞争力强的表现，即使面临大量未知人脸。Here’s the English version for reference:</li>
<li>for: The purpose of this paper is to develop a scalable open-set face recognition approach that can handle large numbers of unfamiliar faces.</li>
<li>methods: The method uses clustering and an ensemble of binary learning algorithms to classify query face samples and retrieve their correct identity from a gallery of hundreds or thousands of subjects.</li>
<li>results: Experimental results show that the approach can achieve competitive performance even when targeting scalability, handling large numbers of unfamiliar faces.<details>
<summary>Abstract</summary>
Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
</details>
<details>
<summary>摘要</summary>
开放集 face recognition 描述一种enario，在训练阶段未经见过的不明人脸出现在测试阶段。不仅需要准确地识别关注人脸，而且也需要采用有效地处理未知脸的方法。这篇文章介绍了一种可扩展的开放集 face 识别方法，可以对包括百万多个主题的人脸库进行识别。该方法包括分集和一个 ensemble of 二分学习算法，以便在测试阶段确定查询脸样本是否属于人脸库，并且使用ensemble提高预测性能。我们在well-known LFW和YTF标准准样本上进行了实验，结果表明，即使targeting可扩展性，也可以实现竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="The-Performance-of-Transferability-Metrics-does-not-Translate-to-Medical-Tasks"><a href="#The-Performance-of-Transferability-Metrics-does-not-Translate-to-Medical-Tasks" class="headerlink" title="The Performance of Transferability Metrics does not Translate to Medical Tasks"></a>The Performance of Transferability Metrics does not Translate to Medical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07444">http://arxiv.org/abs/2308.07444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Levy Chaves, Alceu Bissoto, Eduardo Valle, Sandra Avila</li>
<li>for: 本研究旨在评估七种传输可能性分数在医疗图像分析中的表现，以便更好地选择适合目标数据集的深度学习架构。</li>
<li>methods: 本研究使用了七种传输可能性分数，包括三种基于特征之间的相似度的方法，以及四种基于特征之间的相似度和特征之间的相似度的权重平均值的方法。</li>
<li>results: 本研究在三个医疗图像分析应用中进行了广泛的评估，并发现了 Transferability 分数在医疗图像分析中的表现不一定可靠和一致，需要进一步的研究。<details>
<summary>Abstract</summary>
Transfer learning boosts the performance of medical image analysis by enabling deep learning (DL) on small datasets through the knowledge acquired from large ones. As the number of DL architectures explodes, exhaustively attempting all candidates becomes unfeasible, motivating cheaper alternatives for choosing them. Transferability scoring methods emerge as an enticing solution, allowing to efficiently calculate a score that correlates with the architecture accuracy on any target dataset. However, since transferability scores have not been evaluated on medical datasets, their use in this context remains uncertain, preventing them from benefiting practitioners. We fill that gap in this work, thoroughly evaluating seven transferability scores in three medical applications, including out-of-distribution scenarios. Despite promising results in general-purpose datasets, our results show that no transferability score can reliably and consistently estimate target performance in medical contexts, inviting further work in that direction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>基于知识传递的深度学习（DL）在医疗图像分析中提高了性能，但由于DL模型的数量爆炸式增长，探索所有候选者成为不可能的，因此需要更加经济的选择方法。基于传输性的分数评估方法在这个领域出现，允许效率地计算任何目标数据集上模型准确率的相关分数。然而，由于这些传输性分数在医疗图像 datasets 上的应用仍然不明确，因此这种方法在实践中的应用尚未得到推广。我们在这个工作中填补了这个空白，对七种传输性分数在三个医疗应用中的性能进行了全面评估。虽然在通用数据集上显示了承诺的结果，但我们的结果表明，在医疗上下文中，没有一个可靠和一致地估计目标性能的传输性分数。这对未来的研究提出了挑战。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides"><a href="#Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides" class="headerlink" title="Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides"></a>Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07441">http://arxiv.org/abs/2308.07441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianfa Li, Roxana Khalili, Frederick Lurmann, Nathan Pavlovic, Jun Wu, Yan Xu, Yisi Liu, Karl O’Sharkey, Beate Ritz, Luke Oman, Meredith Franklin, Theresa Bastain, Shohreh F. Farzan, Carrie Breton, Rima Habre</li>
<li>for: 这个论文旨在提高空气质量预测的准确性和可靠性，尤其是对氧气杂合物（NOx）的预测。</li>
<li>methods: 这篇论文使用了机器学习（ML）方法和化学运输模型（CTM）的知识来开发一个physics-informed deep learning框架，用于预测NO2和NOx的分布。这个框架具有减少预测偏差的能力，并能够提供明确的uncertainty估计。</li>
<li>results: 这篇论文的结果表明，使用这个physics-informed deep learning框架可以减少ML模型的预测偏差，并且能够更好地预测NO2和NOx的分布。这个框架还能够提供明确的uncertainty估计，这有助于更好地理解空气质量的变化和风险。<details>
<summary>Abstract</summary>
Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have recognized acute and chronic health and environmental effects. Machine learning (ML) methods have significantly enhanced our capacity to predict NOx concentrations at ground-level with high spatiotemporal resolution but may suffer from high estimation bias since they lack physical and chemical knowledge about air pollution dynamics. Chemical transport models (CTMs) leverage this knowledge; however, accurate predictions of ground-level concentrations typically necessitate extensive post-calibration. Here, we present a physics-informed deep learning framework that encodes advection-diffusion mechanisms and fluid dynamics constraints to jointly predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures fine-scale transport of NO2 and NOx, generates robust spatial extrapolation, and provides explicit uncertainty estimation. The framework fuses knowledge-driven physicochemical principles of CTMs with the predictive power of ML for air quality exposure, health, and policy applications. Our approach offers significant improvements over purely data-driven ML methods and has unprecedented bias reduction in joint NO2 and NOx prediction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks"><a href="#Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks" class="headerlink" title="Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks"></a>Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07439">http://arxiv.org/abs/2308.07439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Abdelraouf, Rohit Gupta, Kyungtae Han</li>
<li>for: 提高先进驾驶辅助系统和自动驾驶车辆的预测性能</li>
<li>methods: 使用时间图 convolutional neural networks (GCN) 和长期快速传递Memory (LSTM) 模型当地交通数据</li>
<li>results: 比较先进的预测性能，尤其是在较长的预测时间范围内，并且在不同的驾驶者环境下显示出较高的预测精度。<details>
<summary>Abstract</summary>
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories. Experimental results demonstrate the superior performance of our personalized GCN-LSTM model, particularly for longer prediction horizons, compared to its generic counterpart. Moreover, the personalized model outperforms individual models created without pre-training, emphasizing the significance of pre-training on a large dataset to avoid overfitting. By incorporating personalization, our approach enhances trajectory prediction accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>预测车辆轨迹的准确性是智能驾驶系统和自动驾驶车辆的关键。现有方法主要依靠大规模的数据集来 derivate 通用的轨迹预测，忽略了个人驾驶模式的特点。为了解决这个空白，我们提出了一种基于互动的个性化车辆轨迹预测方法，该方法利用图像卷积神经网络（GCN）和长短期记忆（LSTM）来模型目标车辆和它们周围的交通之间的空间时间互动。为了个性化预测，我们建立了一个管道，该管道通过转移学习来使用大规模轨迹数据进行初始化，然后通过每个驾驶员的特定驾驶数据进行微调。我们使用人类在Loop的 simulate 来收集个性化自然驾驶轨迹和相应的周围车辆轨迹。实验结果表明我们的个性化GCN-LSTM模型在较长的预测时间范围内表现更出色，特别是比其通用对应模型更出色。此外，个性化模型也比没有预训练的模型更好，这说明了大规模数据集的预训练可以避免过拟合。通过个性化，我们的方法可以提高轨迹预测精度。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Similarity-Loss-for-Neural-Source-Code-Summarization"><a href="#Semantic-Similarity-Loss-for-Neural-Source-Code-Summarization" class="headerlink" title="Semantic Similarity Loss for Neural Source Code Summarization"></a>Semantic Similarity Loss for Neural Source Code Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07429">http://arxiv.org/abs/2308.07429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apcl-research/funcom-useloss">https://github.com/apcl-research/funcom-useloss</a></li>
<li>paper_authors: Chia-Yi Su, Collin McMillan</li>
<li>for: 本研究旨在提出一种改进的损失函数，用于自动生成源代码描述。</li>
<li>methods: 本研究使用一种semantic similarity metric来计算损失，并与传统的一个字一个字损失函数相结合。</li>
<li>results: 对多种基eline进行评估，得到了大多数情况下的改进。<details>
<summary>Abstract</summary>
This paper presents an improved loss function for neural source code summarization. Code summarization is the task of writing natural language descriptions of source code. Neural code summarization refers to automated techniques for generating these descriptions using neural networks. Almost all current approaches involve neural networks as either standalone models or as part of a pretrained large language models e.g., GPT, Codex, LLaMA. Yet almost all also use a categorical cross-entropy (CCE) loss function for network optimization. Two problems with CCE are that 1) it computes loss over each word prediction one-at-a-time, rather than evaluating a whole sentence, and 2) it requires a perfect prediction, leaving no room for partial credit for synonyms. We propose and evaluate a loss function to alleviate this problem. In essence, we propose to use a semantic similarity metric to calculate loss over the whole output sentence prediction per training batch, rather than just loss for each word. We also propose to combine our loss with traditional CCE for each word, which streamlines the training process compared to baselines. We evaluate our approach over several baselines and report an improvement in the vast majority of conditions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>It computes loss for each word prediction individually, rather than evaluating the entire sentence.2. It requires perfect predictions, leaving no room for partial credit for synonyms.To address these issues, the proposed loss function uses a semantic similarity metric to calculate loss over the entire output sentence prediction for each training batch, rather than just for each word. Additionally, the proposed loss function combines with traditional CCE for each word, streamlining the training process compared to baselines. The approach is evaluated over several baselines and shows an improvement in the majority of conditions.In simplified Chinese, the text can be translated as:这篇论文提出了一种改进的损失函数，用于神经源代码概要。神经代码概要是自动生成源代码的自然语言描述的任务。目前大多数方法都使用神经网络作为独立模型或大语言模型的组件，如GPT、Codex、LLaMA。然而，这些模型的优化都使用分类交叉熵损失函数（CCE），它有两个问题：1. 它计算每个单词预测的损失，而不是整个句子。2. 它需要精准预测，不允许任何减少偏差。我们提议使用semantic相似度度量来计算损失，而不是单独计算每个单词的损失。此外，我们还提议将我们的损失函数与传统的CCE相结合，以便在训练过程中对每个单词进行损失计算，而不是直接使用CCE。我们对多个基准进行评估，并发现大多数情况下有改进。</details></li>
</ol>
<hr>
<h2 id="UniBrain-Unify-Image-Reconstruction-and-Captioning-All-in-One-Diffusion-Model-from-Human-Brain-Activity"><a href="#UniBrain-Unify-Image-Reconstruction-and-Captioning-All-in-One-Diffusion-Model-from-Human-Brain-Activity" class="headerlink" title="UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity"></a>UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07428">http://arxiv.org/abs/2308.07428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijian Mai, Zhijun Zhang</li>
<li>for: 这篇论文旨在探讨使用人脑活动诱发的视觉刺激来重建图像和文本描述，以更好地理解人脑和视觉系统之间的连接。</li>
<li>methods: 该论文提出了一种名为UniBrain的一种普适傅 diffusion模型，通过将fMRI voxels转换为图像和文本的latent空间，并通过基于CLIP的fMRI图像和文本条件来导向反向傅 diffusion过程，生成真实的caption和图像。</li>
<li>results: UniBrain在图像重建和描述方面与现有方法相比，表现出较高的qualitative和quantitative性能，并在Natural Scenes Dataset（NSD）数据集上首次实现了图像描述结果。此外，简除试验和功能ROI分析还表明UniBrain的优势和视觉脑 decode的全面意义。<details>
<summary>Abstract</summary>
Image reconstruction and captioning from brain activity evoked by visual stimuli allow researchers to further understand the connection between the human brain and the visual perception system. While deep generative models have recently been employed in this field, reconstructing realistic captions and images with both low-level details and high semantic fidelity is still a challenging problem. In this work, we propose UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity. For the first time, we unify image reconstruction and captioning from visual-evoked functional magnetic resonance imaging (fMRI) through a latent diffusion model termed Versatile Diffusion. Specifically, we transform fMRI voxels into text and image latent for low-level information and guide the backward diffusion process through fMRI-based image and text conditions derived from CLIP to generate realistic captions and images. UniBrain outperforms current methods both qualitatively and quantitatively in terms of image reconstruction and reports image captioning results for the first time on the Natural Scenes Dataset (NSD) dataset. Moreover, the ablation experiments and functional region-of-interest (ROI) analysis further exhibit the superiority of UniBrain and provide comprehensive insight for visual-evoked brain decoding.
</details>
<details>
<summary>摘要</summary>
Image重建和描述从脑动活动诱发的视觉系统的连接，使研究人员更深入了解人脑和视觉系统之间的关系。而最近，深度生成模型在这一领域得到了广泛应用。但是，重现真实的描述和图像，同时具有低级别细节和高semantic faithfulness仍然是一个挑战。在这种情况下，我们提出了UniBrain：基于人脑活动的一种普适的扩展模型，用于 reunifying image reconstruction和 captioning。UniBrain通过将fMRI voxels转换为文本和图像的latent空间，并通过基于CLIP的fMRI图像和文本条件，驱动回传diffusion过程，生成真实的描述和图像。UniBrain在质量和量上比现有方法出色，并在自然场景数据集（NSD）上首次报告图像描述结果。此外，我们还进行了层次 ROI分析和简要实验，以更深入地探索视觉诱发的脑决码。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Intersection-of-Large-Language-Models-and-Agent-Based-Modeling-via-Prompt-Engineering"><a href="#Exploring-the-Intersection-of-Large-Language-Models-and-Agent-Based-Modeling-via-Prompt-Engineering" class="headerlink" title="Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering"></a>Exploring the Intersection of Large Language Models and Agent-Based Modeling via Prompt Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07411">http://arxiv.org/abs/2308.07411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ejunprung/llm-agents">https://github.com/ejunprung/llm-agents</a></li>
<li>paper_authors: Edward Junprung</li>
<li>for: 这个研究旨在使用大语言模型（LLM）来实现人类行为的准确模拟，以探索人类行为在复杂社会系统中的行为和互动方式。</li>
<li>methods: 研究使用了提示工程（inspired by Park et al. (2023)），通过设计了两个人类行为的假设 scenario：一个是两个代理的谈判，另一个是六个代理的谋杀推理游戏。</li>
<li>results: 研究发现，使用LLM可以创造出非常真实的人类行为，包括在谈判和推理游戏中的互动和决策。这些结果表明，LLM可以成为模拟人类行为的有效工具。<details>
<summary>Abstract</summary>
The final frontier for simulation is the accurate representation of complex, real-world social systems. While agent-based modeling (ABM) seeks to study the behavior and interactions of agents within a larger system, it is unable to faithfully capture the full complexity of human-driven behavior. Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways. Our research investigates simulations of human interactions using LLMs. Through prompt engineering, inspired by Park et al. (2023), we present two simulations of believable proxies of human behavior: a two-agent negotiation and a six-agent murder mystery game.
</details>
<details>
<summary>摘要</summary>
最终的前ier для模拟是准确地表现复杂的现实世界社会系统。而代理人模型（ABM）则尝试研究代理人在更大的系统中的行为和互动，但它无法准确地捕捉人类驱动的行为的全部复杂性。大语言模型（LLM），如ChatGPT，在这个瓶颈上出现了作为解决方案，让研究人员能够在前所未有的方式探索人类驱动的互动。我们的研究探讨了使用LLM进行人类互动的模拟。通过提示工程，取得自 Park et al. (2023)，我们展示了两个人类行为的准确代理：一个两个代理的谈判和一个六个代理的谋杀推理游戏。
</details></li>
</ul>
<hr>
<h2 id="PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects"><a href="#PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects" class="headerlink" title="PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects"></a>PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07391">http://arxiv.org/abs/2308.07391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/3dlg-hcvc/paris">https://github.com/3dlg-hcvc/paris</a></li>
<li>paper_authors: Jiayi Liu, Ali Mahdavi-Amiri, Manolis Savva</li>
<li>for:  simultaneous part-level reconstruction and motion parameter estimation for articulated objects</li>
<li>methods: self-supervised, end-to-end architecture with implicit shape and appearance models, optimizes motion parameters jointly without 3D supervision or semantic annotation</li>
<li>results: generalizes better across object categories, outperforms baselines and prior work, improves reconstruction with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, achieves 5% error rate for motion estimation across 10 object categories.<details>
<summary>Abstract</summary>
We address the task of simultaneous part-level reconstruction and motion parameter estimation for articulated objects. Given two sets of multi-view images of an object in two static articulation states, we decouple the movable part from the static part and reconstruct shape and appearance while predicting the motion parameters. To tackle this problem, we present PARIS: a self-supervised, end-to-end architecture that learns part-level implicit shape and appearance models and optimizes motion parameters jointly without any 3D supervision, motion, or semantic annotation. Our experiments show that our method generalizes better across object categories, and outperforms baselines and prior work that are given 3D point clouds as input. Our approach improves reconstruction relative to state-of-the-art baselines with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and achieves 5% error rate for motion estimation across 10 object categories.   Video summary at: https://youtu.be/tDSrROPCgUc
</details>
<details>
<summary>摘要</summary>
我们考虑了同时进行部件重建和运动参数估计的骨架对象问题。给出两个多视图图像集合，我们将可动部分与静止部分分离，重建形状和外观，同时预测运动参数。为解决这个问题，我们提出了PARIS：一种自主、端到端架构，不需要3D超视觉、运动或semantic注解，可以同时学习部件级别的隐式形状和外观模型，并同步优化运动参数。我们的实验表明，我们的方法可以更好地泛化到不同的对象类别，并超越基elines和先前的方法，它们输入3D点云作为输入。我们的方法可以将 Chamfer-L1 距离减少3.94（45.2%） для对象和26.79（84.5%） для部件，并实现了10种对象类别中的运动估计错误率为5%。Video summary: <https://youtu.be/tDSrROPCgUc>
</details></li>
</ul>
<hr>
<h2 id="LLM-Self-Defense-By-Self-Examination-LLMs-Know-They-Are-Being-Tricked"><a href="#LLM-Self-Defense-By-Self-Examination-LLMs-Know-They-Are-Being-Tricked" class="headerlink" title="LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"></a>LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07308">http://arxiv.org/abs/2308.07308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Helbling, Mansi Phute, Matthew Hull, Duen Horng Chau</li>
<li>for: 防止语言模型生成危险内容（如犯罪指南）</li>
<li>methods: 使用大语言模型自身的过滤机制来防止生成危险内容</li>
<li>results: even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.<details>
<summary>Abstract</summary>
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在最近几年内 Popularity 急剧增长，这主要归功于它们可以根据人类提示生成高质量的文本。然而，这些模型也被证明可能生成有害内容（如提供用户提示以commit犯罪）。在文献中，有许多研究探讨如何 Mitigate 这些风险，如通过 reinforcement learning 将模型与人类价值观念进行对齐。然而，研究表明，即使模型已经对齐，也可能受到恶意攻击，这些攻击可以让模型生成有害内容。我们提议一种简单的方法，即让大型语言模型自己过滤其回快。我们当前的结果表明，即使模型没有 fine-tune 对齐人类价值观念，也可以通过语言模型验证来防止它生成有害内容给用户。
</details></li>
</ul>
<hr>
<h2 id="Extend-Wave-Function-Collapse-to-Large-Scale-Content-Generation"><a href="#Extend-Wave-Function-Collapse-to-Large-Scale-Content-Generation" class="headerlink" title="Extend Wave Function Collapse to Large-Scale Content Generation"></a>Extend Wave Function Collapse to Large-Scale Content Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07307">http://arxiv.org/abs/2308.07307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhe Nie, Shaoming Zheng, Zhan Zhuang, Xuan Song</li>
<li>for: 解决 Wave Function Collapse (WFC) 算法在大规模内容生成中的时间复杂性和约束矛盾问题。</li>
<li>methods: 提出 Nested WFC (N-WFC) 算法框架，采用完整和部分完整块集准备策略，可以避免矛盾和回tracking问题，并且可以生成 deterministic 和 periodic 的无限内容。</li>
<li>results: 验证 N-WFC 算法的可行性和适用性，并且通过 weight-brush 系统和游戏设计方法，证明其适用于游戏设计。<details>
<summary>Abstract</summary>
Wave Function Collapse (WFC) is a widely used tile-based algorithm in procedural content generation, including textures, objects, and scenes. However, the current WFC algorithm and related research lack the ability to generate commercialized large-scale or infinite content due to constraint conflict and time complexity costs. This paper proposes a Nested WFC (N-WFC) algorithm framework to reduce time complexity. To avoid conflict and backtracking problems, we offer a complete and sub-complete tileset preparation strategy, which requires only a small number of tiles to generate aperiodic and deterministic infinite content. We also introduce the weight-brush system that combines N-WFC and sub-complete tileset, proving its suitability for game design. Our contribution addresses WFC's challenge in massive content generation and provides a theoretical basis for implementing concrete games.
</details>
<details>
<summary>摘要</summary>
wave function collapse (WFC) 是一种广泛使用的瓷砖式算法 в procedural content generation 中，包括文本、物体和场景等。然而，当前 WFC 算法和相关研究缺乏可商业化的大规模或无限内容生成能力，这是因为约束冲突和时间复杂度成本的问题。本文提出了一种嵌套 WFC（N-WFC）算法框架，以降低时间复杂度。为了避免冲突和回溯问题，我们提供了完整的和半完整的瓷砖集准备策略，只需一小数量的瓷砖可以生成 periodic 和 deterministic 无限内容。我们还介绍了 weight-brush 系统，该系统结合 N-WFC 和半完整瓷砖集，证明其适用于游戏设计。我们的贡献解决了 WFC 在大规模内容生成中的挑战，并提供了实现具体游戏的理论基础。
</details></li>
</ul>
<hr>
<h2 id="Neural-Authorship-Attribution-Stylometric-Analysis-on-Large-Language-Models"><a href="#Neural-Authorship-Attribution-Stylometric-Analysis-on-Large-Language-Models" class="headerlink" title="Neural Authorship Attribution: Stylometric Analysis on Large Language Models"></a>Neural Authorship Attribution: Stylometric Analysis on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07305">http://arxiv.org/abs/2308.07305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tharindu Kumarage, Huan Liu</li>
<li>for: 这篇研究旨在探讨人工智能生成文本的伪造问题，并寻找一种可靠的方法来追溯这些文本的来源。</li>
<li>methods: 研究使用了现有的语言模型（LLMs），包括GPT-4、PaLM和Llama，并对这些模型进行了分析和比较。</li>
<li>results: 研究发现了不同的商业和开源模型之间的区别，并发现了这些模型在不同的语言方面的差异。这些结果可以帮助未来的伪造探测和对抗人工智能生成的伪造文本。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as GPT-4, PaLM, and Llama have significantly propelled the generation of AI-crafted text. With rising concerns about their potential misuse, there is a pressing need for AI-generated-text forensics. Neural authorship attribution is a forensic effort, seeking to trace AI-generated text back to its originating LLM. The LLM landscape can be divided into two primary categories: proprietary and open-source. In this work, we delve into these emerging categories of LLMs, focusing on the nuances of neural authorship attribution. To enrich our understanding, we carry out an empirical analysis of LLM writing signatures, highlighting the contrasts between proprietary and open-source models, and scrutinizing variations within each group. By integrating stylometric features across lexical, syntactic, and structural aspects of language, we explore their potential to yield interpretable results and augment pre-trained language model-based classifiers utilized in neural authorship attribution. Our findings, based on a range of state-of-the-art LLMs, provide empirical insights into neural authorship attribution, paving the way for future investigations aimed at mitigating the threats posed by AI-generated misinformation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT-4、PaLM和Llama已经有效地推动人工生成的文本生成。随着AI生成文本的可能性的滥用，有一个急需AI生成文本科学的发展。人工作文本识别是一种审查AI生成文本的努力，寻求跟踪AI生成文本的来源LLM。LLM领域可以分为两个主要类别： propriety 和 open-source。在这项工作中，我们深入探究这些新兴类别的LLM，强调在 neural authorship attribution 方面的特点。通过对 LLM 的写作特征进行实质分析，包括 lexical、syntactic 和 structural 方面的语言特征，我们探讨了这些特征是否可以生成可读取的结果，并可以增强基于预训练语言模型的扩展语言模型来进行分类。我们的发现，基于一系列现代LLM，提供了实质性的启示，帮助我们更好地理解 neural authorship attribution，并为未来防止AI生成的误导而做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Why-Not-Explaining-Missing-Entailments-with-Evee-Technical-Report"><a href="#Why-Not-Explaining-Missing-Entailments-with-Evee-Technical-Report" class="headerlink" title="Why Not? Explaining Missing Entailments with Evee (Technical Report)"></a>Why Not? Explaining Missing Entailments with Evee (Technical Report)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07294">http://arxiv.org/abs/2308.07294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Alrabbaa, Stefan Borgwardt, Tom Friese, Patrick Koopmann, Mikhail Kotlov</li>
<li>for: 本研究旨在帮助ontology用户更好地理解逻辑推论 derivations。</li>
<li>methods: 本研究使用了描述逻辑理解器和Protégé中的插件，以及新的abduction和 counterexample技术来解释缺失的结论。</li>
<li>results: 本研究提出了一种新的 $\rm E{\scriptsize VEE}$ 插件，可以帮助用户更好地理解ontology中缺失的结论。<details>
<summary>Abstract</summary>
Understanding logical entailments derived by a description logic reasoner is not always straight-forward for ontology users. For this reason, various methods for explaining entailments using justifications and proofs have been developed and implemented as plug-ins for the ontology editor Prot\'eg\'e. However, when the user expects a missing consequence to hold, it is equally important to explain why it does not follow from the ontology. In this paper, we describe a new version of $\rm E{\scriptsize VEE}$, a Prot\'eg\'e plugin that now also provides explanations for missing consequences, via existing and new techniques based on abduction and counterexamples.
</details>
<details>
<summary>摘要</summary>
理解推理推论结论由描述逻辑理解者不一定是直观的，对ontology用户而言。为此，各种用于说明推论使用证明和证据的方法已经开发和实现为Protégé编辑器插件。然而，当用户期望缺失的结论不存在时，也是非常重要的解释为什么不从ontology中推论出来。在这篇论文中，我们描述了一种新版本的 $\rm E{\scriptsize VEE}$，一个Protégé插件，现在还提供缺失结论的解释，通过现有和新的技术基于推理和反例。
</details></li>
</ul>
<hr>
<h2 id="Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding"><a href="#Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding" class="headerlink" title="Cross-Attribute Matrix Factorization Model with Shared User Embedding"></a>Cross-Attribute Matrix Factorization Model with Shared User Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07284">http://arxiv.org/abs/2308.07284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Zeng Fan, Youzhi Liang, Jianguo Jia</li>
<li>for: 这个研究旨在应用深度学习技术来解决推荐系统中的寒冷开始问题，并且考虑用户和项目的特征对推荐系统的影响。</li>
<li>methods: 本研究使用的方法是内容匹配网络（Neural Matrix Factorization，NeuMF），并且将用户和项目的特征考虑进行扩展。</li>
<li>results: 实验结果显示，我们的跨特征网络匹配网络（Cross-Attribute Matrix Factorization，CAMF）模型在MovieLens和Pinterest dataset上具有较高的性能，特别是在资料集稀疏性较高的情况下。<details>
<summary>Abstract</summary>
Over the past few years, deep learning has firmly established its prowess across various domains, including computer vision, speech recognition, and natural language processing. Motivated by its outstanding success, researchers have been directing their efforts towards applying deep learning techniques to recommender systems. Neural collaborative filtering (NCF) and Neural Matrix Factorization (NeuMF) refreshes the traditional inner product in matrix factorization with a neural architecture capable of learning complex and data-driven functions. While these models effectively capture user-item interactions, they overlook the specific attributes of both users and items. This can lead to robustness issues, especially for items and users that belong to the "long tail". Such challenges are commonly recognized in recommender systems as a part of the cold-start problem. A direct and intuitive approach to address this issue is by leveraging the features and attributes of the items and users themselves. In this paper, we introduce a refined NeuMF model that considers not only the interaction between users and items, but also acrossing associated attributes. Moreover, our proposed architecture features a shared user embedding, seamlessly integrating with user embeddings to imporve the robustness and effectively address the cold-start problem. Rigorous experiments on both the Movielens and Pinterest datasets demonstrate the superiority of our Cross-Attribute Matrix Factorization model, particularly in scenarios characterized by higher dataset sparsity.
</details>
<details>
<summary>摘要</summary>
在过去几年中，深度学习在不同领域，如计算机视觉、语音识别和自然语言处理等领域，都有着杰出的成绩。驱动于其出色的成绩，研究人员开始将深度学习技术应用于推荐系统。基于神经网络的共同积分（NCF）和神经矩阵分解（NeuMF）等模型，将传统的内积分换成神经网络架构，能够学习复杂的数据驱动函数。然而，这些模型可能会忽视用户和项目的特定属性，这可能会导致稳定性问题，尤其是对于“长尾”用户和项目。这种问题在推荐系统中广泛存在，通常被称为冷启动问题。在本文中，我们提出了一种改进的NeuMF模型，该模型不仅考虑用户和项目之间的交互，还考虑用户和项目之间的关联属性。此外，我们的提议的架构还包括共享用户嵌入，可以融合用户嵌入，从而提高系统的稳定性和效果地解决冷启动问题。我们在MovieLens和Pinterest数据集上进行了严格的实验，并证明我们的横向积分模型在数据集稀缺的情况下表现出优异性。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Point-Cloud-Segmentation-for-Power-Lines-Inspection-in-Smart-Grid"><a href="#Autonomous-Point-Cloud-Segmentation-for-Power-Lines-Inspection-in-Smart-Grid" class="headerlink" title="Autonomous Point Cloud Segmentation for Power Lines Inspection in Smart Grid"></a>Autonomous Point Cloud Segmentation for Power Lines Inspection in Smart Grid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07283">http://arxiv.org/abs/2308.07283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Kyuroson, Anton Koval, George Nikolakopoulos</li>
<li>for: 本研究旨在提出一种无监督机器学习（ML）框架，用于从 LiDAR 数据中探测、提取和分析高压和低压电缆的特征，以及相关的绿色区域。</li>
<li>methods: 提议的方法包括Initially eliminating ground points based on statistical analysis, denoising and transforming the remaining candidate points using Principle Component Analysis (PCA) and Kd-tree, and then segmenting power lines using a two-stage DBSCAN clustering.</li>
<li>results: 实验结果表明，提议的框架可以效率地探测电缆和进行相关的风险分析。<details>
<summary>Abstract</summary>
LiDAR is currently one of the most utilized sensors to effectively monitor the status of power lines and facilitate the inspection of remote power distribution networks and related infrastructures. To ensure the safe operation of the smart grid, various remote data acquisition strategies, such as Airborne Laser Scanning (ALS), Mobile Laser Scanning (MLS), and Terrestrial Laser Scanning (TSL) have been leveraged to allow continuous monitoring of regional power networks, which are typically surrounded by dense vegetation. In this article, an unsupervised Machine Learning (ML) framework is proposed, to detect, extract and analyze the characteristics of power lines of both high and low voltage, as well as the surrounding vegetation in a Power Line Corridor (PLC) solely from LiDAR data. Initially, the proposed approach eliminates the ground points from higher elevation points based on statistical analysis that applies density criteria and histogram thresholding. After denoising and transforming of the remaining candidate points by applying Principle Component Analysis (PCA) and Kd-tree, power line segmentation is achieved by utilizing a two-stage DBSCAN clustering to identify each power line individually. Finally, all high elevation points in the PLC are identified based on their distance to the newly segmented power lines. Conducted experiments illustrate that the proposed framework is an agnostic method that can efficiently detect the power lines and perform PLC-based hazard analysis.
</details>
<details>
<summary>摘要</summary>
利达（LiDAR）现在是智能电网运行的一个最广泛使用的感知器，用于监测电力线路的状况和远程电力分布网络和相关基础设施的检查。为保证智能电网的安全运行，远程数据获取策略，如空中激光扫描（ALS）、移动激光扫描（MLS）和地面激光扫描（TSL）已经被利用，以实现区域电网的连续监测，这些区域通常被密集的植被环绕着。在这篇文章中，一种无监测机器学习（ML）框架被提议，用于从LiDAR数据中检测、提取和分析高压和低压电力线路的特征，以及周围的植被。首先，提议的方法从高空点的高程点中排除地面点，基于统计分析，应用密度标准和对 histogram 进行阈值设置。接着，通过应用原理Components分析（PCA）和 Kd-tree 转换，对剩下的候选点进行减噪和变换。然后，通过使用两个阶段 DBSCAN 聚合，对每个电力线 individually 进行分类，以实现电力线的分 Segmentation。最后，通过对新分类的高空点进行距离计算，全部在 PLC 中的高空点被标识出来。实验表明，提议的方法是一种无关的方法，可以有效地检测电力线和在 PLC 中进行风险分析。
</details></li>
</ul>
<hr>
<h2 id="Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning"><a href="#Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning" class="headerlink" title="Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning"></a>Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07273">http://arxiv.org/abs/2308.07273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssra Cheriguene, Wael Jaafar, Chaker Abdelaziz Kerrache, Halim Yanikomeroglu, Fatima Zohra Bousbaa, Nasreddine Lagraa</li>
<li>for: 提高Edge Federated Learning（FL）模型的准确性，并且考虑UAV的能源消耗和通信质量等约束。</li>
<li>methods: 提出了一种新的UAV参与者选择策略，即基于地区数据的结构相似度指数平均分数和能源消耗配置的数据高效能观测选择策略（DEEPS）。</li>
<li>results: 通过实验，提出的选择策略比废 randomly选择策略更高的Edge FL模型准确性、训练时间和UAV能源消耗。<details>
<summary>Abstract</summary>
Unmanned aerial vehicle (UAV)-enabled edge federated learning (FL) has sparked a rise in research interest as a result of the massive and heterogeneous data collected by UAVs, as well as the privacy concerns related to UAV data transmissions to edge servers. However, due to the redundancy of UAV collected data, e.g., imaging data, and non-rigorous FL participant selection, the convergence time of the FL learning process and bias of the FL model may increase. Consequently, we investigate in this paper the problem of selecting UAV participants for edge FL, aiming to improve the FL model's accuracy, under UAV constraints of energy consumption, communication quality, and local datasets' heterogeneity. We propose a novel UAV participant selection scheme, called data-efficient energy-aware participant selection strategy (DEEPS), which consists of selecting the best FL participant in each sub-region based on the structural similarity index measure (SSIM) average score of its local dataset and its power consumption profile. Through experiments, we demonstrate that the proposed selection scheme is superior to the benchmark random selection method, in terms of model accuracy, training time, and UAV energy consumption.
</details>
<details>
<summary>摘要</summary>
“无人机（UAV）启用边缘联合学习（FL）已经引起了研究兴趣，由于UAV收集的数据量庞大和多样化，以及UAV数据传输到边缘服务器的隐私问题。然而，由于UAV收集的数据重复，如图像数据，以及不严谨的FL参与者选择，FL学习过程的收敛时间和模型偏见可能增加。因此，我们在本纸中 investigate 选择UAV参与者 для边缘FL，以提高FL模型的准确性，在UAV的能量消耗、通信质量和本地数据的多样性等因素限制下。我们提出了一种新的UAV参与者选择策略，称为数据高效能觉 participant selection strategy（DEEPS），该策略基于每个子区域中的本地数据和能量消耗profile中的结构相似度平均分数来选择最佳的FL参与者。通过实验，我们示出了提案的选择策略与参照方法（随机选择）相比，在模型准确性、训练时间和UAV能量消耗方面具有显著优势。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models"><a href="#EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models" class="headerlink" title="EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models"></a>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07269">http://arxiv.org/abs/2308.07269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyedit">https://github.com/zjunlp/easyedit</a></li>
<li>paper_authors: Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, Huajun Chen</li>
<li>for: 提高LLMs的知识更新和修正能力，以提高其可靠性和通用性。</li>
<li>methods: 支持多种现代知识编辑方法，可以轻松应用于多种well-known LLMs。</li>
<li>results: 在LlaMA-2上进行了知识编辑实验，表明知识编辑比传统精度uning更高效和更通用。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to the outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners to apply knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video at http://knowlm.zjukg.cn/easyedit.mp4.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常会受到知识剖除或误差问题的影响，这意味着它们不知道未经见过的事件或生成文本中含有错误的信息，这由于模型使用的数据过时或噪音有关。为解决这些问题，许多知识编辑方法 для LLM  emerged  --  hoping to subtly inject 或编辑更新的知识或调整不符合预期的行为，同时尽量减少对无关输入的影响。然而，由于不同的知识编辑方法之间存在差异，以及任务设置的变化，当前没有一个标准的实现框架可供社区使用，这限制了实践者在应用知识编辑方法时的能力。为解决这些问题，我们提出了 EasyEdit，一个易于使用的知识编辑框架 для LLM。它支持多种前沿知识编辑方法，并可以 readily 应用于多个知名的 LLM such as T5, GPT-J, LlaMA, etc. 我们在 LlaMA-2 上进行了知识编辑试验，并证明知识编辑超过了传统的精细调整在可靠性和泛化方面的表现。我们在 GitHub 上公布了源代码，并提供了 Google Colab 教程和详细的文档，以便初学者快速入门。此外，我们还提供了在线实时知识编辑系统和 demo 视频，请参考 http://knowlm.zjukg.cn/easyedit.mp4.
</details></li>
</ul>
<hr>
<h2 id="Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI"><a href="#Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI" class="headerlink" title="Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI"></a>Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07247">http://arxiv.org/abs/2308.07247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Poiret, Antoine Grigis, Justin Thomas, Marion Noulhiane</li>
<li>for: 这个研究检查了Rashomon效应对机器学习模型中的知识抽取所带来的挑战。</li>
<li>methods: 这个研究使用了SHAP方法对Rashomon集中的模型进行解释。</li>
<li>results: 实验结果显示，随着样本大小增加，解释的一致性逐渐提高，但在少于128个样本的情况下，解释具有高度的变化性，因此不可靠地抽取知识。然而，在更多的数据下，模型之间的一致性提高，allowing for consensus。 bagging ensemble often had higher agreement。这些结果为我们提供了足够的数据来信任解释的指南。<details>
<summary>Abstract</summary>
The Rash\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous models.
</details>
<details>
<summary>摘要</summary>
瑞索蒙效应对机器学习模型中提取可靠知识带来挑战。这项研究检查了样本大小对模型解释的影响，使用SHAP进行了5个公共数据集的实验。结果显示，随着样本大小增加，解释逐渐协调，但从128个样本开始，解释具有高度的变化， limiting reliable knowledge extraction。然而，随着更多的数据，模型之间的一致性提高，allowing for consensus。 Bagging ensemble often had higher agreement。结果提供了足够数据来信任解释的指导，变化在低样本数量 suggets that conclusion may be unreliable without validation。进一步的工作需要更多的模型类型，数据领域和解释方法。测试 converges in neural networks和模型特定的解释方法会对其具有深远的影响。研究方法可以带来原则性的技术，从ambiguous models中提取知识。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Planning-and-Environment-Aware-Memory-for-Instruction-Following-Embodied-Agents"><a href="#Context-Aware-Planning-and-Environment-Aware-Memory-for-Instruction-Following-Embodied-Agents" class="headerlink" title="Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents"></a>Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07241">http://arxiv.org/abs/2308.07241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi</li>
<li>for: 提高家庭任务完成需要规划一系列的行动，考虑到 previous 动作的后果。</li>
<li>methods: 我们提出了 Context-Aware Planning and Environment-Aware Memory (CAPEAM)，它将Semantic context (例如适合交互的物品)和改变的空间安排和交互对象的状态 (例如交互对象的移动位置)包含在一系列动作中，以推断后续动作。</li>
<li>results: 我们经验表明，搭载CAPEAM的机器人在多个指标中达到了最新的状态前的表现，包括在不同环境中完成交互指令的任务，差异为大致 (+10.70%在未看到环境中)。<details>
<summary>Abstract</summary>
Accomplishing household tasks requires to plan step-by-step actions considering the consequences of previous actions. However, the state-of-the-art embodied agents often make mistakes in navigating the environment and interacting with proper objects due to imperfect learning by imitating experts or algorithmic planners without such knowledge. To improve both visual navigation and object interaction, we propose to consider the consequence of taken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory) that incorporates semantic context (e.g., appropriate objects to interact with) in a sequence of actions, and the changed spatial arrangement and states of interacted objects (e.g., location that the object has been moved to) in inferring the subsequent actions. We empirically show that the agent with the proposed CAPEAM achieves state-of-the-art performance in various metrics using a challenging interactive instruction following benchmark in both seen and unseen environments by large margins (up to +10.70% in unseen env.).
</details>
<details>
<summary>摘要</summary>
完成家务需要规划每一步行动，考虑先前行动的后果。然而，现状的凉身agent经常在环境中导航和与合适的物体交互时出错，因为它们通过专家学习或算法规划而学习的知识不够完善。为了提高视觉导航和物体交互，我们提议考虑行动的后果，通过Context-Aware Planning and Environment-Aware Memory（CAPEAM）来 incorporate semantic context（例如，与物体交互时适用的对象）在一系列动作中，以及交互对象的改变的空间布局和状态（例如，交互对象的移动位置）。我们实验表明，携带我们提议的CAPEAM的代理人在多种 metrics 中表现出STATE-OF-THE-ART的表现，包括在seen和unseen环境中的挑战性交互指令遵循测试中，差异达 +10.70%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.AI_2023_08_15/" data-id="clly4xtbf000vvl88er23g6ad" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.CL_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T11:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.CL_2023_08_15/">cs.CL - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DS4DH-at-SMM4H-2023-Zero-Shot-Adverse-Drug-Events-Normalization-using-Sentence-Transformers-and-Reciprocal-Rank-Fusion"><a href="#DS4DH-at-SMM4H-2023-Zero-Shot-Adverse-Drug-Events-Normalization-using-Sentence-Transformers-and-Reciprocal-Rank-Fusion" class="headerlink" title="DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion"></a>DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12877">http://arxiv.org/abs/2308.12877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Yazdani, Hossein Rouhizadeh, David Vicente Alvarez, Douglas Teodoro</li>
<li>for: 本研究是为了评估一种基于BERT fine-tuning和sentence transformers的社交媒体文本挖掘系统，用于正常化恶性药物事件提到Medical Dictionary for Regulatory Activities（MDRA）词汇。</li>
<li>methods: 本研究采用了两stage方法，首先使用BERT fine-tuning进行实体识别，然后使用sentence transformers和reciprocal-rank fusion进行零 shot正常化。</li>
<li>results: 本研究的结果显示，这种方法在MDRA词汇正常化中得到了44.9%的精度、40.5%的准确率和42.6%的F1分数，超过了共享任务5中的中值性能提高10%，并且在所有参与者中显示出最高性能。这些结果证明了该方法的有效性和在社交媒体文本挖掘领域的应用潜力。<details>
<summary>Abstract</summary>
This paper outlines the performance evaluation of a system for adverse drug event normalization, developed by the Data Science for Digital Health group for the Social Media Mining for Health Applications 2023 shared task 5. Shared task 5 targeted the normalization of adverse drug event mentions in Twitter to standard concepts from the Medical Dictionary for Regulatory Activities terminology. Our system hinges on a two-stage approach: BERT fine-tuning for entity recognition, followed by zero-shot normalization using sentence transformers and reciprocal-rank fusion. The approach yielded a precision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median performance in shared task 5 by 10% and demonstrated the highest performance among all participants. These results substantiate the effectiveness of our approach and its potential application for adverse drug event normalization in the realm of social media text mining.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文介绍了一种基于BERT微调和sentence transformers的社交媒体文本挖掘系统，用于正常化投诉病药事件。该系统采用了两 stageapproach：首先微调BERT进行实体识别，然后使用sentence transformers和reciprocal-rank fusions进行零批normalization。该approach实现了44.9%的精度、40.5%的准确率和42.6%的F1分数，比共享任务5中的中值性能提高10%，并达到了所有参与者中最高的性能。这些结果证明了该approach的有效性，并适用于社交媒体文本挖掘中的投诉病药事件正常化。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Visually-Rich-Document-Understanding-via-Layout-Structure-Modeling"><a href="#Enhancing-Visually-Rich-Document-Understanding-via-Layout-Structure-Modeling" class="headerlink" title="Enhancing Visually-Rich Document Understanding via Layout Structure Modeling"></a>Enhancing Visually-Rich Document Understanding via Layout Structure Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07777">http://arxiv.org/abs/2308.07777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiwei Li, Zuchao Li, Xiantao Cai, Bo Du, Hai Zhao</li>
<li>for: 这 paper 的目的是提高文档理解的精度，特别是利用文档结构图模型文档的布局结构知识。</li>
<li>methods: 该 paper 提出了一种名为 GraphLayoutLM 的新型文档理解模型，该模型利用文档结构图模型文档的布局结构知识，并使用图重新排序算法和布局意识多头自注意力层来学习文档布局知识。</li>
<li>results: 该 paper 在多个 benchmark 上达到了最佳成绩，包括 FUNSD、XFUND 和 CORD 等 datasets，并且通过对模型组件的缺省研究，表明了每个组件的贡献。<details>
<summary>Abstract</summary>
In recent years, the use of multi-modal pre-trained Transformers has led to significant advancements in visually-rich document understanding. However, existing models have mainly focused on features such as text and vision while neglecting the importance of layout relationship between text nodes. In this paper, we propose GraphLayoutLM, a novel document understanding model that leverages the modeling of layout structure graph to inject document layout knowledge into the model. GraphLayoutLM utilizes a graph reordering algorithm to adjust the text sequence based on the graph structure. Additionally, our model uses a layout-aware multi-head self-attention layer to learn document layout knowledge. The proposed model enables the understanding of the spatial arrangement of text elements, improving document comprehension. We evaluate our model on various benchmarks, including FUNSD, XFUND and CORD, and achieve state-of-the-art results among these datasets. Our experimental results demonstrate that our proposed method provides a significant improvement over existing approaches and showcases the importance of incorporating layout information into document understanding models. We also conduct an ablation study to investigate the contribution of each component of our model. The results show that both the graph reordering algorithm and the layout-aware multi-head self-attention layer play a crucial role in achieving the best performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在最近几年，基于多modal预训练的Transformers模型在文本 ricoh 理解方面带来了显著的进步。然而，现有的模型主要集中在文本和视觉特征之间，忽略了文档布局关系的重要性。在本文中，我们提出了 GraphLayoutLM 模型，它利用文档布局结构图来注入文档布局知识到模型中。GraphLayoutLM 模型使用图重新排序算法来根据图结构调整文本序列。此外，我们的模型还使用了布局意识多头自注意层来学习文档布局知识。该模型可以理解文本元素的空间排序，从而提高文档理解能力。我们在不同的benchmark上评估了我们的模型，包括FUNSD、XFUND和CORD等，并在这些数据集中达到了状态之最好的结果。我们的实验结果表明，我们提出的方法具有显著的改进，并证明了在文档理解模型中包含布局信息的重要性。我们还进行了一个ablation研究，以 Investigate each component of our model的贡献。结果显示，图重新排序算法和布局意识多头自注意层均对获得最佳性能做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="SPM-Structured-Pretraining-and-Matching-Architectures-for-Relevance-Modeling-in-Meituan-Search"><a href="#SPM-Structured-Pretraining-and-Matching-Architectures-for-Relevance-Modeling-in-Meituan-Search" class="headerlink" title="SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search"></a>SPM: Structured Pretraining and Matching Architectures for Relevance Modeling in Meituan Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07711">http://arxiv.org/abs/2308.07711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Zan, Yaopeng Han, Xiaotian Jiang, Yao Xiao, Yang Yang, Dayao Chen, Sheng Chen</li>
<li>for: 提高生活服务平台上搜索结果的相关性，以提高用户体验。</li>
<li>methods: 提出了一种两stage预训练和匹配架构，使用了查询和文档多个字段作为输入，并使用了有效的信息压缩方法来处理长文档。</li>
<li>results: 经过大规模的实验和在线A&#x2F;B测试，表明提出的架构有效提高了搜索结果的相关性，已经在Meituan上线部署一年多。<details>
<summary>Abstract</summary>
In e-commerce search, relevance between query and documents is an essential requirement for satisfying user experience. Different from traditional e-commerce platforms that offer products, users search on life service platforms such as Meituan mainly for product providers, which usually have abundant structured information, e.g. name, address, category, thousands of products. Modeling search relevance with these rich structured contents is challenging due to the following issues: (1) there is language distribution discrepancy among different fields of structured document, making it difficult to directly adopt off-the-shelf pretrained language model based methods like BERT. (2) different fields usually have different importance and their length vary greatly, making it difficult to extract document information helpful for relevance matching.   To tackle these issues, in this paper we propose a novel two-stage pretraining and matching architecture for relevance matching with rich structured documents. At pretraining stage, we propose an effective pretraining method that employs both query and multiple fields of document as inputs, including an effective information compression method for lengthy fields. At relevance matching stage, a novel matching method is proposed by leveraging domain knowledge in search query to generate more effective document representations for relevance scoring. Extensive offline experiments and online A/B tests on millions of users verify that the proposed architectures effectively improve the performance of relevance modeling. The model has already been deployed online, serving the search traffic of Meituan for over a year.
</details>
<details>
<summary>摘要</summary>
在电商搜索中，搜索结果的相关性是用户体验的关键要求。与传统电商平台不同，用户在生活服务平台such as Meituan上查询主要是为了找到供应商，这些供应商通常有很多结构化信息，例如名称、地址、类别、千种产品。使用这些丰富的结构化内容进行搜索相关性模型化是有挑战的，因为：（1）不同的结构化文档字段存在语言分布差异，使得直接采用市场上已有预训练语言模型的方法如BERT不太可能。（2）不同的字段通常有不同的重要性和长度，使得提取文档信息有帮助于相关性匹配的部分很困难。为解决这些问题，本文提出了一种新的两Stage预训练和匹配架构，用于与结构化文档进行相关性模型化。预训练阶段，我们提出了一种有效的预训练方法，该方法使用查询和多个文档字段作为输入，并使用有效的信息压缩方法来处理长字段。匹配阶段，我们提出了一种基于搜索查询领域知识的新匹配方法，该方法可以更有效地生成文档表示，用于相关性分数。广泛的Offline实验和在线A/B测试表明，提出的架构有效地提高了相关性模型的性能。该模型已经在Meituan上线服务了一年多。
</details></li>
</ul>
<hr>
<h2 id="Better-Zero-Shot-Reasoning-with-Role-Play-Prompting"><a href="#Better-Zero-Shot-Reasoning-with-Role-Play-Prompting" class="headerlink" title="Better Zero-Shot Reasoning with Role-Play Prompting"></a>Better Zero-Shot Reasoning with Role-Play Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07702">http://arxiv.org/abs/2308.07702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HLT-NLP/Role-Play-Prompting">https://github.com/HLT-NLP/Role-Play-Prompting</a></li>
<li>paper_authors: Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou</li>
<li>for: 这种研究旨在探讨LLMs中的角色扮演如何影响其理解能力。</li>
<li>methods: 研究使用了一种策略性的角色扮演提示方法，在零基eline设定下测试了12种不同的理解准则，包括代数、常识理解、 симвоlic理解等。</li>
<li>results: 研究结果表明，使用角色扮演提示可以在大多数数据集上超越标准的零基eline方法，其中AQuA的准确率由53.5%提高到63.8%，Last Letter的准确率由23.8%提高到84.2%。这表明角色扮演提示可以提高LLMs的上下文理解和链条思维能力。<details>
<summary>Abstract</summary>
Modern large language models (LLMs), such as ChatGPT, exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities like a Linux terminal. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, encompassing arithmetic, commonsense reasoning, symbolic reasoning, and more. Leveraging models such as ChatGPT and Llama 2, our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%. Beyond enhancing contextual understanding, we posit that role-play prompting serves as an implicit Chain-of-Thought (CoT) trigger, thereby improving the quality of reasoning. By comparing our approach with the Zero-Shot-CoT technique, which prompts the model to "think step by step", we further demonstrate that role-play prompting can generate a more effective CoT. This highlights its potential to augment the reasoning capabilities of LLMs.
</details>
<details>
<summary>摘要</summary>
现代大型语言模型（LLM），如ChatGPT，显示出了很强的角色扮演能力，可以不仅扮演人类角色，还可以模拟非人类Entity，如Linux终端。这种多样性使得它们能够模拟人类间的复杂互动和行为，以及模拟特定的对象或系统。虽然这些能力提高了用户参与度和引入了新的交互方式，但LLM的理解能力下的影响仍未得到足够的探索。在这项研究中，我们提出了一种策略性的角色扮演提问方法，并评估其在零基础设定下的性能。通过使用ChatGPT和Llama 2这两种模型，我们的实验结果表明，角色扮演提问在大多数数据集上都能够超越标准的零基础设定。特别是，AQuA的准确率由53.5%提高到63.8%，Last Letter的准确率由23.8%提高到84.2%。除了提高上下文理解，我们认为角色扮演提问可以作为隐藏链条（Chain-of-Thought，CoT）触发器，因此改善LLM的理解质量。通过与零基础CoT技术进行比较，我们进一步证明了角色扮演提问可以生成更有效的CoT。这说明它可以增强LLM的理解能力。
</details></li>
</ul>
<hr>
<h2 id="Attention-Is-Not-All-You-Need-Anymore"><a href="#Attention-Is-Not-All-You-Need-Anymore" class="headerlink" title="Attention Is Not All You Need Anymore"></a>Attention Is Not All You Need Anymore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07661">http://arxiv.org/abs/2308.07661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Zhe Chen</li>
<li>for: 本文提出了一种用于减少Transformer架构中自注意机制的计算和内存复杂性的drop-in替换方案，以提高Transformer的性能。</li>
<li>methods: 本文提出的Extractor可以作为Transformer的自注意机制替换，并且可以减少计算和内存复杂性。</li>
<li>results: 实验结果表明，使用Extractor可以提高Transformer的性能，并且它的计算路径更短，可以更快速地完成计算。<details>
<summary>Abstract</summary>
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our understanding.
</details>
<details>
<summary>摘要</summary>
近年来，受欢迎的Transformer架构在许多应用领域取得了很大成功，包括自然语言处理和计算机视觉。许多现有的工作尝试通过减少Transformer中的自注意机制的计算和内存复杂性，但是性能是Transformer继续成功的关键。在这篇论文中，一种可替换Transformer中的自注意机制，称为Extractor，被提议。实验结果表明，将自注意机制替换为Extractor可以提高Transformer的性能。此外，提议的Extractor可能比自注意机制更快速，因为它有许多短的计算路径。此外，在文本生成中的序列预测问题被形式化为变量长 discrete-time Markov链，并根据我们的理解对Transformer进行了评估。
</details></li>
</ul>
<hr>
<h2 id="SEER-Super-Optimization-Explorer-for-HLS-using-E-graph-Rewriting-with-MLIR"><a href="#SEER-Super-Optimization-Explorer-for-HLS-using-E-graph-Rewriting-with-MLIR" class="headerlink" title="SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with MLIR"></a>SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with MLIR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07654">http://arxiv.org/abs/2308.07654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianyi Cheng, Samuel Coward, Lorenzo Chelini, Rafael Barbalho, Theo Drane</li>
<li>for:  This paper aims to improve the performance of hardware designs produced by high-level synthesis (HLS) tools by automatically rewriting software programs into efficient HLS code.</li>
<li>methods:  The proposed method, called SEER, uses an e-graph data structure to efficiently explore equivalent implementations of a program at scale, and orchestrates existing software compiler passes and hardware synthesis optimizers.</li>
<li>results:  The paper shows that SEER achieves up to 38x the performance within 1.4x the area of the original program, and outperforms manually optimized designs produced by hardware experts in an Intel-provided case study.<details>
<summary>Abstract</summary>
High-level synthesis (HLS) is a process that automatically translates a software program in a high-level language into a low-level hardware description. However, the hardware designs produced by HLS tools still suffer from a significant performance gap compared to manual implementations. This is because the input HLS programs must still be written using hardware design principles.   Existing techniques either leave the program source unchanged or perform a fixed sequence of source transformation passes, potentially missing opportunities to find the optimal design. We propose a super-optimization approach for HLS that automatically rewrites an arbitrary software program into efficient HLS code that can be used to generate an optimized hardware design. We developed a toolflow named SEER, based on the e-graph data structure, to efficiently explore equivalent implementations of a program at scale. SEER provides an extensible framework, orchestrating existing software compiler passes and hardware synthesis optimizers.   Our work is the first attempt to exploit e-graph rewriting for large software compiler frameworks, such as MLIR. Across a set of open-source benchmarks, we show that SEER achieves up to 38x the performance within 1.4x the area of the original program. Via an Intel-provided case study, SEER demonstrates the potential to outperform manually optimized designs produced by hardware experts.
</details>
<details>
<summary>摘要</summary>
高级合成（HLS）是一个过程，它自动将高级语言程序转换为低级硬件描述。然而，由HLS工具生成的硬件设计仍然受到性能差距的影响，这是因为输入HLS程序仍需遵循硬件设计原则。现有的技术可能会留下程序源代码不变，或者执行固定的源代码转换步骤，可能会错过优化的机会。我们提出了一种超优化方法，它可以自动将任何软件程序转换为高效的HLS代码，可以生成优化的硬件设计。我们开发了一个名为SEER的工具流，基于e-graph数据结构，以高效地探索相当的实现方式。SEER提供了可扩展的框架，可以启用现有的软件编译器过程和硬件合成优化器。我们的工作是首次利用e-graph重写来大规模的软件编译框架，如MLIR。对一组开源 benchmark 进行测试，我们发现SEER可以达到38倍的性能，在1.4倍的面积内。通过Intel提供的案例研究，SEER还能够超越由硬件专家手动优化的设计。
</details></li>
</ul>
<hr>
<h2 id="Steering-Language-Generation-Harnessing-Contrastive-Expert-Guidance-and-Negative-Prompting-for-Coherent-and-Diverse-Synthetic-Data-Generation"><a href="#Steering-Language-Generation-Harnessing-Contrastive-Expert-Guidance-and-Negative-Prompting-for-Coherent-and-Diverse-Synthetic-Data-Generation" class="headerlink" title="Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation"></a>Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07645">http://arxiv.org/abs/2308.07645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles O’Neill, Yuan-Sen Ting, Ioana Ciuca, Jack Miller, Thang Bui</li>
<li>for: 提高大语言模型生成的数据质量和多样性，以便下游模型训练和实际数据利用。</li>
<li>methods: 引入对比专家指导，以确保领域遵循性，并使用现有真实数据和 sintetic 示例作为负例准入，以保证多样性和 authenticty。</li>
<li>results: 比前一些生成数据技术提高表现，在三个不同任务中（假设生成、恶意和非恶意评论生成、常识理解任务生成） Displaying better balance between data diversity and coherence。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) hold immense potential to generate synthetic data of high quality and utility, which has numerous applications from downstream model training to practical data utilisation. However, contemporary models, despite their impressive capacities, consistently struggle to produce both coherent and diverse data. To address the coherency issue, we introduce contrastive expert guidance, where the difference between the logit distributions of fine-tuned and base language models is emphasised to ensure domain adherence. In order to ensure diversity, we utilise existing real and synthetic examples as negative prompts to the model. We deem this dual-pronged approach to logit reshaping as STEER: Semantic Text Enhancement via Embedding Repositioning. STEER operates at inference-time and systematically guides the LLMs to strike a balance between adherence to the data distribution (ensuring semantic fidelity) and deviation from prior synthetic examples or existing real datasets (ensuring diversity and authenticity). This delicate balancing act is achieved by dynamically moving towards or away from chosen representations in the latent space. STEER demonstrates improved performance over previous synthetic data generation techniques, exhibiting better balance between data diversity and coherency across three distinct tasks: hypothesis generation, toxic and non-toxic comment generation, and commonsense reasoning task generation. We demonstrate how STEER allows for fine-tuned control over the diversity-coherency trade-off via its hyperparameters, highlighting its versatility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LogPrompt-Prompt-Engineering-Towards-Zero-Shot-and-Interpretable-Log-Analysis"><a href="#LogPrompt-Prompt-Engineering-Towards-Zero-Shot-and-Interpretable-Log-Analysis" class="headerlink" title="LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis"></a>LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07610">http://arxiv.org/abs/2308.07610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yanqing Zhao, Yuhang Chen, Hao Yang, Yanfei Jiang, Xun Chen</li>
<li>for: 本文提出了一种新的零shot和可解释的系统事件分析方法，以提高系统维护和工程生命周期中的可靠性和抗抗锋性。</li>
<li>methods: 本文使用大型自然语言模型（LLM）进行零shot系统事件分析任务，并采用了一系列高级提示策略，以提高LLM的性能。</li>
<li>results: 实验结果显示， LogPrompt 在九个公开的评估数据集上，在两个任务上表现出色，比既有方法（使用千余个日志）高于50%。此外， LogPrompt 的可解释性得到了专业人员的高度评估（4.42&#x2F;5）。<details>
<summary>Abstract</summary>
Automated log analysis is crucial in modern software-intensive systems for ensuring reliability and resilience throughout software maintenance and engineering life cycles. Existing methods perform tasks such as log parsing and log anomaly detection by providing a single prediction value without interpretation. However, given the increasing volume of system events, the limited interpretability of analysis results hinders analysts' trust and their ability to take appropriate actions. Moreover, these methods require substantial in-domain training data, and their performance declines sharply (by up to 62.5%) in online scenarios involving unseen logs from new domains, a common occurrence due to rapid software updates. In this paper, we propose LogPrompt, a novel zero-shot and interpretable log analysis approach. LogPrompt employs large language models (LLMs) to perform zero-shot log analysis tasks via a suite of advanced prompt strategies tailored for log tasks, which enhances LLMs' performance by up to 107.5% compared with simple prompts. Experiments on nine publicly available evaluation datasets across two tasks demonstrate that LogPrompt, despite using no training data, outperforms existing approaches trained on thousands of logs by up to around 50%. We also conduct a human evaluation of LogPrompt's interpretability, with six practitioners possessing over 10 years of experience, who highly rated the generated content in terms of usefulness and readability (averagely 4.42/5). LogPrompt also exhibits remarkable compatibility with open-source and smaller-scale LLMs, making it flexible for practical deployment.
</details>
<details>
<summary>摘要</summary>
现代软件强调系统中，自动化日志分析是关键要素，以确保软件稳定性和恢复能力在维护和工程生命周期中。现有方法可以完成日志分析任务，如日志分析和异常日志检测，但是这些方法通常只提供单个预测值而不具备解释。由于系统事件的增加，以及分析结果的有限可读性，分析人员对结果的信任和他们对结果的应用能力受到限制。此外，这些方法通常需要大量域内训练数据，并且在在线enario中（新领域的日志）发现的日志上进行分析时，其性能会下降（最多下降62.5%）。在这篇论文中，我们提出了一种新的零shot和可解释的日志分析方法——LogPrompt。LogPrompt使用大型自然语言模型（LLMs）来实现零shot日志分析任务，通过一组适用于日志任务的高级提示策略，提高LLMs的性能（最多提高107.5%）。我们在九个公共可用的评估数据集上进行了九个任务的实验，并证明了LogPrompt，即使没有使用任何训练数据，可以与已经训练 thousands of logs 的现有方法相比，在两个任务上提高性能（最多提高50%）。我们还进行了人类评估LogPrompt的可解释性，六位具有超过10年经验的实践者对生成的内容进行了评估，并评估结果表明，生成的内容在有用性和可读性方面得分4.42/5。此外，LogPrompt还表现出了remarkable的兼容性，可以与开源和较小规模的LLMs进行实际应用。
</details></li>
</ul>
<hr>
<h2 id="VBD-MT-Chinese-Vietnamese-Translation-Systems-for-VLSP-2022"><a href="#VBD-MT-Chinese-Vietnamese-Translation-Systems-for-VLSP-2022" class="headerlink" title="VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022"></a>VBD-MT Chinese-Vietnamese Translation Systems for VLSP 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07601">http://arxiv.org/abs/2308.07601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Long Trieu, Song Kiet Bui, Tan Minh Tran, Van Khanh Tran, Hai An Nguyen</li>
<li>for: 本研究参加了2022年VLSP机器翻译共同任务。</li>
<li>methods: 我们基于神经网络模型的Transformer模型，使用了强大的多语言干扰预测模型mBART进行建构。我们还应用了一种采样方法来进行反向翻译，以利用大规模的可用单语言数据。此外，我们还应用了一些提高翻译质量的方法，包括拟合和后处理。</li>
<li>results: 我们在公共测试集上 achievement 38.9 BLEU在中越翻译和38.0 BLEU在越中翻译 tasks，这些成绩超过了一些强大的基eline。<details>
<summary>Abstract</summary>
We present our systems participated in the VLSP 2022 machine translation shared task. In the shared task this year, we participated in both translation tasks, i.e., Chinese-Vietnamese and Vietnamese-Chinese translations. We build our systems based on the neural-based Transformer model with the powerful multilingual denoising pre-trained model mBART. The systems are enhanced by a sampling method for backtranslation, which leverage large scale available monolingual data. Additionally, several other methods are applied to improve the translation quality including ensembling and postprocessing. We achieve 38.9 BLEU on ChineseVietnamese and 38.0 BLEU on VietnameseChinese on the public test sets, which outperform several strong baselines.
</details>
<details>
<summary>摘要</summary>
我们在VLSP 2022机器翻译共同任务中提交了我们的系统。本年度共同任务中，我们参与了中越文和越文中翻译两个任务。我们基于神经网络模型的Transformer模型，并使用大规模可用的单语言数据进行采样方法进行增强。此外，我们还应用了多种方法来提高翻译质量，包括集成和后处理。在公共测试集上，我们取得了38.9的BLEU指标在中越文翻译和38.0的BLEU指标在越文中翻译，这些成绩超过了一些强大的基线。
</details></li>
</ul>
<hr>
<h2 id="A-User-Centered-Evaluation-of-Spanish-Text-Simplification"><a href="#A-User-Centered-Evaluation-of-Spanish-Text-Simplification" class="headerlink" title="A User-Centered Evaluation of Spanish Text Simplification"></a>A User-Centered Evaluation of Spanish Text Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07556">http://arxiv.org/abs/2308.07556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian de Wynter, Anthony Hevia, Si-Qing Chen</li>
<li>for: 这个论文的目的是评估西班牙语文本简化（TS）系统的生产性，通过复杂句子和复杂词语识别两个 corpora 进行评估。</li>
<li>methods: 这个论文使用了神经网络来比较西班牙语特有的阅读性分数，并显示神经网络在预测用户TS首选项上一直表现出色。作者们还发现，多语言模型在同一任务上下降性能，但所有模型往往围绕幻数统计特征，如句子长度，进行围绕。</li>
<li>results: 作者们发现，神经网络在同一任务上一直表现出色，而且可以准确预测用户TS首选项。同时，作者们发现多语言模型在同一任务上下降性能，并且发现所有模型往往围绕幻数统计特征，如句子长度，进行围绕。<details>
<summary>Abstract</summary>
We present an evaluation of text simplification (TS) in Spanish for a production system, by means of two corpora focused in both complex-sentence and complex-word identification. We compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences regarding TS. As part of our analysis, we find that multilingual models underperform against equivalent Spanish-only models on the same task, yet all models focus too often on spurious statistical features, such as sentence length. We release the corpora in our evaluation to the broader community with the hopes of pushing forward the state-of-the-art in Spanish natural language processing.
</details>
<details>
<summary>摘要</summary>
我们对西班牙语文本简化（TS）的评估进行了一种生产系统的研究，通过两个聚合了复杂句子和复杂词的字句 corpus 进行了评估。我们将西班牙语特有的阅读性分数与神经网络进行比较，并发现后者在预测用户对TS的偏好时表现更好。在我们的分析中，我们发现了多语言模型在同一任务上下降表现，然而所有模型都太过注重干扰性的统计特征，如句子长度。我们将我们的评估 corpora 公开发布给广泛的社区，希望能够推动西班牙自然语言处理领域的前沿。
</details></li>
</ul>
<hr>
<h2 id="Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization"><a href="#Improving-CTC-AED-model-with-integrated-CTC-and-auxiliary-loss-regularization" class="headerlink" title="Improving CTC-AED model with integrated-CTC and auxiliary loss regularization"></a>Improving CTC-AED model with integrated-CTC and auxiliary loss regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08449">http://arxiv.org/abs/2308.08449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daobin Zhu, Xiangdong Su, Hongbin Zhang</li>
<li>for:  automatic speech recognition (ASR)</li>
<li>methods:  Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training, with two fusion methods (DAL and PMP) and auxiliary loss regularization</li>
<li>results:  Experimental results show that DAL method performs better in attention rescoring, while PMP method excels in CTC prefix beam search and greedy search.<details>
<summary>Abstract</summary>
Connectionist temporal classification (CTC) and attention-based encoder decoder (AED) joint training has been widely applied in automatic speech recognition (ASR). Unlike most hybrid models that separately calculate the CTC and AED losses, our proposed integrated-CTC utilizes the attention mechanism of AED to guide the output of CTC. In this paper, we employ two fusion methods, namely direct addition of logits (DAL) and preserving the maximum probability (PMP). We achieve dimensional consistency by adaptively affine transforming the attention results to match the dimensions of CTC. To accelerate model convergence and improve accuracy, we introduce auxiliary loss regularization for accelerated convergence. Experimental results demonstrate that the DAL method performs better in attention rescoring, while the PMP method excels in CTC prefix beam search and greedy search.
</details>
<details>
<summary>摘要</summary>
Connectionist temporal classification (CTC) 和 attention-based encoder decoder (AED) 的共同训练已经广泛应用在自动语音识别（ASR）中。与大多数混合模型不同，我们提出的集成-CTC 使用 AED 的注意力机制来导引 CTC 的输出。在这篇论文中，我们采用了两种合并方法，namely direct addition of logits (DAL) 和 preserving the maximum probability (PMP)。我们通过适应性折射变换来保持维度的一致性，以适应 CTC 的维度。为了加速模型的启动和提高准确性，我们引入了辅助损失补偿。实验结果表明，DAL 方法在注意力重新评分中表现更好，而 PMP 方法在 CTC 前缀搜索和扩散搜索中表现更好。
</details></li>
</ul>
<hr>
<h2 id="CALYPSO-LLMs-as-Dungeon-Masters’-Assistants"><a href="#CALYPSO-LLMs-as-Dungeon-Masters’-Assistants" class="headerlink" title="CALYPSO: LLMs as Dungeon Masters’ Assistants"></a>CALYPSO: LLMs as Dungeon Masters’ Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07540">http://arxiv.org/abs/2308.07540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/northern-lights-province/calypso-aiide-artifact">https://github.com/northern-lights-province/calypso-aiide-artifact</a></li>
<li>paper_authors: Andrew Zhu, Lara J. Martin, Andrew Head, Chris Callison-Burch</li>
<li>for: 这篇论文的目的是探讨用大自然语言模型（LLM）在桌面角色扮演游戏（D&amp;D）中的应用场景，以及这些技术在桌面游戏中的可能性。</li>
<li>methods: 该论文使用了大自然语言模型（GPT-3和ChatGPT）来生成合理的自然语言文本，并通过与游戏导ilder（DM）进行形成评估，以确定LLM在D&amp;D中的应用场景。</li>
<li>results: 研究发现，当给DM们提供LLM-力Point的支持时，他们表示可以 direktly present高品质的自然语言文本给玩家，以及低品质的想法，以便继续保持创作主义。这种方法可以帮助DMs在游戏中提供更多的创新和灵感，而无需干扰他们的创作过程。<details>
<summary>Abstract</summary>
The role of a Dungeon Master, or DM, in the game Dungeons & Dragons is to perform multiple tasks simultaneously. The DM must digest information about the game setting and monsters, synthesize scenes to present to other players, and respond to the players' interactions with the scene. Doing all of these tasks while maintaining consistency within the narrative and story world is no small feat of human cognition, making the task tiring and unapproachable to new players. Large language models (LLMs) like GPT-3 and ChatGPT have shown remarkable abilities to generate coherent natural language text. In this paper, we conduct a formative evaluation with DMs to establish the use cases of LLMs in D&D and tabletop gaming generally. We introduce CALYPSO, a system of LLM-powered interfaces that support DMs with information and inspiration specific to their own scenario. CALYPSO distills game context into bite-sized prose and helps brainstorm ideas without distracting the DM from the game. When given access to CALYPSO, DMs reported that it generated high-fidelity text suitable for direct presentation to players, and low-fidelity ideas that the DM could develop further while maintaining their creative agency. We see CALYPSO as exemplifying a paradigm of AI-augmented tools that provide synchronous creative assistance within established game worlds, and tabletop gaming more broadly.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Finding-Stakeholder-Material-Information-from-10-K-Reports-using-Fine-Tuned-BERT-and-LSTM-Models"><a href="#Finding-Stakeholder-Material-Information-from-10-K-Reports-using-Fine-Tuned-BERT-and-LSTM-Models" class="headerlink" title="Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models"></a>Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07522">http://arxiv.org/abs/2308.07522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Zitian Chen</li>
<li>For: The paper aims to identify stakeholder-material information in annual 10-K reports to help companies and investors efficiently extract material information.* Methods: The authors fine-tuned BERT models and RNN models with LSTM layers to identify stakeholder-material information, using business expert-labeled training data.* Results: The best model achieved an accuracy of 0.904 and an F1 score of 0.899 in test data, significantly outperforming the baseline model.<details>
<summary>Abstract</summary>
All public companies are required by federal securities law to disclose their business and financial activities in their annual 10-K reports. Each report typically spans hundreds of pages, making it difficult for human readers to identify and extract the material information efficiently. To solve the problem, I have fine-tuned BERT models and RNN models with LSTM layers to identify stakeholder-material information, defined as statements that carry information about a company's influence on its stakeholders, including customers, employees, investors, and the community and natural environment. The existing practice uses keyword search to identify such information, which is my baseline model. Using business expert-labeled training data of nearly 6,000 sentences from 62 10-K reports published in 2022, the best model has achieved an accuracy of 0.904 and an F1 score of 0.899 in test data, significantly above the baseline model's 0.781 and 0.749 respectively. Furthermore, the same work was replicated on more granular taxonomies, based on which four distinct groups of stakeholders (i.e., customers, investors, employees, and the community and natural environment) are tested separately. Similarly, fined-tuned BERT models outperformed LSTM and the baseline. The implications for industry application and ideas for future extensions are discussed.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)所有公开公司都需要根据联邦证券法披露其业务和财务活动在每年的10-K报告中。每份报告通常包含数百页的内容，使得人类读者很难快速 identificar和提取重要信息。为解决这个问题，我已经细化BERT模型和RNN模型的LSTM层来标识利益相关者材料信息，其定义为公司对利益相关者（包括客户、员工、投资者和社区和自然环境）的影响信息。现行做法使用关键词搜索来标识这类信息，这是我的基线模型。使用2022年62份10-K报告中的商业专家标注训练数据（约6,000句），最佳模型在测试数据中达到了0.904的准确率和0.899的F1得分，与基线模型的0.781和0.749分别显著上升。此外，同样的工作也在更细化的分类中进行了重复，基于这四个不同的利益相关者组（即客户、投资者、员工和社区和自然环境）进行了分开测试。同样，细化BERT模型也超过了LSTM和基线模型。关于业务应用和未来扩展的想法都是讨论的。
</details></li>
</ul>
<hr>
<h2 id="Data-Race-Detection-Using-Large-Language-Models"><a href="#Data-Race-Detection-Using-Large-Language-Models" class="headerlink" title="Data Race Detection Using Large Language Models"></a>Data Race Detection Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07505">http://arxiv.org/abs/2308.07505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Le Chen, Xianzhong Ding, Murali Emani, Tristan Vanderbruggen, Pei-hung Lin, Chuanhua Liao</li>
<li>for: 本研究旨在探讨一种基于大语言模型（LLM）的数据竞争检测方法，以代替手动创建资源投入庞大的工具。</li>
<li>methods: 本研究使用了提示工程和精度调整技术，创建了专门的DRB-ML数据集，并使用了代表性的LLM和开源LLM进行评估。</li>
<li>results: 研究显示，LLM可以成为数据竞争检测的可能性，但是它们还无法与传统数据竞争检测工具相比提供详细的变量对 causing 数据竞争的信息。<details>
<summary>Abstract</summary>
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SOTASTREAM-A-Streaming-Approach-to-Machine-Translation-Training"><a href="#SOTASTREAM-A-Streaming-Approach-to-Machine-Translation-Training" class="headerlink" title="SOTASTREAM: A Streaming Approach to Machine Translation Training"></a>SOTASTREAM: A Streaming Approach to Machine Translation Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07489">http://arxiv.org/abs/2308.07489</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marian-nmt/sotastream">https://github.com/marian-nmt/sotastream</a></li>
<li>paper_authors: Matt Post, Thamme Gowda, Roman Grundkiewicz, Huda Khayrallah, Rohit Jain, Marcin Junczys-Dowmunt</li>
<li>for: 这 paper aims to address the limitations of traditional data preparation methods for machine translation toolkits, which can be time-consuming, expensive, and cumbersome.</li>
<li>methods: The proposed approach separates the generation of data from its consumption, allowing for on-the-fly modifications and eliminating the need for a separate pre-processing step.</li>
<li>results: The proposed approach reduces training time, adds flexibility, reduces experiment management complexity, and reduces disk space without affecting the accuracy of the trained models.Here’s the simplified Chinese text:</li>
<li>for: 这 paper 的目的是解决传统机器翻译工具集的数据准备方法的限制，这些方法可能会占用很多时间、成本和复杂度。</li>
<li>methods: 提议的方法是将数据生成和数据消耗分离开来，这样允许在实际使用过程中进行实时修改，并完全消除预处理步骤。</li>
<li>results: 提议的方法可以减少训练时间、添加灵活性、减少实验管理复杂度和减少磁盘空间，而不影响训练出来的模型的准确性。<details>
<summary>Abstract</summary>
Many machine translation toolkits make use of a data preparation step wherein raw data is transformed into a tensor format that can be used directly by the trainer. This preparation step is increasingly at odds with modern research and development practices because this process produces a static, unchangeable version of the training data, making common training-time needs difficult (e.g., subword sampling), time-consuming (preprocessing with large data can take days), expensive (e.g., disk space), and cumbersome (managing experiment combinatorics). We propose an alternative approach that separates the generation of data from the consumption of that data. In this approach, there is no separate pre-processing step; data generation produces an infinite stream of permutations of the raw training data, which the trainer tensorizes and batches as it is consumed. Additionally, this data stream can be manipulated by a set of user-definable operators that provide on-the-fly modifications, such as data normalization, augmentation or filtering. We release an open-source toolkit, SOTASTREAM, that implements this approach: https://github.com/marian-nmt/sotastream. We show that it cuts training time, adds flexibility, reduces experiment management complexity, and reduces disk space, all without affecting the accuracy of the trained models.
</details>
<details>
<summary>摘要</summary>
许多机器翻译工具包括一个数据准备步骤，将原始数据转换成可直接用于训练的张量格式。这个过程在现代研发实践中变得越来越不合适，因为这会生成一个静态、不可变的版本的训练数据，使得一些常见的训练时间需求（如字符抽样）变得困难、时间consuming（处理大量数据可以花费多天）、昂贵（如磁盘空间）和困难（实验组合管理）。我们提出一种新的方法，即将数据生成与数据消耗分离开来。在这种方法中，没有单独的预处理步骤；数据生成生成了无限长的Permutation序列，这些 permutation被训练者张量化并批处理，直到它们被消耗。此外，这个数据流可以通过用户定义的操作符进行实时修改，例如数据Normalization、扩展或筛选。我们发布了一个开源工具kit，SOTASTREAM，实现了这种方法：https://github.com/marian-nmt/sotastream。我们表明，它可以减少训练时间，添加灵活性，降低实验管理复杂度，并降低磁盘空间，而无需影响训练出来的模型准确性。
</details></li>
</ul>
<hr>
<h2 id="O-1-Self-training-with-Oracle-and-1-best-Hypothesis"><a href="#O-1-Self-training-with-Oracle-and-1-best-Hypothesis" class="headerlink" title="O-1: Self-training with Oracle and 1-best Hypothesis"></a>O-1: Self-training with Oracle and 1-best Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07486">http://arxiv.org/abs/2308.07486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murali Karthick Baskar, Andrew Rosenberg, Bhuvana Ramabhadran, Kartik Audhkhasi</li>
<li>for: 提高Speech Recognition训练的准确率和评估 metrics</li>
<li>methods: 使用O-1自适应目标函数，可以处理both超级vised和无级vised数据，并且可以减少训练偏见</li>
<li>results: O-1对SpeechStew数据集和一个大规模的内部数据集进行评估，与EMBR相比，O-1可以将实际和oracle表现之间的差距减少80%，并且在不同的SpeechStew数据集上实现13%-25%的相对改善，对EMBR训练的内部数据集也可以减少12%的差距。总的来说，O-1可以提高WER的准确率9%。<details>
<summary>Abstract</summary>
We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\% relative compared to EMBR which bridges the gap by 43\% relative. O-1 achieves 13\% to 25\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.
</details>
<details>
<summary>摘要</summary>
我们介绍O-1，一个新的自我训练目标，用于降低训练偏见和统一训练和评估指标 для语音识别。O-1是EMBR的快速版本，可以提高oracle假设，并可以处理both监控和无监控数据。我们透过使用O-1目标，在公开ailable的SpeechStew数据集和一个大规模的内部数据集上进行评估。在SpeechStew上，O-1目标可以关闭实际和oracle性能之间的差距 by 80% relative compared to EMBR，而EMBR则可以关闭差距 by 43% relative。O-1在不同的SpeechStew数据集上的表现亮眼，比EMBR高13%到25% relative，并且与oracle WER之间的差距降低12% relative。总的来说，O-1对EMBR的WER进行了9%的相对改善，证明了O-1目标的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Development-and-Evaluation-of-Three-Chatbots-for-Postpartum-Mood-and-Anxiety-Disorders"><a href="#Development-and-Evaluation-of-Three-Chatbots-for-Postpartum-Mood-and-Anxiety-Disorders" class="headerlink" title="Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders"></a>Development and Evaluation of Three Chatbots for Postpartum Mood and Anxiety Disorders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07407">http://arxiv.org/abs/2308.07407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuewen Yao, Miriam Mikhelson, S. Craig Watkins, Eunsol Choi, Edison Thomaz, Kaya de Barbaro</li>
<li>for: 本研究的目的是开发一些聊天机器人，以提供新生婴期护理者所需的情感支持。</li>
<li>methods: 我们使用了规则引导的和生成模型，以提供上下文特定的同情支持。</li>
<li>results: 我们的规则引导模型表现最佳，其输出与真实参考数据几乎相同，同时含有最高水平的同情。人工用户对规则引导聊天机器人表示喜欢，因为它的回答具有上下文特定和人类化的特点。生成模型也能生成同情的回答，但由于训练数据的限制，它的回答经常具有含糊不清的问题。<details>
<summary>Abstract</summary>
In collaboration with Postpartum Support International (PSI), a non-profit organization dedicated to supporting caregivers with postpartum mood and anxiety disorders, we developed three chatbots to provide context-specific empathetic support to postpartum caregivers, leveraging both rule-based and generative models. We present and evaluate the performance of our chatbots using both machine-based metrics and human-based questionnaires. Overall, our rule-based model achieves the best performance, with outputs that are close to ground truth reference and contain the highest levels of empathy. Human users prefer the rule-based chatbot over the generative chatbot for its context-specific and human-like replies. Our generative chatbot also produced empathetic responses and was described by human users as engaging. However, limitations in the training dataset often result in confusing or nonsensical responses. We conclude by discussing practical benefits of rule-based vs. generative models for supporting individuals with mental health challenges. In light of the recent surge of ChatGPT and BARD, we also discuss the possibilities and pitfalls of large language models for digital mental healthcare.
</details>
<details>
<summary>摘要</summary>
合作 Postpartum Support International (PSI) 非营利组织，我们开发了三个聊天机器人，以提供适应性强的同理支持给孕后照顾者，利用规则基本和生成模型。我们对聊天机器人的表现进行评估，使用机器人和人类Questionnaire。总的来说，我们的规则基本模型实现了最好的表现，输出与真实参照接近，同时具有最高水平的同理。人类用户对规则基本聊天机器人的喜欢度最高，因为它的回答具有Context-specific和人类化的特点。我们的生成模型也生成了同理的回答，但是训练数据的限制导致它们的回答有时会很混乱或无意义。我们 conclude 规则基本模型和生成模型在支持人们 mental health 挑战时的实际效用，以及 ChatGPT 和 BARD 等大语言模型在数字 mental healthcare 中的可能性和风险。
</details></li>
</ul>
<hr>
<h2 id="Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models"><a href="#Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models" class="headerlink" title="Text Injection for Capitalization and Turn-Taking Prediction in Speech Models"></a>Text Injection for Capitalization and Turn-Taking Prediction in Speech Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07395">http://arxiv.org/abs/2308.07395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaan Bijwadia, Shuo-yiin Chang, Weiran Wang, Zhong Meng, Hao Zhang, Tara N. Sainath</li>
<li>for: 提高 auxiliary 任务表现（非ASR任务）</li>
<li>methods: 使用文本注入（JEIT）训练 ASR 模型，并在两个 auxiliary 任务上进行训练</li>
<li>results: 文本注入方法可以提高长尾数据的首字母排序性能，并提高转接推断精度<details>
<summary>Abstract</summary>
Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.
</details>
<details>
<summary>摘要</summary>
文本注入技术可以用于自动语音识别（ASR），其中使用无对应的文本数据来补充带有音频数据的对应数据，有效地降低了单词错误率。本研究探讨了文本注入技术在辅助任务中的应用，这些任务通常是END-TO-END模型完成的非ASR任务。在这个工作中，我们使用了结合端到端和内部语言模型训练（JEIT）作为我们的文本注入算法，用于训练一个ASR模型，该模型完成了两个辅助任务。第一个是字母大小 normalization 任务，第二个是判断用户是否已经完成了在数字助手交互中的对话转移。我们的实验结果表明，我们的文本注入方法可以提高长尾数据中的字母大小正确率，并提高了对话转移检测的准确率。
</details></li>
</ul>
<hr>
<h2 id="Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech"><a href="#Using-Text-Injection-to-Improve-Recognition-of-Personal-Identifiers-in-Speech" class="headerlink" title="Using Text Injection to Improve Recognition of Personal Identifiers in Speech"></a>Using Text Injection to Improve Recognition of Personal Identifiers in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07393">http://arxiv.org/abs/2308.07393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yochai Blau, Rohan Agrawal, Lior Madmony, Gary Wang, Andrew Rosenberg, Zhehuai Chen, Zorik Gekhman, Genady Beryozkin, Parisa Haghani, Bhuvana Ramabhadran</li>
<li>for: 提高自动语音识别（ASR）系统中个人特定信息（PII）的识别率。</li>
<li>methods: 使用文本插入法将假文本替换PII类别，以提高训练数据中PII类别的识别率。</li>
<li>results: 在医疗记录中提高了名称和日期的回忆率，同时提高了总的word error rate（WER）。对数字序列也显示了改善Character Error Rate和句子准确率。<details>
<summary>Abstract</summary>
Accurate recognition of specific categories, such as persons' names, dates or other identifiers is critical in many Automatic Speech Recognition (ASR) applications. As these categories represent personal information, ethical use of this data including collection, transcription, training and evaluation demands special care. One way of ensuring the security and privacy of individuals is to redact or eliminate Personally Identifiable Information (PII) from collection altogether. However, this results in ASR models that tend to have lower recognition accuracy of these categories. We use text-injection to improve the recognition of PII categories by including fake textual substitutes of PII categories in the training data using a text injection method. We demonstrate substantial improvement to Recall of Names and Dates in medical notes while improving overall WER. For alphanumeric digit sequences we show improvements to Character Error Rate and Sentence Accuracy.
</details>
<details>
<summary>摘要</summary>
准确地识别特定类别，如人名、日期等标识信息，在自动语音识别（ASR）应用中是非常重要的。这些类别代表个人信息，因此对这些数据的采集、译写、训练和评估需要特殊的注意。一种方法是完全不收集人类标识信息（PII），但这会导致ASR模型对这些类别的识别精度下降。我们使用文本插入法来提高PII类别的识别精度，通过在训练数据中插入假文本substitute来实现。我们在医疗笔记中示出了大幅提高名称和日期的回忆率，同时提高总的word Error Rate。对于字符串数字序列，我们示出了字符错误率和句子准确率的改善。
</details></li>
</ul>
<hr>
<h2 id="Platypus-Quick-Cheap-and-Powerful-Refinement-of-LLMs"><a href="#Platypus-Quick-Cheap-and-Powerful-Refinement-of-LLMs" class="headerlink" title="Platypus: Quick, Cheap, and Powerful Refinement of LLMs"></a>Platypus: Quick, Cheap, and Powerful Refinement of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07317">http://arxiv.org/abs/2308.07317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arielnlee/Platypus">https://github.com/arielnlee/Platypus</a></li>
<li>paper_authors: Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz</li>
<li>for: 这个论文是为了描述一个名为Platypus的家族 Large Language Models (LLMs)，它们在 HuggingFace 的开放 LLM Leaderboard 上达到了最高的表现并现在位于第一名。</li>
<li>methods: 这个论文使用了一个名为 Open-Platypus 的精心准备和合并 LoRA 模块，以保留预训练 LLMs 的强大优先知识，同时将特定领域知识带到表面。</li>
<li>results:  Platypus 家族在量化 LLM 度量上表现出色，在模型尺寸上占据了全球 Open LLM leaderboard 的排名，而使用的 fine-tuning 数据和总计算量只是其他 state-of-the-art fine-tuned LLMs 所需的一小部分。例如，一个 13B Platypus 模型可以在单个 A100 GPU 上使用 25k 问题进行 5 小时的训练。这是 Open-Platypus 数据集的质量的证明，并开启了更多改进的可能性。<details>
<summary>Abstract</summary>
We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: https://platypus-llm.github.io
</details>
<details>
<summary>摘要</summary>
我们现在提出了$\textbf{ Platypus}$家族，这是一些精心调整和合并的大语言模型（LLMs），它在HuggingFace的开源LLM排名榜上 currently stands at first place as of the release date of this work. 在这个工作中，我们描述了我们的手动抽象 dataset $\textbf{Open-Platypus}$，这是其他开放数据集的一个子集，并且 $\textit{我们向公众发布了这些数据}$。我们的过程包括了精心调整和合并LoRA模块，以保留预训练LLMs的强大优先知识，同时将特定领域知识带到表面。我们还尽可能地检查测试数据泄露和训练数据污染，以便未来的研究。特别是，Platypus家族在量化LLM指标中表现出色，在模型尺寸上占据全球开源LLM排名榜首位，而使用的是比其他 state-of-the-art 精心调整LLMs的一部分的精心调整数据和总计算资源。例如，一个13B Platypus模型可以在 $\textit{单个}$ A100 GPU 上使用 25k 问题，在 5 小时内训练完成。这是一个证明我们 Open-Platypus 数据集的质量，并开创了更多的改进机会。项目页面：https://platypus-llm.github.io
</details></li>
</ul>
<hr>
<h2 id="The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation"><a href="#The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation" class="headerlink" title="The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation"></a>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07286">http://arxiv.org/abs/2308.07286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat</li>
<li>for: 这篇论文是为了提供一种自动评估机器翻译（MT）系统的方法，以便在MT系统的快速迭代发展中进行评估。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）的理解和在场景学习能力，并让它们标注翻译中的错误。</li>
<li>results: 研究发现，使用AutoMQM技术可以提高MT系统的性能，特别是使用更大的模型时。此外，AutoMQM还提供了解释性的错误块，与人工标注相Alignment。<details>
<summary>Abstract</summary>
Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.
</details>
<details>
<summary>摘要</summary>
自动评估机器翻译（MT）是翻译系统的快速迭代发展的重要工具。虽然已经取得了较大的进步，但当前的度量仍然缺乏详细的错误标注，如多维质量指标（MQM）。在这篇文章中，我们帮助填补这个空白，并提出了AutoMQM技术，它利用大型自然语言模型（LLM）的理解和上下文学习能力，并让它们标注和分类翻译中的错误。我们首先通过对最近的LLM，如PaLM和PaLM-2，进行简单的分数预测提问，并研究了标注数据的影响。然后，我们评估了AutoMQM技术，并发现它在比只是提问分数时提高性能（尤其是大型模型），并提供了解释性的错误排序。
</details></li>
</ul>
<hr>
<h2 id="Comparison-between-parameter-efficient-techniques-and-full-fine-tuning-A-case-study-on-multilingual-news-article-classification"><a href="#Comparison-between-parameter-efficient-techniques-and-full-fine-tuning-A-case-study-on-multilingual-news-article-classification" class="headerlink" title="Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification"></a>Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07282">http://arxiv.org/abs/2308.07282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olesya Razuvayevskaya, Ben Wu, Joao A. Leite, Freddy Heppell, Ivan Srba, Carolina Scarton, Kalina Bontcheva, Xingyi Song</li>
<li>for: 本研究旨在investigating parameter-efficient fine-tuning techniques的影响于多语言文本分类任务（类型、框架和说服技巧检测），包括不同的输入长度、预测类数和分类难度。</li>
<li>methods: 本研究使用了Adaptors和LoRA技术来实现parameter-efficient fine-tuning，并进行了对不同训练场景（训练原始多语言数据、翻译成英语和英语只数据）和不同语言的深入分析。</li>
<li>results: 研究发现，在多语言文本分类任务中，Adaptors和LoRA技术可以减少训练时间和计算成本，并且在某些情况下可以提高性能。<details>
<summary>Abstract</summary>
Adapters and Low-Rank Adaptation (LoRA) are parameter-efficient fine-tuning techniques designed to make the training of language models more efficient. Previous results demonstrated that these methods can even improve performance on some classification tasks. This paper complements the existing research by investigating how these techniques influence the classification performance and computation costs compared to full fine-tuning when applied to multilingual text classification tasks (genre, framing, and persuasion techniques detection; with different input lengths, number of predicted classes and classification difficulty), some of which have limited training data. In addition, we conduct in-depth analyses of their efficacy across different training scenarios (training on the original multilingual data; on the translations into English; and on a subset of English-only data) and different languages. Our findings provide valuable insights into the applicability of the parameter-efficient fine-tuning techniques, particularly to complex multilingual and multilabel classification tasks.
</details>
<details>
<summary>摘要</summary>
这篇文章进一步探讨了微调和低阶化适应（LoRA）技术的影响，它们是用于对语言模型进行更有效的训练。过往的研究显示这些技术可以提高一些分类任务的性能。本文在多种多元文本分类任务（文类、几何、说服等）中进行了广泛的实验，包括有限的训练数据。此外，我们还进行了不同训练enario（训练原始多种语言数据；训练英文翻译；和使用英文数据subset）和不同语言的深入分析。我们的发现将有价值的帮助在复杂的多种语言和多类分类任务中应用这些参数有效的微调技术。
</details></li>
</ul>
<hr>
<h2 id="Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning"><a href="#Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning" class="headerlink" title="Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning"></a>Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07272">http://arxiv.org/abs/2308.07272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi Li, Yu Lan, Chao Shen</li>
<li>for: 提高几何学NLU任务中的表现，减少专家知识和人工干预。</li>
<li>methods: 对PLMs进行对话分析，设计可读性检测 metric，使用RL框架和政策网络进行优化。</li>
<li>results: 在四个开源数据集上，DP_2O方法在几何学NLU任务中的表现高于SOTA方法1.52%，并且具有良好的通用性、Robustness和普适性。<details>
<summary>Abstract</summary>
Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that $DP_2O$ has good universality, robustness, and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.CL_2023_08_15/" data-id="clly4xtbt0029vl88a7hldfcp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/cs.LG_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T10:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/cs.LG_2023_08_15/">cs.LG - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Dyadic-Reinforcement-Learning"><a href="#Dyadic-Reinforcement-Learning" class="headerlink" title="Dyadic Reinforcement Learning"></a>Dyadic Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07843">http://arxiv.org/abs/2308.07843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/statisticalreinforcementlearninglab/roadmap2.0testbed">https://github.com/statisticalreinforcementlearninglab/roadmap2.0testbed</a></li>
<li>paper_authors: Shuangning Li, Lluis Salvat Niell, Sung Won Choi, Inbal Nahum-Shani, Guy Shani, Susan Murphy</li>
<li>for: 该论文旨在提高健康结果，通过在日常生活中提供便捷的干预方法。</li>
<li>methods: 该论文提出了一种基于在线演进学习算法，以个性化干预发送方式，基于Contextual因素和target人和他们的护理伴侣之前的反应。</li>
<li>results: 通过在模拟场景和实际数据集上进行实验研究，提出了一种 bayesian和层次的dyadic RL算法，并证明了其可预测性。<details>
<summary>Abstract</summary>
Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation studies on both toy scenarios and on a realistic test bed constructed from data collected in a mobile health study.
</details>
<details>
<summary>摘要</summary>
Mobile health aimed to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proved crucial in helping individuals manage burdensome medical conditions. This presented opportunities in mobile health to design interventions that targeted the dyadic relationship - the relationship between a target person and their care partner - with the aim of enhancing social support. In this paper, we developed dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impacted the dyad across multiple time intervals. The developed dyadic RL was Bayesian and hierarchical. We formally introduced the problem setup, developed dyadic RL, and established a regret bound. We demonstrated dyadic RL's empirical performance through simulation studies on both toy scenarios and on a realistic test bed constructed from data collected in a mobile health study.
</details></li>
</ul>
<hr>
<h2 id="Simple-and-Efficient-Partial-Graph-Adversarial-Attack-A-New-Perspective"><a href="#Simple-and-Efficient-Partial-Graph-Adversarial-Attack-A-New-Perspective" class="headerlink" title="Simple and Efficient Partial Graph Adversarial Attack: A New Perspective"></a>Simple and Efficient Partial Graph Adversarial Attack: A New Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07834">http://arxiv.org/abs/2308.07834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pasalab/pga">https://github.com/pasalab/pga</a></li>
<li>paper_authors: Guanghui Zhu, Mengyu Chen, Chunfeng Yuan, Yihua Huang</li>
<li>for: 提高图 neural network 的 robustness和安全性，针对图中所有节点的全球攻击方法。</li>
<li>methods: 提出一种全新的partial graph attack（PGA）方法，选择易于攻击的节点作为攻击目标，并提出了一种层次目标选择策略、一种成本效果较高的锚点选择策略和一种迭代循环增强的迭代式攻击方法。</li>
<li>results: 对比其他现有的图全球攻击方法，PGA可以实现显著提高攻击效果和攻击效率。<details>
<summary>Abstract</summary>
As the study of graph neural networks becomes more intensive and comprehensive, their robustness and security have received great research interest. The existing global attack methods treat all nodes in the graph as their attack targets. Although existing methods have achieved excellent results, there is still considerable space for improvement. The key problem is that the current approaches rigidly follow the definition of global attacks. They ignore an important issue, i.e., different nodes have different robustness and are not equally resilient to attacks. From a global attacker's view, we should arrange the attack budget wisely, rather than wasting them on highly robust nodes. To this end, we propose a totally new method named partial graph attack (PGA), which selects the vulnerable nodes as attack targets. First, to select the vulnerable items, we propose a hierarchical target selection policy, which allows attackers to only focus on easy-to-attack nodes. Then, we propose a cost-effective anchor-picking policy to pick the most promising anchors for adding or removing edges, and a more aggressive iterative greedy-based attack method to perform more efficient attacks. Extensive experimental results demonstrate that PGA can achieve significant improvements in both attack effect and attack efficiency compared to other existing graph global attack methods.
</details>
<details>
<summary>摘要</summary>
Our approach consists of three key components:1. Hierarchical target selection policy: This policy allows attackers to focus on easy-to-attack nodes, reducing the overall cost of the attack.2. Cost-effective anchor-picking policy: This policy selects the most promising anchors for adding or removing edges, maximizing the attack effect while minimizing the cost.3. Iterative greedy-based attack method: This method performs more efficient attacks by iteratively adding or removing edges based on the selected anchors.Our extensive experimental results show that PGA achieves significant improvements in both attack effect and attack efficiency compared to other existing graph global attack methods.
</details></li>
</ul>
<hr>
<h2 id="REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science"><a href="#REFORMS-Reporting-Standards-for-Machine-Learning-Based-Science" class="headerlink" title="REFORMS: Reporting Standards for Machine Learning Based Science"></a>REFORMS: Reporting Standards for Machine Learning Based Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07832">http://arxiv.org/abs/2308.07832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayash Kapoor, Emily Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail, Odd Erik Gundersen, Jake M. Hofman, Jessica Hullman, Michael A. Lones, Momin M. Malik, Priyanka Nanayakkara, Russell A. Poldrack, Inioluwa Deborah Raji, Michael Roberts, Matthew J. Salganik, Marta Serra-Garcia, Brandon M. Stewart, Gilles Vandewiele, Arvind Narayanan</li>
<li>For: The paper aims to provide clear reporting standards for machine learning (ML) based science to address the issues of validity, reproducibility, and generalizability in scientific research.* Methods: The paper presents the REFORMS checklist, a set of 32 questions and guidelines developed based on a consensus of 19 researchers across various disciplines.* Results: The REFORMS checklist can serve as a resource for researchers, referees, and journals to ensure transparency and reproducibility in ML-based scientific research.Here is the information in Simplified Chinese text:* For: 这篇论文目标是提供机器学习（ML）基于科学研究的清晰报告标准，以解决科学研究中有效性、可重现性和普适性的问题。* Methods: 论文提出了REFORMS检查表（Reporting Standards For Machine Learning Based Science），这是基于19位研究者来自计算机科学、数据科学、数学、社会科学和生物医学等领域的共识，包括32个问题和对应的指南。* Results: REFORMS检查表可以为研究者、审稿人和杂志编辑提供一个资源，以确保机器学习基于科学研究的透明度和可重现性。<details>
<summary>Abstract</summary>
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing a study, for referees when reviewing papers, and for journals when enforcing standards for transparency and reproducibility.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）方法在科学研究中广泛应用，但是其应用也伴随着有效性、可重复性和普遍性的失败。这些失败可能会阻碍科学进步，导致无效的宣称得到共识，并且可能会下降机器学习基于科学的威信。机器学习方法经常在不同领域应用并失败，这使我们意识到了需要提供明确的报告标准。基于过去的文献检索，我们提出了REFORMS检查列表（Reporting Standards For Machine Learning Based Science）。它包括32个问题和一对拥有相同目标的指南。REFORMS是由19名来自计算机科学、数据科学、数学、社会科学和生物医学科学的研究人员共识而成，它可以作为研究人员设计和实施研究时的参考，同时也可以用于审稿人们审核文章，以及杂志 enforcing 透明度和可重复性的标准。
</details></li>
</ul>
<hr>
<h2 id="CMISR-Circular-Medical-Image-Super-Resolution"><a href="#CMISR-Circular-Medical-Image-Super-Resolution" class="headerlink" title="CMISR: Circular Medical Image Super-Resolution"></a>CMISR: Circular Medical Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08567">http://arxiv.org/abs/2308.08567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Honggui Li, Maria Trocan, Dimitri Galayko, Mohamad Sawan</li>
<li>for: 提高医疗影像超分辨率（MISR）的性能，提出一种基于循环反馈的医疗影像超分辨率框架（CMISR）。</li>
<li>methods: 使用循环反馈机制，分为本地反馈和全局反馈两类，实现了关键点稳定性和稳定性。</li>
<li>results: CMISR在三种缩放因子和三个开源医疗影像dataset上的实验结果表明，其在重建性能方面胜过传统MISR，特别适用于医疗影像中具有强的边缘或激烈对比。<details>
<summary>Abstract</summary>
Classical methods of medical image super-resolution (MISR) utilize open-loop architecture with implicit under-resolution (UR) unit and explicit super-resolution (SR) unit. The UR unit can always be given, assumed, or estimated, while the SR unit is elaborately designed according to various SR algorithms. The closed-loop feedback mechanism is widely employed in current MISR approaches and can efficiently improve their performance. The feedback mechanism may be divided into two categories: local and global feedback. Therefore, this paper proposes a global feedback-based closed-cycle framework, circular MISR (CMISR), with unambiguous UR and SR elements. Mathematical model and closed-loop equation of CMISR are built. Mathematical proof with Taylor-series approximation indicates that CMISR has zero recovery error in steady-state. In addition, CMISR holds plug-and-play characteristic which can be established on any existing MISR algorithms. Five CMISR algorithms are respectively proposed based on the state-of-the-art open-loop MISR algorithms. Experimental results with three scale factors and on three open medical image datasets show that CMISR is superior to MISR in reconstruction performance and is particularly suited to medical images with strong edges or intense contrast.
</details>
<details>
<summary>摘要</summary>
传统的医疗影像超分辨 (MISR) 方法使用开放式架构，其中隐式下解 (UR) 单元和显式超分辨 (SR) 单元是分开的。UR单元可以被给定、 Assuming 或估算，而SR单元则根据不同的SR算法进行精心设计。现有的MISR方法广泛采用了关闭着反馈机制，可以有效提高其性能。反馈机制可以分为两类：本地反馈和全球反馈。因此，本文提出了一种基于全球反馈的循环式框架，即循环MISR (CMISR)，其中UR和SR元素具有明确的定义。我们建立了CMISR的数学模型和关闭着方程，并通过泰勒级数拟合得出了CMISR在稳态状态下的零回归误差。此外，CMISR具有插件和玩儿特性，可以在任何现有的MISR算法基础上实现。我们根据现有的开放式MISR算法，分别提出了5种CMISR算法。实验结果表明，CMISR在重建性能方面高于MISR，特别适用于医疗影像中具有强的边缘或激烈的对比。
</details></li>
</ul>
<hr>
<h2 id="Cerberus-A-Deep-Learning-Hybrid-Model-for-Lithium-Ion-Battery-Aging-Estimation-and-Prediction-Based-on-Relaxation-Voltage-Curves"><a href="#Cerberus-A-Deep-Learning-Hybrid-Model-for-Lithium-Ion-Battery-Aging-Estimation-and-Prediction-Based-on-Relaxation-Voltage-Curves" class="headerlink" title="Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves"></a>Cerberus: A Deep Learning Hybrid Model for Lithium-Ion Battery Aging Estimation and Prediction Based on Relaxation Voltage Curves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07824">http://arxiv.org/abs/2308.07824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Xiang, Bo Jiang, Haifeng Dai</li>
<li>For: The paper aims to estimate and predict the capacity aging of lithium-ion batteries using a hybrid model based on deep learning, which can accurately forecast the future capacity of the batteries.* Methods: The model uses historical capacity decay data and extracts salient features from charge and discharge relaxation processes to estimate the present capacity and predict future capacity.* Results: The model achieves a mean absolute percentage error (MAPE) of 0.29% under a charging condition of 0.25C, demonstrating its effectiveness in estimating and predicting capacity aging using real-world relaxation processes and historical capacity records within battery management systems (BMS).<details>
<summary>Abstract</summary>
The degradation process of lithium-ion batteries is intricately linked to their entire lifecycle as power sources and energy storage devices, encompassing aspects such as performance delivery and cycling utilization. Consequently, the accurate and expedient estimation or prediction of the aging state of lithium-ion batteries has garnered extensive attention. Nonetheless, prevailing research predominantly concentrates on either aging estimation or prediction, neglecting the dynamic fusion of both facets. This paper proposes a hybrid model for capacity aging estimation and prediction based on deep learning, wherein salient features highly pertinent to aging are extracted from charge and discharge relaxation processes. By amalgamating historical capacity decay data, the model dynamically furnishes estimations of the present capacity and forecasts of future capacity for lithium-ion batteries. Our approach is validated against a novel dataset involving charge and discharge cycles at varying rates. Specifically, under a charging condition of 0.25C, a mean absolute percentage error (MAPE) of 0.29% is achieved. This outcome underscores the model's adeptness in harnessing relaxation processes commonly encountered in the real world and synergizing with historical capacity records within battery management systems (BMS), thereby affording estimations and prognostications of capacity decline with heightened precision.
</details>
<details>
<summary>摘要</summary>
锂离子电池的衰退过程与其整个生命周期深度相关，涵盖性能提供和能量存储等方面。因此，正确和快速地估计或预测锂离子电池的衰退状况备受广泛关注。然而，现有研究主要集中在 either 衰退估计或预测，忽视了这两个方面的动态融合。本文提出了一种基于深度学习的锂离子电池容量衰退估计和预测模型，其中抽象出了具有衰退相关性的充电和充放电过程特征。通过结合历史容量衰退数据，模型在实时提供了当前容量的估计和未来容量的预测，并且在充电条件下0.25C下达到了0.29%的平均绝对百分比误差（MAPE）。这一结果表明模型能够充分利用实际世界中常见的充电和充放电过程，同时与锂离子电池管理系统（BMS）中的历史容量记录相结合，从而为容量衰退的估计和预测提供了更高精度。
</details></li>
</ul>
<hr>
<h2 id="Deep-reinforcement-learning-for-process-design-Review-and-perspective"><a href="#Deep-reinforcement-learning-for-process-design-Review-and-perspective" class="headerlink" title="Deep reinforcement learning for process design: Review and perspective"></a>Deep reinforcement learning for process design: Review and perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07822">http://arxiv.org/abs/2308.07822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Qinghe Gao, Artur M. Schweidtmann</li>
<li>for: 本研究旨在探讨人工智能技术如何加速化学工业中的可再生能源和原料供应转型。</li>
<li>methods: 本研究使用深度强化学习，一种机器学习技术，来解决化学工程中复杂的决策问题，并且探讨了这些技术在过程设计中的应用前景。</li>
<li>results: 本研究对现有的深度强化学习在过程设计中的应用进行了抽象和评估，并探讨了未来这些技术在化学工程中的发展前景。<details>
<summary>Abstract</summary>
The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design. We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward. Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering.
</details>
<details>
<summary>摘要</summary>
“对于化学工业中的可再生能源和原料供应转型，需要新的概念过程设计方法。现在，人工智能技术的突破发展带来了加速这个转型的机遇。特别是深度强化学习，一种机器学习的 subclass，在解决复杂决策问题和推动可持续过程设计方面表现出了潜力。我们通过三个主要元素：（i）信息表示，（ii）代理架构，以及（iii）环境和奖励，总结了现代研究的深度强化学习在过程设计方面的状况。此外，我们还讨论了下一步的挑战和未来研究的前景，以探索深度强化学习在化学工程中的潜力。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-the-Cost-of-Learning-in-Queueing-Systems"><a href="#Quantifying-the-Cost-of-Learning-in-Queueing-Systems" class="headerlink" title="Quantifying the Cost of Learning in Queueing Systems"></a>Quantifying the Cost of Learning in Queueing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07817">http://arxiv.org/abs/2308.07817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Freund, Thodoris Lykouris, Wentao Weng</li>
<li>For:  This paper is written for researchers and practitioners interested in queueing systems and their optimal control, particularly in the context of parameter uncertainty.* Methods: The paper proposes a new metric called the Cost of Learning in Queueing (CLQ) to quantify the maximum increase in time-averaged queue length caused by parameter uncertainty. The authors also propose a unified analysis framework that bridges Lyapunov and bandit analysis to establish the results.* Results: The paper characterizes the CLQ of a single-queue multi-server system and extends the results to multi-queue multi-server systems and networks of queues. The authors show that the CLQ is a useful metric for evaluating the performance of queueing systems in the presence of parameter uncertainty.<details>
<summary>Abstract</summary>
Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.   In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues. In establishing our results, we propose a unified analysis framework for CLQ that bridges Lyapunov and bandit analysis, which could be of independent interest.
</details>
<details>
<summary>摘要</summary>
queueing 系统是广泛应用的随机模型，有用cases在通信网络、医疗、服务系统等。虽然其优化控制得到了广泛的研究，但大多数现有方法假设系统参数具有完美的知识。然而，这种假设在实践中rarely holds，因此引起了一种Recent Line of Work on Bandit Learning for Queueing Systems。这个流行的研究方向主要关注 asymptotic performance of the proposed algorithms。在这篇论文中，我们 argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems, which typically occurs in the early stage。 Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty。 We characterize the CLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues。在证明我们的结果时，我们提出了一个统一的分析框架 для CLQ，该框架可以将 Lyapunov 和 bandit 分析相结合，这可能会对独立的研究有益。
</details></li>
</ul>
<hr>
<h2 id="Fairness-and-Privacy-in-Federated-Learning-and-Their-Implications-in-Healthcare"><a href="#Fairness-and-Privacy-in-Federated-Learning-and-Their-Implications-in-Healthcare" class="headerlink" title="Fairness and Privacy in Federated Learning and Their Implications in Healthcare"></a>Fairness and Privacy in Federated Learning and Their Implications in Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07805">http://arxiv.org/abs/2308.07805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UVA-MLSys/DS7406">https://github.com/UVA-MLSys/DS7406</a></li>
<li>paper_authors: Navya Annapareddy, Jade Preston, Judy Fox<br>for: This paper aims to provide an overview of the typical lifecycle of fair federated learning in research and an updated taxonomy to account for the current state of fairness in implementations, with a focus on the healthcare domain.methods: The paper uses a decentralized approach to training machine learning models, called federated learning, to address data security, privacy, and vulnerability considerations.results: The paper highlights the challenges of implementing fairness in federated learning, including node data not being independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and the heterogeneity of different clients within a network with respect to dataset bias and size. The paper also provides added insight into the implications and challenges of implementing and supporting fairness in federated learning in the healthcare domain.<details>
<summary>Abstract</summary>
Currently, many contexts exist where distributed learning is difficult or otherwise constrained by security and communication limitations. One common domain where this is a consideration is in Healthcare where data is often governed by data-use-ordinances like HIPAA. On the other hand, larger sample sizes and shared data models are necessary to allow models to better generalize on account of the potential for more variability and balancing underrepresented classes. Federated learning is a type of distributed learning model that allows data to be trained in a decentralized manner. This, in turn, addresses data security, privacy, and vulnerability considerations as data itself is not shared across a given learning network nodes. Three main challenges to federated learning include node data is not independent and identically distributed (iid), clients requiring high levels of communication overhead between peers, and there is the heterogeneity of different clients within a network with respect to dataset bias and size. As the field has grown, the notion of fairness in federated learning has also been introduced through novel implementations. Fairness approaches differ from the standard form of federated learning and also have distinct challenges and considerations for the healthcare domain. This paper endeavors to outline the typical lifecycle of fair federated learning in research as well as provide an updated taxonomy to account for the current state of fairness in implementations. Lastly, this paper provides added insight into the implications and challenges of implementing and supporting fairness in federated learning in the healthcare domain.
</details>
<details>
<summary>摘要</summary>
当前，有许多情况存在分布式学习是困难或受到安全和通信限制的情况。一个常见的领域是医疗领域，数据经常受到数据使用规定如HIPAA的限制。然而，更大的样本大小和共享数据模型是必要的，以使模型更好地泛化，因为可能存在更多的变化和平衡不足表示的类别。分布式学习是一种分布式学习模型，它使得数据在分布式学习网络中被训练，并解决了数据安全、隐私和抵触问题，因为数据本身不被分布式学习网络中的节点共享。主要挑战包括节点数据不是独立和同分布（iid）、客户需要高度的同域通信开销和客户网络中的数据偏好和大小不均。随着领域的发展，对分布式学习的公平性也被引入，并通过新的实现方式。公平性方法与标准的分布式学习不同，也有特殊的挑战和医疗领域中的考虑。本文尝试将研究中的公平分布式学习的典型生命周期和更新的分类表示出来，并提供了对当前公平性实现的更多的深入视角。最后，本文还提供了在实施和支持公平分布式学习在医疗领域中的挑战和问题。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Noise-Covariance-Estimation-under-Colored-Noise-using-Dynamic-Expectation-Maximization"><a href="#Adaptive-Noise-Covariance-Estimation-under-Colored-Noise-using-Dynamic-Expectation-Maximization" class="headerlink" title="Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization"></a>Adaptive Noise Covariance Estimation under Colored Noise using Dynamic Expectation Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07797">http://arxiv.org/abs/2308.07797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ajitham123/DEM_NCM">https://github.com/ajitham123/DEM_NCM</a></li>
<li>paper_authors: Ajith Anil Meera, Pablo Lanillos</li>
<li>for: 这篇论文是为了提出一个新的脑心理静电组织（Brain-Inspired Algorithm），用于精确地估计动态系统中的噪音协调矩阵（Noise Covariance Matrix，NCM）。</li>
<li>methods: 这个算法extend了Dynamic Expectation Maximization（DEM）算法，以在线上估计噪音协调矩阵和状态估计，并且可以适应彩色噪音（colored noise）的情况。</li>
<li>results: 透过Randomized numerical simulations，我们展示了我们的估计方法在彩色噪音下比基准方法（Variational Bayes）更好，并且在高彩色噪音情况下也能够实现更好的结果。<details>
<summary>Abstract</summary>
The accurate estimation of the noise covariance matrix (NCM) in a dynamic system is critical for state estimation and control, as it has a major influence in their optimality. Although a large number of NCM estimation methods have been developed, most of them assume the noises to be white. However, in many real-world applications, the noises are colored (e.g., they exhibit temporal autocorrelations), resulting in suboptimal solutions. Here, we introduce a novel brain-inspired algorithm that accurately and adaptively estimates the NCM for dynamic systems subjected to colored noise. Particularly, we extend the Dynamic Expectation Maximization algorithm to perform both online noise covariance and state estimation by optimizing the free energy objective. We mathematically prove that our NCM estimator converges to the global optimum of this free energy objective. Using randomized numerical simulations, we show that our estimator outperforms nine baseline methods with minimal noise covariance estimation error under colored noise conditions. Notably, we show that our method outperforms the best baseline (Variational Bayes) in joint noise and state estimation for high colored noise. We foresee that the accuracy and the adaptive nature of our estimator make it suitable for online estimation in real-world applications.
</details>
<details>
<summary>摘要</summary>
预测动态系统中噪声 covariance 矩阵（NCM）的准确性是控制和状态估计中关键的，因为它对系统的优化产生了很大的影响。虽然有很多 NCMEstimation 方法已经开发，但大多数它们假设噪声是白噪声（即噪声无相关性）。然而，在实际应用中，噪声通常是染色的（即噪声展现了时间自相关性），从而导致估计结果不佳。在这篇文章中，我们介绍了一种基于脑神经元的新算法，可以准确地适应 colored noise 的动态系统 NCM 估计。我们在 Dynamic Expectation Maximization 算法的基础上扩展了该算法，以在线进行噪声 covariance 和状态估计，并通过优化自由能对象来实现。我们数学证明了我们的 NCMEstimation 算法 converge 到 global optimum 的自由能对象上。使用随机数字 simulations，我们示出了我们的估计算法在噪声 Conditions 下比基eline方法（Variational Bayes）的噪声 covariance 估计误差较低。特别是，我们示出了我们的方法在高染色噪声 Conditions 下与 Variational Bayes 的联合噪声和状态估计表现较好。我们认为我们的方法的准确性和适应性使其适用于实际应用中的在线估计。
</details></li>
</ul>
<hr>
<h2 id="Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance"><a href="#Implementing-Quantum-Generative-Adversarial-Network-qGAN-and-QCBM-in-Finance" class="headerlink" title="Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance"></a>Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08448">http://arxiv.org/abs/2308.08448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Santanu Ganguly</li>
<li>for: 这个论文探讨了应用量子机器学习（QML）在金融领域的未来研究方向，以及在金融世界中各种应用的QML模型。</li>
<li>methods: 本文使用实际的金融数据和模拟环境，比较了不同QML模型的性能，包括qGAN和QCBM等。</li>
<li>results: 研究显示，QML在金融领域可能具有未来的优势，并且qGAN模型在某些情况下表现出了明显的优势。<details>
<summary>Abstract</summary>
Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material & molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
</details>
<details>
<summary>摘要</summary>
量子机器学习（QML）是一个跨学科领域，包括量子计算和经典机器学习（ML），被认为是未来量子机器的发展将首先影响的两个领域之一。现在，量子计算机已经在药物发现、物质和分子模拟以及金融领域中使用。在这个工作中，我们讨论了在金融领域应用量子机器学习（QML）的一些新 aktive研究领域。我们讨论了一些在金融界引起了广泛关注的QML模型，如quantum generative adversarial networks（qGAN）和Quantum Circuit Born Machine（QCBM）等，并使用实际世界金融数据进行比较。对于qGAN，我们定义了量子电路 для批分类器和生成器，并显示了未来量子优势的承诺。
</details></li>
</ul>
<hr>
<h2 id="Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models"><a href="#Informed-Named-Entity-Recognition-Decoding-for-Generative-Language-Models" class="headerlink" title="Informed Named Entity Recognition Decoding for Generative Language Models"></a>Informed Named Entity Recognition Decoding for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07791">http://arxiv.org/abs/2308.07791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Deußer, Lars Hillebrand, Christian Bauckhage, Rafet Sifa</li>
<li>for: 本研究旨在提出一种简单 yet effective的方法，即 Informed Named Entity Recognition Decoding (iNERD)，用于循环的 named entity recognition 任务。</li>
<li>methods: 该方法利用当前的生成模型来进行语言理解，并采用一种有知识的 decoding 方式，将信息提取 tasks 与开放式文本生成结合，从而提高性能并消除所有的幻觉。</li>
<li>results: 通过在 eight 个 named entity recognition 数据集上评估五种生成语言模型，研究发现该方法在未知实体类集合环境下表现出优异的适应能力，特别是在不知道实体类集合时，表现更加出色。<details>
<summary>Abstract</summary>
Ever-larger language models with ever-increasing capabilities are by now well-established text processing tools. Alas, information extraction tasks such as named entity recognition are still largely unaffected by this progress as they are primarily based on the previous generation of encoder-only transformer models. Here, we propose a simple yet effective approach, Informed Named Entity Recognition Decoding (iNERD), which treats named entity recognition as a generative process. It leverages the language understanding capabilities of recent generative models in a future-proof manner and employs an informed decoding scheme incorporating the restricted nature of information extraction into open-ended text generation, improving performance and eliminating any risk of hallucinations. We coarse-tune our model on a merged named entity corpus to strengthen its performance, evaluate five generative language models on eight named entity recognition datasets, and achieve remarkable results, especially in an environment with an unknown entity class set, demonstrating the adaptability of the approach.
</details>
<details>
<summary>摘要</summary>
现代语言模型不断增长，功能也不断提高。可是，信息提取任务，如名实Recognition，仍然受到这些进步的影响很少，因为它们基本上是基于上一代encoder-only transformer模型。在这里，我们提出了一种简单 yet effective的方法，即 Informed Named Entity Recognition Decoding（iNERD）。它将名实Recognition视为生成过程，利用当前的生成模型对语言理解能力，并采用了了知情 decode 策略，将开放式文本生成和信息提取相结合，提高性能，并完全消除任何幻觉的风险。我们在合并的名实Corpus上粗略调整我们的模型，使其在八个名实Recognition 数据集上表现出色，特别是在未知类型集的环境中，表现出了适应性。
</details></li>
</ul>
<hr>
<h2 id="DiffV2S-Diffusion-based-Video-to-Speech-Synthesis-with-Vision-guided-Speaker-Embedding"><a href="#DiffV2S-Diffusion-based-Video-to-Speech-Synthesis-with-Vision-guided-Speaker-Embedding" class="headerlink" title="DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding"></a>DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07787">http://arxiv.org/abs/2308.07787</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joannahong/diffv2s">https://github.com/joannahong/diffv2s</a></li>
<li>paper_authors: Jeongsoo Choi, Joanna Hong, Yong Man Ro</li>
<li>for: 这个研究的目的是提高视频到语音合成的精度和可理解性，使得可以从视频输入中恰当地重建出高质量的语音。</li>
<li>methods: 这个研究使用了一种新的视觉导向说话嵌入表示器，它使用了一个自我超vised预训练模型和提问调整技术来提取嵌入。此外，它还使用了一种扩散基于的视频到语音合成模型，称为DiffV2S，其 conditioned于提取的嵌入和输入视频帧中的视觉表示。</li>
<li>results: 这个研究的实验结果表明，DiffV2S可以保持输入视频帧中的音素细节，同时创造一个高度可理解的mel-spectrogram，其中每个说话者的身份都被保留。相比之下，DiffV2S的表现比之前的视频到语音合成技术更高。<details>
<summary>Abstract</summary>
Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations, we further develop a diffusion-based video-to-speech synthesis model, so called DiffV2S, conditioned on those speaker embeddings and the visual representation extracted from the input video. The proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.
</details>
<details>
<summary>摘要</summary>
近期研究已经展示了视频到语音合成的出色成果，即通过视觉输入重建语音。然而，之前的研究往往因缺乏足够的指导，使模型很难准确地推理出正确的内容和合适的声音。为解决这问题，他们采用了外部的 speaker embedding 作为引导，从参考听力信息中提取出speaker embedding。然而，在推理时不一定能获取相应的音频信息，特别是在推理时。在这篇论文中，我们提出了一种新的视频引导的 speaker embedding EXTRACTOR，使用自我超vised pre-trained模型和提示调整技术。通过这种方法，我们可以从输入视频信息中提取出丰富的 speaker embedding信息，而不需要外部的音频信息。使用提取的视频引导 speaker embedding表示，我们进一步开发了一种扩散基于的视频到语音合成模型，称为DiffV2S。DiffV2S 模型通过 Conditioned on those speaker embeddings and the visual representation extracted from the input video, the proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-generative-modelling-for-autonomous-robots"><a href="#Hierarchical-generative-modelling-for-autonomous-robots" class="headerlink" title="Hierarchical generative modelling for autonomous robots"></a>Hierarchical generative modelling for autonomous robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07775">http://arxiv.org/abs/2308.07775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yuan, Noor Sajid, Karl Friston, Zhibin Li</li>
<li>for:  investigate the fundamental aspect of motor control in autonomous robotic operations, and develop a hierarchical generative model to achieve versatile sensorimotor control.</li>
<li>methods:  use hierarchical generative modeling, multi-level planning, and numerical&#x2F;physical simulation to achieve autonomous completion of complex tasks.</li>
<li>results:  demonstrate the effectiveness of using human-inspired motor control algorithms, and show the ability of a humanoid robot to retrieve, transport, open, walk through a door, approach, and kick a football, while showing robust performance in presence of body damage and ground irregularities.Here’s the summary in Traditional Chinese:</li>
<li>for: 研究自主机械操作中的动作控制基础，并开发一个嵌入式生成模型以实现多标的感知动作控制。</li>
<li>methods: 使用嵌入式生成模型、多层规划和数据&#x2F;物理模拟来完成自主任务。</li>
<li>results: 显示人类动作控制算法的效果，并展示一个人型机器人能够自主完成复杂任务，例如抓取、运输、开启、通过门、与足球进行踢动作，并在身体损坏和地面不平的情况下保持Robust性。<details>
<summary>Abstract</summary>
Humans can produce complex whole-body motions when interacting with their surroundings, by planning, executing and combining individual limb movements. We investigated this fundamental aspect of motor control in the setting of autonomous robotic operations. We approach this problem by hierarchical generative modelling equipped with multi-level planning-for autonomous task completion-that mimics the deep temporal architecture of human motor control. Here, temporal depth refers to the nested time scales at which successive levels of a forward or generative model unfold, for example, delivering an object requires a global plan to contextualise the fast coordination of multiple local movements of limbs. This separation of temporal scales also motivates robotics and control. Specifically, to achieve versatile sensorimotor control, it is advantageous to hierarchically structure the planning and low-level motor control of individual limbs. We use numerical and physical simulation to conduct experiments and to establish the efficacy of this formulation. Using a hierarchical generative model, we show how a humanoid robot can autonomously complete a complex task that necessitates a holistic use of locomotion, manipulation, and grasping. Specifically, we demonstrate the ability of a humanoid robot that can retrieve and transport a box, open and walk through a door to reach the destination, approach and kick a football, while showing robust performance in presence of body damage and ground irregularities. Our findings demonstrated the effectiveness of using human-inspired motor control algorithms, and our method provides a viable hierarchical architecture for the autonomous completion of challenging goal-directed tasks.
</details>
<details>
<summary>摘要</summary>
人类可以生成复杂全身运动when interacting with他们的环境，通过规划、执行和组合个体肢体运动。我们在自主 робоック操作中调查了这一基本问题。我们采用层次生成模型，带有多级规划，以模仿人类motor控制的深度 temporal architecture。在这里， temporal depth 指的是成功层次模型 unfold 的不同时间尺度，例如，为了交付物品，需要一个全局规划，以Contextualize 多个快速协调的肢体运动。这种层次分离也驱动了机器人和控制。具体来说，以实现多样化的感知动作控制，是通过层次结构的规划和低级动作控制来实现的。我们通过数字和物理模拟进行实验，并证明了这种形式的有效性。使用层次生成模型，我们展示了一个人型机器人可以自主完成一个复杂任务，需要整体的运动、抓取、搬运和踢球等多种功能。Specifically，我们示出了一个人型机器人可以拾取和运输一个箱子，通过门打开和走进去到目的地，并且在踢球时表现出了Robust performance 的特点。我们的发现表明了使用人类 inspirited motor control算法的有效性，而我们的方法提供了一种可靠的层次建筑，以便自主完成具有挑战性的目标指导任务。
</details></li>
</ul>
<hr>
<h2 id="A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection"><a href="#A-Graph-Encoder-Decoder-Network-for-Unsupervised-Anomaly-Detection" class="headerlink" title="A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection"></a>A Graph Encoder-Decoder Network for Unsupervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07774">http://arxiv.org/abs/2308.07774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Mesgaran, A. Ben Hamza</li>
<li>for: 检测图граFC中异常节点</li>
<li>methods: 使用自适应图形编码器-解码器模型，学习异常分数函数，将节点排序根据其异常程度。编码阶段使用新型的LCPool方法，通过本地化约束的线性编码来生成团 assignment matrix，解决了传统方法中学习参数的问题，提高了效率和可解释性。解码阶段使用LCUnpool方法重construct原始图гра的结构和节点特征。</li>
<li>results: 在六个基准数据集上进行了实验评估，结果表明该方法在比较状态前的异常检测方法中表现出色，超过了现有方法。<details>
<summary>Abstract</summary>
A key component of many graph neural networks (GNNs) is the pooling operation, which seeks to reduce the size of a graph while preserving important structural information. However, most existing graph pooling strategies rely on an assignment matrix obtained by employing a GNN layer, which is characterized by trainable parameters, often leading to significant computational complexity and a lack of interpretability in the pooling process. In this paper, we propose an unsupervised graph encoder-decoder model to detect abnormal nodes from graphs by learning an anomaly scoring function to rank nodes based on their degree of abnormality. In the encoding stage, we design a novel pooling mechanism, named LCPool, which leverages locality-constrained linear coding for feature encoding to find a cluster assignment matrix by solving a least-squares optimization problem with a locality regularization term. By enforcing locality constraints during the coding process, LCPool is designed to be free from learnable parameters, capable of efficiently handling large graphs, and can effectively generate a coarser graph representation while retaining the most significant structural characteristics of the graph. In the decoding stage, we propose an unpooling operation, called LCUnpool, to reconstruct both the structure and nodal features of the original graph. We conduct empirical evaluations of our method on six benchmark datasets using several evaluation metrics, and the results demonstrate its superiority over state-of-the-art anomaly detection approaches.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 的一个重要 компонент是聚合操作，它想要将 graphs 的大小减少，保留重要的结构信息。然而，大多数现有的 graph 聚合策略依赖一个由 GNN 层所得到的对称矩阵，这个矩阵通常具有可读的参数，往往导致计算复杂和模型解释不足。在这篇文章中，我们提出了一个无supervised graph encoder-decoder模型，用于侦测 graphs 中的异常点。在编码阶段，我们设计了一个名为 LCPool 的新的聚合机制，通过本地性受限的线性编码来找到一个对称矩阵，并通过解决一个最小二乘问题来找到一个最佳的对称矩阵。由于在编码过程中强制 enforcing 本地性限制，LCPool 可以免除学习参数，可以高效地处理大型 graphs，并且可以将原始图的主要结构特征传递到更粗糙的表示中。在解码阶段，我们提出了一个名为 LCUnpool 的解码操作，用于重建原始图的结构和节点特征。我们在六个 benchmark dataset 上进行了实验评估，结果显示我们的方法在与现有的侦测方法比较之下表现出色。
</details></li>
</ul>
<hr>
<h2 id="MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization"><a href="#MOLE-MOdular-Learning-FramEwork-via-Mutual-Information-Maximization" class="headerlink" title="MOLE: MOdular Learning FramEwork via Mutual Information Maximization"></a>MOLE: MOdular Learning FramEwork via Mutual Information Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07772">http://arxiv.org/abs/2308.07772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianchao Li, Yulong Pei</li>
<li>for: 这个论文是为了介绍一种异步和本地学习框架，即Module Learning Framework（MOLE）。</li>
<li>methods: 这个框架将神经网络归一化为层，通过矩阵乘法来定义训练目标，并逐渐训练每个模块以达到最大化矩阵乘法的目标。</li>
<li>results: 实验表明，MOLE可以在向量-, 网格-和图形数据上进行高效的训练，并且可以解决图形数据上的节点级和图级任务。因此，MOLE已经在不同类型的数据上得到了实验证明。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper is to introduce an asynchronous and local learning framework for neural networks, named Modular Learning Framework (MOLE). This framework modularizes neural networks by layers, defines the training objective via mutual information for each module, and sequentially trains each module by mutual information maximization. MOLE makes the training become local optimization with gradient-isolated across modules, and this scheme is more biologically plausible than BP. We run experiments on vector-, grid- and graph-type data. In particular, this framework is capable of solving both graph- and node-level tasks for graph-type data. Therefore, MOLE has been experimentally proven to be universally applicable to different types of data.
</details>
<details>
<summary>摘要</summary>
这篇论文旨在介绍一种异步和本地学习框架 для神经网络，名为模块学习框架（MOLE）。这个框架将神经网络归类为层，通过每个模块的互信息定义训练目标，并逐渐训练每个模块以互信息最大化。MOLE使得训练变成了本地优化，梯度在模块之间隔离，这种方式更加生物学可能性高于BP。我们在矢量-, 网格-和图形数据上进行了实验，并证明MOLE可以解决图形数据中的图级和节点级任务。因此，MOLE在不同类型的数据上都有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients"><a href="#NeFL-Nested-Federated-Learning-for-Heterogeneous-Clients" class="headerlink" title="NeFL: Nested Federated Learning for Heterogeneous Clients"></a>NeFL: Nested Federated Learning for Heterogeneous Clients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07761">http://arxiv.org/abs/2308.07761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Honggu Kang, Seohyeon Cha, Jinwoo Shin, Jongmyeong Lee, Joonhyuk Kang</li>
<li>for: 这个研究旨在解决联合学习（Federated Learning，FL）训练过程中缓态或无法进行训练的客户端（即慢车）对整个训练时间的影响，以及实现训练模型的更好可扩展性。</li>
<li>methods: 本研究提出了一个称为嵌套联合学习（NeFL）的架构，它可以将模型分解为多个子模型，并使用深度和宽度的扩展来实现。NeFL还使用了解析方程（ODEs）来调整步长大小，以便在不同的客户端上进行训练。</li>
<li>results: 透过一系列实验，本研究表明NeFL可以实现训练模型的更好可扩展性，特别是在最差的子模型（例如CIFAR-10上的8.33提升）。此外，NeFL与最近的FL研究相互协调。<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be trained with a larger amount of data. Through a series of experiments, we demonstrate that NeFL leads to significant gains, especially for the worst-case submodel (e.g., 8.33 improvement on CIFAR-10). Furthermore, we demonstrate NeFL aligns with recent studies in FL.
</details>
<details>
<summary>摘要</summary>
Federated learning (FL) 是一种有前途的方法，可以保持隐私性在分布式学习中。然而，在 FL 训练管线中，慢速或无法进行训练的客户端（即废物）会导致总训练时间增加和性能下降。系统多样性，包括不同的计算和网络带宽，已经被解决以减少废物的影响。先前的研究把模型分成了两部分来解决问题，但是这些方法具有较少的度量自由度，对于模型架构而言。我们提出了嵌套 federated learning（NeFL），一个通用的框架，可以快速地将模型分成子模型，使用深度和宽度的扩展。NeFL 通过将模型视为解决 ordinary differential equations（ODEs）的解，并使用适应步长来实现。为了解决不同架构下训练多个子模型时出现的不一致，我们将一些参数分离。NeFL 允许资源有限的客户端能够有效地加入 FL 管线，并让模型在更多数据上进行训练。通过一系列实验，我们展示了 NeFL 对 CIFAR-10 等数据集的进步，特别是最差的子模型（例如，8.33 倍进步）。此外，我们还证明 NeFL 与最近的 FL 研究相关。
</details></li>
</ul>
<hr>
<h2 id="Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification"><a href="#Forward-Backward-Reasoning-in-Large-Language-Models-for-Verification" class="headerlink" title="Forward-Backward Reasoning in Large Language Models for Verification"></a>Forward-Backward Reasoning in Large Language Models for Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07758">http://arxiv.org/abs/2308.07758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, James T. Kwok</li>
<li>for: 提高理解任务中的推理能力</li>
<li>methods: 使用反向推理和前向推理的组合方法</li>
<li>results: 实验结果表明，FOBAR方法在多个数据集和三种LLM中表现出状元水平的推理能力<details>
<summary>Abstract</summary>
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., "\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}" Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.
</details>
<details>
<summary>摘要</summary>
链式思维（CoT）提示法在多种理解任务中表现出色。自康凝 \citep{wang2023selfconsistency}提出了采样多个理解链，以便通过不同的答案而产生多个可能性。在这篇论文中，我们提出了一种使用反向思维的新方法，用于验证候选答案。我们将问题中的一个token用${\bf x}$进行遮盖，然后询问LLM predict该遮盖的token，当提供一个简单的模板，即 "\textit{\textbf{如果我们知道上面的问题的答案是 \{一个候选答案\}, то值Unknown变量${\bf x}$是什么？}"。 intuitionally，LLM可以成功预测遮盖的token，如果提供的候选答案是正确的。我们还提出了FOBAR，用于将前向和反向思维相结合，以估算候选答案的概率。我们在六个数据集和三个LLM上进行了广泛的实验，实验结果表明，FOBAR在多种理解 bencmarks 上达到了领先的性能。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks"><a href="#Exploiting-Sparsity-in-Automotive-Radar-Object-Detection-Networks" class="headerlink" title="Exploiting Sparsity in Automotive Radar Object Detection Networks"></a>Exploiting Sparsity in Automotive Radar Object Detection Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07748">http://arxiv.org/abs/2308.07748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marius Lippke, Maurice Quach, Sascha Braun, Daniel Köhler, Michael Ulrich, Bastian Bischoff, Wei Yap Tan</li>
<li>for: 这 paper 的目的是提出一种基于 sparse convolutional neural network 的对象检测方法，用于解决自动驾驶系统中的环境感知问题。</li>
<li>methods: 该 paper 使用了 grid-based detection 和 sparse backbone 架构，并提出了 sparse kernel point pillars (SKPP) 和 dual voxel point convolutions (DVPC) 等技术来解决 радиар特有的挑战。</li>
<li>results: 该 paper 在 nuScenes 数据集上进行了评测，并证明了 SKPP-DPVCN 架构可以比基线和前一个状态的对象检测方法提高 Car AP4.0 的性能，并降低了平均缩放错误 (ASE) 值。<details>
<summary>Abstract</summary>
Having precise perception of the environment is crucial for ensuring the secure and reliable functioning of autonomous driving systems. Radar object detection networks are one fundamental part of such systems. CNN-based object detectors showed good performance in this context, but they require large compute resources. This paper investigates sparse convolutional object detection networks, which combine powerful grid-based detection with low compute resources. We investigate radar specific challenges and propose sparse kernel point pillars (SKPP) and dual voxel point convolutions (DVPC) as remedies for the grid rendering and sparse backbone architectures. We evaluate our SKPP-DPVCN architecture on nuScenes, which outperforms the baseline by 5.89% and the previous state of the art by 4.19% in Car AP4.0. Moreover, SKPP-DPVCN reduces the average scale error (ASE) by 21.41% over the baseline.
</details>
<details>
<summary>摘要</summary>
“精准感知环境是自动驾驶系统的关键，以确保其安全和可靠运行。雷达对象检测网络是这种系统的基本组件之一。使用CNN的对象检测器显示了良好的性能，但它们需要大量的计算资源。这篇论文研究了稀疏 convolutional 对象检测网络，它们将强大的格子基础与低计算资源相结合。我们研究了雷达特有挑战，并提出了 sparse kernel point pillars（SKPP）和 dual voxel point convolutions（DVPC）来解决grid rendering和稀疏脊梁架构的问题。我们评估了我们的 SKPP-DPVCN 架构在 nuScenes 上，其与基准值相比提高了4.19%，并且与前一个状态的艺术提高了5.89%。此外，SKPP-DPVCN 还下降了平均扩散误差（ASE）的21.41%。”
</details></li>
</ul>
<hr>
<h2 id="Real-Robot-Challenge-2022-Learning-Dexterous-Manipulation-from-Offline-Data-in-the-Real-World"><a href="#Real-Robot-Challenge-2022-Learning-Dexterous-Manipulation-from-Offline-Data-in-the-Real-World" class="headerlink" title="Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World"></a>Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07741">http://arxiv.org/abs/2308.07741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nico Gürtler, Felix Widmaier, Cansu Sancaktar, Sebastian Blaes, Pavel Kolev, Stefan Bauer, Manuel Wüthrich, Markus Wulfmeier, Martin Riedmiller, Arthur Allshire, Qiang Wang, Robert McCarthy, Hangyeol Kim, Jongchan Baek Pohang, Wookyong Kwon, Shanliang Qian, Yasunori Toshimitsu, Mike Yan Michelis, Amirhossein Kazemipour, Arman Raayatsanati, Hehui Zheng, Barnabasa Gavin Cangan, Bernhard Schölkopf, Georg Martius</li>
<li>for: 本研究的目的是bridge reinforcement learning（RL）和机器人共同体，让参与者通过实际操作real robot来验证RL算法的性能。</li>
<li>methods: 本研究使用了现有的real robot dataset，并提供了丰富的软件文档和初始化阶段，使参与者可以轻松地在real robot上进行学习和评估。</li>
<li>results: 研究发现，winning teams使用的方法可以在real robot上实现高效的dexterous manipulation任务，并且比预期的state-of-the-art offline RL算法更高效。<details>
<summary>Abstract</summary>
Experimentation on real robots is demanding in terms of time and costs. For this reason, a large part of the reinforcement learning (RL) community uses simulators to develop and benchmark algorithms. However, insights gained in simulation do not necessarily translate to real robots, in particular for tasks involving complex interactions with the environment. The Real Robot Challenge 2022 therefore served as a bridge between the RL and robotics communities by allowing participants to experiment remotely with a real robot - as easily as in simulation.   In the last years, offline reinforcement learning has matured into a promising paradigm for learning from pre-collected datasets, alleviating the reliance on expensive online interactions. We therefore asked the participants to learn two dexterous manipulation tasks involving pushing, grasping, and in-hand orientation from provided real-robot datasets. An extensive software documentation and an initial stage based on a simulation of the real set-up made the competition particularly accessible. By giving each team plenty of access budget to evaluate their offline-learned policies on a cluster of seven identical real TriFinger platforms, we organized an exciting competition for machine learners and roboticists alike.   In this work we state the rules of the competition, present the methods used by the winning teams and compare their results with a benchmark of state-of-the-art offline RL algorithms on the challenge datasets.
</details>
<details>
<summary>摘要</summary>
实验在真正机器人上具有时间和成本的限制，因此许多学习强化（RL）社区使用模拟器来开发和比较算法。然而，在实际环境中的交互性较复杂时，在模拟器上获得的 Insight 可能不准确。为了 bridging 这两个社区，我们在2022年的真机器人挑战中让参与者通过远程控制真机器人来进行实验，与在模拟器上进行实验一样简单。在过去几年中，离线学习强化学习（offline RL）已经成熟为一种有前途的学习方法，可以避免在临时便捷的在线交互中花费高昂的成本。因此，我们要求参与者通过学习提供的真机器人数据集来完成两项灵活的机械操作任务，包括推动、抓取和手中 orienting。为了使参与者更加方便地参与到竞赛中，我们提供了广泛的软件文档和一个基于真实设置的初始阶段。为了让每个团队有足够的访问预算来评估他们在一群七个相同的真机器人平台上的离线学习策略，我们组织了一场吸引了机器人学家和学习机器人之间的精彩竞赛。在这篇文章中，我们介绍了竞赛规则，表明赢家们使用的方法，并与当前的离线RL算法在挑战数据集上的比较。
</details></li>
</ul>
<hr>
<h2 id="Domain-Aware-Fine-Tuning-Enhancing-Neural-Network-Adaptability"><a href="#Domain-Aware-Fine-Tuning-Enhancing-Neural-Network-Adaptability" class="headerlink" title="Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability"></a>Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07728">http://arxiv.org/abs/2308.07728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee</li>
<li>For: 本研究旨在提出一种新的方法，以便在不同目标领域进行精度的适应和性能优化。* Methods: 本研究使用了域名映射和精度评估来缓解特征扭曲问题，并通过Linear Probing和精度调整来优化头层。* Results: 对比基eline方法，本研究的方法在域外数据上显示出较高的性能，并且可以减少特征扭曲。<details>
<summary>Abstract</summary>
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor. By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets. Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion.
</details>
<details>
<summary>摘要</summary>
“已成为各领域的普遍采用方法，微型网络组件的精致调整已成为一个广泛应用的方法。然而，这可能会导致原有具备强化泛化能力的预训网络组件的扭曲。缓和预训网络组件的扭曲是非常重要的。 recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. However, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integration of linear probing and fine-tuning to optimize the head layer with gradual adaptation of the feature extractor. By leveraging batch normalization layers and integrating linear probing and fine-tuning, our DAFT significantly mitigates feature distortion and achieves improved model performance on both in-distribution and out-of-distribution datasets. Extensive experiments demonstrate that our method outperforms other baseline methods, demonstrating its effectiveness in not only improving performance but also mitigating feature distortion.”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Fast-Machine-Unlearning-Without-Retraining-Through-Selective-Synaptic-Dampening"><a href="#Fast-Machine-Unlearning-Without-Retraining-Through-Selective-Synaptic-Dampening" class="headerlink" title="Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening"></a>Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07707">http://arxiv.org/abs/2308.07707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/if-loops/selective-synaptic-dampening">https://github.com/if-loops/selective-synaptic-dampening</a></li>
<li>paper_authors: Jack Foster, Stefan Schoepf, Alexandra Brintrup</li>
<li>for: 本研究旨在解决机器学习模型忘记Specific information的挑战，以遵守数据隐私法规和 removing harmful, manipulated, or outdated information。</li>
<li>methods: 本研究提出了一种名为Selective Synaptic Dampening（SSD）的两步，Post hoc，无需重新训练的方法，它快速、高效，不需要长期存储训练数据。</li>
<li>results: 对比 existed unlearning 方法，SSD 的性能与重新训练方法相当，这表明了无需重新训练的后置式忘记方法的可行性。<details>
<summary>Abstract</summary>
Machine unlearning, the ability for a machine learning model to forget, is becoming increasingly important to comply with data privacy regulations, as well as to remove harmful, manipulated, or outdated information. The key challenge lies in forgetting specific information while protecting model performance on the remaining data. While current state-of-the-art methods perform well, they typically require some level of retraining over the retained data, in order to protect or restore model performance. This adds computational overhead and mandates that the training data remain available and accessible, which may not be feasible. In contrast, other methods employ a retrain-free paradigm, however, these approaches are prohibitively computationally expensive and do not perform on par with their retrain-based counterparts. We present Selective Synaptic Dampening (SSD), a novel two-step, post hoc, retrain-free approach to machine unlearning which is fast, performant, and does not require long-term storage of the training data. First, SSD uses the Fisher information matrix of the training and forgetting data to select parameters that are disproportionately important to the forget set. Second, SSD induces forgetting by dampening these parameters proportional to their relative importance to the forget set with respect to the wider training data. We evaluate our method against several existing unlearning methods in a range of experiments using ResNet18 and Vision Transformer. Results show that the performance of SSD is competitive with retrain-based post hoc methods, demonstrating the viability of retrain-free post hoc unlearning approaches.
</details>
<details>
<summary>摘要</summary>
机器学习模型的忘记能力，也就是机器学习模型的“忘记”，在符合数据隐私法规以及移除有害、操纵或过时信息方面变得越来越重要。然而，现有的状态 искусственный智能技术通常需要一定的重新训练，以保护或恢复模型在保留的数据上的性能。这会增加计算开销，并且需要训练数据保持可用和可访问，这可能不是可行的。相比之下，其他方法采用一种不需要重新训练的方法，但这些方法的计算成本过高，并且性能不如重新训练的方法。我们提出了一种新的两步、Post Hoc、无需重新训练的机器学习忘记方法：选择性神经元减弱（SSD）。首先，SSD使用训练和忘记数据的 Fisher 信息矩阵来选择对忘记集数据的重要参数。然后，SSD 通过对这些参数进行减弱，使其与忘记集数据相对更重要的参数相比，来实现忘记。我们在使用 ResNet18 和 Vision Transformer 进行了一系列实验，结果表明 SSD 的性能与重新训练后的Post Hoc方法相当竞争，这说明了无需重新训练的忘记方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models"><a href="#Exploring-Transfer-Learning-in-Medical-Image-Segmentation-using-Vision-Language-Models" class="headerlink" title="Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models"></a>Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07706">http://arxiv.org/abs/2308.07706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanchan Poudel, Manish Dhakal, Prasiddha Bhandari, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal</li>
<li>for: 本研究旨在提高医疗领域图像分割 task 的效果，通过文本引导来增强视觉特征。</li>
<li>methods: 本研究使用多Modal vision-language模型，包括图像描述和图像特征，以捕捉图像描述中的semantic信息，提高医疗领域图像分割 task 的效果。</li>
<li>results: 研究发现，existings vision language模型在多个 dataset 上的传输性不高，需要进行手动调整或 fine-tuning 以适应医疗领域。而通过生成不同的图像描述来训练模型，可以提高模型的性能。<details>
<summary>Abstract</summary>
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.   To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations in model performance based on the generated prompts.   Our findings highlight the distribution shift between the open-domain images and the medical domain and show that the segmentation models trained on open-domain images are not directly transferrable to the medical field. But their performance can be increased by finetuning them in the medical datasets. We report the zero-shot and finetuned segmentation performance of 4 Vision Language Models (VLMs) on 11 medical datasets using 9 types of prompts derived from 14 attributes.
</details>
<details>
<summary>摘要</summary>
医疗图像分割是医疗领域内的重要应用之一。虽然现有的分割模型已经证明有效，但是将文本指导integrated到图像特征中以提高分割性能仍然是一个有限的领域。现有的分割模型主要是在开放领域图像上训练，这引发了对其直接适用性在医疗领域的担忧。为了解决这些挑战，我们提议使用多Modal vision-language模型来捕捉图像描述和图像中的semantic信息，以便分割多种医疗图像。本研究对多个数据集进行了广泛的评估，以评估现有的视力语言模型在医疗领域的转移性。此外，我们还引入了未经见过的图像描述，并证明了基于生成的提示的模型性能的很大变化。我们的发现表明了开放领域图像和医疗领域之间的分布差异，并证明了训练在开放领域图像上的分割模型不能直接应用于医疗领域。但是，通过finetuning，可以提高这些模型在医疗数据集上的性能。我们对4种视力语言模型在11个医疗数据集上进行了零容量和finetuning的分割性能测试，使用9种基于14个特征的提示。
</details></li>
</ul>
<hr>
<h2 id="Parametric-entropy-based-Cluster-Centriod-Initialization-for-k-means-clustering-of-various-Image-datasets"><a href="#Parametric-entropy-based-Cluster-Centriod-Initialization-for-k-means-clustering-of-various-Image-datasets" class="headerlink" title="Parametric entropy based Cluster Centriod Initialization for k-means clustering of various Image datasets"></a>Parametric entropy based Cluster Centriod Initialization for k-means clustering of various Image datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07705">http://arxiv.org/abs/2308.07705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faheem Hussayn, Shahid M Shah</li>
<li>for: 这篇论文的目的是提出一种基于参数 entropy 的 k-means 初始化方法，以提高 k-means 算法在图像数据上的表现。</li>
<li>methods: 该论文使用了多种参数 entropy 来初始化 k-means 算法的中心点，并对不同的图像 dataset 进行了测试。</li>
<li>results: 研究发现，不同的 dataset 使用不同的参数 entropy 可以提供更好的结果，而且提议的方法可以提高 k-means 算法在图像数据上的表现。 例如，在 Satellite、Toys、Fruits、Cars 等 dataset 上，使用 Taneja entropy、Kapur entropy、Aczel Daroczy entropy 和 Sharma Mittal entropy 等参数 entropy 可以提供更好的结果。<details>
<summary>Abstract</summary>
One of the most employed yet simple algorithm for cluster analysis is the k-means algorithm. k-means has successfully witnessed its use in artificial intelligence, market segmentation, fraud detection, data mining, psychology, etc., only to name a few. The k-means algorithm, however, does not always yield the best quality results. Its performance heavily depends upon the number of clusters supplied and the proper initialization of the cluster centroids or seeds. In this paper, we conduct an analysis of the performance of k-means on image data by employing parametric entropies in an entropy based centroid initialization method and propose the best fitting entropy measures for general image datasets. We use several entropies like Taneja entropy, Kapur entropy, Aczel Daroczy entropy, Sharma Mittal entropy. We observe that for different datasets, different entropies provide better results than the conventional methods. We have applied our proposed algorithm on these datasets: Satellite, Toys, Fruits, Cars, Brain MRI, Covid X-Ray.
</details>
<details>
<summary>摘要</summary>
一种非常常用但简单的聚类分析算法是k-means算法。k-means算法在人工智能、市场 segmentation、诈骗探测、数据挖掘、心理学等领域都有广泛的应用，只是名些。然而，k-means算法并不总是能够提供最佳的结果。其性能很大程度上取决于提供的聚类数量和聚类中心点或种子的初始化。在这篇论文中，我们通过使用参数 entropy 来初始化聚类中心点，并提出了适用于普通图像数据集的最佳 entropy 度量。我们使用了多种 entropy，如Taneja entropy、Kapur entropy、Aczel Daroczy entropy和Sharma Mittal entropy。我们发现，不同的数据集，不同的 entropy 度量会提供更好的结果。我们在这些数据集上应用了我们的提议的算法：卫星、玩具、水果、汽车、脑Magnetic Resonance Imaging（MRI）、Covid X-Ray。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Network-Initialization-for-Medical-AI-Models-Using-Large-Scale-Unlabeled-Natural-Images"><a href="#Enhancing-Network-Initialization-for-Medical-AI-Models-Using-Large-Scale-Unlabeled-Natural-Images" class="headerlink" title="Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images"></a>Enhancing Network Initialization for Medical AI Models Using Large-Scale, Unlabeled Natural Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07688">http://arxiv.org/abs/2308.07688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung</li>
<li>for: 这个研究的目的是探索可以使用非医学影像进行自主学习预训（SSL），以提高医学影像分析中的人工智能（AI）精度。</li>
<li>methods: 我们使用了一个视觉转化器，并将其初始化为（i）SSL预训自然影像（DINOv2）、（ii）SL预训自然影像（ImageNet dataset）和（iii）SL预训颈部X线成像（MIMIC-CXR dataset）。</li>
<li>results: 我们在6个大型全球颈部X线成像数据集上进行了过80万张颈部X线成像的测试，并识别了20多种不同的医学影像找到结果。我们的SSL预训策略不仅在所有数据集上比ImageNet预训（P&lt;0.001）表现更好，甚至在某些情况下还超过了SL在MIMIC-CXR数据集上的表现。<details>
<summary>Abstract</summary>
Pre-training datasets, like ImageNet, have become the gold standard in medical image analysis. However, the emergence of self-supervised learning (SSL), which leverages unlabeled data to learn robust features, presents an opportunity to bypass the intensive labeling process. In this study, we explored if SSL for pre-training on non-medical images can be applied to chest radiographs and how it compares to supervised pre-training on non-medical images and on medical images. We utilized a vision transformer and initialized its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on chest radiographs from the MIMIC-CXR database. We tested our approach on over 800,000 chest radiographs from six large global datasets, diagnosing more than 20 different imaging findings. Our SSL pre-training on curated images not only outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest that selecting the right pre-training strategy, especially with SSL, can be pivotal for improving artificial intelligence (AI)'s diagnostic accuracy in medical imaging. By demonstrating the promise of SSL in chest radiograph analysis, we underline a transformative shift towards more efficient and accurate AI models in medical imaging.
</details>
<details>
<summary>摘要</summary>
预训 datasets，如 ImageNet，已成为医学影像分析的标准。然而，自动学习（SSL）技术的出现，可以使用无标签数据学习强大的特征，可能以替代复杂的标签过程。在这项研究中，我们研究了是否可以将SSL预训短图用于骨胸影像，并与其他两种预训方法进行比较。我们使用了一种视觉转换器，并将其参数初始化为（i）SSL预训自然图像（DINOv2），（ii）SL预训自然图像（ImageNet dataset），和（iii）SL预训骨胸影像（MIMIC-CXR数据库）。我们对超过800,000个骨胸影像进行测试，识别了 более20种不同的医学影像发现。我们的SSL预训策略不仅在所有数据集上超过ImageNet预训策略（P<0.001），而且在某些情况下， même exceeded SL在MIMIC-CXR数据库上。我们的发现表明，选择合适的预训策略，特别是使用SSL，可以对医学影像识别精度进行改进。通过证明SSL在骨胸影像分析中的推荐，我们强调了医学影像识别模型的更有效和精度的转型。
</details></li>
</ul>
<hr>
<h2 id="DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models"><a href="#DiffGuard-Semantic-Mismatch-Guided-Out-of-Distribution-Detection-using-Pre-trained-Diffusion-Models" class="headerlink" title="DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models"></a>DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07687">http://arxiv.org/abs/2308.07687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cure-lab/diffguard">https://github.com/cure-lab/diffguard</a></li>
<li>paper_authors: Ruiyuan Gao, Chenchen Zhao, Lanqing Hong, Qiang Xu</li>
<li>for: 本研究的目的是提出一种基于扩展模型的Semantic Out-of-Distribution（OOD）检测方法，以提高图像分类器的OOD检测性能。</li>
<li>methods: 该方法使用了 conditional Generative Adversarial Network（cGAN）来增大图像空间中的semantic mismatch，并且使用pre-trained diffusion models来实现Semantic mismatch-guided OOD detection。</li>
<li>results: 实验结果表明，DiffGuard可以在Cifar-10和ImageNet上达到州-of-the-art的OOD检测性能，并且可以与现有的OOD检测技术相结合以获得更高的检测性能。<details>
<summary>Abstract</summary>
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and the original input image. We also present several test-time techniques to further strengthen such differences. Experimental results show that DiffGuard is effective on both Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily combined with existing OOD detection techniques to achieve state-of-the-art OOD detection results.
</details>
<details>
<summary>摘要</summary>
（简化中文）给定一个分类器，则它的Out-of-Distribution（OOD）样本的内在特性是与所有法定类型的内容不同，即semantic mismatch。有一项最近的工作直接应用它到OOD检测中，使用conditional Generative Adversarial Network（cGAN）来扩大图像空间中的semantic mismatch。尽管在小 dataset上达到了惊人的OOD检测性能，但是在ImageNet scale dataset上不可能因为cGAN的训练是不可能的。因为diffusion模型比cGAN更容易训练，在这项工作中，我们直接使用预训练的diffusion模型来实现semantic mismatch-guided OOD检测，名为DiffGuard。具体来说，给定一个OOD输入图像和分类器预测的标签，我们尝试通过在这些条件下重建OOD图像，并与原始输入图像进行比较，以扩大semantic difference。我们还提供了多种测试时技术来进一步强化这种差异。实验结果表明，DiffGuard是效果好的，在Cifar-10和ImageNet中的困难情况下都能够达到state-of-the-art的OOD检测结果。
</details></li>
</ul>
<hr>
<h2 id="Portfolio-Selection-via-Topological-Data-Analysis"><a href="#Portfolio-Selection-via-Topological-Data-Analysis" class="headerlink" title="Portfolio Selection via Topological Data Analysis"></a>Portfolio Selection via Topological Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07944">http://arxiv.org/abs/2308.07944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Sokerin, Kristian Kuznetsov, Elizaveta Makhneva, Alexey Zaytsev</li>
<li>for: 投资决策中的资产组合管理是一项重要的任务，但传统方法往往无法实现合理的性能。</li>
<li>methods: 本文提出了一种两阶段的投资资产组合建立方法，首先生成时间序列表示，然后进行划分。该方法利用了 topological data analysis（TDA）特征来生成表示，从而揭示时间序列数据中的Topological结构。</li>
<li>results: 实验结果显示，我们提出的方法在不同时间帧下具有superior性能，与其他方法相比，这种性能的稳定性和可靠性得到了证明。这些结果表明TDA可以作为一种强大的工具来选择资产组合。<details>
<summary>Abstract</summary>
Portfolio management is an essential part of investment decision-making. However, traditional methods often fail to deliver reasonable performance. This problem stems from the inability of these methods to account for the unique characteristics of multivariate time series data from stock markets. We present a two-stage method for constructing an investment portfolio of common stocks. The method involves the generation of time series representations followed by their subsequent clustering. Our approach utilizes features based on Topological Data Analysis (TDA) for the generation of representations, allowing us to elucidate the topological structure within the data. Experimental results show that our proposed system outperforms other methods. This superior performance is consistent over different time frames, suggesting the viability of TDA as a powerful tool for portfolio selection.
</details>
<details>
<summary>摘要</summary>
资产管理是投资决策的重要组成部分，但传统方法通常无法提供合理的性能。这个问题源于这些方法无法考虑股票市场多元时间序列数据的特殊特征。我们提出了一种两阶段方法，用于建立公司股票投资组合。该方法包括生成时间序列表示，然后进行归一化。我们的方法利用基于拓扑数据分析（TDA）的特征来生成表示，从而揭示时间序列数据中的拓扑结构。实验结果显示，我们的提议系统在不同时间框架下具有优秀的性能，这表明TDA可以成为资产选择中的强大工具。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Post-Training-Quantization-Challenging-the-Status-Quo"><a href="#Gradient-Based-Post-Training-Quantization-Challenging-the-Status-Quo" class="headerlink" title="Gradient-Based Post-Training Quantization: Challenging the Status Quo"></a>Gradient-Based Post-Training Quantization: Challenging the Status Quo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07662">http://arxiv.org/abs/2308.07662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edouard Yvinec, Arnaud Dapogny, Kevin Bailly</li>
<li>for: 这篇论文的目的是提出一种新的量化方法，以提高量化深度神经网络的效率和可扩展性。</li>
<li>methods: 这篇论文使用了Gradient-based post-training quantization（GPTQ）方法，并且挑战了常用的GPTQ方法设计。具体来说，这篇论文提出了一些最佳实践方法，例如调整参数选择、增强特征变数、选择参考集等，以提高量化的效率和可扩展性。</li>
<li>results: 这篇论文的实验结果显示，这些最佳实践方法可以实现 significiant 的性能改进（例如，在ViT模型上，使用4位量化可以提高6.819点的表现），这些结果显示了这篇论文的量化方法的可行性和有效性。<details>
<summary>Abstract</summary>
Quantization has become a crucial step for the efficient deployment of deep neural networks, where floating point operations are converted to simpler fixed point operations. In its most naive form, it simply consists in a combination of scaling and rounding transformations, leading to either a limited compression rate or a significant accuracy drop. Recently, Gradient-based post-training quantization (GPTQ) methods appears to be constitute a suitable trade-off between such simple methods and more powerful, yet expensive Quantization-Aware Training (QAT) approaches, particularly when attempting to quantize LLMs, where scalability of the quantization process is of paramount importance. GPTQ essentially consists in learning the rounding operation using a small calibration set. In this work, we challenge common choices in GPTQ methods. In particular, we show that the process is, to a certain extent, robust to a number of variables (weight selection, feature augmentation, choice of calibration set). More importantly, we derive a number of best practices for designing more efficient and scalable GPTQ methods, regarding the problem formulation (loss, degrees of freedom, use of non-uniform quantization schemes) or optimization process (choice of variable and optimizer). Lastly, we propose a novel importance-based mixed-precision technique. Those guidelines lead to significant performance improvements on all the tested state-of-the-art GPTQ methods and networks (e.g. +6.819 points on ViT for 4-bit quantization), paving the way for the design of scalable, yet effective quantization methods.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:量化已成为深度神经网络的高效部署的关键步骤，将浮点操作转换为更简单的固定点操作。在最简单的形式下，它只是通过缩放和圆拟操作的组合来实现压缩率的受限或减少准确率。在最近，Gradient-based post-training quantization（GPTQ）方法变得更加重要，它们在尝试量化LLMs时， scalability of the quantization process是关键。GPTQ主要是通过学习圆拟操作来实现，使用一个小量化集。在这项工作中，我们挑战了GPTQ方法的常见选择。具体来说，我们发现该过程在一些变量（weight选择、特征增强、选择量化集）的影响下具有一定的抗预测性。此外，我们还提出了一些优化GPTQ方法的最佳实践，包括问题表示（损失、自由度、非均匀量化方案）和优化过程（变量和优化器选择）。最后，我们提出了一种重要性基于混合精度技术。这些指南导致了所有测试的现有GPTQ方法和网络（例如，+6.819点在ViT中 для 4比特量化）获得了显著性能提高，开启了可扩展、有效的量化方法的设计。
</details></li>
</ul>
<hr>
<h2 id="Attention-Is-Not-All-You-Need-Anymore"><a href="#Attention-Is-Not-All-You-Need-Anymore" class="headerlink" title="Attention Is Not All You Need Anymore"></a>Attention Is Not All You Need Anymore</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07661">http://arxiv.org/abs/2308.07661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Zhe Chen</li>
<li>for: 提高 transformer 性能</li>
<li>methods: 提出一种drop-in replacement self-attention mechanism，称为Extractor</li>
<li>results: 实验结果表明，将 self-attention mechanism  replaced with Extractor 可以提高 transformer 性能，并且可以更快than self-attention mechanism。<details>
<summary>Abstract</summary>
In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a drop-in replacement for the self-attention mechanism in the Transformer, called the Extractor, is proposed. Experimental results show that replacing the self-attention mechanism with the Extractor improves the performance of the Transformer. Furthermore, the proposed Extractor has the potential to run faster than the self-attention since it has a much shorter critical path of computation. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our understanding.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Commit-Message-Generation-to-History-Aware-Commit-Message-Completion"><a href="#From-Commit-Message-Generation-to-History-Aware-Commit-Message-Completion" class="headerlink" title="From Commit Message Generation to History-Aware Commit Message Completion"></a>From Commit Message Generation to History-Aware Commit Message Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07655">http://arxiv.org/abs/2308.07655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jetbrains-research/commit_message_generation">https://github.com/jetbrains-research/commit_message_generation</a></li>
<li>paper_authors: Aleksandra Eliseeva, Yaroslav Sokolov, Egor Bogomolov, Yaroslav Golubev, Danny Dig, Timofey Bryksin</li>
<li>for: 提高 commits 的质量和个性化程度，使开发者更容易跟踪变更和协作。</li>
<li>methods: 利用 previous commit history 作为额外 контекст，通过 completion 和 generation 两种方法来生成高质量 commits。</li>
<li>results: 结果显示，在某些情况下，使用 completion 方法可以达到更高的质量和个性化程度，而使用历史信息可以提高 CMG 模型在生成任务中的表现。<details>
<summary>Abstract</summary>
Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages.   In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT-3.5-turbo. Our results show that in some contexts, commit message completion shows better results than generation, and that while in general GPT-3.5-turbo performs worse, it shows potential for long and detailed messages. As for the history, the results show that historical information improves the performance of CMG models in the generation task, and the performance of GPT-3.5-turbo in both generation and completion.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>软件开发中的提交消息非常重要，它允许开发者跟踪更改并协作有效。尽管它们的重要性，但大多数提交消息缺乏重要信息，因为写好提交消息是时间consuming和繁琐的。有活跃的研究在提交消息生成（CMG）领域，但尚未得到广泛的实践应用。我们认为，如果我们可以将注重点从提交消息生成转移到提交消息完成，并使用之前的提交历史作为更多的上下文，我们可以大幅提高提交消息质量和个性化度。  在这篇论文中，我们提出并评估了两个新的想法。由于现有的数据集缺乏历史数据，我们收集和分享了一个新的数据集called CommitChronicle，包含20种编程语言的10.7万个提交。我们使用这个数据集来评估完成设定和使用历史上下文来评估当前CMG模型和GPT-3.5-turbo的表现。我们的结果表明，在某些情况下，提交消息完成比生成更好，而且GPT-3.5-turbo在详细的消息中表现较差，但在某些情况下具有潜在的潜力。对于历史信息，我们的结果表明，历史信息可以提高CMG模型在生成任务中的表现，并且GPT-3.5-turbo在生成和完成任务中的表现。
</details></li>
</ul>
<hr>
<h2 id="Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping"><a href="#Ternary-Singular-Value-Decomposition-as-a-Better-Parameterized-Form-in-Linear-Mapping" class="headerlink" title="Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping"></a>Ternary Singular Value Decomposition as a Better Parameterized Form in Linear Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07641">http://arxiv.org/abs/2308.07641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boyu Chen, Hanxuan Chen, Jiao He, Fengyu Sun, Shangling Jui</li>
<li>for: 这个论文的目的是提出一种简单 yet novel的线性映射方法来实现优秀的网络压缩性能。</li>
<li>methods: 这个论文使用的方法是一种叫做ternary SVD（TSVD）的 pseudo SVD，其限制了 $U$ 和 $V$ 矩阵在 SVD 中的形式为 ${\pm 1, 0}$ 的三元矩阵。这意味着在计算 $U(\cdot)$ 和 $V(\cdot)$ 时只需要使用加法操作。</li>
<li>results: 实验结果表明，TSVD 可以在不同类型的网络和任务中实现当今基eline模型如 ConvNext、Swim、BERT 和大型语言模型 OPT 的状态级压缩性能。<details>
<summary>Abstract</summary>
We present a simple yet novel parameterized form of linear mapping to achieves remarkable network compression performance: a pseudo SVD called Ternary SVD (TSVD).   Unlike vanilla SVD, TSVD limits the $U$ and $V$ matrices in SVD to ternary matrices form in $\{\pm 1, 0\}$. This means that instead of using the expensive multiplication instructions, TSVD only requires addition instructions when computing $U(\cdot)$ and $V(\cdot)$.   We provide direct and training transition algorithms for TSVD like Post Training Quantization and Quantization Aware Training respectively. Additionally, we analyze the convergence of the direct transition algorithms in theory.   In experiments, we demonstrate that TSVD can achieve state-of-the-art network compression performance in various types of networks and tasks, including current baseline models such as ConvNext, Swim, BERT, and large language model like OPT.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单 yet novel的线性映射参数化方法，可以实现出色的网络压缩性能：一种叫做ternary SVD（TSVD）的 Pseudo SVD。 unlike vanilla SVD, TSVD限制了 $U$ 和 $V$ 矩阵在 SV 中仅能是三元矩阵（\{\pm 1, 0\}）。这意味着在计算 $U(\cdot)$ 和 $V(\cdot)$ 时，TSVD 只需要使用加法指令，而不需要使用昂贵的乘法指令。我们提供了直接迁移算法和训练迁移算法，如Post Training Quantization 和 Quantization Aware Training 等。此外，我们还对直接迁移算法的整体性进行了理论分析。在实验中，我们表明了 TSVD 可以在不同类型的网络和任务上实现state-of-the-art的压缩性能，包括当前基eline模型 ConvNext、Swim、BERT 以及大型语言模型 OPT。
</details></li>
</ul>
<hr>
<h2 id="Backpropagation-Path-Search-On-Adversarial-Transferability"><a href="#Backpropagation-Path-Search-On-Adversarial-Transferability" class="headerlink" title="Backpropagation Path Search On Adversarial Transferability"></a>Backpropagation Path Search On Adversarial Transferability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07625">http://arxiv.org/abs/2308.07625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoer Xu, Zhangxuan Gu, Jianping Zhang, Shiwen Cui, Changhua Meng, Weiqiang Wang</li>
<li>for: 防御深度神经网络受到敌意例之攻击，需要在部署前测试模型的可靠性。</li>
<li>methods: 基于传输的攻击者使用拷贝模型构建敌意例，然后将其传输到黑盒环境中部署的受害者模型。为了增强攻击性能，结构基于的攻击者修改了反propagation路径，但现有的结构基于的攻击者忽略了 convolution 模块，并使用伪函数来修改反propagation图。</li>
<li>results: 我们提出了 backPropagation pAth Search (PAS)，解决了上述两个问题。我们首先提出了 SkipConv，用于调整 convolution 模块的反propagation路径。以免攻击路径过拟合 surrogate 模型，我们还构建了 DAG 基于搜索空间，使用一步靠近法评估路径，并使用 bayesian 优化来搜索最佳路径。我们在各种传输设置下进行了广泛的实验，显示 PAS 可以大幅提高攻击成功率，包括常训练的模型和防御模型。<details>
<summary>Abstract</summary>
Deep neural networks are vulnerable to adversarial examples, dictating the imperativeness to test the model's robustness before deployment. Transfer-based attackers craft adversarial examples against surrogate models and transfer them to victim models deployed in the black-box situation. To enhance the adversarial transferability, structure-based attackers adjust the backpropagation path to avoid the attack from overfitting the surrogate model. However, existing structure-based attackers fail to explore the convolution module in CNNs and modify the backpropagation graph heuristically, leading to limited effectiveness. In this paper, we propose backPropagation pAth Search (PAS), solving the aforementioned two problems. We first propose SkipConv to adjust the backpropagation path of convolution by structural reparameterization. To overcome the drawback of heuristically designed backpropagation paths, we further construct a DAG-based search space, utilize one-step approximation for path evaluation and employ Bayesian Optimization to search for the optimal path. We conduct comprehensive experiments in a wide range of transfer settings, showing that PAS improves the attack success rate by a huge margin for both normally trained and defense models.
</details>
<details>
<summary>摘要</summary>
深度神经网络容易受到敌意例际的攻击，因此在部署之前测试模型的可靠性是非常重要的。转移基于攻击者通过附加模型制造敌意例并将其传递到部署在黑盒子情况下的受害模型。为增强敌意例的可传递性，结构基于攻击者可以修改归并征求的路径，以避免攻击过拟合附加模型。然而，现有的结构基于攻击者未能探索CNN中的卷积模块，并修改归并图表使用了规则性的方法，导致效果有限。在这篇论文中，我们提出了backPropagation pAth Search（PAS），解决以下两个问题。我们首先提出了SkipConv，用于调整卷积后的归并路径。为了超越规则性设计的归并路径的缺点，我们进一步构建了DAG基本搜索空间，使用一步逼近方法评估归并路径，并使用bayesian优化来搜索最佳路径。我们在各种转移设置下进行了广泛的实验，结果显示，PAS可以大幅提高攻击成功率，包括常训练的模型和防御模型。
</details></li>
</ul>
<hr>
<h2 id="A-Multilayer-Perceptron-based-Fast-Sunlight-Assessment-for-the-Conceptual-Design-of-Residential-Neighborhoods-under-Chinese-Policy"><a href="#A-Multilayer-Perceptron-based-Fast-Sunlight-Assessment-for-the-Conceptual-Design-of-Residential-Neighborhoods-under-Chinese-Policy" class="headerlink" title="A Multilayer Perceptron-based Fast Sunlight Assessment for the Conceptual Design of Residential Neighborhoods under Chinese Policy"></a>A Multilayer Perceptron-based Fast Sunlight Assessment for the Conceptual Design of Residential Neighborhoods under Chinese Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07616">http://arxiv.org/abs/2308.07616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Jiang, Xiong Liang, Yu-Cheng Zhou, Yong Tian, Shengli Xu, Jia-Rui Lin, Zhiliang Ma, Shiji Yang, Hao Zhou</li>
<li>for: 本研究旨在应用深度学习技术来加速建筑设计阶段的日照时数 simulations，以减少计算时间和提高设计效率。</li>
<li>methods: 本研究提出了一个多层感知器（Multilayer Perceptron，MLP）基本的一阶预测方法，可以快速地预测建筑物的日照时数。方法首先将建筑物分解为多个立方体形状的部分，然后运用一个一阶预测模型来预测每个部分的日照时数。</li>
<li>results: 经过三个 numeral experiments，包括水平层和倾斜分析、模拟运算和优化，结果显示，本方法可以将计算时间降低到1&#x2F;84<del>1&#x2F;50，并保持96.5%</del>98%的准确性。此外，基于提案的模型，也开发了一个实用的住宅区布局规划插件 для Rhino 7&#x2F;Grasshopper。<details>
<summary>Abstract</summary>
In Chinese building codes, it is required that residential buildings receive a minimum number of hours of natural, direct sunlight on a specified winter day, which represents the worst sunlight condition in a year. This requirement is a prerequisite for obtaining a building permit during the conceptual design of a residential project. Thus, officially sanctioned software is usually used to assess the sunlight performance of buildings. These software programs predict sunlight hours based on repeated shading calculations, which is time-consuming. This paper proposed a multilayer perceptron-based method, a one-stage prediction approach, which outputs a shading time interval caused by the inputted cuboid-form building. The sunlight hours of a site can be obtained by calculating the union of the sunlight time intervals (complement of shading time interval) of all the buildings. Three numerical experiments, i.e., horizontal level and slope analysis, and simulation-based optimization are carried out; the results show that the method reduces the computation time to 1/84~1/50 with 96.5%~98% accuracies. A residential neighborhood layout planning plug-in for Rhino 7/Grasshopper is also developed based on the proposed model. This paper indicates that deep learning techniques can be adopted to accelerate sunlight hour simulations at the conceptual design phase.
</details>
<details>
<summary>摘要</summary>
中国建筑标准要求住宅建筑在指定的冬季日子上得到最少的自然、直接日光时间，这是为了获得建筑许可证的必要条件。因此，官方批准的软件通常用于评估建筑的日光性能。这些软件计算出日光时间基于重复的遮挡计算，这是时间消耗大。本文提出了基于多层感知器的方法，一种一stage预测方法，它输出一个输入的立方体形建筑物遮挡时间间隔。通过计算所有建筑物的遮挡时间间隔的并集（补做遮挡时间间隔），可以获得建筑地点的日光时间。三个数学实验，即水平层和坡度分析，以及基于仿真优化的模拟，都表明了该方法可以将计算时间减少到1/84~1/50，并保持96.5%~98%的准确性。此外，基于提议的模型也开发了一个基于Rhino 7/Grasshopper的住宅街区规划插件。本文表明，深度学习技术可以在概念设计阶段加速日光时间的估算。
</details></li>
</ul>
<hr>
<h2 id="Searching-for-Novel-Chemistry-in-Exoplanetary-Atmospheres-using-Machine-Learning-for-Anomaly-Detection"><a href="#Searching-for-Novel-Chemistry-in-Exoplanetary-Atmospheres-using-Machine-Learning-for-Anomaly-Detection" class="headerlink" title="Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection"></a>Searching for Novel Chemistry in Exoplanetary Atmospheres using Machine Learning for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07604">http://arxiv.org/abs/2308.07604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roy T. Forestano, Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu</li>
<li>for: 本研究旨在开发新的快速高效的机器学习方法，用于检测望远镜观测数据中异常的行星，以找到具有不同化学成分的行星和可能的生物标志物。</li>
<li>methods: 本研究使用了两种流行的异常检测方法：本地异常因子和一类支持向量机器学习。</li>
<li>results: 研究成功地应用了这两种方法于大量的人工数据库中，并通过ROC曲线评估和比较了两种方法的性能。<details>
<summary>Abstract</summary>
The next generation of telescopes will yield a substantial increase in the availability of high-resolution spectroscopic data for thousands of exoplanets. The sheer volume of data and number of planets to be analyzed greatly motivate the development of new, fast and efficient methods for flagging interesting planets for reobservation and detailed analysis. We advocate the application of machine learning (ML) techniques for anomaly (novelty) detection to exoplanet transit spectra, with the goal of identifying planets with unusual chemical composition and even searching for unknown biosignatures. We successfully demonstrate the feasibility of two popular anomaly detection methods (Local Outlier Factor and One Class Support Vector Machine) on a large public database of synthetic spectra. We consider several test cases, each with different levels of instrumental noise. In each case, we use ROC curves to quantify and compare the performance of the two ML techniques.
</details>
<details>
<summary>摘要</summary>
下一代望远镜将提供大量高分辨率光谱数据，用于千个外围星球的分析。数据量和星球数量的增加，大大推动了新的快速高效的方法的开发，用于标注有趣的星球进行重新观测和详细分析。我们提议通过机器学习（ML）技术进行外围星球谱spectra中异常（新型）检测，以找到不寻常的化学组成和甚至搜索未知生物标志。我们成功地在大规模公共数据库中使用synthetic spectra进行了两种流行的异常检测方法的实验（Local Outlier Factor和One Class Support Vector Machine），并在不同的实rumental noise水平下进行了多个测试case。在每个测试case中，我们使用ROC曲线来评估和比较两种ML技术的性能。
</details></li>
</ul>
<hr>
<h2 id="Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning"><a href="#Generating-Personas-for-Games-with-Multimodal-Adversarial-Imitation-Learning" class="headerlink" title="Generating Personas for Games with Multimodal Adversarial Imitation Learning"></a>Generating Personas for Games with Multimodal Adversarial Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07598">http://arxiv.org/abs/2308.07598</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Ahlberg, Alessandro Sestini, Konrad Tollmar, Linus Gisslén</li>
<li>for: 本研究旨在开发一种可以生成多个个性化策略的协同学习方法，以便模拟人类游戏玩家的多种玩法。</li>
<li>methods: 本研究使用了多模块生成 adversarial imitation learning（MultiGAIL）方法，通过在单机器模型中学习多个专家策略，并使用多个评估器来学习环境奖励。</li>
<li>results: 实验结果表明，MultiGAIL方法可以在连续和离散动作空间中的两个环境中生成多个个性化策略，并且在这些环境中表现出色。<details>
<summary>Abstract</summary>
Reinforcement learning has been widely successful in producing agents capable of playing games at a human level. However, this requires complex reward engineering, and the agent's resulting policy is often unpredictable. Going beyond reinforcement learning is necessary to model a wide range of human playstyles, which can be difficult to represent with a reward function. This paper presents a novel imitation learning approach to generate multiple persona policies for playtesting. Multimodal Generative Adversarial Imitation Learning (MultiGAIL) uses an auxiliary input parameter to learn distinct personas using a single-agent model. MultiGAIL is based on generative adversarial imitation learning and uses multiple discriminators as reward models, inferring the environment reward by comparing the agent and distinct expert policies. The reward from each discriminator is weighted according to the auxiliary input. Our experimental analysis demonstrates the effectiveness of our technique in two environments with continuous and discrete action spaces.
</details>
<details>
<summary>摘要</summary>
� Reinforcement learning 已经成功地将机器人训练到人类水准。但是，这需要复杂的奖励工程，并且机器人的结果策略可能是随机的。为了模型人类玩家的广泛风格，超出奖励学习是必要的。这篇论文介绍了一种具有多个人格的循环学习方法，即多模倍GAIL（MultiGAIL）。MultiGAIL使用辅助输入参数来学习不同的人格，使用多个批评者作为奖励模型，推算环境奖励通过比较机器人和具体专家策略。奖励从每个批评者被权重根据辅助输入。我们的实验分析表明我们的技术在维度和整数动作空间的两个环境中具有效果。
</details></li>
</ul>
<hr>
<h2 id="High-Probability-Risk-Bounds-via-Sequential-Predictors"><a href="#High-Probability-Risk-Bounds-via-Sequential-Predictors" class="headerlink" title="High-Probability Risk Bounds via Sequential Predictors"></a>High-Probability Risk Bounds via Sequential Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07588">http://arxiv.org/abs/2308.07588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dirk van der Hoeven, Nikita Zhivotovskiy, Nicolò Cesa-Bianchi</li>
<li>for: 这 paper written for what?	+ 这 paper 目的是提供一种在线学习方法，可以在 minimal assumptions 下提供sequential regret bounds和in-expectation risk bounds。</li>
<li>methods: 这 paper 使用哪些方法?	+ 这 paper 使用 online learning methods，包括 general online learning algorithms 和 second-order correction to the loss function。</li>
<li>results: 这 paper 得到了哪些结果?	+ 这 paper 得到了nearly optimal high-probability risk bounds for several classical statistical estimation problems，such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation。<details>
<summary>Abstract</summary>
Online learning methods yield sequential regret bounds under minimal assumptions and provide in-expectation risk bounds for statistical learning. However, despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper nature of our estimators enables significant improvements in the dependencies on various problem parameters. Finally, we discuss some computational advantages of our sequential algorithms over their existing batch counterparts.
</details>
<details>
<summary>摘要</summary>
在线学习方法提供序列 regret bound nder minimal 假设，并提供预期 риск bound  для统计学学习。然而， despite the apparent advantage of online guarantees over their statistical counterparts, recent findings indicate that in many important cases, regret bounds may not guarantee tight high-probability risk bounds in the statistical setting. In this work, we show that online to batch conversions applied to general online learning algorithms can bypass this limitation. Via a general second-order correction to the loss function defining the regret, we obtain nearly optimal high-probability risk bounds for several classical statistical estimation problems, such as discrete distribution estimation, linear regression, logistic regression, and conditional density estimation. Our analysis relies on the fact that many online learning algorithms are improper, as they are not restricted to use predictors from a given reference class. The improper nature of our estimators enables significant improvements in the dependencies on various problem parameters. Finally, we discuss some computational advantages of our sequential algorithms over their existing batch counterparts.
</details></li>
</ul>
<hr>
<h2 id="Temporal-Interest-Network-for-Click-Through-Rate-Prediction"><a href="#Temporal-Interest-Network-for-Click-Through-Rate-Prediction" class="headerlink" title="Temporal Interest Network for Click-Through Rate Prediction"></a>Temporal Interest Network for Click-Through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08487">http://arxiv.org/abs/2308.08487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenweichen/DSIN">https://github.com/shenweichen/DSIN</a></li>
<li>paper_authors: Haolin Zhou, Junwei Pan, Xinyi Zhou, Xihua Chen, Jie Jiang, Xiaofeng Gao, Guihai Chen</li>
<li>for: 预测点击率 (CTR) 的预测，研究者发现了用户行为历史记录的四元相关性（行为语义、目标语义、行为时间和目标时间）对性能的影响。</li>
<li>methods: 研究者使用了各种用户行为方法，包括 Semantic Embedding 和 Temporal Encoding，以及 Target-Aware Attention 和 Target-Aware Representation。</li>
<li>results: 研究者发现，现有方法无法学习这种四元相关性，而他们提出的 Temporal Interest Network (TIN) 可以有效地捕捉这种相关性，并在 Amazon 和 Alibaba 数据集上进行了广泛的评估，并与最佳基eline相比，TIN 表现出了0.43% 和 0.29% 的提升。<details>
<summary>Abstract</summary>
The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.   In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and the target. We achieve this by incorporating target-aware temporal encoding, in addition to semantic embedding, to represent behaviors and the target. Furthermore, we deploy target-aware attention, along with target-aware representation, to explicitly conduct the 4-way interaction. We performed comprehensive evaluations on the Amazon and Alibaba datasets. Our proposed TIN outperforms the best-performing baselines by 0.43\% and 0.29\% on two datasets, respectively. Comprehensive analysis and visualization show that TIN is indeed capable of learning the quadruple correlation effectively, while all existing methods fail to do so. We provide our implementation of TIN in Tensorflow.
</details>
<details>
<summary>摘要</summary>
历史用户行为特征是预测点击率(CTR)的一个最重要的特征，因为它们具有强的 semantics和时间相关性。 Although literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and the target. We achieve this by incorporating target-aware temporal encoding, in addition to semantic embedding, to represent behaviors and the target. Furthermore, we deploy target-aware attention, along with target-aware representation, to explicitly conduct the 4-way interaction. We performed comprehensive evaluations on the Amazon and Alibaba datasets. Our proposed TIN outperforms the best-performing baselines by 0.43% and 0.29% on two datasets, respectively. Comprehensive analysis and visualization show that TIN is indeed capable of learning the quadruple correlation effectively, while all existing methods fail to do so. We provide our implementation of TIN in Tensorflow.
</details></li>
</ul>
<hr>
<h2 id="IoT-Data-Trust-Evaluation-via-Machine-Learning"><a href="#IoT-Data-Trust-Evaluation-via-Machine-Learning" class="headerlink" title="IoT Data Trust Evaluation via Machine Learning"></a>IoT Data Trust Evaluation via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11638">http://arxiv.org/abs/2308.11638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Timothy Tadj, Reza Arablouei, Volkan Dedeoglu<br>for:This paper aims to address the lack of publicly-available datasets for evaluating the trustworthiness of IoT data by proposing a data synthesis method called random walk infilling (RWI) to augment existing trustworthy datasets with untrustworthy data.methods:The proposed method uses RWI to generate untrustworthy data from existing trustworthy datasets, and extracts new features from IoT time-series sensor data that capture its auto-correlation and cross-correlation with neighboring sensors. These features are used to learn ML models for recognizing the trustworthiness of IoT sensor data.results:The proposed method outperforms existing ML-based approaches to IoT data trust evaluation, and a semi-supervised ML approach that requires only about 10% of the data labeled offers competitive performance while being more practical. The results also show that the ML models learned from datasets augmented via RWI generalize well to unseen data.<details>
<summary>Abstract</summary>
Various approaches based on supervised or unsupervised machine learning (ML) have been proposed for evaluating IoT data trust. However, assessing their real-world efficacy is hard mainly due to the lack of related publicly-available datasets that can be used for benchmarking. Since obtaining such datasets is challenging, we propose a data synthesis method, called random walk infilling (RWI), to augment IoT time-series datasets by synthesizing untrustworthy data from existing trustworthy data. Thus, RWI enables us to create labeled datasets that can be used to develop and validate ML models for IoT data trust evaluation. We also extract new features from IoT time-series sensor data that effectively capture its auto-correlation as well as its cross-correlation with the data of the neighboring (peer) sensors. These features can be used to learn ML models for recognizing the trustworthiness of IoT sensor data. Equipped with our synthesized ground-truth-labeled datasets and informative correlation-based feature, we conduct extensive experiments to critically examine various approaches to evaluating IoT data trust via ML. The results reveal that commonly used ML-based approaches to IoT data trust evaluation, which rely on unsupervised cluster analysis to assign trust labels to unlabeled data, perform poorly. This poor performance can be attributed to the underlying unsubstantiated assumption that clustering provides reliable labels for data trust, a premise that is found to be untenable. The results also show that the ML models learned from datasets augmented via RWI while using the proposed features generalize well to unseen data and outperform existing related approaches. Moreover, we observe that a semi-supervised ML approach that requires only about 10% of the data labeled offers competitive performance while being practically more appealing compared to the fully-supervised approaches.
</details>
<details>
<summary>摘要</summary>
各种基于指导或无指导机器学习（ML）的方法已经为评估互联网器件数据（IoT）的可信度提出了多种方法。然而，评估它们在实际世界中的有效性很难，主要因为缺乏相关的公共可用数据集，用于比较。由于获取这些数据集困难，我们提议一种数据生成方法，即随机扩散填充（RWI），以增强IoT时间序数据集。通过将可信数据 Synthesize into不可信数据，我们可以创建可用于开发和验证ML模型的标注数据集。我们还提取了IoT时间序感知器数据中有效地捕捉自动相关性以及与邻近（邻居）感知器数据的相关性。这些特征可以用来学习识别IoT感知器数据的可信度。配备我们生成的标注数据集和有用的相关特征，我们进行了广泛的实验，critically examine了多种基于ML的IoT数据可信度评估方法。结果显示，常用的ML基于方法，通过不supervised cluster analysis将无标签数据分类为可信数据，表现不佳。这些结果可以归结于这些方法下的一个不实际的假设，即分类提供可靠的数据可信度标签。结果还表明，使用RWI生成的数据集和我们提出的特征来学习ML模型，在未看到数据时generalize well，并且超过现有相关方法。此外，我们发现 semi-supervised ML方法，只需要约10%的数据标注，可以提供竞争力强的性能，而且在实际应用中更加吸引人。
</details></li>
</ul>
<hr>
<h2 id="Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory"><a href="#Story-Visualization-by-Online-Text-Augmentation-with-Context-Memory" class="headerlink" title="Story Visualization by Online Text Augmentation with Context Memory"></a>Story Visualization by Online Text Augmentation with Context Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07575">http://arxiv.org/abs/2308.07575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonseivnl/cmota">https://github.com/yonseivnl/cmota</a></li>
<li>paper_authors: Daechul Ahn, Daneul Kim, Gwangmo Song, Seung Hwan Kim, Honglak Lee, Dongyeop Kang, Jonghyun Choi</li>
<li>for: 提高文本描述到图像生成 task 中的语言多样性抗锋性。</li>
<li>methods: 提出了一种基于 Bi-directional Transformer 框架的内存架构，并在训练时使用在线文本增强来生成多个 pseudo-descriptions 作为补做性超级vision 的权威指导。</li>
<li>results: 在 Pororo-SV 和 Flintstones-SV 两个受欢迎的 SV  benchmark 上，提出的方法与现状相比，在多个纪录中表现出优于其他方法，包括 FID、人物 F1、帧精度、 BLEU-2&#x2F;3 和 R-precision 等指标。<details>
<summary>Abstract</summary>
Story visualization (SV) is a challenging text-to-image generation task for the difficulty of not only rendering visual details from the text descriptions but also encoding a long-term context across multiple sentences. While prior efforts mostly focus on generating a semantically relevant image for each sentence, encoding a context spread across the given paragraph to generate contextually convincing images (e.g., with a correct character or with a proper background of the scene) remains a challenge. To this end, we propose a novel memory architecture for the Bi-directional Transformer framework with an online text augmentation that generates multiple pseudo-descriptions as supplementary supervision during training for better generalization to the language variation at inference. In extensive experiments on the two popular SV benchmarks, i.e., the Pororo-SV and Flintstones-SV, the proposed method significantly outperforms the state of the arts in various metrics including FID, character F1, frame accuracy, BLEU-2/3, and R-precision with similar or less computational complexity.
</details>
<details>
<summary>摘要</summary>
story visualization (SV) 是一个具有挑战性的文本到图像生成任务，因为不仅需要从文本描述中提取视觉细节，还需要在多句话中编码长期上下文。而现有的尝试主要是为每句文本生成Semantically relevant的图像，但是在保持场景背景和人物性别正确的情况下，在整个段落上编码上下文并生成情节感地投入的图像仍然是一个挑战。为此，我们提议一种新的内存架构，用于Bi-directional Transformer框架的在线文本增强，在训练过程中生成多个假描述作为补做性的超vision，以提高语言变化的适应性。在两个流行的 SV 标准测试集上，即Pororo-SV 和 Flintstones-SV，我们的方法与现有的状态arius signicantly outperform，包括 FID、character F1、frame accuracy、BLEU-2/3 和 R-precision 等多个维度的指标，同时具有相似或更少的计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-data-generation-method-for-hybrid-image-tabular-data-using-two-generative-adversarial-networks"><a href="#Synthetic-data-generation-method-for-hybrid-image-tabular-data-using-two-generative-adversarial-networks" class="headerlink" title="Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks"></a>Synthetic data generation method for hybrid image-tabular data using two generative adversarial networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07573">http://arxiv.org/abs/2308.07573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Kikuchi, Shouhei Hanaoka, Takahiro Nakao, Tomomi Takenaga, Yukihiro Nomura, Harushi Mori, Takeharu Yoshikawa</li>
<li>for: 提供一种生成静脉呼吸图像和结构化表格数据的新方法，以解决医疗隐私和数据共享问题。</li>
<li>methods: 使用自适应GAN和条件表格GAN模型，将大型公共数据库（pDB）中的静脉呼吸图像维度减少，并将图像编码器与原始数据库（oDB）中的图像进行对应。</li>
<li>results: 成功生成了多样化的合成医疗记录，保持了图像和表格数据之间的协调性，并通过视觉评估、分布分析和分类任务进行评估。<details>
<summary>Abstract</summary>
The generation of synthetic medical records using generative adversarial networks (GANs) has become increasingly important for addressing privacy concerns and promoting data sharing in the medical field. In this paper, we propose a novel method for generating synthetic hybrid medical records consisting of chest X-ray images (CXRs) and structured tabular data (including anthropometric data and laboratory tests) using an auto-encoding GAN ({\alpha}GAN) and a conditional tabular GAN (CTGAN). Our approach involves training a {\alpha}GAN model on a large public database (pDB) to reduce the dimensionality of CXRs. We then applied the trained encoder of the GAN model to the images in original database (oDB) to obtain the latent vectors. These latent vectors were combined with tabular data in oDB, and these joint data were used to train the CTGAN model. We successfully generated diverse synthetic records of hybrid CXR and tabular data, maintaining correspondence between them. We evaluated this synthetic database (sDB) through visual assessment, distribution of interrecord distances, and classification tasks. Our evaluation results showed that the sDB captured the features of the oDB while maintaining the correspondence between the images and tabular data. Although our approach relies on the availability of a large-scale pDB containing a substantial number of images with the same modality and imaging region as those in the oDB, this method has the potential for the public release of synthetic datasets without compromising the secondary use of data.
</details>
<details>
<summary>摘要</summary>
现代生成技术在医疗领域得到了广泛应用，特别是使用生成对抗网络（GANs）生成合成医疗记录，以解决隐私问题和促进数据共享。在这篇论文中，我们提出了一种新的方法，使用自动编码GAN（αGAN）和条件表格GAN（CTGAN）生成 hybrid 的胸部X射线图像（CXR）和结构化表格数据（包括人体测量数据和实验室测试）。我们的方法包括在大规模公共数据库（pDB）中训练αGAN模型，以减少CXR的维度。然后，我们将训练过的GAN模型的编码器应用到oDB中的图像上，以获取秘密 вектор。这些秘密 вектор与表格数据在oDB中进行结合，并将这些联合数据用于训练CTGAN模型。我们成功地生成了多样性的合成记录，保持了图像和表格数据之间的协调。我们通过视觉评估、记录间距离分布和分类任务进行评估。我们的评估结果表明，sDB捕捉了oDB中的特征，同时保持了图像和表格数据之间的协调。虽然我们的方法需要大规模的pDB，但这种方法具有公共发布合成数据库的潜在优势，不会损害数据的次要使用。
</details></li>
</ul>
<hr>
<h2 id="Ske2Grid-Skeleton-to-Grid-Representation-Learning-for-Action-Recognition"><a href="#Ske2Grid-Skeleton-to-Grid-Representation-Learning-for-Action-Recognition" class="headerlink" title="Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition"></a>Ske2Grid: Skeleton-to-Grid Representation Learning for Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07571">http://arxiv.org/abs/2308.07571</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osvai/ske2grid">https://github.com/osvai/ske2grid</a></li>
<li>paper_authors: Dongqi Cai, Yangyuxuan Kang, Anbang Yao, Yurong Chen</li>
<li>for: 这篇论文提出了一种基于骨架的动作识别表征学习框架，即Ske2Grid，以提高骨架基рован动作识别的准确率。</li>
<li>methods: 该框架使用了三种新的设计方法：图节点指定变换（GIT）、上升变换（UPT）和进步学习策略（PLS）。GIT用于构建一个固定大小的网格图像块，而UPT用于填充网格图像块中的节点。PLS用于解决一步UPT的严格性问题，并且可以逐步提高表征能力。</li>
<li>results: 实验表明，Ske2Grid在六个主流骨架基рован动作识别数据集上表现出色，与现有GCN基于解决方案相比，无需添加其他特殊设计。代码和模型可以在<a target="_blank" rel="noopener" href="https://github.com/OSVAI/Ske2Grid%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/OSVAI/Ske2Grid上下载。</a><details>
<summary>Abstract</summary>
This paper presents Ske2Grid, a new representation learning framework for improved skeleton-based action recognition. In Ske2Grid, we define a regular convolution operation upon a novel grid representation of human skeleton, which is a compact image-like grid patch constructed and learned through three novel designs. Specifically, we propose a graph-node index transform (GIT) to construct a regular grid patch through assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, an up-sampling transform (UPT) is learned to interpolate the skeleton graph nodes for filling the grid patch to the full. To resolve the problem when the one-step UPT is aggressive and further exploit the representation capability of the grid patch with increasing spatial size, a progressive learning strategy (PLS) is proposed which decouples the UPT into multiple steps and aligns them to multiple paired GITs through a compact cascaded design learned progressively. We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. Experiments show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without bells and whistles. Code and models are available at https://github.com/OSVAI/Ske2Grid
</details>
<details>
<summary>摘要</summary>
First, we propose a graph-node index transform (GIT) to construct a regular grid patch by assigning the nodes in the skeleton graph one by one to the desired grid cells. To ensure that GIT is a bijection and enrich the expressiveness of the grid representation, we also learn an up-sampling transform (UPT) to interpolate the skeleton graph nodes for filling the grid patch to the full.To further exploit the representation capability of the grid patch with increasing spatial size, we propose a progressive learning strategy (PLS) that decouples the UPT into multiple steps and aligns them to multiple paired GITs through a compact cascaded design learned progressively.We construct networks upon prevailing graph convolution networks and conduct experiments on six mainstream skeleton-based action recognition datasets. The results show that our Ske2Grid significantly outperforms existing GCN-based solutions under different benchmark settings, without any additional modifications or tricks. The code and models are available at https://github.com/OSVAI/Ske2Grid.
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Learning-with-Multiple-Imputations-on-Non-Random-Missing-Labels"><a href="#Semi-Supervised-Learning-with-Multiple-Imputations-on-Non-Random-Missing-Labels" class="headerlink" title="Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels"></a>Semi-Supervised Learning with Multiple Imputations on Non-Random Missing Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07562">http://arxiv.org/abs/2308.07562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Lu, Michael Ma, Huaze Xu, Zixi Xu</li>
<li>for: 这个论文主要针对 semi-supervised learning (SSL) 中的三个主要问题：missing at random (MAR)、missing completely at random (MCAR) 和 missing not at random (MNAR)。</li>
<li>methods: 这篇论文提出了两种新的方法，用于combine multiple imputation models，以提高准确性和减少偏见。第一种方法是使用多个插值模型，创建信任区间，并应用一个阈值来忽略低信任 pseudo-labels。第二种方法是our new method，SSL with De-biased Imputations (SSL-DI)，通过过滤不准确的数据，找到一个准确可靠的子集，然后将这个子集插值到另一个 SSL 模型中，以减少偏见。</li>
<li>results: 该论文的实验结果表明，提出的方法可以在 MCAR 和 MNAR  Situations 中效果地减少偏见，并在类别准确率方面与现有方法相比，表现出较高的性能。<details>
<summary>Abstract</summary>
Semi-Supervised Learning (SSL) is implemented when algorithms are trained on both labeled and unlabeled data. This is a very common application of ML as it is unrealistic to obtain a fully labeled dataset. Researchers have tackled three main issues: missing at random (MAR), missing completely at random (MCAR), and missing not at random (MNAR). The MNAR problem is the most challenging of the three as one cannot safely assume that all class distributions are equal. Existing methods, including Class-Aware Imputation (CAI) and Class-Aware Propensity (CAP), mostly overlook the non-randomness in the unlabeled data. This paper proposes two new methods of combining multiple imputation models to achieve higher accuracy and less bias. 1) We use multiple imputation models, create confidence intervals, and apply a threshold to ignore pseudo-labels with low confidence. 2) Our new method, SSL with De-biased Imputations (SSL-DI), aims to reduce bias by filtering out inaccurate data and finding a subset that is accurate and reliable. This subset of the larger dataset could be imputed into another SSL model, which will be less biased. The proposed models have been shown to be effective in both MCAR and MNAR situations, and experimental results show that our methodology outperforms existing methods in terms of classification accuracy and reducing bias.
</details>
<details>
<summary>摘要</summary>
《半supervised学习（SSL）实现方法》在实际应用中，通常不可能获得完全标注数据集，因此SSL成为了非常常见的应用。研究人员面临着三个主要问题：Random missing（MAR）、完全随机缺失（MCAR）和非随机缺失（MNAR）。MNAR问题是三个问题中最为困难，因为不可能安全地假设所有类别分布相同。现有的方法，包括Class-Aware Imputation（CAI）和Class-Aware Propensity（CAP），几乎忽略了不Random的未标注数据。本文提出了两种新的方法，用于 combinig多个替补模型，以达到更高的准确率和更少的偏见。1. 我们使用多个替补模型，创建信任区间，并应用一个阈值，以忽略低信任 Pseudo-labels。2. 我们的新方法，SSL with De-biased Imputations（SSL-DI），旨在减少偏见，通过筛选不准确的数据，并找到一个准确可靠的子集，并将这个子集用于另一个SSL模型，以减少偏见。我们的方法在MCAR和MNAR情况下都有显著的优势，实验结果表明，我们的方法在分类精度和减少偏见方面都超过了现有方法。
</details></li>
</ul>
<hr>
<h2 id="A-User-Centered-Evaluation-of-Spanish-Text-Simplification"><a href="#A-User-Centered-Evaluation-of-Spanish-Text-Simplification" class="headerlink" title="A User-Centered Evaluation of Spanish Text Simplification"></a>A User-Centered Evaluation of Spanish Text Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07556">http://arxiv.org/abs/2308.07556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian de Wynter, Anthony Hevia, Si-Qing Chen</li>
<li>for: 评估西班牙文简化（TS）生产系统中的表现，通过复杂句子和复杂词语识别两个字库。</li>
<li>methods: 使用神经网络对西班牙语TS进行评估，并与最常见的西班牙语特有可读性分数进行比较，发现神经网络在预测用户TS偏好时表现更好。</li>
<li>results: 发现多语言模型在同一任务中下降表现，而所有模型往往围绕毫不重要的统计特征（如句子长度）集中焦点。<details>
<summary>Abstract</summary>
We present an evaluation of text simplification (TS) in Spanish for a production system, by means of two corpora focused in both complex-sentence and complex-word identification. We compare the most prevalent Spanish-specific readability scores with neural networks, and show that the latter are consistently better at predicting user preferences regarding TS. As part of our analysis, we find that multilingual models underperform against equivalent Spanish-only models on the same task, yet all models focus too often on spurious statistical features, such as sentence length. We release the corpora in our evaluation to the broader community with the hopes of pushing forward the state-of-the-art in Spanish natural language processing.
</details>
<details>
<summary>摘要</summary>
我们对西班牙文简化文本（TS）进行了评估，使用了两个文本库，专注于复杂句子和复杂单词识别。我们比较了最常见的西班牙语特有的可读性分数，以及神经网络，并发现后者在预测用户对TS的偏好时表现更好。在我们的分析中，我们发现了许多多语言模型在同一任务上表现较差，但所有模型很多时候强调无关紧要的统计特征，如句子长度。我们将我们的评估 corpora 发布给广泛的社区，以促进西班牙自然语言处理领域的进步。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Antidote-Improved-Pointwise-Certifications-against-Poisoning-Attacks"><a href="#Enhancing-the-Antidote-Improved-Pointwise-Certifications-against-Poisoning-Attacks" class="headerlink" title="Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks"></a>Enhancing the Antidote: Improved Pointwise Certifications against Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07553">http://arxiv.org/abs/2308.07553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Liu, Andrew C. Cullen, Paul Montague, Sarah M. Erfani, Benjamin I. P. Rubinstein</li>
<li>for: 防止毒素攻击影响模型行为</li>
<li>methods: 使用渐进隐私和随机采样 Gaussian 机制确保测试实例对 finite 数量毒素样例的不变性</li>
<li>results: 提供更大于先前证明的攻击Robustness 保证<details>
<summary>Abstract</summary>
Poisoning attacks can disproportionately influence model behaviour by making small changes to the training corpus. While defences against specific poisoning attacks do exist, they in general do not provide any guarantees, leaving them potentially countered by novel attacks. In contrast, by examining worst-case behaviours Certified Defences make it possible to provide guarantees of the robustness of a sample against adversarial attacks modifying a finite number of training samples, known as pointwise certification. We achieve this by exploiting both Differential Privacy and the Sampled Gaussian Mechanism to ensure the invariance of prediction for each testing instance against finite numbers of poisoned examples. In doing so, our model provides guarantees of adversarial robustness that are more than twice as large as those provided by prior certifications.
</details>
<details>
<summary>摘要</summary>
毒素攻击可以夹击模型行为，通过小量修改训练集来让模型表现出巨大的影响。虽然有针对特定毒素攻击的防御方法，但这些防御方法通常不提供任何保证，因此可能会被新的攻击所打砸。相比之下，通过检查最坏情况的证明 Certified Defences，我们可以提供对测试实例的预测结果进行保证，并 garantuee 测试实例对于有限多个毒素样本的修改后的预测结果的一致性。通过利用差分隐私和随机 Gaussian 机制，我们的模型可以保证对于每个测试实例，对于有限多个毒素样本的修改后的预测结果的一致性。这使得我们的模型可以提供更大于之前证明的防御 robustness。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts"><a href="#Domain-Adaptation-via-Minimax-Entropy-for-Real-Bogus-Classification-of-Astronomical-Alerts" class="headerlink" title="Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts"></a>Domain Adaptation via Minimax Entropy for Real&#x2F;Bogus Classification of Astronomical Alerts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07538">http://arxiv.org/abs/2308.07538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Cabrera-Vives, César Bolivar, Francisco Förster, Alejandra M. Muñoz Arancibia, Manuel Pérez-Carrasco, Esteban Reyes</li>
<li>for: 这个论文旨在研究域域适应（Domain Adaptation）技术，用于实时分析大量天文数据。</li>
<li>methods: 该论文使用了四个不同的数据集（HiTS、DES、ATLAS和ZTF），研究这些数据集之间的域Shift，并通过微调和半supervised深度适应来提高一个简单的深度学习分类模型。</li>
<li>results: 研究发现，微调和MME模型可以在目标数据集上提高基本模型的准确率，但MME模型不会对源数据集产生影响。只需要一些来自目标数据集的标注项（一个或 fewer），微调和MME模型就可以显著提高分类精度。<details>
<summary>Abstract</summary>
Time domain astronomy is advancing towards the analysis of multiple massive datasets in real time, prompting the development of multi-stream machine learning models. In this work, we study Domain Adaptation (DA) for real/bogus classification of astronomical alerts using four different datasets: HiTS, DES, ATLAS, and ZTF. We study the domain shift between these datasets, and improve a naive deep learning classification model by using a fine tuning approach and semi-supervised deep DA via Minimax Entropy (MME). We compare the balanced accuracy of these models for different source-target scenarios. We find that both the fine tuning and MME models improve significantly the base model with as few as one labeled item per class coming from the target dataset, but that the MME does not compromise its performance on the source dataset.
</details>
<details>
<summary>摘要</summary>
时域天文学在实时处理大量数据方面升级，导致多流机器学习模型的发展。在这项工作中，我们研究天文学警报真假分类中的领域适应（DA），使用四个不同的数据集：HiTS、DES、ATLAS和ZTF。我们研究这些数据集之间的领域差异，并通过微调和半supervised深度DA via Minimax Entropy（MME）提高了基本模型的性能。我们对不同的源目标场景进行比较，发现两者都可以大幅提高基本模型，但MME不会在目标集中妥协性能。
</details></li>
</ul>
<hr>
<h2 id="KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification"><a href="#KMF-Knowledge-Aware-Multi-Faceted-Representation-Learning-for-Zero-Shot-Node-Classification" class="headerlink" title="KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification"></a>KMF: Knowledge-Aware Multi-Faceted Representation Learning for Zero-Shot Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08563">http://arxiv.org/abs/2308.08563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Likang Wu, Junji Jiang, Hongke Zhao, Hao Wang, Defu Lian, Mengdi Zhang, Enhong Chen</li>
<li>for:  zero-shot node classification (ZNC) task in graph data analysis, to predict nodes from unseen classes</li>
<li>methods:  Knowledge-Aware Multi-Faceted (KMF) framework that enhances label semantics via extracted KG-based topics, and reconstructs node content to a topic-level representation</li>
<li>results:  extensive experiments on several public graph datasets, demonstrating effectiveness and generalization of KMF compared to state-of-the-art baselines, and an application of zero-shot cross-domain recommendation.<details>
<summary>Abstract</summary>
Recently, Zero-Shot Node Classification (ZNC) has been an emerging and crucial task in graph data analysis. This task aims to predict nodes from unseen classes which are unobserved in the training process. Existing work mainly utilizes Graph Neural Networks (GNNs) to associate features' prototypes and labels' semantics thus enabling knowledge transfer from seen to unseen classes. However, the multi-faceted semantic orientation in the feature-semantic alignment has been neglected by previous work, i.e. the content of a node usually covers diverse topics that are relevant to the semantics of multiple labels. It's necessary to separate and judge the semantic factors that tremendously affect the cognitive ability to improve the generality of models. To this end, we propose a Knowledge-Aware Multi-Faceted framework (KMF) that enhances the richness of label semantics via the extracted KG (Knowledge Graph)-based topics. And then the content of each node is reconstructed to a topic-level representation that offers multi-faceted and fine-grained semantic relevancy to different labels. Due to the particularity of the graph's instance (i.e., node) representation, a novel geometric constraint is developed to alleviate the problem of prototype drift caused by node information aggregation. Finally, we conduct extensive experiments on several public graph datasets and design an application of zero-shot cross-domain recommendation. The quantitative results demonstrate both the effectiveness and generalization of KMF with the comparison of state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
近期，零批节点分类（ZNC）已成为图数据分析中emerging和重要的任务。这个任务的目标是从训练过程中未见过的类型中预测节点。现有的工作主要利用图神经网络（GNNs）来协调特征的抽象和标签的 semantics，从而实现知识传递从见到未见类型。然而，先前的工作忽略了多元的semantic orientation在特征-semanticAlignment中，即节点的内容通常涵盖多个标签的 semantics，这些标签相关的多个话题。为了提高模型的通用性，我们提出了一种知识具有Multi-Faceted框架（KMF），该框架可以提高标签的 semantics richness via 提取的知识图（KG）基于话题。然后，每个节点的内容被重建到话题级别的表示，这种表示提供了多个标签的多元和细化的semantic relevancy。由于图的实例（即节点）表示的特殊性，我们开发了一种 novel geometric constraint来缓解由节点信息汇集所引起的 prototype drift问题。最后，我们在多个公共图据集上进行了广泛的实验，并设计了零shot Cross-Domain recommender。量化结果表明，KMF的效果和通用性在比较现有的基准下都显著。
</details></li>
</ul>
<hr>
<h2 id="Projection-Free-Methods-for-Stochastic-Simple-Bilevel-Optimization-with-Convex-Lower-level-Problem"><a href="#Projection-Free-Methods-for-Stochastic-Simple-Bilevel-Optimization-with-Convex-Lower-level-Problem" class="headerlink" title="Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem"></a>Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07536">http://arxiv.org/abs/2308.07536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jincheng Cao, Ruichen Jiang, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, Aryan Mokhtari</li>
<li>For: 该论文研究了一类随机二级优化问题，即随机简单二级优化问题，其中我们寻找一个最优的解，满足一个随机对象函数的最小化。* Methods: 我们提出了一种新的随机二级优化方法，利用随机割辑来近似下一级问题的解，然后通过条件梯度更新和减少误差来控制随机导数引起的错误。* Results: 我们证明了，对于凹函数上的上一级问题，我们的方法需要 $\tilde{\mathcal{O}(\max{1&#x2F;\epsilon_f^{2},1&#x2F;\epsilon_g^{2}})$ 随机票 query，以获得一个 $\epsilon_f$-优化的上一级解和 $\epsilon_g$-优化的下一级解。这个证明超越了之前的最佳记录 $\mathcal{O}(\max{1&#x2F;\epsilon_f^{4},1&#x2F;\epsilon_g^{4}})$. 更进一步，对于非凹函数上的上一级问题，我们的方法需要最多 $\tilde{\mathcal{O}(\max{1&#x2F;\epsilon_f^{3},1&#x2F;\epsilon_g^{3}})$ 随机票 query，以找到一个 $(\epsilon_f, \epsilon_g)$-静点。在finite-sum设定下，我们证明了我们的方法需要 $\tilde{\mathcal{O}(\sqrt{n}&#x2F;\epsilon)$ 和 $\tilde{\mathcal{O}(\sqrt{n}&#x2F;\epsilon^{2})$ 随机票 query，它们取决于对象函数是 convex 还是 non-convex。<details>
<summary>Abstract</summary>
In this paper, we study a class of stochastic bilevel optimization problems, also known as stochastic simple bilevel optimization, where we minimize a smooth stochastic objective function over the optimal solution set of another stochastic convex optimization problem. We introduce novel stochastic bilevel optimization methods that locally approximate the solution set of the lower-level problem via a stochastic cutting plane, and then run a conditional gradient update with variance reduction techniques to control the error induced by using stochastic gradients. For the case that the upper-level function is convex, our method requires $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\}) $ stochastic oracle queries to obtain a solution that is $\epsilon_f$-optimal for the upper-level and $\epsilon_g$-optimal for the lower-level. This guarantee improves the previous best-known complexity of $\mathcal{O}(\max\{1/\epsilon_f^{4},1/\epsilon_g^{4}\})$. Moreover, for the case that the upper-level function is non-convex, our method requires at most $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{3},1/\epsilon_g^{3}\}) $ stochastic oracle queries to find an $(\epsilon_f, \epsilon_g)$-stationary point. In the finite-sum setting, we show that the number of stochastic oracle calls required by our method are $\tilde{\mathcal{O}(\sqrt{n}/\epsilon)$ and $\tilde{\mathcal{O}(\sqrt{n}/\epsilon^{2})$ for the convex and non-convex settings, respectively, where $\epsilon=\min \{\epsilon_f,\epsilon_g\}$.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究一类随机二重优化问题，即随机简单二重优化问题，其中我们寻找最优解集的最小值。我们提出了一种新的随机二重优化方法，使用随机割辑来近似下一级问题的解集，然后运行一个 conditional gradient update 的方法来控制由随机导数引起的误差。对于凸Upper-level函数的情况，我们的方法需要 $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{2},1/\epsilon_g^{2}\}) $ 随机oracle查询来获得一个 $\epsilon_f$-优化的上一级解和 $\epsilon_g$-优化的下一级解。这个 garantía提高了之前的最佳 Complexity 的 $\mathcal{O}(\max\{1/\epsilon_f^{4},1/\epsilon_g^{4}\})$.而对于非凸 Upper-level函数的情况，我们的方法需要最多 $\tilde{\mathcal{O}(\max\{1/\epsilon_f^{3},1/\epsilon_g^{3}\}) $ 随机oracle查询来找到一个$(\epsilon_f, \epsilon_g)$-静点。在finite-sum设定下，我们证明了我们的方法需要 $\tilde{\mathcal{O}(\sqrt{n}/\epsilon)$ 和 $\tilde{\mathcal{O}(\sqrt{n}/\epsilon^{2})$ 随机oracle查询，其中 $\epsilon = \min \{\epsilon_f, \epsilon_g\}$.
</details></li>
</ul>
<hr>
<h2 id="Inverse-Lithography-Physics-informed-Deep-Neural-Level-Set-for-Mask-Optimization"><a href="#Inverse-Lithography-Physics-informed-Deep-Neural-Level-Set-for-Mask-Optimization" class="headerlink" title="Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization"></a>Inverse Lithography Physics-informed Deep Neural Level Set for Mask Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12299">http://arxiv.org/abs/2308.12299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing-Yu Ma, Shaogang Hao</li>
<li>for: 这篇论文主要是为了提出一种基于深度学习的 inverse lithography physics-informed deep neural level set（ILDLS）方法，用于mask优化。</li>
<li>methods: 这篇论文使用了深度学习（DL）方法，并将 inverse lithography physics  incorporated into DL 框架中。具体来说，这篇论文使用了 level set 基于的 inverse lithography technology（ILT）作为层，并在这个层中进行了mask预测和修正。</li>
<li>results: 相比于纯度DL和ILT，ILDLS可以减少计算时间的数个数量级，同时提高了印刷可能性和过程窗口（PW）。这篇论文的结果表明，ILDLS可以提供一种高效的mask优化解决方案。<details>
<summary>Abstract</summary>
As the feature size of integrated circuits continues to decrease, optical proximity correction (OPC) has emerged as a crucial resolution enhancement technology for ensuring high printability in the lithography process. Recently, level set-based inverse lithography technology (ILT) has drawn considerable attention as a promising OPC solution, showcasing its powerful pattern fidelity, especially in advanced process. However, massive computational time consumption of ILT limits its applicability to mainly correcting partial layers and hotspot regions. Deep learning (DL) methods have shown great potential in accelerating ILT. However, lack of domain knowledge of inverse lithography limits the ability of DL-based algorithms in process window (PW) enhancement and etc. In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set based-ILT as a layer within the DL framework and iteratively conducts mask prediction and correction to significantly enhance printability and PW in comparison with results from pure DL and ILT. With this approach, computation time is reduced by a few orders of magnitude versus ILT. By gearing up DL with knowledge of inverse lithography physics, ILDLS provides a new and efficient mask optimization solution.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an inverse lithography physics-informed deep neural level set (ILDLS) approach for mask optimization. This approach utilizes level set-based ILT as a layer within the DL framework and iteratively conducts mask prediction and correction to significantly enhance printability and PW in comparison with results from pure DL and ILT. With this approach, computation time is reduced by a few orders of magnitude versus ILT. By combining DL with knowledge of inverse lithography physics, ILDLS provides a new and efficient mask optimization solution.
</details></li>
</ul>
<hr>
<h2 id="FeatGeNN-Improving-Model-Performance-for-Tabular-Data-with-Correlation-based-Feature-Extraction"><a href="#FeatGeNN-Improving-Model-Performance-for-Tabular-Data-with-Correlation-based-Feature-Extraction" class="headerlink" title="FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction"></a>FeatGeNN: Improving Model Performance for Tabular Data with Correlation-based Feature Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07527">http://arxiv.org/abs/2308.07527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sammuel Ramos Silva, Rodrigo Silva</li>
<li>for: 提高机器学习模型性能和统计分析中获取更多信息</li>
<li>methods: 使用 corr 函数作为池化函数，从数据矩阵中提取和创建新特征</li>
<li>results: 在多个标准测试集上比较 FeatGeNN 与现有 AutoFE 方法，显示 FeatGeNN 可以提高模型性能。 corr 函数可以作为 tabular 数据中 pooling 函数的有力的替代方案。<details>
<summary>Abstract</summary>
Automated Feature Engineering (AutoFE) has become an important task for any machine learning project, as it can help improve model performance and gain more information for statistical analysis. However, most current approaches for AutoFE rely on manual feature creation or use methods that can generate a large number of features, which can be computationally intensive and lead to overfitting. To address these challenges, we propose a novel convolutional method called FeatGeNN that extracts and creates new features using correlation as a pooling function. Unlike traditional pooling functions like max-pooling, correlation-based pooling considers the linear relationship between the features in the data matrix, making it more suitable for tabular data. We evaluate our method on various benchmark datasets and demonstrate that FeatGeNN outperforms existing AutoFE approaches regarding model performance. Our results suggest that correlation-based pooling can be a promising alternative to max-pooling for AutoFE in tabular data applications.
</details>
<details>
<summary>摘要</summary>
自动Feature工程（AutoFE）已成为机器学习项目中重要的任务，因为它可以提高模型性能并提供更多的统计分析信息。然而，现有的AutoFE方法大多依赖于手动创建特征或使用生成大量特征的方法，这可能会占用大量计算资源并导致过拟合。为解决这些挑战，我们提出了一种新的卷积方法 called FeatGeNN，它通过对数据矩阵中特征之间的相关性进行抽象，从而生成新的特征。与传统的最大值抽取函数不同，相关性基于的抽取函数更适合逻辑数据。我们在多个 benchmark 数据集上评估了我们的方法，并证明了 FeatGeNN 在AutoFE中超过了现有的方法。我们的结果表明，相关性基于的抽取函数可以成为 tabular 数据应用中 AutoFE 中的一个有前途的代替方案。
</details></li>
</ul>
<hr>
<h2 id="Potential-of-Deep-Operator-Networks-in-Digital-Twin-enabling-Technology-for-Nuclear-System"><a href="#Potential-of-Deep-Operator-Networks-in-Digital-Twin-enabling-Technology-for-Nuclear-System" class="headerlink" title="Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System"></a>Potential of Deep Operator Networks in Digital Twin-enabling Technology for Nuclear System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07523">http://arxiv.org/abs/2308.07523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuma Kobayashi, Syed Bahauddin Alam</li>
<li>for: 这个研究旨在提出一种可靠且高精度的数据零层级模型（DeepONet），用于数位双（DT）系统中的核工程应用。</li>
<li>methods: 这个研究使用DeepONet方法，将函数作为输入数据，从训练数据中构建了算子G。</li>
<li>results: DeepONet方法在解决困难的粒子运输问题上展现了杰出的预测精度，比传统机器学习方法更高。<details>
<summary>Abstract</summary>
This research introduces the Deep Operator Network (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) systems for nuclear engineering. With the increasing importance of nuclear energy as a carbon-neutral solution, adopting DT technology has become crucial to enhancing operational efficiencies, safety, and predictive capabilities in nuclear engineering applications. DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods. Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem. By taking functions as input data and constructing the operator $G$ from training data, DeepONet can handle diverse and complex scenarios effectively. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing these challenges will further enhance the method's practicality and reliability. Overall, DeepONet presents a promising and transformative tool for nuclear engineering research and applications. Its accurate prediction and computational efficiency capabilities can revolutionize DT systems, advancing nuclear engineering research. This study marks an important step towards harnessing the power of surrogate modeling techniques in critical engineering domains.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这项研究介绍了深度操作网络（DeepONet）作为核动力工程中数字双（DT）系统中的稳定和准确的模拟方法。随着核能作为碳中和解方案的重要性增加，采用DT技术已经成为核动力工程应用中提高操作效率、安全性和预测能力的关键。DeepONet在多个核心问题上表现出了惊人的预测精度，超越传统机器学习方法。通过广泛的 benchmarking 和评估，这项研究展示了 DeepONet 在解决复杂的粒子传输问题时的扩展性和计算效率。通过将函数作为输入数据，从训练数据中构建操作符 $G$，DeepONet 可以有效地处理多样化和复杂的场景。然而， DeepONet 的应用也暴露了优化传感器布局和模型评估的挑战，这些挑战在实际应用中是关键的。解决这些挑战将进一步提高 DeepONet 的实用性和可靠性。总的来说，DeepONet 提供了核动力工程研究和应用中的一个可靠和转型的工具。它的准确预测和计算效率能力可以对 DT 系统进行革命性的改进，推动核动力工程研究。这项研究标志着使用表达式模拟技术在关键工程领域的应用的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning"><a href="#Nonlinearity-Feedback-and-Uniform-Consistency-in-Causal-Structural-Learning" class="headerlink" title="Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning"></a>Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07520">http://arxiv.org/abs/2308.07520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyan Wang</li>
<li>for: 这个论文的目的是找到自动搜索方法，用于从观察数据中学习 causal structure。</li>
<li>methods: 这个论文使用的方法包括 modifying Strong Faithfulness 和 relaxing sufficiency assumption，以扩展 causal discovery 方法的应用范围。</li>
<li>results: 这个论文的研究结果表明，通过 relaxing 简化假设，可以扩展 causal discovery 方法的应用范围，并且可以学习 causal structure with latent variables。<details>
<summary>Abstract</summary>
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfulness when applied to the Gaussian family of distributions, (ii) can be applied to non-Gaussian families of distributions, and (iii) under the assumption that the modified version of Strong Faithfulness holds, can be used to show the uniform consistency of a modified causal discovery algorithm; relaxing the sufficiency assumption to learn causal structures with latent variables. Given the importance of inferring cause-and-effect relationships for understanding and forecasting complex systems, the work in this thesis of relaxing various simplification assumptions is expected to extend the causal discovery method to be applicable in a wider range with diversified causal mechanism and statistical phenomena.
</details>
<details>
<summary>摘要</summary>
目的是找到自动搜寻方法，以了解观察资料中的 causal 结构。在某些情况下，所有兴趣的 causal 机制中的所有变量都是观察到的，而任务是预测一个观察到的变量对另一个变量的影响。相比之下，有时候兴趣的变量不是直接观察到的，而是从资料中的现象推断出来的。这些被称为潜在变量。例如，心理学中的智商不能直接观察，因此研究人员会尝试透过不同的指标，如智商测验，来评估。在这种情况下， causal 发现算法可以探测到底层结构和联系，以显示 causal 连接between latent 变量和观察到的变量。本论文关注两个问题：提供一个弱 faithfulness 定义，其中（i）在 Gaussian 家族中的分布下是弱 faithfulness 的，（ii）可以应用到非 Gaussian 家族的分布，以及（iii）在 modified 稳定假设下，可以用来证明一个修改版的 causal 发现算法的均匀一致性。将适当的假设放宽，以学习包含潜在变量的 causal 结构。由于推断 causal 关系的重要性，这项工作预期能够将 causal 发现方法扩展到更广泛的应用，包括多元的 causal 机制和 statistically 现象。
</details></li>
</ul>
<hr>
<h2 id="Distilling-Knowledge-from-Resource-Management-Algorithms-to-Neural-Networks-A-Unified-Training-Assistance-Approach"><a href="#Distilling-Knowledge-from-Resource-Management-Algorithms-to-Neural-Networks-A-Unified-Training-Assistance-Approach" class="headerlink" title="Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach"></a>Distilling Knowledge from Resource Management Algorithms to Neural Networks: A Unified Training Assistance Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07511">http://arxiv.org/abs/2308.07511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longfei Ma, Nan Cheng, Xiucheng Wang, Zhisheng Yin, Haibo Zhou, Wei Quan</li>
<li>for: 提高多用户通信系统中信号干扰比例（SINR）优化的精度和速度。</li>
<li>methods: 使用知识储存（KD）技术和人工神经网络（NN）结合 traditional SINR 优化方法，以提高无监督和强化学习方法的性能和速度。</li>
<li>results: 在模拟结果中，提出的AD方法比传统学习方法更高效，并且能够解决无监督学习和强化学习中常见的问题，如获取优质解决方案和避免过度适应。<details>
<summary>Abstract</summary>
As a fundamental problem, numerous methods are dedicated to the optimization of signal-to-interference-plus-noise ratio (SINR), in a multi-user setting. Although traditional model-based optimization methods achieve strong performance, the high complexity raises the research of neural network (NN) based approaches to trade-off the performance and complexity. To fully leverage the high performance of traditional model-based methods and the low complexity of the NN-based method, a knowledge distillation (KD) based algorithm distillation (AD) method is proposed in this paper to improve the performance and convergence speed of the NN-based method, where traditional SINR optimization methods are employed as ``teachers" to assist the training of NNs, which are ``students", thus enhancing the performance of unsupervised and reinforcement learning techniques. This approach aims to alleviate common issues encountered in each of these training paradigms, including the infeasibility of obtaining optimal solutions as labels and overfitting in supervised learning, ensuring higher convergence performance in unsupervised learning, and improving training efficiency in reinforcement learning. Simulation results demonstrate the enhanced performance of the proposed AD-based methods compared to traditional learning methods. Remarkably, this research paves the way for the integration of traditional optimization insights and emerging NN techniques in wireless communication system optimization.
</details>
<details>
<summary>摘要</summary>
Traditional model-based optimization methods have been widely used to optimize signal-to-interference-plus-noise ratio (SINR) in multi-user settings, but these methods are often complex and have high computational complexity. To address this issue, this paper proposes a knowledge distillation (KD) based algorithm distillation (AD) method that combines the strengths of traditional model-based methods and neural network (NN) based approaches. The proposed method uses traditional SINR optimization methods as "teachers" to assist the training of NNs, which are "students", to improve the performance and convergence speed of the NN-based method. This approach can alleviate common issues encountered in each of these training paradigms, such as the infeasibility of obtaining optimal solutions as labels and overfitting in supervised learning, ensuring higher convergence performance in unsupervised learning, and improving training efficiency in reinforcement learning. Simulation results show that the proposed AD-based methods outperform traditional learning methods, paving the way for the integration of traditional optimization insights and emerging NN techniques in wireless communication system optimization.Here's the word-for-word translation of the text into Simplified Chinese:多种方法都是为了优化信号干扰比例（SINR）的多用户设置中的问题。虽然传统的模型基于优化方法具有强大的表现，但它们的计算复杂性高。为了解决这个问题，这篇文章提出了知识储备（KD）基于算法储备（AD）方法。该方法使用传统的SINR优化方法作为“教师”，以帮助训练神经网络（NN），作为“学生”，从而提高NN的性能和速度。这种方法可以解决每个训练模式中的常见问题，如获得优化解为标签的不可能性和超参数过敏，确保更高的整合性表现，并提高循环学习的训练效率。实验结果表明，提议的AD-based方法比传统的学习方法更高效。这些研究开创了传统优化思想和新兴神经网络技术在无线通信系统优化中的集成。
</details></li>
</ul>
<hr>
<h2 id="Data-Race-Detection-Using-Large-Language-Models"><a href="#Data-Race-Detection-Using-Large-Language-Models" class="headerlink" title="Data Race Detection Using Large Language Models"></a>Data Race Detection Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07505">http://arxiv.org/abs/2308.07505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Le Chen, Xianzhong Ding, Murali Emani, Tristan Vanderbruggen, Pei-hung Lin, Chuanhua Liao</li>
<li>for: 本文旨在探讨使用大语言模型（LLMs）来检测数据竞争问题，以代替手动创建资源密集的工具。</li>
<li>methods: 本文提出了一种基于提示工程和精度调整技术的新的数据竞争检测方法，使用了特制的DRB-ML数据集，并对代表性的LLMs和开源LLMs进行了评估和精度调整。</li>
<li>results: 实验表明，LLMs可以成为数据竞争检测的可能的方法，但是它们仍无法与传统的数据竞争检测工具相比提供详细的变量对 causing数据竞争的信息。<details>
<summary>Abstract</summary>
Large language models (LLMs) are demonstrating significant promise as an alternate strategy to facilitate analyses and optimizations of high-performance computing programs, circumventing the need for resource-intensive manual tool creation. In this paper, we explore a novel LLM-based data race detection approach combining prompting engineering and fine-tuning techniques. We create a dedicated dataset named DRB-ML, which is derived from DataRaceBench, with fine-grain labels showing the presence of data race pairs and their associated variables, line numbers, and read/write information. DRB-ML is then used to evaluate representative LLMs and fine-tune open-source ones. Our experiment shows that LLMs can be a viable approach to data race detection. However, they still cannot compete with traditional data race detection tools when we need detailed information about variable pairs causing data races.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）正在展示出很大的承诺，用作高性能计算程序分析和优化的代替策略，减少手动工具的创建成本。在这篇论文中，我们探索了一种基于提示工程和精细调整技术的新型LLM数据竞争检测方法。我们创建了一个专门的数据集名为DRB-ML，它是基于DataRaceBench的数据集，并具有精细的标签，显示数据竞争对的存在、相关变量、行号、读写信息等。然后，我们使用DRB-ML评估了一些代表性的LLM，并对开源LLM进行了精细调整。我们的实验表明，LLM可以成为数据竞争检测的有效方法，但是它们仍无法与传统的数据竞争检测工具相比，提供详细的变量对 causing 数据竞争的信息。
</details></li>
</ul>
<hr>
<h2 id="ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting"><a href="#ST-MLP-A-Cascaded-Spatio-Temporal-Linear-Framework-with-Channel-Independence-Strategy-for-Traffic-Forecasting" class="headerlink" title="ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting"></a>ST-MLP: A Cascaded Spatio-Temporal Linear Framework with Channel-Independence Strategy for Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07496">http://arxiv.org/abs/2308.07496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zepu Wang, Yuqi Nie, Peng Sun, Nam H. Nguyen, John Mulvey, H. Vincent Poor</li>
<li>For: 本研究旨在提高智能交通系统中的流量管理优化，通过提出一种简洁准确的交通预测模型。* Methods: 本文提出了一种基于多层感知器（MLP）模块和线性层的简洁空间时间图 neural network（STGNN）模型，利用时间信息、空间信息和预定的图结构，并实现了通道独立策略。* Results: 实验结果显示，ST-MLP模型在准确率和计算效率两个方面都有较高的表现，比其他模型和现有的STGNNs架构更高。<details>
<summary>Abstract</summary>
The criticality of prompt and precise traffic forecasting in optimizing traffic flow management in Intelligent Transportation Systems (ITS) has drawn substantial scholarly focus. Spatio-Temporal Graph Neural Networks (STGNNs) have been lauded for their adaptability to road graph structures. Yet, current research on STGNNs architectures often prioritizes complex designs, leading to elevated computational burdens with only minor enhancements in accuracy. To address this issue, we propose ST-MLP, a concise spatio-temporal model solely based on cascaded Multi-Layer Perceptron (MLP) modules and linear layers. Specifically, we incorporate temporal information, spatial information and predefined graph structure with a successful implementation of the channel-independence strategy - an effective technique in time series forecasting. Empirical results demonstrate that ST-MLP outperforms state-of-the-art STGNNs and other models in terms of accuracy and computational efficiency. Our finding encourages further exploration of more concise and effective neural network architectures in the field of traffic forecasting.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "prompt and precise" is translated as "快速和准确" (kuài sù and zhèng jí) to emphasize the importance of timeliness and accuracy in traffic forecasting.* "Spatio-Temporal Graph Neural Networks" is translated as "空间时间图 neural network" (kōng jiān shí jiān tiě xiào) to emphasize the graph structure and the combination of spatial and temporal information.* " Multi-Layer Perceptron" is translated as "多层感知器" (duō cèng kàn shì qì) to emphasize the hierarchical structure of the model.* "channel-independence strategy" is translated as "通道独立策略" (tōng dào dāo lì bàng yì) to emphasize the technique's ability to improve the model's performance without relying on specific channel information.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Tracking-of-a-Single-Rigid-Body-Character-in-Various-Environments"><a href="#Adaptive-Tracking-of-a-Single-Rigid-Body-Character-in-Various-Environments" class="headerlink" title="Adaptive Tracking of a Single-Rigid-Body Character in Various Environments"></a>Adaptive Tracking of a Single-Rigid-Body Character in Various Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07491">http://arxiv.org/abs/2308.07491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taesoo Kwon, Taehong Gu, Jaewon Ahn, Yoonsang Lee</li>
<li>for:  This paper proposes a deep reinforcement learning method for simulating full-body human motions in various scenarios, with the goal of adapting to unobserved environmental changes and controller transitions without requiring additional learning.</li>
<li>methods:  The proposed method uses the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and trains a policy to track a reference motion using deep reinforcement learning. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows the SRB character to follow the reference motion.</li>
<li>results:  The proposed method is demonstrated to be sample-efficient and able to cope with environments that have not been experienced during learning, such as running on uneven terrain or pushing a box, and transitions between learned policies, without any additional learning. The policy can be efficiently trained within 30 minutes on an ultraportable laptop.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文提出了一种基于深度学习的人体全身动作模拟方法，以适应不同的环境和控制器转换，无需进行额外的学习。</li>
<li>methods: 该方法使用中心动力学模型（CDM）表示全身人体为单一静体（SRB），并通过深度强化学习训练一个策略来跟踪参照动作。SRB模拟被形式化为quadratic programming（QP）问题，策略输出一个动作，使SRB人体按照参照动作进行动作。</li>
<li>results: 该方法能够快速地在ultraportable笔记本电脑上进行高效地训练，并在不同的环境中进行适应，如跑在不平的地面上或推Push一个箱子，以及控制器转换。<details>
<summary>Abstract</summary>
Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows the SRB character to follow the reference motion. We demonstrate that our policy, efficiently trained within 30 minutes on an ultraportable laptop, has the ability to cope with environments that have not been experienced during learning, such as running on uneven terrain or pushing a box, and transitions between learned policies, without any additional learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: desde la introducción de DeepMimic [Peng et al. 2018], la investigación subsiguiente se ha centrado en expandir el repertorio de movimientos simulados en diversas escenarios. En este estudio, proponemos un enfoque alternativo para lograr esto, un método de aprendizaje por refuerzo profundo basado en la simulación de un cuerpo rígido único (SRB). Usando el modelo de dinámica centroidal (CDM) para expresar el cuerpo rígido completo como un SRB y entrenar una política para seguir una referencia de movimiento, podemos obtener una política que es capaz de adaptarse a cambios ambientales no observados y transiciones de controlador sin necesidad de aprendizaje adicional. Debido a la reducción de la dimensión del espacio de estado y acción, el proceso de aprendizaje es eficiente en muestras. El movimiento final es generado de manera plausible físicamente, basado en el estado de la simulación del SRB. La simulación del SRB se formulates como un problema de programación cuadrática (QP), y la política devuelve una acción que permite al SRB seguir la referencia de movimiento. Demostramos que nuestra política, entrenada eficientemente dentro de 30 minutos en un portátil ultra, tiene la capacidad de adaptarse a entornos que no se han experimentado durante el aprendizaje, como correr sobre terreno irregular o empujar una caja, y transiciones entre políticas aprendidas, sin aprendizaje adicional.
</details></li>
</ul>
<hr>
<h2 id="O-1-Self-training-with-Oracle-and-1-best-Hypothesis"><a href="#O-1-Self-training-with-Oracle-and-1-best-Hypothesis" class="headerlink" title="O-1: Self-training with Oracle and 1-best Hypothesis"></a>O-1: Self-training with Oracle and 1-best Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07486">http://arxiv.org/abs/2308.07486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murali Karthick Baskar, Andrew Rosenberg, Bhuvana Ramabhadran, Kartik Audhkhasi</li>
<li>for: 本研究旨在提出一种新的自教程目标函数O-1，以减少训练偏见和统一训练和评估 метри。</li>
<li>methods: O-1是EMBR的快速变体，可以使用 Both supervised和Unsupervised数据，并且可以提高 oracle假设。</li>
<li>results: 对于SpeechStew数据集和一个大规模的内部数据集，O-1对识别效果有13%-25%的相对提升，与EMBR相比，O-1在不同的SpeechStew数据集上提高了80%的相对幅度，并在内部数据集上与oracle WER之间减少了12%的差距。总的来说，O-1在大规模数据集上实现了9%的相对提升。<details>
<summary>Abstract</summary>
We introduce O-1, a new self-training objective to reduce training bias and unify training and evaluation metrics for speech recognition. O-1 is a faster variant of Expected Minimum Bayes Risk (EMBR), that boosts the oracle hypothesis and can accommodate both supervised and unsupervised data. We demonstrate the effectiveness of our approach in terms of recognition on publicly available SpeechStew datasets and a large-scale, in-house data set. On Speechstew, the O-1 objective closes the gap between the actual and oracle performance by 80\% relative compared to EMBR which bridges the gap by 43\% relative. O-1 achieves 13\% to 25\% relative improvement over EMBR on the various datasets that SpeechStew comprises of, and a 12\% relative gap reduction with respect to the oracle WER over EMBR training on the in-house dataset. Overall, O-1 results in a 9\% relative improvement in WER over EMBR, thereby speaking to the scalability of the proposed objective for large-scale datasets.
</details>
<details>
<summary>摘要</summary>
我们引入O-1，一个新的自我训练目标，以减少训练偏见和统一训练和评估度量 для语音识别。O-1是EMBR的更快版本，可以提高oracle假设和处理bothsupervised和无监督数据。我们透过使用O-1目标，在公开ailable的SpeechStew数据集和大规模内部数据集上显示了效果。在SpeechStew上，O-1目标可以与oracle性能相对比较，将实际性能与oracle性能之间的差距降低了80%相对数据。在SpeechStew的不同数据集上，O-1目标可以与EMBR目标相比，实现13%至25%的相对改善，并且对于内部数据集的oracle WER进行了12%的相对降低。总之，O-1目标对EMBR目标的9%相对改善，说明了这个目标的扩展性。
</details></li>
</ul>
<hr>
<h2 id="OCDaf-Ordered-Causal-Discovery-with-Autoregressive-Flows"><a href="#OCDaf-Ordered-Causal-Discovery-with-Autoregressive-Flows" class="headerlink" title="OCDaf: Ordered Causal Discovery with Autoregressive Flows"></a>OCDaf: Ordered Causal Discovery with Autoregressive Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07480">http://arxiv.org/abs/2308.07480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vahidzee/ocdaf">https://github.com/vahidzee/ocdaf</a></li>
<li>paper_authors: Hamidreza Kamkari, Vahid Zehtab, Vahid Balazadeh, Rahul G. Krishnan</li>
<li>for: 学习 causal graphs 从观察数据中学习 causal graphs</li>
<li>methods: 使用 order-based 方法，通过 continuous search 算法找到 causal structures</li>
<li>results: 在 Sachs 和 SynTReN benchmark 上达到 state-of-the-art 性能，并在多种 parametic 和 nonparametric synthetic datasets 中验证了identifiability theory 的正确性。<details>
<summary>Abstract</summary>
We propose OCDaf, a novel order-based method for learning causal graphs from observational data. We establish the identifiability of causal graphs within multivariate heteroscedastic noise models, a generalization of additive noise models that allow for non-constant noise variances. Drawing upon the structural similarities between these models and affine autoregressive normalizing flows, we introduce a continuous search algorithm to find causal structures. Our experiments demonstrate state-of-the-art performance across the Sachs and SynTReN benchmarks in Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). Furthermore, we validate our identifiability theory across various parametric and nonparametric synthetic datasets and showcase superior performance compared to existing baselines.
</details>
<details>
<summary>摘要</summary>
我们提出了OCDaf，一种基于顺序的方法，用于从观察数据中学习 causal 图。我们证明了 causal 图在多变量非常性噪声模型中可以uniquely 特征标识，这是常量噪声模型的推广。基于这些模型和 afine autoregressive normalizing flows 的结构相似性，我们引入了连续搜索算法来找到 causal 结构。我们的实验表明在 Sachs 和 SynTReN benchmark 上的状态当前性表现，以及在 Structural Hamming Distance (SHD) 和 Structural Intervention Distance (SID) 中的优秀表现。此外，我们还验证了我们的可 identificability 理论，并在不同的参数和非参数 synthetic 数据上显示了superior的表现，与现有的基准值相比。
</details></li>
</ul>
<hr>
<h2 id="Symphony-Optimized-Model-Serving-using-Centralized-Orchestration"><a href="#Symphony-Optimized-Model-Serving-using-Centralized-Orchestration" class="headerlink" title="Symphony: Optimized Model Serving using Centralized Orchestration"></a>Symphony: Optimized Model Serving using Centralized Orchestration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07470">http://arxiv.org/abs/2308.07470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lequn Chen, Weixin Deng, Anirudh Canumalla, Yu Xin, Matthai Philipose, Arvind Krishnamurthy</li>
<li>for: 提高深度神经网络（DNN）模型推理的加速率和服务级别目标（SLO）。</li>
<li>methods: 使用中央化调度系统，可以扩展到百万个请求每秒，并将万个GPU进行协调。使用非工作保存的调度算法，可以实现高批处理效率，同时也可以启用灵活自适应扩展。</li>
<li>results: 通过广泛的实验表明，Symphony系统比前一代系统高效性可以达到4.7倍。<details>
<summary>Abstract</summary>
The orchestration of deep neural network (DNN) model inference on GPU clusters presents two significant challenges: achieving high accelerator efficiency given the batching properties of model inference while meeting latency service level objectives (SLOs), and adapting to workload changes both in terms of short-term fluctuations and long-term resource allocation. To address these challenges, we propose Symphony, a centralized scheduling system that can scale to millions of requests per second and coordinate tens of thousands of GPUs. Our system utilizes a non-work-conserving scheduling algorithm capable of achieving high batch efficiency while also enabling robust autoscaling. Additionally, we developed an epoch-scale algorithm that allocates models to sub-clusters based on the compute and memory needs of the models. Through extensive experiments, we demonstrate that Symphony outperforms prior systems by up to 4.7x higher goodput.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）模型推理在GPU集群中的协调表现两大挑战：实现批处理性质下的加速器效率，同时满足响应时间服务水平目标（SLO）。为解决这些挑战，我们提议了Symphony，一个可扩展到百万个请求每秒的中央调度系统，可以协调数万个GPU。我们的系统使用非工作保持式调度算法，可以实现高批处理效率，同时也允许自动扩缩。此外，我们还开发了一种时间尺度算法，将模型分配到子集群基于计算和内存需求。通过广泛的实验，我们证明Symphony比先前系统高效性更高，最高可以达4.7倍。
</details></li>
</ul>
<hr>
<h2 id="Omega-Regular-Reward-Machines"><a href="#Omega-Regular-Reward-Machines" class="headerlink" title="Omega-Regular Reward Machines"></a>Omega-Regular Reward Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07469">http://arxiv.org/abs/2308.07469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Moritz Hahn, Mateo Perez, Sven Schewe, Fabio Somenzi, Ashutosh Trivedi, Dominik Wojtczak</li>
<li>for: 这篇论文旨在探讨如何透过奖励机制来训练智能代理人（Agent）来完成任务，但是设计合适的奖励机制是训练成功的关键。</li>
<li>methods: 这篇论文使用了奖励机制的两种形式：奖励机器和欧姆regular语言，以表达不同类型的学习目标。</li>
<li>results: 论文提出了奖励机器和欧姆regular语言的 интеграción，以实现更加表达力和有效的奖励机制，并提出了一种基于模型自由学习算法的ε-优化策略来对奖励机器进行计算。通过实验，证明了提出的方法的有效性。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) is a powerful approach for training agents to perform tasks, but designing an appropriate reward mechanism is critical to its success. However, in many cases, the complexity of the learning objectives goes beyond the capabilities of the Markovian assumption, necessitating a more sophisticated reward mechanism. Reward machines and omega-regular languages are two formalisms used to express non-Markovian rewards for quantitative and qualitative objectives, respectively. This paper introduces omega-regular reward machines, which integrate reward machines with omega-regular languages to enable an expressive and effective reward mechanism for RL. We present a model-free RL algorithm to compute epsilon-optimal strategies against omega-egular reward machines and evaluate the effectiveness of the proposed algorithm through experiments.
</details>
<details>
<summary>摘要</summary>
《强化学习（RL）是一种强大的方法，用于训练代理人员执行任务，但设计合适的奖励机制是RL的成功关键。然而，在许多情况下，学习目标的复杂性超出了Markov预测的能力，需要更加复杂的奖励机制。奖励机器和ωRegular语言是两种用于表达非Markov奖励的形式主义，分别用于量化和质量目标。本文提出了ωRegular奖励机器，它将奖励机器与ωRegular语言集成，以实现RL中的表达和效果的奖励机制。我们提出了一种无模型RL算法，用于计算ε优策略对ωRegular奖励机器，并通过实验评估提出的算法的效果。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="There-Is-a-Digital-Art-History"><a href="#There-Is-a-Digital-Art-History" class="headerlink" title="There Is a Digital Art History"></a>There Is a Digital Art History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07464">http://arxiv.org/abs/2308.07464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Gracetyty/art-gallery">https://github.com/Gracetyty/art-gallery</a></li>
<li>paper_authors: Leonardo Impett, Fabian Offert</li>
<li>for: 本文重新评问 Johanna Drucker 十年前的问题：是否有一种数字艺术历史？在大规模变换器基础模型的出现下，传统类型的神经网络已经成为数字艺术历史的一部分，但是这些模型的知识价值和方法价值尚未得到系统的分析。</li>
<li>methods: 本文主要分析了两个方面：一是大规模视模型中新编码的视觉文化，对数字艺术历史有很大的影响；二是使用当今大规模视模型研究艺术史和城市规划等领域的技术案例，提出了一种新的批判方法，该方法考虑模型和其应用之间的知识杂糅。</li>
<li>results: 本文的结果表明，大规模视模型在艺术史和城市规划等领域的应用需要一种新的批判方法，该方法可以通过读取研究数据集和训练数据集的视觉意识来批判模型和其应用的知识杂糅。<details>
<summary>Abstract</summary>
In this paper, we revisit Johanna Drucker's question, "Is there a digital art history?" -- posed exactly a decade ago -- in the light of the emergence of large-scale, transformer-based vision models. While more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net visual culture, and they continuously solidify and concretize this canon through their already widespread application in all aspects of digital life. On the other hand, based on two technical case studies of utilizing a contemporary large-scale visual model to investigate basic questions from the fields of art history and urbanism, we suggest that such systems require a new critical methodology that takes into account the epistemic entanglement of a model and its applications. This new methodology reads its corpora through a neural model's training data, and vice versa: the visual ideologies of research datasets and training datasets become entangled.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新探讨了 Johanna Drucker 提出的问题：“是否有数字艺术历史？” ——  exactly a decade ago —— 在大规模变换器基础模型的出现下。 although more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net visual culture, and they continuously solidify and concretize this canon through their already widespread application in all aspects of digital life. On the other hand, based on two technical case studies of utilizing a contemporary large-scale visual model to investigate basic questions from the fields of art history and urbanism, we suggest that such systems require a new critical methodology that takes into account the epistemic entanglement of a model and its applications. This new methodology reads its corpora through a neural model's training data, and vice versa: the visual ideologies of research datasets and training datasets become entangled.Here's a word-for-word translation of the text into Simplified Chinese:在这篇论文中，我们重新探讨了 Johanna Drucker 提出的问题：“是否有数字艺术历史？” —— exact 10 年前 —— 在大规模变换器基础模型的出现下。 although more traditional types of neural networks have long been part of digital art history, and digital humanities projects have recently begun to use transformer models, their epistemic implications and methodological affordances have not yet been systematically analyzed. We focus our analysis on two main aspects that, together, seem to suggest a coming paradigm shift towards a "digital" art history in Drucker's sense. On the one hand, the visual-cultural repertoire newly encoded in large-scale vision models has an outsized effect on digital art history. The inclusion of significant numbers of non-photographic images allows for the extraction and automation of different forms of visual logics. Large-scale vision models have "seen" large parts of the Western visual canon mediated by Net visual culture, and they continuously solidify and concretize this canon through their already widespread application in all aspects of digital life. On the other hand, based on two technical case studies of utilizing a contemporary large-scale visual model to investigate basic questions from the fields of art history and urbanism, we suggest that such systems require a new critical methodology that takes into account the epistemic entanglement of a model and its applications. This new methodology reads its corpora through a neural model's training data, and vice versa: the visual ideologies of research datasets and training datasets become entangled.
</details></li>
</ul>
<hr>
<h2 id="Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis"><a href="#Inductive-Knowledge-Graph-Completion-with-GNNs-and-Rules-An-Analysis" class="headerlink" title="Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis"></a>Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07942">http://arxiv.org/abs/2308.07942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anilakash/indkgc">https://github.com/anilakash/indkgc</a></li>
<li>paper_authors: Akash Anil, Víctor Gutiérrez-Basulto, Yazmín Ibañéz-García, Steven Schockaert</li>
<li>for: 本研究旨在解释 inductive knowledge graph completion 任务中，模型如何学习推理规则，并用于预测测试图库中的链接。</li>
<li>methods: 本研究使用了规则基于的方法，并研究了一些变种来解决具体的问题。</li>
<li>results: 研究发现，变种方法可以减少不可靠的实体的影响，并且可以保持 interpretability 优势。而且，一种变种方法，可以不断地利用整个知识图，并且一直高于 NBFNet 的性能。<details>
<summary>Abstract</summary>
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which means that they largely keep the interpretability advantage of rule-based methods. Moreover, we show that a further variant, which does look at the full KG, consistently outperforms NBFNet.
</details>
<details>
<summary>摘要</summary>
任务是完成 inductive 知识图结构需要模型学习从训练图中推导出推理模式，以便在测试图上进行预测。 规则基本方法似乎适合这个任务，但在实践中它们在比基于图神经网络（GNNS）的方法下表现出现下降。我们认为这是因为两个因素：（i）不可能的实体没有被排序，（ii）只考虑最有用的路径来确定链接预测答案的信任度。为了分析这些因素的影响，我们研究了一些 variants of rule-based approach，它们专门解决这些问题。我们发现这些模型可以达到与 NBFNet 相似的性能，而且它们只使用了一小部分的证据，这意味着它们保留了规则基本方法的解释性优势。此外，我们还证明了一个 variant，它查看了整个知识图，可以一直高于 NBFNet 的性能。
</details></li>
</ul>
<hr>
<h2 id="GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction"><a href="#GRU-D-Weibull-A-Novel-Real-Time-Individualized-Endpoint-Prediction" class="headerlink" title="GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction"></a>GRU-D-Weibull: A Novel Real-Time Individualized Endpoint Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07452">http://arxiv.org/abs/2308.07452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyang Ruan, Liwei Wang, Charat Thongprayoon, Wisit Cheungpasitporn, Hongfang Liu</li>
<li>for: 这个研究的目的是提出一种新的方法，GRU-D-Weibull，用于模型Weibull分布，以实现个人化终点预测和人口水平风险管理。</li>
<li>methods: 这个方法使用了门控Recurrent Unit（GRU）和衰减（D）来模型Weibull分布，并实现了实时个人化终点预测和人口水平风险管理。</li>
<li>results: 使用了6879名CKD4阶段4患者的 cohort，我们评估了GRU-D-Weibull在终点预测中的表现。GRU-D-Weibull在终点预测中的C-指数在指定日期为<del>0.7，而后续4.3年的跟踪中为</del>0.77，与随机生存树相当。我们的方法实现了终点预测中的绝对L1损失为<del>1.1年（SD 0.95），并在4年的跟踪中达到最低值为</del>0.45年（SD0.3），与其他方法相比显著出众。GRU-D-Weibull consistently constrained the predicted survival probability within a smaller and more fixed range compared to other models throughout the follow-up period.<details>
<summary>Abstract</summary>
Accurate prediction models for individual-level endpoints and time-to-endpoints are crucial in clinical practice. In this study, we propose a novel approach, GRU-D-Weibull, which combines gated recurrent units with decay (GRU-D) to model the Weibull distribution. Our method enables real-time individualized endpoint prediction and population-level risk management. Using a cohort of 6,879 patients with stage 4 chronic kidney disease (CKD4), we evaluated the performance of GRU-D-Weibull in endpoint prediction. The C-index of GRU-D-Weibull was ~0.7 at the index date and increased to ~0.77 after 4.3 years of follow-up, similar to random survival forest. Our approach achieved an absolute L1-loss of ~1.1 years (SD 0.95) at the CKD4 index date and a minimum of ~0.45 years (SD0.3) at 4 years of follow-up, outperforming competing methods significantly. GRU-D-Weibull consistently constrained the predicted survival probability at the time of an event within a smaller and more fixed range compared to other models throughout the follow-up period. We observed significant correlations between the error in point estimates and missing proportions of input features at the index date (correlations from ~0.1 to ~0.3), which diminished within 1 year as more data became available. By post-training recalibration, we successfully aligned the predicted and observed survival probabilities across multiple prediction horizons at different time points during follow-up. Our findings demonstrate the considerable potential of GRU-D-Weibull as the next-generation architecture for endpoint risk management, capable of generating various endpoint estimates for real-time monitoring using clinical data.
</details>
<details>
<summary>摘要</summary>
准确的预测模型对个体级终点和时间到终点是临床实践中非常重要。在本研究中，我们提出了一种新的方法，即GRU-D-Weibull，它将闭包隐藏单元（GRU-D）与减少（Decay）结合以模型Weibull分布。我们的方法可以实现实时个体化终点预测和人口级风险管理。使用6,879名CKD4阶段4慢性肾病患者的 cohort，我们评估了GRU-D-Weibull在终点预测中的性能。GRU-D-Weibull的C指数在指定日期为 approximately 0.7，并在4.3年后跟踪 period 后提高到approximately 0.77，与随机生存森林相似。我们的方法在终点预测中实现了约1.1年的绝对L1损失（SD 0.95），并在4年后的跟踪期间保持在约0.45年（SD0.3）以上，与其他方法相比显著超越。GRU-D-Weibull在跟踪期间一直压制了预测生存概率的误差，并在不同的跟踪时间点保持在更小和固定的范围内表现出色。我们发现在指定日期的输入特征损失率和缺失比例之间存在显著相关性（相关性从approximately 0.1到approximately 0.3），这种相关性随着更多数据的获得而逐渐减少。通过后期重新训练，我们成功地将预测和观测生存概率Alignment在不同的预测时间点和跟踪时间点。我们的发现表明GRU-D-Weibull可以作为下一代结构，用于实时终点风险管理，可以通过临床数据生成多个终点预测。
</details></li>
</ul>
<hr>
<h2 id="Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data"><a href="#Open-set-Face-Recognition-using-Ensembles-trained-on-Clustered-Data" class="headerlink" title="Open-set Face Recognition using Ensembles trained on Clustered Data"></a>Open-set Face Recognition using Ensembles trained on Clustered Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07445">http://arxiv.org/abs/2308.07445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Henrique Vareto, William Robson Schwartz</li>
<li>for: 开放集面识别场景下，Unknown人物会在测试阶段出现，需要精准识别个人并有效地处理不熟悉的面孔。这篇论文描述了一种可扩展的开放集面识别方法，用于千计多个人的 галерее。</li>
<li>methods: 方法包括聚类和一个ensemble of binary learning algorithms，用于确定查询面孔样本是否属于face gallery，并且正确地确定它们的身份。</li>
<li>results: 实验表明，即使targeting scalability，也可以达到竞争性的性能。<details>
<summary>Abstract</summary>
Open-set face recognition describes a scenario where unknown subjects, unseen during the training stage, appear on test time. Not only it requires methods that accurately identify individuals of interest, but also demands approaches that effectively deal with unfamiliar faces. This work details a scalable open-set face identification approach to galleries composed of hundreds and thousands of subjects. It is composed of clustering and an ensemble of binary learning algorithms that estimates when query face samples belong to the face gallery and then retrieves their correct identity. The approach selects the most suitable gallery subjects and uses the ensemble to improve prediction performance. We carry out experiments on well-known LFW and YTF benchmarks. Results show that competitive performance can be achieved even when targeting scalability.
</details>
<details>
<summary>摘要</summary>
开放集 face recognition 描述一种情况，在训练阶段未看到的未知人脸在测试阶段出现。不仅需要准确地识别关心人脸，还需要采用有效地处理不熟悉的人脸方法。这篇文章介绍了一种可扩展的开放集face标识方法，用于数百或千个主题的图库。它包括 clustering 和一个 ensemble of binary learning algorithms，可以确定测试人脸样本是否属于图库，并且可以 accurately retrieve their correct identity。该方法选择了最适合的图库主题，并使用 ensemble 进行改进预测性能。我们在well-known LFW 和 YTF  benchmark上进行了实验，结果显示，可以达到竞争性的性能，即使targeting scalability。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides"><a href="#Physics-Informed-Deep-Learning-to-Reduce-the-Bias-in-Joint-Prediction-of-Nitrogen-Oxides" class="headerlink" title="Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides"></a>Physics-Informed Deep Learning to Reduce the Bias in Joint Prediction of Nitrogen Oxides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07441">http://arxiv.org/abs/2308.07441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianfa Li, Roxana Khalili, Frederick Lurmann, Nathan Pavlovic, Jun Wu, Yan Xu, Yisi Liu, Karl O’Sharkey, Beate Ritz, Luke Oman, Meredith Franklin, Theresa Bastain, Shohreh F. Farzan, Carrie Breton, Rima Habre</li>
<li>for: 这个论文主要是为了提高地面氮氧化物（NOx）的预测，以便更好地了解它们对健康和环境的影响。</li>
<li>methods: 这篇论文使用机器学习（ML）方法，但是这些方法缺乏物理和化学知识，因此可能会产生高度估计偏差。作者们提出了一种physics-informed deep learning框架，该框架可以编码扩散-扩散机制和流体动力约束，以提高NO2和NOx预测的准确性。</li>
<li>results: 作者们发现，该框架可以减少ML模型的估计偏差，并且可以提供精确的空气质量推算和明确的不确定性评估。此外，该框架还可以捕捉NO2和NOx的细致传输，并提供了可靠的空间抽象。<details>
<summary>Abstract</summary>
Atmospheric nitrogen oxides (NOx) primarily from fuel combustion have recognized acute and chronic health and environmental effects. Machine learning (ML) methods have significantly enhanced our capacity to predict NOx concentrations at ground-level with high spatiotemporal resolution but may suffer from high estimation bias since they lack physical and chemical knowledge about air pollution dynamics. Chemical transport models (CTMs) leverage this knowledge; however, accurate predictions of ground-level concentrations typically necessitate extensive post-calibration. Here, we present a physics-informed deep learning framework that encodes advection-diffusion mechanisms and fluid dynamics constraints to jointly predict NO2 and NOx and reduce ML model bias by 21-42%. Our approach captures fine-scale transport of NO2 and NOx, generates robust spatial extrapolation, and provides explicit uncertainty estimation. The framework fuses knowledge-driven physicochemical principles of CTMs with the predictive power of ML for air quality exposure, health, and policy applications. Our approach offers significant improvements over purely data-driven ML methods and has unprecedented bias reduction in joint NO2 and NOx prediction.
</details>
<details>
<summary>摘要</summary>
燃烧产生的大气氮氧化物（NOx）已被认定为有急性和长期健康和环境影响。机器学习（ML）方法已经大幅提高我们预测地面NOx浓度的能力，但这些方法可能会受到高估偏差因为没有物理和化学知识关于空气污染动力学。化学运输模型（CTM）利用这些知识，但精确预测地面浓度通常需要广泛的后调整。在这里，我们介绍一个具有物理知识的深度学习框架，该框架编码了扩散运输机制和流体动力学约束，以预测NO2和NOx的 JOINT 预测和减少ML模型偏差21-42%。我们的方法能够捕捉精确的NO2和NOx的细胞运输，生成坚固的空间推导，并提供明确的 uncertainty 估计。这个框架融合了物理化学知识驱动的CTMs和ML的预测力，实现了空气质量露地暴露、健康和政策应用中的融合。我们的方法具有与对纯数据驱动ML方法的比较，无 precedent 的偏差减少在 JOINT NO2和NOx 预测中。
</details></li>
</ul>
<hr>
<h2 id="Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks"><a href="#Interaction-Aware-Personalized-Vehicle-Trajectory-Prediction-Using-Temporal-Graph-Neural-Networks" class="headerlink" title="Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks"></a>Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07439">http://arxiv.org/abs/2308.07439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Abdelraouf, Rohit Gupta, Kyungtae Han</li>
<li>for: 预测汽车轨迹的精度是自动驾驶系统和高级驾驶助手系统中的关键。现有方法主要依靠大规模的数据集来生成通用的轨迹预测，忽略了每位司机的个性驾驶模式。</li>
<li>methods: 我们提出了一种基于交互aware的个性化轨迹预测方法，该方法利用temporal graph neural networks（GCN）和Long Short-Term Memory（LSTM）模型了target vehicles和它们周围的交通之间的空间-时间交互。为了个性化预测，我们设立了一个管道，该管道通过转移学习来使模型在大规模轨迹数据集上进行初始化预training，然后在每位司机的具体驾驶数据上进行细化调整。</li>
<li>results: 我们的个性化GCN-LSTM模型在较长的预测时间范围内表现出优于其通用版本，并且与没有预training的个体模型相比，具有更高的预测精度。此外，我们的个性化模型还能够避免过拟合现象，强调了大规模数据集的预training对个性化预测的重要性。通过个性化，我们的方法提高了轨迹预测精度。<details>
<summary>Abstract</summary>
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories. Experimental results demonstrate the superior performance of our personalized GCN-LSTM model, particularly for longer prediction horizons, compared to its generic counterpart. Moreover, the personalized model outperforms individual models created without pre-training, emphasizing the significance of pre-training on a large dataset to avoid overfitting. By incorporating personalization, our approach enhances trajectory prediction accuracy.
</details>
<details>
<summary>摘要</summary>
提高汽车轨迹预测精度是当前驱动助手系统和自动驱动技术的关键。现有方法主要依靠大规模数据集上的通用轨迹预测，忽略了个人驾驶模式的特殊性。为了解决这个差距，我们提出了一种依赖于互动的个性化汽车轨迹预测方法，该方法利用图гра拓扑神经网络（GCN）和长短期记忆（LSTM）模型了目标汽车和它们周围交通的空间时间互动。为了个性化预测，我们建立了一个管道，其中模型首先在大规模轨迹数据集上进行预训练，然后对每名司机进行细化调整，使用具有特定驾驶模式的自适应驾驶数据。我们使用人工在车Loop模拟器收集个性化自然驾驶轨迹和相应的围绕汽车轨迹。实验结果表明我们的个性化GCN-LSTM模型在较长预测时间 horizons 上表现出色，特别是比其通用对应模型更高。此外，个性化模型也比不进行预训练的模型（无预训练）表现更好，这说明了预训练大数据集可以避免过拟合。通过包含个性化，我们的方法可以提高轨迹预测精度。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Deep-Spatio-Temporal-Attention-Based-Model-for-Parkinson’s-Disease-Diagnosis-Using-Resting-State-EEG-Signals"><a href="#A-Hybrid-Deep-Spatio-Temporal-Attention-Based-Model-for-Parkinson’s-Disease-Diagnosis-Using-Resting-State-EEG-Signals" class="headerlink" title="A Hybrid Deep Spatio-Temporal Attention-Based Model for Parkinson’s Disease Diagnosis Using Resting State EEG Signals"></a>A Hybrid Deep Spatio-Temporal Attention-Based Model for Parkinson’s Disease Diagnosis Using Resting State EEG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07436">http://arxiv.org/abs/2308.07436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niloufar Delfan, Mohammadreza Shahsavari, Sadiq Hussain, Robertas Damaševičius, U. Rajendra Acharya</li>
<li>for: 这个研究的目的是为了开发一个自动化的 Parkinson’s disease 诊断模型，使用休息状态 EEG 信号。</li>
<li>methods: 这个模型使用了一种混合模型，包括卷积神经网络 (CNN)、双向关键缓冲网络 (Bi-GRU) 和注意机制。</li>
<li>results: 研究结果显示，提案的模型可以高度准确地诊断 Parkinson’s disease，并且在不同测试数据上也能够获得高性能。此外，模型还能够对部分输入信息的损失具有耐性。<details>
<summary>Abstract</summary>
Parkinson's disease (PD), a severe and progressive neurological illness, affects millions of individuals worldwide. For effective treatment and management of PD, an accurate and early diagnosis is crucial. This study presents a deep learning-based model for the diagnosis of PD using resting state electroencephalogram (EEG) signal. The objective of the study is to develop an automated model that can extract complex hidden nonlinear features from EEG and demonstrate its generalizability on unseen data. The model is designed using a hybrid model, consists of convolutional neural network (CNN), bidirectional gated recurrent unit (Bi-GRU), and attention mechanism. The proposed method is evaluated on three public datasets (Uc San Diego Dataset, PRED-CT, and University of Iowa (UI) dataset), with one dataset used for training and the other two for evaluation. The results show that the proposed model can accurately diagnose PD with high performance on both the training and hold-out datasets. The model also performs well even when some part of the input information is missing. The results of this work have significant implications for patient treatment and for ongoing investigations into the early detection of Parkinson's disease. The suggested model holds promise as a non-invasive and reliable technique for PD early detection utilizing resting state EEG.
</details>
<details>
<summary>摘要</summary>
Parkinson's disease（PD）是一种严重和进行的神经疾病，影响全球数百万人。为了有效地治疗和管理PD，准确早期诊断是非常重要。本研究提出了基于深度学习的PD诊断模型，使用休息态电энцефаogram（EEG）信号。研究的目标是开发一个自动化的模型，可以从EEG中提取复杂隐藏的非线性特征，并在未看过数据上进行普适性评估。模型采用混合模型，包括卷积神经网络（CNN）、双向闭包Recurrent Unit（Bi-GRU）和注意机制。本研究在三个公共数据集（UC San Diego数据集、PRED-CT数据集和University of Iowa（UI）数据集）进行评估，其中一个数据集用于训练，另外两个数据集用于评估。结果表明，提议的模型可以准确地诊断PD，并且在训练和剩下数据集上都有高性能。此外，模型还能够在一部分输入信息缺失时保持良好的性能。本研究结果对患者治疗和PD早期检测的研究有着重要意义。建议的模型具有非侵入性和可靠性，可能成为PD早期诊断的非侵入性和可靠的技术。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Distribution-Shift-in-RTB-Markets-via-Exponential-Tilting"><a href="#Addressing-Distribution-Shift-in-RTB-Markets-via-Exponential-Tilting" class="headerlink" title="Addressing Distribution Shift in RTB Markets via Exponential Tilting"></a>Addressing Distribution Shift in RTB Markets via Exponential Tilting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07424">http://arxiv.org/abs/2308.07424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minji Kim, Seong Jin Lee, Bumsik Kim</li>
<li>for: This paper aims to address the issue of distribution shift in machine learning models, specifically in the context of Real-Time Bidding (RTB) market models.</li>
<li>methods: The proposed method is called Exponential Tilt Reweighting Alignment (ExTRA), which uses importance weights to minimize the KL divergence between the weighted source and target datasets. The ExTRA method can operate using labeled source data and unlabeled target data.</li>
<li>results: The paper evaluates the effectiveness of the ExTRA method through simulated real-world data, demonstrating its ability to address distribution shift and improve the performance of machine learning models.<details>
<summary>Abstract</summary>
Distribution shift in machine learning models can be a primary cause of performance degradation. This paper delves into the characteristics of these shifts, primarily motivated by Real-Time Bidding (RTB) market models. We emphasize the challenges posed by class imbalance and sample selection bias, both potent instigators of distribution shifts. This paper introduces the Exponential Tilt Reweighting Alignment (ExTRA) algorithm, as proposed by Marty et al. (2023), to address distribution shifts in data. The ExTRA method is designed to determine the importance weights on the source data, aiming to minimize the KL divergence between the weighted source and target datasets. A notable advantage of this method is its ability to operate using labeled source data and unlabeled target data. Through simulated real-world data, we investigate the nature of distribution shift and evaluate the applicacy of the proposed model.
</details>
<details>
<summary>摘要</summary>
Distribution shift in machine learning models can be a primary cause of performance degradation. This paper explores the characteristics of these shifts, primarily motivated by Real-Time Bidding (RTB) market models. We highlight the challenges posed by class imbalance and sample selection bias, both powerful instigators of distribution shifts. This paper introduces the Exponential Tilt Reweighting Alignment (ExTRA) algorithm, as proposed by Marty et al. (2023), to address distribution shifts in data. The ExTRA method determines the importance weights on the source data to minimize the KL divergence between the weighted source and target datasets. A notable advantage of this method is its ability to operate using labeled source data and unlabeled target data. Through simulated real-world data, we investigate the nature of distribution shift and evaluate the applicability of the proposed model.Here's the word-for-word translation:分布shift在机器学习模型中可以是表现下降的主要原因。这篇论文探讨分布shift的特点，主要受Real-Time Bidding（RTB）市场模型的激发。我们强调分布shift的挑战，包括分类偏度和样本选择偏见，这两者都是分布shift的强力引起者。这篇论文介绍Marty等人（2023）提出的扩展tilt重要性补做算法（ExTRA），用于Addressing分布shift in data。ExTRA方法通过确定源数据中的重要性Weight来减少源数据和目标数据之间的KL偏度。这种方法的一个优点是它可以使用标注的源数据和无标注的目标数据进行操作。通过实际的世界数据 simulate，我们investigate分布shift的本质和提出方法的适用性。
</details></li>
</ul>
<hr>
<h2 id="U-Turn-Diffusion"><a href="#U-Turn-Diffusion" class="headerlink" title="U-Turn Diffusion"></a>U-Turn Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07421">http://arxiv.org/abs/2308.07421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamidreza Behjoo, Michael Chertkov<br>for:这种Diffusion模型是用于生成人工图像的。methods:这些模型基于动态助手时间机制，其中Score函数来自输入图像。results:我们的研究发现了评估Diffusion模型效果的标准：生成过程中快速谱相关性的能力直接影响生成图像质量。此外，我们还提出了“U-Turn扩散”技术，该技术通过组合前向、U-turn和后向过程，生成一个接近独立同分布（i.i.d）样本。<details>
<summary>Abstract</summary>
We present a comprehensive examination of score-based diffusion models of AI for generating synthetic images. These models hinge upon a dynamic auxiliary time mechanism driven by stochastic differential equations, wherein the score function is acquired from input images. Our investigation unveils a criterion for evaluating efficiency of the score-based diffusion models: the power of the generative process depends on the ability to de-construct fast correlations during the reverse/de-noising phase. To improve the quality of the produced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn Diffusion technique starts with the standard forward diffusion process, albeit with a condensed duration compared to conventional settings. Subsequently, we execute the standard reverse dynamics, initialized with the concluding configuration from the forward process. This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates a synthetic image approximating an independent and identically distributed (i.i.d.) sample from the probability distribution implicitly described via input samples. To analyze relevant time scales we employ various analytical tools, including auto-correlation analysis, weighted norm of the score-function analysis, and Kolmogorov-Smirnov Gaussianity test. The tools guide us to establishing that the Kernel Intersection Distance, a metric comparing the quality of synthetic samples with real data samples, is minimized at the optimal U-turn time.
</details>
<details>
<summary>摘要</summary>
我们对基于分数的扩散模型的人工智能生成 sintetic 图像进行了全面的检验。这些模型基于动态辅助时间机制驱动的随机 diffeq 方程，其中分数函数从输入图像中获取。我们的调查发现一个评估分数基于扩散模型的效率的标准：生成过程中快速相关的破坏速率决定了扩散模型的能效性。为了提高生成的 sintetic 图像质量，我们提出了“U-Turn 扩散”技术。U-Turn 扩散技术从标准的前向扩散过程开始，但是与传统设置相比，它具有缩短的时间长度。然后，我们执行标准的反向动力学，初始化为前向过程的结束配置。这种 U-Turn 扩散过程，结合前向、U-turn 和反向过程，创造了一个约束为独立同分布（i.i.d.）随机变量的 sintetic 图像。为了分析相关的时间尺度，我们使用了多种分析工具，包括自相关分析、分数函数的Weighted  нор 分析和kolmogorov-smirnov  Gaussianity 测试。这些工具引导我们确定了最佳 U-turn 时间，使得权重 norm 分数函数的质量最佳。
</details></li>
</ul>
<hr>
<h2 id="Locally-Adaptive-and-Differentiable-Regression"><a href="#Locally-Adaptive-and-Differentiable-Regression" class="headerlink" title="Locally Adaptive and Differentiable Regression"></a>Locally Adaptive and Differentiable Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07418">http://arxiv.org/abs/2308.07418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxuan Han, Varun Shankar, Jeff M Phillips, Chenglong Ye</li>
<li>for: 提出了一种基于本地学习模型的全球连续可导模型框架，以寻求处理数据中存在不同密度或函数值规模的问题。</li>
<li>methods: 使用权重加权平均方法将本地学习模型在相应的地方进行连续拟合，以实现全球连续可导模型。</li>
<li>results: 在推理中，该模型可以更快地达到统计准确性，并在各种实际应用中提供了改进的表现。<details>
<summary>Abstract</summary>
Over-parameterized models like deep nets and random forests have become very popular in machine learning. However, the natural goals of continuity and differentiability, common in regression models, are now often ignored in modern overparametrized, locally-adaptive models. We propose a general framework to construct a global continuous and differentiable model based on a weighted average of locally learned models in corresponding local regions. This model is competitive in dealing with data with different densities or scales of function values in different local regions. We demonstrate that when we mix kernel ridge and polynomial regression terms in the local models, and stitch them together continuously, we achieve faster statistical convergence in theory and improved performance in various practical settings.
</details>
<details>
<summary>摘要</summary>
现代机器学习中，过参数化模型如深度网络和随机森林已经非常流行。然而，传统机器学习模型中的稳定性和导数性目标，通常被现代过参数化、地方适应型模型所忽略。我们提出了一种通用框架，用于基于本地学习模型的权重平均构建全球连续和导数可达的模型。这种模型能够在不同的地方域上处理数据的不同浓度或函数值的比例。我们示出，当混合核ridge和多项式回归项在本地模型中，并将其们连续叠加时，我们可以在理论上更快地达到统计征 converge，并在各种实际场景中提高表现。
</details></li>
</ul>
<hr>
<h2 id="Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models"><a href="#Text-Injection-for-Capitalization-and-Turn-Taking-Prediction-in-Speech-Models" class="headerlink" title="Text Injection for Capitalization and Turn-Taking Prediction in Speech Models"></a>Text Injection for Capitalization and Turn-Taking Prediction in Speech Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07395">http://arxiv.org/abs/2308.07395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaan Bijwadia, Shuo-yiin Chang, Weiran Wang, Zhong Meng, Hao Zhang, Tara N. Sainath</li>
<li>for: 提高auxiliary任务性能（非ASR任务）</li>
<li>methods: 使用文本注入法（JEIT）对ASR模型进行训练，并同时完成两个auxiliary任务</li>
<li>results: 提高了长尾数据的首字母排序性能，以及提高了对话转接检测的受检率<details>
<summary>Abstract</summary>
Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall.
</details>
<details>
<summary>摘要</summary>
文本注入技术用于自动语音识别（ASR），其中不带文本数据用于补充带有音频文本数据的训练，已经显示出了词错率的明显改善。本研究探讨了文本注入的应用于 auxiliary task，这些任务通常由一个端到端模型完成。在这个工作中，我们使用联合端到端和内部语言模型训练算法（JEIT）来训练一个 ASR 模型，该模型同时完成了两个 auxiliary task。第一个是字母大小Normalization 任务，第二个是对话交换预测任务，它们是在数字助手交互中确定用户是否已经完成了对话转换。我们的文本注入方法可以提高长尾数据的字母大小正确率，并提高对话交换检测精度。
</details></li>
</ul>
<hr>
<h2 id="DISBELIEVE-Distance-Between-Client-Models-is-Very-Essential-for-Effective-Local-Model-Poisoning-Attacks"><a href="#DISBELIEVE-Distance-Between-Client-Models-is-Very-Essential-for-Effective-Local-Model-Poisoning-Attacks" class="headerlink" title="DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks"></a>DISBELIEVE: Distance Between Client Models is Very Essential for Effective Local Model Poisoning Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07387">http://arxiv.org/abs/2308.07387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indu Joshi, Priyank Upadhya, Gaurav Kumar Nayak, Peter Schüffler, Nassir Navab</li>
<li>for: 该研究旨在探讨 Federated Learning 如何解决医疗数据隐私问题，并研究如何防止恶意客户端攻击 Federated 系统。</li>
<li>methods: 该研究提出了一种新的本地模型欺骗攻击（DISBELIEVE），该攻击可以在 Robust Aggregation 方法下降低本地模型的性能，从而影响全局模型的性能。</li>
<li>results: 实验结果表明，DISBELIEVE 攻击可以在三个公共可用的医疗图像集上显著降低 Robust Aggregation 方法的性能，并且在自然图像集上也有较好的效果。<details>
<summary>Abstract</summary>
Federated learning is a promising direction to tackle the privacy issues related to sharing patients' sensitive data. Often, federated systems in the medical image analysis domain assume that the participating local clients are \textit{honest}. Several studies report mechanisms through which a set of malicious clients can be introduced that can poison the federated setup, hampering the performance of the global model. To overcome this, robust aggregation methods have been proposed that defend against those attacks. We observe that most of the state-of-the-art robust aggregation methods are heavily dependent on the distance between the parameters or gradients of malicious clients and benign clients, which makes them prone to local model poisoning attacks when the parameters or gradients of malicious and benign clients are close. Leveraging this, we introduce DISBELIEVE, a local model poisoning attack that creates malicious parameters or gradients such that their distance to benign clients' parameters or gradients is low respectively but at the same time their adverse effect on the global model's performance is high. Experiments on three publicly available medical image datasets demonstrate the efficacy of the proposed DISBELIEVE attack as it significantly lowers the performance of the state-of-the-art \textit{robust aggregation} methods for medical image analysis. Furthermore, compared to state-of-the-art local model poisoning attacks, DISBELIEVE attack is also effective on natural images where we observe a severe drop in classification performance of the global model for multi-class classification on benchmark dataset CIFAR-10.
</details>
<details>
<summary>摘要</summary>
Federated learning 是一个有前途的方向，以解决分享患者敏感数据时的隐私问题。在医疗影像分析领域，联邦系统经常假设参与的本地客户端是诚实的。然而，一些研究表明，可以引入一组恶意客户端，使联邦设置受损，global模型性能下降。为解决这个问题，一些robust汇集方法被提出，以防止这些攻击。我们发现，大多数当前的state-of-the-art robust汇集方法都是依赖本地客户端和良好客户端之间的距离，这使得它们容易受到本地模型毒 poisoning攻击，当本地客户端和良好客户端之间的参数或梯度距离很近时。基于这一点，我们介绍了DISBELIEVE，一种本地模型毒 poisoning攻击，可以创造出谎言的参数或梯度，使其与良好客户端之间的距离很近，但同时对全局模型性能产生严重的影响。我们在三个公共可用的医疗影像数据集上进行了实验，并证明了我们提出的DISBELIEVE攻击的有效性，可以在医疗影像分析领域对当前的robust汇集方法进行重要的攻击。此外，我们还发现，DISBELIEVE攻击也能够在自然图像领域中效果，在CIFAR-10benchmark数据集上，全局模型的多类分类性能下降很严重。
</details></li>
</ul>
<hr>
<h2 id="DiffSED-Sound-Event-Detection-with-Denoising-Diffusion"><a href="#DiffSED-Sound-Event-Detection-with-Denoising-Diffusion" class="headerlink" title="DiffSED: Sound Event Detection with Denoising Diffusion"></a>DiffSED: Sound Event Detection with Denoising Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07293">http://arxiv.org/abs/2308.07293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Bhosale, Sauradip Nag, Diptesh Kanojia, Jiankang Deng, Xiatian Zhu</li>
<li>for: 这个论文的目标是提出一种基于生成学习的声音时间边界检测方法，以提高声音事件检测的精度和效率。</li>
<li>methods: 该方法使用一种基于扩散过程的梯度下降算法，通过在适应性Transformer核心网络框架中对含杂的提议进行修复，以将含杂的提议转换为高质量的事件时间边界。</li>
<li>results: 对于Urban-SED和EPIC-Sounds数据集，该方法在训练和测试中均显示出优于现有方法的性能，具有40%以上的更快的训练速度。<details>
<summary>Abstract</summary>
Sound Event Detection (SED) aims to predict the temporal boundaries of all the events of interest and their class labels, given an unconstrained audio sample. Taking either the splitand-classify (i.e., frame-level) strategy or the more principled event-level modeling approach, all existing methods consider the SED problem from the discriminative learning perspective. In this work, we reformulate the SED problem by taking a generative learning perspective. Specifically, we aim to generate sound temporal boundaries from noisy proposals in a denoising diffusion process, conditioned on a target audio sample. During training, our model learns to reverse the noising process by converting noisy latent queries to the groundtruth versions in the elegant Transformer decoder framework. Doing so enables the model generate accurate event boundaries from even noisy queries during inference. Extensive experiments on the Urban-SED and EPIC-Sounds datasets demonstrate that our model significantly outperforms existing alternatives, with 40+% faster convergence in training.
</details>
<details>
<summary>摘要</summary>
声音事件检测（SED）目标是预测各个事件的时间边界和类别标签，给一个未Constrained的音频样本。现有的所有方法都是从推理学教学角度来解决SED问题，包括分割和分类（即帧级）策略或更为原理性的事件级模型。在这项工作中，我们重新定义了SED问题，通过一种生成学学习角度来解决。具体来说，我们目标是通过降噪过程中的批量梯度下降来生成各个事件的时间边界，使用搅拌Transformer推理框架。在训练中，我们的模型学习了将噪音提取过程反转，将噪音潜在提取到真实的样本。这使得我们的模型能够在推理中生成准确的事件边界，即使噪音提取过程不准确。我们在Urban-SED和EPIC-Sounds数据集上进行了广泛的实验，结果表明，我们的模型与现有的方法相比，在训练时间上提高了40%以上。
</details></li>
</ul>
<hr>
<h2 id="The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation"><a href="#The-Devil-is-in-the-Errors-Leveraging-Large-Language-Models-for-Fine-grained-Machine-Translation-Evaluation" class="headerlink" title="The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation"></a>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07286">http://arxiv.org/abs/2308.07286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat</li>
<li>for: 提高机器翻译系统的质量</li>
<li>methods: 利用大语言模型的推理和在场景学习能力，询问模型 identificar和分类翻译错误</li>
<li>results: 与只提示分数prompting比较，AutoMQM可以提高模型的性能，特别是大型模型，并提供可读性通过错误块与人工标注相对应<details>
<summary>Abstract</summary>
Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.
</details>
<details>
<summary>摘要</summary>
自动评估机器翻译（MT）是一种关键的工具，帮助快速迭代MT系统的发展。虽然已经做出了很大的进步，但当前的 метрикиlack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). 在这篇论文中，我们帮助填补这一漏洞，提出了AutoMQM，一种提示技术，利用大型自然语言模型（LLMs）的思维和Context learning能力，让它们标识和分类翻译中的错误。我们开始通过对最近的LLMs，如PaLM和PaLM-2，进行简单的分数预测提示，然后研究了abeled data的影响通过 Context learning和 fine-tuning。最后，我们评估了AutoMQM与PaLM-2模型，发现它可以提高性能，特别是对于更大的模型，而且提供了可读性通过错误跨度与人类标注相对应。
</details></li>
</ul>
<hr>
<h2 id="Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding"><a href="#Cross-Attribute-Matrix-Factorization-Model-with-Shared-User-Embedding" class="headerlink" title="Cross-Attribute Matrix Factorization Model with Shared User Embedding"></a>Cross-Attribute Matrix Factorization Model with Shared User Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07284">http://arxiv.org/abs/2308.07284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Zeng Fan, Youzhi Liang, Jianguo Jia</li>
<li>for: 提高推荐系统的准确率和稳定性，特别是对于“长尾”用户和 Item。</li>
<li>methods: 使用神经网络抽象来捕捉用户-项目交互，同时考虑用户和项目的特性和属性，以解决冷启始问题。</li>
<li>results: 对于 MovieLens 和 Pinterest 数据集，我们的 Cross-Attribute Matrix Factorization 模型在 sparse 数据场景下显示出优于常见方法的性能。<details>
<summary>Abstract</summary>
Over the past few years, deep learning has firmly established its prowess across various domains, including computer vision, speech recognition, and natural language processing. Motivated by its outstanding success, researchers have been directing their efforts towards applying deep learning techniques to recommender systems. Neural collaborative filtering (NCF) and Neural Matrix Factorization (NeuMF) refreshes the traditional inner product in matrix factorization with a neural architecture capable of learning complex and data-driven functions. While these models effectively capture user-item interactions, they overlook the specific attributes of both users and items. This can lead to robustness issues, especially for items and users that belong to the "long tail". Such challenges are commonly recognized in recommender systems as a part of the cold-start problem. A direct and intuitive approach to address this issue is by leveraging the features and attributes of the items and users themselves. In this paper, we introduce a refined NeuMF model that considers not only the interaction between users and items, but also acrossing associated attributes. Moreover, our proposed architecture features a shared user embedding, seamlessly integrating with user embeddings to imporve the robustness and effectively address the cold-start problem. Rigorous experiments on both the Movielens and Pinterest datasets demonstrate the superiority of our Cross-Attribute Matrix Factorization model, particularly in scenarios characterized by higher dataset sparsity.
</details>
<details>
<summary>摘要</summary>
过去几年，深度学习在不同领域，包括计算机视觉、语音识别和自然语言处理等领域，展示了它的卓越。驱动于其成功，研究人员开始尝试应用深度学习技术到推荐系统中。尽管Neural Collaborative Filtering（NCF）和Neural Matrix Factorization（NeuMF）模型能够有效地捕捉用户-项目交互，但它们忽略了用户和项目的特定属性。这可能导致 robustness 问题，特别是对于数据中的"长尾"用户和项目。这种问题在推荐系统中通常被称为冷启始问题。为解决这个问题，我们在这篇论文中引入了一种改进的NeuMF模型，该模型不仅考虑用户和项目之间的交互，还考虑用户和项目之间的属性相互关系。此外，我们的提议的体系还具有共享用户嵌入，可以融合用户嵌入，从而提高系统的稳定性和效果地解决冷启始问题。我们在MovieLens和Pinterest数据集上进行了严格的实验，结果显示我们的 Cross-Attribute Matrix Factorization模型在数据集稀疏程度较高的情况下表现出色。
</details></li>
</ul>
<hr>
<h2 id="Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning"><a href="#Data-Efficient-Energy-Aware-Participant-Selection-for-UAV-Enabled-Federated-Learning" class="headerlink" title="Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning"></a>Data-Efficient Energy-Aware Participant Selection for UAV-Enabled Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07273">http://arxiv.org/abs/2308.07273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssra Cheriguene, Wael Jaafar, Chaker Abdelaziz Kerrache, Halim Yanikomeroglu, Fatima Zohra Bousbaa, Nasreddine Lagraa</li>
<li>for: 本研究旨在提高边缘 federated learning（FL）模型的准确性，通过选择合适的无人机参与者，并且考虑无人机的能源消耗、通信质量和本地数据的不同性。</li>
<li>methods: 本研究提出了一种新的无人机参与者选择策略，即基于数据效率和能源占用率的能源意识参与者选择策略（DEEPS），该策略通过选择每个子区域中最佳的FL参与者，基于本地数据的结构相似度指数平均分数和能源占用资料来实现。</li>
<li>results: 通过实验，本研究表明，对于边缘FL，使用DEEPS策略可以提高模型准确性、减少训练时间和无人机的能源消耗，相比于随机选择策略。<details>
<summary>Abstract</summary>
Unmanned aerial vehicle (UAV)-enabled edge federated learning (FL) has sparked a rise in research interest as a result of the massive and heterogeneous data collected by UAVs, as well as the privacy concerns related to UAV data transmissions to edge servers. However, due to the redundancy of UAV collected data, e.g., imaging data, and non-rigorous FL participant selection, the convergence time of the FL learning process and bias of the FL model may increase. Consequently, we investigate in this paper the problem of selecting UAV participants for edge FL, aiming to improve the FL model's accuracy, under UAV constraints of energy consumption, communication quality, and local datasets' heterogeneity. We propose a novel UAV participant selection scheme, called data-efficient energy-aware participant selection strategy (DEEPS), which consists of selecting the best FL participant in each sub-region based on the structural similarity index measure (SSIM) average score of its local dataset and its power consumption profile. Through experiments, we demonstrate that the proposed selection scheme is superior to the benchmark random selection method, in terms of model accuracy, training time, and UAV energy consumption.
</details>
<details>
<summary>摘要</summary>
“无人航空器（UAV）启动的边缘联合学习（FL）已经引起了研究者们的探索，因为UAV所收集的数据量巨大且多样，同时也存在资料传输到边缘服务器的隐私问题。然而，由于UAV收集的数据存在重复性，例如影像数据，以及不充分的FL参与者选择，FL学习过程的参数调整和模型偏好可能会增加。因此，本文研究UAV参与者选择的问题，以提高FL模型的准确性，并且遵循UAV的能源消耗、通信质量和本地数据的多样性限制。我们提出了一个 novel UAV参与者选择策略，called 数据效率能源注意的参与者选择策略（DEEPS），它是根据每个子区域中的本地数据和能源消耗观察所得到的结构相似度平均分数（SSIM）的平均分数，选择每个子区域中最佳的 FL 参与者。经过实验，我们发现，提案的选择策略与参考随机选择方法相比，在于模型准确性、训练时间和UAV能源消耗方面均有优势。”
</details></li>
</ul>
<hr>
<h2 id="Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning"><a href="#Dialogue-for-Prompting-a-Policy-Gradient-Based-Discrete-Prompt-Optimization-for-Few-shot-Learning" class="headerlink" title="Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning"></a>Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07272">http://arxiv.org/abs/2308.07272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi Li, Yu Lan, Chao Shen</li>
<li>For: 提高几何语言处理（NLP）任务中的几何学习效果，以及解决现有的精度优化方法的问题。* Methods: 使用对话Alignment策略生成可读性提示集，并提出高效的提示筛选指标来选择高质量提示。然后，通过policy梯度学习算法来匹配提示和输入。* Results: 在四个开源数据集上，DP_2O方法在几何学习设定下的准确率高于当前最佳方法的1.52%，并且在不同的任务和数据集上都有good的通用性、稳定性和泛化能力。<details>
<summary>Abstract</summary>
Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization ($DP_2O$) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, $DP_2O$ outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that $DP_2O$ has good universality, robustness, and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models"><a href="#EasyEdit-An-Easy-to-use-Knowledge-Editing-Framework-for-Large-Language-Models" class="headerlink" title="EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models"></a>EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07269">http://arxiv.org/abs/2308.07269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyedit">https://github.com/zjunlp/easyedit</a></li>
<li>paper_authors: Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, Huajun Chen</li>
<li>for: 这个论文的目的是提出一个轻松使用的知识编辑框架，以便在大语言模型（LLMs）上应用多种 cutting-edge 知识编辑方法。</li>
<li>methods: 该论文使用了多种知识编辑方法，包括粘贴、替换、剪辑等，以及一些自动生成的方法。</li>
<li>results: 该论文的实验结果表明，使用知识编辑方法可以超过传统的精度调整，并且具有更好的一致性和普适性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to the outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners to apply knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video at http://knowlm.zjukg.cn/easyedit.mp4.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常会受到知识割裂或错误问题的影响，这意味着它们不知道未看过的事件或生成文本中含有错误的 факти due to outdated/noisy data。为了解决这个问题，许多知识编辑方法 для LLM 已经出现 -- 目的是通过微妙地将更新的知识或不适合的行为进行调整，以最小化对无关输入的影响。然而，由于不同的知识编辑方法和任务设置的差异，现在没有一个标准的实现框架可以供社区使用，这限制了实践者将知识编辑应用于应用程序。为了解决这些问题，我们提出了 EasyEdit，一个易于使用的知识编辑框架 для LLM。它支持许多最新的知识编辑方法，并可以轻松地应用到许多已知的 LLM，如 T5、GPT-J、LlaMA 等。我们在 LlaMA-2 上进行了知识编辑实验，结果显示，知识编辑超过了传统精细调整的可靠性和应用性。我们在 GitHub 上发布了源代码，并提供 Google Colab 教学和详细的文档，以便初学者开始。此外，我们还提供了线上系统 для实时知识编辑，以及一个网页demo video，请参考 http://knowlm.zjukg.cn/easyedit.mp4。
</details></li>
</ul>
<hr>
<h2 id="LCE-An-Augmented-Combination-of-Bagging-and-Boosting-in-Python"><a href="#LCE-An-Augmented-Combination-of-Bagging-and-Boosting-in-Python" class="headerlink" title="LCE: An Augmented Combination of Bagging and Boosting in Python"></a>LCE: An Augmented Combination of Bagging and Boosting in Python</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07250">http://arxiv.org/abs/2308.07250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/localcascadeensemble/lce">https://github.com/localcascadeensemble/lce</a></li>
<li>paper_authors: Kevin Fauvel, Élisa Fromont, Véronique Masson, Philippe Faverdin, Alexandre Termier</li>
<li>for: 本研究开发了一个高性能、可扩展、易用的Python包lcensemble，用于对 классификация和回归问题进行通用任务。</li>
<li>methods: 本研究使用了Local Cascade Ensemble（LCE）机器学习方法，它将Random Forest和XGBoost两种现状顶峰方法融合，以实现更好的泛化预测器。</li>
<li>results: lcensemble可以与scikit-learn集成，并且可以与scikit-learn的管道和模型选择工具互动。它在处理大规模数据时表现出了高性能。<details>
<summary>Abstract</summary>
lcensemble is a high-performing, scalable and user-friendly Python package for the general tasks of classification and regression. The package implements Local Cascade Ensemble (LCE), a machine learning method that further enhances the prediction performance of the current state-of-the-art methods Random Forest and XGBoost. LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. The package is compatible with scikit-learn, therefore it can interact with scikit-learn pipelines and model selection tools. It is distributed under the Apache 2.0 license, and its source code is available at https://github.com/LocalCascadeEnsemble/LCE.
</details>
<details>
<summary>摘要</summary>
LCensemble 是一个高性能、可扩展、易用的 Python 包，用于执行分类和回归的通用任务。该包实现了本地随机森林 ensemble（LCE）机器学习方法，该方法可以进一步提高当前状态的艺术法和 XGBoost 方法的预测性能。LCE 结合了它们的优点，采用了补做的多样化方法，从而获得一个更好的总体预测器。该包与 scikit-learn 兼容，因此可以与 scikit-learn 管道和模型选择工具进行交互。它根据 Apache 2.0 license 分发，源代码可以在 https://github.com/LocalCascadeEnsemble/LCE 上 obtener。
</details></li>
</ul>
<hr>
<h2 id="Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI"><a href="#Can-we-Agree-On-the-Rashomon-Effect-and-the-Reliability-of-Post-Hoc-Explainable-AI" class="headerlink" title="Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI"></a>Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07247">http://arxiv.org/abs/2308.07247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Poiret, Antoine Grigis, Justin Thomas, Marion Noulhiane</li>
<li>for: 这项研究探讨了使用SHAP在Rashomon集中获得可靠知识的挑战。</li>
<li>methods: 研究使用5个公共数据集进行实验，发现采样大小的增加可以提高模型的解释的一致性。但在少量采样下（&lt;128个样本），解释具有高度的变化性，因此不可靠地抽取知识。然而，随着更多的数据，模型之间的一致性提高，允许达成共识。bagging ensemble通常具有更高的一致性。</li>
<li>results: 研究结果表明，要在少量采样下（&lt;128个样本）进行验证，以确保结论的可靠性。此外，对于不同的模型类型、数据领域和解释方法，进一步的研究是必要的。测试神经网络和特定解释方法的收敛性也是有价值的。本研究的方法指向了可靠地从模糊模型中提取知识的原则方法。<details>
<summary>Abstract</summary>
The Rash\=omon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rash\=omon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous models.
</details>
<details>
<summary>摘要</summary>
“落差omon效应对机器学习模型知识抽取带来挑战。这项研究研究了模型在Rashomon集中的解释如何受样本大小影响。使用SHAP进行实验，发现随着样本大小增加，解释的一致性逐渐提高。但是，从128个样本开始，解释呈现高度的变化， limiting 可靠知识抽取。然而，通过更多的数据，模型之间的一致性提高，allowing for consensus。 bagging  ensemble 通常具有更高的一致性。结果为我们提供了足够数据来信任解释的指南。低样本数时的变化表明，不进行验证，得出的结论可能不可靠。未来的工作应该进一步探索更多的模型类型、数据领域和解释方法。测试神经网络和特定模型解释方法的受样本大小影响也是有价值的。研究进行的方法指向了有良好原则的模型解释技术。”
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Generator-Loss-Function-for-Generative-Adversarial-Networks"><a href="#A-Unifying-Generator-Loss-Function-for-Generative-Adversarial-Networks" class="headerlink" title="A Unifying Generator Loss Function for Generative Adversarial Networks"></a>A Unifying Generator Loss Function for Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07233">http://arxiv.org/abs/2308.07233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Veiner, Fady Alajaji, Bahman Gharesifard</li>
<li>for: 这个论文主要关注的是 dual-objective generative adversarial network (GAN) 的 $\alpha$-parametrized generator loss function，用于替代原始 GAN 系统中的 classical discriminator loss function。</li>
<li>methods: 这个论文提出了一种基于 symmetric class probability estimation 类型的 generator loss function，称为 $\mathcal{L}_\alpha$，并使用这个loss function来定义 $\mathcal{L}_\alpha$-GAN 系统。</li>
<li>results: 研究人员通过分析 generator 的优化问题，发现 generator 的优化问题可以表示为一个 Jensen-$f_\alpha$- divergence 的最小化问题，其中 $f_\alpha$ 是一个 convex 函数，具体表示为 loss function $\mathcal{L}_\alpha$。此外，这个 $\mathcal{L}_\alpha$-GAN 问题还可以恢复一些在文献中提出的 GAN 问题，包括 VanillaGAN、LSGAN、L$k$GAN 和 $({\alpha_D},{\alpha_G})$-GAN 中的 $\alpha_D&#x3D;1$。最后，在 MNIST、CIFAR-10 和 Stacked MNIST 三个数据集上进行了实验，以证明不同的例子的 $\mathcal{L}_\alpha$-GAN 系统的性能。<details>
<summary>Abstract</summary>
A unifying $\alpha$-parametrized generator loss function is introduced for a dual-objective generative adversarial network (GAN), which uses a canonical (or classical) discriminator loss function such as the one in the original GAN (VanillaGAN) system. The generator loss function is based on a symmetric class probability estimation type function, $\mathcal{L}_\alpha$, and the resulting GAN system is termed $\mathcal{L}_\alpha$-GAN. Under an optimal discriminator, it is shown that the generator's optimization problem consists of minimizing a Jensen-$f_\alpha$-divergence, a natural generalization of the Jensen-Shannon divergence, where $f_\alpha$ is a convex function expressed in terms of the loss function $\mathcal{L}_\alpha$. It is also demonstrated that this $\mathcal{L}_\alpha$-GAN problem recovers as special cases a number of GAN problems in the literature, including VanillaGAN, Least Squares GAN (LSGAN), Least $k$th order GAN (L$k$GAN) and the recently introduced $(\alpha_D,\alpha_G)$-GAN with $\alpha_D=1$. Finally, experimental results are conducted on three datasets, MNIST, CIFAR-10, and Stacked MNIST to illustrate the performance of various examples of the $\mathcal{L}_\alpha$-GAN system.
</details>
<details>
<summary>摘要</summary>
文本中引入了一种对称$\alpha$-参数化生成器损失函数，用于一个双目标生成对抗网络（GAN）系统。生成器损失函数基于一种对称的класси型概率估计函数，$\mathcal{L}_\alpha$，并将系统称为$\mathcal{L}_\alpha$-GAN。在理想的权衡器下， generator的优化问题可以表示为最小化一种Jensen-$f_\alpha$-分配，这是自然推广Jensen-Shannon分配的一种自然推广，其中$f_\alpha$是一个对称的束缚函数，它与损失函数$\mathcal{L}_\alpha$有关。此外，这个$\mathcal{L}_\alpha$-GAN问题还能够恢复一些文献中的GAN问题，包括VanillaGAN、LSGAN、L$k$GAN和$(\alpha_D,\alpha_G)$-GAN中的$\alpha_D=1$。最后，对三个数据集（MNIST、CIFAR-10和Stacked MNIST）进行了实验，以示出不同的$\mathcal{L}_\alpha$-GAN系统的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/cs.LG_2023_08_15/" data-id="clly4xtdv006nvl88hmzj06zg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/15/eess.IV_2023_08_15/" class="article-date">
  <time datetime="2023-08-15T09:00:00.000Z" itemprop="datePublished">2023-08-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/15/eess.IV_2023_08_15/">eess.IV - 2023-08-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract"><a href="#Targeted-Multispectral-Filter-Array-Design-for-Endoscopic-Cancer-Detection-in-the-Gastrointestinal-Tract" class="headerlink" title="Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract"></a>Targeted Multispectral Filter Array Design for Endoscopic Cancer Detection in the Gastrointestinal Tract</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07947">http://arxiv.org/abs/2308.07947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaela Taylor-Williams, Ran Tao, Travis W Sawyer, Dale J Waterhouse, Jonghee Yoon, Sarah E Bohndiek</li>
<li>for: 检测肠胃肉体中疾病的颜色差异，以提高疾病检测的准确性。</li>
<li>methods: 使用自定义多spectral filter arrays (MSFAs)，并使用开源工具箱Opti-MSFA进行优化设计。</li>
<li>results: 结果显示，MSFA设计具有高分类精度，表明将在未来实施在检查器硬件中可能有助于提高肠胃肉体检测的早期检测。<details>
<summary>Abstract</summary>
Colour differences between healthy and diseased tissue in the gastrointestinal tract are detected visually by clinicians during white light endoscopy (WLE); however, the earliest signs of disease are often just a slightly different shade of pink compared to healthy tissue. Here, we propose to target alternative colours for imaging to improve contrast using custom multispectral filter arrays (MSFAs) that could be deployed in an endoscopic chip-on-tip configuration. Using an open-source toolbox, Opti-MSFA, we examined the optimal design of MSFAs for early cancer detection in the gastrointestinal tract. The toolbox was first extended to use additional classification models (k-Nearest Neighbour, Support Vector Machine, and Spectral Angle Mapper). Using input spectral data from published clinical trials examining the oesophagus and colon, we optimised the design of MSFAs with 3 to 9 different bands. We examined the variation of the spectral and spatial classification accuracy as a function of number of bands. The MSFA designs have high classification accuracies, suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance. Optimal MSFA configurations can achieve similar classification accuracies as the full spectral data in an implementation that could be realised in far simpler hardware. The reduced number of spectral bands could enable future deployment of multispectral imaging in an endoscopic chip-on-tip configuration.
</details>
<details>
<summary>摘要</summary>
医生在白光endooscopy（WLE）中可以通过视觉检测肠道内健康和疾病组织的颜色差异。然而，疾病的早期征象通常只是健康组织的微妙变化。我们提议使用自定义多spectral filter array（MSFA）来提高对比度。我们使用了一个开源工具箱，Opti-MSFA，来调查最佳MSFA的设计。我们首先扩展了工具箱，使其支持更多的分类模型（k-最近邻居、支持向量机和spectral angle mapper）。使用来自已发布临床试验的迷你镜诊断数据，我们优化了MSFA的3到9个频谱带的设计。我们分析了频谱和空间分类精度的变化与频谱带数的关系。我们发现，最佳MSFA配置具有高分类精度， suggesting that future implementation in endoscopy hardware could potentially enable improved early detection of disease in the gastrointestinal tract during routine screening and surveillance。最佳MSFA配置可以实现与全spectral数据相同的分类精度，但具有更少的频谱带数，这可能使得未来在endooscopy中实现多spectral imaging的方式更加简单。
</details></li>
</ul>
<hr>
<h2 id="DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation"><a href="#DSFNet-Convolutional-Encoder-Decoder-Architecture-Combined-Dual-GCN-and-Stand-alone-Self-attention-by-Fast-Normalized-Fusion-for-Polyps-Segmentation" class="headerlink" title="DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation"></a>DSFNet: Convolutional Encoder-Decoder Architecture Combined Dual-GCN and Stand-alone Self-attention by Fast Normalized Fusion for Polyps Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07946">http://arxiv.org/abs/2308.07946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juntong Fan, Tieyong Zeng, Dayang Wang<br>for:This paper aims to address the challenging task of polyp segmentation in colonoscopy images using a novel U-shaped network called DSFNet.methods:The proposed DSFNet combines the advantages of Dual-GCN and self-attention mechanisms, including a feature enhancement block module and a stand-alone self-attention module, as well as a Fast Normalized Fusion method for efficient feature fusion.results:The proposed model surpasses other state-of-the-art models in terms of Dice, MAE, and IoU on two public datasets (Endoscene and Kvasir-SEG), and ablation studies verify the efficacy and effectiveness of each module. The proposed model has great clinical significance for polyp segmentation in colonoscopy images.Here is the Chinese version of the three key points:for:本研究使用一种新型的U型网络，即DSFNet，解决了医学内视镜中肠道肿瘤分 segmentation的挑战性问题。methods:提议的DSFNet结合了 dual-GCN和自注意机制的优点，包括特征增强块模块和独立自注意模块，以及一种高效的快Normalized Fusion方法。results:实验结果表明，提议的模型在两个公共数据集（Endoscene和Kvasir-SEG）上比其他状态对比模型在多个指标（Dice、MAE和IoU）上表现出优异，并进行了ablation研究来验证每个模块的有效性和效iveness。结果表明，提议的模型在医学中具有大的临床意义。<details>
<summary>Abstract</summary>
In the past few decades, deep learning technology has been widely used in medical image segmentation and has made significant breakthroughs in the fields of liver and liver tumor segmentation, brain and brain tumor segmentation, video disc segmentation, heart image segmentation, and so on. However, the segmentation of polyps is still a challenging task since the surface of the polyps is flat and the color is very similar to that of surrounding tissues. Thus, It leads to the problems of the unclear boundary between polyps and surrounding mucosa, local overexposure, and bright spot reflection. To counter this problem, this paper presents a novel U-shaped network, namely DSFNet, which effectively combines the advantages of Dual-GCN and self-attention mechanisms. First, we introduce a feature enhancement block module based on Dual-GCN module as an attention mechanism to enhance the feature extraction of local spatial and structural information with fine granularity. Second, the stand-alone self-attention module is designed to enhance the integration ability of the decoding stage model to global information. Finally, the Fast Normalized Fusion method with trainable weights is used to efficiently fuse the corresponding three feature graphs in encoding, bottleneck, and decoding blocks, thus promoting information transmission and reducing the semantic gap between encoder and decoder. Our model is tested on two public datasets including Endoscene and Kvasir-SEG and compared with other state-of-the-art models. Experimental results show that the proposed model surpasses other competitors in many indicators, such as Dice, MAE, and IoU. In the meantime, ablation studies are also conducted to verify the efficacy and effectiveness of each module. Qualitative and quantitative analysis indicates that the proposed model has great clinical significance.
</details>
<details>
<summary>摘要</summary>
在过去几十年中，深度学习技术在医学影像分割领域得到广泛应用，并在肝脏和肝癌分割、脑和脑癌分割、视频碟分割、心脏影像分割等领域取得了显著突破。然而，肿瘤分割仍然是一项挑战性的任务，因为肿瘤表面平滑，颜色与周围组织相似，导致边界不清晰、局部过曝光和光泽反射等问题。为解决这些问题，本文提出了一种新的U型网络，即DSFNet，该网络效果地结合了DUAL-GCN模块和自注意机制。首先，我们引入了基于DUAL-GCN模块的特征增强块模块，以增强本地空间和结构信息的特征提取。其次，我们设计了独立的自注意模块，以提高解码阶段模型的全局信息集成能力。最后，我们使用可学习权重的快速 нормализа化融合方法，以有效地融合编码、瓶颈和解码块中的三个特征图，从而提高信息传递和减少编码器和解码器之间的semantic gap。我们的模型在Endoscene和Kvasir-SEG两个公共数据集上进行测试，与其他当前顶尖模型进行比较。实验结果表明，我们的模型在多个指标上超过其他竞争对手，包括 dice、MAE和IoU等指标。同时，我们还进行了ablation研究，以验证每个模块的有效性和效果。 qualitative和quantitative分析表明，我们的模型在临床上具有很大的价值。
</details></li>
</ul>
<hr>
<h2 id="An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease"><a href="#An-Interpretable-Machine-Learning-Model-with-Deep-Learning-based-Imaging-Biomarkers-for-Diagnosis-of-Alzheimer’s-Disease" class="headerlink" title="An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease"></a>An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07778">http://arxiv.org/abs/2308.07778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Kang, Bo Li, Janne M. Papma, Lize C. Jiskoot, Peter Paul De Deyn, Geert Jan Biessels, Jurgen A. H. R. Claassen, Huub A. M. Middelkoop, Wiesje M. van der Flier, Inez H. G. B. Ramakers, Stefan Klein, Esther E. Bron</li>
<li>for: 预测阿尔ц海默病（AD）的早期诊断。</li>
<li>methods: combines Explainable Boosting Machines (EBM) with deep learning-based feature extraction，提供了每个特征的重要性。</li>
<li>results: 在Alzheimer’s Disease Neuroimaging Initiative（ADNI）数据集上 achieved accuracy of 0.883和area-under-the-curve（AUC）of 0.970 on AD和control分类，并在一个外部测试集上 achieved accuracy of 0.778和AUC of 0.887 on AD和主观认知下降（SCD）分类。<details>
<summary>Abstract</summary>
Machine learning methods have shown large potential for the automatic early diagnosis of Alzheimer's Disease (AD). However, some machine learning methods based on imaging data have poor interpretability because it is usually unclear how they make their decisions. Explainable Boosting Machines (EBMs) are interpretable machine learning models based on the statistical framework of generalized additive modeling, but have so far only been used for tabular data. Therefore, we propose a framework that combines the strength of EBM with high-dimensional imaging data using deep learning-based feature extraction. The proposed framework is interpretable because it provides the importance of each feature. We validated the proposed framework on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, achieving accuracy of 0.883 and area-under-the-curve (AUC) of 0.970 on AD and control classification. Furthermore, we validated the proposed framework on an external testing set, achieving accuracy of 0.778 and AUC of 0.887 on AD and subjective cognitive decline (SCD) classification. The proposed framework significantly outperformed an EBM model using volume biomarkers instead of deep learning-based features, as well as an end-to-end convolutional neural network (CNN) with optimized architecture.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression"><a href="#Dynamic-Low-Rank-Instance-Adaptation-for-Universal-Neural-Image-Compression" class="headerlink" title="Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression"></a>Dynamic Low-Rank Instance Adaptation for Universal Neural Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07733">http://arxiv.org/abs/2308.07733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llvy21/duic">https://github.com/llvy21/duic</a></li>
<li>paper_authors: Yue Lv, Jinxi Xiang, Jun Zhang, Wenming Yang, Xiao Han, Wei Yang</li>
<li>for: 本研究旨在提高 neural image compression 的环境不同时的表现，即addressing the domain gap between training datasets (natural images) and inference datasets (e.g., artistic images).</li>
<li>methods: 我们提出了一种基于low-rank adaptation的方法，包括在客户端decoder中进行低级别矩阵分解，并将更新了适应参数传输到客户端。此外，我们还引入了一种动态阀网，以确定需要适应的层。</li>
<li>results: 我们的方法可以 universal across diverse image datasets，并且在out-of-domain图像上表现出较好的表现，与非适应方法相比，平均BD-rate提高约19%。此外，我们的方法还可以在不同的图像压缩架构上进行改进。<details>
<summary>Abstract</summary>
The latest advancements in neural image compression show great potential in surpassing the rate-distortion performance of conventional standard codecs. Nevertheless, there exists an indelible domain gap between the datasets utilized for training (i.e., natural images) and those utilized for inference (e.g., artistic images). Our proposal involves a low-rank adaptation approach aimed at addressing the rate-distortion drop observed in out-of-domain datasets. Specifically, we perform low-rank matrix decomposition to update certain adaptation parameters of the client's decoder. These updated parameters, along with image latents, are encoded into a bitstream and transmitted to the decoder in practical scenarios. Due to the low-rank constraint imposed on the adaptation parameters, the resulting bit rate overhead is small. Furthermore, the bit rate allocation of low-rank adaptation is \emph{non-trivial}, considering the diverse inputs require varying adaptation bitstreams. We thus introduce a dynamic gating network on top of the low-rank adaptation method, in order to decide which decoder layer should employ adaptation. The dynamic adaptation network is optimized end-to-end using rate-distortion loss. Our proposed method exhibits universality across diverse image datasets. Extensive results demonstrate that this paradigm significantly mitigates the domain gap, surpassing non-adaptive methods with an average BD-rate improvement of approximately $19\%$ across out-of-domain images. Furthermore, it outperforms the most advanced instance adaptive methods by roughly $5\%$ BD-rate. Ablation studies confirm our method's ability to universally enhance various image compression architectures.
</details>
<details>
<summary>摘要</summary>
最新的神经网络图像压缩技术具有可能超越传统标准编解码器的环境-质量表现的潜在力量。然而，存在一个不可缺少的领域差距（domain gap），即训练集（natural images）和推理集（e.g., artistic images）之间的差异。我们的提议是一种基于低级数的适应方法，用于Addressing the rate-distortion drop observed in out-of-domain datasets。具体来说，我们通过低级数矩阵分解更新客户端的解码器某些适应参数。这些更新后的参数，加上图像latent，被编码到一个bit流中并在实际应用场景中传输。由于低级数约束对适应参数的影响，bit流扩展的负担小。此外，低级数适应的bit流分配是非易的，需要根据各种输入的多样性进行变动的适应。我们因此引入了一种基于低级数适应的动态阻止网络，以确定哪些解码层应该使用适应。这个动态适应网络通过练习环境-质量损失来优化。我们的提议在多种图像压缩架构上 universality，并且在多种图像数据集上进行了广泛的测试。结果显示，这种方法可以有效 mitigate the domain gap，与非适应方法相比，平均BD-rate提高约19%，而与最先进的实例适应方法相比，BD-rate提高约5%。剖析研究证明了我们的方法可以通过universal enhancement来改善多种图像压缩架构。
</details></li>
</ul>
<hr>
<h2 id="A-deep-deformable-residual-learning-network-for-SAR-images-segmentation"><a href="#A-deep-deformable-residual-learning-network-for-SAR-images-segmentation" class="headerlink" title="A deep deformable residual learning network for SAR images segmentation"></a>A deep deformable residual learning network for SAR images segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07627">http://arxiv.org/abs/2308.07627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Yulin Huang, Jianyu Yang</li>
<li>for: 这篇论文是为了提出一个基于深度学习网络的新方法 для SAR 对象分类，以提高 SAR 对象分类的精度和速度。</li>
<li>methods: 本文使用的方法包括对于 SAR 图像进行深度学习网络的建立，并将对象分类问题转化为一个对于图像进行分类的问题。另外，本文还使用了扭转卷网络和复原学习块，以提高网络的准确性和稳定性。</li>
<li>results: 根据 MSTAR 资料集的实验结果显示，提出的深度对象分类网络能够实现高精度和高速度的 SAR 对象分类，并且比传统方法更加精确和可靠。<details>
<summary>Abstract</summary>
Reliable automatic target segmentation in Synthetic Aperture Radar (SAR) imagery has played an important role in the SAR fields. Different from the traditional methods, Spectral Residual (SR) and CFAR detector, with the recent adavance in machine learning theory, there has emerged a novel method for SAR target segmentation, based on the deep learning networks. In this paper, we proposed a deep deformable residual learning network for target segmentation that attempts to preserve the precise contour of the target. For this, the deformable convolutional layers and residual learning block are applied, which could extract and preserve the geometric information of the targets as much as possible. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) data set, experimental results have shown the superiority of the proposed network for the precise targets segmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>启用简化中文</SYS>静电Synthetic Aperture Radar（SAR）图像中的自动目标分割可靠性在SAR领域中发挥了重要作用。与传统方法不同，我们提出了基于深度学习网络的新方法，即吸引强度差（SR）和CFAR探测器。在这篇论文中，我们提出了一种深度变形剩余学习网络，用于目标分割，以保留目标精确的轮廓。为了实现这一目标，我们使用了变形卷积层和剩余学习块，以提取和保留目标的几何信息。基于Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集，我们的实验结果表明，提出的网络可以准确地分割精确目标。
</details></li>
</ul>
<hr>
<h2 id="GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis"><a href="#GAMER-MRIL-identifies-Disability-Related-Brain-Changes-in-Multiple-Sclerosis" class="headerlink" title="GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis"></a>GAMER-MRIL identifies Disability-Related Brain Changes in Multiple Sclerosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07611">http://arxiv.org/abs/2308.07611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-Jui Lu, Benjamin Odry, Muhamed Barakovic, Matthias Weigel, Robin Sandkühler, Reza Rahmanzadeh, Xinjie Chen, Mario Ocampo-Pineda, Jens Kuhle, Ludwig Kappos, Philippe Cattin, Cristina Granziera</li>
<li>for: 这个研究的目的是为了确定多创性疾病（MS）患者的残障相关的脑部变化。</li>
<li>methods: 这个研究使用了全脑量化MRI（qMRI）、神经网络（CNN）和可解释方法，将MS患者分为严重残障和不严重残障两组。</li>
<li>results: 研究获得了0.885的测试效果，qT1是残障相关最敏感的测量指标，其次是neurite density index（NDI）。这个研究还发现了残障相关的脑部区域，包括 corticospinal tract，这些区域与患者的残障分数有 statistically significant 的相关性（ρ&#x3D;-0.37和0.44）。<details>
<summary>Abstract</summary>
Objective: Identifying disability-related brain changes is important for multiple sclerosis (MS) patients. Currently, there is no clear understanding about which pathological features drive disability in single MS patients. In this work, we propose a novel comprehensive approach, GAMER-MRIL, leveraging whole-brain quantitative MRI (qMRI), convolutional neural network (CNN), and an interpretability method from classifying MS patients with severe disability to investigating relevant pathological brain changes. Methods: One-hundred-sixty-six MS patients underwent 3T MRI acquisitions. qMRI informative of microstructural brain properties was reconstructed, including quantitative T1 (qT1), myelin water fraction (MWF), and neurite density index (NDI). To fully utilize the qMRI, GAMER-MRIL extended a gated-attention-based CNN (GAMER-MRI), which was developed to select patch-based qMRI important for a given task/question, to the whole-brain image. To find out disability-related brain regions, GAMER-MRIL modified a structure-aware interpretability method, Layer-wise Relevance Propagation (LRP), to incorporate qMRI. Results: The test performance was AUC=0.885. qT1 was the most sensitive measure related to disability, followed by NDI. The proposed LRP approach obtained more specifically relevant regions than other interpretability methods, including the saliency map, the integrated gradients, and the original LRP. The relevant regions included the corticospinal tract, where average qT1 and NDI significantly correlated with patients' disability scores ($\rho$=-0.37 and 0.44). Conclusion: These results demonstrated that GAMER-MRIL can classify patients with severe disability using qMRI and subsequently identify brain regions potentially important to the integrity of the mobile function. Significance: GAMER-MRIL holds promise for developing biomarkers and increasing clinicians' trust in NN.
</details>
<details>
<summary>摘要</summary>
目标：为多发性硬化病（MS）患者 Identify 负面功能相关的脑变化。现在，没有明确的认知，哪些生理学特征驱动单个MS患者的残疾。在这种工作中，我们提议了一种全新的全脑量化MRI（qMRI）、卷积神经网络（CNN）和可解释方法，从分类MS患者严重残疾到研究相关的脑变化。方法：一百六十六名MS患者通过3T MRI成像。重要的质量MRI（qMRI）信息，包括质量T1（qT1）、脑白质含量（MWF）和神经纤维数（NDI），都被重构。为了全面利用qMRI，我们扩展了基于闭合注意力的CNN（GAMER-MRI），并将其应用到整个脑图像。为了找出残疾相关的脑区域，我们修改了层次相关传播（LRP）方法，以包括qMRI。结果：测试性能为AUC=0.885。qT1是残疾相关度最高的指标，其次是NDI。我们的LRP方法在特定任务/问题中更有特点地找出了残疾相关的脑区域，比如脑束束络（ corticospinal tract），其中qT1和NDI的平均值与患者残疾分数（ρ=-0.37和0.44）有 statistically significant 相关性。结论：这些结果表明，GAMER-MRIL可以通过qMRI和CNN来分类患者严重残疾，并且可以特定残疾相关的脑区域。这些结果表明GAMER-MRIL具有开发生物标志物和增加临床医生对NN的信任的潜力。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation"><a href="#Benchmarking-Scalable-Epistemic-Uncertainty-Quantification-in-Organ-Segmentation" class="headerlink" title="Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation"></a>Benchmarking Scalable Epistemic Uncertainty Quantification in Organ Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07506">http://arxiv.org/abs/2308.07506</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadie1/medseguq">https://github.com/jadie1/medseguq</a></li>
<li>paper_authors: Jadie Adams, Shireen Y. Elhabian</li>
<li>for:  aid diagnosis and treatment planning</li>
<li>methods:  epistemic uncertainty quantification methods in organ segmentation</li>
<li>results:  comprehensive benchmarking study to evaluate the accuracy, uncertainty calibration, and scalability of different methods<details>
<summary>Abstract</summary>
Deep learning based methods for automatic organ segmentation have shown promise in aiding diagnosis and treatment planning. However, quantifying and understanding the uncertainty associated with model predictions is crucial in critical clinical applications. While many techniques have been proposed for epistemic or model-based uncertainty estimation, it is unclear which method is preferred in the medical image analysis setting. This paper presents a comprehensive benchmarking study that evaluates epistemic uncertainty quantification methods in organ segmentation in terms of accuracy, uncertainty calibration, and scalability. We provide a comprehensive discussion of the strengths, weaknesses, and out-of-distribution detection capabilities of each method as well as recommendations for future improvements. These findings contribute to the development of reliable and robust models that yield accurate segmentations while effectively quantifying epistemic uncertainty.
</details>
<details>
<summary>摘要</summary>
深度学习基于方法为自动器官 segmentation 表现出了许多批处的可能性，帮助诊断和治疗规划。然而，量化和理解模型预测结果中的uncertainty是在重要的临床应用中关键。虽然许多技术被提出用于知识 Based uncertainty estimation，但是没有一种方法在医学图像分析中被强调。这篇论文提供了一项全面的比较研究，评估了器官 segmentation 中epistemic uncertainty quantification 方法的准确性、uncertainty calibration和可扩展性。我们提供了每种方法的优缺点、out-of-distribution检测能力和未来改进的建议。这些发现有助于开发可靠和可靠的模型，以获得准确的 segmentation 结果，同时有效地量化epistemic uncertainty。
</details></li>
</ul>
<hr>
<h2 id="Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions"><a href="#Brain-Tumor-Detection-Based-on-a-Novel-and-High-Quality-Prediction-of-the-Tumor-Pixel-Distributions" class="headerlink" title="Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions"></a>Brain Tumor Detection Based on a Novel and High-Quality Prediction of the Tumor Pixel Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07495">http://arxiv.org/abs/2308.07495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Sun, Chunyan Wang<br>for:* The paper proposes a system for detecting brain tumors in 3D MRI brain scans of Flair modality.methods:* The system uses a 2D histogram presentation to comprehend the gray-level distribution and pixel-location distribution of a 3D object.* It exploits the left-right asymmetry of a brain structure to establish particular 2D histograms, which are then modulated to attenuate irrelevant elements.* The system predicts the tumor pixel distribution in 3 steps, on the axial, coronal, and sagittal slice series, respectively.results:* The system delivers very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs.* The system achieves this at an extremely low computation cost and without the need for training.Here is the answer in Simplified Chinese text:for:* 这篇论文提出了一种用于检测大脑肿瘤的系统，该系统使用3D MRI脑部扫描的FLAIR模式。methods:* 该系统使用2D histogram展示来理解肿瘤区域的灰度分布和像素位置分布。* 它利用脑结构的左右偏好来建立特定的2D histogram，并对其进行减小。* 系统在axial、coronal和sagittal slice series上预测肿瘤像素分布，并在每个步骤中使用预测结果来identify&#x2F;remove肿瘤自由的 slice。results:* 系统实现了非常好的肿瘤检测结果，与单模态输入的state-of-the-art CNN系统相当。* 系统在计算成本非常低的情况下实现了这一结果，而无需训练。<details>
<summary>Abstract</summary>
In this paper, we propose a system to detect brain tumor in 3D MRI brain scans of Flair modality. It performs 2 functions: (a) predicting gray-level and locational distributions of the pixels in the tumor regions and (b) generating tumor mask in pixel-wise precision. To facilitate 3D data analysis and processing, we introduced a 2D histogram presentation that comprehends the gray-level distribution and pixel-location distribution of a 3D object. In the proposed system, particular 2D histograms, in which tumor-related feature data get concentrated, are established by exploiting the left-right asymmetry of a brain structure. A modulation function is generated from the input data of each patient case and applied to the 2D histograms to attenuate the element irrelevant to the tumor regions. The prediction of the tumor pixel distribution is done in 3 steps, on the axial, coronal and sagittal slice series, respectively. In each step, the prediction result helps to identify/remove tumor-free slices, increasing the tumor information density in the remaining data to be applied to the next step. After the 3-step removal, the 3D input is reduced to a minimum bounding box of the tumor region. It is used to finalize the prediction and then transformed into a 3D tumor mask, by means of gray level thresholding and low-pass-based morphological operations. The final prediction result is used to determine the critical threshold. The proposed system has been tested extensively with the data of more than one thousand patient cases in the datasets of BraTS 2018~21. The test results demonstrate that the predicted 2D histograms have a high degree of similarity with the true ones. The system delivers also very good tumor detection results, comparable to those of state-of-the-art CNN systems with mono-modality inputs, which is achieved at an extremely low computation cost and no need for training.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种系统，用于检测脑肿瘤在3D MRI脑内部分模式下的检测。它执行了两个功能：（a）预测肿瘤区域中像素的灰度和位坐标分布，（b）生成精准的肿瘤面积。为了便于3D数据分析和处理，我们引入了2D分布图表示法，该法涵盖了肿瘤区域中像素的灰度分布和位坐标分布。在我们提出的系统中，特定的2D分布，在其中肿瘤相关特征数据受集中，被建立了。然后，通过对输入数据的每个患者案例中的偏置函数进行应用，以减少不相关于肿瘤区域的元素。肿瘤像素分布预测在 axial、coronal 和 sagittal 三个方向上进行了3步骤逐步进行，每步骤结果帮助确定/移除肿瘤不存在的剖面，从而提高肿瘤信息的浓度在剩下的数据中，并应用到下一步。经过3步 removals，输入3D数据被减少到最小 bounding box 的肿瘤区域。它被用于最终预测，并使用灰度阈值和低通过的杂谱操作来转换为3D肿瘤面积。测试结果表明，预测的2D分布与实际分布有高度的相似性。系统还提供了非常好的肿瘤检测结果，与STATE-OF-THE-ART CNN系统的单模式输入相比，并且在极低的计算成本下达到了这一点，无需训练。
</details></li>
</ul>
<hr>
<h2 id="Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis"><a href="#Space-Object-Identification-and-Classification-from-Hyperspectral-Material-Analysis" class="headerlink" title="Space Object Identification and Classification from Hyperspectral Material Analysis"></a>Space Object Identification and Classification from Hyperspectral Material Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07481">http://arxiv.org/abs/2308.07481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimiliano Vasile, Lewis Walker, Andrew Campbell, Simao Marto, Paul Murray, Stephen Marshall, Vasili Savitski</li>
<li>for: 本研究旨在提取不知名空间物体的干涉特征信息，并使用这些信息来确定物体的物理组成。</li>
<li>methods: 本研究使用了两种材料标识和分类技术：一种基于机器学习，另一种基于最小二乘匹配known спектры库。通过这些信息，一种监督式机器学习算法用于将物体分类为不同类别，根据检测到物体上的材料。</li>
<li>results: 研究结果表明，当材料库缺失一种物质时，材料分类方法的行为会受到影响。此外，研究还发现在不理想的天气条件下，材料分类方法的行为也会受到影响。最终，文章将展示一些初步的空间物体识别和分类结果。<details>
<summary>Abstract</summary>
This paper presents a data processing pipeline designed to extract information from the hyperspectral signature of unknown space objects. The methodology proposed in this paper determines the material composition of space objects from single pixel images. Two techniques are used for material identification and classification: one based on machine learning and the other based on a least square match with a library of known spectra. From this information, a supervised machine learning algorithm is used to classify the object into one of several categories based on the detection of materials on the object. The behaviour of the material classification methods is investigated under non-ideal circumstances, to determine the effect of weathered materials, and the behaviour when the training library is missing a material that is present in the object being observed. Finally the paper will present some preliminary results on the identification and classification of space objects.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hyperspectral signature" is translated as "多спектル特征" (duō yán jīng)* "material composition" is translated as "物质组成" (wù zhì zhōng)* "machine learning" is translated as "机器学习" (jī shì xué xí)* "least square match" is translated as "最小二乘匹配" (zuì xiǎo èr chuī pīng pái)* "training library" is translated as "训练库" (xùn xí kù)* "weathered materials" is translated as "气候变化的材料" (qì hòu biàn gē de zhì lǐ)
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression"><a href="#Probabilistic-MIMO-U-Net-Efficient-and-Accurate-Uncertainty-Estimation-for-Pixel-wise-Regression" class="headerlink" title="Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression"></a>Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07477">http://arxiv.org/abs/2308.07477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonbaumann/mimo-unet">https://github.com/antonbaumann/mimo-unet</a></li>
<li>paper_authors: Anton Baumann, Thomas Roßberg, Michael Schmitt</li>
<li>for: 这个论文的目的是提高机器学习模型的可靠性和可读性，特别是在高度关键的实际应用场景中。</li>
<li>methods: 这篇论文使用了多输入多输出（MIMO）框架，利用深度神经网络的过参数化来进行像素级回归任务。它还引入了一种同步多个子网络性能的新程序。</li>
<li>results: 对两个正交的数据集进行了全面的评估，显示MIMO U-Net模型具有与现有模型相当的准确率，更好的折衔在正常数据上，robust的对于异常数据检测能力，并且具有较小的参数大小和更快的推理时间。代码可以在github.com&#x2F;antonbaumann&#x2F;MIMO-Unet中找到。<details>
<summary>Abstract</summary>
Uncertainty estimation in machine learning is paramount for enhancing the reliability and interpretability of predictive models, especially in high-stakes real-world scenarios. Despite the availability of numerous methods, they often pose a trade-off between the quality of uncertainty estimation and computational efficiency. Addressing this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework -- an approach exploiting the overparameterization of deep neural networks -- for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. For that purpose, we adapted the U-Net architecture to train multiple subnetworks within a single model, harnessing the overparameterization in deep neural networks. Additionally, we introduce a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and considerable improvements in parameter size and inference time. Code available at github.com/antonbaumann/MIMO-Unet
</details>
<details>
<summary>摘要</summary>
Machine learning 中的不确定性估计是对预测模型的可靠性和可解释性提高的重要环节,特别是在高度的实际应用场景中。Despite the numerous methods available, they often involve a trade-off between the quality of uncertainty estimation and computational efficiency. To address this challenge, we present an adaptation of the Multiple-Input Multiple-Output (MIMO) framework for pixel-wise regression tasks. Our MIMO variant expands the applicability of the approach from simple image classification to broader computer vision domains. To achieve this, we adapted the U-Net architecture to train multiple subnetworks within a single model, leveraging the overparameterization in deep neural networks. Additionally, we propose a novel procedure for synchronizing subnetwork performance within the MIMO framework. Our comprehensive evaluations of the resulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracy to existing models, superior calibration on in-distribution data, robust out-of-distribution detection capabilities, and significant improvements in parameter size and inference time. 相关代码可以在github.com/antonbaumann/MIMO-Unet 上找到。
</details></li>
</ul>
<hr>
<h2 id="Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation"><a href="#Large-kernel-Attention-for-Efficient-and-Robust-Brain-Lesion-Segmentation" class="headerlink" title="Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation"></a>Large-kernel Attention for Efficient and Robust Brain Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07251">http://arxiv.org/abs/2308.07251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liamchalcroft/mdunet">https://github.com/liamchalcroft/mdunet</a></li>
<li>paper_authors: Liam Chalcroft, Ruben Lourenço Pereira, Mikael Brudfors, Andrew S. Kayser, Mark D’Esposito, Cathy J. Price, Ioannis Pappas, John Ashburner</li>
<li>for: 这个论文是为了解决医疗影像分类 задачі中的深度学习模型效果不高、缺乏调节不变性的问题。</li>
<li>methods: 这个论文提出了一种基于全条件对称化Transformer块的U-Net架构，以模型3D脑膜疾病分类中的长距离互动。</li>
<li>results: 这个模型能够提供最大的折衔点，即性能与现有State-of-the-art相当，并且具有调节不变性和对称性的优点。<details>
<summary>Abstract</summary>
Vision transformers are effective deep learning models for vision tasks, including medical image segmentation. However, they lack efficiency and translational invariance, unlike convolutional neural networks (CNNs). To model long-range interactions in 3D brain lesion segmentation, we propose an all-convolutional transformer block variant of the U-Net architecture. We demonstrate that our model provides the greatest compromise in three factors: performance competitive with the state-of-the-art; parameter efficiency of a CNN; and the favourable inductive biases of a transformer. Our public implementation is available at https://github.com/liamchalcroft/MDUNet .
</details>
<details>
<summary>摘要</summary>
“vision transformer”是深度学习模型，用于视觉任务，包括医疗图像分割。然而，它缺乏效率和翻译不变性，与卷积神经网络（CNN）不同。为了在3D脑损害分割中模型长距离交互，我们提议一种具有U-Net架构的所有卷积转换器块变体。我们示示了我们的模型在三个因素中均提供了最大的妥协：与现状前景竞争性的性能; 参数效率与CNN相同; 以及转换器的有利假设。我们的公共实现可以在https://github.com/liamchalcroft/MDUNet上找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/15/eess.IV_2023_08_15/" data-id="clly4xtga00f3vl882x64dz3i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.LG_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.LG_2023_08_14/">cs.LG - 2023-08-14 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift"><a href="#Distance-Matters-For-Improving-Performance-Estimation-Under-Covariate-Shift" class="headerlink" title="Distance Matters For Improving Performance Estimation Under Covariate Shift"></a>Distance Matters For Improving Performance Estimation Under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07223">http://arxiv.org/abs/2308.07223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/melanibe/distance_matters_performance_estimation">https://github.com/melanibe/distance_matters_performance_estimation</a></li>
<li>paper_authors: Mélanie Roschewitz, Ben Glocker</li>
<li>for: 本研究旨在提高 covariate shift 下的性能估算，尤其是在敏感应用场景下。</li>
<li>methods: 该研究提出了一种基于 distance 的方法，通过检查测试样本与预期的训练分布之间的距离，以避免基于不可靠的模型输出来估算性能。</li>
<li>results: 研究在 13 个图像分类任务上进行了实验，并在各种自然和 sintetic 分布shift 下达到了 median 相对 MAE 改进率为 27%，并在 10 个任务中达到了最佳基eline。<details>
<summary>Abstract</summary>
Performance estimation under covariate shift is a crucial component of safe AI model deployment, especially for sensitive use-cases. Recently, several solutions were proposed to tackle this problem, most leveraging model predictions or softmax confidence to derive accuracy estimates. However, under dataset shifts, confidence scores may become ill-calibrated if samples are too far from the training distribution. In this work, we show that taking into account distances of test samples to their expected training distribution can significantly improve performance estimation under covariate shift. Precisely, we introduce a "distance-check" to flag samples that lie too far from the expected distribution, to avoid relying on their untrustworthy model outputs in the accuracy estimation step. We demonstrate the effectiveness of this method on 13 image classification tasks, across a wide-range of natural and synthetic distribution shifts and hundreds of models, with a median relative MAE improvement of 27% over the best baseline across all tasks, and SOTA performance on 10 out of 13 tasks. Our code is publicly available at https://github.com/melanibe/distance_matters_performance_estimation.
</details>
<details>
<summary>摘要</summary>
性能估计下 covariate shift 是安全 AI 模型部署中的一个关键组件，尤其是在敏感应用场景下。最近，一些解决方案被提出来解决这个问题，大多数都是基于模型预测或软max信任来 derive 准确性估计。然而，在数据集 shift 下，信任度分数可能会变得不准确，如果样本太far away from the training distribution。在这种情况下，我们表明可以通过考虑测试样本与预期的训练分布之间的距离来进行性能估计。我们引入了一种"距离检查"来检测测试样本是否位于预期的训练分布中，以避免基于不可靠的模型输出来进行准确性估计。我们在 13 个图像分类任务上进行了实验，包括自然和合成分布 shift，以及多达百个模型， median 相对误差改进率为 27%，并在所有任务上达到最佳基eline的表现。我们的代码可以在 <https://github.com/melanibe/distance_matters_performance_estimation> 上获取。
</details></li>
</ul>
<hr>
<h2 id="AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes"><a href="#AudioFormer-Audio-Transformer-learns-audio-feature-representations-from-discrete-acoustic-codes" class="headerlink" title="AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes"></a>AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07221">http://arxiv.org/abs/2308.07221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LZH-0225/AudioFormer">https://github.com/LZH-0225/AudioFormer</a></li>
<li>paper_authors: Zhaohui Li, Haitao Wang, Xinghua Jiang</li>
<li>for: 这篇论文是为了学习音频特征表示，通过自然语言理解（NLU）的新角度，并使用神经网络音码器模型生成抽象的音频代码，然后使用这些代码训练马斯克隐藏语言模型（MLM）来获得高质量的音频表示。</li>
<li>methods: 这篇论文使用了一种新的多 positivesample Contrastive（MPC）学习方法，它可以学习多个抽象的音频代码之间的共同表示，从而提高音频表示质量。具体来说，首先使用一个神经网络音码器模型生成抽象的音频代码，然后使用这些代码训练一个马斯克隐藏语言模型（MLM），最后使用MPC学习方法来学习多个抽象的音频代码之间的共同表示。</li>
<li>results: 根据实验结果，AudioFormer在多个数据集上达到了显著提高的性能，甚至超过了一些音视频多模态分类模型。具体来说，AudioFormer在AudioSet（2M,20K）、FSD50K等数据集上的性能分别为53.9、45.1和65.6。<details>
<summary>Abstract</summary>
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures collaborative representations among distinct positive samples. Our research outcomes demonstrate that AudioFormer attains significantly improved performance compared to prevailing monomodal audio classification models across multiple datasets,and even outperforms audio-visual multimodal classification models on select datasets. Specifically,our approach achieves remarkable results on datasets including AudioSet (2M,20K),and FSD50K,with performance scores of 53.9,45.1,and 65.6,respectively. We have openly shared both the code and models: https://github.com/LZH-0225/AudioFormer.git.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法 named AudioFormer，它通过获取逻辑音频编码并进一步练习其为音频分类任务进行学习。我们首先提出了一种新的视角，即视音频分类任务为自然语言理解（NLU）的一种形式。利用现有的神经网络音频编码器模型，我们生成了逻辑音频编码，并使用其训练一个封面语言模型（MLM），从而获得了高质量的音频特征表示。此外，我们还开拓了多个正样本对比（MPC）学习方法的应用。这种方法可以在同一个音频输入中学习多个独立的逻辑音频编码之间的共同表示。在我们的实验中，我们将逻辑音频编码视为文本数据，并使用cloze-like方法训练一个封面语言模型，最终获得了高质量的音频表示。尤其是，MPC学习技术可以有效捕捉多个正样本之间的协作表示。我们的研究结果显示，AudioFormer在多个数据集上达到了 significatively提高的性能，甚至超过了多模态音视频分类模型在一些数据集上。具体来说，我们的方法在AudioSet（2M,20K）、FSD50K等数据集上获得了53.9、45.1和65.6的性能分数。我们已经在 GitHub 上公开了代码和模型：https://github.com/LZH-0225/AudioFormer.git。
</details></li>
</ul>
<hr>
<h2 id="Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data"><a href="#Generating-Individual-Trajectories-Using-GPT-2-Trained-from-Scratch-on-Encoded-Spatiotemporal-Data" class="headerlink" title="Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data"></a>Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07940">http://arxiv.org/abs/2308.07940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taizo Horikomi, Shouji Fujimoto, Atushi Ishikawa, Takayuki Mizuno</li>
<li>for: 本研究用于构建一个基于GPT-2语言模型的深度学习模型，用于生成受环境因素和个人特征 influencing的日常行走路径。</li>
<li>methods: 研究使用了地理坐标转换为特定的位置符号，并将每天的行走路径表示为一个序列符号。通过训练GPT-2架构，实现了从零开始训练一个深度学习模型，用于生成受环境因素和个人特征 influencing的日常行走路径。</li>
<li>results: 研究得出了一个基于GPT-2语言模型的深度学习模型，可以生成受环境因素和个人特征 influencing的日常行走路径。这种模型可以帮助我们更好地理解人们的日常活动行为，并且可以用于评估不同环境和个人特征对行走路径的影响。<details>
<summary>Abstract</summary>
Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
</details>
<details>
<summary>摘要</summary>
据米榊、藤本、石川等人的研究（Front. Phys. 2022），我们将地理坐标表示为纬度和经度转化为不同的空间尺度下的特征位置符号。我们将每个日常路径作为一个序列符号，通过将唯一的时间间隔符号添加到位置符号中来嵌入它。使用GPT-2架构的自然语言模型，我们从零开始训练这个序列符号，以构建一个可以逐步生成个人日常路径的深度学习模型。environmental factor such as weather conditions和个人属性such as gender and age通过特殊符号表示，我们通过在GPT-2架构上训练这些符号和路径，可以生成受环境因素和个人属性 influencing的 trajectory。
</details></li>
</ul>
<hr>
<h2 id="Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Pediatric-Brain-Tumors-A-Novel-Approach-Using-the-CBTN-CONNECT-ASNR-MICCAI-BraTS-PEDs-2023-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data"></a>Automated Ensemble-Based Segmentation of Pediatric Brain Tumors: A Novel Approach Using the CBTN-CONNECT-ASNR-MICCAI BraTS-PEDs 2023 Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07212">http://arxiv.org/abs/2308.07212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashidhar Reddy Javaji, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这个研究的目的是为了发展用于脑膜癌的诊断技术和治疗方法。</li>
<li>methods: 这个研究使用了深度学习技术，使用了Magnetic Resonance Imaging（MRI）模式，并导入了一种新的组合方法，包括ONet和修改过的UNet，以及新的损失函数。</li>
<li>results: 这个研究获得了2023年BraTS-PEDs挑战赛的精确分类模型。使用扩展资料，包括单独和composite变数，以确保模型的稳定性和准确性。组合策略，结合ONet和UNet模型，展现了更高的效率和精确性。 lesion_wise dice scores为0.52、0.72和0.78，证明了这种组合方法的优势。<details>
<summary>Abstract</summary>
Brain tumors remain a critical global health challenge, necessitating advancements in diagnostic techniques and treatment methodologies. In response to the growing need for age-specific segmentation models, particularly for pediatric patients, this study explores the deployment of deep learning techniques using magnetic resonance imaging (MRI) modalities. By introducing a novel ensemble approach using ONet and modified versions of UNet, coupled with innovative loss functions, this study achieves a precise segmentation model for the BraTS-PEDs 2023 Challenge. Data augmentation, including both single and composite transformations, ensures model robustness and accuracy across different scanning protocols. The ensemble strategy, integrating the ONet and UNet models, shows greater effectiveness in capturing specific features and modeling diverse aspects of the MRI images which result in lesion_wise dice scores of 0.52, 0.72 and 0.78 for enhancing tumor, tumor core and whole tumor labels respectively. Visual comparisons further confirm the superiority of the ensemble method in accurate tumor region coverage. The results indicate that this advanced ensemble approach, building upon the unique strengths of individual models, offers promising prospects for enhanced diagnostic accuracy and effective treatment planning for brain tumors in pediatric brains.
</details>
<details>
<summary>摘要</summary>
脑肿仍然是全球医疗挑战，需要进一步的技术创新和治疗方法。为了应对儿童患者的年龄特定分 segmentation模型的增长需求，本研究利用深度学习技术和Magnetic Resonance Imaging（MRI）模式，探讨一种新的集成方法。通过引入ONet和修改版本的UNet模型，以及创新的损失函数，本研究实现了高精度的分割模型，为BraTS-PEDs 2023 Challenge提供了精准的分割结果。数据扩展，包括单个和复合变换，使模型具有不同扫描协议下的Robustness和准确性。集成策略，将ONet和UNet模型集成在一起，表现出更高的特征捕捉和多样化图像模型化能力，最终得到了lesion_wise dice分割率为0.52、0.72和0.78，用于涉及肿块、肿块核心和整个肿块等标签。视觉比较还证明了集成方法在精准肿块覆盖方面的优势。结果表明，这种高级集成方法，基于各个模型的特点优势，对儿童脑肿的诊断精度和有效的治疗规划具有替代性。
</details></li>
</ul>
<hr>
<h2 id="Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning"><a href="#Unified-Data-Free-Compression-Pruning-and-Quantization-without-Fine-Tuning" class="headerlink" title="Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning"></a>Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07209">http://arxiv.org/abs/2308.07209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shipeng Bai, Jun Chen, Xintian Shen, Yixuan Qian, Yong Liu</li>
<li>for: 降低神经网络的执行时间和内存占用</li>
<li>methods: 同时施行减少和量化，不需要原始训练数据集</li>
<li>results: 在大规模图像分类任务上实现了显著的改善，比如在ImageNet dataset上与 State-of-the-Art 方法相比，使用30% 减少率和6位量化，ResNet-34 网络上获得20.54%的准确率提升。<details>
<summary>Abstract</summary>
Structured pruning and quantization are promising approaches for reducing the inference time and memory footprint of neural networks. However, most existing methods require the original training dataset to fine-tune the model. This not only brings heavy resource consumption but also is not possible for applications with sensitive or proprietary data due to privacy and security concerns. Therefore, a few data-free methods are proposed to address this problem, but they perform data-free pruning and quantization separately, which does not explore the complementarity of pruning and quantization. In this paper, we propose a novel framework named Unified Data-Free Compression(UDFC), which performs pruning and quantization simultaneously without any data and fine-tuning process. Specifically, UDFC starts with the assumption that the partial information of a damaged(e.g., pruned or quantized) channel can be preserved by a linear combination of other channels, and then derives the reconstruction form from the assumption to restore the information loss due to compression. Finally, we formulate the reconstruction error between the original network and its compressed network, and theoretically deduce the closed-form solution. We evaluate the UDFC on the large-scale image classification task and obtain significant improvements over various network architectures and compression methods. For example, we achieve a 20.54% accuracy improvement on ImageNet dataset compared to SOTA method with 30% pruning ratio and 6-bit quantization on ResNet-34.
</details>
<details>
<summary>摘要</summary>
《结构化截割和量化是神经网络减少推理时间和内存占用的有力方法。然而，大多数现有方法需要原始训练数据来细化模型，这不仅带来了重量级的资源占用，而且对于敏感或商业机密数据来说，由于隐私和安全问题，无法进行训练。因此，一些无数据方法被提出，但它们只是分别进行无数据截割和量化，没有利用截割和量化的衔接。在本文中，我们提出了一种名为统一无数据压缩（UDFC）的新框架，它在无数据情况下同时进行截割和量化。具体来说，UDFC从假设损坏（例如截割或量化）通道的部分信息可以通过其他通道的线性组合来保留，然后从假设中 derive 重建形式来恢复因压缩而产生的信息损失。最后，我们将重建错误 между 原始网络和压缩后的网络，并理论上解出closed-form解决方案。我们对大规模图像分类任务进行评估，并在不同的网络架构和压缩方法下获得了显著的改进。例如，我们在ImageNet数据集上达到了30%截割率和6位量化的SOTA方法比20.54%的精度提升。》
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-the-Training-of-Neural-Support-Vector-Machines"><a href="#Algorithms-for-the-Training-of-Neural-Support-Vector-Machines" class="headerlink" title="Algorithms for the Training of Neural Support Vector Machines"></a>Algorithms for the Training of Neural Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07204">http://arxiv.org/abs/2308.07204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Lars Simon, Manuel Radons</li>
<li>for: 本研究使用神经支持向量机（NSVM）结构，以汲取领域知识在模型设计中。</li>
<li>methods: 本文提出了一组基于 Pegasos 算法的 NSVM 训练算法，并通过解决一系列标准机器学习任务来证明其效果。</li>
<li>results: 本研究通过实验和分析，证明了 NSVM 在一些标准机器学习任务中的表现，并验证了领域知识的Integration在模型设计中的重要性。<details>
<summary>Abstract</summary>
Neural support vector machines (NSVMs) allow for the incorporation of domain knowledge in the design of the model architecture. In this article we introduce a set of training algorithms for NSVMs that leverage the Pegasos algorithm and provide a proof of concept by solving a set of standard machine learning tasks.
</details>
<details>
<summary>摘要</summary>
神经支持向量机器 (NSVM) 允许在模型建立的架构中包含领域知识。在这篇文章中，我们介绍了一组用 Pegasos 算法进行训练的 NSVM 训练算法，并通过解决一组标准机器学习任务来提供证明。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Neural-Categorical-Priors-for-Physics-Based-Character-Control"><a href="#Neural-Categorical-Priors-for-Physics-Based-Character-Control" class="headerlink" title="Neural Categorical Priors for Physics-Based Character Control"></a>Neural Categorical Priors for Physics-Based Character Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07200">http://arxiv.org/abs/2308.07200</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Tencent-RoboticsX/NCP">https://github.com/Tencent-RoboticsX/NCP</a></li>
<li>paper_authors: Qingxu Zhu, He Zhang, Mengting Lan, Lei Han</li>
<li>for: 这paper aimed to propose a new learning framework for controlling physics-based characters with naturalistic behaviors.</li>
<li>methods: The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips, and then uses a discrete information bottleneck and prior shifting to generate high-quality life-like behaviors.</li>
<li>results: The proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism, as demonstrated through comprehensive experiments using humanoid characters on two challenging downstream tasks.Here’s the Chinese version of the three key points:</li>
<li>for: 这paper的目标是提出一种新的学习框架，用于控制基于物理的角色表现出自然的行为。</li>
<li>methods: 提议的方法使用了反馈学习（RL）来跟踪和模仿生活中的自然运动，然后使用一种简化信息瓶颈和征识偏移来生成高质量的自然行为。</li>
<li>results: 提议的框架可以控制角色表现出较高质量的行为策略、多样性和真实性，这得到通过对人工智能角色进行了两个复杂的下游任务的实验证明。<details>
<summary>Abstract</summary>
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervision of the encoder's output, it follows the original motion clip distribution in the dataset and could lead to imbalanced behaviors in our setting. To address the issue, we further propose a technique named prior shifting to adjust the prior distribution using curiosity-driven RL. The outcome distribution is demonstrated to offer sufficient behavioral diversity and significantly facilitates upper-level policy learning for downstream tasks. We conduct comprehensive experiments using humanoid characters on two challenging downstream tasks, sword-shield striking and two-player boxing game. Our results demonstrate that the proposed framework is capable of controlling the character to perform considerably high-quality movements in terms of behavioral strategies, diversity, and realism. Videos, codes, and data are available at https://tencent-roboticsx.github.io/NCP/.
</details>
<details>
<summary>摘要</summary>
最近的研究发展强化 reuse motion prior 技术已经证明其能够生成自然的行为。在这篇论文中，我们提出一种新的学习框架，用于控制基于物理的人物，并且能够提高 Motion 质量和多样性，至今为止的现有方法。我们使用 reinforcement learning（RL）来初始化和模仿生命like 运动从未结构化运动clip 中提取有用信息，并使用 discrete information bottleneck，与 Vector Quantized Variational AutoEncoder（VQ-VAE）相同。这种结构压缩运动clip 中最重要的信息，并将其压缩成一个紧凑的、有用的幂论空间中。通过在训练过的 categorical prior distribution 中采样代码，可以生成高质量的生命like 行为。尽管这个 prior distribution 可以通过encoder的输出进行训练，但它遵循原始运动clip 的分布，这可能会导致行为偏斜。为了解决这个问题，我们提出了一种名为 prior shifting 的技术，通过 Curiosity-driven RL 来调整 prior distribution。结果显示，我们的方法可以提供足够的行为多样性，并且能够帮助上层策略学习以下渠道任务。我们在人iform 角色上进行了全面的实验，并使用剑盾战斗和两个玩家简易拳击游戏。我们的结果表明，我们的框架能够控制人iform 角色进行较高质量的运动，包括行为策略、多样性和真实性。视频、代码和数据可以在https://tencent-roboticsx.github.io/NCP/ 获取。
</details></li>
</ul>
<hr>
<h2 id="Explaining-Black-Box-Models-through-Counterfactuals"><a href="#Explaining-Black-Box-Models-through-Counterfactuals" class="headerlink" title="Explaining Black-Box Models through Counterfactuals"></a>Explaining Black-Box Models through Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07198">http://arxiv.org/abs/2308.07198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliatrustworthyai/counterfactualexplanations.jl">https://github.com/juliatrustworthyai/counterfactualexplanations.jl</a></li>
<li>paper_authors: Patrick Altmeyer, Arie van Deursen, Cynthia C. S. Liem</li>
<li>for: 用于解释人工智能模型的输出</li>
<li>methods: 使用Counterfactual Explanations（CE）和Algorithmic Recourse（AR）生成解释和修复方法</li>
<li>results: 可以提供实用和现实的修复方法，帮助改善模型的输出结果<details>
<summary>Abstract</summary>
We present CounterfactualExplanations.jl: a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box models in Julia. CE explain how inputs into a model need to change to yield specific model predictions. Explanations that involve realistic and actionable changes can be used to provide AR: a set of proposed actions for individuals to change an undesirable outcome for the better. In this article, we discuss the usefulness of CE for Explainable Artificial Intelligence and demonstrate the functionality of our package. The package is straightforward to use and designed with a focus on customization and extensibility. We envision it to one day be the go-to place for explaining arbitrary predictive models in Julia through a diverse suite of counterfactual generators.
</details>
<details>
<summary>摘要</summary>
我们介绍CounterfactualExplanations.jl：一个用于生成反对方案解释（CE）和算法补救（AR）的 julia 套件。CE 解释了模型对于特定预测所需的输入更改，这些解释可以提供AR：一组可行和有效的改善结果的建议。在这篇文章中，我们讨论了CE 在可解释人工智能中的用途，并详细介绍套件的功能。套件易于使用，设计为可自定义和扩展。我们将它作为 julia 中解释任何预测模型的首选之地。
</details></li>
</ul>
<hr>
<h2 id="gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling"><a href="#gSASRec-Reducing-Overconfidence-in-Sequential-Recommendation-Trained-with-Negative-Sampling" class="headerlink" title="gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling"></a>gSASRec: Reducing Overconfidence in Sequential Recommendation Trained with Negative Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07192">http://arxiv.org/abs/2308.07192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asash/gsasrec">https://github.com/asash/gsasrec</a></li>
<li>paper_authors: Aleksandr Petrov, Craig Macdonald</li>
<li>for: This paper aims to address the issue of overconfidence in recommendation models, specifically in the popular SASRec model, and to propose a novel loss function and improved model that can mitigate overconfidence and improve performance.</li>
<li>methods: The paper proposes a novel Generalised Binary Cross-Entropy Loss function (gBCE) and a modified version of SASRec called gSASRec, which deploys an increased number of negatives and the gBCE loss to mitigate overconfidence.</li>
<li>results: The paper shows through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem, and can outperform BERT4Rec in terms of NDCG score (e.g. +9.47% on the MovieLens-1M dataset) while requiring less training time (e.g. -73% training time on MovieLens-1M). Additionally, gSASRec is suitable for large datasets with more than 1 million items, unlike BERT4Rec.<details>
<summary>Abstract</summary>
A large catalogue size is one of the central challenges in training recommendation models: a large number of items makes them memory and computationally inefficient to compute scores for all items during training, forcing these models to deploy negative sampling. However, negative sampling increases the proportion of positive interactions in the training data, and therefore models trained with negative sampling tend to overestimate the probabilities of positive interactions a phenomenon we call overconfidence. While the absolute values of the predicted scores or probabilities are not important for the ranking of retrieved recommendations, overconfident models may fail to estimate nuanced differences in the top-ranked items, resulting in degraded performance. In this paper, we show that overconfidence explains why the popular SASRec model underperforms when compared to BERT4Rec. This is contrary to the BERT4Rec authors explanation that the difference in performance is due to the bi-directional attention mechanism. To mitigate overconfidence, we propose a novel Generalised Binary Cross-Entropy Loss function (gBCE) and theoretically prove that it can mitigate overconfidence. We further propose the gSASRec model, an improvement over SASRec that deploys an increased number of negatives and the gBCE loss. We show through detailed experiments on three datasets that gSASRec does not exhibit the overconfidence problem. As a result, gSASRec can outperform BERT4Rec (e.g. +9.47% NDCG on the MovieLens-1M dataset), while requiring less training time (e.g. -73% training time on MovieLens-1M). Moreover, in contrast to BERT4Rec, gSASRec is suitable for large datasets that contain more than 1 million items.
</details>
<details>
<summary>摘要</summary>
庞大的目录大小是训练推荐模型的中心挑战之一：大量的项目使得计算分数的计算成本高昂，使得这些模型不能在训练过程中计算所有项目的分数，因此需要使用负样本。然而，使用负样本增加了正交互动的比例在训练数据中，因此模型受负样本训练后会过度估计正交互动，这种现象我们称为过自信。这会导致模型估计排名顺序中的差异不准确，从而导致性能下降。在这篇论文中，我们表明了SASRec模型在比较BERT4Rec时的下降性能是由于过自信而不是BI-directional attention机制的解释。为了 Mitigate overconfidence，我们提出了一种通用二进制十字积分损失函数（gBCE），并证明了它可以 Mitigate overconfidence。此外，我们还提出了一种改进SASRec模型的gSASRec模型，该模型通过增加负样本数和gBCE损失函数来减少过自信。我们通过对三个数据集进行详细的实验，证明了gSASRec模型不受过自信问题。因此，gSASRec可以在MovieLens-1M数据集上超过BERT4Rec（+9.47% NDCG），同时具有较少的训练时间(-73% 训练时间）。此外，gSASRec模型适用于大于100万个项目的大数据集。
</details></li>
</ul>
<hr>
<h2 id="Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity"><a href="#Improving-ICD-based-semantic-similarity-by-accounting-for-varying-degrees-of-comorbidity" class="headerlink" title="Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity"></a>Improving ICD-based semantic similarity by accounting for varying degrees of comorbidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07359">http://arxiv.org/abs/2308.07359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Janosch Schneider, Marius Adler, Christoph Ammer-Herrmenau, Alexander Otto König, Ulrich Sax, Jonas Hügel</li>
<li>for: 这篇论文的目的是为了找到类似的病人，以便评估治疗结果和临床决策支持。</li>
<li>methods: 这篇论文使用了世界各地医疗病理分类（ICD）代码，将病人的病理特征转换为数据集，然后使用Semantic Similarity算法进行相似性计算。</li>
<li>results: 这篇论文的结果显示，使用了我们提出的标准化运算符 Lateral Epicritical Density 和 Bipartite Graph Matching 的 комbination，可以实现最高的相似性分析效果，与专家评价的真实相似性相符。<details>
<summary>Abstract</summary>
Finding similar patients is a common objective in precision medicine, facilitating treatment outcome assessment and clinical decision support. Choosing widely-available patient features and appropriate mathematical methods for similarity calculations is crucial. International Statistical Classification of Diseases and Related Health Problems (ICD) codes are used worldwide to encode diseases and are available for nearly all patients. Aggregated as sets consisting of primary and secondary diagnoses they can display a degree of comorbidity and reveal comorbidity patterns. It is possible to compute the similarity of patients based on their ICD codes by using semantic similarity algorithms. These algorithms have been traditionally evaluated using a single-term expert rated data set.   However, real-word patient data often display varying degrees of documented comorbidities that might impair algorithm performance. To account for this, we present a scale term that considers documented comorbidity-variance. In this work, we compared the performance of 80 combinations of established algorithms in terms of semantic similarity based on ICD-code sets. The sets have been extracted from patients with a C25.X (pancreatic cancer) primary diagnosis and provide a variety of different combinations of ICD-codes. Using our scale term we yielded the best results with a combination of level-based information content, Leacock & Chodorow concept similarity and bipartite graph matching for the set similarities reaching a correlation of 0.75 with our expert's ground truth. Our results highlight the importance of accounting for comorbidity variance while demonstrating how well current semantic similarity algorithms perform.
</details>
<details>
<summary>摘要</summary>
在精度医学中，找到类似的患者是一项常见的目标，以便评估治疗结果和临床决策支持。选择广泛可用的患者特征和适当的数学方法进行相似计算是关键。国际疾病分类法（ICD）代码在全球使用，可以为大多数患者提供代码。将这些代码聚合成集合，包括主要和次要诊断，可以显示患者的诊断程度和诊断模式。可以使用语义相似算法计算患者之间的相似性。这些算法通常通过专家评分的数据集来评估。但在实际患者数据中，患者通常有不同程度的记录的相关疾病，这可能会影响算法性能。为解决这个问题，我们提出了一个权重因素，该因素考虑了记录的相关疾病变化。在这种情况下，我们比较了80种已知算法的语义相似性，基于ICD代码集。这些代码集来自患有C25.X（肝癌）主诊断的患者，并提供了不同的ICD代码组合。使用我们的权重因素，我们得到了最佳的结果，与专家的真实ground truth相匹配，相似度为0.75。我们的结果表明了考虑相关疾病变化的重要性，同时也展示了当前语义相似算法的性能。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks"><a href="#Conformal-Predictions-Enhanced-Expert-guided-Meshing-with-Graph-Neural-Networks" class="headerlink" title="Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks"></a>Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07358">http://arxiv.org/abs/2308.07358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahnobari/autosurf">https://github.com/ahnobari/autosurf</a></li>
<li>paper_authors: Amin Heyrani Nobari, Justin Rey, Suhas Kodali, Matthew Jones, Faez Ahmed</li>
<li>for: 这个论文的目的是自动生成CFD模型的网格，以提高计算流体力学的精度和效率。</li>
<li>methods: 这个论文使用图解树神经网络（GNN）和专家指导来自动生成CFD模型的网格。它还提出了一种新的3D分割算法，以及一种将预测从3D分割模型项目到CAD表面的方法。</li>
<li>results: 论文通过一个实际案例研究表明，自动生成的网格与专家生成的网格相比较，具有相似的质量，并且使得计算机程序能够正确地计算结果。此外，论文还比较了自动生成网格和适应重新分割的方法，发现自动生成网格比适应重新分割更快。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/ahnobari/AutoSurf%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ahnobari/AutoSurf上获取。</a><details>
<summary>Abstract</summary>
Computational Fluid Dynamics (CFD) is widely used in different engineering fields, but accurate simulations are dependent upon proper meshing of the simulation domain. While highly refined meshes may ensure precision, they come with high computational costs. Similarly, adaptive remeshing techniques require multiple simulations and come at a great computational cost. This means that the meshing process is reliant upon expert knowledge and years of experience. Automating mesh generation can save significant time and effort and lead to a faster and more efficient design process. This paper presents a machine learning-based scheme that utilizes Graph Neural Networks (GNN) and expert guidance to automatically generate CFD meshes for aircraft models. In this work, we introduce a new 3D segmentation algorithm that outperforms two state-of-the-art models, PointNet++ and PointMLP, for surface classification. We also present a novel approach to project predictions from 3D mesh segmentation models to CAD surfaces using the conformal predictions method, which provides marginal statistical guarantees and robust uncertainty quantification and handling. We demonstrate that the addition of conformal predictions effectively enables the model to avoid under-refinement, hence failure, in CFD meshing even for weak and less accurate models. Finally, we demonstrate the efficacy of our approach through a real-world case study that demonstrates that our automatically generated mesh is comparable in quality to expert-generated meshes and enables the solver to converge and produce accurate results. Furthermore, we compare our approach to the alternative of adaptive remeshing in the same case study and find that our method is 5 times faster in the overall process of simulation. The code and data for this project are made publicly available at https://github.com/ahnobari/AutoSurf.
</details>
<details>
<summary>摘要</summary>
计算流体动力学（CFD）在不同的工程领域都广泛应用，但是准确的计算受到域的适当精细化的约束。高精度的精细化可能确保准确性，但是来自高计算成本。同时，适应精细化技术需要多次 simulations和高计算成本。这意味着精细化过程取决于专家知识和年代经验。自动生成精细化可以节省很多时间和努力，并且可以加速设计过程。本文提出了基于机器学习的方案，利用图像神经网络（GNN）和专家指导来自动生成飞机模型的CFD精细化。在这个工作中，我们提出了一种新的3D分割算法，其在surface classification方面比PointNet++和PointMLP两种状态态模型更高效。我们还提出了一种将预测从3D分割模型 projet到CAD surface的方法，使用确ensional predictions方法，该方法提供了边缘统计保证和稳定的不确定性评估和处理。我们发现，通过添加确ensional predictions，我们的方法可以避免精细化失败，即下REFINE。最后，我们通过一个实际的案例研究证明了我们的自动生成精细化与专家生成精细化相比质量相同，并且使得计算器能够 converges和生成准确结果。此外，我们与适应精细化的相同案例进行比较，发现我们的方法比适应精细化5倍快。我们将项目的代码和数据公开发布在https://github.com/ahnobari/AutoSurf上。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements"><a href="#Efficient-Learning-of-Quantum-States-Prepared-With-Few-Non-Clifford-Gates-II-Single-Copy-Measurements" class="headerlink" title="Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements"></a>Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates II: Single-Copy Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07175">http://arxiv.org/abs/2308.07175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang</li>
<li>for: 学习 $n$-qubit 量子状态，输出由具有最多 $t$ 单位逻辑门的电路。</li>
<li>methods: 使用单Copy测量来学习这类状态，而不需要双Copy测量。</li>
<li>results: 可以在 $\mathsf{poly}(n,2^t,1&#x2F;\epsilon)$ 时间和样本数量内学习这类状态，与之前所有的算法相同。<details>
<summary>Abstract</summary>
Recent work has shown that $n$-qubit quantum states output by circuits with at most $t$ single-qubit non-Clifford gates can be learned to trace distance $\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples. All prior algorithms achieving this runtime use entangled measurements across two copies of the input state. In this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements.
</details>
<details>
<summary>摘要</summary>
最近的工作表明，$n$-粒子量子状态由具有最多$t$个单元素非束地 gates生成的电路可以使用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和样本来跟踪距离$\epsilon$。所有先前的算法达到这个 runtime 都使用了两份输入状态的排合测试。在这种工作中，我们给出了同样的效率的算法，可以使用单份输入状态来学习同一类型的状态。Note:* " $n$-qubit quantum states" is translated as " $n$-粒子量子状态" (n-qubit quantum states)* "circuits with at most $t$ single-qubit non-Clifford gates" is translated as "具有最多$t$个单元素非束地 gates的电路" (circuits with at most t single-qubit non-Clifford gates)* "can be learned to trace distance $\epsilon$ using $\mathsf{poly}(n,2^t,1/\epsilon)$ time and samples" is translated as "可以使用$\mathsf{poly}(n,2^t,1/\epsilon)$时间和样本来跟踪距离$\epsilon$" (can be learned to trace distance ε using polynomial time and samples)* "All prior algorithms achieving this runtime use entangled measurements across two copies of the input state" is translated as "所有先前的算法达到这个 runtime 都使用了两份输入状态的排合测试" (all previous algorithms achieving this runtime use entangled measurements across two copies of the input state)* "In this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements" is translated as "在这种工作中，我们给出了同样的效率的算法，可以使用单份输入状态来学习同一类型的状态" (in this work, we give a similarly efficient algorithm that learns the same class of states using only single-copy measurements)
</details></li>
</ul>
<hr>
<h2 id="PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation"><a href="#PitchNet-A-Fully-Convolutional-Neural-Network-for-Pitch-Estimation" class="headerlink" title="PitchNet: A Fully Convolutional Neural Network for Pitch Estimation"></a>PitchNet: A Fully Convolutional Neural Network for Pitch Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07170">http://arxiv.org/abs/2308.07170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremy Cochoy</li>
<li>for: 这项研究旨在提高人声中的抑 pitch 检测精度，以便在音乐和语音处理领域中进行更加精准的抑 pitch EXTraction。</li>
<li>methods: 该研究提出了一种基于卷积神经网络的 “PitchNet”，用于从人声中提取抑 pitch。该网络结合自相关函数和深度学习技术，以便优化抑 pitch 检测的精度。</li>
<li>results: 对于 synthetic sounds、opera recordings 和 time-stretched vowels 等数据集的评估表明，PitchNet 能够准确地检测人声中的抑 pitch。这项研究为音乐和语音处理领域中的抑 pitch EXTraction 开创了新的可能性。<details>
<summary>Abstract</summary>
In the domain of music and sound processing, pitch extraction plays a pivotal role. This research introduces "PitchNet", a convolutional neural network tailored for pitch extraction from the human singing voice, including acapella performances. Integrating autocorrelation with deep learning techniques, PitchNet aims to optimize the accuracy of pitch detection. Evaluation across datasets comprising synthetic sounds, opera recordings, and time-stretched vowels demonstrates its efficacy. This work paves the way for enhanced pitch extraction in both music and voice settings.
</details>
<details>
<summary>摘要</summary>
在音乐和声音处理领域中，抽取高度扮演着关键性的角色。本研究介绍“PitchNet”，一种适用于人声歌唱中的声调抽取 convolutional neural network（CNN）。通过对深度学习技术与自相关函数的组合，PitchNet目标是提高声调检测精度。对于 synthetic sounds、opera recording 和时间压缩词汇等数据集进行评估，PitchNet 的效果得到证明。这项工作将为音乐和声音设置中的声调抽取带来进一步的改进。Note: "Simplified Chinese" refers to the standardized form of Chinese used in mainland China, which is different from Traditional Chinese used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models"><a href="#SPEGTI-Structured-Prediction-for-Efficient-Generative-Text-to-Image-Models" class="headerlink" title="SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models"></a>SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10997">http://arxiv.org/abs/2308.10997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadeep Jayasumana, Daniel Glasner, Srikumar Ramalingam, Andreas Veit, Ayan Chakrabarti, Sanjiv Kumar</li>
<li>for: 提高文本生成图像模型的计算效率，使其能够更快地生成高质量的图像。</li>
<li>methods: 使用Markov Random Field（MRF）模型来加速 Muse 模型的推理过程，从而提高图像生成的计算效率。</li>
<li>results: 通过使用 MRF 模型，可以significantly reduce the required number of Muse prediction steps，并且在各个空间位置上编码图像元素之间的兼容性，以提高图像质量和计算效率。<details>
<summary>Abstract</summary>
Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MRF inference as a differentiable neural-network layer. Our full model, SPEGTI, uses this proposed MRF model to speed up Muse by 1.5X with no loss in output image quality.
</details>
<details>
<summary>摘要</summary>
现代文本到图像生成模型可以生成高质量的图像，这些图像不仅具有摄影真实性，还能够准确地反映文本提示。然而，这种质量来自于费时的计算成本：大多数这些模型都是迭代的，需要多次运行推理，以确保不同区域的图像与文本提示保持一致。在这种情况下，我们提出了一种轻量级的方法，使用Markov随机场（MRF）模型来实现不同区域图像的兼容性。这种方法与最近提出的Muse模型结合使用，并且可以减少Muse预测步骤的数量，从而大幅降低计算成本。我们的全模型SPEGTI使用这种MRF模型，可以帮助Muse快速推理1.5倍，而无需 sacrifi额外的图像质量。
</details></li>
</ul>
<hr>
<h2 id="Pairing-interacting-protein-sequences-using-masked-language-modeling"><a href="#Pairing-interacting-protein-sequences-using-masked-language-modeling" class="headerlink" title="Pairing interacting protein sequences using masked language modeling"></a>Pairing interacting protein sequences using masked language modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07136">http://arxiv.org/abs/2308.07136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bitbol-lab/diffpalm">https://github.com/bitbol-lab/diffpalm</a></li>
<li>paper_authors: Umberto Lupo, Damiano Sgarbossa, Anne-Florence Bitbol</li>
<li>For: The paper aims to predict which proteins interact together from their amino acid sequences.* Methods: The paper uses protein language models trained on multiple sequence alignments, specifically MSA Transformer and the EvoFormer module of AlphaFold, to pair interacting protein sequences.* Results: The proposed method, DiffPALM, outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments and improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer.Here’s the same information in Simplified Chinese:* For: 文章目标是从蛋白质序列中预测哪些蛋白质进行互作。* Methods: 文章使用多个序列对 alignment 训练的蛋白质语言模型，特别是 MSA Transformer 和 AlphaFold 的 EvoFormer 模块，来对互作蛋白质序列进行对应。* Results: 提案的方法 DiffPALM 在难度较高的多个序列对上表现出优于现有的共演化基本方法，并在一些细胞蛋白质复合物的结构预测中达到竞争性表现。<details>
<summary>Abstract</summary>
Predicting which proteins interact together from amino-acid sequences is an important task. We develop a method to pair interacting protein sequences which leverages the power of protein language models trained on multiple sequence alignments, such as MSA Transformer and the EvoFormer module of AlphaFold. We formulate the problem of pairing interacting partners among the paralogs of two protein families in a differentiable way. We introduce a method called DiffPALM that solves it by exploiting the ability of MSA Transformer to fill in masked amino acids in multiple sequence alignments using the surrounding context. MSA Transformer encodes coevolution between functionally or structurally coupled amino acids. We show that it captures inter-chain coevolution, while it was trained on single-chain data, which means that it can be used out-of-distribution. Relying on MSA Transformer without fine-tuning, DiffPALM outperforms existing coevolution-based pairing methods on difficult benchmarks of shallow multiple sequence alignments extracted from ubiquitous prokaryotic protein datasets. It also outperforms an alternative method based on a state-of-the-art protein language model trained on single sequences. Paired alignments of interacting protein sequences are a crucial ingredient of supervised deep learning methods to predict the three-dimensional structure of protein complexes. DiffPALM substantially improves the structure prediction of some eukaryotic protein complexes by AlphaFold-Multimer, without significantly deteriorating any of those we tested. It also achieves competitive performance with using orthology-based pairing.
</details>
<details>
<summary>摘要</summary>
预测 protein sequences 中的互作对是一项重要任务。我们开发了一种方法，可以将 protein sequence 中的互作对级联起来，这种方法利用了多个序列对 alignment（如 MSA Transformer 和 EvoFormer 模块）训练的 protein language model。我们将这个问题转化为一个可导的问题，并提出了一种名为 DiffPALM 的方法来解决它。DiffPALM 利用了 MSA Transformer 可以填充遮盖的氨基酸，通过周围的上下文来填充它们。MSA Transformer 编码了功能或结构上的氨基酸之间的共演化，我们表明它可以在单链数据上训练，并在不需要微调的情况下在多链数据上进行预测。与现有的共演化基本方法相比，DiffPALM 在困难的多链对 alignments 上表现出色，同时也在不需要微调的情况下进行预测。此外，DiffPALM 还可以和一种基于 state-of-the-art 蛋白质语言模型进行比较，并且在一些欧化蛋白质复合物的结构预测中表现出色。Paired alignments of interacting protein sequences 是深度学习方法预测蛋白质复合物的重要组成部分。DiffPALM 在这些复合物的结构预测中提供了重要的改进。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-is-All-a-Graph-Needs"><a href="#Natural-Language-is-All-a-Graph-Needs" class="headerlink" title="Natural Language is All a Graph Needs"></a>Natural Language is All a Graph Needs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07134">http://arxiv.org/abs/2308.07134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, Yongfeng Zhang</li>
<li>for: 本研究旨在探讨 Whether large language models (LLMs) can replace graph neural networks (GNNs) as the foundation model for graphs.</li>
<li>methods: 我们提出了 InstructGLM (Instruction-finetuned Graph Language Model)，使用自然语言指令设计了高度可扩展的 prompt，并使用自然语言描述图像的几何结构和节点特征。</li>
<li>results: 我们的方法在 ogbn-arxiv、Cora 和 PubMed 数据集上超过了所有竞争 GNN 基elines，这说明了我们的方法的有效性，并且推照generative大型语言模型为图机器学习的基础模型。<details>
<summary>Abstract</summary>
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scalable prompts based on natural language instructions, and use natural language to describe the geometric structure and node features of the graph for instruction tuning an LLM to perform learning and inference on graphs in a generative manner. Our method exceeds all competitive GNN baselines on ogbn-arxiv, Cora and PubMed datasets, which demonstrates the effectiveness of our method and sheds light on generative large language models as the foundation model for graph machine learning.
</details>
<details>
<summary>摘要</summary>
“大规模预训练语言模型，如ChatGPT，对人工智能多种研究领域产生了革命性的影响。基于Transformers的大语言模型（LLM）逐渐取代了CNNs和RNNs，统一了计算机视觉和自然语言处理领域。与独立存在的数据，如图像、视频或文本，相比，图表是一种包含丰富结构和关系信息的数据类型。同时，自然语言作为最表达力强的媒介，能够描述复杂结构。然而，将图学学习问题 integrate into the generative language modeling framework的现有工作很有限。随着大语言模型的重要性不断增长，我们需要探索 Whether LLMs可以取代GNNs作为图学基础模型。本文提出InstructGLM（基于natural language instruction的图语言模型），系统地设计了可扩展的提示，使用自然语言描述图表的结构和节点特征，并使用LLM进行图学学习和推理。我们的方法在ogbn-arxiv、Cora和PubMed数据集上都超过了所有的竞争GNN基elines，这 demonstates了我们的方法的有效性，并照亮了大语言模型作为图学基础模型的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS"><a href="#Implementation-of-The-Future-of-Drug-Discovery-QuantumBased-Machine-Learning-Simulation-QMLS" class="headerlink" title="Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)"></a>Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08561">http://arxiv.org/abs/2308.08561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yew Kee Wong, Yifan Zhou, Yan Shing Liang, Haichuan Qiu, Yu Xi Wu, Bin He</li>
<li>for: 这份研究目的是为了缩短药物开发过程的时间和成本，以及创新一种能够在三到六个月内完成整个R&amp;D过程，并且只需五十到八十千美元的成本。</li>
<li>methods: 这篇研究使用的方法包括机器学习分子生成（MLMG）和量子模拟（QS），两者共同实现了精确地预测药物的结构和功能。MLMG根据目标蛋白质的分子结构来生成可能的击中者，而QS则对这些分子进行筛选，以确定它们对目标蛋白质的反应和紧缩效果。</li>
<li>results: 这篇研究的结果显示，使用机器学习和量子模拟的融合方法可以快速生成高效的药物材料，并且可以实现对药物的评估和筛选。这些材料可以在几个月内完成整个R&amp;D过程，并且可以降低成本至五十到八十千美元。<details>
<summary>Abstract</summary>
The Research & Development (R&D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction effectiveness and safety, creating a few dozen pre-clinical-trail-ready drugs. This paper is based on our first paper, where we pitched the concept of machine learning combined with quantum simulations. In this paper we will go over the detailed design and framework of QMLS, including MLMG, MLMV, and QS.
</details>
<details>
<summary>摘要</summary>
研发（R&D）阶段是药品开发的长途和昂贵的过程。为了革新这个过程，我们提出了新的概念——量子机器学学习（QMLS），可以缩短整个R&D阶段的时间至3-6个月，并降低成本至50-80万美元。在潜在药物生成（Hit Generation）阶段，机器学学习分子生成（MLMG）根据目标蛋白质的分子结构生成可能的潜在药物，而量子模拟（QS）则从初步试验中筛选出与目标蛋白质具有强烈反应和结合效果的分子。在药物优化阶段，得到的分子 variants 由机器学学习分子变化（MLMV）进行了数十个变化，而其他分子则只进行了几个变化。最后，所有优化后的分子都会经过多轮QS筛选，以确保它们具有高效性和安全性，从而生成数十个前期临床药物。本文是我们之前的第一篇论文的续写，我们在这篇文章中将详细介绍QMLS的设计和框架，包括MLMG、MLMV和QS。
</details></li>
</ul>
<hr>
<h2 id="A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns"><a href="#A-Time-aware-tensor-decomposition-for-tracking-evolving-patterns" class="headerlink" title="A Time-aware tensor decomposition for tracking evolving patterns"></a>A Time-aware tensor decomposition for tracking evolving patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07126">http://arxiv.org/abs/2308.07126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Chatzis, Max Pfeffer, Pedro Lind, Evrim Acar</li>
<li>for: 本研究旨在提取时间序列数据中的慢慢发展模式，并且能够考虑时间序列中的变化。</li>
<li>methods: 本文提出了一种基于PARAFAC2的时间regularization方法，即 temporal PARAFAC2（tPARAFAC2），用于抽取时间序列数据中的慢慢发展模式。</li>
<li>results: 经过广泛的实验 validate that tPARAFAC2可以准确地捕捉时间序列数据中的慢慢发展模式，并且表现比PARAFAC2和时间平滑矩阵因子化regularization方法更好。<details>
<summary>Abstract</summary>
Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate("Time-evolving data sets can often be arranged as a higher-order tensor with one of the modes being the time mode. While tensor factorizations have been successfully used to capture the underlying patterns in such higher-order data sets, the temporal aspect is often ignored, allowing for the reordering of time points. In recent studies, temporal regularizers are incorporated in the time mode to tackle this issue. Nevertheless, existing approaches still do not allow underlying patterns to change in time (e.g., spatial changes in the brain, contextual changes in topics). In this paper, we propose temporal PARAFAC2 (tPARAFAC2): a PARAFAC2-based tensor factorization method with temporal regularization to extract gradually evolving patterns from temporal data. Through extensive experiments on synthetic data, we demonstrate that tPARAFAC2 can capture the underlying evolving patterns accurately, performing better than PARAFAC2 and coupled matrix factorization with temporal smoothness regularization.") result:时间演化数据集经常可以被视为一个高阶张量，其中一个方向是时间方向。虽然tensor分解已经成功地用于捕捉高阶数据集中的下面模式，但是时间方面通常被忽略，允许时间点的重新排序。在最近的研究中，temporal regularizers被添加到时间方面以解决这个问题。然而，现有的方法仍然不允许下面模式在时间上发生变化（例如，大脑中的空间变化，话题中的上下文变化）。在这篇论文中，我们提议时间PARAFAC2（tPARAFAC2）：基于PARAFAC2的张量分解方法，带有时间正则化，以EXTRACT从时间数据中逐渐发展的模式。通过对 sintetic数据进行了广泛的实验，我们示出了tPARAFAC2可以准确地捕捉下面模式，并且perform better than PARAFAC2和 Coupled Matrix Factorization with temporal smoothness regularization。
</details></li>
</ul>
<hr>
<h2 id="Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers"><a href="#Active-Bird2Vec-Towards-End-to-End-Bird-Sound-Monitoring-with-Transformers" class="headerlink" title="Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers"></a>Active Bird2Vec: Towards End-to-End Bird Sound Monitoring with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07121">http://arxiv.org/abs/2308.07121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Rauch, Raphael Schwinger, Moritz Wirth, Bernhard Sick, Sven Tomforde, Christoph Scholz</li>
<li>for: 本研究旨在推动鸟叫声监测领域的终端学习转移，通过结合自动学习（SSL）和深度活动学习（DAL），以便直接处理原始音频数据，并生成高质量鸟叫声表示。</li>
<li>methods: 本研究使用变换器模型，并通过自动学习生成高质量鸟叫声表示，以便加速环境变化评估和决策过程。此外，通过深度活动学习，减少人工标注数据的依赖，提高了鸟叫声识别任务的效果。</li>
<li>results: 本研究通过对不同变换器模型进行比较分析，评估它们在鸟叫声识别任务中的效果。同时，通过使用Huggingface Datasets，生成了一个完整的任务集，以便提高未来的比较性和可重现性。<details>
<summary>Abstract</summary>
We propose a shift towards end-to-end learning in bird sound monitoring by combining self-supervised (SSL) and deep active learning (DAL). Leveraging transformer models, we aim to bypass traditional spectrogram conversions, enabling direct raw audio processing. ActiveBird2Vec is set to generate high-quality bird sound representations through SSL, potentially accelerating the assessment of environmental changes and decision-making processes for wind farms. Additionally, we seek to utilize the wide variety of bird vocalizations through DAL, reducing the reliance on extensively labeled datasets by human experts. We plan to curate a comprehensive set of tasks through Huggingface Datasets, enhancing future comparability and reproducibility of bioacoustic research. A comparative analysis between various transformer models will be conducted to evaluate their proficiency in bird sound recognition tasks. We aim to accelerate the progression of avian bioacoustic research and contribute to more effective conservation strategies.
</details>
<details>
<summary>摘要</summary>
我们提议将学习方法转换为终端学习，通过结合自动生成监督（SSL）和深度活动学习（DAL），利用变换器模型，以直接处理原始音频数据，并不需要传统的spectrogram转换。我们通过ActiveBird2Vec生成高质量的鸟叫表示，通过SSL可能加速环境变化评估和风车决策过程，同时通过DAL减少人工标注数据的依赖，提高生物听音研究的可比性和可重复性。我们计划使用Huggingface集成数据，并进行不同变换器模型之间的比较分析，以评估它们在鸟叫识别任务中的效果。我们希望通过加速鸟类生物听音研究，为生态保护策略做出更有效的贡献。
</details></li>
</ul>
<hr>
<h2 id="Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases"><a href="#Neural-radiance-fields-in-the-industrial-and-robotics-domain-applications-research-opportunities-and-use-cases" class="headerlink" title="Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases"></a>Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07118">http://arxiv.org/abs/2308.07118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maftej/iisnerf">https://github.com/maftej/iisnerf</a></li>
<li>paper_authors: Eugen Šlapak, Enric Pardo, Matúš Dopiriak, Taras Maksymyuk, Juraj Gazda</li>
<li>for: 这篇论文旨在探讨基于提供训练图像的神经辐射场（NeRF）在不同工业子领域的应用前景，以及未来研究方向。</li>
<li>methods: 本论文使用NeRF来学习3D场景表示，并提供了一系列证明NeRF在工业领域的应用可行性的实验。这些实验包括基于NeRF的视频压缩技术和使用NeRF进行3D运动估计以避免碰撞。</li>
<li>results: 在视频压缩实验中，我们获得了1920x1080和300x168分辨率下的压缩率为48%和74%。在3D动画中使用D-NeRF进行3D运动估计，得到了平均PSNR值为23 dB和SSIM值为0.97。<details>
<summary>Abstract</summary>
The proliferation of technologies, such as extended reality (XR), has increased the demand for high-quality three-dimensional (3D) graphical representations. Industrial 3D applications encompass computer-aided design (CAD), finite element analysis (FEA), scanning, and robotics. However, current methods employed for industrial 3D representations suffer from high implementation costs and reliance on manual human input for accurate 3D modeling. To address these challenges, neural radiance fields (NeRFs) have emerged as a promising approach for learning 3D scene representations based on provided training 2D images. Despite a growing interest in NeRFs, their potential applications in various industrial subdomains are still unexplored. In this paper, we deliver a comprehensive examination of NeRF industrial applications while also providing direction for future research endeavors. We also present a series of proof-of-concept experiments that demonstrate the potential of NeRFs in the industrial domain. These experiments include NeRF-based video compression techniques and using NeRFs for 3D motion estimation in the context of collision avoidance. In the video compression experiment, our results show compression savings up to 48\% and 74\% for resolutions of 1920x1080 and 300x168, respectively. The motion estimation experiment used a 3D animation of a robotic arm to train Dynamic-NeRF (D-NeRF) and achieved an average peak signal-to-noise ratio (PSNR) of disparity map with the value of 23 dB and an structural similarity index measure (SSIM) 0.97.
</details>
<details>
<summary>摘要</summary>
技术的普及，如扩展现实（XR），提高了高品质三维图形表示的需求。工业三维应用包括计算机支持设计（CAD）、Finite Element分析（FEA）、扫描和机器人。然而，现有的工业三维表示方法受到高实施成本和人工输入的限制，以获得准确的三维模型。为解决这些挑战，神经辐射场（NeRF）已经出现为了学习基于提供训练图像的三维场景表示方法。尽管有关NeRF的兴趣在不断增长，但它们在不同的工业子领域的潜在应用仍然未得到了足够的探索。在这篇论文中，我们提供了工业应用场景中NeRF的全面检查，并提供未来研究方向的指导。我们还提供了一系列的证明性实验，以示NeRF在工业领域的潜在应用。这些实验包括基于NeRF的视频压缩技术和使用NeRF进行3D运动估计，以避免碰撞。在视频压缩实验中，我们的结果表明，对于分辨率为1920x1080和300x168的视频，可以实现压缩率为48%和74%。在3D动画中使用D-NeRF进行3D运动估计实验，我们获得了平均的干扰比率（PSNR）为23 dB和结构相似度指标（SSIM）为0.97。
</details></li>
</ul>
<hr>
<h2 id="iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN"><a href="#iSTFTNet2-Faster-and-More-Lightweight-iSTFT-Based-Neural-Vocoder-Using-1D-2D-CNN" class="headerlink" title="iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN"></a>iSTFTNet2: Faster and More Lightweight iSTFT-Based Neural Vocoder Using 1D-2D CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07117">http://arxiv.org/abs/2308.07117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, Shogo Seki</li>
<li>for: 快速、轻量级、高精度的语音合成</li>
<li>methods: 使用快速和轻量级的1D CNN作为基础网络，并将一些神经网络替换为iSTFT，以提高速度和轻量化。</li>
<li>results: iSTFTNet2比iSTFTNet更快速和轻量级，且音质相对保持不变。可以在<a target="_blank" rel="noopener" href="https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/%E4%B8%AD%E4%B8%8B%E8%BD%BD%E9%9F%B3%E9%A2%91%E6%A0%B7%E6%9C%AC%E3%80%82">https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/中下载音频样本。</a><details>
<summary>Abstract</summary>
The inverse short-time Fourier transform network (iSTFTNet) has garnered attention owing to its fast, lightweight, and high-fidelity speech synthesis. It obtains these characteristics using a fast and lightweight 1D CNN as the backbone and replacing some neural processes with iSTFT. Owing to the difficulty of a 1D CNN to model high-dimensional spectrograms, the frequency dimension is reduced via temporal upsampling. However, this strategy compromises the potential to enhance the speed. Therefore, we propose iSTFTNet2, an improved variant of iSTFTNet with a 1D-2D CNN that employs 1D and 2D CNNs to model temporal and spectrogram structures, respectively. We designed a 2D CNN that performs frequency upsampling after conversion in a few-frequency space. This design facilitates the modeling of high-dimensional spectrograms without compromising the speed. The results demonstrated that iSTFTNet2 made iSTFTNet faster and more lightweight with comparable speech quality. Audio samples are available at https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（iSTFTNet）在最近引起了关注，因为它具有快速、轻量级和高精度的语音生成特点。它使用快速和轻量级的1D CNN作为基础模型，并将一些神经网络过程替换为iSTFT。由于1D CNN在模型高维спект罗格的问题上难以处理，因此在时间增サンプリング的策略可能会增加速度的约束。为了解决这个问题，我们提出了iSTFTNet2，它是iSTFTNet的改进版本，使用1D-2D CNN来模型时间和спект罗格结构。我们设计了一个2D CNN，它在几个频率空间中进行频率增サンプリング。这种设计允许模型高维спект罗格无需增加速度约束。结果表明，iSTFTNet2使得iSTFTNet更快速和轻量级，同时保持语音质量的同等性。有关audio samples的详细信息请参考https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/istftnet2/.
</details></li>
</ul>
<hr>
<h2 id="Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting"><a href="#Ada-QPacknet-–-adaptive-pruning-with-bit-width-reduction-as-an-efficient-continual-learning-method-without-forgetting" class="headerlink" title="Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting"></a>Ada-QPacknet – adaptive pruning with bit width reduction as an efficient continual learning method without forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07939">http://arxiv.org/abs/2308.07939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcin Pietroń, Dominik Żurek, Kamil Faber, Roberto Corizzo</li>
<li>for: 这个论文是为了解决深度学习模型在动态和复杂环境中学习效率差的问题而写的。</li>
<li>methods: 这个论文提出了一种基于架构的 kontinuous learning 方法，称为 Ada-QPacknet，它通过提取每个任务的子网络来实现。这种方法的关键特点是它的容量，它使用高效的线性和非线性归一化方法来减少模型的大小。</li>
<li>results: 在Well-known CL 场景中，hybrid 8和4位量化实现了浮点子网络的相似准确性。这个方法比大多数 CL 策略在任务和类增量enario中表现出色，并且在Well-known episode combinations 中测试了这个算法，与最流行的 CL 策略进行了比较。<details>
<summary>Abstract</summary>
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combinations and compared with most popular algorithms. Results show that proposed approach outperforms most of the CL strategies in task and class incremental scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach"><a href="#Age-Stratified-Differences-in-Morphological-Connectivity-Patterns-in-ASD-An-sMRI-and-Machine-Learning-Approach" class="headerlink" title="Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach"></a>Age-Stratified Differences in Morphological Connectivity Patterns in ASD: An sMRI and Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07356">http://arxiv.org/abs/2308.07356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokul Manoj, Sandeep Singh Sengar, Jac Fredo Agastinose Ronickom</li>
<li>for: 这个研究的目的是为了比较不同年龄组的自闭症诊断使用形态特征（MF）和形态连接特征（MCF）的效果。</li>
<li>methods: 这个研究使用了两个公共可用的数据库—ABIDE-I和ABIDE-II—获得了Structural Magnetic Resonance Imaging（sMRI）数据，并将数据 pré-processed using a standard pipeline，然后将数据分割成根据Destrieux atlas的148个区域，EXTRACTED área、厚度、体积和平均弯曲信息，并使用了统计t检测（p&lt;0.05）来标识特征，然后使用Random Forest（RF）分类器进行训练。</li>
<li>results: 研究结果表明，6岁到11岁的年龄组的性能最高，其次是6岁到18岁和11岁到18岁的年龄组，在MF和MCF中都有高的表现。总的来说，MCF与RF在6岁到11岁的年龄组中表现最好，其准确率、F1 score、回归率和准确率分别为75.8%、83.1%、86%和80.4%。<details>
<summary>Abstract</summary>
Purpose: Age biases have been identified as an essential factor in the diagnosis of ASD. The objective of this study was to compare the effect of different age groups in classifying ASD using morphological features (MF) and morphological connectivity features (MCF). Methods: The structural magnetic resonance imaging (sMRI) data for the study was obtained from the two publicly available databases, ABIDE-I and ABIDE-II. We considered three age groups, 6 to 11, 11 to 18, and 6 to 18, for our analysis. The sMRI data was pre-processed using a standard pipeline and was then parcellated into 148 different regions according to the Destrieux atlas. The area, thickness, volume, and mean curvature information was then extracted for each region which was used to create a total of 592 MF and 10,878 MCF for each subject. Significant features were identified using a statistical t-test (p<0.05) which was then used to train a random forest (RF) classifier. Results: The results of our study suggested that the performance of the 6 to 11 age group was the highest, followed by the 6 to 18 and 11 to 18 ages in both MF and MCF. Overall, the MCF with RF in the 6 to 11 age group performed better in the classification than the other groups and produced an accuracy, F1 score, recall, and precision of 75.8%, 83.1%, 86%, and 80.4%, respectively. Conclusion: Our study thus demonstrates that morphological connectivity and age-related diagnostic model could be an effective approach to discriminating ASD.
</details>
<details>
<summary>摘要</summary>
目的：识别自闭症（ASD）的年龄因素已被证明是关键因素。本研究的目的是比较不同年龄组的分类ASD使用形态特征（MF）和形态连接特征（MCF）的效果。方法：我们从公共数据库ABIDE-I和ABIDE-II中获得了structural magnetic resonance imaging（sMRI）数据。我们分为三个年龄组：6-11岁、11-18岁和6-18岁进行分析。经过标准化处理后，sMRI数据被分割成根据Desitrieux大脑 Atlase所分的148个区域。然后，每个区域中的面积、厚度、体积和平均弯曲信息被提取，并用于创建共计592个MF和10878个MCF。通过统计t检测（p<0.05）进行了特征选择，并用于训练随机森林（RF）分类器。结果：我们的研究结果表明，6-11岁年龄组的性能最高，然后是6-18岁和11-18岁年龄组，在MF和MCF中都是如此。总的来说，在6-11岁年龄组中，MCF与RF的结合使得分类性能更高，其中的准确率、F1分数、回归率和精度分别为75.8%、83.1%、86%和80.4%。结论：这些结果表明，使用形态连接和年龄相关的诊断模型可以有效地识别ASD。
</details></li>
</ul>
<hr>
<h2 id="InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models"><a href="#InsTag-Instruction-Tagging-for-Analyzing-Supervised-Fine-tuning-of-Large-Language-Models" class="headerlink" title="#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"></a>#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07074">http://arxiv.org/abs/2308.07074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ofa-sys/instag">https://github.com/ofa-sys/instag</a></li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou</li>
<li>for: 这篇论文的目的是提高基础模型的 instruction-following 能力，并通过训练细化（SFT）来实现这一目标。</li>
<li>methods: 该论文使用了一种名为 InsTag 的开源细化标注工具，用于标注 SFT 数据集中的样本，并定义了 instrucion 多样性和复杂性的量化分析。</li>
<li>results: 研究发现，通过使用 InsTag 选择的6000个多样性和复杂性的样本，可以提高基础模型的表现，并且与训练数据量相比，TagLM 模型的表现更高。<details>
<summary>Abstract</summary>
Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.
</details>
<details>
<summary>摘要</summary>
基础语言模型通过监督微调（SFT）获得指令遵从能力。多样性和复杂性被视为成功SFT数据集的关键因素，但其定义还未得到明确的量化分析。本工作提出InsTag，一种开放集标记器，用于在SFT数据集中标记样本基于 semantics和意图，并定义指令多样性和复杂性。我们获得了6.6K个标签来描述全面的用户查询。然后我们分析了流行的开源SFT数据集，发现模型能力随着数据集的多样性和复杂性增加。基于这个观察，我们提出了基于InsTag的数据选择器，选择6K个多样性和复杂性最高的样本从开源数据集进行练习。经过微调，我们获得了TagLM模型，其性能在MT-Bench评估中较开源模型高，证明了查询多样性和复杂性的重要性。我们在https://github.com/OFA-Sys/InsTag上开源了InsTag。
</details></li>
</ul>
<hr>
<h2 id="Machine-Unlearning-Solutions-and-Challenges"><a href="#Machine-Unlearning-Solutions-and-Challenges" class="headerlink" title="Machine Unlearning: Solutions and Challenges"></a>Machine Unlearning: Solutions and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07061">http://arxiv.org/abs/2308.07061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Xu, Zihan Wu, Cong Wang, Xiaohua Jia</li>
<li>for: 本研究旨在提供一份系统性的机器学习忘记研究分类和分析，以便为 Selective Data Removal（SDR）技术的发展提供指导。</li>
<li>methods: 本研究分类了现有的机器学习忘记研究，包括精确忘记和近似忘记两种方法。精确忘记方法可以完全除去训练数据的影响，而近似忘记方法可以有效地减少影响。</li>
<li>results: 本研究对现有的机器学习忘记方法进行了 kritische 分析，并提出了未来研究的方向。通过这种分析，研究人员可以更好地了解机器学习忘记技术的优缺点，并为实际应用提供指导。<details>
<summary>Abstract</summary>
Machine learning models may inadvertently memorize sensitive, unauthorized, or malicious data, posing risks of privacy violations, security breaches, and performance deterioration. To address these issues, machine unlearning has emerged as a critical technique to selectively remove specific training data points' influence on trained models. This paper provides a comprehensive taxonomy and analysis of machine unlearning research. We categorize existing research into exact unlearning that algorithmically removes data influence entirely and approximate unlearning that efficiently minimizes influence through limited parameter updates. By reviewing the state-of-the-art solutions, we critically discuss their advantages and limitations. Furthermore, we propose future directions to advance machine unlearning and establish it as an essential capability for trustworthy and adaptive machine learning. This paper provides researchers with a roadmap of open problems, encouraging impactful contributions to address real-world needs for selective data removal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:机器学习模型可能偶发性记忆敏感、未授权或黑客数据，导致隐私违反、安全泄露和性能下降。为解决这些问题，机器忘记技术已经成为一种重要的解决方案，可以选择性地删除训练模型中具有特定影响的数据点。本文提供了机器忘记的全面分类和分析，并评估了现有的研究。我们将现有的研究分为单精度忘记和近似忘记两种，并评估了它们的优点和限制。此外，我们还提出了未来的方向，以推进机器忘记的发展，并将其视为可靠和适应式机器学习的重要能力。本文为研究人员提供了一个开启问题的路线图，促进了影响性的贡献，以解决实际需求中的选择性数据移除。
</details></li>
</ul>
<hr>
<h2 id="Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review"><a href="#Diagnosis-of-Scalp-Disorders-using-Machine-Learning-and-Deep-Learning-Approach-–-A-Review" class="headerlink" title="Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review"></a>Diagnosis of Scalp Disorders using Machine Learning and Deep Learning Approach – A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07052">http://arxiv.org/abs/2308.07052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrishabh Tiwari, Jatin Moolchandani, Shamla Mantri</li>
<li>for: 这篇论文主要针对scalp病的诊断和分类。</li>
<li>methods: 该论文使用了深度学习技术，包括Convolutional Neural Networks（CNN）和 Fully Connected Networks（FCN），以及一个APP，以帮助诊断scalp病。</li>
<li>results: 该论文的实验结果表明，使用深度学习模型可以高精度地诊断scalp病，其中一个方法的准确率为97.41%-99.09%，另一个方法的准确率为82.9%，而使用机器学习算法也可以高精度地诊断健康的scalp和脱发病。<details>
<summary>Abstract</summary>
The morbidity of scalp diseases is minuscule compared to other diseases, but the impact on the patient's life is enormous. It is common for people to experience scalp problems that include Dandruff, Psoriasis, Tinea-Capitis, Alopecia and Atopic-Dermatitis. In accordance with WHO research, approximately 70% of adults have problems with their scalp. It has been demonstrated in descriptive research that hair quality is impaired by impaired scalp, but these impacts are reversible with early diagnosis and treatment. Deep Learning advances have demonstrated the effectiveness of CNN paired with FCN in diagnosing scalp and skin disorders. In one proposed Deep-Learning-based scalp inspection and diagnosis system, an imaging microscope and a trained model are combined with an app that classifies scalp disorders accurately with an average precision of 97.41%- 99.09%. Another research dealt with classifying the Psoriasis using the CNN with an accuracy of 82.9%. As part of another study, an ML based algorithm was also employed. It accurately classified the healthy scalp and alopecia areata with 91.4% and 88.9% accuracy with SVM and KNN algorithms. Using deep learning models to diagnose scalp related diseases has improved due to advancements i computation capabilities and computer vision, but there remains a wide horizon for further improvements.
</details>
<details>
<summary>摘要</summary>
scalp 疾病的恶性相对其他疾病较少，但对病人的生活影响巨大。人们常经历披裤屑、 Psoriasis、Tinea-Capitis、Alopecia 和 Atopic-Dermatitis 等 scalp 问题。根据Who研究，约70%的成年人有 scalp 问题。研究表明，损害的毛发质量是由于损害 scalp 引起的，但这些影响可以通过早期诊断和治疗而reverse。深度学习技术的进步使得用 Deep Learning 模型进行 scalp 检查和诊断系统的精度提高了。在一个提议的 Deep-Learning-based scalp 检查和诊断系统中，一个图像镜和一个训练模型被与一个APP结合，可以准确地分类 scalp 疾病，其精度为97.41%-99.09%。另一项研究则是使用 CNN 分类 Psoriasis，其精度为82.9%。在另一项研究中，一种 ML 基本的算法也被应用，它可以准确地分类健康的 scalp 和 Alopecia areata，其精度为91.4% 和 88.9%。使用深度学习模型进行 scalp 相关疾病的诊断，由于计算机能力和计算机视觉的进步，已经得到了进一步改进的空间，但还有很大的可能性空间。
</details></li>
</ul>
<hr>
<h2 id="Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems"><a href="#Fourier-neural-operator-for-learning-solutions-to-macroscopic-traffic-flow-models-Application-to-the-forward-and-inverse-problems" class="headerlink" title="Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems"></a>Fourier neural operator for learning solutions to macroscopic traffic flow models: Application to the forward and inverse problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07051">http://arxiv.org/abs/2308.07051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bilal Thonnam Thodi, Sai Venkata Ramana Ambadipudi, Saif Eddin Jabari</li>
<li>for: 这个论文是用来研究深度学习方法在交通流动中的应用，特别是用于解决非线性半导体方程的问题。</li>
<li>methods: 这个论文使用了一种名为 нейрон运算器框架，它可以将不同和稀疏的交通数据映射到完整的交通状况中。在训练中，使用了一种名为 физи学信息冲激（π）-FNO的算子，它在训练中添加了一个物理损失函数，以便在训练中提高冲击预测。</li>
<li>results: 通过使用LWR交通流模型， authors发现了在预测环形路网和城市信号灯道路上的density dynamics的高精度预测。此外，他们发现了一个可以使用简单的交通密度动态，例如由2-3个汽车队列和1-2个交通信号ecycle组成的数据，并且可以预测具有不同汽车队列分布和多个交通信号cycle（大于2）的密度动态，并且误差在可接受范围内。在适当的模型架构和训练数据下，插值误差呈线性增长。添加物理正则化可以帮助学习长期交通密度动态，特别是在 periodic boundary data 上。<details>
<summary>Abstract</summary>
Deep learning methods are emerging as popular computational tools for solving forward and inverse problems in traffic flow. In this paper, we study a neural operator framework for learning solutions to nonlinear hyperbolic partial differential equations with applications in macroscopic traffic flow models. In this framework, an operator is trained to map heterogeneous and sparse traffic input data to the complete macroscopic traffic state in a supervised learning setting. We chose a physics-informed Fourier neural operator ($\pi$-FNO) as the operator, where an additional physics loss based on a discrete conservation law regularizes the problem during training to improve the shock predictions. We also propose to use training data generated from random piecewise constant input data to systematically capture the shock and rarefied solutions. From experiments using the LWR traffic flow model, we found superior accuracy in predicting the density dynamics of a ring-road network and urban signalized road. We also found that the operator can be trained using simple traffic density dynamics, e.g., consisting of $2-3$ vehicle queues and $1-2$ traffic signal cycles, and it can predict density dynamics for heterogeneous vehicle queue distributions and multiple traffic signal cycles $(\geq 2)$ with an acceptable error. The extrapolation error grew sub-linearly with input complexity for a proper choice of the model architecture and training data. Adding a physics regularizer aided in learning long-term traffic density dynamics, especially for problems with periodic boundary data.
</details>
<details>
<summary>摘要</summary>
深度学习方法在交通流动问题中得到广泛应用。在这篇论文中，我们研究了一种神经网络框架，用于解决非线性偏微分方程的问题，并应用于大规模交通流模型。在这个框架中，一个算子被训练，以将不同和稀疏的交通数据映射到完整的交通状态中。我们选择了一种带有物理约束的 fourier神经网络（π-FNO）作为算子，其中在训练过程中添加了物理损失，以提高震动预测。我们还提出了使用随机划分的杂ync constant输入数据来系统地捕捉震动和稀疏解。从实验使用LWR交通流模型来看，我们发现了在density动力学中的高精度预测。我们还发现算子可以通过简单的交通密度动力学，例如由2-3辆汽车队列和1-2个交通信号周期组成的系统，来预测密度动力学。此外，我们发现算子可以在不同的汽车队列分布和多个交通信号周期（至少2个）下预测密度动力学，并且误差在输入复杂性增长的速度下逐渐增加。添加物理约束可以帮助学习长期交通密度动力学，特别是在 periodic boundry data 的问题上。
</details></li>
</ul>
<hr>
<h2 id="UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering"><a href="#UIPC-MF-User-Item-Prototype-Connection-Matrix-Factorization-for-Explainable-Collaborative-Filtering" class="headerlink" title="UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering"></a>UIPC-MF: User-Item Prototype Connection Matrix Factorization for Explainable Collaborative Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07048">http://arxiv.org/abs/2308.07048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Pan, Von-Wun Soo</li>
<li>for: 提高推荐系统的准确率和可解释性</li>
<li>methods: 使用prototype-based matrix factorization方法，即UIPC-MF，其中用户和 Item 都关联有一组原型，以增强推荐的可解释性</li>
<li>results: 相比其他原型基eline方法，UIPC-MF 在三个 dataset 上显示出较高的 Hit Ratio 和 Normalized Discounted Cumulative Gain，同时也提供了更好的透明度。<details>
<summary>Abstract</summary>
Recommending items to potentially interested users has been an important commercial task that faces two main challenges: accuracy and explainability. While most collaborative filtering models rely on statistical computations on a large scale of interaction data between users and items and can achieve high performance, they often lack clear explanatory power. We propose UIPC-MF, a prototype-based matrix factorization method for explainable collaborative filtering recommendations. In UIPC-MF, both users and items are associated with sets of prototypes, capturing general collaborative attributes. To enhance explainability, UIPC-MF learns connection weights that reflect the associative relations between user and item prototypes for recommendations. UIPC-MF outperforms other prototype-based baseline methods in terms of Hit Ratio and Normalized Discounted Cumulative Gain on three datasets, while also providing better transparency.
</details>
<details>
<summary>摘要</summary>
推荐预测已经是电商中一项非常重要的任务，面临着两个主要挑战：准确性和可读性。大多数共同推荐模型通过大规模的用户-物品交互数据进行统计计算，可以达到高性能，但通常缺乏明确的解释力。我们提出了UIPC-MF，一种基于原型的矩阵分解方法，用于可读性推荐。在UIPC-MF中，用户和物品都关联到一组原型，捕捉总的共同特征。为了增强可读性，UIPC-MF学习用户和物品原型之间的关联Weight，用于推荐。UIPC-MF在三个数据集上比基eline方法具有更高的 Hit Ratio 和 Normalized Discounted Cumulative Gain，同时也提供了更好的透明度。
</details></li>
</ul>
<hr>
<h2 id="No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning"><a href="#No-Regularization-is-Needed-An-Efficient-and-Effective-Model-for-Incomplete-Label-Distribution-Learning" class="headerlink" title="No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning"></a>No Regularization is Needed: An Efficient and Effective Model for Incomplete Label Distribution Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07047">http://arxiv.org/abs/2308.07047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Songcan Chen</li>
<li>for: 本研究目的是解决因缺失数据而导致的不完整的分类器（Incomplete LDL，InLDL）性能下降问题，而不需要任何显式正则化。</li>
<li>methods: 我们提出使用分类器自身的标签分布作为正则化的先验知识，并设计了一种权重 schemes来强调小度和缺失度。</li>
<li>results: 我们的模型具有四个优点：1）没有需要显式正则化；2）具有闭式解决方案和易于实现（只需几行代码）；3）与大规模数据集相比，计算复杂度 linear；4）与当前状态对齐水平性能。<details>
<summary>Abstract</summary>
Label Distribution Learning (LDL) assigns soft labels, a.k.a. degrees, to a sample. In reality, it is always laborious to obtain complete degrees, giving birth to the Incomplete LDL (InLDL). However, InLDL often suffers from performance degeneration. To remedy it, existing methods need one or more explicit regularizations, leading to burdensome parameter tuning and extra computation. We argue that label distribution itself may provide useful prior, when used appropriately, the InLDL problem can be solved without any explicit regularization. In this paper, we offer a rational alternative to use such a prior. Our intuition is that large degrees are likely to get more concern, the small ones are easily overlooked, whereas the missing degrees are completely neglected in InLDL. To learn an accurate label distribution, it is crucial not to ignore the small observed degrees but to give them properly large weights, while gradually increasing the weights of the missing degrees. To this end, we first define a weighted empirical risk and derive upper bounds between the expected risk and the weighted empirical risk, which reveals in principle that weighting plays an implicit regularization role. Then, by using the prior of degrees, we design a weighted scheme and verify its effectiveness. To sum up, our model has four advantages, it is 1) model selection free, as no explicit regularization is imposed; 2) with closed form solution (sub-problem) and easy-to-implement (a few lines of codes); 3) with linear computational complexity in the number of samples, thus scalable to large datasets; 4) competitive with state-of-the-arts even without any explicit regularization.
</details>
<details>
<summary>摘要</summary>
标签分布学习（LDL）将软标签，即学习度，赋予样本。在实际应用中，完整的学习度往往很困难寻求，从而产生了偏差的LDL（InLDL）问题。然而，InLDL经常会导致性能下降。现有方法通常需要一或多个显式正则化，这会增加参数调整的复杂性和额外计算。我们认为标签分布本身可以提供有用的前提，当正确使用时，InLDL问题可以解决无需显式正则化。在本文中，我们提出了一种合理的使用此前提的方法。我们的启发是，大度标签更有可能受到注意，小度标签容易被忽略，而缺失的度标签完全被偏差的InLDL忽略。为了学习准确的标签分布，非常重要不是忽略小 observed degree，而是给它们适当的大量重要，同时逐渐增加缺失的度标签的重要性。我们首先定义了权重化的empirical risk，并 derivated upper bound между预期风险和权重化empirical risk，这 revelas in principle that weighting plays an implicit regularization role。然后，通过使用度标签的前提，我们设计了权重方案，并证明其效果。总之，我们的模型具有以下四个优点：1) 无需显式正则化，因为不需要任何显式正则化; 2) 具有关闭式解（sub-problem）和易于实现（只需几行代码）; 3) 对大量数据集 scales linearly，因此可扩展性好; 4) 与状态 искусственный지标下相当竞争，即无需显式正则化。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Flow-Networks"><a href="#Bayesian-Flow-Networks" class="headerlink" title="Bayesian Flow Networks"></a>Bayesian Flow Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07037">http://arxiv.org/abs/2308.07037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stefanradev93/BayesFlow">https://github.com/stefanradev93/BayesFlow</a></li>
<li>paper_authors: Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, Faustino Gomez</li>
<li>for: 本研究提出了抽象概率流网络（BFN），一种新的生成模型，其中抽象概率流网络的参数通过 bayesian 推断在噪声数据样本的灯光下进行修改，然后通过神经网络输出第二个相互dependent的分布。</li>
<li>methods: 该研究提出了一种基于 bayesian 推断的生成过程，其中从简单的先验开始，逐步更新两个分布，得到一个类似于反diffusion 模型的生成过程，但是更加简单，不需要前向过程。</li>
<li>results: 在图像模型task上，BFNs  achieved competitive log-likelihoods on dynamically binarized MNIST and CIFAR-10，并在文本8字符级语言模型任务上超越所有已知的批diffusion 模型。<details>
<summary>Abstract</summary>
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文介绍了 bayesian flow networks (BFNs)，一种新的生成模型，其中bayesian inference modify了一组独立分布的参数，然后将这些修改后的分布作为输入传递给神经网络，生成一个第二个、相互关联的分布。这个过程类似于Diffusion模型的反向过程，但是更加简单，不需要前向过程。模型可以处理整数、连续和整数化数据，并且使用本地差分导数，使得梯度导航和几步生成在整数领域如语言模型中变得更加容易。损失函数直接优化数据压缩，并不限制网络架构。在实验中，BFNs在 dynamically binarized MNIST和CIFAR-10上的图像模型 task中 achieved competitive log-likelihoods，并在text8 character-level语言模型任务上超过了所有已知的整数 diffusion models。
</details></li>
</ul>
<hr>
<h2 id="S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields"><a href="#S3IM-Stochastic-Structural-SIMilarity-and-Its-Unreasonable-Effectiveness-for-Neural-Fields" class="headerlink" title="S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields"></a>S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07032">http://arxiv.org/abs/2308.07032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/madaoer/s3im_nerf">https://github.com/madaoer/s3im_nerf</a></li>
<li>paper_authors: Zeke Xie, Xindi Yang, Yujie Yang, Qi Sun, Yixiang Jiang, Haoran Wang, Yunfeng Cai, Mingming Sun</li>
<li>for: 该论文旨在提高NeRF和相关神经场方法的可视化质量，使其能够更好地渲染未知视角的场景图像。</li>
<li>methods: 该论文提出了一种非本地多重训练方法，通过一种新的随机结构相似性（S3IM）损失函数，将多个数据点处理为一个整体，而不是独立处理每个输入。</li>
<li>results: 对于八种视角合成任务和八种表面重建任务，S3IM可以减少测试MSE损失率超过90%，提高F1分数198%和Chamfer-$L_{1}$距离64%。此外，S3IM可以在缺少输入、损坏图像和动态场景下保持稳定性。<details>
<summary>Abstract</summary>
Recently, Neural Radiance Field (NeRF) has shown great success in rendering novel-view images of a given scene by learning an implicit representation with only posed RGB images. NeRF and relevant neural field methods (e.g., neural surface representation) typically optimize a point-wise loss and make point-wise predictions, where one data point corresponds to one pixel. Unfortunately, this line of research failed to use the collective supervision of distant pixels, although it is known that pixels in an image or scene can provide rich structural information. To the best of our knowledge, we are the first to design a nonlocal multiplex training paradigm for NeRF and relevant neural field methods via a novel Stochastic Structural SIMilarity (S3IM) loss that processes multiple data points as a whole set instead of process multiple inputs independently. Our extensive experiments demonstrate the unreasonable effectiveness of S3IM in improving NeRF and neural surface representation for nearly free. The improvements of quality metrics can be particularly significant for those relatively difficult tasks: e.g., the test MSE loss unexpectedly drops by more than 90% for TensoRF and DVGO over eight novel view synthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distance reduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM is consistently robust even with sparse inputs, corrupted images, and dynamic scenes.
</details>
<details>
<summary>摘要</summary>
近期，神经辐射场（NeRF）已经取得了在渲染新视图图像中的成功，通过学习含义几何表示，只使用配置好的RGB图像。NeRF和相关的神经场方法（例如神经表面表示）通常是点 wise 损失优化和点 wise 预测，其中一个数据点对应一个像素。尽管这一线索的研究把握不到远程像素的集合supervision，即图像或场景中的像素可以提供丰富的结构信息。据我们所知，我们是首先设计了非本地多样training paradigm for NeRF和相关神经场方法，通过一种新的随机Structural SIMilarity（S3IM）损失函数，将多个数据点处理为一个整体而不是独立处理多个输入。我们的广泛实验表明，S3IM可以减少TensoRF和DVGO的测试MSE损失超过90%，并且 NeuS 的F-score提高198%，Chamfer $L_{1}$ 距离减少64%。此外，S3IM还能够在缺少输入、损坏图像和动态场景下保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer"><a href="#Bayesian-Physics-Informed-Neural-Network-for-the-Forward-and-Inverse-Simulation-of-Engineered-Nano-particles-Mobility-in-a-Contaminated-Aquifer" class="headerlink" title="Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer"></a>Bayesian Physics-Informed Neural Network for the Forward and Inverse Simulation of Engineered Nano-particles Mobility in a Contaminated Aquifer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07352">http://arxiv.org/abs/2308.07352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Nilabh, Fidel Grandia</li>
<li>For: This paper aims to develop a predictive tool for the mobility of engineered nanoparticles (ENPs) in groundwater, to support the development of an efficient remediation strategy for polluted groundwater sites.* Methods: The paper uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the mobility of ENPs within an aquifer, and to quantify the uncertainty in the predictions.* Results: The forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility, and the inverse model output is used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.Here’s the Chinese translation of the three key information points:* For: 这篇论文的目的是为了开发一种能够预测Engineered Nanoparticles（ENPs）在地下水中的移动性，以支持污染地下水站的清理和环境重建。* Methods: 这篇论文使用了一种 Bayesian Physics-Informed Neural Network（B-PINN）框架，来模拟ENPs在aquifer中的移动性，并量化预测结果的不确定性。* Results: 前向模型表明B-PINN在准确预测ENPs移动性的能力，而反向模型输出可以用来预测aquifer中ENPs移动性的主导参数。这项研究 demonstarte了这种工具的能力，可以为开发有效的地下水清理策略提供预测性的信息。<details>
<summary>Abstract</summary>
Globally, there are many polluted groundwater sites that need an active remediation plan for the restoration of local ecosystem and environment. Engineered nanoparticles (ENPs) have proven to be an effective reactive agent for the in-situ degradation of pollutants in groundwater. While the performance of these ENPs has been highly promising on the laboratory scale, their application in real field case conditions is still limited. The complex transport and retention mechanisms of ENPs hinder the development of an efficient remediation strategy. Therefore, a predictive tool to comprehend the transport and retention behavior of ENPs is highly required. The existing tools in the literature are dominated with numerical simulators, which have limited flexibility and accuracy in the presence of sparse datasets and the aquifer heterogeneity. This work uses a Bayesian Physics-Informed Neural Network (B-PINN) framework to model the nano-particles mobility within an aquifer. The result from the forward model demonstrates the effective capability of B-PINN in accurately predicting the ENPs mobility and quantifying the uncertainty. The inverse model output is then used to predict the governing parameters for the ENPs mobility in a small-scale aquifer. The research demonstrates the capability of the tool to provide predictive insights for developing an efficient groundwater remediation strategy.
</details>
<details>
<summary>摘要</summary>
全球有很多污染的地下水点，需要有效的活动整治计划以恢复当地生态环境。工程化的奈米颗粒（ENPs）在地下水中的吸附和分解作用已经在室内实验室中得到了证明，但是在实际场景中的应用仍然受限。奈米颗粒的复杂的运输和保持机制限制了整治策略的发展。因此，一个可预测奈米颗粒的运输和保持行为的工具是非常重要。现有的文献中的工具主要是数值模拟器，它们在缺乏数据和地下水异常性时的灵活性和准确性受到限制。本研究使用泛函神经网络（B-PINN）框架来模拟奈米颗粒在aquifer中的 mobilidad。前向模型的输出结果表明B-PINN在准确预测奈米颗粒 mobilidad和评估不确定性的能力。逆向模型输出被用来预测 governing parameters 的奈米颗粒 mobilidad在小规模 aquifer 中。研究表明工具的可预测性可以为开发有效的地下水整治策略提供先进的预测性 Insight。
</details></li>
</ul>
<hr>
<h2 id="IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse"><a href="#IOB-Integrating-Optimization-Transfer-and-Behavior-Transfer-for-Multi-Policy-Reuse" class="headerlink" title="IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse"></a>IOB: Integrating Optimization Transfer and Behavior Transfer for Multi-Policy Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07351">http://arxiv.org/abs/2308.07351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Li, Hao Li, Jin Zhang, Zhen Wang, Peng Liu, Chongjie Zhang</li>
<li>for: 本研究旨在提出一种新的转移学习RL方法，以便在新任务上快速解决问题。</li>
<li>methods: 本方法使用actor-critic框架中的Q函数来导引策略选择，选择最大一步改进策略作为目标策略。我们同时实现了优化转移和行为转移（IOB），通过规范学习策略以便模仿指导策略，并将其与行为策略相结合。</li>
<li>results: 我们的方法在标准任务上超越了状态之前的转移RL基准值，并在连续学习场景中提高了最终性和知识传递性。此外，我们证明了我们的优化转移技术可以提高目标策略学习。<details>
<summary>Abstract</summary>
Humans have the ability to reuse previously learned policies to solve new tasks quickly, and reinforcement learning (RL) agents can do the same by transferring knowledge from source policies to a related target task. Transfer RL methods can reshape the policy optimization objective (optimization transfer) or influence the behavior policy (behavior transfer) using source policies. However, selecting the appropriate source policy with limited samples to guide target policy learning has been a challenge. Previous methods introduce additional components, such as hierarchical policies or estimations of source policies' value functions, which can lead to non-stationary policy optimization or heavy sampling costs, diminishing transfer effectiveness. To address this challenge, we propose a novel transfer RL method that selects the source policy without training extra components. Our method utilizes the Q function in the actor-critic framework to guide policy selection, choosing the source policy with the largest one-step improvement over the current target policy. We integrate optimization transfer and behavior transfer (IOB) by regularizing the learned policy to mimic the guidance policy and combining them as the behavior policy. This integration significantly enhances transfer effectiveness, surpasses state-of-the-art transfer RL baselines in benchmark tasks, and improves final performance and knowledge transferability in continual learning scenarios. Additionally, we show that our optimization transfer technique is guaranteed to improve target policy learning.
</details>
<details>
<summary>摘要</summary>
人类具有 reuse previously learned policies 来解决新任务的能力，同时 reinforcement learning (RL) 代理也可以通过将知识传递到相关的目标任务中来实现这一点。传输 RL 方法可以改变政策优化目标（优化传输）或者影响行为政策（行为传输）使用源政策。然而，在有限样本情况下选择合适的源政策是一大挑战。先前的方法可能会添加额外的组件，如层次政策或源政策价值函数的估计，这可能会导致非站点政策优化或者大量的样本成本，这将导致传输效果减退。为解决这个挑战，我们提出了一种新的传输 RL 方法，不需要训练额外的组件。我们利用 actor-critic 框架中的 Q 函数来导引政策选择，选择目标政策中一步改进最大的源政策。我们将优化传输和行为传输（IOB）相结合，通过规则化学习的政策来模仿指导政策，并将其与行为政策相结合。这种结合显著提高了传输效果，超越了基eline的传输 RL 标准 benchmark 任务，并在连续学习场景中提高了最终性和知识传递性。此外，我们证明了我们的优化传输技术能够提高目标政策学习。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training"><a href="#Efficient-Neural-PDE-Solvers-using-Quantization-Aware-Training" class="headerlink" title="Efficient Neural PDE-Solvers using Quantization Aware Training"></a>Efficient Neural PDE-Solvers using Quantization Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07350">http://arxiv.org/abs/2308.07350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Winfried van den Dool, Tijmen Blankevoort, Max Welling, Yuki M. Asano</li>
<li>for: 用 neural networks 代替经典数学方法解决 Partial Differential Equations (PDEs) 的应用，以减少计算成本。</li>
<li>methods: 使用现有的量化方法来降低计算成本，并保持性能。</li>
<li>results: 在四个标准 PDE 数据集和三个网络架构上，发现量化训练可以降低计算成本三个数量级，而且在不同设置下都能够保持性能。此外，我们还证明了在大多数情况下，只有通过包含量化来达到 Pareto 优化。<details>
<summary>Abstract</summary>
In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of state-of-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs performance is almost always achieved only by incorporating quantization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads"><a href="#Learning-to-Optimize-LSM-trees-Towards-A-Reinforcement-Learning-based-Key-Value-Store-for-Dynamic-Workloads" class="headerlink" title="Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads"></a>Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07013">http://arxiv.org/abs/2308.07013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingheng Mo, Fanchao Chen, Siqiang Luo, Caihua Shan</li>
<li>For: 提高静态负荷下的系统性能，即使面临动态负荷。* Methods: 使用Reinforcement Learning（RL）引导LSM树转换，并提出了一种新的LSM树结构（FLSM树）以优化压缩策略的转换。* Results: 比较RL和传统的方法，RusKey在多种负荷下显示了4倍的终端性能优势，而无需先知工作负荷知识。<details>
<summary>Abstract</summary>
LSM-trees are widely adopted as the storage backend of key-value stores. However, optimizing the system performance under dynamic workloads has not been sufficiently studied or evaluated in previous work. To fill the gap, we present RusKey, a key-value store with the following new features: (1) RusKey is a first attempt to orchestrate LSM-tree structures online to enable robust performance under the context of dynamic workloads; (2) RusKey is the first study to use Reinforcement Learning (RL) to guide LSM-tree transformations; (3) RusKey includes a new LSM-tree design, named FLSM-tree, for an efficient transition between different compaction policies -- the bottleneck of dynamic key-value stores. We justify the superiority of the new design with theoretical analysis; (4) RusKey requires no prior workload knowledge for system adjustment, in contrast to state-of-the-art techniques. Experiments show that RusKey exhibits strong performance robustness in diverse workloads, achieving up to 4x better end-to-end performance than the RocksDB system under various settings.
</details>
<details>
<summary>摘要</summary>
LSM树是键值存储系统的常用后端存储方式。然而，在动态负荷下优化系统性能尚未得到充分研究和评估。为填补这一空白，我们提出了RusKey，一个具有以下新特点的键值存储系统：1. RusKey是首次在线上调度LSM树结构，以实现对动态负荷下的robust性表现;2. RusKey是首次使用强化学习（RL）引导LSM树转换;3. RusKey包含一种新的LSM树设计，名为FLSM树，用于高效地在不同压缩策略之间进行过渡;4. RusKey不需要先知工作负荷信息，与当前技术相比，更加灵活和易用。我们通过理论分析证明了新设计的优越性。实验结果显示，RusKey在多种工作负荷下表现出强大的性能稳定性，与RocksDB系统在不同设置下达到4倍的终端性能。
</details></li>
</ul>
<hr>
<h2 id="Greedy-online-change-point-detection"><a href="#Greedy-online-change-point-detection" class="headerlink" title="Greedy online change point detection"></a>Greedy online change point detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07012">http://arxiv.org/abs/2308.07012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jou-Hui Ho, Felipe Tobar</li>
<li>for: 寻找时间序列中的变化点，以提高变化点检测的准确率和效率。</li>
<li>methods: 使用Greedy Online Change Point Detection（GOCPD）方法，该方法通过最大化数据来自两个独立模型（temporal）的概率来找到变化点。</li>
<li>results: 在synthetic数据和实际世界单variate和多variate设置中，GOCPD方法能够快速减少false discovery rate，并且在某些情况下比传统方法更高效。<details>
<summary>Abstract</summary>
Standard online change point detection (CPD) methods tend to have large false discovery rates as their detections are sensitive to outliers. To overcome this drawback, we propose Greedy Online Change Point Detection (GOCPD), a computationally appealing method which finds change points by maximizing the probability of the data coming from the (temporal) concatenation of two independent models. We show that, for time series with a single change point, this objective is unimodal and thus CPD can be accelerated via ternary search with logarithmic complexity. We demonstrate the effectiveness of GOCPD on synthetic data and validate our findings on real-world univariate and multivariate settings.
</details>
<details>
<summary>摘要</summary>
常规在线变点检测（CPD）方法通常会有较大的假阳性率，因为它们对异常值敏感。为了解决这个缺点，我们提议了简单在线变点检测（GOCPD）方法，它通过最大化数据来自两个独立模型（时间排序）的概率来检测变点。我们证明，对具有单个变点的时间序列，这个目标函数是单峰型，因此可以通过三元搜索来加速CPD，其复杂度为幂函数。我们在 sintetic 数据上证明了 GOCPD 的有效性，并在实际的单VAR 和多VAR 设置中验证了我们的结论。
</details></li>
</ul>
<hr>
<h2 id="Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning"><a href="#Aggregating-Intrinsic-Information-to-Enhance-BCI-Performance-through-Federated-Learning" class="headerlink" title="Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning"></a>Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11636">http://arxiv.org/abs/2308.11636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Liu, Yuanyuan Chen, Anran Li, Yi Ding, Han Yu, Cuntai Guan<br>for: 这个研究旨在解决脑computer interfaces（BCI）建立高性能深度学习模型的长期挑战， BCIs 的数据多样性问题。methods: 这个研究提出了一个弹性联边学习（FLEEG）框架，让不同装备的数据可以在训练过程中合作。每个客户端都有自己的特定数据集，并训练一个层次化的专门化模型，以处理不同的数据格式。服务器则处理训练过程，将来自所有数据集的知识融合，以提高总性能。results: 这个框架在脑意图（MI）分类任务中，与9个由不同设备收集的EEG数据集合作，可以提高分类性能达16.7%。可视化结果显示，提案的框架可以让本地模型专注在任务相关的区域，从而获得更好的性能。<details>
<summary>Abstract</summary>
Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and trains a hierarchical personalized model to manage diverse data formats and facilitate information exchange. Meanwhile, the server coordinates the training procedure to harness knowledge gleaned from all datasets, thus elevating overall performance. The framework has been evaluated in Motor Imagery (MI) classification with nine EEG datasets collected by different devices but implementing the same MI task. Results demonstrate that the proposed frame can boost classification performance up to 16.7% by enabling knowledge sharing between multiple datasets, especially for smaller datasets. Visualization results also indicate that the proposed framework can empower the local models to put a stable focus on task-related areas, yielding better performance. To the best of our knowledge, this is the first end-to-end solution to address this important challenge.
</details>
<details>
<summary>摘要</summary>
BCIs 长期面临不充分数据的挑战，建立高性能深度学习模型困难。虽然许多研究机构和机构收集了多个 EEG 数据集，但是共享多个站点的 EEG 数据仍然困难，主要因为设备的不一致性。这个挑战的重要性无法被过度估计，因为数据多样性对模型的稳定性具有关键作用。然而，现有的研究很少讨论这个问题，通常是在单个数据集内进行模型训练，常在 между subject 或 session 上下文中进行。在这种情况下，我们提出了一种层次个性化 Federated Learning EEG 解码（FLEEG）框架，以解决这个挑战。这种创新的框架标志着 BCIs 新的学习模式，使得不同数据格式的数据集可以在模型训练过程中合作。每个客户端被分配特定数据集，并训练一个层次个性化模型，以处理多个数据格式的多样性，并且促进信息交换。同时，服务器协调训练过程，以利用所有数据集中所获得的知识，从而提高整体性能。我们在 Motor Imagery（MI） 分类任务中使用了 nine EEG 数据集，每个数据集由不同的设备收集，但都实现了相同的 MI 任务。结果表明，我们的框架可以提高分类性能达到 16.7%，尤其是对小数据集的提高。可视化结果还表明，我们的框架可以让本地模型固定焦点于任务相关区域，从而提高性能。到目前为止，我们的解决方案是 BCIs 首次尝试的综合解决方案。
</details></li>
</ul>
<hr>
<h2 id="Deep-convolutional-neural-networks-for-cyclic-sensor-data"><a href="#Deep-convolutional-neural-networks-for-cyclic-sensor-data" class="headerlink" title="Deep convolutional neural networks for cyclic sensor data"></a>Deep convolutional neural networks for cyclic sensor data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06987">http://arxiv.org/abs/2308.06987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Payman Goodarzi, Yannick Robin, Andreas Schütze, Tizian Schneider</li>
<li>for: 本研究旨在探讨基于感测器的维保维护，并应用深度学习技术来解决多感测器系统中的复杂性问题。</li>
<li>methods: 本研究使用了一个 hidraulic system testbed dataset，并比较了三个模型的性能：基线模型使用了 conventional methods，单个CNN模型使用了 early sensor fusion，并且二路CNN模型（2L-CNN）使用了 late sensor fusion。</li>
<li>results: 基线模型使用了 late sensor fusion，可以达到1%的测试错误率，但CNN模型由于感测器的多样性而遇到了问题，导致测试错误率达到20.5%。此外，我们还进行了每感测器都进行独立的训练，并观察到了几个感测器的准确率变化。此外，2L-CNN模型的性能表现了显著的改善，当考虑了最佳和最差的感测器时，错误率下降了33%。本研究重申了多感测器系统中的复杂性问题需要有效地解决。<details>
<summary>Abstract</summary>
Predictive maintenance plays a critical role in ensuring the uninterrupted operation of industrial systems and mitigating the potential risks associated with system failures. This study focuses on sensor-based condition monitoring and explores the application of deep learning techniques using a hydraulic system testbed dataset. Our investigation involves comparing the performance of three models: a baseline model employing conventional methods, a single CNN model with early sensor fusion, and a two-lane CNN model (2L-CNN) with late sensor fusion. The baseline model achieves an impressive test error rate of 1% by employing late sensor fusion, where feature extraction is performed individually for each sensor. However, the CNN model encounters challenges due to the diverse sensor characteristics, resulting in an error rate of 20.5%. To further investigate this issue, we conduct separate training for each sensor and observe variations in accuracy. Additionally, we evaluate the performance of the 2L-CNN model, which demonstrates significant improvement by reducing the error rate by 33% when considering the combination of the least and most optimal sensors. This study underscores the importance of effectively addressing the complexities posed by multi-sensor systems in sensor-based condition monitoring.
</details>
<details>
<summary>摘要</summary>
预测维护在产业系统不间断运行和降低系统故障风险的角色非常重要。本研究使用液压系统测试 datasets 进行探索，并应用深度学习技术进行condition monitoring。我们的调查包括比较三种模型的性能：基eline模型使用 convent ional 方法、单个CNN模型（1L-CNN）在早期整合感知器、以及两个CNN模型（2L-CNN）在晚期整合感知器。基eline模型通过使用晚期整合，实现了1%的测试错误率。然而，CNN模型由于感知器的多样性，导致20.5%的错误率。为了进一步调查这一问题，我们进行了每个感知器分别进行训练，并观察了准确性的变化。此外，我们还评估了2L-CNN模型的性能，其在考虑最佳和最差的感知器组合时显示了33%的下降。这一研究强调了condition monitoring中多感知器系统的复杂性需要得到有效地处理。
</details></li>
</ul>
<hr>
<h2 id="pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems"><a href="#pNNCLR-Stochastic-Pseudo-Neighborhoods-for-Contrastive-Learning-based-Unsupervised-Representation-Learning-Problems" class="headerlink" title="pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems"></a>pNNCLR: Stochastic Pseudo Neighborhoods for Contrastive Learning based Unsupervised Representation Learning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06983">http://arxiv.org/abs/2308.06983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Momojit Biswas, Himanshu Buckchash, Dilip K. Prasad</li>
<li>for: 本文是为了提高 nearest neighbor 基于自助学习（SSL）的图像识别问题的表现而写的。</li>
<li>methods: 本文使用 nearest neighbor 方法，并引入 pseudo nearest neighbors (pNN) 来控制支持集质量，以提高表现。 另外，文中还使用了一种抽样策略和一种平滑重量更新策略来稳定 nearest neighbor 基于学习的uncertainty。</li>
<li>results: 根据文中的评估结果，提出的方法与基eline nearest neighbor 方法相比，在多个公共图像识别和医学图像识别 dataset 上表现出了8%的提升。此外，该方法与其他之前提出的 SSL 方法相比也具有相似的表现。<details>
<summary>Abstract</summary>
Nearest neighbor (NN) sampling provides more semantic variations than pre-defined transformations for self-supervised learning (SSL) based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline (pNNCLR) to the nearest neighbor based SSL approach (NNCLR). To this end, we introduce pseudo nearest neighbors (pNN) to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.
</details>
<details>
<summary>摘要</summary>
近邻采样（NN）提供了更多的 semantic variation than pre-defined transformation for self-supervised learning（SSL）based image recognition problems. However, its performance is restricted by the quality of the support set, which holds positive samples for the contrastive loss. In this work, we show that the quality of the support set plays a crucial role in any nearest neighbor based method for SSL. We then provide a refined baseline（pNNCLR）to the nearest neighbor based SSL approach（NNCLR）. To this end, we introduce pseudo nearest neighbors（pNN）to control the quality of the support set, wherein, rather than sampling the nearest neighbors, we sample in the vicinity of hard nearest neighbors by varying the magnitude of the resultant vector and employing a stochastic sampling strategy to improve the performance. Additionally, to stabilize the effects of uncertainty in NN-based learning, we employ a smooth-weight-update approach for training the proposed network. Evaluation of the proposed method on multiple public image recognition and medical image recognition datasets shows that it performs up to 8 percent better than the baseline nearest neighbor method, and is comparable to other previously proposed SSL methods.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach"><a href="#Routing-Recovery-for-UAV-Networks-with-Deliberate-Attacks-A-Reinforcement-Learning-based-Approach" class="headerlink" title="Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach"></a>Routing Recovery for UAV Networks with Deliberate Attacks: A Reinforcement Learning based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06973">http://arxiv.org/abs/2308.06973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijie He, Ziye Jia, Chao Dong, Wei Wang, Yilu Cao, Yang Yang, Qihui Wu</li>
<li>for: 本文关注在无人航空器（UAV）网络中的路由计划和恢复，以解决UAV网络受到意外攻击的问题。</li>
<li>methods: 本文提出了一种基于节点重要性的攻击模型，并设计了一种节点重要性排名机制，考虑了节点和链接重要性。此外，本文还提出了一种基于强化学习的智能算法，以恢复UAV网络中的路由路径当UAVs被攻击。</li>
<li>results: 数据 simulate 结果表明，提出的机制比其他相关方法更为有效。<details>
<summary>Abstract</summary>
The unmanned aerial vehicle (UAV) network is popular these years due to its various applications. In the UAV network, routing is significantly affected by the distributed network topology, leading to the issue that UAVs are vulnerable to deliberate damage. Hence, this paper focuses on the routing plan and recovery for UAV networks with attacks. In detail, a deliberate attack model based on the importance of nodes is designed to represent enemy attacks. Then, a node importance ranking mechanism is presented, considering the degree of nodes and link importance. However, it is intractable to handle the routing problem by traditional methods for UAV networks, since link connections change with the UAV availability. Hence, an intelligent algorithm based on reinforcement learning is proposed to recover the routing path when UAVs are attacked. Simulations are conducted and numerical results verify the proposed mechanism performs better than other referred methods.
</details>
<details>
<summary>摘要</summary>
“无人航空器（UAV）网络在近年得到广泛应用，但是它们的路由却受到分布网络架构的影响，导致UAV易受到意外攻击。因此，本文关注于UAV网络路由规划和恢复，并对攻击后路由路径的恢复进行了研究。具体来说，我们设计了一种基于节点重要性的攻击模型，并提出了一种考虑节点度和链接重要性的节点重要性排名机制。然而，由于UAV网络中链接的连接变化，传统方法无法有效地处理UAV网络的路由问题。因此，我们提出了一种基于强化学习算法的智能路由恢复方法，以恢复在攻击后的路由路径。我们通过实验和数值结果发现，提议的机制在攻击后路由恢复方面表现更好 than其他已知方法。”Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation"><a href="#AutoAssign-Automatic-Shared-Embedding-Assignment-in-Streaming-Recommendation" class="headerlink" title="AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation"></a>AutoAssign+: Automatic Shared Embedding Assignment in Streaming Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06965">http://arxiv.org/abs/2308.06965</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus">https://github.com/Applied-Machine-Learning-Lab/AutoAssign-Plus</a></li>
<li>paper_authors: Ziru Liu, Kecheng Chen, Fengyi Song, Bo Chen, Xiangyu Zhao, Huifeng Guo, Ruiming Tang</li>
<li>for: addressing new user IDs or item IDs in streaming recommender systems</li>
<li>methods: utilizes reinforcement learning-driven framework with an Identity Agent and critic network to dynamically determine shared embeddings and retain&#x2F;eliminate ID features</li>
<li>results: significantly enhances recommendation performance and reduces memory usage by approximately 20-30%<details>
<summary>Abstract</summary>
In the domain of streaming recommender systems, conventional methods for addressing new user IDs or item IDs typically involve assigning initial ID embeddings randomly. However, this practice results in two practical challenges: (i) Items or users with limited interactive data may yield suboptimal prediction performance. (ii) Embedding new IDs or low-frequency IDs necessitates consistently expanding the embedding table, leading to unnecessary memory consumption. In light of these concerns, we introduce a reinforcement learning-driven framework, namely AutoAssign+, that facilitates Automatic Shared Embedding Assignment Plus. To be specific, AutoAssign+ utilizes an Identity Agent as an actor network, which plays a dual role: (i) Representing low-frequency IDs field-wise with a small set of shared embeddings to enhance the embedding initialization, and (ii) Dynamically determining which ID features should be retained or eliminated in the embedding table. The policy of the agent is optimized with the guidance of a critic network. To evaluate the effectiveness of our approach, we perform extensive experiments on three commonly used benchmark datasets. Our experiment results demonstrate that AutoAssign+ is capable of significantly enhancing recommendation performance by mitigating the cold-start problem. Furthermore, our framework yields a reduction in memory usage of approximately 20-30%, verifying its practical effectiveness and efficiency for streaming recommender systems.
</details>
<details>
<summary>摘要</summary>
在流媒体推荐系统领域，传统的新用户ID或项目ID处理方法通常是随机分配初始ID embedding。然而，这种做法会导致两个实际挑战：（i）有限交互数据的物品或用户可能会得到下标性的预测性能。（ii）为新ID或低频ID分配 embedding 表需要持续扩展 embedding 表，从而导致不必要的内存浪费。为了解决这些问题，我们提出了一个基于强化学习的框架，即 AutoAssign+。具体来说，AutoAssign+ 使用一个 Identity Agent 作为 actor 网络，该网络在两个角色中发挥作用：（i）通过将低频ID分配到一小set of shared embedding来提高初始化 embedding，并（ii）在 embedding 表中动态确定需要保留或删除的 ID 特征。Policy 网络的优化受到批评网络的指导。为评估我们的方法的效果，我们在三个常用的数据集上进行了广泛的实验。实验结果表明，AutoAssign+ 能够有效解决冷启点问题，并且减少内存使用率约 20-30%，证明我们的方法在流媒体推荐系统中具有实际效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis"><a href="#Graph-Structural-Residuals-A-Learning-Approach-to-Diagnosis" class="headerlink" title="Graph Structural Residuals: A Learning Approach to Diagnosis"></a>Graph Structural Residuals: A Learning Approach to Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06961">http://arxiv.org/abs/2308.06961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Lukas Augustin, Oliver Niggemann</li>
<li>for: 提出了一种新的框架，将模型基于诊断结合深度图结构学习。</li>
<li>methods: 利用数据学习系统的下面结构，并通过两个不同的图邻元矩阵来提供动态观察。</li>
<li>results: 通过实验示范了一种数据驱动的诊断方法的潜在优势。<details>
<summary>Abstract</summary>
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
</details>
<details>
<summary>摘要</summary>
传统的模型基于诊断方法是通过构建明确的系统模型来进行，这可能是劳动密集且需要专家知识的。在这篇论文中，我们提出了一种新的框架，它结合了模型基于诊断和深度图结构学习的概念。这种数据驱动的方法利用数据来学习系统的下面结构，并提供动态观察结果，表示为两个不同的图邻接矩阵。我们的工作使得图结构学习与模型基于诊断的集成变得自然和简单，我们的主要贡献包括：1. 重新定义系统表示、观察和缺陷的构造2. 提出了两种不同的自我超级VI持模型建立方法3. 通过对振荡器系统进行实验，证明我们的数据驱动诊断方法的潜力。
</details></li>
</ul>
<hr>
<h2 id="Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks"><a href="#Search-to-Fine-tune-Pre-trained-Graph-Neural-Networks-for-Graph-level-Tasks" class="headerlink" title="Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks"></a>Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06960">http://arxiv.org/abs/2308.06960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Wang, Shimin Di, Lei Chen, Xiaofang Zhou</li>
<li>for: 本研究旨在提高预训练GNNs的表现，并设计一种适应性更高的微调策略。</li>
<li>methods: 我们提出了一种基于搜索的微调策略，称为S2PGNN，可以适应不同的下游任务和数据集。</li>
<li>results: 我们在10个著名的预训练GNNs上进行了实验，并证明了S2PGNN可以在不同的下游任务和数据集上提高模型的表现，并且比现有的微调策略更高效。<details>
<summary>Abstract</summary>
Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search to fine-tune pre-trained graph neural networks for graph-level tasks (S2PGNN), which adaptively design a suitable fine-tuning framework for the given labeled data on the downstream task. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at \url{https://anonymous.4open.science/r/code_icde2024-A9CB/}.
</details>
<details>
<summary>摘要</summary>
近些年来，图 нейрон网络（GNNs）在许多图关联任务中显示出前无似的成功。然而，GNNs still faces the label scarcity issue like other neural networks do. Therefore, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only a few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs to improve the model performance in this paper. Given a pre-trained GNN, we propose to search for a suitable fine-tuning framework for the given labeled data on the downstream task, which we call S2PGNN. To ensure the improvement brought by searching fine-tuning strategy, we carefully summarize a proper search space of fine-tuning framework that is suitable for GNNs. The empirical studies show that S2PGNN can be implemented on the top of 10 famous pre-trained GNNs and consistently improve their performance. Besides, S2PGNN achieves better performance than existing fine-tuning strategies within and outside the GNN area. Our code is publicly available at [uri].
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II"><a href="#Data-Driven-Allocation-of-Preventive-Care-With-Application-to-Diabetes-Mellitus-Type-II" class="headerlink" title="Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II"></a>Data-Driven Allocation of Preventive Care With Application to Diabetes Mellitus Type II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06959">http://arxiv.org/abs/2308.06959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Kraus, Stefan Feuerriegel, Maytal Saar-Tsechansky</li>
<li>for: 这篇论文的目的是为了提出一个可靠的、成本效益的决策模型，用于分配预防性治疗给有风险的病人。</li>
<li>methods: 本论文使用了Counterfactual推理、机器学习和优化技术，搭建了可扩展的决策模型，可以利用现代医疗记录中的高维度医疗数据。</li>
<li>results: 根据89,191名 prediabetic 病人的电子医疗记录进行评估，我们的决策模型可以与现有的实践相比，实现每年的成本储储11亿美元。此外，我们还分析了不同预算水平下的成本效益。<details>
<summary>Abstract</summary>
Problem Definition. Increasing costs of healthcare highlight the importance of effective disease prevention. However, decision models for allocating preventive care are lacking.   Methodology/Results. In this paper, we develop a data-driven decision model for determining a cost-effective allocation of preventive treatments to patients at risk. Specifically, we combine counterfactual inference, machine learning, and optimization techniques to build a scalable decision model that can exploit high-dimensional medical data, such as the data found in modern electronic health records. Our decision model is evaluated based on electronic health records from 89,191 prediabetic patients. We compare the allocation of preventive treatments (metformin) prescribed by our data-driven decision model with that of current practice. We find that if our approach is applied to the U.S. population, it can yield annual savings of $1.1 billion. Finally, we analyze the cost-effectiveness under varying budget levels.   Managerial Implications. Our work supports decision-making in health management, with the goal of achieving effective disease prevention at lower costs. Importantly, our decision model is generic and can thus be used for effective allocation of preventive care for other preventable diseases.
</details>
<details>
<summary>摘要</summary>
问题定义：医疗费用的增长强调了疾病预防的重要性。然而，疾病预防投入决策模型缺失。方法ология/结果：在这篇论文中，我们开发了基于数据驱动的疾病预防投入决策模型，用于确定对患有风险的患者进行成本效果的预防治疗分配。我们结合了Counterfactual推理、机器学习和优化技术，构建了可扩展的决策模型，可以利用现代电子医疗记录中的高维医疗数据。我们的决策模型通过对89191名 prediabetic 患者的电子医疗记录进行评估。我们比较了我们的数据驱动决策模型与当前做法分配预防治疗（metformin）的情况。我们发现，如果我们的方法应用于美国人口，可以每年节省11亿美元。最后，我们分析了不同预算水平下的成本效果。管理意义：我们的工作支持医疗管理决策，以实现有效的疾病预防，并降低成本。重要的是，我们的决策模型是通用的，可以用于有效地分配预防治疗其他预防性疾病。
</details></li>
</ul>
<hr>
<h2 id="CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets"><a href="#CEmb-SAM-Segment-Anything-Model-with-Condition-Embedding-for-Joint-Learning-from-Heterogeneous-Datasets" class="headerlink" title="CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets"></a>CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06957">http://arxiv.org/abs/2308.06957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongik Shin, Beomsuk Kim, Seungjun Baek</li>
<li>for: 这篇论文是用于探讨如何将不同类型的静脉影像融合为一个共同的数据集，以提高医疗影像分类模型的通用能力。</li>
<li>methods: 这篇论文使用的方法是将不同类型的静脉影像融合为一个共同的数据集，然后使用Segment Anything模型（SAM）来进行静脉影像分类。此外，论文还提出了一个名为Condition Embedding block（CEmb）的新方法，可以将不同数据集的特性统计学 Normalization。</li>
<li>results: 实验结果显示，CEmb-SAM比基eline方法在静脉影像分类 задачі中表现更好，特别是在 péripheral nerves和breast cancer领域。这些结果显示了Cemb-SAM在医疗影像分类任务中学习不同数据集的能力。<details>
<summary>Abstract</summary>
Automated segmentation of ultrasound images can assist medical experts with diagnostic and therapeutic procedures. Although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. In this paper, we consider the problem of jointly learning from heterogeneous datasets so that the model can improve generalization abilities by leveraging the inherent variability among datasets. We merge the heterogeneous datasets into one dataset and refer to each component dataset as a subgroup. We propose to train a single segmentation model so that the model can adapt to each sub-group. For robust segmentation, we leverage recently proposed Segment Anything model (SAM) in order to incorporate sub-group information into the model. We propose SAM with Condition Embedding block (CEmb-SAM) which encodes sub-group conditions and combines them with image embeddings from SAM. The conditional embedding block effectively adapts SAM to each image sub-group by incorporating dataset properties through learnable parameters for normalization. Experiments show that CEmb-SAM outperforms the baseline methods on ultrasound image segmentation for peripheral nerves and breast cancer. The experiments highlight the effectiveness of Cemb-SAM in learning from heterogeneous datasets in medical image segmentation tasks.
</details>
<details>
<summary>摘要</summary>
自动分割超音波图像可以帮助医疗专家进行诊断和治疗过程。although using the common modality of ultrasound, one typically needs separate datasets in order to segment, for example, different anatomical structures or lesions with different levels of malignancy. 在这篇论文中，我们考虑了将异构数据集合在一起，以便模型可以提高通用能力，并且可以利用数据集的内在多样性。我们将异构数据集合为一个数据集，并将每个子组数据集称为子组。我们提议使用单个分割模型，以便模型可以适应每个子组。为了增强分割稳定性，我们利用最近提出的 Segment Anything model (SAM)，并在SAM中添加 Condition Embedding block (CEmb)，以编码子组条件并与图像嵌入结合。 conditional embedding block Effectively adapts SAM to each image subgroup by incorporating dataset properties through learnable parameters for normalization. 实验显示，CEmb-SAM在超音波图像分割任务中超过基eline方法表现，特别是在 péripheral nerves 和乳腺癌中。这些实验证明了 CEmb-SAM 在医学图像分割任务中学习异构数据集的有效性。
</details></li>
</ul>
<hr>
<h2 id="Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels"><a href="#Channel-Wise-Contrastive-Learning-for-Learning-with-Noisy-Labels" class="headerlink" title="Channel-Wise Contrastive Learning for Learning with Noisy Labels"></a>Channel-Wise Contrastive Learning for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06952">http://arxiv.org/abs/2308.06952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Kang, Sheng Liu, Huaxi Huang, Tongliang Liu</li>
<li>for: 本文针对受损标签学习（LNL）问题，旨在训练一个能够识别实际类别的分类器。</li>
<li>methods: 本文提出了通道级别对比学习（CWCL）方法，通过对不同通道进行对比学习，分离真实标签信息和噪声。</li>
<li>results: 对多个基准数据集进行评估，本文的方法与现有方法相比，具有更高的识别率和更好的抗噪声性。<details>
<summary>Abstract</summary>
In real-world datasets, noisy labels are pervasive. The challenge of learning with noisy labels (LNL) is to train a classifier that discerns the actual classes from given instances. For this, the model must identify features indicative of the authentic labels. While research indicates that genuine label information is embedded in the learned features of even inaccurately labeled data, it's often intertwined with noise, complicating its direct application. Addressing this, we introduce channel-wise contrastive learning (CWCL). This method distinguishes authentic label information from noise by undertaking contrastive learning across diverse channels. Unlike conventional instance-wise contrastive learning (IWCL), CWCL tends to yield more nuanced and resilient features aligned with the authentic labels. Our strategy is twofold: firstly, using CWCL to extract pertinent features to identify cleanly labeled samples, and secondly, progressively fine-tuning using these samples. Evaluations on several benchmark datasets validate our method's superiority over existing approaches.
</details>
<details>
<summary>摘要</summary>
实际数据集中，噪声标签是普遍存在的。学习噪声标签（LNL）的挑战是训练一个可以从给定的实例中分辨实际的类别的分类器。为此，模型必须标识表示实际标签的特征。虽然研究表明，正确的标签信息在噪声标签的数据中被学习的特征中嵌入，但它通常与噪声杂mix在一起，使其直接应用更加复杂。为解决这个问题，我们引入通道wise contrastive learning（CWCL）。这种方法通过在多个通道进行对比学习来 отличи出实际标签信息和噪声。与传统的实例wise contrastive learning（IWCL）不同，CWCL往往可以生成更加细化和鲜明的特征，与实际标签更加相似。我们的策略是两重的：首先，使用CWCL提取重要的特征，以便从干净标注的样本中分辨实际标签信息；其次，逐渐练化使用这些样本。我们在多个标准 benchmark 数据集上进行了评估，并证明了我们的方法与现有方法相比具有superiority。
</details></li>
</ul>
<hr>
<h2 id="Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding"><a href="#Knowing-Where-to-Focus-Event-aware-Transformer-for-Video-Grounding" class="headerlink" title="Knowing Where to Focus: Event-aware Transformer for Video Grounding"></a>Knowing Where to Focus: Event-aware Transformer for Video Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06947">http://arxiv.org/abs/2308.06947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/eatr">https://github.com/jinhyunj/eatr</a></li>
<li>paper_authors: Jinhyun Jang, Jungin Park, Jin Kim, Hyeongjun Kwon, Kwanghoon Sohn</li>
<li>for: 这 paper 的目的是提出一种事件意识的动态时刻查询方法，以便模型可以根据输入视频的内容和位置信息来更好地预测时刻。</li>
<li>methods: 这 paper 使用了一种叫做槽注意机制的事件分解技术，以及一种名叫网关融合变换层的时刻查询融合技术，来实现事件意识的动态时刻查询。</li>
<li>results: 根据实验结果，这 paper 的事件意识动态时刻查询方法在多个视频锚点识别 benchmark 上表现出了比州前方法更高的效果和效率。<details>
<summary>Abstract</summary>
Recent DETR-based video grounding models have made the model directly predict moment timestamps without any hand-crafted components, such as a pre-defined proposal or non-maximum suppression, by learning moment queries. However, their input-agnostic moment queries inevitably overlook an intrinsic temporal structure of a video, providing limited positional information. In this paper, we formulate an event-aware dynamic moment query to enable the model to take the input-specific content and positional information of the video into account. To this end, we present two levels of reasoning: 1) Event reasoning that captures distinctive event units constituting a given video using a slot attention mechanism; and 2) moment reasoning that fuses the moment queries with a given sentence through a gated fusion transformer layer and learns interactions between the moment queries and video-sentence representations to predict moment timestamps. Extensive experiments demonstrate the effectiveness and efficiency of the event-aware dynamic moment queries, outperforming state-of-the-art approaches on several video grounding benchmarks.
</details>
<details>
<summary>摘要</summary>
现代基于DETR的录像落幕模型已经直接预测时刻无需任何手工Component，如预先定义的提案或非最大抑制，通过学习时刻查询。然而，这些输入不具预设的时刻查询无法考虑录像的自然时间结构，仅提供有限的位置信息。在本文中，我们提出了事件意识的动态时刻查询，让模型能够根据输入内容和位置信息进行事件对话。为此，我们提出了两种逻辑：1）事件逻辑，使用槽注意力机制来捕捉录像中的特定事件单位；2）时刻逻辑，将时刻查询与输入句子的数据融合，透过闸道融合对应层来学习时刻查询与录像句子表示之间的互动，以预测时刻。实验结果显示了事件意识的动态时刻查询的有效性和高效性，在多个录像落幕 bencmarks 上出perform state-of-the-art 方法。
</details></li>
</ul>
<hr>
<h2 id="Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis"><a href="#Semantic-aware-Network-for-Aerial-to-Ground-Image-Synthesis" class="headerlink" title="Semantic-aware Network for Aerial-to-Ground Image Synthesis"></a>Semantic-aware Network for Aerial-to-Ground Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06945">http://arxiv.org/abs/2308.06945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinhyunj/sanet">https://github.com/jinhyunj/sanet</a></li>
<li>paper_authors: Jinhyun Jang, Taeyong Song, Kwanghoon Sohn</li>
<li>for: 这个论文旨在解决飞行图像与地面图像的同构问题，以实现将飞行图像中的元素转映到地面图像中。</li>
<li>methods: 该论文提出了一个新的框架，通过增强结构对应和Semantic意识来解决这个问题。它 introduce了一种新的Semantic-attentive feature transformation模块，可以重建复杂的地理结构。此外，论文还提出了Semantic-aware的损失函数，通过利用预训练的分类网络，让网络synthesize出realistic的对象。</li>
<li>results: 对比之前的方法和简化研究，论文的方法得到了较高的效果， both qualitatively and quantitatively。<details>
<summary>Abstract</summary>
Aerial-to-ground image synthesis is an emerging and challenging problem that aims to synthesize a ground image from an aerial image. Due to the highly different layout and object representation between the aerial and ground images, existing approaches usually fail to transfer the components of the aerial scene into the ground scene. In this paper, we propose a novel framework to explore the challenges by imposing enhanced structural alignment and semantic awareness. We introduce a novel semantic-attentive feature transformation module that allows to reconstruct the complex geographic structures by aligning the aerial feature to the ground layout. Furthermore, we propose semantic-aware loss functions by leveraging a pre-trained segmentation network. The network is enforced to synthesize realistic objects across various classes by separately calculating losses for different classes and balancing them. Extensive experiments including comparisons with previous methods and ablation studies show the effectiveness of the proposed framework both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:空中到地面图像合成是一个emerging和挑战性的问题，目标是将空中图像转换为地面图像。由于空中和地面图像的布局和对象表示方式之间存在巨大差异，现有的方法通常无法将空中场景中的组件转移到地面场景中。在这篇论文中，我们提出了一个新的框架，以强化结构对Alignment和Semantic意识。我们引入了一个新的启发式Semantic-attentive特征转换模块，以将空中特征转换为地面布局。此外，我们提出了Semantic-aware的损失函数，通过利用预训练的分割网络来强制网络在不同类型的对象上synthesize出真实的对象。我们进行了广泛的实验，包括与前方法进行比较和简要的ablation研究，以证明我们的框架的有效性。
</details></li>
</ul>
<hr>
<h2 id="Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning"><a href="#Insurance-pricing-on-price-comparison-websites-via-reinforcement-learning" class="headerlink" title="Insurance pricing on price comparison websites via reinforcement learning"></a>Insurance pricing on price comparison websites via reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06935">http://arxiv.org/abs/2308.06935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanut Treetanthiploet, Yufei Zhang, Lukasz Szpruch, Isaac Bowers-Barnard, Henrietta Ridley, James Hickey, Chris Pearce</li>
<li>for: 本研究旨在为保险公司在价格比较网站（PCW）上形ulation Effective 价格策略提供技术支持。</li>
<li>methods: 本研究使用了强化学习（RL）框架，通过结合模型基于和模型自由方法来学习价格策略。</li>
<li>results: 比较 experiment 表明，我们的方法在sample efficiency和累积奖励方面超过了6个参考方法，只有在市场情况具有完美信息时才能达到相同水平。<details>
<summary>Abstract</summary>
The emergence of price comparison websites (PCWs) has presented insurers with unique challenges in formulating effective pricing strategies. Operating on PCWs requires insurers to strike a delicate balance between competitive premiums and profitability, amidst obstacles such as low historical conversion rates, limited visibility of competitors' actions, and a dynamic market environment. In addition to this, the capital intensive nature of the business means pricing below the risk levels of customers can result in solvency issues for the insurer. To address these challenges, this paper introduces reinforcement learning (RL) framework that learns the optimal pricing policy by integrating model-based and model-free methods. The model-based component is used to train agents in an offline setting, avoiding cold-start issues, while model-free algorithms are then employed in a contextual bandit (CB) manner to dynamically update the pricing policy to maximise the expected revenue. This facilitates quick adaptation to evolving market dynamics and enhances algorithm efficiency and decision interpretability. The paper also highlights the importance of evaluating pricing policies using an offline dataset in a consistent fashion and demonstrates the superiority of the proposed methodology over existing off-the-shelf RL/CB approaches. We validate our methodology using synthetic data, generated to reflect private commercially available data within real-world insurers, and compare against 6 other benchmark approaches. Our hybrid agent outperforms these benchmarks in terms of sample efficiency and cumulative reward with the exception of an agent that has access to perfect market information which would not be available in a real-world set-up.
</details>
<details>
<summary>摘要</summary>
互联网价格比较网站（PCW）的出现对保险公司带来了独特的挑战。在PCW上运营时，保险公司需要维护一个折衔的平衡，以确保竞争力和利润之间的协调。这些挑战包括历史 conversions 率低、竞争对手动作的有限可见性以及动态的市场环境。此外，保险业务具有资本投入的特点，因此低于客户风险水平的价格可能会导致资本问题。为解决这些挑战，本文提出了一种基于强化学习（RL）框架的价格策略优化方法。RL框架结合模型基于和模型自由两种方法，通过在离线环境中训练代理人，避免冷启始问题，然后在Contextual Bandit（CB）上使用模型自由算法动态更新价格策略，以最大化预期收益。这有助于快速适应市场动态变化，提高算法效率和决策可读性。文章还强调了评估价格策略的离线数据集的一致性，并证明提议的方法在已有的RL/CB方法中表现出色。我们验证了方法使用人工生成的数据，模拟了实际保险公司的私人数据，并与6个参考方法进行比较。我们的混合代理人在样本效益和累积奖励方面都超越参考方法，只有具有完美市场信息的代理人能够在实际场景中匹配我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models"><a href="#Predicting-Listing-Prices-In-Dynamic-Short-Term-Rental-Markets-Using-Machine-Learning-Models" class="headerlink" title="Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models"></a>Predicting Listing Prices In Dynamic Short Term Rental Markets Using Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06929">http://arxiv.org/abs/2308.06929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Chapman, Seifey Mohammad, Kimberly Villegas</li>
<li>for: 预测Airbnb租赁价格，帮助hosts优化收益和助旅行者做出 Informed 预订决策。</li>
<li>methods: 使用机器学习模型方法，包括sentiment分析，对Airbnb租赁价格进行预测和分析。</li>
<li>results: 通过对Airbnb租赁价格进行预测和分析，提高了hosts的收益和旅行者的决策能力。<details>
<summary>Abstract</summary>
Our research group wanted to take on the difficult task of predicting prices in a dynamic market. And short term rentals such as Airbnb listings seemed to be the perfect proving ground to do such a thing. Airbnb has revolutionized the travel industry by providing a platform for homeowners to rent out their properties to travelers. The pricing of Airbnb rentals is prone to high fluctuations, with prices changing frequently based on demand, seasonality, and other factors. Accurate prediction of Airbnb rental prices is crucial for hosts to optimize their revenue and for travelers to make informed booking decisions. In this project, we aim to predict the prices of Airbnb rentals using a machine learning modeling approach.   Our project expands on earlier research in the area of analyzing Airbnb rental prices by taking a methodical machine learning approach as well as incorporating sentiment analysis into our feature engineering. We intend to gain a deeper understanding on periodic changes of Airbnb rental prices. The primary objective of this study is to construct an accurate machine learning model for predicting Airbnb rental prices specifically in Austin, Texas. Our project's secondary objective is to identify the key factors that drive Airbnb rental prices and to investigate how these factors vary across different locations and property types.
</details>
<details>
<summary>摘要</summary>
我们的研究小组想要解决难度较大的价格预测问题，并选择短期租赁如Airbnb列表作为证明场景。Airbnb已经革命化旅游业，提供了为房东租出房屋给旅行者的平台。Airbnb租赁价格受到高度波动的影响，价格随着需求、季节和其他因素而变化频繁。正确预测Airbnb租赁价格对房东来说是提高收益的关键，对旅行者来说也是为了做出 Informed 预订决策。在这个项目中，我们使用机器学习模型预测Airbnb租赁价格。我们的项目在对Airbnb租赁价格分析方面进行了深入探索，并且通过包括情感分析在内的特征工程来扩展我们的研究。我们的主要目标是在得克萨斯州奥斯汀建立一个准确的机器学习模型，用于预测Airbnb租赁价格。项目的次要目标是Identify 租赁价格的关键因素，以及这些因素在不同的地点和财产类型中的变化趋势。
</details></li>
</ul>
<hr>
<h2 id="CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor"><a href="#CBA-Improving-Online-Continual-Learning-via-Continual-Bias-Adaptor" class="headerlink" title="CBA: Improving Online Continual Learning via Continual Bias Adaptor"></a>CBA: Improving Online Continual Learning via Continual Bias Adaptor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06925">http://arxiv.org/abs/2308.06925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wqza/cba-online-cl">https://github.com/wqza/cba-online-cl</a></li>
<li>paper_authors: Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, Deyu Meng</li>
<li>for: 这篇论文目的是为了解决在线上持续学习（Continual Learning，CL）中的分布变化问题，以确保模型能够稳定地学习新的知识和固化先前学习的知识。</li>
<li>methods: 本文提出了一个增强器（Continual Bias Adaptor，CBA）模组，用于在训练过程中将分类器网络调整为适应不断变化的分布，以避免模型对先前学习的知识忘记和偏向新的任务。</li>
<li>results: 本文透过实验证明了CBA模组能够有效地解决分布变化问题，并且不会增加训练过程中的计算成本和记忆遗传。<details>
<summary>Abstract</summary>
Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases toward the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that the classifier network is able to learn a stable consolidation of previously learned tasks. In the testing stage, CBA can be removed which introduces no additional computation cost and memory overhead. We theoretically reveal the reason why the proposed method can effectively alleviate catastrophic distribution shifts, and empirically demonstrate its effectiveness through extensive experiments based on four rehearsal-based baselines and three public continual learning benchmarks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在线 continual learning (CL) 目标是在非站点数据流中学习新知识并固化先前学习的知识。由于训练环境的时间变化，学习到的模型很容易忘记先前学习的知识，偏向新接收的任务。为解决这个问题，我们提议一个 Continual Bias Adaptor (CBA) 模块，用于在训练过程中增强分类网络，以适应不断变化的分布，使得分类网络能够稳定地固化先前学习的任务。在测试阶段，CBA可以被移除，无需额外的计算成本和内存占用。我们理论上解释了我们提议的方法可以有效缓解悬危分布变化的问题，并通过了四种基eline和三个公共 continual learning 标准 benchmark 的实验来证明其效果。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings"><a href="#A-Novel-Ehanced-Move-Recognition-Algorithm-Based-on-Pre-trained-Models-with-Positional-Embeddings" class="headerlink" title="A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings"></a>A Novel Ehanced Move Recognition Algorithm Based on Pre-trained Models with Positional Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10822">http://arxiv.org/abs/2308.10822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wen, Jie Wang, Xiaodong Qiao</li>
<li>for: 本研究旨在提高中文科技论文摘要中的 Move 识别精度。</li>
<li>methods: 该研究提出了一种基于改进预训练模型和扩展网络含attend mechanism的新型加强 Move 识别算法。</li>
<li>results: 实验结果显示，该算法在分 Split 数据集上比原始数据集提高13.37%的精度，并与基本对比模型提高7.55%的精度。<details>
<summary>Abstract</summary>
The recognition of abstracts is crucial for effectively locating the content and clarifying the article. Existing move recognition algorithms lack the ability to learn word position information to obtain contextual semantics. This paper proposes a novel enhanced move recognition algorithm with an improved pre-trained model and a gated network with attention mechanism for unstructured abstracts of Chinese scientific and technological papers. The proposed algorithm first performs summary data segmentation and vocabulary training. The EP-ERNIE$\_$AT-GRU framework is leveraged to incorporate word positional information, facilitating deep semantic learning and targeted feature extraction. Experimental results demonstrate that the proposed algorithm achieves 13.37$\%$ higher accuracy on the split dataset than on the original dataset and a 7.55$\%$ improvement in accuracy over the basic comparison model.
</details>
<details>
<summary>摘要</summary>
“抽象概念识别是科技文献检索和理解的关键。现有的移动识别算法无法学习单词位置信息，导致Contextual semantics的学习受限。本文提出了一种基于EP-ERNIE$\_$AT-GRU框架的增强移动识别算法，用于处理中文科技文献抽象。该算法首先进行摘要数据分 segmentation和词汇训练。实验结果表明，提posed算法在分组数据集上的准确率高于原始数据集13.37%，并且与基本比较模型相比提高7.55%。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CausalLM-is-not-optimal-for-in-context-learning"><a href="#CausalLM-is-not-optimal-for-in-context-learning" class="headerlink" title="CausalLM is not optimal for in-context learning"></a>CausalLM is not optimal for in-context learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06912">http://arxiv.org/abs/2308.06912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Ding, Tomer Levinboim, Jialin Wu, Sebastian Goodman, Radu Soricut</li>
<li>for: 本研究探讨了使用前缀语言模型（prefixLM）和 causalLanguage Model（causalLM）在卷积Transformer上的受限学习性能的比较。</li>
<li>methods: 本研究采用了理论方法来分析prefixLM和causalLM的参数构造下的收敛行为。</li>
<li>results: 研究发现，prefixLM在线性回归问题上 converges to its optimal solution，而causalLM的收敛动态类似于在线梯度下降算法，并不一定是最优解，即使数据量无限大。Empirical experiments over synthetic and real tasks verify that causalLM consistently underperforms prefixLM in all settings。<details>
<summary>Abstract</summary>
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.
</details>
<details>
<summary>摘要</summary>
近期实验证据表明，基于转换器的增强学习在使用前缀语言模型（前缀LM）下表现更好，因为所有的增强样本都可以相互听说，而不使用 causalLM（ causalLM），它使用自动循环注意力，禁止增强样本听说未来的样本。虽然这种结果是直观的，但从理论角度不是很了解。在这篇论文中，我们采取了理论方法，分析了 prefixLM 和 causalLM 下的参数构造下的收敛行为。我们的分析表明，两种LM类型都会在一定的参数构造下收敛到其站点点，但是 prefixLM 会收敛到线性回归的优化解，而 causalLM 的收敛动力则类似于在线上的梯度下降算法，这并不是保证优化的，即使样本数量在无穷大。我们通过实验证明， causalLM 在所有设置下一直表现出下降性。
</details></li>
</ul>
<hr>
<h2 id="GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text"><a href="#GIT-Mol-A-Multi-modal-Large-Language-Model-for-Molecular-Science-with-Graph-Image-and-Text" class="headerlink" title="GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"></a>GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06911">http://arxiv.org/abs/2308.06911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Liu, Yiming Ren, Zhixiang Ren</li>
<li>for: 这个论文主要是为了提出一种多ModalLanguage模型，用于融合Graph、Image和Text信息，以提高分子数据的表示和生成能力。</li>
<li>methods: 这种模型使用GIT-Former架构，可以将所有modalities映射到一个共同准则空间中，以便进行多ModalLanguage的表示和计算。</li>
<li>results: 该研究提出了一种创新的任意语言分子翻译策略，提高了分子描述率10%-15%，提高了属性预测精度5%-10%，并提高了分子生成有效性20%。<details>
<summary>Abstract</summary>
Large language models have made significant strides in natural language processing, paving the way for innovative applications including molecular representation and generation. However, most existing single-modality approaches cannot capture the abundant and complex information in molecular data. Here, we introduce GIT-Mol, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space. Our study develops an innovative any-to-language molecular translation strategy and achieves a 10%-15% improvement in molecular captioning, a 5%-10% accuracy increase in property prediction, and a 20% boost in molecule generation validity compared to baseline or single-modality models.
</details>
<details>
<summary>摘要</summary>
大型语言模型在自然语言处理方面做出了重要进步，开辟了创新应用，如分子表示和生成。然而，现有的单一模式方法无法捕捉分子数据中的丰富和复杂信息。在这里，我们介绍GIT-Mol，一个多模式大语言模型，它结合结构граф、图像和文本信息，包括简单分子输入系统（SMILES）和分子描述。为了实现多modal分子数据的整合，我们提出GIT-Former，一个新的架构，可以将所有模式转换到一个统一的潜在空间中。我们的研究开发了一种创新的任何语言分子翻译策略，并在分子描述、性能预测和分子生成领域中实现了10%-15%的改进，5%-10%的精度提高和20%的额外验证。
</details></li>
</ul>
<hr>
<h2 id="Generative-Interpretation"><a href="#Generative-Interpretation" class="headerlink" title="Generative Interpretation"></a>Generative Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06907">http://arxiv.org/abs/2308.06907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yonathanarbel/generativeinterpretation">https://github.com/yonathanarbel/generativeinterpretation</a></li>
<li>paper_authors: Yonathan A. Arbel, David Hoffman</li>
<li>for: 这篇论文旨在提出一种新的合同理解方法，使用大型自然语言模型来估计合同意义。</li>
<li>methods: 该论文采用了实践案例研究的方法，通过使用AI模型来查看不同的应用场景，并使用实际的合同文档来证明AI模型的能力。</li>
<li>results: 该论文显示了AI模型可以帮助法官和仲裁人员更好地理解合同的意义，量化含义的混淆度，并填充合同中的缺失。同时，该论文还描述了使用这些模型时的限制和风险，以及它们对法律实践和合同理论的影响。<details>
<summary>Abstract</summary>
We introduce generative interpretation, a new approach to estimating contractual meaning using large language models. As AI triumphalism is the order of the day, we proceed by way of grounded case studies, each illustrating the capabilities of these novel tools in distinct ways. Taking well-known contracts opinions, and sourcing the actual agreements that they adjudicated, we show that AI models can help factfinders ascertain ordinary meaning in context, quantify ambiguity, and fill gaps in parties' agreements. We also illustrate how models can calculate the probative value of individual pieces of extrinsic evidence. After offering best practices for the use of these models given their limitations, we consider their implications for judicial practice and contract theory. Using LLMs permits courts to estimate what the parties intended cheaply and accurately, and as such generative interpretation unsettles the current interpretative stalemate. Their use responds to efficiency-minded textualists and justice-oriented contextualists, who argue about whether parties will prefer cost and certainty or accuracy and fairness. Parties--and courts--would prefer a middle path, in which adjudicators strive to predict what the contract really meant, admitting just enough context to approximate reality while avoiding unguided and biased assimilation of evidence. As generative interpretation offers this possibility, we argue it can become the new workhorse of contractual interpretation.
</details>
<details>
<summary>摘要</summary>
我们引入生成式解释，一种新的方法来估计合同意义使用大型语言模型。在人工智能胜利的时代，我们采用实地案例来证明这些新工具的能力，每个案例都展示了这些工具在不同方面的能力。我们使用知名合同案例，并提供了实际协议，以示AI模型可以帮助事实发现者在文本上确定常见意义，衡量模糊性，并填充党们的协议中的空白。我们还示出了模型可以计算个别外部证据的证据价值。接着，我们提供了使用这些模型的最佳实践，以及其限制。我们考虑了这些模型在法律实践和合同理论方面的影响，并 argues that these models can become the new workhorse of contractual interpretation.Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls"><a href="#Federated-Classification-in-Hyperbolic-Spaces-via-Secure-Aggregation-of-Convex-Hulls" class="headerlink" title="Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls"></a>Federated Classification in Hyperbolic Spaces via Secure Aggregation of Convex Hulls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06895">http://arxiv.org/abs/2308.06895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurav Prakash, Jin Sima, Chao Pan, Eli Chien, Olgica Milenkovic</li>
<li>for: 这个论文是为了研究在几何空间中进行分布式学习，以保护数据隐私。</li>
<li>methods: 该论文提出了一种基于几何空间的分布式学习方法，包括在Poincaré圆柱中分布式的支持向量机学习和一种基于整数B_h序列的标签恢复方法。</li>
<li>results: 该论文的实验结果表明，使用几何空间进行分布式学习可以提高分类精度，并且可以保护数据隐私。<details>
<summary>Abstract</summary>
Hierarchical and tree-like data sets arise in many applications, including language processing, graph data mining, phylogeny and genomics. It is known that tree-like data cannot be embedded into Euclidean spaces of finite dimension with small distortion. This problem can be mitigated through the use of hyperbolic spaces. When such data also has to be processed in a distributed and privatized setting, it becomes necessary to work with new federated learning methods tailored to hyperbolic spaces. As an initial step towards the development of the field of federated learning in hyperbolic spaces, we propose the first known approach to federated classification in hyperbolic spaces. Our contributions are as follows. First, we develop distributed versions of convex SVM classifiers for Poincar\'e discs. In this setting, the information conveyed from clients to the global classifier are convex hulls of clusters present in individual client data. Second, to avoid label switching issues, we introduce a number-theoretic approach for label recovery based on the so-called integer $B_h$ sequences. Third, we compute the complexity of the convex hulls in hyperbolic spaces to assess the extent of data leakage; at the same time, in order to limit the communication cost for the hulls, we propose a new quantization method for the Poincar\'e disc coupled with Reed-Solomon-like encoding. Fourth, at server level, we introduce a new approach for aggregating convex hulls of the clients based on balanced graph partitioning. We test our method on a collection of diverse data sets, including hierarchical single-cell RNA-seq data from different patients distributed across different repositories that have stringent privacy constraints. The classification accuracy of our method is up to $\sim 11\%$ better than its Euclidean counterpart, demonstrating the importance of privacy-preserving learning in hyperbolic spaces.
</details>
<details>
<summary>摘要</summary>
Hierarchical和树状数据集在许多应用中出现，包括语言处理、图数据挖掘、phylogeny和 genomics。已知树状数据无法在 finite 维 Euclidian 空间中嵌入，这问题可以通过使用抽象空间来缓解。在分布式和隐私化设置下，需要采用新的联邦学习方法，这是一个新的领域。作为这个领域的初步，我们提出了首个在抽象空间上的联邦分类方法。我们的贡献如下：1. 我们开发了分布式版本的凸 Support Vector Machine（SVM）分类器，用于Poincaré盘上的数据。在这个设置中，客户端上的信息是各个客户端数据中的封闭集。2. 为了避免标签交换问题，我们引入了一种数学推理的方法，基于 so-called 整数 $B_h$ 序列。3. 我们计算了抽象空间中的凸闭复杂度，以评估数据泄露程度，同时，我们提出了一种新的归一化方法，用于限制归一化成本。4. 在服务器端，我们引入了一种新的客户端归一化方法，基于平衡图分 partitioning。我们测试了我们的方法在多种多样的数据集上，包括层次单元 RNA-seq 数据集，来自不同的病人和不同的存储库，这些存储库具有严格的隐私限制。我们的方法的分类精度与其欧氏凸缩形相比，提高了约 11%，这表明了隐私保护在抽象空间上的学习的重要性。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders"><a href="#Bridging-Offline-Online-Evaluation-with-a-Time-dependent-and-Popularity-Bias-free-Offline-Metric-for-Recommenders" class="headerlink" title="Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders"></a>Bridging Offline-Online Evaluation with a Time-dependent and Popularity Bias-free Offline Metric for Recommenders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06885">http://arxiv.org/abs/2308.06885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Kasalický, Rodrigo Alves, Pavel Kordík</li>
<li>for: 研究和比较在线表现的Offline评估指标的选择，以提高live推荐系统中的选择。</li>
<li>methods: 使用减小流行item和考虑交易时间的评估方法，以提高选择的准确性。</li>
<li>results: 五个大型实际live数据中的平均结果，用于帮助学术社区更好地理解Offline评估和优化标准的选择。<details>
<summary>Abstract</summary>
The evaluation of recommendation systems is a complex task. The offline and online evaluation metrics for recommender systems are ambiguous in their true objectives. The majority of recently published papers benchmark their methods using ill-posed offline evaluation methodology that often fails to predict true online performance. Because of this, the impact that academic research has on the industry is reduced. The aim of our research is to investigate and compare the online performance of offline evaluation metrics. We show that penalizing popular items and considering the time of transactions during the evaluation significantly improves our ability to choose the best recommendation model for a live recommender system. Our results, averaged over five large-size real-world live data procured from recommenders, aim to help the academic community to understand better offline evaluation and optimization criteria that are more relevant for real applications of recommender systems.
</details>
<details>
<summary>摘要</summary>
评估推荐系统是一项复杂的任务。在线和离线评估指标对于推荐系统存在很多不确定性。大多数最近发表的论文使用不确定的离线评估方法来评估其方法的性能，这常导致实际上线性能和学术界的影响相差甚大。我们的研究目的是研究和比较离线评估指标对实时推荐系统的在线性能的影响。我们发现，对流行 item 的惩罚和在评估过程中考虑交易时间可以显著提高我们选择最佳推荐模型的能力。我们的结果，基于五个大型实际应用中的真实数据，希望能帮助学术界更好地理解推荐系统的离线评估和优化标准，以便更好地应用于实际应用中。
</details></li>
</ul>
<hr>
<h2 id="Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning"><a href="#Multi-Receiver-Task-Oriented-Communications-via-Multi-Task-Deep-Learning" class="headerlink" title="Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning"></a>Multi-Receiver Task-Oriented Communications via Multi-Task Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06884">http://arxiv.org/abs/2308.06884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus</li>
<li>for: 这 paper 研究了任务对应的通信系统，在 transmitter 与多个 receivers 之间进行任务完成和数据传输。</li>
<li>methods: 这 paper 提出了一种基于多任务深度学习的共同编码器和个别解码器的方法，用于对多个任务和多个接收器进行共同优化。</li>
<li>results: 实验结果表明，相比单任务传输系统，多任务传输系统可以更好地适应变化的通信频道条件，并且可以提高任务特定的目标 completions，同时减少传输负担。<details>
<summary>Abstract</summary>
This paper studies task-oriented, otherwise known as goal-oriented, communications, in a setting where a transmitter communicates with multiple receivers, each with its own task to complete on a dataset, e.g., images, available at the transmitter. A multi-task deep learning approach that involves training a common encoder at the transmitter and individual decoders at the receivers is presented for joint optimization of completing multiple tasks and communicating with multiple receivers. By providing efficient resource allocation at the edge of 6G networks, the proposed approach allows the communications system to adapt to varying channel conditions and achieves task-specific objectives while minimizing transmission overhead. Joint training of the encoder and decoders using multi-task learning captures shared information across tasks and optimizes the communication process accordingly. By leveraging the broadcast nature of wireless communications, multi-receiver task-oriented communications (MTOC) reduces the number of transmissions required to complete tasks at different receivers. Performance evaluation conducted on the MNIST, Fashion MNIST, and CIFAR-10 datasets (with image classification considered for different tasks) demonstrates the effectiveness of MTOC in terms of classification accuracy and resource utilization compared to single-task-oriented communication systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity"><a href="#Quantifying-Outlierness-of-Funds-from-their-Categories-using-Supervised-Similarity" class="headerlink" title="Quantifying Outlierness of Funds from their Categories using Supervised Similarity"></a>Quantifying Outlierness of Funds from their Categories using Supervised Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06882">http://arxiv.org/abs/2308.06882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Desai, Ashmita Dhiman, Tushar Sharma, Deepika Sharma, Dhagash Mehta, Stefano Pasquali</li>
<li>for: 本研究旨在量化基金分类错误的影响，以便改善投资管理决策。</li>
<li>methods: 本研究使用机器学习方法，将基金分类错误形式化为距离度量学习问题，并计算每个数据点的类别异常度量。</li>
<li>results: 研究发现，基金分类错误与未来回报之间存在强相关关系，并讨论了这些结果的意义。<details>
<summary>Abstract</summary>
Mutual fund categorization has become a standard tool for the investment management industry and is extensively used by allocators for portfolio construction and manager selection, as well as by fund managers for peer analysis and competitive positioning. As a result, a (unintended) miscategorization or lack of precision can significantly impact allocation decisions and investment fund managers. Here, we aim to quantify the effect of miscategorization of funds utilizing a machine learning based approach. We formulate the problem of miscategorization of funds as a distance-based outlier detection problem, where the outliers are the data-points that are far from the rest of the data-points in the given feature space. We implement and employ a Random Forest (RF) based method of distance metric learning, and compute the so-called class-wise outlier measures for each data-point to identify outliers in the data. We test our implementation on various publicly available data sets, and then apply it to mutual fund data. We show that there is a strong relationship between the outlier measures of the funds and their future returns and discuss the implications of our findings.
</details>
<details>
<summary>摘要</summary>
资金基金分类已成为投资管理行业的标准工具，广泛用于配置股票和选择基金管理人，以及基金管理人对准竞对手的分析和竞争位置。因此，任何不当或精度不够的分类可能会对分配决策产生重大影响，并且对投资基金的管理人也有重要影响。在这种情况下，我们想要量化基金分类错误的影响，并使用机器学习的方法来解决这个问题。我们将基金分类问题定义为一个距离度量学习问题，其中异常值是与其他数据点之间的距离最大的数据点。我们使用随机森林（RF）方法来学习距离度量，并计算每个数据点的类别异常度量来确定异常值。我们在各种公开available的数据集上进行测试，然后应用于基金数据。我们发现基金的异常度量和未来回报之间存在强相关性，并讨论了这些发现的意义。
</details></li>
</ul>
<hr>
<h2 id="AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation"><a href="#AutoSeqRec-Autoencoder-for-Efficient-Sequential-Recommendation" class="headerlink" title="AutoSeqRec: Autoencoder for Efficient Sequential Recommendation"></a>AutoSeqRec: Autoencoder for Efficient Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06878">http://arxiv.org/abs/2308.06878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sliu675/autoseqrec">https://github.com/sliu675/autoseqrec</a></li>
<li>paper_authors: Sijia Liu, Jiahao Liu, Hansu Gu, Dongsheng Li, Tun Lu, Peng Zhang, Ning Gu</li>
<li>for: 这篇论文旨在提出一种适合进行sequential recommendation tasks的增量推荐模型，即AutoSeqRec。</li>
<li>methods: AutoSeqRec 使用 autoencoder 架构，包括一个Encoder和三个Decoder。这些 комponents 考虑了用户-项目互动矩阵和项目转移矩阵的rows和columns。重建用户-项目互动矩阵可以捕捉用户长期偏好，而项目转移矩阵的rows和columns可以表示用户短期的兴趣。</li>
<li>results: 该论文的实验结果显示，AutoSeqRec 在精度方面比较高，并且具有优秀的可靠性和效率。<details>
<summary>Abstract</summary>
Sequential recommendation demonstrates the capability to recommend items by modeling the sequential behavior of users. Traditional methods typically treat users as sequences of items, overlooking the collaborative relationships among them. Graph-based methods incorporate collaborative information by utilizing the user-item interaction graph. However, these methods sometimes face challenges in terms of time complexity and computational efficiency. To address these limitations, this paper presents AutoSeqRec, an incremental recommendation model specifically designed for sequential recommendation tasks. AutoSeqRec is based on autoencoders and consists of an encoder and three decoders within the autoencoder architecture. These components consider both the user-item interaction matrix and the rows and columns of the item transition matrix. The reconstruction of the user-item interaction matrix captures user long-term preferences through collaborative filtering. In addition, the rows and columns of the item transition matrix represent the item out-degree and in-degree hopping behavior, which allows for modeling the user's short-term interests. When making incremental recommendations, only the input matrices need to be updated, without the need to update parameters, which makes AutoSeqRec very efficient. Comprehensive evaluations demonstrate that AutoSeqRec outperforms existing methods in terms of accuracy, while showcasing its robustness and efficiency.
</details>
<details>
<summary>摘要</summary>
sequential recommendation 示示了推荐ITEM的能力，通过模型用户的顺序行为。传统方法通常将用户视为ITEM的序列，忽略了用户之间的协作关系。基于图的方法可以利用用户-ITEM交互图，并 integrable 用户之间的协作信息。然而，这些方法有时会面临时间复杂度和计算效率的限制。为了解决这些限制，本文提出了AutoSeqRec，一种适用于顺序推荐任务的递增推荐模型。AutoSeqRec基于自适应器，包括一个Encoder和三个解码器在自适应器架构中。这些组件考虑了用户-ITEM交互矩阵和用户-ITEM交互矩阵的行列。重建用户-ITEM交互矩阵可以捕捉用户长期的偏好，通过共同筛选。此外，用户-ITEM交互矩阵的行列表示ITEM的出度和入度跳跃行为，可以模型用户短期的兴趣。在进行递增推荐时，只需更新输入矩阵，无需更新参数，这使得AutoSeqRec非常有效率。 comprehensive evaluations 表明AutoSeqRec在准确性方面高于现有方法，同时展现出了其稳定性和效率。
</details></li>
</ul>
<hr>
<h2 id="SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer"><a href="#SpeechX-Neural-Codec-Language-Model-as-a-Versatile-Speech-Transformer" class="headerlink" title="SpeechX: Neural Codec Language Model as a Versatile Speech Transformer"></a>SpeechX: Neural Codec Language Model as a Versatile Speech Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06873">http://arxiv.org/abs/2308.06873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka</li>
<li>for: 这篇论文旨在探讨一种能够实现多种语音转文字和语音处理任务的新型语音生成模型。</li>
<li>methods: 该模型使用了语音-文本提示的 Audio-Text 模型，并使用多任务学习和任务取向提示来实现一体化和可扩展的模型。</li>
<li>results: 实验结果表明，该模型在多种任务中表现出色，包括零shot TTS、噪声减少、目标说话人抽取、语音除去和背景噪声下的语音编辑等，并在不同任务中与专门的模型进行比较，表现相对或超过专门的模型。<details>
<summary>Abstract</summary>
Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.
</details>
<details>
<summary>摘要</summary>
近期的生成演说模型，基于音频文本提示，已经实现了高质量的零处理文本识别。然而，现有的模型仍然面临着处理多样化的音频文本演说生成任务的限制，包括转换输入speech和处理陌生频谱条件下的音频捕获。本文介绍SpeechX，一种多功能的演说生成模型，能够零处理TTS和多种演说转换任务，处理干净和噪音信号。SpeechX结合神经编码语言模型和多任务学习，使用任务висимы的提示，实现了一个简单、扩展的模型，并提供了一个通用的方法，用于挖掘文本输入在演说增强和转换任务中的作用。实验结果表明SpeechX在不同任务中具有优秀的表现，包括零处理TTS、噪音抑制、目标说话人EXTRACTION、演说除去和演说编辑等，与专门的模型相比，在任务中表现相似或更好。可以查看https://aka.ms/speechx的示例。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition"><a href="#Semi-Supervised-Dual-Stream-Self-Attentive-Adversarial-Graph-Contrastive-Learning-for-Cross-Subject-EEG-based-Emotion-Recognition" class="headerlink" title="Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition"></a>Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11635">http://arxiv.org/abs/2308.11635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weishan Ye, Zhiguo Zhang, Min Zhang, Fei Teng, Li Zhang, Linling Li, Gan Huang, Jianhong Wang, Dong Ni, Zhen Liang<br>for: 这篇论文是为了解决识别情绪的困难问题，具体来说是使用EEG数据进行情绪识别。methods: 该论文提出了一种半监督的双流自注意力对抗图像对比学习框架（简称DS-AGC），该框架包括两个平行的流程，一个是提取非结构的EEG特征，另一个是提取结构的EEG特征。results: 该论文的实验结果表明，在两个标准数据库（SEED和SEED-IV）上，提出的模型在不同的受测人数据下的 incomplete label 条件下表现出色，比如平均提高5.83%和6.99%。这表明该模型有效地解决了识别情绪的标签稀缺问题。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments conducted on two benchmark databases (SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out cross-validation evaluation scheme show that the proposed model outperforms existing methods under different incomplete label conditions (with an average improvement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition.
</details>
<details>
<summary>摘要</summary>
电enzephalography (EEG) 是一种客观的表征工具，具有推荐的应用前景。然而，数据标注的缺乏仍然是这个领域的主要挑战，这限制了EEG基于情感认知的广泛应用。在这篇论文中，一种半supervised dual-stream Self-Attentive Adversarial Graph Contrastive learning框架（简称DS-AGC）被提出，以解决跨个体EEG基于情感认知的数据标注稀缺的问题。DS-AGC框架包括两个平行流，用于提取非结构和结构EEG特征。非结构流利用半supervised多元适应方法，以减轻来源领域、无标注领域和目标领域之间的分布差异。结构流发展了图像异常学学习方法，以从多个EEG通道中提取有效的图像特征表示。此外，一个自注意力融合模块被开发，用于特征融合、样本选择和情感认知，强调EEG特征更加关注情感和数据样本在标注领域中的更加相似性。经过对两个参考数据库（SEED和SEED-IV）的 semi-supervised cross-subject leave-one-subject-out cross-validation评估，提出的模型在不同的未完全标注条件下（SEED上提高了5.83%，SEED-IV上提高了6.99%）表现出色，表明它有效地解决了跨个体EEG基于情感认知的数据标注稀缺问题。
</details></li>
</ul>
<hr>
<h2 id="Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks"><a href="#Effect-of-Choosing-Loss-Function-when-Using-T-batching-for-Representation-Learning-on-Dynamic-Networks" class="headerlink" title="Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks"></a>Effect of Choosing Loss Function when Using T-batching for Representation Learning on Dynamic Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06862">http://arxiv.org/abs/2308.06862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/erfanloghmani/effect-of-loss-function-tbatching">https://github.com/erfanloghmani/effect-of-loss-function-tbatching</a></li>
<li>paper_authors: Erfan Loghmani, MohammadAmin Fazli</li>
<li>for: 这个论文旨在提出新的方法来实现动态network上的表征学习，并且利用时间信息来改善模型的精度和效率。</li>
<li>methods: 本论文使用了T-batching技术来训练动态网络模型，并提出了两种新的损失函数来解决训练损失的问题。</li>
<li>results: 实验结果显示，使用提案的两种损失函数可以超越原始的损失函数，实现更好的训练性能，并且在实际的动态网络上显示了更高的精度和效率。<details>
<summary>Abstract</summary>
Representation learning methods have revolutionized machine learning on networks by converting discrete network structures into continuous domains. However, dynamic networks that evolve over time pose new challenges. To address this, dynamic representation learning methods have gained attention, offering benefits like reduced learning time and improved accuracy by utilizing temporal information.   T-batching is a valuable technique for training dynamic network models that reduces training time while preserving vital conditions for accurate modeling. However, we have identified a limitation in the training loss function used with t-batching. Through mathematical analysis, we propose two alternative loss functions that overcome these issues, resulting in enhanced training performance.   We extensively evaluate the proposed loss functions on synthetic and real-world dynamic networks. The results consistently demonstrate superior performance compared to the original loss function. Notably, in a real-world network characterized by diverse user interaction histories, the proposed loss functions achieved more than 26.9% enhancement in Mean Reciprocal Rank (MRR) and more than 11.8% improvement in Recall@10. These findings underscore the efficacy of the proposed loss functions in dynamic network modeling.
</details>
<details>
<summary>摘要</summary>
“现代学习方法已经革命化机器学习领域中的网络结构，将离散网络结构转化为连续领域。然而，在时间演变的网络上 pose 新的挑战。为 Addressing 这些挑战，动态表示学习方法受到了关注，它们可以利用时间信息来提高模型的准确性和速度。 T-batching 是训练动态网络模型的有价值技术，它可以降低训练时间的同时保持模型的准确性。然而，我们发现了 t-batching 的训练损失函数中的一个限制。通过数学分析，我们提出了两种替代的损失函数，它们可以解决这些问题，从而提高训练性能。我们对 synthetic 和实际的动态网络进行了广泛的评估，结果 consistently 表明了我们提出的损失函数的超越性。特别是在一个实际网络中，其中用户交互历史多样化，我们的提议损失函数可以提高 Mean Reciprocal Rank (MRR) 的值超过 26.9%，并提高 Recall@10 的值超过 11.8%。这些发现讲述了我们提出的损失函数在动态网络模型中的效果。”
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning"><a href="#Optimizing-Offensive-Gameplan-in-the-National-Basketball-Association-with-Machine-Learning" class="headerlink" title="Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning"></a>Optimizing Offensive Gameplan in the National Basketball Association with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06851">http://arxiv.org/abs/2308.06851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eamon Mukhopadhyay</li>
<li>for: 这篇论文是为了检验篮球metric的有效性而写的。</li>
<li>methods: 这篇论文使用了机器学习技术来模型已有的metric，并选择了一组特有的特征来评估metric的效果。</li>
<li>results: 该论文发现ORTG指数（由Dean Oliver提出）与不同的NBA战术类型有 statistically significant的相关性，但是使用神经网络回归模型而不是线性回归模型表现更好。<details>
<summary>Abstract</summary>
Throughout the analytical revolution that has occurred in the NBA, the development of specific metrics and formulas has given teams, coaches, and players a new way to see the game. However - the question arises - how can we verify any metrics? One method would simply be eyeball approximation (trying out many different gameplans) and/or trial and error - an estimation-based and costly approach. Another approach is to try to model already existing metrics with a unique set of features using machine learning techniques. The key to this approach is that with these features that are selected, we can try to gauge the effectiveness of these features combined, rather than using individual analysis in simple metric evaluation. If we have an accurate model, it can particularly help us determine the specifics of gameplan execution. In this paper, the statistic ORTG (Offensive Rating, developed by Dean Oliver) was found to have a correlation with different NBA playtypes using both a linear regression model and a neural network regression model, although ultimately, a neural network worked slightly better than linear regression. Using the accuracy of the models as a justification, the next step was to optimize the output of the model with test examples, which would demonstrate the combination of features to best achieve a highly functioning offense.
</details>
<details>
<summary>摘要</summary>
在NBA analytics革命中，发展特定指标和公式为球队、教练和球员提供了一种新的视角。然而，问题出现：如何验证这些指标呢？一种方法是通过试错和错误的方式来估算，这是一种估算基于估计的和昂贵的方法。另一种方法是使用机器学习技术来模型已有的指标，并选择一组独特的特征来评估这些指标的效果。如果我们有一个准确的模型，那么它可以帮助我们确定游戏计划执行的 especifics。在这篇论文中，由Dean Oliver开发的ORTG指标（进攻评估指标）与不同的NBA游戏类型之间显示了相关性，使用线性回归模型和神经网络回归模型，其中神经网络模型在精度上略微高一些。使用模型的准确性作为正当化，接下来的步骤是使用测试例子来优化模型的输出，以示出合理的游戏计划执行。
</details></li>
</ul>
<hr>
<h2 id="When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA"><a href="#When-Monte-Carlo-Dropout-Meets-Multi-Exit-Optimizing-Bayesian-Neural-Networks-on-FPGA" class="headerlink" title="When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA"></a>When Monte-Carlo Dropout Meets Multi-Exit: Optimizing Bayesian Neural Networks on FPGA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06849">http://arxiv.org/abs/2308.06849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/os-hxfan/bayesnn_fpga">https://github.com/os-hxfan/bayesnn_fpga</a></li>
<li>paper_authors: Hongxiang Fan, Hao Chen, Liam Castelli, Zhiqiang Que, He Li, Kenneth Long, Wayne Luk</li>
<li>for: 提高 Bayesian Neural Networks（BayesNNs）的实际应用，因为它们的算法复杂性和硬件性能妨碍了它们的应用。</li>
<li>methods: 该文提出了一种基于 Monte-Carlo Dropout（MCD）的多出口 BayesNN，可以实现准确预测，同时具有低算法复杂性。</li>
<li>results: 我们的自动生成的加速器在能效率方面高于 CPU、GPU 和其他现有硬件实现。<details>
<summary>Abstract</summary>
Bayesian Neural Networks (BayesNNs) have demonstrated their capability of providing calibrated prediction for safety-critical applications such as medical imaging and autonomous driving. However, the high algorithmic complexity and the poor hardware performance of BayesNNs hinder their deployment in real-life applications. To bridge this gap, this paper proposes a novel multi-exit Monte-Carlo Dropout (MCD)-based BayesNN that achieves well-calibrated predictions with low algorithmic complexity. To further reduce the barrier to adopting BayesNNs, we propose a transformation framework that can generate FPGA-based accelerators for multi-exit MCD-based BayesNNs. Several novel optimization techniques are introduced to improve hardware performance. Our experiments demonstrate that our auto-generated accelerator achieves higher energy efficiency than CPU, GPU, and other state-of-the-art hardware implementations.
</details>
<details>
<summary>摘要</summary>
bayesian neural networks (bayesNNs) 有能力提供准确的预测，用于安全关键应用程序，如医疗影像和自动驾驶。然而，bayesNNs 的算法复杂度和硬件性能妨碍其在实际应用中部署。为 bridge 这个差距，这篇文章提议一种新的多出口 Monte Carlo Dropout (MCD) 基于的 bayesNN，可以实现准确的预测，同时具有低的算法复杂度。此外，我们还提出了一种转换框架，可以生成 FPGA 加速器，用于multi-exit MCD 基于的 bayesNNs。我们还引入了一些新的优化技术，以提高硬件性能。我们的实验示例，我们自动生成的加速器在能耗效率方面高于 CPU、GPU 和其他现有硬件实现。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Topological-Graph-Neural-Networks-with-Paths"><a href="#Generalizing-Topological-Graph-Neural-Networks-with-Paths" class="headerlink" title="Generalizing Topological Graph Neural Networks with Paths"></a>Generalizing Topological Graph Neural Networks with Paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06838">http://arxiv.org/abs/2308.06838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Truong, Peter Chin</li>
<li>for: 该论文旨在探讨图 neural network (GNN) 在多种领域中的发展，但它们受到一种理论限制，即1-Weisfeiler-Lehmann测试。</li>
<li>methods: 该论文提出了一种以路径为中心的方法，强调图中的路径结构。该方法可以建立更一般的topological视角，并与其他已知的topological领域之间建立联系。</li>
<li>results: 该论文通过对多个benchmark进行测试，发现该方法可以超越之前的技术，在不假设图的子结构的前提下，达到状态艺术性的性能。<details>
<summary>Abstract</summary>
While Graph Neural Networks (GNNs) have made significant strides in diverse areas, they are hindered by a theoretical constraint known as the 1-Weisfeiler-Lehmann test. Even though latest advancements in higher-order GNNs can overcome this boundary, they typically center around certain graph components like cliques or cycles. However, our investigation goes a different route. We put emphasis on paths, which are inherent in every graph. We are able to construct a more general topological perspective and form a bridge to certain established theories about other topological domains. Interestingly, without any assumptions on graph sub-structures, our approach surpasses earlier techniques in this field, achieving state-of-the-art performance on several benchmarks.
</details>
<details>
<summary>摘要</summary>
While 图解网络（GNNs）在多个领域取得了 significiant progress, 它们受到一种理论限制，称为一个Weisfeiler-Lehmann测试。 latest advancements in higher-order GNNs可以突破这个boundary，但它们通常围绕某些图Component like cliques or cycles进行设计。 然而，我们的研究采取了不同的方向。 我们强调了路径，这是所有图的内在特征。 我们可以构建一个更通用的topological perspective，并与其他已知的topological domains建立联系。  Interestingly，无需任何图子结构的假设，我们的方法超越了之前在这个领域的技术，在多个标准测试上达到了state-of-the-art性能。
</details></li>
</ul>
<hr>
<h2 id="InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models"><a href="#InTune-Reinforcement-Learning-based-Data-Pipeline-Optimization-for-Deep-Recommendation-Models" class="headerlink" title="InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models"></a>InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08500">http://arxiv.org/abs/2308.08500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Lingyi Liu, Pablo Delgado, Prasanna Padmanabhan</li>
<li>for: 这 paper 的目的是研究深度学习基于推荐模型（DLRM）的训练方法，以及如何优化这些方法以提高效率和可扩展性。</li>
<li>methods: 这 paper 使用了人工智能的强化学习（RL）算法，以学习训练机器的 CPU 资源分配策略，以提高数据加载并行并发行为。</li>
<li>results: 这 paper 的实验结果表明，使用 InTune 可以在只需几分钟之内构建优化的数据管道配置，并且可以轻松地与现有训练工作流Integrate into existing workflows。 InTune 可以提高在线数据加载速率，从而降低模型执行时间的浪费和提高效率。<details>
<summary>Abstract</summary>
Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.   In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization to adopt. Our studies lead us to design and build a new solution for data pipeline optimization, InTune.   InTune employs a reinforcement learning (RL) agent to learn how to distribute the CPU resources of a trainer machine across a DLRM data pipeline to more effectively parallelize data loading and improve throughput. Our experiments show that InTune can build an optimized data pipeline configuration within only a few minutes, and can easily be integrated into existing training workflows. By exploiting the responsiveness and adaptability of RL, InTune achieves higher online data ingestion rates than existing optimizers, thus reducing idle times in model execution and increasing efficiency. We apply InTune to our real-world cluster, and find that it increases data ingestion throughput by as much as 2.29X versus state-of-the-art data pipeline optimizers while also improving both CPU & GPU utilization.
</details>
<details>
<summary>摘要</summary>
InTune 使用了强化学习（RL）代理来学习如何在 DLRM 数据管道中分配训练机器的 CPU 资源，以更有效地并行数据加载并提高吞吐量。我们的实验表明，InTune 可以在只需几分钟之内构建优化数据管道配置，并且可以轻松地与现有训练工作流 integrate。通过RL的响应和适应性，InTune 可以在现有优化器的基础上提高在线数据接收速率，从而降低模型执行时间的浪费和提高效率。我们对实际集群进行应用，发现 InTune 可以提高数据接收吞吐量达到 2.29 倍，同时提高 CPU 和 GPU 资源利用率。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM"><a href="#An-Ensemble-Approach-to-Question-Classification-Integrating-Electra-Transformer-GloVe-and-LSTM" class="headerlink" title="An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM"></a>An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06828">http://arxiv.org/abs/2308.06828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham</li>
<li>for: 本研究旨在提出一种新的集成方法，用于问题分类，利用现代模型——Electra、GloVe和LSTM。</li>
<li>methods: 该模型使用了Electra、GloVe和LSTM三种现代模型，通过集成这些模型的优势，提供了一种robust和高效的问题分类解决方案。</li>
<li>results: 对于TREC数据集，提出的集成模型在所有评估指标上都超过了BERT、RoBERTa和DistilBERT等其他现代模型，实现了0.8的测试集准确率。这些结果表明集成方法在问题分类任务中具有显著的优势，并且鼓励进一步探索集成方法在自然语言处理领域中的应用。<details>
<summary>Abstract</summary>
This paper introduces a novel ensemble approach for question classification using state-of-the-art models -- Electra, GloVe, and LSTM. The proposed model is trained and evaluated on the TREC dataset, a well-established benchmark for question classification tasks. The ensemble model combines the strengths of Electra, a transformer-based model for language understanding, GloVe, a global vectors for word representation, and LSTM, a recurrent neural network variant, providing a robust and efficient solution for question classification. Extensive experiments were carried out to compare the performance of the proposed ensemble approach with other cutting-edge models, such as BERT, RoBERTa, and DistilBERT. Our results demonstrate that the ensemble model outperforms these models across all evaluation metrics, achieving an accuracy of 0.8 on the test set. These findings underscore the effectiveness of the ensemble approach in enhancing the performance of question classification tasks, and invite further exploration of ensemble methods in natural language processing.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的集成方法 для问题分类，使用当今最佳模型——Electra、GloVe和LSTM。该模型在TREC数据集上进行训练和评估，TREC数据集是问题分类任务的常见 benchmarck。集成模型结合了Electra、GloVe和LSTM的优势，提供了一个可靠和高效的问题分类解决方案。我们进行了广泛的实验，与其他最新的模型，如BERT、RoBERTa和DistilBERT进行比较。我们的结果表明，集成模型在所有评估指标上都超过了这些模型，在测试集上达到了0.8的准确率。这些结果证明了集成方法在问题分类任务中的效iveness，并邀请了进一步的对natural language processing领域中的集成方法进行探索。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number"><a href="#Reinforcement-Graph-Clustering-with-Unknown-Cluster-Number" class="headerlink" title="Reinforcement Graph Clustering with Unknown Cluster Number"></a>Reinforcement Graph Clustering with Unknown Cluster Number</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06827">http://arxiv.org/abs/2308.06827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueliu1999/awesome-deep-graph-clustering">https://github.com/yueliu1999/awesome-deep-graph-clustering</a></li>
<li>paper_authors: Yue Liu, Ke Liang, Jun Xia, Xihong Yang, Sihang Zhou, Meng Liu, Xinwang Liu, Stan Z. Li<br>for: 这个研究旨在提供一个不需要先知cluster number的深度图 clustering方法，并且与对图的不确定性进行适应。methods: 我们提出了一个名为Reinforcement Graph Clustering（RGC）的新方法，它通过强化学习机制让cluster number决定和无监督表现学习融合到一个整体框架中。在我们的方法中，首先learn出具有对称预测任务的描述性node表现，然后考虑 both node和cluster状态，以获得更加准确的图 clustering结果。results: 我们的方法在实验中表现出了优异的效能和效率，并且能够在对图中实现更好的适应性和稳定性。此外，我们还提供了一个包含多种深度图 clustering方法的 коллекции（paper、code和dataset），可以帮助研究人员更好地进行深度图 clustering的研究。<details>
<summary>Abstract</summary>
Deep graph clustering, which aims to group nodes into disjoint clusters by neural networks in an unsupervised manner, has attracted great attention in recent years. Although the performance has been largely improved, the excellent performance of the existing methods heavily relies on an accurately predefined cluster number, which is not always available in the real-world scenario. To enable the deep graph clustering algorithms to work without the guidance of the predefined cluster number, we propose a new deep graph clustering method termed Reinforcement Graph Clustering (RGC). In our proposed method, cluster number determination and unsupervised representation learning are unified into a uniform framework by the reinforcement learning mechanism. Concretely, the discriminative node representations are first learned with the contrastive pretext task. Then, to capture the clustering state accurately with both local and global information in the graph, both node and cluster states are considered. Subsequently, at each state, the qualities of different cluster numbers are evaluated by the quality network, and the greedy action is executed to determine the cluster number. In order to conduct feedback actions, the clustering-oriented reward function is proposed to enhance the cohesion of the same clusters and separate the different clusters. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. The source code of RGC is shared at https://github.com/yueliu1999/RGC and a collection (papers, codes and, datasets) of deep graph clustering is shared at https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering on Github.
</details>
<details>
<summary>摘要</summary>
深度图 clustering，目标是通过神经网络在无监督情况下将节点分组到不同的分支，在过去几年内吸引了广泛的关注。although 现有的方法已经大幅提高了性能，但是它们依赖于准确预定的分支数量，这在实际场景中并不总是可用。为了使深度图 clustering 算法不受预定分支数量的限制，我们提出了一种新的深度图 clustering 方法，称为奖励图 clustering（RGC）。在我们的提议方法中，集群数量决定和无监督表示学习被统一到一个奖励学习机制中。具体来说，首先通过对比预测任务学习描述性的节点表示。然后，为了准确地捕捉图中的集群状态，包括节点状态和集群状态。在每个状态下，通过质量网络评估不同的分支数量的质量，并执行滥购行动来确定分支数量。为了进行反馈行动，我们提出了一种集群 oriented 奖励函数，以增强同一个集群之间的凝结度和不同集群之间的分离度。我们的实验证明了我们的提议方法的效果和效率。RGC 的源代码可以在 GitHub 上获取：https://github.com/yueliu1999/RGC，而深度图 clustering 相关的代码、论文和数据集可以在 GitHub 上获取：https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering。
</details></li>
</ul>
<hr>
<h2 id="Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning"><a href="#Approximate-and-Weighted-Data-Reconstruction-Attack-in-Federated-Learning" class="headerlink" title="Approximate and Weighted Data Reconstruction Attack in Federated Learning"></a>Approximate and Weighted Data Reconstruction Attack in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06822">http://arxiv.org/abs/2308.06822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Wang, Yongcun Song, Enrique Zuazua</li>
<li>for: 本研究旨在攻击 Federated Learning（FL）中 horizontal Federated Averaging（FedAvg）场景中客户端的模型参数共享。</li>
<li>methods: 我们提出了一种 interpolation-based approximation 方法，可以使 fedavg 场景中客户端的模型参数攻击成为可能。此外，我们还设计了一种层Weighted loss function，可以提高数据重建质量。</li>
<li>results: 我们的 approximate and weighted attack（AWA）方法在不同评价指标中均表现出优于现有方法，特别是在图像数据重建中。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed learning paradigm that enables multiple clients to collaborate on building a machine learning model without sharing their private data. Although FL is considered privacy-preserved by design, recent data reconstruction attacks demonstrate that an attacker can recover clients' training data based on the parameters shared in FL. However, most existing methods fail to attack the most widely used horizontal Federated Averaging (FedAvg) scenario, where clients share model parameters after multiple local training steps. To tackle this issue, we propose an interpolation-based approximation method, which makes attacking FedAvg scenarios feasible by generating the intermediate model updates of the clients' local training processes. Then, we design a layer-wise weighted loss function to improve the data quality of reconstruction. We assign different weights to model updates in different layers concerning the neural network structure, with the weights tuned by Bayesian optimization. Finally, experimental results validate the superiority of our proposed approximate and weighted attack (AWA) method over the other state-of-the-art methods, as demonstrated by the substantial improvement in different evaluation metrics for image data reconstructions.
</details>
<details>
<summary>摘要</summary>
Федератированное обучение (FL) 是一种分布式学习 paradigma，允许多个客户端共同建立一个机器学习模型，无需共享其私人数据。虽然 FL 被视为隐私保护的设计，但最近的数据重建攻击表明，攻击者可以根据在 FL 中共享的参数重建客户端的训练数据。然而，现有方法大多不能攻击最常用的水平 Federated Averaging（FedAvg）场景，在这种场景下，客户端在多个本地训练步骤后共享模型参数。为解决这个问题，我们提出了一种 interpolating-based 方法，可以在 FedAvg 场景中生成客户端的本地训练过程中的中间模型更新。然后，我们设计了层wise 权重损失函数，以提高重建数据的质量。我们对模型更新在不同层中分配不同权重，并通过 Bayesian 优化调整这些权重。最后，我们对 AWA 方法进行实验 validate，并证明其在不同评价指标上具有明显的提高。
</details></li>
</ul>
<hr>
<h2 id="SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection"><a href="#SoK-Realistic-Adversarial-Attacks-and-Defenses-for-Intelligent-Network-Intrusion-Detection" class="headerlink" title="SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection"></a>SoK: Realistic Adversarial Attacks and Defenses for Intelligent Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06819">http://arxiv.org/abs/2308.06819</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Vitorino, Isabel Praça, Eva Maia</li>
<li>For: The paper is written to provide a comprehensive overview of the state-of-the-art adversarial learning approaches for realistic example generation in the context of Network Intrusion Detection (NID) using machine learning (ML) models.* Methods: The paper consolidates and summarizes various adversarial attack methods and defense strategies, specifically tailored for the NID domain and realistic network traffic flows.* Results: The paper identifies open challenges and fundamental properties required for realistic adversarial examples in NID, providing guidelines for future research to ensure adequacy for real communication networks.Here’s the same information in Simplified Chinese text:* For: 这篇论文是为了提供网络入侵检测（NID）领域中机器学习（ML）模型的现状摘要，包括最新的敌对学习方法和防御策略。* Methods: 论文总结了各种敌对攻击方法和防御策略，特别适用于NID领域和真实的网络流量。* Results: 论文描述了NID领域中敌对学习模型的开放挑战和基本要求，并提供了未来研究的指导方针，以确保实际网络通信的合理性。<details>
<summary>Abstract</summary>
Machine Learning (ML) can be incredibly valuable to automate anomaly detection and cyber-attack classification, improving the way that Network Intrusion Detection (NID) is performed. However, despite the benefits of ML models, they are highly susceptible to adversarial cyber-attack examples specifically crafted to exploit them. A wide range of adversarial attacks have been created and researchers have worked on various defense strategies to safeguard ML models, but most were not intended for the specific constraints of a communication network and its communication protocols, so they may lead to unrealistic examples in the NID domain. This Systematization of Knowledge (SoK) consolidates and summarizes the state-of-the-art adversarial learning approaches that can generate realistic examples and could be used in real ML development and deployment scenarios with real network traffic flows. This SoK also describes the open challenges regarding the use of adversarial ML in the NID domain, defines the fundamental properties that are required for an adversarial example to be realistic, and provides guidelines for researchers to ensure that their future experiments are adequate for a real communication network.
</details>
<details>
<summary>摘要</summary>
This Systematization of Knowledge (SoK) consolidates and summarizes the state-of-the-art adversarial learning approaches that can generate realistic examples and can be used in real ML development and deployment scenarios with real network traffic flows. This SoK also identifies the open challenges regarding the use of adversarial ML in the NID domain, defines the fundamental properties that are required for an adversarial example to be realistic, and provides guidelines for researchers to ensure that their future experiments are adequate for a real communication network.
</details></li>
</ul>
<hr>
<h2 id="SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning"><a href="#SAILOR-Structural-Augmentation-Based-Tail-Node-Representation-Learning" class="headerlink" title="SAILOR: Structural Augmentation Based Tail Node Representation Learning"></a>SAILOR: Structural Augmentation Based Tail Node Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06801">http://arxiv.org/abs/2308.06801</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jie-re/sailor">https://github.com/jie-re/sailor</a></li>
<li>paper_authors: Jie Liao, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, Zibin Zheng</li>
<li>for: 提高链接结构中tail节点的表示性</li>
<li>methods: 提出了一种基于 структур增强的tail节点表示学习框架，名为SAILOR</li>
<li>results: 对公共评估数据进行了广泛的实验，显示SAILOR可以显著提高tail节点的表示性，并超越当前的基准值<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved state-of-the-art performance in representation learning for graphs recently. However, the effectiveness of GNNs, which capitalize on the key operation of message propagation, highly depends on the quality of the topology structure. Most of the graphs in real-world scenarios follow a long-tailed distribution on their node degrees, that is, a vast majority of the nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes since they lack structural information. In the pursuit of promoting the expressiveness of GNNs for tail nodes, we explore how the deficiency of structural information deteriorates the performance of tail nodes and propose a general Structural Augmentation based taIL nOde Representation learning framework, dubbed as SAILOR, which can jointly learn to augment the graph structure and extract more informative representations for tail nodes. Extensive experiments on public benchmark datasets demonstrate that SAILOR can significantly improve the tail node representations and outperform the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 在最近的表示学习中达到了状态的极品性表现。然而，GNNS的效果，它们基于消息传递操作，强度取决于图结构的质量。大多数实际场景中的图follows a long-tailed distribution on node degrees, that is, most nodes in the graph are tail nodes with only a few connected edges. GNNs produce inferior node representations for tail nodes due to the lack of structural information. 为了提高GNNS的表达能力 для尾节点，我们研究了尾节点表示力下降的原因和提出了一种通用的结构扩充based taIL node representation learning框架，名为SAILOR，可以同时学习扩充图结构和提取更有用的尾节点表示。我们在公共 benchmark datasets上进行了广泛的实验，显示SAILOR可以显著提高尾节点表示和超越状态的基eline。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.LG_2023_08_14/" data-id="clly4xtdw006tvl889xrse1d2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/cs.SD_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/cs.SD_2023_08_14/">cs.SD - 2023-08-14 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations"><a href="#Integrating-Emotion-Recognition-with-Speech-Recognition-and-Speaker-Diarisation-for-Conversations" class="headerlink" title="Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations"></a>Integrating Emotion Recognition with Speech Recognition and Speaker Diarisation for Conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07145">http://arxiv.org/abs/2308.07145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/w-wu/steer">https://github.com/w-wu/steer</a></li>
<li>paper_authors: Wen Wu, Chao Zhang, Philip C. Woodland</li>
<li>for: 提高自动情感识别（AER）的精度和效果，使其能够在对话系统中应用。</li>
<li>methods:  integrate AER with automatic speech recognition（ASR）和speaker diarisation（SD），共同训练一个系统，并使用分布式编码器建立不同的输出层。</li>
<li>results: 在IEMOCAP dataset上进行测试，提议的系统与两个基准系统相比，在AER、ASR和SD三个任务中均表现出色，并且在时间权重 emotions 和 speaker classification 错误上采用了两种评价指标。<details>
<summary>Abstract</summary>
Although automatic emotion recognition (AER) has recently drawn significant research interest, most current AER studies use manually segmented utterances, which are usually unavailable for dialogue systems. This paper proposes integrating AER with automatic speech recognition (ASR) and speaker diarisation (SD) in a jointly-trained system. Distinct output layers are built for four sub-tasks including AER, ASR, voice activity detection and speaker classification based on a shared encoder. Taking the audio of a conversation as input, the integrated system finds all speech segments and transcribes the corresponding emotion classes, word sequences, and speaker identities. Two metrics are proposed to evaluate AER performance with automatic segmentation based on time-weighted emotion and speaker classification errors. Results on the IEMOCAP dataset show that the proposed system consistently outperforms two baselines with separately trained single-task systems on AER, ASR and SD.
</details>
<details>
<summary>摘要</summary>
尽管自动情感识别（AER）在最近几年内受到了广泛的研究兴趣，但大多数当前AER研究使用手动分割的语音，这些语音通常不可用于对话系统。这篇论文提议将AER、自动语音识别（ASR）和 speaker分类（SD）集成为一个集成系统。该系统使用共享Encoder生成了四个子任务的特征输出层，包括AER、ASR、语音活动检测和 speaker分类。将对话的音频作为输入，该集成系统可以找到所有的语音段落，并将对应的情感类别、词序列和Speaker标识转化为文本。为评估AER性能，提出了两种指标，即基于时间权重的情感错误和Speaker错误。results表明，提议的系统在IEMOCAP dataset上比基eline两个独立的单任务系统在AER、ASR和SD领域具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera"><a href="#VoxBlink-X-Large-Speaker-Verification-Dataset-on-Camera" class="headerlink" title="VoxBlink: X-Large Speaker Verification Dataset on Camera"></a>VoxBlink: X-Large Speaker Verification Dataset on Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07056">http://arxiv.org/abs/2308.07056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuke Lin, Xiaoyi Qin, Ming Cheng, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for: 本研究做出了一个新的和广泛的语音认可数据集，包括噪音38k个标识&#x2F;1.45M次语音（VoxBlink）和相对干净的18k个标识&#x2F;1.02M次语音（VoxBlink-Clean） для训练。</li>
<li>methods: 我们首先建立了一个自动化和可扩展的数据提取管道，从YouTube上下载了60,000个用户的短视频，并从这些视频中自动提取了相关的语音和视频段落。</li>
<li>results: 我们的实验结果表明，将VoxBlink-Clean数据集用于训练，可以提高语音认可性能，比如13%-30%的提升，不同的后向架构之间。这个数据集即将公开发布。<details>
<summary>Abstract</summary>
In this paper, we contribute a novel and extensive dataset for speaker verification, which contains noisy 38k identities/1.45M utterances (VoxBlink) and relatively cleaned 18k identities/1.02M (VoxBlink-Clean) utterances for training. Firstly, we accumulate a 60K+ users' list with their avatars and download their short videos on YouTube. We then established an automatic and scalable pipeline to extract relevant speech and video segments from these videos. To our knowledge, the VoxBlink dataset is one of the largest speaker recognition datasets available. Secondly, we conduct a series of experiments based on different backbones trained on a mix of the VoxCeleb2 and the VoxBlink-Clean. Our findings highlight a notable performance improvement, ranging from 13% to 30%, across different backbone architectures upon integrating our dataset for training. The dataset will be made publicly available shortly.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一个新的和广泛的说话人验证数据集，包括噪音38k个人/1.45万个语音（VoxBlink）和相对清晰的18k个人/1.02万个语音（VoxBlink-Clean） для训练。首先，我们积累了60,000个用户的名单和他们的aviator，然后下载了YouTube上的短视频。我们然后建立了一个自动化和可扩展的管道，以提取视频和语音段落。根据我们所知，VoxBlink数据集是目前最大的说话人识别数据集之一。其次，我们进行了基于不同的后准据体系的实验，发现在将我们的数据集用于训练时，其性能提升范围为13%到30%。这些数据将在不久的将来公开。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder"><a href="#Improving-Audio-Visual-Speech-Recognition-by-Lip-Subword-Correlation-Based-Visual-Pre-training-and-Cross-Modal-Fusion-Encoder" class="headerlink" title="Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder"></a>Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08488">http://arxiv.org/abs/2308.08488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mispchallenge/misp-icme-avsr">https://github.com/mispchallenge/misp-icme-avsr</a></li>
<li>paper_authors: Yusheng Dai, Hang Chen, Jun Du, Xiaofei Ding, Ning Ding, Feijun Jiang, Chin-Hui Lee</li>
<li>for: 本研究旨在提高自动语音识别系统的音视频联合识别系统（AVSR）性能，并在预训练和精度调整框架下实现这一目标。</li>
<li>methods: 本研究提出了两种新技术来提高AVSR的性能，包括利用叙述形态学生 lip shapes 和 syllable-level subword units 的相关性来确定准确的帧级句子界限，以及使用主要训练参数进行多个跨Modal的注意力层来充分利用多Modal的共轭性。</li>
<li>results: 实验结果表明，使用这两种技术可以提高AVSR系统的性能，并在MISP2021-AVSR数据集上达到比 estado-of-the-art 系统更高的性能水平，使用的训练数据量也相对较少。<details>
<summary>Abstract</summary>
In recent research, slight performance improvement is observed from automatic speech recognition systems to audio-visual speech recognition systems in the end-to-end framework with low-quality videos. Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem. In this paper, we propose two novel techniques to improve audio-visual speech recognition (AVSR) under a pre-training and fine-tuning training framework. First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes. This enables accurate alignment of video and audio streams during visual model pre-training and cross-modal fusion. Next, we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity. Experiments on the MISP2021-AVSR data set show the effectiveness of the two proposed techniques. Together, using only a relatively small amount of training data, the final system achieves better performances than state-of-the-art systems with more complex front-ends and back-ends.
</details>
<details>
<summary>摘要</summary>
近期研究发现，自动语音识别系统到Audio-Visual语音识别系统在端到端框架下有轻微的性能提升，但是存在不匹配的协调速率和特殊的输入表示之间的问题。在这篇论文中，我们提出了两种新的技巧来提高Audio-Visual语音识别（AVSR）在预训练和精度调整训练框架下。首先，我们探索了拼音和字节水平的叙述单元之间的相关性，以确定良好的帧级叙述边界。这使得视频和音频流之间的对齐变得精准，从而提高了视频和音频流之间的混合。其次，我们提出了一种受主要训练参数 guideline的Audio-Visual混合抽象Encoder（CMFE）神经网络，以便在多个跨模态扩散层中使用主要训练参数，以便充分利用多模态的共轭性。实验表明，使用这两种技巧可以提高系统的性能，并且只需使用相对较少的训练数据。最终系统可以在与更复杂的前端和后端的系统相比，达到更好的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Cinematic-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Cinematic Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06981">http://arxiv.org/abs/2308.06981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, Kai Li, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了2023年 зву隔离挑战（SDX’23）的电影幂分融合（CDX）轨迹。</li>
<li>methods: 论文详细介绍了比赛的结构和使用的数据集，特别是新构建的CDXDB23隐藏数据集，以及参与者所采用的最成功的方法。</li>
<li>results: 相比干杯餐 fork基线，专门在 simulated Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而开放排行榜上的最佳系统则看到了5.7dB的显著提升。<details>
<summary>Abstract</summary>
This paper summarizes the cinematic demixing (CDX) track of the Sound Demixing Challenge 2023 (SDX'23). We provide a comprehensive summary of the challenge setup, detailing the structure of the competition and the datasets used. Especially, we detail CDXDB23, a new hidden dataset constructed from real movies that was used to rank the submissions. The paper also offers insights into the most successful approaches employed by participants. Compared to the cocktail-fork baseline, the best-performing system trained exclusively on the simulated Divide and Remaster (DnR) dataset achieved an improvement of 1.8dB in SDR whereas the top performing system on the open leaderboard, where any data could be used for training, saw a significant improvement of 5.7dB.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了2023年 зву频分离挑战（SDX'23）的电影式分离（CDX）轨迹。我们提供了竞赛设置的完整摘要，包括竞赛结构和使用的数据集。特别是，我们详细介绍了CDXDB23，一个新的隐藏数据集，从真实电影中构建而成，用于评估参赛系统的表现。文章还提供了参与者采用的最成功方法的折衔。相比干杯叉基线，专门在 simulate 的 Divide and Remaster（DnR）数据集上训练的系统得到了1.8dB的SDR提升，而在开放排行榜上，任何数据可以用于训练的系统则得到了显著的5.7dB的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track"><a href="#The-Sound-Demixing-Challenge-2023-unicode-x2013-Music-Demixing-Track" class="headerlink" title="The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track"></a>The Sound Demixing Challenge 2023 $\unicode{x2013}$ Music Demixing Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06979">http://arxiv.org/abs/2308.06979</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zfturbo/mvsep-mdx23-music-separation-model">https://github.com/zfturbo/mvsep-mdx23-music-separation-model</a></li>
<li>paper_authors: Giorgio Fabbro, Stefan Uhlich, Chieh-Hsin Lai, Woosung Choi, Marco Martínez-Ramírez, Weihsiang Liao, Igor Gadelha, Geraldo Ramos, Eddie Hsu, Hugo Rodrigues, Fabian-Robert Stöter, Alexandre Défossez, Yi Luo, Jianwei Yu, Dipam Chakraborty, Sharada Mohanty, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Nabarun Goswami, Tatsuya Harada, Minseok Kim, Jun Hyung Lee, Yuanliang Dong, Xinran Zhang, Jiafeng Liu, Yuki Mitsufuji</li>
<li>for: 这篇论文描述了Sound Demixing Challenge（SDX’23）的音乐分离（MDX）轨迹。</li>
<li>methods: 论文介绍了MDX系统在训练数据中出现错误的情况下的训练方法，并提出了一种对MDX系统训练数据设计的错误形式化。</li>
<li>results: 论文描述了SDXDB23_LabelNoise和SDXDB23_Bleeding1两个新的数据集，以及在SDX’23中获得最高分的方法。此外，论文还对上一届音乐分离比赛（Music Demixing Challenge 2021）的赛果进行了直接比较，发现当用MDXDB21进行评估时，最佳实现在标准MSS形式下获得了1.6dB的信号至噪声比提高。此外，论文还进行了基于人类听觉评价的听测，并对系统的感知质量进行了报告。最后，论文提供了比赛组织方式的反思和未来版本的展望。<details>
<summary>Abstract</summary>
This paper summarizes the music demixing (MDX) track of the Sound Demixing Challenge (SDX'23). We provide a summary of the challenge setup and introduce the task of robust music source separation (MSS), i.e., training MSS models in the presence of errors in the training data. We propose a formalization of the errors that can occur in the design of a training dataset for MSS systems and introduce two new datasets that simulate such errors: SDXDB23_LabelNoise and SDXDB23_Bleeding1. We describe the methods that achieved the highest scores in the competition. Moreover, we present a direct comparison with the previous edition of the challenge (the Music Demixing Challenge 2021): the best performing system under the standard MSS formulation achieved an improvement of over 1.6dB in signal-to-distortion ratio over the winner of the previous competition, when evaluated on MDXDB21. Besides relying on the signal-to-distortion ratio as objective metric, we also performed a listening test with renowned producers/musicians to study the perceptual quality of the systems and report here the results. Finally, we provide our insights into the organization of the competition and our prospects for future editions.
</details>
<details>
<summary>摘要</summary>
In Simplified Chinese:这篇文章介绍了Sound Demixing Challenge（SDX'23）的音乐分离（MDX）轨迹，包括音乐来源分离（MSS）的Robust Training数据集的设计和两个新的数据集：SDXDB23_LabelNoise和SDXDB23_Bleeding1。文章介绍了在比赛中得分最高的方法，并对上一届音乐分离挑战（Music Demixing Challenge 2021）的赛果进行比较。结果显示，使用标准MSS形式化的最佳系统在MDXDB21上的信号至噪比高于上一届赛事的冠军的赛果。此外，文章还执行了由知名的制作人/音乐人组织的听力测试，以研究系统的主观质量。最后，文章提供了比赛组织和未来版本的前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/cs.SD_2023_08_14/" data-id="clly4xter00a2vl88gy4pdm7l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/14/eess.IV_2023_08_14/" class="article-date">
  <time datetime="2023-08-13T16:00:00.000Z" itemprop="datePublished">2023-08-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/14/eess.IV_2023_08_14/">eess.IV - 2023-08-14 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data"><a href="#Automated-Ensemble-Based-Segmentation-of-Adult-Brain-Tumors-A-Novel-Approach-Using-the-BraTS-AFRICA-Challenge-Data" class="headerlink" title="Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data"></a>Automated Ensemble-Based Segmentation of Adult Brain Tumors: A Novel Approach Using the BraTS AFRICA Challenge Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07214">http://arxiv.org/abs/2308.07214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiranjeewee Prasad Koirala, Sovesh Mohapatra, Advait Gosai, Gottfried Schlaug</li>
<li>for: 这篇论文旨在利用深度学习对多Modalities MRI数据进行脑肿瘤精准分割，以优化在 SUB-SAHARAN AFRICA 患者群体中的诊断和治疗。</li>
<li>methods: 这篇论文提出了一种ensemble方法，包括eleven个不同的变种，基于三种核心架构：UNet3D、ONet3D 和 SphereNet3D，以及修改的损失函数。</li>
<li>results: 研究发现， ensemble方法可以在多Modalities MRI数据上提高脑肿瘤分割精度，特别是在 age-和 population-based 分割模型方面。 Results表明， ensemble方法的 dice分数为 0.82、0.82 和 0.87 分别用于提高脑肿瘤、脑肿瘤核心和全脑肿瘤标签。<details>
<summary>Abstract</summary>
Brain tumors, particularly glioblastoma, continue to challenge medical diagnostics and treatments globally. This paper explores the application of deep learning to multi-modality magnetic resonance imaging (MRI) data for enhanced brain tumor segmentation precision in the Sub-Saharan Africa patient population. We introduce an ensemble method that comprises eleven unique variations based on three core architectures: UNet3D, ONet3D, SphereNet3D and modified loss functions. The study emphasizes the need for both age- and population-based segmentation models, to fully account for the complexities in the brain. Our findings reveal that the ensemble approach, combining different architectures, outperforms single models, leading to improved evaluation metrics. Specifically, the results exhibit Dice scores of 0.82, 0.82, and 0.87 for enhancing tumor, tumor core, and whole tumor labels respectively. These results underline the potential of tailored deep learning techniques in precisely segmenting brain tumors and lay groundwork for future work to fine-tune models and assess performance across different brain regions.
</details>
<details>
<summary>摘要</summary>
脑肿，特别是 glioblastoma，仍然在全球医疗领域面临挑战。这篇论文探讨了深度学习在多Modal magnetic resonance imaging（MRI）数据上进行脑肿分 segmentation的精度提高。我们引入了一个ensemble方法，包括11个独特的变种，基于三个核心体系：UNet3D、ONet3D和SphereNet3D，以及修改的损失函数。该研究强调了需要根据年龄和人口进行分 segmentation模型，以全面考虑脑肿的复杂性。我们的发现表明， ensemble方法，将不同的体系结合起来，表现出了提高评价指标的效果。特别是，结果显示 dice分数为0.82、0.82和0.87，用于加强肿体、肿体核心和整个肿体标签。这些结果高亮了深度学习技术在精度地分 segmentation脑肿的潜在优势，并为未来细化模型和评价不同脑区的表现提供了基础。
</details></li>
</ul>
<hr>
<h2 id="SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation"><a href="#SAM-Meets-Robotic-Surgery-An-Empirical-Study-on-Generalization-Robustness-and-Adaptation" class="headerlink" title="SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation"></a>SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07156">http://arxiv.org/abs/2308.07156</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Wang, Mobarakol Islam, Mengya Xu, Yang Zhang, Hongliang Ren</li>
<li>for: 本研究探讨了Segment Anything Model（SAM）在 роботиче外科中的 robustness和零shot泛化能力。</li>
<li>methods: 本研究使用了SAM模型，并对其进行了多种场景探讨，包括提示和无提示的情况，以及不同的提示方法。</li>
<li>results: 研究发现，SAM模型在提示情况下表现出了很好的零shot泛化能力，但在无提示情况下或者 Instrument部分重叠时，模型很难正确地分类Instrument。此外，模型在复杂的外科手术场景下也表现不佳，尤其是在血液、反射、模糊和阴影等情况下。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) serves as a fundamental model for semantic segmentation and demonstrates remarkable generalization capabilities across a wide range of downstream scenarios. In this empirical study, we examine SAM's robustness and zero-shot generalizability in the field of robotic surgery. We comprehensively explore different scenarios, including prompted and unprompted situations, bounding box and points-based prompt approaches, as well as the ability to generalize under corruptions and perturbations at five severity levels. Additionally, we compare the performance of SAM with state-of-the-art supervised models. We conduct all the experiments with two well-known robotic instrument segmentation datasets from MICCAI EndoVis 2017 and 2018 challenges. Our extensive evaluation results reveal that although SAM shows remarkable zero-shot generalization ability with bounding box prompts, it struggles to segment the whole instrument with point-based prompts and unprompted settings. Furthermore, our qualitative figures demonstrate that the model either failed to predict certain parts of the instrument mask (e.g., jaws, wrist) or predicted parts of the instrument as wrong classes in the scenario of overlapping instruments within the same bounding box or with the point-based prompt. In fact, SAM struggles to identify instruments in complex surgical scenarios characterized by the presence of blood, reflection, blur, and shade. Additionally, SAM is insufficiently robust to maintain high performance when subjected to various forms of data corruption. We also attempt to fine-tune SAM using Low-rank Adaptation (LoRA) and propose SurgicalSAM, which shows the capability in class-wise mask prediction without prompt. Therefore, we can argue that, without further domain-specific fine-tuning, SAM is not ready for downstream surgical tasks.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 是一种基本模型 для semantics segmentation，它在各种下游场景中表现出了很好的普适性。在这个实验性研究中，我们研究了SAM在 робо学手术场景中的 robustness和零shot普适性。我们全面探讨了不同的场景，包括提示和无提示的情况，以及 bounding box 和点based提示方法。此外，我们还评估了 SAM 与当前顶尖指导学习模型的性能比较。我们在 MICCAI EndoVis 2017 和 2018 挑战中获得的两个 robotic instrument segmentation 数据集进行了所有的实验。我们的广泛的评估结果表明，虽然 SAM 在 bounding box 提示下显示出了remarkable零shot普适性，但在点based提示和无提示情况下，它很难正确地分类整个工具。此外，我们的资深图示表明，当工具在同一个 bounding box 内或者点based提示情况下，模型会预测错误的部分或者完全错过certain parts of the instrument mask（例如，钩子、臂部）。实际上，SAM 在复杂的外科手术场景中，即血肉泛滥、反射、模糊和抑吸的情况下，也很难分类工具。此外，SAM 对数据损害不具备充分的Robustness，无法保持高性能。为了解决这些问题，我们尝试使用 LoRA 进行微调，并提出了 SurgicalSAM，它可以在无提示情况下进行类别 маска预测。因此，我们可以 argue ，无需进一步领域特定的微调，SAM 不够准备于下游外科任务。
</details></li>
</ul>
<hr>
<h2 id="FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving"><a href="#FocusFlow-Boosting-Key-Points-Optical-Flow-Estimation-for-Autonomous-Driving" class="headerlink" title="FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving"></a>FocusFlow: Boosting Key-Points Optical Flow Estimation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07104">http://arxiv.org/abs/2308.07104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhonghuayi/focusflow_official">https://github.com/zhonghuayi/focusflow_official</a></li>
<li>paper_authors: Zhonghua Yi, Hao Shi, Kailun Yang, Qi Jiang, Yaozu Ye, Ze Wang, Kaiwei Wang</li>
<li>for: 提高数据驱动的光流估算精度，特别是关键点方面。</li>
<li>methods: 提出点 clouds模型，并使用混合损失函数和特定点控制损失函数进行多个点精度的监督。 Condition Control Encoder (CCE) 将经典特征编码器替换为 Condition Feature Encoder (CFE)，并将帧特征编码器 (FFE) 与 CFE 进行控制相互传输。</li>
<li>results: 与普通的数据驱动光流估算方法相比，FocusFlow 在关键点方面提高了精度，并且可以与普通的特征编码器进行比较，在整个帧上也能达到类似或更高的性能。<details>
<summary>Abstract</summary>
Key-point-based scene understanding is fundamental for autonomous driving applications. At the same time, optical flow plays an important role in many vision tasks. However, due to the implicit bias of equal attention on all points, classic data-driven optical flow estimation methods yield less satisfactory performance on key points, limiting their implementations in key-point-critical safety-relevant scenarios. To address these issues, we introduce a points-based modeling method that requires the model to learn key-point-related priors explicitly. Based on the modeling method, we present FocusFlow, a framework consisting of 1) a mix loss function combined with a classic photometric loss function and our proposed Conditional Point Control Loss (CPCL) function for diverse point-wise supervision; 2) a conditioned controlling model which substitutes the conventional feature encoder by our proposed Condition Control Encoder (CCE). CCE incorporates a Frame Feature Encoder (FFE) that extracts features from frames, a Condition Feature Encoder (CFE) that learns to control the feature extraction behavior of FFE from input masks containing information of key points, and fusion modules that transfer the controlling information between FFE and CFE. Our FocusFlow framework shows outstanding performance with up to +44.5% precision improvement on various key points such as ORB, SIFT, and even learning-based SiLK, along with exceptional scalability for most existing data-driven optical flow methods like PWC-Net, RAFT, and FlowFormer. Notably, FocusFlow yields competitive or superior performances rivaling the original models on the whole frame. The source code will be available at https://github.com/ZhonghuaYi/FocusFlow_official.
</details>
<details>
<summary>摘要</summary>
“键点基本Scene理解是自动驾驶应用的基础。同时，光流扮演了许多视觉任务中重要的角色。然而，由于预设所有点都受到同等的注意力， класи型数据驱动的光流估计方法对键点的表现不如预期，从而限制它们在键点敏感的安全相关enario中的实现。为解决这些问题，我们介绍了一个点 cloud Modeling 方法，让模型Explicitly learn键点相关的先验知识。基于这个方法，我们发表了FocusFlow框架，包括以下几个部分：1) 一个mix损失函数和类别摄影损失函数以及我们提出的Conditional Point Control Loss (CPCL)函数 для多点精确指导; 2) 一个受控制的模型，将传统的Feature Encoder取代为我们提出的Condition Control Encoder (CCE)。CCE包括Frame Feature Encoder (FFE)、Condition Feature Encoder (CFE) 和融合模块，从输入mask中学习控制FFE的特征提取行为，并将控制信息转移到FFE和CFE之间。我们的FocusFlow框架在不同的键点上显示出惊人的表现，包括ORB、SIFT 和甚至学习式SiLK，并且具有卓越的扩展性，可以与现有的大多数数据驱动的光流方法相容。尤其是，FocusFlow在整幅图上表现竞争或超越原始模型。代码将在https://github.com/ZhonghuaYi/FocusFlow_official中公开。”
</details></li>
</ul>
<hr>
<h2 id="When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation"><a href="#When-Deep-Learning-Meets-Multi-Task-Learning-in-SAR-ATR-Simultaneous-Target-Recognition-and-Segmentation" class="headerlink" title="When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation"></a>When Deep Learning Meets Multi-Task Learning in SAR ATR: Simultaneous Target Recognition and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07093">http://arxiv.org/abs/2308.07093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Jifang Pei, Zhiyong Wang, Yulin Huang, Junjie Wu, Haiguang Yang, Jianyu Yang</li>
<li>for: 本研究旨在提出一种基于多任务学习的Synthetic Aperture Radar（SAR）自动目标识别（ATR）方法，以实现精准的目标类别和精确的目标形态同时识别。</li>
<li>methods: 该方法基于深度学习理论，提出了一种新的多任务学习框架，包括两个主要结构：编码器和解码器。编码器用于抽取不同缩放级别的图像特征，而解码器则是一个任务特有的结构，通过使用这些抽取的特征进行适应性和优化地满足不同识别和分割任务的特征需求。</li>
<li>results: 基于Moving and Stationary Target Acquisition and Recognition（MSTAR）数据集的实验结果表明，提出的方法在识别和分割任务中具有优越性。<details>
<summary>Abstract</summary>
With the recent advances of deep learning, automatic target recognition (ATR) of synthetic aperture radar (SAR) has achieved superior performance. By not being limited to the target category, the SAR ATR system could benefit from the simultaneous extraction of multifarious target attributes. In this paper, we propose a new multi-task learning approach for SAR ATR, which could obtain the accurate category and precise shape of the targets simultaneously. By introducing deep learning theory into multi-task learning, we first propose a novel multi-task deep learning framework with two main structures: encoder and decoder. The encoder is constructed to extract sufficient image features in different scales for the decoder, while the decoder is a tasks-specific structure which employs these extracted features adaptively and optimally to meet the different feature demands of the recognition and segmentation. Therefore, the proposed framework has the ability to achieve superior recognition and segmentation performance. Based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, experimental results show the superiority of the proposed framework in terms of recognition and segmentation.
</details>
<details>
<summary>摘要</summary>
Our approach is based on a deep learning framework with two main structures: an encoder and a decoder. The encoder is designed to extract comprehensive image features at multiple scales, while the decoder is a task-specific structure that adaptively and optimally utilizes these features to meet the diverse demands of recognition and segmentation. This allows our framework to achieve superior performance in both recognition and segmentation.Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset demonstrate the superiority of our proposed framework. With the ability to accurately recognize and precisely segment targets, our approach offers a significant improvement over traditional SAR ATR methods.
</details></li>
</ul>
<hr>
<h2 id="Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks"><a href="#Deepbet-Fast-brain-extraction-of-T1-weighted-MRI-using-Convolutional-Neural-Networks" class="headerlink" title="Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks"></a>Deepbet: Fast brain extraction of T1-weighted MRI using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07003">http://arxiv.org/abs/2308.07003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Fisch, Stefan Zumdick, Carlotta Barkhau, Daniel Emden, Jan Ernsting, Ramona Leenings, Kelvin Sarink, Nils R. Winter, Benjamin Risse, Udo Dannlowski, Tim Hahn</li>
<li>for: 这个论文主要是为了提出一个高精度、快速的Magnetic Resonance Imaging（MRI）数据中的脑部分 segmentation工具，以取代传统的脑部分分类方法。</li>
<li>methods: 这个论文使用了现代的深度学习方法，包括LinkNet的现代UNet架构，在两个阶段预测过程中进行预测。这将提高了脑部分分类的性能，在测验中得到了一个新的州OF-THE-ART性能， median Dice score（DSC）为99.0%，比现有的模型高出2.2%和1.9%。</li>
<li>results: 这个论文的模型可以实现高精度的脑部分分类，Dice score（DSC）高于96.9%，并且更敏感于噪音。此外，这个模型可以将脑部分分类的时间加速到了约10倍，可以在低级硬件上处理一个数据仅需2秒钟。<details>
<summary>Abstract</summary>
Brain extraction in magnetic resonance imaging (MRI) data is an important segmentation step in many neuroimaging preprocessing pipelines. Image segmentation is one of the research fields in which deep learning had the biggest impact in recent years enabling high precision segmentation with minimal compute. Consequently, traditional brain extraction methods are now being replaced by deep learning-based methods. Here, we used a unique dataset comprising 568 T1-weighted (T1w) MR images from 191 different studies in combination with cutting edge deep learning methods to build a fast, high-precision brain extraction tool called deepbet. deepbet uses LinkNet, a modern UNet architecture, in a two stage prediction process. This increases its segmentation performance, setting a novel state-of-the-art performance during cross-validation with a median Dice score (DSC) of 99.0% on unseen datasets, outperforming current state of the art models (DSC = 97.8% and DSC = 97.9%). While current methods are more sensitive to outliers, resulting in Dice scores as low as 76.5%, deepbet manages to achieve a Dice score of > 96.9% for all samples. Finally, our model accelerates brain extraction by a factor of ~10 compared to current methods, enabling the processing of one image in ~2 seconds on low level hardware.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 数据中的脑部EXTRACTION是许多神经成像预处理管道中重要的 segmentation 步骤。图像 segmentation 是深度学习在过去几年中对神经成像领域产生了最大的影响，使得传统的脑部EXTRACTION 方法被深度学习基于的方法所取代。在本文中，我们使用了568张T1-weighted (T1w) MRI图像和 cutting-edge deep learning 方法建立了一个快速、高精度的脑部EXTRACTION 工具 called deepbet。deepbet 使用了 LinkNet，一种现代的 U-Net 架构，在两个阶段预测过程中。这使得它的 segmentation 性能得到了提高，在跨验证中 median Dice 分数 (DSC) 为 99.0%，超过当前的状态对照模型 (DSC = 97.8%和DSC = 97.9%)。而现有方法更敏感于异常值，导致 Dice 分数只有 76.5%，而 deepbet 则能够达到 > 96.9% 的 Dice 分数 для所有样本。最后，我们的模型将脑部EXTRACTION 加速了约10倍，使得一个图像只需要 ~2秒钟的处理时间。
</details></li>
</ul>
<hr>
<h2 id="How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation"><a href="#How-inter-rater-variability-relates-to-aleatoric-and-epistemic-uncertainty-a-case-study-with-deep-learning-based-paraspinal-muscle-segmentation" class="headerlink" title="How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation"></a>How inter-rater variability relates to aleatoric and epistemic uncertainty: a case study with deep learning-based paraspinal muscle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06964">http://arxiv.org/abs/2308.06964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parinaz Roshanzamir, Hassan Rivaz, Joshua Ahn, Hamza Mirza, Neda Naghdi, Meagan Anstruther, Michele C. Battié, Maryse Fortin, Yiming Xiao</li>
<li>for: This paper aims to explore the relationship between inter-rater variability and uncertainties in deep learning models for medical image segmentation, and to compare the performance of different label fusion strategies and DL models.</li>
<li>methods: The paper uses test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties, and compares the performance of UNet and TransUNet with two label fusion strategies.</li>
<li>results: The study reveals the interplay between inter-rater variability and uncertainties, and shows that choices of label fusion strategies and DL models can affect the resulting segmentation performance.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是探讨医学影像分割任务中间质量标注人员之间的差异对深度学习模型的不确定性的关系，以及不同的标签汇集策略和深度学习模型的性能比较。</li>
<li>methods: 该论文使用测试时数据增强（TTA）、测试时dropout（TTD）和深度ensemble来测量 aleatoric 和 epistemic 不确定性，并比较 UNet 和 TransUNet 的性能。</li>
<li>results: 研究发现，医学影像分割任务中间质量标注人员之间的差异会影响深度学习模型的不确定性，并且选择不同的标签汇集策略和深度学习模型可以affect segmentation性能。<details>
<summary>Abstract</summary>
Recent developments in deep learning (DL) techniques have led to great performance improvement in medical image segmentation tasks, especially with the latest Transformer model and its variants. While labels from fusing multi-rater manual segmentations are often employed as ideal ground truths in DL model training, inter-rater variability due to factors such as training bias, image noise, and extreme anatomical variability can still affect the performance and uncertainty of the resulting algorithms. Knowledge regarding how inter-rater variability affects the reliability of the resulting DL algorithms, a key element in clinical deployment, can help inform better training data construction and DL models, but has not been explored extensively. In this paper, we measure aleatoric and epistemic uncertainties using test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to explore their relationship with inter-rater variability. Furthermore, we compare UNet and TransUNet to study the impacts of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, affected by choices of label fusion strategies and DL models.
</details>
<details>
<summary>摘要</summary>
In this paper, we use test-time augmentation (TTA), test-time dropout (TTD), and deep ensemble to measure aleatoric and epistemic uncertainties and explore their relationship with inter-rater variability. We also compare UNet and TransUNet to study the impact of Transformers on model uncertainty with two label fusion strategies. We conduct a case study using multi-class paraspinal muscle segmentation from T2w MRIs. Our study reveals the interplay between inter-rater variability and uncertainties, which is influenced by choices of label fusion strategies and DL models.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Stress-Testing-in-Medical-Image-Classification"><a href="#Robustness-Stress-Testing-in-Medical-Image-Classification" class="headerlink" title="Robustness Stress Testing in Medical Image Classification"></a>Robustness Stress Testing in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06889">http://arxiv.org/abs/2308.06889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mobarakol/robustness_stress_testing">https://github.com/mobarakol/robustness_stress_testing</a></li>
<li>paper_authors: Mobarakol Islam, Zeju Li, Ben Glocker</li>
<li>For: 评估医学图像疾病检测算法的 клиниче验证性能。* Methods: 使用进行挑战测试来评估模型的可靠性和不同类型和地区的表现差异。* Results: 表明了一些模型在不同的图像挑战测试中的Robustness和公平性。还发现预训练特征对下游的可靠性产生了重要的影响。<details>
<summary>Abstract</summary>
Deep neural networks have shown impressive performance for image-based disease detection. Performance is commonly evaluated through clinical validation on independent test sets to demonstrate clinically acceptable accuracy. Reporting good performance metrics on test sets, however, is not always a sufficient indication of the generalizability and robustness of an algorithm. In particular, when the test data is drawn from the same distribution as the training data, the iid test set performance can be an unreliable estimate of the accuracy on new data. In this paper, we employ stress testing to assess model robustness and subgroup performance disparities in disease detection models. We design progressive stress testing using five different bidirectional and unidirectional image perturbations with six different severity levels. As a use case, we apply stress tests to measure the robustness of disease detection models for chest X-ray and skin lesion images, and demonstrate the importance of studying class and domain-specific model behaviour. Our experiments indicate that some models may yield more robust and equitable performance than others. We also find that pretraining characteristics play an important role in downstream robustness. We conclude that progressive stress testing is a viable and important tool and should become standard practice in the clinical validation of image-based disease detection models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/14/eess.IV_2023_08_14/" data-id="clly4xtgc00fbvl887kts3w5e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/cs.LG_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/cs.LG_2023_08_13/">cs.LG - 2023-08-13 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP"><a href="#Faithful-to-Whom-Questioning-Interpretability-Measures-in-NLP" class="headerlink" title="Faithful to Whom? Questioning Interpretability Measures in NLP"></a>Faithful to Whom? Questioning Interpretability Measures in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06795">http://arxiv.org/abs/2308.06795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Crothers, Herna Viktor, Nathalie Japkowicz</li>
<li>for: 这篇论文主要是为了量化模型解释性的方法。</li>
<li>methods: 这篇论文使用了基于循环屏蔽输入Token的方法来计算 faithfulness 度量。</li>
<li>results: 论文发现现有的 faithfulness 度量不适合比较不同的神经网络文本分类器的解释性，因为屏蔽输入的样本频繁出现在训练时未看到的分布外。<details>
<summary>Abstract</summary>
A common approach to quantifying model interpretability is to calculate faithfulness metrics based on iteratively masking input tokens and measuring how much the predicted label changes as a result. However, we show that such metrics are generally not suitable for comparing the interpretability of different neural text classifiers as the response to masked inputs is highly model-specific. We demonstrate that iterative masking can produce large variation in faithfulness scores between comparable models, and show that masked samples are frequently outside the distribution seen during training. We further investigate the impact of adversarial attacks and adversarial training on faithfulness scores, and demonstrate the relevance of faithfulness measures for analyzing feature salience in text adversarial attacks. Our findings provide new insights into the limitations of current faithfulness metrics and key considerations to utilize them appropriately.
</details>
<details>
<summary>摘要</summary>
一种常见的方法量化模型解释性是计算基于 iteratively 掩码输入Token 的 faithfulness 度量。然而，我们显示这些度量不适合比较不同的神经网络文本分类器的解释性，因为掩码输入的响应是高度特定的。我们示出了 iterative 掩码可以导致大量的 faithfulness 分数变化，并且masked 样本 часто处于训练过程中未见过的分布之外。我们进一步研究了对 faithfulness 度量的影响，以及对文本 adversarial 攻击的分析。我们的发现为现有的 faithfulness 度量带来新的认识和关键考虑因素。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-at-a-Fraction-with-Pruned-Quaternions"><a href="#Neural-Networks-at-a-Fraction-with-Pruned-Quaternions" class="headerlink" title="Neural Networks at a Fraction with Pruned Quaternions"></a>Neural Networks at a Fraction with Pruned Quaternions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06780">http://arxiv.org/abs/2308.06780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smlab-niser/quartLT22">https://github.com/smlab-niser/quartLT22</a></li>
<li>paper_authors: Sahel Mohammad Iqbal, Subhankar Mishra</li>
<li>for: 这个研究旨在测试适用于具有限制性的处理器的现代神经网络，以及使用高维度数据嵌入（如复数或四元数）来实现更好的优化。</li>
<li>methods: 这个研究使用了剪除来简化神经网络中的 Parameters，并在分类任务上进行了不同架构和数据集的实验。</li>
<li>results: 研究发现，在某些架构和任务上，将神经网络转换为复数值的版本可以在具有很高简化水平的情况下提供更高的准确性。例如，在CIFAR-10中使用Conv-4架构，将 Parameters 剪除至3%以下，复数值版本可以与原始模型相比提高超过10%的准确性。<details>
<summary>Abstract</summary>
Contemporary state-of-the-art neural networks have increasingly large numbers of parameters, which prevents their deployment on devices with limited computational power. Pruning is one technique to remove unnecessary weights and reduce resource requirements for training and inference. In addition, for ML tasks where the input data is multi-dimensional, using higher-dimensional data embeddings such as complex numbers or quaternions has been shown to reduce the parameter count while maintaining accuracy. In this work, we conduct pruning on real and quaternion-valued implementations of different architectures on classification tasks. We find that for some architectures, at very high sparsity levels, quaternion models provide higher accuracies than their real counterparts. For example, at the task of image classification on CIFAR-10 using Conv-4, at $3\%$ of the number of parameters as the original model, the pruned quaternion version outperforms the pruned real by more than $10\%$. Experiments on various network architectures and datasets show that for deployment in extremely resource-constrained environments, a sparse quaternion network might be a better candidate than a real sparse model of similar architecture.
</details>
<details>
<summary>摘要</summary>
现代神经网络的参数数量逐渐增加，这使得具有有限计算能力的设备上进行训练和推理变得困难。剪枝技术可以将不必要的权重从神经网络中移除，以降低训练和推理的资源需求。此外，在多维输入数据的机器学习任务上，使用高维数域嵌入，如复数或四元数，可以降低参数数量而保持准确性。在这种情况下，我们对不同的架构和数据集进行了剪枝和权重融合的实验。我们发现，在某些架构上，在非常高的精灵度水平上，使用四元数模型可以提高准确性，比如在CIFAR-10上使用Conv-4，在3%的参数数量下，剪枝后的四元数模型的准确性高于剪枝后的实数模型的10%以上。各种网络架构和数据集的实验表明，在极其有限的资源环境下，一个稀疏的四元数网络可能比同样的架构的实数稀疏网络更适合进行部署。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations"><a href="#A-Survey-on-Deep-Neural-Network-Pruning-Taxonomy-Comparison-Analysis-and-Recommendations" class="headerlink" title="A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations"></a>A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06767">http://arxiv.org/abs/2308.06767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hrcheng1066/awesome-pruning">https://github.com/hrcheng1066/awesome-pruning</a></li>
<li>paper_authors: Hongrong Cheng, Miao Zhang, Javen Qinfeng Shi</li>
<li>for: 本文提供了一份涵盖现有研究工作的深度神经网络减少报告，以便更好地理解现有的方法和技术。</li>
<li>methods: 本文分类了现有的减少方法，包括一般&#x2F;特定速度减少、 WHEN TO PRUNE、HOW TO PRUNE 和减少与其他压缩技术的融合。</li>
<li>results: 本文进行了七对对照设定的比较分析，探讨了不同级别的监督和不同应用场景，以便更好地了解现有方法的共同点和区别。<details>
<summary>Abstract</summary>
Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of seven pairs of contrast settings for pruning (e.g., unstructured/structured) and explore emerging topics, including post-training pruning, different levels of supervision for pruning, and broader applications (e.g., adversarial robustness) to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide some valuable recommendations on selecting pruning methods and prospect promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络，特别是最近的大型语言模型，具有庞大的计算和存储资源需求。为实现资源有限环境中部署现代模型和加速推理时间，研究人员已经不断探索剪裁技术作为神经网络压缩的流行研究方向。然而，目前的相关评论综述缺乏。为解决这问题，在这篇评论中，我们提供了一份全面的评论综述，分为以下四个方面：1) 通用/特定加速，2) 何时剪裁，3) 如何剪裁，和4) 剪裁与其他压缩技术的融合。然后，我们对七对对比设定进行了仔细的比较分析（例如，无结构/结构），并探讨了emerging topics，如后处理剪裁、不同水平的监督剪裁和更广泛的应用（例如，防御性鲁棒性），以抛光现有方法的相似和不同，并为未来的研究铺垫基础。为便于未来的研究，我们建立了一个 curaated 的数据集、网络和评估集。最后，我们提供了一些有价值的建议，包括选择剪裁方法和未来研究方向。我们在 GitHub 上建立了一个存储库，请参考 <https://github.com/hrcheng1066/awesome-pruning>。
</details></li>
</ul>
<hr>
<h2 id="Conic-Descent-Redux-for-Memory-Efficient-Optimization"><a href="#Conic-Descent-Redux-for-Memory-Efficient-Optimization" class="headerlink" title="Conic Descent Redux for Memory-Efficient Optimization"></a>Conic Descent Redux for Memory-Efficient Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07343">http://arxiv.org/abs/2308.07343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingcong Li, Georgios B. Giannakis</li>
<li>for: 该论文旨在提高首项壳programming的效率和精度，并应用于信号处理和机器学习领域。</li>
<li>methods: 该论文提出了三个方面的改进：一是具有直观的几何解释，二是基于对偶问题的理论基础，三是一种新的批处理算法。</li>
<li>results: 研究发现，首项壳programming可以通过增加矩阵的权重来加速对偶解释，并且可以通过采用偏好的初始值来加速搜索过程。<details>
<summary>Abstract</summary>
Conic programming has well-documented merits in a gamut of signal processing and machine learning tasks. This contribution revisits a recently developed first-order conic descent (CD) solver, and advances it in three aspects: intuition, theory, and algorithmic implementation. It is found that CD can afford an intuitive geometric derivation that originates from the dual problem. This opens the door to novel algorithmic designs, with a momentum variant of CD, momentum conic descent (MOCO) exemplified. Diving deeper into the dual behavior CD and MOCO reveals: i) an analytically justified stopping criterion; and, ii) the potential to design preconditioners to speed up dual convergence. Lastly, to scale semidefinite programming (SDP) especially for low-rank solutions, a memory efficient MOCO variant is developed and numerically validated.
</details>
<details>
<summary>摘要</summary>
带有较好的记录的圆形编程在信号处理和机器学习任务中具有良好的优点。本贡献将最近开发的首览圆形下降（CD）解决方案进行三个方面的提高：直观、理论和算法实现。研究发现CD可以从对准问题的 dual 问题中得到直观的几何 derivation，这打开了新的算法设计的门户，例如旋转圆形下降（MOCO）。钻 deeper into CD和MOCO的双重行为，发现：i) 可以分析正确的停止标准；ii) 可以设计加速对准速度的预conditioners。最后，为优化semidefinite程序（SDP），尤其是低维解决方案，我们开发了内存高效的MOCO变体并 NUMERICALLY 验证了其正确性。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Class-incremental-Learning-A-Survey"><a href="#Few-shot-Class-incremental-Learning-A-Survey" class="headerlink" title="Few-shot Class-incremental Learning: A Survey"></a>Few-shot Class-incremental Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06764">http://arxiv.org/abs/2308.06764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinghua Zhang, Li Liu, Olli Silven, Matti Pietikäinen, Dewen Hu</li>
<li>for: 这篇论文的目的是为了提供对几拟学习（Few-shot Class-Incremental Learning，FSCIL）的系统性和全面的综述。</li>
<li>methods: 这篇论文使用了多种方法来探讨FSCIL，包括问题定义、主要挑战的不可靠的实验准确风险和稳定性-多样性矛盾、通用方案和相关的增量学习和几拟学习方法。</li>
<li>results: 这篇论文提供了各种FSCIL中的分类方法和对象检测方法，包括数据基于、结构基于和优化基于的方法，以及 anchor-free和 anchor-based的对象检测方法。<details>
<summary>Abstract</summary>
Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in machine learning, as it necessitates the continuous learning of new classes from sparse labeled training samples without forgetting previous knowledge. While this field has seen recent progress, it remains an active area of exploration. This paper aims to provide a comprehensive and systematic review of FSCIL. In our in-depth examination, we delve into various facets of FSCIL, encompassing the problem definition, the discussion of primary challenges of unreliable empirical risk minimization and the stability-plasticity dilemma, general schemes, and relevant problems of incremental learning and few-shot learning. Besides, we offer an overview of benchmark datasets and evaluation metrics. Furthermore, we introduce the classification methods in FSCIL from data-based, structure-based, and optimization-based approaches and the object detection methods in FSCIL from anchor-free and anchor-based approaches. Beyond these, we illuminate several promising research directions within FSCIL that merit further investigation.
</details>
<details>
<summary>摘要</summary>
《几个shot类增长学习（FSCIL）》是机器学习中的一个特殊挑战，它需要不断学习新的类型从罕见的标签训练样本中学习，而不会忘记之前的知识。尽管这个领域已经有了一定的进步，但仍然是一个活跃的研究领域。本文的目的是提供了FSCIL的全面和系统性的综述。在我们的深入检查中，我们探讨了FSCIL的各个方面，包括问题定义、不可靠的实际风险最小化和稳定-柔软之间的矛盾、通用方案和相关的增量学习和几个shot学习问题。此外，我们介绍了FSCIL中的数据集和评价指标。此外，我们还介绍了FSCIL中的分类方法，包括数据基于、结构基于和优化基于的方法，以及对象检测方法，包括固定和无固定的方法。此外，我们还释明了FSCIL中的一些有前途的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining"><a href="#Discovering-the-Symptom-Patterns-of-COVID-19-from-Recovered-and-Deceased-Patients-Using-Apriori-Association-Rule-Mining" class="headerlink" title="Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining"></a>Discovering the Symptom Patterns of COVID-19 from Recovered and Deceased Patients Using Apriori Association Rule Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06763">http://arxiv.org/abs/2308.06763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Dehghani, Zahra Yazdanparast, Mobin Mohammadi</li>
<li>for: 本研究旨在利用关联规则挖掘技术对 COVID-19 患者症状进行分析，以提供临床医生管理疾病的有用信息。</li>
<li>methods: 本研究使用 Apriori 算法进行关联规则挖掘，分析 COVID-19 患者2875条病例记录，并发现最常见的症状为呼吸困难（72%）、咳嗽（64%）、发热（59%）、衰弱（18%）、 мышьяк（14.5%）和喉咙痛（12%）。</li>
<li>results: 本研究发现，Apriori 算法可以帮助临床医生更好地理解 COVID-19 疾病的表现形式，并提供有价值的信息来帮助他们更好地诊断和治疗疾病。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has a devastating impact globally, claiming millions of lives and causing significant social and economic disruptions. In order to optimize decision-making and allocate limited resources, it is essential to identify COVID-19 symptoms and determine the severity of each case. Machine learning algorithms offer a potent tool in the medical field, particularly in mining clinical datasets for useful information and guiding scientific decisions. Association rule mining is a machine learning technique for extracting hidden patterns from data. This paper presents an application of association rule mining based Apriori algorithm to discover symptom patterns from COVID-19 patients. The study, using 2875 records of patient, identified the most common symptoms as apnea (72%), cough (64%), fever (59%), weakness (18%), myalgia (14.5%), and sore throat (12%). The proposed method provides clinicians with valuable insight into disease that can assist them in managing and treating it effectively.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对全球造成了毁灭性的影响，负死亡人数和社会经济秩序受到了重大的影响。为了优化决策和分配有限的资源，必须能够识别 COVID-19 的症状和每个患者的严重程度。机器学习算法在医疗领域提供了一种极其有用的工具，特别是在挖掘医疗数据中找到有用信息并导引科学决策。在本文中，我们使用 Apriori 算法来应用关联规则挖掘技术，从 COVID-19 患者的记录中挖掘症状模式。研究使用了 2875 个病人记录，并发现最常见的症状为呼吸停止（72%）、咳嗽（64%）、发热（59%）、衰竭（18%）、肌肉疼痛（14.5%）和喉咙痛（12%）。该方法为临床医生提供了有价值的病理知识，可以帮助他们更好地诊断和治疗疾病。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization"><a href="#Heterogeneous-Multi-Agent-Reinforcement-Learning-via-Mirror-Descent-Policy-Optimization" class="headerlink" title="Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization"></a>Heterogeneous Multi-Agent Reinforcement Learning via Mirror Descent Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06741">http://arxiv.org/abs/2308.06741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Mehdi Nasiri, Mansoor Rezghi</li>
<li>for: 这个论文旨在解决多Agent Reinforcement Learning（MARL）中协同努力的挑战，其中Agent具有不同能力和个人策略。</li>
<li>methods: 该论文提出了一种基于镜像下降法的多Agent策略优化算法（HAMDPO），利用多Agent优化减少问题中的策略更新，保证总体性能提高。</li>
<li>results: 该论文通过在Multi-Agent MuJoCo和StarCraftII任务上评估HAMDPO算法，并证明其在相比之前的状态静态算法（HATRPO和HAPPO）的稳定性和性能提高。<details>
<summary>Abstract</summary>
This paper presents an extension of the Mirror Descent method to overcome challenges in cooperative Multi-Agent Reinforcement Learning (MARL) settings, where agents have varying abilities and individual policies. The proposed Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO) algorithm utilizes the multi-agent advantage decomposition lemma to enable efficient policy updates for each agent while ensuring overall performance improvements. By iteratively updating agent policies through an approximate solution of the trust-region problem, HAMDPO guarantees stability and improves performance. Moreover, the HAMDPO algorithm is capable of handling both continuous and discrete action spaces for heterogeneous agents in various MARL problems. We evaluate HAMDPO on Multi-Agent MuJoCo and StarCraftII tasks, demonstrating its superiority over state-of-the-art algorithms such as HATRPO and HAPPO. These results suggest that HAMDPO is a promising approach for solving cooperative MARL problems and could potentially be extended to address other challenging problems in the field of MARL.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:这篇论文提出了一种新的方法 called Heterogeneous-Agent Mirror Descent Policy Optimization (HAMDPO)，用于解决合作多代理演算学习（MARL）中的挑战，其中代理有不同的能力和个人策略。HAMDPO算法使用多代理优势分解 Lemma 来有效地更新代理策略，并保证总性的性能提高。通过迭代更新代理策略的近似解决方案，HAMDPO算法保证稳定性和性能提高。此外，该算法可以处理不同类型的动作空间，包括连续和离散动作空间，并在多种 MARL 问题中进行应用。作者们在 Multi-Agent MuJoCo 和 StarCraftII 任务上评估了 HAMDPO 算法，并证明其在当前状态的算法中具有优势，例如 HATRPO 和 HAPPO 等算法。这些结果表明，HAMDPO 是一种有前途的方法，可以解决合作 MARL 问题，并可能扩展到其他难题。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection"><a href="#Weighted-Sparse-Partial-Least-Squares-for-Joint-Sample-and-Feature-Selection" class="headerlink" title="Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection"></a>Weighted Sparse Partial Least Squares for Joint Sample and Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06740">http://arxiv.org/abs/2308.06740</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenwenmin/wspls">https://github.com/wenwenmin/wspls</a></li>
<li>paper_authors: Wenwen Min, Taosheng Xu, Chris Ding</li>
<li>For: The paper proposes a method for joint sample and feature selection in data fusion using sparse partial least squares (sPLS) with $\ell_\infty&#x2F;\ell_0$-norm constrained weighted sparse PLS (wsPLS) and extends it to multi-view data fusion.* Methods: The proposed method uses the $\ell_\infty&#x2F;\ell_0$-norm constrains to select a subset of samples and the Kurdyka-\L{ojasiewicz}~property to ensure global convergence of the algorithm.* Results: The proposed method is demonstrated to be efficient through numerical and biomedical data experiments, and is shown to outperform traditional sPLS methods in terms of computational efficiency and accuracy.<details>
<summary>Abstract</summary>
Sparse Partial Least Squares (sPLS) is a common dimensionality reduction technique for data fusion, which projects data samples from two views by seeking linear combinations with a small number of variables with the maximum variance. However, sPLS extracts the combinations between two data sets with all data samples so that it cannot detect latent subsets of samples. To extend the application of sPLS by identifying a specific subset of samples and remove outliers, we propose an $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS ($\ell_\infty/\ell_0$-wsPLS) method for joint sample and feature selection, where the $\ell_\infty/\ell_0$-norm constrains are used to select a subset of samples. We prove that the $\ell_\infty/\ell_0$-norm constrains have the Kurdyka-\L{ojasiewicz}~property so that a globally convergent algorithm is developed to solve it. Moreover, multi-view data with a same set of samples can be available in various real problems. To this end, we extend the $\ell_\infty/\ell_0$-wsPLS model and propose two multi-view wsPLS models for multi-view data fusion. We develop an efficient iterative algorithm for each multi-view wsPLS model and show its convergence property. As well as numerical and biomedical data experiments demonstrate the efficiency of the proposed methods.
</details>
<details>
<summary>摘要</summary>
“稀疏部分最小倍数（sPLS）是一种常见的维度减少技术 для数据融合，它通过寻找两个视图中数据样本之间的线性组合来降低维度。然而，sPLS会捕捉两个数据集中所有数据样本的组合，因此无法检测隐藏的样本subset。为了扩展sPLS的应用，我们提出了一种使用 $\ell_\infty/\ell_0$-norm constrained weighted sparse PLS（$\ell_\infty/\ell_0$-wsPLS）方法，该方法可以同时进行样本选择和特征选择。我们证明了 $\ell_\infty/\ell_0$-norm constrains possess Kurdyka-\L{ojasiewicz} 性质，因此可以开发一个全球收敛的算法来解决它。此外，多视图数据中可以有同一个样本集。为此，我们扩展了 $\ell_\infty/\ell_0$-wsPLS 模型，并提出了两种多视图 wsPLS 模型 для多视图数据融合。我们开发了一个高效的迭代算法，并证明其收敛性。numerical和生物医学数据实验表明提出的方法的效率。”Note: Simplified Chinese is a simplified version of Chinese that is used in mainland China and is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data"><a href="#Probabilistic-Imputation-for-Time-series-Classification-with-Missing-Data" class="headerlink" title="Probabilistic Imputation for Time-series Classification with Missing Data"></a>Probabilistic Imputation for Time-series Classification with Missing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06738">http://arxiv.org/abs/2308.06738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout">https://github.com/yuneg11/SupNotMIWAE-with-ObsDropout</a></li>
<li>paper_authors: SeungHyun Kim, Hyunsu Kim, EungGu Yun, Hwangrae Lee, Jaehun Lee, Juho Lee</li>
<li>for: 用于处理多变量时间序列数据中的缺失值</li>
<li>methods: 使用深度生成模型进行缺失值填充，并使用分类器进行信号分类</li>
<li>results: 通过实验表明，该方法可以有效地处理多变量时间序列数据中的缺失值，并且可以提高信号分类的准确率<details>
<summary>Abstract</summary>
Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that na\"ively combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
多变量时间序列数据在实际应用中通常含有较大的缺失值。现有的主流方法为这种缺失值是单纯地假设缺失值为零、平均值或相邻时间步的值。然而，这些简单策略并不考虑数据生成过程，更重要的是，它们不能有效地捕捉预测中的不确定性，因为缺失值有多个可能性。在这篇论文中，我们提出了一种新的概率 frameworks for classification with multivariate time series data with missing values。我们的模型包括两部分：深度生成模型和分类器。 extending the existing deep generative models to better capture the structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation。分类器部分接受时间序列数据和假设的缺失值，并将时间序列数据分类，并且是通过捕捉多个可能性的假设来捕捉预测中的不确定性。然而，我们发现在直接组合生成模型和分类器时，可能会导致生成模型不生成有用的假设值，从而影响分类的准确性。为了解决这问题，我们提出了一种新的正则化技术，可以推动模型生成有用的假设值，以便于分类。通过对实际时间序列数据进行广泛的实验，我们证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Precipitation-nowcasting-with-generative-diffusion-models"><a href="#Precipitation-nowcasting-with-generative-diffusion-models" class="headerlink" title="Precipitation nowcasting with generative diffusion models"></a>Precipitation nowcasting with generative diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06733">http://arxiv.org/abs/2308.06733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models">https://github.com/fmerizzi/Precipitation-nowcasting-with-generative-diffusion-models</a></li>
<li>paper_authors: Andrea Asperti, Fabio Merizzi, Alberto Paparella, Giorgio Pedrazzi, Matteo Angelinelli, Stefano Colamonaco</li>
<li>for: 本研究使用数字天气预测方法进行气象预测，旨在提高预测的准确性和可靠性。</li>
<li>methods: 本研究使用了泛化激发模型（Diffusion Model）来处理气象预测任务，并通过将多个激发模型 ensemble起来，使用post处理网络来组合可能的天气场景，以获得最终的可能性预测结果。</li>
<li>results: 对于中欧2016-2021年的 hourly 数据，与已有的U-Net模型相比，泛化激发模型在气象预测任务中表现出了明显的优势，并且可以substantially 提高总体性能。<details>
<summary>Abstract</summary>
In recent years traditional numerical methods for accurate weather prediction have been increasingly challenged by deep learning methods. Numerous historical datasets used for short and medium-range weather forecasts are typically organized into a regular spatial grid structure. This arrangement closely resembles images: each weather variable can be visualized as a map or, when considering the temporal axis, as a video. Several classes of generative models, comprising Generative Adversarial Networks, Variational Autoencoders, or the recent Denoising Diffusion Models have largely proved their applicability to the next-frame prediction problem, and is thus natural to test their performance on the weather prediction benchmarks. Diffusion models are particularly appealing in this context, due to the intrinsically probabilistic nature of weather forecasting: what we are really interested to model is the probability distribution of weather indicators, whose expected value is the most likely prediction.   In our study, we focus on a specific subset of the ERA-5 dataset, which includes hourly data pertaining to Central Europe from the years 2016 to 2021. Within this context, we examine the efficacy of diffusion models in handling the task of precipitation nowcasting. Our work is conducted in comparison to the performance of well-established U-Net models, as documented in the existing literature. Our proposed approach of Generative Ensemble Diffusion (GED) utilizes a diffusion model to generate a set of possible weather scenarios which are then amalgamated into a probable prediction via the use of a post-processing network. This approach, in comparison to recent deep learning models, substantially outperformed them in terms of overall performance.
</details>
<details>
<summary>摘要</summary>
近年来，传统的数学方法 для准确的天气预测遭受了深度学习方法的挑战。历史数据库用于短距离和中距离天气预测通常按照一定的正方形格结构进行组织。这种设置与图像非常相似，每个天气变量都可以视为地图或者在考虑时间轴时视为视频。许多类型的生成模型，包括生成对抗网络、自适应变换器和最近的干扰扩散模型，在下一帧预测问题上都有广泛的应用，因此在天气预测标准benchmark上进行测试是自然的。干扩散模型在这个上特别有吸引力，因为天气预测的本质是 probabilistic 的：我们真正 interesseted 的是天气指标的概率分布，而这个分布的期望值是最有可能的预测。在我们的研究中，我们关注了ERA-5数据集的一个特定子集，包括2016年至2021年中欧每小时的数据。在这个上下文中，我们研究了干扩散模型在降水预测中的效果。我们的方法与文献中已有的U-Net模型相比，并使用生成ensemble扩散（GED）模型，该模型使用干扩散模型生成一系列可能的天气场景，然后通过使用后处理网络将这些场景融合成一个可能的预测。与最新的深度学习模型相比，我们的方法在总性表现上substantially outperform了它们。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables"><a href="#Generalized-Independent-Noise-Condition-for-Estimating-Causal-Structure-with-Latent-Variables" class="headerlink" title="Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables"></a>Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06718">http://arxiv.org/abs/2308.06718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, Kun Zhang</li>
<li>for: 学习 causal structure 中存在 latent variables 的挑战任务 (learning causal structure with latent variables)</li>
<li>methods: 提出 Generalized Independent Noise (GIN) 条件，用于 Linear non-Gaussian acyclic causal models 中包含 latent variables (propose a GIN condition for linear non-Gaussian acyclic causal models with latent variables)</li>
<li>results: 提供 necessary and sufficient graphical criteria of the GIN condition, 并可以快速 estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs) (provide necessary and sufficient graphical criteria of the GIN condition and can efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models)<details>
<summary>Abstract</summary>
We investigate the challenging task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To address this, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic causal models. Roughly speaking, GIN implies the existence of an exogenous set $\mathcal{S}$ relative to the parent set of $\mathbf{Y}$ (w.r.t. the causal ordering), such that $\mathcal{S}$ d-separates $\mathbf{Y}$ from $\mathbf{Z}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the underlying causal structure of a LiNGLaH is identifiable in light of GIN conditions under mild assumptions. Experimental results show the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们研究一个复杂的任务：在含有隐变量的情况下学习 causal structure。包括找到隐变量的位置和它们的数量，以及确定隐变量和观测变量之间的 causal 关系。为解决这个问题，我们提议一种 Generalized Independent Noise (GIN) 条件，该条件在 linear non-Gaussian acyclic causal models 中包含隐变量，并且确定了某些观测变量的线性组合和其他观测变量的独立性。具体来说，对两个观测变量 $\mathbf{Y}$ 和 $\mathbf{Z}$，GIN 条件成立只要 $\omega^\intercal \mathbf{Y}$ 和 $\mathbf{Z}$ 独立，其中 $\omega$ 是由 $\mathbf{Y}$ 和 $\mathbf{Z}$ 之间的叠加协方差确定的非零参数向量。然后，我们给出了 linear non-Gaussian acyclic causal models 中 GIN 条件的必要和 suficient 图形 критери产。总之，GIN 条件等价于隐变量集 $\mathcal{S}$ 对于 $\mathbf{Y}$ 的父集（根据 causal 顺序）是独立的，并且 $\mathcal{S}$  separates $\mathbf{Y}$ 和 $\mathbf{Z}$。具体来说，如果没有干扰者，则 causal 关系可以看作一种特殊情况。我们还发现，GIN 条件和隐变量的 causal 结构之间存在着直接的关系。因此，我们可以通过 GIN 条件和一种合理的搜索过程来效率地估计 Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs)，其中隐变量可能也有 causal 关系，并且可能会 suivre 一种层次结构。我们证明了 LiNGLaHs 的下面 causal 结构是可Identifiable的，只要 GIN 条件 在轻量级的假设下成立。实验结果表明了我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards"><a href="#Estimating-and-Incentivizing-Imperfect-Knowledge-Agents-with-Hidden-Rewards" class="headerlink" title="Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards"></a>Estimating and Incentivizing Imperfect-Knowledge Agents with Hidden Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06717">http://arxiv.org/abs/2308.06717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilgin Dogan, Zuo-Jun Max Shen, Anil Aswani</li>
<li>For: This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal, with the goal of addressing the challenges of information asymmetry and incentive design in real-life scenarios.* Methods: The paper introduces a novel estimator that uses the history of principal’s incentives and agent’s choices to estimate the agent’s unknown rewards, and proposes a data-driven incentive policy within a multi-armed bandit (MAB) framework.* Results: The paper proves the finite-sample consistency of the estimator and a rigorous regret bound for the principal, and provides simulations to demonstrate the applicability of the framework to green energy aggregator contracts.<details>
<summary>Abstract</summary>
In practice, incentive providers (i.e., principals) often cannot observe the reward realizations of incentivized agents, which is in contrast to many principal-agent models that have been previously studied. This information asymmetry challenges the principal to consistently estimate the agent's unknown rewards by solely watching the agent's decisions, which becomes even more challenging when the agent has to learn its own rewards. This complex setting is observed in various real-life scenarios ranging from renewable energy storage contracts to personalized healthcare incentives. Hence, it offers not only interesting theoretical questions but also wide practical relevance. This paper explores a repeated adverse selection game between a self-interested learning agent and a learning principal. The agent tackles a multi-armed bandit (MAB) problem to maximize their expected reward plus incentive. On top of the agent's learning, the principal trains a parallel algorithm and faces a trade-off between consistently estimating the agent's unknown rewards and maximizing their own utility by offering adaptive incentives to lead the agent. For a non-parametric model, we introduce an estimator whose only input is the history of principal's incentives and agent's choices. We unite this estimator with a proposed data-driven incentive policy within a MAB framework. Without restricting the type of the agent's algorithm, we prove finite-sample consistency of the estimator and a rigorous regret bound for the principal by considering the sequential externality imposed by the agent. Lastly, our theoretical results are reinforced by simulations justifying applicability of our framework to green energy aggregator contracts.
</details>
<details>
<summary>摘要</summary>
在实践中，奖励提供者（即主体）通常无法观察奖励的实现，这与许多主体-代理模型不同。这种信息不对称性使得主体在 solely 观察代理人的决策后难以一直估计代理人的未知奖励。这种复杂的设定在各种实际场景中出现，如可再生能源存储合同和个性化奖励。因此，它不仅提出了有趣的理论问题，还具有广泛的实际 relevance。本文研究了一个 repeat 的反对选择游戏，在这个游戏中，一个自私的学习代理人和一个学习主体之间发生对抗。代理人面临一个多重武器bandit（MAB）问题，以最大化他们的预期奖励加上奖励。主体则在代理人学习的同时，训练一个并行的算法，面临着在估计代理人的未知奖励和自己的利用之间的负担。为了不假设代理人的算法类型，我们引入了一个仅基于奖励提供者历史和代理人选择的估计器。我们将这个估计器与一个基于 MAB 框架的数据驱动奖励策略结合。我们证明了该估计器在非 Parametric 模型下的finite-sample consistency和主体的正确偏误 bound。最后，我们通过实验证明了我们的框架在绿色能源聚合合同中的适用性。
</details></li>
</ul>
<hr>
<h2 id="CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation"><a href="#CDR-Conservative-Doubly-Robust-Learning-for-Debiased-Recommendation" class="headerlink" title="CDR: Conservative Doubly Robust Learning for Debiased Recommendation"></a>CDR: Conservative Doubly Robust Learning for Debiased Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08461">http://arxiv.org/abs/2308.08461</a></li>
<li>repo_url: None</li>
<li>paper_authors: ZiJie Song, JiaWei Chen, Sheng Zhou, QiHao Shi, Yan Feng, Chun Chen, Can Wang</li>
<li>For: 本研究旨在解决推荐系统中的偏见问题，提高推荐系统的准确性和可靠性。* Methods: 本研究使用了双重Robust学习策略（DR），并提出了一种名为保守双重Robust策略（CDR）来解决偏见问题。* Results: 实验结果表明，CDR可以显著提高推荐系统的性能，同时减少偏见的频率。<details>
<summary>Abstract</summary>
In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.   To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
</details>
<details>
<summary>摘要</summary>
在推荐系统（RS）中，用户行为数据是观察性的而不是实验性的，导致数据中存在广泛的偏见。因此，解决偏见问题成为了推荐系统领域的主要挑战。近些年来，双重稳健学习（DR）已经受到了广泛关注，因为它在性能和稳健性方面表现出色。然而，我们的实验结果表明，现有的DR方法受到了叫做“毒补假设”的问题的影响，即假设的插入数据与事实有很大差异，导致计算结果变得对抗性很强。为解决这个问题，本文提出了保守的双重稳健策略（CDR），通过检验插入数据的均值和方差来筛选掉不符合事实的插入数据。理论分析表明，CDR可以降低方差并提高尾部上限。此外，我们的实验调查表明，CDR可以显著提高性能，并且可以减少毒补假设的频率。
</details></li>
</ul>
<hr>
<h2 id="Learning-on-Graphs-with-Out-of-Distribution-Nodes"><a href="#Learning-on-Graphs-with-Out-of-Distribution-Nodes" class="headerlink" title="Learning on Graphs with Out-of-Distribution Nodes"></a>Learning on Graphs with Out-of-Distribution Nodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06714">http://arxiv.org/abs/2308.06714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songyyyy/kdd22-oodgat">https://github.com/songyyyy/kdd22-oodgat</a></li>
<li>paper_authors: Yu Song, Donglin Wang</li>
<li>for: 本文旨在解决图像学中存在非标准节点的问题，特别是检测图像中的异常节点和分类其他节点为已知类别。</li>
<li>methods: 本文提出了一种基于图像注意力网络的异常检测方法，称为Out-of-Distribution Graph Attention Network (OODGAT)，它通过显式地模型不同类型节点之间的交互来分离异常节点和正常节点。</li>
<li>results: EXTENSIVE EXPERIMENTS表明，OODGAT在异常检测方面比现有方法大幅提高，而且与已知类别分类中的性能相当或更好。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are state-of-the-art models for performing prediction tasks on graphs. While existing GNNs have shown great performance on various tasks related to graphs, little attention has been paid to the scenario where out-of-distribution (OOD) nodes exist in the graph during training and inference. Borrowing the concept from CV and NLP, we define OOD nodes as nodes with labels unseen from the training set. Since a lot of networks are automatically constructed by programs, real-world graphs are often noisy and may contain nodes from unknown distributions. In this work, we define the problem of graph learning with out-of-distribution nodes. Specifically, we aim to accomplish two tasks: 1) detect nodes which do not belong to the known distribution and 2) classify the remaining nodes to be one of the known classes. We demonstrate that the connection patterns in graphs are informative for outlier detection, and propose Out-of-Distribution Graph Attention Network (OODGAT), a novel GNN model which explicitly models the interaction between different kinds of nodes and separate inliers from outliers during feature propagation. Extensive experiments show that OODGAT outperforms existing outlier detection methods by a large margin, while being better or comparable in terms of in-distribution classification.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 是现代图学习模型中的状态机器。而现有的 GNN 模型在许多图学习任务中表现出色，但是对于图中存在异常节点（OOD）的情况却很少受到关注。基于 CV 和 NLP 中的概念，我们定义 OOD 节点为训练集中未见过的标签。由于现实世界中的图 oftentimes 是自动生成的，因此真实的图可能会包含来自未知分布的节点。在这工作中，我们定义了图学习中的 OOD 节点问题。特别是，我们希望完成两个任务：1）检测图中不属于已知分布的节点，2）将剩下的节点分类为已知类别之一。我们发现图中的连接模式是有用的异常检测信息，并提出了 Out-of-Distribution Graph Attention Network (OODGAT)，一种新的 GNN 模型，可以在特征传播过程中分离异常节点和正常节点。我们的实验表明，OODGAT 在异常检测方面超过现有的方法，而且在已知分布下的分类性能也不丢下。
</details></li>
</ul>
<hr>
<h2 id="The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems"><a href="#The-Hard-Constraint-PINNs-for-Interface-Optimal-Control-Problems" class="headerlink" title="The Hard-Constraint PINNs for Interface Optimal Control Problems"></a>The Hard-Constraint PINNs for Interface Optimal Control Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06709">http://arxiv.org/abs/2308.06709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianyouzeng/pinns-interface-optimal-control">https://github.com/tianyouzeng/pinns-interface-optimal-control</a></li>
<li>paper_authors: Ming-Chih Lai, Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</li>
<li>for: 解决部分泛函方程（PDEs）中的优化控制问题，包括界面和一些控制约束。</li>
<li>methods: 使用物理学 informed neural networks（PINNs）和最近开发的缺陷捕捉神经网络，解决 mesh-free 和可扩展的 PDEs 优化控制问题。</li>
<li>results: 提出了一种具有硬约束的 PINNs 方法，可以 garantuee 精确满足界面和边界条件，并且可以分离学习 PDEs 和约束。其效率在一些椭球和Parabolic 界面优化控制问题中得到了承诺。<details>
<summary>Abstract</summary>
We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a novel neural network architecture. The resulting hard-constraint PINNs approach guarantees that both the boundary and interface conditions can be satisfied exactly and they are decoupled from the learning of the PDEs. Its efficiency is promisingly validated by some elliptic and parabolic interface optimal control problems.
</details>
<details>
<summary>摘要</summary>
我们证明，用物理知识整合的神经网络（PINNs），可以与最近开发的阶段错误捕捉神经网络（DCNNs）一起解决受到部分数据方程式（PDEs）的最佳控制问题。这个算法是不含网点的和可扩展到不同的PDEs，并且确保控制限制。由于边界和界面条件，以及PDEs，都是转化为转量的loss函数中的软类似，因此需要同时学习它们。但是，这会导致调整这些类似的变量的问题和训练神经网络的问题。为了解决这些问题和保证数据精度，我们提议将边界和界面条件转化为硬类似的PINNs架构。这个方法可以保证边界和界面条件能够精确地满足，并且与PDEs的学习分开。我们在一些elliptic和parabolic interface最佳控制问题中调查了这个方法的效率，结果显示了承认的效果。
</details></li>
</ul>
<hr>
<h2 id="Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model"><a href="#Generating-observation-guided-ensembles-for-data-assimilation-with-denoising-diffusion-probabilistic-model" class="headerlink" title="Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model"></a>Generating observation guided ensembles for data assimilation with denoising diffusion probabilistic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06708">http://arxiv.org/abs/2308.06708</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yasahi-hpc/generative-enkf">https://github.com/yasahi-hpc/generative-enkf</a></li>
<li>paper_authors: Yuuichi Asahi, Yuta Hasegawa, Naoyuki Onodera, Takashi Shimokawabe, Hayato Shiba, Yasuhiro Idomura</li>
<li>for: 本文提出了一种 ensemble data assimilation 方法，使用 pseudo ensemble 生成的推 diffusion  probabilistic model。</li>
<li>methods: 本方法使用 trained 模型生成不寻常的 ensemble，通过 variance 在 ensemble 中的差异来提高数据融合的性能。</li>
<li>results: 对比传统 ensemble data assimilation 方法，本方法在 simulation model 不完美的情况下显示更好的性能。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper presents an ensemble data assimilation method using the pseudo ensembles generated by denoising diffusion probabilistic model. Since the model is trained against noisy and sparse observation data, this model can produce divergent ensembles close to observations. Thanks to the variance in generated ensembles, our proposed method displays better performance than the well-established ensemble data assimilation method when the simulation model is imperfect.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种ensemble数据融合方法，使用 Pseudo Ensemble 生成的随机模型。由于模型在噪声和缺失观测数据上训练，因此可以生成具有较高差异的 ensemble，使得我们的提议方法在模型不完美的情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods"><a href="#Understanding-the-robustness-difference-between-stochastic-gradient-descent-and-adaptive-gradient-methods" class="headerlink" title="Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods"></a>Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06703">http://arxiv.org/abs/2308.06703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avery Ma, Yangchen Pan, Amir-massoud Farahmand</li>
<li>For: 这个论文的目的是研究权重迭代法和适应性权重迭代法在训练深度神经网络时的表现。* Methods: 这个论文使用了随机梯度下降（SGD）和适应性梯度下降（Adam、RMSProp）等方法来训练深度神经网络。* Results: 研究发现，使用SGD训练的神经网络比使用适应性梯度下降方法更具robustness，并且在输入变化时表现更好。此外，研究还发现了自然 dataset 中的无关频率，并证明了使用适应性梯度下降方法可能会导致模型对这些变化敏感。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam and RMSProp, have been widely used in training deep neural networks. We empirically show that while the difference between the standard generalization performance of models trained using these methods is small, those trained using SGD exhibit far greater robustness under input perturbations. Notably, our investigation demonstrates the presence of irrelevant frequencies in natural datasets, where alterations do not affect models' generalization performance. However, models trained with adaptive methods show sensitivity to these changes, suggesting that their use of irrelevant frequencies can lead to solutions sensitive to perturbations. To better understand this difference, we study the learning dynamics of gradient descent (GD) and sign gradient descent (signGD) on a synthetic dataset that mirrors natural signals. With a three-dimensional input space, the models optimized with GD and signGD have standard risks close to zero but vary in their adversarial risks. Our result shows that linear models' robustness to $\ell_2$-norm bounded changes is inversely proportional to the model parameters' weight norm: a smaller weight norm implies better robustness. In the context of deep learning, our experiments show that SGD-trained neural networks show smaller Lipschitz constants, explaining the better robustness to input perturbations than those trained with adaptive gradient methods.
</details>
<details>
<summary>摘要</summary>
Stochastic gradient descent (SGD) 和 adaptive gradient 方法，如 Adam 和 RMSProp，在深度神经网络训练中广泛应用。我们实验显示，虽然这些方法的标准化预测性表现相似，但SGD 训练的模型在输入扰动下的Robustness 表现强度远胜 adaptive 方法。尤其是，我们的调查发现自然数据集中存在无关频率，其变化不会影响模型的预测性表现。然而，使用 adaptive 方法训练的模型对这些变化具有敏感性，表明它们使用无关频率可能会导致敏感解决方案。为了更好地理解这种差异，我们研究了梯度 descend (GD) 和签名梯度 descend (signGD) 在人工数据集上的学习动态。在三维输入空间中，使用 GD 和 signGD 优化的模型具有标准风险几乎为零，但它们在 $\ell_2$-norm 约束的变化下的风险异常大。我们的结果表明，线性模型对 $\ell_2$-norm 约束的变化的Robustness 与模型参数的权重 нор呈反比关系：小权重 нор呈指示更好的Robustness。在深度学习中，我们的实验表明，SGD 训练的神经网络显示更小的 Lipschitz 常数，解释它们在输入扰动下的更好的Robustness 比 adaptive 方法训练的模型。
</details></li>
</ul>
<hr>
<h2 id="Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection"><a href="#Camouflaged-Image-Synthesis-Is-All-You-Need-to-Boost-Camouflaged-Detection" class="headerlink" title="Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection"></a>Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06701">http://arxiv.org/abs/2308.06701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Zhang, Can Qin, Yu Yin, Yun Fu</li>
<li>for: 提高深度学习模型检测掩蔽物的能力</li>
<li>methods: 使用生成模型生成真实的掩蔽图像，用于训练现有的物体检测模型</li>
<li>results: 在三个数据集（COD10k、CAMO和CHAMELEON）上表现出色，超过当前状态的方法表现，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-and-play data generation and augmentation module for existing camouflaged object detection tasks and provides a novel way to introduce more diversity and distributions into current camouflage datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>隐形对象在自然场景中摒除成为深度学习模型检测和生成的挑战。隐形对象检测是计算机视觉中重要的应用领域之一，但这一研究领域受到有限的数据可用性的限制。我们提出了一种框架，用于增强自然场景中隐形对象的检测。我们的方法使用生成模型生成真实的隐形图像，这些图像可以用来训练现有的对象检测模型。我们的框架在三个数据集（COD10k、CAMO和CHAMELEON）上表现出比前一个状态的方法更高的性能，这说明了我们的方法的有效性。这种方法可以作为现有隐形对象检测任务的数据生成和扩展模块，并提供一种新的多样性和分布的引入方式，以提高当前的隐形图像数据集。
</details></li>
</ul>
<hr>
<h2 id="SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency"><a href="#SimMatchV2-Semi-Supervised-Learning-with-Graph-Consistency" class="headerlink" title="SimMatchV2: Semi-Supervised Learning with Graph Consistency"></a>SimMatchV2: Semi-Supervised Learning with Graph Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06692">http://arxiv.org/abs/2308.06692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingkai-zheng/simmatchv2">https://github.com/mingkai-zheng/simmatchv2</a></li>
<li>paper_authors: Mingkai Zheng, Shan You, Lang Huang, Chen Luo, Fei Wang, Chen Qian, Chang Xu</li>
<li>for: This paper is written for the purpose of proposing a new semi-supervised learning algorithm called SimMatchV2, which can improve the performance of image classification tasks with limited labeled data.</li>
<li>methods: The SimMatchV2 algorithm uses a graph-based approach to formulate various consistencies between labeled and unlabeled data, and utilizes a message passing mechanism to improve the performance of the algorithm.</li>
<li>results: The paper reports that SimMatchV2 achieves state-of-the-art performance on multiple semi-supervised learning benchmarks, with Top-1 Accuracy of 71.9% and 76.2% on ImageNet using 1% and 10% labeled examples, respectively.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍一种新的半监督学习算法SimMatchV2，可以在有限的标注数据的情况下提高图像分类任务的性能。</li>
<li>methods: SimMatchV2算法使用图表的方式来定义各种半监督数据之间的一致性，并利用消息传递机制来提高算法的性能。</li>
<li>results: 论文报告SimMatchV2在多个半监督学习benchmark上达到了状态的最佳性能，ImageNet上使用1%和10%标注样本时，Top-1准确率分别达到71.9%和76.2%。<details>
<summary>Abstract</summary>
Semi-Supervised image classification is one of the most fundamental problem in computer vision, which significantly reduces the need for human labor. In this paper, we introduce a new semi-supervised learning algorithm - SimMatchV2, which formulates various consistency regularizations between labeled and unlabeled data from the graph perspective. In SimMatchV2, we regard the augmented view of a sample as a node, which consists of a label and its corresponding representation. Different nodes are connected with the edges, which are measured by the similarity of the node representations. Inspired by the message passing and node classification in graph theory, we propose four types of consistencies, namely 1) node-node consistency, 2) node-edge consistency, 3) edge-edge consistency, and 4) edge-node consistency. We also uncover that a simple feature normalization can reduce the gaps of the feature norm between different augmented views, significantly improving the performance of SimMatchV2. Our SimMatchV2 has been validated on multiple semi-supervised learning benchmarks. Notably, with ResNet-50 as our backbone and 300 epochs of training, SimMatchV2 achieves 71.9\% and 76.2\% Top-1 Accuracy with 1\% and 10\% labeled examples on ImageNet, which significantly outperforms the previous methods and achieves state-of-the-art performance. Code and pre-trained models are available at \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2}.
</details>
<details>
<summary>摘要</summary>
“半支持学习图像分类是计算机视觉领域中最基本的问题之一，它可以大幅减少人工劳动。在这篇论文中，我们介绍了一种新的半支持学习算法——SimMatchV2，它通过图形视角来形式化各种一致性 regularization。在SimMatchV2中，我们将每个样本的扩展视图视为一个节点，该节点包含标签和其对应的表示。不同的节点之间连接起来，这些连接由节点表示的相似度来度量。受图形理论中的消息传递和节点分类的启发，我们提出了四种一致性，namely 1) 节点-节点一致性、2) 节点-边一致性、3) 边-边一致性、4) 边-节点一致性。我们还发现，一个简单的特征 нормализа可以大幅减少不同扩展视图之间的特征 нор值差距，从而显著提高 SimMatchV2 的性能。我们的 SimMatchV2 在多个半支持学习 benchmark 上进行验证，与 ResNet-50 作为 backing 和 300  epoch 训练，SimMatchV2 在 ImageNet 上 achiev 71.9% 和 76.2% Top-1 Accuracy with 1% 和 10% 标注样本，在前一些方法中显著超越，实现了状态的杰出性。代码和预训练模型可以在 \href{https://github.com/mingkai-zheng/SimMatchV2}{https://github.com/mingkai-zheng/SimMatchV2} 上获取。”
</details></li>
</ul>
<hr>
<h2 id="MDB-Interactively-Querying-Datasets-and-Models"><a href="#MDB-Interactively-Querying-Datasets-and-Models" class="headerlink" title="MDB: Interactively Querying Datasets and Models"></a>MDB: Interactively Querying Datasets and Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06686">http://arxiv.org/abs/2308.06686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaditya Naik, Adam Stein, Yinjun Wu, Eric Wong, Mayur Naik</li>
<li>for: 本文为了帮助开发人员系统地调试机器学习模型中出现的错误。</li>
<li>methods: 本文使用了函数编程和关系代数来构建对数据集和模型预测数据进行表达的查询框架。</li>
<li>results: 我们的实验表明，使用MDB可以比其他基elines快速40%短 queries，并且开发人员可以成功地构建复杂的查询来描述机器学习模型的错误。<details>
<summary>Abstract</summary>
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
</details>
<details>
<summary>摘要</summary>
developers 需要可以系统地调试机器学习管道中出现的错误。我们提出了MDB，一个用于交互式查询数据集和模型的调试框架。MDB将函数编程与关系代数结合起来，以建立表达性的查询数据集和模型预测中的问题。查询可重用和容易修改，允许调试者快速 iterate和细化查询，以描述和Characterize错误和模型行为。我们在对自动驾驶视频、大语言模型和医疗记录进行对象检测、偏见探测、图像分类和数据填充任务中进行了实验，结果显示MDB可以提高查询速度和查询长度，相比于其他基elines。在用户研究中，我们发现开发者可以成功地构建复杂的查询，以描述机器学习模型的错误。
</details></li>
</ul>
<hr>
<h2 id="Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations"><a href="#Separable-Gaussian-Neural-Networks-Structure-Analysis-and-Function-Approximations" class="headerlink" title="Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations"></a>Separable Gaussian Neural Networks: Structure, Analysis, and Function Approximations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06679">http://arxiv.org/abs/2308.06679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Xing, Jianqiao Sun<br>for: 用于 tri-variate function approximations 和 complex geometry 函数近似methods: 使用 Separable Gaussian Neural Network (SGNN)，利用 Gaussian 函数的分离性，将输入数据拆分成多列并在平行层中逐步 feeding them into uni-variate Gaussian functionsresults: 与 GRBFNN 相比，SGNN 可以实现100倍减少计算时间，同时保持相同精度水平，并且在approximating 函数 with complex geometry 方面可以达到三个数量级更高的精度。同时，SGNN 也比 DNNs with RuLU 和 Sigmoid 函数更易于调试和优化。<details>
<summary>Abstract</summary>
The Gaussian-radial-basis function neural network (GRBFNN) has been a popular choice for interpolation and classification. However, it is computationally intensive when the dimension of the input vector is high. To address this issue, we propose a new feedforward network - Separable Gaussian Neural Network (SGNN) by taking advantage of the separable property of Gaussian functions, which splits input data into multiple columns and sequentially feeds them into parallel layers formed by uni-variate Gaussian functions. This structure reduces the number of neurons from O(N^d) of GRBFNN to O(dN), which exponentially improves the computational speed of SGNN and makes it scale linearly as the input dimension increases. In addition, SGNN can preserve the dominant subspace of the Hessian matrix of GRBFNN in gradient descent training, leading to a similar level of accuracy to GRBFNN. It is experimentally demonstrated that SGNN can achieve 100 times speedup with a similar level of accuracy over GRBFNN on tri-variate function approximations. The SGNN also has better trainability and is more tuning-friendly than DNNs with RuLU and Sigmoid functions. For approximating functions with complex geometry, SGNN can lead to three orders of magnitude more accurate results than a RuLU-DNN with twice the number of layers and the number of neurons per layer.
</details>
<details>
<summary>摘要</summary>
Gaussian-radial-basis function neural network (GRBFNN) 是一种广泛使用的插值和分类方法。然而，当输入向量维度高时，GRBFNN 的计算复杂性会增加很多。为了解决这个问题，我们提出了一个新的前向网络 - Separable Gaussian Neural Network (SGNN)，通过利用 Gaussian 函数的分离性，将输入数据分成多列，然后将其顺序地输入到由单变量 Gaussian 函数所组成的平行层中。这样的结构可以将 GRBFNN 的neuron 数量由 O(N^d) 降至 O(dN)，从而将 computacional speed 加速到 exponentially ，并且让 SGNN 在输入维度增加时阶段性地提高。此外，SGNN 可以保留 GRBFNN 的主对角线 Hessian 矩阵的主对角线，使得在梯度下降训练中可以达到相似的精度水准。实验表明，SGNN 可以在 tri-variate 函数插值中实现 100 倍的速度提升，并且保持相似的精度水准。此外，SGNN 的训练性和适配性比 DNNs  WITH RuLU 和 Sigmoid 函数更好。当插值函数具有复杂的几何结构时，SGNN 可以实现三倍的精度提升。
</details></li>
</ul>
<hr>
<h2 id="A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks"><a href="#A-deep-learning-framework-for-multi-scale-models-based-on-physics-informed-neural-networks" class="headerlink" title="A deep learning framework for multi-scale models based on physics-informed neural networks"></a>A deep learning framework for multi-scale models based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06672">http://arxiv.org/abs/2308.06672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Wang, Yanzhong Yao, Jiawei Guo, Zhiming Gao</li>
<li>for: 解决多Scale问题</li>
<li>methods: 修改损失函数，对不同级别的损失项应用不同数量的Power运算，使损失函数中各个损失项的级别相近</li>
<li>results: 能同时优化不同级别的损失项，扩展PINN的应用范围<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINN) combine deep neural networks with the solution of partial differential equations (PDEs), creating a new and promising research area for numerically solving PDEs. Faced with a class of multi-scale problems that include loss terms of different orders of magnitude in the loss function, it is challenging for standard PINN methods to obtain an available prediction. In this paper, we propose a new framework for solving multi-scale problems by reconstructing the loss function. The framework is based on the standard PINN method, and it modifies the loss function of the standard PINN method by applying different numbers of power operations to the loss terms of different magnitudes, so that the individual loss terms composing the loss function have approximately the same order of magnitude among themselves. In addition, we give a grouping regularization strategy, and this strategy can deal well with the problem which varies significantly in different subdomains. The proposed method enables loss terms with different magnitudes to be optimized simultaneously, and it advances the application of PINN for multi-scale problems.
</details>
<details>
<summary>摘要</summary>
物理学 informed neural networks (PINN) combine deep neural networks 与解决 partial differential equations (PDEs) 的解，创造了一个新的研究领域，用于数值解决 PDEs。面临多个尺度问题，其中loss function中的损失项有不同的量级，标准 PINN 方法难以获得可靠预测。在这篇论文中，我们提出了一种新的多尺度问题解决框架。该框架基于标准 PINN 方法，并对不同量级的损失项进行不同数量的幂运算，使得各个损失项组成的损失函数具有相似的量级。此外，我们还提出了一种分组常数化策略，可以有效地处理不同子区域中变化很大的问题。该方法可以同时优化不同量级的损失项，并推动 PINN 在多尺度问题上的应用。
</details></li>
</ul>
<hr>
<h2 id="Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent"><a href="#Law-of-Balance-and-Stationary-Distribution-of-Stochastic-Gradient-Descent" class="headerlink" title="Law of Balance and Stationary Distribution of Stochastic Gradient Descent"></a>Law of Balance and Stationary Distribution of Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06671">http://arxiv.org/abs/2308.06671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Ziyin, Hongchao Li, Masahito Ueda</li>
<li>for: 本文研究了Stochastic Gradient Descent（SGD）算法如何训练神经网络，特别是SGD在神经网络的高维和潜在不稳定的损失函数空间中如何导航。</li>
<li>methods: 本文使用了Symmetry的概念来研究SGD的训练过程，并证明了SGD在损失函数包含Symmetry时可以减轻损失函数的不稳定性。</li>
<li>results: 本文研究发现，SGD在深度和宽度具有某些特定的Symmetry时可以导致神经网络的站点分布具有复杂非线性现象，如相转化、破碎Ergodicity和强制转换。这些现象只存在于深度具有某些特定Symmetry的神经网络中，这表明了深度和浅度模型之间的基本区别。<details>
<summary>Abstract</summary>
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details>
<details>
<summary>摘要</summary>
SGD算法是我们用来训练神经网络的算法，但是它在神经网络的高度不对称和缺失散射的损失函数空间中 Navigation remains poorly understood. In this work, we prove that SGD的小批量噪声规范化解决方案带有批处理的散射过程，当损失函数具有扩大Symmetry时。由于在对称性存在时，SGD动力学与批处理的差异最大，我们的理论表明损失函数的对称性是SGD工作的重要检验。我们 then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundamental difference between deep and shallow models.
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges"><a href="#Foundation-Models-in-Smart-Agriculture-Basics-Opportunities-and-Challenges" class="headerlink" title="Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges"></a>Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06668">http://arxiv.org/abs/2308.06668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/agriculture-foundation-models">https://github.com/jiajiali04/agriculture-foundation-models</a></li>
<li>paper_authors: Jiajia Li, Mingle Xu, Lirong Xiang, Dong Chen, Weichao Zhuang, Xunyuan Yin, Zhaojian Li</li>
<li>for: 这项研究旨在探索基于 Machine Learning 和 Deep Learning 的智能农业领域中的Foundation Models（基础模型）的潜力。</li>
<li>methods: 我们首先回顾了最新的基础模型在通用计算机科学领域，并将它们分为四类：语言基础模型、视觉基础模型、多modal基础模型以及强化学习基础模型。然后，我们详细介绍了在农业领域开发基础模型的过程，以及它们在智能农业中的潜在应用。</li>
<li>results: 通过本研究，我们对智能农业领域中基础模型的应用提出了新的研究方向，并提供了一个概念工具和技术背景来促进理解问题空间和探索新的研究方向。此外，我们还讨论了在开发基础模型时存在的独特挑战，包括模型训练、验证和部署。通过这项研究，我们对农业 AI 系统的发展作出了贡献，并介绍了基础模型作为一种可能地减少大量标注数据的潜在解决方案。<details>
<summary>Abstract</summary>
The past decade has witnessed the rapid development of ML and DL methodologies in agricultural systems, showcased by great successes in variety of agricultural applications. However, these conventional ML/DL models have certain limitations: They heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, foundation models have demonstrated remarkable successes in language and vision tasks across various domains. These models are trained on a vast amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field. To this end, we first review recent FMs in the general computer science domain and categorize them into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Subsequently, we outline the process of developing agriculture FMs and discuss their potential applications in smart agriculture. We also discuss the unique challenges associated with developing AFMs, including model training, validation, and deployment. Through this study, we contribute to the advancement of AI in agriculture by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.
</details>
<details>
<summary>摘要</summary>
过去一代，机器学习（ML）和深度学习（DL）方法在农业系统中得到了迅速发展，其中很多成果在各种农业应用中得到了证明。然而，传统的ML/DL模型具有一些局限性：它们需要大量、昂贵的标注数据进行训练，需要专业的技术人员进行开发和维护，而且主要针对特定任务，缺乏普适性。最近，基础模型（Foundation Models，FMs）在语言和视觉任务中获得了非常成功的结果，它们在多个领域和模式上训练，并且可以通过微调和少量任务特定的标注数据来完成多种任务。Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture fields. Therefore, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, we present conceptual tools and technical background to facilitate the understanding of the problem space and uncover new research directions in this field.首先，我们回顾了最近的FMs在通用计算机科学领域中的发展，并将它们分为四类：语言FMs、视觉FMs、多模式FMs和强化学习FMs。然后，我们详细介绍了农业FMs的开发过程，并讨论了它们在智能农业中的潜在应用。我们还讨论了开发农业FMs的独特挑战，包括模型训练、验证和部署。通过本研究，我们对农业AI的发展做出了贡献，通过引入AFMs作为一种可靠的替代方案，以减少对广泛标注数据的依赖，提高农业AI系统的效率、有效性和普适性。
</details></li>
</ul>
<hr>
<h2 id="ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN"><a href="#ALGAN-Time-Series-Anomaly-Detection-with-Adjusted-LSTM-GAN" class="headerlink" title="ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN"></a>ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06663">http://arxiv.org/abs/2308.06663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abul Bashar, Richi Nayak</li>
<li>for: 这篇论文目的是提出一个新的生成对抗网络模型（ALGAN），用于不监控的时间序列资料中的异常检测。</li>
<li>methods: 这篇论文使用的方法是基于LSTM网络的对抗网络（GAN）模型，并且对输出进行调整以提高异常检测精度。</li>
<li>results: 根据实验结果，ALGAN在46个真实世界单 Variate时间序列数据集和多个领域的大量多 Variate时间序列数据集上的异常检测精度较高，比较传统、神经网络基于的和其他GAN型方法更好。<details>
<summary>Abstract</summary>
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
</details>
<details>
<summary>摘要</summary>
《时序数据异常检测使用生成对抗网络》Introduction:时序数据异常检测是各个领域的常见问题，如制造、医疗影像和网络安全等。在这些领域中，检测时序数据中异常点的异常行为是非常重要的。Recently, Generative Adversarial Networks (GANs) have been shown to be effective in detecting anomalies in time series data. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting.Methodology:我们的方法包括以下几个部分：1. 生成对抗网络模型（GAN）的概述2. 基于LSTM网络的异常检测模型（ALGAN）的提出3. 实验设计和结果分析Results:我们对46个实际时序数据集进行了实验，并对多个领域的大量多变量时序数据进行了分析。结果表明，ALGAN在无监督的情况下，对时序数据中异常点的检测性能有显著提高。在单变量和多变量时序数据中，ALGAN都能够准确地检测异常点。Conclusion:本文提出了一种基于GAN的新方法，可以在无监督的情况下，提高时序数据中异常点的检测性能。我们的实验结果表明，ALGAN在多个领域中都能够准确地检测异常点。这种方法可以广泛应用于各个领域中的时序数据异常检测问题。
</details></li>
</ul>
<hr>
<h2 id="Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features"><a href="#Benign-Shortcut-for-Debiasing-Fair-Visual-Recognition-via-Intervention-with-Shortcut-Features" class="headerlink" title="Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features"></a>Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08482">http://arxiv.org/abs/2308.08482</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiiizhang/shortcutDebiasing">https://github.com/yiiizhang/shortcutDebiasing</a></li>
<li>paper_authors: Yi Zhang, Jitao Sang, Junyang Wang, Dongmei Jiang, Yaowei Wang</li>
<li>for: 降低机器学习模型中的偏见风险，特别是在社会应用中，如雇用、银行和刑事司法等领域。</li>
<li>methods: 我们提出了一种新的短路减震方法（Shortcut Debiasing），通过在训练阶段将偏见特征转换为短路特征，然后使用 causal intervention 来消除短路特征 durante la inferencia。</li>
<li>results: 我们在多个 benchmark 数据集上应用了短路减震方法，并实现了与状态前的减震方法相比的显著改善 both accuracy 和 fairness。<details>
<summary>Abstract</summary>
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate shortcut features during inference. The key idea of \emph{Shortcut Debiasing} is to design controllable shortcut features to on one hand replace bias features in contributing to the target task during the training stage, and on the other hand be easily removed by intervention during the inference stage. This guarantees the learning of the target task does not hinder the elimination of bias features. We apply \emph{Shortcut Debiasing} to several benchmark datasets, and achieve significant improvements over the state-of-the-art debiasing methods in both accuracy and fairness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks"><a href="#Polar-Collision-Grids-Effective-Interaction-Modelling-for-Pedestrian-Trajectory-Prediction-in-Shared-Space-Using-Collision-Checks" class="headerlink" title="Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks"></a>Polar Collision Grids: Effective Interaction Modelling for Pedestrian Trajectory Prediction in Shared Space Using Collision Checks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06654">http://arxiv.org/abs/2308.06654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 预测步行者的轨迹是自动驾驶汽车安全导航中的关键能力，特别是在与步行者共享空间时。步行者在共享空间中的运动受到汽车和其他步行者的影响，因此，可以准确地模拟步行者-汽车和步行者-步行者的互动，可以提高步行者轨迹预测模型的准确性。</li>
<li>methods: 我们提出了一种基于启发的方法，通过计算碰撞风险来选择互动对象。我们将关注与目标步行者之间的碰撞风险，并使用时间到碰撞和两个对象的接近方向角来编码互动效果。我们还提出了一种新的极天球碰撞网格图，以便更好地表示互动效果。</li>
<li>results: 我们的结果表明，使用我们提出的方法可以比基eline方法更加准确地预测步行者的轨迹，特别是在HBS数据集上。<details>
<summary>Abstract</summary>
Predicting pedestrians' trajectories is a crucial capability for autonomous vehicles' safe navigation, especially in spaces shared with pedestrians. Pedestrian motion in shared spaces is influenced by both the presence of vehicles and other pedestrians. Therefore, effectively modelling both pedestrian-pedestrian and pedestrian-vehicle interactions can increase the accuracy of the pedestrian trajectory prediction models. Despite the huge literature on ways to encode the effect of interacting agents on a pedestrian's predicted trajectory using deep-learning models, limited effort has been put into the effective selection of interacting agents. In the majority of cases, the interaction features used are mainly based on relative distances while paying less attention to the effect of the velocity and approaching direction in the interaction formulation. In this paper, we propose a heuristic-based process of selecting the interacting agents based on collision risk calculation. Focusing on interactions of potentially colliding agents with a target pedestrian, we propose the use of time-to-collision and the approach direction angle of two agents for encoding the interaction effect. This is done by introducing a novel polar collision grid map. Our results have shown predicted trajectories closer to the ground truth compared to existing methods (used as a baseline) on the HBS dataset.
</details>
<details>
<summary>摘要</summary>
预测行人轨迹是自动驾驶车辆安全导航中的关键能力，特别是在与行人共享空间时。行人运动在共享空间中受到车辆和其他行人的影响。因此，可以准确模拟行人与其他行人和车辆之间的互动，以提高行人轨迹预测模型的准确性。尽管有庞大的文献关于使用深度学习模型来编码互动对行人预测轨迹的影响，但是有限的努力被投入到有效选择互动者方面。在大多数情况下，互动特征主要基于相对距离，而忽略了互动形式中速度和接近方向的效果。在这篇文章中，我们提出了一种基于碰撞风险计算的互动者选择规则。我们关注了可能碰撞的两个代理人之间的时间到碰撞和接近方向角的互动效果。我们通过引入一种新的极地碰撞格图来实现这一点。我们的结果显示，与基eline方法相比，我们的方法在HBS数据集上预测轨迹更加准确。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation"><a href="#Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation" class="headerlink" title="Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"></a>Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06644">http://arxiv.org/abs/2308.06644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation">https://github.com/jwrh/Accelerating-Diffusion-based-Combinatorial-Optimization-Solvers-by-Progressive-Distillation</a></li>
<li>paper_authors: Junwei Huang, Zhiqing Sun, Yiming Yang</li>
<li>for: 提高NP-完全 combinatorial优化问题的解决速度</li>
<li>methods: 使用进步压缩来减少推理步骤数</li>
<li>results: 在TSP-50 dataset上，提高推理速度16倍，性能下降0.019%<details>
<summary>Abstract</summary>
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
</details>
<details>
<summary>摘要</summary>
GRAPH-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition"><a href="#Advances-in-Self-Supervised-Learning-for-Synthetic-Aperture-Sonar-Data-Processing-Classification-and-Pattern-Recognition" class="headerlink" title="Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition"></a>Advances in Self-Supervised Learning for Synthetic Aperture Sonar Data Processing, Classification, and Pattern Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11633">http://arxiv.org/abs/2308.11633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Sheffield, Frank E. Bobe III, Bradley Marchand, Matthew S. Emigh</li>
<li>for: 本研究提出了一种基于自助学习的SAS数据处理方法，以解决SAS数据处理中缺乏标注数据的问题。</li>
<li>methods: 本研究使用了MoCo-SAS方法，即基于自助学习的SAS数据处理、分类和模式识别方法。</li>
<li>results: 实验结果表明，MoCo-SAS方法在SAS数据处理中显著超过了传统的指导学习方法，并且在F1分数方面得到了显著改善。这些发现提出了使用自助学习进行SAS数据处理的潜在可能性，并且对水下对象检测和分类提供了新的思路。<details>
<summary>Abstract</summary>
Synthetic Aperture Sonar (SAS) imaging has become a crucial technology for underwater exploration because of its unique ability to maintain resolution at increasing ranges, a characteristic absent in conventional sonar techniques. However, the effective application of deep learning to SAS data processing is often limited due to the scarcity of labeled data. To address this challenge, this paper proposes MoCo-SAS that leverages self-supervised learning (SSL) for SAS data processing, classification, and pattern recognition. The experimental results demonstrate that MoCo-SAS significantly outperforms traditional supervised learning methods, as evidenced by significant improvements observed in terms of the F1-score. These findings highlight the potential of SSL in advancing the state-of-the-art in SAS data processing, offering promising avenues for enhanced underwater object detection and classification.
</details>
<details>
<summary>摘要</summary>
这篇研究论文提出了一个名为MoCo-SAS的自动学习方法，用于对水下探索中的Synthetic Aperture Sonar（SAS）数据进行处理、分类和图像识别。这种方法利用自动学习的自我指导学习（SSL）技术，以提高SAS数据处理的精度和效率。实验结果显示，MoCo-SAS方法与传统的超级vised learning方法相比，有着明显的改善，特别是在F1分数上。这些结果显示出SSL在SAS数据处理中的应用潜力，并开启了更进一步的水下物体探测和分类技术的可能性。
</details></li>
</ul>
<hr>
<h2 id="ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss"><a href="#ADRMX-Additive-Disentanglement-of-Domain-Features-with-Remix-Loss" class="headerlink" title="ADRMX: Additive Disentanglement of Domain Features with Remix Loss"></a>ADRMX: Additive Disentanglement of Domain Features with Remix Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06624">http://arxiv.org/abs/2308.06624</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berkerdemirel/ADRMX">https://github.com/berkerdemirel/ADRMX</a></li>
<li>paper_authors: Berker Demirel, Erchan Aptoula, Huseyin Ozkan</li>
<li>for: 这个论文的目的是解决多个源领域中模型在新未经见过的领域中的泛化问题。</li>
<li>methods: 这个论文提出了一种新的架构 named Additive Disentanglement of Domain Features with Remix Loss (ADRMX)，它使用了添加式分解策略将域特征与域 invariants 相结合，以提高模型的泛化能力。</li>
<li>results: 经过广泛的实验，ADRMX 在 DomainBed 上实现了最佳性能。<details>
<summary>Abstract</summary>
The common assumption that train and test sets follow similar distributions is often violated in deployment settings. Given multiple source domains, domain generalization aims to create robust models capable of generalizing to new unseen domains. To this end, most of existing studies focus on extracting domain invariant features across the available source domains in order to mitigate the effects of inter-domain distributional changes. However, this approach may limit the model's generalization capacity by relying solely on finding common features among the source domains. It overlooks the potential presence of domain-specific characteristics that could be prevalent in a subset of domains, potentially containing valuable information. In this work, a novel architecture named Additive Disentanglement of Domain Features with Remix Loss (ADRMX) is presented, which addresses this limitation by incorporating domain variant features together with the domain invariant ones using an original additive disentanglement strategy. Moreover, a new data augmentation technique is introduced to further support the generalization capacity of ADRMX, where samples from different domains are mixed within the latent space. Through extensive experiments conducted on DomainBed under fair conditions, ADRMX is shown to achieve state-of-the-art performance. Code will be made available at GitHub after the revision process.
</details>
<details>
<summary>摘要</summary>
通常假设训练集和测试集遵循类似的分布是在部署场景下不成立。给定多个源领域，领域泛化目标是创建抗辐射的模型，以便在新未看过的领域中进行泛化。然而，现有的研究通常是通过找到源领域中共同的特征来减轻交领域分布变化的影响。这可能会限制模型的泛化能力，因为它仅仅依据源领域中共同的特征来泛化。这些研究忽略了可能存在的领域特有特征，这些特征可能在一些领域中占据主导地位，并且可能包含有价值信息。在这项工作中，一种新的架构名为加法解决方案（ADRMX）被提出，它解决了这一限制，通过将领域特征和领域不变特征结合在一起使用一种原始的加法解决方案。此外，一种新的数据增强技术也被引入，以支持ADRMX的泛化能力，其中各个领域的样本在离散空间中混合。经过对DomainBed进行了广泛的实验，ADRMX被证明可以在公正的条件下实现状态的泛化性能。代码将在GitHub上公布 после修订过程。
</details></li>
</ul>
<hr>
<h2 id="Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks"><a href="#Can-Unstructured-Pruning-Reduce-the-Depth-in-Deep-Neural-Networks" class="headerlink" title="Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?"></a>Can Unstructured Pruning Reduce the Depth in Deep Neural Networks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06619">http://arxiv.org/abs/2308.06619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Liao, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione</li>
<li>for: 降低深度神经网络的大小 while maintaining performance</li>
<li>methods: 引入Entropy Guided Pruning（EGP）算法，优先采用含有低 entropy 的连接进行剪除</li>
<li>results: 实验结果表明，EGP可以有效地剪除深度神经网络，同时保持竞争性性能水平<details>
<summary>Abstract</summary>
Pruning is a widely used technique for reducing the size of deep neural networks while maintaining their performance. However, such a technique, despite being able to massively compress deep models, is hardly able to remove entire layers from a model (even when structured): is this an addressable task? In this study, we introduce EGP, an innovative Entropy Guided Pruning algorithm aimed at reducing the size of deep neural networks while preserving their performance. The key focus of EGP is to prioritize pruning connections in layers with low entropy, ultimately leading to their complete removal. Through extensive experiments conducted on popular models like ResNet-18 and Swin-T, our findings demonstrate that EGP effectively compresses deep neural networks while maintaining competitive performance levels. Our results not only shed light on the underlying mechanism behind the advantages of unstructured pruning, but also pave the way for further investigations into the intricate relationship between entropy, pruning techniques, and deep learning performance. The EGP algorithm and its insights hold great promise for advancing the field of network compression and optimization. The source code for EGP is released open-source.
</details>
<details>
<summary>摘要</summary>
《剪除技术在深度神经网络中减小大小而保持性能的应用广泛。然而，这种技术，即使能够压缩深度模型，几乎不能完全从模型中移除整层（即使结构化）：是这个任务可解决吗？在这项研究中，我们介绍了EGP算法，一种基于熵指导的剪除算法，用于减小深度神经网络大小，保持性能水平。EGP的关键焦点是优先剪除层次熵低的连接，从而导致其完全移除。经过广泛的实验，我们发现EGP能够有效地减小深度神经网络，同时保持竞争性能水平。我们的结果不仅揭示了剪除技术的优势，还探讨了剪除、熵和深度学习性能之间的复杂关系。EGP算法和其理解拥有推动深度网络压缩和优化领域的前景。EGP算法的源代码已经公开发布。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness"><a href="#On-the-Interplay-of-Convolutional-Padding-and-Adversarial-Robustness" class="headerlink" title="On the Interplay of Convolutional Padding and Adversarial Robustness"></a>On the Interplay of Convolutional Padding and Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06612">http://arxiv.org/abs/2308.06612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Gavrikov, Janis Keuper</li>
<li>for: 本文旨在研究 padding 对 adversarial attack 的影响，以及不同 padding 模式对 adversarial robustness 的影响。</li>
<li>methods: 本文使用了 convolutional neural network (CNN) 和 adversarial attack 的方法，并进行了 padding 的分析和对比。</li>
<li>results: 本文发现了 perturbation anomalies 在图像边缘区域，这些区域是 padding 的应用区域。此外，本文还发现了不同 padding 模式对 adversarial robustness 的影响。<details>
<summary>Abstract</summary>
It is common practice to apply padding prior to convolution operations to preserve the resolution of feature-maps in Convolutional Neural Networks (CNN). While many alternatives exist, this is often achieved by adding a border of zeros around the inputs. In this work, we show that adversarial attacks often result in perturbation anomalies at the image boundaries, which are the areas where padding is used. Consequently, we aim to provide an analysis of the interplay between padding and adversarial attacks and seek an answer to the question of how different padding modes (or their absence) affect adversarial robustness in various scenarios.
</details>
<details>
<summary>摘要</summary>
通常情况下，在卷积神经网络（CNN）中，会将padding应用于特征地图以保持其分辨率。虽然有很多替代方案，通常是通过添加边界上的零值来实现。在这项工作中，我们发现了对抗攻击通常会在图像边界上产生异常的扰动，这是padding使用的区域。因此，我们想进行对padding和对抗攻击之间的交互分析，并查找不同的padding模式（或其缺失）对对抗鲁棒性在不同的场景中的影响。
</details></li>
</ul>
<hr>
<h2 id="LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net"><a href="#LadleNet-Translating-Thermal-Infrared-Images-to-Visible-Light-Images-Using-A-Scalable-Two-stage-U-Net" class="headerlink" title="LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net"></a>LadleNet: Translating Thermal Infrared Images to Visible Light Images Using A Scalable Two-stage U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06603">http://arxiv.org/abs/2308.06603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ach-1914/ladlenet">https://github.com/ach-1914/ladlenet</a></li>
<li>paper_authors: Tonghui Zou</li>
<li>for: 这篇论文的目的是为了将抛光热成像（TIR）图像转换成可见光成像（VI）图像，这个问题在各个领域都有广泛的应用，例如TIR-VI图像匹配和融合。</li>
<li>methods: 这篇论文提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net concatenation结构，并添加了跳过连接和精细特征聚合技术，从而提高了模型性能。</li>
<li>results: 在KAIST数据集上测试和分析了LadleNet和LadleNet+两种方法，结果显示，LadleNet+在图像清晰度和感知质量方面达到了当前最佳性能。<details>
<summary>Abstract</summary>
The translation of thermal infrared (TIR) images to visible light (VI) images presents a challenging task with potential applications spanning various domains such as TIR-VI image registration and fusion. Leveraging supplementary information derived from TIR image conversions can significantly enhance model performance and generalization across these applications. However, prevailing issues within this field include suboptimal image fidelity and limited model scalability. In this paper, we introduce an algorithm, LadleNet, based on the U-Net architecture. LadleNet employs a two-stage U-Net concatenation structure, augmented with skip connections and refined feature aggregation techniques, resulting in a substantial enhancement in model performance. Comprising 'Handle' and 'Bowl' modules, LadleNet's Handle module facilitates the construction of an abstract semantic space, while the Bowl module decodes this semantic space to yield mapped VI images. The Handle module exhibits extensibility by allowing the substitution of its network architecture with semantic segmentation networks, thereby establishing more abstract semantic spaces to bolster model performance. Consequently, we propose LadleNet+, which replaces LadleNet's Handle module with the pre-trained DeepLabv3+ network, thereby endowing the model with enhanced semantic space construction capabilities. The proposed method is evaluated and tested on the KAIST dataset, accompanied by quantitative and qualitative analyses. Compared to existing methodologies, our approach achieves state-of-the-art performance in terms of image clarity and perceptual quality. The source code will be made available at https://github.com/Ach-1914/LadleNet/tree/main/.
</details>
<details>
<summary>摘要</summary>
《热传 инфра红（TIR）图像到可见光（VI）图像的翻译 задача具有各种应用领域的潜在投入，如TIR-VI图像匹配和融合。利用TIR图像转换得到的补充信息可以显著提高模型性能和泛化性。然而，现有的问题包括低效图像准确性和限制模型可扩展性。在本文中，我们提出了一种算法，即LadleNet，基于U-Net架构。LadleNet使用了两个阶段的U-Net叠加结构，并添加了跳跃连接和细化特征聚合技术，从而实现了模型性能的明显提高。LadleNet由“托”和“碗”模块组成，其中“托”模块建立了一个抽象 semantic space，而“碗”模块将这个 semantic space 转换为生成的VI图像。“托”模块具有可扩展性，可以将其网络架构替换为semantic segmentation网络，从而建立更加抽象的semantic space，进一步提高模型性能。因此，我们提出了LadleNet+，其将LadleNet的“托”模块替换为预训练的DeepLabv3+网络，从而增强模型的semantic space建构能力。我们的方法在KAIST数据集上进行评估和测试，并通过量化和质量分析进行比较。与现有方法相比，我们的方法在图像清晰度和感知质量上达到了状态 искусственный的性能。代码将在https://github.com/Ach-1914/LadleNet/tree/main/中提供。》
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/cs.LG_2023_08_13/" data-id="clly4xtdw006rvl8824eaf05x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_13" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/13/eess.IV_2023_08_13/" class="article-date">
  <time datetime="2023-08-12T16:00:00.000Z" itemprop="datePublished">2023-08-13</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/13/eess.IV_2023_08_13/">eess.IV - 2023-08-13 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature"><a href="#Shape-guided-Conditional-Latent-Diffusion-Models-for-Synthesising-Brain-Vasculature" class="headerlink" title="Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature"></a>Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06781">http://arxiv.org/abs/2308.06781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Deo, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila</li>
<li>For: The paper aims to generate realistic 3D segmentations of the Circle of Willis (CoW) using a conditional latent diffusion model with shape and anatomical guidance, in order to advance research on cerebrovascular diseases and refine clinical interventions.* Methods: The authors propose a novel generative approach using a conditional latent diffusion model, which incorporates shape guidance to better preserve vessel continuity and demonstrate superior performance compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE.* Results: The authors observed that their model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches, with an FID score 53% better than the best-performing GAN-based model.Here are the three points in Simplified Chinese text:* For: 本研究旨在使用 conditional latent diffusion model 生成真实的 3D CoW 分割，以提高脑血管疾病研究和临床 interven 的技术水平。* Methods: 作者们提出了一种新的生成方法，使用 conditional latent diffusion model，该模型具有形态指导，以更好地保持血管连续性。* Results: 作者们发现，他们的模型可以生成更加真实的 CoW 变体，并且与其他方法相比，有 53% 更高的视觉质量。<details>
<summary>Abstract</summary>
The Circle of Willis (CoW) is the part of cerebral vasculature responsible for delivering blood to the brain. Understanding the diverse anatomical variations and configurations of the CoW is paramount to advance research on cerebrovascular diseases and refine clinical interventions. However, comprehensive investigation of less prevalent CoW variations remains challenging because of the dominance of a few commonly occurring configurations. We propose a novel generative approach utilising a conditional latent diffusion model with shape and anatomical guidance to generate realistic 3D CoW segmentations, including different phenotypical variations. Our conditional latent diffusion model incorporates shape guidance to better preserve vessel continuity and demonstrates superior performance when compared to alternative generative models, including conditional variants of 3D GAN and 3D VAE. We observed that our model generated CoW variants that are more realistic and demonstrate higher visual fidelity than competing approaches with an FID score 53\% better than the best-performing GAN-based model.
</details>
<details>
<summary>摘要</summary>
圆形的威廉圈（CoW）是脑血管系统中带来脑部血液的部分。了解各种不同的CoW结构和配置是研究脑血管疾病的进步和优化临床 intervención的关键。然而，对于较少seen CoW变化的全面调查仍然是一个挑战，因为一些常见的配置占据了主导地位。我们提出了一种新的生成方法，利用决定性液态扩散模型，包括形态指导来生成真实的3D CoW分割结果，包括不同的fenotipical变化。我们的决定性液态扩散模型包含形态指导，以更好地保持血管连续性，并且与其他生成模型相比，包括 conditional GAN和3D VAE的 conditional变种，显示出更高的性能。我们观察到，我们的模型生成的CoW变化更加真实，与竞争方法的FID分数比53%高。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches"><a href="#Unsupervised-Image-Denoising-in-Real-World-Scenarios-via-Self-Collaboration-Parallel-Generative-Adversarial-Branches" class="headerlink" title="Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches"></a>Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06776">http://arxiv.org/abs/2308.06776</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxin0/scpgabnet">https://github.com/linxin0/scpgabnet</a></li>
<li>paper_authors: Xin Lin, Chao Ren, Xiao Liu, Jie Huang, Yinjie Lei</li>
<li>for: 提高无监督图像净化的性能，不需要大规模的对照数据集。</li>
<li>methods: 基于生成对抗网络的不监督方法，通过多个denoiser的级联使用，逐渐提高净化性能。</li>
<li>results: 与现有无监督方法相比，提出了新的SC策略，可以减少对GAN-based净化框架的计算复杂性，同时提高图像净化性能。实验结果表明，该方法可以在无监督下实现更高的净化性能。<details>
<summary>Abstract</summary>
Deep learning methods have shown remarkable performance in image denoising, particularly when trained on large-scale paired datasets. However, acquiring such paired datasets for real-world scenarios poses a significant challenge. Although unsupervised approaches based on generative adversarial networks offer a promising solution for denoising without paired datasets, they are difficult in surpassing the performance limitations of conventional GAN-based unsupervised frameworks without significantly modifying existing structures or increasing the computational complexity of denoisers. To address this problem, we propose a SC strategy for multiple denoisers. This strategy can achieve significant performance improvement without increasing the inference complexity of the GAN-based denoising framework. Its basic idea is to iteratively replace the previous less powerful denoiser in the filter-guided noise extraction module with the current powerful denoiser. This process generates better synthetic clean-noisy image pairs, leading to a more powerful denoiser for the next iteration. This baseline ensures the stability and effectiveness of the training network. The experimental results demonstrate the superiority of our method over state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
深度学习方法在图像除噪方面表现了非常出色，特别是在大规模对应数据集上训练的情况下。然而，在实际场景中获得这些对应数据集是一项非常困难的任务。尽管使用生成对抗网络来实现无监督的推荐方法可以解决这个问题，但是这些方法往往难以超越传统的GAN基础架构下的性能限制，而且不需要明显地修改现有结构或增加推荐器的计算复杂度。为解决这个问题，我们提出了一种SC策略。这种策略可以在GAN基础架构下实现明显的性能提升，而无需增加推荐器的推理复杂度。其基本思想是在滤波器导向噪音提取模块中，iteratively替换之前的较弱推荐器，使得当前的强大推荐器可以在下一轮中使用。这个过程生成了更好的干净清噪图像对，从而导致更强大的推荐器。这个基准保证了训练网络的稳定性和效果。实验结果表明，我们的方法在无监督方法中表现出了superiority。
</details></li>
</ul>
<hr>
<h2 id="Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes"><a href="#Tissue-Segmentation-of-Thick-Slice-Fetal-Brain-MR-Scans-with-Guidance-from-High-Quality-Isotropic-Volumes" class="headerlink" title="Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes"></a>Tissue Segmentation of Thick-Slice Fetal Brain MR Scans with Guidance from High-Quality Isotropic Volumes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06762">http://arxiv.org/abs/2308.06762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Huang, Xukun Zhang, Zhiming Cui, He Zhang, Geng Chen, Dinggang Shen</li>
<li>for: 这个研究旨在提高胎儿脑 MR 影像中的组织分类精度，以便重建高精度的胎儿脑 MR 影像量和评估胎儿脑发展。</li>
<li>methods: 这个研究使用了领域适应技术，将高品质的胎儿脑 MR 影像作为指导，对厚层胎儿脑 MR 影像进行分类。</li>
<li>results: 研究结果显示，这个方法可以很好地改善胎儿脑 MR 影像中的组织分类精度，与现有的方法相比，表现更加出色。<details>
<summary>Abstract</summary>
Accurate tissue segmentation of thick-slice fetal brain magnetic resonance (MR) scans is crucial for both reconstruction of isotropic brain MR volumes and the quantification of fetal brain development. However, this task is challenging due to the use of thick-slice scans in clinically-acquired fetal brain data. To address this issue, we propose to leverage high-quality isotropic fetal brain MR volumes (and also their corresponding annotations) as guidance for segmentation of thick-slice scans. Due to existence of significant domain gap between high-quality isotropic volume (i.e., source data) and thick-slice scans (i.e., target data), we employ a domain adaptation technique to achieve the associated knowledge transfer (from high-quality <source> volumes to thick-slice <target> scans). Specifically, we first register the available high-quality isotropic fetal brain MR volumes across different gestational weeks to construct longitudinally-complete source data. To capture domain-invariant information, we then perform Fourier decomposition to extract image content and style codes. Finally, we propose a novel Cycle-Consistent Domain Adaptation Network (C2DA-Net) to efficiently transfer the knowledge learned from high-quality isotropic volumes for accurate tissue segmentation of thick-slice scans. Our C2DA-Net can fully utilize a small set of annotated isotropic volumes to guide tissue segmentation on unannotated thick-slice scans. Extensive experiments on a large-scale dataset of 372 clinically acquired thick-slice MR scans demonstrate that our C2DA-Net achieves much better performance than cutting-edge methods quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
通过借助高质量的ISO体积脑MR图像（以及其相应的标注），我们提议利用这些图像作为厚层扫描图像的指导。由于高质量ISO体积图像和厚层扫描图像之间存在域之间的差距，我们采用域适应技术来实现相关的知识传递。特别是，我们首先将可用的高质量ISO体积脑MR图像进行了 longitudinally-complete的注册，以构建不同 gestational 周的完整的源数据。然后，我们通过Fourier分解来提取图像内容和风格代码。最后，我们提出了一种名为C2DA-Net的循环相互适应域适应网络，以高效地传递高质量ISO体积图像中学习的知识来进行厚层扫描图像的精准组织分割。我们的C2DA-Net可以充分利用一小组标注的ISO体积图像来导引厚层扫描图像的组织分割。我们对372个临床获取的厚层扫描MR扫描图像进行了广泛的实验，并证明了我们的C2DA-Net在量和质量上都与当今的方法相比较为出色。
</details></li>
</ul>
<hr>
<h2 id="FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table"><a href="#FastLLVE-Real-Time-Low-Light-Video-Enhancement-with-Intensity-Aware-Lookup-Table" class="headerlink" title="FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table"></a>FastLLVE: Real-Time Low-Light Video Enhancement with Intensity-Aware Lookup Table</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06749">http://arxiv.org/abs/2308.06749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenhao-li-777/fastllve">https://github.com/wenhao-li-777/fastllve</a></li>
<li>paper_authors: Wenhao Li, Guangyang Wu, Wenyi Wang, Peiran Ren, Xiaohong Liu</li>
<li>for: 提高低光照视频质量，保持视频的时间协调性。</li>
<li>methods: 利用Look-Up-Table（LUT）技术，实现高效的低光照视频提高。设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，以适应低动态范围问题。</li>
<li>results: 实验结果表明，我们的方法在质量和时间协调性两个方面均达到了领先水平。与现有的单帧图像基于方法相比，我们的方法可以在1080p视频中实现50+帧&#x2F;秒的处理速度，并且可以保持高质量结果。<details>
<summary>Abstract</summary>
Low-Light Video Enhancement (LLVE) has received considerable attention in recent years. One of the critical requirements of LLVE is inter-frame brightness consistency, which is essential for maintaining the temporal coherence of the enhanced video. However, most existing single-image-based methods fail to address this issue, resulting in flickering effect that degrades the overall quality after enhancement. Moreover, 3D Convolution Neural Network (CNN)-based methods, which are designed for video to maintain inter-frame consistency, are computationally expensive, making them impractical for real-time applications. To address these issues, we propose an efficient pipeline named FastLLVE that leverages the Look-Up-Table (LUT) technique to maintain inter-frame brightness consistency effectively. Specifically, we design a learnable Intensity-Aware LUT (IA-LUT) module for adaptive enhancement, which addresses the low-dynamic problem in low-light scenarios. This enables FastLLVE to perform low-latency and low-complexity enhancement operations while maintaining high-quality results. Experimental results on benchmark datasets demonstrate that our method achieves the State-Of-The-Art (SOTA) performance in terms of both image quality and inter-frame brightness consistency. More importantly, our FastLLVE can process 1,080p videos at $\mathit{50+}$ Frames Per Second (FPS), which is $\mathit{2 \times}$ faster than SOTA CNN-based methods in inference time, making it a promising solution for real-time applications. The code is available at https://github.com/Wenhao-Li-777/FastLLVE.
</details>
<details>
<summary>摘要</summary>
低光照视频增强（LLVE）在过去几年内得到了广泛关注。一个重要的需求是 между帧亮度一致性，以保持视频增强后的时间一致性。然而，大多数单张图像基本方法无法解决这个问题，导致干扰效应，从而降低总质量。此外，基于3D convolutional neural network（CNN）的方法，它们是为视频维护间帧一致性而设计的，但它们计算成本高，使其在实时应用中不实际。为解决这些问题，我们提出了一个高效的排序管道，即快速LLVE，该管道利用Look-Up-Table（LUT）技术保持间帧亮度一致性。特别是，我们设计了一个可学习的Intensity-Aware LUT（IA-LUT）模块，用于自适应增强，解决低动态问题在低光照场景中。这使得快速LLVE可以在低延迟和低复杂度下进行增强操作，同时保持高质量结果。实验结果表明，我们的方法在标准测试集上达到了状态方法（SOTA）的性能，并且在帧率和亮度一致性两个指标上均有显著提高。此外，我们的快速LLVE可以处理1080P视频，并在50+帧/秒的速度下进行增强，这比SOTA CNN基本方法在推理时间上快两倍，使其成为实时应用的优秀解决方案。代码可以在https://github.com/Wenhao-Li-777/FastLLVE上获取。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising"><a href="#Self-supervised-Noise2noise-Method-Utilizing-Corrupted-Images-with-a-Modular-Network-for-LDCT-Denoising" class="headerlink" title="Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising"></a>Self-supervised Noise2noise Method Utilizing Corrupted Images with a Modular Network for LDCT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06746">http://arxiv.org/abs/2308.06746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyuan01/self-supervised-noise2noise-for-ldct">https://github.com/xyuan01/self-supervised-noise2noise-for-ldct</a></li>
<li>paper_authors: Yuting Zhu, Qiang He, Yudong Yao, Yueyang Teng</li>
<li>for: 这篇论文旨在提出一种基于单静电 Tomatoes CT（LDCT）数据的自我监督噪声降低方法，并不需要对比的噪音和清洁数据。</li>
<li>methods: 本研究使用了一种组合自我监督噪声模型和降低噪声策略，包括在LDCT图像中添加多次相似的噪音，并使用这些次生噪音做为训练数据。</li>
<li>results: 实验结果显示，提案的方法在Mayo LDCT数据集上比前一些深度学习方法更有效率。<details>
<summary>Abstract</summary>
Deep learning is a very promising technique for low-dose computed tomography (LDCT) image denoising. However, traditional deep learning methods require paired noisy and clean datasets, which are often difficult to obtain. This paper proposes a new method for performing LDCT image denoising with only LDCT data, which means that normal-dose CT (NDCT) is not needed. We adopt a combination including the self-supervised noise2noise model and the noisy-as-clean strategy. First, we add a second yet similar type of noise to LDCT images multiple times. Note that we use LDCT images based on the noisy-as-clean strategy for corruption instead of NDCT images. Then, the noise2noise model is executed with only the secondary corrupted images for training. We select a modular U-Net structure from several candidates with shared parameters to perform the task, which increases the receptive field without increasing the parameter size. The experimental results obtained on the Mayo LDCT dataset show the effectiveness of the proposed method compared with that of state-of-the-art deep learning methods. The developed code is available at https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT.
</details>
<details>
<summary>摘要</summary>
深度学习是低剂量 computed tomography（LDCT）图像减噪的非常有前途的技术。然而，传统的深度学习方法通常需要对噪声和清晰图像的对应对进行预处理，这可能具有困难。这篇论文提出了一种使用仅LDCT数据进行LDCT图像减噪的新方法。我们采用了一种组合，包括自我超级vised noise2noise模型和噪声作为清晰Strategy。首先，我们在LDCT图像中添加了多个类似的噪声。请注意，我们使用LDCT图像来实现损害代替NDCT图像。然后，我们在噪声模型中进行训练，使用只有次噪声图像。我们选择了一种模块化U-Net结构，其中共享参数来完成任务，这将增加了感知场景而不会增加参数的大小。我们在Mayo LDCT数据集上进行实验，并证明了提议方法的有效性，比对已有的深度学习方法更高。开发的代码可以在https://github.com/XYuan01/Self-supervised-Noise2Noise-for-LDCT中找到。
</details></li>
</ul>
<hr>
<h2 id="Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation"><a href="#Polyp-SAM-Can-A-Text-Guided-SAM-Perform-Better-for-Polyp-Segmentation" class="headerlink" title="Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?"></a>Polyp-SAM++: Can A Text Guided SAM Perform Better for Polyp Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06623">http://arxiv.org/abs/2308.06623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RisabBiswas/Polyp-SAM-PlusPlus">https://github.com/RisabBiswas/Polyp-SAM-PlusPlus</a></li>
<li>paper_authors: Risab Biswas</li>
<li>for: 本研究旨在提高肠Rectal cancer的诊断和治疗，通过使用文本提示来改进SAM模型，以提高肠脏膜膜蛋白分 segmentation的精度和稳定性。</li>
<li>methods: 本研究使用的是Segment Anything Model (SAM)，并通过文本提示来改进SAM模型，提高其对肠脏膜膜蛋白分 segmentation的能力。</li>
<li>results: 研究表明，使用文本提示的SAM模型可以提高肠脏膜膜蛋白分 segmentation的精度和稳定性，并且比未使用文本提示的SAM模型更好地处理不同的肠脏膜膜蛋白分样本。<details>
<summary>Abstract</summary>
Meta recently released SAM (Segment Anything Model) which is a general-purpose segmentation model. SAM has shown promising results in a wide variety of segmentation tasks including medical image segmentation. In the field of medical image segmentation, polyp segmentation holds a position of high importance, thus creating a model which is robust and precise is quite challenging. Polyp segmentation is a fundamental task to ensure better diagnosis and cure of colorectal cancer. As such in this study, we will see how Polyp-SAM++, a text prompt-aided SAM, can better utilize a SAM using text prompting for robust and more precise polyp segmentation. We will evaluate the performance of a text-guided SAM on the polyp segmentation task on benchmark datasets. We will also compare the results of text-guided SAM vs unprompted SAM. With this study, we hope to advance the field of polyp segmentation and inspire more, intriguing research. The code and other details will be made publically available soon at https://github.com/RisabBiswas/Polyp-SAM++.
</details>
<details>
<summary>摘要</summary>
Meta 最近发布了 SAM（Segment Anything Model），这是一种通用分割模型。SAM 在各种分割任务中表现出了扎实的成果，包括医学影像分割。在医学影像分割领域，肿瘤分割具有非常高的重要性，因此创建一个精准和Robust的模型是非常挑战性的。肿瘤分割是检测和治疗抗Rectal cancer的基本任务之一。在这项研究中，我们将看到Polyp-SAM++ 是如何使用文本提示来更好地利用 SAM 进行肿瘤分割。我们将对 Polyp-SAM++ 在标准数据集上进行评估，并与不提示 SAM 进行比较。我们希望通过这项研究，推动肿瘤分割领域的进步，并鼓励更多的有趣的研究。代码和其他细节将于 https://github.com/RisabBiswas/Polyp-SAM++ 上公开。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/13/eess.IV_2023_08_13/" data-id="clly4xtg900f1vl887spt0yve" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_12" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/12/cs.LG_2023_08_12/" class="article-date">
  <time datetime="2023-08-11T16:00:00.000Z" itemprop="datePublished">2023-08-12</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/12/cs.LG_2023_08_12/">cs.LG - 2023-08-12 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning"><a href="#CoverNav-Cover-Following-Navigation-Planning-in-Unstructured-Outdoor-Environment-with-Deep-Reinforcement-Learning" class="headerlink" title="CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning"></a>CoverNav: Cover Following Navigation Planning in Unstructured Outdoor Environment with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06594">http://arxiv.org/abs/2308.06594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Anjan Basak, Derrik E. Asher</li>
<li>For: 本研究旨在提出一种基于深度强化学习（DRL）算法，帮助无人地面车辆在隐蔽的情况下安全地导航到预定的目的地。* Methods: 本研究使用了DRL算法，计算了地方成本图，帮助机器人选择低高度的路径，并在检测到观察者时，使用自然障碍物（如岩石、房屋、瘫痪车辆、树木等）作为隐蔽物。* Results: 研究表明，CoverNav可以在 Unity 模拟环境中保证动态可行性，并在不同高度场景下实现最大目标距离和成功率。与当前最佳方法相比，CoverNav 没有妥协精度。<details>
<summary>Abstract</summary>
Autonomous navigation in offroad environments has been extensively studied in the robotics field. However, navigation in covert situations where an autonomous vehicle needs to remain hidden from outside observers remains an underexplored area. In this paper, we propose a novel Deep Reinforcement Learning (DRL) based algorithm, called CoverNav, for identifying covert and navigable trajectories with minimal cost in offroad terrains and jungle environments in the presence of observers. CoverNav focuses on unmanned ground vehicles seeking shelters and taking covers while safely navigating to a predefined destination. Our proposed DRL method computes a local cost map that helps distinguish which path will grant the maximal covertness while maintaining a low cost trajectory using an elevation map generated from 3D point cloud data, the robot's pose, and directed goal information. CoverNav helps robot agents to learn the low elevation terrain using a reward function while penalizing it proportionately when it experiences high elevation. If an observer is spotted, CoverNav enables the robot to select natural obstacles (e.g., rocks, houses, disabled vehicles, trees, etc.) and use them as shelters to hide behind. We evaluate CoverNav using the Unity simulation environment and show that it guarantees dynamically feasible velocities in the terrain when fed with an elevation map generated by another DRL based navigation algorithm. Additionally, we evaluate CoverNav's effectiveness in achieving a maximum goal distance of 12 meters and its success rate in different elevation scenarios with and without cover objects. We observe competitive performance comparable to state of the art (SOTA) methods without compromising accuracy.
</details>
<details>
<summary>摘要</summary>
自主导航在非路面环境中已经得到了机器人学Field的广泛研究。然而，在情报人员发现自动驾驶车辆的情况下，自主导航仍然是一个未得到充分研究的领域。在这篇论文中，我们提出了一种基于深度优化学习（DRL）算法，称为CoverNav，用于在非路面环境中寻找最佳隐蔽和可行的轨迹，并且尽量降低成本。CoverNav关注于无人地面车辆在安全地 navigate到预定目的地点时，找到遮盾和避险的方法。我们提出的DRL方法计算了当地的成本地图，以帮助选择最佳隐蔽的路径，同时维护低成本轨迹。如果检测到了观察者，CoverNav允许机器人使用自然障碍物（如岩石、房屋、瘫痪车辆、树木等）作为遮盾，隐藏自己。我们使用Unity simulate环境评估CoverNav，并证明它在地形图生成自 another DRL基 Navigation algorithm时能够保证动态可行速度。此外，我们在不同高度场景下评估CoverNav的效果，并发现其与SOTA方法相比，没有妥协精度。
</details></li>
</ul>
<hr>
<h2 id="Value-Distributional-Model-Based-Reinforcement-Learning"><a href="#Value-Distributional-Model-Based-Reinforcement-Learning" class="headerlink" title="Value-Distributional Model-Based Reinforcement Learning"></a>Value-Distributional Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06590">http://arxiv.org/abs/2308.06590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Carlos E. Luis, Alessandro G. Bottero, Julia Vinogradska, Felix Berkenkamp, Jan Peters</li>
<li>for: 这个论文目的是为了解决sequential decision-making任务中的uncertainty quantification问题。</li>
<li>methods: 这个论文使用了model-based Bayesian reinforcement learning的方法，其中的目标是学习Markov决策过程中参数不确定性induced的 posterior distribution over value functions。</li>
<li>results: 论文的实验表明，EQR算法可以在 continuous-control tasks 中比Established model-based和model-free算法表现出性能优势。<details>
<summary>Abstract</summary>
Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>量化政策长期表现的不确定性是解决sequential decision-making任务的重要问题。我们从model-based Bayesian reinforcement learning的视角 изуча这个问题，目标是学习Markov决策过程中参数（эпистемиче）不确定性引起的 posterior distribution over value functions。先前的工作只考虑了这些分布的一些瞬间或假设了特定的分布形式，例如 Gaussian。 inspirited by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function。 Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. 评估在多个连续控制任务上表现出与已有的model-based和model-free算法相比的性能优势。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Approximate-Answering-of-Graph-Queries"><a href="#Approximate-Answering-of-Graph-Queries" class="headerlink" title="Approximate Answering of Graph Queries"></a>Approximate Answering of Graph Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06585">http://arxiv.org/abs/2308.06585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Cochez, Dimitrios Alivanistos, Erik Arakelyan, Max Berrendorf, Daniel Daza, Mikhail Galkin, Pasquale Minervini, Mathias Niepert, Hongyu Ren</li>
<li>for: 本文旨在介绍几种方法，以帮助回答含有不完整信息的知识图（KG）中的查询。</li>
<li>methods: 本文提出了多种方法，包括基于预测、基于潜在相似性、基于证据等方法，以满足不同类型的查询需求。</li>
<li>results: 这些方法可以帮助解决各种查询问题，如答案推断、 Entity Disambiguation、 Relation extraction 等。但是，这些方法受到图数据不完整和不准确的限制。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) are inherently incomplete because of incomplete world knowledge and bias in what is the input to the KG. Additionally, world knowledge constantly expands and evolves, making existing facts deprecated or introducing new ones. However, we would still want to be able to answer queries as if the graph were complete. In this chapter, we will give an overview of several methods which have been proposed to answer queries in such a setting. We will first provide an overview of the different query types which can be supported by these methods and datasets typically used for evaluation, as well as an insight into their limitations. Then, we give an overview of the different approaches and describe them in terms of expressiveness, supported graph types, and inference capabilities.
</details>
<details>
<summary>摘要</summary>
知识图（KG）自然而然地是不完整的，因为世界知识的不完整和输入KG中的偏见。此外，世界知识不断扩展和发展，使现有的事实过时或引入新的事实。然而，我们仍然希望能够回答问题，作为如果图完整一样。在这章中，我们将给出不同类型的查询支持的方法的概述，以及通常用于评估的数据集，以及这些方法的局限性。然后，我们将对不同的方法进行描述，包括表达力、支持的图类型和推理能力。
</details></li>
</ul>
<hr>
<h2 id="A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence"><a href="#A-new-solution-and-concrete-implementation-steps-for-Artificial-General-Intelligence" class="headerlink" title="A new solution and concrete implementation steps for Artificial General Intelligence"></a>A new solution and concrete implementation steps for Artificial General Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09721">http://arxiv.org/abs/2308.09721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongcong Chen, Ting Zeng, Jun Zhang</li>
<li>for: 本文旨在探讨大型模型技术路径的局限性，并提出解决这些局限性的方案，以实现true AGI。</li>
<li>methods: 本文使用了现有技术和方法，包括注意机制、深度学习和补偿学习，并提出了一种新的解决方案。</li>
<li>results: 本文提出的解决方案可以解决大型模型技术路径中的缺陷，并实现true AGI。<details>
<summary>Abstract</summary>
At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. In this paper, we will reveal how to achieve true AGI step by step.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:现在，主流人工智能通常采用“注意机制+深度学习”+“奖励学习”的技术路径。这种方法在AIGC（人工智能生成内容）领域已经取得了 significanthistorical achievements[ 2][13 ], triggering a technological wave of big models. However, in areas that require interaction with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are costly and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, to achieve Artificial General Intelligence (AGI) that can be applied to any field, we need to leverage both existing technologies and address the limitations of existing technologies, thereby further developing the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the inherent defects of large models. Through this paper, we will reveal how to achieve true AGI step by step.
</details></li>
</ul>
<hr>
<h2 id="EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction"><a href="#EquiDiff-A-Conditional-Equivariant-Diffusion-Model-For-Trajectory-Prediction" class="headerlink" title="EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction"></a>EquiDiff: A Conditional Equivariant Diffusion Model For Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06564">http://arxiv.org/abs/2308.06564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kehua Chen, Xianda Chen, Zihan Yu, Meixin Zhu, Hai Yang</li>
<li>for: 预测自动驾驶车辆的未来轨迹是关键的，以确保安全和效率地运行。</li>
<li>methods: 我们提出了一种基于深度生成模型的轨迹预测方法，即EquiDiff。EquiDiff基于 Conditional Diffusion 模型，通过历史信息和随机抽样 Gaussian 噪声来生成未来轨迹。</li>
<li>results: 我们在 NGSIM 数据集上进行了广泛的实验，并证明了 EquiDiff 在短期预测方面的性能较高，但在长期预测方面有些较高的错误率。此外，我们还进行了一个ablation study，以调查各组件对预测精度的贡献。<details>
<summary>Abstract</summary>
Accurate trajectory prediction is crucial for the safe and efficient operation of autonomous vehicles. The growing popularity of deep learning has led to the development of numerous methods for trajectory prediction. While deterministic deep learning models have been widely used, deep generative models have gained popularity as they learn data distributions from training data and account for trajectory uncertainties. In this study, we propose EquiDiff, a deep generative model for predicting future vehicle trajectories. EquiDiff is based on the conditional diffusion model, which generates future trajectories by incorporating historical information and random Gaussian noise. The backbone model of EquiDiff is an SO(2)-equivariant transformer that fully utilizes the geometric properties of location coordinates. In addition, we employ Recurrent Neural Networks and Graph Attention Networks to extract social interactions from historical trajectories. To evaluate the performance of EquiDiff, we conduct extensive experiments on the NGSIM dataset. Our results demonstrate that EquiDiff outperforms other baseline models in short-term prediction, but has slightly higher errors for long-term prediction. Furthermore, we conduct an ablation study to investigate the contribution of each component of EquiDiff to the prediction accuracy. Additionally, we present a visualization of the generation process of our diffusion model, providing insights into the uncertainty of the prediction.
</details>
<details>
<summary>摘要</summary>
准确预测车辆轨迹是自动驾驶车辆运行的安全和效率的关键。随着深度学习的普及，许多方法 для轨迹预测得到了开发。而深度生成模型在训练数据中学习数据分布，并考虑轨迹不确定性，因此在这种情况下变得更加受欢迎。在这项研究中，我们提出了EquiDiff，一种基于 conditional diffusion 模型的深度生成模型，用于预测未来车辆轨迹。EquiDiff 使用 SO(2)-equivariant transformer 作为底层模型，并使用循环神经网络和 Graph Attention Networks 提取历史轨迹中的社会交互。为了评估EquiDiff的性能，我们在 NGSIM 数据集上进行了广泛的实验。我们的结果表明，EquiDiff 在短期预测方面表现出色，但是在长期预测方面有些微的错误。此外，我们进行了ablation study，以investigate EquiDiff 中每个组件对预测精度的贡献。此外，我们还提供了生成过程中 diffusion 模型的视觉化，为预测不确定性提供了更多的信息。
</details></li>
</ul>
<hr>
<h2 id="Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System"><a href="#Human-Behavior-based-Personalized-Meal-Recommendation-and-Menu-Planning-Social-System" class="headerlink" title="Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System"></a>Human Behavior-based Personalized Meal Recommendation and Menu Planning Social System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06549">http://arxiv.org/abs/2308.06549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Anika Rahman Joyita, Md. Golam Rabiul Alam, Mohammad Mehedi Hassan, Md. Rafiul Hassan, Raffaele Gravina</li>
<li>for: 这个研究的目的是为了提供一种基于情感计算的餐Menu建议系统，以满足用户的情感需求和营养需求。</li>
<li>methods: 这个系统使用了问卷调查和偏好认知来获取用户的餐食偏好，并使用EEG信号检测用户对不同餐食的情感反应。然后，使用一种层次ensemble方法预测餐食的情感反应，并使用TOPSIS算法生成一个基于预测结果的餐Menu。</li>
<li>results: 实验结果表明，提出的情感计算、餐Menu建议和自动菜单规划算法都能够在不同评价参数下表现良好。<details>
<summary>Abstract</summary>
The traditional dietary recommendation systems are basically nutrition or health-aware where the human feelings on food are ignored. Human affects vary when it comes to food cravings, and not all foods are appealing in all moods. A questionnaire-based and preference-aware meal recommendation system can be a solution. However, automated recognition of social affects on different foods and planning the menu considering nutritional demand and social-affect has some significant benefits of the questionnaire-based and preference-aware meal recommendations. A patient with severe illness, a person in a coma, or patients with locked-in syndrome and amyotrophic lateral sclerosis (ALS) cannot express their meal preferences. Therefore, the proposed framework includes a social-affective computing module to recognize the affects of different meals where the person's affect is detected using electroencephalography signals. EEG allows to capture the brain signals and analyze them to anticipate affective toward a food. In this study, we have used a 14-channel wireless Emotive Epoc+ to measure affectivity for different food items. A hierarchical ensemble method is applied to predict affectivity upon multiple feature extraction methods and TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) is used to generate a food list based on the predicted affectivity. In addition to the meal recommendation, an automated menu planning approach is also proposed considering a person's energy intake requirement, affectivity, and nutritional values of the different menus. The bin-packing algorithm is used for the personalized menu planning of breakfast, lunch, dinner, and snacks. The experimental findings reveal that the suggested affective computing, meal recommendation, and menu planning algorithms perform well across a variety of assessment parameters.
</details>
<details>
<summary>摘要</summary>
传统的饮食建议系统基本上是nutrition或健康意识的，忽略了人类的情感 toward food。人类的食欲情绪 varying degree，不同的情感状态下不同的食物都不能吸引人。问卷式和偏好意识的饭单推荐系统可以是一种解决方案。然而，通过自动认知不同食物的社会情感影响和根据饮食需求和社会情感规划饭单，有一些显著的优点。例如，患有严重疾病、昏迷状态或 locked-in syndrome 和 amyotrophic lateral sclerosis (ALS) 的患者无法表达他们的饭单首选。因此，我们的框架包括一个社交情感计算模块，用于识别不同饭物中的情感。我们使用了14核心无线Emotive Epoc+来测量不同食物的情感。我们使用了一种层次ensemble方法来预测情感，并使用TOPSIS (技术 дляOrder of Preference by Similarity to Ideal Solution)来生成基于预测情感的食品列表。此外，我们还提出了一种自动饭单规划方法，考虑人类的能量摄入需求、情感和不同饭单的营养价值。使用了bin-packing算法进行个性化饭单规划的早餐、午餐、晚餐和快餐。实验结果表明，我们提出的情感计算、饭单推荐和饭单规划算法在多种评估参数下表现良好。
</details></li>
</ul>
<hr>
<h2 id="Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters"><a href="#Digital-elevation-model-correction-in-urban-areas-using-extreme-gradient-boosting-land-cover-and-terrain-parameters" class="headerlink" title="Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters"></a>Digital elevation model correction in urban areas using extreme gradient boosting, land cover and terrain parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06545">http://arxiv.org/abs/2308.06545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chukwuma Okolie, Jon Mills, Adedayo Adeleke, Julian Smit</li>
<li>For: The paper aims to enhance the accuracy of medium-resolution digital elevation models (DEMs) in urban areas, specifically in Cape Town, South Africa, for hydrological and environmental modelling.* Methods: The authors use the extreme gradient boosting (XGBoost) ensemble algorithm to correct the DEMs, with eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, and more.* Results: The corrected DEMs achieved significant accuracy gains, with a root mean square error (RMSE) improvement of 46-53% for Copernicus DEM and 72-73% for AW3D DEM, compared to other proposed methods. These results demonstrate the potential of gradient boosted trees for enhancing DEM quality and improving hydrological modelling in urban catchments.Here is the same information in Simplified Chinese text, as requested:* For: 这个论文的目的是提高城市区域中的数字高程模型（DEM）的准确性，以便于水文和环境模型。* Methods: 作者使用极限Gradient Boosting（XGBoost）ensemble算法来修正DEM，使用的predictor变量包括高程、城市脚印、坡度、方向、表面荒凉、地形位置指数、地形荒凉指数、地形表面 текстура等 eleven个变量。* Results: 修正后的DEM实现了显著的准确性提高，比如 Copernicus DEM的RMSE提高46-53%，AW3D DEM的RMSE提高72-73%，与其他提议的方法相比。这些结果表明极限Gradient Boosting树可以提高DEM的质量，并且为城市catchments中的水文模型提供改善。<details>
<summary>Abstract</summary>
The accuracy of digital elevation models (DEMs) in urban areas is influenced by numerous factors including land cover and terrain irregularities. Moreover, building artifacts in global DEMs cause artificial blocking of surface flow pathways. This compromises their quality and adequacy for hydrological and environmental modelling in urban landscapes where precise and accurate terrain information is needed. In this study, the extreme gradient boosting (XGBoost) ensemble algorithm is adopted for enhancing the accuracy of two medium-resolution 30m DEMs over Cape Town, South Africa: Copernicus GLO-30 and ALOS World 3D (AW3D). XGBoost is a scalable, portable and versatile gradient boosting library that can solve many environmental modelling problems. The training datasets are comprised of eleven predictor variables including elevation, urban footprints, slope, aspect, surface roughness, topographic position index, terrain ruggedness index, terrain surface texture, vector roughness measure, forest cover and bare ground cover. The target variable (elevation error) was calculated with respect to highly accurate airborne LiDAR. After training and testing, the model was applied for correcting the DEMs at two implementation sites. The correction achieved significant accuracy gains which are competitive with other proposed methods. The root mean square error (RMSE) of Copernicus DEM improved by 46 to 53% while the RMSE of AW3D DEM improved by 72 to 73%. These results showcase the potential of gradient boosted trees for enhancing the quality of DEMs, and for improved hydrological modelling in urban catchments.
</details>
<details>
<summary>摘要</summary>
地数模型（DEM）在城市地区的准确性受到多种因素的影响，包括地表覆盖物和地形 irregularities。此外，全球 DEM 中的建筑物略导致表面流道路径的人工堵塞，从而降低其质量和适用性 для水文环境模型在城市景观中，需要精准和准确的地形信息。在这种研究中，我们采用了极限拟合搅拌（XGBoost）ensemble算法来提高两个中等分辨率 30 m DEM 的准确性，即 Copernicus GLO-30 和 ALOS World 3D（AW3D）。XGBoost 是一种可扩展、可移植和多样的拟合搅拌库，可以解决许多环境模型问题。训练数据集包括 eleven 个预测变量，包括高程、城市脚印、坡度、方向、表面粗糙度、地形坡度指数、地形表面文化、向量粗糙度度量、森林覆盖率和裸地覆盖率。target variable （高程误差）与高精度飞行 LiDAR 进行计算。之后，模型被应用于修正 DEM 的两个实施场景。修正后，DEM 的Root Mean Square Error（RMSE）提高了46%到53%，AW3D DEM 的 RMSE 提高了72%到73%。这些结果显示了拟合搅拌树的潜在可能性，以及对城市流域水文模型的改进。
</details></li>
</ul>
<hr>
<h2 id="Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models"><a href="#Dealing-with-Small-Datasets-for-Deep-Learning-in-Medical-Imaging-An-Evaluation-of-Self-Supervised-Pre-Training-on-CT-Scans-Comparing-Contrastive-and-Masked-Autoencoder-Methods-for-Convolutional-Models" class="headerlink" title="Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models"></a>Dealing with Small Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06534">http://arxiv.org/abs/2308.06534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wolfda95/ssl-medicalimagining-cl-mae">https://github.com/wolfda95/ssl-medicalimagining-cl-mae</a></li>
<li>paper_authors: Daniel Wolf, Tristan Payer, Catharina Silvia Lisson, Christoph Gerhard Lisson, Meinrad Beer, Timo Ropinski, Michael Götz</li>
<li>for: 这篇论文旨在探讨deep learning在医疗影像领域中的应用，以减少诊断错误、轻量化医生工作负担，并加快诊断。</li>
<li>methods: 这篇论文使用了自动标注学习方法，包括对大量无标注影像进行自动标注。</li>
<li>results: 研究发现，使用SparK预训方法可以更好地适应小型标注数据，并且在诊断任务中表现更好。<details>
<summary>Abstract</summary>
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task. The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares state-of-the-art contrastive learning methods with the recently introduced masked autoencoder approach "SparK" for convolutional neural networks (CNNs) on medical images. Therefore we pre-train on a large unannotated CT image dataset and fine-tune on several CT classification tasks. Due to the challenge of obtaining sufficient annotated training data in medical imaging, it is of particular interest to evaluate how the self-supervised pre-training methods perform when fine-tuning on small datasets. By experimenting with gradually reducing the training dataset size for fine-tuning, we find that the reduction has different effects depending on the type of pre-training chosen. The SparK pre-training method is more robust to the training dataset size than the contrastive methods. Based on our results, we propose the SparK pre-training for medical imaging tasks with only small annotated datasets.
</details>
<details>
<summary>摘要</summary>
深度学习在医疗影像领域可能减少诊断错误风险，减轻放射学家的工作负担，并加速诊断。深度学习模型的训练需要大量和准确的数据集，并将所有训练样本标注。然而，在医疗影像领域，特定任务的标注数据集经常很小，这可能由标注的复杂性、访问限制或疾病的罕见性引起。为解决这个挑战，可以使用自动标注学习的方法进行深度学习模型的预训练。在预训练后，只需要小量的标注数据集来精度地调整模型 для特定任务。医疗影像领域最受欢迎的自动标注预训练方法是对比学习。然而，最近的自然图像处理研究表明，遮盲 autoencoder 方法有很强的潜在性。我们的工作比较了当前状态的对比学习方法和新引入的遮盲 autoencoder 方法 "SparK" 在医疗影像中的 convolutional neural networks (CNNs) 上。因此，我们预训练在大量无注释 CT 图像数据集上，并在多个 CT 分类任务上进行精度调整。由于医疗影像领域获得足够的注释训练数据是困难的，因此特别关心自动标注预训练方法在小型注释数据集上的性能。通过逐渐减少 fine-tuning 数据集大小的实验，我们发现降低的效果与预训练方法的类型有很大的差异。SparK 预训练方法在训练数据集尺寸减少后表现更加稳定。根据我们的结果，我们建议使用 SparK 预训练方法进行医疗影像任务，只需要小量的注释训练数据。
</details></li>
</ul>
<hr>
<h2 id="Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices"><a href="#Learning-Abstract-Visual-Reasoning-via-Task-Decomposition-A-Case-Study-in-Raven-Progressive-Matrices" class="headerlink" title="Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices"></a>Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06528">http://arxiv.org/abs/2308.06528</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jakubkwiatkowski/abstract_compositional_transformer">https://github.com/jakubkwiatkowski/abstract_compositional_transformer</a></li>
<li>paper_authors: Jakub Kwiatkowski, Krzysztof Krawiec</li>
<li>for: 本研究旨在提高 Ravens 进步矩阵（RPM）问题的抽象逻辑能力，通过预测图像中对象的视觉特征和排序来选择答案。</li>
<li>methods: 本研究使用了一种基于 transformer 框架的深度学习模型，通过预测图像中对象的视觉特征和排序来选择答案。研究还考虑了不同的图像分割方法和自我指导学习策略。</li>
<li>results: 实验结果表明，本研究的模型不仅超越了当前最佳方法，还提供了有趣的思路和部分解释，以帮助理解问题的含义。此外，模型的设计还使其具有免疫一些已知 RPM  bencmarks 中的偏见的能力。<details>
<summary>Abstract</summary>
One of the challenges in learning to perform abstract reasoning is that problems are often posed as monolithic tasks, with no intermediate subgoals. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both contexts and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning is challenging and most contemporary solvers tend to be opaque. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, predicts the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details>
<details>
<summary>摘要</summary>
一个挑战在抽象逻辑学习中是，问题经常是单一任务，没有中间目标。在萨瑟进步矩阵（RPM）中，任务是根据Context选择可用的答案，Context和答案都是复杂的图像，包含多个物体在不同的空间排列。由于高级目标是唯一的指导，学习是困难的，而大多数当代解决方案都是透明的。在这种研究中，我们提出了基于变换器蓝图的深度学习架构，而不是直接选择上述高级目标，而是预测图像中物体的视觉属性和排列。 obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.Here's the translation in Traditional Chinese:一个挑战在抽象逻辑学习中是，问题经常是单一任务，没有中间目标。在萨瑟进步矩阵（RPM）中，任务是根据Context选择可用的答案，Context和答案都是复杂的图像，包含多个物体在不同的空间排列。由于高级目标是唯一的指导，学习是困难的，而大多数当代解决方案都是透明的。在这种研究中，我们提出了基于变数器蓝图的深度学习架构，而不是直接选择上述高级目标，而是预测图像中物体的视觉属性和排列。 obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to exist in some RPM benchmarks.
</details></li>
</ul>
<hr>
<h2 id="SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models"><a href="#SLoRA-Federated-Parameter-Efficient-Fine-Tuning-of-Language-Models" class="headerlink" title="SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models"></a>SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06522">http://arxiv.org/abs/2308.06522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Babakniya, Ahmed Roushdy Elkordy, Yahya H. Ezzeldin, Qingfeng Liu, Kee-Bong Song, Mostafa El-Khamy, Salman Avestimehr</li>
<li>for: 这个论文主要针对的是如何使用 parameter efficient fine-tuning (PEFT) 方法在 Federated Learning (FL) 中进行语言任务的训练。</li>
<li>methods: 本文使用了 FL 技术和 PEFT 方法，并进行了实验研究以探讨在不同的数据场景下的可行性和挑战。</li>
<li>results: 实验结果表明，当用户数据变得更加多样化时，PEFT 方法与全量精度训练的性能差距逐渐增大。为了bridge这个性能差距，本文提出了一种名为 SLoRA 的方法，通过一种新的数据驱动初始化技术来超越 LoRA 在高多样数据场景下的限制。SLoRA 方法可以实现与全量精度训练相当的性能，并且可以减少训练时间，并且可以减少稀疏更新的数量。<details>
<summary>Abstract</summary>
Transfer learning via fine-tuning pre-trained transformer models has gained significant success in delivering state-of-the-art results across various NLP tasks. In the absence of centralized data, Federated Learning (FL) can benefit from distributed and private data of the FL edge clients for fine-tuning. However, due to the limited communication, computation, and storage capabilities of edge devices and the huge sizes of popular transformer models, efficient fine-tuning is crucial to make federated training feasible. This work explores the opportunities and challenges associated with applying parameter efficient fine-tuning (PEFT) methods in different FL settings for language tasks. Specifically, our investigation reveals that as the data across users becomes more diverse, the gap between fully fine-tuning the model and employing PEFT methods widens. To bridge this performance gap, we propose a method called SLoRA, which overcomes the key limitations of LoRA in high heterogeneous data scenarios through a novel data-driven initialization technique. Our experimental results demonstrate that SLoRA achieves performance comparable to full fine-tuning, with significant sparse updates with approximately $\sim 1\%$ density while reducing training time by up to $90\%$.
</details>
<details>
<summary>摘要</summary>
通过精细调整已经训练过的变换器模型，通过中央化数据的缺失， Federated Learning (FL) 可以利用分布式和私有的 Edge 客户端数据进行 fine-tuning。然而，由于 Edge 设备的通信、计算和存储能力的限制，以及各种变换器模型的巨大大小，高效的 fine-tuning 是在 federated 训练中实现可行的。这种工作探讨了在语言任务中应用 parameter efficient fine-tuning (PEFT) 方法的机会和挑战。具体来说，我们的调查发现，当用户数据变得更加多样化时，具有完全 fine-tuning 模型和使用 PEFT 方法之间的性能差距变得更加明显。为了补做这个性能差距，我们提出了一种名为 SLoRA 的方法，通过一种新的数据驱动初始化技术，超越 LoRA 在高多样化数据场景中的关键限制。我们的实验结果表明，SLoRA 可以与完全 fine-tuning 性能相似，使用约 $\sim 1\%$ 的稀疏更新，同时降低训练时间达到 $90\%$。
</details></li>
</ul>
<hr>
<h2 id="One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training"><a href="#One-bit-Flip-is-All-You-Need-When-Bit-flip-Attack-Meets-Model-Training" class="headerlink" title="One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training"></a>One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07934">http://arxiv.org/abs/2308.07934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianshuod/tba">https://github.com/jianshuod/tba</a></li>
<li>paper_authors: Jianshuo Dong, Han Qiu, Yiming Li, Tianwei Zhang, Yuanjie Li, Zeqi Lai, Chao Zhang, Shu-Tao Xia</li>
<li>for: 保持深度神经网络（DNNs）的安全性，因为它们在实际设备上广泛部署。</li>
<li>methods: 使用记忆FAULT INJECT技术，如行ammer，对量化模型进行攻击。只需要一些位置的变化，目标模型可以变成一个随机估计或者恶意功能模型。</li>
<li>results: 在基准数据集上，攻击者可以轻松地通过flipping一个关键位的变化，将高风险模型转换为恶意模型。此外，我们的攻击还能够绕过一些防御机制。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/jianshuod/TBA%7D">https://github.com/jianshuod/TBA}</a> 上复制。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipping only one critical bit} on average in the deployment stage. Moreover, our attack still poses a significant threat even when defenses are employed. The codes for reproducing main experiments are available at \url{https://github.com/jianshuod/TBA}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks"><a href="#Performance-Analysis-for-Resource-Constrained-Decentralized-Federated-Learning-Over-Wireless-Networks" class="headerlink" title="Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks"></a>Performance Analysis for Resource Constrained Decentralized Federated Learning Over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06496">http://arxiv.org/abs/2308.06496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhigang Yan, Dong Li</li>
<li>for: 这个研究旨在测试和优化内存和通信参数以提高 Federated Learning（FL）的可靠性和效率。</li>
<li>methods: 这个研究使用了分布式 Federated Learning（DFL）框架，并使用了不同的通信方案（数位和分数）来分析它们的通信效率。</li>
<li>results: 研究发现，在不同的通信方案下，这个框架可以提供内存和通信参数的优化，以提高模型的训练效率和可靠性。<details>
<summary>Abstract</summary>
Federated learning (FL) can lead to significant communication overhead and reliance on a central server. To address these challenges, decentralized federated learning (DFL) has been proposed as a more resilient framework. DFL involves parameter exchange between devices through a wireless network. This study analyzes the performance of resource-constrained DFL using different communication schemes (digital and analog) over wireless networks to optimize communication efficiency. Specifically, we provide convergence bounds for both digital and analog transmission approaches, enabling analysis of the model performance trained on DFL. Furthermore, for digital transmission, we investigate and analyze resource allocation between computation and communication and convergence rates, obtaining its communication complexity and the minimum probability of correction communication required for convergence guarantee. For analog transmission, we discuss the impact of channel fading and noise on the model performance and the maximum errors accumulation with convergence guarantee over fading channels. Finally, we conduct numerical simulations to evaluate the performance and convergence rate of convolutional neural networks (CNNs) and Vision Transformer (ViT) trained in the DFL framework on fashion-MNIST and CIFAR-10 datasets. Our simulation results validate our analysis and discussion, revealing how to improve performance by optimizing system parameters under different communication conditions.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 可能会带来重要的通信负担和依赖中央服务器。为了解决这些挑战，分散式 federated learning (DFL) 已经被提议作为一个更可靠的框架。DFL 中各个设备之间的参数交换通过无线网络。本研究分析了受限制的 DFL 在无线网络上的执行效率，使用不同的通信方案（数位和分散）。特别是，我们提供了两种通信方案的整合边界值，以便分析模型在 DFL 中的表现。此外，我们还调查了在数位传输中的资源分配和计算和通信的复杂度，以及它们对模型表现的影响。另外，我们还分析了随机传输中频道折射和噪音对模型表现的影响，以及在折射频道上累累的最大错误累累。最后，我们对 fashion-MNIST 和 CIFAR-10 数据集上的 CNNs 和 ViT 在 DFL 框架中进行了数值模拟，以评估其表现和融合率。我们的模拟结果证实了我们的分析和讨论，显示了如何通过优化系统参数来改善表现，不同的通信条件下。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding"><a href="#Flexible-Keyword-Spotting-based-on-Homogeneous-Audio-Text-Embedding" class="headerlink" title="Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding"></a>Flexible Keyword Spotting based on Homogeneous Audio-Text Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06472">http://arxiv.org/abs/2308.06472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumari Nishu, Minsik Cho, Paul Dixon, Devang Naik</li>
<li>for: 这个论文的目的是提出一种高效的关键词检测方法，以便在 audio-text  embedding 空间中快速检测任意关键词。</li>
<li>methods: 该方法使用一个 audio-compliant 文本编码器，将文本转换为phonemes使用 G2P 模型，然后将phonemes转换为嵌入使用表示性强的音频编码器生成的phoneme вектор。此外，该方法还使用杂音词生成来提高audio-text embedding验证器的强度。</li>
<li>results: 实验结果表明，该方法在 Libriphrase 难 dataset 上超过了州�类-国度的Result（AUC：84.21% → 92.7%，EER：23.36% → 14.4%）， indicating that our scheme can efficiently detect arbitrary keywords in audio-text embedding space with high accuracy.<details>
<summary>Abstract</summary>
Spotting user-defined/flexible keywords represented in text frequently uses an expensive text encoder for joint analysis with an audio encoder in an embedding space, which can suffer from heterogeneous modality representation (i.e., large mismatch) and increased complexity. In this work, we propose a novel architecture to efficiently detect arbitrary keywords based on an audio-compliant text encoder which inherently has homogeneous representation with audio embedding, and it is also much smaller than a compatible text encoder. Our text encoder converts the text to phonemes using a grapheme-to-phoneme (G2P) model, and then to an embedding using representative phoneme vectors, extracted from the paired audio encoder on rich speech datasets. We further augment our method with confusable keyword generation to develop an audio-text embedding verifier with strong discriminative power. Experimental results show that our scheme outperforms the state-of-the-art results on Libriphrase hard dataset, increasing Area Under the ROC Curve (AUC) metric from 84.21% to 92.7% and reducing Equal-Error-Rate (EER) metric from 23.36% to 14.4%.
</details>
<details>
<summary>摘要</summary>
通常，用户定义/灵活关键词在文本中的检测使用昂贵的文本编码器进行联合分析与音频编码器在嵌入空间，这可能会导致不同类型的表达媒体表示（大匹配度差）和增加复杂性。在这种工作中，我们提出一种新的架构，可以有效地检测任意关键词基于兼容音频编码器的文本编码器，该编码器自然具有同 Audio embedding的同一个表示形式，而且比兼容的文本编码器更小。我们的文本编码器将文本转换为音频的phoneme使用图eme-to-phoneme（G2P）模型，然后将其转换为嵌入使用表示音频编码器中的可表示性phoneme вектор。我们进一步增强我们的方法，通过可混淆关键词生成来开发一个具有强 дискриминатив力的音频-文本嵌入验证器。实验结果表明，我们的方案在Libriphrase hard数据集上的Result outperform了状态的aru的结果，从84.21%提高到92.7%，并从23.36%降低到14.4%。
</details></li>
</ul>
<hr>
<h2 id="Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest"><a href="#Volterra-Accentuated-Non-Linear-Dynamical-Admittance-VANYA-to-model-Deforestation-An-Exemplification-from-the-Amazon-Rainforest" class="headerlink" title="Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest"></a>Volterra Accentuated Non-Linear Dynamical Admittance (VANYA) to model Deforestation: An Exemplification from the Amazon Rainforest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06471">http://arxiv.org/abs/2308.06471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karthik R., Ramamoorthy A.</li>
<li>for: 这篇论文是为了研究森林覆盖率的预测，特别是通过VANYA模型，包括预测动物食肉动力学。</li>
<li>methods: 该论文使用了时间序列数据和人工智能技术，包括神经网络和推论算法。</li>
<li>results: 论文通过对亚马逊雨林数据进行预测，显示了VANYA模型的可靠性和准确性，并与其他预测器如LSTM、N-BEATS、RCN进行比较。<details>
<summary>Abstract</summary>
Intelligent automation supports us against cyclones, droughts, and seismic events with recent technology advancements. Algorithmic learning has advanced fields like neuroscience, genetics, and human-computer interaction. Time-series data boosts progress. Challenges persist in adopting these approaches in traditional fields. Neural networks face comprehension and bias issues. AI's expansion across scientific areas is due to adaptable descriptors and combinatorial argumentation. This article focuses on modeling Forest loss using the VANYA Model, incorporating Prey Predator Dynamics. VANYA predicts forest cover, demonstrated on Amazon Rainforest data against other forecasters like Long Short-Term Memory, N-BEATS, RCN.
</details>
<details>
<summary>摘要</summary>
智能自动化支持我们面对风暴、旱情和地震等自然灾害，由于最新的技术进步。算法学习在 neuroscience、遗传学和人机交互等领域得到了进步，时间序列数据也促进了进步。然而，在传统领域采用这些方法还存在挑战。神经网络具有理解和偏见问题。AI在科学领域的扩张归功于可变描述符和组合说服。本文通过使用VANYA模型，包括猎Predator Dinamics，预测森林覆盖率，并与Long Short-Term Memory、N-BEATS、RCN等预测器进行比较。
</details></li>
</ul>
<hr>
<h2 id="Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization"><a href="#Tiny-and-Efficient-Model-for-the-Edge-Detection-Generalization" class="headerlink" title="Tiny and Efficient Model for the Edge Detection Generalization"></a>Tiny and Efficient Model for the Edge Detection Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06468">http://arxiv.org/abs/2308.06468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xavysp/teed">https://github.com/xavysp/teed</a></li>
<li>paper_authors: Xavier Soria, Yachuan Li, Mohammad Rouhani, Angel D. Sappa</li>
<li>for: 本研究旨在提高图像Edge detection的简洁性、效率和通用性，对现有的State-of-the-art（SOTA）Edge detection模型进行改进。</li>
<li>methods: 本文提出了一种名为Tiny and Efficient Edge Detector（TEED）的轻量级卷积神经网络，只有58K个参数，比SOTA模型少了99.8%。该模型训练在BIPED dataset上只需30分钟左右，每个epoch只需5分钟左右。</li>
<li>results: 本文的提出的模型具有训练容易、快速收敛的特点，并且预测的边映射质量高。此外，本文还提出了一个新的边检测测试集，包括图像Edge detection和图像分割中常用的样本。代码可以在<a target="_blank" rel="noopener" href="https://github.com/xavysp/TEED%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/xavysp/TEED上下载。</a><details>
<summary>Abstract</summary>
Most high-level computer vision tasks rely on low-level image operations as their initial processes. Operations such as edge detection, image enhancement, and super-resolution, provide the foundations for higher level image analysis. In this work we address the edge detection considering three main objectives: simplicity, efficiency, and generalization since current state-of-the-art (SOTA) edge detection models are increased in complexity for better accuracy. To achieve this, we present Tiny and Efficient Edge Detector (TEED), a light convolutional neural network with only $58K$ parameters, less than $0.2$% of the state-of-the-art models. Training on the BIPED dataset takes $less than 30 minutes$, with each epoch requiring $less than 5 minutes$. Our proposed model is easy to train and it quickly converges within very first few epochs, while the predicted edge-maps are crisp and of high quality. Additionally, we propose a new dataset to test the generalization of edge detection, which comprises samples from popular images used in edge detection and image segmentation. The source code is available in https://github.com/xavysp/TEED.
</details>
<details>
<summary>摘要</summary>
大多数高级计算机视觉任务都基于低级图像操作作为初始过程。操作如边检测、图像提高和超分解，为更高级图像分析提供基础。在这项工作中，我们考虑了三个主要目标：简单、高效和通用，因为当前状态之arte（SOTA）边检测模型在精度方面增加了复杂度。为 достичь这一目标，我们提出了小型和高效的边检测器（TEED），这是一个具有58000个参数的小 convolutional neural network，相对于状态之arte模型的0.2%。在BIPE dataset上训练TEED只需几分钟时间，每个epoch仅需5分钟或更少。我们的提议的模型容易训练，快速 converges于第一些epoch，而预测的边映射具有高质量。此外，我们还提出了一个新的边检测检验集，该集包括来自popular图像的边检测和图像分割领域的样本。代码可以在https://github.com/xavysp/TEED中下载。
</details></li>
</ul>
<hr>
<h2 id="Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks"><a href="#Not-So-Robust-After-All-Evaluating-the-Robustness-of-Deep-Neural-Networks-to-Unseen-Adversarial-Attacks" class="headerlink" title="Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks"></a>Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06467">http://arxiv.org/abs/2308.06467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Garaev, Bader Rasheed, Adil Khan</li>
<li>for: 挑战当代防御机制对于攻击性质变化的测试</li>
<li>methods: 使用 adversarial attacks  manipulate input data 以测试 DNN 的强项和普遍性</li>
<li>results: 发现 DNN 对于 $L_2$ 和 $L_{\infty}$ 攻击性质的差异，并通过对 DNN 表现的分析和可视化获得更深入的理解。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have gained prominence in various applications, such as classification, recognition, and prediction, prompting increased scrutiny of their properties. A fundamental attribute of traditional DNNs is their vulnerability to modifications in input data, which has resulted in the investigation of adversarial attacks. These attacks manipulate the data in order to mislead a DNN. This study aims to challenge the efficacy and generalization of contemporary defense mechanisms against adversarial attacks. Specifically, we explore the hypothesis proposed by Ilyas et. al, which posits that DNN image features can be either robust or non-robust, with adversarial attacks targeting the latter. This hypothesis suggests that training a DNN on a dataset consisting solely of robust features should produce a model resistant to adversarial attacks. However, our experiments demonstrate that this is not universally true. To gain further insights into our findings, we analyze the impact of adversarial attack norms on DNN representations, focusing on samples subjected to $L_2$ and $L_{\infty}$ norm attacks. Further, we employ canonical correlation analysis, visualize the representations, and calculate the mean distance between these representations and various DNN decision boundaries. Our results reveal a significant difference between $L_2$ and $L_{\infty}$ norms, which could provide insights into the potential dangers posed by $L_{\infty}$ norm attacks, previously underestimated by the research community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors"><a href="#A-One-dimensional-HEVC-video-steganalysis-method-using-the-Optimality-of-Predicted-Motion-Vectors" class="headerlink" title="A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors"></a>A One-dimensional HEVC video steganalysis method using the Optimality of Predicted Motion Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06464">http://arxiv.org/abs/2308.06464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Li, Minqing Zhang, Ke Niu, Yingnan Zhang, Xiaoyuan Yang</li>
<li>for: 本研究旨在提高掩埋检测性能，对高效视频编码标准（HEVC）中的动态vector域基于视频掩埋进行检测。</li>
<li>methods: 该研究提出了基于优化的动态vectorprediction（AMVP）技术的一种掩埋特征，即HEVC视频中的信息投射可能会破坏当地优化的动态vectorprediction（MVP）。然后，定义HEVC视频中的MVP优化率作为掩埋检测特征。</li>
<li>results: 通过在两个通用数据集上进行检测，研究发现，对于所有的覆盖视频，MVP优化率都为100%，而对于所有的掩埋视频，MVP优化率小于100%。因此，该掩埋方法可以准确地分辨覆盖视频和掩埋视频，并且在实际应用中具有无模型训练和低计算复杂度。<details>
<summary>Abstract</summary>
Among steganalysis techniques, detection against motion vector (MV) domain-based video steganography in High Efficiency Video Coding (HEVC) standard remains a hot and challenging issue. For the purpose of improving the detection performance, this paper proposes a steganalysis feature based on the optimality of predicted MVs with a dimension of one. Firstly, we point out that the motion vector prediction (MVP) of the prediction unit (PU) encoded using the Advanced Motion Vector Prediction (AMVP) technique satisfies the local optimality in the cover video. Secondly, we analyze that in HEVC video, message embedding either using MVP index or motion vector differences (MVD) may destroy the above optimality of MVP. And then, we define the optimal rate of MVP in HEVC video as a steganalysis feature. Finally, we conduct steganalysis detection experiments on two general datasets for three popular steganography methods and compare the performance with four state-of-the-art steganalysis methods. The experimental results show that the proposed optimal rate of MVP for all cover videos is 100\%, while the optimal rate of MVP for all stego videos is less than 100\%. Therefore, the proposed steganography scheme can accurately distinguish between cover videos and stego videos, and it is efficiently applied to practical scenarios with no model training and low computational complexity.
</details>
<details>
<summary>摘要</summary>
在隐藏分析技术中，对高效视频编码标准（HEVC）中的动态 vector domain-based 视频隐藏技术进行检测仍然是一个热点和挑战。为了提高检测性能，本文提出了基于预测动态 vector（MVP）的隐藏特征。首先，我们指出了HEVC视频中的预测单元（PU）使用高级动态 vector prediction（AMVP）技术预测的动态 vector prediction（MVP）满足了本地优化性。其次，我们分析了在HEVC视频中，使用MVP index或动态 vector differences（MVD）进行消息嵌入可能会破坏MVP的优化性。然后，我们定义HEVC视频中的MVP优化率作为隐藏特征。最后，我们对两个通用数据集上三种流行的隐藏方法进行了隐藏检测实验，并与四种现状顶尖隐藏检测方法进行比较。实验结果显示，提议的MVP优化率对所有封装视频是100%，而对所有隐藏视频是less than 100%。因此，提议的隐藏方法可以准确地分辨封装视频和隐藏视频，并且可以应用于实际场景中无模型训练和低计算复杂度。
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Knowledge-Distillation"><a href="#Multi-Label-Knowledge-Distillation" class="headerlink" title="Multi-Label Knowledge Distillation"></a>Multi-Label Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06453">http://arxiv.org/abs/2308.06453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penghui-yang/l2d">https://github.com/penghui-yang/l2d</a></li>
<li>paper_authors: Penghui Yang, Ming-Kun Xie, Chen-Chen Zong, Lei Feng, Gang Niu, Masashi Sugiyama, Sheng-Jun Huang</li>
<li>for: 该 paper 是关于多标签学习的知识储存方法的研究，它针对现有的知识储存方法在多标签学习场景中的局限性，并提出了一种新的多标签知识储存方法。</li>
<li>methods: 该 paper 使用了分类预测器和学生网络，并将知识储存于 label-wise embeddings 中。它还利用了分类预测器的准确率来提高学生网络的分类性能。</li>
<li>results: 实验结果表明，该 paper 的方法可以避免知识冲突现象，并在多个 benchmark 数据集上达到了superior的性能。<details>
<summary>Abstract</summary>
Existing knowledge distillation methods typically work by imparting the knowledge of output logits or intermediate feature maps from the teacher network to the student network, which is very successful in multi-class single-label learning. However, these methods can hardly be extended to the multi-label learning scenario, where each instance is associated with multiple semantic labels, because the prediction probabilities do not sum to one and feature maps of the whole example may ignore minor classes in such a scenario. In this paper, we propose a novel multi-label knowledge distillation method. On one hand, it exploits the informative semantic knowledge from the logits by dividing the multi-label learning problem into a set of binary classification problems; on the other hand, it enhances the distinctiveness of the learned feature representations by leveraging the structural information of label-wise embeddings. Experimental results on multiple benchmark datasets validate that the proposed method can avoid knowledge counteraction among labels, thus achieving superior performance against diverse comparing methods. Our code is available at: https://github.com/penghui-yang/L2D
</details>
<details>
<summary>摘要</summary>
传统的知识填充方法通常是通过将教师网络的输出LOGIT或中间特征图传递给学生网络，这在多类单标学习中非常成功。然而，这些方法几乎无法扩展到多标学习场景，因为预测概率不同类别之间不相加，特征图中涉及到小类时可能被忽略。在这篇论文中，我们提出了一种新的多标知识填充方法。一方面，它利用多标学习问题的分类器的semantic知识，将问题分解成一系列的binary分类问题。另一方面，它利用标签wise嵌入结构来增强学习的特征表示的分化性。我们的实验结果表明，提案的方法可以避免标签之间的知识冲突，从而在多种 comparing方法的比较中达到更高的性能。我们的代码可以在：https://github.com/penghui-yang/L2D 查看。
</details></li>
</ul>
<hr>
<h2 id="Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More"><a href="#Latent-Random-Steps-as-Relaxations-of-Max-Cut-Min-Cut-and-More" class="headerlink" title="Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More"></a>Latent Random Steps as Relaxations of Max-Cut, Min-Cut, and More</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06448">http://arxiv.org/abs/2308.06448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Chanpuriya, Cameron Musco</li>
<li>for: 本 paper 是为了探讨图像 clustering 中的缺省结构，并提出了一种基于非正式矩阵分解的 probabilistic 模型，用于统一 clustering 和简化图像。</li>
<li>methods: 本 paper 使用的方法是基于非正式矩阵分解的一种 probabilistic 模型，用于模型图像的结构。该模型通过 Random Walk 过程的 фактор化来实现 clustering 和简化图像的同时进行。</li>
<li>results: 本 paper 的结果表明，使用该方法可以很好地处理具有缺省结构的图像，并且可以很好地处理一些涉及多类别的不约分类任务。<details>
<summary>Abstract</summary>
Algorithms for node clustering typically focus on finding homophilous structure in graphs. That is, they find sets of similar nodes with many edges within, rather than across, the clusters. However, graphs often also exhibit heterophilous structure, as exemplified by (nearly) bipartite and tripartite graphs, where most edges occur across the clusters. Grappling with such structure is typically left to the task of graph simplification. We present a probabilistic model based on non-negative matrix factorization which unifies clustering and simplification, and provides a framework for modeling arbitrary graph structure. Our model is based on factorizing the process of taking a random walk on the graph. It permits an unconstrained parametrization, allowing for optimization via simple gradient descent. By relaxing the hard clustering to a soft clustering, our algorithm relaxes potentially hard clustering problems to a tractable ones. We illustrate our algorithm's capabilities on a synthetic graph, as well as simple unsupervised learning tasks involving bipartite and tripartite clustering of orthographic and phonological data.
</details>
<details>
<summary>摘要</summary>
algorithm для clustering 通常是找到同类结点的结构。即它们在集群内部找到多个边，而不是跨集群。但是图 oftentimes 也具有异类结构，如（几乎）二分图和三分图，其中大多数边在集群之间。对这种结构的处理通常被归入图简化任务。我们提出了一种基于非负矩阵因子化的概率模型，该模型结合了 clustering 和简化，并提供了对任意图结构的模型化框架。我们的模型基于对图进行随机游走的过程的分解。它允许不受限制的参数化，通过简单的梯度下降优化。通过宽松化硬 clustering 到软 clustering，我们的算法将硬 clustering 问题转化为可解决的问题。我们在一个 sintetic 图上以及一些无监督学习任务中应用了我们的算法，包括orthographic 和 phonological 数据的二分和三分 clustering。
</details></li>
</ul>
<hr>
<h2 id="A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing"><a href="#A-Sequential-Meta-Transfer-SMT-Learning-to-Combat-Complexities-of-Physics-Informed-Neural-Networks-Application-to-Composites-Autoclave-Processing" class="headerlink" title="A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing"></a>A Sequential Meta-Transfer (SMT) Learning to Combat Complexities of Physics-Informed Neural Networks: Application to Composites Autoclave Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06447">http://arxiv.org/abs/2308.06447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miladramzy/sequentialmetatransferpinns">https://github.com/miladramzy/sequentialmetatransferpinns</a></li>
<li>paper_authors: Milad Ramezankhani, Abbas S. Milani</li>
<li>for: 解决非线性偏微分方程（PDE）问题，即使在长时间域内。</li>
<li>methods: 使用 физи学 informed neural networks（PINNs）和sequential meta-transfer（SMT）学习框架。</li>
<li>results: 比传统PINNs更高效地解决复杂系统问题，并且具有更好的适应性。<details>
<summary>Abstract</summary>
Physics-Informed Neural Networks (PINNs) have gained popularity in solving nonlinear partial differential equations (PDEs) via integrating physical laws into the training of neural networks, making them superior in many scientific and engineering applications. However, conventional PINNs still fall short in accurately approximating the solution of complex systems with strong nonlinearity, especially in long temporal domains. Besides, since PINNs are designed to approximate a specific realization of a given PDE system, they lack the necessary generalizability to efficiently adapt to new system configurations. This entails computationally expensive re-training from scratch for any new change in the system. To address these shortfalls, in this work a novel sequential meta-transfer (SMT) learning framework is proposed, offering a unified solution for both fast training and efficient adaptation of PINNs in highly nonlinear systems with long temporal domains. Specifically, the framework decomposes PDE's time domain into smaller time segments to create "easier" PDE problems for PINNs training. Then for each time interval, a meta-learner is assigned and trained to achieve an optimal initial state for rapid adaptation to a range of related tasks. Transfer learning principles are then leveraged across time intervals to further reduce the computational cost.Through a composites autoclave processing case study, it is shown that SMT is clearly able to enhance the adaptability of PINNs while significantly reducing computational cost, by a factor of 100.
</details>
<details>
<summary>摘要</summary>
physics-informed neural networks (PINNs) 已经在解决非线性偏微分方程 (PDEs) 中获得了广泛应用，通过将物理法则 integrate 到 neural networks 的训练中，使其在科学和工程应用中脱颖而出。然而，传统 PINNs 仍然缺乏对复杂系统的准确描述能力，特别是在长时间领域中。此外，由于 PINNs 是设计来描述特定的 PDE 系统实现，因此缺乏能够快速适应新系统配置的一般化能力。这会导致 computationally expensive re-training from scratch 的问题。为了解决这些不足，这个研究提出了一个新的sequential meta-transfer (SMT) 学习框架，可以提供快速训练和高效适应 PINNs 的解决方案。具体来说，这个框架将 PDE 的时间领域 decomposed 为 smaller time segments，则将每个时间段赋予一个 meta-learner 进行训练，以实现快速适应一系列相关任务的能力。然后，通过将 transfer learning 原则应用到时间intervals，进一步降低 computional cost。通过一个 composite autoclave processing 案例研究，显示了 SMT 能够优化 PINNs 的适应能力，同时大幅降低 computional cost，比例为 100。
</details></li>
</ul>
<hr>
<h2 id="Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data"><a href="#Neural-Latent-Aligner-Cross-trial-Alignment-for-Learning-Representations-of-Complex-Naturalistic-Neural-Data" class="headerlink" title="Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data"></a>Neural Latent Aligner: Cross-trial Alignment for Learning Representations of Complex, Naturalistic Neural Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06443">http://arxiv.org/abs/2308.06443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheol Jun Cho, Edward F. Chang, Gopala K. Anumanchipalli</li>
<li>for: 本研究旨在解决神经科学中复杂行为的神经实现问题，即找到真实表示神经数据的方法。</li>
<li>methods: 我们提出了一种新的无监督学习框架，神经幽默对应器（NLA），来找到有效、行为相关的神经表示。该方法通过对重复尝试的表示进行对应来学习交叉尝试中的共同信息。此外，我们还提出了一种完全可导时间扭曲模型（TWM）来解决尝试的时间不同问题。</li>
<li>results: 当应用于自然说话的内部电rocorticography（ECoG）数据时，我们的模型可以更好地表示行为，特别是在更低的维度空间中。TWM被验证了通过测量行为协调性 между对应的尝试。我们的框架比基线模型更好地学习了交叉尝试中的共同表示，并且当Visualized时，替换 manifold 显示了在尝试中共享的神经轨迹。<details>
<summary>Abstract</summary>
Understanding the neural implementation of complex human behaviors is one of the major goals in neuroscience. To this end, it is crucial to find a true representation of the neural data, which is challenging due to the high complexity of behaviors and the low signal-to-ratio (SNR) of the signals. Here, we propose a novel unsupervised learning framework, Neural Latent Aligner (NLA), to find well-constrained, behaviorally relevant neural representations of complex behaviors. The key idea is to align representations across repeated trials to learn cross-trial consistent information. Furthermore, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space. The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.
</details>
<details>
<summary>摘要</summary>
The key idea of NLA is to align representations across repeated trials to learn cross-trial consistent information. To achieve this, we propose a novel, fully differentiable time warping model (TWM) to resolve the temporal misalignment of trials. When applied to intracranial electrocorticography (ECoG) of natural speaking, our model learns better representations for decoding behaviors than the baseline models, especially in lower dimensional space.The TWM is empirically validated by measuring behavioral coherence between aligned trials. The proposed framework learns more cross-trial consistent representations than the baselines, and when visualized, the manifold reveals shared neural trajectories across trials.Here is the translation in Simplified Chinese:理解人类复杂行为的神经实现是神经科学的一个主要目标。为了实现这一目标，寻找真实的神经数据表示是非常困难的，因为行为的复杂性和神经信号的噪声比（SNR）都很低。在这里，我们提出了一种新的无监督学习框架——神经潜在适应器（NLA），以找到行为相关的神经表示。NLA的关键思想是将重复尝试的表示进行对齐，以学习跨试验可靠信息。为了实现这一点，我们提出了一种全部可导的时间折叠模型（TWM），以解决试验的时间不同问题。当应用于自然语言说话的电rocorticalography（ECoG）时，我们的模型可以比基eline模型更好地学习行为的表示，特别是在lower dimensional space中。TWM的实验验证了我们的模型可以更好地处理行为听起来的听起来的听起来，并且当Visualize的时候，曾经的折叠 manifold  revelas shared neural trajectories across trials。
</details></li>
</ul>
<hr>
<h2 id="A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media"><a href="#A-Domain-adaptive-Physics-informed-Neural-Network-for-Inverse-Problems-of-Maxwell’s-Equations-in-Heterogeneous-Media" class="headerlink" title="A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media"></a>A Domain-adaptive Physics-informed Neural Network for Inverse Problems of Maxwell’s Equations in Heterogeneous Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06436">http://arxiv.org/abs/2308.06436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Piao, Hong Gu, Aina Wang, Pan Qin</li>
<li>for: 解决Maxwell方程在不同媒质中的逆问题</li>
<li>methods: 使用Physics-informed neural networks (PINNs)和适应域训练策略</li>
<li>results: 在两个案例研究中证明了domain-adaptive PINN的有效性<details>
<summary>Abstract</summary>
Maxwell's equations are a collection of coupled partial differential equations (PDEs) that, together with the Lorentz force law, constitute the basis of classical electromagnetism and electric circuits. Effectively solving Maxwell's equations is crucial in various fields, like electromagnetic scattering and antenna design optimization. Physics-informed neural networks (PINNs) have shown powerful ability in solving PDEs. However, PINNs still struggle to solve Maxwell's equations in heterogeneous media. To this end, we propose a domain-adaptive PINN (da-PINN) to solve inverse problems of Maxwell's equations in heterogeneous media. First, we propose a location parameter of media interface to decompose the whole domain into several sub-domains. Furthermore, the electromagnetic interface conditions are incorporated into a loss function to improve the prediction performance near the interface. Then, we propose a domain-adaptive training strategy for da-PINN. Finally, the effectiveness of da-PINN is verified with two case studies.
</details>
<details>
<summary>摘要</summary>
马克斯威尔方程是一系列联动部分偏微分方程（PDEs），与 Лорен茨力学定律共同组成经典电磁学和电路Circuit。有效解决马克斯威尔方程是许多领域的关键，如电磁散射和天线设计优化。physics-informed neural networks（PINNs）已经表现出解决PDEs的强大能力。然而，PINNs仍然在不同媒体中解决马克斯威尔方程困难。为此，我们提出了域 adaptive PINN（da-PINN）解决Maxwell方程的 inverse problem在不同媒体中。首先，我们提出了媒体界面位置参数，将整个领域分解成多个子领域。然后，我们在损失函数中包含了电磁界面条件，以提高预测性能 near the interface。最后，我们提出了适应域训练策略 для da-PINN。Finally, da-PINN的效果被两个案例验证。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration"><a href="#Learn-Single-horizon-Disease-Evolution-for-Predictive-Generation-of-Post-therapeutic-Neovascular-Age-related-Macular-Degeneration" class="headerlink" title="Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration"></a>Learn Single-horizon Disease Evolution for Predictive Generation of Post-therapeutic Neovascular Age-related Macular Degeneration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06432">http://arxiv.org/abs/2308.06432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhan Zhang, Kun Huang, Mingchao Li, Songtao Yuan, Qiang Chen</li>
<li>for: 这 paper 的目的是预测 age-related macular degeneration (nAMD) 的发展。</li>
<li>methods: 这 paper 使用的方法包括 feature encoder、graph evolution module 和 feature decoder。具体来说，feature encoder 将输入 SD-OCT 图像转换为深度特征，然后 graph evolution module 预测了疾病发展过程在高维 latent space 中，并输出了预测的深度特征。最后，feature decoder 将预测的深度特征转换回 SD-OCT 图像。</li>
<li>results: 这 paper 的结果表明，SHENet 可以生成高质量的预测 SD-OCT 图像，同时保持疾病结构和内容的准确性。 qualitative 评估也表明，SHENet 的生成的 SD-OCT 图像比其他方法更有Visual effect。<details>
<summary>Abstract</summary>
Most of the existing disease prediction methods in the field of medical image processing fall into two classes, namely image-to-category predictions and image-to-parameter predictions. Few works have focused on image-to-image predictions. Different from multi-horizon predictions in other fields, ophthalmologists prefer to show more confidence in single-horizon predictions due to the low tolerance of predictive risk. We propose a single-horizon disease evolution network (SHENet) to predictively generate post-therapeutic SD-OCT images by inputting pre-therapeutic SD-OCT images with neovascular age-related macular degeneration (nAMD). In SHENet, a feature encoder converts the input SD-OCT images to deep features, then a graph evolution module predicts the process of disease evolution in high-dimensional latent space and outputs the predicted deep features, and lastly, feature decoder recovers the predicted deep features to SD-OCT images. We further propose an evolution reinforcement module to ensure the effectiveness of disease evolution learning and obtain realistic SD-OCT images by adversarial training. SHENet is validated on 383 SD-OCT cubes of 22 nAMD patients based on three well-designed schemes based on the quantitative and qualitative evaluations. Compared with other generative methods, the generative SD-OCT images of SHENet have the highest image quality. Besides, SHENet achieves the best structure protection and content prediction. Qualitative evaluations also demonstrate that SHENet has a better visual effect than other methods. SHENet can generate post-therapeutic SD-OCT images with both high prediction performance and good image quality, which has great potential to help ophthalmologists forecast the therapeutic effect of nAMD.
</details>
<details>
<summary>摘要</summary>
大多数现有的疾病预测方法在医学图像处理领域都属于两类，即图像到类别预测和图像到参数预测。只有少数作品强调图像到图像预测。与其他多个 horizons 预测不同，眼科医生更偏向于在单个 horizons 上展示更高的预测信任度，这是因为眼科疾病风险预测的偏好。我们提出了单个 horizon 疾病进化网络（SHENet），用于预测基于前治疗 SD-OCT 图像的后治疗 SD-OCT 图像。在 SHENet 中，一个特征编码器将输入 SD-OCT 图像转换为深度特征，然后一个图像进化模块预测疾病进化的过程在高维latent空间中，并输出预测的深度特征。最后，特征解码器将预测的深度特征恢复为 SD-OCT 图像。我们还提出了进化权威模块，以确保疾病进化学习的效果和获得实际的 SD-OCT 图像。SHENet 在 383 个 SD-OCT 立方体上基于三种良好的方案进行验证，并通过量化和质量评价来评估其效果。与其他生成方法相比，SHENet 生成的 SD-OCT 图像的生成质量最高。此外，SHENet 还实现了最好的结构保护和内容预测。质量评价还表明，SHENet 的视觉效果比其他方法更好。SHENet 可以生成具有高预测性和好的图像质量的后治疗 SD-OCT 图像，这有很大的潜在价值，可以帮助眼科医生预测 nAMD 的治疗效果。
</details></li>
</ul>
<hr>
<h2 id="Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science"><a href="#Genetic-heterogeneity-analysis-using-genetic-algorithm-and-network-science" class="headerlink" title="Genetic heterogeneity analysis using genetic algorithm and network science"></a>Genetic heterogeneity analysis using genetic algorithm and network science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06429">http://arxiv.org/abs/2308.06429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhendong Sha, Yuanzhu Chen, Ting Hu</li>
<li>For: This paper is written to address the challenges of identifying disease susceptible genetic variables using genome-wide association studies (GWAS) due to genetic heterogeneity and feature interactions.* Methods: The paper introduces a novel feature selection mechanism for GWAS called Feature Co-selection Network (FCSNet), which extracts heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA) and a non-linear machine learning algorithm to detect feature interactions.* Results: The paper shows the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis, and applies the novel approach to a case-control colorectal cancer GWAS dataset, resulting in synthetic features that explain the genetic heterogeneity in an additional case-only GWAS dataset.Here’s the simplified Chinese version of the three key points:* For: 这篇论文是为了解决基因组宽协调研究（GWAS）中疾病抵触性基因变量的难题，因为基因多样性和特征交互。* Methods: 论文提出了一种新的特征选择机制，即特征共选网络（FCSNet），它从多个独立的特征选择跑程中提取了多种不同的基因变量，并使用一种进化学习算法（GA）和一种非线性机器学习算法来检测特征交互。* Results: 论文通过synthetic数据分析表明了GA基于的特征选择方法的效果，并应用了这种新方法到一个case-control大肠癌GWAS数据集中，得到了解释基因多样性的Synthetic特征。<details>
<summary>Abstract</summary>
Through genome-wide association studies (GWAS), disease susceptible genetic variables can be identified by comparing the genetic data of individuals with and without a specific disease. However, the discovery of these associations poses a significant challenge due to genetic heterogeneity and feature interactions. Genetic variables intertwined with these effects often exhibit lower effect-size, and thus can be difficult to be detected using machine learning feature selection methods. To address these challenges, this paper introduces a novel feature selection mechanism for GWAS, named Feature Co-selection Network (FCSNet). FCS-Net is designed to extract heterogeneous subsets of genetic variables from a network constructed from multiple independent feature selection runs based on a genetic algorithm (GA), an evolutionary learning algorithm. We employ a non-linear machine learning algorithm to detect feature interaction. We introduce the Community Risk Score (CRS), a synthetic feature designed to quantify the collective disease association of each variable subset. Our experiment showcases the effectiveness of the utilized GA-based feature selection method in identifying feature interactions through synthetic data analysis. Furthermore, we apply our novel approach to a case-control colorectal cancer GWAS dataset. The resulting synthetic features are then used to explain the genetic heterogeneity in an additional case-only GWAS dataset.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:通过全 genomic协同asso ciation研究 (GWAS)，可以通过比较患病者和无病者的基因数据来确定疾病抵触性的基因变量。然而，发现这些相互作用具有一定的挑战，因为基因多样性和特征互动。基因变量与这些效应相互作用的情况经常表现出较低的效果大小，因此可能会难以通过机器学习特征选择方法探测。为解决这些挑战，本文提出了一种新的特征选择机制，名为特征合选网络 (FCSNet)。FCS-Net 是基于多个独立的特征选择跑目的基因算法 (GA) 构建的网络，并使用一种非线性机器学习算法探测特征互动。我们还引入了一个社区风险分数 (CRS)，用于量化每个变量subset 的疾病相关度。我们的实验表明，使用我们提出的 GA 基因选择方法可以通过 sintetic 数据分析来识别特征互动。此外，我们还应用了我们的新方法到一个 case-control 大陆癌 GWAS 数据集。得到的 sintetic 特征后来用于解释一个额外的 case-only GWAS 数据集中的基因多样性。
</details></li>
</ul>
<hr>
<h2 id="Multiclass-Learnability-Does-Not-Imply-Sample-Compression"><a href="#Multiclass-Learnability-Does-Not-Imply-Sample-Compression" class="headerlink" title="Multiclass Learnability Does Not Imply Sample Compression"></a>Multiclass Learnability Does Not Imply Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06424">http://arxiv.org/abs/2308.06424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chirag Pabbaraju</li>
<li>for: 本研究证明了一个假设集合可以 admit 一种样本压缩 schemes，即对于每个样本被标注为来自该假设集合中的某个假设，只需保留一小样本，可以推断出整个样本的标签。</li>
<li>methods: 本研究使用了learnable binary hypothesis class 和 multiclass hypothesis class，以及其们的VC dimension 和 DS dimension。</li>
<li>results: 本研究发现，learnable binary hypothesis class 总是可以 admit 一种样本压缩 schemes，但是 learnable multiclass hypothesis class 则不一定可以 admit 样本压缩 schemes，即不一定可以通过保留一小样本来推断出整个样本的标签。<details>
<summary>Abstract</summary>
A hypothesis class admits a sample compression scheme, if for every sample labeled by a hypothesis from the class, it is possible to retain only a small subsample, using which the labels on the entire sample can be inferred. The size of the compression scheme is an upper bound on the size of the subsample produced. Every learnable binary hypothesis class (which must necessarily have finite VC dimension) admits a sample compression scheme of size only a finite function of its VC dimension, independent of the sample size. For multiclass hypothesis classes, the analog of VC dimension is the DS dimension. We show that the analogous statement pertaining to sample compression is not true for multiclass hypothesis classes: every learnable multiclass hypothesis class, which must necessarily have finite DS dimension, does not admit a sample compression scheme of size only a finite function of its DS dimension.
</details>
<details>
<summary>摘要</summary>
一个假设集合满足样本压缩方案，如果每个样本被标注为从集合中的假设，那么只需保留一小样本，使得整个样本上的标签可以被推断出。压缩方案的大小是样本上的子样本的上界。每个可学习的二进制假设集合（必然具有有限VC维度）总是具有一个只具有有限函数与样本大小无关的压缩方案。对多类假设集合，相应的VC维度的概念是DS维度。我们显示了，相应的假设集合不是真的：每个可学习的多类假设集合，必然具有有限DS维度，但并不总是具有只具有有限函数与DS维度无关的压缩方案。
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation"><a href="#Sensitivity-Aware-Mixed-Precision-Quantization-and-Width-Optimization-of-Deep-Neural-Networks-Through-Cluster-Based-Tree-Structured-Parzen-Estimation" class="headerlink" title="Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation"></a>Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06422">http://arxiv.org/abs/2308.06422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedarmin Azizi, Mahdi Nazemi, Arash Fayyazi, Massoud Pedram</li>
<li>for: 这个研究旨在提高深度学习模型的设计优化，以提高模型的效率。</li>
<li>methods: 本研究使用了一种新的搜寻机制，可以自动选择个别神经网络层的最佳位元和层宽。这些搜寻机制利用了希腊数-基于的删除，以确保删除不必要的参数。然后，我们使用了一种基于对应的树结构的Parzen估计器，以建立优化的对应模型。</li>
<li>results: 我们的方法在知名的数据集上进行了严谨的测试，与现有的方法相比， recording an impressive 20% decrease in model size without compromising accuracy. In addition, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available.<details>
<summary>Abstract</summary>
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 20% decrease in model size without compromising accuracy. Additionally, our method boasts a 12x reduction in search time relative to the best search-focused strategies currently available. As a result, our proposed method represents a leap forward in neural network design optimization, paving the way for quick model design and implementation in settings with limited resources, thereby propelling the potential of scalable deep learning solutions.
</details>
<details>
<summary>摘要</summary>
深度学习模型的复杂性和计算需求逐渐增长，因此选择最佳的神经网络层宽和批处理层宽成为了至关重要的一环。这项工作提出了一种新的搜索机制，可以自动选择具有最佳性能的神经网络层宽和批处理层宽。通过利用层次结构的Parzen估计器来构建封闭的搜索空间，我们可以快速地探索不同的建筑方案，并快速地找到最佳的设计。我们通过对知名数据集进行严格的测试，证明了我们的方法与现有方法相比，可以减少模型大小20%，同时保持准确性。此外，我们的方法可以减少搜索时间12倍，相比于目前最佳的搜索焦点策略。因此，我们的提议方法 represents a significant advance in neural network design optimization, paving the way for rapid model design and implementation in resource-constrained settings, and thereby accelerating the potential of scalable deep learning solutions.
</details></li>
</ul>
<hr>
<h2 id="Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review"><a href="#Pedestrian-Trajectory-Prediction-in-Pedestrian-Vehicle-Mixed-Environments-A-Systematic-Review" class="headerlink" title="Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review"></a>Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06419">http://arxiv.org/abs/2308.06419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</li>
<li>for: 本研究旨在提供一种实用的行人轨迹预测算法，用于自动驾驶车辆（AV）在与行人共同使用的空间中规划路径。</li>
<li>methods: 本文系统性地查询了在文献中提出的不同方法，用于模拟行人轨迹预测在交通工具存在下。文中还详细讨论了与交通工具交互对行人未来动向的影响，以及不同变量如预测不确定性和行为差异如何在先前提出的预测模型中被考虑。</li>
<li>results: 文中提出了1260个唯一的同行评审文章，从ACM数字图书馆、IEEE Xplore和Scopus数据库中搜索到。64篇文章符合包含和排除条件，因此被包含在最终审查中。文中还提供了各种轨迹数据集的概述，包括行人和交通工具的轨迹数据。文中还讨论了未来研究中的潜在漏洞和方向，如更有效的交互代理在深度学习方法中定义，以及更多的混合交通环境中的数据采集。<details>
<summary>Abstract</summary>
Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed.
</details>
<details>
<summary>摘要</summary>
планирование пути автономного транспортного средства (АВ) в пространстве, где присутствуют пешеходы, требует рассмотрения предполагаемых траекторий пешеходов в будущем. практический алгоритм предсказания траекторий пешеходов для использования АВов в неструктурированных средах должен учитывать влияние взаимодействия автомобиля с пешеходами на будущие движения пешеходов. в этом смысле, эта статья систематически обзорывает разные методы, предложенные в литературе для моделирования предсказания траекторий пешеходов в присутствии автомобилей, которые могут быть применены в неструктурированных средах. эта статья также рассматривает конкретные аспекты взаимодействия пешехода-автомобиль (в сравнении с взаимодействием пешеходов-пешеходов) и обсуждает, как различные переменные, такие как неопределенности предсказаний и различия в поведении, учитываются в предыдущих моделях предсказания. following PRISMA guidelines, articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. a total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. an overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments, are discussed.
</details></li>
</ul>
<hr>
<h2 id="Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering"><a href="#Learning-Bayesian-Networks-with-Heterogeneous-Agronomic-Data-Sets-via-Mixed-Effect-Models-and-Hierarchical-Clustering" class="headerlink" title="Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering"></a>Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via Mixed-Effect Models and Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06399">http://arxiv.org/abs/2308.06399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Vallegi, Marco Scutari, Federico Mattia Stefanini</li>
<li>for:  agronomic studies, handling hierarchical data with complex networks of causal relationships</li>
<li>methods:  Bayesian networks (BNs) with integrated random effects, based on linear mixed-effects models</li>
<li>results:  enhanced structural learning, discovery of new connections, improved model specification, reduction in prediction errors from 28% to 17%<details>
<summary>Abstract</summary>
Research involving diverse but related data sets, where associations between covariates and outcomes may vary, is prevalent in various fields including agronomic studies. In these scenarios, hierarchical models, also known as multilevel models, are frequently employed to assimilate information from different data sets while accommodating their distinct characteristics. However, their structure extend beyond simple heterogeneity, as variables often form complex networks of causal relationships.   Bayesian networks (BNs) provide a powerful framework for modelling such relationships using directed acyclic graphs to illustrate the connections between variables. This study introduces a novel approach that integrates random effects into BN learning. Rooted in linear mixed-effects models, this approach is particularly well-suited for handling hierarchical data. Results from a real-world agronomic trial suggest that employing this approach enhances structural learning, leading to the discovery of new connections and the improvement of improved model specification. Furthermore, we observe a reduction in prediction errors from 28\% to 17\%. By extending the applicability of BNs to complex data set structures, this approach contributes to the effective utilisation of BNs for hierarchical agronomic data. This, in turn, enhances their value as decision-support tools in the field.
</details>
<details>
<summary>摘要</summary>
研究涉及多元相关数据集，其中covariates和结果变量之间存在关系的现象，在各个领域，如农学研究，非常普遍。在这些情况下，层次模型，也称为多级模型，经常被使用，以融合不同数据集的信息，同时适应它们的特点。然而，这些结构超出了简单的不同性，因为变量经常形成复杂的 causal 关系网络。� Bayesian networks（BNs）提供了一个强大的模型化这些关系的框架，使用导向的无环图来示出变量之间的连接。本研究提出了一种新的方法，将随机效应 integrate into BN 学习。基于线性混合效应模型，这种方法特别适用于处理层次数据。实际 agronomic 试验结果表明，通过使用这种方法，可以提高结构学习，发现新的连接，并提高模型规定的精度。此外，我们发现预测错误率从28%降至17%。通过扩展 BNs 的应用范围，使其能够更好地处理复杂数据集结构，这种方法增加了 BNs 作为决策支持工具的价值。
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models"><a href="#Detecting-and-Preventing-Hallucinations-in-Large-Vision-Language-Models" class="headerlink" title="Detecting and Preventing Hallucinations in Large Vision Language Models"></a>Detecting and Preventing Hallucinations in Large Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06394">http://arxiv.org/abs/2308.06394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Gunjal, Jihan Yin, Erhan Bas<br>for:The paper aims to address the issue of hallucinations in instruction-tuned large vision language models (LVLMs) for visual question answering (VQA).methods:The authors introduce a new dataset called M-HalDetect, which is a multi-modal hallucination detection dataset for detailed image descriptions. They also propose a novel optimization method called Fine-grained Direct Preference Optimization (FDPO) to reduce hallucinations in LVLMs.results:The authors evaluate the effectiveness of M-HalDetect and FDPO on several state-of-the-art LVLMs, including InstructBLIP, LLaVA, and mPLUG-OWL. They find that M-HalDetect can reduce hallucination rates in InstructBLIP by 41%, and FDPO can reduce hallucination rates by 55%. Additionally, they find that their reward model generalizes well to other multi-modal models and has a strong correlation with human evaluated accuracy scores.<details>
<summary>Abstract</summary>
Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a (M)ultimodal (Hal)lucination (Detect)ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling. We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores.
</details>
<details>
<summary>摘要</summary>
现代化的启示抽象语言模型（LVLM）在多模态任务上进行总结的能力已经得到了显著提高，尤其是在视觉问答（VQA）领域。然而，使模型生成具有详细Visualgrounding的回答仍然是一个挑战。我们发现，even the current state-of-the-art LVLMs（InstructBLIP）仍然包含了30%的虚假文本，包括不存在的物体、不准确的描述和关系。为解决这个问题，我们介绍了M-HalDetect，一个多模态虚假检测 dataset，可以用于训练和测试模型，以避免虚假检测。M-HalDetect包含16k细致的VQA例子，使其成为了首个多模态虚假检测 dataset。与前一代研究只考虑对象虚假，我们还注释了不准确的实体描述和关系。为证明M-HalDetect的潜在性，我们通过我们的新的精细直接偏好优化（FDPO）来优化InstructBLIP。我们还通过多模态 reward models来训练精细的多模态奖励模型，并通过best-of-n拒绝采样来评估其效果。我们对FDPO和拒绝采样进行了人工评估，并发现它们可以降低InstructBLIP中的虚假率 by 41%和55%。此外，我们发现我们的奖励模型可以泛化到其他多模态模型，降低LLaVA和mPLUG-OWL中的虚假率 by 15%和57%，并与人类评估准确率有强相关性。
</details></li>
</ul>
<hr>
<h2 id="Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion"><a href="#Phoneme-Hallucinator-One-shot-Voice-Conversion-via-Set-Expansion" class="headerlink" title="Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion"></a>Phoneme Hallucinator: One-shot Voice Conversion via Set Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06382">http://arxiv.org/abs/2308.06382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PhonemeHallucinator/Phoneme_Hallucinator">https://github.com/PhonemeHallucinator/Phoneme_Hallucinator</a></li>
<li>paper_authors: Siyuan Shan, Yang Li, Amartya Banerjee, Junier B. Oliva</li>
<li>for: 这篇论文的目的是提出一种新的语音变换技术，以提高语音变换的Intelligibility和Speaker Similarity。</li>
<li>methods: 这篇论文使用了一种新的模型，即“Phoneme Hallucinator”，可以基于短时间内的目标说话人声音（例如3秒）生成多样化和高质量的目标说话人音频。</li>
<li>results: 对比于现有的语音变换方法，“Phoneme Hallucinator”在Intelligibility和Speaker Similarity两个指标上均表现出色，并且不需要文本标注和支持任意转换。<details>
<summary>Abstract</summary>
Voice conversion (VC) aims at altering a person's voice to make it sound similar to the voice of another person while preserving linguistic content. Existing methods suffer from a dilemma between content intelligibility and speaker similarity; i.e., methods with higher intelligibility usually have a lower speaker similarity, while methods with higher speaker similarity usually require plenty of target speaker voice data to achieve high intelligibility. In this work, we propose a novel method \textit{Phoneme Hallucinator} that achieves the best of both worlds. Phoneme Hallucinator is a one-shot VC model; it adopts a novel model to hallucinate diversified and high-fidelity target speaker phonemes based just on a short target speaker voice (e.g. 3 seconds). The hallucinated phonemes are then exploited to perform neighbor-based voice conversion. Our model is a text-free, any-to-any VC model that requires no text annotations and supports conversion to any unseen speaker. Objective and subjective evaluations show that \textit{Phoneme Hallucinator} outperforms existing VC methods for both intelligibility and speaker similarity.
</details>
<details>
<summary>摘要</summary>
声音转换（VC）目标是使一个人的声音与另一个人的声音相似，同时保持语言内容的正确性。现有的方法受到一种权衡问题：即具有更高的智能可读性通常具有较低的说话人类似性，而具有更高的说话人类似性通常需要大量的目标说话人声音数据来实现高度的智能可读性。在这项工作中，我们提出了一种新的方法——《phoneme hallucinator》。这是一个一枚VC模型，它采用了一种新的模型来幻化具有多样性和高精度的目标说话人声音，基于短时间内的目标说话人声音（例如3秒）。这些幻化的声音然后被利用来进行邻居基于的声音转换。我们的模型是文本 libre，任何到任何的VC模型，不需要文本注释，并且支持转换到任何未看过的说话人。对象和主观评估表明，《phoneme hallucinator》比既有VC方法更高的智能可读性和说话人类似性。
</details></li>
</ul>
<hr>
<h2 id="DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System"><a href="#DCNFIS-Deep-Convolutional-Neuro-Fuzzy-Inference-System" class="headerlink" title="DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System"></a>DCNFIS: Deep Convolutional Neuro-Fuzzy Inference System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06378">http://arxiv.org/abs/2308.06378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba Yeganejou, Kimia Honari, Ryan Kluzinski, Scott Dick, Michael Lipsett, James Miller</li>
<li>for: 提高可解释人工智能中的透明度和准确性之间的负面选择。</li>
<li>methods: 使用深度神经网络和规则引擎结合深度学习模型，设计了一种新的深度征函数逻辑决策系统（DCNFIS），并证明DCNFIS可以与现有的卷积神经网络相比，在四个常见的数据集上达到相同的准确性。</li>
<li>results: DCNFIS可以在Fashion-MNIST数据集上生成saliency map，并且对这些解释进行了进一步的研究。<details>
<summary>Abstract</summary>
A key challenge in eXplainable Artificial Intelligence is the well-known tradeoff between the transparency of an algorithm (i.e., how easily a human can directly understand the algorithm, as opposed to receiving a post-hoc explanation), and its accuracy. We report on the design of a new deep network that achieves improved transparency without sacrificing accuracy. We design a deep convolutional neuro-fuzzy inference system (DCNFIS) by hybridizing fuzzy logic and deep learning models and show that DCNFIS performs as accurately as three existing convolutional neural networks on four well-known datasets. We furthermore that DCNFIS outperforms state-of-the-art deep fuzzy systems. We then exploit the transparency of fuzzy logic by deriving explanations, in the form of saliency maps, from the fuzzy rules encoded in DCNFIS. We investigate the properties of these explanations in greater depth using the Fashion-MNIST dataset.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在可解释人工智能中是论文知名的质量和可读性之间的贸易。我们报告了一种新的深度网络的设计，该网络可以提高可读性而不 sacrificing 精度。我们设计了一种深度卷积神经推理系统（DCNFIS），通过将神经网络和推理逻辑模型相结合，并证明 DCNFIS 与三种现有的卷积神经网络在四个常见数据集上的性能相同。此外，我们还证明 DCNFIS 在深度推理系统中表现更好。然后，我们利用推理逻辑的可读性，从 DCNFIS 中提取出解释，以干扰 maps 的形式。我们对 Fashion-MNIST 数据集进行更深入的调查，以explore 这些解释的性质。
</details></li>
</ul>
<hr>
<h2 id="UAMM-UBET-Automated-Market-Maker"><a href="#UAMM-UBET-Automated-Market-Maker" class="headerlink" title="UAMM: UBET Automated Market Maker"></a>UAMM: UBET Automated Market Maker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06375">http://arxiv.org/abs/2308.06375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jiwoong Im, Alexander Kondratskiy, Vincent Harvey, Hsuan-Wei Fu</li>
<li>for: 这篇论文是为了解决传统自适应市场制定价机制（AMM）的局限性，提出了一种新的价格计算方法——UBET AMM（UAMM）。</li>
<li>methods: UAMM使用外部市场价格和流动性池的不稳定损失来计算价格，并保持愿意产品曲线的定量性。关键元素是根据目标均衡来确定合适的滑动量，以避免流动性池的不稳定损失。</li>
<li>results: 我们的方法可以在有效的外部市场价格下消除投资机会。<details>
<summary>Abstract</summary>
Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.
</details>
<details>
<summary>摘要</summary>
自动化市场制造机制（AMM）是分布式交易所（DEX）中使用的价格计算机制。传统的 AMM 方法受限于基于自己的流动性池价格计算，不考虑外部市场或流动性提供者风险管理。在这篇论文中，我们提出了一种新的方法，称为 UBET AMM（UAMM），它根据外部市场价格和流动性池的不稳定损失计算价格。尽管依赖于外部市场价格，我们的方法保持了恒定的产品曲线的属性，当计算滑块时。UBET AMM 的关键元素是确定合适的滑块量，以达到目标均衡。这种办法鼓励流动性池减少不稳定损失。我们示出，当外部市场价格高效时，我们的方法可以消除投资机会。
</details></li>
</ul>
<hr>
<h2 id="Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems"><a href="#Topic-Level-Bayesian-Surprise-and-Serendipity-for-Recommender-Systems" class="headerlink" title="Topic-Level Bayesian Surprise and Serendipity for Recommender Systems"></a>Topic-Level Bayesian Surprise and Serendipity for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06368">http://arxiv.org/abs/2308.06368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ton-moy/surprise-and-serendipity">https://github.com/ton-moy/surprise-and-serendipity</a></li>
<li>paper_authors: Tonmoy Hasan, Razvan Bunescu<br>for: This paper aims to mitigate the filter bubble problem in recommender systems by incorporating serendipity into the recommendation process.methods: The paper proposes a content-based formulation of serendipity that is rooted in Bayesian surprise, and uses this formulation to measure the serendipity of items after they are consumed and rated by the user. The paper also introduces a collaborative-filtering component that identifies similar users.results: The experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.<details>
<summary>Abstract</summary>
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 books read by 4 users in terms of their time-dependent, topic-level surprise. Experimental evaluations show that models that use Bayesian surprise correlate much better with the manual annotations of topic-level surprise than distance-based heuristics, and also obtain better serendipitous item recommendation performance.
</details>
<details>
<summary>摘要</summary>
一个推荐系统可以将推荐项目单独根据用户的项目点击历史进行最佳化，从而创建一个范本径（filter bubble），使用户不会获得来自新、未见类别的项目。为了解决这个问题，我们可以推荐项目具有高度的意外性，即让用户惊喜的项目，这些项目很可能会获得高度的评价。在这篇论文中，我们提出了一个基于 bayesian 的内容基式，用于衡量项目的意外性，并且与协同推荐 ком成组件相结合，从而为用户提供意外性高的项目推荐。为了促进项目级模型的surprise和serendipity的评估，我们创建了一个基于goodreads的阅读历史数据集，包含26,000名用户和1,300,000本书，并 manually annotate 449本书，其中4名用户在不同的时间点阅读这些书籍。实验结果显示，使用 bayesian  surprise 可以与距离基于的规律更好地与手动标注的题目级surprise相对较好，并且也可以获得更好的意外性项目推荐性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Distributions-via-Monte-Carlo-Marginalization"><a href="#Learning-Distributions-via-Monte-Carlo-Marginalization" class="headerlink" title="Learning Distributions via Monte-Carlo Marginalization"></a>Learning Distributions via Monte-Carlo Marginalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06352">http://arxiv.org/abs/2308.06352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenqiu Zhao, Guanfang Dong, Anup Basu</li>
<li>for: 学习难以求解的分布。</li>
<li>methods: 使用参数化分布模型（如混合型分布）来 aproximate 难以求解的分布，并使用 Monte-Carlo Marginalization 和 Kernel Density Estimation 解决计算复杂性和优化过程不可导的问题。</li>
<li>results: 提出了一种可以学习复杂分布的方法，该方法可以替代变量推理（VAE），并在标准数据集和 sintetic 数据上进行了实验，证明了该方法的效果。<details>
<summary>Abstract</summary>
We propose a novel method to learn intractable distributions from their samples. The main idea is to use a parametric distribution model, such as a Gaussian Mixture Model (GMM), to approximate intractable distributions by minimizing the KL-divergence. Based on this idea, there are two challenges that need to be addressed. First, the computational complexity of KL-divergence is unacceptable when the dimensions of distributions increases. The Monte-Carlo Marginalization (MCMarg) is proposed to address this issue. The second challenge is the differentiability of the optimization process, since the target distribution is intractable. We handle this problem by using Kernel Density Estimation (KDE). The proposed approach is a powerful tool to learn complex distributions and the entire process is differentiable. Thus, it can be a better substitute of the variational inference in variational auto-encoders (VAE). One strong evidence of the benefit of our method is that the distributions learned by the proposed approach can generate better images even based on a pre-trained VAE's decoder. Based on this point, we devise a distribution learning auto-encoder which is better than VAE under the same network architecture. Experiments on standard dataset and synthetic data demonstrate the efficiency of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于从样本中学习不可解 Distribution。主要想法是使用参数化分布模型，如 Gaussian Mixture Model（GMM），来近似不可解 Distribution，并且通过最小化KL-分布来实现。然而，存在两个挑战：首先，在分布维度增加时，KL-分布的计算复杂度过高；其次，目标分布是不可导的，因此Optimization过程中的导数不存在。我们解决了这两个问题，使用Monte-Carlo Marginalization（MCMarg）和Kernel Density Estimation（KDE）。我们的方法可以学习复杂的分布，整个过程都是导数可导的，因此可以作为VAE中的更好的替补。我们的方法可以在标准数据集和synthetic数据上实现，并且在具有相同网络架构下，我们设计了一个更好的分布学习自动编码器，比VAE更好。Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have been translated differently in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Mirror-Diffusion-Models"><a href="#Mirror-Diffusion-Models" class="headerlink" title="Mirror Diffusion Models"></a>Mirror Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06342">http://arxiv.org/abs/2308.06342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cran/DIMORA">https://github.com/cran/DIMORA</a></li>
<li>paper_authors: Jaesung Tae</li>
<li>for: 本研究旨在应用扩散模型到分类数据领域，并提出了一种基于镜像Langevin算法的受限采样问题的理论框架。</li>
<li>methods: 本文提出了一种基于镜像扩散模型的受限采样算法，并在简单的顺序扩散问题上进行了实验 validate。</li>
<li>results: 研究表明，镜像扩散模型在受限采样问题上具有良好的性能，并且可以在各种流行的领域，如图像和文本生成等，进行自然的扩展。<details>
<summary>Abstract</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.
</details>
<details>
<summary>摘要</summary>
Diffusion models have successfully been applied to generative tasks in various continuous domains. However, applying diffusion to discrete categorical data remains a non-trivial task. Moreover, generation in continuous domains often requires clipping in practice, which motivates the need for a theoretical framework for adapting diffusion to constrained domains. Inspired by the mirror Langevin algorithm for the constrained sampling problem, in this theoretical report we propose Mirror Diffusion Models (MDMs). We demonstrate MDMs in the context of simplex diffusion and propose natural extensions to popular domains such as image and text generation.Here's the translation in Simplified Chinese:diffusion模型在各种连续领域中已经成功应用于生成任务。然而，将diffusion应用于分类数据仍然是一个非常困难的任务。此外，在连续领域中的生成通常需要剪辑在实践中，这引发了需要适应受限的领域的理论框架。以mirror langevin算法为 inspirations，在这份理论报告中我们提出了镜像扩散模型（MDM）。我们在简单领域中示例了MDM，并提出了自然的扩展到流行的领域，如图像和文本生成。
</details></li>
</ul>
<hr>
<h2 id="Size-Lowerbounds-for-Deep-Operator-Networks"><a href="#Size-Lowerbounds-for-Deep-Operator-Networks" class="headerlink" title="Size Lowerbounds for Deep Operator Networks"></a>Size Lowerbounds for Deep Operator Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06338">http://arxiv.org/abs/2308.06338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirbit Mukherjee, Amartya Roy</li>
<li>for: 本研究目的是为了确定深度运算网络（DeepONet）解决杂项问题所需的数据大小下限。</li>
<li>methods: 本研究使用了数据依赖的下界来证明深度运算网络需要一定的数据大小来实现低训练错误。特别是，我们证明在$n$个数据点上获得低训练错误需要通过增加分支网络和主干网络的公共输出维度的扩展。</li>
<li>results: 我们通过实验示例，表明在固定模型大小下，通过增加公共输出维度，可以实现 monotonic 下降的训练错误。此外，我们还发现，随着数据大小的增加，训练错误会 quadratic 下降。<details>
<summary>Abstract</summary>
Deep Operator Networks are an increasingly popular paradigm for solving regression in infinite dimensions and hence solve families of PDEs in one shot. In this work, we aim to establish a first-of-its-kind data-dependent lowerbound on the size of DeepONets required for them to be able to reduce empirical error on noisy data. In particular, we show that for low training errors to be obtained on $n$ data points it is necessary that the common output dimension of the branch and the trunk net be scaling as $\Omega \left ( {\sqrt{n}} \right )$. This inspires our experiments with DeepONets solving the advection-diffusion-reaction PDE, where we demonstrate the possibility that at a fixed model size, to leverage increase in this common output dimension and get monotonic lowering of training error, the size of the training data might necessarily need to scale quadratically with it.
</details>
<details>
<summary>摘要</summary>
深度运算网络（DeepONet）是一种越来越受欢迎的方法，用于解决无穷维度上的回归问题，并且可以一步解决多个偏微分方程（PDE）。在这项工作中，我们想要建立一个数据依赖的下界，以确定深度运算网络的大小是否足够减少噪声数据上的实际错误。我们显示，为了在 $n$ 个数据点上获得低训练错误， THENET 的通用输出维度和树网络的输出维度必须Scaling as $\Omega \left ( \sqrt{n} \right )$.这种情况 inspirits我们对 DeepONet 解决扩散吸引反应PDE的实验，我们示出，随着模型大小不变，通过增加common output维度，在fixed模型大小下，可以实现 monotonic 下降的训练错误。这意味着，在训练数据规模增加时，可能需要随着common output维度的增加，来降低训练错误。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector"><a href="#Foundation-Model-is-Efficient-Multimodal-Multitask-Model-Selector" class="headerlink" title="Foundation Model is Efficient Multimodal Multitask Model Selector"></a>Foundation Model is Efficient Multimodal Multitask Model Selector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06262">http://arxiv.org/abs/2308.06262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/multitask-model-selector">https://github.com/opengvlab/multitask-model-selector</a></li>
<li>paper_authors: Fanqing Meng, Wenqi Shao, Zhanglin Peng, Chonghe Jiang, Kaipeng Zhang, Yu Qiao, Ping Luo<br>for:This paper addresses an under-explored problem in the field of multi-modal multi-task learning: predicting the performance of pre-trained neural networks on various tasks without fine-tuning them.methods:The proposed method, called EMMS (Efficient Multi-task Model Selector), employs large-scale foundation models to transform diverse label formats into a unified noisy label embedding, and uses a simple weighted linear regression to estimate a model’s transferability.results:EMMS achieves significant performance gains (9.0%, 26.3%, 20.1%, 54.8%, 12.2%) and speedup (5.13x, 6.29x, 3.59x, 6.19x, 5.66x) compared to the state-of-the-art method LogME, while being fast and effective for assessing the transferability of pre-trained models in a multi-task scenario.<details>
<summary>Abstract</summary>
This paper investigates an under-explored but important problem: given a collection of pre-trained neural networks, predicting their performance on each multi-modal task without fine-tuning them, such as image recognition, referring, captioning, visual question answering, and text question answering. A brute-force approach is to finetune all models on all target datasets, bringing high computational costs. Although recent-advanced approaches employed lightweight metrics to measure models' transferability,they often depend heavily on the prior knowledge of a single task, making them inapplicable in a multi-modal multi-task scenario. To tackle this issue, we propose an efficient multi-task model selector (EMMS), which employs large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS can estimate a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee. Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models, making it the first model selection method in the multi-task scenario. For instance, compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves 9.0\%, 26.3\%, 20.1\%, 54.8\%, 12.2\% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, while bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose an efficient multi-task model selector (EMMS), which uses large-scale foundation models to transform diverse label formats such as categories, texts, and bounding boxes of different downstream tasks into a unified noisy label embedding. EMMS estimates a model's transferability through a simple weighted linear regression, which can be efficiently solved by an alternating minimization algorithm with a convergence guarantee.Extensive experiments on 5 downstream tasks with 24 datasets show that EMMS is fast, effective, and generic enough to assess the transferability of pre-trained models. Compared with the state-of-the-art method LogME enhanced by our label embeddings, EMMS achieves a 9.0%, 26.3%, 20.1%, 54.8%, and 12.2% performance gain on image recognition, referring, captioning, visual question answering, and text question answering, respectively, while bringing a 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock time, respectively. The code is available at https://github.com/OpenGVLab/Multitask-Model-Selector.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Resilience-with-Neural-Networks"><a href="#Predicting-Resilience-with-Neural-Networks" class="headerlink" title="Predicting Resilience with Neural Networks"></a>Predicting Resilience with Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06309">http://arxiv.org/abs/2308.06309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karen da Mata, Priscila Silva, Lance Fiondella</li>
<li>for: 这个论文探讨了系统能够抵抗破坏性事件的能力，并应用于多个领域。</li>
<li>methods: 该论文提出了三种人工神经网络（ANN）方法，包括 искусственный神经网络（ANN）、循环神经网络（RNN）和长短期记忆神经网络（LSTM），用于模拟和预测系统性能，包括负因素和正因素对系统抵抗力的影响。</li>
<li>results: 研究结果显示，人工神经网络模型在所有评价指标上都超过了传统模型，特别是LSTM模型的可变R平方和预测误差分别下降了34倍和60%以上。这些结果表明，人工神经网络模型可能在许多重要领域中找到实际应用。<details>
<summary>Abstract</summary>
Resilience engineering studies the ability of a system to survive and recover from disruptive events, which finds applications in several domains. Most studies emphasize resilience metrics to quantify system performance, whereas recent studies propose statistical modeling approaches to project system recovery time after degradation. Moreover, past studies are either performed on data after recovering or limited to idealized trends. Therefore, this paper proposes three alternative neural network (NN) approaches including (i) Artificial Neural Networks, (ii) Recurrent Neural Networks, and (iii) Long-Short Term Memory (LSTM) to model and predict system performance, including negative and positive factors driving resilience to quantify the impact of disruptive events and restorative activities. Goodness-of-fit measures are computed to evaluate the models and compared with a classical statistical model, including mean squared error and adjusted R squared. Our results indicate that NN models outperformed the traditional model on all goodness-of-fit measures. More specifically, LSTMs achieved an over 60\% higher adjusted R squared, and decreased predictive error by 34-fold compared to the traditional method. These results suggest that NN models to predict resilience are both feasible and accurate and may find practical use in many important domains.
</details>
<details>
<summary>摘要</summary>
这篇研究探讨了系统的恢复能力，包括系统在干扰事件后的恢复和回复。这些研究通常强调系统的恢复指标数量，而最近的研究则提出了使用统计模型估计系统的复原时间。然而，以往的研究通常是在资料复原后进行，或仅仅是对理想化趋势进行研究。因此，这篇研究提出了三种人工神经网络（ANN）方法，包括人工神经网络（ANN）、回传神经网络（RNN）和长短期内存（LSTM），以模拟和预测系统的性能，包括负面和正面因素的影响，以量化干扰事件的影响和恢复活动。我们使用了一个传统的统计模型，包括平均方差和调整乘根，来评估模型的适合度。我们的结果显示，ANN模型在所有适合度检查中表现较好，特别是LSTM模型在调整乘根上高于60%，且预测误差下降34倍。这些结果表明，ANN模型可以实际地和精确地预测系统的恢复能力，并在许多重要领域中找到实际应用。
</details></li>
</ul>
<hr>
<h2 id="FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods"><a href="#FunnyBirds-A-Synthetic-Vision-Dataset-for-a-Part-Based-Analysis-of-Explainable-AI-Methods" class="headerlink" title="FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods"></a>FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06248">http://arxiv.org/abs/2308.06248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visinf/funnybirds">https://github.com/visinf/funnybirds</a></li>
<li>paper_authors: Robin Hesse, Simone Schaub-Meyer, Stefan Roth</li>
<li>for: The paper is written for the field of explainable artificial intelligence (XAI), specifically to address the challenge of evaluating the effectiveness of XAI methods in a fully automatic and systematic manner.</li>
<li>methods: The paper proposes a novel synthetic vision dataset called FunnyBirds and accompanying automatic evaluation protocols to evaluate XAI methods. The dataset allows for semantically meaningful image interventions, such as removing individual object parts, which enables analyzing explanations on a part level and estimating ground-truth part importances.</li>
<li>results: The paper reports results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner. The results show that the proposed evaluation protocols are effective in identifying the strengths and weaknesses of different XAI methods.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了解释人工智能（XAI）领域的一个挑战，具体来说是如何在完全自动和系统atic的方式下评估XAI方法的有效性。</li>
<li>methods: 论文提出了一个新的 sintetic vision dataset名为FunnyBirds，以及一系列的自动评估协议，用于评估XAI方法。该dataset允许进行semantic meaningful的图像干扰，例如 removing各个物体部分，这使得可以分析解释在每个部分上的含义。</li>
<li>results: 论文报告了24种不同的神经网络模型和XAI方法的结果，这些结果表明了评估方法的优劣。<details>
<summary>Abstract</summary>
The field of explainable artificial intelligence (XAI) aims to uncover the inner workings of complex deep neural models. While being crucial for safety-critical domains, XAI inherently lacks ground-truth explanations, making its automatic evaluation an unsolved problem. We address this challenge by proposing a novel synthetic vision dataset, named FunnyBirds, and accompanying automatic evaluation protocols. Our dataset allows performing semantically meaningful image interventions, e.g., removing individual object parts, which has three important implications. First, it enables analyzing explanations on a part level, which is closer to human comprehension than existing methods that evaluate on a pixel level. Second, by comparing the model output for inputs with removed parts, we can estimate ground-truth part importances that should be reflected in the explanations. Third, by mapping individual explanations into a common space of part importances, we can analyze a variety of different explanation types in a single common framework. Using our tools, we report results for 24 different combinations of neural models and XAI methods, demonstrating the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details>
<details>
<summary>摘要</summary>
领域的解释人工智能（XAI）目标是探索复杂的深度神经网络模型的内部工作原理。虽然在安全关键领域非常重要，但XAI自然lacks ground-truth explanations，making its automatic evaluation an unsolved problem。我们解决这个挑战 by proposing a novel synthetic vision dataset named FunnyBirds， accompanied by automatic evaluation protocols。我们的数据集允许进行semantically meaningful image interventions，例如移除个体物体部分，这有三个重要的后果。首先，它允许分析解释在部件层次上进行分析，这更加接近人类理解的水平than existing methods that evaluate on a pixel level。其次，通过比较模型输出对具有移除部件的输入的比较，我们可以估算ground-truth part importances，这些importances应该被反映在解释中。最后，将各种解释映射到一个共同的部件importances空间，我们可以分析多种不同的解释类型在一个共同框架中。使用我们的工具，我们报告了24种不同的神经网络模型和XAI方法的结果，这些结果 Demonstrate the strengths and weaknesses of the assessed methods in a fully automatic and systematic manner.
</details></li>
</ul>
<hr>
<h2 id="Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression"><a href="#Private-Distribution-Learning-with-Public-Data-The-View-from-Sample-Compression" class="headerlink" title="Private Distribution Learning with Public Data: The View from Sample Compression"></a>Private Distribution Learning with Public Data: The View from Sample Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06239">http://arxiv.org/abs/2308.06239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shai Ben-David, Alex Bie, Clément L. Canonne, Gautam Kamath, Vikrant Singhal</li>
<li>for: 本文研究了在公共数据上进行隐私学习的问题，即公共私有学习。learner 是 Given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.</li>
<li>methods: 本文使用了 sample compression scheme for $\mathcal Q$ 和 list learning 来研究公共私有学习的可行性。</li>
<li>results: 本文的结果包括：(1) 回归 previous results on Gaussians over $\mathbb R^d$ ；(2) 适用于任意 $k$-mixtures of Gaussians over $\mathbb R^d$ 的新结果，包括学习复杂性上下文和分布转移抗性学习者的结果，以及 closure properties for public-private learnability under taking mixtures and products of distributions。此外，通过连接到 list learning，本文还证明了对 Gaussian 在 $\mathbb R^d$ 中，至少需要 $d$ 个公共样本来保证私有学习可行性，这与知道的Upper bound of $d+1$ 公共样本很接近。<details>
<summary>Abstract</summary>
We study the problem of private distribution learning with access to public data. In this setup, which we refer to as public-private learning, the learner is given public and private samples drawn from an unknown distribution $p$ belonging to a class $\mathcal Q$, with the goal of outputting an estimate of $p$ while adhering to privacy constraints (here, pure differential privacy) only with respect to the private samples.   We show that the public-private learnability of a class $\mathcal Q$ is connected to the existence of a sample compression scheme for $\mathcal Q$, as well as to an intermediate notion we refer to as list learning. Leveraging this connection: (1) approximately recovers previous results on Gaussians over $\mathbb R^d$; and (2) leads to new ones, including sample complexity upper bounds for arbitrary $k$-mixtures of Gaussians over $\mathbb R^d$, results for agnostic and distribution-shift resistant learners, as well as closure properties for public-private learnability under taking mixtures and products of distributions. Finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.
</details>
<details>
<summary>摘要</summary>
我们研究了公共-私人学习问题，在这种设置下，学习者被公共和私人样本所访问，目标是输出一个未知分布$p$的估计，同时遵循隐私限制（这里是纯度ifferential privacy）只与私人样本相关。我们证明了公共-私人学习可能性与样本压缩 schemes for $\mathcal Q$ 以及 list learning 的存在有关，并且利用这种关系：1. 约束 previous results on Gaussians over $\mathbb R^d$ 的 approximately recovery;2. 导致新的结论，包括 $k$-mixtures of Gaussians over $\mathbb R^d$ 的 sample complexity upper bounds, agnostic 和 distribution-shift resistant learners,以及 distribution under taking mixtures and products of distributions. finally, via the connection to list learning, we show that for Gaussians in $\mathbb R^d$, at least $d$ public samples are necessary for private learnability, which is close to the known upper bound of $d+1$ public samples.
</details></li>
</ul>
<hr>
<h2 id="MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features"><a href="#MaxFloodCast-Ensemble-Machine-Learning-Model-for-Predicting-Peak-Inundation-Depth-And-Decoding-Influencing-Features" class="headerlink" title="MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features"></a>MaxFloodCast: Ensemble Machine Learning Model for Predicting Peak Inundation Depth And Decoding Influencing Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06228">http://arxiv.org/abs/2308.06228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Chun Lee, Lipai Huang, Federico Antolini, Matthew Garcia, Andrew Juanb, Samuel D. Brody, Ali Mostafavi</li>
<li>for: 提供快速、准确和可靠的洪水信息，以支持洪水事件中的决策者、紧急管理人员和基础设施运营人员。</li>
<li>methods: 使用机器学习模型MaxFloodCast，基于物理基础的水动力学模拟，在哈里斯县进行训练，以实现高效和可解释的洪水涵顶深度预测。</li>
<li>results: MaxFloodCast模型在未见数据上达到了0.949的平均R-squared值和0.61 ft的Root Mean Square Error，表明其可靠地预测洪水涵顶深度。验证了飓风哈维和飓风伊梅拉达，MaxFloodCast模型显示出在近实时洪水管理和紧急应急管理中的潜在作用。<details>
<summary>Abstract</summary>
Timely, accurate, and reliable information is essential for decision-makers, emergency managers, and infrastructure operators during flood events. This study demonstrates a proposed machine learning model, MaxFloodCast, trained on physics-based hydrodynamic simulations in Harris County, offers efficient and interpretable flood inundation depth predictions. Achieving an average R-squared of 0.949 and a Root Mean Square Error of 0.61 ft on unseen data, it proves reliable in forecasting peak flood inundation depths. Validated against Hurricane Harvey and Storm Imelda, MaxFloodCast shows the potential in supporting near-time floodplain management and emergency operations. The model's interpretability aids decision-makers in offering critical information to inform flood mitigation strategies, to prioritize areas with critical facilities and to examine how rainfall in other watersheds influences flood exposure in one area. The MaxFloodCast model enables accurate and interpretable inundation depth predictions while significantly reducing computational time, thereby supporting emergency response efforts and flood risk management more effectively.
</details>
<details>
<summary>摘要</summary>
时尚、准确、可靠的信息是决策者、紧急管理者和基础设施操作者在洪水事件中的基本需求。这个研究显示了一个提议的机器学习模型MaxFloodCast，在哈里斯县基于物理学 hydrodynamic 模拟中训练，可以提供高效和可解释的洪水淹没深度预测。在未见数据上，它实现了0.949的平均R-squared和0.61 ft的Root Mean Square Error，证明它在预测洪水淹没深度方面具有可靠性。验证了飓风哈维和飓风Imelda，MaxFloodCast显示了在近实时洪水平原管理和紧急作业中的潜在应用 potential。模型的可解释性帮助决策者对洪水缓和策略提供重要信息，优先级有critical facilities的区域，并考虑在一个区域中的降雨影响洪水暴露。MaxFloodCast模型可以提供高效和可解释的洪水淹没深度预测，同时大幅降低计算时间，以更好地支持紧急 Response efforts和洪水风险管理。
</details></li>
</ul>
<hr>
<h2 id="Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms"><a href="#Automated-Sizing-and-Training-of-Efficient-Deep-Autoencoders-using-Second-Order-Algorithms" class="headerlink" title="Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms"></a>Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06221">http://arxiv.org/abs/2308.06221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanishka Tyagi, Chinmay Rane, Michael Manry</li>
<li>for: 该论文旨在提出一种多步训练方法，用于设计通用线性分类器。</li>
<li>methods: 该方法包括初始化多类线性分类器，然后通过权重裁剪来降低验证错误，同时通过类似于霍-卡希普规则来改进期望输出。接着，输出推导器被拟合成为一个通用的多层感知器中的抑制器。</li>
<li>results: 该paper通过对多层感知器的搜索、验证错误的最小化和权重裁剪来提高总性能。此外，paper还提出了一种批处理算法，用于优化隐藏层大小和训练轮数。最终，paper通过对多层感知器进行权重裁剪和增长来提高其性能。<details>
<summary>Abstract</summary>
We propose a multi-step training method for designing generalized linear classifiers. First, an initial multi-class linear classifier is found through regression. Then validation error is minimized by pruning of unnecessary inputs. Simultaneously, desired outputs are improved via a method similar to the Ho-Kashyap rule. Next, the output discriminants are scaled to be net functions of sigmoidal output units in a generalized linear classifier. We then develop a family of batch training algorithm for the multi layer perceptron that optimizes its hidden layer size and number of training epochs. Next, we combine pruning with a growing approach. Later, the input units are scaled to be the net function of the sigmoidal output units that are then feed into as input to the MLP. We then propose resulting improvements in each of the deep learning blocks thereby improving the overall performance of the deep architecture. We discuss the principles and formulation regarding learning algorithms for deep autoencoders. We investigate several problems in deep autoencoders networks including training issues, the theoretical, mathematical and experimental justification that the networks are linear, optimizing the number of hidden units in each layer and determining the depth of the deep learning model. A direct implication of the current work is the ability to construct fast deep learning models using desktop level computational resources. This, in our opinion, promotes our design philosophy of building small but powerful algorithms. Performance gains are demonstrated at each step. Using widely available datasets, the final network's ten fold testing error is shown to be less than that of several other linear, generalized linear classifiers, multi layer perceptron and deep learners reported in the literature.
</details>
<details>
<summary>摘要</summary>
我们提出了一种多步训练方法用于设计通用线性分类器。首先，通过回归获得初始多类线性分类器。然后，通过剔除不必要的输入，降低验证错误。同时，通过类似于霍-卡希普规则进行改进。接着，输出推定器被映射到通用线性分类器中的sigmoid输出单元中的核函数。我们然后开发了一种批处理训练算法，以优化隐藏层大小和训练轮次数。接着，我们结合剔除和增长策略。最后，输入单元被映射到sigmoid输出单元中的核函数，然后被输入到多层感知器中。我们提出了改进每个深度学习块的方法，从而提高整体深度学习模型的性能。我们讨论了深度学习算法的学习原理和形式化表述，并 investigate了深度学习网络中的许多问题，包括训练问题、理论、数学和实验上的正当性。我们的研究表明，通过使用桌面级计算机资源，可以快速构建高性能的深度学习模型。这与我们的设计哲学相吻合，即建立小而强大的算法。我们的实验表明，使用常用的数据集，最终网络的十倍测试错误比其他线性、通用线性分类器、多层感知器和深度学习者报道的较低。
</details></li>
</ul>
<hr>
<h2 id="Change-Point-Detection-With-Conceptors"><a href="#Change-Point-Detection-With-Conceptors" class="headerlink" title="Change Point Detection With Conceptors"></a>Change Point Detection With Conceptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06213">http://arxiv.org/abs/2308.06213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/noahgade/changepointdetectionwithconceptors">https://github.com/noahgade/changepointdetectionwithconceptors</a></li>
<li>paper_authors: Noah D. Gade, Jordan Rodu</li>
<li>for:  Identifying changes in the data generating process in time series with increasing dimension and temporal dependence.</li>
<li>methods:  Using a conceptor matrix to learn the characteristic dynamics of a specified training window, and a random recurrent neural network to featurize the data.</li>
<li>results:  A method that provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes and applied to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze.<details>
<summary>Abstract</summary>
Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on simulations from several classes of processes, and we evaluate performance with clustering metrics, graphical methods, and observed Type 1 error control. We apply our method to publicly available neural data from rats experiencing bouts of non-REM sleep prior to exploration of a radial maze.
</details>
<details>
<summary>摘要</summary>
非线性变换点检测目的是检测时序序列中数据生成过程中的变化点。这个问题在独立Identical Distribution（i.i.d）数据上得到了广泛的研究，但是随着维度和时间相关性的增加，问题变得更加复杂。为了检测最多一个变换点，我们提议使用特征动力矩阵来学习指定的训练窗口中数据的特征动力。相关的随机回归神经网络作为数据的特征化器，变换点通过单variate量化的距离来确定与一个表征动力矩阵所生成的空间之间的距离。这种模型无关的方法可以提供有价值的可能性点，供进一步研究。我们证明，在某些假设下，该方法可以提供一个一致的变换点估计，并通过移动块bootstrap来生成quantile估计。我们在一些类型的过程的 simulations中测试了该方法，并根据集成度、图形方法和观察到的类型一错控制来评估性能。我们将该方法应用于公共可用的非 REM睡眠前的大鼠脑电信号。
</details></li>
</ul>
<hr>
<h2 id="Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey"><a href="#Safety-in-Traffic-Management-Systems-A-Comprehensive-Survey" class="headerlink" title="Safety in Traffic Management Systems: A Comprehensive Survey"></a>Safety in Traffic Management Systems: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06204">http://arxiv.org/abs/2308.06204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlu Du, Ankan Dash, Jing Li, Hua Wei, Guiling Wang</li>
<li>for: 本研究提供了交通管理系统安全性的全面审查，包括交通管理系统中的安全问题、现有研究的当前状况以及提高交通管理系统安全性的技术和方法。</li>
<li>methods: 本研究审视了交通管理系统中的各种安全问题，包括技术问题、人因问题和管理问题，并评估了现有研究的当前状况和发展趋势。</li>
<li>results: 本研究结果表明，确保交通管理系统安全性是一个复杂的问题，需要考虑技术、人因和管理因素。现有的研究主要集中在技术方面，未来研究应该更加着重于人因和管理方面。<details>
<summary>Abstract</summary>
Traffic management systems play a vital role in ensuring safe and efficient transportation on roads. However, the use of advanced technologies in traffic management systems has introduced new safety challenges. Therefore, it is important to ensure the safety of these systems to prevent accidents and minimize their impact on road users. In this survey, we provide a comprehensive review of the literature on safety in traffic management systems. Specifically, we discuss the different safety issues that arise in traffic management systems, the current state of research on safety in these systems, and the techniques and methods proposed to ensure the safety of these systems. We also identify the limitations of the existing research and suggest future research directions.
</details>
<details>
<summary>摘要</summary>
交通管理系统在公路交通中扮演至关重要的角色，但是使用先进科技在交通管理系统中带来了新的安全挑战。因此，确保交通管理系统的安全性是非常重要的，以预防事故和最小化对交通路用者的影响。在本调查中，我们提供了交通管理系统安全的全面文献评审。具体来说，我们讨论了交通管理系统中不同的安全问题，现有的研究状况，以及确保交通管理系统安全的技术和方法。我们还识别了现有研究的限制，并建议未来研究的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/12/cs.LG_2023_08_12/" data-id="clly4xtdv006pvl88hgyaere6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/6/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="extend next" rel="next" href="/page/8/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">108</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
