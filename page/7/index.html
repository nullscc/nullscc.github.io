
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="http://nullscc.github.io/page/7/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/cs.SD_2023_07_29/" class="article-date">
  <time datetime="2023-07-28T16:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/cs.SD_2023_07_29/">cs.SD - 2023-07-29 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MSStyleTTS-Multi-Scale-Style-Modeling-with-Hierarchical-Context-Information-for-Expressive-Speech-Synthesis"><a href="#MSStyleTTS-Multi-Scale-Style-Modeling-with-Hierarchical-Context-Information-for-Expressive-Speech-Synthesis" class="headerlink" title="MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis"></a>MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context Information for Expressive Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16012">http://arxiv.org/abs/2307.16012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Lei, Yixuan Zhou, Liyang Chen, Zhiyong Wu, Xixin Wu, Shiyin Kang, Helen Meng</li>
<li>for: 这篇论文旨在提高人机交互场景中的自然Expressive speech synthesis，如Audiobooks、Podcasts和语音助手等。</li>
<li>methods: 该论文提出了一种基于多层次上下文信息的 Style modeling 方法，以capture和预测不同级别的样式信息，从而实现自然Expressive speech synthesis。两个子模块，包括多层次样式抽取器和多层次样式预测器，与基于 FastSpeech 2 的语音模型一起训练。预测器利用了层次结构关系来探索上下文信息，并预测样式嵌入的多级别。抽取器从真实的语音样本中提取多层次样式嵌入，并直接导航式预测样式。</li>
<li>results: 论文的实验结果表明，提出的方法在域内和域外的 audiobook 数据集上显著超越了三个基线。此外，我们还进行了上下文信息和多层次样式表示的分析，这些分析结果从未被讨论过。<details>
<summary>Abstract</summary>
Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants. Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence. Whereas, context information in neighboring sentences and multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech. In this paper, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence. Two sub-modules, including multi-scale style extractor and multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model. The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level and subword-level. The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction. Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines. In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.
</details>
<details>
<summary>摘要</summary>
干脆的语音合成是许多人机交互场景中的关键，如Audiobook、Podcast和语音助手。先前的工作主要关注在一个层次上从当前句子中预测样式嵌入。然而，邻近句子的上下文信息和人类语音中的多级样式特征被忽略，使得将多句子文本转化为自然和干脆的语音很困难。在这篇论文中，我们提出了MSStyleTTS方法，用于启发式语音合成，以捕捉和预测不同级别的样式。我们在 FastSpeech 2 基础模型中训练了两个子模块：多级样式抽取器和多级样式预测器。预测器通过考虑语音结构关系来探索层次上下文信息，并预测样式嵌入。抽取器从真实语音中提取多级样式嵌入，并直接引导样式预测。我们对域外和域内 Audiobook 数据集进行评估，结果显示，我们的方法与基eline比较出色。此外，我们还进行了上下文信息和多级样式表示的分析，这些分析从未被讨论过。
</details></li>
</ul>
<hr>
<h2 id="Moisesdb-A-dataset-for-source-separation-beyond-4-stems"><a href="#Moisesdb-A-dataset-for-source-separation-beyond-4-stems" class="headerlink" title="Moisesdb: A dataset for source separation beyond 4-stems"></a>Moisesdb: A dataset for source separation beyond 4-stems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15913">http://arxiv.org/abs/2307.15913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moises-ai/moises-db">https://github.com/moises-ai/moises-db</a></li>
<li>paper_authors: Igor Pereira, Felipe Araújo, Filip Korzeniowski, Richard Vogl</li>
<li>for: 这篇论文是为了介绍一个新的音乐源分离数据集，即MoisesDB dataset，用于音乐源分离。</li>
<li>methods: 该数据集包含240首歌曲，来自45位艺术家，涵盖了12种 музыкальных类型。每首歌曲都有其各自的音频源，分为两级层次的概念分类法，以便建立和评估细化的音乐源分离系统。</li>
<li>results: 本文提供了一个简单易用的Python库，可以下载、处理和使用MoisesDB数据集。此外，文章还提供了数据集的详细文档和分析，以及一些基准结果，用于评估不同精度的开源分离模型。<details>
<summary>Abstract</summary>
In this paper, we introduce the MoisesDB dataset for musical source separation. It consists of 240 tracks from 45 artists, covering twelve musical genres. For each song, we provide its individual audio sources, organized in a two-level hierarchical taxonomy of stems. This will facilitate building and evaluating fine-grained source separation systems that go beyond the limitation of using four stems (drums, bass, other, and vocals) due to lack of data. To facilitate the adoption of this dataset, we publish an easy-to-use Python library to download, process and use MoisesDB. Alongside a thorough documentation and analysis of the dataset contents, this work provides baseline results for open-source separation models for varying separation granularities (four, five, and six stems), and discuss their results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Musical Source Separation的MoisesDB数据集。它包含240首歌曲，来自45位艺术家，涵盖了12种音乐类型。每首歌曲都有其自己的声音来源，按照两级层次的分类系统组织，可以促进建立和评估细致的音乐源分离系统。为了促进这个数据集的采用，我们在Python库中发布了一个易于使用的下载、处理和使用的MoisesDB库。同时，我们也提供了详细的文档和数据集的分析，以及不同的分离精度（四、五、六个声音来源）的基准结果。
</details></li>
</ul>
<hr>
<h2 id="UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models"><a href="#UniBriVL-Robust-Universal-Representation-and-Generation-of-Audio-Driven-Diffusion-Models" class="headerlink" title="UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models"></a>UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15898">http://arxiv.org/abs/2307.15898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Fang, Bowen Gao, Yangjian Wu, Jingwen Cai, Teik Toe Teoh</li>
<li>for: 本研究旨在提出一种基于视界语言（BriVL）的全新语言表示学习方法，以实现多模态应用。</li>
<li>methods: 该方法使用UniBriVL将音频、图像和文本 embedding到共享空间中，解决了语言表示学习中的重要挑战，并能够有效捕捉音频和图像之间的相关性。</li>
<li>results: 实验结果表明，UniBriVL在下游任务中表现出色，并且可以从音频中生成合适的图像。这种方法有很多应用可能性，如语音识别、音频处理和描述系统等。<details>
<summary>Abstract</summary>
Multimodal large models have been recognized for their advantages in various performance and downstream tasks. The development of these models is crucial towards achieving general artificial intelligence in the future. In this paper, we propose a novel universal language representation learning method called UniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal BriVL embeds audio, image, and text into a shared space, enabling the realization of various multimodal applications. Our approach addresses major challenges in robust language (both text and audio) representation learning and effectively captures the correlation between audio and image. Additionally, we demonstrate the qualitative evaluation of the generated images from UniBriVL, which serves to highlight the potential of our approach in creating images from audio. Overall, our experimental results demonstrate the efficacy of UniBriVL in downstream tasks and its ability to choose appropriate images from audio. The proposed approach has the potential for various applications such as speech recognition, music signal processing, and captioning systems.
</details>
<details>
<summary>摘要</summary>
多Modal大型模型已被认可为在多种表现和下游任务中具有优势。这些模型的开发对于实现未来的通用人工智能是关键。本文提出了一种新的通用语言表示学习方法，即UniBriVL，该方法基于bridging-vision-and-language（BriVL）。这种通用BriVL嵌入音频、图像和文本到共享空间中，使得实现多种多Modal应用程序 becomes possible。我们的方法解决了文本和音频语言表示学习中的主要挑战，并有效地捕捉 audio和图像之间的相关性。此外，我们还对UniBriVL生成的图像进行质量评估，以强调我们的方法在创建图像 FROM audio 方面的潜在性。总的来说，我们的实验结果表明UniBriVL在下游任务中的效果和其能够选择合适的图像 FROM audio。该方法在语音识别、音频信号处理和captioning系统等应用中具有潜在的应用前景。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/29/cs.SD_2023_07_29/" data-id="cllshxsp2004o2u88a5yj4vy9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/eess.AS_2023_07_29/" class="article-date">
  <time datetime="2023-07-28T16:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/eess.AS_2023_07_29/">eess.AS - 2023-07-29 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="METTS-Multilingual-Emotional-Text-to-Speech-by-Cross-speaker-and-Cross-lingual-Emotion-Transfer"><a href="#METTS-Multilingual-Emotional-Text-to-Speech-by-Cross-speaker-and-Cross-lingual-Emotion-Transfer" class="headerlink" title="METTS: Multilingual Emotional Text-to-Speech by Cross-speaker and Cross-lingual Emotion Transfer"></a>METTS: Multilingual Emotional Text-to-Speech by Cross-speaker and Cross-lingual Emotion Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15951">http://arxiv.org/abs/2307.15951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinfa Zhu, Yi Lei, Tao Li, Yongmao Zhang, Hongbin Zhou, Heng Lu, Lei Xie</li>
<li>for: 本研究的目的是提高多语言文本译音系统的表达情感能力。</li>
<li>methods: 该方法基于DelightfulTTS模型，并提出了多级情感模型、形态shift信息扰动和vector量化情感匹配等设计。</li>
<li>results: 实验结果表明，METTS模型能够有效地提高cross-speaker和cross-lingual情感传递，并且实现了良好的自然性和情感多样性。<details>
<summary>Abstract</summary>
Previous multilingual text-to-speech (TTS) approaches have considered leveraging monolingual speaker data to enable cross-lingual speech synthesis. However, such data-efficient approaches have ignored synthesizing emotional aspects of speech due to the challenges of cross-speaker cross-lingual emotion transfer - the heavy entanglement of speaker timbre, emotion, and language factors in the speech signal will make a system produce cross-lingual synthetic speech with an undesired foreign accent and weak emotion expressiveness. This paper proposes the Multilingual Emotional TTS (METTS) model to mitigate these problems, realizing both cross-speaker and cross-lingual emotion transfer. Specifically, METTS takes DelightfulTTS as the backbone model and proposes the following designs. First, to alleviate the foreign accent problem, METTS introduces multi-scale emotion modeling to disentangle speech prosody into coarse-grained and fine-grained scales, producing language-agnostic and language-specific emotion representations, respectively. Second, as a pre-processing step, formant shift-based information perturbation is applied to the reference signal for better disentanglement of speaker timbre in the speech. Third, a vector quantization-based emotion matcher is designed for reference selection, leading to decent naturalness and emotion diversity in cross-lingual synthetic speech. Experiments demonstrate the good design of METTS.
</details>
<details>
<summary>摘要</summary>
previous的多语言文本到语音（TTS）方法都是利用单语言说话人数据来实现跨语言语音合成。然而，这些数据效率的方法忽略了语音表达情感的Synthesize aspect，因为跨说话人跨语言情感传递的挑战——语音信号中的说话人特征、情感和语言因素之间存在强相互关系，使得系统生成的跨语言合成语音具有不 DESirable的外国口音和弱情感表达能力。这篇文章提出了多语言情感TTS（METTS）模型，以解决这些问题。specifically，METTS使用DelightfulTTS作为基础模型，并提出以下设计：first，为了缓解外国口音问题，METTS引入多级情感模型，将语音PROsody分解成粗级和细级两个级别，生成语言共通和语言特定的情感表达，分别用于语言独立和语言特定的情感表达。second，作为预处理步骤，METTS使用形材移动信息对参照信号进行扰动，以提高语音特征的分离。third，METTS设计了一个基于 вектор量化的情感匹配器，用于选择参照信号，从而实现了Decent的自然性和情感多样性在跨语言合成语音中。实验表明METTS的好设计。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/29/eess.AS_2023_07_29/" data-id="cllshxspn006p2u88d03l46ja" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/29/eess.IV_2023_07_29/" class="article-date">
  <time datetime="2023-07-28T16:00:00.000Z" itemprop="datePublished">2023-07-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/29/eess.IV_2023_07_29/">eess.IV - 2023-07-29 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Why-is-thermal-imaging-textureless"><a href="#Why-is-thermal-imaging-textureless" class="headerlink" title="Why is thermal imaging textureless"></a>Why is thermal imaging textureless</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15800">http://arxiv.org/abs/2307.15800</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanglinbao/hadar">https://github.com/fanglinbao/hadar</a></li>
<li>paper_authors: Fanglin Bao, Shubhankar Jape, Andrew Schramka, Junjie Wang, Tim E. McGraw, Zubin Jacob<br>for:* 这篇论文旨在解释和克服夜视中的幽灵效应。methods:* 这篇论文使用了TeX视力来重建几何 texture。results:* TeX视力能够成功地重建几何 texture，而传统的夜视技术则无法做到。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Thermal imaging can enable night vision but is usually textureless, well-known as the ghosting effect. The mechanism of this ghosting effect has recently been explained, and TeX vision has been proposed to overcome the ghosting effect. However, it is still unknown for realistic scenarios with non-uniform temperature whether TeX vision can correctly recover geometric textures and how its performance is compared with traditional thermal imaging. Here, we focus on the interplay of geometric textures and non-uniform temperature which is common in realistic thermal imaging, and demonstrate the failure of traditional approaches while TeX vision successfully recovers geometric textures. We also analyze important yet unexplored aspects of the TeX vision theory, and demonstrate a true night vision like broad daylight with the experimentally more feasible Bayer-filter setup. This deepens the understanding of the ghosting effect and bridges the gap between the TeX vision theory and the consumer thermal-imaging market.
</details>
<details>
<summary>摘要</summary>
热影像可以启用夜视，但通常是无文本的，这被称为“幽灵效应”。这个幽灵效应的机制最近才被解释，而TeX视力则被提出来解决幽灵效应。然而，在实际情况下，非uniform温度下TeX视力能够正确地恢复地形文本吗，与传统热影像相比如何 perfomance呢？我们在这里关注实际情况下的地形文本与非uniform温度之间的互动，并证明传统方法无法正确地恢复地形文本，而TeX视力则成功地做到了这一点。我们还分析了TeX视力理论中尚未研究的重要方面，并在实际可行的Bayer滤波器设置下实现了真正的夜视如昼光。这将深入了解幽灵效应，并将TeX视力理论与消费者热影像市场之间的差距bridged。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/29/eess.IV_2023_07_29/" data-id="cllshxsq9008p2u886fvf02wx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.LG_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/cs.LG_2023_07_28/">cs.LG - 2023-07-28 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TriadNet-Sampling-free-predictive-intervals-for-lesional-volume-in-3D-brain-MR-images"><a href="#TriadNet-Sampling-free-predictive-intervals-for-lesional-volume-in-3D-brain-MR-images" class="headerlink" title="TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images"></a>TriadNet: Sampling-free predictive intervals for lesional volume in 3D brain MR images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15638">http://arxiv.org/abs/2307.15638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/benolmbrt/TriadNet">https://github.com/benolmbrt/TriadNet</a></li>
<li>paper_authors: Benjamin Lambert, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for: 用于评估脑损害（如血栓或肿瘤）的体积，以估计患者的预后和导航治疗策略。</li>
<li>methods: 使用深度卷积神经网络（CNN）进行分割，并同时提供评估范围，以帮助估计脑损害的体积。</li>
<li>results: 在BraTS 2021数据集上，TriadNet方法比其他解决方案更加高效，可在一秒钟内提供评估范围和脑损害体积。<details>
<summary>Abstract</summary>
The volume of a brain lesion (e.g. infarct or tumor) is a powerful indicator of patient prognosis and can be used to guide the therapeutic strategy. Lesional volume estimation is usually performed by segmentation with deep convolutional neural networks (CNN), currently the state-of-the-art approach. However, to date, few work has been done to equip volume segmentation tools with adequate quantitative predictive intervals, which can hinder their usefulness and acceptation in clinical practice. In this work, we propose TriadNet, a segmentation approach relying on a multi-head CNN architecture, which provides both the lesion volumes and the associated predictive intervals simultaneously, in less than a second. We demonstrate its superiority over other solutions on BraTS 2021, a large-scale MRI glioblastoma image database.
</details>
<details>
<summary>摘要</summary>
���sterreich brain lesion (e.g. infarct or tumor) 的尺寸是一个非常重要的患者预测指标，可以用来引导治疗策略。 lesional volume estimation 通常是使用深度卷积神经网络 (CNN) 进行，目前是领域的先进方法。然而，到目前为止，几乎没有工作是将量 segmentation 工具与适当的量预测 інтервал相结合，这可能会妨碍它们在临床实践中的使用和接受度。在这个工作中，我们提出了 TriadNet，一种基于多头 CNN 架构的 segmentation 方法，可以同时提供lesion 的尺寸和相应的预测 интервал，几乎在一秒内。我们在 BraTS 2021 大规模 MRI 肿瘤图像数据库上进行了比较，该方法的超越性在�relation 中被证明。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-Machine-Learning-Methods-for-Lane-Change-Intention-Recognition-Using-Vehicle-Trajectory-Data"><a href="#A-Comparative-Analysis-of-Machine-Learning-Methods-for-Lane-Change-Intention-Recognition-Using-Vehicle-Trajectory-Data" class="headerlink" title="A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data"></a>A Comparative Analysis of Machine Learning Methods for Lane Change Intention Recognition Using Vehicle Trajectory Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15625">http://arxiv.org/abs/2307.15625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renteng Yuan</li>
<li>for: 本研究旨在提高自动驾驶车辆对周围环境的理解，认识安全隐患，以提高交通安全性。</li>
<li>methods: 本研究使用不同机器学习方法来识别LC意图从高维时序数据中。</li>
<li>results: 结果显示， ensemble方法可以减少类型II和类型III分类错误的影响，而LightGBM Algorithm在模型训练效率方面与XGBoost Algorithm进行了六倍提升。<details>
<summary>Abstract</summary>
Accurately detecting and predicting lane change (LC)processes can help autonomous vehicles better understand their surrounding environment, recognize potential safety hazards, and improve traffic safety. This paper focuses on LC processes and compares different machine learning methods' performance to recognize LC intention from high-dimensionality time series data. To validate the performance of the proposed models, a total number of 1023 vehicle trajectories is extracted from the CitySim dataset. For LC intention recognition issues, the results indicate that with ninety-eight percent of classification accuracy, ensemble methods reduce the impact of Type II and Type III classification errors. Without sacrificing recognition accuracy, the LightGBM demonstrates a sixfold improvement in model training efficiency than the XGBoost algorithm.
</details>
<details>
<summary>摘要</summary>
Lane 变化（LC）过程的准确探测和预测可以帮助自动驾驶车辆更好地了解它所在环境，认出安全隐患，提高交通安全性。本文关注LC过程，比较不同机器学习方法在 recognize LC 意图从高维时序数据中的表现。为验证提案模型的性能，从CitySim数据集中抽取了1023辆车辆的轨迹。对LC意图认识问题，结果表明， ensemble 方法可以降低 Type II 和 Type III 类错误的影响，并不 sacrificing 认识精度。LightGBM 算法比 XGBoost 算法速度6倍提高，无需牺牲认识精度。
</details></li>
</ul>
<hr>
<h2 id="Shrink-Perturb-Improves-Architecture-Mixing-during-Population-Based-Training-for-Neural-Architecture-Search"><a href="#Shrink-Perturb-Improves-Architecture-Mixing-during-Population-Based-Training-for-Neural-Architecture-Search" class="headerlink" title="Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search"></a>Shrink-Perturb Improves Architecture Mixing during Population Based Training for Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15621">http://arxiv.org/abs/2307.15621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/awesomelemon/pbt-nas">https://github.com/awesomelemon/pbt-nas</a></li>
<li>paper_authors: Alexander Chebykin, Arkadiy Dushatskiy, Tanja Alderliesten, Peter A. N. Bosman</li>
<li>for: 这个研究显示同时对 neural network 进行训练和混合是一种可行的方法来进行神经建筑搜索 (Neural Architecture Search, NAS)。</li>
<li>methods: 这个方法使用 Population Based Training (PBT) 算法来进行参数优化，并在训练过程中运用 shrink-perturb 技术来替换不好performing 网络，并将其替换为良好performing 网络的结果。</li>
<li>results: PBT-NAS 在具有挑战性的任务（如图像生成和强化学习）上表现出色，较baseline (随机搜索和突变基于 PBT) 高效。<details>
<summary>Abstract</summary>
In this work, we show that simultaneously training and mixing neural networks is a promising way to conduct Neural Architecture Search (NAS). For hyperparameter optimization, reusing the partially trained weights allows for efficient search, as was previously demonstrated by the Population Based Training (PBT) algorithm. We propose PBT-NAS, an adaptation of PBT to NAS where architectures are improved during training by replacing poorly-performing networks in a population with the result of mixing well-performing ones and inheriting the weights using the shrink-perturb technique. After PBT-NAS terminates, the created networks can be directly used without retraining. PBT-NAS is highly parallelizable and effective: on challenging tasks (image generation and reinforcement learning) PBT-NAS achieves superior performance compared to baselines (random search and mutation-based PBT).
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们显示同时训练和混合神经网络是对神经建筑搜索（NAS）的有前途的方法。为Hyperparameter优化，重用部分训练过的权重可以实现高效的搜索，这已经由人口基本训练（PBT）算法所证明。我们提议PBT-NAS，即将PBT适应到NAS中，在训练过程中通过将不好performing网络 populations中替换为well-performing网络的结果和权重使用缩短perturb技术来改进建筑。PBT-NAS终止后，创造的网络可以直接使用无需重新训练。PBT-NAS高度并行化和有效：在复杂任务（图像生成和强化学习）中，PBT-NAS比基eline（随机搜索和变换基eline PBT）表现出色。
</details></li>
</ul>
<hr>
<h2 id="Robust-Distortion-free-Watermarks-for-Language-Models"><a href="#Robust-Distortion-free-Watermarks-for-Language-Models" class="headerlink" title="Robust Distortion-free Watermarks for Language Models"></a>Robust Distortion-free Watermarks for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15593">http://arxiv.org/abs/2307.15593</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jthickstun/watermark">https://github.com/jthickstun/watermark</a></li>
<li>paper_authors: Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang</li>
<li>for: 本研究旨在对于autoregressive语言模型中植入水印，以确保文本的 Integrity 和 Authenticity。</li>
<li>methods: 本研究使用了两种抽样方法：倒数抽样和对数最小抽样。实际上，我们将随机水印钥匙映射到语言模型中的样本中，以生成水印文本。检测水印文本的方法是让任何知道水印钥匙的人将文本与随机数据进行比较。</li>
<li>results: 本研究透过实际应用三个语言模型（OPT-1.3B、LLaMA-7B和Alpaca-7B），评估了这种水印方法的 statistically 的能力和对各种重建攻击的Robustness。结果显示，对OPT-1.3B和LLaMA-7B模型，我们可以在40-50%的字串替换、插入或删除后，仍然可靠地检测水印文本（p ≤ 0.01），并且可以在35个字串上进行检测。对Alpaca-7B模型，我们实现了对对话响应的水印可行性研究，但因响应的 entropy 较低，检测难度较高，只有约25%的响应可以在p ≤ 0.01下检测，并且水印也较不抵抗某些自动重建攻击。<details>
<summary>Abstract</summary>
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.
</details>
<details>
<summary>摘要</summary>
我们提出了一种植入水印在文本中的方法，可以在文本中植入水印而不改变文本的分布，最多Generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from 35 tokens even after corrupting between 40-50% of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around 25% of the responses -- whose median length is around 100 tokens -- are detectable with $p \leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-structure-of-cognitive-tasks-with-transfer-learning"><a href="#Evaluating-the-structure-of-cognitive-tasks-with-transfer-learning" class="headerlink" title="Evaluating the structure of cognitive tasks with transfer learning"></a>Evaluating the structure of cognitive tasks with transfer learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02408">http://arxiv.org/abs/2308.02408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruno Aristimunha, Raphael Y. de Camargo, Walter H. Lopez Pinaya, Sylvain Chevallier, Alexandre Gramfort, Cedric Rommel<br>for:This study aims to investigate the transferability of deep learning representations between different EEG decoding tasks, with the goal of mitigating data scarcity in this setting.methods:The authors use state-of-the-art decoding models and conduct extensive experiments on two recently released EEG datasets, ERP CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks. They measure the transferability of learned representations by pre-training deep neural networks on one task and assessing their ability to decode subsequent tasks.results:The authors find that significant improvements in decoding performance can be obtained through transfer learning, with gains of up to 28% compared to the pure supervised approach. They also discover evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations. The transfer maps generated provide insights into the hierarchical relations between cognitive tasks, enhancing our understanding of how these tasks are connected from a neuroscientific standpoint.Here is the result in Simplified Chinese text:for: 这个研究的目的是 investigate EEG 解oding 中深度学习表示性的转移性，以减轻这种设定中的数据稀缺性。methods: 作者使用最新的解oding 模型，在两个最近发布的 EEG 数据集 ERP CORE 和 M$^3$CV 上进行了广泛的实验，这两个数据集包含了140名参与者和11种不同的认知任务。作者通过在一个任务上预训深度神经网络，然后评估它对接下来的任务的解码性能。results: 作者发现，通过转移学习可以获得显著的解码性能提升，相比约28% 的纯监督方法。他们还发现，certain 解码方法会引起特定和窄的脑活动，而其他方法则受益于在广泛的表示上进行预训。转移图生成的结果提供了认知任务之间的层次关系的了解，从神经科学的角度来看，这些任务之间存在着很强的相互关联。<details>
<summary>Abstract</summary>
Electroencephalography (EEG) decoding is a challenging task due to the limited availability of labelled data. While transfer learning is a promising technique to address this challenge, it assumes that transferable data domains and task are known, which is not the case in this setting. This study investigates the transferability of deep learning representations between different EEG decoding tasks. We conduct extensive experiments using state-of-the-art decoding models on two recently released EEG datasets, ERP CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks. We measure the transferability of learned representations by pre-training deep neural networks on one task and assessing their ability to decode subsequent tasks. Our experiments demonstrate that, even with linear probing transfer, significant improvements in decoding performance can be obtained, with gains of up to 28% compare with the pure supervised approach. Additionally, we discover evidence that certain decoding paradigms elicit specific and narrow brain activities, while others benefit from pre-training on a broad range of representations. By revealing which tasks transfer well and demonstrating the benefits of transfer learning for EEG decoding, our findings have practical implications for mitigating data scarcity in this setting. The transfer maps generated also provide insights into the hierarchical relations between cognitive tasks, hence enhancing our understanding of how these tasks are connected from a neuroscientific standpoint.
</details>
<details>
<summary>摘要</summary>
电子脑波图像分析（EEG）解oding是一项复杂的任务，因为标注数据的可用性有限。而转移学习是一种有前途的技术，但它假设了可以确定的数据领域和任务，这并不是这个设置的情况。这项研究探讨了EEG解oding任务中深度学习表示的传输性。我们进行了广泛的实验，使用最新的解oding模型在两个最近发布的EEG数据集上进行了测试，包括ERP CORE和M$^3$CV数据集，这两个数据集包含了140名参与者和11种不同的认知任务。我们测量了传输学习中获得的表示的传输性，通过在一个任务上预训练深度神经网络，并评估其对后续任务的解oding性能。我们的实验结果表明，即使使用线性探索转移，也可以获得显著的提高，与纯粹的监督方法相比，提高达28%。此外，我们发现了一些解oding方案会引起特定和窄频范围的大脑活动，而其他解oding方案则需要预训练在广泛的表示上。我们的发现可以实际地 mitigate数据缺乏问题在这个设置下，并且提供了转移学习在EEG解oding中的实际应用。此外，我们生成的转移图也提供了认知任务之间的层次关系的新的视角，从神经科学的角度来看，这有助于我们更好地理解这些任务之间的连接。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-algorithms-for-k-center-on-graphs"><a href="#Dynamic-algorithms-for-k-center-on-graphs" class="headerlink" title="Dynamic algorithms for k-center on graphs"></a>Dynamic algorithms for k-center on graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15557">http://arxiv.org/abs/2307.15557</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swati1024/torrents">https://github.com/swati1024/torrents</a></li>
<li>paper_authors: Emilio Cruciani, Sebastian Forster, Gramoz Goranci, Yasamin Nazari, Antonis Skarlatos</li>
<li>for: 这paper是为了解决动态图中的$k$-center问题，目标是将输入分成$k$个集合，使最大距离任何数据点与最近的中心之间的距离最小化。</li>
<li>methods: 这paper使用了 deterministic decremental $(2+\epsilon)$-approximation algorithm和randomized incremental $(4+\epsilon)$-approximation algorithm，两者具有规化更新时间 $kn^{o(1)}$  для权重图。</li>
<li>results: 这paper得到了一个fully dynamic $(2+\epsilon)$-approximation algorithm，其更新时间与现有最佳上限 bounds for maintaining $(1+\epsilon)$-approximate single-source distances in graphs几乎相同。<details>
<summary>Abstract</summary>
In this paper we give the first efficient algorithms for the $k$-center problem on dynamic graphs undergoing edge updates. In this problem, the goal is to partition the input into $k$ sets by choosing $k$ centers such that the maximum distance from any data point to the closest center is minimized. It is known that it is NP-hard to get a better than $2$ approximation for this problem.   While in many applications the input may naturally be modeled as a graph, all prior works on $k$-center problem in dynamic settings are on metrics. In this paper, we give a deterministic decremental $(2+\epsilon)$-approximation algorithm and a randomized incremental $(4+\epsilon)$-approximation algorithm, both with amortized update time $kn^{o(1)}$ for weighted graphs. Moreover, we show a reduction that leads to a fully dynamic $(2+\epsilon)$-approximation algorithm for the $k$-center problem, with worst-case update time that is within a factor $k$ of the state-of-the-art upper bound for maintaining $(1+\epsilon)$-approximate single-source distances in graphs. Matching this bound is a natural goalpost because the approximate distances of each vertex to its center can be used to maintain a $(2+\epsilon)$-approximation of the graph diameter and the fastest known algorithms for such a diameter approximation also rely on maintaining approximate single-source distances.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了首个高效的动态图-$k$-中心问题的算法。在这个问题中，我们需要将输入分成$k$个集合，选择$k$个中心，使得最大的数据点与最近的中心之间的距离最小化。已知这是NP困难的，不能获得更好于2的近似值。在许多应用中，输入可以自然地表示为图，但所有先前的动态设置-$k$-中心问题的研究都是基于度量。在这篇论文中，我们提供了一个 deterministic 递减$(2+\epsilon)$-近似算法和一个随机化增量$(4+\epsilon)$-近似算法，两者具有 $kn^{o(1)}$ 的增量时间。此外，我们还证明了一种减少，导致一个完全动态$(2+\epsilon)$-近似算法，其最坏情况升级时间与当前最佳Upper bound  für maintaining $(1+\epsilon)$-近似单源距离在图上相同。这个目标是一个自然的目标，因为每个顶点与其中心的近似距离可以用来维护一个$(2+\epsilon)$-近似的图 diameters，而最快known algorithms for such a diameters approximation也 rely on maintaining approximate single-source distances。
</details></li>
</ul>
<hr>
<h2 id="On-the-Trade-off-Between-Efficiency-and-Precision-of-Neural-Abstraction"><a href="#On-the-Trade-off-Between-Efficiency-and-Precision-of-Neural-Abstraction" class="headerlink" title="On the Trade-off Between Efficiency and Precision of Neural Abstraction"></a>On the Trade-off Between Efficiency and Precision of Neural Abstraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15546">http://arxiv.org/abs/2307.15546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Edwards, Mirco Giacobbe, Alessandro Abate</li>
<li>for: 这个论文的目的是提出新的神经抽象方法，以便形式地近似复杂非线性动力模型。</li>
<li>methods: 这个论文使用了神经网络和正式推理 Synthesis 技术来生成神经抽象模型，并使用了不同类型的激活函数（如 sigmoid 函数）来生成不同类型的神经抽象模型。</li>
<li>results: 该论文通过实验表明，不同类型的神经抽象模型在精度和生成时间、安全验证时间等方面存在负面选择，同时也提出了一种基于 reachability 计算的方法来加速神经抽象模型的安全验证。<details>
<summary>Abstract</summary>
Neural abstractions have been recently introduced as formal approximations of complex, nonlinear dynamical models. They comprise a neural ODE and a certified upper bound on the error between the abstract neural network and the concrete dynamical model. So far neural abstractions have exclusively been obtained as neural networks consisting entirely of $ReLU$ activation functions, resulting in neural ODE models that have piecewise affine dynamics, and which can be equivalently interpreted as linear hybrid automata. In this work, we observe that the utility of an abstraction depends on its use: some scenarios might require coarse abstractions that are easier to analyse, whereas others might require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, namely either piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We employ formal inductive synthesis procedures to generate neural abstractions that result in dynamical models with these semantics. Empirically, we demonstrate the trade-off that these different neural abstraction templates have vis-a-vis their precision and synthesis time, as well as the time required for their safety verification (done via reachability computation). We improve existing synthesis techniques to enable abstraction of higher-dimensional models, and additionally discuss the abstraction of complex neural ODEs to improve the efficiency of reachability analysis for these models.
</details>
<details>
<summary>摘要</summary>
In this work, we recognize that the utility of an abstraction depends on its use: some scenarios may require coarse abstractions that are easier to analyze, while others may require more complex, refined abstractions. We therefore consider neural abstractions of alternative shapes, such as piecewise constant or nonlinear non-polynomial (specifically, obtained via sigmoidal activations). We use formal inductive synthesis procedures to generate neural abstractions that result in dynamical models with these semantics.Empirically, we demonstrate the trade-off between these different neural abstraction templates in terms of their precision and synthesis time, as well as the time required for their safety verification (done via reachability computation). We improve existing synthesis techniques to enable abstraction of higher-dimensional models, and additionally discuss the abstraction of complex neural ODEs to improve the efficiency of reachability analysis for these models.
</details></li>
</ul>
<hr>
<h2 id="Beating-Backdoor-Attack-at-Its-Own-Game"><a href="#Beating-Backdoor-Attack-at-Its-Own-Game" class="headerlink" title="Beating Backdoor Attack at Its Own Game"></a>Beating Backdoor Attack at Its Own Game</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15539">http://arxiv.org/abs/2307.15539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damianliumin/non-adversarial_backdoor">https://github.com/damianliumin/non-adversarial_backdoor</a></li>
<li>paper_authors: Min Liu, Alberto Sangiovanni-Vincentelli, Xiangyu Yue</li>
<li>for: 防御深度神经网络（DNNs）免疫背门攻击，使其在恶势力数据上不受影响，但是在触发特定模式后会 manipulate 网络行为。</li>
<li>methods: 基于背门攻击的防御框架，通过在恶势力样本上注入非对抗性背门，对攻击者的背门在恶势力数据上产生限制，而无影响于干净数据。</li>
<li>results: 对多个 benchmark 和不同的架构进行了广泛的实验，结果显示，我们的方法可以 achieve state-of-the-art 防御效果，同时具有最低的干净数据性能下降。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. Considering the surprising defense ability displayed by our framework, we call for more attention to utilizing backdoor for backdoor defense. Code is available at https://github.com/damianliumin/non-adversarial_backdoor.
</details>
<details>
<summary>摘要</summary>
Inspired by the stealthiness and effectiveness of backdoor attacks, we propose a simple yet highly effective defense framework that injects non-adversarial backdoors targeting poisoned samples. We follow the general steps of backdoor attacks and detect a small set of suspected samples, then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has little influence on clean data.Our defense can be carried out during data preprocessing, without modifying the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. The results show that our method achieves state-of-the-art defense effectiveness with the least amount of performance drop on clean data.Considering the surprising defense ability displayed by our framework, we suggest paying more attention to utilizing backdoors for backdoor defense. Our code is available at <https://github.com/damianliumin/non-adversarial_backdoor>.
</details></li>
</ul>
<hr>
<h2 id="RFID-Assisted-Indoor-Localization-Using-Hybrid-Wireless-Data-Fusion"><a href="#RFID-Assisted-Indoor-Localization-Using-Hybrid-Wireless-Data-Fusion" class="headerlink" title="RFID-Assisted Indoor Localization Using Hybrid Wireless Data Fusion"></a>RFID-Assisted Indoor Localization Using Hybrid Wireless Data Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02410">http://arxiv.org/abs/2308.02410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abouzar Ghavami, Ali Abedi</li>
<li>for: 本研究旨在提出一种混合部分基地站indoor地位定位方法，用于跟踪indoor环境中的 объекты。</li>
<li>methods: 该方法使用开发的Radio Frequency Identification（RFID）跟踪设备和多种Internet of Things（IoT）无线通信协议，并将RFID标签安装在每个部分的边缘。RFID跟踪设备确定部分，并使用提posed的无线混合方法计算 objet的位置。</li>
<li>results: 实验结果验证了提出的分析结果，并verify了不同的Bluetooth、WiFi和ZigBee技术中的RSSI基地站位置估计。<details>
<summary>Abstract</summary>
Wireless localization is essential for tracking objects in indoor environments. Internet of Things (IoT) enables localization through its diverse wireless communication protocols. In this paper, a hybrid section-based indoor localization method using a developed Radio Frequency Identification (RFID) tracking device and multiple IoT wireless technologies is proposed. In order to reduce the cost of the RFID tags, the tags are installed only on the borders of each section. The RFID tracking device identifies the section, and the proposed wireless hybrid method finds the location of the object inside the section. The proposed hybrid method is analytically driven by linear location estimates obtained from different IoT wireless technologies. The experimental results using developed RFID tracking device and RSSI-based localization for Bluetooth, WiFi and ZigBee technologies verifies the analytical results.
</details>
<details>
<summary>摘要</summary>
无线地位是内部环境中物品跟踪的重要组成部分。互联网物品（IoT）为地位定位提供了多种无线通信协议。本文提出了一种混合部分基于RFID跟踪设备和多种IoT无线技术的hybrid地位定位方法。为了降低RFID标签的成本，标签仅在每个部分的边界安装。RFID跟踪设备识别 section，提案的无线混合方法在 section 中确定物品的位置。提议的混合方法由不同的IoT无线技术提供的线性位置估计驱动。实验结果使用开发的RFID跟踪设备和基于Bluetooth、WiFi和ZigBee技术的RSSI地位定位证明了分析结果。
</details></li>
</ul>
<hr>
<h2 id="The-Applicability-of-Federated-Learning-to-Official-Statistics"><a href="#The-Applicability-of-Federated-Learning-to-Official-Statistics" class="headerlink" title="The Applicability of Federated Learning to Official Statistics"></a>The Applicability of Federated Learning to Official Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15503">http://arxiv.org/abs/2307.15503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Stock, Oliver Hauke, Julius Weißmann, Hannes Federrath</li>
<li>for: 这个研究探讨了 Federated Learning（FL）在官方统计领域的潜力，并证明了 FL 模型的性能与中央学习方法相当。同时，通过保护数据持有者隐私，FL 可以开放更广泛的数据访问，最终提高官方统计。</li>
<li>methods: 该研究通过三个不同的应用场景进行了模拟，包括医疗保险数据集、细颗粒污染数据集和移动 ради oscillopection data set。所有这些数据集都来自于官方统计领域相关领域。我们提供了中央和 FL 算法的比较分析，并对每个模拟进行了详细的分析。在所有三个应用场景中，我们成功地使用 FL 模型来达到中央模型标准准的性能。</li>
<li>results: 我们的关键观察和其意义对于将这些模拟应用于实践中transfer的总结。我们结论出，FL 有可能在未来官方统计领域中emerge as a pivotal technology。<details>
<summary>Abstract</summary>
This work investigates the potential of Federated Learning (FL) for official statistics and shows how well the performance of FL models can keep up with centralized learning methods. At the same time, its utilization can safeguard the privacy of data holders, thus facilitating access to a broader range of data and ultimately enhancing official statistics. By simulating three different use cases, important insights on the applicability of the technology are gained. The use cases are based on a medical insurance data set, a fine dust pollution data set and a mobile radio coverage data set - all of which are from domains close to official statistics. We provide a detailed analysis of the results, including a comparison of centralized and FL algorithm performances for each simulation. In all three use cases, we were able to train models via FL which reach a performance very close to the centralized model benchmarks. Our key observations and their implications for transferring the simulations into practice are summarized. We arrive at the conclusion that FL has the potential to emerge as a pivotal technology in future use cases of official statistics.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "official statistics" is translated as "官方统计" (guāngrò tōngjī)* "Federated Learning" is translated as "联合学习" (liánhé xuéxí)* "centralized learning methods" is translated as "中央化学习方法" (zhōngyānghuà xuéxí fāngfa)* "data holders" is translated as "数据持有者" (shùdà jiāyuxiǎ)* "broader range of data" is translated as "更广泛的数据" (gèng guǎngkuò de shùdà)* "ultimately enhancing official statistics" is translated as "最终提高官方统计" (zui zhì tīgēng guāngrò tōngjī)* "simulations" is translated as "模拟" (móxī)* "use cases" is translated as "应用场景" (yìngyòu jiàngxiàng)* "medical insurance data set" is translated as "医疗保险数据集" (yīkuò bǎoxiǎo shùdiàn jī)* "fine dust pollution data set" is translated as "细排毒污染数据集" (xì purguī chóngwù róngshí shùdiàn jī)* "mobile radio coverage data set" is translated as "移动通信覆盖数据集" (yídòng tōngxìn fùhài shùdiàn jī)
</details></li>
</ul>
<hr>
<h2 id="AbDiffuser-Full-Atom-Generation-of-In-Vitro-Functioning-Antibodies"><a href="#AbDiffuser-Full-Atom-Generation-of-In-Vitro-Functioning-Antibodies" class="headerlink" title="AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies"></a>AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05027">http://arxiv.org/abs/2308.05027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karolis Martinkus, Jan Ludwiczak, Kyunghyun Cho, Wei-Ching Liang, Julien Lafrance-Vanasse, Isidro Hotzel, Arvind Rajpal, Yan Wu, Richard Bonneau, Vladimir Gligorijevic, Andreas Loukas</li>
<li>for: 这篇论文是为了设计高级别的抗体3D结构和序列而设计的一种equivariant和physics-informed扩散模型。</li>
<li>methods: 该模型基于一种新的蛋白质结构表示方法，使用了一种新的对称保持方案，并利用了强大的扩散约束来提高减噪过程。</li>
<li>results: 数值实验表明，AbDiffuser可以准确地生成与参照集的序列和结构性质匹配的抗体。实验室实验证明，所有16个HER2抗体的表达水平很高，选择的设计被证明是高级别紧绑物。<details>
<summary>Abstract</summary>
We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of selected designs were tight binders.
</details>
<details>
<summary>摘要</summary>
我们介绍AbDiffuser，一个具有对称和物理知识散射模型，用于同时生成抗体三维结构和序列。AbDiffuser基于一个新的蛋白结构表示方法，利用一个新的保持蛋白结构的架构，并利用强大的散射假设来改善降噪过程。我们的方法可以改善蛋白 diffusion  by leveraging domain knowledge and physics-based constraints; 可以处理序列长度变化; 并可以降低内存复杂度，将蛋白质量减少一个级数。我们在silico和in vitro验证AbDiffuser。numeraical experiments showcase AbDiffuser的能力可以生成蛋白质量 closely track 引用集的序列和结构属性。实验室实验确认所有16个HER2抗体的发现都是高水平的表达，并且57.1%的选择设计是紧致系结者。
</details></li>
</ul>
<hr>
<h2 id="Curiosity-Driven-Reinforcement-Learning-based-Low-Level-Flight-Control"><a href="#Curiosity-Driven-Reinforcement-Learning-based-Low-Level-Flight-Control" class="headerlink" title="Curiosity-Driven Reinforcement Learning based Low-Level Flight Control"></a>Curiosity-Driven Reinforcement Learning based Low-Level Flight Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15724">http://arxiv.org/abs/2307.15724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a-ramezani/cdrl-l2fc_u_hcm">https://github.com/a-ramezani/cdrl-l2fc_u_hcm</a></li>
<li>paper_authors: Amir Ramezani Dooraki, Alexandros Iosifidis</li>
<li>for: 本研究旨在开发一种基于Curiosity的自主学习控制算法，以便 quadcopter 可以穿过障碍物并控制飞机的 Ya 方向指向所需的位置。</li>
<li>methods: 本研究使用了 prediction error 来实现 Curiosity 的新方法，并在 on-policy、off-policy、on-policy plus Curiosity 和提议算法上进行了测试。</li>
<li>results: results 表明，提议算法可以学习优质策略并最大化奖励，其他算法无法实现。<details>
<summary>Abstract</summary>
Curiosity is one of the main motives in many of the natural creatures with measurable levels of intelligence for exploration and, as a result, more efficient learning. It makes it possible for humans and many animals to explore efficiently by searching for being in states that make them surprised with the goal of learning more about what they do not know. As a result, while being curious, they learn better. In the machine learning literature, curiosity is mostly combined with reinforcement learning-based algorithms as an intrinsic reward. This work proposes an algorithm based on the drive of curiosity for autonomous learning to control by generating proper motor speeds from odometry data. The quadcopter controlled by our proposed algorithm can pass through obstacles while controlling the Yaw direction of the quad-copter toward the desired location. To achieve that, we also propose a new curiosity approach based on prediction error. We ran tests using on-policy, off-policy, on-policy plus curiosity, and the proposed algorithm and visualized the effect of curiosity in evolving exploration patterns. Results show the capability of the proposed algorithm to learn optimal policy and maximize reward where other algorithms fail to do so.
</details>
<details>
<summary>摘要</summary>
《尝试性》是许多自然生物智商水平可测量的动物主要动机之一，用于探索和学习。它使得人类和许多动物可以效率地探索，通过搜索被 surprisal 的目的来学习更多关于他们所不知道的事物。因此，在尝试性的情况下，他们的学习效果更好。在机器学习文献中，尝试性通常与回报学习基于的算法结合使用。这项工作提出一种基于尝试性的自主学习控制算法，通过生成适当的电机速度从遥感数据来控制四旋翼机。我们的提议的算法可以让四旋翼机通过障碍物而行，同时控制机器人的横坐标向所需的位置。为了实现这一目标，我们还提出了一种基于预测错误的新的尝试性方法。我们在使用在政策、离政策、在政策 plus 尝试性和我们的算法进行测试，并将尝试性的影响在演化探索模式中视觉化。结果显示我们的算法可以学习优化策略，并在其他算法无法做到的情况下获得最高奖励。
</details></li>
</ul>
<hr>
<h2 id="From-continuous-time-formulations-to-discretization-schemes-tensor-trains-and-robust-regression-for-BSDEs-and-parabolic-PDEs"><a href="#From-continuous-time-formulations-to-discretization-schemes-tensor-trains-and-robust-regression-for-BSDEs-and-parabolic-PDEs" class="headerlink" title="From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs"></a>From continuous-time formulations to discretization schemes: tensor trains and robust regression for BSDEs and parabolic PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15496">http://arxiv.org/abs/2307.15496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lorenzrichter/PDE-backward-solver">https://github.com/lorenzrichter/PDE-backward-solver</a></li>
<li>paper_authors: Lorenz Richter, Leon Sallandt, Nikolas Nüsken</li>
<li>for: 解决高维纬度的partial differential equations（PDEs）数学应用中的困难问题，特别是谱系束问题。</li>
<li>methods: 使用Monte Carlo方法和变分方法，并使用神经网络来近似函数。</li>
<li>results: 提出了一种基于tensor train的新方法，可以在高维纬度下实现高精度和高效计算。该方法可以在迭代计算中实现一个有利的平衡 между精度和计算效率。<details>
<summary>Abstract</summary>
The numerical approximation of partial differential equations (PDEs) poses formidable challenges in high dimensions since classical grid-based methods suffer from the so-called curse of dimensionality. Recent attempts rely on a combination of Monte Carlo methods and variational formulations, using neural networks for function approximation. Extending previous work (Richter et al., 2021), we argue that tensor trains provide an appealing framework for parabolic PDEs: The combination of reformulations in terms of backward stochastic differential equations and regression-type methods holds the promise of leveraging latent low-rank structures, enabling both compression and efficient computation. Emphasizing a continuous-time viewpoint, we develop iterative schemes, which differ in terms of computational efficiency and robustness. We demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency. While previous methods have been either accurate or fast, we have identified a novel numerical strategy that can often combine both of these aspects.
</details>
<details>
<summary>摘要</summary>
“对于高维度的偏微分方程（PDEs） numerically 的推估 pose formidable 挑战，因为经典的格子基方法受到称为“对维度的诅咒”的限制。 latest attempts  rely on  комби��ination of Monte Carlo 方法和量�ltiple�formulations，使用神经网络 для函数�approximation。 extending previous work (Richter et al., 2021)， we argue that tensor trains provide an appealing framework for parabolic PDEs：the combination of reformulations in terms of backward stochastic differential equations and regression-type methods can leverage latent low-rank structures, enabling both compression and efficient computation。 emphasizing a continuous-time viewpoint， we develop iterative schemes， which differ in terms of computational efficiency and robustness。 we demonstrate both theoretically and numerically that our methods can achieve a favorable trade-off between accuracy and computational efficiency。 while previous methods have been either accurate or fast， we have identified a novel numerical strategy that can often combine both of these aspects。”Note: The translation is in Simplified Chinese, which is one of the two standard varieties of Chinese. The other variety is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="FeedbackLogs-Recording-and-Incorporating-Stakeholder-Feedback-into-Machine-Learning-Pipelines"><a href="#FeedbackLogs-Recording-and-Incorporating-Stakeholder-Feedback-into-Machine-Learning-Pipelines" class="headerlink" title="FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines"></a>FeedbackLogs: Recording and Incorporating Stakeholder Feedback into Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15475">http://arxiv.org/abs/2307.15475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Barker, Emma Kallina, Dhananjay Ashok, Katherine M. Collins, Ashley Casovan, Adrian Weller, Ameet Talwalkar, Valerie Chen, Umang Bhatt</li>
<li>for: 这篇论文是为了描述一种用于记录多个参与者反馈的机器学习（ML）管道的方法。</li>
<li>methods: 该方法使用FeedbackLogs，一种补充现有ML管道文档的添加物，来跟踪多个参与者的反馈输入。每个日志记录了反馈收集过程中重要的细节，反馈自身，以及如何将反馈更新到ML管道中。</li>
<li>results: 该论文提出了一种形式化的反馈收集过程，并提供了具体的应用场景，例如使用FeedbackLogs作为算法审核证据和记录参与者反馈更新的工具。<details>
<summary>Abstract</summary>
Even though machine learning (ML) pipelines affect an increasing array of stakeholders, there is little work on how input from stakeholders is recorded and incorporated. We propose FeedbackLogs, addenda to existing documentation of ML pipelines, to track the input of multiple stakeholders. Each log records important details about the feedback collection process, the feedback itself, and how the feedback is used to update the ML pipeline. In this paper, we introduce and formalise a process for collecting a FeedbackLog. We also provide concrete use cases where FeedbackLogs can be employed as evidence for algorithmic auditing and as a tool to record updates based on stakeholder feedback.
</details>
<details>
<summary>摘要</summary>
即使机器学习（ML）管道影响到越来越多的各种利益相关者，仍有很少关于如何记录和汇报利益各方的工作。我们提议使用FeedbackLogs，作为现有的ML管道文档的补充，来跟踪多个利益各方的输入。每个日志记录了反馈收集过程中重要的细节，反馈本身，以及如何使用反馈更新ML管道。在这篇论文中，我们介绍了收集FeedbackLog的过程，并提供了具体的应用场景，如算法审核的证据和利益各方反馈更新的工具。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Noisy-Label-Learning-in-Real-world-Annotation-Scenarios-from-the-Noise-type-Perspective"><a href="#Rethinking-Noisy-Label-Learning-in-Real-world-Annotation-Scenarios-from-the-Noise-type-Perspective" class="headerlink" title="Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective"></a>Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16889">http://arxiv.org/abs/2307.16889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuxiailab/protosemi">https://github.com/fuxiailab/protosemi</a></li>
<li>paper_authors: Renyu Zhu, Haoyu Liu, Runze Wu, Minmin Lin, Tangjie Lv, Changjie Fan, Haobo Wang</li>
<li>for: 本研究强调学习受损标签的问题，即在实际标注场景中的噪声问题。噪声可分为两类：事实噪声和抽象噪声。</li>
<li>methods: 我们提出了一种基于样本选择的噪声标签学习方法，称为Proto-semi。Proto-semi首先将所有样本分为确定的和不确定的数据集via warm-up。然后，利用确定数据集，构建prototype vector来捕捉类征。接着，计算不确定样本与prototype vector之间的距离，以便噪声分类。根据这些距离，标签是否需要更正或保留，从而实现标签的更新。最后，我们引入了一种半supervised学习方法来增强训练。</li>
<li>results: 在一个实际标注数据集上进行了实验，证明Proto-semi在面临受损标签问题时具有强大的鲁棒性。同时，我们的prototype-based repartitioning策略被证明可以有效地减少噪声标签的不良影响。我们的代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/fuxiAIlab/ProtoSemi%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/fuxiAIlab/ProtoSemi上下载。</a><details>
<summary>Abstract</summary>
In this paper, we investigate the problem of learning with noisy labels in real-world annotation scenarios, where noise can be categorized into two types: factual noise and ambiguity noise. To better distinguish these noise types and utilize their semantics, we propose a novel sample selection-based approach for noisy label learning, called Proto-semi. Proto-semi initially divides all samples into the confident and unconfident datasets via warm-up. By leveraging the confident dataset, prototype vectors are constructed to capture class characteristics. Subsequently, the distances between the unconfident samples and the prototype vectors are calculated to facilitate noise classification. Based on these distances, the labels are either corrected or retained, resulting in the refinement of the confident and unconfident datasets. Finally, we introduce a semi-supervised learning method to enhance training. Empirical evaluations on a real-world annotated dataset substantiate the robustness of Proto-semi in handling the problem of learning from noisy labels. Meanwhile, the prototype-based repartitioning strategy is shown to be effective in mitigating the adverse impact of label noise. Our code and data are available at https://github.com/fuxiAIlab/ProtoSemi.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在实际标注场景中学习含噪标签的问题，噪声可以分为两类：事实噪声和模糊噪声。为了更好地 отли奇这两种噪声的 semantics，我们提出了一种基于样本选择的含噪标签学习方法，称为Proto-semi。Proto-semi首先将所有样本分为信任度高和低两个集合 via warm-up。通过利用信任度高的集合，我们构建了类特征的原型向量。然后，我们计算了不确定样本和原型向量之间的距离，以便噪声分类。根据这些距离，我们是否更正或保留标签，从而实现了标签的纠正和提升。最后，我们引入了半supervised学习方法，以提高训练的效果。实际评估表明，Proto-semi在处理含噪标签学习问题中具有坚定的可靠性。同时，我们的prototype-based重新分配策略能够减少噪声标签的负面影响。我们的代码和数据可以在https://github.com/fuxiAIlab/ProtoSemi上获取。
</details></li>
</ul>
<hr>
<h2 id="Testing-the-Depth-of-ChatGPT’s-Comprehension-via-Cross-Modal-Tasks-Based-on-ASCII-Art-GPT3-5’s-Abilities-in-Regard-to-Recognizing-and-Generating-ASCII-Art-Are-Not-Totally-Lacking"><a href="#Testing-the-Depth-of-ChatGPT’s-Comprehension-via-Cross-Modal-Tasks-Based-on-ASCII-Art-GPT3-5’s-Abilities-in-Regard-to-Recognizing-and-Generating-ASCII-Art-Are-Not-Totally-Lacking" class="headerlink" title="Testing the Depth of ChatGPT’s Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5’s Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking"></a>Testing the Depth of ChatGPT’s Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5’s Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16806">http://arxiv.org/abs/2307.16806</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Bayani</li>
<li>for: 本研究旨在检验GPT3.5模型在视觉任务中的能力，包括图像识别、图像分解和图像生成等。</li>
<li>methods: 该研究使用GPT3.5模型进行图像处理，包括图像识别、图像分解和图像生成等任务。</li>
<li>results: 研究发现GPT3.5模型在视觉任务中表现出色，能够准确地识别图像，并且在图像分解和图像生成任务中也表现出优异。<details>
<summary>Abstract</summary>
Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
</details>
<details>
<summary>摘要</summary>
在过去的八个月里，ChatGPT和它的基础模型GPT3.5已经吸引了巨大的关注，因为它们具有杰出的能力和可访问性。虽然一些专业人员已经发表了许多关于这些模型的论文，但是输入和EXTRACTED FROM这些网络的信息都是自然语言文本或者带有编程语言的样式的。 Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities，在这个工作中我们 examines GPT3.5的适用性在视觉任务中，其输入内容为ASCII艺术而没有明显的概括。我们进行了对模型在图像识别任务、图像部分知识和图像生成任务中的表现的实验。
</details></li>
</ul>
<hr>
<h2 id="LUCID-GAN-Conditional-Generative-Models-to-Locate-Unfairness"><a href="#LUCID-GAN-Conditional-Generative-Models-to-Locate-Unfairness" class="headerlink" title="LUCID-GAN: Conditional Generative Models to Locate Unfairness"></a>LUCID-GAN: Conditional Generative Models to Locate Unfairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15466">http://arxiv.org/abs/2307.15466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/integrated-intelligence-lab/canonical_sets">https://github.com/integrated-intelligence-lab/canonical_sets</a></li>
<li>paper_authors: Andres Algaba, Carmen Mazijn, Carina Prunkl, Jan Danckaert, Vincent Ginis</li>
<li>for: 检测黑盒模型中的不正当偏见</li>
<li>methods: 使用LUCID-GAN生成潜在Input，通过描述模型的机制来暴露不正当偏见</li>
<li>results: 在UCI Adult和COMPAS数据集上实验表明，LUCID-GAN可以无需访问训练数据，检测黑盒模型中的不正当偏见<details>
<summary>Abstract</summary>
Most group fairness notions detect unethical biases by computing statistical parity metrics on a model's output. However, this approach suffers from several shortcomings, such as philosophical disagreement, mutual incompatibility, and lack of interpretability. These shortcomings have spurred the research on complementary bias detection methods that offer additional transparency into the sources of discrimination and are agnostic towards an a priori decision on the definition of fairness and choice of protected features. A recent proposal in this direction is LUCID (Locating Unfairness through Canonical Inverse Design), where canonical sets are generated by performing gradient descent on the input space, revealing a model's desired input given a preferred output. This information about the model's mechanisms, i.e., which feature values are essential to obtain specific outputs, allows exposing potential unethical biases in its internal logic. Here, we present LUCID-GAN, which generates canonical inputs via a conditional generative model instead of gradient-based inverse design. LUCID-GAN has several benefits, including that it applies to non-differentiable models, ensures that canonical sets consist of realistic inputs, and allows to assess proxy and intersectional discrimination. We empirically evaluate LUCID-GAN on the UCI Adult and COMPAS data sets and show that it allows for detecting unethical biases in black-box models without requiring access to the training data.
</details>
<details>
<summary>摘要</summary>
大多数群衡不偏观方法都是通过计算统计平衡指标来检测不正义的偏见。但这种方法存在多个缺点，如哲学意见不一致、不相容性和没有解释力。这些缺点导致了对于补充的偏见检测方法的研究，这些方法可以提供更多的透明度，了解歧视的来源，并且不需要先知定义公平和保护特征。一个最近的提案是LUCID（找到不公的方法），它使用了梯度下降来生成可能的输入，以揭露模型对特定输出的需要的输入。这个信息可以暴露模型内部的不公义偏见。在这篇文章中，我们提出了LUCID-GAN，它使用了 conditional generative model 生成可能的输入，而不是使用梯度下降的 inverse design。LUCID-GAN 有多个优点，包括可以应用于非微分的模型、 canonical sets 会包含实际的输入和可以评估代表和交叉性歧视。我们在 UCI 成人和 COMPAS 数据集上进行了实验，评估 LUCID-GAN 可以检测黑盒模型中的不公义偏见，而不需要存取训练数据。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-machine-learning-shock-capturing-technique-for-high-order-solvers"><a href="#Unsupervised-machine-learning-shock-capturing-technique-for-high-order-solvers" class="headerlink" title="Unsupervised machine-learning shock-capturing technique for high-order solvers"></a>Unsupervised machine-learning shock-capturing technique for high-order solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00086">http://arxiv.org/abs/2308.00086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrés Mateo-Gabín, Kenza Tlales, Eusebio Valero, Esteban Ferrer, Gonzalo Rubio</li>
<li>for: 这篇论文的目的是提出一种基于 Gaussian Mixture Models（GMM）的无监督机器学习捕捉冲击算法，以提高高精度 Computational Fluid Dynamics（CFD）代码的稳定性和效率。</li>
<li>methods: 这篇论文使用了 GMM 测器，该测器可以无需受训练数据来捕捉冲击，并且在多种测试 caso 中展现了优异的性能，与现有的竞争方案相比。</li>
<li>results: 这篇论文的研究结果显示，GMM 测器在高 Reynolds 数的冲击测试 caso 中具有同等的效果，与 fine-tuned 的竞争方案相比，而且可以在复杂的 geometries 和多种流场配置中运作。<details>
<summary>Abstract</summary>
We present a novel unsupervised machine learning shock capturing algorithm based on Gaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable accuracy in detecting shocks and is robust across diverse test cases without the need for parameter tuning. We compare the GMM-based sensor with state-of-the-art alternatives. All methods are integrated into a high-order compressible discontinuous Galerkin solver where artificial viscosity can be modulated to capture shocks. Supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors. %The nodal DG aproach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement. The adaptive nature and ability to function without extensive training datasets make this GMM-based sensor suitable for complex geometries and varied flow configurations. Our study reveals the potential of unsupervised machine learning methods, exemplified by the GMM sensor, to improve the robustness and efficiency of advanced CFD codes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的无监督机器学习冲击捕捉算法基于泛函混合模型（GMM）。我们的GMM感测器在捕捉冲击方面表现出了很好的准确性，并且在多种测试 случа件中具有很好的稳定性，无需进行参数调整。我们比较了GMM基于感测器与当前的状态艺术方法。所有方法都被 integrate into a high-order可 compressible discontinuous Galerkin solver中，其中人工粘性可以用来捕捉冲击。supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors。%The nodal DG approach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement。我们的研究表明了无监督机器学习方法，例如GMM感测器，可以提高高级CFD代码的稳定性和效率。
</details></li>
</ul>
<hr>
<h2 id="Worrisome-Properties-of-Neural-Network-Controllers-and-Their-Symbolic-Representations"><a href="#Worrisome-Properties-of-Neural-Network-Controllers-and-Their-Symbolic-Representations" class="headerlink" title="Worrisome Properties of Neural Network Controllers and Their Symbolic Representations"></a>Worrisome Properties of Neural Network Controllers and Their Symbolic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15456">http://arxiv.org/abs/2307.15456</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mimuw-rl/worrisome-nn">https://github.com/mimuw-rl/worrisome-nn</a></li>
<li>paper_authors: Jacek Cyranka, Kevin E M Church, Jean-Philippe Lessard</li>
<li>for: 研究控制器的稳定性问题在简单的强化学习 benchmark 问题中。</li>
<li>methods: 使用神经网络控制器和其低神经和符号抽象。</li>
<li>results: 发现控制器可以生成许多 persistently 低返回解决方案，这是一个非常不 desireable 的性能特性，可以被敌人轻松地利用。  simpler controllers 更易生成 persistently bad solutions。 提供了一种系统的Robustness study algorithm，并证明存在 persistently solutions 和，在某些情况下， periodic orbits。<details>
<summary>Abstract</summary>
We raise concerns about controllers' robustness in simple reinforcement learning benchmark problems. We focus on neural network controllers and their low neuron and symbolic abstractions. A typical controller reaching high mean return values still generates an abundance of persistent low-return solutions, which is a highly undesirable property, easily exploitable by an adversary. We find that the simpler controllers admit more persistent bad solutions. We provide an algorithm for a systematic robustness study and prove existence of persistent solutions and, in some cases, periodic orbits, using a computer-assisted proof methodology.
</details>
<details>
<summary>摘要</summary>
我们对控制器的稳定性表示关切，尤其是在简单的强化学习问题中。我们主要关注神经网络控制器的低神经和符号抽象。一个typical控制器可以 дости得高mean return值，但仍然生成丰富的持续性低返回解，这是一个非常不愿意的属性，易于被敌人利用。我们发现简单的控制器承认更多持续性坏解。我们提供了一个系统atic robustness研究的算法，并证明存在持续解和，在一些情况下，周期性orbits，使用了计算机助理的证明方法。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Payload-Thermal-Control"><a href="#Autonomous-Payload-Thermal-Control" class="headerlink" title="Autonomous Payload Thermal Control"></a>Autonomous Payload Thermal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15438">http://arxiv.org/abs/2307.15438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro D. Mousist</li>
<li>for: 这篇研究是为了解决小型卫星中热控制系统的挑战，特别是当电子元件和科学仪器聚集在一起时，热 dissipation 问题的影响。</li>
<li>methods: 这篇研究使用了深度强化学习框架，包括 Soft Actor-Critic 算法，以学习在小型卫星上的热控制策略。</li>
<li>results: 实验结果显示，提案的框架能够学习控制负载处理功率，以维持 payload 的热度在操作范围内。这些结果表明，该框架可以辅助传统热控制系统，并且可以在小型卫星中提供更好的热控制功能。<details>
<summary>Abstract</summary>
In small satellites there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the near proximity of the electronics makes power dissipation difficult, with the risk of not being able to control the temperature appropriately, reducing component lifetime and mission performance. To address this challenge, taking advantage of the advent of increasing intelligence on board satellites, a deep reinforcement learning based framework that uses Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated both in a naive simulated environment and in a real space edge processing computer that will be shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature under operational ranges, complementing traditional thermal control systems.
</details>
<details>
<summary>摘要</summary>
在小卫星中，due to limited space, there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the proximity of electronics makes power dissipation difficult, which can lead to inadequate temperature control, reducing component lifetime and mission performance. To address this challenge, a deep reinforcement learning-based framework that uses the Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated in both a simulated environment and a real space edge processing computer that will be used in the future IMAGIN-e mission and hosted in the ISS. Experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature within operational ranges, complementing traditional thermal control systems.
</details></li>
</ul>
<hr>
<h2 id="Improvable-Gap-Balancing-for-Multi-Task-Learning"><a href="#Improvable-Gap-Balancing-for-Multi-Task-Learning" class="headerlink" title="Improvable Gap Balancing for Multi-Task Learning"></a>Improvable Gap Balancing for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15429">http://arxiv.org/abs/2307.15429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanqidai/igb4mtl">https://github.com/yanqidai/igb4mtl</a></li>
<li>paper_authors: Yanqi Dai, Nanyi Fei, Zhiwu Lu</li>
<li>for: 提高多任务学习（MTL）中性能的研究，尤其是在使用梯度均衡而不是损失均衡。</li>
<li>methods: 提出了两种新的改进可能均衡算法（IGB），其中一种使用了简单的规则，另一种则是首次在MTL中应用深度强化学习。两种算法均 dynamically assigns任务权重来实现可 improvable gap 均衡。</li>
<li>results: 通过对两个 benchmark 数据集进行广泛的实验表明，我们的 IGB 算法在通过损失均衡和梯度均衡进行改进后，能够达到最佳性能，并且在两种算法结合使用情况下，能够实现进一步的改进。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/YanqiDai/IGB4MTL">https://github.com/YanqiDai/IGB4MTL</a> 上获取。<details>
<summary>Abstract</summary>
In multi-task learning (MTL), gradient balancing has recently attracted more research interest than loss balancing since it often leads to better performance. However, loss balancing is much more efficient than gradient balancing, and thus it is still worth further exploration in MTL. Note that prior studies typically ignore that there exist varying improvable gaps across multiple tasks, where the improvable gap per task is defined as the distance between the current training progress and desired final training progress. Therefore, after loss balancing, the performance imbalance still arises in many cases. In this paper, following the loss balancing framework, we propose two novel improvable gap balancing (IGB) algorithms for MTL: one takes a simple heuristic, and the other (for the first time) deploys deep reinforcement learning for MTL. Particularly, instead of directly balancing the losses in MTL, both algorithms choose to dynamically assign task weights for improvable gap balancing. Moreover, we combine IGB and gradient balancing to show the complementarity between the two types of algorithms. Extensive experiments on two benchmark datasets demonstrate that our IGB algorithms lead to the best results in MTL via loss balancing and achieve further improvements when combined with gradient balancing. Code is available at https://github.com/YanqiDai/IGB4MTL.
</details>
<details>
<summary>摘要</summary>
在多任务学习（MTL）中，梯度均衡在最近几年内吸引了更多的研究兴趣，因为它经常会导致更好的性能。然而，损失均衡是梯度均衡的更加有效的方法，因此仍然值得进一步的探索。注意，先前的研究通常忽略了多个任务之间存在的变化可以提高的差距，其中每个任务的改进差距是指从当前的训练进度到期望的最终训练进度之间的距离。因此，在许多情况下， после损失均衡，性能差距仍然存在。在这篇论文中，我们采用损失均衡框架，提出了两种新的可 improvable gap 均衡（IGB）算法 для MTL：一个使用简单的规则，另一个（在MTL中首次）使用深度强化学习。特别是，不直接在 MTL 中平衡损失，IGB 算法选择动态分配任务权重以进行 improvable gap 均衡。此外，我们将 IGB 和梯度均衡结合使用，以示它们之间的补充性。我们在两个标准数据集上进行了广泛的实验，结果表明，我们的 IGB 算法在 MTL 中通过损失均衡得到最佳结果，并在与梯度均衡结合使用时得到进一步的改进。代码可以在 <https://github.com/YanqiDai/IGB4MTL> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Implicit-neural-representation-for-change-detection"><a href="#Implicit-neural-representation-for-change-detection" class="headerlink" title="Implicit neural representation for change detection"></a>Implicit neural representation for change detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15428">http://arxiv.org/abs/2307.15428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Naylor, Diego Di Carlo, Arianna Traviglia, Makoto Yamada, Marco Fiorucci<br>for:这种论文的目的是检测在两个不同时间在同一地区获取的空气borne LiDAR点云之间的变化。methods:这种方法包括两个组件：神经场（NF） для连续形态重建和加aussian Mixture Model（GMM） для分类变化。NF提供了不固定格式的表示方式，可以编码不匹配的时间支持，并可以正则化以增加高频环境和减少噪声。results:这种方法在一个 benchmark 数据集上达到了10%的提升，并在实际应用中证明了 illegal excavation（掘寻）的发现。<details>
<summary>Abstract</summary>
Detecting changes that occurred in a pair of 3D airborne LiDAR point clouds, acquired at two different times over the same geographical area, is a challenging task because of unmatching spatial supports and acquisition system noise. Most recent attempts to detect changes on point clouds are based on supervised methods, which require large labelled data unavailable in real-world applications. To address these issues, we propose an unsupervised approach that comprises two components: Neural Field (NF) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes. NF offer a grid-agnostic representation to encode bi-temporal point clouds with unmatched spatial support that can be regularised to increase high-frequency details and reduce noise. The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities. We apply our method to a benchmark dataset of simulated LiDAR point clouds for urban sprawling. The dataset offers different challenging scenarios with different resolutions, input modalities and noise levels, allowing a multi-scenario comparison of our method with the current state-of-the-art. We boast the previous methods on this dataset by a 10% margin in intersection over union metric. In addition, we apply our methods to a real-world scenario to identify illegal excavation (looting) of archaeological sites and confirm that they match findings from field experts.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>检测两个3D空中LiDAR点云在不同时间点上的变化是一项具有棘手和获取系统噪声的挑战。大多数最新的变化检测方法基于有监督的方法，需要庞大的标注数据，这些数据在实际应用中不可获得。为解决这些问题，我们提出了一种无监督方法，包括两个组件：神经场（NF） для连续形态重建和 Gaussian Mixture Model（GMM） для分类变化。NF提供了不受格子约束的表示方式，可以编码不匹配的时间点云，并可以通过增强高频率细节和减少噪声来规范。在每个时间戳中比较重建的结果，可以在自由的空间缩放比例上实现显著提高检测能力。我们在一个 simulated LiDAR点云 benchmark dataset 上应用了我们的方法，该dataset 包括不同的挑战场景，例如不同的分辨率、输入模式和噪声水平。我们在这个dataset上比较了我们的方法与当前状态之最先进的方法，并在 intersection over union 指标上提高了10%的margin。此外，我们还应用了我们的方法到一个真实世界的场景，以检测考古遗产泥棒（looting），并证明与场地专家的发现相匹配。
</details></li>
</ul>
<hr>
<h2 id="Deep-Generative-Models-Synthetic-Tabular-Data-and-Differential-Privacy-An-Overview-and-Synthesis"><a href="#Deep-Generative-Models-Synthetic-Tabular-Data-and-Differential-Privacy-An-Overview-and-Synthesis" class="headerlink" title="Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis"></a>Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15424">http://arxiv.org/abs/2307.15424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Conor Hassan, Robert Salomone, Kerrie Mengersen</li>
<li>for: 本文提供了深入的对深度生成模型在生成数据领域的最新发展，特意关注了表格数据。</li>
<li>methods: 本文使用深度生成模型来生成synthetic数据，并讨论了使用这些模型的优势和相关挑战。</li>
<li>results: 本文详细介绍了使用深度生成模型生成表格数据的方法，并讨论了数据normalization、隐私问题和模型评估等问题。<details>
<summary>Abstract</summary>
This article provides a comprehensive synthesis of the recent developments in synthetic data generation via deep generative models, focusing on tabular datasets. We specifically outline the importance of synthetic data generation in the context of privacy-sensitive data. Additionally, we highlight the advantages of using deep generative models over other methods and provide a detailed explanation of the underlying concepts, including unsupervised learning, neural networks, and generative models. The paper covers the challenges and considerations involved in using deep generative models for tabular datasets, such as data normalization, privacy concerns, and model evaluation. This review provides a valuable resource for researchers and practitioners interested in synthetic data generation and its applications.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇文章提供了对深度生成模型在生成数据方面的最新发展的全面概述，特色是对表格数据的应用。我们特别强调了在隐私敏感数据的场景下使用深度生成模型的重要性。此外，我们还提到了使用深度生成模型的优势，以及对这些模型的下降学习、神经网络和生成模型的解释。文章还讨论了使用深度生成模型处理表格数据时的挑战和考虑因素，如数据normalization、隐私问题和模型评估。这篇文章对研究人员和实践者感兴趣的人来说是一个有价值的资源。
</details></li>
</ul>
<hr>
<h2 id="Is-One-Epoch-All-You-Need-For-Multi-Fidelity-Hyperparameter-Optimization"><a href="#Is-One-Epoch-All-You-Need-For-Multi-Fidelity-Hyperparameter-Optimization" class="headerlink" title="Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?"></a>Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15422">http://arxiv.org/abs/2307.15422</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deephyper/benchmark">https://github.com/deephyper/benchmark</a></li>
<li>paper_authors: Romain Egele, Isabelle Guyon, Yixuan Sun, Prasanna Balaprakash</li>
<li>for: 本研究旨在提高机器学习模型的优化，使用多模型精度水平（Multi-fidelity HPO）来减少计算成本。</li>
<li>methods: 本研究使用了多种代表性的多模型精度水平优化方法，并与简单的基准线进行比较。基准线是在训练一个循环后，保留Top-K个模型，然后进行进一步训练选择最佳模型。</li>
<li>results: 结果显示，基准线 surprisingly achieved similar results to its counterparts, while requiring an order of magnitude less computation。分析学习曲线表明，存在一些主导的学习曲线，这解释了基准线的成功。这表明研究人员应该（1）总是使用建议的基准线和（2）拓宽多模型精度水平优化的 benchmarks 来包括更复杂的情况。<details>
<summary>Abstract</summary>
Hyperparameter optimization (HPO) is crucial for fine-tuning machine learning models but can be computationally expensive. To reduce costs, Multi-fidelity HPO (MF-HPO) leverages intermediate accuracy levels in the learning process and discards low-performing models early on. We compared various representative MF-HPO methods against a simple baseline on classical benchmark data. The baseline involved discarding all models except the Top-K after training for only one epoch, followed by further training to select the best model. Surprisingly, this baseline achieved similar results to its counterparts, while requiring an order of magnitude less computation. Upon analyzing the learning curves of the benchmark data, we observed a few dominant learning curves, which explained the success of our baseline. This suggests that researchers should (1) always use the suggested baseline in benchmarks and (2) broaden the diversity of MF-HPO benchmarks to include more complex cases.
</details>
<details>
<summary>摘要</summary>
（简化中文）机器学习模型精细调整（HPO）是重要的，但可能会占用大量计算资源。为了降低成本，多权限HPO（MF-HPO）利用学习过程中的中间准确级别，早期弃掉低性能的模型。我们对多种代表性的MF-HPO方法进行了比较，并将其与简单的基准线进行了比较。这个基准线是在几个epoch后，仅保留Top-K模型，然后进行进一步的训练来选择最佳模型。很奇怪地，这个基准线能够与其他方法达到类似的结果，而且需要一个数量级的计算量更少。我们对经典数据集的学习曲线进行分析，发现了一些主导的学习曲线，这解释了我们的基准线的成功。这表明研究者应该（1）在benchmark中使用我们的基准线，（2）扩展MF-HPO benchmark的多样性，以包括更复杂的场景。
</details></li>
</ul>
<hr>
<h2 id="The-Initial-Screening-Order-Problem"><a href="#The-Initial-Screening-Order-Problem" class="headerlink" title="The Initial Screening Order Problem"></a>The Initial Screening Order Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15398">http://arxiv.org/abs/2307.15398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jose M. Alvarez, Salvatore Ruggieri</li>
<li>For: 本研究目标是解决初步屏选择问题，这是候选人选择过程中的一个关键步骤。* Methods: 本研究使用了人类化排序算法，以便在候选人池中找到第一个k个适合的候选人，而不是最佳k个适合的候选人。* Results: 研究发现，在不均衡的候选人池（例如有更多男性than女性候选人）下，人类化排序算法可能会受到不平等的努力影响，导致对保护的、下 Representatives of the group are less likely to be selected than non-protected, over-represented groups.  Additionally, the research proves other fairness results under human-like screening.<details>
<summary>Abstract</summary>
In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. Our main contribution is the formalization of the initial screening order problem which, we argue, opens the path for future extensions of the current works on ranking algorithms, fairness, and automation for screening procedures.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了候选人初步排序问题，这是候选人筛选过程中的一个关键步骤。候选人类似于人类屏选员的目标是找到候选人池中的前k个适合者，而不是最佳k个适合者。候选人池的初步排序对选择的候选人集合产生了很大的影响。我们证明，在候选人池不均衡（例如，有更多♂ than ♀ candidates）的情况下，人类屏选员可能会受到不平等的努力，这会对保护的、少数群体进行不公正的决策。我们还证明了其他的公平性结果。这些研究基于与一家大公司的合作，以更好地了解其招聘过程，以便在将来扩展当前的排名算法、公平、自动化招聘过程中的研究。我们的主要贡献在于对初步排序问题的形式化，我们认为这会开启未来的扩展。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Interpolation-Learning-with-Shallow-Univariate-ReLU-Networks"><a href="#Noisy-Interpolation-Learning-with-Shallow-Univariate-ReLU-Networks" class="headerlink" title="Noisy Interpolation Learning with Shallow Univariate ReLU Networks"></a>Noisy Interpolation Learning with Shallow Univariate ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15396">http://arxiv.org/abs/2307.15396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirmit Joshi, Gal Vardi, Nathan Srebro</li>
<li>for: 这个论文研究了采用 interpolate with minimum norm（两层ReLU网络）进行不同损失函数（$L_p$）下的噪声单变量回归问题中的极限欠拟合行为。</li>
<li>methods: 这个论文使用了 minimum norm（两层ReLU网络）来模型不同损失函数（$L_p$）下的噪声单变量回归问题。</li>
<li>results: 研究发现，使用 $L_1$ 损失函数时，欠拟合行为会减少，而使用 $L_p$ 损失函数 ($p&lt;2$) 时，也会减少欠拟合行为，但是使用 $L_p$ 损失函数 ($p\geq 2$) 时，欠拟合行为会变得更加严重。<details>
<summary>Abstract</summary>
We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression. We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p<2$, but catastrophic for $p\geq 2$.
</details>
<details>
<summary>摘要</summary>
我们研究在插值（使用最小二乘（$\ell_2$) 权重）两层ReLU网络中的极值拟合行为。我们显示，对于$L_1$损失函数，极化行为得到控制；对于任何$L_p$损失函数（$p<2），也能够控制极化行为，但对于$p\geq 2$，则会出现极端的拟合现象。Note that I've used the traditional Chinese characters for "拟合" (jìyù) and "损失函数" (shèshī fúnxìn), which are more commonly used in academic writing.
</details></li>
</ul>
<hr>
<h2 id="Does-Full-Waveform-Inversion-Benefit-from-Big-Data"><a href="#Does-Full-Waveform-Inversion-Benefit-from-Big-Data" class="headerlink" title="Does Full Waveform Inversion Benefit from Big Data?"></a>Does Full Waveform Inversion Benefit from Big Data?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15388">http://arxiv.org/abs/2307.15388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Jin, Yinan Feng, Shihang Feng, Hanchen Wang, Yinpeng Chen, Benjamin Consolvo, Zicheng Liu, Youzuo Lin</li>
<li>for: 这篇论文研究了大数据对深度学习模型在全波形推 revert （FWI）中的影响。</li>
<li>methods: 这篇论文使用了 OpenFWI，一个最近发布的大规模多结构数据集，来训练和评估深度学习模型。</li>
<li>results: 实验表明，随着数据集的增大，深度学习模型的性能和泛化性也会提高。此外，模型容量需要与数据大小成正比以获得优化的效果。<details>
<summary>Abstract</summary>
This paper investigates the impact of big data on deep learning models for full waveform inversion (FWI). While it is well known that big data can boost the performance of deep learning models in many tasks, its effectiveness has not been validated for FWI. To address this gap, we present an empirical study that investigates how deep learning models in FWI behave when trained on OpenFWI, a collection of large-scale, multi-structural datasets published recently. Particularly, we train and evaluate the FWI models on a combination of 10 2D subsets in OpenFWI that contain 470K data pairs in total. Our experiments demonstrate that larger datasets lead to better performance and generalization of deep learning models for FWI. We further demonstrate that model capacity needs to scale in accordance with data size for optimal improvement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Co-attention-Graph-Pooling-for-Efficient-Pairwise-Graph-Interaction-Learning"><a href="#Co-attention-Graph-Pooling-for-Efficient-Pairwise-Graph-Interaction-Learning" class="headerlink" title="Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning"></a>Co-attention Graph Pooling for Efficient Pairwise Graph Interaction Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15377">http://arxiv.org/abs/2307.15377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leejunhyun/coattentiongraphpooling">https://github.com/leejunhyun/coattentiongraphpooling</a></li>
<li>paper_authors: Junhyun Lee, Bumsoo Kim, Minji Jeon, Jaewoo Kang</li>
<li>for: 处理和学习图structured数据</li>
<li>methods: 使用 co-attention 在图pooling 中提取交互作表示</li>
<li>results: 在实际数据集上表现竞争力强，与现有方法相比，计算复杂性较低<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have proven to be effective in processing and learning from graph-structured data. However, previous works mainly focused on understanding single graph inputs while many real-world applications require pair-wise analysis for graph-structured data (e.g., scene graph matching, code searching, and drug-drug interaction prediction). To this end, recent works have shifted their focus to learning the interaction between pairs of graphs. Despite their improved performance, these works were still limited in that the interactions were considered at the node-level, resulting in high computational costs and suboptimal performance. To address this issue, we propose a novel and efficient graph-level approach for extracting interaction representations using co-attention in graph pooling. Our method, Co-Attention Graph Pooling (CAGPool), exhibits competitive performance relative to existing methods in both classification and regression tasks using real-world datasets, while maintaining lower computational complexity.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) 有效地处理和学习图Structured data。然而，前一些工作主要关注单个图输入的理解，而实际应用中 часто需要对图Structured data进行对比分析（例如，场景图匹配、代码搜索和药物交互预测）。为此，最近的工作强调了对对Graph Structured data的对比分析。尽管它们的性能得到改进，但是它们仍然受到节点级别的交互所限，导致计算成本高、性能下降。为解决这个问题，我们提出了一种新的和高效的图级别方法，即协同注意力图Pooling（CAGPool）。我们的方法在实际数据集上展现了与现有方法相当的竞争性，而且计算复杂度较低。
</details></li>
</ul>
<hr>
<h2 id="Conflict-free-joint-decision-by-lag-and-zero-lag-synchronization-in-laser-network"><a href="#Conflict-free-joint-decision-by-lag-and-zero-lag-synchronization-in-laser-network" class="headerlink" title="Conflict-free joint decision by lag and zero-lag synchronization in laser network"></a>Conflict-free joint decision by lag and zero-lag synchronization in laser network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15373">http://arxiv.org/abs/2307.15373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hisako Ito, Takatomo Mihana, Ryoichi Horisaki, Makoto Naruse</li>
<li>for: 这个论文探讨了用激光网络作为光学加速器解决竞争多重臂炮问题。</li>
<li>methods: 该研究使用了激光网络实现了协同决策，并通过零延迟和延迟同步实现了冲突避免和决策效果。</li>
<li>results: 实验表明，该系统在基本2名玩家、2个槽scenario中实现了低冲突率和高奖励，并且展示了该系统的可扩展性。<details>
<summary>Abstract</summary>
With the end of Moore's Law and the increasing demand for computing, photonic accelerators are garnering considerable attention. This is due to the physical characteristics of light, such as high bandwidth and multiplicity, and the various synchronization phenomena that emerge in the realm of laser physics. These factors come into play as computer performance approaches its limits. In this study, we explore the application of a laser network, acting as a photonic accelerator, to the competitive multi-armed bandit problem. In this context, conflict avoidance is key to maximizing environmental rewards. We experimentally demonstrate cooperative decision-making using zero-lag and lag synchronization within a network of four semiconductor lasers. Lag synchronization of chaos realizes effective decision-making and zero-delay synchronization is responsible for the realization of the collision avoidance function. We experimentally verified a low collision rate and high reward in a fundamental 2-player, 2-slot scenario, and showed the scalability of this system. This system architecture opens up new possibilities for intelligent functionalities in laser dynamics.
</details>
<details>
<summary>摘要</summary>
随着莫尔定律的结束和计算机能力的增长，光学加速器正在吸引广泛的关注。这是因为光的物理特性，如带宽和多重性，以及光学同步现象的多种表现。这些因素在计算机性能接近限制时变得重要。在这项研究中，我们探讨了使用激光网络作为光学加速器，解决竞争多臂优化问题。在这种情况下，避免冲突是最大化环境奖励的关键。我们通过实验证明了协同决策中的零延迟和延迟同步可以实现有效的决策，并证明了冲突避免功能的实现。我们在基本的2个玩家、2个槽场景中实际验证了低冲突率和高奖励。这种系统架构开启了新的智能功能在激光动力学中。
</details></li>
</ul>
<hr>
<h2 id="Toward-Transparent-Sequence-Models-with-Model-Based-Tree-Markov-Model"><a href="#Toward-Transparent-Sequence-Models-with-Model-Based-Tree-Markov-Model" class="headerlink" title="Toward Transparent Sequence Models with Model-Based Tree Markov Model"></a>Toward Transparent Sequence Models with Model-Based Tree Markov Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15367">http://arxiv.org/abs/2307.15367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chan Hsu, Wei-Chun Huang, Jun-Ting Wu, Chih-Yuan Li, Yihuang Kang</li>
<li>for: 本研究旨在解决复杂黑盒机器学习模型在序列数据上的解释性问题。</li>
<li>methods: 本研究提出了基于树的隐藏Markov模型（MOB-HSMM），这是一种具有解释性的模型，用于在医学集中检测高死亡风险事件和找到隐藏的死亡风险相关性。该模型利用了深度神经网络（DNN）中的知识来提高预测性能，同时提供了明确的解释。</li>
<li>results: 我们的实验结果表明，通过使用LSTM学习序列模式，并将其传递给MOB树，可以提高模型的预测性能。将MOB树与隐藏Markov模型（HSMM）结合在一起，可以揭示出可能的和解释的序列。<details>
<summary>Abstract</summary>
In this study, we address the interpretability issue in complex, black-box Machine Learning models applied to sequence data. We introduce the Model-Based tree Hidden Semi-Markov Model (MOB-HSMM), an inherently interpretable model aimed at detecting high mortality risk events and discovering hidden patterns associated with the mortality risk in Intensive Care Units (ICU). This model leverages knowledge distilled from Deep Neural Networks (DNN) to enhance predictive performance while offering clear explanations. Our experimental results indicate the improved performance of Model-Based trees (MOB trees) via employing LSTM for learning sequential patterns, which are then transferred to MOB trees. Integrating MOB trees with the Hidden Semi-Markov Model (HSMM) in the MOB-HSMM enables uncovering potential and explainable sequences using available information.
</details>
<details>
<summary>摘要</summary>
在这种研究中，我们解决了复杂黑盒机器学习模型应用于序列数据中的解释性问题。我们介绍了基于模型的树隐藏半马尔可夫模型（MOB-HSMM），这是一种自然解释的模型，用于检测高死亡风险事件和找到隐藏在ICU中的死亡风险相关的Pattern。这个模型利用了深度神经网络（DNN）提供的知识来提高预测性能，同时提供明确的解释。我们的实验结果表明，通过使用LSTM学习序列模式，可以提高模型树（MOB trees）的性能。将MOB树与隐藏半马尔可夫模型（HSMM）结合在一起，可以揭示可用信息中的可能和解释性序列。
</details></li>
</ul>
<hr>
<h2 id="Confident-Feature-Ranking"><a href="#Confident-Feature-Ranking" class="headerlink" title="Confident Feature Ranking"></a>Confident Feature Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15361">http://arxiv.org/abs/2307.15361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bitya Neuhof, Yuval Benjamini</li>
<li>for: 本文提出了一种方法，用于解决 interpretability 问题，即 feature importance 值的解释方法。</li>
<li>methods: 本文使用了 pairwise comparisons 方法，以生成一个稳定的排名，同时也生成了相应的信任区间。</li>
<li>results: 本文 garantía que包含了“真实”（无限大样本）排名，并且允许选择 top-k 集。<details>
<summary>Abstract</summary>
Interpretation of feature importance values often relies on the relative order of the features rather than on the value itself, referred to as ranking. However, the order may be unstable due to the small sample sizes used in calculating the importance values. We propose that post-hoc importance methods produce a ranking and simultaneous confident intervals for the rankings. Based on pairwise comparisons of the feature importance values, our method is guaranteed to include the ``true'' (infinite sample) ranking with high probability and allows for selecting top-k sets.
</details>
<details>
<summary>摘要</summary>
通常来说，特征重要性值的解释通过特征之间的相对排序而进行，而不是直接查看值本身。这种排序方式被称为排名。然而，由于使用小样本计算特征重要性值的时候，排名可能不稳定。我们提议使用 posterior 重要性方法生成排名和同时确定范围。这种方法可以 garantizar在无穷样本情况下包含“真实”排名，并允许选择top-k集。Note: "posterior" in Chinese is "后期" (hòujiè), and "simultaneous" is "同时" (tóngshí).
</details></li>
</ul>
<hr>
<h2 id="Med-HALT-Medical-Domain-Hallucination-Test-for-Large-Language-Models"><a href="#Med-HALT-Medical-Domain-Hallucination-Test-for-Large-Language-Models" class="headerlink" title="Med-HALT: Medical Domain Hallucination Test for Large Language Models"></a>Med-HALT: Medical Domain Hallucination Test for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15343">http://arxiv.org/abs/2307.15343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Logesh Kumar Umapathi, Ankit Pal, Malaikannan Sankarasubbu</li>
<li>for: 本研究探讨了大语言模型（LLM）中的幻觉问题，特别是在医疗领域。幻觉可能会在医疗应用中产生严重的后果。</li>
<li>methods: 我们提出了一个新的指标和数据集，即医疗领域幻觉测试（Med-HALT），用于评估和减少幻觉。Med-HALT 数据集来自多个国家和不同的医疗检查，包括多种创新的测试方法。</li>
<li>results: 我们对主流 LLM 进行了评估，包括 Text Davinci、GPT-3.5、LlaMa-2、MPT 和 Falcon，发现了这些模型在幻觉问题上的显著差异。本研究提供了数据集的详细信息，推动了透明度和可重现性。通过这项工作，我们希望为医疗领域中更安全和可靠的语言模型的开发做出贡献。我们的指标可以在 medhalt.github.io 找到。<details>
<summary>Abstract</summary>
This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.   Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io
</details>
<details>
<summary>摘要</summary>
Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, and found significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. The Med-HALT benchmark can be found at medhalt.github.io.
</details></li>
</ul>
<hr>
<h2 id="The-Radon-Signed-Cumulative-Distribution-Transform-and-its-applications-in-classification-of-Signed-Images"><a href="#The-Radon-Signed-Cumulative-Distribution-Transform-and-its-applications-in-classification-of-Signed-Images" class="headerlink" title="The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images"></a>The Radon Signed Cumulative Distribution Transform and its applications in classification of Signed Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15339">http://arxiv.org/abs/2307.15339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rohdelab/PyTransKit">https://github.com/rohdelab/PyTransKit</a></li>
<li>paper_authors: Le Gong, Shiying Li, Naqib Sad Pathan, Mohammad Shifat-E-Rabbi, Gustavo K. Rohde, Abu Hasnat Mohammad Rubaiyat, Sumati Thareja</li>
<li>for: 这种新的图像表示技术基于运输学和最优运输学 mathematics, 用于图像的分类。</li>
<li>methods: 该方法结合了较为常见的拉登变换和签名总额变换，用于将图像转换为一种新的表示形式。</li>
<li>results: 该方法可以更准确地表示签名图像中的信息内容，从而实现更高的分类精度。Here is the same information in Traditional Chinese:</li>
<li>for: 这种新的图像表示技术基于运输学和最佳运输学 mathematics, 用于图像的分类。</li>
<li>methods: 这方法结合了较为常见的拉登变换和签名总额变换，用于将图像转换为一种新的表示形式。</li>
<li>results: 这方法可以更准确地表示签名图像中的信息内容，从而实现更高的分类精度。<details>
<summary>Abstract</summary>
Here we describe a new image representation technique based on the mathematics of transport and optimal transport. The method relies on the combination of the well-known Radon transform for images and a recent signal representation method called the Signed Cumulative Distribution Transform. The newly proposed method generalizes previous transport-related image representation methods to arbitrary functions (images), and thus can be used in more applications. We describe the new transform, and some of its mathematical properties and demonstrate its ability to partition image classes with real and simulated data. In comparison to existing transport transform methods, as well as deep learning-based classification methods, the new transform more accurately represents the information content of signed images, and thus can be used to obtain higher classification accuracies. The implementation of the proposed method in Python language is integrated as a part of the software package PyTransKit, available on Github.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的图像表示技术，基于运输和最优运输的数学。该方法通过结合已知的快速傅立叶变换和最近的有标签分布变换来实现。该新提议的方法可以应用于任意函数（图像），因此可以用于更多的应用。我们描述了新的变换，以及其数学性质，并通过实际数据和模拟数据示例来说明其能够更好地分类图像。与现有的运输变换方法和深度学习基于分类方法相比，新的变换可以更准确地表示签名图像的信息内容，因此可以获得更高的分类精度。我们在Python语言中实现了该方法，并将其integrated into the software package PyTransKit，可以在Github上下载。
</details></li>
</ul>
<hr>
<h2 id="Staging-E-Commerce-Products-for-Online-Advertising-using-Retrieval-Assisted-Image-Generation"><a href="#Staging-E-Commerce-Products-for-Online-Advertising-using-Retrieval-Assisted-Image-Generation" class="headerlink" title="Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation"></a>Staging E-Commerce Products for Online Advertising using Retrieval Assisted Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15326">http://arxiv.org/abs/2307.15326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueh-Ning Ku, Mikhail Kuznetsov, Shaunak Mishra, Paloma de Juan</li>
<li>for: 提高 dynamically generated product ads（DPA）的图像质量，以提高点击率（CTR）。</li>
<li>methods: 使用生成对抗网络（GAN）和检索帮助GAN（Retrieval Assisted GAN，RA-GAN）生成产品图像的stage background。</li>
<li>results: 通过对比各种方法，显示了复制粘贴阶段（Copy-Paste Staging）的效果，并通过人工评估和线上指标评价得到了良好的结果。此外，还实现了产品图像动画的创建，从而实现了视频广告的生成。<details>
<summary>Abstract</summary>
Online ads showing e-commerce products typically rely on the product images in a catalog sent to the advertising platform by an e-commerce platform. In the broader ads industry such ads are called dynamic product ads (DPA). It is common for DPA catalogs to be in the scale of millions (corresponding to the scale of products which can be bought from the e-commerce platform). However, not all product images in the catalog may be appealing when directly re-purposed as an ad image, and this may lead to lower click-through rates (CTRs). In particular, products just placed against a solid background may not be as enticing and realistic as a product staged in a natural environment. To address such shortcomings of DPA images at scale, we propose a generative adversarial network (GAN) based approach to generate staged backgrounds for un-staged product images. Generating the entire staged background is a challenging task susceptible to hallucinations. To get around this, we introduce a simpler approach called copy-paste staging using retrieval assisted GANs. In copy paste staging, we first retrieve (from the catalog) staged products similar to the un-staged input product, and then copy-paste the background of the retrieved product in the input image. A GAN based in-painting model is used to fill the holes left after this copy-paste operation. We show the efficacy of our copy-paste staging method via offline metrics, and human evaluation. In addition, we show how our staging approach can enable animations of moving products leading to a video ad from a product image.
</details>
<details>
<summary>摘要</summary>
在线广告通常显示电商产品时，会使用电商平台提供的目录，其中包括大量的产品图像。在更广泛的广告业界，这种广告方式被称为动态产品广告（DPA）。这些目录通常包含数百万个产品图像，但不 всех图像都能够直接重用为广告图像，这可能导致更低的点击率（CTR）。特别是，拥有简单背景的产品可能不那么吸引人和真实。为解决这种大规模DPA图像的缺陷，我们提出了基于生成对抗网络（GAN）的方法，生成产品在自然环境中的舞台。生成整个舞台是一项复杂的任务，易于幻化。为此，我们提出了一种简单的方法：复制粘贴舞台使用检索帮助GAN。在这种方法中，我们首先从目录中检索与输入产品相似的已经舞台化产品，然后将其中的背景粘贴到输入图像中。GAN基于填充模型用于填充这些粘贴后的孔隙。我们通过在线指标和人工评估展示了我们的复制粘贴方法的效果。此外，我们还展示了如何使用我们的舞台方法生成动画产品图像，从而将产品图像转换成视频广告。
</details></li>
</ul>
<hr>
<h2 id="Partial-observations-coarse-graining-and-equivariance-in-Koopman-operator-theory-for-large-scale-dynamical-systems"><a href="#Partial-observations-coarse-graining-and-equivariance-in-Koopman-operator-theory-for-large-scale-dynamical-systems" class="headerlink" title="Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems"></a>Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15325">http://arxiv.org/abs/2307.15325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Peitz, Hans Harder, Feliks Nüske, Friedrich Philipp, Manuel Schaller, Karl Worthmann</li>
<li>for: This paper focuses on addressing the challenge of using the Koopman operator for large-scale systems with partial observations or coarse graining.</li>
<li>methods: The authors propose a new method that takes into account the symmetries in the system dynamics to improve the efficiency of the Koopman operator approximation.</li>
<li>results: The authors show that their method can massively increase the model efficiency and provide a more accurate approximation of the Koopman operator for the underlying system, using the Kuramoto–Sivashinsky equation as a numerical example.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文关注使用Koopman算子来分析、预测和控制复杂系统，但是只有部分观察数据或者归约到更粗糙的系统。</li>
<li>methods: 作者们提出了一种新的方法，该方法利用系统动力学的对称性来提高Koopman算子的近似精度。</li>
<li>results: 作者们表明，他们的方法可以很大程度地提高模型的效率，并且可以更 precisely  aproximate the Koopman operator for the underlying system，使用 Kuramoto–Sivashinsky 方程作为数值示例。<details>
<summary>Abstract</summary>
The Koopman operator has become an essential tool for data-driven analysis, prediction and control of complex systems, the main reason being the enormous potential of identifying linear function space representations of nonlinear dynamics from measurements. Until now, the situation where for large-scale systems, we (i) only have access to partial observations (i.e., measurements, as is very common for experimental data) or (ii) deliberately perform coarse graining (for efficiency reasons) has not been treated to its full extent. In this paper, we address the pitfall associated with this situation, that the classical EDMD algorithm does not automatically provide a Koopman operator approximation for the underlying system if we do not carefully select the number of observables. Moreover, we show that symmetries in the system dynamics can be carried over to the Koopman operator, which allows us to massively increase the model efficiency. We also briefly draw a connection to domain decomposition techniques for partial differential equations and present numerical evidence using the Kuramoto--Sivashinsky equation.
</details>
<details>
<summary>摘要</summary>
科普曼算子已成为数据驱动分析、预测和控制复杂系统的重要工具，主要原因是可以从测量数据中提取非线性动力学的线性函数空间表示。然而，在大规模系统中，我们通常只有部分观察数据（例如，测量数据）或者故意粗略化（为了提高效率）。在这篇论文中，我们探讨这种情况下经典EDMD算法不会自动为下面系统提供库曼操作符的近似值，并且显示系统动力学Symmetry可以传递到库曼操作符，从而巨大提高模型效率。此外，我们 briefly drew a connection to域 decompositions techniques for partial differential equations, and presented numerical evidence using the Kuramoto--Sivashinsky equation.Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may be different.
</details></li>
</ul>
<hr>
<h2 id="Robust-Visual-Sim-to-Real-Transfer-for-Robotic-Manipulation"><a href="#Robust-Visual-Sim-to-Real-Transfer-for-Robotic-Manipulation" class="headerlink" title="Robust Visual Sim-to-Real Transfer for Robotic Manipulation"></a>Robust Visual Sim-to-Real Transfer for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15320">http://arxiv.org/abs/2307.15320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Garcia, Robin Strudel, Shizhe Chen, Etienne Arlaud, Ivan Laptev, Cordelia Schmid</li>
<li>for: 本研究旨在帮助学习视omyotor策略在模拟环境中更加安全和有效，但是由于模拟和实际数据之间的差异，模拟训练的策略通常无法在实际机器人上成功。这种情况下，一种常见的方法是域随机化（DR）。</li>
<li>methods: 本研究系统atically explores visual域随机化方法，并对其进行了严格的 benchmarking。特别是，我们提出了一个离线代理任务——立方体localization，以选择DR参数的Texture Randomization、Lighting Randomization、物体颜色变化和摄像头参数。我们发现DR参数在离线代理任务和在线策略之间具有类似的影响。因此，我们使用离线优化的DR参数来训练视omyotor策略在模拟中，并直接将其应用到实际机器人上。</li>
<li>results: 我们的方法可以在一个多样化的机器人抓取任务上 achieve 93% 的成功率。此外，我们还评估了政策在实际场景中的Robustness，并发现我们在模拟中训练的策略在实际场景中表现更加稳定和有效。 codes、模拟环境、实际机器人数据和训练模型都可以在<a target="_blank" rel="noopener" href="https://www.di.ens.fr/willow/research/robust_s2r/">https://www.di.ens.fr/willow/research/robust_s2r/</a> obtained。<details>
<summary>Abstract</summary>
Learning visuomotor policies in simulation is much safer and cheaper than in the real world. However, due to discrepancies between the simulated and real data, simulator-trained policies often fail when transferred to real robots. One common approach to bridge the visual sim-to-real domain gap is domain randomization (DR). While previous work mainly evaluates DR for disembodied tasks, such as pose estimation and object detection, here we systematically explore visual domain randomization methods and benchmark them on a rich set of challenging robotic manipulation tasks. In particular, we propose an off-line proxy task of cube localization to select DR parameters for texture randomization, lighting randomization, variations of object colors and camera parameters. Notably, we demonstrate that DR parameters have similar impact on our off-line proxy task and on-line policies. We, hence, use off-line optimized DR parameters to train visuomotor policies in simulation and directly apply such policies to a real robot. Our approach achieves 93% success rate on average when tested on a diverse set of challenging manipulation tasks. Moreover, we evaluate the robustness of policies to visual variations in real scenes and show that our simulator-trained policies outperform policies learned using real but limited data. Code, simulation environment, real robot datasets and trained models are available at https://www.di.ens.fr/willow/research/robust_s2r/.
</details>
<details>
<summary>摘要</summary>
学习视мотор策略在模拟中比实际世界更安全和便宜。然而，由于模拟和实际数据之间的差异，模拟训练的策略通常在实际机器人上失败。一种常见的方法是域随机化（DR）来bridging模拟和实际的视域域随机化。在这种情况下，我们系统地探索了视域随机化方法，并对其进行了丰富的检验。我们提出了一个离线代理任务，即立方体 Localization，以选择DR参数进行XTURE随机化、照明随机化、物体颜色变化和摄像头参数的选择。我们发现DR参数在离线代理任务和在线策略之间具有类似的影响。因此，我们使用离线优化的DR参数来训练视мотор策略在模拟中，并直接将其应用到实际机器人上。我们的方法在一组多样化的机器人抓取任务中取得了93%的成功率。此外，我们评估了在实际场景中视图变化的影响，并发现我们在模拟中训练的策略在实际数据上表现更加稳定和高效。我们的代码、模拟环境、实际机器人数据和训练模型可以在https://www.di.ens.fr/willow/research/robust_s2r/取得。
</details></li>
</ul>
<hr>
<h2 id="SAP-sLDA-An-Interpretable-Interface-for-Exploring-Unstructured-Text"><a href="#SAP-sLDA-An-Interpretable-Interface-for-Exploring-Unstructured-Text" class="headerlink" title="SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text"></a>SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01420">http://arxiv.org/abs/2308.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charumathi Badrinath, Weiwei Pan, Finale Doshi-Velez</li>
<li>for: 该论文目的是提出一种基于LDA的半监督人工智能方法，用于从文本集中学习主题，并保持文档之间含义上的相互关系。</li>
<li>methods: 该方法使用了LDA算法，并在人工智能 loop中添加了一些约束来保持文档之间的相互关系。</li>
<li>results: 在 sintetic corpora 上，该方法可以生成更加 interpretable 的低维度投影，比基eline方法更好。在真实的 corpus 上，该方法也 obtaint 质量相似的结果。<details>
<summary>Abstract</summary>
A common way to explore text corpora is through low-dimensional projections of the documents, where one hopes that thematically similar documents will be clustered together in the projected space. However, popular algorithms for dimensionality reduction of text corpora, like Latent Dirichlet Allocation (LDA), often produce projections that do not capture human notions of document similarity. We propose a semi-supervised human-in-the-loop LDA-based method for learning topics that preserve semantically meaningful relationships between documents in low-dimensional projections. On synthetic corpora, our method yields more interpretable projections than baseline methods with only a fraction of labels provided. On a real corpus, we obtain qualitatively similar results.
</details>
<details>
<summary>摘要</summary>
一种常见的文本资料探索方法是通过文本文档的低维度投影，希望将主题相似的文档在投影空间中归一化。然而，受欢迎的文本维度减少算法，如潜在 Dirichlet 分配（LDA），经常生成投影，不会捕捉人类的文档相似性。我们提出了一种半监督人类在循环中 LDA 基于方法，用于学习保留含义意义的文档关系。在 sintetic  corpora 上，我们的方法可以得到更加可读的投影，只需提供一小部分的标签。在真实 corpora 上，我们获得了类似的结果。
</details></li>
</ul>
<hr>
<h2 id="DiffKendall-A-Novel-Approach-for-Few-Shot-Learning-with-Differentiable-Kendall’s-Rank-Correlation"><a href="#DiffKendall-A-Novel-Approach-for-Few-Shot-Learning-with-Differentiable-Kendall’s-Rank-Correlation" class="headerlink" title="DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall’s Rank Correlation"></a>DiffKendall: A Novel Approach for Few-Shot Learning with Differentiable Kendall’s Rank Correlation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15317">http://arxiv.org/abs/2307.15317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaipeng Zheng, Huishuai Zhang, Weiran Huang</li>
<li>for: 增强几何学习中的类别不熟悉问题，通过重要性排名特征通道的importance来改善类别识别性能。</li>
<li>methods: 使用Kendall的排名相关度代替几何相似度метри克，并提出一种特殊设计的可导损失函数来解决非导数问题。</li>
<li>results: 对各种领域和数据集进行了广泛的实验，证明了排名相关度基于方法可以大幅提高几何学习性能。<details>
<summary>Abstract</summary>
Few-shot learning aims to adapt models trained on the base dataset to novel tasks where the categories are not seen by the model before. This often leads to a relatively uniform distribution of feature values across channels on novel classes, posing challenges in determining channel importance for novel tasks. Standard few-shot learning methods employ geometric similarity metrics such as cosine similarity and negative Euclidean distance to gauge the semantic relatedness between two features. However, features with high geometric similarities may carry distinct semantics, especially in the context of few-shot learning. In this paper, we demonstrate that the importance ranking of feature channels is a more reliable indicator for few-shot learning than geometric similarity metrics. We observe that replacing the geometric similarity metric with Kendall's rank correlation only during inference is able to improve the performance of few-shot learning across a wide range of datasets with different domains. Furthermore, we propose a carefully designed differentiable loss for meta-training to address the non-differentiability issue of Kendall's rank correlation. Extensive experiments demonstrate that the proposed rank-correlation-based approach substantially enhances few-shot learning performance.
</details>
<details>
<summary>摘要</summary>
几个shot学习目标是使模型从基础数据集中适应新任务，其中类别未曾由模型见过。这经常导致新任务频道值的相对均匀分布，从而增加了决定频道重要性的挑战。标准的几个shot学习方法使用几何相似度度量，如cos相似性和负Euclidean距离，来衡量两个特征之间的semantic相似性。但是，具有高几何相似度的特征可能拥有不同的 semantics，特别在几个shot学习上。在这篇论文中，我们表明了特征通道的重要性排名是几个shot学习中更可靠的指标，而不是几何相似度度量。我们发现，在推理过程中只是将几何相似度度量替换为Kendall的排名相关性可以在各种领域的数据集上提高几个shot学习的性能。此外，我们提出了一种特殊的可导损失函数，用于meta-训练，以解决Kendall的排名相关性的不导数性问题。广泛的实验表明，我们的排名相关性基本上提高了几个shot学习的性能。
</details></li>
</ul>
<hr>
<h2 id="Differential-Evolution-Algorithm-based-Hyper-Parameters-Selection-of-Transformer-Neural-Network-Model-for-Load-Forecasting"><a href="#Differential-Evolution-Algorithm-based-Hyper-Parameters-Selection-of-Transformer-Neural-Network-Model-for-Load-Forecasting" class="headerlink" title="Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting"></a>Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15299">http://arxiv.org/abs/2307.15299</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anuvabsen1/meta-transformer">https://github.com/anuvabsen1/meta-transformer</a></li>
<li>paper_authors: Anuvab Sen, Arul Rhik Mazumder, Udayon Sen</li>
<li>for: 预测电力负荷的精度是许多领域的关键，但是传统的统计模型很难准确地捕捉动态电力系统的复杂 dinamics。因此，时间序列模型（ARIMA）和深度学习模型（ANN、LSTM、GRU等）经常被使用，并经常获得更高的成功。</li>
<li>methods: 本文分析了在负荷预测中使用Recently发展的Transformer模型的效果。Transformer模型具有学习长距离依赖关系的能力，因此它们有可能改善负荷预测的准确性。我们使用了多种metaheuristics，包括不同的Differential Evolution算法来找出最佳的超参数。</li>
<li>results: 我们通过对不同metaheuristics算法和Transformer模型的组合进行比较，发现这些组合在负荷预测中的性能都非常高。具体来说，使用Differential Evolution算法来优化Transformer模型可以减少负荷预测的偏差和误差。<details>
<summary>Abstract</summary>
Accurate load forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of dynamic power systems remains a challenge for traditional statistical models. For these reasons, time-series models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly deployed and often experience higher success. In this paper, we analyze the efficacy of the recently developed Transformer-based Neural Network model in Load forecasting. Transformer models have the potential to improve Load forecasting because of their ability to learn long-range dependencies derived from their Attention Mechanism. We apply several metaheuristics namely Differential Evolution to find the optimal hyperparameters of the Transformer-based Neural Network to produce accurate forecasts. Differential Evolution provides scalable, robust, global solutions to non-differentiable, multi-objective, or constrained optimization problems. Our work compares the proposed Transformer based Neural Network model integrated with different metaheuristic algorithms by their performance in Load forecasting based on numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced Transformer-based Neural Network models in Load forecasting accuracy and provide optimal hyperparameters for each model.
</details>
<details>
<summary>摘要</summary>
准确的负荷预测在多个领域发挥重要作用，但是正确地捕捉动态电力系统的复杂动态还是传统统计模型的挑战。因此，时间序列模型（ARIMA）和深度学习模型（ANN、LSTM、GRU等）通常被部署，并经常获得更高的成功。在这篇论文中，我们分析了在负荷预测中使用最近开发的Transformer基于神经网络模型的效果。Transformer模型具有学习长距离依赖关系的能力，因此它们有可能改善负荷预测的准确性。我们使用了多种metaheuristics，包括不同的Differential Evolution算法，以找到最佳的Hyperparameter，以生成高精度的预测结果。我们的工作对不同metaheuristic算法 integrated with Transformer基于神经网络模型的性能进行比较，并根据numerical metrics such as Mean Squared Error (MSE)和Mean Absolute Percentage Error (MAPE)进行评价。我们的发现表明metaheuristic强化的Transformer基于神经网络模型在负荷预测准确性方面具有潜在的潜力，并且提供了每个模型的最佳Hyperparameter。
</details></li>
</ul>
<hr>
<h2 id="Learning-Nonlinear-Projections-for-Reduced-Order-Modeling-of-Dynamical-Systems-using-Constrained-Autoencoders"><a href="#Learning-Nonlinear-Projections-for-Reduced-Order-Modeling-of-Dynamical-Systems-using-Constrained-Autoencoders" class="headerlink" title="Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders"></a>Learning Nonlinear Projections for Reduced-Order Modeling of Dynamical Systems using Constrained Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15288">http://arxiv.org/abs/2307.15288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/grmacchio/romnet_chaos2023">https://github.com/grmacchio/romnet_chaos2023</a></li>
<li>paper_authors: Samuel E. Otto, Gregory R. Macchio, Clarence W. Rowley</li>
<li>for: 这种新发展的减少模型技术用于近似非线性动力系统的低维度抽象，从数据学习出来的抽象 manifold 上的动态系统。</li>
<li>methods: 我们使用受限的自动encoder神经网络来学习 manifold 和投影纤维，并使用不同的权重矩阵和强制活动函数来保证encoder是解码器的左逆。我们还引入了新的动力学感知成本函数，以便学习倾斜投影纤维，考虑到快速动力学和非正态感知机制。</li>
<li>results: 我们通过一个简单的三个状态模型，对某种在一个流体中的一个粗体后形成的涡流进行了示例研究，并证明了我们的方法可以有效地模型普通动态系统的抽象。此外，我们还提出了一些用于构建高维系统的减少模型的技术，包括一种新的稀热性激发 penalty，以避免encoder weights缩回的问题。<details>
<summary>Abstract</summary>
Recently developed reduced-order modeling techniques aim to approximate nonlinear dynamical systems on low-dimensional manifolds learned from data. This is an effective approach for modeling dynamics in a post-transient regime where the effects of initial conditions and other disturbances have decayed. However, modeling transient dynamics near an underlying manifold, as needed for real-time control and forecasting applications, is complicated by the effects of fast dynamics and nonnormal sensitivity mechanisms. To begin to address these issues, we introduce a parametric class of nonlinear projections described by constrained autoencoder neural networks in which both the manifold and the projection fibers are learned from data. Our architecture uses invertible activation functions and biorthogonal weight matrices to ensure that the encoder is a left inverse of the decoder. We also introduce new dynamics-aware cost functions that promote learning of oblique projection fibers that account for fast dynamics and nonnormality. To demonstrate these methods and the specific challenges they address, we provide a detailed case study of a three-state model of vortex shedding in the wake of a bluff body immersed in a fluid, which has a two-dimensional slow manifold that can be computed analytically. In anticipation of future applications to high-dimensional systems, we also propose several techniques for constructing computationally efficient reduced-order models using our proposed nonlinear projection framework. This includes a novel sparsity-promoting penalty for the encoder that avoids detrimental weight matrix shrinkage via computation on the Grassmann manifold.
</details>
<details>
<summary>摘要</summary>
近期发展的减少模型技术目的是通过从数据学习低维度拟合来近似非线性动力系统。这是一种有效的方法，在卷积过程后的吸收器恢复过程中模型动力系统的吸收器恢复过程。然而，在近似动态系统的恢复过程中，因为快速动力和非正常敏感机制的影响，模型化吸收器恢复过程是复杂的。为解决这些问题，我们提出了一个参数化的非线性投影框架，其中拟合的拟合网络使用卷积神经网络学习数据，并使用卷积神经网络的权重矩阵来保证投影网络的可逆性。我们还引入了新的动力敏感成本函数，以便在学习投影纤维时考虑快速动力和非正常性。为了证明我们的方法和特定挑战，我们提供了一个详细的三个状态模型的涡流排斥示例，该模型具有可以计算出的二维慢拟合。在未来应用于高维系统时，我们还提出了一些构建高效减少模型的技术，包括一种新的缺省值激活函数，以避免因计算格先替换而导致的负面weight矩阵缩小。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Approximation-of-Zonoids-and-Uniform-Approximation-by-Shallow-Neural-Networks"><a href="#Optimal-Approximation-of-Zonoids-and-Uniform-Approximation-by-Shallow-Neural-Networks" class="headerlink" title="Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks"></a>Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15285">http://arxiv.org/abs/2307.15285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan W. Siegel</li>
<li>for: 本研究探讨了两个相关的问题。第一个问题是判断任意ζonoid在 $\mathbb{R}^{d+1}$ 中可以在 Hausdorff 距离上被近似为一个SUM OF $n$ 条直线段。第二个问题是确定 shallow ReLU$^k$ 神经网络在其变化空间上的优化approximation rate。</li>
<li>methods: 我们使用了新的技术来解决第一个问题，并在 $d\neq 2,3$ 的情况下已经完成了解决。当 $d&#x3D;2,3$ 时，我们关闭了 logarithmic gap，完成了所有维度的解决。在第二个问题上，我们的技术可以在 $k\geq 1$ 时对 uniform norm 进行优化approximation，并能够对目标函数和其导数进行均匀approximation。</li>
<li>results: 我们的研究结果显示，我们可以在 $d\neq 2,3$ 的情况下提供优化的approximation rate，并且在 $d&#x3D;2,3$ 时关闭了 logarithmic gap。此外，我们的技术还可以在 shallow ReLU$^k$ 神经网络上实现更好的approximation rate。<details>
<summary>Abstract</summary>
We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
</details>
<details>
<summary>摘要</summary>
我们研究以下两个相关的问题。第一个问题是确定任意㴒形在 $\mathbb{R}^{d+1} $ 可以被对准 Hausdorff 距离的近似，使用 $n$ 条直线段。第二个问题是 Determine  shallow ReLU $^k$ 神经网络在其变数空间上的最佳近似率，我们的技术可以对 $k\geq 1$ 进行改进，并允许对目标函数和其 derivatives 进行均匀近似。Here's the breakdown of the translation:* 我们 (wǒ men) - we* 研究 (yan jiu) - study* 以下 (yǐ xià) - the following* 两个 (liǎng ge) - two* 相关 (xiāng guān) - related* 问题 (wèn tí) - problems* 第一个 (dì yī ge) - the first* 问题 (wèn tí) - problem* 是 (shì) - is* 确定 (kāo dìng) - to determine* 任意 (rèn yì) - arbitrary* 㴒形 (dào xíng) - zonoid* 在 (zhī) - in* $\mathbb{R}^{d+1}$ (REAL d+1) - the space of all real $(d+1)$-tuples* 可以 (kěn yǐ) - can* 被 (bèi) - be* 对准 (duì zhèng) - approximated* Hausdorff 距离 (Hausdorff yuè lǐ) - Hausdorff distance* 使用 (fù yòng) - using* $n$ (n) - a positive integer* 条 (tiě) - line* 直线段 (zhí xiàn tiě) - line segment* 第二个问题 (dì èr ge wèn tí) - the second problem* Determine (dì tí) - determine*  shallow ReLU $^k$ (piān shū ReLU $^k$) - shallow ReLU $^k$ neural networks* 在其变数空间上 (zhī qiè yàng yòu zhèng xiàng) - on their variation spaces* 最佳近似率 (mài jiā qióng yì lǚ) - optimal approximation rates* 我们的技术 (wǒ men de jì shù) - our techniques* 可以 (kěn yǐ) - can* 对 (duì) - towards* $k\geq 1$ (k ge 1) - when $k\geq 1$* 进行改进 (jìn zuò gǎi jì) - to improve* 并 (bìn) - and* 允许 (yùn xū) - allow* 对目标函数 (duì mù zhì fù) - towards the target function* 和其 derivatives (hè qí yī jī) - and its derivatives* 进行均匀近似 (jìn zuò jìng yì qióng) - to achieve uniform approximation
</details></li>
</ul>
<hr>
<h2 id="VeriGen-A-Large-Language-Model-for-Verilog-Code-Generation"><a href="#VeriGen-A-Large-Language-Model-for-Verilog-Code-Generation" class="headerlink" title="VeriGen: A Large Language Model for Verilog Code Generation"></a>VeriGen: A Large Language Model for Verilog Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00708">http://arxiv.org/abs/2308.00708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, Siddharth Garg</li>
<li>for: 本研究探讨了大型自然语言模型（LLMs）在自动硬件设计方面的能力，通过生成高质量的Verilog代码来提高设计效率。</li>
<li>methods: 我们在这种研究中使用了预先训练的LLMs，对Verilog数据集 compile from GitHub和Verilog教科书进行了微调。我们使用了特制的测试环境，包括自定义问题集和测试架构，来评估生成的Verilog代码的函数正确性。</li>
<li>results: 我们的微调后的开源CodeGen-16B模型在一个特制的测试集上表现出优于商业化状态的GPT-3.5-turbo模型，提高了1.1%的总性能。在面对更多和更复杂的问题集时，我们发现微调后的模型与状态之前的GPT-3.5-turbo模型竞争，在某些场景下 even excelling。特别是，它在不同类型问题中生成符合语法规则的Verilog代码方面提高了41%，这 highlights the potential of smaller, in-house LLMs in hardware design automation。<details>
<summary>Abstract</summary>
In this study, we explore the capability of Large Language Models (LLMs) to automate hardware design by generating high-quality Verilog code, a common language for designing and modeling digital systems. We fine-tune pre-existing LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We evaluate the functional correctness of the generated Verilog code using a specially designed test suite, featuring a custom problem set and testing benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase. Upon testing with a more diverse and complex problem set, we find that the fine-tuned model shows competitive performance against state-of-the-art gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41% improvement in generating syntactically correct Verilog code across various problem categories compared to its pre-trained counterpart, highlighting the potential of smaller, in-house LLMs in hardware design automation.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了大型自然语言模型（LLMs）是否能自动设计硬件，通过生成高质量的Verilog代码，这是数字系统设计和模拟的通用语言。我们对已有的LLMs进行了精度调整，使用从GitHub和Verilog教材中编译的Verilog数据集。我们使用专门设计的测试 suite，包括自定义问题集和测试框架，来评估生成的Verilog代码的功能正确性。我们发现，我们的精度调整的开源CodeGen-16B模型在一个更加多样化和复杂的问题集上测试时，与商业State-of-the-art GPT-3.5-turbo模型相当，并在某些场景下表现出色。特别是，它在不同类别的问题集中生成了41%的正确Verilog代码，与其预训练版本相比，这 highlights the potential of smaller, in-house LLMs in hardware design automation。
</details></li>
</ul>
<hr>
<h2 id="Recovering-high-quality-FODs-from-a-reduced-number-of-diffusion-weighted-images-using-a-model-driven-deep-learning-architecture"><a href="#Recovering-high-quality-FODs-from-a-reduced-number-of-diffusion-weighted-images-using-a-model-driven-deep-learning-architecture" class="headerlink" title="Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture"></a>Recovering high-quality FODs from a reduced number of diffusion-weighted images using a model-driven deep learning architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15273">http://arxiv.org/abs/2307.15273</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jbartlett6/sdnet">https://github.com/jbartlett6/sdnet</a></li>
<li>paper_authors: J Bartlett, C E Davey, L A Johnston, J Duan</li>
<li>for: 提高 diffusion-weighted imaging (DWI) 的重建精度和速度</li>
<li>methods: 使用深度学习，特别是圆柱体减解网络 (SDNet)，对 DWI 信号进行重建</li>
<li>results: 与state-of-the-art FOD 超分辨网络 (FOD-Net) 相比，SDNet 具有竞争力的性能，并且可以通过调整 fixel 分类 penalty 来提高下游 fixel 基于分析的性能。<details>
<summary>Abstract</summary>
Fibre orientation distribution (FOD) reconstruction using deep learning has the potential to produce accurate FODs from a reduced number of diffusion-weighted images (DWIs), decreasing total imaging time. Diffusion acquisition invariant representations of the DWI signals are typically used as input to these methods to ensure that they can be applied flexibly to data with different b-vectors and b-values; however, this means the network cannot condition its output directly on the DWI signal. In this work, we propose a spherical deconvolution network, a model-driven deep learning FOD reconstruction architecture, that ensures intermediate and output FODs produced by the network are consistent with the input DWI signals. Furthermore, we implement a fixel classification penalty within our loss function, encouraging the network to produce FODs that can subsequently be segmented into the correct number of fixels and improve downstream fixel-based analysis. Our results show that the model-based deep learning architecture achieves competitive performance compared to a state-of-the-art FOD super-resolution network, FOD-Net. Moreover, we show that the fixel classification penalty can be tuned to offer improved performance with respect to metrics that rely on accurately segmented of FODs. Our code is publicly available at https://github.com/Jbartlett6/SDNet .
</details>
<details>
<summary>摘要</summary>
Diffusion-weighted imaging (DWI)的扩展重构（FOD）使用深度学习可以生成高精度的FOD，从而减少总的扫描时间。通常使用Diffusion acquisition invariant representations（DAI）来作为输入，以确保这些方法可以适应不同的b- вектор和b-值数据。然而，这意味着网络无法直接基于DWI信号来condition its output。在这种工作中，我们提出了一种圆拟网络（SDNet），一种基于模型的深度学习FOD重构架构，以确保输入DWI信号和网络输出FOD之间的一致性。此外，我们在损失函数中添加了粒子分类 penalty，以促进网络生成的FOD可以 subsequentialmente 被正确地分割为粒子，提高下游粒子基于分析的性能。我们的结果表明，模型基于的深度学习架构可以与状态 Ell的FOD超分辨率网络（FOD-Net）相比，并且我们表明，粒子分类 penalty可以调整以提高基于粒子分类的性能。我们的代码公开在 GitHub上，请参考https://github.com/Jbartlett6/SDNet。
</details></li>
</ul>
<hr>
<h2 id="An-Overview-Of-Temporal-Commonsense-Reasoning-and-Acquisition"><a href="#An-Overview-Of-Temporal-Commonsense-Reasoning-and-Acquisition" class="headerlink" title="An Overview Of Temporal Commonsense Reasoning and Acquisition"></a>An Overview Of Temporal Commonsense Reasoning and Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00002">http://arxiv.org/abs/2308.00002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Wenzel, Adam Jatowt</li>
<li>for: 本研究旨在提高语言模型对时间常识的理解和应用，以便更好地解决时间自然语言处理任务。</li>
<li>methods: 研究使用各种增强技术和评估策略来提高语言模型的时间常识理解能力，并对不同数据集进行评估。</li>
<li>results: 尽管使用增强技术可以提高语言模型的性能，但这些模型仍然无法接近人类水平在时间常识领域的理解和解决能力。<details>
<summary>Abstract</summary>
Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning tasks over temporal common sense properties, such as the typical occurrence times, orderings, or durations of events. We further emphasize the need for careful interpretation of research to guard against overpromising evaluation results in light of the shallow reasoning present in transformers. This can be achieved by appropriately preparing datasets and suitable evaluation metrics.
</details>
<details>
<summary>摘要</summary>
时间共识理解指的是理解phrases、actions和events的typical temporal context，并使用这种知识来解决问题。这种 trait 是 temporal natural language processing 任务的重要组成部分，可能的应用包括时间线概要、时间问答和时间自然语言推理。现代研究表明，虽然大语言模型能够生成正确的语法结构和解决分类任务，但它们经常采取缩短的思维方式，容易受到simple linguistic trap的影响。这篇文章提供了 temporal commonsense reasoning 领域的研究概述，特别是通过多种扩充和其评估在不断增长的数据集上。然而，这些扩充模型仍然无法接近人类在时间常识性属性上的理解，例如事件的典型发生时间、顺序或持续时间。我们还强调需要仔细 интерпретирова research 结果，以避免因 transformers 的浅层思维而过分评估。这可以通过适当准备数据集和合适的评估指标来实现。
</details></li>
</ul>
<hr>
<h2 id="Is-this-model-reliable-for-everyone-Testing-for-strong-calibration"><a href="#Is-this-model-reliable-for-everyone-Testing-for-strong-calibration" class="headerlink" title="Is this model reliable for everyone? Testing for strong calibration"></a>Is this model reliable for everyone? Testing for strong calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15247">http://arxiv.org/abs/2307.15247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jjfeng/testing_strong_calibration">https://github.com/jjfeng/testing_strong_calibration</a></li>
<li>paper_authors: Jean Feng, Alexej Gossmann, Romain Pirracchio, Nicholas Petrick, Gene Pennello, Berkman Sahiner</li>
<li>for: 这个论文的目的是检验一个风险预测模型的准确性，尤其是在面临各种人口群体时。</li>
<li>methods: 该论文使用了一种基于分割点检测的新测试方法，具体来说是使用一部分数据训练一系列候选模型，然后使用另一部分数据进行分数基本和累加总和测试。</li>
<li>results: 该论文的结果表明， compared to现有方法，该新测试方法在模拟研究中具有更高的检测力和在风险预测模型的准确性检测中更 than doubled the power。<details>
<summary>Abstract</summary>
In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed residuals along this sequence if a poorly calibrated subgroup exists. This lets us reframe the problem of calibration testing into one of changepoint detection, for which powerful methods already exist. We begin with introducing a sample-splitting procedure where a portion of the data is used to train a suite of candidate models for predicting the residual, and the remaining data are used to perform a score-based cumulative sum (CUSUM) test. To further improve power, we then extend this adaptive CUSUM test to incorporate cross-validation, while maintaining Type I error control under minimal assumptions. Compared to existing methods, the proposed procedure consistently achieved higher power in simulation studies and more than doubled the power when auditing a mortality risk prediction model.
</details>
<details>
<summary>摘要</summary>
在一个良好准备的风险预测模型中，平均预测概率与实际事件率之间的差距相对较小，这些模型在多元人口中也是可靠的，并满足了强的算法公平性。然而，对模型的准备进行强制适应测试是一项具有挑战性的任务，尤其是 для机器学习（ML）算法，因为数据中的可能子组的数量太多。因此，常见的做法是只对一些预先定义的子组进行准备测试。然而，现有的goodness-of-fit测试技术并不适用于弱信号的情况或者小 subgroup，因为它们可能过分分解数据或者完全无法分解数据。我们提出了一种新的测试过程，基于以下想法：如果我们可以重新排序观测值，那么在这个序列中，如果存在一个不准确预测的 subgroup，那么预测和实际值之间的关系应该发生变化。这让我们可以将准备测试转化为变点检测问题，这个问题已经有强大的解决方法。我们开始于使用一种分 splitting 技术，其中一部分数据用于训练一系列候选模型，用于预测差异，剩下的数据用于performing 一种score-based cumulative sum（CUSUM）测试。为了进一步提高力量，我们然后延展这种适应 CUSUM 测试，并在保持类型 I 错误控制的情况下，进行cross-validation。与现有方法相比，我们的方法在模拟研究中一直保持高的力量，并在预测人口风险模型时，力量超过了两倍。
</details></li>
</ul>
<hr>
<h2 id="A-Practical-Recipe-for-Federated-Learning-Under-Statistical-Heterogeneity-Experimental-Design"><a href="#A-Practical-Recipe-for-Federated-Learning-Under-Statistical-Heterogeneity-Experimental-Design" class="headerlink" title="A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design"></a>A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15245">http://arxiv.org/abs/2307.15245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mmorafah/fedzoo-bench">https://github.com/mmorafah/fedzoo-bench</a></li>
<li>paper_authors: Mahdi Morafah, Weijia Wang, Bill Lin</li>
<li>for: 本研究旨在探讨 Federated Learning (FL) 在数据不一致情况下的应用，并提供一个系统性的研究方法来了解 FL 的实验设置对性能的影响。</li>
<li>methods: 本研究使用了多种 FL 算法和实验设置，包括22种state-of-the-art方法的实现，以及一系列标准化和可定制的特性。</li>
<li>results: 研究发现，FL 实验设置对性能的影响是复杂的，并且存在多种因素的交互效应。研究还提供了一些启示和建议，以帮助设计一个有意义和奖励性的 FL 实验设置。<details>
<summary>Abstract</summary>
Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench, an open-source library based on PyTorch with pre-implementation of 22 state-of-the-art methods, and a broad set of standardized and customizable features available at https://github.com/MMorafah/FedZoo-Bench. We also provide a comprehensive comparison of several state-of-the-art (SOTA) methods to better understand the current state of the field and existing limitations.
</details>
<details>
<summary>摘要</summary>
federaired learning（FL）已经是近年来的一个热点研究领域。有很多研究在FL中使用不同的实验设置，但是即使有很多论文，现场的进步状况还是不清楚。许多研究使用不一致的实验设置，而且没有全面的研究对FL特有的实验变量对结果和实用经验的影响。此外，存在多个标准和干扰变量，这使得问题的不一致和混乱更加严重。在这项工作中，我们提供了FL特有的实验变量的首次全面研究，探讨它们之间的关系和性能结果的影响。我们还为社区提供了FedZoo-Bench，一个基于PyTorch的开源库，包含22种当前领先的方法的预实现，以及一系列标准化和可定制的特性。此外，我们还对多种当前领先方法进行了全面的比较，以更好地了解当前领域的状况和存在的局限性。
</details></li>
</ul>
<hr>
<h2 id="Sustainable-Transparency-in-Recommender-Systems-Bayesian-Ranking-of-Images-for-Explainability"><a href="#Sustainable-Transparency-in-Recommender-Systems-Bayesian-Ranking-of-Images-for-Explainability" class="headerlink" title="Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability"></a>Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01196">http://arxiv.org/abs/2308.01196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Paz-Ruza, Amparo Alonso-Betanzos, Berta Guijarro-Berdiñas, Brais Cancela, Carlos Eiras-Franco</li>
<li>for: 这个论文的目的是提高个性化解释的效果，以增强推荐系统的透明度和用户信任。</li>
<li>methods: 该论文使用了视觉内容生成的用户创建的图像，并采用了搜索引擎的排名算法，以实现更高效的个性化解释。</li>
<li>results: 相比之前的模型，该模型在六个真实世界数据集上表现出了明显的优异，并且具有了更好的效率和更小的模型大小，可以降低训练和推荐过程中的碳排放。<details>
<summary>Abstract</summary>
Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effective personalized explanations for a given recommendation, leading to a suboptimal learning process and larger model sizes. To address these limitations, we present BRIE, a novel model designed to tackle the existing challenges by adopting a more adequate learning goal based on Bayesian Pairwise Ranking, enabling it to achieve consistently superior performance than state-of-the-art models in six real-world datasets, while exhibiting remarkable efficiency, emitting up to 75% less CO${_2}$ during training and inference with a model up to 64 times smaller than previous approaches.
</details>
<details>
<summary>摘要</summary>
为解决这些限制，我们提出了 BRIE，一种新的模型，采用基于 bayesian pairwise ranking的更适合的学习目标，使得它能够在六个实际数据集上表现出 consistently superior performance，而且具有很好的效率，在训练和推理过程中排放 CO${_2}$ 的含量可以下降至 75%，模型也可以比前一些方法小得多，达到 64 倍。
</details></li>
</ul>
<hr>
<h2 id="Open-Problems-and-Fundamental-Limitations-of-Reinforcement-Learning-from-Human-Feedback"><a href="#Open-Problems-and-Fundamental-Limitations-of-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"></a>Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15217">http://arxiv.org/abs/2307.15217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell</li>
<li>for: 本研究旨在系мати化人类反馈强化学习（RLHF）的问题和限制，以及相关方法的改进和补充。</li>
<li>methods: 本研究使用了RLHF和相关方法来训练大型自然语言模型（LLM）。</li>
<li>results: 本研究提出了对RLHF系统的审核和公布标准，以提高社会监督RLHF系统的能力。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
</details>
<details>
<summary>摘要</summary>
人工智能学习增强法（RLHF）是一种训练人工智能系统与人类目标相一致的技术。RLHF已经成为现代大语言模型（LLM）的训练中心方法。尽管如此，RLHF的问题和限制尚未得到了相对较少的公共研究。在这篇论文中，我们（1）抽查RLHF和相关方法的开放问题和基本限制;（2）概述RLHF在实践中的理解、改进和补充方法;（3）提议RLHF系统的审核和披露标准，以提高社会对RLHF系统的监督。我们的工作强调RLHF的限制，并高亮了在开发更安全的人工智能系统方面需要多种方法的合作。
</details></li>
</ul>
<hr>
<h2 id="PromptStyler-Prompt-driven-Style-Generation-for-Source-free-Domain-Generalization"><a href="#PromptStyler-Prompt-driven-Style-Generation-for-Source-free-Domain-Generalization" class="headerlink" title="PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization"></a>PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15199">http://arxiv.org/abs/2307.15199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhyeong Cho, Gilhyun Nam, Sungyeon Kim, Hunmin Yang, Suha Kwak</li>
<li>for: 本研究旨在提出一种基于文本描述的图像预测方法，能够在无需使用图像的情况下进行域广泛化。</li>
<li>methods: 该方法使用文本特征（如“狗照片”）来代表相关的图像特征，并利用cross-modal转移现象来学习多种样式。</li>
<li>results: 该方法可以在PACS、VLCS、OfficeHome和DomainNet等 dataset上达到状态之册的表现，而无需使用任何图像进行训练。<details>
<summary>Abstract</summary>
In a joint vision-language space, a text feature (e.g., from "a photo of a dog") could effectively represent its relevant image features (e.g., from dog photos). Also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. From these observations, we propose PromptStyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. The proposed method learns to generate a variety of style features (from "a S* style of a") via learnable style word vectors for pseudo-words S*. To ensure that learned styles do not distort content information, we force style-content features (from "a S* style of a [class]") to be located nearby their corresponding content features (from "[class]") in the joint vision-language space. After learning style word vectors, we train a linear classifier using synthesized style-content features. PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, even though it does not require any images for training.
</details>
<details>
<summary>摘要</summary>
在共同视觉语言空间中，文本特征（如从“一张狗照片”）可以有效表示相关的图像特征（如狗照片中的特征）。此外，一项研究还证明了这个共同空间的跨Modal传送现象。从这些观察结果，我们提出了PromptStyler，它在这个共同空间中模拟了多种分布转移，使用提示而不需要使用任何图像来处理源无法适应化。我们的方法学习生成多种风格特征（如“a S* 风格的”）的learnable风格词Vector，以便在合理的领域内生成多种风格特征。为保证学习的风格特征不会扭曲内容信息，我们强制在共同视觉语言空间中的风格-内容特征（如“a S* 风格的 [类]”）与其相应的内容特征（如“[类]”）之间的距离尽量小。之后，我们使用生成的风格-内容特征进行线性分类训练，PromptStyler在PACS、VLCS、OfficeHome和DomainNet上达到了最佳状态，即使不需要任何图像进行训练。
</details></li>
</ul>
<hr>
<h2 id="Identifying-acute-illness-phenotypes-via-deep-temporal-interpolation-and-clustering-network-on-physiologic-signatures"><a href="#Identifying-acute-illness-phenotypes-via-deep-temporal-interpolation-and-clustering-network-on-physiologic-signatures" class="headerlink" title="Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures"></a>Identifying acute illness phenotypes via deep temporal interpolation and clustering network on physiologic signatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15719">http://arxiv.org/abs/2307.15719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanfang Ren, Yanjun Li, Tyler J. Loftus, Jeremy Balch, Kenneth L. Abbott, Shounak Datta, Matthew M. Ruppert, Ziyuan Guan, Benjamin Shickel, Parisa Rashidi, Tezcan Ozrazgat-Baslanti, Azra Bihorac<br>for:This paper aims to identify distinct patient phenotypes based on vital sign data within the first six hours of hospital admission, with the goal of supporting early clinical decisions and improving patient outcomes.methods:The authors used a deep temporal interpolation and clustering network to analyze vital sign data from a single-center, longitudinal EHR dataset of 75,762 adults admitted to a tertiary care center for at least six hours. They derived four distinct patient phenotypes based on patterns of vital sign data.results:The authors found that the four patient phenotypes had distinct categories of disease and outcomes. Phenotype A had the most comorbid diseases and a higher rate of prolonged respiratory insufficiency, acute kidney injury, sepsis, and three-year mortality. Phenotypes B and C had mild organ dysfunction, while Phenotype D had early and persistent hypotension and a high rate of early surgery. The clustering results did not simply repeat other acuity assessments, and the tool may impact triage decisions and clinical decision-support under time constraints.<details>
<summary>Abstract</summary>
Initial hours of hospital admission impact clinical trajectory, but early clinical decisions often suffer due to data paucity. With clustering analysis for vital signs within six hours of admission, patient phenotypes with distinct pathophysiological signatures and outcomes may support early clinical decisions. We created a single-center, longitudinal EHR dataset for 75,762 adults admitted to a tertiary care center for 6+ hours. We proposed a deep temporal interpolation and clustering network to extract latent representations from sparse, irregularly sampled vital sign data and derived distinct patient phenotypes in a training cohort (n=41,502). Model and hyper-parameters were chosen based on a validation cohort (n=17,415). Test cohort (n=16,845) was used to analyze reproducibility and correlation with biomarkers. The training, validation, and testing cohorts had similar distributions of age (54-55 yrs), sex (55% female), race, comorbidities, and illness severity. Four clusters were identified. Phenotype A (18%) had most comorbid disease with higher rate of prolonged respiratory insufficiency, acute kidney injury, sepsis, and three-year mortality. Phenotypes B (33%) and C (31%) had diffuse patterns of mild organ dysfunction. Phenotype B had favorable short-term outcomes but second-highest three-year mortality. Phenotype C had favorable clinical outcomes. Phenotype D (17%) had early/persistent hypotension, high rate of early surgery, and substantial biomarker rate of inflammation but second-lowest three-year mortality. After comparing phenotypes' SOFA scores, clustering results did not simply repeat other acuity assessments. In a heterogeneous cohort, four phenotypes with distinct categories of disease and outcomes were identified by a deep temporal interpolation and clustering network. This tool may impact triage decisions and clinical decision-support under time constraints.
</details>
<details>
<summary>摘要</summary>
<<SYS>> hospital 入院初期影响临床轨迹，但早期临床决策 often 受到数据缺乏的困扰。通过在入院第六个小时内对生命 Parameters进行集成分析，患者可能会被分为不同的疾病类型和结果。我们在一所三级医疗机构中收录了75,762名成人的长期电子医疗纪录（EHR）数据，并提出了一种深度时间 interpolate 和 clustering 网络，以EXTRACT 缺乏的生命 Parameters 数据中的潜在表示。我们在训练集（n=41,502）中提出了四种不同的患者类型，其中每种类型都有明确的疾病特征和结果。我们根据验证集（n=17,415）中的模型和 гиперпараметры进行选择。测试集（n=16,845）用于确认可重复性和与生物标志物相关性。训练、验证和测试集中年龄（54-55岁）、性别（55%为女性）、种族、合并疾病和疾病严重程度具有类似分布。四种类型中的第一个（18%）有最多的相关疾病和高得 respiratory insufficiency、肾脏损害、 septic shock 和三年 mortality。第二个类型（33%）和第三个类型（31%）有散发性轻度器官功能不全的特征。第二个类型（33%）在短期内有 favorable 的临床结果，但在三年内 mortality 第二高。第三个类型（31%）有 favorable 的临床结果。第四个类型（17%）有早期/持续低血压、高 rate of early surgery 和严重的Inflammation 水平，但在三年内 mortality 第二低。在一个多样化的人群中，这种工具可能会影响抢救决策和临床决策支持系统。
</details></li>
</ul>
<hr>
<h2 id="The-Marginal-Value-of-Momentum-for-Small-Learning-Rate-SGD"><a href="#The-Marginal-Value-of-Momentum-for-Small-Learning-Rate-SGD" class="headerlink" title="The Marginal Value of Momentum for Small Learning Rate SGD"></a>The Marginal Value of Momentum for Small Learning Rate SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15196">http://arxiv.org/abs/2307.15196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, Zhiyuan Li</li>
<li>for: 该文章目的是为了解释权重动量在随机优化中的作用。</li>
<li>methods: 该文章使用了权重动量的方法来优化神经网络。</li>
<li>results: 实验表明，权重动量在实际训练 régime中没有明显的提升作用，尤其是在小学习率下。<details>
<summary>Abstract</summary>
Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.
</details>
<details>
<summary>摘要</summary>
偏好是知道可以加速凸降谷的梯度搜索在强度 convex 的设置中，不包括随机梯度噪声。在随机优化中，如训练神经网络，民间传说表示偏好可以减少随机梯度更新的幅度，但以前的理论分析不能找到偏好提供任何可证明的加速。本文的理论结果解释了偏好在随机设置中的作用，表明在小学习率下，偏好不会提供任何可证明的加速。实验表明，偏好对优化和泛化在实际训练 режи具有有限的 benefita，包括从scratch开始训练 ImageNet 和 fine-tuning 语言模型在下游任务上。
</details></li>
</ul>
<hr>
<h2 id="Learning-in-Repeated-Multi-Unit-Pay-As-Bid-Auctions"><a href="#Learning-in-Repeated-Multi-Unit-Pay-As-Bid-Auctions" class="headerlink" title="Learning in Repeated Multi-Unit Pay-As-Bid Auctions"></a>Learning in Repeated Multi-Unit Pay-As-Bid Auctions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15193">http://arxiv.org/abs/2307.15193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rigel Galgana, Negin Golrezaei</li>
<li>For: The paper is written to address the problem of learning how to bid in repeated multi-unit pay-as-bid auctions, with the goal of achieving a high revenue for the seller while also maximizing the welfare of the bidders.* Methods: The paper uses dynamic programming (DP) to obtain the optimal solution to the offline problem, and leverages the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings.* Results: The paper achieves an upper bound on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$ respectively, and demonstrates through numerical results that the resulting market dynamics mainly converge to a welfare maximizing equilibrium where bidders submit uniform bids. Additionally, the paper shows that the pay-as-bid auction consistently generates significantly higher revenue compared to its popular alternative, the uniform price auction.Here is the answer in Simplified Chinese text:* For: 本文是为了解决 repeat 的 multi-unit pay-as-bid 拍卖中的投标问题，以实现卖家获得高收益，同时 maximize 投标者的利益。* Methods: 本文使用动态规划（DP）获取 offline 问题的优化解决方案，并利用 DP 的结构设计在全信息和抽象反馈下的在线学习算法，其时间复杂度和空间复杂度均为多阶 polynomial。* Results: 本文达到了 regret 的Upper bound为 $O(M\sqrt{T\log |\mathcal{B}|})$ 和 $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$，并通过数学结果表明，market dynamics 在 converge 到一个利益最大化的均衡点，在这个点上，投标者都会提交 uniform 投标。此外，文章还证明了 pay-as-bid 拍卖在 popular 的 uniform price 拍卖上consistently 获得更高的收益。<details>
<summary>Abstract</summary>
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings. We achieve an upper bound on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$ respectively, where $M$ is the number of units demanded by the bidder, $T$ is the total number of auctions, and $|\mathcal{B}|$ is the size of the discretized bid space. We accompany these results with a regret lower bound, which match the linear dependency in $M$. Our numerical results suggest that when all agents behave according to our proposed no regret learning algorithms, the resulting market dynamics mainly converge to a welfare maximizing equilibrium where bidders submit uniform bids. Lastly, our experiments demonstrate that the pay-as-bid auction consistently generates significantly higher revenue compared to its popular alternative, the uniform price auction.
</details>
<details>
<summary>摘要</summary>
驱动 by 碳排放交易制度、储蓄拍卖和购买拍卖，这些拍卖都涉及到多个单位的拍卖，我们考虑了如何在重复的多个单位支付-为-拍卖中学习投标。在每个拍卖中，大量（相同）的物品需要分配给最大提交投标价格为每个赢得拍卖价格。在支付-为-拍卖问题上学习投标是具有 combinatorial 特性的挑战。我们通过关注线上设置来解决这个挑战，bidder 在仅有过去其他投标者的投标历史的情况下优化其投标向量。我们表明了在线时间DP（动态编程）算法可以在 polynomial 时间内解决这个问题。我们利用DP算法的结构来设计在线学习算法，其时间复杂度和存储空间复杂度均为polynomial。在全信息和抽象反馈设置下，我们实现了对 regret 的上限为 $O(M\sqrt{T\log |\mathcal{B}|})$ 和 $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$，其中 $M$ 是投标者需要的单位数量， $T$ 是总共 auctions 数量，并且 $|\mathcal{B}|$ 是投标向量空间的大小。我们的数学结果表明，当所有代理人按照我们所提议的无回归学习算法行为时，市场动力主要 converge 到一个减少最大化的均衡，其中投标者会提交均匀投标。最后，我们的实验表明，支付-为-拍卖拍卖在许多情况下可以生成较高的收益，相比于它的流行替代方案 uniform 价格拍卖。
</details></li>
</ul>
<hr>
<h2 id="f-Divergence-Minimization-for-Sequence-Level-Knowledge-Distillation"><a href="#f-Divergence-Minimization-for-Sequence-Level-Knowledge-Distillation" class="headerlink" title="f-Divergence Minimization for Sequence-Level Knowledge Distillation"></a>f-Divergence Minimization for Sequence-Level Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15190">http://arxiv.org/abs/2307.15190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manga-uofa/fdistill">https://github.com/manga-uofa/fdistill</a></li>
<li>paper_authors: Yuqiao Wen, Zichao Li, Wenyu Du, Lili Mou</li>
<li>for: 本文提出了一种基于 generalized f-divergence 函数的序列级知识填充框架 (f-DISTILL)，用于实现语言处理领域中的知识填充。</li>
<li>methods: 本文提出了四种基于 f-DISTILL 框架的知识填充方法，并证明了现有的 SeqKD 和 ENGINE 方法是 f-DISTILL 方法的近似。此外，本文还提出了一种步骤 decomposition 方法，将不可算式的序列级分割转换为可算式的单词级损失函数。</li>
<li>results: 实验结果表明，本文提出的方法比现有的知识填充方法高效，并且使用 симметричного 填充损失函数可以更好地让学生模型学习教师分布。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an f-DISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our f-DISTILL methods. We further derive step-wise decomposition for our f-DISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution.
</details>
<details>
<summary>摘要</summary>
知识填充（KD）是将知识从大型模型传递到小型模型的过程。随着自然语言处理领域中模型的增长，KD技术在受到压力，以压缩语言模型的规模。在这项工作中，我们提出了f-DISTILL框架，将序列级知识填充形式化为最小化一个通用f-散度函数。我们提出了四种填充变种，并证明了现有的SeqKD和ENGINE方法是我们f-DISTILL方法的近似方法。我们还 deriv了step-wise decomposition，将不可贪算的序列级散度 decomposed into可计算的单词级损失。实验结果表明，我们的方法在四个数据集上表现比现有的KD方法更好，并且我们的对称填充损失可以更好地让学生学习教师分布。
</details></li>
</ul>
<hr>
<h2 id="Rotation-Invariant-Random-Features-Provide-a-Strong-Baseline-for-Machine-Learning-on-3D-Point-Clouds"><a href="#Rotation-Invariant-Random-Features-Provide-a-Strong-Baseline-for-Machine-Learning-on-3D-Point-Clouds" class="headerlink" title="Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds"></a>Rotation-Invariant Random Features Provide a Strong Baseline for Machine Learning on 3D Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06271">http://arxiv.org/abs/2308.06271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meliao/rotation-invariant-random-features">https://github.com/meliao/rotation-invariant-random-features</a></li>
<li>paper_authors: Owen Melia, Eric Jonas, Rebecca Willett</li>
<li>for:  This paper is written for researchers and practitioners in the field of machine learning, particularly those interested in rotation-invariant methods for 3D point cloud data.</li>
<li>methods:  The paper proposes a simple and general-purpose method for learning rotation-invariant functions of 3D point cloud data using a random features approach. The method is based on the random features method of Rahimi &amp; Recht (2007) and is extended to be invariant to three-dimensional rotations.</li>
<li>results:  The paper shows through experiments that the proposed method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. Additionally, the method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task, and has an order of magnitude smaller prediction latency than competing kernel methods.Here is the result in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究机器学习领域的研究者和实践者而写的，特别是关心 rotate-invariant 方法的人。</li>
<li>methods: 该篇论文提出了一种简单而通用的方法，用于学习三维点云数据上的 rotate-invariant 函数，使用随机特征方法。</li>
<li>results: 论文通过实验表明，提议的方法与通用的 rotate-invariant 神经网络相比，在标准分子性质预测标准 datasets QM7 和 QM9 上具有相同或更高的性能，并且在 ModelNet40 形态分类任务上提供了一个 rotate-invariant 基准。此外，该方法的预测延迟只有一个数量级的比较小于竞争性kernel方法。<details>
<summary>Abstract</summary>
Rotational invariance is a popular inductive bias used by many fields in machine learning, such as computer vision and machine learning for quantum chemistry. Rotation-invariant machine learning methods set the state of the art for many tasks, including molecular property prediction and 3D shape classification. These methods generally either rely on task-specific rotation-invariant features, or they use general-purpose deep neural networks which are complicated to design and train. However, it is unclear whether the success of these methods is primarily due to the rotation invariance or the deep neural networks. To address this question, we suggest a simple and general-purpose method for learning rotation-invariant functions of three-dimensional point cloud data using a random features approach. Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. We show through experiments that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.
</details>
<details>
<summary>摘要</summary>
“旋转协变是机器学习领域中广泛使用的抽象假设，如计算机视觉和量子化学机器学习。旋转不变的机器学习方法在许多任务上设置了状态的典范，包括分子性质预测和3D形状分类。这些方法通常是通过任务特定的旋转不变特征或通用的深度神经网络来实现的，后者具有复杂的设计和训练问题。然而，是否 rotation协变的成功主要归功于旋转不变性还是深度神经网络仍然存在一定的问题。为了解决这个问题，我们提出了一种简单和通用的方法来学习三维点云数据上的旋转不变函数，使用随机特征方法。 Specifically, we extend the random features method of Rahimi & Recht 2007 by deriving a version that is invariant to three-dimensional rotations and showing that it is fast to evaluate on point cloud data. Through experiments, we show that our method matches or outperforms the performance of general-purpose rotation-invariant neural networks on standard molecular property prediction benchmark datasets QM7 and QM9. We also show that our method is general-purpose and provides a rotation-invariant baseline on the ModelNet40 shape classification task. Finally, we show that our method has an order of magnitude smaller prediction latency than competing kernel methods.”Note: Please keep in mind that the translation is in Simplified Chinese, and the grammar and vocabulary may be different from Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="RCT-Rejection-Sampling-for-Causal-Estimation-Evaluation"><a href="#RCT-Rejection-Sampling-for-Causal-Estimation-Evaluation" class="headerlink" title="RCT Rejection Sampling for Causal Estimation Evaluation"></a>RCT Rejection Sampling for Causal Estimation Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15176">http://arxiv.org/abs/2307.15176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kakeith/rct_rejection_sampling">https://github.com/kakeith/rct_rejection_sampling</a></li>
<li>paper_authors: Katherine A. Keith, Sergey Feldman, David Jurgens, Jonathan Bragg, Rohit Bhattacharya</li>
<li>For: The paper aims to evaluate the effectiveness of adjusting for confounding in observational data using machine learning methods for causal estimation, specifically in high-dimensional covariate settings such as text data, genomics, and the behavioral social sciences.* Methods: The paper proposes a new sampling algorithm called RCT rejection sampling, which uses subsampling randomized controlled trials (RCTs) to create confounded observational datasets and compares the causal effects from the RCTs to those from the observational data. The algorithm is designed to provide theoretical guarantees of causal identification and low bias in the estimation of causal effects.* Results: The paper shows that the proposed algorithm results in low bias when evaluated on synthetic data, and provides finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. Additionally, the paper presents a novel, real-world RCT consisting of approximately 70k observations and text data as high-dimensional covariates, which serves as a proof of concept for the proposed method.<details>
<summary>Abstract</summary>
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -- researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm indeed results in low bias when oracle estimators are evaluated on the confounded samples, which is not always the case for a previously proposed algorithm. In addition to this identification result, we highlight several finite data considerations for evaluation designers who plan to use RCT rejection sampling on their own datasets. As a proof of concept, we implement an example evaluation pipeline and walk through these finite data considerations with a novel, real-world RCT -- which we release publicly -- consisting of approximately 70k observations and text data as high-dimensional covariates. Together, these contributions build towards a broader agenda of improved empirical evaluation for causal estimation.
</details>
<details>
<summary>摘要</summary>
干扰是观察数据中 causal 效应的重要障碍。在高维 covariates 的设置下（如文本数据、 genomics 或行为社会科学），研究人员已经提出了适应机器学习方法以实现 causal 估计的方法。然而，实际评估这些调整方法的问题具有挑战性和局限性。在这种工作中，我们基于一种有前途的评估策略，即使用 randomized controlled trials (RCTs) 来生成干扰 Observational 数据，并使用 RCTs 的平均 causal 效应作为真实参照值。我们提出了一种新的抽样算法，称为 RCT 拒绝抽样，并提供了理论保证，表明在观察数据中， causal 识别存在。使用 sintetic 数据，我们表明我们的算法确实具有低偏度特性，当 oracle 估计器在干扰样本上进行评估时。此外，我们还高亮了评估设计师在使用 RCT 拒绝抽样时需要考虑的一些finite data 问题。作为证明，我们实现了一个示例评估管线，并详细介绍了这些finite data 问题。此外，我们还公开发布了一个 novel 的实际 RCT，包含约70k 个观察值和文本数据作为高维 covariates。总的来说，这些贡献共同推动了观察数据中 causal 估计的有效评估。
</details></li>
</ul>
<hr>
<h2 id="Causative-Cyberattacks-on-Online-Learning-based-Automated-Demand-Response-Systems"><a href="#Causative-Cyberattacks-on-Online-Learning-based-Automated-Demand-Response-Systems" class="headerlink" title="Causative Cyberattacks on Online Learning-based Automated Demand Response Systems"></a>Causative Cyberattacks on Online Learning-based Automated Demand Response Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15175">http://arxiv.org/abs/2307.15175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samrat Acharya, Yury Dvorkin, Ramesh Karri</li>
<li>for: 这 paper 是 investigate AI-based Demand Response (DR) 系统的安全性问题，具体来说是研究用户数据的敏感性和攻击者可能的攻击方式。</li>
<li>methods: 这 paper 使用了人工智能 (AI) 技术来学习用户的能源消耗习惯，并使用这些知识来设计最佳的 DR 激励方案。</li>
<li>results: 研究发现了 AI-based DR 系统的敏感性问题，并提出了一种基于实际数据的攻击策略，包括 manipulate real-time DR 激励、DR 事件数据传输和 DR 用户响应的攻击。<details>
<summary>Abstract</summary>
Power utilities are adopting Automated Demand Response (ADR) to replace the costly fuel-fired generators and to preempt congestion during peak electricity demand. Similarly, third-party Demand Response (DR) aggregators are leveraging controllable small-scale electrical loads to provide on-demand grid support services to the utilities. Some aggregators and utilities have started employing Artificial Intelligence (AI) to learn the energy usage patterns of electricity consumers and use this knowledge to design optimal DR incentives. Such AI frameworks use open communication channels between the utility/aggregator and the DR customers, which are vulnerable to \textit{causative} data integrity cyberattacks. This paper explores vulnerabilities of AI-based DR learning and designs a data-driven attack strategy informed by DR data collected from the New York University (NYU) campus buildings. The case study demonstrates the feasibility and effects of maliciously tampering with (i) real-time DR incentives, (ii) DR event data sent to DR customers, and (iii) responses of DR customers to the DR incentives.
</details>
<details>
<summary>摘要</summary>
电力公司正在采用自动化需求应答（ADR）来取代昂贵的燃料发电机和预防峰值电力需求压力。同时，第三方需求应答（DR）聚集者也在利用可控的小规模电力负荷来为Utilities提供即时电力支持服务。一些聚集者和Utilities已经开始使用人工智能（AI）来学习电力消耗者的能源使用模式，并使用这些知识来设计优化的DR激励计划。这些AI框架使用Utility/聚集者和DR客户之间的开放通信频道，这些通信频道容易受到 causative 数据完整性攻击。这篇论文探讨了 AI 基本 DR 学习的漏洞，并设计了基于 DR 数据的数据驱动攻击策略。 case study 显示了在 NYU 校园建筑物上发生的攻击的可行性和效果，包括（i）实时 DR 激励，（ii）DR 事件数据在 DR 客户身上发送，以及（iii）DR 客户对 DR 激励的应答。
</details></li>
</ul>
<hr>
<h2 id="PredictChain-Empowering-Collaboration-and-Data-Accessibility-for-AI-in-a-Decentralized-Blockchain-based-Marketplace"><a href="#PredictChain-Empowering-Collaboration-and-Data-Accessibility-for-AI-in-a-Decentralized-Blockchain-based-Marketplace" class="headerlink" title="PredictChain: Empowering Collaboration and Data Accessibility for AI in a Decentralized Blockchain-based Marketplace"></a>PredictChain: Empowering Collaboration and Data Accessibility for AI in a Decentralized Blockchain-based Marketplace</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15168">http://arxiv.org/abs/2307.15168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-and-blockchain/s23_predictchain">https://github.com/ai-and-blockchain/s23_predictchain</a></li>
<li>paper_authors: Matthew T. Pisano, Connor J. Patterson, Oshani Seneviratne</li>
<li>for: 这个论文旨在提供一个以区块链技术为基础的预测机器学习模型市场（PredictChain），以解决限制存储资源和训练数据的问题，并促进预测机器学习模型的共享和应用。</li>
<li>methods: 这个论文使用了区块链技术，建立了一个分布式的机器学习模型市场，让用户上传数据进行训练，或者向已训练的模型提出查询。 nodes within the blockchain network 将运行这些模型，并且提供一些典型的机器学习模型，例如成本、速度、简洁、能力和成本效益。</li>
<li>results: 这个论文的结果显示，PredictChain 可以提供一个可靠、安全、开放的机器学习模型市场，促进数据共享和模型的应用，并且可以减少依赖中央云端提供商。<details>
<summary>Abstract</summary>
Limited access to computing resources and training data poses significant challenges for individuals and groups aiming to train and utilize predictive machine learning models. Although numerous publicly available machine learning models exist, they are often unhosted, necessitating end-users to establish their computational infrastructure. Alternatively, these models may only be accessible through paid cloud-based mechanisms, which can prove costly for general public utilization. Moreover, model and data providers require a more streamlined approach to track resource usage and capitalize on subsequent usage by others, both financially and otherwise. An effective mechanism is also lacking to contribute high-quality data for improving model performance. We propose a blockchain-based marketplace called "PredictChain" for predictive machine-learning models to address these issues. This marketplace enables users to upload datasets for training predictive machine learning models, request model training on previously uploaded datasets, or submit queries to trained models. Nodes within the blockchain network, equipped with available computing resources, will operate these models, offering a range of archetype machine learning models with varying characteristics, such as cost, speed, simplicity, power, and cost-effectiveness. This decentralized approach empowers users to develop improved models accessible to the public, promotes data sharing, and reduces reliance on centralized cloud providers.
</details>
<details>
<summary>摘要</summary>
限制式计算资源和训练数据对个人和组织来说是训练和使用预测机器学习模型的主要挑战。虽然有很多公共可用的机器学习模型存在，但它们 часто是无主的，需要用户建立自己的计算基础设施。另外，这些模型可能只通过付费的云计算机制提供，这可能对普通用户来说是昂贵的。此外，模型和数据提供者需要一种更加流畅的方式来跟踪资源使用并从其他人的使用中获得经济和其他形式的回报。此外，一种有效的数据贡献机制也缺乏，以提高模型性能。我们提议一个基于区块链的市场平台，称为“预测链”（PredictChain），以解决这些问题。这个市场平台允许用户上传数据集用于训练预测机器学习模型，请求已上传数据集上的模型训练，或者提交已训练的模型来解决问题。节点 dentro el red de blockchain, equipados con recursos de cálculo disponibles, operarán estos modelos, ofreciendo una variedad de modelos de aprendizaje automático de arquetipo, como cost, velocidad, sencillez, poder y eficiencia de costos. Este enfoque descentralizado permite a los usuarios desarrollar modelos mejorados que sean accesibles al público en general, fomenta la compartición de datos y reduce la dependencia de los proveedores de nubes centralizados.
</details></li>
</ul>
<hr>
<h2 id="VISU-at-WASSA-2023-Shared-Task-Detecting-Emotions-in-Reaction-to-News-Stories-Leveraging-BERT-and-Stacked-Embeddings"><a href="#VISU-at-WASSA-2023-Shared-Task-Detecting-Emotions-in-Reaction-to-News-Stories-Leveraging-BERT-and-Stacked-Embeddings" class="headerlink" title="VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings"></a>VISU at WASSA 2023 Shared Task: Detecting Emotions in Reaction to News Stories Leveraging BERT and Stacked Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15164">http://arxiv.org/abs/2307.15164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Kumar, Sushmita Singh, Prayag Tiwari</li>
<li>for: 本研究旨在开发深度学习模型，用于从新闻文章中检测情感表达。</li>
<li>methods: 该研究使用了word embedding表示法，并采用了适应性的预处理策略，以捕捉情感表达中的细节。实验使用了静态和上下文嵌入（个体和堆叠），以及BIaLSTM和Transformer模型。</li>
<li>results: 本研究在WASSA 2023共享任务中排名第十，Macro F1-Score为0.2717，证明了实施的方法在小型和不均衡数据集上的有效性。<details>
<summary>Abstract</summary>
Our system, VISU, participated in the WASSA 2023 Shared Task (3) of Emotion Classification from essays written in reaction to news articles. Emotion detection from complex dialogues is challenging and often requires context/domain understanding. Therefore in this research, we have focused on developing deep learning (DL) models using the combination of word embedding representations with tailored prepossessing strategies to capture the nuances of emotions expressed. Our experiments used static and contextual embeddings (individual and stacked) with Bidirectional Long short-term memory (BiLSTM) and Transformer based models. We occupied rank tenth in the emotion detection task by scoring a Macro F1-Score of 0.2717, validating the efficacy of our implemented approaches for small and imbalanced datasets with mixed categories of target emotions.
</details>
<details>
<summary>摘要</summary>
我们的系统，VISU，参与了2023年WASSA分享任务（3）的情感分类从新闻文章的反应文章中。情感检测从复杂对话中是挑战性的，因此在这些研究中，我们专注于开发深度学习（DL）模型，使用Word embedding表示法和定制的预处理策略来捕捉表达出的情感细节。我们的实验使用静态和上下文嵌入（个人和堆叠），以及BiLSTM和Transformer基于模型。我们在情感检测任务中占据了第十名，得分 macro F1-Score为0.2717，证明了我们实施的方法在小样本和杂种类目的情感检测任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="R-LPIPS-An-Adversarially-Robust-Perceptual-Similarity-Metric"><a href="#R-LPIPS-An-Adversarially-Robust-Perceptual-Similarity-Metric" class="headerlink" title="R-LPIPS: An Adversarially Robust Perceptual Similarity Metric"></a>R-LPIPS: An Adversarially Robust Perceptual Similarity Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15157">http://arxiv.org/abs/2307.15157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/saraghazanfari/r-lpips">https://github.com/saraghazanfari/r-lpips</a></li>
<li>paper_authors: Sara Ghazanfari, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Alexandre Araujo</li>
<li>for: 这个论文的目的是提出一种robust learned perceptual image patch similarity（R-LPIPS）度量，以提高计算机视觉中的相似性度量的安全性。</li>
<li>methods: 这个论文使用了经过 adversarial 训练的深度特征来提出 R-LPIPS 度量，并通过了一系列实验证明 R-LPIPS 度量的超越性。</li>
<li>results: 论文通过了一系列实验证明，R-LPIPS 度量可以更好地抵抗针对图像的黑客攻击，并且在大规模应用中具有更高的安全性。<details>
<summary>Abstract</summary>
Similarity metrics have played a significant role in computer vision to capture the underlying semantics of images. In recent years, advanced similarity metrics, such as the Learned Perceptual Image Patch Similarity (LPIPS), have emerged. These metrics leverage deep features extracted from trained neural networks and have demonstrated a remarkable ability to closely align with human perception when evaluating relative image similarity. However, it is now well-known that neural networks are susceptible to adversarial examples, i.e., small perturbations invisible to humans crafted to deliberately mislead the model. Consequently, the LPIPS metric is also sensitive to such adversarial examples. This susceptibility introduces significant security concerns, especially considering the widespread adoption of LPIPS in large-scale applications. In this paper, we propose the Robust Learned Perceptual Image Patch Similarity (R-LPIPS) metric, a new metric that leverages adversarially trained deep features. Through a comprehensive set of experiments, we demonstrate the superiority of R-LPIPS compared to the classical LPIPS metric. The code is available at https://github.com/SaraGhazanfari/R-LPIPS.
</details>
<details>
<summary>摘要</summary>
Computer vision 领域内，相似度度量有着重要的作用，用于捕捉图像的含义。近年来，高级相似度度量，如学习后的图像特征相似度度量（LPIPS），得到了广泛应用。这些度量利用训练过的神经网络提取出来的深度特征，并在评估图像相似度时表现出了人类观察者的很好的一致性。然而，现在已经广泛接受的是，神经网络受到了针对性攻击（adversarial examples）的影响，即小于人类可见的干扰被辅助到模型中，以诱导模型进行误分类。这种攻击引入了重要的安全问题，尤其是在大规模应用中。在本文中，我们提出了一种robust learned perceptual image patch similarity（R-LPIPS）度量，该度量利用针对性训练的深度特征。通过一系列实验，我们证明了R-LPIPS度量比 классиical LPIPS度量更加高效。代码可以在 GitHub 上找到：https://github.com/SaraGhazanfari/R-LPIPS。
</details></li>
</ul>
<hr>
<h2 id="A-B-Testing-and-Best-arm-Identification-for-Linear-Bandits-with-Robustness-to-Non-stationarity"><a href="#A-B-Testing-and-Best-arm-Identification-for-Linear-Bandits-with-Robustness-to-Non-stationarity" class="headerlink" title="A&#x2F;B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity"></a>A&#x2F;B Testing and Best-arm Identification for Linear Bandits with Robustness to Non-stationarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15154">http://arxiv.org/abs/2307.15154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Xiong, Romain Camilleri, Maryam Fazel, Lalit Jain, Kevin Jamieson</li>
<li>for: 本研究探讨了线性弹性环境中的固定预算最佳臂标识问题（Fixed-budget Best-arm Identification，BAI）。</li>
<li>methods: 我们提出了一种新的算法$\mathsf{P1}$-$\mathsf{RAGE}$，该算法可以在非站ARY情况下具有较高的鲁棒性和快速的标识速度。</li>
<li>results: 我们证明了$\mathsf{P1}$-$\mathsf{RAGE}$算法的错误概率与G最佳设计相比具有较高的鲁棒性，并且在有利的情况下与最佳站ARY情况下的算法相比较快。<details>
<summary>Abstract</summary>
We investigate the fixed-budget best-arm identification (BAI) problem for linear bandits in a potentially non-stationary environment. Given a finite arm set $\mathcal{X}\subset\mathbb{R}^d$, a fixed budget $T$, and an unpredictable sequence of parameters $\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$, an algorithm will aim to correctly identify the best arm $x^* := \arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$ with probability as high as possible. Prior work has addressed the stationary setting where $\theta_t = \theta_1$ for all $t$ and demonstrated that the error probability decreases as $\exp(-T /\rho^*)$ for a problem-dependent constant $\rho^*$. But in many real-world $A/B/n$ multivariate testing scenarios that motivate our work, the environment is non-stationary and an algorithm expecting a stationary setting can easily fail. For robust identification, it is well-known that if arms are chosen randomly and non-adaptively from a G-optimal design over $\mathcal{X}$ at each time then the error probability decreases as $\exp(-T\Delta^2_{(1)}/d)$, where $\Delta_{(1)} = \min_{x \neq x^*} (x^* - x)^\top \frac{1}{T}\sum_{t=1}^T \theta_t$. As there exist environments where $\Delta_{(1)}^2/ d \ll 1/ \rho^*$, we are motivated to propose a novel algorithm $\mathsf{P1}$-$\mathsf{RAGE}$ that aims to obtain the best of both worlds: robustness to non-stationarity and fast rates of identification in benign settings. We characterize the error probability of $\mathsf{P1}$-$\mathsf{RAGE}$ and demonstrate empirically that the algorithm indeed never performs worse than G-optimal design but compares favorably to the best algorithms in the stationary setting.
</details>
<details>
<summary>摘要</summary>
我们研究了一个固定预算最佳臂识别（BAI）问题，在线性抽象环境中。我们有一个finite arm集$\mathcal{X}\subset\mathbb{R}^d$,一个固定预算$T$,并且有一个不可预测的数列$\left\lbrace\theta_t\right\rbrace_{t=1}^{T}$。我们的算法将尝试在可能不寻常的环境下，正确地识别最佳臂$x^* := \arg\max_{x\in\mathcal{X}}x^\top\sum_{t=1}^{T}\theta_t$，对应率越高越好。先前的工作已经处理过静止环境，在其中$\theta_t = \theta_1$ для所有$t$，并证明了错误率随着$T/\rho^*$的幂函数降低。但在实际的$A/B/n$多重试验enario中，环境通常是非站ARY，导致预期的站ARY环境下的算法可能会失败。为了实现预算的稳定性和快速的识别，我们提出了一个新的算法$\mathsf{P1}$-$\mathsf{RAGE}$，它将尝试在不可预测的环境下获得最佳的两个世界：稳定性和快速的识别。我们详细描述了$\mathsf{P1}$-$\mathsf{RAGE}$的错误率，并证明了它在实际上从不过不如G-优化设计，但与最佳的站ARY环境下的算法相比，它的表现却相当出色。
</details></li>
</ul>
<hr>
<h2 id="R-Block-Regularized-Block-of-Dropout-for-convolutional-networks"><a href="#R-Block-Regularized-Block-of-Dropout-for-convolutional-networks" class="headerlink" title="R-Block: Regularized Block of Dropout for convolutional networks"></a>R-Block: Regularized Block of Dropout for convolutional networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15150">http://arxiv.org/abs/2307.15150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liqi Wang, Qiya Hu</li>
<li>for: 这 paper 是为了提出一种基于 R-Block 的 convolutional layer REGULARIZATION 技术，以提高 convolutional neural network 的性能。</li>
<li>methods: 这 paper 使用了一种名为 R-Block 的 mutual learning 训练策略，该策略在每个样本上最小化两个生成的差异最大化子模型的输出分布之间的loss。此外，paper 还提出了两种方法来构建子模型。</li>
<li>results:  experiments 表明，R-Block 可以达到比其他已知结构化 dropout 变体更好的性能，而且 paper 的方法构建子模型也超过了其他方法。<details>
<summary>Abstract</summary>
Dropout as a regularization technique is widely used in fully connected layers while is less effective in convolutional layers. Therefore more structured forms of dropout have been proposed to regularize convolutional networks. The disadvantage of these methods is that the randomness introduced causes inconsistency between training and inference. In this paper, we apply a mutual learning training strategy for convolutional layer regularization, namely R-Block, which forces two outputs of the generated difference maximizing sub models to be consistent with each other. Concretely, R-Block minimizes the losses between the output distributions of two sub models with different drop regions for each sample in the training dataset. We design two approaches to construct such sub models. Our experiments demonstrate that R-Block achieves better performance than other existing structured dropout variants. We also demonstrate that our approaches to construct sub models outperforms others.
</details>
<details>
<summary>摘要</summary>
“Dropout”作为调整技巧广泛使用于全连接层，但在卷积层中效果较差。因此，更结构化的Dropout方法被提议来调整卷积网络。这些方法的缺点是它们引入了随机性，导致训练和推导过程中的不一致。在这篇文章中，我们运用了互动学习训练策略，即R-Block，以调整卷积层。具体来说，R-Block将两个生成的差异最大化子模型的出力分布对于每个训练数据点进行最小化。我们还设计了两种子模型构造方法。我们的实验结果显示，R-Block可以比其他已知结构化Dropout变种更好地表现。此外，我们的子模型构造方法也比其他方法更好。
</details></li>
</ul>
<hr>
<h2 id="Distilled-Feature-Fields-Enable-Few-Shot-Language-Guided-Manipulation"><a href="#Distilled-Feature-Fields-Enable-Few-Shot-Language-Guided-Manipulation" class="headerlink" title="Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation"></a>Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07931">http://arxiv.org/abs/2308.07931</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, Phillip Isola</li>
<li>for: 该论文旨在bridging 2D图像特征和3D几何学之间的差距，以便机器人 manipulate 任务中更好地理解3D几何学。</li>
<li>methods: 该论文使用了distilled feature fields来结合精度的3D几何学和丰富的语义特征，并使用了几何学和语义特征来实现几个shot学习的 grasping 和 placing 任务。</li>
<li>results: 该论文通过使用来自 CLIP 视力语言模型的特征抽取来实现自然语言上的 object 指定，并能够在不同的表达和新类别对象中进行普适化。<details>
<summary>Abstract</summary>
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
</details>
<details>
<summary>摘要</summary>
自我监督和语言监督的图像模型含有重要的世界知识，这种知识对于总结非常重要。然而，许多 робо械任务需要精准的3D几何理解，而这种理解通常在2D图像特征中缺失。这项工作尝试将2D-to-3D的知识泵浦 bridge，通过使用精炼的特征场来结合准确的3D几何和丰富的semantics。我们提出了一种几何学学习方法，可以在几个步骤内实现6DOF抓取和置放。使用从视觉语言模型CLIP提取的特征，我们提出了一种通过自然语言文本指定新的物品进行操作的方法，并证明其能够通过未看过的表达和新类别的物品进行总结。
</details></li>
</ul>
<hr>
<h2 id="On-Normalised-Discounted-Cumulative-Gain-as-an-Offline-Evaluation-Metric-for-Top-n-Recommendation"><a href="#On-Normalised-Discounted-Cumulative-Gain-as-an-Offline-Evaluation-Metric-for-Top-n-Recommendation" class="headerlink" title="On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation"></a>On (Normalised) Discounted Cumulative Gain as an Offline Evaluation Metric for Top-$n$ Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15053">http://arxiv.org/abs/2307.15053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Jeunen, Ivan Potapov, Aleksei Ustimenko</li>
<li>For: The paper is focused on evaluating the effectiveness of recommendation methods using offline evaluation metrics, specifically Normalised Discounted Cumulative Gain (nDCG), and investigating when nDCG can be considered an unbiased estimator of online reward.* Methods: The paper uses a critical analysis of nDCG and its assumptions to investigate its utility in recommendation evaluation. The authors provide a derivation of nDCG from first principles and show that normalising the metric can be inconsistent and limit its practical utility.* Results: The paper presents a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, showing that the unbiased DCG estimates strongly correlate with online reward, even when some of the metric’s inherent assumptions are violated. However, the normalised variant of nDCG does not maintain this correlation, suggesting that it may be limited in practical utility.<details>
<summary>Abstract</summary>
Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-$n$ recommendation for many years.   Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.
</details>
<details>
<summary>摘要</summary>
通常，推荐方法会被评估通过两种方式：在线实验（即“金标准”）或者离线评估过程，目的是估算在线实验的结果。在文献中，一些离线评估指标有被采纳，其中一个常见的指标是normalized Discounted Cumulative Gain（nDCG）。这个指标在许多年里被广泛采用，高于nDCG值被视为新方法的州际之最。我们对这种方法进行批判性的分析，检查在线实验的结果时，这些指标是否能够 aproximate 的准确。我们正式地表述了必须要将DCG作为无偏度估计的假设，并提供了这个指标的 derive 的过程，并 highlight 了我们与信息检索领域的传统用法不同的地方。重要的是，我们发现了normalizing 该指标后，它变得不一致，即在DCG是无偏度的情况下，对照竞争方法的排名可能会颠倒。通过一个大规模推荐平台上的在线和离线实验相关分析，我们发现了我们的无偏度DCG估计与在线奖励强相关，即使某些指标的内在假设被违反。但是，normalized DCG 的实际用途受到限制。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Approach-for-Arabic-Offline-Handwritten-Text-Recognition"><a href="#A-Transformer-based-Approach-for-Arabic-Offline-Handwritten-Text-Recognition" class="headerlink" title="A Transformer-based Approach for Arabic Offline Handwritten Text Recognition"></a>A Transformer-based Approach for Arabic Offline Handwritten Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15045">http://arxiv.org/abs/2307.15045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleh Momeni, Bagher BabaAli</li>
<li>for: 本研究旨在解决手写文本识别问题，具体是对于缺乏并发性的现有方法。</li>
<li>methods: 我们提出了两种新的架构：Transformer Transducer和标准的序列化Transformer，并对其性能进行了比较。这两种架构都使用了注意力机制，因此更容易并行化和简化。</li>
<li>results: 我们在arabic khatt dataset上进行了评估，并证明了我们的方法可以超越当前状态的方法，提高了手写文本识别的准确率。<details>
<summary>Abstract</summary>
Handwriting recognition is a challenging and critical problem in the fields of pattern recognition and machine learning, with applications spanning a wide range of domains. In this paper, we focus on the specific issue of recognizing offline Arabic handwritten text. Existing approaches typically utilize a combination of convolutional neural networks for image feature extraction and recurrent neural networks for temporal modeling, with connectionist temporal classification used for text generation. However, these methods suffer from a lack of parallelization due to the sequential nature of recurrent neural networks. Furthermore, these models cannot account for linguistic rules, necessitating the use of an external language model in the post-processing stage to boost accuracy. To overcome these issues, we introduce two alternative architectures, namely the Transformer Transducer and the standard sequence-to-sequence Transformer, and compare their performance in terms of accuracy and speed. Our approach can model language dependencies and relies only on the attention mechanism, thereby making it more parallelizable and less complex. We employ pre-trained Transformers for both image understanding and language modeling. Our evaluation on the Arabic KHATT dataset demonstrates that our proposed method outperforms the current state-of-the-art approaches for recognizing offline Arabic handwritten text.
</details>
<details>
<summary>摘要</summary>
手写Recognition是一个挑战性和重要的问题在 Pattern recognition 和机器学习 领域，它的应用领域广泛，包括文本识别、语音识别、图像识别等。在这篇论文中，我们专注于特定的问题，即Offline 的阿拉伯文手写文本识别。现有的方法通常使用 Convolutional Neural Networks  для图像特征提取和Recurrent Neural Networks  для时间模型，并使用 connectionist 类型的文本生成。然而，这些方法受到缺乏并行化的缺点，因为Recurrent Neural Networks 的Sequential Nature。此外，这些模型无法考虑语言规则，因此需要使用外部语言模型来提高准确性。为了解决这些问题，我们引入了两种 alternate 架构，namely Transformer Transducer 和标准的sequence-to-sequence Transformer，并比较了它们的性能。我们的方法可以模型语言依赖关系，只靠注意机制，因此更加并行化和简单。我们使用预训练的 Transformers  для图像理解和语言模型。我们的评估在阿拉伯文 KHATT 数据集上表明，我们的提议方法在Offline 阿拉伯文手写文本识别方面超过了当前状态的术语。
</details></li>
</ul>
<hr>
<h2 id="Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models"><a href="#Universal-and-Transferable-Adversarial-Attacks-on-Aligned-Language-Models" class="headerlink" title="Universal and Transferable Adversarial Attacks on Aligned Language Models"></a>Universal and Transferable Adversarial Attacks on Aligned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15043">http://arxiv.org/abs/2307.15043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llm-attacks/llm-attacks">https://github.com/llm-attacks/llm-attacks</a></li>
<li>paper_authors: Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson</li>
<li>for: 这项研究的目的是提出一种简单 yet effective的攻击方法，使得已经被适应的语言模型产生不良行为。</li>
<li>methods: 该方法通过适应搜索技术和梯度下降方法自动生成攻击 suffix，以提高对语言模型的攻击效果。</li>
<li>results: 研究发现，生成的攻击 suffix 能够在多种黑盒模型上 induce 不良行为，包括 ChatGPT、Bard 和 Claude 等。此外，该方法还可以在多个模型和多个查询中进行 transferred 攻击。<details>
<summary>Abstract</summary>
Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.   Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.
</details>
<details>
<summary>摘要</summary>
因为“out-of-the-box”的大型自然语言模型可以生成大量不适的内容， latest work 强调了对这些模型进行对齐，以避免不良生成。 although there has been some success in circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods.  Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at <https://github.com/llm-attacks/llm-attacks>.
</details></li>
</ul>
<hr>
<h2 id="Detecting-Morphing-Attacks-via-Continual-Incremental-Training"><a href="#Detecting-Morphing-Attacks-via-Continual-Incremental-Training" class="headerlink" title="Detecting Morphing Attacks via Continual Incremental Training"></a>Detecting Morphing Attacks via Continual Incremental Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15105">http://arxiv.org/abs/2307.15105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Pellegrini, Guido Borghi, Annalisa Franco, Davide Maltoni</li>
<li>for: 该论文旨在解决在数据传输和存储限制下，无法构建单一数据集，进行批处理训练的问题。</li>
<li>methods: 该论文使用了不同数据源和Continual Learning（CL）模式来实现incremental training。</li>
<li>results: 实验结果显示，Learning without Forgetting（LwF）方法在这种场景下表现出色，并且对Morphing Attack Detection和Object Classification任务进行了调整和优化。<details>
<summary>Abstract</summary>
Scenarios in which restrictions in data transfer and storage limit the possibility to compose a single dataset -- also exploiting different data sources -- to perform a batch-based training procedure, make the development of robust models particularly challenging. We hypothesize that the recent Continual Learning (CL) paradigm may represent an effective solution to enable incremental training, even through multiple sites. Indeed, a basic assumption of CL is that once a model has been trained, old data can no longer be used in successive training iterations and in principle can be deleted. Therefore, in this paper, we investigate the performance of different Continual Learning methods in this scenario, simulating a learning model that is updated every time a new chunk of data, even of variable size, is available. Experimental results reveal that a particular CL method, namely Learning without Forgetting (LwF), is one of the best-performing algorithms. Then, we investigate its usage and parametrization in Morphing Attack Detection and Object Classification tasks, specifically with respect to the amount of new training data that became available.
</details>
<details>
<summary>摘要</summary>
具有限制数据传输和存储的场景下， compose a single dataset 并利用不同的数据源进行批处理训练过程的发展变得特别困难。我们假设，最近的 Continual Learning（CL） paradigm 可能是一种有效的解决方案，以允许逐步训练，即使在多个站点之间。实际上， CL 的基本假设是，一旦模型已经训练过，then old data 不能再被用于后续的训练迭代，并且可以被删除。因此，在这篇论文中，我们 investigate 不同的 CL 方法的性能，模拟一个可以在新数据批量可用时更新的学习模型。实验结果表明，一种特定的 CL 方法，即 Learning without Forgetting（LwF），是最佳性能的算法之一。然后，我们 investigate LwF 在 Morphing Attack Detection 和 Object Classification 任务中的使用和参数化，特别是新训练数据的Amount。
</details></li>
</ul>
<hr>
<h2 id="Speeding-up-Fourier-Neural-Operators-via-Mixed-Precision"><a href="#Speeding-up-Fourier-Neural-Operators-via-Mixed-Precision" class="headerlink" title="Speeding up Fourier Neural Operators via Mixed Precision"></a>Speeding up Fourier Neural Operators via Mixed Precision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15034">http://arxiv.org/abs/2307.15034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuraloperator/neuraloperator">https://github.com/neuraloperator/neuraloperator</a></li>
<li>paper_authors: Colin White, Renbo Tu, Jean Kossaifi, Gennady Pekhimenko, Kamyar Azizzadenesheli, Anima Anandkumar</li>
<li>For: 学习部分� differential equation（PDE）解operator的代理映射。* Methods: 使用Fourier neural operator（FNO），并对其进行混合精度训练。* Results: 减少训练时间和内存使用（最多34%），减少训练时间和内存使用，而无需减少准确性，在 Navier-Stokes 和 Darcy 流Equation 上进行了一个研究。<details>
<summary>Abstract</summary>
The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memory usage (up to 34%), with little or no reduction in accuracy, on the Navier-Stokes and Darcy flow equations. Combined with the recently proposed tensorized FNO (Kossaifi et al., 2023), the resulting model has far better performance while also being significantly faster than the original FNO.
</details>
<details>
<summary>摘要</summary>
“傅欧恩抽象（FNO）是一种强大的技术，用于学习对应扩散方程（PDE）解析器的代理映射。实际应用中，通常需要大量的高分辨率数据点，训练时间和内存使用是重要的瓶颈。虽然现有混合精度训练技术可以应用于标准神经网络，但这些技术仅适用于実数型数据和有限维度上。对于FNO而言，由于快 Fourier  трансформа是一种近似（由于精度误差），因此我们不需要在全精度下进行运算。在这个研究中，我们（i）评估FNO的内存和运算时间profile，（ii）进行混合精度训练的numerical stability研究，以及（iii）开发一个可以很快地训练FNO的训练程式，可以对 Navier-Stokes 和 Darcy 流运动方程获得较好的性能，并且在34%的几何中实现了训练时间和内存的减少，而且几乎没有影响精度。 combinined with the recently proposed tensorized FNO（Kossaifi et al., 2023），得到的模型具有许多更好的性能，并且比原始FNO更快。”
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Graph-Transformer-for-Deepfake-Detection"><a href="#Self-Supervised-Graph-Transformer-for-Deepfake-Detection" class="headerlink" title="Self-Supervised Graph Transformer for Deepfake Detection"></a>Self-Supervised Graph Transformer for Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15019">http://arxiv.org/abs/2307.15019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aminollah Khormali, Jiann-Shiun Yuan</li>
<li>for: 本研究旨在开发一个可靠的深伪检测系统，能够在不同的数据集上保持高度的检测性能，并且能够承受常见的后期处理扰动。</li>
<li>methods: 本研究提出了一种基于视觉转换器架构的特征提取器，利用自我超VI中的自我抑制学习方法进行预训练，并将图 convolutional neural network 和转换器推荐器结合使用，以提高对抗 manipulate 性能。</li>
<li>results: 经过多种挑战性实验，包括在不同数据集上的性能测试、跨数据集检测、跨扰动检测以及对常见后期处理扰动的Robustness测试，提出的深伪检测框架在所有方面都达到了现状之上的表现。<details>
<summary>Abstract</summary>
Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
深圳检测方法已经在给定数据集上显示了有 promise的结果，可以训练和测试在同一个数据集上。然而，其性能在未看过的样本上会很差。因此，一个可靠的深圳检测系统必须保持中立于伪造类型、外观和质量上，以确保普适的检测性能。 despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Therefore, this study introduces a deepfake detection framework, which leverages a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework consists of three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-distribution performance, cross-dataset, cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.
</details></li>
</ul>
<hr>
<h2 id="Samplable-Anonymous-Aggregation-for-Private-Federated-Data-Analysis"><a href="#Samplable-Anonymous-Aggregation-for-Private-Federated-Data-Analysis" class="headerlink" title="Samplable Anonymous Aggregation for Private Federated Data Analysis"></a>Samplable Anonymous Aggregation for Private Federated Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15017">http://arxiv.org/abs/2307.15017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Talwar, Shan Wang, Audra McMillan, Vojta Jina, Vitaly Feldman, Bailey Basile, Aine Cahill, Yi Sheng Chan, Mike Chatzidakis, Junye Chen, Oliver Chick, Mona Chitnis, Suman Ganta, Yusuf Goren, Filip Granqvist, Kristine Guo, Frederic Jacobs, Omid Javidbakht, Albert Liu, Richard Low, Dan Mascenik, Steve Myers, David Park, Wonhee Park, Gianni Parsa, Tommy Pauly, Christian Priebe, Rehan Rishi, Guy Rothblum, Michael Scaria, Linmao Song, Congzheng Song, Karl Tarbe, Sebastian Vogt, Luke Winstrom, Shundong Zhou</li>
<li>for: 该 paper 的目的是设计私有统计和私有联合学习协议，以便每个设备都可以保持自己的私人数据。</li>
<li>methods: 该 paper 使用了一种简单的原理，可以实现许多常用的算法，并且可以在私有设备上进行隐私计算，不需要强信任假设。</li>
<li>results: 该 paper 提出了一个系统架构，实现了该原理，并进行了安全分析。<details>
<summary>Abstract</summary>
We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.
</details>
<details>
<summary>摘要</summary>
我团队在 revisit 私人统计和私人联合学习问题上做出了一些贡献。我们的第一个贡献是提出了一种简单的基本原理，可以有效地实现许多通常使用的算法，同时允许保持隐私计算的紧密性，无需强大的信任假设。其次，我们提出了一个实现这种基本原理的系统架构，并进行了安全分析。
</details></li>
</ul>
<hr>
<h2 id="How-Good-is-Google-Bard’s-Visual-Understanding-An-Empirical-Study-on-Open-Challenges"><a href="#How-Good-is-Google-Bard’s-Visual-Understanding-An-Empirical-Study-on-Open-Challenges" class="headerlink" title="How Good is Google Bard’s Visual Understanding? An Empirical Study on Open Challenges"></a>How Good is Google Bard’s Visual Understanding? An Empirical Study on Open Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15016">http://arxiv.org/abs/2307.15016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htqin/googlebard-visunderstand">https://github.com/htqin/googlebard-visunderstand</a></li>
<li>paper_authors: Haotong Qin, Ge-Peng Ji, Salman Khan, Deng-Ping Fan, Fahad Shahbaz Khan, Luc Van Gool</li>
<li>for: 这项研究旨在评估Google Bard在多模态生成器中对视觉数据（图像）的理解和解释能力，以探讨其在各种复杂计算机视觉问题中的表现。</li>
<li>methods: 该研究使用Google Bard处理图像和文本提问的能力，并在15个多样化任务场景中测试其性能，包括普通、掩蔽、医疗、水下和Remote Sensing等。</li>
<li>results: 研究发现，Google Bard在这些视觉任务场景中表现不佳，表明现有的视觉理解 gap 需要被覆盖，未来的发展应该更加注重视觉理解。<details>
<summary>Abstract</summary>
Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in the field of conversational AI. Notably, Bard has recently been updated to handle visual inputs alongside text prompts during conversations. Given Bard's impressive track record in handling textual inputs, we explore its capabilities in understanding and interpreting visual data (images) conditioned by text questions. This exploration holds the potential to unveil new insights and challenges for Bard and other forthcoming multi-modal Generative models, especially in addressing complex computer vision problems that demand accurate visual and language understanding. Specifically, in this study, we focus on 15 diverse task scenarios encompassing regular, camouflaged, medical, under-water and remote sensing data to comprehensively evaluate Bard's performance. Our primary finding indicates that Bard still struggles in these vision scenarios, highlighting the significant gap in vision-based understanding that needs to be bridged in future developments. We expect that this empirical study will prove valuable in advancing future models, leading to enhanced capabilities in comprehending and interpreting fine-grained visual data. Our project is released on https://github.com/htqin/GoogleBard-VisUnderstand
</details>
<details>
<summary>摘要</summary>
Google的Bard已经成为对OpenAI的ChatGPT的 conversational AI的强力竞争对手。值得注意的是，Bard最近更新了可以处理视觉输入的功能，以便与文本提问一起进行对话。由于Bard在处理文本输入的表现印象，我们探索了它在理解和解释视觉数据（图像）时的能力。这种探索拥有探索新的发现和挑战，特别是在解决复杂的计算机视觉问题时，需要精准的视觉和语言理解。在这项研究中，我们关注了15种多样化的任务场景，包括常见、潜规、医疗、水下和远程感知数据，以全面评估Bard的表现。我们的主要发现表明Bard在这些视觉场景中仍然受到挑战，这 highlights了未来发展中需要跨越的视觉理解障碍。我们预计这项实证研究将为未来的模型发展提供价值，导致更高级的视觉数据解释和理解。我们的项目在https://github.com/htqin/GoogleBard-VisUnderstand上发布。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Synthetic-Active-Particles-for-Physical-Reservoir-Computing"><a href="#Harnessing-Synthetic-Active-Particles-for-Physical-Reservoir-Computing" class="headerlink" title="Harnessing Synthetic Active Particles for Physical Reservoir Computing"></a>Harnessing Synthetic Active Particles for Physical Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15010">http://arxiv.org/abs/2307.15010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangzun Wang, Frank Cichos</li>
<li>for: 这篇论文是研究生物系统中信息处理的一种新方法，即基于活动过程网络的增强机器学习。</li>
<li>methods: 这篇论文使用了一种叫做物理储存计算的方法，其中使用了自适应的微型推进系统来实现信息处理。</li>
<li>results: 研究发现，这种方法可以减少噪声并实现预测任务，并且可以用于研究生物系统中信息处理的自组织系统。<details>
<summary>Abstract</summary>
The processing of information is an indispensable property of living systems realized by networks of active processes with enormous complexity. They have inspired many variants of modern machine learning one of them being reservoir computing, in which stimulating a network of nodes with fading memory enables computations and complex predictions. Reservoirs are implemented on computer hardware, but also on unconventional physical substrates such as mechanical oscillators, spins, or bacteria often summarized as physical reservoir computing. Here we demonstrate physical reservoir computing with a synthetic active microparticle system that self-organizes from an active and passive component into inherently noisy nonlinear dynamical units. The self-organization and dynamical response of the unit is the result of a delayed propulsion of the microswimmer to a passive target. A reservoir of such units with a self-coupling via the delayed response can perform predictive tasks despite the strong noise resulting from Brownian motion of the microswimmers. To achieve efficient noise suppression, we introduce a special architecture that uses historical reservoir states for output. Our results pave the way for the study of information processing in synthetic self-organized active particle systems.
</details>
<details>
<summary>摘要</summary>
生物系统中信息处理是不可或缺的属性，由活动过程网络实现，其复杂性很高。它们激发了现代机器学习的许多变种，其中一种是储量计算，通过刺激网络节点的淡出记忆来实现计算和复杂预测。储量可以在计算机硬件上实现，也可以在不典型的物理基础上实现，如机械振荡、磁矢量或细菌等，通常称为物理储量计算。在这个研究中，我们使用合成活动微部件系统，该系统由活动和无功能组成部件自组织而成，具有内生的噪声和非线性动力学行为。我们发现，通过延迟微部件的推进到无功能目标，可以实现预测任务，并且可以使用历史储量状态来降低噪声。我们的结果开启了人工自组织活动粒子系统中信息处理的研究之路。
</details></li>
</ul>
<hr>
<h2 id="Verifiable-Feature-Attributions-A-Bridge-between-Post-Hoc-Explainability-and-Inherent-Interpretability"><a href="#Verifiable-Feature-Attributions-A-Bridge-between-Post-Hoc-Explainability-and-Inherent-Interpretability" class="headerlink" title="Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability"></a>Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15007">http://arxiv.org/abs/2307.15007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usha Bhalla, Suraj Srinivas, Himabindu Lakkaraju</li>
<li>for: 这个论文的目的是解释模型行为，提高模型的可解释性和可验证性。</li>
<li>methods: 这个论文提出了一种名为Verifiability Tuning（VerT）的方法，它可以将黑盒模型转化成具有可验证和 faithful 的特征归因方法。</li>
<li>results: 该方法在 semi-synthetic 和实际世界数据集上进行了广泛的实验，结果表明，VerT 可以生成可验证和 faithful 的特征归因，同时保持了模型的预测性能。<details>
<summary>Abstract</summary>
With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge the gap between the aforementioned strategies by proposing Verifiability Tuning (VerT), a method that transforms black-box models into models that naturally yield faithful and verifiable feature attributions. We begin by introducing a formal theoretical framework to understand verifiability and show that attributions produced by standard models cannot be verified. We then leverage this framework to propose a method to build verifiable models and feature attributions out of fully trained black-box models. Finally, we perform extensive experiments on semi-synthetic and real-world datasets, and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain.
</details>
<details>
<summary>摘要</summary>
随着机器学习模型在各种实际应用中的普及，研究人员和实践者们都强调了模型行为的解释的需求。为此，先前的 литера图中提出了两种广泛的解释策略：后处解释方法通过强调模型预测中的关键特征来解释复杂的黑盒模型行为，但先前的研究表明，这些解释可能不准确，甚至更加担忧的是我们无法验证它们。另一方面，具有内在可解释性的模型通过显式地编码解释到模型架构中，因此其解释自然准确和可验证，但它们通常具有较强的表达力限制。在这个工作中，我们想要将上述两种策略相互连接起来，我们提出了一种名为Verifiability Tuning（VerT）的方法，可以将黑盒模型转化成可以自然地生成准确和可验证的特征归因的模型。我们首先介绍了一个正式的理论框架，以理解验证性，并证明标准模型生成的归因无法验证。然后，我们利用这个框架来提出一种建立可验证模型和特征归因的方法。最后，我们在半 sintética和实际数据集上进行了广泛的实验，并证明VerT可以生成具有以下两个特点的模型：1）归因是正确和可验证的，2）与原始黑盒模型相同。
</details></li>
</ul>
<hr>
<h2 id="Improved-Neural-Radiance-Fields-Using-Pseudo-depth-and-Fusion"><a href="#Improved-Neural-Radiance-Fields-Using-Pseudo-depth-and-Fusion" class="headerlink" title="Improved Neural Radiance Fields Using Pseudo-depth and Fusion"></a>Improved Neural Radiance Fields Using Pseudo-depth and Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03772">http://arxiv.org/abs/2308.03772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingliang Li, Qiang Zhou, Chaohui Yu, Zhengda Lu, Jun Xiao, Zhibin Wang, Fan Wang</li>
<li>for: 本研究主要针对 novel view synthesis 和 dense geometry modeling 问题，旨在提高 NeRF 模型的渲染质量和渲染精度。</li>
<li>methods: 我们提出了一种 multi-scale encoding volume 的方法，并将其与 NeRF 模型结合使用，以便更好地捕捉实际场景中的各种大小对象&#x2F;结构的几何信息。我们还提出了一种同时进行 depth prediction 和 radiance field reconstruction 的方法，以提高渲染的准确性。</li>
<li>results: 我们的方法在 novel view synthesis 和 dense geometry modeling 两个领域都显示出了优于传统方法的性能。具体来说，我们的方法可以在不需要Scene-specific optimization的情况下，实现高质量的渲染和准确的深度映射。<details>
<summary>Abstract</summary>
Since the advent of Neural Radiance Fields, novel view synthesis has received tremendous attention. The existing approach for the generalization of radiance field reconstruction primarily constructs an encoding volume from nearby source images as additional inputs. However, these approaches cannot efficiently encode the geometric information of real scenes with various scale objects/structures. In this work, we propose constructing multi-scale encoding volumes and providing multi-scale geometry information to NeRF models. To make the constructed volumes as close as possible to the surfaces of objects in the scene and the rendered depth more accurate, we propose to perform depth prediction and radiance field reconstruction simultaneously. The predicted depth map will be used to supervise the rendered depth, narrow the depth range, and guide points sampling. Finally, the geometric information contained in point volume features may be inaccurate due to occlusion, lighting, etc. To this end, we propose enhancing the point volume feature from depth-guided neighbor feature fusion. Experiments demonstrate the superior performance of our method in both novel view synthesis and dense geometry modeling without per-scene optimization.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:自NeRF出现以来，新视图合成受到了广泛的关注。现有的总体化NeRF重建方法主要从邻近源图像中构建编码量为附加输入。然而，这些方法无法有效地编码实际场景中的各种比例物体/结构的 геометрической信息。在这项工作中，我们提议构建多尺度编码量和提供多尺度几何信息给NeRF模型。为使构造的量最接近物体场景中的表面和渲染深度更准确，我们提议同时预测深度图和NeRF重建。预测的深度图将被用来监督渲染深度，缩小深度范围，并导引点抽取。最后，由点云特征含有的几何信息可能因 occlusion、照明等原因而不准确。为此，我们提议通过深度导向邻居特征融合提高点云特征。实验表明我们的方法在新视图合成和密集几何建模中具有优于其他方法的性能，无需Scene优化。
</details></li>
</ul>
<hr>
<h2 id="Detection-of-Children-Abuse-by-Voice-and-Audio-Classification-by-Short-Time-Fourier-Transform-Machine-Learning-implemented-on-Nvidia-Edge-GPU-device"><a href="#Detection-of-Children-Abuse-by-Voice-and-Audio-Classification-by-Short-Time-Fourier-Transform-Machine-Learning-implemented-on-Nvidia-Edge-GPU-device" class="headerlink" title="Detection of Children Abuse by Voice and Audio Classification by Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU device"></a>Detection of Children Abuse by Voice and Audio Classification by Short-Time Fourier Transform Machine Learning implemented on Nvidia Edge GPU device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15101">http://arxiv.org/abs/2307.15101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuqi Yan, Yingxian Chen, W. W. T. Fok</li>
<li>for: 这个实验的目的是使用机器学习方法探测儿童虐待的情况，以增加儿童的安全性。</li>
<li>methods: 这个实验使用机器学习来分类和识别儿童的声音，并预测声音是否为哭泣、尖叫或笑声。如果发现儿童哭泣或尖叫，则会发送警告至相关人员，以便在监控盲区中识别和对待儿童的情况。</li>
<li>results: 这个实验使用声音和影像分类来增加儿童虐待探测的精度，并获得了约92%的准确率。<details>
<summary>Abstract</summary>
The safety of children in children home has become an increasing social concern, and the purpose of this experiment is to use machine learning applied to detect the scenarios of child abuse to increase the safety of children. This experiment uses machine learning to classify and recognize a child's voice and predict whether the current sound made by the child is crying, screaming or laughing. If a child is found to be crying or screaming, an alert is immediately sent to the relevant personnel so that they can perceive what the child may be experiencing in a surveillance blind spot and respond in a timely manner. Together with a hybrid use of video image classification, the accuracy of child abuse detection can be significantly increased. This greatly reduces the likelihood that a child will receive violent abuse in the nursery and allows personnel to stop an imminent or incipient child abuse incident in time. The datasets collected from this experiment is entirely from sounds recorded on site at the children home, including crying, laughing, screaming sound and background noises. These sound files are transformed into spectrograms using Short-Time Fourier Transform, and then these image data are imported into a CNN neural network for classification, and the final trained model can achieve an accuracy of about 92% for sound detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT儿童安全在儿童HOME已成为社会关注的问题，该实验的目的是使用机器学习方法检测儿童虐待情况，以提高儿童的安全性。这个实验使用机器学习来分类和识别儿童的声音，预测当前儿童发出的声音是否是哭泣、喊叫或 laughter。如果发现儿童哭泣或喊叫，则立即发送警报给相关人员，以便他们在监控隐蔽区域内timely perceive儿童的情况，并采取应对措施。与视频图像分类结合使用，可以显著提高儿童虐待检测的准确率。这将减少儿童在寻常培养中接受暴力虐待的可能性，并让人员在时间上采取制止儿童虐待事件的措施。实验所收集的数据全部来自儿童HOME的声音记录，包括哭泣、喊叫、笑声和背景噪音。这些声音文件被转换成spectrograms使用Short-Time Fourier Transform，然后这些图像数据被导入到CNN神经网络中进行分类，最终训练出的模型可以达到约92%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Thinker-Learning-to-Plan-and-Act"><a href="#Thinker-Learning-to-Plan-and-Act" class="headerlink" title="Thinker: Learning to Plan and Act"></a>Thinker: Learning to Plan and Act</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14993">http://arxiv.org/abs/2307.14993</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous-scrl/thinker">https://github.com/anonymous-scrl/thinker</a></li>
<li>paper_authors: Stephen Chung, Ivan Anokhin, David Krueger</li>
<li>for:  This paper proposes a novel approach called the Thinker algorithm, which enables reinforcement learning agents to autonomously interact with and utilize a learned world model.</li>
<li>methods: The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model, allowing agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment.</li>
<li>results: The algorithm achieves state-of-the-art performance and competitive results in the game of Sokoban and the Atari 2600 benchmark, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions.<details>
<summary>Abstract</summary>
We propose the Thinker algorithm, a novel approach that enables reinforcement learning agents to autonomously interact with and utilize a learned world model. The Thinker algorithm wraps the environment with a world model and introduces new actions designed for interacting with the world model. These model-interaction actions enable agents to perform planning by proposing alternative plans to the world model before selecting a final action to execute in the environment. This approach eliminates the need for hand-crafted planning algorithms by enabling the agent to learn how to plan autonomously and allows for easy interpretation of the agent's plan with visualization. We demonstrate the algorithm's effectiveness through experimental results in the game of Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves state-of-the-art performance and competitive results, respectively. Visualizations of agents trained with the Thinker algorithm demonstrate that they have learned to plan effectively with the world model to select better actions. The algorithm's generality opens a new research direction on how a world model can be used in reinforcement learning and how planning can be seamlessly integrated into an agent's decision-making process.
</details>
<details>
<summary>摘要</summary>
我们提出了“思考算法”，一种新的方法，允许强化学习代理人自主地与学习的世界模型交互。思考算法将环境包装在世界模型中，并引入了专门为与世界模型交互的模型交互动作。这些动作允许代理人通过提议不同的计划给世界模型，然后选择环境中执行的最终动作。这种方法消除了手动制定的计划算法的需要，让代理人可以自主学习计划，并且可以轻松地通过视化来解释代理人的计划。我们通过在扑克游戏和Atari 2600 benchmark中的实验成果，证明了思考算法的有效性和竞争性。视觉化的代理人训练结果显示，它们已经学习了如何通过世界模型选择更好的动作。这种算法的通用性开启了一个新的研究方向，即如何在强化学习中使用世界模型，以及如何在代理人决策过程中协调计划。
</details></li>
</ul>
<hr>
<h2 id="Incrementally-Computable-Neural-Networks-Efficient-Inference-for-Dynamic-Inputs"><a href="#Incrementally-Computable-Neural-Networks-Efficient-Inference-for-Dynamic-Inputs" class="headerlink" title="Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs"></a>Incrementally-Computable Neural Networks: Efficient Inference for Dynamic Inputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14988">http://arxiv.org/abs/2307.14988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Or Sharir, Anima Anandkumar</li>
<li>for: 这个论文的目的是解决深度学习中处理动态输入的挑战，如感知数据或用户输入。例如，一个基于AI的写作助手需要在文档编辑过程中实时更新建议。</li>
<li>methods: 这篇论文采用了逐步计算方法，寻求重用计算结果以降低计算成本。但是传统的网络架构中的紧密连接带来一大问题，因为even minor input changes会在网络中宣传并限制信息的重用。为解决这个问题，我们使用 вектор量化来精细化网络中的中间值，从而筛除无用的修改和隐藏神经元的值。</li>
<li>results: 我们在应用vector量化技术于transformers架构上创建了一种高效的逐步计算算法，其复杂度与修改输入的百分数成正比。我们的实验表明，在对OPT-125M预训练语言模型进行适应后，可以实现与传统方法相同的准确率，同时减少了12.1倍（中位数）的操作数量来处理序列中的原子修改。<details>
<summary>Abstract</summary>
Deep learning often faces the challenge of efficiently processing dynamic inputs, such as sensor data or user inputs. For example, an AI writing assistant is required to update its suggestions in real time as a document is edited. Re-running the model each time is expensive, even with compression techniques like knowledge distillation, pruning, or quantization. Instead, we take an incremental computing approach, looking to reuse calculations as the inputs change. However, the dense connectivity of conventional architectures poses a major obstacle to incremental computation, as even minor input changes cascade through the network and restrict information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs. Our experiments with adapting the OPT-125M pre-trained language model demonstrate comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits.
</details>
<details>
<summary>摘要</summary>
然而，传统的网络架构中的紧密连接带来了主要的障碍，因为 Even small input changes can propagate through the network and limit the information reuse. To address this, we use vector quantization to discretize intermediate values in the network, which filters out noisy and unnecessary modifications to hidden neurons, facilitating the reuse of their values. We apply this approach to the Transformers architecture, creating an efficient incremental inference algorithm with complexity proportional to the fraction of the modified inputs.我们通过对 OPT-125M 预训练语言模型进行适应，实验表明可以 achieve comparable accuracy on document classification while requiring 12.1X (median) fewer operations for processing sequences of atomic edits.
</details></li>
</ul>
<hr>
<h2 id="Take-A-Photo-3D-to-2D-Generative-Pre-training-of-Point-Cloud-Models"><a href="#Take-A-Photo-3D-to-2D-Generative-Pre-training-of-Point-Cloud-Models" class="headerlink" title="Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models"></a>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14971">http://arxiv.org/abs/2307.14971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangzy22/tap">https://github.com/wangzy22/tap</a></li>
<li>paper_authors: Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu</li>
<li>for: 提高3D视觉模型的性能</li>
<li>methods: 使用交叉注意机制生成不同指定姿态的视图图像，以供3D模型进行预训练</li>
<li>results: 在ScanObjectNN分类和ShapeNetPart segmentation任务上达到了state-of-the-art表现，并且可以跨 Architecture-oriented 方法进行改进Translation:</li>
<li>for: 提高3D视觉模型的性能</li>
<li>methods: 使用交叉注意机制生成不同指定姿态的视图图像，以供3D模型进行预训练</li>
<li>results: 在ScanObjectNN分类和ShapeNetPart segmentation任务上达到了state-of-the-art表现，并且可以跨 Architecture-oriented 方法进行改进<details>
<summary>Abstract</summary>
With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of generative pre-training. In this paper, we propose a novel 3D-to-2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the superiority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also effective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNetPart segmentation tasks. Code is available at https://github.com/wangzy22/TAP.
</details>
<details>
<summary>摘要</summary>
随着MAE领导的面照模型的潮流，生成先进教育显示了在2D视觉中性能的潜在提升 potential.然而，在3D视觉中，基于Transformer的脊梁和点云的无序性限制了生成先进教育的进一步发展.在本文中，我们提议一种适用于任何点云模型的3D-to-2D生成先进教育方法。我们提议通过交叉注意机制来生成不同指导姿态的视图图像作为预教程。通过生成视图图像，我们可以对3D背景进行更精确的监督，从而帮助3D背景更好地理解点云的几何结构和立体关系。实验结果表明，我们的提议的3D-to-2D生成先进教育方法比前期方法更有优势。此外，我们的方法也可以提高基于建筑方法的性能，在ScanObjectNN分类和ShapeNetPart分割任务上达到了状态之arte的表现。代码可以在https://github.com/wangzy22/TAP中找到。
</details></li>
</ul>
<hr>
<h2 id="Learning-locally-dominant-force-balances-in-active-particle-systems"><a href="#Learning-locally-dominant-force-balances-in-active-particle-systems" class="headerlink" title="Learning locally dominant force balances in active particle systems"></a>Learning locally dominant force balances in active particle systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14970">http://arxiv.org/abs/2307.14970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Sturm, Suryanarayana Maddu, Ivo F. Sbalzarini</li>
<li>for: 本研究使用混合不监督聚类和稀疏推理算法来学习自组织活动粒子系统中的地方主导力平衡，解释大规模模式的形成。</li>
<li>methods: 本研究使用一种经典的液体动力学模型，该模型可以生成各种模式，如星形和移动浓度带。我们使用数据驱动分析发现，在某些情况下，扩散带的形成是基于地方对性的排列互动驱动的，而稳定的星形则是由强 particle间互动引起的负压缩性所形成的。</li>
<li>results: 我们的方法可以在不同的模型中揭示物理共同点，并且与分析缩放关系和实验观测相当一致。<details>
<summary>Abstract</summary>
We use a combination of unsupervised clustering and sparsity-promoting inference algorithms to learn locally dominant force balances that explain macroscopic pattern formation in self-organized active particle systems. The self-organized emergence of macroscopic patterns from microscopic interactions between self-propelled particles can be widely observed nature. Although hydrodynamic theories help us better understand the physical basis of this phenomenon, identifying a sufficient set of local interactions that shape, regulate, and sustain self-organized structures in active particle systems remains challenging. We investigate a classic hydrodynamic model of self-propelled particles that produces a wide variety of patterns, like asters and moving density bands. Our data-driven analysis shows that propagating bands are formed by local alignment interactions driven by density gradients, while steady-state asters are shaped by a mechanism of splay-induced negative compressibility arising from strong particle interactions. Our method also reveals analogous physical principles of pattern formation in a system where the speed of the particle is influenced by local density. This demonstrates the ability of our method to reveal physical commonalities across models. The physical mechanisms inferred from the data are in excellent agreement with analytical scaling arguments and experimental observations.
</details>
<details>
<summary>摘要</summary>
我们使用无监督聚类和稀盐推理算法来学习自组织活动粒子系统中的地方主导力平衡，解释大规模Pattern形成。自组织emergence的大规模模式在自然界广泛存在。虽然水力学理论可以帮助我们更好地理解这种现象的物理基础，但是在活动粒子系统中确定足够的地方交互可以形成、调控和维护自组织结构仍然是挑战。我们研究了一个 класси型的自力推动粒子模型，该模型可以生成各种模式，如星形和移动密度带。我们的数据驱动分析表明，传播带是由地方对称交互驱动的密度梯度而成形，而稳定的星形则是由强粒子交互所致的负压缩性所形成。我们的方法还发现了在粒子速度受到地方密度影响的系统中的物理共同点，这表明了我们的方法的可行性。Physical mechanisms inferred from the data are in excellent agreement with analytical scaling arguments and experimental observations.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/28/cs.LG_2023_07_28/" data-id="cllshxsoe002e2u88b38p5hnl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/cs.SD_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/cs.SD_2023_07_28/">cs.SD - 2023-07-28 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection"><a href="#All-for-One-and-One-For-All-Deep-learning-based-feature-fusion-for-Synthetic-Speech-Detection" class="headerlink" title="All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection"></a>All-for-One and One-For-All: Deep learning-based feature fusion for Synthetic Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15555">http://arxiv.org/abs/2307.15555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Mari, Davide Salvi, Paolo Bestagini, Simone Milani</li>
<li>for: 防止深度学习和计算机视觉技术的滥用，检测和预测ounterfeit multimedia content的生成和使用。</li>
<li>methods: 使用三种不同的特征集提出在文献中，并将其融合以实现更好的性能，超过当前的解决方案。</li>
<li>results: 在不同的场景和数据集上进行测试，证明系统具有防御反应技术和普适性能。<details>
<summary>Abstract</summary>
Recent advances in deep learning and computer vision have made the synthesis and counterfeiting of multimedia content more accessible than ever, leading to possible threats and dangers from malicious users. In the audio field, we are witnessing the growth of speech deepfake generation techniques, which solicit the development of synthetic speech detection algorithms to counter possible mischievous uses such as frauds or identity thefts. In this paper, we consider three different feature sets proposed in the literature for the synthetic speech detection task and present a model that fuses them, achieving overall better performances with respect to the state-of-the-art solutions. The system was tested on different scenarios and datasets to prove its robustness to anti-forensic attacks and its generalization capabilities.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "deep learning" and "computer vision" were translated as "深度学习" and "计算机视觉" respectively, which are the common terms used in Simplified Chinese.* "synthesis" and "counterfeiting" were translated as "合成" and "假制" respectively, which are the most common terms used in Simplified Chinese.* "malicious users" was translated as "黑客" which is a common term used in Simplified Chinese to refer to malicious hackers or attackers.* "speech deepfake generation techniques" was translated as "语音深度假技术" which is a combination of "语音" (speech), "深度" (deep), and "假技术" (deepfake technique).* "synthetic speech detection algorithms" was translated as "合成语音检测算法" which is a combination of "合成" (synthetic), "语音" (speech), and "检测" (detection).* "anti-forensic attacks" was translated as "反法医攻击" which is a combination of "反" (against), "法" (law), "医" (medical), and "攻击" (attack).
</details></li>
</ul>
<hr>
<h2 id="Automated-approach-for-source-location-in-shallow-waters"><a href="#Automated-approach-for-source-location-in-shallow-waters" class="headerlink" title="Automated approach for source location in shallow waters"></a>Automated approach for source location in shallow waters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15491">http://arxiv.org/abs/2307.15491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niclas-angele/source_localization">https://github.com/niclas-angele/source_localization</a></li>
<li>paper_authors: Angèle Niclas, Josselin Garnier</li>
<li>for: 这个论文旨在提出一种完全自动化的方法，用于在浅水环境中恢复源位和媒体参数。</li>
<li>methods: 论文使用了一系列理论工具来理解扭曲方法的稳定性，并提出了一种自动分解记录信号中的模态组分的方法。</li>
<li>results: 论文使用实验数据测试了该方法，并得到了在实际场景中的有效性。<details>
<summary>Abstract</summary>
This paper proposes a fully automated method for recovering the location of a source and medium parameters in shallow waters. The scenario involves an unknown source emitting low-frequency sound waves in a shallow water environment, and a single hydrophone recording the signal. Firstly, theoretical tools are introduced to understand the robustness of the warping method and to propose and analyze an automated way to separate the modal components of the recorded signal. Secondly, using the spectrogram of each modal component, the paper investigates the best way to recover the modal travel times and provides stability estimates. Finally, a penalized minimization algorithm is presented to recover estimates of the source location and medium parameters. The proposed method is tested on experimental data of right whale gunshot and combustive sound sources, demonstrating its effectiveness in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种完全自动化的方法，用于在浅水环境中恢复源点和媒体参数。情况是一个未知源在浅水环境中发射低频声波，并且单个水微phone记录了信号。首先，论文介绍了一些理论工具，以理解折叠方法的稳定性，并提出了一种自动分解记录信号的模态组分的方法。然后，使用每个模态组分的spectrogram，论文研究了最好的方法来恢复模态旅行时间，并提供了稳定性估计。最后，论文提出了一种假定最小化算法，用于恢复源点和媒体参数的估计。该方法在实验数据中证明了其效果性，包括右鲸鱼枪声和燃烧声源。
</details></li>
</ul>
<hr>
<h2 id="Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding"><a href="#Minimally-Supervised-Speech-Synthesis-with-Conditional-Diffusion-Model-and-Language-Model-A-Comparative-Study-of-Semantic-Coding" class="headerlink" title="Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding"></a>Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15484">http://arxiv.org/abs/2307.15484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Hao Ni, He Qu, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>For: 提出了一种基于扩散模型和提取器构造的文本译话系统，以便使用 minimal supervision 进行训练，并解决高维度和波形质量问题。* Methods: 使用了扩散模型来模型语义编码，并引入了variational autoencoders和表达瓶颈来改进提示表示能力。Duration diffusion model 也被设计来实现多种表达。* Results: 相比基eline方法，提出的方法得到了更好的表现，并提供了一个网站以便听到样本 зву频。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. To address the challenges associated with high dimensionality and waveform distortion in discrete representations, we propose Diff-LM-Speech, which models semantic embeddings into mel-spectrogram based on diffusion models and introduces a prompt encoder structure based on variational autoencoders and prosody bottlenecks to improve prompt representation capabilities. Autoregressive language models often suffer from missing and repeated words, while non-autoregressive frameworks face expression averaging problems due to duration prediction models. To address these issues, we propose Tetra-Diff-Speech, which designs a duration diffusion model to achieve diverse prosodic expressions. While we expect the information content of semantic coding to be between that of text and acoustic coding, existing models extract semantic coding with a lot of redundant information and dimensionality explosion. To verify that semantic coding is not necessary, we propose Tri-Diff-Speech. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples.
</details>
<details>
<summary>摘要</summary>
近些年来，关于具有少量监督的文本识语（TTS）方法的兴趣增长。为了解决高维度和波形扭曲的问题，我们提出了Diff-LM-Speech，它基于扩散模型来模型语义表示，并在mel-spectrogram上引入了variational autoencoders和慢征瓶颈来提高描述符能力。autoregressive语言模型经常会出现缺失和重复的单词，而非autoregressive框架则面临表达平均化问题，这是因为duration预测模型。为解决这些问题，我们提出了Tetra-Diff-Speech，它通过duration扩散模型来实现多种表达。而我们预期semantic coding中信息量介于文本和音频编码之间。现有模型会提取大量的重复信息和维度爆炸。为验证semantic coding不是必要的，我们提出了Tri-Diff-Speech。实验结果表明，我们提出的方法在比基eline方法表现出色。我们提供了一个网站，包含了audio samples。
</details></li>
</ul>
<hr>
<h2 id="The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022"><a href="#The-FlySpeech-Audio-Visual-Speaker-Diarization-System-for-MISP-Challenge-2022" class="headerlink" title="The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022"></a>The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge 2022</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15400">http://arxiv.org/abs/2307.15400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhang, Huan Zhao, Yue Li, Bowen Pang, Yannan Wang, Hongji Wang, Wei Rao, Qing Wang, Lei Xie</li>
<li>for: 这篇论文描述了在ICASSP 2022年第二届多Modal信息基于语音处理~(\textbf{MISP})挑战中提交的飞Speech speaker分类系统。</li>
<li>methods: 我们开发了一个端到端的音频视频 speaker分类~(AVSD)系统，该系统包括一个唇编码器、一个说话人编码器和一个音频视频解码器。具体来说，我们为了解决分离训练导致的质量下降，我们将说话人编码器和音频视频解码器共同训练。此外，我们利用大量预训练的说话人提取器来初始化说话人编码器。</li>
<li>results: 我们的实验结果表明，在一些公共语音和视频数据集上，我们的AVSD系统可以具有高度的准确率和稳定性。<details>
<summary>Abstract</summary>
This paper describes the FlySpeech speaker diarization system submitted to the second \textbf{M}ultimodal \textbf{I}nformation Based \textbf{S}peech \textbf{P}rocessing~(\textbf{MISP}) Challenge held in ICASSP 2022. We develop an end-to-end audio-visual speaker diarization~(AVSD) system, which consists of a lip encoder, a speaker encoder, and an audio-visual decoder. Specifically, to mitigate the degradation of diarization performance caused by separate training, we jointly train the speaker encoder and the audio-visual decoder. In addition, we leverage the large-data pretrained speaker extractor to initialize the speaker encoder.
</details>
<details>
<summary>摘要</summary>
这篇论文描述了我们在ICASSP 2022年第二届多Modal信息基于语音处理~(\textbf{MISP}) 挑战中提交的 FlySpeech 语音排序系统。我们开发了一个端到端的音频视频排序系统（AVSD），该系统包括一个唇编码器、一个说话人编码器和一个音频视频解码器。具体来说，为了解决分离训练导致的排序性能下降，我们将说话人编码器和音频视频解码器并行训练。此外，我们利用大量预训练的说话人提取器来初始化说话人编码器。
</details></li>
</ul>
<hr>
<h2 id="Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions"><a href="#Improving-Audio-Text-Retrieval-via-Hierarchical-Cross-Modal-Interaction-and-Auxiliary-Captions" class="headerlink" title="Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions"></a>Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15344">http://arxiv.org/abs/2307.15344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Xin, Yuexian Zou</li>
<li>for: 提高语音文本检索性能（ATR）</li>
<li>methods: 层次跨模态交互（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现全面多模态semantic比较。</li>
<li>results: 实验表明，我们的HCI方法可以明显提高ATR性能，而且我们的auxiliary captions（AC）框架也在多个数据集上显示稳定性。<details>
<summary>Abstract</summary>
Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets.
</details>
<details>
<summary>摘要</summary>
现有的音频文本搜寻（ATR）方法通常是建立整个音频clip和完整的caption句子之间的对比，而忽略细微的跨媒体关系，例如短段和短句或帧和单词。在这篇文章中，我们引入了一个层次跨媒体互动（HCI）方法，同时探索clip-sentence、segment-phrase和frame-word关系，实现了多modal semantic comparison的全面探索。此外，我们还提出了一个使用预训掌握的captioner生成的auxiliary captions（AC）来进行音频和生成caption之间的特征互动，这个方法可以提高音频表示的性能，并且与原始ATR匹配分支相 komplementary。音频和生成caption可以形成新的音频文本对，用于训练。实验结果显示，我们的HCI方法能够对ATR表现提高。此外，我们的AC框架也在多个数据集上显示稳定的性能提升。
</details></li>
</ul>
<hr>
<h2 id="PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement"><a href="#PCNN-A-Lightweight-Parallel-Conformer-Neural-Network-for-Efficient-Monaural-Speech-Enhancement" class="headerlink" title="PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement"></a>PCNN: A Lightweight Parallel Conformer Neural Network for Efficient Monaural Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15251">http://arxiv.org/abs/2307.15251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinmeng Xu, Weiping Tu, Yuhong Yang</li>
<li>for: 这篇论文目的是提出一种能够有效融合卷积神经网络和转换器的Speech增强方法。</li>
<li>methods: 该方法使用了卷积神经网络和转换器两种不同的架构，并通过特制的多支分支扩展层和自适应时频注意力模块来挖掘本地Format模式和全球结构表示。</li>
<li>results: 实验结果表明，该方法在大多数评价标准中表现更好于当前State-of-the-art方法，而且保持最低的模型参数数量。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNN) and Transformer have wildly succeeded in multimedia applications. However, more effort needs to be made to harmonize these two architectures effectively to satisfy speech enhancement. This paper aims to unify these two architectures and presents a Parallel Conformer for speech enhancement. In particular, the CNN and the self-attention (SA) in the Transformer are fully exploited for local format patterns and global structure representations. Based on the small receptive field size of CNN and the high computational complexity of SA, we specially designed a multi-branch dilated convolution (MBDC) and a self-channel-time-frequency attention (Self-CTFA) module. MBDC contains three convolutional layers with different dilation rates for the feature from local to non-local processing. Experimental results show that our method performs better than state-of-the-art methods in most evaluation criteria while maintaining the lowest model parameters.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）和转换器（Transformer）在多媒体应用中获得了很大成功。然而，为了有效融合这两种架构，还需要更多的努力。这篇论文目的是将这两种架构融合在一起，并提出了平行转换器（Parallel Conformer） для语音增强。具体来说，CNN和转换器中的自注意（SA）都被完全利用了，以捕捉本地格式模式和全球结构表示。由于小感知场大小和自注意的计算复杂度，我们专门设计了多支束层核（MBDC）和自频时空尺度注意（Self-CTFA）模块。MBDC包括三层核心层，每层都有不同的扩散率，以捕捉从本地到非本地处理的特征。实验结果表明，我们的方法在大多数评估标准下表现更好，而且保持最低的模型参数。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Visual-Acoustic-Matching"><a href="#Self-Supervised-Visual-Acoustic-Matching" class="headerlink" title="Self-Supervised Visual Acoustic Matching"></a>Self-Supervised Visual Acoustic Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15064">http://arxiv.org/abs/2307.15064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Somayazulu, Changan Chen, Kristen Grauman</li>
<li>for: 这个论文的目的是提出一种自主学习的视觉音频匹配方法，以便在Target acoustic environment中重新生成音频clip，使其具有Target environment的音频特征。</li>
<li>methods: 这个方法使用conditional GAN框架，并提出了一种新的度量 metric来衡量剩余的音频信息水平。它通过自动分离room acoustics和重新生成音频，来实现视觉音频匹配。</li>
<li>results: 该方法在多个复杂的数据集上表现出色，并且可以在各种真实世界的音频和环境下进行匹配。论文的实验结果表明，它超过了现有的状态则艺的表现。<details>
<summary>Abstract</summary>
Acoustic matching aims to re-synthesize an audio clip to sound as if it were recorded in a target acoustic environment. Existing methods assume access to paired training data, where the audio is observed in both source and target environments, but this limits the diversity of training data or requires the use of simulated data or heuristics to create paired samples. We propose a self-supervised approach to visual acoustic matching where training samples include only the target scene image and audio -- without acoustically mismatched source audio for reference. Our approach jointly learns to disentangle room acoustics and re-synthesize audio into the target environment, via a conditional GAN framework and a novel metric that quantifies the level of residual acoustic information in the de-biased audio. Training with either in-the-wild web data or simulated data, we demonstrate it outperforms the state-of-the-art on multiple challenging datasets and a wide variety of real-world audio and environments.
</details>
<details>
<summary>摘要</summary>
干擦匹配目标是将音频clip重新synthesize为如果在目标听取环境中录制的样本。现有方法假设有对应的培训数据，其中包括源和目标环境中的音频，但这限制了培训数据的多样性或需要使用模拟数据或规则来创建对应的样本。我们提出了一种自主超vised Approach to visual acoustic matching，其中培训样本只包括目标场景图像和音频，而不包括不匹配的源音频参考。我们的方法同时学习分离房间听取特性和重新synthesize音频到目标环境中，通过单个GAN框架和一种新的度量量化干擦听取信息在去除biased audio中的水平。我们在使用实际网络数据或模拟数据进行培训时，demonstrate其超过了现有状态的多个挑战性数据集和多种真实的音频和环境。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/28/cs.SD_2023_07_28/" data-id="cllshxsp3004s2u88gzzm80tf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/eess.AS_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/eess.AS_2023_07_28/">eess.AS - 2023-07-28 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Acoustic-Echo-Suppression-with-Condition-Aware-Training"><a href="#Efficient-Acoustic-Echo-Suppression-with-Condition-Aware-Training" class="headerlink" title="Efficient Acoustic Echo Suppression with Condition-Aware Training"></a>Efficient Acoustic Echo Suppression with Condition-Aware Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15630">http://arxiv.org/abs/2307.15630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Seidel, Pejman Mowlaee, Tim Fingscheidt</li>
<li>for: 这篇论文的目的是提出一个改进的卷积回传神经网络（CRN）架构，以提高双话（DT）情况下的深度声音控制性。</li>
<li>methods: 这篇论文使用了一种具有卷积Encoder和解oder，并具有回传瓶颈的CRN架构，并且这种架构比前一代的模型更加简单，具有更好的性能。</li>
<li>results: 根据实验结果显示，这种改进的CRN架构不仅比前一代的FCRN和CRUSE模型更加简单，而且在DT情况下也有更好的性能。<details>
<summary>Abstract</summary>
The topic of deep acoustic echo control (DAEC) has seen many approaches with various model topologies in recent years. Convolutional recurrent networks (CRNs), consisting of a convolutional encoder and decoder encompassing a recurrent bottleneck, are repeatedly employed due to their ability to preserve nearend speech even in double-talk (DT) condition. However, past architectures are either computationally complex or trade off smaller model sizes with a decrease in performance. We propose an improved CRN topology which, compared to other realizations of this class of architectures, not only saves parameters and computational complexity, but also shows improved performance in DT, outperforming both baseline architectures FCRN and CRUSE. Striving for a condition-aware training, we also demonstrate the importance of a high proportion of double-talk and the missing value of nearend-only speech in DAEC training data. Finally, we show how to control the trade-off between aggressive echo suppression and near-end speech preservation by fine-tuning with condition-aware component loss functions.
</details>
<details>
<summary>摘要</summary>
“深层听频延迟控制（DAEC）在过去几年中有很多方法和不同的模型结构。卷积回归网络（CRN）因其能保持靠近的speech在双语（DT）条件下，被广泛使用。然而，过去的建筑方案都是计算复杂或是减少模型大小的代价是性能下降。我们提出了改进的CRN顶点，与其他类型的建筑方案相比，不仅减少参数和计算复杂度，还表现出DT条件下的提高性能，超过了基eline馈料FCRN和CRUSE。我们还证明了在DAEC训练数据中高占比的双语和缺失的靠近只speech的重要性。最后，我们示出了控制听频延迟和靠近speech保持的质量的权衡，通过condition-aware组件损失函数的微调。”Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Time-Frequency-Generative-Adversarial-based-method-for-Audio-Packet-Loss-Concealment"><a href="#A-Time-Frequency-Generative-Adversarial-based-method-for-Audio-Packet-Loss-Concealment" class="headerlink" title="A Time-Frequency Generative Adversarial based method for Audio Packet Loss Concealment"></a>A Time-Frequency Generative Adversarial based method for Audio Packet Loss Concealment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15611">http://arxiv.org/abs/2307.15611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aircarlo/bin2bin-gan-plc">https://github.com/aircarlo/bin2bin-gan-plc</a></li>
<li>paper_authors: Carlo Aironi, Samuele Cornell, Luca Serafini, Stefano Squartini</li>
<li>for: 这篇论文旨在提高VoIP传输中 packet loss 的影响，使用生成对抗法修复丢失的封包。</li>
<li>methods: 本论文使用生成对抗网络（GAN）的改进版本 pix2pix 框架，将丢失的封包转换为非损失的语音spectrogram。</li>
<li>results: 实验结果显示，提议的方法在高 packet loss 率和大差距情况下表现出显著优势，能更好地保持语音结构信息。<details>
<summary>Abstract</summary>
Packet loss is a major cause of voice quality degradation in VoIP transmissions with serious impact on intelligibility and user experience. This paper describes a system based on a generative adversarial approach, which aims to repair the lost fragments during the transmission of audio streams. Inspired by the powerful image-to-image translation capability of Generative Adversarial Networks (GANs), we propose bin2bin, an improved pix2pix framework to achieve the translation task from magnitude spectrograms of audio frames with lost packets, to noncorrupted speech spectrograms. In order to better maintain the structural information after spectrogram translation, this paper introduces the combination of two STFT-based loss functions, mixed with the traditional GAN objective. Furthermore, we employ a modified PatchGAN structure as discriminator and we lower the concealment time by a proper initialization of the phase reconstruction algorithm. Experimental results show that the proposed method has obvious advantages when compared with the current state-of-the-art methods, as it can better handle both high packet loss rates and large gaps.
</details>
<details>
<summary>摘要</summary>
packet loss 是 VoIP 传输中的一个主要 causative factor，它会对语音质量产生严重的影响，从而影响用户体验。本文描述了一种基于生成敌对推议的系统，该系统可以在audio流传输过程中重建丢失的封包。引用了生成敌对网络（GANs）的强大图像到图像翻译能力，我们提出了bin2bin，一种改进的 pix2pix 框架，用于从损坏的音频帧矩阵中翻译到不损坏的语音矩阵。为了更好地保持音频翻译后的结构信息，本文引入了两种 STFT 基于的损失函数的组合，混合了传统的 GAN 目标函数。此外，我们使用了修改后的 PatchGAN 结构来担任推议器，并通过适当的初始化方式下降掩蔽时间。实验结果表明，提出的方法在与当前状态艺术方法相比，具有明显的优势，能够更好地处理高 packet loss 率和大的差距。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/28/eess.AS_2023_07_28/" data-id="cllshxspl006j2u88dju20suz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/28/eess.IV_2023_07_28/" class="article-date">
  <time datetime="2023-07-27T16:00:00.000Z" itemprop="datePublished">2023-07-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/28/eess.IV_2023_07_28/">eess.IV - 2023-07-28 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond"><a href="#A-Survey-on-Deep-Learning-in-Medical-Image-Registration-New-Technologies-Uncertainty-Evaluation-Metrics-and-Beyond" class="headerlink" title="A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond"></a>A Survey on Deep Learning in Medical Image Registration: New Technologies, Uncertainty, Evaluation Metrics, and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15615">http://arxiv.org/abs/2307.15615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyu Chen, Yihao Liu, Shuwen Wei, Zhangxing Bian, Shalini Subramanian, Aaron Carass, Jerry L. Prince, Yong Du</li>
<li>for: 本研究团队提出了一种基于深度学习的医学图像注册方法，以推动医学图像注册领域的发展。</li>
<li>methods: 本研究使用了多种深度学习网络，包括ResNet和U-Net，以及不同的相似度度量和减杂化正则化。</li>
<li>results: 本研究提出了一种全面的深度学习基于图像注册方法，包括网络架构、损失函数和注册不确定性估计。此外，本研究还提出了用于评估深度学习模型在注册任务中的评价指标。<details>
<summary>Abstract</summary>
Over the past decade, deep learning technologies have greatly advanced the field of medical image registration. The initial developments, such as ResNet-based and U-Net-based networks, laid the groundwork for deep learning-driven image registration. Subsequent progress has been made in various aspects of deep learning-based registration, including similarity measures, deformation regularizations, and uncertainty estimation. These advancements have not only enriched the field of deformable image registration but have also facilitated its application in a wide range of tasks, including atlas construction, multi-atlas segmentation, motion estimation, and 2D-3D registration. In this paper, we present a comprehensive overview of the most recent advancements in deep learning-based image registration. We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.
</details>
<details>
<summary>摘要</summary>
We begin with a concise introduction to the core concepts of deep learning-based image registration. Then, we delve into innovative network architectures, loss functions specific to registration, and methods for estimating registration uncertainty. Additionally, this paper explores appropriate evaluation metrics for assessing the performance of deep learning models in registration tasks. Finally, we highlight the practical applications of these novel techniques in medical imaging and discuss the future prospects of deep learning-based image registration.Translated into Simplified Chinese:过去十年，深度学习技术在医疗影像registrations中取得了大量进步。初期发展，如ResNet基于和U-Net基于的网络，为深度学习驱动的影像registrations奠定了基础。后续的进步包括相似度度量、形态规范和注意力估计等方面，这些进步不仅涌现了弹性影像registrations的领域，还为各种任务，如建立Atlas、多Atlas分割、运动估计和2D-3Dregistrations提供了应用。在本文中，我们提供了最新的深度学习基于影像registrations的概括。我们从核心概念的介绍开始，然后探讨了新的网络架构、特定于registrations的损失函数和注意力估计方法。此外，这篇论文还探讨了评价深度学习模型在registrations任务中表现的适当评价指标。最后，我们强调了这些新技术在医疗影像中的实际应用和未来前景。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction"><a href="#Integrated-Digital-Reconstruction-of-Welded-Components-Supporting-Improved-Fatigue-Life-Prediction" class="headerlink" title="Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction"></a>Integrated Digital Reconstruction of Welded Components: Supporting Improved Fatigue Life Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15604">http://arxiv.org/abs/2307.15604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anders Faarbæk Mikkelstrup, Morten Kristiansen</li>
<li>for: 提高锚固结构的质量和安全性</li>
<li>methods: 使用自动化高频机械冲击（HFMI）处理、图像处理、简单滤波技术和非线性优化算法对拼接部位进行数字重建</li>
<li>results: 实现了 generic、cost-effective、flexible 和 rapid 的数字重建方法，帮助提高锚固结构的设计、质量监控和HFMI处理记录<details>
<summary>Abstract</summary>
In the design of offshore jacket foundations, fatigue life is crucial. Post-weld treatment has been proposed to enhance the fatigue performance of welded joints, where particularly high-frequency mechanical impact (HFMI) treatment has been shown to improve fatigue performance significantly. Automated HFMI treatment has improved quality assurance and can lead to cost-effective design when combined with accurate fatigue life prediction. However, the finite element method (FEM), commonly used for predicting fatigue life in complex or multi-axial joints, relies on a basic CAD depiction of the weld, failing to consider the actual weld geometry and defects. Including the actual weld geometry in the FE model improves fatigue life prediction and possible crack location prediction but requires a digital reconstruction of the weld. Current digital reconstruction methods are time-consuming or require specialised scanning equipment and potential component relocation. The proposed framework instead uses an industrial manipulator combined with a line scanner to integrate digital reconstruction as part of the automated HFMI treatment setup. This approach applies standard image processing, simple filtering techniques, and non-linear optimisation for aligning and merging overlapping scans. A screened Poisson surface reconstruction finalises the 3D model to create a meshed surface. The outcome is a generic, cost-effective, flexible, and rapid method that enables generic digital reconstruction of welded parts, aiding in component design, overall quality assurance, and documentation of the HFMI treatment.
</details>
<details>
<summary>摘要</summary>
在海上钻井基础设计中，腐蚀性 жизни是关键。Post-weld处理被提议来提高焊接缝合的腐蚀性能，特别是高频机械冲击（HFMI）处理，可以大幅提高腐蚀性能。自动化HFMI处理可以提高质量保证和降低设计成本，当与准确的腐蚀生命预测结合使用时。然而，通用finite element方法（FEM），常用于预测复杂或多轴缝合的腐蚀生命，基于简化的CAD描述，忽略了实际焊接形状和缺陷。包含实际焊接形状在FEM模型中可以提高腐蚀生命预测和可能的裂化位置预测，但需要数字重建焊接。当前的数字重建方法需要大量时间或特殊的扫描设备，以及可能的组件重新位置。我们提出的框架则使用工业护手机和线扫描器结合数字重建，并通过标准的图像处理、简单的滤波技术和非线性优化来对 overlap 扫描进行对接和合并。屏幕Poisson面重建最终生成3D模型，生成一个通用、成本效果、灵活、快速的方法，以便在焊接部件的设计、总质量保证和HFMI处理的文档中进行数字重建。
</details></li>
</ul>
<hr>
<h2 id="OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes"><a href="#OAFuser-Towards-Omni-Aperture-Fusion-for-Light-Field-Semantic-Segmentation-of-Road-Scenes" class="headerlink" title="OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes"></a>OAFuser: Towards Omni-Aperture Fusion for Light Field Semantic Segmentation of Road Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15588">http://arxiv.org/abs/2307.15588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feibryantkit/oafuser">https://github.com/feibryantkit/oafuser</a></li>
<li>paper_authors: Fei Teng, Jiaming Zhang, Kunyu Peng, Kailun Yang, Yaonan Wang, Rainer Stiefelhagen</li>
<li>for: 增强自动驾驶场景理解的图像Semantic Segmentation，使用光场相机提供了丰富的angular和空间信息。</li>
<li>methods: 提议Omni-Aperture Fusion模型（OAFuser），利用中心视图的denseContext和sub-aperture图像中的angular信息来生成semantically-consistent的结果，同时采用Sub-Aperture Fusion Module（SAFM）将sub-aperture图像嵌入angular特征中，不需要额外内存成本。</li>
<li>results: 在UrbanLF-Real和-Syn数据集上实现了state-of-the-art性能，在UrbanLF-Real Extended数据集上达到了84.93%的mIoU记录，比前一个最佳记录提高+4.53%。<details>
<summary>Abstract</summary>
Light field cameras can provide rich angular and spatial information to enhance image semantic segmentation for scene understanding in the field of autonomous driving. However, the extensive angular information of light field cameras contains a large amount of redundant data, which is overwhelming for the limited hardware resource of intelligent vehicles. Besides, inappropriate compression leads to information corruption and data loss. To excavate representative information, we propose an Omni-Aperture Fusion model (OAFuser), which leverages dense context from the central view and discovers the angular information from sub-aperture images to generate a semantically-consistent result. To avoid feature loss during network propagation and simultaneously streamline the redundant information from the light field camera, we present a simple yet very effective Sub-Aperture Fusion Module (SAFM) to embed sub-aperture images into angular features without any additional memory cost. Furthermore, to address the mismatched spatial information across viewpoints, we present Center Angular Rectification Module (CARM) realized feature resorting and prevent feature occlusion caused by asymmetric information. Our proposed OAFuser achieves state-of-the-art performance on the UrbanLF-Real and -Syn datasets and sets a new record of 84.93% in mIoU on the UrbanLF-Real Extended dataset, with a gain of +4.53%. The source code of OAFuser will be made publicly available at https://github.com/FeiBryantkit/OAFuser.
</details>
<details>
<summary>摘要</summary>
光场相机可以提供rich的ANGLE和空间信息，以增强图像Semantic Segmentation，以提高自动驾驶场景理解。然而，光场相机的广泛ANGLE信息包含大量冗余数据，对智能车辆的硬件资源造成拥堵。此外，不当压缩可能会导致信息损害和数据丢失。为了提取代表性信息，我们提议了一种Omni-Aperture Fusion模型（OAFuser），它利用中心视图的dense上下文和子视图图像ANGLE信息来生成具有相同semantic consistency的结果。为了避免网络传播过程中的特征损失和同时压缩光场相机中的冗余信息，我们提出了一种简单 yet powerful的Sub-Aperture Fusion模块（SAFM），可以将子视图图像与ANGLE特征进行嵌入，无需额外内存成本。此外，为了解决不同视角之间的匹配问题，我们提出了Center Angular Rectification Module（CARM），通过实现特征重定向和避免特征堵塞，解决了由不同视角所导致的匹配问题。我们的OAFuser模型在UrbanLF-Real和UrbanLF-Syn数据集上达到了状态机器人的性能记录，具有84.93%的mIoU在UrbanLF-Real Extended数据集上，与前一个记录之间的增幅为+4.53%。OAFuser模型的源代码将于https://github.com/FeiBryantkit/OAFuser上公开。
</details></li>
</ul>
<hr>
<h2 id="Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space"><a href="#Defocus-Blur-Synthesis-and-Deblurring-via-Interpolation-and-Extrapolation-in-Latent-Space" class="headerlink" title="Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space"></a>Defocus Blur Synthesis and Deblurring via Interpolation and Extrapolation in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15461">http://arxiv.org/abs/2307.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nis-research/linear-latent-blur">https://github.com/nis-research/linear-latent-blur</a></li>
<li>paper_authors: Ioana Mazilu, Shunxin Wang, Sven Dummer, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio</li>
<li>For: 这项研究旨在提高微scopic图像质量，以便更好地进行医疗诊断和疾病分析。* Methods: 该项目使用自适应柱面网络和正则化技术，以实现对图像的杂谱和解析。* Results: 研究人员通过训练自适应网络并应用正则化技术，实现了对图像的杂谱和解析，提高了微scopic图像的质量，并可以用作数据增强技术。<details>
<summary>Abstract</summary>
Though modern microscopes have an autofocusing system to ensure optimal focus, out-of-focus images can still occur when cells within the medium are not all in the same focal plane, affecting the image quality for medical diagnosis and analysis of diseases. We propose a method that can deblur images as well as synthesize defocus blur. We train autoencoders with implicit and explicit regularization techniques to enforce linearity relations among the representations of different blur levels in the latent space. This allows for the exploration of different blur levels of an object by linearly interpolating/extrapolating the latent representations of images taken at different focal planes. Compared to existing works, we use a simple architecture to synthesize images with flexible blur levels, leveraging the linear latent space. Our regularized autoencoders can effectively mimic blur and deblur, increasing data variety as a data augmentation technique and improving the quality of microscopic images, which would be beneficial for further processing and analysis.
</details>
<details>
<summary>摘要</summary>
modern microscopes 已经有自动对焦系统，但是仍然可能出现不清晰的图像，因为细胞在媒体中不 все在同一个 фокус平面上，这会影响医疗诊断和疾病分析的图像质量。我们提出了一种方法，可以对图像进行恢复和模拟杂化。我们使用隐式和显式正则化技术来规范各个混杂水平的表示在隐藏空间中的线性关系。这allow for随着不同 фокус平面上图像的latent representation的线性 interpolate/extrapolate，以探索不同混杂水平的对象图像。相比之下，我们使用简单的建筑来 sintesize图像，并且可以在隐藏空间中Linearly interpolate/extrapolate不同混杂水平，从而增加数据的多样性，并提高微scopic图像的质量，这将对进一步处理和分析产生有利影响。
</details></li>
</ul>
<hr>
<h2 id="ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology"><a href="#ERCPMP-An-Endoscopic-Image-and-Video-Dataset-for-Colorectal-Polyps-Morphology-and-Pathology" class="headerlink" title="ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology"></a>ERCPMP: An Endoscopic Image and Video Dataset for Colorectal Polyps Morphology and Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15444">http://arxiv.org/abs/2307.15444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojgan Forootan, Mohsen Rajabnia, Ahmad R Mafi, Hamed Azhdari Tehrani, Erfan Ghadirzadeh, Mahziar Setayeshfar, Zahra Ghaffari, Mohammad Tashakoripour, Mohammad Reza Zali, Hamidreza Bolhasani</li>
<li>for: This paper is written for the purpose of developing accurate algorithms for medical prediction, detection, diagnosis, treatment, and prognosis, specifically for colorectal polyps.</li>
<li>methods: The paper uses a dataset called ERCPMP, which contains demographic, morphological, and pathological data, endoscopic images, and videos of 191 patients with colorectal polyps. The dataset includes information based on the latest international gastroenterology classification references, such as Paris, Pit, and JNET classification.</li>
<li>results: The paper provides a dataset that can be used for the development and evaluation of algorithms for the recognition of colorectal polyps morphology and pathology. The dataset is available on Elsevier Mendeley Dataverse and is currently under development, with the latest version accessible via a specific website.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发准确的医疗预测、检测、诊断、治疗和预后评估算法，特别是为了Rectal Polyps。</li>
<li>methods: 这篇论文使用了名为ERCPMP的数据集，该数据集包含了191名患有Rectal Polyps的病人的民生、形态和病理数据，以及这些病人的endoscopic图像和视频。数据集包括根据最新的国际肠胃病学分类标准，如Paris、Pit和JNET分类标准。</li>
<li>results: 这篇论文提供了一个用于开发和评估Recognize Colorectal Polyps Morphology and Pathology的算法的数据集。数据集可以在Elsevier Mendeley Dataverse上获取，并且目前正在开发中，最新的版本可以通过特定的网站获取。<details>
<summary>Abstract</summary>
In the recent years, artificial intelligence (AI) and its leading subtypes, machine learning (ML) and deep learning (DL) and their applications are spreading very fast in various aspects such as medicine. Today the most important challenge of developing accurate algorithms for medical prediction, detection, diagnosis, treatment and prognosis is data. ERCPMP is an Endoscopic Image and Video Dataset for Recognition of Colorectal Polyps Morphology and Pathology. This dataset contains demographic, morphological and pathological data, endoscopic images and videos of 191 patients with colorectal polyps. Morphological data is included based on the latest international gastroenterology classification references such as Paris, Pit and JNET classification. Pathological data includes the diagnosis of the polyps including Tubular, Villous, Tubulovillous, Hyperplastic, Serrated, Inflammatory and Adenocarcinoma with Dysplasia Grade & Differentiation. The current version of this dataset is published and available on Elsevier Mendeley Dataverse and since it is under development, the latest version is accessible via: https://databiox.com.
</details>
<details>
<summary>摘要</summary>
在最近的几年中，人工智能（AI）和其主要分支——机器学习（ML）和深度学习（DL）在各种领域的应用速度很快，如医学。今天最重要的挑战是开发准确的医疗算法，包括预测、检测、诊断、治疗和预后。ERCPMP是一个endorscopic Image和视频Dataset，用于识别肠RECTALPolyp的形态和病理学特征。该数据集包括191名患者的肠RECTALPolyp的民生、形态和病理学数据，以及endooscopic图像和视频。形态数据按照最新的国际肠胃科分类标准进行编码，包括Paris、Pit和JNET分类。病理数据包括肠RECTALPolyp的诊断，包括管肠、芽孢、管芽孢、高级瘤、炎性、卵极和adenocarcinoma，以及分化度和分化度。当前版本的数据集已经发布，可以在Elsevier Mendeley Dataverse上获取，而最新版本可以通过以下链接获取：https://databiox.com。
</details></li>
</ul>
<hr>
<h2 id="RAWIW-RAW-Image-Watermarking-Robust-to-ISP-Pipeline"><a href="#RAWIW-RAW-Image-Watermarking-Robust-to-ISP-Pipeline" class="headerlink" title="RAWIW: RAW Image Watermarking Robust to ISP Pipeline"></a>RAWIW: RAW Image Watermarking Robust to ISP Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15443">http://arxiv.org/abs/2307.15443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kang Fu, Xiaohong Liu, Jun Jia, Zicheng Zhang, Yicong Peng, Jia Wang, Guangtao Zhai</li>
<li>for: 这篇论文的目的是为了提供一种基于深度学习的 RAW 图像水印保护方法，以保护 RAW 图像的版权。</li>
<li>methods: 该方法使用了深度学习网络来实现 RAW 图像水印保护，并将 copyright 信息直接嵌入 RAW 图像中，以便在不同的 post-processing 方法后可以提取 copyright 信息。</li>
<li>results: 该方法在实验中成功地实现了跨频道的版权保护，并且可以在不同的传输损害 circumstance 下保持图像质量和水印的可靠性。<details>
<summary>Abstract</summary>
Invisible image watermarking is essential for image copyright protection. Compared to RGB images, RAW format images use a higher dynamic range to capture the radiometric characteristics of the camera sensor, providing greater flexibility in post-processing and retouching. Similar to the master recording in the music industry, RAW images are considered the original format for distribution and image production, thus requiring copyright protection. Existing watermarking methods typically target RGB images, leaving a gap for RAW images. To address this issue, we propose the first deep learning-based RAW Image Watermarking (RAWIW) framework for copyright protection. Unlike RGB image watermarking, our method achieves cross-domain copyright protection. We directly embed copyright information into RAW images, which can be later extracted from the corresponding RGB images generated by different post-processing methods. To achieve end-to-end training of the framework, we integrate a neural network that simulates the ISP pipeline to handle the RAW-to-RGB conversion process. To further validate the generalization of our framework to traditional ISP pipelines and its robustness to transmission distortion, we adopt a distortion network. This network simulates various types of noises introduced during the traditional ISP pipeline and transmission. Furthermore, we employ a three-stage training strategy to strike a balance between robustness and concealment of watermarking. Our extensive experiments demonstrate that RAWIW successfully achieves cross-domain copyright protection for RAW images while maintaining their visual quality and robustness to ISP pipeline distortions.
</details>
<details>
<summary>摘要</summary>
隐形图像水印是图像版权保护的关键技术。相比RGB图像，RAW格式图像利用更高的动态范围来捕捉相机传感器的 радиметрические特征，提供更大的后处理和修剪灵活性。类似于音乐行业中的母带录制，RAW图像被视为原始格式，需要版权保护。现有的水印方法通常针对RGB图像，留下了RAW图像的空白。为了解决这个问题，我们提出了首个基于深度学习的RAW图像水印（RAWIW）框架，用于版权保护。与RGB图像水印不同，我们的方法实现了跨频道的版权保护。我们直接嵌入版权信息到RAW图像中，可以在不同的后处理方法生成的RGB图像中提取。为实现整个框架的端到端训练，我们将神经网络与ISP管道相互关联，以处理RAW图像到RGB图像的转换过程。此外，我们采用了一个扰动网络，以模拟传输过程中引入的各种噪声。此外，我们采用了三stage训练策略，以保持水印的鲁棒性和隐藏性。我们的广泛实验表明，RAWIW成功实现了RAW图像的跨频道版权保护，保持了图像的视觉质量和传输过程中的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression"><a href="#MLIC-Linear-Complexity-Multi-Reference-Entropy-Modeling-for-Learned-Image-Compression" class="headerlink" title="MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression"></a>MLIC++: Linear Complexity Multi-Reference Entropy Modeling for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15421">http://arxiv.org/abs/2307.15421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiangweibeta/mlic">https://github.com/jiangweibeta/mlic</a></li>
<li>paper_authors: Wei Jiang, Ronggang Wang</li>
<li>for: 这篇论文是为了提出一种基于多参考 entropy 模型的学习图像压缩方法，以提高图像压缩的效率和质量。</li>
<li>methods: 该方法使用 linear complexity global correlations capturing，通过软max操作的减法分解，以取代之前的 attention 方法。</li>
<li>results: 相比 VTM-17.0，该方法可以在 PSNR 指标下减少 BD-rate 12.44%，并且更高效。Code 将在 GitHub 上提供。<details>
<summary>Abstract</summary>
Recently, multi-reference entropy model has been proposed, which captures channel-wise, local spatial, and global spatial correlations. Previous works adopt attention for global correlation capturing, however, the quadratic cpmplexity limits the potential of high-resolution image coding. In this paper, we propose the linear complexity global correlations capturing, via the decomposition of softmax operation. Based on it, we propose the MLIC$^{++}$, a learned image compression with linear complexity for multi-reference entropy modeling. Our MLIC$^{++}$ is more efficient and it reduces BD-rate by 12.44% on the Kodak dataset compared to VTM-17.0 when measured in PSNR. Code will be available at https://github.com/JiangWeibeta/MLIC.
</details>
<details>
<summary>摘要</summary>
最近，多参照 entropy 模型已经被提出，这个模型可以捕捉通道级、本地空间和全局空间相关性。先前的工作采用了注意力来捕捉全局相关性，但是quadratic 复杂度限制了高分辨率图像编码的潜力。在这篇论文中，我们提议使用线性复杂度全球相关性捕捉，通过软max操作的分解。基于这，我们提议了MLIC++，一种学习图像压缩的线性复杂度全球相关性模型。我们的 MLIC++ 比VTM-17.0在Kodak数据集上BD-rate下降12.44%，相对PSNR测量下降。代码将在 GitHub 上发布，地址为https://github.com/JiangWeibeta/MLIC。
</details></li>
</ul>
<hr>
<h2 id="Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function"><a href="#Fast-Dust-Sand-Image-Enhancement-Based-on-Color-Correction-and-New-Membership-Function" class="headerlink" title="Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function"></a>Fast Dust Sand Image Enhancement Based on Color Correction and New Membership Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15230">http://arxiv.org/abs/2307.15230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Hakem Alsaeedi, Suha Mohammed Hadi, Yarub Alazzawi</li>
<li>for: 提高灰尘环境中图像质量和可见度</li>
<li>methods: 基于颜色修正和新成员函数，提出了一种新的图像增强模型，包括三个阶段：色差修正、雾气除除和对比和亮度提高</li>
<li>results: 通过对多个真实的灰尘图像进行测试和评估，研究发现该解决方案比现有研究更有效地除去红色和黄色投影，提供高质量和数量的灰尘图像<details>
<summary>Abstract</summary>
Images captured in dusty environments suffering from poor visibility and quality. Enhancement of these images such as sand dust images plays a critical role in various atmospheric optics applications. In this work, proposed a new model based on Color Correction and new membership function to enhance san dust images. The proposed model consists of three phases: correction of color shift, removal of haze, and enhancement of contrast and brightness. The color shift is corrected using a new membership function to adjust the values of U and V in the YUV color space. The Adaptive Dark Channel Prior (A-DCP) is used for haze removal. The stretching contrast and improving image brightness are based on Contrast Limited Adaptive Histogram Equalization (CLAHE). The proposed model tests and evaluates through many real sand dust images. The experimental results show that the proposed solution is outperformed the current studies in terms of effectively removing the red and yellow cast and provides high quality and quantity dust images.
</details>
<details>
<summary>摘要</summary>
图像 capture in 尘埃环境中，由于低可见度和质量，需要进行图像增强。在这种应用中，提出了一种基于颜色修正和新成员函数的图像增强模型。该模型包括三个阶段：颜色偏移 correction，霾除，以及对比和亮度提高。颜色偏移使用新的成员函数来调整 YUV 色彩空间中 U 和 V 的值。使用 Adaptive Dark Channel Prior (A-DCP) 进行霾除。对比和亮度提高基于 Contrast Limited Adaptive Histogram Equalization (CLAHE)。提出的模型在多个真实的沙尘图像上进行测试和评估。实验结果表明，提出的解决方案在效果上超过现有研究，可以有效地去除红色和黄色投影，并提供高质量和量的尘埃图像。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework"><a href="#Generative-AI-for-Medical-Imaging-extending-the-MONAI-Framework" class="headerlink" title="Generative AI for Medical Imaging: extending the MONAI Framework"></a>Generative AI for Medical Imaging: extending the MONAI Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15208">http://arxiv.org/abs/2307.15208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/project-monai/generativemodels">https://github.com/project-monai/generativemodels</a></li>
<li>paper_authors: Walter H. L. Pinaya, Mark S. Graham, Eric Kerfoot, Petru-Daniel Tudosiu, Jessica Dafflon, Virginia Fernandez, Pedro Sanchez, Julia Wolleb, Pedro F. da Costa, Ashay Patel, Hyungjin Chung, Can Zhao, Wei Peng, Zelong Liu, Xueyan Mei, Oeslle Lucena, Jong Chul Ye, Sotirios A. Tsaftaris, Prerna Dogra, Andrew Feng, Marc Modat, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso</li>
<li>For: The paper aims to provide a freely available open-source platform for training, evaluating, and deploying generative models for medical imaging applications.* Methods: The paper presents MONAI Generative Models, which include a variety of architectures such as diffusion models, autoregressive transformers, and GANs. The models are implemented in a generalizable fashion, allowing for extension to 2D or 3D scenarios and different modalities such as CT, MRI, and X-Ray data.* Results: The paper demonstrates the effectiveness of the proposed platform by reproducing state-of-the-art studies in a standardized way and providing pre-trained models for the community. The results show that the models can be extended to different anatomical areas and modalities, and the platform is modular and extensible, ensuring long-term maintainability and future feature extension.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文的目的是提供医疗影像领域的自由开源平台，用于训练、评估和部署生成模型。</li>
<li>methods: 论文提出了MONAI生成模型，包括各种架构，如扩散模型、自然语言转换模型和GAN等。这些模型被实现在可扩展的方式下，可以扩展到2D或3D场景和不同的感知频谱数据。</li>
<li>results: 论文通过 reproduce state-of-the-art studies in a standardized way和提供社区可用的预训练模型，证明了提案的平台的效果。结果显示，模型可以扩展到不同的解剖区域和感知频谱数据，并且平台具有可扩展和维护的特点，以便将来扩展功能。<details>
<summary>Abstract</summary>
Recent advances in generative AI have brought incredible breakthroughs in several areas, including medical imaging. These generative models have tremendous potential not only to help safely share medical data via synthetic datasets but also to perform an array of diverse applications, such as anomaly detection, image-to-image translation, denoising, and MRI reconstruction. However, due to the complexity of these models, their implementation and reproducibility can be difficult. This complexity can hinder progress, act as a use barrier, and dissuade the comparison of new methods with existing works. In this study, we present MONAI Generative Models, a freely available open-source platform that allows researchers and developers to easily train, evaluate, and deploy generative models and related applications. Our platform reproduces state-of-art studies in a standardised way involving different architectures (such as diffusion models, autoregressive transformers, and GANs), and provides pre-trained models for the community. We have implemented these models in a generalisable fashion, illustrating that their results can be extended to 2D or 3D scenarios, including medical images with different modalities (like CT, MRI, and X-Ray data) and from different anatomical areas. Finally, we adopt a modular and extensible approach, ensuring long-term maintainability and the extension of current applications for future features.
</details>
<details>
<summary>摘要</summary>
最近的生成AI技术突破有很大的进步，特别是在医学影像领域。这些生成模型具有很大的潜力，不仅可以帮助安全地分享医学数据通过生成的数据集，还可以执行多种多样的应用，如异常检测、图像到图像翻译、减噪和MRI重建。然而，由于这些模型的复杂性，其实现和复制可能会困难。这种复杂性可能会阻碍进步，成为使用障碍和阻碍新方法与现有工作的比较。在本研究中，我们介绍MONAI生成模型平台，这是一个免费、开源的平台，允许研究人员和开发人员轻松地训练、评估和部署生成模型和相关应用。我们的平台可以复制现状的研究，使用不同的架构（如扩散模型、自适应变换和GAN），并提供了社区可用的预训练模型。我们实现了这些模型的普适性，说明它们的结果可以扩展到2D或3D场景，包括医学影像不同模式（如CT、MRI和X射数据）和不同解剖区域。最后，我们采用了模块化和可扩展的方法，确保长期维护和未来特性的扩展。
</details></li>
</ul>
<hr>
<h2 id="Sparsity-aware-coding-for-single-photon-sensitive-vision-using-Selective-Sensing"><a href="#Sparsity-aware-coding-for-single-photon-sensitive-vision-using-Selective-Sensing" class="headerlink" title="Sparsity aware coding for single photon sensitive vision using Selective Sensing"></a>Sparsity aware coding for single photon sensitive vision using Selective Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15184">http://arxiv.org/abs/2307.15184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yizhou Lu, Trevor Seets, Ehsan Ahmadi, Felipe Gutierrez-Barragan, Andreas Velten</li>
<li>for: 提高图像技术的表达能力</li>
<li>methods: 利用训练数据学习偏好，优化编码策略为下游分类任务</li>
<li>results: 比传统编码策略更高的编码性能和总准确率，适用于光子计数设备下的图像分类任务<details>
<summary>Abstract</summary>
Optical coding has been widely adopted to improve the imaging techniques. Traditional coding strategies developed under additive Gaussian noise fail to perform optimally in the presence of Poisson noise. It has been observed in previous studies that coding performance varies significantly between these two noise models. In this work, we introduce a novel approach called selective sensing, which leverages training data to learn priors and optimizes the coding strategies for downstream classification tasks. By adapting to the specific characteristics of photon-counting sensors, the proposed method aims to improve coding performance under Poisson noise and enhance overall classification accuracy. Experimental and simulated results demonstrate the effectiveness of selective sensing in comparison to traditional coding strategies, highlighting its potential for practical applications in photon counting scenarios where Poisson noise are prevalent.
</details>
<details>
<summary>摘要</summary>
光学编码已广泛应用以提高成像技术。传统的编码策略在添加性 Gaussian 噪声下发展而来，不能在Poisson噪声下表现优化。先前的研究发现，编码性能在这两种噪声模型之间有很大差异。在这种工作中，我们提出了一种新的方法 called 选择感知，通过使用训练数据来学习假设和优化编码策略，以提高下游分类任务的性能。适应光子计数器的特点，提出的方法可以在Poisson噪声下提高编码性能并提高总分类精度。实验和 simulate 结果表明，选择感知方法比传统编码策略更有效，这 highlights 其在光子计数器场景中的应用潜力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/28/eess.IV_2023_07_28/" data-id="cllshxsqa008r2u88d0zu07uy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/cs.LG_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/cs.LG_2023_07_27/">cs.LG - 2023-07-27 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Federated-Model-Aggregation-via-Self-Supervised-Priors-for-Highly-Imbalanced-Medical-Image-Classification"><a href="#Federated-Model-Aggregation-via-Self-Supervised-Priors-for-Highly-Imbalanced-Medical-Image-Classification" class="headerlink" title="Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification"></a>Federated Model Aggregation via Self-Supervised Priors for Highly Imbalanced Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14959">http://arxiv.org/abs/2307.14959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fed-mas">https://github.com/xmed-lab/fed-mas</a></li>
<li>paper_authors: Marawan Elbatel, Hualiang Wang, Robert Martí, Huazhu Fu, Xiaomeng Li</li>
<li>for: 这篇论文主要针对医疗领域 Federated Learning 中的高度不均衡数据集，包括皮肤损伤和肠道图像。现有的 Federated 方法在高度不均衡数据集上主要是优化全局模型，而不考虑医疗影像中的内部类别差异。</li>
<li>methods: 本论文使用公共可用的自动学习助记网络（如 MoCo-V2）在每个客户端上进行本地预训练，并发现使用共享 auxiliary 预训练模型可以获得一致异常度测量。基于这些发现，我们 derivate了一种动态均衡模型聚合方法（MAS）来导引全局模型优化。</li>
<li>results: Fed-MAS 可以与不同的本地学习方法结合使用，以实现高度可靠和无偏见的全局模型。我们的代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/Fed-MAS%7D">https://github.com/xmed-lab/Fed-MAS}</a> 上找到。<details>
<summary>Abstract</summary>
In the medical field, federated learning commonly deals with highly imbalanced datasets, including skin lesions and gastrointestinal images. Existing federated methods under highly imbalanced datasets primarily focus on optimizing a global model without incorporating the intra-class variations that can arise in medical imaging due to different populations, findings, and scanners. In this paper, we study the inter-client intra-class variations with publicly available self-supervised auxiliary networks. Specifically, we find that employing a shared auxiliary pre-trained model, like MoCo-V2, locally on every client yields consistent divergence measurements. Based on these findings, we derive a dynamic balanced model aggregation via self-supervised priors (MAS) to guide the global model optimization. Fed-MAS can be utilized with different local learning methods for effective model aggregation toward a highly robust and unbiased global model. Our code is available at \url{https://github.com/xmed-lab/Fed-MAS}.
</details>
<details>
<summary>摘要</summary>
医疗领域中，联合学习经常面临高度不均衡的数据集，包括皮肤病变和Digestive tract影像。现有的联合方法在高度不均衡数据集上主要是优化全球模型，而不考虑医疗影像中的内部类别差异，这些差异可能 arise due to different populations, findings, and scanners。在这篇论文中，我们研究了客户端间内部类别差异，使用公共可用的无监督辅助网络。我们发现，在每个客户端上使用共享的辅助预训练模型，如MoCo-V2，可以获得一致的分化度测量。基于这些发现，我们 derive了一种动态平衡的模型聚合方法（MAS），以导引全球模型优化。Fed-MAS可以与不同的本地学习方法结合使用，以实现高度可靠和无偏的全球模型。我们的代码可以在 GitHub上找到：https://github.com/xmed-lab/Fed-MAS。
</details></li>
</ul>
<hr>
<h2 id="Multi-Source-Domain-Adaptation-through-Dataset-Dictionary-Learning-in-Wasserstein-Space"><a href="#Multi-Source-Domain-Adaptation-through-Dataset-Dictionary-Learning-in-Wasserstein-Space" class="headerlink" title="Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space"></a>Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14953">http://arxiv.org/abs/2307.14953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eddardd/demo-dadil">https://github.com/eddardd/demo-dadil</a></li>
<li>paper_authors: Eduardo Fernandes Montesuma, Fred Ngolè Mboula, Antoine Souloumiac</li>
<li>for: 解决多源领域适应（MSDA）问题，即在多个标注源频道中传递知识，并且避免数据分布偏移。</li>
<li>methods: 提出了一种基于字典学习和最优运输的MSDA框架。对每个频道进行解释，表示每个频道为一个 Wasserstein 质量中心，并提出了一种基于字典的两种新方法：DaDil-R 和 DaDiL-E。</li>
<li>results: 在 Caltech-Office、Office 31 和 CRWU 三个标准测试集上进行评估，并与之前的状态前进行了3.15%、2.29% 和 7.71% 的改进。最后，我们表明了 interpolations 在学习的atom集中的 Wasserstein 树可以泛化到目标频道。<details>
<summary>Abstract</summary>
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance. Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain.
</details>
<details>
<summary>摘要</summary>
We represent each domain in MSDA as an empirical distribution, and express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We then propose a novel algorithm, DaDiL, for learning via mini-batches, which involves (i) learning atom distributions, and (ii) computing a matrix of barycentric coordinates.Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, which uses the reconstruction of labeled samples in the target domain, and DaDiL-E, which uses the ensembling of classifiers learned on atom distributions. We evaluate our methods on three benchmark datasets: Caltech-Office, Office 31, and CRWU, and show that our approach achieves state-of-the-art performance, improving upon previous results by 3.15%, 2.29%, and 7.71% in classification accuracy.Finally, we demonstrate that interpolations in the Wasserstein hull of learned atoms can provide data that can generalize to the target domain, providing a promising avenue for future research.
</details></li>
</ul>
<hr>
<h2 id="Network-Fault-tolerant-and-Byzantine-resilient-Social-Learning-via-Collaborative-Hierarchical-Non-Bayesian-Learning"><a href="#Network-Fault-tolerant-and-Byzantine-resilient-Social-Learning-via-Collaborative-Hierarchical-Non-Bayesian-Learning" class="headerlink" title="Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning"></a>Network Fault-tolerant and Byzantine-resilient Social Learning via Collaborative Hierarchical Non-Bayesian Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14952">http://arxiv.org/abs/2307.14952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Connor Mclaughlin, Matthew Ding, Denis Edogmus, Lili Su</li>
<li>for:  addresses the problem of non-Bayesian learning over networks that are vulnerable to communication failures and external adversarial attacks.</li>
<li>methods:  proposes a hierarchical robust push-sum algorithm, a sparse information fusion rule, and a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.</li>
<li>results:  achieves average consensus despite frequent packet-dropping link failures and external adversarial attacks, and solves the non-Bayesian learning problem via running multiple dynamics.Here is the simplified Chinese text:</li>
<li>for:  Addresses 非托 bayesian 学习问题，即在受到通信故障和外部恶意攻击的网络上。</li>
<li>methods: 提出了层次系统架构，并使用了 packets 损失链接故障和外部恶意攻击的鲁棒推 sum 算法， sparse 信息融合规则，以及一种 packet-dropping  fault-tolerant 非托 bayesian 学习算法，并提供了可证明的收敛保证。</li>
<li>results:  Achieves 平均consensus ，并在 packet-dropping 链接故障和外部恶意攻击下解决了非托 bayesian 学习问题，并通过运行多个动力来解决这个问题。<details>
<summary>Abstract</summary>
As the network scale increases, existing fully distributed solutions start to lag behind the real-world challenges such as (1) slow information propagation, (2) network communication failures, and (3) external adversarial attacks. In this paper, we focus on hierarchical system architecture and address the problem of non-Bayesian learning over networks that are vulnerable to communication failures and adversarial attacks. On network communication, we consider packet-dropping link failures.   We first propose a hierarchical robust push-sum algorithm that can achieve average consensus despite frequent packet-dropping link failures. We provide a sparse information fusion rule between the parameter server and arbitrarily selected network representatives. Then, interleaving the consensus update step with a dual averaging update with Kullback-Leibler (KL) divergence as the proximal function, we obtain a packet-dropping fault-tolerant non-Bayesian learning algorithm with provable convergence guarantees.   On external adversarial attacks, we consider Byzantine attacks in which the compromised agents can send maliciously calibrated messages to others (including both the agents and the parameter server). To avoid the curse of dimensionality of Byzantine consensus, we solve the non-Bayesian learning problem via running multiple dynamics, each of which only involves Byzantine consensus with scalar inputs. To facilitate resilient information propagation across sub-networks, we use a novel Byzantine-resilient gossiping-type rule at the parameter server.
</details>
<details>
<summary>摘要</summary>
随着网络规模的增加，现有的完全分布式解决方案开始落后于实际世界中的挑战，如（1）慢速信息传播，（2）网络通信失败，以及（3）外部敌意攻击。在这篇论文中，我们关注层次系统架构，并解决非bayesian学习问题，面临网络通信失败和外部敌意攻击。对于网络通信，我们考虑 packet-dropping 链接故障。我们首先提出一种层次可靠推轮算法，可以在 packet-dropping 链接故障情况下达到平均共识。我们提供一种稀缺信息融合规则，使得参数服务器和随机选择的网络代表进行简单的信息交换。然后，将整体协同更新步骤与 dual averaging 更新步骤结合，使得 packet-dropping 故障tolerant 非bayesian 学习算法具有可证明的收敛保证。对于外部敌意攻击，我们考虑 Byzantine 攻击，在这种情况下，被入侵的代理会发送卑势化的消息到其他代理（包括代理和参数服务器）。为了避免 Byzantine 协同收敛的维度约束，我们使用多个动态，每个动态仅涉及 Byzantine 协同，并使用一种新的 Byzantine 抗性的聊天规则。为了促进网络下扩散信息的稳定传递，我们使用一种新的 Byzantine 抗性的聊天规则。
</details></li>
</ul>
<hr>
<h2 id="A-Self-Adaptive-Penalty-Method-for-Integrating-Prior-Knowledge-Constraints-into-Neural-ODEs"><a href="#A-Self-Adaptive-Penalty-Method-for-Integrating-Prior-Knowledge-Constraints-into-Neural-ODEs" class="headerlink" title="A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs"></a>A Self-Adaptive Penalty Method for Integrating Prior Knowledge Constraints into Neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14940">http://arxiv.org/abs/2307.14940</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferrás</li>
<li>for: 模拟自然系统的持续动力学行为</li>
<li>methods: 使用神经ordinary differential equation（Neural ODE）模型</li>
<li>results: 提出了一种自适应罚函数算法，以便模拟受限制的自然系统Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to model the continuous dynamics of natural systems, specifically using Neural ODEs.</li>
<li>methods: The paper proposes a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. This algorithm dynamically adjusts the penalty parameters based on the data.</li>
<li>results: The proposed approach is validated using three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments show that the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.<details>
<summary>Abstract</summary>
The continuous dynamics of natural systems has been effectively modelled using Neural Ordinary Differential Equations (Neural ODEs). However, for accurate and meaningful predictions, it is crucial that the models follow the underlying rules or laws that govern these systems. In this work, we propose a self-adaptive penalty algorithm for Neural ODEs to enable modelling of constrained natural systems. The proposed self-adaptive penalty function can dynamically adjust the penalty parameters. The explicit introduction of prior knowledge helps to increase the interpretability of Neural ODE -based models. We validate the proposed approach by modelling three natural systems with prior knowledge constraints: population growth, chemical reaction evolution, and damped harmonic oscillator motion. The numerical experiments and a comparison with other penalty Neural ODE approaches and \emph{vanilla} Neural ODE, demonstrate the effectiveness of the proposed self-adaptive penalty algorithm for Neural ODEs in modelling constrained natural systems. Moreover, the self-adaptive penalty approach provides more accurate and robust models with reliable and meaningful predictions.
</details>
<details>
<summary>摘要</summary>
自然系统的连续动力学已经非常有效地使用神经常微方程（Neural ODE）来模拟。但是，为了获得准确和有意义的预测，模型需要遵循下面的规则或法律来控制这些系统。在这种工作中，我们提议一种自适应罚函数算法来使神经常微方程模型受到约束。这种自适应罚函数可以动态调整罚参数。通过直接引入先知知识，可以增加神经常微方程模型的解释性。我们验证了提议的方法，通过模拟三个自然系统受约束的模型：人口增长、化学反应演化和抑制响应振荡。计算实验和与其他罚Neural ODE方法和"vanilla"Neural ODE进行比较，表明提议的自适应罚函数算法对神经常微方程模型的受约束模型具有更高的有效性和可靠性。此外，自适应罚approach还可以提供更准确和稳定的预测。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Interaction-Aware-Interval-Analysis-of-Neural-Network-Feedback-Loops"><a href="#Efficient-Interaction-Aware-Interval-Analysis-of-Neural-Network-Feedback-Loops" class="headerlink" title="Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops"></a>Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14938">http://arxiv.org/abs/2307.14938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saber Jafarpour, Akash Harapanahalli, Samuel Coogan</li>
<li>for: 本文提出一种 computationally efficient 框架，用于计算 interval reachability 系统中 neural network 控制器的行为下的不确定性。</li>
<li>methods: 本文使用 inclusion functions 将开loop系统和神经网络控制器嵌入更大的 embedding system 中，以便使用单个轨迹来拟合原始系统的行为下 uncertainty。提出了两种不同的构建关闭loop embedding system 的方法，它们分别考虑了系统和控制器之间的交互方式。</li>
<li>results: 本文在 Python 框架 ReachMM 中实现了这种方法，并在各种例子和 benchmark 上示出了高效性和可扩展性。<details>
<summary>Abstract</summary>
In this paper, we propose a computationally efficient framework for interval reachability of systems with neural network controllers. Our approach leverages inclusion functions for the open-loop system and the neural network controller to embed the closed-loop system into a larger-dimensional embedding system, where a single trajectory over-approximates the original system's behavior under uncertainty. We propose two methods for constructing closed-loop embedding systems, which account for the interactions between the system and the controller in different ways. The interconnection-based approach considers the worst-case evolution of each coordinate separately by substituting the neural network inclusion function into the open-loop inclusion function. The interaction-based approach uses novel Jacobian-based inclusion functions to capture the first-order interactions between the open-loop system and the controller by leveraging state-of-the-art neural network verifiers. Finally, we implement our approach in a Python framework called ReachMM to demonstrate its efficiency and scalability on benchmarks and examples ranging to $200$ state dimensions.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种计算效率高的扩Interval可达性框架，用于系统控制器是神经网络的系统。我们的方法利用包含函数来包含开 Loop系统和神经网络控制器，将关闭Loop系统嵌入到更大的嵌入系统中，以便单个轨迹过度度量原始系统的行为下 uncertainty。我们提出了两种方法用于构建关闭Loop嵌入系统，这两种方法均考虑了系统和控制器之间的互动。基于连接的方法将每个坐标的最坏情况演化分别substitue神经网络包含函数到开 Loop包含函数中。基于互动的方法使用新的Jacobian包含函数来捕捉开 Loop系统和控制器之间的首次互动，通过利用现有神经网络验证器。最后，我们在Python框架ReachMM中实现了我们的方法，以示其效率和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="PanGu-Coder2-Boosting-Large-Language-Models-for-Code-with-Ranking-Feedback"><a href="#PanGu-Coder2-Boosting-Large-Language-Models-for-Code-with-Ranking-Feedback" class="headerlink" title="PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback"></a>PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14936">http://arxiv.org/abs/2307.14936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, Qianxiang Wang</li>
<li>for: 本研究旨在提高预训练Code大语言模型（Code LLM）的代码生成性能。</li>
<li>methods: 本研究提出了一种新的RRTF（ Rank Responses to align Test&amp;Teacher Feedback）框架，用于有效地和高效地提高预训练Code LLM的代码生成性能。</li>
<li>results: 在OpenAI HumanEval benchmark上，PanGu-Coder2实现了62.20%的pass@1分数，并在CoderEval和LeetCode benchmark上表现出色，Consistently outperforming all previous Code LLMs。<details>
<summary>Abstract</summary>
Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型 для代码（代码 LLM）正在繁荣。每周新发布的新型号都在代码生成任务上表现出色。各种方法被提出来提高预训练代码 LLM 的代码生成性能，例如监督微调、指令优化、强化学习等。在这篇论文中，我们提出了一种新的 RRTF（排名回应对测试&教师反馈）框架，可以有效地和高效地提高预训练大型语言模型的代码生成性能。在这个框架下，我们介绍了 PanGu-Coder2，它在 OpenAI HumanEval benchmark 上取得了 62.20% 的 pass@1 分数。此外，通过对 CoderEval 和 LeetCode  benchmark 进行广泛的评估，我们表明 PanGu-Coder2 在所有前一代代码 LLM 之上卓越表现。
</details></li>
</ul>
<hr>
<h2 id="Solving-Data-Quality-Problems-with-Desbordante-a-Demo"><a href="#Solving-Data-Quality-Problems-with-Desbordante-a-Demo" class="headerlink" title="Solving Data Quality Problems with Desbordante: a Demo"></a>Solving Data Quality Problems with Desbordante: a Demo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14935">http://arxiv.org/abs/2307.14935</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Chernishev, Michael Polyntsov, Anton Chizhov, Kirill Stupakov, Ilya Shchuckin, Alexander Smirnov, Maxim Strutovsky, Alexey Shlyonskikh, Mikhail Firsov, Stepan Manannikov, Nikita Bobrov, Daniil Goncharov, Ilia Barutkin, Vladislav Shalnev, Kirill Muraviev, Anna Rakhmukova, Dmitriy Shcheka, Anton Chernikov, Mikhail Vyrodov, Yaroslav Kurbatov, Maxim Fofanov, Sergei Belokonnyi, Pavel Anosov, Arthur Saliou, Eduard Gaisin, Kirill Smirnov</li>
<li>for: 提高现代数据驱动行业中数据 profiling 的效率和可靠性，并提供对现有工具的良好整合。</li>
<li>methods: 使用可扩展的 C++ 核心，实现高效缓存和快速排序，并提供可靠的数据探索和检测功能。</li>
<li>results: 通过多种实际场景的示例，包括 typo 检测、数据重复检测和数据异常检测，表明 Desbordante 可以高效地解决不同的数据质量问题。<details>
<summary>Abstract</summary>
Data profiling is an essential process in modern data-driven industries. One of its critical components is the discovery and validation of complex statistics, including functional dependencies, data constraints, association rules, and others.   However, most existing data profiling systems that focus on complex statistics do not provide proper integration with the tools used by contemporary data scientists. This creates a significant barrier to the adoption of these tools in the industry. Moreover, existing systems were not created with industrial-grade workloads in mind. Finally, they do not aim to provide descriptive explanations, i.e. why a given pattern is not found. It is a significant issue as it is essential to understand the underlying reasons for a specific pattern's absence to make informed decisions based on the data.   Because of that, these patterns are effectively rest in thin air: their application scope is rather limited, they are rarely used by the broader public. At the same time, as we are going to demonstrate in this presentation, complex statistics can be efficiently used to solve many classic data quality problems.   Desbordante is an open-source data profiler that aims to close this gap. It is built with emphasis on industrial application: it is efficient, scalable, resilient to crashes, and provides explanations. Furthermore, it provides seamless Python integration by offloading various costly operations to the C++ core, not only mining.   In this demonstration, we show several scenarios that allow end users to solve different data quality problems. Namely, we showcase typo detection, data deduplication, and data anomaly detection scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified ChineseData profiling 是现代数据驱动行业中的一个重要过程。其中一个关键组件是发现和验证复杂统计，包括函数依赖关系、数据约束、关联规则等。然而，大多数现有的数据 profiling 系统，它们专注于复杂统计，并不提供适合当代数据科学家使用的合理集成。这创造了一个很大的障碍物，阻碍了这些工具在行业中的普及。此外，现有系统并不是为现代工作荟载设计的。最重要的是，它们不提供描述性解释，即为什么某个模式不存在。这是一个重要的问题，因为需要理解数据下面的根本原因，才能基于数据做出 Informed 决策。因此，这些模式效果是“浮空”的，它们的应用范围很限定，并且只有少数人使用。在这个演示中，我们将展示一些使用复杂统计解决不同数据质量问题的场景。具体来说，我们将展示 typo 检测、数据减重、数据异常检测等场景。Desbordante 是一个开源的数据 profiler，它专注于工业应用。它具有高效、可扩展、可靠性和描述性解释等特点。此外，它提供了PYTHON 集成，通过将多种费时操作委托给 C++ 核心来实现，不仅是探采。在这个演示中，我们将展示 Desbordante 如何解决不同数据质量问题。>>Here's the translation:<<SYS>>现代数据驱动行业中的一个重要过程是数据 profiling。这个过程中的一个关键组件是发现和验证复杂统计，例如函数依赖关系、数据约束、关联规则等。然而，大多数现有的数据 profiling 系统不适合当代数据科学家使用，因为它们不提供适合工业应用的合理集成。这创造了一个很大的障碍物，阻碍了这些工具在行业中的普及。现有系统并不是为现代工作荟载设计的。最重要的是，它们不提供描述性解释，即为什么某个模式不存在。这是一个重要的问题，因为需要理解数据下面的根本原因，才能基于数据做出 Informed 决策。因此，这些模式效果是“浮空”的，它们的应用范围很限定，并且只有少数人使用。在这个演示中，我们将展示一些使用复杂统计解决不同数据质量问题的场景。具体来说，我们将展示 typo 检测、数据减重、数据异常检测等场景。Desbordante 是一个开源的数据 profiler，它专注于工业应用。它具有高效、可扩展、可靠性和描述性解释等特点。此外，它提供了PYTHON 集成，通过将多种费时操作委托给 C++ 核心来实现，不仅是探采。在这个演示中，我们将展示 Desbordante 如何解决不同数据质量问题。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Model-Based-Shielding-for-Safe-Reinforcement-Learning"><a href="#Approximate-Model-Based-Shielding-for-Safe-Reinforcement-Learning" class="headerlink" title="Approximate Model-Based Shielding for Safe Reinforcement Learning"></a>Approximate Model-Based Shielding for Safe Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00707">http://arxiv.org/abs/2308.00707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sacktock/ambs">https://github.com/sacktock/ambs</a></li>
<li>paper_authors: Alexander W. Goodall, Francesco Belardinelli</li>
<li>for: 这篇论文目的是为了解决RL在实际世界中应用的问题，特别是在安全关键系统中。</li>
<li>methods: 这篇论文提出了一种名为approximate model-based shielding（AMBS）的原则，用于验证RL策略是否符合给定的安全约束。AMBS不需要知道系统的安全相关动态。</li>
<li>results: 论文提供了一个强有力的理论依据，并在一组Atari游戏中表明AMBS在安全意识方面表现出色，超过其他安全意识方法。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) has shown great potential for solving complex tasks in a variety of domains. However, applying RL to safety-critical systems in the real-world is not easy as many algorithms are sample-inefficient and maximising the standard RL objective comes with no guarantees on worst-case performance. In this paper we propose approximate model-based shielding (AMBS), a principled look-ahead shielding algorithm for verifying the performance of learned RL policies w.r.t. a set of given safety constraints. Our algorithm differs from other shielding approaches in that it does not require prior knowledge of the safety-relevant dynamics of the system. We provide a strong theoretical justification for AMBS and demonstrate superior performance to other safety-aware approaches on a set of Atari games with state-dependent safety-labels.
</details>
<details>
<summary>摘要</summary>
利用增强学习（RL）解决复杂任务的潜力已经被证明了，但是在实际世界中应用RL到安全关键系统上并不容易，因为许多算法是采样不充分的，并且最大化标准RL目标不提供最坏情况性能的保证。在这篇论文中，我们提出了一种名为准确模型基于遮盾（AMBS）的原则正确的遮盾算法，用于验证RL策略与一组给定的安全约束之间的关系。我们的算法与其他安全意识的方法不同，不需要知道系统的安全相关动力学。我们提供了强有力的理论基础，并在一组Atari游戏中的状态依赖安全标签上示出了我们的方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Polyphonic-Multitrack-Music-Generation"><a href="#Graph-based-Polyphonic-Multitrack-Music-Generation" class="headerlink" title="Graph-based Polyphonic Multitrack Music Generation"></a>Graph-based Polyphonic Multitrack Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14928">http://arxiv.org/abs/2307.14928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emanuelecosenza/polyphemus">https://github.com/emanuelecosenza/polyphemus</a></li>
<li>paper_authors: Emanuele Cosenza, Andrea Valenti, Davide Bacciu</li>
<li>for: 这篇论文是为了研究用深度学习系统来生成乐曲，特别是使用图表来模型多重符号音乐。</li>
<li>methods: 该论文提出了一种新的图表表示法，并使用深度Variational Autoencoder来生成乐曲的结构和内容。这种方法可以根据指定的乐器和时间来控制生成过程，从而实现人机交互式的音乐合作。</li>
<li>results: 经过训练的模型能够生成愉悦的短和长乐曲，并可以实际地 interpolate  между它们，生成具有律动和和声一致性的乐曲。图表可视化表明模型可以将其隐藏空间按照知道的音乐概念进行组织。<details>
<summary>Abstract</summary>
Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent. Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts.
</details>
<details>
<summary>摘要</summary>
图可以用来模拟多音轨 симвоlic music， где每个音和每个和声可以在不同的音乐层次结构中相互关联。然而，现有的研究很少考虑图表 Representation在深度学习系统中的应用。这篇论文填补了这个空白，并 introduce a novel graph representation for music and a deep Variational Autoencoder that generates the structure and content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music.通过分离音乐图的结构和内容，可以根据特定的乐器和时间指定生成。这打开了一种新的人机交互方式，即音乐合作创作。经过训练模型于现有的 MIDI 数据集，实验显示，模型能够生成愉悦的短和长乐 sequences，并可以实际地在之间 interpolate，生成具有和谐和节奏的音乐。最后，Embeddings 的可视化表明，模型能够在 latent space 中组织 according to known musical concepts。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Performance-of-Deep-Learning-Model-for-Material-Segmentation-on-Two-HPC-Systems"><a href="#Benchmarking-Performance-of-Deep-Learning-Model-for-Material-Segmentation-on-Two-HPC-Systems" class="headerlink" title="Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems"></a>Benchmarking Performance of Deep Learning Model for Material Segmentation on Two HPC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14921">http://arxiv.org/abs/2307.14921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Warren R. Williams, S. Ross Glandon, Luke L. Morris, Jing-Ru C. Cheng</li>
<li>for: 本研究的目的是提供高性能HPC系统的性能评估方法，以便提高任务调度器的性能。</li>
<li>methods: 本研究开发了一种基于机器学习模型的benchmark工具，该工具在GPU加速节点上进行物质分 segmentation分析，并使用MMdnn工具库和MINC-2500数据集。</li>
<li>results: 对两个ERDC DSRC系统（Onyx和Vulcanite）进行性能测试，结果显示，虽然Vulcanite在许多benchmark中具有更快的模型时间，但它也更容易受到环境因素的影响，导致性能下降，而Onyx的模型时间在benchmark中具有更高的一致性。<details>
<summary>Abstract</summary>
Performance Benchmarking of HPC systems is an ongoing effort that seeks to provide information that will allow for increased performance and improve the job schedulers that manage these systems. We develop a benchmarking tool that utilizes machine learning models and gathers performance data on GPU-accelerated nodes while they perform material segmentation analysis. The benchmark uses a ML model that has been converted from Caffe to PyTorch using the MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on two ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while Vulcanite has faster model times in a large number of benchmarks, and it is also more subject to some environmental factors that can cause performances slower than Onyx. In contrast the model times from Onyx are consistent across benchmarks.
</details>
<details>
<summary>摘要</summary>
高性能计算机系统的性能测试是一项不断进行的努力，旨在提供可以提高性能和改进作业调度器的信息。我们开发了一个性能测试工具，该工具利用机器学习模型并在加速节点上进行材料分割分析时收集性能数据。这个benchmark使用通过Caffe到PyTorch的MMdnn工具包和MINC-2500数据集 converts的ML模型。在ERDC DSRC系统Onyx和Vulcanite上进行性能测试，数据显示，虽然Vulcanite在许多benchmark中具有更快的模型时间，但它也更容易受到环境因素的影响，导致性能 slower than Onyx。相比之下，Onyx上的模型时间在benchmark中具有一致性。
</details></li>
</ul>
<hr>
<h2 id="NSA-Naturalistic-Support-Artifact-to-Boost-Network-Confidence"><a href="#NSA-Naturalistic-Support-Artifact-to-Boost-Network-Confidence" class="headerlink" title="NSA: Naturalistic Support Artifact to Boost Network Confidence"></a>NSA: Naturalistic Support Artifact to Boost Network Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14917">http://arxiv.org/abs/2307.14917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijith Sharma, Phil Munz, Apurva Narayan<br>for: 这种研究旨在提高视觉人工智能系统对自然环境的抗衰减能力。methods: 该研究提议使用自然支持物件（NSA）来提高预测confidence分数。NSA通过DC-GAN进行artifact Training，以确保在场景中具有高视觉准确性。results: 对于Imagenette dataset中的自然衰减，NSA能够提高预测confidence分数四倍。此外，NSA还能够提高对抗攻击的准确率平均提高8%。此外，通过精度图来分析NSA的作用，可以了解它们如何提高预测confidence。<details>
<summary>Abstract</summary>
Visual AI systems are vulnerable to natural and synthetic physical corruption in the real-world. Such corruption often arises unexpectedly and alters the model's performance. In recent years, the primary focus has been on adversarial attacks. However, natural corruptions (e.g., snow, fog, dust) are an omnipresent threat to visual AI systems and should be considered equally important. Many existing works propose interesting solutions to train robust models against natural corruption. These works either leverage image augmentations, which come with the additional cost of model training, or place suspicious patches in the scene to design unadversarial examples. In this work, we propose the idea of naturalistic support artifacts (NSA) for robust prediction. The NSAs are shown to be beneficial in scenarios where model parameters are inaccessible and adding artifacts in the scene is feasible. The NSAs are natural looking objects generated through artifact training using DC-GAN to have high visual fidelity in the scene. We test against natural corruptions on the Imagenette dataset and observe the improvement in prediction confidence score by four times. We also demonstrate NSA's capability to increase adversarial accuracy by 8\% on average. Lastly, we qualitatively analyze NSAs using saliency maps to understand how they help improve prediction confidence.
</details>
<details>
<summary>摘要</summary>
视觉人工智能系统容易受到自然和人工物理损害的影响，这种损害通常会不断发生，影响模型的性能。在过去几年，主要关注点是对抗性攻击。然而，自然损害（如雪、雾、尘埃）对视觉人工智能系统是一种普遍存在的威胁，应该得到同等重视。许多现有的研究提出了许多有趣的解决方案，如通过图像扩展来训练Robust模型，或者在场景中添加不可信的质地来设计不可 adversarial例子。在这个工作中，我们提出了自然支持物件（NSA）的想法，用于Robust预测。NSA通过使用DC-GAN进行 artifact训练，在场景中生成高可识别度的自然looking对象。我们对Imagenette数据集进行了对自然损害的测试，并观察到预测信心分数提高四倍。此外，我们还证明NSA可以提高对抗率平均8%。最后，我们使用saliency map来 качеitative分析NSA，以便更好地理解它们如何提高预测 confidence。
</details></li>
</ul>
<hr>
<h2 id="Clustering-of-illustrations-by-atmosphere-using-a-combination-of-supervised-and-unsupervised-learning"><a href="#Clustering-of-illustrations-by-atmosphere-using-a-combination-of-supervised-and-unsupervised-learning" class="headerlink" title="Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning"></a>Clustering of illustrations by atmosphere using a combination of supervised and unsupervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15099">http://arxiv.org/abs/2307.15099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keisuke Kubota, Masahiro Okuda<br>for:这 paper 是为了解决图像描述中的氛围问题，通过将图像分类为不同的氛围来提高搜索和推荐的效果。methods:这 paper 使用了双向学习和自动标注来解决氛围分类问题，并使用了supervised learning和Unsupervised learning来获得特征向量。results:实验结果表明，这 paper 的方法可以比传统方法更好地对图像进行分类，并且可以准确地捕捉图像中的氛围特征。<details>
<summary>Abstract</summary>
The distribution of illustrations on social media, such as Twitter and Pixiv has increased with the growing popularity of animation, games, and animated movies. The "atmosphere" of illustrations plays an important role in user preferences. Classifying illustrations by atmosphere can be helpful for recommendations and searches. However, assigning clear labels to the elusive "atmosphere" and conventional supervised classification is not always practical. Furthermore, even images with similar colors, edges, and low-level features may not have similar atmospheres, making classification based on low-level features challenging. In this paper, this problem is solved using both supervised and unsupervised learning with pseudo-labels. The feature vectors are obtained using the supervised method with pseudo-labels that contribute to an ambiguous atmosphere. Further, clustering is performed based on these feature vectors. Experimental analyses show that our method outperforms conventional methods in human-like clustering on datasets manually classified by humans.
</details>
<details>
<summary>摘要</summary>
社交媒体上的插图分布量增加，与动画、游戏和动画电影的流行相关。插图的“氛围”在用户喜好中扮演重要角色。根据氛围进行分类可以有助于推荐和搜索。但是，将氛围论坛到明确的标签是不实用的。此外，即使颜色、边缘和低级特征都相似，插图的氛围可能不同，从而使基于低级特征的分类困难。在这篇论文中，我们解决了这个问题，使用了超级vised和无监督学习，并使用pseudo标签。通过这些特征向量，我们进行了团 clustering。实验分析表明，我们的方法在人类化 clustering 中超出了传统方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Session-Based-Transformer-Recommendations-using-Optimized-Negative-Sampling-and-Loss-Functions"><a href="#Scaling-Session-Based-Transformer-Recommendations-using-Optimized-Negative-Sampling-and-Loss-Functions" class="headerlink" title="Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions"></a>Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14906">http://arxiv.org/abs/2307.14906</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/otto-de/tron">https://github.com/otto-de/tron</a></li>
<li>paper_authors: Timo Wilm, Philipp Normann, Sophie Baumeister, Paul-Vincent Kobow</li>
<li>for: 这篇论文是为了提出一种可扩展的会话基于转换器推荐算法，以解决现有模型的缺乏扩展性和性能问题。</li>
<li>methods: 该论文使用了最佳负样本选择和列式损失函数，以提高推荐准确性。</li>
<li>results: 对于大规模电商数据集，TRON显示了与当前方法相比的推荐质量提高，同时保持与SASRec相似的训练速度。实际应用中，TRON实现了与SASRec相比18.14%的点击率提高。<details>
<summary>Abstract</summary>
This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.
</details>
<details>
<summary>摘要</summary>
这个研究介绍了TRON，一种可扩展的会话基于转移器推荐器，使用优化的负样本选择。为了解决现有模型如SASRec和GRU4Rec+的可扩展性和性能限制，TRON integrates top-k负样本选择和listwise损失函数，以提高推荐准确性。对于相关的大规模电商数据集，评估表明TRON可以超越当前方法的推荐质量，同时保持与SASRec相似的训练速度。一次实际A/B测试显示，TRON相比SASRec提高了18.14%的点击率，这highlights TRON在实际场景中的潜力。为了进一步研究，我们在https://github.com/otto-de/TRON上提供了源代码，并在https://github.com/otto-de/recsys-dataset上提供了匿名数据集。
</details></li>
</ul>
<hr>
<h2 id="CodeLens-An-Interactive-Tool-for-Visualizing-Code-Representations"><a href="#CodeLens-An-Interactive-Tool-for-Visualizing-Code-Representations" class="headerlink" title="CodeLens: An Interactive Tool for Visualizing Code Representations"></a>CodeLens: An Interactive Tool for Visualizing Code Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14902">http://arxiv.org/abs/2307.14902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuejun Guo, Seifeddine Bettaieb, Qiang Hu, Yves Le Traon, Qiang Tang</li>
<li>for: 提供了一个可视化代码表示的工具，帮助开发者快速理解不同类型的代码表示，以及代码表示所代表的输入。</li>
<li>methods: 支持多种程式语言，包括Java、Python和JavaScript，并且支持四种代码表示方法，包括字符串序列、抽象 syntax tree (AST)、资料流graph (DFG) 和控制流graph (CFG)。</li>
<li>results: 为开发者提供了一个可用于多种代码表示的可视化互动环境，帮助开发者快速理解代码表示，并且获取代码表示所代表的输入。<details>
<summary>Abstract</summary>
Representing source code in a generic input format is crucial to automate software engineering tasks, e.g., applying machine learning algorithms to extract information. Visualizing code representations can further enable human experts to gain an intuitive insight into the code. Unfortunately, as of today, there is no universal tool that can simultaneously visualise different types of code representations. In this paper, we introduce a tool, CodeLens, which provides a visual interaction environment that supports various representation methods and helps developers understand and explore them. CodeLens is designed to support multiple programming languages, such as Java, Python, and JavaScript, and four types of code representations, including sequence of tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow graph (CFG). By using CodeLens, developers can quickly visualize the specific code representation and also obtain the represented inputs for models of code. The Web-based interface of CodeLens is available at http://www.codelens.org. The demonstration video can be found at http://www.codelens.org/demo.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将源代码表示format化为通用输入格式是软件工程任务的关键之一，例如应用机器学习算法提取信息。可视化代码表示可以帮助人工专家获得直观印象。 unfortunately，到目前为止，没有一个通用工具可同时可视化不同类型的代码表示。在这篇论文中，我们介绍了一个工具——CodeLens，它提供了一个可视化交互环境，支持多种代码表示方法，帮助开发者理解和探索代码。CodeLens支持多种编程语言，如Java、Python和JavaScript，以及四种代码表示方法，包括序列化token、抽象语法树（AST）、数据流图（DFG）和控制流图（CFG）。通过使用CodeLens，开发者可快速可视化特定的代码表示，并获得代码表示的输入数据。Web-based Interface of CodeLens可在http://www.codelens.org上获取。示例视频可在http://www.codelens.org/demo找到。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Improved-Synthetic-Aperture-Sonar-Target-Recognition"><a href="#Self-Supervised-Learning-for-Improved-Synthetic-Aperture-Sonar-Target-Recognition" class="headerlink" title="Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition"></a>Self-Supervised Learning for Improved Synthetic Aperture Sonar Target Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15098">http://arxiv.org/abs/2307.15098</a></li>
<li>repo_url: None</li>
<li>paper_authors: BW Sheffield</li>
<li>for: 本研究探讨了基于自动学习（SSL）的目标识别方法在激光孔干成像中的应用，以解决水下环境下传统计算机视觉技术的不足。</li>
<li>methods: 本研究使用了两种知名的SSL算法：MoCov2和BYOL，以及一个常见的supervised learning模型：ResNet18，进行比较。</li>
<li>results: 结果表明，当使用少量标签时，SSL模型可以超过完全监督学习模型，但当使用所有标签时，它们不能超过完全监督学习模型。这些结果表明SSL可以作为监督学习的可靠替代方案，并且可以降低数据标签的时间和成本。<details>
<summary>Abstract</summary>
This study explores the application of self-supervised learning (SSL) for improved target recognition in synthetic aperture sonar (SAS) imagery. The unique challenges of underwater environments make traditional computer vision techniques, which rely heavily on optical camera imagery, less effective. SAS, with its ability to generate high-resolution imagery, emerges as a preferred choice for underwater imaging. However, the voluminous high-resolution SAS data presents a significant challenge for labeling; a crucial step for training deep neural networks (DNNs).   SSL, which enables models to learn features in data without the need for labels, is proposed as a potential solution to the data labeling challenge in SAS. The study evaluates the performance of two prominent SSL algorithms, MoCov2 and BYOL, against the well-regarded supervised learning model, ResNet18, for binary image classification tasks. The findings suggest that while both SSL models can outperform a fully supervised model with access to a small number of labels in a few-shot scenario, they do not exceed it when all the labels are used.   The results underscore the potential of SSL as a viable alternative to traditional supervised learning, capable of maintaining task performance while reducing the time and costs associated with data labeling. The study also contributes to the growing body of evidence supporting the use of SSL in remote sensing and could stimulate further research in this area.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Cascaded-Cross-Modal-Transformer-for-Request-and-Complaint-Detection"><a href="#Cascaded-Cross-Modal-Transformer-for-Request-and-Complaint-Detection" class="headerlink" title="Cascaded Cross-Modal Transformer for Request and Complaint Detection"></a>Cascaded Cross-Modal Transformer for Request and Complaint Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15097">http://arxiv.org/abs/2307.15097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ristea/ccmt">https://github.com/ristea/ccmt</a></li>
<li>paper_authors: Nicolae-Catalin Ristea, Radu Tudor Ionescu</li>
<li>for: 检测客户请求和投诉在电话对话中</li>
<li>methods:  combine speech和文本译本使用自动语音识别（ASR）模型和不同语言BERT基于模型，并使用wave2vec2.0音频特征</li>
<li>results: 在ACM Multimedia 2023 Computational Paralinguistics Challenge的请求子挑战中，我们的系统达到了不Weighted average recall（UAR）的65.41%和85.87%，分别对应于投诉和请求类别。<details>
<summary>Abstract</summary>
We propose a novel cascaded cross-modal transformer (CCMT) that combines speech and text transcripts to detect customer requests and complaints in phone conversations. Our approach leverages a multimodal paradigm by transcribing the speech using automatic speech recognition (ASR) models and translating the transcripts into different languages. Subsequently, we combine language-specific BERT-based models with Wav2Vec2.0 audio features in a novel cascaded cross-attention transformer model. We apply our system to the Requests Sub-Challenge of the ACM Multimedia 2023 Computational Paralinguistics Challenge, reaching unweighted average recalls (UAR) of 65.41% and 85.87% for the complaint and request classes, respectively.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的层次跨模态转换器（CCMT），该模型结合语音和文本译文来检测电话对话中的客户请求和投诉。我们的方法采用多模式观念，通过自动语音识别（ASR）模型将语音转录为不同语言的文本，然后将语言特定的 BERT 基于模型与 Wav2Vec2.0 音频特征在一种新的层次跨注意力转换器模型中结合。我们对 ACM Multimedia 2023 计算语言学挑战的请求子挑战问题进行应用，实现无权重平均回归率（UAR）为 65.41% 和 85.87%  для投诉和请求类别 respectively。
</details></li>
</ul>
<hr>
<h2 id="Generative-convective-parametrization-of-dry-atmospheric-boundary-layer"><a href="#Generative-convective-parametrization-of-dry-atmospheric-boundary-layer" class="headerlink" title="Generative convective parametrization of dry atmospheric boundary layer"></a>Generative convective parametrization of dry atmospheric boundary layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14857">http://arxiv.org/abs/2307.14857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Heyder, Juan Pedro Mellado, Jörg Schumacher</li>
<li>for: 这个论文的目的是为了开发一种基于生成 adversarial network的湍流 parametrization，用于 kilometer-scale Earth system models。</li>
<li>methods: 这个论文使用了生成机器学习算法，基于 direct numerical simulation 数据进行训练。</li>
<li>results: 这个模型能够预测湍流场的非均匀统计特征，包括气压波的强度和高度分布，以及气压波的横向组织结构。模型的预测结果与标准两方程或多气流扩展模型相符。<details>
<summary>Abstract</summary>
Turbulence parametrizations will remain a necessary building block in kilometer-scale Earth system models. In convective boundary layers, where the mean vertical gradients of conserved properties such as potential temperature and moisture are approximately zero, the standard ansatz which relates turbulent fluxes to mean vertical gradients via an eddy diffusivity has to be extended by mass flux parametrizations for the typically asymmetric up- and downdrafts in the atmospheric boundary layer. In this work, we present a parametrization for a dry convective boundary layer based on a generative adversarial network. The model incorporates the physics of self-similar layer growth following from the classical mixed layer theory by Deardorff. This enhances the training data base of the generative machine learning algorithm and thus significantly improves the predicted statistics of the synthetically generated turbulence fields at different heights inside the boundary layer. The algorithm training is based on fully three-dimensional direct numerical simulation data. Differently to stochastic parametrizations, our model is able to predict the highly non-Gaussian transient statistics of buoyancy fluctuations, vertical velocity, and buoyancy flux at different heights thus also capturing the fastest thermals penetrating into the stabilized top region. The results of our generative algorithm agree with standard two-equation or multi-plume stochastic mass-flux schemes. The present parametrization provides additionally the granule-type horizontal organization of the turbulent convection which cannot be obtained in any of the other model closures. Our work paves the way to efficient data-driven convective parametrizations in other natural flows, such as moist convection, upper ocean mixing, or convection in stellar interiors.
</details>
<details>
<summary>摘要</summary>
“湍流 parametrizations 将继续作为 Earth 系统模型中必需的构建块。在湍流边层中， где保守量的平均垂直梯度近乎为零，标准推理，将湍流 fluxes 关联到平均垂直梯度via 湍流散度，需要进一步扩展为质量流 parametrizations。在这个工作中，我们提出了一种基于生成对抗网络的湍流 parametrization。该模型包括自similar层成长的物理学，这将提高训练数据集的生成机器学习算法，并因此改善预测在不同高度内边层的湍流场的统计。我们的模型训练基于三维直接数值计算数据。与抽象参量化不同，我们的模型能够预测湍流场的非高斯散度特征，包括垂直速度、湍流 flux 和湍流能量的快速变化。我们的结果与标准两方程或多柱抽象液体散度模型相符。我们的 parametrization 提供了扩展的 granule 类型的湍流组织，这不能在其他任何模型 closure 中获得。我们的工作开创了数据驱动的湍流 parametrization 的可能性，可以应用于其他自然流体中，如湿气湍流、上层水温混合或星系内部湍流。”
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density"><a href="#Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density" class="headerlink" title="Counterfactual Explanations for Graph Classification Through the Lenses of Density"></a>Counterfactual Explanations for Graph Classification Through the Lenses of Density</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14849">http://arxiv.org/abs/2307.14849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density">https://github.com/carlo-abrate/Counterfactual-Explanations-for-Graph-Classification-Through-the-Lenses-of-Density</a></li>
<li>paper_authors: Carlo Abrate, Giulia Preti, Francesco Bonchi</li>
<li>for: 这paper主要用于提出一种基于浓度的对比例例行对图像分类器的解释方法，以便更好地理解图像分类器的决策过程。</li>
<li>methods: 该paper使用了一种通过调整图像中的稠密结构来生成对比例例行的图像，包括打开或关闭三角形、以及基于最大 clique的方法。</li>
<li>results: 实验结果表明，采用浓度作为对比例例行的单位可以生成更加灵活和可读的解释方法，并且可以在7个大脑网络数据集上证明这种方法的有效性。<details>
<summary>Abstract</summary>
Counterfactual examples have emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, i.e., removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.
</details>
<details>
<summary>摘要</summary>
counterfactual 例子在 graph classification 中 emerged as an effective approach to produce simple and understandable post-hoc explanations. In the context of graph classification, previous work has focused on generating counterfactual explanations by manipulating the most elementary units of a graph, such as removing an existing edge, or adding a non-existing one. In this paper, we claim that such language of explanation might be too fine-grained, and turn our attention to some of the main characterizing features of real-world complex networks, such as the tendency to close triangles, the existence of recurring motifs, and the organization into dense modules. We thus define a general density-based counterfactual search framework to generate instance-level counterfactual explanations for graph classifiers, which can be instantiated with different notions of dense substructures. In particular, we show two specific instantiations of this general framework: a method that searches for counterfactual graphs by opening or closing triangles, and a method driven by maximal cliques. We also discuss how the general method can be instantiated to exploit any other notion of dense substructures, including, for instance, a given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7 brain network datasets and compare the counterfactual statements generated according to several widely-used metrics. Results confirm that adopting a semantic-relevant unit of change like density is essential to define versatile and interpretable counterfactual explanation methods.
</details></li>
</ul>
<hr>
<h2 id="Kernelised-Normalising-Flows"><a href="#Kernelised-Normalising-Flows" class="headerlink" title="Kernelised Normalising Flows"></a>Kernelised Normalising Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14839">http://arxiv.org/abs/2307.14839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eshant English, Matthias Kirchler, Christoph Lippert</li>
<li>for: 本研究旨在提出一种基于kernels的normalizing flows模型，以提高模型表达力和数据效应性。</li>
<li>methods: 该模型采用了kernel化的方法，将传统的神经网络变换替换为kernel化的变换，以提高模型的表达能力和参数效率。</li>
<li>results: 实验结果表明，kernel化的normalizing flows模型可以与基于神经网络的模型相比，在低数据量 régime下具有竞争力或更高的表达能力，同时具有更好的数据效应性。<details>
<summary>Abstract</summary>
Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
</details>
<details>
<summary>摘要</summary>
通常的流程模型具有可逆的架构，但是这种要求对表达能力带来限制，因此需要许多参数和创新的架构来实现可Acceptable的结果。而流程模型主要依靠神经网络变换来实现表达设计，而其他变换方法受到了有限的关注。在这项工作中，我们介绍了 Ferumal flow，一种新的内核化正常流程模型，它将内核 integrate into the framework。我们的结果表明，内核化流可以与神经网络基于的流相比，在参数效率方面具有竞争力，并且在数据稀缺情况下表现特别出色。
</details></li>
</ul>
<hr>
<h2 id="Building-RadiologyNET-Unsupervised-annotation-of-a-large-scale-multimodal-medical-database"><a href="#Building-RadiologyNET-Unsupervised-annotation-of-a-large-scale-multimodal-medical-database" class="headerlink" title="Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database"></a>Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08517">http://arxiv.org/abs/2308.08517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateja Napravnik, Franko Hržić, Sebastian Tschauner, Ivan Štajduhar<br>for: This paper aims to address the challenge of annotating large medical radiology image databases by proposing an automated, unsupervised approach for clustering images based on their semantic similarity.methods: The proposed approach uses a combination of feature extractors from multiple data sources, including images, DICOM metadata, and narrative diagnoses. The features are then integrated into a multimodal representation and clustered using k-means and k-medoids algorithms.results: The results show that fusing the embeddings of all three data sources together results in the most concise clusters, indicating that this approach is effective in unsupervised clustering of large-scale medical data. The proposed method has the potential to create a much larger and more fine-grained annotated dataset of medical radiology images.<details>
<summary>Abstract</summary>
Background and objective: The usage of machine learning in medical diagnosis and treatment has witnessed significant growth in recent years through the development of computer-aided diagnosis systems that are often relying on annotated medical radiology images. However, the availability of large annotated image datasets remains a major obstacle since the process of annotation is time-consuming and costly. This paper explores how to automatically annotate a database of medical radiology images with regard to their semantic similarity.   Material and methods: An automated, unsupervised approach is used to construct a large annotated dataset of medical radiology images originating from Clinical Hospital Centre Rijeka, Croatia, utilising multimodal sources, including images, DICOM metadata, and narrative diagnoses. Several appropriate feature extractors are tested for each of the data sources, and their utility is evaluated using k-means and k-medoids clustering on a representative data subset.   Results: The optimal feature extractors are then integrated into a multimodal representation, which is then clustered to create an automated pipeline for labelling a precursor dataset of 1,337,926 medical images into 50 clusters of visually similar images. The quality of the clusters is assessed by examining their homogeneity and mutual information, taking into account the anatomical region and modality representation.   Conclusion: The results suggest that fusing the embeddings of all three data sources together works best for the task of unsupervised clustering of large-scale medical data, resulting in the most concise clusters. Hence, this work is the first step towards building a much larger and more fine-grained annotated dataset of medical radiology images.
</details>
<details>
<summary>摘要</summary>
背景和目标：随着医疗机器学习技术的发展，医疗诊断和治疗中的计算机支持诊断系统得到了广泛应用，但大量注释医疗影像数据集的可 availability 仍然是一个主要障碍。这篇论文探讨如何自动注释医疗影像数据库中的semantic similarity。材料和方法：使用自动化、无监督的方法，使用来自克林尼克医院中心的医疗影像数据，包括图像、DICOM元数据和描述诊断。对每种数据源，选择合适的特征提取器，并对选择的数据子集进行k-means和k-medoids归一化 clustering。结果：最佳特征提取器被集成到多模式表示中，并对 precursor 数据集进行自动化标注，将1,337,926个医疗影像分为50个视觉相似的集群。评估结果表明，将所有数据源的嵌入都 fusion 起来是最佳的选择，可以获得最紧凑的集群。因此，这是建立更大和更细化的注释医疗影像数据集的第一步。
</details></li>
</ul>
<hr>
<h2 id="Fading-memory-as-inductive-bias-in-residual-recurrent-networks"><a href="#Fading-memory-as-inductive-bias-in-residual-recurrent-networks" class="headerlink" title="Fading memory as inductive bias in residual recurrent networks"></a>Fading memory as inductive bias in residual recurrent networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14823">http://arxiv.org/abs/2307.14823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Igor Dubinin, Felix Effenberger</li>
<li>for: 这 paper 探讨了 residual connections 在 RNN 中的影响，以提高 task 性能和网络动态特性。</li>
<li>methods: 作者使用了 weakly coupled residual recurrent networks (WCRNNs)，并 investigate 了 residual connections 对网络性能、动态特性和记忆特性的影响。</li>
<li>results: 研究发现，不同类型的 residual connections 可以提供不同的 inductive bias，提高网络表达能力。 especailly, residual connections 可以使网络在静止边缘附近的动态特性， capitalize 数据特征的spectral properties，以及实现异质记忆特性。 更进一步，作者还展示了如何扩展到非线性 residual 和 introducing weakly coupled residual initialization scheme for Elman RNNs.<details>
<summary>Abstract</summary>
Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks to capitalize on characteristic spectral properties of the data, and (iii) result in heterogeneous memory properties are shown to increase practical expressivity. In addition, we demonstrate how our results can be extended to non-linear residuals and introduce a weakly coupled residual initialization scheme that can be used for Elman RNNs
</details>
<details>
<summary>摘要</summary>
剩下的连接（residual connections）已经被提议为网络架构层基的偏好，以降低反向传播算法中的扩散和消失梯度问题，并提高任务性能。然而，关于具有剩下的连接的RNN（Recurrent Neural Network）的动态和忘记性特性所知之少。在这里，我们介绍了弱相互连接的剩下RNN（Weakly Coupled Residual Recurrent Network，WCRNN），其中剩下的连接导致明确的Lyapunov exponent，并使得研究RNN的忘记性特性变得可行。我们 investigate了WCRNN中剩下连接的影响，包括网络性能、网络动态和忘记性特性在内的几个任务 benchmark。我们发现，不同的剩下连接形式可以提供不同的偏好，以提高网络表达能力。具体来说，剩下连接可以：（i）导致网络动态在边缘附近，（ii）让网络利用数据的特征频率特性，以及（iii）使得网络具有不同的忘记性特性。此外，我们还证明了我们的结果可以扩展到非线性剩下连接，并提出了一种弱相互连接初始化方案，可以应用于Elman RNN。
</details></li>
</ul>
<hr>
<h2 id="Likely-Light-and-Accurate-Context-Free-Clusters-based-Trajectory-Prediction"><a href="#Likely-Light-and-Accurate-Context-Free-Clusters-based-Trajectory-Prediction" class="headerlink" title="Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction"></a>Likely, Light, and Accurate Context-Free Clusters-based Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14788">http://arxiv.org/abs/2307.14788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiago Rodrigues de Almeida, Oscar Martinez Mozos</li>
<li>for: 预测道路交通网络中自主系统的未来路径，以适应未知性。</li>
<li>methods: 提出了一种多stage probabilistic方法，包括路径变换到位差空间、时间序列归一化、路径提议和排名提议。新引入了深度特征归一化方法，自适应GAN，可以更好地适应分布变化。</li>
<li>results: 对人行道和道路代理人 trajectory 数据进行比较，全系统超过了上下文深度生成模型，同时与点估计模型相当，可以准确预测道路交通网络中自主系统的未来路径。<details>
<summary>Abstract</summary>
Autonomous systems in the road transportation network require intelligent mechanisms that cope with uncertainty to foresee the future. In this paper, we propose a multi-stage probabilistic approach for trajectory forecasting: trajectory transformation to displacement space, clustering of displacement time series, trajectory proposals, and ranking proposals. We introduce a new deep feature clustering method, underlying self-conditioned GAN, which copes better with distribution shifts than traditional methods. Additionally, we propose novel distance-based ranking proposals to assign probabilities to the generated trajectories that are more efficient yet accurate than an auxiliary neural network. The overall system surpasses context-free deep generative models in human and road agents trajectory data while performing similarly to point estimators when comparing the most probable trajectory.
</details>
<details>
<summary>摘要</summary>
自动化系统在路运输网络中需要智能机制来预测未来。本文提出了一种多个阶段 probabilistic 方法 для路径预测：路径变换到位移空间，分聚运动时序序列，路径提议和排名提议。我们引入了一种新的深度特征划分方法，基于自我条件 GAN，可以更好地处理分布转移。此外，我们提出了一种新的距离基于排名提议，用于将生成的路径分配概率，这种方法比 auxiliary 神经网络更高效 yet 准确。总体系统在人员和道路代理 trajectory 数据中超越了上下文深度生成模型，同时与点估计相比，最有可能的路径预测的准确性也有所提高。
</details></li>
</ul>
<hr>
<h2 id="Emotion4MIDI-a-Lyrics-based-Emotion-Labeled-Symbolic-Music-Dataset"><a href="#Emotion4MIDI-a-Lyrics-based-Emotion-Labeled-Symbolic-Music-Dataset" class="headerlink" title="Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset"></a>Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14783">http://arxiv.org/abs/2307.14783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/serkansulun/lyricsemotions">https://github.com/serkansulun/lyricsemotions</a></li>
<li>paper_authors: Serkan Sulun, Pedro Oliveira, Paula Viana</li>
<li>for: 这个论文是为了创建一个大规模的情感标注的 симвоlic music dataset（12k首midi乐曲）而写的。</li>
<li>methods: 作者首先在GoEmotions dataset上训练了情感分类模型，实现了状态空间最佳的结果，并且使用这些模型对两个大规模的midi dataset中的歌词进行应用。</li>
<li>results: 该dataset覆盖了一系列细化的情感，为研究音乐和情感之间的连接，以及开发基于具体情感的音乐生成模型提供了一个非常有价值的资源。<details>
<summary>Abstract</summary>
We present a new large-scale emotion-labeled symbolic music dataset consisting of 12k MIDI songs. To create this dataset, we first trained emotion classification models on the GoEmotions dataset, achieving state-of-the-art results with a model half the size of the baseline. We then applied these models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide range of fine-grained emotions, providing a valuable resource to explore the connection between music and emotions and, especially, to develop models that can generate music based on specific emotions. Our code for inference, trained models, and datasets are available online.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的大规模情感标注的符号音乐数据集，包含12个MIDI歌曲。为创建这个数据集，我们首先在GoEmotions数据集上训练情感分类模型，实现了状态之arte的结果，模型半大小比基线。然后，我们应用了这些模型到两个大规模MIDI数据集中的歌词上。我们的数据集覆盖了各种细化的情感，提供了一个 ценный资源，探索音乐和情感之间的连接，特别是开发根据特定情感生成音乐的模型。我们在线上提供了推理代码、训练模型和数据集。
</details></li>
</ul>
<hr>
<h2 id="MATNilm-Multi-appliance-task-Non-intrusive-Load-Monitoring-with-Limited-Labeled-Data"><a href="#MATNilm-Multi-appliance-task-Non-intrusive-Load-Monitoring-with-Limited-Labeled-Data" class="headerlink" title="MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data"></a>MATNilm: Multi-appliance-task Non-intrusive Load Monitoring with Limited Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14778">http://arxiv.org/abs/2307.14778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jxiong22/matnilm">https://github.com/jxiong22/matnilm</a></li>
<li>paper_authors: Jing Xiong, Tianqi Hong, Dongbo Zhao, Yu Zhang</li>
<li>for: 该论文目的是提出一种基于多应用程序任务框架和培根监测的非扰式电力监测（NILM）方法，以提高家庭应用程序的状态和消耗电力的精度和效率。</li>
<li>methods: 该方法使用了一种培根监测框架，其中每个应用程序都有一个共享层次拆分结构，以实现每个应用程序的回归和分类任务。此外，该方法还使用了一种两维注意机制，以捕捉所有应用程序之间的空间时间相关性。</li>
<li>results:  simulation results show that the proposed approach significantly improves the performance of NILM, with relative errors reduced by more than 50% on average. The approach also achieves comparable test performance with only one day of training data and limited appliance operation profiles.<details>
<summary>Abstract</summary>
Non-intrusive load monitoring (NILM) identifies the status and power consumption of various household appliances by disaggregating the total power usage signal of an entire house. Efficient and accurate load monitoring facilitates user profile establishment, intelligent household energy management, and peak load shifting. This is beneficial for both the end-users and utilities by improving the overall efficiency of a power distribution network. Existing approaches mainly focus on developing an individual model for each appliance. Those approaches typically rely on a large amount of household-labeled data which is hard to collect. In this paper, we propose a multi-appliance-task framework with a training-efficient sample augmentation (SA) scheme that boosts the disaggregation performance with limited labeled data. For each appliance, we develop a shared-hierarchical split structure for its regression and classification tasks. In addition, we also propose a two-dimensional attention mechanism in order to capture spatio-temporal correlations among all appliances. With only one-day training data and limited appliance operation profiles, the proposed SA algorithm can achieve comparable test performance to the case of training with the full dataset. Finally, simulation results show that our proposed approach features a significantly improved performance over many baseline models. The relative errors can be reduced by more than 50% on average. The codes of this work are available at https://github.com/jxiong22/MATNilm
</details>
<details>
<summary>摘要</summary>
非侵入式卷积监测（NILM）可以识别家庭各种设备的状态和能量消耗。这样的监测可以帮助建立用户profile，实现智能家庭能源管理和峰值负荷延迟。这对 endpoint 用户和供应商都是有利的，因为它可以提高总能源分配网络的效率。现有的方法主要集中在开发每个设备的个性化模型。这些方法通常需要大量的标注数据，但这些数据很难收集。在这篇论文中，我们提出了一个多设备任务框架，并使用训练效率高的样本扩展（SA）策略来提高分解性能。对于每个设备，我们开发了共享层次分割结构，用于其预测和分类任务。此外，我们还提出了两个维度的注意力机制，以捕捉所有设备之间的空间时间相关性。只需一天的训练数据和有限的设备操作 profiling，我们的SA算法可以达到与整个数据集训练时的比较好的测试性能。最后，我们的实验结果显示，我们的提案的方法在许多基线模型之上显示出了显著的改善。相对误差可以降低超过50%的平均值。代码这个工作可以在 <https://github.com/jxiong22/MATNilm> 查看。
</details></li>
</ul>
<hr>
<h2 id="Towards-Practicable-Sequential-Shift-Detectors"><a href="#Towards-Practicable-Sequential-Shift-Detectors" class="headerlink" title="Towards Practicable Sequential Shift Detectors"></a>Towards Practicable Sequential Shift Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14758">http://arxiv.org/abs/2307.14758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Cobb, Arnaud Van Looveren</li>
<li>for: 检测机器学习模型中的分布变化，以避免模型性能下降。</li>
<li>methods: existin works relevant to their satisfaction, and recommend impactful directions for future research.</li>
<li>results:  identificaiton of three desiderata crucial to the practicable deployment of sequential shift detectors.<details>
<summary>Abstract</summary>
There is a growing awareness of the harmful effects of distribution shift on the performance of deployed machine learning models. Consequently, there is a growing interest in detecting these shifts before associated costs have time to accumulate. However, desiderata of crucial importance to the practicable deployment of sequential shift detectors are typically overlooked by existing works, precluding their widespread adoption. We identify three such desiderata, highlight existing works relevant to their satisfaction, and recommend impactful directions for future research.
</details>
<details>
<summary>摘要</summary>
有增长的认识到分布转移对已经部署的机器学习模型表现的负面影响。因此，有增长的兴趣检测这些转移，以避免成本累累。然而，现有的工作通常忽视了重要的实用部署顺序转移检测的要求，这使得它们在实际应用中没有得到广泛采用。我们认为有三个如此的要求，提到现有的工作，并建议未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Fair-Machine-Unlearning-Data-Removal-while-Mitigating-Disparities"><a href="#Fair-Machine-Unlearning-Data-Removal-while-Mitigating-Disparities" class="headerlink" title="Fair Machine Unlearning: Data Removal while Mitigating Disparities"></a>Fair Machine Unlearning: Data Removal while Mitigating Disparities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14754">http://arxiv.org/abs/2307.14754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Oesterling, Jiaqi Ma, Flavio P. Calmon, Hima Lakkaraju</li>
<li>for: 本研究旨在提供一种可靠地忘记数据实例的机器学习方法，以保障个人隐私和公平性。</li>
<li>methods: 本研究使用了一种新的机器学习方法，可以高效地和可靠地忘记数据实例，同时保持集体公平性。</li>
<li>results: 实验结果表明，本方法可以高效地忘记数据实例，并且保持集体公平性。<details>
<summary>Abstract</summary>
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fairness. We derive theoretical results which demonstrate that our method can provably unlearn data instances while maintaining fairness objectives. Extensive experimentation with real-world datasets highlight the efficacy of our method at unlearning data instances while preserving fairness.
</details>
<details>
<summary>摘要</summary>
In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fairness. We derive theoretical results that demonstrate our method can provably unlearn data instances while maintaining fairness objectives. Extensive experiments with real-world datasets show that our method is effective at unlearning data instances while preserving fairness.
</details></li>
</ul>
<hr>
<h2 id="FLARE-Fingerprinting-Deep-Reinforcement-Learning-Agents-using-Universal-Adversarial-Masks"><a href="#FLARE-Fingerprinting-Deep-Reinforcement-Learning-Agents-using-Universal-Adversarial-Masks" class="headerlink" title="FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks"></a>FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14751">http://arxiv.org/abs/2307.14751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssg-research/FLARE">https://github.com/ssg-research/FLARE</a></li>
<li>paper_authors: Buse G. A. Tekgul, N. Asokan</li>
<li>for: 防止深度强化学习策略（DRL）的非法复制和使用</li>
<li>methods: 使用非可转移的通用敌意掩蔽（perturbations）生成对抗示例，并将这些掩蔽用作指纹来验证盗取的DRL策略的真实所属</li>
<li>results: FLARE效果很好（100% 动作一致率），不会误告发独立策略（无false positives），并且对模型修改攻击和更有经验的敌对者进行攻击也具有较好的Robustness。<details>
<summary>Abstract</summary>
We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable candidates for fingerprints due to the inherent characteristics of DRL policies. The spatio-temporal dynamics of DRL problems and sequential decision-making process make characterizing the decision boundary of DRL policies more difficult, as well as searching for universal masks that capture the geometry of it.
</details>
<details>
<summary>摘要</summary>
我们提出了FLARE，第一个验证怀疑深度强化学习（DRL）策略是否为另一个（受害者）策略的伪造 Mechanism。我们首先显示了可以找到不可转移的通用敌方攻击库（perturbations），即可以将攻击者从受害者策略中获得独特的攻击例子，但不能获得独立训练的策略中的攻击例子。FLARE使用这些库作为指纹，用于验证伪造的DRL策略的真实所有权。我们的实验评估显示FLARE有100%的动作一致率（action agreement），并不会误判独立的策略（no false positives）。FLARE还是对模型修改攻击和更 Informed 攻击者的攻击而言，不会轻松避免。我们还显示了不同的通用攻击库可能不适合指纹，因为深度强化学习问题的空间时间动态和继续决策过程使得characterizing DRL策略的决策边界更加困难，以及搜寻适合的通用库。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Image-Completion-and-Enhancement-using-GANs"><a href="#Semantic-Image-Completion-and-Enhancement-using-GANs" class="headerlink" title="Semantic Image Completion and Enhancement using GANs"></a>Semantic Image Completion and Enhancement using GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14748">http://arxiv.org/abs/2307.14748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyansh Saxena, Raahat Gupta, Akshat Maheshwari, Saumil Maheshwari</li>
<li>for: 这篇论文主要用于描述如何使用生成对抗网络（GAN）来实现图像完成和提高 зада务。</li>
<li>methods: 这篇论文使用的方法主要是基于GAN的架构，用于实现图像完成和提高。</li>
<li>results: 这篇论文的研究结果表明，GAN可以有效地完成和提高图像，并且可以保持图像的详细信息。<details>
<summary>Abstract</summary>
Semantic inpainting or image completion alludes to the task of inferring arbitrary large missing regions in images based on image semantics. Since the prediction of image pixels requires an indication of high-level context, this makes it significantly tougher than image completion, which is often more concerned with correcting data corruption and removing entire objects from the input image. On the other hand, image enhancement attempts to eliminate unwanted noise and blur from the image, along with sustaining most of the image details. Efficient image completion and enhancement model should be able to recover the corrupted and masked regions in images and then refine the image further to increase the quality of the output image. Generative Adversarial Networks (GAN), have turned out to be helpful in picture completion tasks. In this chapter, we will discuss the underlying GAN architecture and how they can be used used for image completion tasks.
</details>
<details>
<summary>摘要</summary>
semantic inpainting or image completion 涉及到根据图像 semantics 推断大量缺失区域的图像。由于预测图像像素需要高级上下文指示，这使得其 significatively 更加复杂于图像完成，而图像完成通常更关注于修复数据损害和从输入图像中除去 объек 的。然而，图像提高尝试去除不必要的噪声和模糊，同时保持大部分图像细节。高效的图像完成和提高模型应该能够回复缺失和masked 区域的图像，然后进一步提高输出图像的质量。生成对抗网络（GAN）在图像完成任务中表现出了有利的效果。在这章中，我们将讨论GAN的基本架构和如何使其用于图像完成任务。
</details></li>
</ul>
<hr>
<h2 id="A-Strategic-Framework-for-Optimal-Decisions-in-Football-1-vs-1-Shot-Taking-Situations-An-Integrated-Approach-of-Machine-Learning-Theory-Based-Modeling-and-Game-Theory"><a href="#A-Strategic-Framework-for-Optimal-Decisions-in-Football-1-vs-1-Shot-Taking-Situations-An-Integrated-Approach-of-Machine-Learning-Theory-Based-Modeling-and-Game-Theory" class="headerlink" title="A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory"></a>A Strategic Framework for Optimal Decisions in Football 1-vs-1 Shot-Taking Situations: An Integrated Approach of Machine Learning, Theory-Based Modeling, and Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14732">http://arxiv.org/abs/2307.14732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calvinyeungck/analyzing-two-agents-interaction-in-football-shot-taking-situations">https://github.com/calvinyeungck/analyzing-two-agents-interaction-in-football-shot-taking-situations</a></li>
<li>paper_authors: Calvin C. K. Yeung, Keisuke Fujii<br>for:这篇论文的目的是分析足球比赛中攻击者和防守者之间的复杂互动，并提供一个数据驱动和理论基础的分析方法。methods:这篇论文使用了游戏理论和机器学习模型来分析攻击者和防守者之间的竞争情况，并提出了一个新的评价指标——预期进球目标概率（xSOT），以评价球员的行为，即使投射不中目标也能够提供有价值的信息。results:经验 validate了这个框架，并与基线和减少模型进行比较。此外，发现xSOT和现有指标之间存在高度的相似性，这表明xSOT提供了有价值的信息。最后，通过对2022年世界杯和2020年欧洲锦标赛的一个投射情况进行分析， illustrate了这个框架的应用。<details>
<summary>Abstract</summary>
Complex interactions between two opposing agents frequently occur in domains of machine learning, game theory, and other application domains. Quantitatively analyzing the strategies involved can provide an objective basis for decision-making. One such critical scenario is shot-taking in football, where decisions, such as whether the attacker should shoot or pass the ball and whether the defender should attempt to block the shot, play a crucial role in the outcome of the game. However, there are currently no effective data-driven and/or theory-based approaches to analyzing such situations. To address this issue, we proposed a novel framework to analyze such scenarios based on game theory, where we estimate the expected payoff with machine learning (ML) models, and additional features for ML models were extracted with a theory-based shot block model. Conventionally, successes or failures (1 or 0) are used as payoffs, while a success shot (goal) is extremely rare in football. Therefore, we proposed the Expected Probability of Shot On Target (xSOT) metric to evaluate players' actions even if the shot results in no goal; this allows for effective differentiation and comparison between different shots and even enables counterfactual shot situation analysis. In our experiments, we have validated the framework by comparing it with baseline and ablated models. Furthermore, we have observed a high correlation between the xSOT and existing metrics. This alignment of information suggests that xSOT provides valuable insights. Lastly, as an illustration, we studied optimal strategies in the World Cup 2022 and analyzed a shot situation in EURO 2020.
</details>
<details>
<summary>摘要</summary>
在机器学习、游戏理论等领域，两个对立代理经常发生复杂的互动。量化分析这些策略可以提供客观的决策基础。一个典型的情况是足球中的射击，决策是否射球或传球，以及是否阻止射击都对游戏的结果产生重要影响。然而，目前没有有效的数据驱动和/或理论基础的方法来分析这些情况。为解决这个问题，我们提出了一种新的分析框架，基于游戏理论，我们使用机器学习（ML）模型来估计射击的预期收益，并从理论基础上提取了适用于ML模型的附加特征。传统上，成功或失败（1或0）被用作奖励，而射击成功（进球）在足球中是非常罕见的。因此，我们提出了射击点对象概率（xSOT）指标，以评估球员的行为，即使射击无法得分，这些指标允许有效地区分和比较不同的射击，甚至允许对射击情况进行对照分析。在我们的实验中，我们 validate了框架，并与基线和减少模型进行比较。此外，我们发现xSOT和现有指标之间存在高度的相关性。这种对应信息表示xSOT提供了有价值的信息。最后，我们以2022年世界杯和2020年欧锦赛作为例子，分析了一个射击情况。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Silent-Failures-in-Medical-Image-Classification"><a href="#Understanding-Silent-Failures-in-Medical-Image-Classification" class="headerlink" title="Understanding Silent Failures in Medical Image Classification"></a>Understanding Silent Failures in Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14729">http://arxiv.org/abs/2307.14729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iml-dkfz/sf-visuals">https://github.com/iml-dkfz/sf-visuals</a></li>
<li>paper_authors: Till J. Bungert, Levin Kobelke, Paul F. Jaeger</li>
<li>for: 预防静默失败，以确保医疗应用中的分类系统可靠。</li>
<li>methods: 使用 confidence scoring functions (CSFs) 检测和预防静默失败。</li>
<li>results: none of the benchmarked CSFs can reliably prevent silent failures, indicating a need for a deeper understanding of the root causes of failures in the data.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of various examples, we demonstrate how this tool can help researchers gain insight into the requirements for safe application of classification systems in the medical domain. The open-source benchmark and tool are at: https://github.com/IML-DKFZ/sf-visuals.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:要确保医疗应用中的分类系统可靠使用，避免悬念性失败是非常重要。这可以通过设计更加鲁棒的分类器来避免失败，或者使用信任分数函数（CSF）来检测剩下的失败。图像分类中的主要失败来源之一是在训练数据和部署数据之间的分布shift。为了了解医疗影像中的现状，我们进行了首次全面的分析，比较了各种 CSF 在四种生物医学任务和多样化的分布shift 下的表现。结果显示，none of the benchmarked CSFs 可靠地防止悬念性失败。这表明，更深入了解数据中的失败根源是必要的。为此，我们介绍了 SF-Visuals，一种可互动地分析工具，使用幽默空间划分来visualize  shift 和失败。通过多个示例，我们示出了这个工具如何帮助研究人员了解在医疗领域中安全应用分类系统的需求。开源 benchmark 和工具可以在：https://github.com/IML-DKFZ/sf-visuals 中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-Spoken-Language-on-Speech-Enhancement-using-Self-Supervised-Speech-Representation-Loss-Functions"><a href="#The-Effect-of-Spoken-Language-on-Speech-Enhancement-using-Self-Supervised-Speech-Representation-Loss-Functions" class="headerlink" title="The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions"></a>The Effect of Spoken Language on Speech Enhancement using Self-Supervised Speech Representation Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14502">http://arxiv.org/abs/2307.14502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leto19/commonvoice-demand">https://github.com/leto19/commonvoice-demand</a></li>
<li>paper_authors: George Close, Thomas Hain, Stefan Goetze</li>
<li>for: 这项研究旨在探讨自动提高抖音（SE）系统中使用自动提高抖音（SSSR）作为特征变换在损失函数中的效果。</li>
<li>methods: 本研究使用了不同语言组合和网络结构的自动提高抖音（SSSR）来训练和测试SE系统。</li>
<li>results: 研究发现，训练自动提高抖音的语言对提高性能的影响较小，但是训练数据量的影响很大。<details>
<summary>Abstract</summary>
Recent work in the field of speech enhancement (SE) has involved the use of self-supervised speech representations (SSSRs) as feature transformations in loss functions. However, in prior work, very little attention has been paid to the relationship between the language of the audio used to train the self-supervised representation and that used to train the SE system. Enhancement models trained using a loss function which incorporates a self-supervised representation that shares exactly the language of the noisy data used to train the SE system show better performance than those which do not match exactly. This may lead to enhancement systems which are language specific and as such do not generalise well to unseen languages, unlike models trained using traditional spectrogram or time domain loss functions. In this work, SE models are trained and tested on a number of different languages, with self-supervised representations which themselves are trained using different language combinations and with differing network structures as loss function representations. These models are then tested across unseen languages and their performances are analysed. It is found that the training language of the self-supervised representation appears to have a minor effect on enhancement performance, the amount of training data of a particular language, however, greatly affects performance.
</details>
<details>
<summary>摘要</summary>
最近在语音增强（SE）领域的研究中，有使用自然语言自我监督语音表示（SSSRs）作为损失函数中的特征变换。然而，在先前的工作中，对于使用自然语言来训练自我监督表示和语音增强系统之间的关系，几乎没有关注。在不匹配的语言条件下训练语音增强系统时，使用具有相同语言的自我监督表示可能会导致更好的性能，而不匹配的语言可能会导致语音增强系统不易泛化到未看过的语言。在这项工作中，我们训练和测试了多种不同语言的语音增强模型，使用不同的语言组合和网络结构作为损失函数表示。这些模型在未看过的语言上进行测试，其性能分析结果表明，训练语音增强模型的语言对性能的影响相对较小，但是训练数据的量对性能的影响很大。
</details></li>
</ul>
<hr>
<h2 id="Robust-vertebra-identification-using-simultaneous-node-and-edge-predicting-Graph-Neural-Networks"><a href="#Robust-vertebra-identification-using-simultaneous-node-and-edge-predicting-Graph-Neural-Networks" class="headerlink" title="Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks"></a>Robust vertebra identification using simultaneous node and edge predicting Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02509">http://arxiv.org/abs/2308.02509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imfusiongmbh/vid-vertebra-identification-dataset">https://github.com/imfusiongmbh/vid-vertebra-identification-dataset</a></li>
<li>paper_authors: Vincent Bürgin, Raphael Prevost, Marijn F. Stollenga</li>
<li>for: 验证 Automatic vertebra localization and identification in CT scans 的重要性，并提出一种简单的ipeline来实现这一目标。</li>
<li>methods: 使用 U-Net 预测 vertebra 的位置和orientation，并使用单个图像神经网络来关联和分类 vertebra。</li>
<li>results: 方法可以准确地关联正确的体部和肋骨特征点，忽略假阳性结果，并在标准 VerSe 挑战任务中表现竞争力。<details>
<summary>Abstract</summary>
Automatic vertebra localization and identification in CT scans is important for numerous clinical applications. Much progress has been made on this topic, but it mostly targets positional localization of vertebrae, ignoring their orientation. Additionally, most methods employ heuristics in their pipeline that can be sensitive in real clinical images which tend to contain abnormalities. We introduce a simple pipeline that employs a standard prediction with a U-Net, followed by a single graph neural network to associate and classify vertebrae with full orientation. To test our method, we introduce a new vertebra dataset that also contains pedicle detections that are associated with vertebra bodies, creating a more challenging landmark prediction, association and classification task. Our method is able to accurately associate the correct body and pedicle landmarks, ignore false positives and classify vertebrae in a simple, fully trainable pipeline avoiding application-specific heuristics. We show our method outperforms traditional approaches such as Hungarian Matching and Hidden Markov Models. We also show competitive performance on the standard VerSe challenge body identification task.
</details>
<details>
<summary>摘要</summary>
自动骨vertebra位置和识别在CT扫描图中是许多临床应用的重要任务。许多研究已经进行了这方面的进步，但大多数方法都忽略了骨vertebra的方向。此外，大多数方法还使用了一些规则来处理实际的临床图像，这些图像通常含有异常。我们提出了一个简单的管道，其中使用标准预测和U-Net，然后使用单个图гра树神经网络来关联和分类骨vertebra。为测试我们的方法，我们提出了一个新的骨vertebra数据集，该数据集还包含了骨体的找到和关联。我们的方法能够准确地关联正确的骨体和骨脊的标记，忽略假阳性和分类骨vertebra。我们显示我们的方法超过传统的方法，如匈牙利匹配和隐马尔可夫模型。我们还显示我们的方法在标准VerSe挑战体部识别任务中 exhibits 竞争性的表现。
</details></li>
</ul>
<hr>
<h2 id="TimeGNN-Temporal-Dynamic-Graph-Learning-for-Time-Series-Forecasting"><a href="#TimeGNN-Temporal-Dynamic-Graph-Learning-for-Time-Series-Forecasting" class="headerlink" title="TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting"></a>TimeGNN: Temporal Dynamic Graph Learning for Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14680">http://arxiv.org/abs/2307.14680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nancy Xu, Chrysoula Kosma, Michalis Vazirgiannis</li>
<li>for: 预测时间序列数据，帮助解决多种科学和工程领域中的重要应用问题。</li>
<li>methods: 使用图神经网络方法，同时学习图structure和时间序列数据的 correlations，以便更好地预测时间序列。</li>
<li>results: 相比其他状态对比方法，时间GNNSince achieves 4-80倍快于其他状态对比方法，而且预测性能相似。<details>
<summary>Abstract</summary>
Time series forecasting lies at the core of important real-world applications in many fields of science and engineering. The abundance of large time series datasets that consist of complex patterns and long-term dependencies has led to the development of various neural network architectures. Graph neural network approaches, which jointly learn a graph structure based on the correlation of raw values of multivariate time series while forecasting, have recently seen great success. However, such solutions are often costly to train and difficult to scale. In this paper, we propose TimeGNN, a method that learns dynamic temporal graph representations that can capture the evolution of inter-series patterns along with the correlations of multiple series. TimeGNN achieves inference times 4 to 80 times faster than other state-of-the-art graph-based methods while achieving comparable forecasting performance
</details>
<details>
<summary>摘要</summary>
时序序列预测在许多科学和工程领域的实际应用中具有核心地位。由于大量的时序序列数据集中含有复杂的模式和长期关系，因此引发了许多神经网络架构的发展。图神经网络方法，它同时学习基于时序序列值的相关性建立图structure，在预测时已经取得了很大成功。然而，这些解决方案经常具有高成本和难以扩展的缺点。在本文中，我们提出了TimeGNN方法，它可以在实时预测过程中学习动态的时间序列图表示，同时捕捉多个序列之间的演变和相关性。TimeGNN在对其他状态艺术图法进行比较时，实现了4-80倍 быстре的推理速度，并具有相似的预测性能。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-wind-turbines-power-with-physics-informed-neural-networks-and-evidential-uncertainty-quantification"><a href="#Prediction-of-wind-turbines-power-with-physics-informed-neural-networks-and-evidential-uncertainty-quantification" class="headerlink" title="Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification"></a>Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14675">http://arxiv.org/abs/2307.14675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alfonso Gijón, Ainhoa Pujana-Goitia, Eugenio Perea, Miguel Molina-Solana, Juan Gómez-Romero</li>
<li>for: 这个研究的目的是优化风力机操作和维护，通过早期缺陷探测和精准预测风机发电功率。</li>
<li>methods: 这个研究使用数据驱动方法，使用物理约束的启发式神经网络来复制历史数据，并提供了对输出变量的高度准确预测。</li>
<li>results: 研究结果表明，使用物理约束的启发式神经网络可以高度准确地预测风机发电功率、扭矩和功率系数，并且可以提供对预测结果的不确定性的估计。<details>
<summary>Abstract</summary>
The ever-growing use of wind energy makes necessary the optimization of turbine operations through pitch angle controllers and their maintenance with early fault detection. It is crucial to have accurate and robust models imitating the behavior of wind turbines, especially to predict the generated power as a function of the wind speed. Existing empirical and physics-based models have limitations in capturing the complex relations between the input variables and the power, aggravated by wind variability. Data-driven methods offer new opportunities to enhance wind turbine modeling of large datasets by improving accuracy and efficiency. In this study, we used physics-informed neural networks to reproduce historical data coming from 4 turbines in a wind farm, while imposing certain physical constraints to the model. The developed models for regression of the power, torque, and power coefficient as output variables showed great accuracy for both real data and physical equations governing the system. Lastly, introducing an efficient evidential layer provided uncertainty estimations of the predictions, proved to be consistent with the absolute error, and made possible the definition of a confidence interval in the power curve.
</details>
<details>
<summary>摘要</summary>
随着风能的不断发展，风机操作的优化变得必要，特别是通过扭角控制器和其维护，以早期发现FAULT。准确和可靠的风机模型对预测风速的输出功率具有关键性，但现有的empirical和物理基础模型具有限制，尤其是在风速变化的情况下。数据驱动方法可以提高风机模型的准确率和效率，并且可以补做现有模型的缺陷。在这个研究中，我们使用物理知识束缚神经网络来复制历史数据，并对输出变量（功率、扭矩和功率系数）进行回归。我们发现，这些模型具有很高的准确率，并且可以准确地预测风机的输出。最后，我们引入了一个高效的证据层，以获得预测结果的不确定性评估，并证明了其与绝对误差之间的一致。这种方法可以为风机模型的建立提供一个信度评估。
</details></li>
</ul>
<hr>
<h2 id="Bipartite-Ranking-Fairness-through-a-Model-Agnostic-Ordering-Adjustment"><a href="#Bipartite-Ranking-Fairness-through-a-Model-Agnostic-Ordering-Adjustment" class="headerlink" title="Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment"></a>Bipartite Ranking Fairness through a Model Agnostic Ordering Adjustment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14668">http://arxiv.org/abs/2307.14668</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuis15/xorder">https://github.com/cuis15/xorder</a></li>
<li>paper_authors: Sen Cui, Weishen Pan, Changshui Zhang, Fei Wang</li>
<li>for: 本文关注在两类样本（正确和错误）的协同排序enario中，学习一个排序函数，以便正确类样本高于错误类样本。</li>
<li>methods: 我们提出了一种模型不含的后处理框架xOrder，以实现在协同排序中保持算法排序性能和公平性。我们优化了一个权重和untility为定义最佳折叠路径，并使用动态编程过程解决。xOrder可以与不同的分类模型和公平度量 метрик相容，包括supervised和Unsupervised公平度量。</li>
<li>results: 我们在四个 benchmark 数据集和两个实际世界病人电子医疗记录库中评估了我们的提议算法。xOrder在不同的数据集和度量上具有一个更好的平衡，在不同的分组下MITIGATEscore分布的变化。此外，我们还提供了一些针对性的分析结果，表明xOrder在小样本和大分布分类分数的情况下具有Robust性。<details>
<summary>Abstract</summary>
Algorithmic fairness has been a serious concern and received lots of interest in machine learning community. In this paper, we focus on the bipartite ranking scenario, where the instances come from either the positive or negative class and the goal is to learn a ranking function that ranks positive instances higher than negative ones. While there could be a trade-off between fairness and performance, we propose a model agnostic post-processing framework xOrder for achieving fairness in bipartite ranking and maintaining the algorithm classification performance. In particular, we optimize a weighted sum of the utility as identifying an optimal warping path across different protected groups and solve it through a dynamic programming process. xOrder is compatible with various classification models and ranking fairness metrics, including supervised and unsupervised fairness metrics. In addition to binary groups, xOrder can be applied to multiple protected groups. We evaluate our proposed algorithm on four benchmark data sets and two real-world patient electronic health record repositories. xOrder consistently achieves a better balance between the algorithm utility and ranking fairness on a variety of datasets with different metrics. From the visualization of the calibrated ranking scores, xOrder mitigates the score distribution shifts of different groups compared with baselines. Moreover, additional analytical results verify that xOrder achieves a robust performance when faced with fewer samples and a bigger difference between training and testing ranking score distributions.
</details>
<details>
<summary>摘要</summary>
《算法公平性在机器学习社区中已经引起了很多关注。在这篇论文中，我们关注了二分类排名场景， instances 来自正确或错误类别，并且目标是学习一个排名函数，将正确类别的 instances 高于错误类别的 instances。尽管存在性能和公平性之间的交易，我们提出了一种模型无关的后处理框架 xOrder，以实现在二分类排名中保持算法分类性能的同时保证公平性。具体来说，我们优化了一个权重和排名公平度之间的平衡，通过动态规划过程解决。xOrder 与不同的分类模型和公平度度量相容，并且可以应用于多个保护组。我们在四个基本数据集和两个实际电子医疗纪录库上评估了我们的提议算法。 xOrder 在不同的数据集和度量上均可以寻求一个更好的平衡 между算法实用性和排名公平性。从折衔分配的排名得分视图来看，xOrder 可以减少不同组的分配得分的偏移。此外，额外的分析结果表明，xOrder 在样本数少和测试排名分布与训练排名分布之间的差异较大时表现更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Decoding-the-Secrets-of-Machine-Learning-in-Malware-Classification-A-Deep-Dive-into-Datasets-Feature-Extraction-and-Model-Performance"><a href="#Decoding-the-Secrets-of-Machine-Learning-in-Malware-Classification-A-Deep-Dive-into-Datasets-Feature-Extraction-and-Model-Performance" class="headerlink" title="Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance"></a>Decoding the Secrets of Machine Learning in Malware Classification: A Deep Dive into Datasets, Feature Extraction, and Model Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14657">http://arxiv.org/abs/2307.14657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eurecom-s3/decodingmlsecretsofwindowsmalwareclassification">https://github.com/eurecom-s3/decodingmlsecretsofwindowsmalwareclassification</a></li>
<li>paper_authors: Savino Dambra, Yufei Han, Simone Aonzo, Platon Kotzias, Antonino Vitale, Juan Caballero, Davide Balzarotti, Leyla Bilge</li>
<li>for: 本研究旨在探讨机器学习模型在恶意软件检测和分类中的关键因素。</li>
<li>methods: 我们使用最新的机器学习模型对恶意软件进行检测和分类，并对收集到的数据进行分类。</li>
<li>results: 我们发现静态特征比动态特征更好地表现，并且将静态和动态特征相结合只有微量提高静态特征的表现。我们还发现packing与分类精度无关，并且在动态特征中缺失行为会严重降低表现。此外，我们发现在不同家族数量的情况下，模型的表现会随着家族数量的增加而变化。最后，我们发现使用 uniform 分布的样本数据可以更好地泛化到未经看过的数据。<details>
<summary>Abstract</summary>
Many studies have proposed machine-learning (ML) models for malware detection and classification, reporting an almost-perfect performance. However, they assemble ground-truth in different ways, use diverse static- and dynamic-analysis techniques for feature extraction, and even differ on what they consider a malware family. As a consequence, our community still lacks an understanding of malware classification results: whether they are tied to the nature and distribution of the collected dataset, to what extent the number of families and samples in the training dataset influence performance, and how well static and dynamic features complement each other.   This work sheds light on those open questions. by investigating the key factors influencing ML-based malware detection and classification. For this, we collect the largest balanced malware dataset so far with 67K samples from 670 families (100 samples each), and train state-of-the-art models for malware detection and family classification using our dataset. Our results reveal that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We discover no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance. We also demonstrate how a larger number of families to classify make the classification harder, while a higher number of samples per family increases accuracy. Finally, we find that models trained on a uniform distribution of samples per family better generalize on unseen data.
</details>
<details>
<summary>摘要</summary>
To do this, we collected the largest balanced malware dataset to date, consisting of 67,000 samples from 670 families (100 samples each). We then trained state-of-the-art models for malware detection and family classification using our dataset. Our results show that static features perform better than dynamic features, and that combining both only provides marginal improvement over static features. We also found no correlation between packing and classification accuracy, and that missing behaviors in dynamically-extracted features highly penalize their performance.Furthermore, we demonstrated that a larger number of families to classify makes the classification harder, while a higher number of samples per family increases accuracy. Additionally, we found that models trained on a uniform distribution of samples per family better generalize on unseen data.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Parameter-Sensitivity-of-Regional-Climate-Models-–-A-Case-Study-of-the-WRF-Model-for-Heat-Extremes-over-Southeast-Australia"><a href="#Machine-Learning-based-Parameter-Sensitivity-of-Regional-Climate-Models-–-A-Case-Study-of-the-WRF-Model-for-Heat-Extremes-over-Southeast-Australia" class="headerlink" title="Machine Learning based Parameter Sensitivity of Regional Climate Models – A Case Study of the WRF Model for Heat Extremes over Southeast Australia"></a>Machine Learning based Parameter Sensitivity of Regional Climate Models – A Case Study of the WRF Model for Heat Extremes over Southeast Australia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14654">http://arxiv.org/abs/2307.14654</a></li>
<li>repo_url: None</li>
<li>paper_authors: P. Jyoteeshkumar Reddy, Sandeep Chinta, Richard Matear, John Taylor, Harish Baki, Marcus Thatcher, Jatin Kala, Jason Sharples</li>
<li>For: This paper aims to investigate the sensitivity of Weather Research and Forecasting (WRF) model parameters to surface meteorological variables during extreme heat events in southeast Australia.* Methods: The paper uses a machine learning (ML) surrogate-based global sensitivity analysis method to identify the sensitivity of 24 adjustable parameters in seven different physics schemes of the WRF model.* Results: The study finds that only three parameters are important for the considered meteorological variables, and these results are consistent for the two different extreme heat events.Here’s the same information in Simplified Chinese text:* For: 这篇论文旨在investigateWRF模型参数对表面地理变量的敏感性，特别是在澳大利亚东南部的极热事件中。* Methods: 这篇论文使用机器学习（ML）Surrogate基于全球敏感分析方法来确定WRF模型参数的敏感性。* Results: 研究发现仅有3个参数对考虑的地理变量具有重要作用，这些结果在两个不同的极热事件中都是一致的。<details>
<summary>Abstract</summary>
Heatwaves and bushfires cause substantial impacts on society and ecosystems across the globe. Accurate information of heat extremes is needed to support the development of actionable mitigation and adaptation strategies. Regional climate models are commonly used to better understand the dynamics of these events. These models have very large input parameter sets, and the parameters within the physics schemes substantially influence the model's performance. However, parameter sensitivity analysis (SA) of regional models for heat extremes is largely unexplored. Here, we focus on the southeast Australian region, one of the global hotspots of heat extremes. In southeast Australia Weather Research and Forecasting (WRF) model is the widely used regional model to simulate extreme weather events across the region. Hence in this study, we focus on the sensitivity of WRF model parameters to surface meteorological variables such as temperature, relative humidity, and wind speed during two extreme heat events over southeast Australia. Due to the presence of multiple parameters and their complex relationship with output variables, a machine learning (ML) surrogate-based global sensitivity analysis method is considered for the SA. The ML surrogate-based Sobol SA is used to identify the sensitivity of 24 adjustable parameters in seven different physics schemes of the WRF model. Results show that out of these 24, only three parameters, namely the scattering tuning parameter, multiplier of saturated soil water content, and profile shape exponent in the momentum diffusivity coefficient, are important for the considered meteorological variables. These SA results are consistent for the two different extreme heat events. Further, we investigated the physical significance of sensitive parameters. This study's results will help in further optimising WRF parameters to improve model simulation.
</details>
<details>
<summary>摘要</summary>
世界各地的热浪和森林火灾会对社会和生态系统造成重大影响。为了开发有效的避免和适应策略，需要更好地了解热极值的情况。区域气象模型通常用于更好地理解这些事件的动力学。这些模型有非常大的输入参数集，并且physics scheme中的参数对模型性能有很大的影响。然而，区域模型参数敏感性分析（SA）对热极值的模型参数的影响还很少研究。本研究在澳大利亚南东部地区进行了研究，这是全球热极值的热点之一。在这个地区，Weather Research and Forecasting（WRF）模型是广泛使用的区域模型，用于模拟极端天气事件。因此，本研究将在WRF模型参数的敏感性分析中强调surface meteorological变量，如温度、相对湿度和风速。为了处理多个参数和它们复杂的关系，本研究采用机器学习（ML）surrogate-based Sobol SA方法进行敏感性分析。结果显示，24个可调参数中，只有三个参数（散射调整参数、满足饱和 soil water content multiplier和profile shape exponent in momentum diffusivity coefficient）对surface meteorological变量具有重要影响。这些SA结果在两个不同的极端热事件中均相互一致。此外，我们还对敏感参数进行了物理意义的调查。本研究的结果将有助于进一步优化WRF参数，以提高模型的预测。
</details></li>
</ul>
<hr>
<h2 id="Speed-Limits-for-Deep-Learning"><a href="#Speed-Limits-for-Deep-Learning" class="headerlink" title="Speed Limits for Deep Learning"></a>Speed Limits for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14653">http://arxiv.org/abs/2307.14653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RishabhP19/Traffic-Surveillance">https://github.com/RishabhP19/Traffic-Surveillance</a></li>
<li>paper_authors: Inbar Seroussi, Alexander A. Alemi, Moritz Helias, Zohar Ringel</li>
<li>for: 本研究探讨了现代神经网络是否可以最优地训练。</li>
<li>methods: 本文使用了最近的热力学进步，将神经网络的初始权重分布与完全训练后的权重分布之间的速度上限确定。</li>
<li>results: 研究发现，对于线性和线性可变的神经网络（如神经汇kernel），在某些可能的尺度下 assumption 下，学习是在尺度上优化的。这些结果与小规模实验表明，权重分布在初始化后不久就会进入一个非优化的短暂阶段，然后是一个更长的优化阶段。<details>
<summary>Abstract</summary>
State-of-the-art neural networks require extreme computational power to train. It is therefore natural to wonder whether they are optimally trained. Here we apply a recent advancement in stochastic thermodynamics which allows bounding the speed at which one can go from the initial weight distribution to the final distribution of the fully trained network, based on the ratio of their Wasserstein-2 distance and the entropy production rate of the dynamical process connecting them. Considering both gradient-flow and Langevin training dynamics, we provide analytical expressions for these speed limits for linear and linearizable neural networks e.g. Neural Tangent Kernel (NTK). Remarkably, given some plausible scaling assumptions on the NTK spectra and spectral decomposition of the labels -- learning is optimal in a scaling sense. Our results are consistent with small-scale experiments with Convolutional Neural Networks (CNNs) and Fully Connected Neural networks (FCNs) on CIFAR-10, showing a short highly non-optimal regime followed by a longer optimal regime.
</details>
<details>
<summary>摘要</summary>
现代神经网络需要极高的计算能力来训练，因此自然会思考是否最优地训练。我们在材料科学中应用最新的温度动力学技术，可以界定从初始权重分布到全部训练后的神经网络权重分布之间的速度上限，基于这两个分布之间的伪拟合距离和动力学过程的热力学生产率。我们考虑了梯度流和拉杆训练动力学，并提供了分析表达式，其中包括线性和线性可变神经网络等例如神经 Tangent Kernel（NTK）。很remarkably，对于一些可能的 NTK  спектrum 和标签的特征分解的假设，我们发现学习是在一定的缩放意义上最优的。我们的结果与小规模的 CIFAR-10 上的 Convolutional Neural Networks（CNNs）和 Fully Connected Neural Networks（FCNs）的实验结果相符，显示一个短暂的非优化期 followed by a longer optimal period。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Frequency-U-Net-for-Denoising-Diffusion-Probabilistic-Models"><a href="#Spatial-Frequency-U-Net-for-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models"></a>Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14648">http://arxiv.org/abs/2307.14648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Yuan, Linjie Li, Jianfeng Wang, Zhengyuan Yang, Kevin Lin, Zicheng Liu, Lijuan Wang</li>
<li>for: 这个论文是用来研究在波лет空间中使用潮汐传播概率模型 (DDPM) 进行视觉合成。</li>
<li>methods: 这个论文使用了一个新的架构 SFUNet，它特意设计来有效地捕捉波лет变换所表示的图像中的相互联系。在标准的潮汐净化 U-Net 中，我们增加了2D潮汐条件和频率对应层，以同时模型空间和频率领域中的联系。</li>
<li>results: 这个研究发现，使用我们的架构可以在 CIFAR-10、FFHQ、LSUN-Bedroom 和 LSUN-Church 数据集上生成高品质的图像，比过 pixel-based 网络。<details>
<summary>Abstract</summary>
In this paper, we study the denoising diffusion probabilistic model (DDPM) in wavelet space, instead of pixel space, for visual synthesis. Considering the wavelet transform represents the image in spatial and frequency domains, we carefully design a novel architecture SFUNet to effectively capture the correlation for both domains. Specifically, in the standard denoising U-Net for pixel data, we supplement the 2D convolutions and spatial-only attention layers with our spatial frequency-aware convolution and attention modules to jointly model the complementary information from spatial and frequency domains in wavelet data. Our new architecture can be used as a drop-in replacement to the pixel-based network and is compatible with the vanilla DDPM training process. By explicitly modeling the wavelet signals, we find our model is able to generate images with higher quality on CIFAR-10, FFHQ, LSUN-Bedroom, and LSUN-Church datasets, than the pixel-based counterpart.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了在wavelet空间中使用扩散概率模型（DDPM）进行视觉合成。我们注意到，wavelet变换可以在空间和频率域中表示图像，因此我们 méticulously设计了一种新的架构SFUNet，以有效地捕捉这两个频率域之间的相关性。具体来说，在标准的干扰U-Net中，我们补充了2D卷积和专门关注空间频率信息的卷积和注意力模块，以同时模型空间和频率频率域中的补做信息。我们的新架构可以与标准的像素数据网络进行互换，并且与普通的DDPM训练过程相容。通过显式地模型wavelet信号，我们发现我们的模型在CIFAR-10、FFHQ、LSUN-Bedroom和LSUN-Church数据集上能够生成高质量的图像，比标准的像素数据网络更高。
</details></li>
</ul>
<hr>
<h2 id="MVMR-FS-Non-parametric-feature-selection-algorithm-based-on-Maximum-inter-class-Variation-and-Minimum-Redundancy"><a href="#MVMR-FS-Non-parametric-feature-selection-algorithm-based-on-Maximum-inter-class-Variation-and-Minimum-Redundancy" class="headerlink" title="MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy"></a>MVMR-FS : Non-parametric feature selection algorithm based on Maximum inter-class Variation and Minimum Redundancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14643">http://arxiv.org/abs/2307.14643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haitao Nie, Shengbo Zhang, Bin Xie</li>
<li>for: 本研究旨在解决 feature selection 中的 relevance 和 redundancy 问题，提出了一种基于最大间类差和最小重复度的非参数化Feature Selection算法（MVMR-FS）。</li>
<li>methods: 本研究使用了 supervised 和Unsupervised 预测器构成kernel density estimation来捕捉特征之间的相似性和差异，然后提出了 maximum inter-class variation和minimum redundancy（MVMR）的 критери来衡量特征的相关性和重复度。</li>
<li>results: 与前十种状态顶方法进行比较，MVMR-FS 实现了最高的平均准确率，提高了准确率5%到11%。<details>
<summary>Abstract</summary>
How to accurately measure the relevance and redundancy of features is an age-old challenge in the field of feature selection. However, existing filter-based feature selection methods cannot directly measure redundancy for continuous data. In addition, most methods rely on manually specifying the number of features, which may introduce errors in the absence of expert knowledge. In this paper, we propose a non-parametric feature selection algorithm based on maximum inter-class variation and minimum redundancy, abbreviated as MVMR-FS. We first introduce supervised and unsupervised kernel density estimation on the features to capture their similarities and differences in inter-class and overall distributions. Subsequently, we present the criteria for maximum inter-class variation and minimum redundancy (MVMR), wherein the inter-class probability distributions are employed to reflect feature relevance and the distances between overall probability distributions are used to quantify redundancy. Finally, we employ an AGA to search for the feature subset that minimizes the MVMR. Compared with ten state-of-the-art methods, MVMR-FS achieves the highest average accuracy and improves the accuracy by 5% to 11%.
</details>
<details>
<summary>摘要</summary>
age-old challenge 在feature选择领域是如何准确地测量特征相关性和重复性。然而，现有的筛选方法无法直接测量连续数据中的重复性。此外，大多数方法需要手动指定特征的数量，这可能会导致专家知识不足的情况下出现错误。在本文中，我们提出了一种非参数化特征选择算法基于最大 между类差异和最小重复性，简称MVMR-FS。我们首先引入了监督和无监督核密度估计器，用于捕捉特征之间的相似性和总体分布的不同。接着，我们介绍了MVMR的标准，其中用于反映特征相关性的inter-class probability distribution，以及用于衡量特征重复性的 distances between overall probability distributions。最后，我们使用AGA进行搜索，以找到最小化MVMR的特征子集。与十种当前状态的方法相比，MVMR-FS achieve最高的平均准确率，提高了5%到11%。
</details></li>
</ul>
<hr>
<h2 id="Linear-Convergence-of-Black-Box-Variational-Inference-Should-We-Stick-the-Landing"><a href="#Linear-Convergence-of-Black-Box-Variational-Inference-Should-We-Stick-the-Landing" class="headerlink" title="Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?"></a>Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14642">http://arxiv.org/abs/2307.14642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyurae Kim, Yian Ma, Jacob R. Gardner</li>
<li>for: 这paper是为了证明黑盒变量推断（BBVI）与控制变量的使用，特别是使用扣板降落（STL）估计器，在完美变量家族特定下 converges at a geometric rate。</li>
<li>methods: 这paper使用的方法包括证明STL估计器的偏导数方差为quadratic bound，以及对已有的closed-form entropy gradient estimators进行改进，以获得更好的非假设性质量保证。</li>
<li>results: 这paper的结果表明，使用BBVI和STL估计器可以在完美变量家族下 converges at a geometric rate，并且可以使用 projeted stochastic gradient descent来实现。此外，paper还提供了对closed-form entropy gradient estimators的改进，以及对其非假设性质量保证的explicit non-asymptotic complexity guarantees。<details>
<summary>Abstract</summary>
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
</details>
<details>
<summary>摘要</summary>
我们证明黑盒变量推断（BBVI）与控制变量，尤其是粘降（STL）估计器，在完美变量家族 спецификация下 converge 于 геометри（传统上称为“线性”）速率。特别是，我们证明 STL 估计器的梯度方差呈 quadratic 形式，包括变量家族不准确的情况。与先前的二阶 variance 条件研究相结合，这直接意味着 BBVI 使用投影式随机梯度下降 converge。我们还改进了现有的关于关闭形式Entropy Gradient估计器的分析，使其与 STL 估计器进行比较，并提供了非含极限性质保证。
</details></li>
</ul>
<hr>
<h2 id="Fact-Checking-of-AI-Generated-Reports"><a href="#Fact-Checking-of-AI-Generated-Reports" class="headerlink" title="Fact-Checking of AI-Generated Reports"></a>Fact-Checking of AI-Generated Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14634">http://arxiv.org/abs/2307.14634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Razi Mahmood, Ge Wang, Mannudeep Kalra, Pingkun Yan</li>
<li>for: 这篇论文旨在探讨如何使用生成型人工智能（AI）生成真实的医疗影像报告，以减少临床过程中的时间和成本。</li>
<li>methods: 本研究使用了一种新的方法，即基于图像和文本的对映来验证AI生成的报告。这个方法通过学习图像和文本之间的相互关联，将真实和伪造的句子区分开来。</li>
<li>results: 研究发现，这个方法可以实时验证AI生成的报告，并将伪造的句子移除，以提高医疗过程中的精确性和可靠性。这个工具有助于未来的生成AI方法，以责任地使用AI实现医疗过程的加速。<details>
<summary>Abstract</summary>
With advances in generative artificial intelligence (AI), it is now possible to produce realistic-looking automated reports for preliminary reads of radiology images. This can expedite clinical workflows, improve accuracy and reduce overall costs. However, it is also well-known that such models often hallucinate, leading to false findings in the generated reports. In this paper, we propose a new method of fact-checking of AI-generated reports using their associated images. Specifically, the developed examiner differentiates real and fake sentences in reports by learning the association between an image and sentences describing real or potentially fake findings. To train such an examiner, we first created a new dataset of fake reports by perturbing the findings in the original ground truth radiology reports associated with images. Text encodings of real and fake sentences drawn from these reports are then paired with image encodings to learn the mapping to real/fake labels. The utility of such an examiner is demonstrated for verifying automatically generated reports by detecting and removing fake sentences. Future generative AI approaches can use the resulting tool to validate their reports leading to a more responsible use of AI in expediting clinical workflows.
</details>
<details>
<summary>摘要</summary>
随着生成式人工智能（AI）的进步，现在可以生成具有真实look的自动报告，以便加速临床工作流程，提高准确性并降低总成本。然而，这些模型经常“幻想”，导致生成的报告中的假发现。在这篇论文中，我们提议一种新的实验室检查AI生成的报告的方法。具体来说，我们开发了一个新的检查器，可以在报告中分辨真实和假的句子。为了训练这个检查器，我们首先创建了一个新的假报告数据集，其中对原始的真实股票报告中的发现进行了修改。然后，我们对真实和假句子的文本编码和图像编码进行了对应，以学习将真实和假标签映射到句子和图像中。我们证明了这种检查器可以用于检查自动生成的报告，并且可以检测并移除假的句子。未来的生成AI方法可以使用这种工具来验证他们的报告，从而实现负责任的AI使用。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Reservoir-Computing-and-its-Interdisciplinary-Applications-Beyond-Traditional-Machine-Learning"><a href="#A-Survey-on-Reservoir-Computing-and-its-Interdisciplinary-Applications-Beyond-Traditional-Machine-Learning" class="headerlink" title="A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning"></a>A Survey on Reservoir Computing and its Interdisciplinary Applications Beyond Traditional Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15092">http://arxiv.org/abs/2307.15092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Zhang, Danilo Vasconcellos Vargas</li>
<li>for: 本研究评论文章的目的是对储量计算（RC）的最新发展进行统一的回顾，从机器学习到物理、生物和神经科学。</li>
<li>methods: 本文使用的方法包括早期RC模型的介绍，以及当前状态的RC模型和其应用。同时，文章还介绍了模拟大脑机制的研究。</li>
<li>results: 本文对RC的发展进行了统一的回顾，并介绍了它在不同领域的应用，包括机器学习、物理、生物和神经科学。此外，文章还提供了新的发展perspective，包括储量设计、编程框架的统一、物理实现和与认知神经科学和进化相互作用。<details>
<summary>Abstract</summary>
Reservoir computing (RC), first applied to temporal signal processing, is a recurrent neural network in which neurons are randomly connected. Once initialized, the connection strengths remain unchanged. Such a simple structure turns RC into a non-linear dynamical system that maps low-dimensional inputs into a high-dimensional space. The model's rich dynamics, linear separability, and memory capacity then enable a simple linear readout to generate adequate responses for various applications. RC spans areas far beyond machine learning, since it has been shown that the complex dynamics can be realized in various physical hardware implementations and biological devices. This yields greater flexibility and shorter computation time. Moreover, the neuronal responses triggered by the model's dynamics shed light on understanding brain mechanisms that also exploit similar dynamical processes. While the literature on RC is vast and fragmented, here we conduct a unified review of RC's recent developments from machine learning to physics, biology, and neuroscience. We first review the early RC models, and then survey the state-of-the-art models and their applications. We further introduce studies on modeling the brain's mechanisms by RC. Finally, we offer new perspectives on RC development, including reservoir design, coding frameworks unification, physical RC implementations, and interaction between RC, cognitive neuroscience and evolution.
</details>
<details>
<summary>摘要</summary>
储池计算（RC），最初应用于时间信号处理，是一种循环神经网络，其neurons randomly连接。一旦初始化，连接强度保持不变。这种简单的结构使RC变成一个非线性动力系统，可以将低维度输入映射到高维度空间中。模型的丰富动力、线性分离和记忆容量使得一个简单的直线读取可以生成适用于各种应用的合适响应。RC的应用范围超出机器学习，因为它已经在物理硬件实现和生物设备中实现了复杂的动力。这些实现带来更多的灵活性和更短的计算时间。此外，模型的神经响应也为了解大脑机制提供了新的思路，这些机制也利用类似的动力过程。在文献中，关于RC的研究非常广泛和杂乱，这里我们提供一个统一的RC最新发展的评论，从机器学习到物理、生物和神经科学。我们首先介绍了RC的早期模型，然后评论了当前最佳模型和其应用。我们还介绍了模型大脑机制的研究。最后，我们提出了新的RC发展 perspective，包括储池设计、编程框架统一、物理RC实现和RC、认知神经科学和演化之间的交互。
</details></li>
</ul>
<hr>
<h2 id="Rapid-and-Scalable-Bayesian-AB-Testing"><a href="#Rapid-and-Scalable-Bayesian-AB-Testing" class="headerlink" title="Rapid and Scalable Bayesian AB Testing"></a>Rapid and Scalable Bayesian AB Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14628">http://arxiv.org/abs/2307.14628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srivas Chennu, Andrew Maher, Christian Pangerl, Subash Prabanantham, Jae Hyeon Bae, Jamie Martin, Bud Goswami</li>
<li>for: 该论文旨在帮助企业operator更好地做出决策，通过利用数据学习来提高数字用户体验。</li>
<li>methods: 该论文提出了一种基于 Bayesian 估计的解决方案，用于 Addressing the limitations of current sequential AB testing methodology, such as lack of statistical power, correlations between factors, and inability to pool knowledge from past tests.</li>
<li>results: 论文通过 numercial simulations 和实际应用 demonstrate 了该方法的有用性，包括增加了统计力量、允许顺序测试和早期停止、不受过分风险等。此外，论文还展示了如何使用这种方法来加速未来测试。<details>
<summary>Abstract</summary>
AB testing aids business operators with their decision making, and is considered the gold standard method for learning from data to improve digital user experiences. However, there is usually a gap between the requirements of practitioners, and the constraints imposed by the statistical hypothesis testing methodologies commonly used for analysis of AB tests. These include the lack of statistical power in multivariate designs with many factors, correlations between these factors, the need of sequential testing for early stopping, and the inability to pool knowledge from past tests. Here, we propose a solution that applies hierarchical Bayesian estimation to address the above limitations. In comparison to current sequential AB testing methodology, we increase statistical power by exploiting correlations between factors, enabling sequential testing and progressive early stopping, without incurring excessive false positive risk. We also demonstrate how this methodology can be extended to enable the extraction of composite global learnings from past AB tests, to accelerate future tests. We underpin our work with a solid theoretical framework that articulates the value of hierarchical estimation. We demonstrate its utility using both numerical simulations and a large set of real-world AB tests. Together, these results highlight the practical value of our approach for statistical inference in the technology industry.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BubbleML-A-Multi-Physics-Dataset-and-Benchmarks-for-Machine-Learning"><a href="#BubbleML-A-Multi-Physics-Dataset-and-Benchmarks-for-Machine-Learning" class="headerlink" title="BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning"></a>BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14623">http://arxiv.org/abs/2307.14623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hpcforge/bubbleml">https://github.com/hpcforge/bubbleml</a></li>
<li>paper_authors: Sheikh Md Shakeel Hassan, Arthur Feeney, Akash Dhruv, Jihoon Kim, Youngjoon Suh, Jaiyoung Ryu, Yoonjin Won, Aparna Chandramowlishwaran</li>
<li>for: 该论文主要目的是提供一个可访问的和多样化的数据集，用于机器学习（ML）训练，以更好地理解多物理现象的热相变化。</li>
<li>methods: 该论文使用物理驱动的计算机模拟来提供精准的观测数据，包括各种热泡沸点情况，如 Pool boiling, flow boiling, 和半冷泡沸。该数据集覆盖了广泛的参数，包括不同的重力条件、流速、半冷水位和墙面超热情况，涵盖51个计算。</li>
<li>results: 该论文验证了该数据集的有效性，并展示了其在多种下游任务中的潜在应用，包括光流分析和温度动力学学习。该数据集和其标准 benchmarks 将成为机器学习驱动的多物理热相变化研究的推进者，推动了技术和模型的开发和比较。<details>
<summary>Abstract</summary>
In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse downstream tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b) operator networks for learning temperature dynamics. The BubbleML dataset and its benchmarks serve as a catalyst for advancements in ML-driven research on multi-physics phase change phenomena, enabling the development and comparison of state-of-the-art techniques and models.
</details>
<details>
<summary>摘要</summary>
在热变现象领域，因缺乏可访问的多样化数据集而受到机器学习（ML）训练的挑战。现有的实验数据集经常受限，有限的可用性和罕见的基准数据，妨碍我们对这种复杂多物理现象的理解。为了缓解这个差距，我们提供了BubbleML数据集（https://github.com/HPCForge/BubbleML），利用物理驱动的 simulations提供了各种爆发enario中的准确基准信息，包括 Pool boiling、流泌 boiling 和半冷含气 boiling 等多种情况。这个广泛的数据集覆盖了多种参数，包括不同重力条件、流量、冷凉水位、墙superheat 等，涵盖51个 simulations。BubbleML被验证了对实验观测和趋势的验证，确立了它作为ML研究中不可或缺的资源。此外，我们还在其中引入了两个比较任务：（a）Optical flow分析，捕捉气泡动态；（b）运算网络，学习温度动态。BubbleML数据集和其比较任务serve as a catalyst for advancements in ML-driven research on multi-physics phase change phenomena，激发开发和对state-of-the-art技术和模型的比较。
</details></li>
</ul>
<hr>
<h2 id="Imitating-Complex-Trajectories-Bridging-Low-Level-Stability-and-High-Level-Behavior"><a href="#Imitating-Complex-Trajectories-Bridging-Low-Level-Stability-and-High-Level-Behavior" class="headerlink" title="Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior"></a>Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14619">http://arxiv.org/abs/2307.14619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Block, Daniel Pfrommer, Max Simchowitz</li>
<li>for: studying the imitation of stochastic, non-Markovian, potentially multi-modal expert demonstrations in nonlinear dynamical systems.</li>
<li>methods: invoking low-level controllers (either learned or implicit in position-command control) to stabilize imitation policies around expert demonstrations, with a focus on ensuring “total variation continuity” (TVC) to achieve accurate matching of the demonstrator’s distribution over entire trajectories.</li>
<li>results: the paper provides theoretical guarantees for policies parameterized by diffusion models, showing that if the learner accurately estimates the score of the (noise-augmented) expert policy, then the distribution of imitator trajectories is close to the demonstrator distribution in a natural optimal transport distance, with empirical validation of the algorithmic recommendations.Here’s the Chinese translation of the three key information points:</li>
<li>for: 研究stochoastic, non-Markovian, potentially multi-modal expert示例在非线性动力系统中的模仿。</li>
<li>methods: 通过 invoke low-level控制器（ Either learned或implicit in position-command control）来稳定模仿策略 around expert示例，以确保 “总变量稳定” (TVC) 以实现准确地匹配示例者的分布 over entire trajectories。</li>
<li>results: 文章提供了对政策参数化 by diffusion models的 teorotical guarantees，表明如果学习者准确地估计（noise-augmented）专家策略的Score，那么imitator的分布 will be close to the demonstrator distribution in a natural optimal transport distance,并进行了实验验证算法建议。<details>
<summary>Abstract</summary>
We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers - either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accurately estimates the score of the (noise-augmented) expert policy, then the distribution of imitator trajectories is close to the demonstrator distribution in a natural optimal transport distance. Our analysis constructs intricate couplings between noise-augmented trajectories, a technique that may be of independent interest. We conclude by empirically validating our algorithmic recommendations.
</details>
<details>
<summary>摘要</summary>
我们提出一种理论框架，用于研究复杂专家示范的模仿在非线性动力系统中。我们的框架借鉴低级控制器，ether学习或含有位置控制的隐式控制器，以稳定模仿政策。我们证明，如果（a）有适当的低级稳定保证，并且（b）学习政策具有总变量连续性（TVC）性质，那么模仿者可以准确地模仿专家的动作，并且模仿者的动作分布与专家的动作分布在整个轨迹上几乎相同。我们然后证明，可以通过组合流行的数据扩展约束和一种新的算法技巧来保证TVC的存在：在执行时添加扩展噪声。我们在执行时添加扩展噪声，可以在较低的精度下保证TVC。我们实例化我们的保证，并证明如果学习者准确地估计噪声扩展后的专家政策的分数，那么模仿者的轨迹分布与专家的轨迹分布在自然的优质量度中几乎相同。我们的分析建立了复杂的拓扑关系，这可能是独立的兴趣。我们最后通过实验验证我们的算法建议。
</details></li>
</ul>
<hr>
<h2 id="Self-Contrastive-Graph-Diffusion-Network"><a href="#Self-Contrastive-Graph-Diffusion-Network" class="headerlink" title="Self-Contrastive Graph Diffusion Network"></a>Self-Contrastive Graph Diffusion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14613">http://arxiv.org/abs/2307.14613</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kunzhan/SCDGN">https://github.com/kunzhan/SCDGN</a></li>
<li>paper_authors: Yixian Ma, Kun Zhan</li>
<li>for: 本文是用于提出一种新的自适应图diffusion网络(SCGDN)的框架，用于图像自适应学习。</li>
<li>methods: 本文使用了两个主要组成部分：宏观模块(AttM)和扩散模块(DiFM)。AttM通过聚合高阶结构和特征信息来获得优秀的嵌入，而DiFM通过拉普拉斯扩散学习来均衡每个节点在图中的状态，并允许特征信息和邻接信息在图中协同演化。</li>
<li>results: 本文的实验结果表明，SCGDN可以在图像自适应学习中提供更高的性能，并且可以避免”采样偏见”和语义漂移。<details>
<summary>Abstract</summary>
Augmentation techniques and sampling strategies are crucial in contrastive learning, but in most existing works, augmentation techniques require careful design, and their sampling strategies can only capture a small amount of intrinsic supervision information. Additionally, the existing methods require complex designs to obtain two different representations of the data. To overcome these limitations, we propose a novel framework called the Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two main components: the Attentional Module (AttM) and the Diffusion Module (DiFM). AttM aggregates higher-order structure and feature information to get an excellent embedding, while DiFM balances the state of each node in the graph through Laplacian diffusion learning and allows the cooperative evolution of adjacency and feature information in the graph. Unlike existing methodologies, SCGDN is an augmentation-free approach that avoids "sampling bias" and semantic drift, without the need for pre-training. We conduct a high-quality sampling of samples based on structure and feature information. If two nodes are neighbors, they are considered positive samples of each other. If two disconnected nodes are also unrelated on $k$NN graph, they are considered negative samples for each other. The contrastive objective reasonably uses our proposed sampling strategies, and the redundancy reduction term minimizes redundant information in the embedding and can well retain more discriminative information. In this novel framework, the graph self-contrastive learning paradigm gives expression to a powerful force. SCGDN effectively balances between preserving high-order structure information and avoiding overfitting. The results manifest that SCGDN can consistently generate outperformance over both the contrastive methods and the classical methods.
</details>
<details>
<summary>摘要</summary>
“增强技术和采样策略是对比学习中非常重要的，但现有的方法中的增强技术需要精心设计，采样策略只能捕捉一小部分内在监督信息。此外，现有的方法需要复杂的设计来获得两种不同的数据表示。为了解决这些限制，我们提出了一种新的框架called Self-Contrastive Graph Diffusion Network (SCGDN)。我们的框架包括两个主要组件：Attentional Module (AttM)和Diffusion Module (DiFM)。AttM将高阶结构和特征信息聚合以获得优秀的嵌入，而DiFM通过laplacian diffusion learning来均衡每个节点在图中的状态，allowing the cooperative evolution of adjacency and feature information in the graph。不同于现有的方法ologies，SCGDN是一种无增强 approached that avoids "sampling bias"和semantic drift，不需要预训练。我们采用高质量的采样策略，基于结构和特征信息。如果两个节点是邻居，它们被视为对方的正例样本。如果两个不相关的节点也不在$k$NN图中相互关系，它们被视为对方的负例样本。对比目标函数合理地使用我们提议的采样策略，而减 redundancy reduction term可以减少嵌入中的重复信息，能够良好地保留更多的权威信息。在这种新的框架中，图自身对比学习模式发挥了强大的力量。SCGDN能够平衡保持高阶结构信息和避免过拟合。结果表明，SCGDN可以一直在对比方法和传统方法之上出perform。”
</details></li>
</ul>
<hr>
<h2 id="Complete-and-separate-Conditional-separation-with-missing-target-source-attribute-completion"><a href="#Complete-and-separate-Conditional-separation-with-missing-target-source-attribute-completion" class="headerlink" title="Complete and separate: Conditional separation with missing target source attribute completion"></a>Complete and separate: Conditional separation with missing target source attribute completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14609">http://arxiv.org/abs/2307.14609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Bralios, Efthymios Tzinis, Paris Smaragdis</li>
<li>for:  This paper is written for improving the performance of source separation models by leveraging semantic information about the input mixtures and constituent sources.</li>
<li>methods: The paper uses a pre-trained model to extract additional semantic data from the input mixture, which is then used to improve the separation performance of an uncoupled multi-conditional separation network.</li>
<li>results: The paper demonstrates that the separation performance of the multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Additionally, the approach achieves performance levels that are comparable to those of the best performing specialized single conditional models, providing an easier-to-use alternative.<details>
<summary>Abstract</summary>
Recent approaches in source separation leverage semantic information about their input mixtures and constituent sources that when used in conditional separation models can achieve impressive performance. Most approaches along these lines have focused on simple descriptions, which are not always useful for varying types of input mixtures. In this work, we present an approach in which a model, given an input mixture and partial semantic information about a target source, is trained to extract additional semantic data. We then leverage this pre-trained model to improve the separation performance of an uncoupled multi-conditional separation network. Our experiments demonstrate that the separation performance of this multi-conditional model is significantly improved, approaching the performance of an oracle model with complete semantic information. Furthermore, our approach achieves performance levels that are comparable to those of the best performing specialized single conditional models, thus providing an easier to use alternative.
</details>
<details>
<summary>摘要</summary>
现代源分离方法利用输入混合的semantic信息和组成源的信息，当用于条件分离模型时可以达到吸目的性能。大多数方法都是简单的描述，不一定适用于不同类型的输入混合。在这种工作中，我们提出了一种方法，即给定输入混合和部分semantic信息的target源，训练模型提取额外的semantic数据。然后，我们利用这个预训练模型提高不相互连接的多Conditional分离网络的分离性能。我们的实验表明，这种多Conditional网络的分离性能明显提高，接近完美的oracle模型，并且与专门设计的单Conditional模型性能相当。此外，我们的方法比特化的单Conditional模型更容易使用，提供了一种更容易使用的代替方案。
</details></li>
</ul>
<hr>
<h2 id="HUTFormer-Hierarchical-U-Net-Transformer-for-Long-Term-Traffic-Forecasting"><a href="#HUTFormer-Hierarchical-U-Net-Transformer-for-Long-Term-Traffic-Forecasting" class="headerlink" title="HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting"></a>HUTFormer: Hierarchical U-Net Transformer for Long-Term Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14596">http://arxiv.org/abs/2307.14596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zezhi Shao, Fei Wang, Zhao Zhang, Yuchen Fang, Guangyin Jin, Yongjun Xu</li>
<li>for: 预测交通情况，即基于历史观察数据预测交通条件，是智能交通领域的长期研究主题，而且被广泛认为是智能交通系统的重要组成部分。</li>
<li>methods: 我们提出了一种新的层次U-NetTransformer（HUTFormer）来解决长期交通预测的问题，它包括一个层次编码器和解码器，以同时生成和利用多级表示。特别是，编码器使用窗口自注意力和段合并来提取多级表示，而解码器则使用跨级注意力机制以有效地合并多级表示。</li>
<li>results: 我们在四个交通数据集上进行了广泛的实验，结果表明，我们提出的HUTFormer显著超过了当前交通预测和长时间序列预测基线。<details>
<summary>Abstract</summary>
Traffic forecasting, which aims to predict traffic conditions based on historical observations, has been an enduring research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we propose window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
</details>
<details>
<summary>摘要</summary>
traffic 预测，targeting to predict traffic conditions based on historical observations，has been a long-standing research topic and is widely recognized as an essential component of intelligent transportation. Recent proposals on Spatial-Temporal Graph Neural Networks (STGNNs) have made significant progress by combining sequential models with graph convolution networks. However, due to high complexity issues, STGNNs only focus on short-term traffic forecasting, e.g., 1-hour forecasting, while ignoring more practical long-term forecasting. In this paper, we make the first attempt to explore long-term traffic forecasting, e.g., 1-day forecasting. To this end, we first reveal its unique challenges in exploiting multi-scale representations. Then, we propose a novel Hierarchical U-net TransFormer (HUTFormer) to address the issues of long-term traffic forecasting. HUTFormer consists of a hierarchical encoder and decoder to jointly generate and utilize multi-scale representations of traffic data. Specifically, for the encoder, we propose window self-attention and segment merging to extract multi-scale representations from long-term traffic data. For the decoder, we design a cross-scale attention mechanism to effectively incorporate multi-scale representations. In addition, HUTFormer employs an efficient input embedding strategy to address the complexity issues. Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long time series forecasting baselines.
</details></li>
</ul>
<hr>
<h2 id="MCPA-Multi-scale-Cross-Perceptron-Attention-Network-for-2D-Medical-Image-Segmentation"><a href="#MCPA-Multi-scale-Cross-Perceptron-Attention-Network-for-2D-Medical-Image-Segmentation" class="headerlink" title="MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation"></a>MCPA: Multi-scale Cross Perceptron Attention Network for 2D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14588">http://arxiv.org/abs/2307.14588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/simonustc/mcpa-for-2d-medical-image-segmentation">https://github.com/simonustc/mcpa-for-2d-medical-image-segmentation</a></li>
<li>paper_authors: Liang Xu, Mingxiao Chen, Yi Cheng, Pengfei Shao, Shuwei Shen, Peng Yao, Ronald X. Xu</li>
<li>for: 这个研究的目的是提出一个基于Convolutional Neural Networks (CNN)的双维医疗影像分类模型，以提高医疗影像分类的精度和效能。</li>
<li>methods: 这个模型使用了Transformer模组来强化UNet架构，以更好地捕捉全局特征相关性。此外，模型还导入了多 scales Cross Perceptron模组，以捕捉不同 scales的特征相关性。</li>
<li>results: 在各种公开的医疗影像数据集上进行评估，这个模型实现了顶尖的表现，并且在不同的任务和设备上均有出色的结果。<details>
<summary>Abstract</summary>
The UNet architecture, based on Convolutional Neural Networks (CNN), has demonstrated its remarkable performance in medical image analysis. However, it faces challenges in capturing long-range dependencies due to the limited receptive fields and inherent bias of convolutional operations. Recently, numerous transformer-based techniques have been incorporated into the UNet architecture to overcome this limitation by effectively capturing global feature correlations. However, the integration of the Transformer modules may result in the loss of local contextual information during the global feature fusion process. To overcome these challenges, we propose a 2D medical image segmentation model called Multi-scale Cross Perceptron Attention Network (MCPA). The MCPA consists of three main components: an encoder, a decoder, and a Cross Perceptron. The Cross Perceptron first captures the local correlations using multiple Multi-scale Cross Perceptron modules, facilitating the fusion of features across scales. The resulting multi-scale feature vectors are then spatially unfolded, concatenated, and fed through a Global Perceptron module to model global dependencies. Furthermore, we introduce a Progressive Dual-branch Structure to address the semantic segmentation of the image involving finer tissue structures. This structure gradually shifts the segmentation focus of MCPA network training from large-scale structural features to more sophisticated pixel-level features. We evaluate our proposed MCPA model on several publicly available medical image datasets from different tasks and devices, including the open large-scale dataset of CT (Synapse), MRI (ACDC), fundus camera (DRIVE, CHASE_DB1, HRF), and OCTA (ROSE). The experimental results show that our MCPA model achieves state-of-the-art performance. The code is available at https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation.
</details>
<details>
<summary>摘要</summary>
UNet 架构，基于卷积神经网络（CNN），在医疗影像分析中表现出色。然而，它在捕捉长距离依赖关系方面存在挑战，因为卷积操作具有限定的接收区域和自然偏好。在最近，许多基于转换器的技术被 incorporated 到 UNet 架构中，以有效地捕捉全局特征相关性。然而，将转换模块 интегра到 UNet 架构中可能会导致全局特征相关性的损失。为了解决这些挑战，我们提出了一种名为 Multi-scale Cross Perceptron Attention Network (MCPA) 的2D医疗影像分类模型。MCPA 模型由三个主要组件组成：编码器、解码器和 Cross Perceptron。Cross Perceptron 首先使用多个 Multi-scale Cross Perceptron 模块捕捉本地相关性，以便在不同尺度上进行特征融合。得到的多尺度特征向量然后在空间上展开，并将其 concatenate 并输入到全球 Perceptron 模块，以模拟全局依赖关系。此外，我们还提出了一种进步的双支结构，以解决医疗影像分类中的semantic segmentation问题。这种结构逐渐将 MCPA 网络训练的 segmentation 焦点从大规模结构特征向 pixel-level 特征进行转换。我们在多个公共可用的医疗影像数据集上进行了多种任务和设备的测试，包括 Synapse 等开放式大规模数据集、ACDC 等 MRI 数据集、DRIVE 等fundus camera 数据集、CHASE_DB1 等 OCTA 数据集。实验结果表明，我们的 MCPA 模型在 state-of-the-art 性能。代码可以在 GitHub 上获取：https://github.com/simonustc/MCPA-for-2D-Medical-Image-Segmentation。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Safety-Constraints-in-Autonomous-Navigation-with-Deep-Reinforcement-Learning"><a href="#Evaluation-of-Safety-Constraints-in-Autonomous-Navigation-with-Deep-Reinforcement-Learning" class="headerlink" title="Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning"></a>Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14568">http://arxiv.org/abs/2307.14568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Angulo, Gregory Gorbov, Aleksandr Panov, Konstantin Yakovlev</li>
<li>for: 这个研究旨在高亮安全性因素在自动驾驶系统中的重要性，并通过比较两种学习导航策略：安全和不安全的策略来证明这一点。</li>
<li>methods: 这个研究使用了强化学习算法在自动驾驶系统中进行导航，并对两种策略进行比较，以 highlight the importance of considering safety constraints in the development of autonomous vehicles.</li>
<li>results: 研究结果表明，安全策略可以生成更多的减噪距离（与障碍物之间的距离），并少量碰撞，而不 sacrificing the overall performance。<details>
<summary>Abstract</summary>
While reinforcement learning algorithms have had great success in the field of autonomous navigation, they cannot be straightforwardly applied to the real autonomous systems without considering the safety constraints. The later are crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To highlight the importance of these constraints, in this study, we compare two learnable navigation policies: safe and unsafe. The safe policy takes the constraints into account, while the other does not. We show that the safe policy is able to generate trajectories with more clearance (distance to the obstacles) and makes less collisions while training without sacrificing the overall performance.
</details>
<details>
<summary>摘要</summary>
autonomous navigation 算法已经在场景中取得了很大的成功，但它们不能直接应用于实际的自动驾驶系统中，因为需要考虑安全约束。这些约束是关键，以避免自动车辆在路上发生不安全行为。为了强调这些约束的重要性，在这个研究中，我们比较了两种可学习导航策略：安全和不安全。安全策略考虑了约束，而另一个不考虑。我们显示，安全策略能够生成具有更多的避险距离（距离障碍物）并且 fewer collisions  durante el entrenamiento，而不 sacrificing the overall performance。
</details></li>
</ul>
<hr>
<h2 id="Auto-Tables-Synthesizing-Multi-Step-Transformations-to-Relationalize-Tables-without-Using-Examples"><a href="#Auto-Tables-Synthesizing-Multi-Step-Transformations-to-Relationalize-Tables-without-Using-Examples" class="headerlink" title="Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples"></a>Auto-Tables: Synthesizing Multi-Step Transformations to Relationalize Tables without Using Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14565">http://arxiv.org/abs/2307.14565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lipengcs/auto-tables-benchmark">https://github.com/lipengcs/auto-tables-benchmark</a></li>
<li>paper_authors: Peng Li, Yeye He, Cong Yan, Yue Wang, Surajit Chaudhuri<br>for:* This paper aims to address the problem of non-relational tables in relational databases, specifically the need for complex table-restructuring transformations before these tables can be queried using SQL-based analytics tools.methods:* The authors develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations to transform non-relational tables into standard relational forms for downstream analytics.results:* The authors evaluate the effectiveness of their proposed system using an extensive benchmark of 244 real test cases from user spreadsheets and online forums. Their evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users.<details>
<summary>Abstract</summary>
Relational tables, where each row corresponds to an entity and each column corresponds to an attribute, have been the standard for tables in relational databases. However, such a standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, for which complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for technical and non-technical users alike, as evidenced by large numbers of forum questions in places like StackOverflow and Excel/Power-BI/Tableau forums.   We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages), to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task, by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.
</details>
<details>
<summary>摘要</summary>
Relational tables, where each row represents an entity and each column represents an attribute, have been the standard for tables in relational databases. However, this standard cannot be taken for granted when dealing with tables "in the wild". Our survey of real spreadsheet-tables and web-tables shows that over 30% of such tables do not conform to the relational standard, and complex table-restructuring transformations are needed before these tables can be queried easily using SQL-based analytics tools. Unfortunately, the required transformations are non-trivial to program, which has become a substantial pain point for both technical and non-technical users, as evidenced by large numbers of forum questions in places like Stack Overflow and Excel/Power BI/Tableau forums.We develop an Auto-Tables system that can automatically synthesize pipelines with multi-step transformations (in Python or other languages) to transform non-relational tables into standard relational forms for downstream analytics, obviating the need for users to manually program transformations. We compile an extensive benchmark for this new task by collecting 244 real test cases from user spreadsheets and online forums. Our evaluation suggests that Auto-Tables can successfully synthesize transformations for over 70% of test cases at interactive speeds, without requiring any input from users, making this an effective tool for both technical and non-technical users to prepare data for analytics.
</details></li>
</ul>
<hr>
<h2 id="Understanding-Forward-Process-of-Convolutional-Neural-Network"><a href="#Understanding-Forward-Process-of-Convolutional-Neural-Network" class="headerlink" title="Understanding Forward Process of Convolutional Neural Network"></a>Understanding Forward Process of Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15090">http://arxiv.org/abs/2307.15090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Peixin Tian</li>
<li>for: 这篇论文揭示了深度神经网络（CNNs）的选择性旋转处理。</li>
<li>methods: 论文解释了 activation function 作为一种分辨率机制，将输入数据的旋转性统一和量化。</li>
<li>results: 实验显示，这种定义的方法论网络可以根据统计指标来识别输入数据，可以通过结构化数学工具进行分析。我们的发现还揭示了人工神经网络和人脑的数据处理模式之间的一致性。<details>
<summary>Abstract</summary>
This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" or "简化字符".Translation notes:* "selective rotation" is translated as "选择的旋转" (选择的旋转)* "forward processing" is translated as "前进处理" (前进处理)* "activation function" is translated as "活化函数" (活化函数)* "progress network" is translated as "进步网络" (进步网络)* "statistical indicators" is translated as "统计指标" (统计指标)* "structured mathematical tools" is translated as "结构化数学工具" (结构化数学工具)
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Sleeping-Bandit-Problems-with-Multiple-Plays-Algorithm-and-Ranking-Application"><a href="#Adversarial-Sleeping-Bandit-Problems-with-Multiple-Plays-Algorithm-and-Ranking-Application" class="headerlink" title="Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application"></a>Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14549">http://arxiv.org/abs/2307.14549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjun Yuan, Wei Lee Woon, Ludovik Coba</li>
<li>for: 这 paper 是为了解决在线推荐系统中的睡眠骑士问题，该问题具有固定、敌对损失和不确定的 i.i.d. 分布。</li>
<li>methods: 提出的算法基于睡眠骑士算法，并对单臂选择进行扩展，可以保证理论性能，增量误差 upper bounded by $\bigO(kN^2\sqrt{T\log T})$, где $k$ 是每个时间步选择的臂数，$N$ 是总臂数，$T$ 是时间轴。</li>
<li>results: 该 paper 获得了理论性能，增量误差 upper bounded by $\bigO(kN^2\sqrt{T\log T})$.<details>
<summary>Abstract</summary>
This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.
</details>
<details>
<summary>摘要</summary>
（本文提出了一种有效的算法，用于解决在线推荐系统中的睡着投注问题。该问题包括 bounded,  adversarial 损失以及 unknown i.i.d. 分布 дляarm availability。提出的算法基于单个arm选择的睡着投注算法，并且能够保证理论性能， regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$，where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon。）
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Inductive-Bias-of-Wide-Neural-Networks-by-Modifying-the-Kernel’s-Spectrum"><a href="#Controlling-the-Inductive-Bias-of-Wide-Neural-Networks-by-Modifying-the-Kernel’s-Spectrum" class="headerlink" title="Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum"></a>Controlling the Inductive Bias of Wide Neural Networks by Modifying the Kernel’s Spectrum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14531">http://arxiv.org/abs/2307.14531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amnon Geifman, Daniel Barzilai, Ronen Basri, Meirav Galun</li>
<li>for: 该论文主要目的是提出一种 modify 宽神经网络的方法，以便根据任务需要调整宽神经网络的学习偏好。</li>
<li>methods: 该论文提出了一种新的构造kernel的方法，称为Modified Spectrum Kernels（MSK），可以用于 aproximate 具有愿景值的kernel。此外，该论文还提出了一种基于宽神经网络和Neural Tangent Kernels的对偶方法，称为预conditioned gradient descent方法，可以改变Gradient Descent算法的轨迹。</li>
<li>results: 该论文的实验结果表明，使用Modified Spectrum Kernels和预conditioned gradient descent方法可以在一些情况下获得 polynomial 和 exponential 的训练速度提升，而不会改变最终解。此外，该方法也是 computationally efficient 和简单实现的。<details>
<summary>Abstract</summary>
Wide neural networks are biased towards learning certain functions, influencing both the rate of convergence of gradient descent (GD) and the functions that are reachable with GD in finite training time. As such, there is a great need for methods that can modify this bias according to the task at hand. To that end, we introduce Modified Spectrum Kernels (MSKs), a novel family of constructed kernels that can be used to approximate kernels with desired eigenvalues for which no closed form is known. We leverage the duality between wide neural networks and Neural Tangent Kernels and propose a preconditioned gradient descent method, which alters the trajectory of GD. As a result, this allows for a polynomial and, in some cases, exponential training speedup without changing the final solution. Our method is both computationally efficient and simple to implement.
</details>
<details>
<summary>摘要</summary>
广阶层神经网络具有倾向于学习特定函数的倾向，这影响了梯度下降（GD）的速度和训练时间内可到的函数。因此，有一个很大的需求，即可以根据任务改变这种倾向。为了解决这个问题，我们介绍Modified Spectrum Kernels（MSK），一种新的建构kernel的家族，可以用来 aproximate kernel with desired eigenvalues，即无法known的closed form。我们利用广阶层神经网络和Neural Tangent Kernels的 dual性，提出预调corrected gradient descent方法，这个方法可以改变GD的轨迹。因此，这可以实现 polynomial 和，在一些情况下，exponential training speedup，而不需要变更最终解。我们的方法具有computational efficiency和简单实现。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Estimation-in-Mixed-Membership-Stochastic-Block-Models"><a href="#Optimal-Estimation-in-Mixed-Membership-Stochastic-Block-Models" class="headerlink" title="Optimal Estimation in Mixed-Membership Stochastic Block Models"></a>Optimal Estimation in Mixed-Membership Stochastic Block Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14530">http://arxiv.org/abs/2307.14530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fedor Noskov, Maxim Panov</li>
<li>for: 研究混合成员 Stochastic Block Model（MMSB），用于描述图像中的 overlap community 结构。</li>
<li>methods: 比较不同方法的可重建性，并提出一种新的估计器，实现最小最大 Lower Bound（LB）。</li>
<li>results: 在一系列实验中，证明了新估计器的可靠性和高效性。<details>
<summary>Abstract</summary>
Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. (2008). MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.
</details>
<details>
<summary>摘要</summary>
社区探测是现代网络科学中最关键的问题之一。它的应用可以在各个领域找到，从蛋白质模型到社会网络分析。最近，许多论文研究了过lapping community detection问题，其中每个网络节点可能属于多个社区。在这项工作中，我们考虑了Airoldi等人于2008年提出的混合会员随机块模型（MMSB）。MMSB提供了一个非常通用的社区结构模型化图的方法。我们的中心问题是根据观察网络重建社区之间的关系。我们比较了不同的方法，并证明了最小最大下界 Error的下界。然后，我们提出了一个新的估计器，与这个下界匹配。我们的理论结果在较为通用的模型假设下得到了证明。最后，我们在一系列实验中证明了我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Function-Value-Learning-Adaptive-Learning-Rates-Based-on-the-Polyak-Stepsize-and-Function-Splitting-in-ERM"><a href="#Function-Value-Learning-Adaptive-Learning-Rates-Based-on-the-Polyak-Stepsize-and-Function-Splitting-in-ERM" class="headerlink" title="Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM"></a>Function Value Learning: Adaptive Learning Rates Based on the Polyak Stepsize and Function Splitting in ERM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14528">http://arxiv.org/abs/2307.14528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Garrigos, Robert M. Gower, Fabian Schaipp</li>
<li>For: The paper focuses on solving a finite sum-of-terms problem, also known as empirical risk minimization, using stochastic gradient descent (SGD) with an adaptive step size.* Methods: The paper proposes two variants of SGD, called $\texttt{SPS}<em>+$ and $\texttt{FUVAL}$, which make use of the sampled loss values and gradually learn the loss values at optimality.* Results: The paper shows that $\texttt{SPS}</em>+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth, but the new $\texttt{FUVAL}$ method does not offer any clear theoretical or practical advantage over SGD.Here are the three information points in Simplified Chinese text:* For: 本文解决了一个finite sum-of-terms problem，也就是empirical risk minimization，使用stochastic gradient descent（SGD）的adaptive step size。* Methods: 本文提出了两种SGD变种，即$\texttt{SPS}<em>+$和$\texttt{FUVAL}$，它们利用样本损失值来进行优化。* Results: 本文证明了$\texttt{SPS}</em>+$在Lipschitz non-smooth中达到了最佳known的迭代速率，但新的$\texttt{FUVAL}$方法并没有明显的理论或实践优势。<details>
<summary>Abstract</summary>
Here we develop variants of SGD (stochastic gradient descent) with an adaptive step size that make use of the sampled loss values. In particular, we focus on solving a finite sum-of-terms problem, also known as empirical risk minimization. We first detail an idealized adaptive method called $\texttt{SPS}_+$ that makes use of the sampled loss values and assumes knowledge of the sampled loss at optimality. This $\texttt{SPS}_+$ is a minor modification of the SPS (Stochastic Polyak Stepsize) method, where the step size is enforced to be positive. We then show that $\texttt{SPS}_+$ achieves the best known rates of convergence for SGD in the Lipschitz non-smooth. We then move onto to develop $\texttt{FUVAL}$, a variant of $\texttt{SPS}_+$ where the loss values at optimality are gradually learned, as opposed to being given. We give three viewpoints of $\texttt{FUVAL}$, as a projection based method, as a variant of the prox-linear method, and then as a particular online SGD method. We then present a convergence analysis of $\texttt{FUVAL}$ and experimental results. The shortcomings of our work is that the convergence analysis of $\texttt{FUVAL}$ shows no advantage over SGD. Another shortcomming is that currently only the full batch version of $\texttt{FUVAL}$ shows a minor advantages of GD (Gradient Descent) in terms of sensitivity to the step size. The stochastic version shows no clear advantage over SGD. We conjecture that large mini-batches are required to make $\texttt{FUVAL}$ competitive.   Currently the new $\texttt{FUVAL}$ method studied in this paper does not offer any clear theoretical or practical advantage. We have chosen to make this draft available online nonetheless because of some of the analysis techniques we use, such as the non-smooth analysis of $\texttt{SPS}_+$, and also to show an apparently interesting approach that currently does not work.
</details>
<details>
<summary>摘要</summary>
我们在这里开发出了SGD（测量函数下降）的变体，具有适应步长的优点。具体来说，我们专注于解决一个总和形式的问题，也就是一个empirical risk minimization。我们首先介绍了一个理想化的自适应方法，called $\texttt{SPS}_+$,它使用抽象的损失值，并假设知道抽象损失的最佳值。这个 $\texttt{SPS}_+ $ 是 SPS（ Stochastic Polyak Stepsize）方法的小修改，步长强制为正。我们随后证明了 $\texttt{SPS}_+ $ 在 Lipschitz 非均匀中的最佳知识率。然后我们开发了 $\texttt{FUVAL} $, 这是 $\texttt{SPS}_+ $ 的一个改进版本，损失值在最佳值 Gradually 学习，而不是直接知道。我们从三个不同的角度来探讨 $\texttt{FUVAL} $, 分别是投影基于方法、变形的 prox-linear 方法和在线 SGD 方法。我们随后提供了 $\texttt{FUVAL} $ 的内部分析和实验结果。我们的研究的缺点是 $\texttt{FUVAL} $ 的内部分析不会提供任何优点，而且在某些情况下，SGD 可能比 $\texttt{FUVAL} $ 更为稳定。此外，目前只有全批量版本的 $\texttt{FUVAL} $ 表现出一定的优点，而测量版本则未能获得明显的优点。我们推测需要大小批量才能使 $\texttt{FUVAL} $ 竞争。总之，我们的新方法 $\texttt{FUVAL} $ 目前无法提供任何明显的理论或实践优点。不过，我们使用了一些有趣的分析技巧，例如非均匀分析 $\texttt{SPS}_+ $，以及展示了一个可能不太有用的方法。因此，我们选择发布这份草稿，以便在未来进一步探索这个方向。
</details></li>
</ul>
<hr>
<h2 id="Open-Problems-in-Computer-Vision-for-Wilderness-SAR-and-The-Search-for-Patricia-Wu-Murad"><a href="#Open-Problems-in-Computer-Vision-for-Wilderness-SAR-and-The-Search-for-Patricia-Wu-Murad" class="headerlink" title="Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad"></a>Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14527">http://arxiv.org/abs/2307.14527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crasar/wisar">https://github.com/crasar/wisar</a></li>
<li>paper_authors: Thomas Manzini, Robin Murphy</li>
<li>for: 这篇论文探讨了在日本 Wu-Murad 野外搜救（WSAR）活动中应用两种计算机视觉系统，即可靠的DET 超vised学习模型和无监督的RX spectral分类器，并发现了3个未来研究的方向。</li>
<li>methods: 这篇论文使用了98.9 GB的飞行器影像数据，并应用了EfficientDET 建模和RX spectral分类器来检测缺失人员。</li>
<li>results: 论文发现，现有的方法中只有3种（2个无监督的和1个结构不确定）在实际WSAR操作中被使用，并且选择了EfficientDET 建模和RX spectral分类器作为最佳选择。然而，实际应用中，EfficientDET 模型具有 statistically 等效的性能，但它在实际情况中存在许多假阳性（如 mistakenly 识别树枝和石头为人）和假负性（如不能识别搜救队成员）的问题。<details>
<summary>Abstract</summary>
This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks as people), and false negatives (e.g., failing to identify members of the search team). The poor results in practice for algorithms that showed good results on datasets suggest 3 areas of future research: more realistic datasets for wilderness SAR, computer vision models that are capable of seamlessly handling the variety of imagery that can be collected during actual WSAR operations, and better alignment on performance measures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>More realistic datasets for wilderness SAR: The current datasets used for training and testing computer vision models may not accurately reflect the real-world conditions and variability of imagery collected during actual WSAR operations.2. Computer vision models that can handle diverse imagery: The models need to be able to seamlessly handle the variety of imagery that can be collected during actual WSAR operations, including different lighting conditions, weather, and terrain.3. Better alignment on performance measures: The performance of the models needs to be evaluated using relevant and meaningful metrics that reflect the specific goals and requirements of WSAR operations.The paper also discusses the limitations of the EfficientDET model and the unsupervised spectral RX classifier, including their poor performance in real-world scenarios due to false positives (identifying tree limbs and rocks as people) and false negatives (failing to identify members of the search team).</details></li>
</ol>
<hr>
<h2 id="A-new-algorithm-for-Subgroup-Set-Discovery-based-on-Information-Gain"><a href="#A-new-algorithm-for-Subgroup-Set-Discovery-based-on-Information-Gain" class="headerlink" title="A new algorithm for Subgroup Set Discovery based on Information Gain"></a>A new algorithm for Subgroup Set Discovery based on Information Gain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15089">http://arxiv.org/abs/2307.15089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Gómez-Bravo, Aaron García, Guillermo Vigueras, Belén Ríos, Alejandro Rodríguez-González</li>
<li>For: The paper aims to propose a new pattern discovery algorithm called Information Gained Subgroup Discovery (IGSD) to address the limitations of state-of-the-art pattern discovery algorithms.* Methods: The IGSD algorithm combines Information Gain (IG) and Odds Ratio (OR) as multi-criteria for pattern selection. It also uses a novel approach to explore the subgroup space and evaluate the discovered patterns.* Results: The paper evaluates the performance of IGSD with two state-of-the-art pattern discovery algorithms (FSSD and SSD++) on 11 datasets. The results show that IGSD provides more reliable patterns and better agreement with domain experts than the other two algorithms. Additionally, IGSD provides better OR values, indicating a higher dependence between patterns and targets.Here’s the simplified Chinese text for the three key information points:*  для：本文提出了一种新的模式发现算法 called Information Gained Subgroup Discovery（IGSD），以解决现有模式发现算法的限制。* 方法：IGSD算法将信息增量（IG）和很大比率（OR）作为多个标准来选择模式。它还使用了一种新的方法来探索 subgroup空间和评估发现的模式。* 结果：本文对IGSD算法与两种现有模式发现算法（FSSD和SSD++）在11个数据集上进行了性能评估。结果显示，IGSD算法提供了更可靠的模式和与领域专家的一致性更高。此外，IGSD算法提供了更高的OR值，表明模式和目标之间的依赖性更高。<details>
<summary>Abstract</summary>
Pattern discovery is a machine learning technique that aims to find sets of items, subsequences, or substructures that are present in a dataset with a higher frequency value than a manually set threshold. This process helps to identify recurring patterns or relationships within the data, allowing for valuable insights and knowledge extraction. In this work, we propose Information Gained Subgroup Discovery (IGSD), a new SD algorithm for pattern discovery that combines Information Gain (IG) and Odds Ratio (OR) as a multi-criteria for pattern selection. The algorithm tries to tackle some limitations of state-of-the-art SD algorithms like the need for fine-tuning of key parameters for each dataset, usage of a single pattern search criteria set by hand, usage of non-overlapping data structures for subgroup space exploration, and the impossibility to search for patterns by fixing some relevant dataset variables. Thus, we compare the performance of IGSD with two state-of-the-art SD algorithms: FSSD and SSD++. Eleven datasets are assessed using these algorithms. For the performance evaluation, we also propose to complement standard SD measures with IG, OR, and p-value. Obtained results show that FSSD and SSD++ algorithms provide less reliable patterns and reduced sets of patterns than IGSD algorithm for all datasets considered. Additionally, IGSD provides better OR values than FSSD and SSD++, stating a higher dependence between patterns and targets. Moreover, patterns obtained for one of the datasets used, have been validated by a group of domain experts. Thus, patterns provided by IGSD show better agreement with experts than patterns obtained by FSSD and SSD++ algorithms. These results demonstrate the suitability of the IGSD as a method for pattern discovery and suggest that the inclusion of non-standard SD metrics allows to better evaluate discovered patterns.
</details>
<details>
<summary>摘要</summary>
“ patrern 发现”是一种机器学习技术，旨在找到数据集中出现频率较高的项集、 subsequences 或 substructures。这个过程可以帮助发现数据中的循环模式或关系，从而提供有价值的发现和知识提取。在这个工作中，我们提出了一种新的 patrern 发现算法，即 Information Gained Subgroup Discovery（IGSD）。该算法结合信息增益（IG）和偶极率（OR）作为多个评价标准，用于 patrern 选择。该算法希望解决现有 patrern 发现算法的一些局限性，如手动设置的关键参数、使用单一 patrern 搜索标准、非重叠数据结构等。因此，我们与现有 patrern 发现算法进行比较，包括 FSSD 和 SSD++。我们对 Eleven 个数据集进行了这些算法的评价。为了评价性能，我们还提出了一些不同于标准 patrern 搜索度量的方式，包括 IG、OR 和 p-value。结果显示，FSSD 和 SSD++ 算法提供的 patrern 较为不可靠，而 IGSD 算法对所有数据集都提供了更好的 patrern。此外，IGSD 算法对 patrern 和目标变量之间的相互关系提供了更高的依赖性。此外，IGSD 算法对一个数据集进行了验证，并得到了域专家的认可。因此，IGSD 算法提供的 patrern 更好地匹配专家的意见。这些结果表明 IGSD 算法是一种适合 patrern 发现的方法，并且包括非标准 patrern 搜索度量可以更好地评价发现的 patrern。
</details></li>
</ul>
<hr>
<h2 id="Bug-Characterization-in-Machine-Learning-based-Systems"><a href="#Bug-Characterization-in-Machine-Learning-based-Systems" class="headerlink" title="Bug Characterization in Machine Learning-based Systems"></a>Bug Characterization in Machine Learning-based Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14512">http://arxiv.org/abs/2307.14512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-bugs-2022/replication-package">https://github.com/ml-bugs-2022/replication-package</a></li>
<li>paper_authors: Mohammad Mehdi Morovati, Amin Nikanjam, Florian Tambon, Foutse Khomh, Zhen Ming, Jiang</li>
<li>for: This paper aims to investigate the characteristics of bugs in Machine Learning (ML)-based software systems and the differences between ML and non-ML bugs from a maintenance viewpoint.</li>
<li>methods: The paper uses a dataset of 447,948 GitHub repositories that use one of the three most popular ML frameworks (TensorFlow, Keras, and PyTorch) to extract 300 repositories with the highest number of closed issues. The authors then manually inspect the extracted repositories to exclude non-ML-based systems, and investigate 386 sampled reported issues to determine whether they affect ML components or not.</li>
<li>results: The paper finds that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. The authors also identify the root causes, symptoms, and required fixing time for 109 identified ML bugs, and find that ML bugs have significantly different characteristics compared to non-ML bugs in terms of the complexity of bug-fixing. The results suggest that fixing ML bugs is more costly and ML components are more error-prone compared to non-ML bugs and non-ML components respectively, highlighting the importance of paying attention to the reliability of ML components in ML-based systems.<details>
<summary>Abstract</summary>
Rapid growth of applying Machine Learning (ML) in different domains, especially in safety-critical areas, increases the need for reliable ML components, i.e., a software component operating based on ML. Understanding the bugs characteristics and maintenance challenges in ML-based systems can help developers of these systems to identify where to focus maintenance and testing efforts, by giving insights into the most error-prone components, most common bugs, etc. In this paper, we investigate the characteristics of bugs in ML-based software systems and the difference between ML and non-ML bugs from the maintenance viewpoint. We extracted 447,948 GitHub repositories that used one of the three most popular ML frameworks, i.e., TensorFlow, Keras, and PyTorch. After multiple filtering steps, we select the top 300 repositories with the highest number of closed issues. We manually investigate the extracted repositories to exclude non-ML-based systems. Our investigation involved a manual inspection of 386 sampled reported issues in the identified ML-based systems to indicate whether they affect ML components or not. Our analysis shows that nearly half of the real issues reported in ML-based systems are ML bugs, indicating that ML components are more error-prone than non-ML components. Next, we thoroughly examined 109 identified ML bugs to identify their root causes, symptoms, and calculate their required fixing time. The results also revealed that ML bugs have significantly different characteristics compared to non-ML bugs, in terms of the complexity of bug-fixing (number of commits, changed files, and changed lines of code). Based on our results, fixing ML bugs are more costly and ML components are more error-prone, compared to non-ML bugs and non-ML components respectively. Hence, paying a significant attention to the reliability of the ML components is crucial in ML-based systems.
</details>
<details>
<summary>摘要</summary>
Machine Learning (ML) 在不同领域的快速应用导致了可靠 ML 组件的需求增加，即基于 ML 的软件组件。了解 ML 系统中 bug 的特点和维护挑战可以帮助 ML 系统开发者identify где要集中维护和测试努力，提供了关于最常出现的bug、最常出现的错误等信息。在这篇论文中，我们研究了 ML 系统中 bug 的特点和非 ML 系统中 bug 的区别从维护角度来 investigate。我们从 GitHub 上抽取了使用最Popular ML 框架之一的300个仓库，并经过多个筛选步骤后，选择了最高数量关闭 Issue 的仓库。我们手动验证了提取的仓库，以确保它们是 ML 基于系统。我们的调查包括对386个样本问题的手动检查，以确定它们是否affect ML 组件。我们的分析结果表明，ML 系统中 Reported 的实际问题大约占 ML 系统总数的46.7%，表明 ML 组件比非 ML 组件更容易出现错误。接着，我们对109个 ID 为 ML 错误的问题进行了详细分析，以确定其根本原因、症状和修复时间。结果还表明，ML 错误和非 ML 错误在修复复杂性、修复时间等方面有很大差异。根据我们的结果，修复 ML 错误需要更多的时间和精力，而 ML 组件也更容易出现错误。因此，在 ML 基于系统中，需要付出更大的注意力来确保 ML 组件的可靠性。
</details></li>
</ul>
<hr>
<h2 id="A-Predictive-Model-of-Digital-Information-Engagement-Forecasting-User-Engagement-With-English-Words-by-Incorporating-Cognitive-Biases-Computational-Linguistics-and-Natural-Language-Processing"><a href="#A-Predictive-Model-of-Digital-Information-Engagement-Forecasting-User-Engagement-With-English-Words-by-Incorporating-Cognitive-Biases-Computational-Linguistics-and-Natural-Language-Processing" class="headerlink" title="A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing"></a>A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14500">http://arxiv.org/abs/2307.14500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nimrod Dvir, Elaine Friedman, Suraj Commuri, Fan yang, Jennifer Romano</li>
<li>for: 这种研究旨在开发一种用于数字信息参与度（IE）的预测模型，即READ模型，该模型基于汇集了主要认知偏见和计算语言学的理论框架，以提供一种多维度的信息参与度观察。</li>
<li>methods: 该研究使用了50个随机选择的同义词对（共100个词）从WordNet数据库，通过在线调查（参与者数为80,500人）测量这些词的参与度，以计算READ属性。</li>
<li>results: 研究发现，READ模型能够准确预测一个词的参与度，并在84%的 случа子中能够 correctly distinguish 词与其同义词之间的参与度。该模型在不同领域，如商业、教育、政府和医疗等领域，可能有效地提高内容参与度和AI语言模型的发展。<details>
<summary>Abstract</summary>
This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engaging word from a pair of synonyms with an 84% accuracy rate. The READ model's potential extends across various domains, including business, education, government, and healthcare, where it could enhance content engagement and inform AI language model development and generative text work. Future research should address the model's scalability and adaptability across different domains and languages, thereby broadening its applicability and efficacy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HUGE-Huge-Unsupervised-Graph-Embeddings-with-TPUs"><a href="#HUGE-Huge-Unsupervised-Graph-Embeddings-with-TPUs" class="headerlink" title="HUGE: Huge Unsupervised Graph Embeddings with TPUs"></a>HUGE: Huge Unsupervised Graph Embeddings with TPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14490">http://arxiv.org/abs/2307.14490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brandon Mayer, Anton Tsitsulin, Hendrik Fichtenberger, Jonathan Halcrow, Bryan Perozzi</li>
<li>for: 这篇论文是为了快速分析大规模图像而设计的，以便解决后续机器学习任务。</li>
<li>methods: 论文使用了Tensor Processing Units（TPUs）和可配置的高带宽内存来实现高性能图嵌入架构，以解决大规模图像的嵌入问题。</li>
<li>results: 论文验证了嵌入空间质量在真实和 sintetic 大规模数据集上，并达到了高性能。<details>
<summary>Abstract</summary>
Graphs are a representation of structured data that captures the relationships between sets of objects. With the ubiquity of available network data, there is increasing industrial and academic need to quickly analyze graphs with billions of nodes and trillions of edges. A common first step for network understanding is Graph Embedding, the process of creating a continuous representation of nodes in a graph. A continuous representation is often more amenable, especially at scale, for solving downstream machine learning tasks such as classification, link prediction, and clustering. A high-performance graph embedding architecture leveraging Tensor Processing Units (TPUs) with configurable amounts of high-bandwidth memory is presented that simplifies the graph embedding problem and can scale to graphs with billions of nodes and trillions of edges. We verify the embedding space quality on real and synthetic large-scale datasets.
</details>
<details>
<summary>摘要</summary>
GRAPH Embedding 是一种将图像转换为连续表示的技术，以便更好地解决大规模图像的下游机器学习任务，如分类、链接预测和团 clustering。在大量网络数据的时代，快速分析大规模图像变得越来越重要。我们提出了一种高性能的图 Embedding 架构，利用 tensor Processing Units (TPUs) 和可配置的高带宽内存，可以简化图 Embedding 问题，并可扩展到百亿个节点和万亿个边的图像。我们对真实和 sintetic 大规模数据进行了验证，以证明 embedding 空间质量。Note: "GRAPH" is written in capital letters in Simplified Chinese to emphasize the importance of the concept.
</details></li>
</ul>
<hr>
<h2 id="Role-of-Image-Acquisition-and-Patient-Phenotype-Variations-in-Automatic-Segmentation-Model-Generalization"><a href="#Role-of-Image-Acquisition-and-Patient-Phenotype-Variations-in-Automatic-Segmentation-Model-Generalization" class="headerlink" title="Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization"></a>Role of Image Acquisition and Patient Phenotype Variations in Automatic Segmentation Model Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14482">http://arxiv.org/abs/2307.14482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothy L. Kline, Sumana Ramanathan, Harrison C. Gottlich, Panagiotis Korfiatis, Adriana V. Gregory</li>
<li>for: 这个研究是为了评估自动医疗影像分割模型在不同频谱和疾病类型上的性能和泛化能力。</li>
<li>methods: 研究使用了多种数据集，包括非对照和对照肝脏CT影像数据，并对这些数据进行了100个示例的训练和验证，以便分割肝脏、胆囊和脾脏。最终的模型被测试在100个非对照PKD病人的CT影像上。性能被评估使用了 dice相似度、Jacard相似度、TPR和精度。</li>
<li>results: 研究发现，使用更广泛的数据集可以提高模型的泛化性能和外域性能，无需额外训练或特定的疾病类型。例如，模型通过在25%的数据集上训练，与仅使用域内数据进行训练相同的 dice相似度。I hope that helps!<details>
<summary>Abstract</summary>
Purpose: This study evaluated the out-of-domain performance and generalization capabilities of automated medical image segmentation models, with a particular focus on adaptation to new image acquisitions and disease type.   Materials: Datasets from both non-contrast and contrast-enhanced abdominal CT scans of healthy patients and those with polycystic kidney disease (PKD) were used. A total of 400 images (100 non-contrast controls, 100 contrast controls, 100 non-contrast PKD, 100 contrast PKD) were utilized for training/validation of models to segment kidneys, livers, and spleens, and the final models were then tested on 100 non-contrast CT images of patients affected by PKD. Performance was evaluated using Dice, Jaccard, TPR, and Precision.   Results: Models trained on a diverse range of data showed no worse performance than models trained exclusively on in-domain data when tested on in-domain data. For instance, the Dice similarity of the model trained on 25% from each dataset was found to be non-inferior to the model trained purely on in-domain data.   Conclusions: The results indicate that broader training examples significantly enhances model generalization and out-of-domain performance, thereby improving automated segmentation tools' applicability in clinical settings. The study's findings provide a roadmap for future research to adopt a data-centric approach in medical image AI model development.
</details>
<details>
<summary>摘要</summary>
目的：本研究评估了自动医疗图像分割模型的域外性和总体可扩展性，尤其是适应新图像获取和疾病类型的适应性。材料：来自非对照和增强的腹部CT扫描图像的健康患者和肾脏癌病（PKD）的数据集被使用。总共使用400张图像（100张非对照控制、100张对照控制、100张非对照PKD、100张对照PKD）进行模型训练/验证，并将最终模型测试在100张非对照CT图像上。性能被评估使用 dice、jaccard、TPR和精度。结果：模型训练在多样化的数据上表现和专门训练在域内数据上无分别性能。例如，模型训练使用25%的数据从每个数据集得到的 dice相似性与专门训练在域内数据上的模型相似。结论：结果表明，更广泛的训练示例可以显著提高模型的总体可扩展性和域外性，因此提高自动分割工具在临床应用中的可靠性。这项研究的结果为未来医疗图像AI模型开发提供了一个路线图。
</details></li>
</ul>
<hr>
<h2 id="Equitable-Time-Varying-Pricing-Tariff-Design-A-Joint-Learning-and-Optimization-Approach"><a href="#Equitable-Time-Varying-Pricing-Tariff-Design-A-Joint-Learning-and-Optimization-Approach" class="headerlink" title="Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach"></a>Equitable Time-Varying Pricing Tariff Design: A Joint Learning and Optimization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15088">http://arxiv.org/abs/2307.15088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liudong Chen, Bolun Xu</li>
<li>for: 这个论文的目的是设计合理的时间变化价格政策，以激励消费者减少电力峰值需求，同时保护低收入消费者免受价格涨升的影响。</li>
<li>methods: 该论文提出一种基于学习联合标定优化方法，通过历史价格和需求响应数据来捕捉高维和非线性消费者价格响应行为。</li>
<li>results: 模拟使用实际消费者数据显示，我们的公平价格政策能够保护低收入消费者免受价格涨升的影响，同时有效激励消费者减少峰值需求，使得供应商公司能够实现收益回报。<details>
<summary>Abstract</summary>
Time-varying pricing tariffs incentivize consumers to shift their electricity demand and reduce costs, but may increase the energy burden for consumers with limited response capability. The utility must thus balance affordability and response incentives when designing these tariffs by considering consumers' response expectations. This paper proposes a joint learning-based identification and optimization method to design equitable time-varying tariffs. Our proposed method encodes historical prices and demand response data into a recurrent neural network (RNN) to capture high-dimensional and non-linear consumer price response behaviors. We then embed the RNN into the tariff design optimization, formulating a non-linear optimization problem with a quadratic objective. We propose a gradient-based solution method that achieves fast and scalable computation. Simulation using real-world consumer data shows that our equitable tariffs protect low-income consumers from price surges while effectively motivating consumers to reduce peak demand. The method also ensures revenue recovery for the utility company and achieves robust performance against demand response uncertainties and prediction errors.
</details>
<details>
<summary>摘要</summary>
时间变化的价格批价可以鼓励消费者调整电力需求，从而降低成本，但可能增加有限响应能力的消费者的能源负担。公司因此需要平衡可持续性和响应奖励，当设计时变化价格时。本文提出一种基于学习的同时标识和优化方法，用于设计公平的时变价格。我们使用循环神经网络（RNN）编码历史价格和需求响应数据，以捕捉高维和非线性的消费者价格响应行为。然后，我们将RNNembed到价格设计优化中，形成非线性优化问题的quadratic对象。我们提出一种梯度基于的解决方案，可以实现快速和可扩展的计算。通过使用实际的消费者数据进行模拟，我们发现我们的公平价格可以保护低收入消费者免受价格涨升，同时有效地鼓励消费者减少峰值需求。此外，我们的方法还确保了公司的收益恢复和对需求响应不确定性和预测错误的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Limits-to-Reservoir-Learning"><a href="#Limits-to-Reservoir-Learning" class="headerlink" title="Limits to Reservoir Learning"></a>Limits to Reservoir Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14474">http://arxiv.org/abs/2307.14474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony M. Polloreno</li>
<li>for: 这 paper 是研究机器学习的能力受到物理限制的工作。</li>
<li>methods: 作者使用信息处理容量（IPC）来衡量受到噪声的干扰下，某种特定的循环网络（即散射器）的性能下降。</li>
<li>results: 作者发现，IPC 是最多是一个多项式函数关系于系统大小 n，而且在噪声存在的情况下，这种干扰会使某种函数家族需要 exponential 数量的样本来学习。<details>
<summary>Abstract</summary>
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们约束机器学习的能力基于物理限制所含的计算限制。我们开始思考信息处理容量（IPC），一种正规化的函数收敛率的标准化量。我们使用IPC来度量噪声下机器学习性能的下降，特别是当受到物理限制时。首先，我们证明IPC在系统大小$n$上是最多一个多项式函数。接着，我们 argue that这种下降表明了由泵函数表示的家族需要很多样本来在噪声下学习。最后，我们讨论无噪声情况下同样的$2^n$函数在二分类 зада务中的表现。
</details></li>
</ul>
<hr>
<h2 id="What-Kinds-of-Contracts-Do-ML-APIs-Need"><a href="#What-Kinds-of-Contracts-Do-ML-APIs-Need" class="headerlink" title="What Kinds of Contracts Do ML APIs Need?"></a>What Kinds of Contracts Do ML APIs Need?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14465">http://arxiv.org/abs/2307.14465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samantha Syeda Khairunnesa, Shibbir Ahmed, Sayem Mohammad Imtiaz, Hridesh Rajan, Gary T. Leavens</li>
<li>For: This paper aims to identify the most commonly needed contracts for Machine Learning (ML) APIs and understand the root causes and effects of ML contract violations.* Methods: The authors conducted an empirical study of posts on Stack Overflow to extract 413 informal API specifications for the four most often-discussed ML libraries (TensorFlow, Scikit-learn, Keras, and PyTorch). They used these specifications to understand the common patterns of ML contract violations and the need for advanced ML software expertise to understand ML contracts.* Results: The study found that the most commonly needed contracts for ML APIs are checking constraints on single arguments of an API or on the order of API calls. The authors also noted a need to combine behavioral and temporal contract mining approaches to better understand ML APIs and design effective contract languages.<details>
<summary>Abstract</summary>
Recent work has shown that Machine Learning (ML) programs are error-prone and called for contracts for ML code. Contracts, as in the design by contract methodology, help document APIs and aid API users in writing correct code. The question is: what kinds of contracts would provide the most help to API users? We are especially interested in what kinds of contracts help API users catch errors at earlier stages in the ML pipeline. We describe an empirical study of posts on Stack Overflow of the four most often-discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, our study extracted 413 informal (English) API specifications. We used these specifications to understand the following questions. What are the root causes and effects behind ML contract violations? Are there common patterns of ML contract violations? When does understanding ML contracts require an advanced level of ML software expertise? Could checking contracts at the API level help detect the violations in early ML pipeline stages? Our key findings are that the most commonly needed contracts for ML APIs are either checking constraints on single arguments of an API or on the order of API calls. The software engineering community could employ existing contract mining approaches to mine these contracts to promote an increased understanding of ML APIs. We also noted a need to combine behavioral and temporal contract mining approaches. We report on categories of required ML contracts, which may help designers of contract languages.
</details>
<details>
<summary>摘要</summary>
We conducted an empirical study of posts on Stack Overflow about the four most commonly discussed ML libraries: TensorFlow, Scikit-learn, Keras, and PyTorch. For these libraries, we extracted 413 informal (English) API specifications. We used these specifications to answer the following questions:1. What are the root causes and effects of ML contract violations?2. Are there common patterns of ML contract violations?3. When does understanding ML contracts require an advanced level of ML software expertise?4. Can checking contracts at the API level help detect violations in early ML pipeline stages?Our key findings are that the most commonly needed contracts for ML APIs are:1. Checking constraints on single arguments of an API2. Checking the order of API callsWe also noted a need to combine behavioral and temporal contract mining approaches. Additionally, we categorized the required ML contracts, which may help designers of contract languages.
</details></li>
</ul>
<hr>
<h2 id="Training-Quantum-Boltzmann-Machines-with-Coresets"><a href="#Training-Quantum-Boltzmann-Machines-with-Coresets" class="headerlink" title="Training Quantum Boltzmann Machines with Coresets"></a>Training Quantum Boltzmann Machines with Coresets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14459">http://arxiv.org/abs/2307.14459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Viszlai, Teague Tomesh, Pranav Gokhale, Eric Anschuetz, Frederic T. Chong</li>
<li>for: 加速近期量子算法在半导体设备上的应用，使用核心集技术。</li>
<li>methods: 使用核心集取代全数据集，以降低训练时间。</li>
<li>results: 使用核心集可以大幅减少训练时间，减少量子计算机的计算时间。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Recent work has proposed and explored using coreset techniques for quantum algorithms that operate on classical data sets to accelerate the applicability of these algorithms on near-term quantum devices. We apply these ideas to Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs state sampling are the main computational bottleneck during training. By using a coreset in place of the full data set, we try to minimize the number of steps needed and accelerate the overall training time. In a regime where computational time on quantum computers is a precious resource, we propose this might lead to substantial practical savings. We evaluate this approach on 6x6 binary images from an augmented bars and stripes data set using a QBM with 36 visible units and 8 hidden units. Using an Inception score inspired metric, we compare QBM training times with and without using coresets.
</details>
<details>
<summary>摘要</summary>
近期的工作已经提出了和探索了使用核Set技术来加速近期quantum设备上的量子算法运行，以便在这些算法的应用中减少计算时间。我们在Quantum Boltzmann Machines（QBM）中应用这些想法，其中梯度based步骤的主要计算瓶颈在训练中是Gibbs状态抽样。通过使用核Set而不是全量数据集，我们尝试将训练时间缩短。在计算时间在量子计算机上是珍贵资源的情况下，我们建议这可能导致实质性的实用性级别。我们使用6x6的二进制图像从一个扩展的棒和条纹数据集，使用一个QBM的可见单元为36个，隐藏单元为8个进行训练。使用基于Inception metric的评价标准，我们比较了在使用核Set和没有使用核Set的情况下QBM训练时间。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Maintenance-of-Armoured-Vehicles-using-Machine-Learning-Approaches"><a href="#Predictive-Maintenance-of-Armoured-Vehicles-using-Machine-Learning-Approaches" class="headerlink" title="Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches"></a>Predictive Maintenance of Armoured Vehicles using Machine Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14453">http://arxiv.org/abs/2307.14453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajit Sengupta, Anant Mehta, Prashant Singh Rana</li>
<li>for: 这个研究旨在提出一种基于预测维护的 ensemble 系统，用于预测 armoured 车的维护需求，以提高车辆的运作效率和可靠性。</li>
<li>methods: 该模型采用了多种模型，如光树落准、Random Forest、决策树、Extra Tree Classifier 和 Gradient Boosting，以准确预测车辆的维护需求。 plus 使用了 K-fold 十字验证和 TOPSIS 分析来评估模型的稳定性。</li>
<li>results: 结果显示，提议的系统具有98.93%的准确率、99.80%的精度和99.03%的回归率，能够有效预测车辆的维护需求，从而降低车辆的停机时间和提高运作效率。<details>
<summary>Abstract</summary>
Armoured vehicles are specialized and complex pieces of machinery designed to operate in high-stress environments, often in combat or tactical situations. This study proposes a predictive maintenance-based ensemble system that aids in predicting potential maintenance needs based on sensor data collected from these vehicles. The proposed model's architecture involves various models such as Light Gradient Boosting, Random Forest, Decision Tree, Extra Tree Classifier and Gradient Boosting to predict the maintenance requirements of the vehicles accurately. In addition, K-fold cross validation, along with TOPSIS analysis, is employed to evaluate the proposed ensemble model's stability. The results indicate that the proposed system achieves an accuracy of 98.93%, precision of 99.80% and recall of 99.03%. The algorithm can effectively predict maintenance needs, thereby reducing vehicle downtime and improving operational efficiency. Through comparisons between various algorithms and the suggested ensemble, this study highlights the potential of machine learning-based predictive maintenance solutions.
</details>
<details>
<summary>摘要</summary>
armored vehicles 是特殊化和复杂的机器设备，设计用于高压环境下运行，经常在战斗或战术情况下使用。本研究提出了一种基于预测维护的ensemble系统，用于预测这些车辆的维护需求。提议的模型体系包括了各种模型，如光梯度抛光、随机森林、决策树、附加树分类器和梯度抛光。此外， employ K-fold Cross Validation 和 TOPSIS分析来评估提议的ensemble模型的稳定性。结果表明，提议的系统实现了98.93%的准确率、99.80%的精度和99.03%的回归率。这个算法可以有效预测维护需求，从而减少车辆的停机时间，提高运作效率。通过对不同算法和建议的ensemble进行比较，本研究强调了机器学习基于预测维护解决方案的潜力。
</details></li>
</ul>
<hr>
<h2 id="VISPUR-Visual-Aids-for-Identifying-and-Interpreting-Spurious-Associations-in-Data-Driven-Decisions"><a href="#VISPUR-Visual-Aids-for-Identifying-and-Interpreting-Spurious-Associations-in-Data-Driven-Decisions" class="headerlink" title="VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions"></a>VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14448">http://arxiv.org/abs/2307.14448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/picsolab/vispur">https://github.com/picsolab/vispur</a></li>
<li>paper_authors: Xian Teng, Yongsu Ahn, Yu-Ru Lin</li>
<li>for: 该论文旨在帮助人们在受到大数据和机器学习工具支持的情况下，更好地识别和理解干扰因素导致的假关系。</li>
<li>methods: 该论文提出了一种可见分析框架和一个人类中心的工作流程，用于检测和解决干扰因素导致的假关系。其中包括一个干扰因素仪表板，可以自动标识可能的干扰因素，以及一个 subgroup 视图器，可以视觉化和比较不同 subgroup 的各种模式，以避免因果混乱的误解。</li>
<li>results: 我们通过专家采访和控制的用户实验，证明了我们提出的 “de-paradox” 工作流程和设计的可见分析系统是有效的，帮助人们更好地识别和理解干扰因素导致的假关系，以及做出可识别的决策。<details>
<summary>Abstract</summary>
Big data and machine learning tools have jointly empowered humans in making data-driven decisions. However, many of them capture empirical associations that might be spurious due to confounding factors and subgroup heterogeneity. The famous Simpson's paradox is such a phenomenon where aggregated and subgroup-level associations contradict with each other, causing cognitive confusions and difficulty in making adequate interpretations and decisions. Existing tools provide little insights for humans to locate, reason about, and prevent pitfalls of spurious association in practice. We propose VISPUR, a visual analytic system that provides a causal analysis framework and a human-centric workflow for tackling spurious associations. These include a CONFOUNDER DASHBOARD, which can automatically identify possible confounding factors, and a SUBGROUP VIEWER, which allows for the visualization and comparison of diverse subgroup patterns that likely or potentially result in a misinterpretation of causality. Additionally, we propose a REASONING STORYBOARD, which uses a flow-based approach to illustrate paradoxical phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure accountable decision-making. Through an expert interview and a controlled user experiment, our qualitative and quantitative results demonstrate that the proposed "de-paradox" workflow and the designed visual analytic system are effective in helping human users to identify and understand spurious associations, as well as to make accountable causal decisions.
</details>
<details>
<summary>摘要</summary>
大数据和机器学习工具已经共同强化了人类在基于数据的决策中的能力。然而，许多其中捕捉到了偶合关系，这些关系可能因为干扰因素和 subgroup 多样性而是假的。例如，西мп逊的 парадокс是这种现象，其中总体和 subgroup 级别的关系相互矛盾，导致认知混乱和不能正确地解释和决策。现有工具提供的知识对 humans 来说太少，以至于无法在实践中找到、理解和避免偶合关系的坑。我们提出了 VISPUR，一个视觉分析系统，它提供了一种 causal 分析框架和一个人类中心的工作流程，用于解决偶合关系。这些包括一个 CONFOUNDER DASHBOARD，可以自动Identify possible confounding factors，以及一个 SUBGROUP VIEWER，可以将多个 subgroup 的各种模式视觉化和比较，以便更好地理解和分析 causality。此外，我们还提出了一个 REASONING STORYBOARD，使用流程方式来描述悖论现象，以及一个交互式的 DECISION DIAGNOSIS 面板，帮助确保决策是合理的。经过专家采访和控制的用户试验，我们的质量和量度结果表明，我们提出的 "de-paradox" 工作流程和设计的视觉分析系统都是有效的，帮助人类用户更好地找到、理解和解决偶合关系，以及做出负责任的 causal 决策。
</details></li>
</ul>
<hr>
<h2 id="Neural-Schrodinger-Bridge-with-Sinkhorn-Losses-Application-to-Data-driven-Minimum-Effort-Control-of-Colloidal-Self-assembly"><a href="#Neural-Schrodinger-Bridge-with-Sinkhorn-Losses-Application-to-Data-driven-Minimum-Effort-Control-of-Colloidal-Self-assembly" class="headerlink" title="Neural Schrödinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly"></a>Neural Schrödinger Bridge with Sinkhorn Losses: Application to Data-driven Minimum Effort Control of Colloidal Self-assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14442">http://arxiv.org/abs/2307.14442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Nodozi, Charlie Yan, Mira Khare, Abhishek Halder, Ali Mesbah</li>
<li>for: 这个论文是为了研究溶解自组装的最小努力控制问题而写的。</li>
<li>methods: 这篇论文使用了一种名为“神经学桥”的数据驱动学习和控制框架，以解决一类通过扩展Schrödinger桥问题来处理溶解自组装控制问题。</li>
<li>results: 研究人员通过使用分子动力学模拟数据和神经网络来学习控制拖动和扩散系数，并使用这些系数来训练一个特定于这类控制问题的神经网络，以解决溶解自组装控制问题。<details>
<summary>Abstract</summary>
We show that the minimum effort control of colloidal self-assembly can be naturally formulated in the order-parameter space as a generalized Schr\"odinger bridge problem -- a class of fixed-horizon stochastic optimal control problems that originated in the works of Erwin Schr\"odinger in the early 1930s. In recent years, this class of problems has seen a resurgence of research activities in control and machine learning communities. Different from the existing literature on the theory and computation for such problems, the controlled drift and diffusion coefficients for colloidal self-assembly are typically non-affine in control, and are difficult to obtain from physics-based modeling. We deduce the conditions of optimality for such generalized problems, and show that the resulting system of equations is structurally very different from the existing results in a way that standard computational approaches no longer apply. Thus motivated, we propose a data-driven learning and control framework, named `neural Schr\"odinger bridge', to solve such generalized Schr\"odinger bridge problems by innovating on recent advances in neural networks. We illustrate the effectiveness of the proposed framework using a numerical case study of colloidal self-assembly. We learn the controlled drift and diffusion coefficients as two neural networks using molecular dynamics simulation data, and then use these two to train a third network with Sinkhorn losses designed for distributional endpoint constraints, specific for this class of control problems.
</details>
<details>
<summary>摘要</summary>
我们显示，材料自组装的最小努力控制可以自然地表示为一种通用的Schrödinger桥问题，这是Erwin Schrödinger在30年代初期提出的一类固定时间 horizon随机控制问题。在最近几年，这类问题在控制和机器学习领域中得到了新的研究活动。与现有Literature不同，控制拖拽和扩散系数对材料自组装通常是非线性的，从物理模型中难以获得。我们推导了这类总体最优条件，并发现其系统方程与现有结果 Structurally very different，使得标准计算方法不再适用。因此，我们提出了一种基于数据驱动学习和控制的框架，名为“神经Schrödinger桥”，用于解决这类通用Schrödinger桥问题。我们通过一个数字 caso study of colloidal self-assembly illustrate the effectiveness of the proposed framework，我们通过分子动力学模拟数据学习控制拖拽和扩散系数为两个神经网络，然后使用这两个网络来训练第三个网络，用Sinkhorn损失函数，特定于这类控制问题的分布端点约束。
</details></li>
</ul>
<hr>
<h2 id="Fixed-Integral-Neural-Networks"><a href="#Fixed-Integral-Neural-Networks" class="headerlink" title="Fixed Integral Neural Networks"></a>Fixed Integral Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14439">http://arxiv.org/abs/2307.14439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Kortvelesy</li>
<li>for: 本研究旨在解决通常通过数值方法进行的神经网络中函数 интеграル计算的问题，提供一种能够确定神经网络函数 интеграル的方法。</li>
<li>methods: 本研究使用了一种基于约束的神经网络模型，其中约束直接应用于函数 интеграル中。此外，研究还提出了一种方法来保证函数 интеграル为正，这是许多应用中必需的条件。</li>
<li>results: 研究得到了一种能够确定神经网络函数 интеграル的方法，并且可以应用于许多应用中，如概率分布、距离度量等。此外，研究还发现了一些约束神经网络的特性和应用场景。<details>
<summary>Abstract</summary>
It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
</details>
<details>
<summary>摘要</summary>
通常情况下，使用神经网络学习的函数 интеграル是一个有用的技术。然而，通常是通过数值方法来实现这种 интеграル计算，因为对神经网络学习函数的分析性 интеграル是一个通常被视为无法计算的问题。在这项工作中，我们提出了一种方法来计算神经网络学习函数 $f$ 的分析性 интеграル。这使得神经网络的精确 интеграル可以被计算出来，并且可以通过直接应用约束来 parametrise 受限神经网络。其中，我们还提出了一种方法来约束 $f$ 为正，这是许多应用中的必要条件（例如概率分布、距离度量等）。最后，我们介绍了一些应用场景，where our fixed-integral neural network (FINN) can be used.
</details></li>
</ul>
<hr>
<h2 id="Skill-it-A-Data-Driven-Skills-Framework-for-Understanding-and-Training-Language-Models"><a href="#Skill-it-A-Data-Driven-Skills-Framework-for-Understanding-and-Training-Language-Models" class="headerlink" title="Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models"></a>Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14430">http://arxiv.org/abs/2307.14430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayee F. Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, Christopher Ré</li>
<li>for: 这个论文的目的是研究如何使用有限Token数据进行语言模型的训练，以提高其下游任务的性能。</li>
<li>methods: 该论文提出了一个新的框架，基于人类学习的自然顺序来理解语言模型如何学习从其训练数据中的技能。这个框架使用了一种新的在线数据采样算法，它可以在不同任务之间进行循环训练，从而提高模型的性能。</li>
<li>results: 通过使用这个框架和采样算法， authors 在 Synthetic LEGO 数据集和 Natural Instructions 数据集上进行了实验，并证明了这种方法可以提高模型的性能。在 continual pre-training  Setting中，Skill-It 算法在 LEGO 数据集上取得了36.5个点更高的准确率，而在 fine-tuning  Setting中，它在 target 技能上降低了13.6%的验证损失。此外， authors 还在 RedPajama 数据集上使用了这种方法，并在 1B 个Token数据上训练了一个 3B 参数的语言模型，并取得了高于基eline方法的准确率。<details>
<summary>Abstract</summary>
The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.
</details>
<details>
<summary>摘要</summary>
“训练数据质量对预训练大语言模型（LM）的性能产生很大影响。我们研究如何在固定的字符数限制下选择数据，以确保在任务之间具有良好的下游模型性能。我们提出了一新的框架，基于一个简单的假设：人类在意图的顺序中学习了一系列技能，语言模型也会在它们的训练数据中学习一个自然的顺序。如果这种顺序存在，那么可以用于更好地理解LMs，以及进行数据效率的训练。使用这种假设，我们的框架将技能和相关数据进行了正式的定义。我们首先使用 sintetic 和实际数据示出，这些顺序技能集存在，并且它们的存在可以使更高级别的技能在更少的数据上学习。其次，我们根据我们提出的框架，引入了一种在混合技能上进行线上数据采样的算法，叫Skill-It。在不断预训练和精度训练的情况下，Skill-It 可以更高效地学习多种技能。在 LEGO  sintetic 上，Skill-It 在不断预训练情况下取得了36.5个点更高的准确率。在 Natural Instructions 数据集上，Skill-It 在精度训练情况下降低了目标技能的验证损失13.6%。我们在最近的 RedPajama 数据集上应用了我们的技能框架，在3B-参数 LM 中进行不断预训练，以达到LM Evaluation Harness 的更高准确率，比基eline方法（随机采样数据源）的3B个字符Token。”
</details></li>
</ul>
<hr>
<h2 id="TabR-Unlocking-the-Power-of-Retrieval-Augmented-Tabular-Deep-Learning"><a href="#TabR-Unlocking-the-Power-of-Retrieval-Augmented-Tabular-Deep-Learning" class="headerlink" title="TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning"></a>TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14338">http://arxiv.org/abs/2307.14338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yandex-research/tabular-dl-tabr">https://github.com/yandex-research/tabular-dl-tabr</a></li>
<li>paper_authors: Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, Artem Babenko</li>
<li>for: 该研究是为了解决表格数据问题上的深度学习（DL）模型是否值得投入研究的问题。</li>
<li>methods: 该研究使用了一种基于注意力的搜索组件，将简单的扩展报文Architecture与 Retrieval-based 模型结合在一起，并对注意力机制的几个细节进行了优化。</li>
<li>results: 该研究在一些公共的benchmark上达到了最佳平均性能，超过了其他表格DL模型，并在“GBDT友好”的benchmark上超越了GBDT模型。<details>
<summary>Abstract</summary>
Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.   In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture with an attention-like retrieval component similar to those of many (tabular) retrieval-based models. Then, we highlight several details of the attention mechanism that turn out to have a massive impact on the performance on tabular data problems, but that were not explored in prior work. As a result, we design TabR -- a simple retrieval-based tabular DL model which, on a set of public benchmarks, demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed ``GBDT-friendly'' benchmark (see the first figure).
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型在表格数据问题上 receiving increasingly more attention，而基于梯度抛物线（GBDT）算法的模型仍然是强大的首选解决方案。随着其他领域，如自然语言处理和计算机视觉，多种基于检索的表格DL模型已经被提出。为给定目标对象，一个基于检索的模型将其他相关对象（如最近的邻居）从可用的数据集中检索出来，并使用其特征或甚至标签来进行更好的预测。然而，我们显示出现有的检索基于DL解决方案只提供了微不足的，如果有的，利益于简单的无检索基线。因此，是否使用检索方法是值得追究的问题。在这项工作中，我们给出了一个积极的答案。我们首先将简单的扩散架构逐步增强为包含拟合 Retrieval 的注意力机制，类似于许多（表格）检索基于DL模型。然后，我们强调了一些关于注意力机制的细节，其中一些在对表格数据问题进行表达的时候有很大的影响，但在前期工作中未经探讨。因此，我们设计了 TabR -- 一个简单的检索基于DL模型，在一些公共的 benchmark 上表现出了最好的平均性能，创下了新的状态码，并在一些数据集上 even outperform GBDT 模型（参见第一个图像）。
</details></li>
</ul>
<hr>
<h2 id="Waypoint-Based-Imitation-Learning-for-Robotic-Manipulation"><a href="#Waypoint-Based-Imitation-Learning-for-Robotic-Manipulation" class="headerlink" title="Waypoint-Based Imitation Learning for Robotic Manipulation"></a>Waypoint-Based Imitation Learning for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14326">http://arxiv.org/abs/2307.14326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucys0/awe">https://github.com/lucys0/awe</a></li>
<li>paper_authors: Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao, Chelsea Finn</li>
<li>for: 该论文旨在提出一种自动生成方向点的方法，以便在人工学习中减少错误的汇总。</li>
<li>methods: 该论文提出了一种自动方向点提取（AWE）模块，可以将示例分解为最小的方向点集，以便在指定的误差阈值下 linearly  interpolate approximate  trajectory。</li>
<li>results: 实验和实际应用中，AWE 可以增加状态艺术 algorithm 的成功率，提高了实验和实际应用中的成功率，并且可以减少决策准点数量。<details>
<summary>Abstract</summary>
While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/
</details>
<details>
<summary>摘要</summary>
While imitation learning methods have seen a resurgence of interest for robotic manipulation, the well-known problem of compounding errors continues to affect behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, but waypoint labeling is underspecified and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints that can be interpolated linearly to approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision-making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/.Here's the translation in Traditional Chinese:随着从imitative learning方法的复兴， robotic manipulation 中的well-known problem of compounding errors仍然存在。 Waypoints可以帮助解决这个问题，但是 waypoint labeling 是不够详细的，需要额外的人工supervision。 Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints that can be interpolated linearly to approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by up to 25% in simulation and by 4-28% on real-world bimanual manipulation tasks, reducing the decision-making horizon by up to a factor of 10. Videos and code are available at https://lucys0.github.io/awe/.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Moral-Beliefs-Encoded-in-LLMs"><a href="#Evaluating-the-Moral-Beliefs-Encoded-in-LLMs" class="headerlink" title="Evaluating the Moral Beliefs Encoded in LLMs"></a>Evaluating the Moral Beliefs Encoded in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14324">http://arxiv.org/abs/2307.14324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ninodimontalcino/moralchoice">https://github.com/ninodimontalcino/moralchoice</a></li>
<li>paper_authors: Nino Scherrer, Claudia Shi, Amir Feder, David M. Blei</li>
<li>for: 这个研究探讨了大型自然语言模型（LLM）上的问naire设计、管理、后期处理和评估方法。</li>
<li>methods: 这个研究使用了一种统计方法来揭示LLM中的信仰。研究者们引入了一些统计量和评价指标，以量化LLM“选择”的概率、相关的uncertainty以及选择的一致性。</li>
<li>results: 研究发现，在明确的场景下，大多数模型选择与常识相符的行为。在抽象的场景下，大多数模型表现出uncertainty。此外，一些模型在抽象场景下表现出明确的偏好，特别是关闭源代码模型之间存在一定的一致性。<details>
<summary>Abstract</summary>
This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models "choose" actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM “making a choice,” the associated uncertainty, and the consistency of that choice.2. We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey with 680 high-ambiguity moral scenarios (e.g., “Should I tell a white lie?”) and 687 low-ambiguity moral scenarios (e.g., “Should I stop for a pedestrian on the road?”). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., “do not kill”). We administer the survey to 28 open- and closed-source LLMs.Our findings are as follows:1. In unambiguous scenarios, most models “choose” actions that align with commonsense.2. In ambiguous cases, most models express uncertainty.3. Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording.4. Some models reflect clear preferences in ambiguous scenarios, and closed-source models tend to agree with each other.</details></li>
</ol>
<hr>
<h2 id="Reinforcement-Learning-by-Guided-Safe-Exploration"><a href="#Reinforcement-Learning-by-Guided-Safe-Exploration" class="headerlink" title="Reinforcement Learning by Guided Safe Exploration"></a>Reinforcement Learning by Guided Safe Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14316">http://arxiv.org/abs/2307.14316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisong Yang, Thiago D. Simão, Nils Jansen, Simon H. Tindemans, Matthijs T. J. Spaan</li>
<li>for: 这篇论文的目的是为了帮助RL算法在不知道目标任务的情况下安全地扩展应用。</li>
<li>methods: 这篇论文使用了不受奖励指导的RL方法，在一个控制的环境中训练一个引导者，以便在不知道目标任务时快速适应。在目标任务被揭示后，不允许安全违反。此外，该方法还利用了传输学习来正则化一个目标策略（学生），使其快速解决目标任务。</li>
<li>results: 实验表明，这种方法可以实现安全的传输学习，帮助学生更快地解决目标任务。<details>
<summary>Abstract</summary>
Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows that this method can achieve safe transfer learning and helps the student solve the target task faster.
</details>
<details>
<summary>摘要</summary>
安全性是扩展强化学习（RL）的关键因素。我们通常在实验室中首先训练RL代理人，然后在真实世界中部署。然而，真实世界目标任务可能未知之前部署。无奖RL在扩展RL代理人时不提供奖励。我们考虑了受限的奖励自由设置，其中RL代理人（导师）在安全的环境中学习探索，不需要奖励信号。当目标任务揭示后，安全违反不再允许。因此，导师被用来组织安全行为策略。从传输学学习中，我们还启用了一个目标策略（学生），使其向导师进行Regularization，以解决学生在训练过程中的不可靠性。实验分析表明，这种方法可以实现安全的传输学习，帮助学生更快地解决目标任务。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Deep-Learning-based-Pansharpening-with-Jointly-Enhanced-Spectral-and-Spatial-Fidelity"><a href="#Unsupervised-Deep-Learning-based-Pansharpening-with-Jointly-Enhanced-Spectral-and-Spatial-Fidelity" class="headerlink" title="Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity"></a>Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14403">http://arxiv.org/abs/2307.14403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matciotola/lambda-pnn">https://github.com/matciotola/lambda-pnn</a></li>
<li>paper_authors: Matteo Ciotola, Giovanni Poggi, Giuseppe Scarpa<br>for: 这个论文主要关注于深度学习在多resolution图像缩进中的应用，具体来说是提出一种可以在全分辨率域进行无监督训练的整体框架，以提高图像缩进的性能。methods: 该论文提出了一种新的深度学习基于模型，具有增强的建筑特性和一种新的损失函数，该损失函数同时 promote spectral和spatial图像质量。此外，该模型还使用了一种新的微调策略来提高推理时的适应性。results: 实验表明，提出的方法在具有各种挑战性的测试图像上达到了STATE OF THE ART的性能水平，both in terms of numerical results and visual output。<details>
<summary>Abstract</summary>
In latest years, deep learning has gained a leading role in the pansharpening of multiresolution images. Given the lack of ground truth data, most deep learning-based methods carry out supervised training in a reduced-resolution domain. However, models trained on downsized images tend to perform poorly on high-resolution target images. For this reason, several research groups are now turning to unsupervised training in the full-resolution domain, through the definition of appropriate loss functions and training paradigms. In this context, we have recently proposed a full-resolution training framework which can be applied to many existing architectures.   Here, we propose a new deep learning-based pansharpening model that fully exploits the potential of this approach and provides cutting-edge performance. Besides architectural improvements with respect to previous work, such as the use of residual attention modules, the proposed model features a novel loss function that jointly promotes the spectral and spatial quality of the pansharpened data. In addition, thanks to a new fine-tuning strategy, it improves inference-time adaptation to target images. Experiments on a large variety of test images, performed in challenging scenarios, demonstrate that the proposed method compares favorably with the state of the art both in terms of numerical results and visual output. Code is available online at https://github.com/matciotola/Lambda-PNN.
</details>
<details>
<summary>摘要</summary>
最近几年，深度学习在多尺度图像缩进中扮演了主导角色。由于缺乏地面真实数据，大多数深度学习基于方法在减小分辨率领域进行了supervised训练。然而，在高分辨率目标图像上训练的模型通常表现不佳。为了解决这个问题，许多研究小组现在转向无监督训练在全分辨率领域，通过定义适当的损失函数和训练方法。在这种情况下，我们最近提出了一个全分辨率训练框架，可以应用于许多现有的架构。  我们提出了一个新的深度学习基于缩进模型，该模型完全利用了这种方法的潜力，并提供了顶尖性能。除了以前的建筑改进外，该模型还包括尚未使用的差分注意模块，以及一个新的损失函数，该函数同时Promote spectral和空间数据的质量。此外，通过一种新的精度调整策略，该模型提高了对target图像的推理时适应性。在一个大量的测试图像上，通过在复杂的场景下进行测试，我们的方法与状态对照数据比较，得到了优秀的数值结果和视觉输出。代码可以在https://github.com/matciotola/Lambda-PNN上下载。
</details></li>
</ul>
<hr>
<h2 id="A-Constraint-Enforcement-Deep-Reinforcement-Learning-Framework-for-Optimal-Energy-Storage-Systems-Dispatch"><a href="#A-Constraint-Enforcement-Deep-Reinforcement-Learning-Framework-for-Optimal-Energy-Storage-Systems-Dispatch" class="headerlink" title="A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch"></a>A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14304">http://arxiv.org/abs/2307.14304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning">https://github.com/ShengrenHou/Energy-management-MIP-Deep-Reinforcement-Learning</a></li>
<li>paper_authors: Shengren Hou, Edgar Mauricio Salazar Duque, Peter Palensky, Pedro P. Vergara</li>
<li>for: 这篇论文的目的是提出一个基于深度学习的能源储存系统优化策略，以应对动态价格、需求耗用和可再生能源生产的不确定性。</li>
<li>methods: 这篇论文使用深度神经网络（DNNs）和深度问题学习（DRL）算法，以学习适应分布网络的数据分布，并将其转换为一个混合整数程式（MIP）形式，以考虑环境的操作限制。</li>
<li>results:  comparison simulations show that the proposed MIP-DRL framework can effectively enforce all constraints while delivering high-quality dispatch decisions, outperforming state-of-the-art DRL algorithms and the optimal solution obtained with a perfect forecast of the stochastic variables.<details>
<summary>Abstract</summary>
The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the consideration of the environment's operational constraints. Comprehensive numerical simulations show the superior performance of the proposed MIP-DRL framework, effectively enforcing all constraints while delivering high-quality dispatch decisions when compared with state-of-the-art DRL algorithms and the optimal solution obtained with a perfect forecast of the stochastic variables.
</details>
<details>
<summary>摘要</summary>
优化能量存储系统（ESS）的分发表现出了巨大的挑战，这主要归结于能量价格、消耗量和可再生能源生产的波动性带来的不确定性。通过深度神经网络（DNN）的泛化能力，深度强化学习（DRL）算法可以学习适应分布网络的随机性，并且可以适应不同的环境和操作约束。然而，当前的DRL算法通常无法严格地执行环境和操作约束，有时候甚至提供了不可行的控制动作。为解决这个问题，我们提出了一种DRL框架，可以有效地处理连续动作空间，同时严格执行环境和操作约束。首先，我们在DNN中训练一个动作价值函数。然后，我们将这个动作价值函数转换为混合整数编程（MIP）形式，以便考虑环境的操作约束。通过对数字实验进行了全面的比较，我们发现了我们提出的MIP-DRL框架的优秀表现。它可以坚持所有约束，同时提供高质量的分发决策，与当前的DRL算法和完美预测随机变量的优质策略相比。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-and-Persuasive-Technologies-for-the-Management-and-Delivery-of-Personalized-Recommendations-in-Hotel-Hospitality"><a href="#ChatGPT-and-Persuasive-Technologies-for-the-Management-and-Delivery-of-Personalized-Recommendations-in-Hotel-Hospitality" class="headerlink" title="ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality"></a>ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14298">http://arxiv.org/abs/2307.14298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manolis Remountakis, Konstantinos Kotis, Babis Kourtzis, George E. Tsekouras</li>
<li>for: 这篇论文的目的是探讨推荐系统在酒店互助业中的应用，以及利用语言模型和吸引技术来提高推荐系统的效果。</li>
<li>methods: 这篇论文使用了语言模型ChatGPT和吸引技术来自动化和改进酒店推荐系统。它还分析了用户喜好和在线评论中提取有价值信息，并基于用户 профиль生成个性化推荐。</li>
<li>results: 这篇论文通过一个实验研究了将ChatGPT和吸引技术Integrated into酒店推荐系统的效果，发现这些技术可以提高用户参与度、满意度和酒店收入。<details>
<summary>Abstract</summary>
Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recommendations. By incorporating persuasive techniques, such as social proof, scarcity and personalization, recommender systems can effectively influence user decision-making and encourage desired actions, such as booking a specific hotel or upgrading their room. To investigate the efficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment with a case study involving a hotel recommender system. We aim to study the impact of integrating ChatGPT and persua-sive techniques on user engagement, satisfaction, and conversion rates. The preliminary results demonstrate the potential of these technologies in enhancing the overall guest experience and business performance. Overall, this paper contributes to the field of hotel hospitality by exploring the synergistic relationship between LLMs and persuasive technology in recommender systems, ultimately influencing guest satisfaction and hotel revenue.
</details>
<details>
<summary>摘要</summary>
各种推荐系统在酒店互联网行业已成为不可或缺的工具，帮助提供个性化和适应性的旅客体验。最新的大型自然语言模型（LLM），如ChatGPT，以及吸引技术，已经开创了推荐系统的新可能性。本文探讨了将ChatGPT和吸引技术integrated into hotel hospitality recommender systems的可能性，以提高旅客体验和酒店业绩。首先，我们探讨了ChatGPT的能力，可以理解和生成人类语言，从而提供更准确和上下文感知的推荐。我们介绍了将ChatGPT integrate into recommender systems，包括分析用户偏好、从在线评论中提取有价值信息和基于客户 profiling 生成个性化的推荐。其次，我们调查了吸引技术在使用者行为上的影响，以及如何在推荐系统中应用吸引技术以提高旅客决策的可能性。通过涉及到社会证明、缺失和个性化等吸引技术，推荐系统可以有效地影响用户决策和鼓励旅客选择特定酒店或升级房间。为了评估ChatGPT和吸引技术的效果，我们在一个酒店推荐系统的 caso study中进行了试点。我们的目标是研究将ChatGPT和吸引技术integrated into recommender systems的影响在用户参与度、满意度和转化率上。初步结果表明这些技术在总体客户体验和酒店业绩方面具有潜在的潜力。总的来说，本文对酒店互联网领域的推荐系统做出了贡献，探讨了LLMs和吸引技术之间的相互作用，最终影响客户满意度和酒店收益。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-the-Complexity-of-Splitting-Sequential-Data-Tackling-Challenges-in-Video-and-Time-Series-Analysis"><a href="#Unraveling-the-Complexity-of-Splitting-Sequential-Data-Tackling-Challenges-in-Video-and-Time-Series-Analysis" class="headerlink" title="Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis"></a>Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14294">http://arxiv.org/abs/2307.14294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Botache, Kristina Dingel, Rico Huhnstock, Arno Ehresmann, Bernhard Sick</li>
<li>for: 本文探讨了分割顺序数据的挑战，包括数据获取、数据表示、分割率选择、设置质量标准和选择适当的选择策略。</li>
<li>methods: 本文使用了两个实际应用例──汽车测试台和流体中的粒子跟踪──来探讨分割顺序数据的挑战。</li>
<li>results: 本文通过两个实际应用例的分析，揭示了分割顺序数据的挑战，并提供了一些可能的解决方案。<details>
<summary>Abstract</summary>
Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.
</details>
<details>
<summary>摘要</summary>
分割连续数据，如视频和时间序列数据，是数据分析任务中的一个重要步骤，包括对象跟踪和异常检测。然而，分割连续数据会出现多种挑战，这些挑战可能会影响后续分析的准确性和可靠性。本概念文章探讨分割连续数据的挑战，包括数据收集、数据表示、分割率选择、设置质量标准和选择适合的选择策略。我们通过两个实际例子：汽车测试台和液体中粒子跟踪来探讨这些挑战。
</details></li>
</ul>
<hr>
<h2 id="General-Purpose-Artificial-Intelligence-Systems-GPAIS-Properties-Definition-Taxonomy-Open-Challenges-and-Implications"><a href="#General-Purpose-Artificial-Intelligence-Systems-GPAIS-Properties-Definition-Taxonomy-Open-Challenges-and-Implications" class="headerlink" title="General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications"></a>General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14283">http://arxiv.org/abs/2307.14283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac Triguero, Daniel Molina, Javier Poyatos, Javier Del Ser, Francisco Herrera</li>
<li>for: The paper discusses and proposes a new definition for General-Purpose Artificial Intelligence Systems (GPAIS) and its differentiation based on various factors.</li>
<li>methods: The paper uses existing definitions and proposes a new taxonomy of approaches to realize GPAIS, including the use of AI techniques to improve another AI or foundation models.</li>
<li>results: The paper provides a comprehensive overview of the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一个新的 General-Purpose Artificial Intelligence Systems（GPAIS）定义，并对其进行了不同因素的分类。</li>
<li>methods: 该论文使用了现有的定义，并提出了一种新的实现GPAIS的纲要，包括使用AI技术来改进另一个AI或基础模型。</li>
<li>results: 该论文提供了GPAIS的当前状况，其挑战和前景，对我们社会的影响，以及负责任和可信worthy AI系统和regulation的需要。<details>
<summary>Abstract</summary>
Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.   This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgment of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI or foundation models. As a prime example, we delve into generative AI, aligning them with the terms and concepts presented in the taxonomy. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general-purpose tasks, as they share many common aspects. Finally, we discuss the current state of GPAIS, its challenges and prospects, implications for our society, and the need for responsible and trustworthy AI systems and regulation, with the goal of providing a holistic view of GPAIS.
</details>
<details>
<summary>摘要</summary>
大多数人工智能（AI）应用都是为特定任务设计的，但有许多情况需要一种更通用的AI系统，能够解决多种任务而不需要特定设计。这种AI系统被称为通用人工智能系统（GPAIS）。迄今为止，人工通用智能，能够像人类一样完成任何知识任务，或者甚至超越人类，仍然是一个梦想和科幻。尽管我们仍然远离实现这一点，但GPAIS已经成为人工智能研究的前沿。本文讨论了现有的GPAIS定义，并提出了一个新的定义，允许逐步分化GPAIS类型根据其性质和限制。我们将GPAIS分为关闭世界和开放世界两类，根据它们的自主度和能力，包括适应新任务、在不直接训练的领域中的能力、从少量数据学习、或者主动承认自己的限制等因素。然后，我们提出了一种分类方法，描述了在实现GPAIS方面的研究趋势，如使用AI技术提高另一个AI的性能或基础模型。为了更好地推动不同领域之间的合作研究，我们采用了一种概念和术语的分类方法。我们还讨论了GPAIS的当前状况，挑战和前途，以及对社会的影响和责任的人工智能系统和regulation的需要，以提供一个整体的GPAIS视图。
</details></li>
</ul>
<hr>
<h2 id="Deepfake-Image-Generation-for-Improved-Brain-Tumor-Segmentation"><a href="#Deepfake-Image-Generation-for-Improved-Brain-Tumor-Segmentation" class="headerlink" title="Deepfake Image Generation for Improved Brain Tumor Segmentation"></a>Deepfake Image Generation for Improved Brain Tumor Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14273">http://arxiv.org/abs/2307.14273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roa’a Al-Emaryeen, Sara Al-Nahhas, Fatima Himour, Waleed Mahafza, Omar Al-Kadi</li>
<li>For: This paper aims to improve brain tumor segmentation using deep-fake image generation.* Methods: The proposed approach uses a Generative Adversarial Network (GAN) for image-to-image translation and a U-Net-based convolutional neural network (CNN) for image segmentation, trained with deepfake images.* Results: The proposed approach shows improved performance in terms of image segmentation quality metrics compared to ground truth, and has the potential to assist when training with limited data.Here’s the Chinese translation of the three pieces of information:* For: 这篇论文的目的是提高脑肿分割使用深伪图生成。* Methods: 该方法使用了生成对抗网络（GAN）进行图像到图像的转换，然后使用U-Net基于的卷积神经网络（CNN）进行图像分割，并使用深伪图进行训练。* Results: 该方法在图像分割质量指标方面比基准数据显示出了改善的性能，并有可能帮助在具有有限数据的情况下训练。<details>
<summary>Abstract</summary>
As the world progresses in technology and health, awareness of disease by revealing asymptomatic signs improves. It is important to detect and treat tumors in early stage as it can be life-threatening. Computer-aided technologies are used to overcome lingering limitations facing disease diagnosis, while brain tumor segmentation remains a difficult process, especially when multi-modality data is involved. This is mainly attributed to ineffective training due to lack of data and corresponding labelling. This work investigates the feasibility of employing deep-fake image generation for effective brain tumor segmentation. To this end, a Generative Adversarial Network was used for image-to-image translation for increasing dataset size, followed by image segmentation using a U-Net-based convolutional neural network trained with deepfake images. Performance of the proposed approach is compared with ground truth of four publicly available datasets. Results show improved performance in terms of image segmentation quality metrics, and could potentially assist when training with limited data.
</details>
<details>
<summary>摘要</summary>
随着科技和医疗的进步，疾病的早期发现越来越重要。检测和治疗早期癌变可以挽救生命。但是诊断疾病尚存有一些限制，特别是当涉及多Modal数据时。这主要归结于数据和相应的标注不充分。本研究探讨使用深归化图像生成技术来提高脑肿瘤分 segmentation的可能性。为此，我们使用生成对抗网络进行图像到图像翻译，然后使用基于U-Net convolutional neural network的图像分割算法，并使用深归化图像进行训练。我们对四个公共可用的数据集进行比较，结果表明我们的方法可以提高图像分割质量指标，并可能帮助在有限数据情况下训练。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/27/cs.LG_2023_07_27/" data-id="cllshxsoc00282u8852xagt9y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/cs.SD_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/cs.SD_2023_07_27/">cs.SD - 2023-07-27 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Mitigating-Cross-Database-Differences-for-Learning-Unified-HRTF-Representation"><a href="#Mitigating-Cross-Database-Differences-for-Learning-Unified-HRTF-Representation" class="headerlink" title="Mitigating Cross-Database Differences for Learning Unified HRTF Representation"></a>Mitigating Cross-Database Differences for Learning Unified HRTF Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14547">http://arxiv.org/abs/2307.14547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yutongwen/hrtf_field_norm">https://github.com/yutongwen/hrtf_field_norm</a></li>
<li>paper_authors: Yutong Wen, You Zhang, Zhiyao Duan</li>
<li>for: 这篇论文的目的是提出一种用于实现个人化头部相关转换函数（HRTF）的预测方法，以便在虚拟听觉显示中精确地 пози规音频。</li>
<li>methods: 这篇论文使用机器学习模型来预测个人化HRTF，并使用跨 Databases 的HRTF表现来训练这些模型。</li>
<li>results: 这篇论文的结果显示，透过调整HRTF的频谱响应，可以将不同数据库中的HRTF转换为一个更加共同的表现，并且这些转换后的HRTF无法根据数据库的不同区分。<details>
<summary>Abstract</summary>
Individualized head-related transfer functions (HRTFs) are crucial for accurate sound positioning in virtual auditory displays. As the acoustic measurement of HRTFs is resource-intensive, predicting individualized HRTFs using machine learning models is a promising approach at scale. Training such models require a unified HRTF representation across multiple databases to utilize their respectively limited samples. However, in addition to differences on the spatial sampling locations, recent studies have shown that, even for the common location, HRTFs across databases manifest consistent differences that make it trivial to tell which databases they come from. This poses a significant challenge for learning a unified HRTF representation across databases. In this work, we first identify the possible causes of these cross-database differences, attributing them to variations in the measurement setup. Then, we propose a novel approach to normalize the frequency responses of HRTFs across databases. We show that HRTFs from different databases cannot be classified by their database after normalization. We further show that these normalized HRTFs can be used to learn a more unified HRTF representation across databases than the prior art. We believe that this normalization approach paves the road to many data-intensive tasks on HRTF modeling.
</details>
<details>
<summary>摘要</summary>
个人化的头顶相关转换函数（HRTF）是虚拟听觉显示的精确 зву讯定位的重要因素。由于实际量测HRTF的成本高昂，使用机器学习模型预测个人化HRTF是一个具有潜力的方法。但是，训练这些模型需要一个统一的HRTF表示方式，可以利用不同数据库的有限样本。然而，过去的研究显示，即使在共同的位置上，不同数据库的HRTF之间仍存在明显的差异，这使得很难将HRTF表示统一化。在这个工作中，我们首先识别了差异的可能原因，并将其归因于测量设置的变化。然后，我们提出了一个新的方法来对不同数据库的HRTF进行频谱均衡。我们发现，对不同数据库的HRTF进行均衡后，它们不能被分类到哪怕库。此外，我们还证明了这些均衡后的HRTF可以用来学习一个更统一的HRTF表示方式，比对于先前的方法更好。我们认为，这个均衡方法将开启许多数据密集的HRTF模型任务。
</details></li>
</ul>
<hr>
<h2 id="Modality-Agnostic-Audio-Visual-Deepfake-Detection"><a href="#Modality-Agnostic-Audio-Visual-Deepfake-Detection" class="headerlink" title="Modality-Agnostic Audio-Visual Deepfake Detection"></a>Modality-Agnostic Audio-Visual Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14491">http://arxiv.org/abs/2307.14491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cai Yu, Peng Chen, Jiahe Tian, Jin Liu, Jiao Dai, Xi Wang, Yesheng Chai, Jizhong Han</li>
<li>for: 这个研究旨在开发一个能够探测多模式深圳诈骗的数据类型不对称检测方法，并能够处理缺失模式的情况。</li>
<li>methods: 本研究使用了一个统一的诈骗模式框架，可以探测多模式深圳诈骗并处理缺失模式的情况。另外，我们还提出了一个双标签检测方法，可以独立检测每个模式。</li>
<li>results: 实验结果显示，我们的方法不仅在所有三个音频视觉数据集上都超过了现有的州务检测方法，而且在缺失模式情况下也可以 дости持 satisfying的性能。此外，我们的方法甚至在 JOINT 使用两个单模式方法时超过了它们的性能。<details>
<summary>Abstract</summary>
As AI-generated content (AIGC) thrives, Deepfakes have expanded from single-modality falsification to cross-modal fake content creation, where either audio or visual components can be manipulated. While using two unimodal detectors can detect audio-visual deepfakes, cross-modal forgery clues could be overlooked. Existing multimodal deepfake detection methods typically establish correspondence between the audio and visual modalities for binary real/fake classification, and require the co-occurrence of both modalities. However, in real-world multi-modal applications, missing modality scenarios may occur where either modality is unavailable. In such cases, audio-visual detection methods are less practical than two independent unimodal methods. Consequently, the detector can not always obtain the number or type of manipulated modalities beforehand, necessitating a fake-modality-agnostic audio-visual detector. In this work, we propose a unified fake-modality-agnostic scenarios framework that enables the detection of multimodal deepfakes and handles missing modalities cases, no matter the manipulation hidden in audio, video, or even cross-modal forms. To enhance the modeling of cross-modal forgery clues, we choose audio-visual speech recognition (AVSR) as a preceding task, which effectively extracts speech correlation across modalities, which is difficult for deepfakes to reproduce. Additionally, we propose a dual-label detection approach that follows the structure of AVSR to support the independent detection of each modality. Extensive experiments show that our scheme not only outperforms other state-of-the-art binary detection methods across all three audio-visual datasets but also achieves satisfying performance on detection modality-agnostic audio/video fakes. Moreover, it even surpasses the joint use of two unimodal methods in the presence of missing modality cases.
</details>
<details>
<summary>摘要</summary>
“深刻掌握AI生成内容（AIGC）的发展，深伪（Deepfakes）已经从单模态伪造扩展到跨模态伪造，其中可以操作音频或视觉组件。使用两个单模态检测器可以检测音频视频深伪，但跨模态伪造证据可能会被忽略。现有的多模态深伪检测方法通常在音频和视觉modalities之间建立对应关系，并需要两个模式同时存在。但在实际的多模式应用中，缺失模式场景可能会发生，其中一个或多个模式都不可用。在这种情况下，音频视频检测方法不太实用，因为检测器无法在事先获知哪一个模式被修改。因此，我们需要一种不关心模式的多模态深伪检测方法。在这种方案中，我们提出一种统一的多模态深伪检测框架，可以检测多模态深伪并处理缺失模式场景，无论哪一个模式被修改。为了更好地模型跨模式伪造证据，我们选择了音频视频语音识别（AVSR）作为先前任务，它可以有效提取modalities之间的语音相关性，这是深伪很难复制的。此外，我们还提出了一种双标签检测方法，根据AVSR的结构来支持每个模式独立的检测。我们的方案不仅在所有三个音频视频数据集上超过了其他状态对抗方法的性能，还在缺失模式场景下达到了满意的性能。此外，它 même surpassed the joint use of two unimodal methods in the presence of missing modality cases.”
</details></li>
</ul>
<hr>
<h2 id="Single-Channel-Speech-Enhancement-Using-U-Net-Spiking-Neural-Networks"><a href="#Single-Channel-Speech-Enhancement-Using-U-Net-Spiking-Neural-Networks" class="headerlink" title="Single Channel Speech Enhancement Using U-Net Spiking Neural Networks"></a>Single Channel Speech Enhancement Using U-Net Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14464">http://arxiv.org/abs/2307.14464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abir Riahi, Éric Plourde</li>
<li>for: 提高沟通设备和可靠的语音识别系统的信噪比</li>
<li>methods: 使用基于U-Net架构的脉冲神经网络（SNN）进行沟通增强</li>
<li>results: 比预先达到的状态艺术神经网络（ANN）模型表现更好，并在不同的信号噪比和真实噪音条件下达到了可接受的表现。<details>
<summary>Abstract</summary>
Speech enhancement (SE) is crucial for reliable communication devices or robust speech recognition systems. Although conventional artificial neural networks (ANN) have demonstrated remarkable performance in SE, they require significant computational power, along with high energy costs. In this paper, we propose a novel approach to SE using a spiking neural network (SNN) based on a U-Net architecture. SNNs are suitable for processing data with a temporal dimension, such as speech, and are known for their energy-efficient implementation on neuromorphic hardware. As such, SNNs are thus interesting candidates for real-time applications on devices with limited resources. The primary objective of the current work is to develop an SNN-based model with comparable performance to a state-of-the-art ANN model for SE. We train a deep SNN using surrogate-gradient-based optimization and evaluate its performance using perceptual objective tests under different signal-to-noise ratios and real-world noise conditions. Our results demonstrate that the proposed energy-efficient SNN model outperforms the Intel Neuromorphic Deep Noise Suppression Challenge (Intel N-DNS Challenge) baseline solution and achieves acceptable performance compared to an equivalent ANN model.
</details>
<details>
<summary>摘要</summary>
声音提升（SE）是重要的通信设备或可靠的语音识别系统的关键。虽然传统的人工神经网络（ANN）已经表现出了惊人的表现在SE中，但它们需要显著的计算能力，同时也需要高度的能源成本。在这篇论文中，我们提出了一种使用快速神经网络（SNN）基于U-Net架构的新的方法 дляSE。SNN适用于处理具有时间维度的数据，如语音，并且被认为是能效地实现在神经机器硬件上的。因此，SNN是在有限资源的设备上实现实时应用的优秀选择。目标是在现有的ANN模型性能水平上实现相似的SE模型。我们使用代理梯度基于优化方法来训练深度SNN，并使用感知目标测试来评估其性能。我们的结果表明，我们提出的能效的SNN模型在不同的信号噪比和实际噪声条件下都能够达到适当的性能，并且超越了Intel neuromorphic Deep Noise Suppression Challenge（Intel N-DNS Challenge）基准解决方案。
</details></li>
</ul>
<hr>
<h2 id="WavJourney-Compositional-Audio-Creation-with-Large-Language-Models"><a href="#WavJourney-Compositional-Audio-Creation-with-Large-Language-Models" class="headerlink" title="WavJourney: Compositional Audio Creation with Large Language Models"></a>WavJourney: Compositional Audio Creation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14335">http://arxiv.org/abs/2307.14335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-agi/wavjourney">https://github.com/audio-agi/wavjourney</a></li>
<li>paper_authors: Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang</li>
<li>for: 这 paper 旨在开发一种基于 Large Language Models (LLMs) 的音频内容生成系统，以便在语言和视觉任务中提高人工智能生成内容的能力。</li>
<li>methods: 这 paper 使用了 LLMs 连接多种音频模型，以生成包含演讲、音乐和音效的听写内容。它首先使用 LLMs 生成一份适用于音频storytelling的结构化脚本，然后使用这份脚本生成一个计算机程序，并将每行程序转换为一个特定的音频生成模型或计算操作函数。</li>
<li>results: 这 paper 在多个实际场景中展示了 WavJourney 的实用性，包括科幻、教育和广播剧等。WavJourney 的可靠和可交互的设计，使得人机共创在多轮对话中得到了进一步的创作控制和适应性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown great promise in integrating diverse expert models to tackle intricate language and vision tasks. Despite their significance in advancing the field of Artificial Intelligence Generated Content (AIGC), their potential in intelligent audio content creation remains unexplored. In this work, we tackle the problem of creating audio content with storylines encompassing speech, music, and sound effects, guided by text instructions. We present WavJourney, a system that leverages LLMs to connect various audio models for audio content generation. Given a text description of an auditory scene, WavJourney first prompts LLMs to generate a structured script dedicated to audio storytelling. The audio script incorporates diverse audio elements, organized based on their spatio-temporal relationships. As a conceptual representation of audio, the audio script provides an interactive and interpretable rationale for human engagement. Afterward, the audio script is fed into a script compiler, converting it into a computer program. Each line of the program calls a task-specific audio generation model or computational operation function (e.g., concatenate, mix). The computer program is then executed to obtain an explainable solution for audio generation. We demonstrate the practicality of WavJourney across diverse real-world scenarios, including science fiction, education, and radio play. The explainable and interactive design of WavJourney fosters human-machine co-creation in multi-round dialogues, enhancing creative control and adaptability in audio production. WavJourney audiolizes the human imagination, opening up new avenues for creativity in multimedia content creation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经显示出很大的损征在融合多种专家模型来解决复杂的语言和视觉任务上。尽管它们在人工智能生成内容（AIGC）领域中的潜力仍然未获探索，但它们在智能音频内容创建方面的潜力仍然未获开发。在这个工作中，我们对于创建包含话语、音乐和效果的音频内容进行了探索。我们提出了WavJourney系统，这个系统利用LLM来连接不同的音频模型，以实现音频内容创建。当我们给出了文本描述一个听频场景时，WavJourney首先透过LLM生成一个关于音频故事的结构化脚本。这个音频脚本包括多种音频元素，并且根据它们的空间时间关系进行了组织。作为一个概念表现的音频，这个音频脚本提供了互动和可解释的理由，以便人类参与。接下来，这个音频脚本会被转换为一个计算机程序，每行代码都会调用一个任务特定的音频生成模型或计算操作函数（例如， concatenate、mix）。计算机程序的执行将获得一个可解释的音频生成解决方案。我们在多个实际应用场景中证明了WavJourney的实用性，包括科幻、教育和广播剧。WavJourney的可说明和互动设计增强了人机共创的多轮 діало格，提高了创作控制和适应性。WavJourney声音化了人类的想像力，开启了新的创作可能性在多媒体内容创建领域。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/27/cs.SD_2023_07_27/" data-id="cllshxsp3004q2u88d2k8btby" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/27/eess.AS_2023_07_27/" class="article-date">
  <time datetime="2023-07-26T16:00:00.000Z" itemprop="datePublished">2023-07-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/27/eess.AS_2023_07_27/">eess.AS - 2023-07-27 22:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array"><a href="#Audio-Inputs-for-Active-Speaker-Detection-and-Localization-via-Microphone-Array" class="headerlink" title="Audio Inputs for Active Speaker Detection and Localization via Microphone Array"></a>Audio Inputs for Active Speaker Detection and Localization via Microphone Array</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14739">http://arxiv.org/abs/2307.14739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Berghi, Philip J. B. Jackson</li>
<li>for: 本研究探讨了基于多通道音频的 aktive speaker detection 和位置测定（ASDL）问题。</li>
<li>methods: 该研究使用了 convolutional recurrent neural network（CRNN），使用了多通道音频中的空间声学特征，并对噪声的影响进行了测试。</li>
<li>results: 研究发现，使用GCC-PHAT和SALSA特征可以减少噪声的影响，而 beamforming 方法可以提高ASDL的性能。  Additionally, the study found that the number of channels and the sampling density of the microphone array have a significant impact on the performance of ASDL.<details>
<summary>Abstract</summary>
This study considers the problem of detecting and locating an active talker's horizontal position from multichannel audio captured by a microphone array. We refer to this as active speaker detection and localization (ASDL). Our goal was to investigate the performance of spatial acoustic features extracted from the multichannel audio as the input of a convolutional recurrent neural network (CRNN), in relation to the number of channels employed and additive noise. To this end, experiments were conducted to compare the generalized cross-correlation with phase transform (GCC-PHAT), the spatial cue-augmented log-spectrogram (SALSA) features, and a recently-proposed beamforming method, evaluating their robustness to various noise intensities. The array aperture and sampling density were tested by taking subsets from the 16-microphone array. Results and tests of statistical significance demonstrate the microphones' contribution to performance on the TragicTalkers dataset, which offers opportunities to investigate audio-visual approaches in the future.
</details>
<details>
<summary>摘要</summary>
Here's the simplified Chinese translation:这个研究关注的是从多通道音频记录的扬声器的活动位置探测和定位（ASDL）问题。我们的目标是 investigate CRNN（卷积隐estamp）的输入为多通道音频中的空间音学特征（GCC-PHAT、SALSA）的性能，与通道数和附加噪声之间的关系。为此，我们进行了比较减 correlated with phase transform（GCC-PHAT）、空间cue-augmented log-spectrogram（SALSA）特征和一种最近提出的扬声器方法的性能，对不同噪声强度进行评估。我们还测试了数组的开口和采样密度，使用16个 Microphonearray中的子集。结果和统计测试表明每个 Microphone的贡献，并提供了未来 investigate audio-visualapproaches的机会。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling"><a href="#Physics-Informed-Neural-Network-for-Head-Related-Transfer-Function-Upsampling" class="headerlink" title="Physics Informed Neural Network for Head-Related Transfer Function Upsampling"></a>Physics Informed Neural Network for Head-Related Transfer Function Upsampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14650">http://arxiv.org/abs/2307.14650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling">https://github.com/feima0011/physics-informed-neural-network-for-head-related-transfer-function-upsampling</a></li>
<li>paper_authors: Fei Ma, Thushara D. Abhayapala, Prasanga N. Samarasinghe, Xingyu Chen</li>
<li>for: 提高虚拟听觉体验的准确性，使用physics-informed neural network（PINN）方法进行HRTF上扩。</li>
<li>methods: 利用Helmholtz方程作为更多信息来约束上扩过程，使得生成的增强后HRTF具有物理准确性，并且采用SH分解来控制PINN网络的宽度和深度。</li>
<li>results: 对多个数据集进行比较，PINN方法在 interpolate 和 extrapolate 两种情况下表现出色，较SH方法有更好的性能。<details>
<summary>Abstract</summary>
Head-related transfer functions (HRTFs) capture the spatial and spectral features that a person uses to localize sound sources in space and thus are vital for creating an authentic virtual acoustic experience. However, practical HRTF measurement systems can only provide an incomplete measurement of a person's HRTFs, and this necessitates HRTF upsampling. This paper proposes a physics-informed neural network (PINN) method for HRTF upsampling. Unlike other upsampling methods which are based on the measured HRTFs only, the PINN method exploits the Helmholtz equation as additional information for constraining the upsampling process. This helps the PINN method to generate physically amiable upsamplings which generalize beyond the measured HRTFs. Furthermore, the width and the depth of the PINN are set according to the dimensionality of HRTFs under spherical harmonic (SH) decomposition and the Helmholtz equation. This makes the PINN have an appropriate level of expressiveness and thus does not suffer from under-fitting and over-fitting problems. Numerical experiments confirm the superior performance of the PINN method for HRTF upsampling in both interpolation and extrapolation scenarios over several datasets in comparison with the SH methods.
</details>
<details>
<summary>摘要</summary>
人头相关传函数（HRTF）捕捉声音源在空间中的特征和频谱特征，因此是创建真实的虚拟声学体验的关键。然而，实际的HRTF测量系统只能提供HRTF的不完全测量，因此需要HRTF upsampling。这篇论文提出了基于物理学习神经网络（PINN）方法的HRTF upsampling方法。与其他upsampling方法不同，PINN方法利用Helmholtz方程作为更多的约束来控制upsampling过程。这使得PINN方法能够生成符合物理规则的upsampling，并且在扩展测量HRTF的场景中表现出超过SH方法的优势。此外，PINN的宽度和深度设置与HRTF在圆柱幂分解中的维度和Helmholtz方程相关。这使得PINN具有相应的表达能力，从而不会出现过拟合和下降问题。数值实验证明PINN方法在 interpolación和 extrapolation 场景中对多个数据集表现出了superior performance  contrasted with SH方法。
</details></li>
</ul>
<hr>
<h2 id="NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals"><a href="#NeuroHeed-Neuro-Steered-Speaker-Extraction-using-EEG-Signals" class="headerlink" title="NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals"></a>NeuroHeed: Neuro-Steered Speaker Extraction using EEG Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14303">http://arxiv.org/abs/2307.14303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexu Pan, Marvin Borsdorf, Siqi Cai, Tanja Schultz, Haizhou Li</li>
<li>for: 本研究旨在开发一种基于EEG信号的选择性听说模型，以便在听说场景中提取主要听众的语音信号。</li>
<li>methods: 本研究使用EEG信号来建立一个神经元吸引器，该吸引器与听众的注意力时间相关，以便提取主要听众的语音信号。</li>
<li>results: 实验结果表明，NeuroHeed模型可以有效地提取主要听众的语音信号，并达到高质量、优良 восприятие和 inteligibilty 在两个说话者场景中。<details>
<summary>Abstract</summary>
Humans possess the remarkable ability to selectively attend to a single speaker amidst competing voices and background noise, known as selective auditory attention. Recent studies in auditory neuroscience indicate a strong correlation between the attended speech signal and the corresponding brain's elicited neuronal activities, which the latter can be measured using affordable and non-intrusive electroencephalography (EEG) devices. In this study, we present NeuroHeed, a speaker extraction model that leverages EEG signals to establish a neuronal attractor which is temporally associated with the speech stimulus, facilitating the extraction of the attended speech signal in a cocktail party scenario. We propose both an offline and an online NeuroHeed, with the latter designed for real-time inference. In the online NeuroHeed, we additionally propose an autoregressive speaker encoder, which accumulates past extracted speech signals for self-enrollment of the attended speaker information into an auditory attractor, that retains the attentional momentum over time. Online NeuroHeed extracts the current window of the speech signals with guidance from both attractors. Experimental results demonstrate that NeuroHeed effectively extracts brain-attended speech signals, achieving high signal quality, excellent perceptual quality, and intelligibility in a two-speaker scenario.
</details>
<details>
<summary>摘要</summary>
人类具有选择性听取Single speaker amidst competing voices and background noise的能力，称为选择性听取。最近的听auditory neuroscience研究表明， attended speech signal和对应的大脑活动之间存在强相关关系，可以使用可得性和不侵入性的电enzephalography（EEG）设备来测量。在这个研究中，我们提出了NeuroHeed模型，利用EEG信号来建立一个neuronal attractor，该attractor在时间方面与语音刺激相关。这使得可以在庆酒party scenario中提取获得了注意力的speech signal。我们提出了两种NeuroHeed，一种是Offline NeuroHeed，另一种是在线NeuroHeed。在线NeuroHeed还包括一个自适应 speaker encoder，该encoder在过去提取的speech signal基础上积累 past extracted speech signals，以便在注意力保持的情况下，将注意力集中在获得了注意力的speaker上。在线NeuroHeed在当前窗口中提取speech signal，并且受到两个attractor的引导。实验结果表明，NeuroHeed能够有效地提取大脑注意力的speech signal，实现高质量的信号、优秀的感知质量和智能性在两个speaker scenario中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://nullscc.github.io/2023/07/27/eess.AS_2023_07_27/" data-id="cllshxspn006r2u8863b2dhvq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/6/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/8/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
