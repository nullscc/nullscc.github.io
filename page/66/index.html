
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/66/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/cs.CL_2023_08_19/" class="article-date">
  <time datetime="2023-08-19T11:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/cs.CL_2023_08_19/">cs.CL - 2023-08-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-Empirical-Study-of-CLIP-for-Text-based-Person-Search"><a href="#An-Empirical-Study-of-CLIP-for-Text-based-Person-Search" class="headerlink" title="An Empirical Study of CLIP for Text-based Person Search"></a>An Empirical Study of CLIP for Text-based Person Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10045">http://arxiv.org/abs/2308.10045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flame-chasers/tbps-clip">https://github.com/flame-chasers/tbps-clip</a></li>
<li>paper_authors: Min Cao, Yang Bai, Ziyin Zeng, Mang Ye, Min Zhang<br>for: This paper aims to explore the potential of the visual-language pre-training model CLIP for downstream Text-Based Person Search (TBPS) tasks.methods: The paper conducts a comprehensive empirical study of CLIP for TBPS, including revisiting critical design considerations such as data augmentation and loss function, and implementing practical training tricks.results: The model achieves satisfactory performance without any sophisticated modules, and the probing experiments demonstrate the effectiveness of TBPS-CLIP from various aspects, providing empirical insights and highlighting future research directions.Here’s the simplified Chinese text:for: 这篇论文想要探索CLIP视觉语言预训模型在下游文本人像检索任务上的潜力。methods: 论文通过对CLIP进行全面的实验研究，包括重新评估关键设计因素，如数据增强和损失函数，以及实施实用的训练技巧。results: 模型无需任何复杂模块就可以达到满意性的性能，并通过 probing 实验表明TBPS-CLIP在多个方面的效果，提供了实证意义和未来研究方向。<details>
<summary>Abstract</summary>
Text-based Person Search (TBPS) aims to retrieve the person images using natural language descriptions. Recently, Contrastive Language Image Pretraining (CLIP), a universal large cross-modal vision-language pre-training model, has remarkably performed over various cross-modal downstream tasks due to its powerful cross-modal semantic learning capacity. TPBS, as a fine-grained cross-modal retrieval task, is also facing the rise of research on the CLIP-based TBPS. In order to explore the potential of the visual-language pre-training model for downstream TBPS tasks, this paper makes the first attempt to conduct a comprehensive empirical study of CLIP for TBPS and thus contribute a straightforward, incremental, yet strong TBPS-CLIP baseline to the TBPS community. We revisit critical design considerations under CLIP, including data augmentation and loss function. The model, with the aforementioned designs and practical training tricks, can attain satisfactory performance without any sophisticated modules. Also, we conduct the probing experiments of TBPS-CLIP in model generalization and model compression, demonstrating the effectiveness of TBPS-CLIP from various aspects. This work is expected to provide empirical insights and highlight future CLIP-based TBPS research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GameEval-Evaluating-LLMs-on-Conversational-Games"><a href="#GameEval-Evaluating-LLMs-on-Conversational-Games" class="headerlink" title="GameEval: Evaluating LLMs on Conversational Games"></a>GameEval: Evaluating LLMs on Conversational Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10032">http://arxiv.org/abs/2308.10032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, Nan Duan</li>
<li>for: This paper aims to evaluate large language models (LLMs) through goal-driven conversational games, addressing the limitations of existing evaluation methods.</li>
<li>methods: The proposed approach, called GameEval, treats LLMs as game players and assigns them distinct roles with specific goals achieved through conversations of various forms, such as discussion, question answering, and voting.</li>
<li>results: Extensive experiments show that GameEval can effectively differentiate the capabilities of various LLMs, providing a comprehensive assessment of their integrated abilities to solve complex problems.Here are the three points in Simplified Chinese:</li>
<li>for: 这篇论文目标是通过对话游戏来评估大语言模型（LLM），超越现有评估方法的限制。</li>
<li>methods: 提议的方法是通过将 LLM 当作游戏玩家，赋予它们特定的目标，通过不同的对话形式，如讨论、问答和投票，来评估模型的能力。</li>
<li>results: 广泛的实验表明，GameEval 可以有效地区分不同的 LLM 的能力，为复杂问题的解决提供全面的评估。<details>
<summary>Abstract</summary>
The rapid advancements in large language models (LLMs) have presented challenges in evaluating those models. Existing evaluation methods are either reference-based or preference based, which inevitably need human intervention or introduce test bias caused by evaluator models. In this paper, we propose GameEval, a novel approach to evaluating LLMs through goal-driven conversational games, overcoming the limitations of previous methods. GameEval treats LLMs as game players and assigns them distinct roles with specific goals achieved by launching conversations of various forms, including discussion, question answering, and voting. We design three unique games with cooperative or adversarial objectives, accompanied by corresponding evaluation metrics, to show how this new paradigm comprehensively evaluates model performance.Through extensive experiments, we show that GameEval can effectively differentiate the capabilities of various LLMs, providing a comprehensive assessment of their integrated abilities to solve complex problems. Our public anonymous code is available at https://github.com/GameEval/GameEval.
</details>
<details>
<summary>摘要</summary>
快速发展的大语言模型（LLM）带来了评估这些模型的挑战。现有的评估方法都是基于参考或偏好基础的，因此需要人工干预或引入评估器模型的测试偏见。本文提出了一种新的评估方法——GameEval，通过对LML进行目标驱动的对话游戏来评估其表现。GameEval将LML当作游戏玩家，赋予它们不同的角色和目标，通过发起不同的对话形式，包括讨论、问答和投票，来评估其能力解决复杂问题。我们设计了三个独特的游戏，每个游戏都有合作或对抗目标，并附带了相应的评估指标。我们通过广泛的实验表明，GameEval可以有效地区分不同的LML表现，提供全面评估这些模型的复杂问题解决能力。我们的公共匿名代码可以在https://github.com/GameEval/GameEval上下载。
</details></li>
</ul>
<hr>
<h2 id="ControlRetriever-Harnessing-the-Power-of-Instructions-for-Controllable-Retrieval"><a href="#ControlRetriever-Harnessing-the-Power-of-Instructions-for-Controllable-Retrieval" class="headerlink" title="ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval"></a>ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10025">http://arxiv.org/abs/2308.10025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaihang Pan, Juncheng Li, Hongye Song, Hao Fei, Wei Ji, Shuo Zhang, Jun Lin, Xiaozhong Liu, Siliang Tang</li>
<li>for: 这个研究的目的是提高 dense retrieval 模型在多元搜寻任务中的表现，并且可以让模型适应不同的搜寻意图。</li>
<li>methods: 这个研究使用了 ControlRetriever，一个可以控制 dense retrieval 模型的方法，并且使用了 ControlNet 的基础，将不同的搜寻模型融合到一个整体系统中，并且使用了一个新的 LLM 导向的指令生成和迭代训练策略，将 ControlRetriever 训练成可以适应不同的搜寻任务。</li>
<li>results: 实验结果显示，在 BEIR 评量标准中，ControlRetriever 可以在不需要任务特定调整的情况下，与基eline方法相比，获得了明显的改善，并且在零基eline情况下也实现了州际级的表现。<details>
<summary>Abstract</summary>
Recent studies have shown that dense retrieval models, lacking dedicated training data, struggle to perform well across diverse retrieval tasks, as different retrieval tasks often entail distinct search intents. To address this challenge, in this work we introduce ControlRetriever, a generic and efficient approach with a parameter isolated architecture, capable of controlling dense retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions that explicitly describe retrieval intents in natural language. Leveraging the foundation of ControlNet, which has proven powerful in text-to-image generation, ControlRetriever imbues different retrieval models with the new capacity of controllable retrieval, all while being guided by task-specific instructions. Furthermore, we propose a novel LLM guided Instruction Synthesizing and Iterative Training strategy, which iteratively tunes ControlRetriever based on extensive automatically-generated retrieval data with diverse instructions by capitalizing the advancement of large language models. Extensive experiments show that in the BEIR benchmark, with only natural language descriptions of specific retrieval intent for each task, ControlRetriever, as a unified multi-task retrieval system without task-specific tuning, significantly outperforms baseline methods designed with task-specific retrievers and also achieves state-of-the-art zero-shot performance.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese: latest studies have shown that dense retrieval models, lacking dedicated training data, struggle to perform well across diverse retrieval tasks, as different retrieval tasks often entail distinct search intents. To address this challenge, in this work we introduce ControlRetriever, a generic and efficient approach with a parameter isolated architecture, capable of controlling dense retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions that explicitly describe retrieval intents in natural language. Building on the foundation of ControlNet, which has proven powerful in text-to-image generation, ControlRetriever imbues different retrieval models with the new capacity of controllable retrieval, all while being guided by task-specific instructions. Furthermore, we propose a novel LLM guided Instruction Synthesizing and Iterative Training strategy, which iteratively tunes ControlRetriever based on extensive automatically-generated retrieval data with diverse instructions by capitalizing the advancement of large language models. Extensive experiments show that in the BEIR benchmark, with only natural language descriptions of specific retrieval intent for each task, ControlRetriever, as a unified multi-task retrieval system without task-specific tuning, significantly outperforms baseline methods designed with task-specific retrievers and also achieves state-of-the-art zero-shot performance.
</details></li>
</ul>
<hr>
<h2 id="HICL-Hashtag-Driven-In-Context-Learning-for-Social-Media-Natural-Language-Understanding"><a href="#HICL-Hashtag-Driven-In-Context-Learning-for-Social-Media-Natural-Language-Understanding" class="headerlink" title="HICL: Hashtag-Driven In-Context Learning for Social Media Natural Language Understanding"></a>HICL: Hashtag-Driven In-Context Learning for Social Media Natural Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09985">http://arxiv.org/abs/2308.09985</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albertan017/hicl">https://github.com/albertan017/hicl</a></li>
<li>paper_authors: Hanzhuo Tan, Chunpu Xu, Jing Li, Yuqun Zhang, Zeyang Fang, Zeyu Chen, Baohua Lai</li>
<li>for:  addresses the issue of compromised performance in existing natural language understanding (NLU) models when faced with short and noisy social media content.</li>
<li>methods:  leverages in-context learning (ICL) and a novel hashtag-driven in-context learning (HICL) framework, which pre-trains a model #Encoder using hashtags to drive BERT-based pre-training through contrastive learning, and employs a gradient-based method to identify trigger terms useful in fusing information from both sources.</li>
<li>results:  substantially advances the previous state-of-the-art results on seven downstream tasks, and found that combining source input with a top-retrieved post from #Encoder is more effective than using semantically similar posts, and trigger words can largely benefit in merging context from the source and retrieved posts.Here is the answer in Simplified Chinese text:</li>
<li>for: 解决现有的自然语言理解（NLU）模型在面对短暴露的社交媒体内容时表现不佳的问题。</li>
<li>methods: 利用启发式学习（ICL）和一种带有标签驱动的启发式学习（HICL）框架，通过使用标签驱动BERT预训练的 pré-training，并使用梯度法来确定权重用于将来源和检索出的帖子内容融合。</li>
<li>results: substantially advance了之前的状态值表现结果，并发现将源输入与 #Encoder 预测的top-retrieved帖子融合是比使用相似的帖子更有效的。<details>
<summary>Abstract</summary>
Natural language understanding (NLU) is integral to various social media applications. However, existing NLU models rely heavily on context for semantic learning, resulting in compromised performance when faced with short and noisy social media content. To address this issue, we leverage in-context learning (ICL), wherein language models learn to make inferences by conditioning on a handful of demonstrations to enrich the context and propose a novel hashtag-driven in-context learning (HICL) framework. Concretely, we pre-train a model #Encoder, which employs #hashtags (user-annotated topic labels) to drive BERT-based pre-training through contrastive learning. Our objective here is to enable #Encoder to gain the ability to incorporate topic-related semantic information, which allows it to retrieve topic-related posts to enrich contexts and enhance social media NLU with noisy contexts. To further integrate the retrieved context with the source text, we employ a gradient-based method to identify trigger terms useful in fusing information from both sources. For empirical studies, we collected 45M tweets to set up an in-context NLU benchmark, and the experimental results on seven downstream tasks show that HICL substantially advances the previous state-of-the-art results. Furthermore, we conducted extensive analyzes and found that: (1) combining source input with a top-retrieved post from #Encoder is more effective than using semantically similar posts; (2) trigger words can largely benefit in merging context from the source and retrieved posts.
</details>
<details>
<summary>摘要</summary>
natural language understanding (NLU) 是社交媒体应用程序中的一个重要组成部分。然而，现有的 NLU 模型很重要地依赖于上下文进行 semantic learning，这会导致它们在短 и噪音的社交媒体内容上表现不佳。为解决这个问题，我们利用 in-context learning (ICL)，其中语言模型通过使用一些示例来增强上下文，并提出了一个具有 hash 标签驱动的增Context learning (HICL) 框架。具体来说，我们首先预训 #Encoder，该模型使用 #hash 标签（用户标注的主题标签）来驱动 BERT 基于的预训练。我们的目标是让 #Encoder 能够integrate topic-related semantic information，以便从 retrieve 的上下文中提取相关信息，并在社交媒体 NLU 中增强噪音上下文的表现。此外，我们还使用 Gradient 基本方法来确定激活词，以便将源文本和检索到的上下文 fusion 到一起。为 empirical studies，我们收集了 45 万条 tweet，并设置了一个 in-context NLU benchmark。实验结果显示，HICL 在七个下游任务上substantially advance 了之前的状态的术。此外，我们还进行了广泛的分析，发现：(1) 将源输入与检索到的最佳帖子 fusion 是更有效的 чем使用相似的帖子;(2) 触发词可以很大程度地帮助合并来源和检索到的上下文。
</details></li>
</ul>
<hr>
<h2 id="FinEval-A-Chinese-Financial-Domain-Knowledge-Evaluation-Benchmark-for-Large-Language-Models"><a href="#FinEval-A-Chinese-Financial-Domain-Knowledge-Evaluation-Benchmark-for-Large-Language-Models" class="headerlink" title="FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models"></a>FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09975">http://arxiv.org/abs/2308.09975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sufe-aiflm-lab/fineval">https://github.com/sufe-aiflm-lab/fineval</a></li>
<li>paper_authors: Liwen Zhang, Weige Cai, Zhaowei Liu, Zhi Yang, Wei Dai, Yujie Liao, Qianru Qin, Yifei Li, Xingyu Liu, Zhiqiang Liu, Zhoufan Zhu, Anbo Wu, Xin Guo, Yun Chen</li>
<li>for: 本研究旨在评估大语言模型（LLMs）在金融领域知识上的表现，并提供一个特性rich的评估 benchmark。</li>
<li>methods: 本研究使用了多种提问类型，包括零shot、几shot、答案只、链式思维等，以评估state-of-the-art的中文和英文 LLMS 在金融领域知识上的表现。</li>
<li>results: 结果显示，只有 GPT-4 在不同的提问设置下达到了接近 70% 的准确率，表明 LLMS 在金融领域知识上的可能性很大。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated exceptional performance in various natural language processing tasks, yet their efficacy in more challenging and domain-specific tasks remains largely unexplored. This paper presents FinEval, a benchmark specifically designed for the financial domain knowledge in the LLMs. FinEval is a collection of high-quality multiple-choice questions covering Finance, Economy, Accounting, and Certificate. It includes 4,661 questions spanning 34 different academic subjects. To ensure a comprehensive model performance evaluation, FinEval employs a range of prompt types, including zero-shot and few-shot prompts, as well as answer-only and chain-of-thought prompts. Evaluating state-of-the-art Chinese and English LLMs on FinEval, the results show that only GPT-4 achieved an accuracy close to 70% in different prompt settings, indicating significant growth potential for LLMs in the financial domain knowledge. Our work offers a more comprehensive financial knowledge evaluation benchmark, utilizing data of mock exams and covering a wide range of evaluated LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLMs）已经在各种自然语言处理任务中显示出了卓越表现，然而它们在更加具有挑战性和领域特有性的任务中的表现仍然尚未得到了充分的探索。这篇论文提出了FinEval，一个专门为金融领域知识的benchmark。FinEval包括4,661个高质量多选问题，涵盖了财务、经济、会计和证书等34个学科。为了全面评估模型的表现，FinEval使用了多种提问类型，包括零shot和几shot提问、以及答案只提问和链式思维提问。对现有的中文和英文LLMs进行FinEval的评估，结果显示，只有GPT-4在不同的提问设置中达到了近70%的准确率，这表明LLMs在金融领域知识方面还有很大的成长 potential。我们的工作提供了一个更加全面的金融知识评估benchmark，利用了考试数据和覆盖了各种评估LLMs。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection"><a href="#Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection" class="headerlink" title="Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection"></a>Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09892">http://arxiv.org/abs/2308.09892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bcwarner/sts-select">https://github.com/bcwarner/sts-select</a></li>
<li>paper_authors: Benjamin C. Warner, Ziqi Xu, Simon Haroutounian, Thomas Kannampallil, Chenyang Lu</li>
<li>for: 这篇论文是为了解决问题，即使 survey data 具有较高的特征数量且较低的例子数量， machine learning 模型还是能够预测结果的问题。</li>
<li>methods: 这篇论文使用了 feature selection 来解决这个问题，特别是使用 textual names of features 来评估哪些特征是有用的。</li>
<li>results: 研究发现，使用 STS 来选择特征可以实现更高的性能模型，比较传统的特征选择算法。<details>
<summary>Abstract</summary>
Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey data collected as a part of a clinical study on persistent post-surgical pain (PPSP). The results suggest that features selected with STS can result in higher performance models compared to traditional feature selection algorithms.
</details>
<details>
<summary>摘要</summary>
Survey data often contains a large number of features but only a small number of examples. If machine learning models are used to predict outcomes from this data, they may overfit and have poor generalizability. One solution to this problem is feature selection, which involves selecting a subset of the most relevant features to learn from. A previously unexplored source of information in the feature selection process is the textual names of the features, which may be semantically indicative of which features are relevant to the target outcome. We can use language models (LMs) to evaluate the relationships between feature names and target names and produce semantic textual similarity (STS) scores. These scores can then be used to select features. We compare the performance of STS as a feature selection metric with traditional feature selection algorithms using preliminary survey data collected as part of a clinical study on persistent post-surgical pain (PPSP). The results suggest that features selected with STS can lead to higher performance models.
</details></li>
</ul>
<hr>
<h2 id="Breaking-Language-Barriers-A-Question-Answering-Dataset-for-Hindi-and-Marathi"><a href="#Breaking-Language-Barriers-A-Question-Answering-Dataset-for-Hindi-and-Marathi" class="headerlink" title="Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi"></a>Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09862">http://arxiv.org/abs/2308.09862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maithili Sabane, Onkar Litake, Aman Chadha</li>
<li>for: developing a Question Answering dataset for low-resource languages Hindi and Marathi</li>
<li>methods: novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi</li>
<li>results: release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples, and the best-performing models for both Hindi and MarathiHere’s the simplified Chinese text for the three key points:</li>
<li>for: 为低资源语言� Hinidi 和 Marathi 开发问答集</li>
<li>methods: 使用 novel Approach 将 SQuAD 2.0 集 перевод成 Hinidi 和 Marathi</li>
<li>results: 发布了最大的问答集，每个集 contain 28,000 个样本，并且在两种语言上提供了最佳性能的模型<details>
<summary>Abstract</summary>
The recent advances in deep-learning have led to the development of highly sophisticated systems with an unquenchable appetite for data. On the other hand, building good deep-learning models for low-resource languages remains a challenging task. This paper focuses on developing a Question Answering dataset for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most spoken language worldwide, with 345 million speakers, and Marathi being the 11th most spoken language globally, with 83.2 million speakers, both languages face limited resources for building efficient Question Answering systems. To tackle the challenge of data scarcity, we have developed a novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples. We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which will facilitate further research in these languages. Leveraging similarity tools, our method holds the potential to create datasets in diverse languages, thereby enhancing the understanding of natural language across varied linguistic contexts. Our fine-tuned models, code, and dataset will be made publicly available.
</details>
<details>
<summary>摘要</summary>
Recent advances in deep learning have led to the development of highly sophisticated systems with an insatiable appetite for data. However, building good deep learning models for low-resource languages remains a challenging task. This paper focuses on developing a Question Answering dataset for two such languages - Hindi and Marathi. Despite Hindi being the third most spoken language worldwide with 345 million speakers and Marathi being the 11th most spoken language globally with 83.2 million speakers, both languages face limited resources for building efficient Question Answering systems. To address the challenge of data scarcity, we have developed a novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples. We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which will facilitate further research in these languages. Our method leverages similarity tools, which has the potential to create datasets in diverse languages, thereby enhancing the understanding of natural language across varied linguistic contexts. Our fine-tuned models, code, and dataset will be made publicly available.
</details></li>
</ul>
<hr>
<h2 id="Black-box-Adversarial-Attacks-against-Dense-Retrieval-Models-A-Multi-view-Contrastive-Learning-Method"><a href="#Black-box-Adversarial-Attacks-against-Dense-Retrieval-Models-A-Multi-view-Contrastive-Learning-Method" class="headerlink" title="Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view Contrastive Learning Method"></a>Black-box Adversarial Attacks against Dense Retrieval Models: A Multi-view Contrastive Learning Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09861">http://arxiv.org/abs/2308.09861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, Yixing Fan, Xueqi Cheng</li>
<li>for: 本研究主要针对于 dense retrieval (DR) 模型的 Robustness 问题。</li>
<li>methods: 本研究使用了现有的 adversarial attack 方法，并提出了一种基于 contrastive learning 的新方法来攻击 DR 模型。</li>
<li>results: 实验结果显示，新方法可以对 DR 模型进行有效的攻击，并且可以使用小量文本干扰来诱导模型返回错误结果。<details>
<summary>Abstract</summary>
Neural ranking models (NRMs) and dense retrieval (DR) models have given rise to substantial improvements in overall retrieval performance. In addition to their effectiveness, and motivated by the proven lack of robustness of deep learning-based approaches in other areas, there is growing interest in the robustness of deep learning-based approaches to the core retrieval problem. Adversarial attack methods that have so far been developed mainly focus on attacking NRMs, with very little attention being paid to the robustness of DR models. In this paper, we introduce the adversarial retrieval attack (AREA) task. The AREA task is meant to trick DR models into retrieving a target document that is outside the initial set of candidate documents retrieved by the DR model in response to a query. We consider the decision-based black-box adversarial setting, which is realistic in real-world search engines. To address the AREA task, we first employ existing adversarial attack methods designed for NRMs. We find that the promising results that have previously been reported on attacking NRMs, do not generalize to DR models: these methods underperform a simple term spamming method. We attribute the observed lack of generalizability to the interaction-focused architecture of NRMs, which emphasizes fine-grained relevance matching. DR models follow a different representation-focused architecture that prioritizes coarse-grained representations. We propose to formalize attacks on DR models as a contrastive learning problem in a multi-view representation space. The core idea is to encourage the consistency between each view representation of the target document and its corresponding viewer via view-wise supervision signals. Experimental results demonstrate that the proposed method can significantly outperform existing attack strategies in misleading the DR model with small indiscernible text perturbations.
</details>
<details>
<summary>摘要</summary>
神经排名模型（NRM）和紧凑检索（DR）模型已经导致总体检索性能得到了重大改善。此外，由于深度学习基本概念的不稳定性在其他领域已经证明了其不足，因此对深度学习基本概念的检索问题的Robustness也有增加的兴趣。许多攻击方法主要target NRMs，DR模型几乎没有受到关注。在本文中，我们介绍了抗击式检索任务（AREA）。AREA任务的目标是让DR模型返回一个不在初始候选文档中的目标文档。我们使用现有的NRMs攻击方法，并发现这些方法在DR模型上的表现不如预期差。我们归因这种不一致性于NRMs的交互强调的架构，DR模型采用了一种强调媒体表示的架构。我们提出了一种对DR模型进行攻击的方法，定义为多视图表示空间中的对比学习问题。核心思想是在每个视图中supervise the viewer's representation of the target document and its corresponding viewer via view-wise supervision signals。实验结果表明，我们的方法可以在小型文本干扰下明显超越现有攻击策略。
</details></li>
</ul>
<hr>
<h2 id="How-susceptible-are-LLMs-to-Logical-Fallacies"><a href="#How-susceptible-are-LLMs-to-Logical-Fallacies" class="headerlink" title="How susceptible are LLMs to Logical Fallacies?"></a>How susceptible are LLMs to Logical Fallacies?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09853">http://arxiv.org/abs/2308.09853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Amir-pyh/LOGICOM">https://github.com/Amir-pyh/LOGICOM</a></li>
<li>paper_authors: Amirreza Payandeh, Dan Pluth, Jordan Hosier, Xuesu Xiao, Vijay K. Gurbani</li>
<li>for: 本研究探讨了大型自然语言模型（LLM）在多轮辩论中的合理思维能力，特别是对逻辑错误的影响。</li>
<li>methods: 本研究使用了Logic Competence Measurement Benchmark（LOGICOM），一个用于评估LLM在逻辑错误下的逻辑理解能力的诊断标准。LOGICOM包括两个代理：一个诱导者和一个辩者，在一个争议性主题上进行多轮辩论，诱导者尝试使辩者接受其主张的正确性。</li>
<li>results: 研究发现，LLM可以通过理解来修改其意见。但是，当面临逻辑错误时，GPT-3.5和GPT-4分别被误导41%和69%更多次，相比于逻辑理解时。此外，本研究还提供了一个包含逻辑vs. 逻辑错误的5k+对话的新数据集，并公开发布了源代码和数据集。<details>
<summary>Abstract</summary>
This paper investigates the rational thinking capability of Large Language Models (LLMs) in multi-round argumentative debates by exploring the impact of fallacious arguments on their logical reasoning performance. More specifically, we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM involves two agents: a persuader and a debater engaging in a multi-round debate on a controversial topic, where the persuader tries to convince the debater of the correctness of its claim. First, LOGICOM assesses the potential of LLMs to change their opinions through reasoning. Then, it evaluates the debater's performance in logical reasoning by contrasting the scenario where the persuader employs logical fallacies against one where logical reasoning is used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4 using a dataset containing controversial topics, claims, and reasons supporting them. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their opinion through reasoning. However, when presented with logical fallacies, GPT-3.5 and GPT-4 are erroneously convinced 41% and 69% more often, respectively, compared to when logical reasoning is used. Finally, we introduce a new dataset containing over 5k pairs of logical vs. fallacious arguments. The source code and dataset of this work are made publicly available.
</details>
<details>
<summary>摘要</summary>
We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4, two popular LLMs, using a dataset of controversial topics, claims, and reasons supporting them. Our findings show that both GPT-3.5 and GPT-4 can adjust their opinions through reasoning, but when presented with logical fallacies, they are erroneously convinced 41% and 69% more often, respectively, compared to when logical reasoning is used.To further evaluate the ability of LLMs to distinguish between logical and fallacious arguments, we have created a new dataset containing over 5,000 pairs of logical vs. fallacious arguments. This dataset is publicly available, along with the source code for our benchmark. Our results have important implications for the development of LLMs and their use in real-world applications where logical reasoning is critical.
</details></li>
</ul>
<hr>
<h2 id="Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models"><a href="#Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models" class="headerlink" title="Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models"></a>Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09778">http://arxiv.org/abs/2308.09778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Rajabi, Jana Kosecka</li>
<li>for: 这个研究是用来评估大规模视力语言模型（VLM）在不同的视觉理解任务中表现的，特别是在理解空间关系方面。</li>
<li>methods: 这个研究使用了细化的组合基础grounding技术来评估视觉关系理解任务的性能，并提出了底层方法来排序空间句子并评估模型的表现。</li>
<li>results: 研究发现，现有的视力语言模型在理解空间关系方面表现不佳，有较大的差距和人类表现。而提出的方法可以减少这个差距，并提高模型的表现。<details>
<summary>Abstract</summary>
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and Bansal 2019; Gupta et al. 2022; Kamath et al. 2021) and compare and highlight their abilities to reason about spatial relationships.
</details>
<details>
<summary>摘要</summary>
随着大规模视言语模型（VLM）的发展，有兴趣测试它们在不同的视觉逻辑任务中的表现，如计数、引用表达和通用的视觉问答。本工作的重点是研究这些模型在理解空间关系方面的能力。在过去，这些任务通常通过图像文本匹配（Liu、Emerson、Collier 2022）或视觉问答任务进行评估，两者都显示了较差的表现和人类表现之间的大差。为更好地了解这个差距，我们提出了细化的 composer grounding 技术和底层方法来评估空间关系逻辑任务的表现。我们建议将对象和其位置的描述语Fragment（noun phrase）grounding 结果相加以计算最终的空间句排名。我们在代表性的视觉语言模型（Tan和Bansal 2019；Gupta等 2022；Kamath等 2021）上进行了示例实现，并对它们的空间关系逻辑能力进行了比较和强调。
</details></li>
</ul>
<hr>
<h2 id="YORC-Yoruba-Reading-Comprehension-dataset"><a href="#YORC-Yoruba-Reading-Comprehension-dataset" class="headerlink" title="YORC: Yoruba Reading Comprehension dataset"></a>YORC: Yoruba Reading Comprehension dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09768">http://arxiv.org/abs/2308.09768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anuoluwapo Aremu, Jesujoba O. Alabi, David Ifeoluwa Adelani</li>
<li>for: 这个论文创建了一个新的多选式 йоруба阅读理解数据集，基于 йоруба高中阅读理解考试。</li>
<li>methods: 该论文使用了已有的英文 RACE 数据集进行交叉语言传递，并使用预训练 encoder-only 模型获得基准结果。此外，还使用了大型自然语言模型（LLMs）如 GPT-4 进行推荐。</li>
<li>results: 该论文提供了基准结果和使用 LLMs 的结果。<details>
<summary>Abstract</summary>
In this paper, we create YORC: a new multi-choice Yoruba Reading Comprehension dataset that is based on Yoruba high-school reading comprehension examination. We provide baseline results by performing cross-lingual transfer using existing English RACE dataset based on a pre-trained encoder-only model. Additionally, we provide results by prompting large language models (LLMs) like GPT-4.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们创建了YORC：一个新的多选 йору巴读写理解数据集，该数据集基于 йору巴高中读写理解考试。我们提供了基线结果，使用现有的英语RACE数据集进行cross-lingual转移，并使用预训练的encoder-only模型。此外，我们还提供了使用大语言模型（LLMs）如GPT-4的结果。
</details></li>
</ul>
<hr>
<h2 id="OCR-Language-Models-with-Custom-Vocabularies"><a href="#OCR-Language-Models-with-Custom-Vocabularies" class="headerlink" title="OCR Language Models with Custom Vocabularies"></a>OCR Language Models with Custom Vocabularies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09671">http://arxiv.org/abs/2308.09671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Garst, Reeve Ingle, Yasuhisa Fujii</li>
<li>for: 提高特殊领域文档识别率</li>
<li>methods: 生成领域特定词语模型，附加到通用语模型上，并使用修改后的CTC搜索解码器</li>
<li>results: 降低特殊领域文档词语错误率<details>
<summary>Abstract</summary>
Language models are useful adjuncts to optical models for producing accurate optical character recognition (OCR) results. One factor which limits the power of language models in this context is the existence of many specialized domains with language statistics very different from those implied by a general language model - think of checks, medical prescriptions, and many other specialized document classes. This paper introduces an algorithm for efficiently generating and attaching a domain specific word based language model at run time to a general language model in an OCR system. In order to best use this model the paper also introduces a modified CTC beam search decoder which effectively allows hypotheses to remain in contention based on possible future completion of vocabulary words. The result is a substantial reduction in word error rate in recognizing material from specialized domains.
</details>
<details>
<summary>摘要</summary>
语言模型是Optical Character Recognition（OCR）结果的有用辅助工具。一个限制语言模型在这种情况下的力量是存在许多特殊领域的语言统计数据与一般语言模型所假设的语言统计数据异常不同 - 想象检查、医疗订单等多种专业文档类型。本文提出了一种方法，可以在运行时效率地生成并附加专业领域词汇基于语言模型。为了最好地使用这种模型，本文还提出了一种修改后的CTC搜索解码器，可以让假设中的词语在未来完成的可能性下保持在竞争中。这导致了特殊领域中word error rate的显著减少。
</details></li>
</ul>
<hr>
<h2 id="Red-Teaming-Large-Language-Models-using-Chain-of-Utterances-for-Safety-Alignment"><a href="#Red-Teaming-Large-Language-Models-using-Chain-of-Utterances-for-Safety-Alignment" class="headerlink" title="Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment"></a>Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09662">http://arxiv.org/abs/2308.09662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/declare-lab/red-instruct">https://github.com/declare-lab/red-instruct</a></li>
<li>paper_authors: Rishabh Bhardwaj, Soujanya Poria</li>
<li>For: This paper aims to address the risk of large language models (LLMs) producing harmful outputs and to develop a safety evaluation benchmark for LLMs.* Methods: The paper proposes a new safety evaluation benchmark called RED-EVAL, which uses a red-teaming approach to test the susceptibility of LLMs to harmful prompts. The authors also propose a method called RED-INSTRUCT for aligning LLMs with safe and helpful responses.* Results: The paper shows that even widely deployed LLMs are susceptible to harmful prompts, with more than 65% and 73% of harmful queries eliciting unethical responses from GPT-4 and ChatGPT, respectively. The authors also demonstrate the consistency of RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Finally, the authors show that their proposed safety alignment method (RED-INSTRUCT) can improve the safety of LLMs while preserving their utility.<details>
<summary>Abstract</summary>
Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH).
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在全球引起了风波，它们通过优化下一个单词预测目标来实现巨大多任务能力。然而， LLM 的出现也带来了它们生成有害输出的风险，使得它们不适合大规模部署。在这种情况下，我们提出了一个新的安全评估标准RED-EVAL，它通过红色队伍（red-teaming）来评估 LLM 的安全性。我们发现，广泛部署的模型都是对 CoU（链接词）提示的敏感，可以被破解，从而生成有害输出。此外，我们还证明了 RED-EVAL 在 8 个开源 LLM 上的一致性，它们在红色队伍中生成有害输出的情况超过 86%。接着，我们提出了 RED-INSTRUCT，一种安全对齐 LLM 的方法。它包括两个阶段：1）危险问题收集：通过 CoU 提示，我们收集了一个包含 1.9K 个危险问题、9.5K 个安全问题和 7.3K 个危险对话的数据集，来自 ChatGPT; 2）安全对齐：我们示例了如何使用 conversational 数据集来对 LLM 进行安全对齐，通过负采样损失来减少帮助Response的负损失，并对危险Response进行惩罚。我们的模型 STARLING，基于 Vicuna-7B 的 fine-tune，在 RED-EVAL 和 HHH benchmark 上被观察到更安全地对齐，而不会影响基eline模型（TruthfulQA、MMLU 和 BBH）的实用性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/cs.CL_2023_08_19/" data-id="clpztdndl00akes88a9kp4g2p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/cs.LG_2023_08_19/" class="article-date">
  <time datetime="2023-08-19T10:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/cs.LG_2023_08_19/">cs.LG - 2023-08-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision"><a href="#Efficient-Representation-Learning-for-Healthcare-with-Cross-Architectural-Self-Supervision" class="headerlink" title="Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision"></a>Efficient Representation Learning for Healthcare with Cross-Architectural Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10064">http://arxiv.org/abs/2308.10064</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pranavsinghps1/CASS">https://github.com/pranavsinghps1/CASS</a></li>
<li>paper_authors: Pranav Singh, Jacopo Cirrone</li>
<li>for: 这篇论文的目的是解决在医疗和生物医学应用中的极限计算需求，以实现表现学习的效果。</li>
<li>methods: 本论文提出了一种新的同构自监督学习方法，将传播储存器和对称神经网络（CNN）融合使用，以提高效率。</li>
<li>results:  empirical evaluation 表明，CASS-trained CNNs和Transformers 在四个不同的医疗数据集上比先前的自监督学习方法表现更好，尤其是只使用 1% 标注数据进行调整时。<details>
<summary>Abstract</summary>
In healthcare and biomedical applications, extreme computational requirements pose a significant barrier to adopting representation learning. Representation learning can enhance the performance of deep learning architectures by learning useful priors from limited medical data. However, state-of-the-art self-supervised techniques suffer from reduced performance when using smaller batch sizes or shorter pretraining epochs, which are more practical in clinical settings. We present Cross Architectural - Self Supervision (CASS) in response to this challenge. This novel siamese self-supervised learning approach synergistically leverages Transformer and Convolutional Neural Networks (CNN) for efficient learning. Our empirical evaluation demonstrates that CASS-trained CNNs and Transformers outperform existing self-supervised learning methods across four diverse healthcare datasets. With only 1% labeled data for finetuning, CASS achieves a 3.8% average improvement; with 10% labeled data, it gains 5.9%; and with 100% labeled data, it reaches a remarkable 10.13% enhancement. Notably, CASS reduces pretraining time by 69% compared to state-of-the-art methods, making it more amenable to clinical implementation. We also demonstrate that CASS is considerably more robust to variations in batch size and pretraining epochs, making it a suitable candidate for machine learning in healthcare applications.
</details>
<details>
<summary>摘要</summary>
在医疗和生物医学应用中，极高的计算需求成为了使用表示学习的障碍。表示学习可以提高深度学习架构的性能，但是现有的自我超vised学习技术在使用小批量或短duration pretraining epochs时会导致性能下降，这些epochs更加适合临床应用。为了解决这个挑战，我们提出了跨建筑-自我超vised学习（CASS）方法。这种新的siamesecross-architectural self-supervised learning方法利用了 transformer 和 convolutional neural networks（CNN），以高效地学习。我们的实验证明，CASS-trained CNNs和 transformers 在四种不同的医疗数据集上都超过了现有的自我超vised learning方法。具有1% 标注数据进行 fine-tuning，CASS 可以提高3.8%的平均提升;具有10% 标注数据，它可以提高5.9%;具有100% 标注数据，它可以达到了10.13%的很出色的提升。另外，CASS 可以降低预训练时间，相比之前的方法，它更适合临床应用。我们还证明了 CASS 对批量大小和预训练epochs的变化具有较强的鲁棒性，使其成为医学应用中的适合者。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling"><a href="#Accelerating-Exact-Combinatorial-Optimization-via-RL-based-Initialization-–-A-Case-Study-in-Scheduling" class="headerlink" title="Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling"></a>Accelerating Exact Combinatorial Optimization via RL-based Initialization – A Case Study in Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11652">http://arxiv.org/abs/2308.11652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Yin, Cunxi Yu</li>
<li>for: 本研究旨在提出一种创新的方法，使用机器学习（ML）解决计算图上的 combinatorial 优化问题，例如 EdgeTPU 平台上的计算图调度问题。</li>
<li>methods: 本研究使用了一种两阶段 RL-to-ILP 调度框架，包括三个步骤：1）RL 算法 acted as coarse-grain 调度器，2）解决 relaxation 和 3）精确的 ILP 解。</li>
<li>results: 研究表明，使用本研究的方法可以保证优化和幂等性，同时具有较高的运行时效率，在 EdgeTPU 平台上实现了 ImageNet DNN 计算图的实时推理和加速。<details>
<summary>Abstract</summary>
Scheduling on dataflow graphs (also known as computation graphs) is an NP-hard problem. The traditional exact methods are limited by runtime complexity, while reinforcement learning (RL) and heuristic-based approaches struggle with determinism and solution quality. This research aims to develop an innovative approach that employs machine learning (ML) for addressing combinatorial optimization problems, using scheduling as a case study. The goal is to provide guarantees in optimality and determinism while maintaining the runtime cost of heuristic methods. Specifically, we introduce a novel two-phase RL-to-ILP scheduling framework, which includes three steps: 1) RL solver acts as coarse-grain scheduler, 2) solution relaxation and 3) exact solving via ILP. Our framework demonstrates the same scheduling performance compared with using exact scheduling methods while achieving up to 128 $\times$ speed improvements. This was conducted on actual EdgeTPU platforms, utilizing ImageNet DNN computation graphs as input. Additionally, the framework offers improved on-chip inference runtime and acceleration compared to the commercially available EdgeTPU compiler.
</details>
<details>
<summary>摘要</summary>
“计划在数据流图（也称为计算图）是NP困难的问题。传统的精确方法受到时间复杂度的限制，而RL和规则基本方法则受到权化和解决质量的影响。这项研究旨在开发一种创新的方法，利用机器学习（ML）来解决 combinatorial 优化问题，使用计划为例研究。目标是提供优化和权化的保证，同时保持规则基本方法的运行时间成本。我们介绍了一种新的两阶段RL-to-ILP 计划框架，包括以下三个步骤：1）RL 解决器 acts as 粗糙度调度器，2）解决relaxation和3）精确的解决via ILP。我们的框架示出与使用精确计划方法相同的计划性表现，同时实现了128倍的速度提升。这些研究在实际的 EdgeTPU 平台上进行，使用 ImageNet DNN 计算图作为输入。此外，我们的框架还提供了比商业可用的 EdgeTPU 编译器更好的在处理器上的执行时间和加速。”
</details></li>
</ul>
<hr>
<h2 id="The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field"><a href="#The-Snowflake-Hypothesis-Training-Deep-GNN-with-One-Node-One-Receptive-field" class="headerlink" title="The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field"></a>The Snowflake Hypothesis: Training Deep GNN with One Node One Receptive field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10051">http://arxiv.org/abs/2308.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Wang, Guohao Li, Shilong Wang, Guibin Zhang, Kai Wang, Yang You, Xiaojiang Peng, Yuxuan Liang, Yang Wang</li>
<li>for: This paper aims to improve the performance and interpretability of deep graph neural networks (GNNs) by introducing the Snowflake Hypothesis, which posits that each node in a graph should have its own unique receptive field.</li>
<li>methods: The paper conducts a systematic study of deeper GNN research trajectories, employing the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node. The authors also compare their approach with different aggregation strategies on multiple benchmarks.</li>
<li>results: The paper demonstrates that the Snowflake Hypothesis can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. The authors show that their approach can be applied to various GNN frameworks, enhancing their effectiveness when operating in-depth, and guiding the selection of the optimal network depth in an explainable and generalizable way.Here is the text in Simplified Chinese:</li>
<li>for: 本 paper 的目的是提高深度 graph neural network (GNN) 的性能和可解性，通过引入 snowflake 假设，即每个节点在图中应该有自己独特的接受范围。</li>
<li>methods: 本 paper 通过系统地研究深度 GNN 研究轨迹，使用最简单的梯度和节点级cosine距离来规则每个节点的聚合深度。作者还比较了不同的聚合策略在多个 benchmark 上。</li>
<li>results: 本 paper 表明 snowflake 假设可以作为一个通用的运算，并且在深度 GNN 中显示出巨大的潜力。作者们表明该方法可以应用于不同的 GNN 框架，提高它们在深度下的效果，并且可以在可解的和通用的方式下选择最佳网络深度。<details>
<summary>Abstract</summary>
Despite Graph Neural Networks demonstrating considerable promise in graph representation learning tasks, GNNs predominantly face significant issues with over-fitting and over-smoothing as they go deeper as models of computer vision realm. In this work, we conduct a systematic study of deeper GNN research trajectories. Our findings indicate that the current success of deep GNNs primarily stems from (I) the adoption of innovations from CNNs, such as residual/skip connections, or (II) the tailor-made aggregation algorithms like DropEdge. However, these algorithms often lack intrinsic interpretability and indiscriminately treat all nodes within a given layer in a similar manner, thereby failing to capture the nuanced differences among various nodes. To this end, we introduce the Snowflake Hypothesis -- a novel paradigm underpinning the concept of ``one node, one receptive field''. The hypothesis draws inspiration from the unique and individualistic patterns of each snowflake, proposing a corresponding uniqueness in the receptive fields of nodes in the GNNs.   We employ the simplest gradient and node-level cosine distance as guiding principles to regulate the aggregation depth for each node, and conduct comprehensive experiments including: (1) different training schemes; (2) various shallow and deep GNN backbones, and (3) various numbers of layers (8, 16, 32, 64) on multiple benchmarks (six graphs including dense graphs with millions of nodes); (4) compare with different aggregation strategies. The observational results demonstrate that our hypothesis can serve as a universal operator for a range of tasks, and it displays tremendous potential on deep GNNs. It can be applied to various GNN frameworks, enhancing its effectiveness when operating in-depth, and guiding the selection of the optimal network depth in an explainable and generalizable way.
</details>
<details>
<summary>摘要</summary>
尽管图形神经网络（Graph Neural Networks，GNNs）在图像表示学任务中表现出了很大的承诺，但它们在深度层次上面临着颗粒泛化和颗粒滤波等问题。在这项工作中，我们进行了系统的 deeper GNN 研究轨迹。我们的发现表明，当前深度 GNN 的成功主要归功于（I） adopting innovations from CNNs，如待遇/跳过连接，或（II）适应制的聚合算法如 DropEdge。然而，这些算法通常缺乏内在解释性和不具分辨率地对所有层中的所有节点进行处理，因此无法捕捉不同节点之间的细腻差异。为此，我们提出了“雪花假设”——一种新的思想，它着眼于每个节点具有独特和特殊的感知领域。我们采用 simplest gradient 和节点级 cosine distance 作为引导原则，以REGULATE aggregation depth for each node，并进行了广泛的实验，包括：（1）不同训练方案；（2）不同的 shallow 和 deep GNN 基础架构；（3）不同层数（8, 16, 32, 64）在多个 benchmark 上进行了多种实验。结果表明，我们的假设可以作为一种通用的操作，并且在深度 GNN 中表现出了巨大的潜力。它可以应用于多种 GNN 框架，提高其在深度下的效果，并且可以在可解释的和普遍的方式下选择最佳网络深度。
</details></li>
</ul>
<hr>
<h2 id="Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings"><a href="#Computing-the-Vapnik-Chervonenkis-Dimension-for-Non-Discrete-Settings" class="headerlink" title="Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings"></a>Computing the Vapnik Chervonenkis Dimension for Non-Discrete Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10041">http://arxiv.org/abs/2308.10041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Nechba, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 这个论文的目的是为了开发一种可以计算无约束的VC dimenson的方法。</li>
<li>methods: 该论文使用了Empirical Risk Minimization（ERM）学习模型来Characterize the shattering property of a concept class。</li>
<li>results: 该论文提出了一种可以approximately计算VC dimenson的方法，不再需要约束于概念集或其域集。<details>
<summary>Abstract</summary>
In 1984, Valiant [ 7 ] introduced the Probably Approximately Correct (PAC) learning framework for boolean function classes. Blumer et al. [ 2] extended this model in 1989 by introducing the VC dimension as a tool to characterize the learnability of PAC. The VC dimension was based on the work of Vapnik and Chervonenkis in 1971 [8 ], who introduced a tool called the growth function to characterize the shattering property. Researchers have since determined the VC dimension for specific classes, and efforts have been made to develop an algorithm that can calculate the VC dimension for any concept class. In 1991, Linial, Mansour, and Rivest [4] presented an algorithm for computing the VC dimension in the discrete setting, assuming that both the concept class and domain set were finite. However, no attempts had been made to design an algorithm that could compute the VC dimension in the general setting.Therefore, our work focuses on developing a method to approximately compute the VC dimension without constraints on the concept classes or their domain set. Our approach is based on our finding that the Empirical Risk Minimization (ERM) learning paradigm can be used as a new tool to characterize the shattering property of a concept class.
</details>
<details>
<summary>摘要</summary>
在1984年，强力（7）引入了一种名为“可以近似正确”（PAC）学习框架，用于 boolean 函数类型。布吕默等人（2）在1989年将这个模型扩展了，通过引入 VC 维度来描述学习可能性。VC 维度基于1971年由普纳克和 Червонен科伊所引入的一种工具——生长函数，用于描述分裂性质。后续的研究者已经确定了特定类型的 VC 维度，并尝试了开发一种可以计算任何概念类型的 VC 维度的算法。在1991年，林亚尔、曼索尔和里韦斯特（4）提出了一种算法，用于计算 discrete  Setting 中的 VC 维度，假设概念类型和域集都是 фиксирован的。然而，没有任何尝试过开发一种可以计算总体设定中的 VC 维度的算法。因此，我们的工作将关注于开发一种可以近似计算 VC 维度的方法，不受概念类型或域集的限制。我们的方法基于我们发现，Empirical Risk Minimization（ERM）学习模式可以用作一种新的工具来描述分裂性质。
</details></li>
</ul>
<hr>
<h2 id="Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis"><a href="#Physics-guided-training-of-GAN-to-improve-accuracy-in-airfoil-design-synthesis" class="headerlink" title="Physics-guided training of GAN to improve accuracy in airfoil design synthesis"></a>Physics-guided training of GAN to improve accuracy in airfoil design synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10038">http://arxiv.org/abs/2308.10038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazunari Wada, Katsuyuki Suzuki, Kazuo Yonekura</li>
<li>for: 本研究使用生成对抗网络（GAN）进行机械形状的设计合成，但GANometimes输出物理不合理的形状。例如，当GAN模型在输出空气foil形状时，需要的 aerodynamic性能值会出现显著的错误。这是因为GAN模型只考虑数据，而不考虑下面的 aerodynamic equations。</li>
<li>methods: 本研究提出了基于物理学习的GAN模型训练方法，使得GAN模型学习物理有效性。物理有效性通过外部的通用软件计算，而不是直接在神经网络模型中实现物理方程。此外，生成模型的输出数据通常和训练数据相似，无法生成完全新的形状。但是，由于提议的模型受到物理模型的指导，不使用训练集，因此可以生成完全新的形状。</li>
<li>results: 数值实验表明，提议的模型可以减轻错误，同时输出的形状与训练集不同，但仍满足物理有效性。这超越了现有的GAN模型的局限性。<details>
<summary>Abstract</summary>
Generative adversarial networks (GAN) have recently been used for a design synthesis of mechanical shapes. A GAN sometimes outputs physically unreasonable shapes. For example, when a GAN model is trained to output airfoil shapes that indicate required aerodynamic performance, significant errors occur in the performance values. This is because the GAN model only considers data but does not consider the aerodynamic equations that lie under the data. This paper proposes the physics-guided training of the GAN model to guide the model to learn physical validity. Physical validity is computed using general-purpose software located outside the neural network model. Such general-purpose software cannot be used in physics-informed neural network frameworks, because physical equations must be implemented inside the neural network models. Additionally, a limitation of generative models is that the output data are similar to the training data and cannot generate completely new shapes. However, because the proposed model is guided by a physical model and does not use a training dataset, it can generate completely new shapes. Numerical experiments show that the proposed model drastically improves the accuracy. Moreover, the output shapes differ from those of the training dataset but still satisfy the physical validity, overcoming the limitations of existing GAN models.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GAN）最近在机械形状设计中得到了应用。然而，GAN sometimes outputs不理性的形状。例如，当GAN模型被训练来输出符合需要的 aerodynamic performance 的风 razor shape 时，会出现 significannot 的错误。这是因为GAN模型只考虑数据，而不考虑下面数据的 aerodynamic 方程。这篇论文提议通过 физи学导向的 GAN 模型来引导模型学习物理有效性。物理有效性通过外部通用软件计算，这种外部通用软件无法用于物理学 Informed Neural Network 框架中，因为物理方程必须在神经网络模型中实现。此外，生成模型的一个限制是输出数据与训练数据相似，无法生成完全新的形状。然而，由于提议的模型受到物理模型的导引，不使用训练集，因此可以生成完全新的形状。 num 实验表明，提议的模型可以减少错误，并且输出的形状与训练集不同，但仍满足物理有效性，超越现有 GAN 模型的限制。
</details></li>
</ul>
<hr>
<h2 id="High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison"><a href="#High-Performance-Computing-Applied-to-Logistic-Regression-A-CPU-and-GPU-Implementation-Comparison" class="headerlink" title="High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison"></a>High Performance Computing Applied to Logistic Regression: A CPU and GPU Implementation Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10037">http://arxiv.org/abs/2308.10037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nechbamohammed/swiftlogisticreg">https://github.com/nechbamohammed/swiftlogisticreg</a></li>
<li>paper_authors: Nechba Mohammed, Mouhajir Mohamed, Sedjari Yassine</li>
<li>for: 这个论文旨在提出一个基于GPU的多线程逻辑回传函数（Logistic Regression，LR），以满足巨量数据的运算需求。</li>
<li>methods: 这个实现方式是基于X. Zou等人提出的平行Gradient Descent Logistic Regression算法的直译版本。</li>
<li>results: 我们的GPU基于LR在处理大资料集时的执行时间比CPU基于实现更快，但与相同的f1分数相似。这使得我们的方法尤其有利于实时预测应用，如影像识别、垃圾邮件检测和诈欺检测。<details>
<summary>Abstract</summary>
We present a versatile GPU-based parallel version of Logistic Regression (LR), aiming to address the increasing demand for faster algorithms in binary classification due to large data sets. Our implementation is a direct translation of the parallel Gradient Descent Logistic Regression algorithm proposed by X. Zou et al. [12]. Our experiments demonstrate that our GPU-based LR outperforms existing CPU-based implementations in terms of execution time while maintaining comparable f1 score. The significant acceleration of processing large datasets makes our method particularly advantageous for real-time prediction applications like image recognition, spam detection, and fraud detection. Our algorithm is implemented in a ready-to-use Python library available at : https://github.com/NechbaMohammed/SwiftLogisticReg
</details>
<details>
<summary>摘要</summary>
我们提出了一种高性能的GPU基于的并行Logistic Regression（LR）算法，以满足大量数据集的需求。我们的实现是直接将平行梯度下降Logistic Regression算法提出的X. Zou等人的方案翻译而成。我们的实验表明，我们的GPU基于LR在执行时间方面与CPU基于实现相比具有明显的优势，同时保持了相似的准确率。这种加速处理大数据集的能力使得我们的方法在实时预测应用中，如图像识别、垃圾邮件检测和诈骗检测等方面具有明显的优势。我们的算法已经实现在Python库中，可以在以下地址下下载：https://github.com/NechbaMohammed/SwiftLogisticReg。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets"><a href="#Semi-Supervised-Anomaly-Detection-for-the-Determination-of-Vehicle-Hijacking-Tweets" class="headerlink" title="Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets"></a>Semi-Supervised Anomaly Detection for the Determination of Vehicle Hijacking Tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10036">http://arxiv.org/abs/2308.10036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taahir Aiyoob Patel, Clement N. Nyirenda</li>
<li>for: 本研究旨在使用微博来识别抢夺事件，以帮助旅行者避免成为受害者。</li>
<li>methods: 本研究使用了无监督异常检测算法，包括KNN和CBLOF两种方法，对Twitter上包含“抢夺”关键词的微博进行分析。</li>
<li>results: 比较分析表明，CBLOF方法的准确率为90%，而KNN方法的准确率为89%。CBLOF方法还得到了F1分数为0.8，而KNN方法得到了0.78。因此，CBLOF方法在抢夺微博识别方面表现了一定的优势。未来，将会对大型数据集使用超级vised学习方法进行比较，并使用优化机制提高总性能。<details>
<summary>Abstract</summary>
In South Africa, there is an ever-growing issue of vehicle hijackings. This leads to travellers constantly being in fear of becoming a victim to such an incident. This work presents a new semi-supervised approach to using tweets to identify hijacking incidents by using unsupervised anomaly detection algorithms. Tweets consisting of the keyword "hijacking" are obtained, stored, and processed using the term frequency-inverse document frequency (TF-IDF) and further analyzed by using two anomaly detection algorithms: 1) K-Nearest Neighbour (KNN); 2) Cluster Based Outlier Factor (CBLOF). The comparative evaluation showed that the KNN method produced an accuracy of 89%, whereas the CBLOF produced an accuracy of 90%. The CBLOF method was also able to obtain a F1-Score of 0.8, whereas the KNN produced a 0.78. Therefore, there is a slight difference between the two approaches, in favour of CBLOF, which has been selected as a preferred unsupervised method for the determination of relevant hijacking tweets. In future, a comparison will be done between supervised learning methods and the unsupervised methods presented in this work on larger dataset. Optimisation mechanisms will also be employed in order to increase the overall performance.
</details>
<details>
<summary>摘要</summary>
在南非， vehicular hijacking 是一个日益增长的问题。这使得旅行者们经常处于被劫持的恐惧中。本文提出了一种新的半监督方法，使用社交媒体上的 tweet 来识别劫持事件。使用关键词 "劫持" 获取、存储和处理 tweet，并使用 term frequency-inverse document frequency (TF-IDF) 进行处理。然后使用两种异常检测算法：1）K-Nearest Neighbour (KNN)；2）Cluster Based Outlier Factor (CBLOF)。对比评估表明，KNN 方法的准确率为 89%，而 CBLOF 方法的准确率为 90%。CBLOF 方法还可以获得 F1-Score 0.8，而 KNN 方法只有 0.78。因此，CBLOF 方法有一定的优势，因此被选为劫持 tweet 的半监督方法。未来，将进行大量数据集上的比较，以及优化机制的实现，以提高总性能。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion"><a href="#Effects-of-Convolutional-Autoencoder-Bottleneck-Width-on-StarGAN-based-Singing-Technique-Conversion" class="headerlink" title="Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion"></a>Effects of Convolutional Autoencoder Bottleneck Width on StarGAN-based Singing Technique Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10021">http://arxiv.org/abs/2308.10021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung-Cheng Su, Yung-Chuan Chang, Yi-Wen Liu</li>
<li>for: 本研究旨在评估对声音技巧转换（STC）中CAE核心宽度的影响，以提高转换质量。</li>
<li>methods: 我们采用了基于GAN的多域STC系统，利用WORLD vocoder表示和CAE架构。我们对CAE核心宽度进行了变换，并对转换结果进行主观评估。</li>
<li>results: 结果显示，更宽的CAE核心对于语音清晰度有益，但不一定导致更高的同化度。四种声音技巧中，抽喉声最容易转换，而其他三种声音技巧作为源声音更能实现更加真实的转换。<details>
<summary>Abstract</summary>
Singing technique conversion (STC) refers to the task of converting from one voice technique to another while leaving the original singer identity, melody, and linguistic components intact. Previous STC studies, as well as singing voice conversion research in general, have utilized convolutional autoencoders (CAEs) for conversion, but how the bottleneck width of the CAE affects the synthesis quality has not been thoroughly evaluated. To this end, we constructed a GAN-based multi-domain STC system which took advantage of the WORLD vocoder representation and the CAE architecture. We varied the bottleneck width of the CAE, and evaluated the conversion results subjectively. The model was trained on a Mandarin dataset which features four singers and four singing techniques: the chest voice, the falsetto, the raspy voice, and the whistle voice. The results show that a wider bottleneck corresponds to better articulation clarity but does not necessarily lead to higher likeness to the target technique. Among the four techniques, we also found that the whistle voice is the easiest target for conversion, while the other three techniques as a source produce more convincing conversion results than the whistle.
</details>
<details>
<summary>摘要</summary>
声乐技巧转换（STC）指的是将一种声乐技巧转换为另一种，保持原始歌手身份、旋律和语言元素不变。先前的 STC 研究以及全球声乐转换研究都使用了卷积 autoencoder（CAE）进行转换，但 CAE 瓶颈宽度如何影响转换质量尚未得到全面评估。为了解决这个问题，我们构建了基于 GAN 的多Domain STC 系统，利用了 WORLD  vocoder 表示和 CAE 建筑。我们在 CAE 瓶颈宽度上进行了变量，并对转换结果进行主观评估。系统在普通话 dataset 上训练，该 dataset 包含四名歌手和四种声乐技巧：胸部声、抖音声、咔声和喊声。结果表明，宽瓶颈对声音清晰度有更好的影响，但并不一定导致更高的模仿度。其中四种技巧中，喊声是转换最容易的目标，而其他三种技巧作为源都能够更加自然地转换。
</details></li>
</ul>
<hr>
<h2 id="Semi-Implicit-Variational-Inference-via-Score-Matching"><a href="#Semi-Implicit-Variational-Inference-via-Score-Matching" class="headerlink" title="Semi-Implicit Variational Inference via Score Matching"></a>Semi-Implicit Variational Inference via Score Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10014">http://arxiv.org/abs/2308.10014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longinyu/sivism">https://github.com/longinyu/sivism</a></li>
<li>paper_authors: Longlin Yu, Cheng Zhang</li>
<li>for: 强化变量推理的表达力（Semi-implicit variational inference，SIVI），使得变量推理家族更加丰富和灵活。</li>
<li>methods: 基于分形证据匹配（score matching）的一种新方法，充分利用变量推理家族的层次结构，使得困难的变量推理density可以自然地处理。</li>
<li>results: 与MCMC相比，SIVI-SM方法可以准确地匹配MCMC的准确性，并且在多种推理任务中超过ELBO基于的SIVI方法。<details>
<summary>Abstract</summary>
Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching. We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用半隐式变量推断（SIVI）可以扩大变量家族的表达能力，通过在层次结构中考虑隐式变量分布定义。然而，由于变量分布的计算困难，现有的SIVI方法通常使用代理证据下界（ELBO）或者使用昂贵的内 Loop MCMC 迭代进行培训。在这篇论文中，我们提出了基于代理证据匹配的新方法，称为SIVI-SM。该方法利用层次结构中的 semi-implicit 变量家族，通过代理证据匹配对象来实现一种减少对变量分布的干扰的训练目标。我们表明，SIVI-SM可以准确地与 MCMC 匹配，并且在多种 bayesian 推断任务中超过 ELBO 基于的 SIVI 方法表现。>>>
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Cross-Subject-EEG-Decoding"><a href="#Distributionally-Robust-Cross-Subject-EEG-Decoding" class="headerlink" title="Distributionally Robust Cross Subject EEG Decoding"></a>Distributionally Robust Cross Subject EEG Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11651">http://arxiv.org/abs/2308.11651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiehang Duan, Zhenyi Wang, Gianfranco Doretto, Fang Li, Cui Tao, Donald Adjeroh<br>for: 这个研究旨在提高EEG解oding任务的表现，特别是面对高度不确定和不同类型的资料污染。methods: 这个研究使用分布robust优化和 Wasserstein gradient flow (WGF) 提出了一个原理式的演化方法，通过对数据分布进行演化来提高解oding的类别识别能力。results: 实验结果显示，提案的方法可以与其他数据增强技术结合使用，在严重损坏EEG讯号的情况下表现出色，较前一些竞争基eline。<details>
<summary>Abstract</summary>
Recently, deep learning has shown to be effective for Electroencephalography (EEG) decoding tasks. Yet, its performance can be negatively influenced by two key factors: 1) the high variance and different types of corruption that are inherent in the signal, 2) the EEG datasets are usually relatively small given the acquisition cost, annotation cost and amount of effort needed. Data augmentation approaches for alleviation of this problem have been empirically studied, with augmentation operations on spatial domain, time domain or frequency domain handcrafted based on expertise of domain knowledge. In this work, we propose a principled approach to perform dynamic evolution on the data for improvement of decoding robustness. The approach is based on distributionally robust optimization and achieves robustness by optimizing on a family of evolved data distributions instead of the single training data distribution. We derived a general data evolution framework based on Wasserstein gradient flow (WGF) and provides two different forms of evolution within the framework. Intuitively, the evolution process helps the EEG decoder to learn more robust and diverse features. It is worth mentioning that the proposed approach can be readily integrated with other data augmentation approaches for further improvements. We performed extensive experiments on the proposed approach and tested its performance on different types of corrupted EEG signals. The model significantly outperforms competitive baselines on challenging decoding scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning"><a href="#Disposable-Transfer-Learning-for-Selective-Source-Task-Unlearning" class="headerlink" title="Disposable Transfer Learning for Selective Source Task Unlearning"></a>Disposable Transfer Learning for Selective Source Task Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09971">http://arxiv.org/abs/2308.09971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghee Koh, Hyounguk Shon, Janghyeon Lee, Hyeong Gwon Hong, Junmo Kim</li>
<li>for: 这个论文旨在解决如何在转移学习中保持目标任务的表现，而不是完全抛弃源任务的表现。</li>
<li>methods: 该论文提出了一种新的转移学习方法，即丢弃转移学习（DTL），该方法可以在转移学习过程中 selectively 抛弃源任务的知识。</li>
<li>results: 论文表明，通过使用 Gradient Collision loss（GC loss）可以帮助模型 selectively 抛弃源任务的知识，同时保持目标任务的表现。 GC loss 可以衡量知识泄露的程度，并且可以在新的下游任务上进行重新训练。<details>
<summary>Abstract</summary>
Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Transfer learning is widely used for training deep neural networks (DNN) for building a powerful representation. Even after the pre-trained model is adapted for the target task, the representation performance of the feature extractor is retained to some extent. As the performance of the pre-trained model can be considered the private property of the owner, it is natural to seek the exclusive right of the generalized performance of the pre-trained weight. To address this issue, we suggest a new paradigm of transfer learning called disposable transfer learning (DTL), which disposes of only the source task without degrading the performance of the target task. To achieve knowledge disposal, we propose a novel loss named Gradient Collision loss (GC loss). GC loss selectively unlearns the source knowledge by leading the gradient vectors of mini-batches in different directions. Whether the model successfully unlearns the source task is measured by piggyback learning accuracy (PL accuracy). PL accuracy estimates the vulnerability of knowledge leakage by retraining the scrubbed model on a subset of source data or new downstream data. We demonstrate that GC loss is an effective approach to the DTL problem by showing that the model trained with GC loss retains the performance on the target task with a significantly reduced PL accuracy."中文翻译：转移学习广泛用于训练深度神经网络（DNN）以建立强大的表示。即使预训练模型被适应目标任务，表示性性能的特征提取器仍然保持一定程度的表现。由于预训练模型的性能可以被视为业主的私有财产，因此自然想要寻求预训练权重的权属。为解决这个问题，我们提出了一种新的转移学习方法 called  dispose transfer learning（DTL），它可以不影响目标任务的性能，但是可以消除来源任务。为实现知识抛弃，我们提出了一种新的损失函数名为梯度碰撞损失（GC损失）。GC损失可以 selectively 忘记来源知识，通过导向梯度向量的不同方向。确定模型是否成功忘记来源任务，可以通过猪肉学习精度（PL精度）来衡量。PL精度可以估计知识泄露的敏感度，通过在源数据或新的下游数据上重新训练混凝模型。我们示示GC损失是解决 DTL 问题的有效方法，通过显示GC损失训练的模型在目标任务上保持表现，同时减少了PL精度。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues"><a href="#Tackling-Vision-Language-Tasks-Through-Learning-Inner-Monologues" class="headerlink" title="Tackling Vision Language Tasks Through Learning Inner Monologues"></a>Tackling Vision Language Tasks Through Learning Inner Monologues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09970">http://arxiv.org/abs/2308.09970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diji Yang, Kezhen Chen, Jinmeng Rao, Xiaoyuan Guo, Yawen Zhang, Jie Yang, Yi Zhang</li>
<li>for: 解决复杂的视觉语言问题，使用内启思维过程来优化语言模型和视觉模型之间的拟合。</li>
<li>methods: 提出了一种新的方法 Inner Monologue Multi-Modal Optimization (IMMO)，通过模拟内启思维过程，使得语言模型和视觉模型可以在自然语言交流中互动，提高推理和解释能力。</li>
<li>results: 对两个popular任务进行评估，结果表明，通过模拟内启思维过程，IMMO可以提高视觉语言模型的推理和解释能力，并且可以应用于许多不同的AI问题。<details>
<summary>Abstract</summary>
Visual language tasks require AI models to comprehend and reason with both visual and textual content. Driven by the power of Large Language Models (LLMs), two prominent methods have emerged: (1) the hybrid integration between LLMs and Vision-Language Models (VLMs), where visual inputs are firstly converted into language descriptions by VLMs, serving as inputs for LLMs to generate final answer(s); (2) visual feature alignment in language space, where visual inputs are encoded as embeddings and projected to LLMs' language space via further supervised fine-tuning. The first approach provides light training costs and interpretability but is hard to be optimized in an end-to-end fashion. The second approach presents decent performance, but feature alignment usually requires large amounts of training data and lacks interpretability. To tackle this dilemma, we propose a novel approach, Inner Monologue Multi-Modal Optimization (IMMO), to solve complex vision language problems by simulating inner monologue processes, a cognitive process in which an individual engages in silent verbal communication with themselves. We enable LLMs and VLMs to interact through natural language conversation and propose to use a two-stage training process to learn how to do the inner monologue (self-asking questions and answering questions). IMMO is evaluated on two popular tasks and the results suggest by emulating the cognitive phenomenon of internal dialogue, our approach can enhance reasoning and explanation abilities, contributing to the more effective fusion of vision and language models. More importantly, instead of using predefined human-crafted monologues, IMMO learns this process within the deep learning models, promising wider applicability to many different AI problems beyond vision language tasks.
</details>
<details>
<summary>摘要</summary>
⟨SYS⟩视觉语言任务需要人工智能模型理解和处理视觉和文本内容。受大型语言模型（LLM）的力量驱动，两种主要方法出现：（1）混合 integrate LLMs 和视觉语言模型（VLMs），其中视觉输入首先被VLMs转换为语言描述，然后被LLMs 生成最终答案；（2）视觉特征对齐在语言空间，其中视觉输入被编码为特征并通过进一步的超vised fine-tuning Projected to LLMs 的语言空间。前一种方法具有轻量级训练成本和可读性，但困难在端到端方式优化；后一种方法具有良好的性能，但特征对齐通常需要大量的训练数据和缺乏可读性。为解决这个悖论，我们提出了一种新的方法： Inner Monologue Multi-Modal Optimization（IMMO），用于解决复杂的视觉语言问题。我们使得 LLMs 和 VLMs 通过自然语言对话互动，并提出了在两个阶段训练过程中学习如何做内心对话（自我问答）。IMMO 在两个流行的任务上进行评估，结果表明，通过模拟内心对话这种认知现象，我们的方法可以提高理解和解释能力，为视觉语言模型的更有效融合做出贡献。此外，不同于使用预先定制的人类编写的假象，IMMO 在深度学习模型中学习内心对话过程，提供更广泛的应用可能性。
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation"><a href="#Anomaly-Aware-Semantic-Segmentation-via-Style-Aligned-OoD-Augmentation" class="headerlink" title="Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation"></a>Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09965">http://arxiv.org/abs/2308.09965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zhang, Kaspar Sakmann, William Beluch, Robin Hutmacher, Yumeng Li</li>
<li>for: 这paper是为了帮助标准semantic segmentation模型具备异常感知而写的。</li>
<li>methods: 这paper使用了改进的out-of-distribution数据生成方法，将异常数据与驾驶场景的风格差减少到最小化，从而减轻数据生成过程中的 shortcut。此外，paper还提出了一种简单的练习损失，使得预训练的semantic segmentation模型能够生成“None of the given classes”预测，通过每个像素的异常分数进行异常分割。</li>
<li>results: 与普通的semantic segmentation模型相比，这paper的方法能够减少异常分割的训练时间和精度损失，同时保持原始任务的性能。<details>
<summary>Abstract</summary>
Within the context of autonomous driving, encountering unknown objects becomes inevitable during deployment in the open world. Therefore, it is crucial to equip standard semantic segmentation models with anomaly awareness. Many previous approaches have utilized synthetic out-of-distribution (OoD) data augmentation to tackle this problem. In this work, we advance the OoD synthesis process by reducing the domain gap between the OoD data and driving scenes, effectively mitigating the style difference that might otherwise act as an obvious shortcut during training. Additionally, we propose a simple fine-tuning loss that effectively induces a pre-trained semantic segmentation model to generate a ``none of the given classes" prediction, leveraging per-pixel OoD scores for anomaly segmentation. With minimal fine-tuning effort, our pipeline enables the use of pre-trained models for anomaly segmentation while maintaining the performance on the original task.
</details>
<details>
<summary>摘要</summary>
在自动驾驶中，遇到未知对象是不可避免的，因此需要为标准Semantic segmentation模型增加异常意识。许多前一代方法使用synthetic Out-of-distribution（OoD）数据增强进行了处理。在这种工作中，我们提高了OoD数据生成过程，将驾驶场景和OoD数据域的差异降低到最小，从而有效地消除了可能导致训练中的短cut shortcut。此外，我们提议一种简单的精度调整loss，使得预训练的Semantic segmentation模型能够生成“ none of the given classes”预测，通过每个像素的OoD分数进行异常分 segmentation。只需 minimal fine-tuning effort，我们的管道可以使用预训练模型进行异常分 segmentation，同时保持原始任务的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching"><a href="#Towards-Self-Adaptive-Machine-Learning-Enabled-Systems-Through-QoS-Aware-Model-Switching" class="headerlink" title="Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching"></a>Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09960">http://arxiv.org/abs/2308.09960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sa4s-serc/adamls">https://github.com/sa4s-serc/adamls</a></li>
<li>paper_authors: Shubham Kulkarni, Arya Marda, Karthik Vaidhyanathan</li>
<li>for: 本研究旨在提出一种机器学习模型均衡器，以管理运行时uncertainty，保证机器学习系统的质量。</li>
<li>methods: 本研究使用多个机器学习模型，并 introduce了一种基于MAPLE-K循环的自适应策略，以实现不断自适应机器学习系统。</li>
<li>results: 实验结果表明，AdaMLS可以更好地保证系统和模型的性能，并且在动态环境中提供优化的QoS。<details>
<summary>Abstract</summary>
Machine Learning (ML), particularly deep learning, has seen vast advancements, leading to the rise of Machine Learning-Enabled Systems (MLS). However, numerous software engineering challenges persist in propelling these MLS into production, largely due to various run-time uncertainties that impact the overall Quality of Service (QoS). These uncertainties emanate from ML models, software components, and environmental factors. Self-adaptation techniques present potential in managing run-time uncertainties, but their application in MLS remains largely unexplored. As a solution, we propose the concept of a Machine Learning Model Balancer, focusing on managing uncertainties related to ML models by using multiple models. Subsequently, we introduce AdaMLS, a novel self-adaptation approach that leverages this concept and extends the traditional MAPE-K loop for continuous MLS adaptation. AdaMLS employs lightweight unsupervised learning for dynamic model switching, thereby ensuring consistent QoS. Through a self-adaptive object detection system prototype, we demonstrate AdaMLS's effectiveness in balancing system and model performance. Preliminary results suggest AdaMLS surpasses naive and single state-of-the-art models in QoS guarantees, heralding the advancement towards self-adaptive MLS with optimal QoS in dynamic environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection"><a href="#A-Comparison-of-Adversarial-Learning-Techniques-for-Malware-Detection" class="headerlink" title="A Comparison of Adversarial Learning Techniques for Malware Detection"></a>A Comparison of Adversarial Learning Techniques for Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09958">http://arxiv.org/abs/2308.09958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavla Louthánová, Matouš Kozák, Martin Jureček, Mark Stamp</li>
<li>for: 本文Addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files, to evaluate the effectiveness of different methods for generating adversarial samples and their practical applicability.</li>
<li>methods: The paper uses gradient-based, evolutionary algorithm-based, and reinforcement-based methods to generate adversarial samples, and tests the generated samples against selected antivirus products.</li>
<li>results: The results show that applying optimized modifications to previously detected malware can lead to incorrect classification of the file as benign, and that generated malware samples can be successfully used against detection models other than those used to generate them. The Gym-malware generator, which uses a reinforcement learning approach, has the greatest practical potential, achieving an average sample generation time of 5.73 seconds and the highest average evasion rate of 44.11%. Using the Gym-malware generator in combination with itself improved the evasion rate to 58.35%.<details>
<summary>Abstract</summary>
Machine learning has proven to be a useful tool for automated malware detection, but machine learning models have also been shown to be vulnerable to adversarial attacks. This article addresses the problem of generating adversarial malware samples, specifically malicious Windows Portable Executable files. We summarize and compare work that has focused on adversarial machine learning for malware detection. We use gradient-based, evolutionary algorithm-based, and reinforcement-based methods to generate adversarial samples, and then test the generated samples against selected antivirus products. We compare the selected methods in terms of accuracy and practical applicability. The results show that applying optimized modifications to previously detected malware can lead to incorrect classification of the file as benign. It is also known that generated malware samples can be successfully used against detection models other than those used to generate them and that using combinations of generators can create new samples that evade detection. Experiments show that the Gym-malware generator, which uses a reinforcement learning approach, has the greatest practical potential. This generator achieved an average sample generation time of 5.73 seconds and the highest average evasion rate of 44.11%. Using the Gym-malware generator in combination with itself improved the evasion rate to 58.35%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks"><a href="#To-prune-or-not-to-prune-A-chaos-causality-approach-to-principled-pruning-of-dense-neural-networks" class="headerlink" title="To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks"></a>To prune or not to prune : A chaos-causality approach to principled pruning of dense neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09955">http://arxiv.org/abs/2308.09955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajan Sahu, Shivam Chadha, Nithin Nagaraj, Archana Mathur, Snehanshu Saha</li>
<li>for: 降低神经网络的大小（卷积），不受性能受到影响，是资源有限设备上重要问题。</li>
<li>methods: 通过Weight rankings或penalization criteria，如矩阵大小和重要性，从已 retrained 的剩下 weights 中选择特定的 weights，来实现卷积。</li>
<li>results: 采用这种方法可以保持原始性能，同时保留特征解释性。<details>
<summary>Abstract</summary>
Reducing the size of a neural network (pruning) by removing weights without impacting its performance is an important problem for resource-constrained devices. In the past, pruning was typically accomplished by ranking or penalizing weights based on criteria like magnitude and removing low-ranked weights before retraining the remaining ones. Pruning strategies may also involve removing neurons from the network in order to achieve the desired reduction in network size. We formulate pruning as an optimization problem with the objective of minimizing misclassifications by selecting specific weights. To accomplish this, we have introduced the concept of chaos in learning (Lyapunov exponents) via weight updates and exploiting causality to identify the causal weights responsible for misclassification. Such a pruned network maintains the original performance and retains feature explainability.
</details>
<details>
<summary>摘要</summary>
减小神经网络（减少），不影响其性能，是资源受限设备上非常重要的问题。以前，减少通常通过对权重进行排名或惩罚，根据权重的大小和重要性来移除低排名的权重，然后再 retrained 剩下的权重来实现网络的减少。减少策略可能还会涉及到从网络中移除神经元，以实现所需的网络大小减少。我们将减少形式为学习过程中权重更新中引入了混乱（Lyapunov 分数）的概念，并通过利用 causality 来确定引起错分的权重。这样的减少网络可以保持原始性能，同时保持特征解释性。
</details></li>
</ul>
<hr>
<h2 id="Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning"><a href="#Finding-emergence-in-data-causal-emergence-inspired-dynamics-learning" class="headerlink" title="Finding emergence in data: causal emergence inspired dynamics learning"></a>Finding emergence in data: causal emergence inspired dynamics learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09952">http://arxiv.org/abs/2308.09952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhe Yang, Zhipeng Wang, Kaiwei Liu, Yingqi Rong, Bing Yuan, Jiang Zhang</li>
<li>for: This paper aims to develop a machine learning framework to model complex dynamical systems in a data-driven manner, with a focus on capturing emergent behaviors and properties.</li>
<li>methods: The proposed framework draws inspiration from the theory of causal emergence and uses maximum effective information (EI) to learn macro-dynamics within an emergent latent space.</li>
<li>results: The proposed framework is effective in capturing emergent patterns, learning the coarse-graining strategy, and quantifying the degree of causal emergence in the data. Additionally, the model demonstrates superior generalization ability on environments different from the training dataset.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是通过数据驱动方式模型复杂的动力系统，强调捕捉出emergent行为和质量。</li>
<li>methods: 提议的框架启发自 causal emergence 理论，使用最大有效信息（EI）来学习macro-dinamics在emergent latent space中。</li>
<li>results: 提议的框架能够成功捕捉emergentpattern，学习卷积策略，并量化数据中的 causal emergence度。此外，模型在不同于训练集环境下的实验也表现出了superior generalization能力。<details>
<summary>Abstract</summary>
Modelling complex dynamical systems in a data-driven manner is challenging due to the presence of emergent behaviors and properties that cannot be directly captured by micro-level observational data. Therefore, it is crucial to develop a model that can effectively capture emergent dynamics at the macro-level and quantify emergence based on the available data. Drawing inspiration from the theory of causal emergence, this paper introduces a machine learning framework aimed at learning macro-dynamics within an emergent latent space. The framework achieves this by maximizing the effective information (EI) to obtain a macro-dynamics model with stronger causal effects. Experimental results on both simulated and real data demonstrate the effectiveness of the proposed framework. Not only does it successfully capture emergent patterns, but it also learns the coarse-graining strategy and quantifies the degree of causal emergence in the data. Furthermore, experiments conducted on environments different from the training dataset highlight the superior generalization ability of our model.
</details>
<details>
<summary>摘要</summary>
模拟复杂动力系统在数据驱动方式下是一项挑战，因为存在不可预测的 emergent 行为和质量。为了有效地捕捉 emergent 动力，这篇论文提出了一种基于机器学习的框架，该框架在 emergent 尘肤空间中学习 macro-动力。该框架通过最大化有效信息（EI）来获得具有更强的 causal 效应的 macro-动力模型。实验结果表明，该模型不仅可以成功捕捉 emergent 模式，还可以学习 coarse-graining 策略并量化数据中的 causal emergence 度。此外，在训练数据集以外的环境下进行的实验也表明了我们的模型具有更高的总体化能力。
</details></li>
</ul>
<hr>
<h2 id="Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease"><a href="#Study-on-the-effectiveness-of-AutoML-in-detecting-cardiovascular-disease" class="headerlink" title="Study on the effectiveness of AutoML in detecting cardiovascular disease"></a>Study on the effectiveness of AutoML in detecting cardiovascular disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09947">http://arxiv.org/abs/2308.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: T. V. Afanasieva, A. P. Kuzlyakin, A. V. Komolov<br>for: 这篇论文主要是为了探讨机器学习技术在抗生素敏感疾病预测方面的应用。methods: 这篇论文使用了自动机器学习（AutoML）技术，combined five data sets of cardiovascular disease indicators from the UCI Machine Learning Repository，并用了十三种基本机器学习模型（KNeighborsUnif、KNeighborsDist、LightGBMXT、LightGBM、RandomForestGini、RandomForestEntr、CatBoost、ExtraTreesGini、ExtraTreesEntr、NeuralNetFastA、XGBoost、NeuralNetTorch、LightGBMLarge）。results: 研究发现，自动机器学习模型的结构对于抗生素敏感疾病预测是不同的，具体取决于使用的基本模型的效率和准确率，以及数据预处理方法，尤其是数据标准化技术。研究发现，当将源数据标准化为二进制值时，自动机器学习模型的准确率最高，达到了87.41%至92.3%之间的范围。<details>
<summary>Abstract</summary>
Cardiovascular diseases are widespread among patients with chronic noncommunicable diseases and are one of the leading causes of death, including in the working age. The article presents the relevance of the development and application of patient-oriented systems, in which machine learning (ML) is a promising technology that allows predicting cardiovascular diseases. Automated machine learning (AutoML) makes it possible to simplify and speed up the process of developing AI/ML applications, which is key in the development of patient-oriented systems by application users, in particular medical specialists. The authors propose a framework for the application of automatic machine learning and three scenarios that allowed for data combining five data sets of cardiovascular disease indicators from the UCI Machine Learning Repository to investigate the effectiveness in detecting this class of diseases. The study investigated one AutoML model that used and optimized the hyperparameters of thirteen basic ML models (KNeighborsUnif, KNeighborsDist, LightGBMXT, LightGBM, RandomForestGini, RandomForestEntr, CatBoost, ExtraTreesGini, ExtraTreesEntr, NeuralNetFastA, XGBoost, NeuralNetTorch, LightGBMLarge) and included the most accurate models in the weighted ensemble. The results of the study showed that the structure of the AutoML model for detecting cardiovascular diseases depends not only on the efficiency and accuracy of the basic models used, but also on the scenarios for preprocessing the initial data, in particular, on the technique of data normalization. The comparative analysis showed that the accuracy of the AutoML model in detecting cardiovascular disease varied in the range from 87.41% to 92.3%, and the maximum accuracy was obtained when normalizing the source data into binary values, and the minimum was obtained when using the built-in AutoML technique.
</details>
<details>
<summary>摘要</summary>
心血管疾病是 chronic noncommunicable diseases 中广泛存在的，是死亡的主要原因之一，包括工作年龄期。本文介绍了patient-oriented系统的开发和应用的重要性，机器学习（ML）技术在心血管疾病预测方面的潜在性。自动机器学习（AutoML）技术可以简化和加速AI/ML应用程序的开发过程，这对医疗专业人员 particularly medical specialists 来说是关键。作者提出了一个自动机器学习框架，并通过将五个心血管疾病指标数据集 combine  investigate the effectiveness of detecting this class of diseases。研究使用了一个AutoML模型，该模型使用和优化了十三种基本ML模型（KNeighborsUnif、KNeighborsDist、LightGBMXT、LightGBM、RandomForestGini、RandomForestEntr、CatBoost、ExtraTreesGini、ExtraTreesEntr、NeuralNetFastA、XGBoost、NeuralNetTorch、LightGBMLarge），并包括最佳模型在权重 ensemble 中。研究结果表明，自动机器学习模型的结构在检测心血管疾病方面取决于基本模型的效率和准确率，以及数据预处理方法，特别是数据 нор化技术。比较分析表明，自动机器学习模型在检测心血管疾病方面的准确率在87.41%到92.3%之间，最高准确率为将源数据 нор化为二进制值，最低准确率为使用AutoML技术。
</details></li>
</ul>
<hr>
<h2 id="Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy"><a href="#Dual-Branch-Deep-Learning-Network-for-Detection-and-Stage-Grading-of-Diabetic-Retinopathy" class="headerlink" title="Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy"></a>Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09945">http://arxiv.org/abs/2308.09945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shakibania, Sina Raoufi, Behnam Pourafkham, Hassan Khotanlou, Muharram Mansoorizadeh<br>for: 这个研究的目的是提出一种基于深度学习的糖尿病肠病诊断和分级方法，使用单一的背景照片。methods: 该模型使用了传输学习，利用两个现有的状态流程模型作为特征提取器，并在新的数据集上进行细化。results: 该模型在APTOS 2019 数据集上表现出色，在糖尿病诊断和分级方面都高于现有文献。对于二分类问题，该方法达到了98.50%的准确率、99.46%的敏感度和97.51%的特异度。在分级问题上，它达到了93.00%的квадратиче weights κ值、89.60%的准确率、89.60%的敏感度和97.72%的特异度。该方法可以作为糖尿病肠病诊断和分级工具，为临床决策和患者护理提供重要的帮助。<details>
<summary>Abstract</summary>
Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accuracy of 89.60%, a sensitivity of 89.60%, and a specificity of 97.72%. The proposed approach serves as a reliable screening and stage grading tool for diabetic retinopathy, offering significant potential to enhance clinical decision-making and patient care.
</details>
<details>
<summary>摘要</summary>
糖尿病 RETINOPATHY 是糖尿病的一种严重的并发症，如果不及时治疗，可能会导致永久失明。早期和准确的诊断是成功治疗的关键。这篇论文介绍了一种深度学习方法，用于检测和评分糖尿病 RETINOPATHY，使用单个背部眼图像。我们的模型利用传输学习，使用两个状态之前的权威模型作为特征提取器，并在新数据集上练习 fine-tuning。我们的模型在APTOS 2019 数据集上获得了优秀的性能，在糖尿病 RETINOPATHY 检测和评分方面超过了现有文献。对于二分类，我们的方法达到了 98.50%的准确率，99.46%的敏感度和 97.51%的特异度。在评分方面，我们的方法达到了 93.00%的QUADRATIC WEIGHTED KAPPA，89.60%的准确率，89.60%的敏感度和 97.72%的特异度。我们的方法可以作为糖尿病 RETINOPATHY 检测和评分工具，为临床决策和患者护理提供了重要的可靠性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion"><a href="#On-the-Robustness-of-Open-World-Test-Time-Training-Self-Training-with-Dynamic-Prototype-Expansion" class="headerlink" title="On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion"></a>On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09942">http://arxiv.org/abs/2308.09942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushu-li/owttt">https://github.com/yushu-li/owttt</a></li>
<li>paper_authors: Yushu Li, Xun Xu, Yongyi Su, Kui Jia</li>
<li>for: 该研究旨在提高unknown target domain distribution下的深度学习模型的泛化性，并且在低延迟下实现。</li>
<li>methods: 该研究使用了test-time training&#x2F;adaptation（TTT&#x2F;TTA）技术，并提出了一种 adaptive strong OOD pruning 技术和动态扩展 prototype 技术来提高 OWTTT 的 Robustness。</li>
<li>results: 该研究在 5 个 OWTTT benchmark 上达到了state-of-the-art 性能，并且提供了一个可用的代码库（<a target="_blank" rel="noopener" href="https://github.com/Yushu-Li/OWTTT%EF%BC%89%E3%80%82">https://github.com/Yushu-Li/OWTTT）。</a><details>
<summary>Abstract</summary>
Generalizing deep learning models to unknown target domain distribution with low latency has motivated research into test-time training/adaptation (TTT/TTA). Existing approaches often focus on improving test-time training performance under well-curated target domain data. As figured out in this work, many state-of-the-art methods fail to maintain the performance when the target domain is contaminated with strong out-of-distribution (OOD) data, a.k.a. open-world test-time training (OWTTT). The failure is mainly due to the inability to distinguish strong OOD samples from regular weak OOD samples. To improve the robustness of OWTTT we first develop an adaptive strong OOD pruning which improves the efficacy of the self-training TTT method. We further propose a way to dynamically expand the prototypes to represent strong OOD samples for an improved weak/strong OOD data separation. Finally, we regularize self-training with distribution alignment and the combination yields the state-of-the-art performance on 5 OWTTT benchmarks. The code is available at https://github.com/Yushu-Li/OWTTT.
</details>
<details>
<summary>摘要</summary>
通用深度学习模型到未知目标频谱分布的泛化，以低延迟实现，已经引起了研究者的关注。现有的方法通常是在已经批处理的目标频谱数据上提高测试时训练性能。然而，在受到强度外部数据杂化（OOD）的情况下，许多状态态态方法表现不佳，这主要是因为无法 отличи出强度OOD样本和软OOD样本。为了改善OWTTT的 Robustness，我们首先开发了适应强度OOD排除，使自我训练TTT方法更加高效。然后，我们提议在运行时动态扩展prototype，以便更好地分离强度OOD和软OOD样本。最后，我们将自我训练与分布AlignmentREG regularization相结合，这种结合得到了5个OWTTT benchmark中的状态态表现。相关代码可以在https://github.com/Yushu-Li/OWTTT中找到。
</details></li>
</ul>
<hr>
<h2 id="Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services"><a href="#Practical-Anomaly-Detection-over-Multivariate-Monitoring-Metrics-for-Online-Services" class="headerlink" title="Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services"></a>Practical Anomaly Detection over Multivariate Monitoring Metrics for Online Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09937">http://arxiv.org/abs/2308.09937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpsPAI/CMAnomaly">https://github.com/OpsPAI/CMAnomaly</a></li>
<li>paper_authors: Jinyang Liu, Tianyi Yang, Zhuangbin Chen, Yuxin Su, Cong Feng, Zengyin Yang, Michael R. Lyu</li>
<li>for: 这篇论文是为了提出一个基于协力机器的多变数监控指标异常探测框架，以提高现有系统监控中的异常探测效能。</li>
<li>methods: 本论文使用了一个名为CMAnomaly的异常探测框架，其中包括一个基于协力机器的机制，可以有效地捕捉多变数监控指标之间的相互作用，并且可以使用cost-effective模型来利用这些相互作用进行异常探测。</li>
<li>results: 根据实验结果，CMAnomaly比基于现有模型的 benchmark 高出6.77%到10.68%，并且可以在10倍至20倍的速度下进行异常探测。此外，本论文还详细介绍了在Huawei Cloud上部署CMAnomaly的经验。<details>
<summary>Abstract</summary>
As modern software systems continue to grow in terms of complexity and volume, anomaly detection on multivariate monitoring metrics, which profile systems' health status, becomes more and more critical and challenging. In particular, the dependency between different metrics and their historical patterns plays a critical role in pursuing prompt and accurate anomaly detection. Existing approaches fall short of industrial needs for being unable to capture such information efficiently. To fill this significant gap, in this paper, we propose CMAnomaly, an anomaly detection framework on multivariate monitoring metrics based on collaborative machine. The proposed collaborative machine is a mechanism to capture the pairwise interactions along with feature and temporal dimensions with linear time complexity. Cost-effective models can then be employed to leverage both the dependency between monitoring metrics and their historical patterns for anomaly detection. The proposed framework is extensively evaluated with both public data and industrial data collected from a large-scale online service system of Huawei Cloud. The experimental results demonstrate that compared with state-of-the-art baseline models, CMAnomaly achieves an average F1 score of 0.9494, outperforming baselines by 6.77% to 10.68%, and runs 10X to 20X faster. Furthermore, we also share our experience of deploying CMAnomaly in Huawei Cloud.
</details>
<details>
<summary>摘要</summary>
现代软件系统的复杂性和规模在不断增长，异常检测在多变量监控指标上变得越来越重要和挑战。特别是依赖于不同指标之间的关系以及历史 patrón 的信息在追踪系统的健康状态非常重要。现有的方法无法fficiently capture这些信息，为此，在本文中，我们提出了 CMAnomaly，一种基于合作机器的异常检测框架。该框架使用了对feature和时间维度进行对称的机制，以linear time complexityCapture pairwise interactions。然后，可以使用cost-effective模型来利用监控指标之间的依赖关系和历史 patrón 进行异常检测。我们对公共数据集和industrial数据集进行了广泛的evaluation，结果表明，相比州标baseline模型，CMAnomaly在F1分数方面获得了0.9494的平均分，高于基eline模型6.77%到10.68%，并且运行速度比基eline模型快10到20倍。此外，我们还分享了在Huawei Cloud中部署 CMAnomaly的经验。
</details></li>
</ul>
<hr>
<h2 id="BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions"><a href="#BLIVA-A-Simple-Multimodal-LLM-for-Better-Handling-of-Text-Rich-Visual-Questions" class="headerlink" title="BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"></a>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09936">http://arxiv.org/abs/2308.09936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlpc-ucsd/bliva">https://github.com/mlpc-ucsd/bliva</a></li>
<li>paper_authors: Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu</li>
<li>for: 该研究旨在解决现实世界中常见的图像涉及文本场景下的视觉问答任务，即使图像具有文本背景。</li>
<li>methods: 该研究使用了一种名为BLIVA的新方法，它是基于InstructBLIP的增强版本，通过直接将图像中的编码补丁直接输入到大语言模型中，以帮助模型更好地捕捉图像中的细节。</li>
<li>results: 实验证明，BLIVA比基elineInstructBLIP有17.76%的提升（在OCR-VQAbenchmark中）和7.9%的提升（在Visual Spatial Reasoning benchmark中），并且在实际世界中的图像处理 tasks中表现出色，无论图像中存在文本或不存在。<details>
<summary>Abstract</summary>
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the model to capture intricate details potentially missed during the query decoding process. Empirical evidence demonstrates that our model, BLIVA, significantly enhances performance in processing text-rich VQA benchmarks (up to 17.76\% in OCR-VQA benchmark) and in undertaking typical VQA benchmarks (up to 7.9\% in Visual Spatial Reasoning benchmark), comparing to our baseline InstructBLIP. BLIVA demonstrates significant capability in decoding real-world images, irrespective of text presence. To demonstrate the broad industry applications enabled by BLIVA, we evaluate the model using a new dataset comprising YouTube thumbnails paired with question-answer sets across 13 diverse categories. For researchers interested in further exploration, our code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.git
</details>
<details>
<summary>摘要</summary>
大型语言模型（VLM），它们将大型语言模型（LLM）与视觉理解能力结合，在开放式视觉问答任务上达到了显著的进步。然而，这些模型无法准确地理解带有文本的图像，这是现实生活中常见的情况。标准的图像信息提取方法通常包括学习固定的查询嵌入。这些嵌入用于在LLM中作为软提示输入，然而这种方法受限于字符串数量，可能导致捕捉场景中的文本背景信息。为了改进它们，本研究提出了BLIVA：一种基于InstructBLIP的增强版，它将InstructBLIP的查询嵌入与LLM直接进行编码覆盖，这种方法源于LLaVA。这种方法帮助模型捕捉文本背景信息中的细节，可能在查询解码过程中被遗弃。实验证明，我们的模型BLIVA在处理文本含量高的VQA标准benchmark（OCRA-VQA标准benchmark）中表现出色，提高了17.76%，以及在典型VQA标准benchmark（Visual Spatial Reasoning标准benchmark）中提高了7.9%。BLIVA在真实世界中处理图像，无论图像中是否存在文本。为了展示BLIVA在广泛的工业应用中的可能性，我们使用了一个新的YouTube封面集合，与问题集合在13种不同的类别中进行了评估。对于关心进一步探索的研究人员，我们在https://github.com/mlpc-ucsd/BLIVA.git中提供了代码和模型。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Quantization-in-TVM"><a href="#Analyzing-Quantization-in-TVM" class="headerlink" title="Analyzing Quantization in TVM"></a>Analyzing Quantization in TVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10905">http://arxiv.org/abs/2308.10905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfei Guo</li>
<li>for: 这个论文主要是研究 TVM 中对weight tensor的量化以提高推理时间和内存占用率，但是8位量化并不符合预期，实际比非量化版本更慢。</li>
<li>methods: 该论文使用了 TVM 进行量化和低位计算，并 investigate 了量化下的性能问题，并评估了不同的优化策略。</li>
<li>results: 该论文通过修复图构建错误和测试多种优化策略，最终实现了对 compute-bound 和 memory-bound 任务的优化，并实现了163.88%和194.98%的推理时间提升。<details>
<summary>Abstract</summary>
There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of tasks: computation-bound and memory-bound, and provide a detailed comparison of various optimization techniques in TVM. Through the identification of performance issues, we have successfully improved quantization by addressing a bug in graph building. Furthermore, we analyze multiple optimization strategies to achieve the optimal quantization result. The best experiment achieves 163.88% improvement compared with the TVM compiled baseline in inference time for the compute-bound task and 194.98% for the memory-bound task.
</details>
<details>
<summary>摘要</summary>
在学术文献中有很多论著关于深度学习模型的量化权重tensor以降低推理延迟和内存占用。TVM也具有量化权重和低位计算的能力。although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of tasks: computation-bound and memory-bound, and provide a detailed comparison of various optimization techniques in TVM. Through the identification of performance issues, we have successfully improved quantization by addressing a bug in graph building. Furthermore, we analyze multiple optimization strategies to achieve the optimal quantization result. The best experiment achieves 163.88% improvement compared with the TVM compiled baseline in inference time for the compute-bound task and 194.98% for the memory-bound task.
</details></li>
</ul>
<hr>
<h2 id="East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference"><a href="#East-Efficient-and-Accurate-Secure-Transformer-Framework-for-Inference" class="headerlink" title="East: Efficient and Accurate Secure Transformer Framework for Inference"></a>East: Efficient and Accurate Secure Transformer Framework for Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09923">http://arxiv.org/abs/2308.09923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanchao Ding, Hua Guo, Yewei Guan, Weixin Liu, Jiarong Huo, Zhenyu Guan, Xiyong Zhang</li>
<li>for: 提高Transformer推理的隐私保护</li>
<li>methods: 提出一个名为”East”的框架，包括新的匿名分割多项式评估算法和特殊防范协议，以提高安全性和准确性</li>
<li>results: 对BERT进行应用，并与不加密推理相比，推理精度保持一致，与Iron相比，通信量下降1.8倍，运行时间下降1.2倍<details>
<summary>Abstract</summary>
Transformer has been successfully used in practical applications, such as ChatGPT, due to its powerful advantages. However, users' input is leaked to the model provider during the service. With people's attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework \emph{East} to enable efficient and accurate secure Transformer inference. Firstly, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5$\times$ and 2.5$\times$, compared to prior arts. Secondly, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain the desired functionality. Thirdly, several optimizations are conducted in detail to enhance the overall efficiency. We applied \emph{East} to BERT and the results show that the inference accuracy remains consistent with the plaintext inference without fine-tuning. Compared to Iron, we achieve about 1.8$\times$ lower communication within 1.2$\times$ lower runtime.
</details>
<details>
<summary>摘要</summary>
“transformer”已经在实际应用中得到了成功，例如chatGPT，因为它具有强大的优势。然而，用户的输入会被提供者 during the service，导致隐私问题。为了保护隐私，隐私保护的transformer推察是非常重要的。但是， Designing practical secure protocols for non-linear functions is hard but significant to model performance。在这个工作中，我们提出了一个名为“East”的框架，以实现有效和精确的隐私保护的transformer推察。首先，我们提出了一个新的隐私检查法，并将其应用到活化函数上，这减少了runtime和通信量，比对实际应用中的优先输出还要好。其次，我们对软max和层normalization的安全协议进行了谨慎的设计，以保持所需的功能。最后，我们在细节上进行了详细的优化，以提高整体的效率。我们将“East”应用到BERT，结果显示，在不进行微调的情况下，推察精度与普通的推察相同。与Iron相比，我们的通信量为1.8倍，并且runtime为1.2倍。
</details></li>
</ul>
<hr>
<h2 id="EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning"><a href="#EGANS-Evolutionary-Generative-Adversarial-Network-Search-for-Zero-Shot-Learning" class="headerlink" title="EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning"></a>EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09915">http://arxiv.org/abs/2308.09915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiming Chen, Shihuang Chen, Wenjin Hou, Weiping Ding, Xinge You</li>
<li>for: 提高零shot学习（ZSL）中的类别识别率，使用生成模型（如生成对抗网络（GAN））synthesize视觉样本，以提高ZSL的性能。</li>
<li>methods: 提出了一种自然选择的生成器网络搜索方法（EGANS），通过协同对抗进行 neural architecture search，以获得适应性和稳定性好的生成器和批判器。</li>
<li>results: 在标准CUB、SUN、AWA2和FLO数据集上，EGANS consistently提高了现有的生成ZSL方法的性能，表明了生成ZSL中的进化性搜索在ZSL中的潜在应用。<details>
<summary>Abstract</summary>
Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models (e.g., generative adversarial network (GAN)) are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary generative adversarial network search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: evolution generator architecture search and evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary neural architecture search explores a virgin field in ZSL.
</details>
<details>
<summary>摘要</summary>
EGANS采用了合作双向进化来进行神经网络搜索，包括生成器和分类器。在生成器搜索阶段，我们采用了多对一的对抗训练策略，通过进化搜索来找到最佳的生成器。然后，我们使用类似的进化搜索算法来搜索最佳的分类器。一旦找到了最佳的生成器和分类器，我们就将它们与不同的生成ZSL基线方法结合，以进行ZSL分类。我们的实验表明，EGANS可以在标准的CUB、SUN、AWA2和FLO数据集上提供remarkable的性能提升，这表明了进化神经网络搜索在ZSL中的可能性。
</details></li>
</ul>
<hr>
<h2 id="Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning"><a href="#Never-Explore-Repeatedly-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Never Explore Repeatedly in Multi-Agent Reinforcement Learning"></a>Never Explore Repeatedly in Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09909">http://arxiv.org/abs/2308.09909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Li, Tonghan Wang, Chongjie Zhang, Qianchuan Zhao</li>
<li>for: 本研究旨在提高多智能体强化学习中的探索性能，通过采用内在动机。</li>
<li>methods: 本文提出了动态奖励扩大方法，通过稳定既已探索的区域的奖励，促进更广泛的探索。</li>
<li>results: 实验结果表明，本方法可以在Google研究足球和StarCraft II微管理任务中提高表现，特别在罕见奖励 Setting下。<details>
<summary>Abstract</summary>
In the realm of multi-agent reinforcement learning, intrinsic motivations have emerged as a pivotal tool for exploration. While the computation of many intrinsic rewards relies on estimating variational posteriors using neural network approximators, a notable challenge has surfaced due to the limited expressive capability of these neural statistics approximators. We pinpoint this challenge as the "revisitation" issue, where agents recurrently explore confined areas of the task space. To combat this, we propose a dynamic reward scaling approach. This method is crafted to stabilize the significant fluctuations in intrinsic rewards in previously explored areas and promote broader exploration, effectively curbing the revisitation phenomenon. Our experimental findings underscore the efficacy of our approach, showcasing enhanced performance in demanding environments like Google Research Football and StarCraft II micromanagement tasks, especially in sparse reward settings.
</details>
<details>
<summary>摘要</summary>
在多智能奖励学习领域，内生动机被视为探索的重要工具。然而，计算许多内生奖励的 neural network 近似器表现有限，导致“再次探索”问题的出现，即代理人重复探索任务空间中的封闭区域。为解决此问题，我们提出了动态奖励缩放方法。这种方法通过稳定前期探索区域中的奖励变化，激励代理人更广泛探索，从而缓解再次探索现象。我们的实验结果表明，我们的方法在 Google Research Football 和 StarCraft II 微管理任务中表现出色，尤其在罕见奖励设置下。
</details></li>
</ul>
<hr>
<h2 id="Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks"><a href="#Imputing-Brain-Measurements-Across-Data-Sets-via-Graph-Neural-Networks" class="headerlink" title="Imputing Brain Measurements Across Data Sets via Graph Neural Networks"></a>Imputing Brain Measurements Across Data Sets via Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09907">http://arxiv.org/abs/2308.09907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Wang, Wei Peng, Susan F. Tapert, Qingyu Zhao, Kilian M. Pohl</li>
<li>for: The paper aims to address the issue of missing measurements in publicly available structural MRI data sets, specifically the curvature scores computed by Freesurfer, by proposing a deep learning-based imputation method called Demographic Aware Graph-based Imputation (DAGI).</li>
<li>methods: The DAGI method uses a graph neural network (GNN) to model the dependencies between brain Region of Interests (ROIs) and accounts for demographic differences in brain measurements by feeding the graph encoding into a parallel architecture that simultaneously optimizes a graph decoder to impute values and a classifier to predict demographic factors.</li>
<li>results: The proposed DAGI method is tested on imputing missing Freesurfer measurements of the Adolescent Brain Cognitive Development (ABCD) Study data set (N&#x3D;3760) by training the predictor on publicly released data from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA, N&#x3D;540).<details>
<summary>Abstract</summary>
Publicly available data sets of structural MRIs might not contain specific measurements of brain Regions of Interests (ROIs) that are important for training machine learning models. For example, the curvature scores computed by Freesurfer are not released by the Adolescent Brain Cognitive Development (ABCD) Study. One can address this issue by simply reapplying Freesurfer to the data set. However, this approach is generally computationally and labor intensive (e.g., requiring quality control). An alternative is to impute the missing measurements via a deep learning approach. However, the state-of-the-art is designed to estimate randomly missing values rather than entire measurements. We therefore propose to re-frame the imputation problem as a prediction task on another (public) data set that contains the missing measurements and shares some ROI measurements with the data sets of interest. A deep learning model is then trained to predict the missing measurements from the shared ones and afterwards is applied to the other data sets. Our proposed algorithm models the dependencies between ROI measurements via a graph neural network (GNN) and accounts for demographic differences in brain measurements (e.g. sex) by feeding the graph encoding into a parallel architecture. The architecture simultaneously optimizes a graph decoder to impute values and a classifier in predicting demographic factors. We test the approach, called Demographic Aware Graph-based Imputation (DAGI), on imputing those missing Freesurfer measurements of ABCD (N=3760) by training the predictor on those publicly released by the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA, N=540)...
</details>
<details>
<summary>摘要</summary>
公共可用数据集的结构MRI数据可能不包含特定的脑区域关注点（ROIs）的准确量测量。例如，ABCDFreesurfer的曲线分数不由ABCDFreesurfer发布。一种解决方法是简单地重新应用Freesurfer到数据集中。然而，这种方法通常是计算机和劳动力密集的（例如，需要质量控制）。另一种方法是使用深度学习方法进行填充。然而，现状的深度学习方法是随机缺失值的估计而不是整个测量。我们因此提议将填充问题重新定义为一个预测任务，使用另一个（公共）数据集，该数据集包含缺失的测量和与数据集集中的ROIs进行共享。然后，我们将深度学习模型训练以预测缺失测量，并在训练过程中考虑ROIs之间的依赖关系。我们称这种方法为“规格意识 Graph-based 填充”（DAGI）。我们在NCANDA（N=540）公共发布的数据集上训练了预测器，并在ABCDFreesurfer（N=3760）中进行了填充。
</details></li>
</ul>
<hr>
<h2 id="DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#DPMAC-Differentially-Private-Communication-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning"></a>DPMAC: Differentially Private Communication for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09902">http://arxiv.org/abs/2308.09902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CANVOLCANO/DPMAC">https://github.com/CANVOLCANO/DPMAC</a></li>
<li>paper_authors: Canzhe Zhao, Yanjie Ze, Jing Dong, Baoxiang Wang, Shuai Li</li>
<li>for: 防止多代理人学习中泄露敏感信息</li>
<li>methods: 使用（ε，δ）敏感数据隐私保证，采用Stochastic MessageSender，自动调整学习后的消息分布</li>
<li>results: 实验显示DPMAC在隐私保护场景下比基eline方法有明显优势<details>
<summary>Abstract</summary>
Communication lays the foundation for cooperation in human society and in multi-agent reinforcement learning (MARL). Humans also desire to maintain their privacy when communicating with others, yet such privacy concern has not been considered in existing works in MARL. To this end, we propose the \textit{differentially private multi-agent communication} (DPMAC) algorithm, which protects the sensitive information of individual agents by equipping each agent with a local message sender with rigorous $(\epsilon, \delta)$-differential privacy (DP) guarantee. In contrast to directly perturbing the messages with predefined DP noise as commonly done in privacy-preserving scenarios, we adopt a stochastic message sender for each agent respectively and incorporate the DP requirement into the sender, which automatically adjusts the learned message distribution to alleviate the instability caused by DP noise. Further, we prove the existence of a Nash equilibrium in cooperative MARL with privacy-preserving communication, which suggests that this problem is game-theoretically learnable. Extensive experiments demonstrate a clear advantage of DPMAC over baseline methods in privacy-preserving scenarios.
</details>
<details>
<summary>摘要</summary>
交流为社会和多代理学习（MARL）中的合作基础。人们也希望在交流时保持隐私，但这一点尚未被现有的MARL工作考虑。为此，我们提出了《差分隐私多代理通信算法》（DPMAC），该算法保护每个代理的敏感信息，并在每个代理的本地消息发送器中实现了严格（ε，δ）差分隐私（DP）保证。与直接将消息裁剪为常见的隐私保护方法一样，我们采用了每个代理的随机消息发送器，并将DP要求直接 incorporated into the sender，自动调整学习的消息分布，以解决由DP噪声引起的不稳定性。此外，我们证明了在隐私保护通信下的合作MARL问题是游戏理论上学习可能的。广泛的实验表明DPMAC在隐私保护场景下具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs"><a href="#Contrastive-Learning-based-Imputation-Prediction-Networks-for-In-hospital-Mortality-Risk-Modeling-using-EHRs" class="headerlink" title="Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs"></a>Contrastive Learning-based Imputation-Prediction Networks for In-hospital Mortality Risk Modeling using EHRs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09896">http://arxiv.org/abs/2308.09896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liulab1356/CL-ImpPreNet">https://github.com/liulab1356/CL-ImpPreNet</a></li>
<li>paper_authors: Yuxi Liu, Zhenhao Zhang, Shaowen Qin, Flora D. Salim, Antonio Jimeno Yepes</li>
<li>for: 预测医院内死亡风险从电子医疗记录（EHRs）获得了广泛关注，以提供早期警示患者健康状况，促使医疗专业人员采取有效措施。</li>
<li>methods: 本研究提出了一种基于对比学习的抽象替换网络，用于预测医院内死亡风险。我们的方法引入图分析基于患者划分模型，以组合类似患者的信息。此外，我们的方法还可以将对比学习集成到提议的网络架构中，以增强患者表征学习和预测性能。</li>
<li>results: 在两个真实的EHR数据集上，我们的方法比州分之前的方法在替换和预测任务中都表现出优异。<details>
<summary>Abstract</summary>
Predicting the risk of in-hospital mortality from electronic health records (EHRs) has received considerable attention. Such predictions will provide early warning of a patient's health condition to healthcare professionals so that timely interventions can be taken. This prediction task is challenging since EHR data are intrinsically irregular, with not only many missing values but also varying time intervals between medical records. Existing approaches focus on exploiting the variable correlations in patient medical records to impute missing values and establishing time-decay mechanisms to deal with such irregularity. This paper presents a novel contrastive learning-based imputation-prediction network for predicting in-hospital mortality risks using EHR data. Our approach introduces graph analysis-based patient stratification modeling in the imputation process to group similar patients. This allows information of similar patients only to be used, in addition to personal contextual information, for missing value imputation. Moreover, our approach can integrate contrastive learning into the proposed network architecture to enhance patient representation learning and predictive performance on the classification task. Experiments on two real-world EHR datasets show that our approach outperforms the state-of-the-art approaches in both imputation and prediction tasks.
</details>
<details>
<summary>摘要</summary>
预测医院内 morteo风险从电子医疗记录（EHRs）得到了广泛的关注。这将为医疗专业人员提供早期警示patient的健康状况，以便在时间上采取有效的 intervención. 这个预测任务是挑战的，因为EHR数据本身是不规则的，有许多缺失值以及不同的时间间隔 between medical records. 现有的方法是利用patient的医疗记录中的变量相关性来填充缺失值，并设置时间衰减机制来处理这种不规则性. 本文提出了一种基于对比学习的投入-预测网络，用于预测医院内 morteo风险。我们的方法引入图分析基于patient的分类模型，以分组类似的patient。这样只有类似patient的信息被用于缺失值填充，同时还能够使用个体特定的上下文信息。此外，我们的方法还可以将对比学习integrated into the proposed network architecture，以提高patient表示学习和预测性能。实验表明，我们的方法在两个实际的EHR数据集上比状态之前的方法在投入和预测任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs"><a href="#Knowledge-Transfer-from-High-Resource-to-Low-Resource-Programming-Languages-for-Code-LLMs" class="headerlink" title="Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs"></a>Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09895">http://arxiv.org/abs/2308.09895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, Arjun Guha</li>
<li>for: 提高低资源语言中Code LLM的性能</li>
<li>methods: 使用半人工数据生成高质量数据集，并在这些数据集上练习和评估Code LLM</li>
<li>results: 使用 MultiPL-T 方法生成了大量有效的训练数据，并在 benchmark 问题上达到了state-of-the-art 性能水平，包括 Lua、Racket 和 OCaml。<details>
<summary>Abstract</summary>
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.   This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate tens of thousands of new, validated training items for Racket, OCaml, and Lua from Python. Moreover, we use an open dataset (The Stack) and model (StarCoderBase), which allow us to decontaminate benchmarks and train models on this data without violating the model license.   With MultiPL-T generated data, we present fine-tuned versions of StarCoderBase that achieve state-of-the-art performance for Racket, OCaml, and Lua on benchmark problems. For Lua, our fine-tuned model achieves the same performance as StarCoderBase as Python -- a very high-resource language -- on the MultiPL-E benchmarks. For Racket and OCaml, we double their performance on MultiPL-E, bringing their performance close to higher-resource languages such as Ruby and C#.
</details>
<details>
<summary>摘要</summary>
在过去几年，大型代码模型（Code LLMs）已经开始对编程实践产生重要影响。 Code LLMs 也在编程语言和软件工程研究中出现为基础建筑块。然而，由 Code LLMs 生成的代码质量各不相同，尤其是低资源语言。 Code LLMs 在具有充足训练数据的语言（如 Java、Python 或 JavaScript）上表现出色，但在低资源语言（如 OCaml 和 Racket）上困难。本文提出了一种有效的方法，可以提高 Code LLMs 在低资源语言上表现的方法。我们的方法通过生成高质量的低资源语言数据，然后使用这些数据来练化任何预训练 Code LLM。我们的方法被称为 MultiPL-T，它将高资源语言的训练数据翻译成低资源语言的训练数据。我们使用这种方法生成了数以千计的新的有效训练项目，用于 Racket、OCaml 和 Lua。此外，我们使用公开的数据集（The Stack）和模型（StarCoderBase），可以在这些数据上训练模型，而不违反模型的许可证。使用 MultiPL-T 生成的数据，我们提出了一些精心调整的 StarCoderBase 模型，在benchmark问题上达到了状态之 искусственный智能的性能。对 Lua 来说，我们的调整模型和 Python 的 StarCoderBase 模型在 MultiPL-E benchmark上具有同等的性能。对 Racket 和 OCaml 来说，我们的调整模型可以提高它们的性能，使其接近高资源语言如 Ruby 和 C#。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection"><a href="#Utilizing-Semantic-Textual-Similarity-for-Clinical-Survey-Data-Feature-Selection" class="headerlink" title="Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection"></a>Utilizing Semantic Textual Similarity for Clinical Survey Data Feature Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09892">http://arxiv.org/abs/2308.09892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bcwarner/sts-select">https://github.com/bcwarner/sts-select</a></li>
<li>paper_authors: Benjamin C. Warner, Ziqi Xu, Simon Haroutounian, Thomas Kannampallil, Chenyang Lu</li>
<li>for: 这篇论文是为了提出一种基于文本名称的对Target outcome的Semantic textual similarity（STS）分析方法，以便从 Survey data 中选择最佳的特征集。</li>
<li>methods: 本论文使用了Language models（LMs）来评估特征名称和Target名称之间的semantic textual similarity（STS），并将其用于特征选择。</li>
<li>results: 结果显示，使用STS进行特征选择可以实现更高的模型性能，相比传统的特征选择算法。<details>
<summary>Abstract</summary>
Survey data can contain a high number of features while having a comparatively low quantity of examples. Machine learning models that attempt to predict outcomes from survey data under these conditions can overfit and result in poor generalizability. One remedy to this issue is feature selection, which attempts to select an optimal subset of features to learn upon. A relatively unexplored source of information in the feature selection process is the usage of textual names of features, which may be semantically indicative of which features are relevant to a target outcome. The relationships between feature names and target names can be evaluated using language models (LMs) to produce semantic textual similarity (STS) scores, which can then be used to select features. We examine the performance using STS to select features directly and in the minimal-redundancy-maximal-relevance (mRMR) algorithm. The performance of STS as a feature selection metric is evaluated against preliminary survey data collected as a part of a clinical study on persistent post-surgical pain (PPSP). The results suggest that features selected with STS can result in higher performance models compared to traditional feature selection algorithms.
</details>
<details>
<summary>摘要</summary>
Survey data 可能包含大量的特征，而具有较少的示例数量。机器学习模型在这种情况下预测结果时可能会过拟合，导致泛化性差。一种解决方案是Feature选择，它尝试选择最佳的特征子集来学习。文本特征名称的使用仍然是一个相对未探索的信息来源。我们可以使用语言模型（LM）评估特征名称和目标名称之间的语义文本相似性（STS） scores，以便选择特征。我们对直接使用 STS 选择特征和 minimal-redundancy-maximal-relevance（mRMR）算法进行了评估。我们发现，使用 STS 作为特征选择度量时，可以获得比传统特征选择算法更高的性能。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model"><a href="#Inductive-bias-Learning-Generating-Code-Models-with-Large-Language-Model" class="headerlink" title="Inductive-bias Learning: Generating Code Models with Large Language Model"></a>Inductive-bias Learning: Generating Code Models with Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09890">http://arxiv.org/abs/2308.09890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fuyu-quant/iblm">https://github.com/fuyu-quant/iblm</a></li>
<li>paper_authors: Toma Tanaka, Naofumi Emoto, Tsukasa Yumibayashi</li>
<li>for: 本研究旨在提出一种新的学习方法，即归纳学习（Inductive-Bias Learning，IBL），这种方法结合了归纳学习（ICL）和代码生成技术，以实现高精度的推理和代码生成。</li>
<li>methods: 本研究使用的方法包括归纳学习（ICL）和代码生成技术。通过输入训练数据，IBL可以从Contextual Understanding中找到必需的结构，并生成相应的代码模型，以实现高精度的推理和代码生成。</li>
<li>results: 研究发现，IBL可以实现与ICL和代表性机器学习模型相当的预测精度，并且代码生成结果具有较高的可读性和解释性。此外，IBL代码也是开源的，可以在<a target="_blank" rel="noopener" href="https://github.com/fuyu-quant/IBLM%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/fuyu-quant/IBLM上下载。</a><details>
<summary>Abstract</summary>
Large Language Models(LLMs) have been attracting attention due to a ability called in-context learning(ICL). ICL, without updating the parameters of a LLM, it is possible to achieve highly accurate inference based on rules ``in the context'' by merely inputting a training data into the prompt. Although ICL is a developing field with many unanswered questions, LLMs themselves serves as a inference model, seemingly realizing inference without explicitly indicate ``inductive bias''. On the other hand, a code generation is also a highlighted application of LLMs. The accuracy of code generation has dramatically improved, enabling even non-engineers to generate code to perform the desired tasks by crafting appropriate prompts. In this paper, we propose a novel ``learning'' method called an ``Inductive-Bias Learning (IBL)'', which combines the techniques of ICL and code generation. An idea of IBL is straightforward. Like ICL, IBL inputs a training data into the prompt and outputs a code with a necessary structure for inference (we referred to as ``Code Model'') from a ``contextual understanding''. Despite being a seemingly simple approach, IBL encompasses both a ``property of inference without explicit inductive bias'' inherent in ICL and a ``readability and explainability'' of the code generation. Surprisingly, generated Code Models have been found to achieve predictive accuracy comparable to, and in some cases surpassing, ICL and representative machine learning models. Our IBL code is open source: https://github.com/fuyu-quant/IBLM
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在近年来吸引了很多注意，主要是因为它们的一个能力 called "in-context learning"（ICL）。ICL是不需要更新 LLM 的参数时，可以通过输入训练数据来 achieve highly accurate inference based on "rules in the context"。although ICL is a developing field with many unanswered questions, LLMs themselves serve as an inference model, seemingly realizing inference without explicitly indicating "inductive bias"。另一方面，代码生成也是 LLM 的优点之一。代码生成的精度已经得到了很大改善，使得甚至非工程师也可以通过设计适当的 prompt 来生成代码以完成想要的任务。在这篇文章中，我们提出了一个新的“学习”方法called“对应式学习”（IBL），它结合了 ICL 和代码生成的技术。IBL 的想法是 straightforward。如 ICLL，IBL 通过输入训练数据来生成一个代码模型（我们称之为“内在结构”），并将其与内在的概念“对应”。尽管看起来很简单，但 IBL 包含了 ICL 的“无预先假设”和代码生成的“可读性和解释性”。惊奇的是，生成的代码模型已经被发现可以 achieve predictive accuracy comparable to, and in some cases surpassing, ICL 和代表性机器学习模型。我们的 IBL 代码可以在 GitHub 上找到：https://github.com/fuyu-quant/IBLM。
</details></li>
</ul>
<hr>
<h2 id="DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization"><a href="#DUAW-Data-free-Universal-Adversarial-Watermark-against-Stable-Diffusion-Customization" class="headerlink" title="DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization"></a>DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09889">http://arxiv.org/abs/2308.09889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Ye, Hao Huang, Jiaqi An, Yongtao Wang</li>
<li>for: 保护版权图像从多种自定义方法中，防止抄袭特定风格或主题。</li>
<li>methods: 使用不可见的数据自由universal adversarial watermark（DUAW），在不直接处理版权图像的情况下，在多个版本的SD模型中保护各种版权图像。</li>
<li>results: DUAW可以有效地扰乱自定义SD模型生成的图像，使其可见 both human observers和一个简单的分类器。<details>
<summary>Abstract</summary>
Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted images, serving as a protective measure by inducing significant distortions in the images generated by customized SD models. Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.
</details>
<details>
<summary>摘要</summary>
首先，DUAW是在SD自定义过程中扰乱变量自动编码器的设计。其次，DUAW在无数据上下文中操作，通过使用一个大型自然语言模型（LLM）和预训练的SD模型生成的 sintetic图像进行训练。这种方法可以避免直接处理版权图像，保持其 конфиденциальность。一旦制作完毕，DUAW可以隐身地 integrate到大量版权图像中，作为保护性的水印，对于自定义SD模型生成的图像进行显著的扭曲。实验结果表明，DUAW可以有效地扭曲调整后的SD模型输出，使其可见 both human observer和一个简单的分类器。
</details></li>
</ul>
<hr>
<h2 id="On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design"><a href="#On-Estimating-the-Gradient-of-the-Expected-Information-Gain-in-Bayesian-Experimental-Design" class="headerlink" title="On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design"></a>On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09888">http://arxiv.org/abs/2308.09888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ziq-ao/GradEIG">https://github.com/ziq-ao/GradEIG</a></li>
<li>paper_authors: Ziqiao Ao, Jinglai Li</li>
<li>for: 本研究旨在提高 bayesian 推理中的实验条件，通过优化预期信息增强 (EIG) 的优化。</li>
<li>methods: 本研究提出了两种估算 EIG 的梯度方法：UEEG-MCMC 和 BEEG-AP。UEEG-MCMC 利用 MCMC 生成的 posterior 样本来估算 EIG 梯度，而 BEEG-AP 则强调高效的 simulation 实现，通过重复使用参数样本来估算 EIG 梯度。</li>
<li>results: 理论分析和数值实验表明，UEEG-MCMC 对实际 EIG 值具有良好的稳定性，而 BEEG-AP 在 EIG 值小时显示更高的效率。此外，两种方法在数值实验中都表现出优于多个流行的参考方法。<details>
<summary>Abstract</summary>
Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerical studies illustrate that UEEG-MCMC is robust agains the actual EIG value, while BEEG-AP is more efficient when the EIG value to be optimized is small. Moreover, both methods show superior performance compared to several popular benchmarks in our numerical experiments.
</details>
<details>
<summary>摘要</summary>
bayesian experimental design (BED), which aims to find the optimal experimental conditions for bayesian inference, 通常是要优化预期信息增强(EIG)的目标。为了提高效率，通常需要计算EIG的梯度信息，因此能够估计EIG梯度的能力是BED问题的关键。本工作的主要目标是开发估计EIG梯度的方法，这些方法可以与随机梯度下降算法结合使用，从而实现EIG的高效优化。我们首先引入了 posterior expected representation of EIG gradient with respect to design variables。然后，我们提出了两种估计EIG梯度的方法：UEEG-MCMC，利用MCMC生成的 posterior samples来估计EIG梯度，和BEEG-AP，重点是实现高效的 simulations by repeatedly using parameter samples。我们的理论分析和数学实验表明，UEEG-MCMC 对实际EIG值有良好的稳定性，而BEEG-AP 在EIG值要 optimize 小时更高效。此外，两种方法在我们的数学实验中表现出了较好的性能，比如几种流行的参考方法。
</details></li>
</ul>
<hr>
<h2 id="Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting"><a href="#Calibrating-Uncertainty-for-Semi-Supervised-Crowd-Counting" class="headerlink" title="Calibrating Uncertainty for Semi-Supervised Crowd Counting"></a>Calibrating Uncertainty for Semi-Supervised Crowd Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09887">http://arxiv.org/abs/2308.09887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Li, Xiaoling Hu, Shahira Abousamra, Chao Chen</li>
<li>for: 这个研究的目的是提出一种可靠地测量人群数量的方法，并且能够在半指导式的情况下进行数据训练。</li>
<li>methods: 本研究使用了一种基于模型uncertainty的方法，通过训练一个surrogate函数来控制模型的uncertainty。另外，我们还提出了一个基于匹配的patch-wise surrogate函数，以更好地近似uncertainty的数据。</li>
<li>results: 根据实验结果显示，我们的方法能够生成可靠的uncertainty估计、高质量的pseudolabels，并且在半指导式的人群数量推断 task 上实现了state-of-the-art的表现。<details>
<summary>Abstract</summary>
Semi-supervised crowd counting is an important yet challenging task. A popular approach is to iteratively generate pseudo-labels for unlabeled data and add them to the training set. The key is to use uncertainty to select reliable pseudo-labels. In this paper, we propose a novel method to calibrate model uncertainty for crowd counting. Our method takes a supervised uncertainty estimation strategy to train the model through a surrogate function. This ensures the uncertainty is well controlled throughout the training. We propose a matching-based patch-wise surrogate function to better approximate uncertainty for crowd counting tasks. The proposed method pays a sufficient amount of attention to details, while maintaining a proper granularity. Altogether our method is able to generate reliable uncertainty estimation, high quality pseudolabels, and achieve state-of-the-art performance in semisupervised crowd counting.
</details>
<details>
<summary>摘要</summary>
semi-supervised crowd counting是一项重要又挑战性的任务。一种流行的方法是通过逐步生成 pseudo-labels 来训练无标注数据。关键在于使用uncertainty来选择可靠的 pseudo-labels。在这篇论文中，我们提出了一种新的方法来准确地控制模型的uncertainty。我们使用一种监督性uncertainty估计策略来训练模型，并通过一个匹配基于的 patch-wise 代理函数来更好地估计uncertainty。我们的方法具有充分的细节注意力，同时保持合适的粒度。总之，我们的方法能够生成可靠的uncertainty估计、高质量的 pseudo-labels，并在semi-supervised crowd counting中实现状态前的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case"><a href="#A-Transformer-based-Framework-For-Multi-variate-Time-Series-A-Remaining-Useful-Life-Prediction-Use-Case" class="headerlink" title="A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case"></a>A Transformer-based Framework For Multi-variate Time Series: A Remaining Useful Life Prediction Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09884">http://arxiv.org/abs/2308.09884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oluwaseyi Ogunfowora, Homayoun Najjaran</li>
<li>for: 这个研究是为了提出一个基于encoder-transformer架构的多变量时间序列预测框架，用于预测机器的剩下有用生命时间（RUL）。</li>
<li>methods: 本研究使用了encoder-transformer模型，并进行了三个模型对应实验，以将自然语言领域中的transformer模型应用到时间序列预测中。此外，本研究还提出了一个新的扩展窗口方法，用于让模型耳熟悉机器的初期阶段和衰老路径。</li>
<li>results: 根据试验数据表现，提出的encoder-transformer模型在13个state-of-the-art（SOTA）模型中得到了平均提高137.65%的性能。<details>
<summary>Abstract</summary>
In recent times, Large Language Models (LLMs) have captured a global spotlight and revolutionized the field of Natural Language Processing. One of the factors attributed to the effectiveness of LLMs is the model architecture used for training, transformers. Transformer models excel at capturing contextual features in sequential data since time series data are sequential, transformer models can be leveraged for more efficient time series data prediction. The field of prognostics is vital to system health management and proper maintenance planning. A reliable estimation of the remaining useful life (RUL) of machines holds the potential for substantial cost savings. This includes avoiding abrupt machine failures, maximizing equipment usage, and serving as a decision support system (DSS). This work proposed an encoder-transformer architecture-based framework for multivariate time series prediction for a prognostics use case. We validated the effectiveness of the proposed framework on all four sets of the C-MAPPS benchmark dataset for the remaining useful life prediction task. To effectively transfer the knowledge and application of transformers from the natural language domain to time series, three model-specific experiments were conducted. Also, to enable the model awareness of the initial stages of the machine life and its degradation path, a novel expanding window method was proposed for the first time in this work, it was compared with the sliding window method, and it led to a large improvement in the performance of the encoder transformer model. Finally, the performance of the proposed encoder-transformer model was evaluated on the test dataset and compared with the results from 13 other state-of-the-art (SOTA) models in the literature and it outperformed them all with an average performance increase of 137.65% over the next best model across all the datasets.
</details>
<details>
<summary>摘要</summary>
This work proposes an encoder-transformer architecture-based framework for multivariate time series prediction in a prognostics use case. We validated the effectiveness of the proposed framework on all four datasets of the C-MAPPS benchmark for RUL prediction. To transfer knowledge and application of transformers from the natural language domain to time series, we conducted three model-specific experiments. Additionally, we proposed a novel expanding window method to enhance the model's awareness of the initial stages of machine life and its degradation path. This method was compared with the sliding window method and led to a large improvement in the performance of the encoder transformer model.The proposed encoder-transformer model was evaluated on the test dataset and outperformed 13 other state-of-the-art (SOTA) models in the literature, with an average performance increase of 137.65% over the next best model across all datasets.
</details></li>
</ul>
<hr>
<h2 id="Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning"><a href="#Flamingo-Multi-Round-Single-Server-Secure-Aggregation-with-Applications-to-Private-Federated-Learning" class="headerlink" title="Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning"></a>Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09883">http://arxiv.org/abs/2308.09883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eniac/flamingo">https://github.com/eniac/flamingo</a></li>
<li>paper_authors: Yiping Ma, Jess Woods, Sebastian Angel, Antigoni Polychroniadou, Tal Rabin</li>
<li>for: 这篇论文描述了一种用于安全聚合 Federated Learning 中的数据的系统，即 Flamingo。</li>
<li>methods: Flamingo 使用了一种新的轻量级随机 dropout 抗性协议，以及一种新的客户端邻居选择方法，以确保在客户端离开中阶段的服务器仍然可以获得有意义的结果。</li>
<li>results: Flamingo 可以安全地训练基于 (Extended) MNIST 和 CIFAR-100 数据集的神经网络，并且模型可以在不失去精度的情况下 converge。相比之下，非私有 Federated Learning 系统可以带来更快的结果。<details>
<summary>Abstract</summary>
This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS '20), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al. These techniques help Flamingo reduce the number of interactions between clients and the server, resulting in a significant reduction in the end-to-end runtime for a full training session over prior work. We implement and evaluate Flamingo and show that it can securely train a neural network on the (Extended) MNIST and CIFAR-100 datasets, and the model converges without a loss in accuracy, compared to a non-private federated learning system.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了Flamingo系统，用于在大量客户端上安全地汇集数据。在安全汇集中，服务器将客户端的私有输入汇集到最终结果中，而不会了解每个输入的详细信息。Flamingo针对了联合学习中的多轮设定，在多个汇集（均值）中得到一个好的模型。先前的协议，如Bell et al. (CCS '20)，是为单轮设定而设计，并通过重复协议来适应联合学习设定。Flamingo消除了先前协议的每轮设定需求，并提供了一个轻量级的dropout鲁棒性协议，以确保如果客户端在汇集过程中离开，服务器仍可以获得有意义的结果。此外，Flamingo还引入了一种新的客户端 neighboorhood的选择方法，这些方法帮助Flamingo减少客户端和服务器之间的交互次数，从而导致了对于整个训练会话的结束到终端时间的重要减少。我们实现和评估了Flamingo，并证明它可以安全地训练一个神经网络在（扩展）MNIST和CIFAR-100数据集上，并且模型可以无损地训练完成，相比于非私钥联合学习系统。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-Unlearning"><a href="#Generative-Adversarial-Networks-Unlearning" class="headerlink" title="Generative Adversarial Networks Unlearning"></a>Generative Adversarial Networks Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09881">http://arxiv.org/abs/2308.09881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Sun, Tianqing Zhu, Wenhan Chang, Wanlei Zhou</li>
<li>for: 本研究旨在解决Generative Adversarial Networks (GANs)中的机器学习模型卸载数据的问题，即生成器和判别器的架构特点使得卸载过程可能会导致 latent space 的破坏和模型效果的降低。</li>
<li>methods: 本研究提出了一种替换机制和假标签来有效地解决 generator 卸载和判别器的挑战。基于这种替换机制和假标签，我们提出了一种层次式卸载方法，其中卸载和学习过程Running in a cascaded manner。</li>
<li>results: 我们在 MNIST 和 CIFAR-10 数据集上进行了全面的评估，结果表明，这种层次式卸载方法可以大幅提高项和类卸载效率，比 retraining from scratch 减少时间量达 185x 和 284x。此外，我们发现，即使模型性能受到一定影响，这些影响几乎可以忽略不计（如 64 个图像），并无负面影响下游任务 such as classification。<details>
<summary>Abstract</summary>
As machine learning continues to develop, and data misuse scandals become more prevalent, individuals are becoming increasingly concerned about their personal information and are advocating for the right to remove their data. Machine unlearning has emerged as a solution to erase training data from trained machine learning models. Despite its success in classifiers, research on Generative Adversarial Networks (GANs) is limited due to their unique architecture, including a generator and a discriminator. One challenge pertains to generator unlearning, as the process could potentially disrupt the continuity and completeness of the latent space. This disruption might consequently diminish the model's effectiveness after unlearning. Another challenge is how to define a criterion that the discriminator should perform for the unlearning images. In this paper, we introduce a substitution mechanism and define a fake label to effectively mitigate these challenges. Based on the substitution mechanism and fake label, we propose a cascaded unlearning approach for both item and class unlearning within GAN models, in which the unlearning and learning processes run in a cascaded manner. We conducted a comprehensive evaluation of the cascaded unlearning technique using the MNIST and CIFAR-10 datasets. Experimental results demonstrate that this approach achieves significantly improved item and class unlearning efficiency, reducing the required time by up to 185x and 284x for the MNIST and CIFAR-10 datasets, respectively, in comparison to retraining from scratch. Notably, although the model's performance experiences minor degradation after unlearning, this reduction is negligible when dealing with a minimal number of images (e.g., 64) and has no adverse effects on downstream tasks such as classification.
</details>
<details>
<summary>摘要</summary>
machine learning技术不断发展，同时数据滥用事件的发生也使人们对个人信息变得越来越重要，因此开始提出个人数据权的要求。为了解除训练机器学习模型的数据，机器学习卸学（Machine Unlearning）已成为一种解决方案。然而，对于生成型 adversarial network（GAN）来说，研究尚未充分发展，其特殊的架构包括生成器和分类器，带来了一些挑战。其中一个挑战是生成器卸学，因为这个过程可能会破坏生成器的维度完整性，从而降低模型的效果。另一个挑战是如何定义分类器对卸学图像的标准。在这篇论文中，我们提出了替换机制和假标签，以解决这些挑战。基于替换机制和假标签，我们提出了卸学和学习的层次结构，在这种结构下，卸学和学习过程会在一起运行。我们对MNIST和CIFAR-10 datasets进行了全面的评估，实验结果表明，这种方法可以有效提高项目和类卸学效率，比Retraining from scratch需要的时间减少了185倍和284倍。尤其是在处理少量图像时（例如64个），模型的性能下降非常小，无法影响下游任务 such as classification。
</details></li>
</ul>
<hr>
<h2 id="DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets"><a href="#DatasetEquity-Are-All-Samples-Created-Equal-In-The-Quest-For-Equity-Within-Datasets" class="headerlink" title="DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets"></a>DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09878">http://arxiv.org/abs/2308.09878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/towardsautonomy/datasetequity">https://github.com/towardsautonomy/datasetequity</a></li>
<li>paper_authors: Shubham Shrivastava, Xianling Zhang, Sushruth Nagesh, Armin Parchami</li>
<li>for:  addressed the data imbalance issue in machine learning, specifically in computer vision tasks, by developing a novel method that leverages deep perceptual embeddings and clustering to weigh samples differently during training.</li>
<li>methods:  the proposed method uses sample likelihoods based on image appearance, computed using deep perceptual embeddings and clustering, to weigh samples differently during training with a novel $\textbf{Generalized Focal Loss}$ function.</li>
<li>results:  the proposed method achieves over $200%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset, demonstrating its effectiveness in improving state-of-the-art 3D object detection methods, and its generalizability across different datasets and rare classes.<details>
<summary>Abstract</summary>
Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed $\textbf{Generalized Focal Loss}$ function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness across autonomous driving vision datasets including KITTI and nuScenes. The loss function improves state-of-the-art 3D object detection methods, achieving over $200\%$ AP gains on under-represented classes (Cyclist) in the KITTI dataset. The results demonstrate the method is generalizable, complements existing techniques, and is particularly beneficial for smaller datasets and rare classes. Code is available at: https://github.com/towardsautonomy/DatasetEquity
</details>
<details>
<summary>摘要</summary>
“数据不匹配是机器学习领域的一个公认问题，归结于数据收集成本高、标签难度和数据地域分布。在计算机视觉领域，图像外观偏见对数据分布的偏见仍未得到充分探索。相比于使用类别标签的分布，图像外观表明对象之间的复杂关系，超出了类别标签的提供。使用深度感知特征提取器和归一化 clustering 可以得到更加丰富的数据表示。本文提出了一种novel方法，用于解决机器学习中的数据不匹配问题。该方法计算样本概率基于图像外观使用深度感知特征和归一化，然后使用这些概率将样本 differently 权重 durante 训练，使用我们提议的 $\textbf{通用增强损失}$ 函数。这个损失函数可以轻松地与深度学习算法结合使用。实验证明了该方法在自动驾驶视觉数据集（包括 KITTI 和 nuScenes）上的效果，可以提高状态的较为少见类（自行车手）的 AP 分数超过 200%。结果表明该方法是通用的，可以补做现有技术，特别有利于 smaller 数据集和罕见类。代码可以在 GitHub 上找到：https://github.com/towardsautonomy/DatasetEquity。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation"><a href="#Skill-Transformer-A-Monolithic-Policy-for-Mobile-Manipulation" class="headerlink" title="Skill Transformer: A Monolithic Policy for Mobile Manipulation"></a>Skill Transformer: A Monolithic Policy for Mobile Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09873">http://arxiv.org/abs/2308.09873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Huang, Dhruv Batra, Akshara Rai, Andrew Szot</li>
<li>for: 这篇论文是用于解决长期机器人任务的方法。</li>
<li>methods: 这篇论文使用了条件序列模型和技能模块来解决问题。它使用transformer架构，通过示例轨迹来预测高级技能和全身低级动作，从而实现了任务的可组合性和模块性。</li>
<li>results: 在一个embodied重新排序测试中，这篇论文的方法比基eline achieved a 2.5倍高的成功率，在困难重新排序问题中表现出了稳定的任务规划和低级控制能力。<details>
<summary>Abstract</summary>
We present Skill Transformer, an approach for solving long-horizon robotic tasks by combining conditional sequence modeling and skill modularity. Conditioned on egocentric and proprioceptive observations of a robot, Skill Transformer is trained end-to-end to predict both a high-level skill (e.g., navigation, picking, placing), and a whole-body low-level action (e.g., base and arm motion), using a transformer architecture and demonstration trajectories that solve the full task. It retains the composability and modularity of the overall task through a skill predictor module while reasoning about low-level actions and avoiding hand-off errors, common in modular approaches. We test Skill Transformer on an embodied rearrangement benchmark and find it performs robust task planning and low-level control in new scenarios, achieving a 2.5x higher success rate than baselines in hard rearrangement problems.
</details>
<details>
<summary>摘要</summary>
我们提出了Skill Transformer，一种解决长期机器人任务的方法，通过加入条件序列模型和技能模块性。基于机器人 egocentric和 proprioceptive 观察，Skill Transformer 通过端到端训练，预测高级技能（如导航、捕获、放置）和整体低级动作（如基体和臂动作），使用 transformer 架构和示范轨迹解决整个任务。它保留了总任务的可组合性和模块性通过技能预测模块，并在低级动作和避免手动错误方面进行了合理的理解。我们在embodied rearrangement benchmark上测试了Skill Transformer，并发现它在新的 scenarios 中实现了可靠的任务规划和低级控制，与基eline 的成功率相比，在困难的重新排序问题中达到2.5倍的成功率。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks"><a href="#Tensor-Compressed-Back-Propagation-Free-Training-for-Physics-Informed-Neural-Networks" class="headerlink" title="Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks"></a>Tensor-Compressed Back-Propagation-Free Training for (Physics-Informed) Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09858">http://arxiv.org/abs/2308.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yequan Zhao, Xinling Yu, Zhixiong Chen, Ziyue Liu, Sijia Liu, Zheng Zhang</li>
<li>for: 本研究的目的是提出一个不需要后向传播（Backward Propagation，BP）的框架，用于在边缘设备上训练具有真实性的神经网络。</li>
<li>methods: 我们的技术贡献包括三个方面：首先，我们提出了一种紧凑变量压缩方法来大幅提高零次ORDER（ZO）优化的可扩展性，使得可以处理大于过去ZO方法所能处理的网络大小。其次，我们提出了一种混合式Gradient评估方法来提高ZO训练的效率。最后，我们将我们的BP-free训练框架扩展到物理学 informed neural networks（PINNs）中，并提出了一种稀疏网格方法来估计损失函数中的导数而不使用BP。</li>
<li>results: 我们的BP-free训练只在MNIST数据集上失去了一点精度与标准首次Order训练相比。此外，我们还成功地训练了一个解决20维汉米尔-雅可比-贝尔姆 partial differential equation（PDE）的PINN。这种内存高效和BP-free的方法可能会成为未来资源有限平台（例如FPGA、ASIC、微控制器和光子学材料）上的准 Near-future on-device training。<details>
<summary>Abstract</summary>
Backward propagation (BP) is widely used to compute the gradients in neural network training. However, it is hard to implement BP on edge devices due to the lack of hardware and software resources to support automatic differentiation. This has tremendously increased the design complexity and time-to-market of on-device training accelerators. This paper presents a completely BP-free framework that only requires forward propagation to train realistic neural networks. Our technical contributions are three-fold. Firstly, we present a tensor-compressed variance reduction approach to greatly improve the scalability of zeroth-order (ZO) optimization, making it feasible to handle a network size that is beyond the capability of previous ZO approaches. Secondly, we present a hybrid gradient evaluation approach to improve the efficiency of ZO training. Finally, we extend our BP-free training framework to physics-informed neural networks (PINNs) by proposing a sparse-grid approach to estimate the derivatives in the loss function without using BP. Our BP-free training only loses little accuracy on the MNIST dataset compared with standard first-order training. We also demonstrate successful results in training a PINN for solving a 20-dim Hamiltonian-Jacobi-Bellman PDE. This memory-efficient and BP-free approach may serve as a foundation for the near-future on-device training on many resource-constraint platforms (e.g., FPGA, ASIC, micro-controllers, and photonic chips).
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用后向传播（BP）计算神经网络训练中的梯度是广泛使用的。然而，由于缺乏硬件和软件资源，在边缘设备上实现BP很困难。这有效地增加了设计复杂性和时间到市场的 neural network 训练加速器。本文提出了一个完全不需要BP的框架，只需要前向传播来训练真实的神经网络。我们的技术贡献有三个方面：1. 我们提出了一种压缩tensor的方法，以提高零次梯度优化的扩展性，使得可以处理大于前一代ZO方法所能处理的网络大小。2. 我们提出了一种混合梯度评估方法，以提高ZO训练的效率。3. 我们将BP-free训练框架扩展到物理学习神经网络（PINNs），并提出了一种稀疏网格方法，以便在损失函数中无需使用BP来估算梯度。BP-free训练只在MNIST数据集上失去了微scopic的精度，与标准首次训练相比。我们还成功地训练了一个解决20维汉密尔-雅可比-贝尔干涯方程的PINN。这种内存高效的BP-free方法可能会成为许多有限资源平台（如FPGA、ASIC、微控制器和光学芯片）的未来边缘训练基础。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations"><a href="#Backdoor-Mitigation-by-Correcting-the-Distribution-of-Neural-Activations" class="headerlink" title="Backdoor Mitigation by Correcting the Distribution of Neural Activations"></a>Backdoor Mitigation by Correcting the Distribution of Neural Activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09850">http://arxiv.org/abs/2308.09850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Li, Zhen Xiang, David J. Miller, George Kesidis</li>
<li>for: 本研究探讨了深度神经网络（DNN）中的后门（Trojan）攻击，特别是 Successful 攻击会导致内层活动分布的变化，以及如何通过修复这种变化来实现后门纠正。</li>
<li>methods: 本研究使用了reverse工程技术来恢复 trigger 的原始形式，并通过修复内层活动分布的变化来实现后门纠正。</li>
<li>results: 本研究发现，使用修复后门 trigger 的方法可以有效地纠正后门攻击，并且不需要对 DNN 的参数进行优化。此外，这种方法还可以有效地检测测试实例中是否存在 trigger。<details>
<summary>Abstract</summary>
Backdoor (Trojan) attacks are an important type of adversarial exploit against deep neural networks (DNNs), wherein a test instance is (mis)classified to the attacker's target class whenever the attacker's backdoor trigger is present. In this paper, we reveal and analyze an important property of backdoor attacks: a successful attack causes an alteration in the distribution of internal layer activations for backdoor-trigger instances, compared to that for clean instances. Even more importantly, we find that instances with the backdoor trigger will be correctly classified to their original source classes if this distribution alteration is corrected. Based on our observations, we propose an efficient and effective method that achieves post-training backdoor mitigation by correcting the distribution alteration using reverse-engineered triggers. Notably, our method does not change any trainable parameters of the DNN, but achieves generally better mitigation performance than existing methods that do require intensive DNN parameter tuning. It also efficiently detects test instances with the trigger, which may help to catch adversarial entities in the act of exploiting the backdoor.
</details>
<details>
<summary>摘要</summary>
后门（Trojan）攻击是深度神经网络（DNN）的重要类型敌意攻击，其中测试实例会在攻击者的目标类中被识别，只要攻击者的后门触发器存在。在这篇论文中，我们揭示了后门攻击的一个重要性ptych：成功攻击会导致后门触发器实例的内部层活动分布发生变化，相比于干净实例。进一步地，我们发现，如果这种分布变化得到了修正，那么后门触发器实例将会返回到其原始类别。基于我们的观察，我们提出了一种高效的后门整治方法，通过修正分布变化来实现后门整治。这种方法不会改变 DNN 的可调参数，但可以实现更好的整治性能，并快速检测测试实例中的触发器。
</details></li>
</ul>
<hr>
<h2 id="Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees"><a href="#Enumerating-Safe-Regions-in-Deep-Neural-Networks-with-Provable-Probabilistic-Guarantees" class="headerlink" title="Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees"></a>Enumerating Safe Regions in Deep Neural Networks with Provable Probabilistic Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09842">http://arxiv.org/abs/2308.09842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Marzari, Davide Corsi, Enrico Marchesini, Alessandro Farinelli, Ferdinando Cicalese</li>
<li>for: 确保深度神经网络（DNNs）的可靠性，identifying safe areas是关键。</li>
<li>methods: 我们引入AllDNN-Verification问题，给出了一种有效的 aproximation方法called epsilon-ProVe。</li>
<li>results: 我们的方法可以提供一个紧身的（具有证明的概率保证）下界估计安全区域，并且在不同的标准准比例上进行了实验，证明了我们的方法的可扩展性和有效性。<details>
<summary>Abstract</summary>
Identifying safe areas is a key point to guarantee trust for systems that are based on Deep Neural Networks (DNNs). To this end, we introduce the AllDNN-Verification problem: given a safety property and a DNN, enumerate the set of all the regions of the property input domain which are safe, i.e., where the property does hold. Due to the #P-hardness of the problem, we propose an efficient approximation method called epsilon-ProVe. Our approach exploits a controllable underestimation of the output reachable sets obtained via statistical prediction of tolerance limits, and can provide a tight (with provable probabilistic guarantees) lower estimate of the safe areas. Our empirical evaluation on different standard benchmarks shows the scalability and effectiveness of our method, offering valuable insights for this new type of verification of DNNs.
</details>
<details>
<summary>摘要</summary>
安全区域的标识是深度神经网络（DNN）系统的关键策略，以确保信任性。为此，我们提出了AllDNN-Verification问题：给定一个安全性质和一个DNN，列出安全区域的输入Domaint中的所有区域，即where the property does hold。由于这个问题的P-完备性，我们提出了一种高效的近似方法called epsilon-ProVe。我们的方法利用了可控的输出可达集的统计预测误差，可以提供一个紧靠的（带有可靠的 probabilistic guarantees）下界估计安全区域。我们的实验表明了我们的方法的扩展性和有效性，为这种新的DNN验证提供了有价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis"><a href="#Microscopy-Image-Segmentation-via-Point-and-Shape-Regularized-Data-Synthesis" class="headerlink" title="Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis"></a>Microscopy Image Segmentation via Point and Shape Regularized Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09835">http://arxiv.org/abs/2308.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Li, Mengwei Ren, Thomas Ach, Guido Gerig</li>
<li>for: 这个论文主要针对的是用深度学习方法进行微scopia图像分割，但是现有的方法几乎都需要大量的培训数据，包括完整的对象 outline，这是非常时间consuming和困难的。这篇论文提出了一种使用点标注（即对象中心点）来训练微scopia图像分割模型的方法。</li>
<li>methods: 这篇论文提出了一种三个阶段的框架，包括：	1. 使用点标注生成一个 Pseudo dense segmentation mask，这个mask受到形状约束的限制。	2. 使用一种未经培训的图像生成模型将mask翻译成一个真实的微scopia图像，这个图像受到对象级别一致性的限制。	3. 将Pseudo masks和生成的图像组成一个对应的 dataset，用于训练微scopia图像分割模型。</li>
<li>results:  compared to使用pseudo-labels或基eline生成的图像，模型在使用这种synthesis pipeline训练后表现出了明显的改进。此外，这种方法可以与使用authentic microscopy images with dense labels进行比较，并且达到了相似的性能。代码可以获取。<details>
<summary>Abstract</summary>
Current deep learning-based approaches for the segmentation of microscopy images heavily rely on large amount of training data with dense annotation, which is highly costly and laborious in practice. Compared to full annotation where the complete contour of objects is depicted, point annotations, specifically object centroids, are much easier to acquire and still provide crucial information about the objects for subsequent segmentation. In this paper, we assume access to point annotations only during training and develop a unified pipeline for microscopy image segmentation using synthetically generated training data. Our framework includes three stages: (1) it takes point annotations and samples a pseudo dense segmentation mask constrained with shape priors; (2) with an image generative model trained in an unpaired manner, it translates the mask to a realistic microscopy image regularized by object level consistency; (3) the pseudo masks along with the synthetic images then constitute a pairwise dataset for training an ad-hoc segmentation model. On the public MoNuSeg dataset, our synthesis pipeline produces more diverse and realistic images than baseline models while maintaining high coherence between input masks and generated images. When using the identical segmentation backbones, the models trained on our synthetic dataset significantly outperform those trained with pseudo-labels or baseline-generated images. Moreover, our framework achieves comparable results to models trained on authentic microscopy images with dense labels, demonstrating its potential as a reliable and highly efficient alternative to labor-intensive manual pixel-wise annotations in microscopy image segmentation. The code is available.
</details>
<details>
<summary>摘要</summary>
当前的深度学习基于方法 для微scopia图像分割强调大量的训练数据和精密的标注，但这在实际中是非常昂贵和劳动密集的。相比于全标注，其中包括对象的完整边界，点标注，具体来说是对象的中心点，是训练中得到的 much easier。在这篇论文中，我们假设在训练时可以获得点标注。我们提出了一个整体框架，包括以下三个阶段：1. 从点标注中提取pseudo dense segmentation mask，并受限于形状假设。2. 使用未经准备的图像生成模型，将mask翻译成真实的微scopia图像，并在对象水平进行了规范。3. pseudo masks和生成的图像组成一个对应的对数据集，用于训练专门的分割模型。在公共的 MoNuSeg 数据集上，我们的生成框架生成了更加多样化和真实的图像，同时保持输入权重和生成图像之间的高协调性。当使用同一个分割背bone时，我们在生成数据集上训练的模型显著超过 pseudo-标签或基eline-生成的图像模型。此外，我们的框架可以与 dense标注数据集进行比较，表明它可以作为 dense标注的可靠和高效的替代方案。代码可以获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks"><a href="#Learning-from-A-Single-Graph-is-All-You-Need-for-Near-Shortest-Path-Routing-in-Wireless-Networks" class="headerlink" title="Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks"></a>Learning from A Single Graph is All You Need for Near-Shortest Path Routing in Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09829">http://arxiv.org/abs/2308.09829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yung-Fu Chen, Sen Lin, Anish Arora</li>
<li>For: 本研究提出一种学习算法，用于解决无线网络中的本地路由策略问题。这种算法只需要一些从同一个图中获取的数据样本，可以对所有随机图进行泛化。* Methods: 本研究使用深度神经网络（DNNs）来学习本地路由策略。这些DNNs可以高效地和扩展地学习路由策略，只考虑节点状态和邻居节点状态。* Results: 研究结果显示，使用这种算法可以快速地从一些路由路径中获取样本，并在各种随机图上获得高效和普遍适用的路由策略。此外，这种算法还可以提供 тео리тиче explainability，即为什么使用一个小型的种子图和节点抽样可以有效地学习路由策略。<details>
<summary>Abstract</summary>
We propose a learning algorithm for local routing policies that needs only a few data samples obtained from a single graph while generalizing to all random graphs in a standard model of wireless networks. We thus solve the all-pairs near-shortest path problem by training deep neural networks (DNNs) that efficiently and scalably learn routing policies that are local, i.e., they only consider node states and the states of neighboring nodes. Remarkably, one of these DNNs we train learns a policy that exactly matches the performance of greedy forwarding; another generally outperforms greedy forwarding. Our algorithm design exploits network domain knowledge in several ways: First, in the selection of input features and, second, in the selection of a ``seed graph'' and subsamples from its shortest paths. The leverage of domain knowledge provides theoretical explainability of why the seed graph and node subsampling suffice for learning that is efficient, scalable, and generalizable. Simulation-based results on uniform random graphs with diverse sizes and densities empirically corroborate that using samples generated from a few routing paths in a modest-sized seed graph quickly learns a model that is generalizable across (almost) all random graphs in the wireless network model.
</details>
<details>
<summary>摘要</summary>
我们提出了一种学习算法，用于本地路由策略，只需要几个数据样本，从一个图中获取，而能够泛化到所有随机图中的标准网络模型。我们通过训练深度神经网络（DNN），efficiently和可扩展地学习本地路由策略，即只考虑节点状态和邻居节点状态。奇怪的是，我们训练的一个DNN学习策略与批发转发性能相同；另一个策略则通常超过批发转发性能。我们的算法设计利用网络领域知识，包括输入特征的选择和``seed graph''和节点子集的选择。这种利用领域知识的设计提供了有理解的解释，为何seed graph和节点子集 suffice for learning是有效、可扩展和泛化的。针对随机图中的不同大小和密度的实验结果，表明使用从seed graph中生成的几个路由路径的样本，可以快速学习一个泛化到大多数随机图的模型。
</details></li>
</ul>
<hr>
<h2 id="VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control"><a href="#VL-PET-Vision-and-Language-Parameter-Efficient-Tuning-via-Granularity-Control" class="headerlink" title="VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control"></a>VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09804">http://arxiv.org/abs/2308.09804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henryhzy/vl-pet">https://github.com/henryhzy/vl-pet</a></li>
<li>paper_authors: Zi-Yuan Hu, Yanyang Li, Michael R. Lyu, Liwei Wang</li>
<li>for: 这个论文目的是提出一个具有效的vision-and-language受控 Parametric Tuning（VL-PET）框架，以便实现更好的效率和有效性变数。</li>
<li>methods: 这个论文使用了一种新的粒度控制机制，允许在不同的粒度控制矩阵下逐步实现不同的模组化修改。此外，论文还提出了一些轻量级的PET模组设计，以提高预测和文本生成的能力。</li>
<li>results: 实验结果显示，这个VL-PET框架可以与现有的PET技术相比，在四个影像文本任务和四个影像视频文本任务上表现更好，并且可以实现更好的效率和有效性变数。具体来说，使用VL-PET-大的模型可以与BART-base和T5-base模型相比，在影像文本任务上表现出2.92%（3.41%）和3.37%（7.03%）的改善。<details>
<summary>Abstract</summary>
As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter and LoRA) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the functionality gap between the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these critical issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. We further propose lightweight PET module designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders. Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness and transferability of our VL-PET framework. In particular, our VL-PET-large with lightweight PET module designs significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37% (7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validate the enhanced effect of employing our VL-PET designs on existing PET techniques, enabling them to achieve significant performance improvements. Our code is available at https://github.com/HenryHZY/VL-PET.
</details>
<details>
<summary>摘要</summary>
“随着预训语言模型（PLM）的模型大小迅速增长，全面精通化训练（full fine-tuning）已成为训练和储存模型的瓶颈。在视觉语言（VL）领域，具有实现效益的参数效率训练（PET）技术被提出，以将可变的修改（e.g., Adapter和LoRA） integrate到encoder-decoder PLM 中。这些技术可以通过微小的参数训练，与全面精通化训练相比，实现相似的性能。然而，过度的修改和遗传的差异点（gap）对模型的性能产生负面影响，而现有的PET技术（e.g., VL-Adapter）忽略了这些重要问题。本文提出了一个vision-and-language参数效率训练（VL-PET）框架，以控制修改的具体化程度。通过这个框架，可以从不同的具体化控制矩阵生成多种模型独立的VL-PET模组，以达到更好的效率和效果的调控。此外，我们还提出了一些轻量级PET模组的设计，以增强视觉和文本的整合和模型。实验结果显示，我们的VL-PET-大型以轻量级PET模组设计对image-text任务和video-text任务表现出色，与BART-base和T5-base相比，优于VL-Adapter和LoRA。此外，我们还 validate了我们的VL-PET设计对现有PET技术的优化效果，使其实现更大的性能提升。我们的代码可以在https://github.com/HenryHZY/VL-PET 上获取。”
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification"><a href="#An-Efficient-High-Dimensional-Gene-Selection-Approach-based-on-Binary-Horse-Herd-Optimization-Algorithm-for-Biological-Data-Classification" class="headerlink" title="An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification"></a>An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09791">http://arxiv.org/abs/2308.09791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niloufar Mehrabi, Sayed Pedram Haeri Boroujeni, Elnaz Pashaei</li>
<li>for: 解决复杂高维问题，特别是维度较大的搜索问题。</li>
<li>methods: 使用新的马群优化算法（BHOA）和最小重复最大相关性筛选器（MRMR）组成的гибрид特征选择方法。</li>
<li>results: 对于十个靶场数据集（Lymphoma、Prostate、Brain-1、DLBCL、SRBCT、Leukemia、Ovarian、Colon、Lung、MLL），提出了一种更高效的特征选择方法，比如Gray Wolf（GW）、Particle Swarm Optimization（PSO）和遗传算法（GA）。<details>
<summary>Abstract</summary>
The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm based on the behaviors of horses at different ages. The HOA was introduced recently to solve complex and high-dimensional problems. This paper proposes a binary version of the Horse Herd Optimization Algorithm (BHOA) in order to solve discrete problems and select prominent feature subsets. Moreover, this study provides a novel hybrid feature selection framework based on the BHOA and a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid feature selection, which is more computationally efficient, produces a beneficial subset of relevant and informative features. Since feature selection is a binary problem, we have applied a new Transfer Function (TF), called X-shape TF, which transforms continuous problems into binary search spaces. Furthermore, the Support Vector Machine (SVM) is utilized to examine the efficiency of the proposed method on ten microarray datasets, namely Lymphoma, Prostate, Brain-1, DLBCL, SRBCT, Leukemia, Ovarian, Colon, Lung, and MLL. In comparison to other state-of-the-art, such as the Gray Wolf (GW), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA), the proposed hybrid method (MRMR-BHOA) demonstrates superior performance in terms of accuracy and minimum selected features. Also, experimental results prove that the X-Shaped BHOA approach outperforms others methods.
</details>
<details>
<summary>摘要</summary>
《各种高维问题的解决方法——骏马群优化算法》是一种新的元规则算法，基于不同年龄的马的行为。该算法最近被提出来解决复杂和高维问题。本文提出了一种二进制版的骏马群优化算法（BHOA），以解决离散问题和选择显著特征子。此外，本研究还提出了一种新的半程特征选择框架，基于BHOA和最小冗余最大相关性（MRMR）筛选方法。这种半程特征选择方法更加高效，生成了有益的相关和有用的特征子。由于特征选择是一个二进制问题，我们采用了一种新的转换函数（TF），即X型TF，将连续问题转换为二进制搜索空间。此外，我们使用支持向量机（SVM）来评估提出的方法在10个微陷数据集上的效率，即肿瘤、 próstata、大脑-1、DLBCL、SRBCT、白细胞癌、卵巢、肠癌和MLL等10个数据集。与其他现状之前的方法，如灰狼（GW）、粒子群动 optimization（PSO）和生物学算法（GA）相比，提出的半程特征选择方法（MRMR-BHOA）在精度和选择的最小特征上显示出超过其他方法的优势。此外，实验结果也证明了X型BHOA方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing"><a href="#A-Two-Part-Machine-Learning-Approach-to-Characterizing-Network-Interference-in-A-B-Testing" class="headerlink" title="A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing"></a>A Two-Part Machine Learning Approach to Characterizing Network Interference in A&#x2F;B Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09790">http://arxiv.org/abs/2308.09790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Yuan, Kristen M. Altenburger</li>
<li>for: 提高A&#x2F;B测试的可靠性和精度，解决网络干扰的问题</li>
<li>methods: 使用机器学习算法确定和描述不同类型的网络干扰，自动确定”曝光地图”，解决现有文献中的两大限制</li>
<li>results: 通过synthetic实验和实际大规模测试（1-2万名Instagram用户），证明了我们的方法可以超越传统的设计基于块随机化和分析基于邻居曝光地图的方法，提高A&#x2F;B测试结果的精度和可靠性。<details>
<summary>Abstract</summary>
The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. Overall, our approach not only offers a comprehensive, automated solution for managing network interference and improving the precision of A/B testing results, but it also sheds light on users' mutual influence and aids in the refinement of marketing strategies.
</details>
<details>
<summary>摘要</summary>
controlled experiments 的可靠性，或“A/B测试”，经常受到网络干扰的影响，这导致一个单位的结果受到其他单位的影响。为解决这个挑战，我们提议一种基于机器学习的方法，用于识别和特征化不同类型的网络干扰。我们的方法考虑了隐藏的复杂网络结构，并自动确定“曝光 mapping”，这两者都是现有文献中的主要限制。我们引入“ causal 网络模式”，并使用透明的机器学习模型来确定最适合的曝光 mapping，这些模型能够反映下面网络干扰模式。我们的方法在两个人工实验和一个实际的大规模测试中，对1-2万名INSTAGRAM用户进行验证，并表现出色，超过了传统的设计基于块随机化和分析基于邻居曝光 mapping的方法。总的来说，我们的方法不仅提供了一种全面、自动化的网络干扰管理和A/B测试结果的精度提高的解决方案，还 shed light on 用户之间的互相影响，并帮助优化市场策略。
</details></li>
</ul>
<hr>
<h2 id="Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models"><a href="#Towards-Grounded-Visual-Spatial-Reasoning-in-Multi-Modal-Vision-Language-Models" class="headerlink" title="Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models"></a>Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09778">http://arxiv.org/abs/2308.09778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Rajabi, Jana Kosecka</li>
<li>for: 这个研究旨在评估大规模视语言模型（VLM）在不同的视觉理解任务中表现，包括数字、引用表达和通用的视觉问答任务。</li>
<li>methods: 该研究使用了细化的组合基础grounding方法，并提出了底层方法来评估视空间关系理解任务的表现。</li>
<li>results: 研究发现，当前的VLM模型在理解视空间关系方面表现不佳，与人类表现之间存在较大的差距。<details>
<summary>Abstract</summary>
With the advances in large scale vision-and-language models (VLMs) it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions and general visual question answering. The focus of this work is to study the ability of these models to understanding spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering task, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning task. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and Bansal 2019; Gupta et al. 2022; Kamath et al. 2021) and compare and highlight their abilities to reason about spatial relationships.
</details>
<details>
<summary>摘要</summary>
With the advances in large-scale vision-and-language models (VLMs), it is of interest to assess their performance on various visual reasoning tasks such as counting, referring expressions, and general visual question answering. The focus of this work is to study the ability of these models to understand spatial relations. Previously, this has been tackled using image-text matching (Liu, Emerson, and Collier 2022) or visual question answering tasks, both showing poor performance and a large gap compared to human performance. To better understand the gap, we present fine-grained compositional grounding of spatial relationships and propose a bottom-up approach for ranking spatial clauses and evaluating the performance of spatial relationship reasoning tasks. We propose to combine the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative vision-language models (Tan and Bansal 2019; Gupta et al. 2022; Kamath et al. 2021) and compare and highlight their abilities to reason about spatial relationships.Here's the translation in Traditional Chinese: WITH 大规模的视力语言模型（VLM）的发展，有兴趣评估它们在不同的视觉推理任务上的表现，例如数量、参考表达和通用的视觉问题回答。这个研究的重点是研究这些模型对空间关系的理解。以前，这已经使用了图像和文本匹配（Liu、Emerson和Collier 2022）或视觉问题回答任务，都显示了轻微的表现和人类表现之间的大差。为了更好地理解这个差距，我们提出了细化的实体基底推理和评估视觉关系理解任务的方法。我们提议将对象和其位置的语言表达与它们的位置进行细化的实体基底推理，并将这些证据组合以计算最终排名的视觉关系 clause。我们在代表性的视力语言模型（Tan和Bansal 2019；Gupta et al. 2022；Kamath et al. 2021）上实现了这个方法，并与之比较和强调它们在视觉关系理解方面的能力。
</details></li>
</ul>
<hr>
<h2 id="Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources"><a href="#Time-Series-Predictions-in-Unmonitored-Sites-A-Survey-of-Machine-Learning-Techniques-in-Water-Resources" class="headerlink" title="Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources"></a>Time Series Predictions in Unmonitored Sites: A Survey of Machine Learning Techniques in Water Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09766">http://arxiv.org/abs/2308.09766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jared D. Willard, Charuleka Varadharajan, Xiaowei Jia, Vipin Kumar</li>
<li>for: 预测不监测水体环境变量的精度减少了水资源科学中的长期挑战。大多数新鲜水资源的监测不具备关键环境变量的监测，尽管气候和土地使用变化的影响对水资源产生了越来越严重的影响。</li>
<li>methods: 现代机器学习方法在水文时间序预测中表现出了超越过程知识和经验模型的能力，特别是EXTRACT信息FROM大量多样数据集。</li>
<li>results: 我们回顾了相关的状态艺术应用程序，涵盖了流量和水质预测等水资源预测领域。分析结果表明，以往努力都集中在了深度学习框架上，但对不同类型机器学习方法的比较是 rare 和不充分。我们认为还有几个开放问题需要解决，包括包括动态输入和站点特点、机制理解和空间Context，以及现代机器学习框架中的可解释AI技术。<details>
<summary>Abstract</summary>
Prediction of dynamic environmental variables in unmonitored sites remains a long-standing challenge for water resources science. The majority of the world's freshwater resources have inadequate monitoring of critical environmental variables needed for management. Yet, the need to have widespread predictions of hydrological variables such as river flow and water quality has become increasingly urgent due to climate and land use change over the past decades, and their associated impacts on water resources. Modern machine learning methods increasingly outperform their process-based and empirical model counterparts for hydrologic time series prediction with their ability to extract information from large, diverse data sets. We review relevant state-of-the art applications of machine learning for streamflow, water quality, and other water resources prediction and discuss opportunities to improve the use of machine learning with emerging methods for incorporating watershed characteristics into deep learning models, transfer learning, and incorporating process knowledge into machine learning models. The analysis here suggests most prior efforts have been focused on deep learning learning frameworks built on many sites for predictions at daily time scales in the United States, but that comparisons between different classes of machine learning methods are few and inadequate. We identify several open questions for time series predictions in unmonitored sites that include incorporating dynamic inputs and site characteristics, mechanistic understanding and spatial context, and explainable AI techniques in modern machine learning frameworks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the following text into Simplified Chinese.<</SYS>>现代机器学习方法在水资源预测中表现越来越出色，可以从大量多样数据集中提取信息。然而，预测不监测站点的水文变量仍然是水资源科学领域的长期挑战。全球大多数新鲜水资源尚未实施监测关键环境变量，尤其是气候和土地使用变化的影响。因此，广泛预测河流流量和水质等水资源变量的需求日益紧迫。本文将介绍相关的现代机器学习应用，包括流速、水质和其他水资源预测。我们还将讨论如何在深入学习模型中包含水体特征、知识传播和机器学习模型。分析结果表明，现有的尝试主要集中在美国多个站点的深度学习框架上，但对不同类型机器学习方法的比较 remains 有限。我们确定了一些未解决的问题，包括 incorporating 动态输入和站点特征、机制理解和空间上下文，以及现代机器学习框架中的可解释AI技术。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Taken-by-Surprise-Contrast-effect-for-Similarity-Scores"><a href="#Taken-by-Surprise-Contrast-effect-for-Similarity-Scores" class="headerlink" title="Taken by Surprise: Contrast effect for Similarity Scores"></a>Taken by Surprise: Contrast effect for Similarity Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09765">http://arxiv.org/abs/2308.09765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meetelise/surprise-similarity">https://github.com/meetelise/surprise-similarity</a></li>
<li>paper_authors: Thomas C. Bachlechner, Mario Martone, Marjorie Schillo</li>
<li>for: 本研究旨在提高自然语言处理、信息检索和分类任务中的对象向量表示的准确评估。</li>
<li>methods: 该研究提出了一种名为“奇异分数”的ensemble-normalized相似度度量，它考虑到人类对对象之间的相似程度的感知效应，并在零&#x2F;几shot文档分类任务中显示出10-15%的提高。</li>
<li>results: 研究发现，使用“奇异分数”相似度度量 Typically find 10-15% better performance compared to raw cosine similarity in zero&#x2F;few shot document classification tasks.I hope that helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the $\textit{surprise score}$, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15 % better performance compared to raw cosine similarity. Our code is available at https://github.com/MeetElise/surprise-similarity.
</details>
<details>
<summary>摘要</summary>
非常重要的评估对象vector embedding的相似性，是自然语言处理、信息检索和分类任务中的关键。受欢迎的相似性分数（如归一化相似性）忽略了对象 ensemble 的分布。人类对物体相似性的认知具有Context-dependent的特点，在这种情况下，我们提出了“surprise score”，一种ensemble-normalized相似度度量，它考虑了对象之间的对比效果，并在零/几shot文档分类任务中显示了10-15%的改善性。我们在这些任务中评估了这个度量，并通常发现它比 raw cosine similarity 更好。我们的代码可以在https://github.com/MeetElise/surprise-similarity上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation"><a href="#The-Impact-of-Background-Removal-on-Performance-of-Neural-Networks-for-Fashion-Image-Classification-and-Segmentation" class="headerlink" title="The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation"></a>The Impact of Background Removal on Performance of Neural Networks for Fashion Image Classification and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09764">http://arxiv.org/abs/2308.09764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhui Liang, Ying Liu, Vladimir Vlassov</li>
<li>for: 提高时尚图像数据质量，提高模型性能</li>
<li>methods: 使用突出对象检测进行背景除去</li>
<li>results: 对时尚图像进行背景除去可以提高模型准确率，但是不适用于深度神经网络。<details>
<summary>Abstract</summary>
Fashion understanding is a hot topic in computer vision, with many applications having great business value in the market. Fashion understanding remains a difficult challenge for computer vision due to the immense diversity of garments and various scenes and backgrounds. In this work, we try removing the background from fashion images to boost data quality and increase model performance. Having fashion images of evident persons in fully visible garments, we can utilize Salient Object Detection to achieve the background removal of fashion data to our expectations. A fashion image with the background removed is claimed as the "rembg" image, contrasting with the original one in the fashion dataset. We conducted extensive comparative experiments with these two types of images on multiple aspects of model training, including model architectures, model initialization, compatibility with other training tricks and data augmentations, and target task types. Our experiments show that background removal can effectively work for fashion data in simple and shallow networks that are not susceptible to overfitting. It can improve model accuracy by up to 5% in the classification on the FashionStyle14 dataset when training models from scratch. However, background removal does not perform well in deep neural networks due to incompatibility with other regularization techniques like batch normalization, pre-trained initialization, and data augmentations introducing randomness. The loss of background pixels invalidates many existing training tricks in the model training, adding the risk of overfitting for deep models.
</details>
<details>
<summary>摘要</summary>
现代服装理解是计算机视觉领域热点话题，具有广泛的商业价值。然而，服装理解仍然是计算机视觉中的困难挑战，因为衣服的多样性和不同的场景和背景。在这种工作中，我们尝试将背景从时尚图像中除掉，以提高数据质量并提高模型性能。通过使用突出对象检测来实现背景的除掉，我们称之为“rembg”图像，与原始图像在时尚数据集中进行比较。我们进行了多种比较试验，包括模型架构、模型初始化、与其他训练技巧和数据扩展Compatibility，以及目标任务类型。我们的实验结果表明，背景除掉可以有效地提高时尚数据上的模型准确率。在FashionStyle14数据集上进行分类训练时，背景除掉可以提高模型准确率达到5%。然而，背景除掉不适合深度神经网络，因为它们与其他正则化技术，如批处理normalization、预先初始化和数据扩展，不兼容。失去背景像素会让许多现有的训练技巧无法在模型训练中使用，增加深度模型的风险。
</details></li>
</ul>
<hr>
<h2 id="Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning"><a href="#Data-Compression-and-Inference-in-Cosmology-with-Self-Supervised-Machine-Learning" class="headerlink" title="Data Compression and Inference in Cosmology with Self-Supervised Machine Learning"></a>Data Compression and Inference in Cosmology with Self-Supervised Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09751">http://arxiv.org/abs/2308.09751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl">https://github.com/aizhanaakhmet/data-compression-inference-in-cosmology-with-ssl</a></li>
<li>paper_authors: Aizhan Akhmetzhanova, Siddharth Mishra-Sharma, Cora Dvorkin</li>
<li>for: 这种方法可以快速 SUMMARIZE 大量数据，以便进行下游任务。</li>
<li>methods: 这种方法使用 simulation-based 自我超vised 机器学习，通过创建代表性的摘要来快速 SUMMARIZE 大量数据。</li>
<li>results: 这种方法可以提供高度信息含量的摘要，可以用于准确地推断参数。  Additionally, the method is insensitive to prescribed systematic effects, such as the influence of baryonic physics.<details>
<summary>Abstract</summary>
The influx of massive amounts of data from current and upcoming cosmological surveys necessitates compression schemes that can efficiently summarize the data with minimal loss of information. We introduce a method that leverages the paradigm of self-supervised machine learning in a novel manner to construct representative summaries of massive datasets using simulation-based augmentations. Deploying the method on hydrodynamical cosmological simulations, we show that it can deliver highly informative summaries, which can be used for a variety of downstream tasks, including precise and accurate parameter inference. We demonstrate how this paradigm can be used to construct summary representations that are insensitive to prescribed systematic effects, such as the influence of baryonic physics. Our results indicate that self-supervised machine learning techniques offer a promising new approach for compression of cosmological data as well its analysis.
</details>
<details>
<summary>摘要</summary>
“ cosmological 调查中的大量数据涌入导致了各种压缩方法的实现，以确保仅对数据进行最小的损失。我们介绍了一种方法，利用自动机器学习的思想，在实验增强的情况下，创建大规模数据的代表SUMMARY。在液体动力学 cosmological 实验中执行了这种方法，发现它可以提供高度有用的SUMMARY，可以用于多种下游任务，包括精确的参数推断。我们显示了这种思想可以用来建构对系统性效应不敏感的SUMMARY表示，例如关于生物物理的影响。我们的结果显示自动机器学习技术可以对 cosmological 数据实现高效的压缩和分析。”Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Robust-Monocular-Depth-Estimation-under-Challenging-Conditions"><a href="#Robust-Monocular-Depth-Estimation-under-Challenging-Conditions" class="headerlink" title="Robust Monocular Depth Estimation under Challenging Conditions"></a>Robust Monocular Depth Estimation under Challenging Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09711">http://arxiv.org/abs/2308.09711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/md4all/md4all">https://github.com/md4all/md4all</a></li>
<li>paper_authors: Stefano Gasperini, Nils Morbitzer, HyunJun Jung, Nassir Navab, Federico Tombari</li>
<li>for: 提高单目深度估计的可靠性，并在不同的环境和学习环境下实现可靠性。</li>
<li>methods: 基于现有方法的自我或全自动超vision，通过生成复杂样本并在原始图像上计算标准损失来训练模型。</li>
<li>results: 在nuScenes和Oxford RobotCar等两个公共数据集上进行了广泛的实验，并覆盖了先前的工作，在标准和挑战性条件下表现出色。<details>
<summary>Abstract</summary>
While state-of-the-art monocular depth estimation approaches achieve impressive results in ideal settings, they are highly unreliable under challenging illumination and weather conditions, such as at nighttime or in the presence of rain. In this paper, we uncover these safety-critical issues and tackle them with md4all: a simple and effective solution that works reliably under both adverse and ideal conditions, as well as for different types of learning supervision. We achieve this by exploiting the efficacy of existing methods under perfect settings. Therefore, we provide valid training signals independently of what is in the input. First, we generate a set of complex samples corresponding to the normal training ones. Then, we train the model by guiding its self- or full-supervision by feeding the generated samples and computing the standard losses on the corresponding original images. Doing so enables a single model to recover information across diverse conditions without modifications at inference time. Extensive experiments on two challenging public datasets, namely nuScenes and Oxford RobotCar, demonstrate the effectiveness of our techniques, outperforming prior works by a large margin in both standard and challenging conditions. Source code and data are available at: https://md4all.github.io.
</details>
<details>
<summary>摘要</summary>
当前最前沿单目深度估计方法在理想情况下可以 achieve impressive results，但在具有挑战性的照明和天气条件下，其可靠性却受到了挑战。在这篇论文中，我们揭示了这些安全关键问题，并通过md4all：一个简单而有效的解决方案，可以在不同的条件下工作，包括不良和理想的情况，以及不同类型的学习监督。我们通过利用现有方法在完美情况下的效果，实现了这一点。因此，我们可以在训练时提供有效的训练信号，不abhängig于输入中的内容。首先，我们生成了一组复杂的样本，与传统的训练样本相对应。然后，我们通过引导自我或全自监督，将生成的样本和对应的原始图像之间的标准损失进行训练。这样做的核心思想是，让模型在不同的条件下可以在推理时恢复信息。我们在 nuScenes 和 Oxford RobotCar 两个公共数据集上进行了广泛的实验，并证明了我们的技术的有效性，与之前的工作相比，在标准和挑战性的情况下都有大幅度的提高。软件代码和数据可以在 <https://md4all.github.io> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain"><a href="#Neural-network-quantum-state-study-of-the-long-range-antiferromagnetic-Ising-chain" class="headerlink" title="Neural-network quantum state study of the long-range antiferromagnetic Ising chain"></a>Neural-network quantum state study of the long-range antiferromagnetic Ising chain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09709">http://arxiv.org/abs/2308.09709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jicheol Kim, Dongkyu Kim, Dong-Hee Kim</li>
<li>for:  investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions</li>
<li>methods:  using the variational Monte Carlo method with the restricted Boltzmann machine as a trial wave function ansatz</li>
<li>results:  the central charge deviates from 1&#x2F;2 at a small decay exponent $\alpha_\mathrm{LR}$, and the threshold of the Ising universality and the conformal symmetry is estimated to be in the range of $2 \lesssim \alpha_\mathrm{LR} &lt; 3$.<details>
<summary>Abstract</summary>
We investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions by using the variational Monte Carlo method with the restricted Boltzmann machine being employed as a trial wave function ansatz. In the finite-size scaling analysis with the order parameter and the second R\'enyi entropy, we find that the central charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$ in contrast to the critical exponents staying very close to the short-range (SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting the previously proposed scenario of conformal invariance breakdown. To identify the threshold of the Ising universality and the conformal symmetry, we perform two additional tests for the universal Binder ratio and the conformal field theory (CFT) description of the correlation function. It turns out that both indicate a noticeable deviation from the SR Ising class at $\alpha_\mathrm{LR} < 2$. However, a closer look at the scaled correlation function for $\alpha_\mathrm{LR} \ge 2$ shows a gradual change from the asymptotic line of the CFT verified at $\alpha_\mathrm{LR} = 3$, providing a rough estimate of the threshold being in the range of $2 \lesssim \alpha_\mathrm{LR} < 3$.
</details>
<details>
<summary>摘要</summary>
我们调查量子阶段转变在横向离散链磁铁中，使用统计力学 Monte Carlo 方法，将Restricted Boltzmann Machine作为实验波函数拟合。在 finite-size 拓展分析中，我们发现在小数字 $\alpha_\text{LR}$ 下，中心 CHARGE 偏离 1/2，与短距离铁质值不同，支持之前提出的对称性破坏enario。为了识别阶段对称性和对称性破坏的阈值，我们进行了两个额外的测试：一是通用的 Binder 率，二是对称场论 (CFT) 的描述。结果显示，这两个测试都显示在 $\alpha_\text{LR} < 2$ 的情况下，有明显的偏离短距离铁质值。然而，在 $\alpha_\text{LR} \ge 2$ 的情况下，随着测量尺度的增加，扩展函数的演化逐渐变得更加类似 CFT 预测的 asymptotic 线，提供了一个粗略的估计阈值在 $2 \lesssim \alpha_\text{LR} < 3$ 之间。
</details></li>
</ul>
<hr>
<h2 id="Do-you-know-what-q-means"><a href="#Do-you-know-what-q-means" class="headerlink" title="Do you know what q-means?"></a>Do you know what q-means?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09701">http://arxiv.org/abs/2308.09701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: João F. Doriguello, Alessandro Luongo, Ewin Tang</li>
<li>For: 本文提出了一种改进版本的 “$q$-means” 算法，用于实现 Approximate $k$-means  clustering。* Methods: 本算法使用 QRAM 来准备和测量简单的状态，而不需要使用先前的量子线性代数 primitives。* Results: 本算法的时间复杂度为 $O\big(\frac{k^{2}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$，与先前的算法相比，它维持了对 $N$ 的多阶幂度依赖，并改进了大多数其他参数的依赖。 In addition, a “dequantized” classical algorithm for $\varepsilon$-$k$-means is also presented, which runs in $O\big(\frac{k^{2}{\varepsilon^2}(kd + \log(Nd))\big)$ time and matches the polylogarithmic dependence on $N$ attained by the quantum algorithms.<details>
<summary>Abstract</summary>
Clustering is one of the most important tools for analysis of large datasets, and perhaps the most popular clustering algorithm is Lloyd's iteration for $k$-means. This iteration takes $N$ vectors $v_1,\dots,v_N\in\mathbb{R}^d$ and outputs $k$ centroids $c_1,\dots,c_k\in\mathbb{R}^d$; these partition the vectors into clusters based on which centroid is closest to a particular vector. We present an overall improved version of the "$q$-means" algorithm, the quantum algorithm originally proposed by Kerenidis, Landman, Luongo, and Prakash (2019) which performs $\varepsilon$-$k$-means, an approximate version of $k$-means clustering. This algorithm does not rely on the quantum linear algebra primitives of prior work, instead only using its QRAM to prepare and measure simple states based on the current iteration's clusters. The time complexity is $O\big(\frac{k^{2}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$ and maintains the polylogarithmic dependence on $N$ while improving the dependence on most of the other parameters. We also present a "dequantized" algorithm for $\varepsilon$-$k$-means which runs in $O\big(\frac{k^{2}{\varepsilon^2}(kd + \log(Nd))\big)$ time. Notably, this classical algorithm matches the polylogarithmic dependence on $N$ attained by the quantum algorithms.
</details>
<details>
<summary>摘要</summary>
“集群是大规模数据分析中最重要的工具之一，而最受欢迎的集群算法之一是戴尔斯的$k$-means迭代。这个迭代接受$N$个$v_1,\ldots,v_N\in\mathbb{R}^d$的向量，并输出$k$个中心点$c_1,\ldots,c_k\in\mathbb{R}^d$；这些中心点将向量 partition 成 clusters 基于哪个中心点最近于特定向量。我们提出了一个全面改进的 "$q$-means" 算法，即 kerenedis 等人（2019）提出的量子算法，实现 $\varepsilon$-$k$-means，一种 Approximate 版本的 $k$-means 集群算法。这个算法不依赖于量子线性代数基本操作，而只使用其 QRAM 准备和测量简单状态，基于当前迭代的 clusters。时间复杂度为 $O\big(\frac{k^{2}{\varepsilon^2}(\sqrt{k}d + \log(Nd))\big)$，保持了对 $N$ 的多项式依赖，而改善了大多数其他参数的依赖。我们还提出了一个 "dequantized" 算法 для $\varepsilon$-$k$-means，运行时间为 $O\big(\frac{k^{2}{\varepsilon^2}(kd + \log(Nd))\big)$，并且 notable 地与量子算法的多项式依赖相符。”
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection"><a href="#A-Lightweight-Transformer-for-Faster-and-Robust-EBSD-Data-Collection" class="headerlink" title="A Lightweight Transformer for Faster and Robust EBSD Data Collection"></a>A Lightweight Transformer for Faster and Robust EBSD Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09693">http://arxiv.org/abs/2308.09693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hdong920/ebsd_slice_recovery">https://github.com/hdong920/ebsd_slice_recovery</a></li>
<li>paper_authors: Harry Dong, Sean Donegan, Megna Shah, Yuejie Chi</li>
<li>for: 提高三维电子背托干涉diffraction（EBSD）微scopy数据质量</li>
<li>methods: 使用转换器模型和投影算法进行数据处理和恢复</li>
<li>results: 比existings方法更高的数据恢复精度<details>
<summary>Abstract</summary>
Three dimensional electron back-scattered diffraction (EBSD) microscopy is a critical tool in many applications in materials science, yet its data quality can fluctuate greatly during the arduous collection process, particularly via serial-sectioning. Fortunately, 3D EBSD data is inherently sequential, opening up the opportunity to use transformers, state-of-the-art deep learning architectures that have made breakthroughs in a plethora of domains, for data processing and recovery. To be more robust to errors and accelerate this 3D EBSD data collection, we introduce a two step method that recovers missing slices in an 3D EBSD volume, using an efficient transformer model and a projection algorithm to process the transformer's outputs. Overcoming the computational and practical hurdles of deep learning with scarce high dimensional data, we train this model using only synthetic 3D EBSD data with self-supervision and obtain superior recovery accuracy on real 3D EBSD data, compared to existing methods.
</details>
<details>
<summary>摘要</summary>
三维电子反射干扰diffraction（EBSD）顾问是物理科学中多种应用的关键工具，但它的数据质量可能会在收集过程中呈现大幅波动，特别是通过串行分割。幸运的是，3D EBSD数据是顺序的，这为使用变换器，现代深度学习架构，提供了机会。为了更加鲁棒地处理和加速3D EBSD数据收集，我们介绍了一种两步方法，使用高效的变换器模型和投影算法来处理变换器的输出。通过超越深度学习中的计算和实践障碍，我们使用只有自我超vision的synthetic 3D EBSD数据进行训练，并在实际3D EBSD数据上获得了比现有方法更高的恢复精度。
</details></li>
</ul>
<hr>
<h2 id="Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning"><a href="#Reduced-Order-Modeling-of-a-MOOSE-based-Advanced-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning"></a>Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09691">http://arxiv.org/abs/2308.09691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 本研究旨在开发一种高精度但运行速度快的减少级模型（ROM），用于在深度再强化学习（DRL）控制和优化方法中使用。</li>
<li>methods: 本研究使用了运算学（OL）基于方法，可以学习变量的家族方程。在这种情况下，我们使用了傅ри欧姆算法来构建OL-based ROM。</li>
<li>results: 我们对比了OL-based ROM和深度神经网络基于ROM的性能，发现OL-based ROM的性能更高，运行速度更快。<details>
<summary>Abstract</summary>
Advanced Manufacturing (AM) has gained significant interest in the nuclear community for its potential application on nuclear materials. One challenge is to obtain desired material properties via controlling the manufacturing process during runtime. Intelligent AM based on deep reinforcement learning (DRL) relies on an automated process-level control mechanism to generate optimal design variables and adaptive system settings for improved end-product properties. A high-fidelity thermo-mechanical model for direct energy deposition has recently been developed within the MOOSE framework at the Idaho National Laboratory (INL). The goal of this work is to develop an accurate and fast-running reduced order model (ROM) for this MOOSE-based AM model that can be used in a DRL-based process control and optimization method. Operator learning (OL)-based methods will be employed due to their capability to learn a family of differential equations, in this work, produced by changing process variables in the Gaussian point heat source for the laser. We will develop OL-based ROM using Fourier neural operator, and perform a benchmark comparison of its performance with a conventional deep neural network-based ROM.
</details>
<details>
<summary>摘要</summary>
高级生产技术（高级生产）在核心社区引起了广泛的关注，因为它可能用于核材料的制造。一个挑战是通过控制生产过程中的runtime来获得所需的材料性能。基于深度强化学习（DRL）的智能高级生产通过自动化过程级别控制机制来生成优化的设计变量和适应系统设置，以提高终产品的性能。在美国伊达荷大学（INL）的MOOSE框架中，最近已经开发了一个高精度热力学-机械模型，用于直接能量沟入。该工作的目标是开发一个准确快速的减少阶模型（ROM），用于这个MOOSE基于AM模型的DRL控制和优化方法。我们将使用运算学（OL）基本的方法，因为它可以学习一个家族的微分方程，在这里，由变量的改变生成的 Gaussian 点热源中的激光。我们将开发 OL 基本的 ROM 使用 Fourier 神经网络，并对其性能与一个传统的深度神经网络基本的 ROM 进行比较。
</details></li>
</ul>
<hr>
<h2 id="Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models"><a href="#Graph-of-Thoughts-Solving-Elaborate-Problems-with-Large-Language-Models" class="headerlink" title="Graph of Thoughts: Solving Elaborate Problems with Large Language Models"></a>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09687">http://arxiv.org/abs/2308.09687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spcl/graph-of-thoughts">https://github.com/spcl/graph-of-thoughts</a></li>
<li>paper_authors: Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler</li>
<li>for: 提高大语言模型（LLM）的提问能力，超过链条思维或树思维（ToT）的限制。</li>
<li>methods: 利用Graph of Thoughts（GoT）框架，将LLM生成的信息模型为一个任意图，其中单元为“LLM思维”，弧线表示思维之间的依赖关系。这种方法可以将LLM思维合并成 synergistic 结果，提炼整个思维网络的核心，或通过反馈循环进行思维提升。</li>
<li>results: GoT 可以在不同任务上提供优化，例如在排序任务上提高质量62%，同时降低成本&gt;31%。此外，GoT 可以扩展新的思维转换，因此可以用于开拓新的提问方案。这项工作使得 LLM 的思维更加接近人类思维或大脑机制，如回忆、复杂网络等。<details>
<summary>Abstract</summary>
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by >31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.
</details>
<details>
<summary>摘要</summary>
我们介绍Graph of Thoughts（GoT）框架，这种框架可以在大型语言模型（LLM）中提高提示能力，超越链条思维和树思维（ToT）的限制。GoT的关键思想和主要优势在于将LLM生成的信息视为一个可变图形，其中单元为“LLM思维”，而边表示这些单元之间的依赖关系。这种方法允许将任意LLM思维结合成 synergistic 结果，浓缩整个网络思维的核心，或者通过反馈循环进行思维增强。我们示出GoT在不同任务上具有优于现状的优势，例如比ToT提高排序质量62%，同时降低成本>31%。我们还证明GoT可扩展新的思维转换，因此可以用来开拓新的提示方案。这项工作使得LLM的思维更加接近人类思维或脑机制，如回忆、循环等，这些机制都形成复杂的网络。
</details></li>
</ul>
<hr>
<h2 id="Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions"><a href="#Audiovisual-Moments-in-Time-A-Large-Scale-Annotated-Dataset-of-Audiovisual-Actions" class="headerlink" title="Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions"></a>Audiovisual Moments in Time: A Large-Scale Annotated Dataset of Audiovisual Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09685">http://arxiv.org/abs/2308.09685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mjoannou/audiovisual-moments-in-time">https://github.com/mjoannou/audiovisual-moments-in-time</a></li>
<li>paper_authors: Michael Joannou, Pia Rotshtein, Uta Noppeney</li>
<li>for: 这个论文主要目的是提供一个大规模的audiovisual动作事件集（AVMIT），以便用于计算机模型和人类参与者的研究。</li>
<li>methods: 论文使用了一个大规模的annotation任务，从MIT数据集中选择3秒的audiovisual视频，并让11名参与者标注每个试验 whether the labelled audiovisual动作事件是存在的，以及这个事件是视频中最显著的特征。</li>
<li>results: 论文提供了57,177个audiovisual视频的标注，每个视频由3名参与者独立地评估。在这个初始收集中，论文创建了一个精心测试集，包括16种动作类别，每个类别有60个视频（共960个视频）。论文还提供了2个预计算的audiovisual特征嵌入，使用VGGish&#x2F;YamNet和VGG16&#x2F;EfficientNetB0来降低audiovisual DNN研究的入门难度。<details>
<summary>Abstract</summary>
We present Audiovisual Moments in Time (AVMIT), a large-scale dataset of audiovisual action events. In an extensive annotation task 11 participants labelled a subset of 3-second audiovisual videos from the Moments in Time dataset (MIT). For each trial, participants assessed whether the labelled audiovisual action event was present and whether it was the most prominent feature of the video. The dataset includes the annotation of 57,177 audiovisual videos, each independently evaluated by 3 of 11 trained participants. From this initial collection, we created a curated test set of 16 distinct action classes, with 60 videos each (960 videos). We also offer 2 sets of pre-computed audiovisual feature embeddings, using VGGish/YamNet for audio data and VGG16/EfficientNetB0 for visual data, thereby lowering the barrier to entry for audiovisual DNN research. We explored the advantages of AVMIT annotations and feature embeddings to improve performance on audiovisual event recognition. A series of 6 Recurrent Neural Networks (RNNs) were trained on either AVMIT-filtered audiovisual events or modality-agnostic events from MIT, and then tested on our audiovisual test set. In all RNNs, top 1 accuracy was increased by 2.71-5.94\% by training exclusively on audiovisual events, even outweighing a three-fold increase in training data. We anticipate that the newly annotated AVMIT dataset will serve as a valuable resource for research and comparative experiments involving computational models and human participants, specifically when addressing research questions where audiovisual correspondence is of critical importance.
</details>
<details>
<summary>摘要</summary>
我们介绍了听视频时刻事件（AVMIT）数据集，这是一个大规模的听视频动作事件数据集。在一项广泛的标注任务中，11名参与者标注了MIT数据集中的3秒听视频示例的一部分。每个试验中，参与者评估了听视频动作事件是否存在，以及它是视频中最为出色的特征。该数据集包括57,177个听视频示例，每个示例由3名训练过的参与者独立地评估。从这个初始收集中，我们创建了一个精心选择的测试集，包括16种不同的动作类别，每个类别有60个视频示例（共960个视频示例）。我们还提供了两个预计算的听视频特征嵌入，使用VGGish/YamNet对音频数据进行预处理，并使用VGG16/EfficientNetB0对视频数据进行预处理，从而降低了听视频DNN研究的门槛。我们评估了AVMIT标注和特征嵌入的优势，以提高听视频事件认识性能。我们使用6个回归神经网络（RNN）在AVMIT filtered audiovisual事件或MIT模态不同的事件上训练，然后在我们的听视频测试集上进行测试。在所有RNN中，只训练在 audiovisual事件上的总准确率提高了2.71-5.94%，甚至超过了三倍增加的训练数据量。我们预计这些新的AVMIT标注数据集将成为未来研究和比较实验中的重要资源，特别是在研究问题中，听视频匹配的重要性很高。
</details></li>
</ul>
<hr>
<h2 id="Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states"><a href="#Variational-optimization-of-the-amplitude-of-neural-network-quantum-many-body-ground-states" class="headerlink" title="Variational optimization of the amplitude of neural-network quantum many-body ground states"></a>Variational optimization of the amplitude of neural-network quantum many-body ground states</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09664">http://arxiv.org/abs/2308.09664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Qi Wang, Rong-Qiang He, Zhong-Yi Lu</li>
<li>for: 这篇论文旨在探讨基于神经网络的量子多体ground state搜索方法，并对其进行优化。</li>
<li>methods: 该方法将量子多体变量波函数分解为一个实值神经网络和一个固定的符号结构，然后优化神经网络。神经网络使用了卷积层和径向层，即ResNet。</li>
<li>results: 该方法在三个典型量子多体系统上进行测试，并与传统的变量 Monte Carlo（VMC）和密度矩阵约束 груп（DMRG）方法进行比较。结果显示，对于受挫的Heisenberg $J_1$-$J_2$模型，该方法的结果更好于文献中的复杂值神经网络，表明了 sign structure的优化是困难的。将在未来研究 sign structure的优化。<details>
<summary>Abstract</summary>
Neural-network quantum states (NQSs), variationally optimized by combining traditional methods and deep learning techniques, is a new way to find quantum many-body ground states and gradually becomes a competitor of traditional variational methods. However, there are still some difficulties in the optimization of NQSs, such as local minima, slow convergence, and sign structure optimization. Here, we split a quantum many-body variational wave function into a multiplication of a real-valued amplitude neural network and a sign structure, and focus on the optimization of the amplitude network while keeping the sign structure fixed. The amplitude network is a convolutional neural network (CNN) with residual blocks, namely a ResNet. Our method is tested on three typical quantum many-body systems. The obtained ground state energies are lower than or comparable to those from traditional variational Monte Carlo (VMC) methods and density matrix renormalization group (DMRG). Surprisingly, for the frustrated Heisenberg $J_1$-$J_2$ model, our results are better than those of the complex-valued CNN in the literature, implying that the sign structure of the complex-valued NQS is difficult to be optimized. We will study the optimization of the sign structure of NQSs in the future.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, but the word order and sentence structure may be different from the original text.)
</details></li>
</ul>
<hr>
<h2 id="GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction"><a href="#GiGaMAE-Generalizable-Graph-Masked-Autoencoder-via-Collaborative-Latent-Space-Reconstruction" class="headerlink" title="GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction"></a>GiGaMAE: Generalizable Graph Masked Autoencoder via Collaborative Latent Space Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09663">http://arxiv.org/abs/2308.09663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sycny/gigamae">https://github.com/sycny/gigamae</a></li>
<li>paper_authors: Yucheng Shi, Yushun Dong, Qiaoyu Tan, Jundong Li, Ninghao Liu</li>
<li>for: 这篇论文的目的是提出一个基于自动encoder的自愿式学习框架，以解决现有masked autoencoder模型在graph数据上的普遍化能力不足问题。</li>
<li>methods: 这篇论文提出了一个名为GiGaMAE的新型graph masked autoencoder框架，不同于现有的masked autoencoder模型，这里的模型不是直接从原始graph中重建graph的component（例如特征或边），而是将graph的topology和 attribute信息视为重建目标，以capture更广泛和全面的知识。此外， authors也引入了一个基于mutual information的重建损失函数，这使得模型能够有效地重建多个目标。</li>
<li>results: 在七个标准 benchmark上进行了广泛的实验，结果显示了 GiGaMAE 在三个下游任务上的超越性。 authors hope 这些结果能够照明 foundation models 的设计在graph-structured data上。<details>
<summary>Abstract</summary>
Self-supervised learning with masked autoencoders has recently gained popularity for its ability to produce effective image or textual representations, which can be applied to various downstream tasks without retraining. However, we observe that the current masked autoencoder models lack good generalization ability on graph data. To tackle this issue, we propose a novel graph masked autoencoder framework called GiGaMAE. Different from existing masked autoencoders that learn node presentations by explicitly reconstructing the original graph components (e.g., features or edges), in this paper, we propose to collaboratively reconstruct informative and integrated latent embeddings. By considering embeddings encompassing graph topology and attribute information as reconstruction targets, our model could capture more generalized and comprehensive knowledge. Furthermore, we introduce a mutual information based reconstruction loss that enables the effective reconstruction of multiple targets. This learning objective allows us to differentiate between the exclusive knowledge learned from a single target and common knowledge shared by multiple targets. We evaluate our method on three downstream tasks with seven datasets as benchmarks. Extensive experiments demonstrate the superiority of GiGaMAE against state-of-the-art baselines. We hope our results will shed light on the design of foundation models on graph-structured data. Our code is available at: https://github.com/sycny/GiGaMAE.
</details>
<details>
<summary>摘要</summary>
自我超级学习中的假设掩码自动机（Masked Autoencoder，MAE）在图像或文本表示方面具有出色的表达能力，可以应用于多个下游任务无需重新训练。然而，我们发现当前的假设掩码模型对图数据的泛化能力不足。为解决这个问题，我们提出了一种新的图Masked Autoencoder框架，即GiGaMAE。与现有的假设掩码模型不同，我们在这篇论文中提议通过同时重建图像的整体特征和属性信息来学习图像的表示。这使得我们的模型能够捕捉更加普遍和全面的知识。此外，我们引入了基于缺失信息的重建损失函数，这使得我们的模型能够有效地重建多个目标。我们在三个下游任务上进行了七个数据集的测试，并证明了GiGaMAE在比基eline上表现出色。我们希望我们的结果能够为图数据基础模型的设计提供指导。我们的代码可以在https://github.com/sycny/GiGaMAE中找到。
</details></li>
</ul>
<hr>
<h2 id="Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction"><a href="#Robust-Uncertainty-Quantification-using-Conformalised-Monte-Carlo-Prediction" class="headerlink" title="Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction"></a>Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09647">http://arxiv.org/abs/2308.09647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/team-daniel/mc-cp">https://github.com/team-daniel/mc-cp</a></li>
<li>paper_authors: Daniel Bethell, Simos Gerasimou, Radu Calinescu</li>
<li>for: 这 paper 是为了提供一种可靠的深度学习模型部署方法，以满足安全关键应用的需求。</li>
<li>methods: 这 paper 使用了一种新的 MC 采样 dropout 方法，以及一种基于 CP 的抗随机变量预测方法。这两种方法在运行时可以相互协作，以提高预测的可靠性和精度。</li>
<li>results: 通过对多种分类和回归 benchmark 进行了广泛的实验，这 paper 显示了 MC-CP 方法在对 uncertainty quantification 方面的显著改进，比如 MC dropout、RAPS 和 CQR 等先进方法。MC-CP 方法可以轻松地与现有模型结合使用，从而使其的部署变得更加简单。<details>
<summary>Abstract</summary>
Deploying deep learning models in safety-critical applications remains a very challenging task, mandating the provision of assurances for the dependable operation of these models. Uncertainty quantification (UQ) methods estimate the model's confidence per prediction, informing decision-making by considering the effect of randomness and model misspecification. Despite the advances of state-of-the-art UQ methods, they are computationally expensive or produce conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ method that combines a new adaptive Monte Carlo (MC) dropout method with conformal prediction (CP). MC-CP adaptively modulates the traditional MC dropout at runtime to save memory and computation resources, enabling predictions to be consumed by CP, yielding robust prediction sets/intervals. Throughout comprehensive experiments, we show that MC-CP delivers significant improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in classification and regression benchmarks. MC-CP can be easily added to existing models, making its deployment simple.
</details>
<details>
<summary>摘要</summary>
部署深度学习模型在安全关键应用中仍然是一项非常具有挑战性的任务，需要提供对模型可靠性的保证。不确定量评估（UQ）方法估算模型每次预测的可靠程度，以帮助决策，考虑随机性和模型误差的影响。 despite state-of-the-art UQ 方法的进步，它们可能 computationally expensive 或生成保守的预测集/interval。我们介绍 MC-CP，一种新的hybrid UQ 方法，将新的适应MC dropout 方法与 confirmal prediction 结合。 MC-CP 在运行时动态调整传统MC dropout，以避免占用内存和计算资源，使预测可以被CP 处理，生成 robust预测集/interval。经过了广泛的实验，我们表明 MC-CP 在分类和回归 benchmark 中具有显著的改进，比如 MC dropout、RAPS 和 CQR。 MC-CP 可以轻松地添加到现有模型中，使其的部署变得简单。
</details></li>
</ul>
<hr>
<h2 id="biquality-learn-a-Python-library-for-Biquality-Learning"><a href="#biquality-learn-a-Python-library-for-Biquality-Learning" class="headerlink" title="biquality-learn: a Python library for Biquality Learning"></a>biquality-learn: a Python library for Biquality Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09643">http://arxiv.org/abs/2308.09643</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biquality-learn/biquality-learn">https://github.com/biquality-learn/biquality-learn</a></li>
<li>paper_authors: Pierre Nodet, Vincent Lemaire, Alexis Bondu, Antoine Cornuéjols</li>
<li>For: The paper aims to address the challenges of weak supervision and dataset shifts in machine learning, and proposes a new framework called Biquality Learning.* Methods: The paper proposes a Python library called biquality-learn, which provides a consistent and intuitive API for learning machine learning models from biquality data. The library includes well-proven algorithms and is designed to be accessible and easy to use for everyone.* Results: The paper enables researchers to experiment in a reproducible way on biquality data, and demonstrates the effectiveness of the proposed framework through experiments on several benchmark datasets.<details>
<summary>Abstract</summary>
The democratization of Data Mining has been widely successful thanks in part to powerful and easy-to-use Machine Learning libraries. These libraries have been particularly tailored to tackle Supervised Learning. However, strong supervision signals are scarce in practice, and practitioners must resort to weak supervision. In addition to weaknesses of supervision, dataset shifts are another kind of phenomenon that occurs when deploying machine learning models in the real world. That is why Biquality Learning has been proposed as a machine learning framework to design algorithms capable of handling multiple weaknesses of supervision and dataset shifts without assumptions on their nature and level by relying on the availability of a small trusted dataset composed of cleanly labeled and representative samples. Thus we propose biquality-learn: a Python library for Biquality Learning with an intuitive and consistent API to learn machine learning models from biquality data, with well-proven algorithms, accessible and easy to use for everyone, and enabling researchers to experiment in a reproducible way on biquality data.
</details>
<details>
<summary>摘要</summary>
“数据挖掘的民主化得到了广泛的成功，很大的帮助来自于强大且易用的机器学习库。这些库主要针对超级vised学习。然而，实际中强制督学信号强度很弱，实际operator需要采用弱督学。此外，在机器学习模型实际部署时，数据变化也是一种常见的现象。为此，我们提出了一种机器学习框架---多质量学习（Biquality Learning），旨在设计能够处理多种弱督学和数据变化的算法，不假设它们的性质和水平。我们提出了biquality-learn：一个Python库，用于多质量学习，具有直观和一致的API，可以从多质量数据中学习机器学习模型，具有证明过的算法、易于使用、可重复地进行研究。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/cs.LG_2023_08_19/" data-id="clpztdnkl00qaes88bnjt0fhy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/19/eess.IV_2023_08_19/" class="article-date">
  <time datetime="2023-08-19T09:00:00.000Z" itemprop="datePublished">2023-08-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/19/eess.IV_2023_08_19/">eess.IV - 2023-08-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CRC-ICM-Colorectal-Cancer-Immune-Cell-Markers-Pattern-Dataset"><a href="#CRC-ICM-Colorectal-Cancer-Immune-Cell-Markers-Pattern-Dataset" class="headerlink" title="CRC-ICM: Colorectal Cancer Immune Cell Markers Pattern Dataset"></a>CRC-ICM: Colorectal Cancer Immune Cell Markers Pattern Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10033">http://arxiv.org/abs/2308.10033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Mokhtari, Elham Amjadi, Hamidreza Bolhasani, Zahra Faghih, AmirReza Dehghanian, Marzieh Rezaei</li>
<li>for: This paper aims to investigate the differences in immune checkpoint expression between right and left colon cancer, and to identify potential biomarkers for immunotherapy.</li>
<li>methods: The study uses a dataset of 1756 images from 136 patients with colorectal cancer, stained with specific antibodies for CD3, CD8, CD45RO, PD-1, LAG3, and Tim3.</li>
<li>results: The paper finds that there are differences in immune checkpoint expression between right and left colon cancer, and identifies potential biomarkers for immunotherapy.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是调查大小colon癌的右和左两侧免疫检查点表达的差异，并找到可能的免疫治疗标记物。</li>
<li>methods: 这篇论文使用136名患有colon癌的患者的1756张图像数据集，用特定抗体染色CD3、CD8、CD45RO、PD-1、LAG3和Tim3等。</li>
<li>results: 论文发现右和左两侧colon癌免疫检查点表达有差异，并确定了可能的免疫治疗标记物。<details>
<summary>Abstract</summary>
Colorectal Cancer (CRC) is the second most common cause of cancer death in the world, ad can be identified by the location of the primary tumor in the large intestine: right and left colon, and rectum. Based on the location, CRC shows differences in chromosomal and molecular characteristics, microbiomes incidence, pathogenesis, and outcome. It has been shown that tumors on left and right sides also have different immune landscape, so the prognosis may be different based on the primary tumor locations. It is widely accepted that immune components of the tumor microenvironment (TME) plays a critical role in tumor development. One of the critical regulatory molecules in the TME is immune checkpoints that as the gatekeepers of immune responses regulate the infiltrated immune cell functions. Inhibitory immune checkpoints such as PD-1, Tim3, and LAG3, as the main mechanism of immune suppression in TME overexpressed and result in further development of the tumor. The images of this dataset have been taken from colon tissues of patients with CRC, stained with specific antibodies for CD3, CD8, CD45RO, PD-1, LAG3 and Tim3. The name of this dataset is CRC-ICM and contains 1756 images related to 136 patients. The initial version of CRC-ICM is published on Elsevier Mendeley dataset portal, and the latest version is accessible via: https://databiox.com
</details>
<details>
<summary>摘要</summary>
抗colon癌（CRC）是全球第二常见的癌症死亡原因，可以根据主 tumor 的位置在大肠中进行分类：右colon、左colon 和肛门。根据位置，CRC 会有不同的染色体和分子特征、微生物发生率、生物学过程和结果。有证据显示左和右方主 tumor 的免疫环境有所不同，因此预后可能因主 tumor 的位置而异。免疫组件在肿瘤微环境（TME）中扮演了重要的角色，并且免疫检查点（IC）是免疫回应的关键调节器。对 TME 中的免疫检查点进行抑制可以导致肿瘤的进一步发展。这些数据库包括1756幅图像，来自于136名患有CRC的病人的大肠标本，已经使用特定抗体进行染色，包括CD3、CD8、CD45RO、PD-1、LAG3 和 Tim3。这个数据库名为 CRC-ICM，可以在 Elsevier Mendeley 数据库 порталу上获取，或者通过以下连结：https://databiox.com。
</details></li>
</ul>
<hr>
<h2 id="Deformable-Detection-Transformer-for-Microbubble-Localization-in-Ultrasound-Localization-Microscopy"><a href="#Deformable-Detection-Transformer-for-Microbubble-Localization-in-Ultrasound-Localization-Microscopy" class="headerlink" title="Deformable-Detection Transformer for Microbubble Localization in Ultrasound Localization Microscopy"></a>Deformable-Detection Transformer for Microbubble Localization in Ultrasound Localization Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09845">http://arxiv.org/abs/2308.09845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sepideh K. Gharamaleki, Brandon Helfield, Hassan Rivaz</li>
<li>for: This paper aims to improve the localization of microbubbles (MBs) in ultrasound imaging, which is limited by the half-wavelength resolution of the imaging modality.</li>
<li>methods: The proposed method, DEformable DETR (DE-DETR), uses a multi-scale deformable attention mechanism to distribute attention within a limited budget, improving upon the conventional DETR approach which casts attention upon all grid pixels.</li>
<li>results: The proposed DE-DETR method shows improvement in both precision and recall, as well as the final super-resolution maps, compared to the conventional DETR method, when applied to the task of MB localization in ultrasound imaging.<details>
<summary>Abstract</summary>
To overcome the half a wavelength resolution limitations of ultrasound imaging, microbubbles (MBs) have been utilized widely in the field. Conventional MB localization methods are limited whether by exhaustive parameter tuning or considering a fixed Point Spread Function (PSF) for MBs. This questions their adaptability to different imaging settings or depths. As a result, development of methods that don't rely on manually adjusted parameters is crucial. Previously, we used a transformer-based approach i.e. DEtection TRansformer (DETR) (arXiv:2005.12872v3 and arXiv:2209.11859v1) to address the above mentioned issues. However, DETR suffers from long training times and lower precision for smaller objects. In this paper, we propose the application of DEformable DETR (DE-DETR) ( arXiv:2010.04159) for MB localization to mitigate DETR's above mentioned challenges. As opposed to DETR, where attention is casted upon all grid pixels, DE-DETR utilizes a multi-scale deformable attention to distribute attention within a limited budget. To evaluate the proposed strategy, pre-trained DE-DETR was fine-tuned on a subset of the dataset provided by the IEEE IUS Ultra-SR challenge organizers using transfer learning principles and subsequently we tested the network on the rest of the dataset, excluding the highly correlated frames. The results manifest an improvement both in precision and recall and the final super-resolution maps compared to DETR.
</details>
<details>
<summary>摘要</summary>
为了超越ultrasound imaging中半波长resolution的限制，广泛使用了微气泡（MBs）。传统的MBlocalization方法受限于手动调整的参数或者considering a fixed Point Spread Function（PSF）for MBs。这问题了其适应不同的imaging setting或深度。因此，开发不依赖于手动调整参数的方法是关键。在过去，我们使用了transformer-basedapproach，即DEtection TRansformer（DETR）（arXiv:2005.12872v3和arXiv:2209.11859v1）来解决上述问题。然而，DETR受到训练时间过长和对小对象的精度较低的问题。在这篇论文中，我们提议使用DEformable DETR（DE-DETR）（arXiv:2010.04159）来进行MB localization，以mitigate DETR的上述问题。与DETR不同，DE-DETR使用多尺度可变注意力来分配注意力，而不是对所有的网格像素进行注意力投入。为评估提议的策略，我们先在一个IEEE IUS Ultra-SR challenge提供的数据集上使用了转移学习原理进行先training DE-DETR，然后对剩下的数据集进行测试，排除高相关性的帧。结果显示，与DETR相比，DE-DETR在精度和准确性方面具有显著改进，并且最终的超高分辨率图像也得到了改进。
</details></li>
</ul>
<hr>
<h2 id="Cross-modality-Attention-based-Multimodal-Fusion-for-Non-small-Cell-Lung-Cancer-NSCLC-Patient-Survival-Prediction"><a href="#Cross-modality-Attention-based-Multimodal-Fusion-for-Non-small-Cell-Lung-Cancer-NSCLC-Patient-Survival-Prediction" class="headerlink" title="Cross-modality Attention-based Multimodal Fusion for Non-small Cell Lung Cancer (NSCLC) Patient Survival Prediction"></a>Cross-modality Attention-based Multimodal Fusion for Non-small Cell Lung Cancer (NSCLC) Patient Survival Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09831">http://arxiv.org/abs/2308.09831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruining Deng, Nazim Shaikh, Gareth Shannon, Yao Nie</li>
<li>for: 预测和诊断结果提供估计，用于评估治疗效果和对患者分组。</li>
<li>methods: 跨Modalities的注意力基本多模态融合策略，通过把不同模态特征融合以提高计算机辅助诊断和预测的性能。</li>
<li>results: 在非小细胞肺癌（NSCLC）患者存活预测 task 中，提出的融合策略比单 modal 学习提高了性能，c-index 从 0.5772 和 0.5885 提高到 0.6587。<details>
<summary>Abstract</summary>
Cancer prognosis and survival outcome predictions are crucial for therapeutic response estimation and for stratifying patients into various treatment groups. Medical domains concerned with cancer prognosis are abundant with multiple modalities, including pathological image data and non-image data such as genomic information. To date, multimodal learning has shown potential to enhance clinical prediction model performance by extracting and aggregating information from different modalities of the same subject. This approach could outperform single modality learning, thus improving computer-aided diagnosis and prognosis in numerous medical applications. In this work, we propose a cross-modality attention-based multimodal fusion pipeline designed to integrate modality-specific knowledge for patient survival prediction in non-small cell lung cancer (NSCLC). Instead of merely concatenating or summing up the features from different modalities, our method gauges the importance of each modality for feature fusion with cross-modality relationship when infusing the multimodal features. Compared with single modality, which achieved c-index of 0.5772 and 0.5885 using solely tissue image data or RNA-seq data, respectively, the proposed fusion approach achieved c-index 0.6587 in our experiment, showcasing the capability of assimilating modality-specific knowledge from varied modalities.
</details>
<details>
<summary>摘要</summary>
cancer 诊断和生存结果预测是致命的，它们对于治疗响应的估计和患者分配到不同的治疗组有重要作用。医疗领域对抗癌症的诊断和预测充满多种多样的数据，包括生物像数据和非生物像数据，如基因信息。到目前为止，多模态学习已经显示出了提高临床预测模型性能的潜力，通过提取和综合不同模态的信息。这种方法可能超越单模态学习，从而改善计算机辅助诊断和预测在各种医疗应用中的性能。在这项工作中，我们提出了一种跨模态注意力基于多模态融合管道，用于整合不同模态知识以提高患者存活预测的准确性。不同于将各模态特征直接拼接或加权平均，我们的方法评估每个模态对于特征融合的重要性，并在融合多模态特征时进行跨模态关系的权重评估。与单模态学习相比，我们的融合方法在实验中达到了c-指数0.6587，这说明了我们能够充分利用不同模态之间的关系，从而提高预测的准确性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/19/eess.IV_2023_08_19/" data-id="clpztdns0019fes888xo258x4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.SD_2023_08_18/" class="article-date">
  <time datetime="2023-08-18T15:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.SD_2023_08_18/">cs.SD - 2023-08-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks"><a href="#Compensating-Removed-Frequency-Components-Thwarting-Voice-Spectrum-Reduction-Attacks" class="headerlink" title="Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks"></a>Compensating Removed Frequency Components: Thwarting Voice Spectrum Reduction Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09546">http://arxiv.org/abs/2308.09546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shu Wang, Kun Sun, Qi Li<br>for:This paper aims to address the challenge of detecting harmful content in audio and video available on social media platforms, specifically the vulnerability of automatic speech recognition (ASR) systems to spectrum reduction attacks.methods:The proposed solution is an acoustic compensation system named ACE, which leverages two key observations: frequency component dependencies and perturbation sensitivity. ACE uses a combination of frequency-based compensation and over-the-air perturbations to counter the spectrum reduction attacks and improve the accuracy of ASR systems.results:The experiments show that ACE can effectively reduce up to 87.9% of ASR inference errors caused by spectrum reduction attacks. Additionally, the paper identifies six general types of ASR inference errors and investigates their causes and potential mitigation solutions.<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) provides diverse audio-to-text services for humans to communicate with machines. However, recent research reveals ASR systems are vulnerable to various malicious audio attacks. In particular, by removing the non-essential frequency components, a new spectrum reduction attack can generate adversarial audios that can be perceived by humans but cannot be correctly interpreted by ASR systems. It raises a new challenge for content moderation solutions to detect harmful content in audio and video available on social media platforms. In this paper, we propose an acoustic compensation system named ACE to counter the spectrum reduction attacks over ASR systems. Our system design is based on two observations, namely, frequency component dependencies and perturbation sensitivity. First, since the Discrete Fourier Transform computation inevitably introduces spectral leakage and aliasing effects to the audio frequency spectrum, the frequency components with similar frequencies will have a high correlation. Thus, considering the intrinsic dependencies between neighboring frequency components, it is possible to recover more of the original audio by compensating for the removed components based on the remaining ones. Second, since the removed components in the spectrum reduction attacks can be regarded as an inverse of adversarial noise, the attack success rate will decrease when the adversarial audio is replayed in an over-the-air scenario. Hence, we can model the acoustic propagation process to add over-the-air perturbations into the attacked audio. We implement a prototype of ACE and the experiments show ACE can effectively reduce up to 87.9% of ASR inference errors caused by spectrum reduction attacks. Also, by analyzing residual errors, we summarize six general types of ASR inference errors and investigate the error causes and potential mitigation solutions.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统提供了多种语音到文本服务，帮助人类与机器进行交互。然而，最近的研究发现，ASR系统受到了多种黑客音频攻击。具体来说，通过删除不必要的频率组成部分，可以生成针对ASR系统的恶意音频，这些音频可以被人类听到，但是无法正确地被ASR系统识别。这种攻击提高了社交媒体平台上的内容审核解决方案的挑战。在这篇论文中，我们提出了一种名为ACE的听音补偿系统，以防止频谱减少攻击。我们的系统设计基于两点观察：一是频谱成分之间的相互依赖关系，二是对于攻击音频的扰动敏感性。首先，由于计算Discrete Fourier Transform时必然存在频谱泄漏和射频效应，因此频谱中的相关频率成分会具有高相关性。因此，通过考虑频谱成分之间的内在相互依赖关系，可以通过补偿被删除的频率成分来恢复更多的原始音频。其次，由于攻击音频中删除的频率成分可以看作是对抗噪声的逆，因此在通过空中传播的情况下重新播放攻击音频时，攻击成功率将下降。因此，我们可以模拟听音传播过程，将攻击音频中的频率成分加上了空中传播的扰动。我们实现了ACE的原型，实验结果表明，ACE可以效果地减少ASR推理错误率达87.9%。此外，通过分析剩余错误，我们总结了六种常见的ASR推理错误类型，并分析了错误的原因和可能的修复方案。
</details></li>
</ul>
<hr>
<h2 id="Generative-Machine-Listener"><a href="#Generative-Machine-Listener" class="headerlink" title="Generative Machine Listener"></a>Generative Machine Listener</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09493">http://arxiv.org/abs/2308.09493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanxin Jiang, Lars Villemoes, Arijit Biswas</li>
<li>for: 这个论文用于训练神经网络来预测每对参照和编码声音信号的分布。</li>
<li>methods: 这个方法使用个人侵入式听力测试成绩来训练神经网络，我们称之为生成机器听众（GML）。</li>
<li>results: 与基准系统使用平均分数 regression 相比，我们 observe 下降的外围比率（OR），并可以轻松地预测 confidence interval（CI）。 数据增强技术的引入导致 CI 预测准确率和平均分数预测准确率的提高。<details>
<summary>Abstract</summary>
We show how a neural network can be trained on individual intrusive listening test scores to predict a distribution of scores for each pair of reference and coded input stereo or binaural signals. We nickname this method the Generative Machine Listener (GML), as it is capable of generating an arbitrary amount of simulated listening test data. Compared to a baseline system using regression over mean scores, we observe lower outlier ratios (OR) for the mean score predictions, and obtain easy access to the prediction of confidence intervals (CI). The introduction of data augmentation techniques from the image domain results in a significant increase in CI prediction accuracy as well as Pearson and Spearman rank correlation of mean scores.
</details>
<details>
<summary>摘要</summary>
我们展示了一个神经网络可以在个别关注听力测验成绩上训练，以预测每对参考和编码听力信号的分布。我们称这为“生成机器听者”（GML），因为它可以生成无限量的模拟听力测验数据。相比基准系统使用平均值 regression，我们观察到下峰值値（OR）较低，并可以轻松地预测信号interval prediction（CI）。对于数据增强技术的引入，从影像领域的数据增强技术导致了预测CI的准确性和平均分数和Speedman排名相互联系的提高。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model"><a href="#Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model" class="headerlink" title="Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model"></a>Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09454">http://arxiv.org/abs/2308.09454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Rose Bjare, Stefan Lattner, Gerhard Widmer</li>
<li>for: 这个研究旨在investigate the impact of different sampling techniques on musical qualities such as diversity and structure in natural language processing.</li>
<li>methods: 作者使用了高容量transformer模型，并使用分布 truncation sampling techniques，包括nucleus sampling、”typical sampling”和conventional ancestral sampling，以评估这些抽样策略对音乐质量的影响。</li>
<li>results: 研究发现，probability truncation techniques可能会限制优 optimal circumstances中的多样性和结构性，但在suboptimal circumstances中可能生成更多的音乐样本。<details>
<summary>Abstract</summary>
Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed "typical sampling", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.
</details>
<details>
<summary>摘要</summary>
研究自然语言处理已经证明，训练过程中使用的采样策略会对生成的质量产生重要影响。在这个研究中，我们研究了不同采样技术对音乐质量的影响，特别是多样性和结构。为此，我们训练了一个高容量变换器模型，并使用分布截断采样技术来分析生成的样本。我们使用核心采样、“典型采样”和传统祖先采样三种采样技术，并在优化和不优化情况下进行评估。我们使用对象和主观评估来评估生成的样本质量。我们发现，概率截断技术可能会在优化情况下减少多样性和结构，但在不优化情况下可能会生成更多的音乐样本。
</details></li>
</ul>
<hr>
<h2 id="TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition"><a href="#TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition" class="headerlink" title="TrOMR:Transformer-Based Polyphonic Optical Music Recognition"></a>TrOMR:Transformer-Based Polyphonic Optical Music Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09370">http://arxiv.org/abs/2308.09370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/netease/polyphonic-tromr">https://github.com/netease/polyphonic-tromr</a></li>
<li>paper_authors: Yixuan Li, Huaping Liu, Qiang Jin, Miaomiao Cai, Peng Li</li>
<li>for: 这篇论文旨在提出一种基于变换器的全音频乐识别方法（TrOMR），以提高现实世界场景中的识别精度。</li>
<li>methods: 该方法使用变换器来实现全音频乐识别，并引入了一种新的一致损失函数和合理的数据注释方法来提高识别精度。</li>
<li>results: 广泛的实验表明，TrOMR方法在现实世界场景中比现有的OMR方法高效，特别是对于复杂的乐谱。此外， authors还开发了一个 TrOMR 系统和一个实验室场景数据集，以便进行全面的评估和复制。<details>
<summary>Abstract</summary>
Optical Music Recognition (OMR) is an important technology in music and has been researched for a long time. Previous approaches for OMR are usually based on CNN for image understanding and RNN for music symbol classification. In this paper, we propose a transformer-based approach with excellent global perceptual capability for end-to-end polyphonic OMR, called TrOMR. We also introduce a novel consistency loss function and a reasonable approach for data annotation to improve recognition accuracy for complex music scores. Extensive experiments demonstrate that TrOMR outperforms current OMR methods, especially in real-world scenarios. We also develop a TrOMR system and build a camera scene dataset for full-page music scores in real-world. The code and datasets will be made available for reproducibility.
</details>
<details>
<summary>摘要</summary>
依靠视觉技术的音乐识别（OMR）已经是音乐领域的一项重要技术，已经有很长时间的研究。过去的approach通常基于CNN来理解图像和RNN来分类音乐符号。在这篇论文中，我们提出一种基于转换器的方法，具有出色的全球感知能力，用于端到端多重音乐OMR，称为TrOMR。我们还介绍了一种新的一致性损失函数和合理的数据注释方法，以提高复杂音乐分页的识别精度。广泛的实验表明，TrOMR在现实世界中比现有的OMR方法高效，特别是在复杂音乐分页中。我们还开发了TrOMR系统，并建立了真实世界中的摄像头场景数据集，用于全页音乐分页。代码和数据集将被公开，以便重现。
</details></li>
</ul>
<hr>
<h2 id="Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge"><a href="#Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge" class="headerlink" title="Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge"></a>Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09311">http://arxiv.org/abs/2308.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Yong Man Ro</li>
<li>for: 提高低资源语言 lip reading 模型的开发，通过学习通用语言知识和语言特定知识来提高模型的表现。</li>
<li>methods: 使用高资源语言的speech unit prediction来学习通用语言知识，然后使用Language-specific Memory-augmented Decoder (LMDecoder)来学习语言特定知识。</li>
<li>results: 通过对五种语言（英语、西班牙语、法语、意大利语和葡萄牙语）进行了广泛的实验，证明提案的方法可以有效地提高lip reading模型的表现。<details>
<summary>Abstract</summary>
This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowledge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Spanish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms"><a href="#Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms" class="headerlink" title="Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms"></a>Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09302">http://arxiv.org/abs/2308.09302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ph-w2000/s2pecnet">https://github.com/ph-w2000/s2pecnet</a></li>
<li>paper_authors: Penghui Wen, Kun Hu, Wenxi Yue, Sen Zhang, Wanlei Zhou, Zhiyong Wang</li>
<li>for: 防止深度伪装技术所带来的伪声辨识攻击</li>
<li>methods: 使用多项 spectral pattern 融合 deep learning 方法，包括 coarse-to-fine 组合和 fine-level 组合两种分支</li>
<li>results: 在 ASVspoof2019 LA Challenge 上实现了顶尖性能，EER 为 0.77%<details>
<summary>Abstract</summary>
Robust audio anti-spoofing has been increasingly challenging due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti-spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion-reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spectrograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset: ASVspoof2019 LA Challenge.
</details>
<details>
<summary>摘要</summary>
受深圳技术的提高影响，Robust audio anti-spoofing已经变得越来越困难。虽然spectrograms已经表现出了抗假技术的能力，但多个 spectral pattern的信息还没有得到充分利用，这限制了它们对不同的假攻击的效iveness。因此，我们提出了一种基于深度学习的新方法，即S2pecNet，用于利用多个 spectral pattern来获得robust audio anti-spoofing表示。特别是，至第二顺序的spectral pattern被在粗糙到细节的方式进行融合，并设计了两个支线来从spectral和时间上下文中进行细节级别的融合。再次从融合表示中重建输入spectrograms可以减少潜在的融合信息损失。我们的方法在ASVspoof2019 LA Challenge上 achieved state-of-the-art performance，EER为0.77%。
</details></li>
</ul>
<hr>
<h2 id="V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models"><a href="#V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models" class="headerlink" title="V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models"></a>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09300">http://arxiv.org/abs/2308.09300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, Weidong Cai</li>
<li>for: 本研究强调使用基础模型（FM）来解决跨模态生成问题，具体来说是将视觉输入转化为听取输出。</li>
<li>methods: 该研究使用CLIP、CLAP和AudioLDM三个基础模型，通过设计一种简单 yet有效的映射机制（V2A-Mapper）来 bridge the domain gap，并使用预训练的听取生成FM AudioLDM生成高质量的听取输出。</li>
<li>results: 对比现有方法，该方法需要较少的训练参数（86%），但能够提高FD和CS两个评价指标的表现，具体来说是提高了53%和19%。<details>
<summary>Abstract</summary>
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.
</details>
<details>
<summary>摘要</summary>
Generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. Existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM.We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then, we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound.Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches. We trained with 86% fewer parameters but achieved 53% and 19% improvement in FD and CS, respectively.
</details></li>
</ul>
<hr>
<h2 id="Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries"><a href="#Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries" class="headerlink" title="Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries"></a>Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09089">http://arxiv.org/abs/2308.09089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Wilkins, Justin Salamon, Magdalena Fuentes, Juan Pablo Bello, Oriol Nieto<br>for: 这个论文主要是用于找到适合的声音效果（SFX）来匹配视频中的时刻，并且可以使用视频帧直接作为查询来找到高质量（HQ）的声音效果。methods: 这篇论文使用了多模态框架，包括利用大型语言模型和基础视觉语言模型来将HQ音频和视频桥接起来，创建高可扩展的自动音频视频数据纪录pipeline。它还使用预训练的音频和视觉编码器来训练一种对比学习基本来进行匹配。results: 论文表明，使用自动数据纪录ipeline和对比学习基本来训练的系统可以对HQ音频进行高效的匹配，并且在使用各种数据集上表现出色。此外，这种系统还可以从清晰到野外数据中进行泛化，并且在用户测试中获得了67%的正确率。<details>
<summary>Abstract</summary>
Finding the right sound effects (SFX) to match moments in a video is a difficult and time-consuming task, and relies heavily on the quality and completeness of text metadata. Retrieving high-quality (HQ) SFX using a video frame directly as the query is an attractive alternative, removing the reliance on text metadata and providing a low barrier to entry for non-experts. Due to the lack of HQ audio-visual training data, previous work on audio-visual retrieval relies on YouTube (in-the-wild) videos of varied quality for training, where the audio is often noisy and the video of amateur quality. As such it is unclear whether these systems would generalize to the task of matching HQ audio to production-quality video. To address this, we propose a multimodal framework for recommending HQ SFX given a video frame by (1) leveraging large language models and foundational vision-language models to bridge HQ audio and video to create audio-visual pairs, resulting in a highly scalable automatic audio-visual data curation pipeline; and (2) using pre-trained audio and visual encoders to train a contrastive learning-based retrieval system. We show that our system, trained using our automatic data curation pipeline, significantly outperforms baselines trained on in-the-wild data on the task of HQ SFX retrieval for video. Furthermore, while the baselines fail to generalize to this task, our system generalizes well from clean to in-the-wild data, outperforming the baselines on a dataset of YouTube videos despite only being trained on the HQ audio-visual pairs. A user study confirms that people prefer SFX retrieved by our system over the baseline 67% of the time both for HQ and in-the-wild data. Finally, we present ablations to determine the impact of model and data pipeline design choices on downstream retrieval performance. Please visit our project website to listen to and view our SFX retrieval results.
</details>
<details>
<summary>摘要</summary>
找到合适的声效（SFX）以匹配影像中的时刻是一个困难和耗时的任务，它高度依赖文本 metadata 的质量和完整性。使用影像帧直接作为查询，抽取高品质（HQ）声效是一个吸引人的选择，它可以解除文本 metadata 的依赖，并提供低门槛的入门点 для非专家。由于缺乏 HQ 音频视觉训练数据，过去的音频视觉检索工作都是使用 YouTube 上的各种质量的影片进行训练，其中的音频 oft 是噪音的，影像则是业余质量。这使得这些系统是否能够应用到高品质音频与生产质量影像的匹配问题仍然存在一定的uncertainty。为了解决这个问题，我们提出了一个多Modal 框架，可以根据影像帧提供高品质声效。我们的方法包括：1. 利用大型语言模型和基础的视觉语言模型，将高品质音频和影像 Bridge 到创建音频视觉对，实现了高度排擦的自动音频视觉数据填充管道。2. 使用预训的音频和视觉嵌入器，使用对照式学习 retrained 一个检索系统。我们的系统，使用我们的自动数据填充管道进行训练，与基准相比，显著超过了对 YouTube 上的各种质量影片进行训练的基准。此外，我们的系统具有很好的泛化能力，可以从清洁到实际上的数据进行检索，并且在 YouTube 上的影片上进行检索时，表现更好 than 基准。在一次用户研究中，人们对我们的系统进行检索的时候，偏好我们的系统67%。最后，我们进行了一些范例的ablation，以决定模型和数据管道设计的影响。您可以前往我们的项目网站，聆听和查看我们的 SFX 检索结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.SD_2023_08_18/" data-id="clpztdnnf00y2es8855yi3yb2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.CV_2023_08_18/" class="article-date">
  <time datetime="2023-08-18T13:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.CV_2023_08_18/">cs.CV - 2023-08-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification"><a href="#Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification" class="headerlink" title="Revisiting Skin Tone Fairness in Dermatological Lesion Classification"></a>Revisiting Skin Tone Fairness in Dermatological Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09640">http://arxiv.org/abs/2308.09640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tkalbl/revisitingskintonefairness">https://github.com/tkalbl/revisitingskintonefairness</a></li>
<li>paper_authors: Thorsten Kalb, Kaisar Kushibar, Celia Cintas, Karim Lekadir, Oliver Diaz, Richard Osuala</li>
<li>for: 评估皮肤疾病分类算法的公平性，因为皮肤疾病的表现可能因皮肤色调而异常。</li>
<li>methods: 使用Individual Typology Angle（ITA）来估计皮肤色调，并对皮肤疾病分类算法进行公平性分析。</li>
<li>results: 对ISIC18 dataset进行了四种ITA-based皮肤色调分类方法的比较，发现这些方法之间存在很大的不一致，表明ITA-based皮肤色调估计方法存在风险。此外，研究发现ISIC18 dataset的不具有多样性，限制了其作为公平性分析的测试平台。<details>
<summary>Abstract</summary>
Addressing fairness in lesion classification from dermatological images is crucial due to variations in how skin diseases manifest across skin tones. However, the absence of skin tone labels in public datasets hinders building a fair classifier. To date, such skin tone labels have been estimated prior to fairness analysis in independent studies using the Individual Typology Angle (ITA). Briefly, ITA calculates an angle based on pixels extracted from skin images taking into account the lightness and yellow-blue tints. These angles are then categorised into skin tones that are subsequently used to analyse fairness in skin cancer classification. In this work, we review and compare four ITA-based approaches of skin tone classification on the ISIC18 dataset, a common benchmark for assessing skin cancer classification fairness in the literature. Our analyses reveal a high disagreement among previously published studies demonstrating the risks of ITA-based skin tone estimation methods. Moreover, we investigate the causes of such large discrepancy among these approaches and find that the lack of diversity in the ISIC18 dataset limits its use as a testbed for fairness analysis. Finally, we recommend further research on robust ITA estimation and diverse dataset acquisition with skin tone annotation to facilitate conclusive fairness assessments of artificial intelligence tools in dermatology. Our code is available at https://github.com/tkalbl/RevisitingSkinToneFairness.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文翻译，不是正式文件翻译） Addressing fairness in lesion classification from dermatological images is crucial due to variations in how skin diseases manifest across different skin tones. However, the lack of skin tone labels in public datasets hinders the creation of a fair classifier. To date, such skin tone labels have been estimated before fairness analysis in independent studies using the Individual Typology Angle (ITA). Briefly, ITA calculates an angle based on pixels extracted from skin images, taking into account the lightness and yellow-blue tints. These angles are then categorized into skin tones that are subsequently used to analyze fairness in skin cancer classification. In this work, we review and compare four ITA-based approaches of skin tone classification on the ISIC18 dataset, a common benchmark for assessing skin cancer classification fairness in the literature. Our analyses reveal a high disagreement among previously published studies, demonstrating the risks of ITA-based skin tone estimation methods. Moreover, we investigate the causes of such large discrepancy among these approaches and find that the lack of diversity in the ISIC18 dataset limits its use as a testbed for fairness analysis. Finally, we recommend further research on robust ITA estimation and diverse dataset acquisition with skin tone annotation to facilitate conclusive fairness assessments of artificial intelligence tools in dermatology. Our code is available at https://github.com/tkalbl/RevisitingSkinToneFairness.
</details></li>
</ul>
<hr>
<h2 id="GeoDTR-Toward-generic-cross-view-geolocalization-via-geometric-disentanglement"><a href="#GeoDTR-Toward-generic-cross-view-geolocalization-via-geometric-disentanglement" class="headerlink" title="GeoDTR+: Toward generic cross-view geolocalization via geometric disentanglement"></a>GeoDTR+: Toward generic cross-view geolocalization via geometric disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09624">http://arxiv.org/abs/2308.09624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohan Zhang, Xingyu Li, Waqas Sultani, Chen Chen, Safwan Wshah</li>
<li>for: 这个论文目的是提出一个具有更好的地理构造抽取和对比硬件组合的地图地标准化方法，以提高地图地标的精度和稳定性。</li>
<li>methods: 这个方法使用了一个增强的地理构造抽取模组（GLE），以及一个对比硬件组合（CHSG）来增强模型的训练。</li>
<li>results: 实验结果显示，这个方法在跨区评估中取得了最新的州际顶峰表现 ($16.44%$, $22.71%$, and $17.02%$ 不包括极点转换)，并且保持了与现有最新顶峰相同的同区表现。<details>
<summary>Abstract</summary>
Cross-View Geo-Localization (CVGL) estimates the location of a ground image by matching it to a geo-tagged aerial image in a database. Recent works achieve outstanding progress on CVGL benchmarks. However, existing methods still suffer from poor performance in cross-area evaluation, in which the training and testing data are captured from completely distinct areas. We attribute this deficiency to the lack of ability to extract the geometric layout of visual features and models' overfitting to low-level details. Our preliminary work introduced a Geometric Layout Extractor (GLE) to capture the geometric layout from input features. However, the previous GLE does not fully exploit information in the input feature. In this work, we propose GeoDTR+ with an enhanced GLE module that better models the correlations among visual features. To fully explore the LS techniques from our preliminary work, we further propose Contrastive Hard Samples Generation (CHSG) to facilitate model training. Extensive experiments show that GeoDTR+ achieves state-of-the-art (SOTA) results in cross-area evaluation on CVUSA, CVACT, and VIGOR by a large margin ($16.44\%$, $22.71\%$, and $17.02\%$ without polar transformation) while keeping the same-area performance comparable to existing SOTA. Moreover, we provide detailed analyses of GeoDTR+.
</details>
<details>
<summary>摘要</summary>
cross-view geo-localization (CVGL) 算法估算图像的位置，通过与数据库中geo-标记的空中图像进行匹配。现有研究取得了CVGL标准吗的出色表现。然而，现有方法仍然在跨区评估中表现不佳，其中训练和测试数据来自完全不同的区域。我们认为这是因为无法提取视觉特征的几何布局，以及模型过拟合低级细节。我们的前期工作中引入了几何布局提取器（GLE），但前一版GLE未能充分利用输入特征中的信息。在这项工作中，我们提出了GeoDTR+，它包括改进的GLE模块，可以更好地模型视觉特征之间的相关性。此外，我们还提出了对LS技术的进一步发展，即对比难样本生成（CHSG），以促进模型训练。我们的实验表明，GeoDTR+在跨区评估中达到了CVUSA、CVACT和VIGOR等标准吗的最佳结果（无极化转换），同时保持与同区表现相似的水平。此外，我们还提供了GeoDTR+的详细分析。
</details></li>
</ul>
<hr>
<h2 id="Is-context-all-you-need-Scaling-Neural-Sign-Language-Translation-to-Large-Domains-of-Discourse"><a href="#Is-context-all-you-need-Scaling-Neural-Sign-Language-Translation-to-Large-Domains-of-Discourse" class="headerlink" title="Is context all you need? Scaling Neural Sign Language Translation to Large Domains of Discourse"></a>Is context all you need? Scaling Neural Sign Language Translation to Large Domains of Discourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09622">http://arxiv.org/abs/2308.09622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ozge Mercanoglu Sincan, Necati Cihan Camgoz, Richard Bowden</li>
<li>For: The paper aims to improve the accuracy of sign language translation (SLT) by incorporating contextual information into the translation process.* Methods: The proposed method uses a multi-modal transformer architecture that combines low-level video features, recognized sign glosses, and contextual information from previous sign sequences to generate spoken language translations.* Results: The proposed approach significantly improves SLT performance compared to baseline methods, with nearly double the BLEU-4 scores. The results are evaluated on two datasets: BOBSL and SRF.<details>
<summary>Abstract</summary>
Sign Language Translation (SLT) is a challenging task that aims to generate spoken language sentences from sign language videos, both of which have different grammar and word/gloss order. From a Neural Machine Translation (NMT) perspective, the straightforward way of training translation models is to use sign language phrase-spoken language sentence pairs. However, human interpreters heavily rely on the context to understand the conveyed information, especially for sign language interpretation, where the vocabulary size may be significantly smaller than their spoken language equivalent.   Taking direct inspiration from how humans translate, we propose a novel multi-modal transformer architecture that tackles the translation task in a context-aware manner, as a human would. We use the context from previous sequences and confident predictions to disambiguate weaker visual cues. To achieve this we use complementary transformer encoders, namely: (1) A Video Encoder, that captures the low-level video features at the frame-level, (2) A Spotting Encoder, that models the recognized sign glosses in the video, and (3) A Context Encoder, which captures the context of the preceding sign sequences. We combine the information coming from these encoders in a final transformer decoder to generate spoken language translations.   We evaluate our approach on the recently published large-scale BOBSL dataset, which contains ~1.2M sequences, and on the SRF dataset, which was part of the WMT-SLT 2022 challenge. We report significant improvements on state-of-the-art translation performance using contextual information, nearly doubling the reported BLEU-4 scores of baseline approaches.
</details>
<details>
<summary>摘要</summary>
签语翻译（SLT）是一项复杂的任务，旨在从手语视频中生成口语句子，两者都有不同的语法和单词顺序。从神经机器翻译（NMT）的角度来看，直接使用手语短语-口语句子对的训练翻译模型是最直接的方法。然而，人类 intérpretes 具有很强的上下文理解能力，特别是 для手语 interpretations，其词汇量可能比其口语对应的更小。  基于人类翻译的直观思路，我们提出了一种新的多模态 transformer 架构，用于在上下文意识下进行翻译任务。我们使用上下文来解决视觉较弱的征略，并且使用 complementary transformer encoders，即：(1) 视频编码器， capture 视频中帧级的低级特征; (2) 识别编码器， model 视频中识别的手语词汇; (3) 上下文编码器， capture 前一个手语序列的上下文。我们将这些编码器的信息在最终的 transformer decoder 中结合，以生成口语翻译。  我们在最近发布的 BOBSL 数据集上进行了评估，该数据集包含约 1.2M 个序列，以及 SRF 数据集，该数据集在 WMT-SLT 2022 挑战中出现。我们发现，使用上下文信息可以大幅提高翻译性能，相比基eline方法的 BLEU-4 分数增长约 20%。
</details></li>
</ul>
<hr>
<h2 id="LaRS-A-Diverse-Panoptic-Maritime-Obstacle-Detection-Dataset-and-Benchmark"><a href="#LaRS-A-Diverse-Panoptic-Maritime-Obstacle-Detection-Dataset-and-Benchmark" class="headerlink" title="LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark"></a>LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09618">http://arxiv.org/abs/2308.09618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lojze Žust, Janez Perš, Matej Kristan</li>
<li>for: 本研究目的是提供一个多样化的海上障碍物检测数据集，以便进一步推动海上障碍物检测领域的进步。</li>
<li>methods: 本研究使用了一个新的海上�anoptic障碍物检测benchmark，名为LaRS，该benchmark包含了湖泊、河流和海洋等多个环境下的场景。</li>
<li>results: 本研究通过对27种semantic和panoptic segmentation方法的评估，发现了一些性能杂化和未来研究方向。同时，本研究还提供了一个在线评估服务器，以便对海上障碍物检测方法进行对比和评估。<details>
<summary>Abstract</summary>
The progress in maritime obstacle detection is hindered by the lack of a diverse dataset that adequately captures the complexity of general maritime environments. We present the first maritime panoptic obstacle detection benchmark LaRS, featuring scenes from Lakes, Rivers and Seas. Our major contribution is the new dataset, which boasts the largest diversity in recording locations, scene types, obstacle classes, and acquisition conditions among the related datasets. LaRS is composed of over 4000 per-pixel labeled key frames with nine preceding frames to allow utilization of the temporal texture, amounting to over 40k frames. Each key frame is annotated with 8 thing, 3 stuff classes and 19 global scene attributes. We report the results of 27 semantic and panoptic segmentation methods, along with several performance insights and future research directions. To enable objective evaluation, we have implemented an online evaluation server. The LaRS dataset, evaluation toolkit and benchmark are publicly available at: https://lojzezust.github.io/lars-dataset
</details>
<details>
<summary>摘要</summary>
“水域障碍物探测的进步受到水域环境的多样性不足所阻碍。我们提出了首个水域综合障碍物探测比赛LaRS，包括湖泊、河流和海洋的场景。我们的主要贡献是新的数据集，其中包括不同的录制地点、场景类型、障碍物类型和采样条件，与相关的数据集相比，展现了最大的多样性。LaRS包含了4000帧每帧标注的关键帧，共有9个前置帧，以利用时间Texture，总共有40k帧。每个关键帧都被标注为8个物类、3个物品类和19个全局场景特征。我们报告了27种semantic和综合障碍物分类方法的结果，以及一些性能检验和未来研究方向。为了实现公正评估，我们在线上进行了评估服务器。LaRS数据集、评估工具箱和比赛是公开 disponibile的：https://lojzezust.github.io/lars-dataset”
</details></li>
</ul>
<hr>
<h2 id="Far3D-Expanding-the-Horizon-for-Surround-view-3D-Object-Detection"><a href="#Far3D-Expanding-the-Horizon-for-Surround-view-3D-Object-Detection" class="headerlink" title="Far3D: Expanding the Horizon for Surround-view 3D Object Detection"></a>Far3D: Expanding the Horizon for Surround-view 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09616">http://arxiv.org/abs/2308.09616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Jiang, Shuailin Li, Yingfei Liu, Shihao Wang, Fan Jia, Tiancai Wang, Lijin Han, Xiangyu Zhang</li>
<li>for: 本研究旨在提高3D对象检测范围，特别是长范围检测，以便减少成本和提高效率。</li>
<li>methods: 本研究提出了一种新的稀疏查询基于框架，称为Far3D。该框架利用高质量2D对象假设生成3D适应查询，并引入了视角意识汇集模块，以兼顾不同视野和比例的特征捕捉。此外，该研究还提出了范围调整的3D推净方法，以解决查询错误的协传和长范围任务中的稳定问题。</li>
<li>results: 该研究在Argoverse 2数据集上达到了最佳性能水平，覆盖150米的广泛范围，超过了一些LiDAR基于的方法。此外，Far3D还在nuScenes数据集上表现出了superior的性能。代码即将公布。<details>
<summary>Abstract</summary>
Recently 3D object detection from surround-view images has made notable advancements with its low deployment cost. However, most works have primarily focused on close perception range while leaving long-range detection less explored. Expanding existing methods directly to cover long distances poses challenges such as heavy computation costs and unstable convergence. To address these limitations, this paper proposes a novel sparse query-based framework, dubbed Far3D. By utilizing high-quality 2D object priors, we generate 3D adaptive queries that complement the 3D global queries. To efficiently capture discriminative features across different views and scales for long-range objects, we introduce a perspective-aware aggregation module. Additionally, we propose a range-modulated 3D denoising approach to address query error propagation and mitigate convergence issues in long-range tasks. Significantly, Far3D demonstrates SoTA performance on the challenging Argoverse 2 dataset, covering a wide range of 150 meters, surpassing several LiDAR-based approaches. Meanwhile, Far3D exhibits superior performance compared to previous methods on the nuScenes dataset. The code will be available soon.
</details>
<details>
<summary>摘要</summary>
近些时候，从卫星视图图像中的3D物体检测已经做出了明显的进步，主要是因为其低于部署成本。然而，大多数工作都主要集中在近距离检测上，而长距离检测则尚未得到足够的探索。扩展现有方法直接覆盖长距离范围 poses 计算成本过高和稳定性不稳定。为了解决这些限制，这篇论文提出了一种新的稀疏查询基本框架，名为 Far3D。通过利用高质量的2D物体假设，我们生成了3D适应查询，这些查询与3D全球查询衔接。为了有效地捕捉不同视图和缩放下的特征，我们提出了一种视角意识汇集模块。此外，我们提出了范围调整的3D排除方法，以解决查询错误的传递问题和长距离任务中的稳定性问题。特别是，Far3D在Argoverse 2数据集上达到了SOA性能，覆盖150米的各种距离，超过了一些激光雷达基于的方法。同时，Far3D在nuScenes数据集上表现出色，比前一些方法更高。代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="Language-guided-Human-Motion-Synthesis-with-Atomic-Actions"><a href="#Language-guided-Human-Motion-Synthesis-with-Atomic-Actions" class="headerlink" title="Language-guided Human Motion Synthesis with Atomic Actions"></a>Language-guided Human Motion Synthesis with Atomic Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09611">http://arxiv.org/abs/2308.09611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhzhai/atom">https://github.com/yhzhai/atom</a></li>
<li>paper_authors: Yuanhao Zhai, Mingzhen Huang, Tianyu Luan, Lu Dong, Ifeoma Nwogu, Siwei Lyu, David Doermann, Junsong Yuan</li>
<li>for: 文章主要目的是提出一种语言引导人体运动合成技术，以解决人体行为的自然复杂性和多样性导致的合成问题。</li>
<li>methods: 该方法基于分解人体动作为原子动作的思想，并采用了一种课程学习策略来学习原子动作的组合。在学习过程中，文章首先将复杂的人体动作分解为一组原子动作，然后使用这些学习到的原子动作来组合新的动作，从而提高了对新动作的适应性。此外，文章还引入了一种课程学习训练策略，通过逐渐增加的面积矩阵来促进原子动作的组合。</li>
<li>results: 文章通过了广泛的实验，包括文本引导人体运动和动作引导人体运动任务，证明了ATOM模型的效果。具体来说，ATOM模型能够生成符合人体动作规律的文本引导人体运动序列，并且能够更好地适应新的动作。<details>
<summary>Abstract</summary>
Language-guided human motion synthesis has been a challenging task due to the inherent complexity and diversity of human behaviors. Previous methods face limitations in generalization to novel actions, often resulting in unrealistic or incoherent motion sequences. In this paper, we propose ATOM (ATomic mOtion Modeling) to mitigate this problem, by decomposing actions into atomic actions, and employing a curriculum learning strategy to learn atomic action composition. First, we disentangle complex human motions into a set of atomic actions during learning, and then assemble novel actions using the learned atomic actions, which offers better adaptability to new actions. Moreover, we introduce a curriculum learning training strategy that leverages masked motion modeling with a gradual increase in the mask ratio, and thus facilitates atomic action assembly. This approach mitigates the overfitting problem commonly encountered in previous methods while enforcing the model to learn better motion representations. We demonstrate the effectiveness of ATOM through extensive experiments, including text-to-motion and action-to-motion synthesis tasks. We further illustrate its superiority in synthesizing plausible and coherent text-guided human motion sequences.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "ATOM (ATomic mOtion Modeling) to mitigate this problem, by decomposing actions into atomic actions, and employing a curriculum learning strategy to learn atomic action composition." into Simplified Chinese</SYS>>Here's the translation:使用ATOM（原子动作模型）来解决这个问题，将动作 decomposes into atomic actions，并使用学习 atomic action 的CURRICULUM learning策略来学习atomic action的组合。Here's the breakdown of the translation:* 使用ATOM (使用ATOM)： Uses ATOM.* 解决这个问题 (解决这个问题)： Solves this problem.* decomposes into atomic actions (decomposes into atomic actions)： Decomposes into atomic actions.* 并使用学习 (并使用学习)： And uses learning.* atomic action composition (原子动作的组合)： Atomic action composition.I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-LayerNorm-Tuning-for-Continual-Learning-in-Vision-Transformers"><a href="#On-the-Effectiveness-of-LayerNorm-Tuning-for-Continual-Learning-in-Vision-Transformers" class="headerlink" title="On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers"></a>On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09610">http://arxiv.org/abs/2308.09610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tdemin16/continual-layernorm-tuning">https://github.com/tdemin16/continual-layernorm-tuning</a></li>
<li>paper_authors: Thomas De Min, Massimiliano Mancini, Karteek Alahari, Xavier Alameda-Pineda, Elisa Ricci</li>
<li>for: 降低折冲学习方法中的计算成本，以维持竞争力性的表现。</li>
<li>methods: 将注重层 normalization 为每个持续学习任务，并在推断时根据任务特定的键和预训模型的输出选择参数。</li>
<li>results: 在 ImageNet-R 和 CIFAR-100 上实验显示，我们的方法可以与 {状态域} 的表现相互匹配，而且计算成本较低。<details>
<summary>Abstract</summary>
State-of-the-art rehearsal-free continual learning methods exploit the peculiarities of Vision Transformers to learn task-specific prompts, drastically reducing catastrophic forgetting. However, there is a tradeoff between the number of learned parameters and the performance, making such models computationally expensive. In this work, we aim to reduce this cost while maintaining competitive performance. We achieve this by revisiting and extending a simple transfer learning idea: learning task-specific normalization layers. Specifically, we tune the scale and bias parameters of LayerNorm for each continual learning task, selecting them at inference time based on the similarity between task-specific keys and the output of the pre-trained model. To make the classifier robust to incorrect selection of parameters during inference, we introduce a two-stage training procedure, where we first optimize the task-specific parameters and then train the classifier with the same selection procedure of the inference time. Experiments on ImageNet-R and CIFAR-100 show that our method achieves results that are either superior or on par with {the state of the art} while being computationally cheaper.
</details>
<details>
<summary>摘要</summary>
现代无重复练习 continual learning 方法利用视Transformers的特点学习任务特定提示，以减少忘记性。然而，有参数数量和性能之间的交易，使得这些模型变得 computationally 昂贵。在这个工作中，我们希望减少这个成本，保持竞争力。我们实现这一点通过重新访问和扩展一种简单的转移学习想法：学习任务特定 normalization layers。特别是，我们在每个 continual learning 任务中调整层 normalization 的缩放和偏移参数，在推理时根据任务特定的键和预训练模型输出来选择。为了让分类器在推理过程中正确选择参数，我们引入了一个两阶段训练过程，在第一阶段优化任务特定参数，然后在第二阶段使用同样的选择过程来训练分类器。实验表明，我们的方法在 ImageNet-R 和 CIFAR-100 上 achieves Results that are either superior or on par with {the state of the art} while being computationally cheaper。
</details></li>
</ul>
<hr>
<h2 id="Language-Guided-Diffusion-Model-for-Visual-Grounding"><a href="#Language-Guided-Diffusion-Model-for-Visual-Grounding" class="headerlink" title="Language-Guided Diffusion Model for Visual Grounding"></a>Language-Guided Diffusion Model for Visual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09599">http://arxiv.org/abs/2308.09599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijia Chen, Baochun Li</li>
<li>for:  solves the problem of visual grounding, a cross-modal alignment task, in a generative way.</li>
<li>methods:  uses a language-guided diffusion framework called LG-DVG, which trains the model to progressively reason queried object boxes by denoising a set of noisy boxes with the language guide.</li>
<li>results:  achieves superior performance on five widely used datasets, validating the effectiveness of the proposed framework.<details>
<summary>Abstract</summary>
Visual grounding (VG) tasks involve explicit cross-modal alignment, as semantically corresponding image regions are to be located for the language phrases provided. Existing approaches complete such visual-text reasoning in a single-step manner. Their performance causes high demands on large-scale anchors and over-designed multi-modal fusion modules based on human priors, leading to complicated frameworks that may be difficult to train and overfit to specific scenarios. Even worse, such once-for-all reasoning mechanisms are incapable of refining boxes continuously to enhance query-region matching. In contrast, in this paper, we formulate an iterative reasoning process by denoising diffusion modeling. Specifically, we propose a language-guided diffusion framework for visual grounding, LG-DVG, which trains the model to progressively reason queried object boxes by denoising a set of noisy boxes with the language guide. To achieve this, LG-DVG gradually perturbs query-aligned ground truth boxes to noisy ones and reverses this process step by step, conditional on query semantics. Extensive experiments for our proposed framework on five widely used datasets validate the superior performance of solving visual grounding, a cross-modal alignment task, in a generative way. The source codes are available at \url{https://github.com/iQua/vgbase/tree/DiffusionVG}.
</details>
<details>
<summary>摘要</summary>
Visual grounding（视觉附加）任务需要显式跨模态对齐，即要在图像区域和语言短语之间进行Semantic对应。现有的方法通过单步visual-text reasoning来完成这些任务，其性能需要大量的 anchor和复杂的多模态融合模块，导致复杂的框架难以训练和过拟合特定场景。worse,这些once-for-all reasoning mechanisms是无法不断调整查询区域匹配的。相反，在这篇论文中，我们提出了一种迭代的理解过程，即通过降噪模型来进行语义导向的Diffusion VG框架，具体来说是语言指导降噪框架（LG-DVG）。它通过逐步降噪查询对应的真实框， conditional on查询语义来训练模型。我们在五个广泛使用的dataset上进行了extensive experiments，并证明了我们提出的框架在解决视觉附加任务的cross-modal对齐任务中的超越性。代码可以在 \url{https://github.com/iQua/vgbase/tree/DiffusionVG} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-Architectures-and-Receptive-Fields-for-Appearance-based-Gaze-Estimation"><a href="#Investigation-of-Architectures-and-Receptive-Fields-for-Appearance-based-Gaze-Estimation" class="headerlink" title="Investigation of Architectures and Receptive Fields for Appearance-based Gaze Estimation"></a>Investigation of Architectures and Receptive Fields for Appearance-based Gaze Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09593">http://arxiv.org/abs/2308.09593</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yunhanwang1105/GazeTech">https://github.com/yunhanwang1105/GazeTech</a></li>
<li>paper_authors: Yunhan Wang, Xiangwei Shi, Shalini De Mello, Hyung Jin Chang, Xucong Zhang</li>
<li>for: 这篇论文探讨了深度学习技术在过去十年的快速发展，以及这些技术在人工智能和计算机视觉领域的应用。</li>
<li>methods: 这篇论文提出了多种不同的机制，包括软运算、硬运算、两眼不对称、特征分离、旋转一致和对称学习。大多数方法将单一脸部或多个区域作为输入，但这篇论文强调了基本架构的探讨。</li>
<li>results: 这篇论文发现，对 ResNet 架构进行一些简单的参数调整可以超过大多数现有的州OF-the-art方法在三个popular dataset上的关照眼动测量性能。经过广泛的实验，我们发现了适当的步长数、输入图像分辨率和多区域架构对关照眼动测量性能的影响，并且这些影响随着输入脸部图像质量而改变。我们在三个dataset上取得了顶尖性能，分别为ETH-XGaze 3.64、MPIIFaceGaze 4.50和Gaze360度关照眼动测量错误9.13。<details>
<summary>Abstract</summary>
With the rapid development of deep learning technology in the past decade, appearance-based gaze estimation has attracted great attention from both computer vision and human-computer interaction research communities. Fascinating methods were proposed with variant mechanisms including soft attention, hard attention, two-eye asymmetry, feature disentanglement, rotation consistency, and contrastive learning. Most of these methods take the single-face or multi-region as input, yet the basic architecture of gaze estimation has not been fully explored. In this paper, we reveal the fact that tuning a few simple parameters of a ResNet architecture can outperform most of the existing state-of-the-art methods for the gaze estimation task on three popular datasets. With our extensive experiments, we conclude that the stride number, input image resolution, and multi-region architecture are critical for the gaze estimation performance while their effectiveness dependent on the quality of the input face image. We obtain the state-of-the-art performances on three datasets with 3.64 on ETH-XGaze, 4.50 on MPIIFaceGaze, and 9.13 on Gaze360 degrees gaze estimation error by taking ResNet-50 as the backbone.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术在过去十年的快速发展，面部基于的视线估计吸引了计算机视觉和人机交互研究领域的广泛关注。吸引人的方法被提出，包括软注意力、硬注意力、两个眼睛差异、特征分离、旋转一致性和对比学习。大多数方法以单个脸或多个区域作为输入，然而基本的视线估计建 architecture尚未得到全面探讨。在这篇论文中，我们发现了一个关键的事实：调整一些简单的ResNet架构参数可以超越大多数现有的状态对 gaze estimation 任务的表现。通过我们的广泛的实验，我们结论了 that the stride number, input image resolution, and multi-region architecture are critical for the gaze estimation performance, and their effectiveness depends on the quality of the input face image。我们在三个 dataset 上 obtain 了最佳性能，具体来说是ETH-XGaze的3.64、MPIIFaceGaze的4.50和Gaze360度的9.13度视线估计误差。
</details></li>
</ul>
<hr>
<h2 id="StableVideo-Text-driven-Consistency-aware-Diffusion-Video-Editing"><a href="#StableVideo-Text-driven-Consistency-aware-Diffusion-Video-Editing" class="headerlink" title="StableVideo: Text-driven Consistency-aware Diffusion Video Editing"></a>StableVideo: Text-driven Consistency-aware Diffusion Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09592">http://arxiv.org/abs/2308.09592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rese1f/stablevideo">https://github.com/rese1f/stablevideo</a></li>
<li>paper_authors: Wenhao Chai, Xun Guo, Gaoang Wang, Yan Lu</li>
<li>for: 这篇论文是为了解决Diffusion-based方法在视频编辑中的问题，即如何在保持已有对象的外观不变的情况下，编辑视频。</li>
<li>methods: 这篇论文使用了文本驱动的Diffusion模型，并引入了时间关系来保持对象的外观一致性。具体来说，它使用了一种新的间帧传播机制，将一帧的外观信息传播到下一帧。</li>
<li>results: 该论文的实验结果表明，与现有的视频编辑方法相比，其方法可以 достичь更高的编辑质量和一致性。<details>
<summary>Abstract</summary>
Diffusion-based methods can generate realistic images and videos, but they struggle to edit existing objects in a video while preserving their appearance over time. This prevents diffusion models from being applied to natural video editing in practical scenarios. In this paper, we tackle this problem by introducing temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the edited objects. Specifically, we develop a novel inter-frame propagation mechanism for diffusion video editing, which leverages the concept of layered representations to propagate the appearance information from one frame to the next. We then build up a text-driven video editing framework based on this mechanism, namely StableVideo, which can achieve consistency-aware video editing. Extensive experiments demonstrate the strong editing capability of our approach. Compared with state-of-the-art video editing methods, our approach shows superior qualitative and quantitative results. Our code is available at \href{https://github.com/rese1f/StableVideo}{this https URL}.
</details>
<details>
<summary>摘要</summary>
Diffusion-based方法可以生成真实的图像和视频，但它们在现有视频中编辑已有对象的时候困难保持对象的外观在时间上的一致性。这使得扩散模型在实际的视频编辑场景中无法应用。在这篇论文中，我们解决了这个问题，通过添加时间相关性来改进现有的文本驱动扩散模型，使其能够在编辑过程中保持对象的外观一致性。我们开发了一种新的间帧传播机制，基于层次表示来传播一帧到下一帧的外观信息。然后，我们建立了基于这种机制的文本驱动视频编辑框架，即StableVideo，可以实现一致性感视频编辑。我们的实验表明，我们的方法可以具有优秀的编辑效果，相比之前的视频编辑方法，我们的方法在质量和量上都达到了更高的水平。我们的代码可以在以下 GitHub 上下载：https://github.com/rese1f/StableVideo。
</details></li>
</ul>
<hr>
<h2 id="O-2-Recon-Completing-3D-Reconstruction-of-Occluded-Objects-in-the-Scene-with-a-Pre-trained-2D-Diffusion-Model"><a href="#O-2-Recon-Completing-3D-Reconstruction-of-Occluded-Objects-in-the-Scene-with-a-Pre-trained-2D-Diffusion-Model" class="headerlink" title="O^2-Recon: Completing 3D Reconstruction of Occluded Objects in the Scene with a Pre-trained 2D Diffusion Model"></a>O^2-Recon: Completing 3D Reconstruction of Occluded Objects in the Scene with a Pre-trained 2D Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09591">http://arxiv.org/abs/2308.09591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubin Hu, Sheng Ye, Wang Zhao, Matthieu Lin, Yuze He, Yu-Hui Wen, Ying He, Yong-Jin Liu</li>
<li>for: 该研究旨在提出一种新的完整物体3D重建方法，解决RGB-D视频中物体遮挡问题。</li>
<li>methods: 该方法利用预训练的扩散模型填充2D图像隐藏部分，然后使用这些填充图像优化神经隐藏表示法对每个实例的3D重建。</li>
<li>results: 实验结果表明，该方法在ScanNet场景中可以达到状态之arte accuracy和完整性在对象级RGB-D视频重建中。<details>
<summary>Abstract</summary>
Occlusion is a common issue in 3D reconstruction from RGB-D videos, often blocking the complete reconstruction of objects and presenting an ongoing problem. In this paper, we propose a novel framework, empowered by a 2D diffusion-based in-painting model, to reconstruct complete surfaces for the hidden parts of objects. Specifically, we utilize a pre-trained diffusion model to fill in the hidden areas of 2D images. Then we use these in-painted images to optimize a neural implicit surface representation for each instance for 3D reconstruction. Since creating the in-painting masks needed for this process is tricky, we adopt a human-in-the-loop strategy that involves very little human engagement to generate high-quality masks. Moreover, some parts of objects can be totally hidden because the videos are usually shot from limited perspectives. To ensure recovering these invisible areas, we develop a cascaded network architecture for predicting signed distance field, making use of different frequency bands of positional encoding and maintaining overall smoothness. Besides the commonly used rendering loss, Eikonal loss, and silhouette loss, we adopt a CLIP-based semantic consistency loss to guide the surface from unseen camera angles. Experiments on ScanNet scenes show that our proposed framework achieves state-of-the-art accuracy and completeness in object-level reconstruction from scene-level RGB-D videos.
</details>
<details>
<summary>摘要</summary>
干扰是RGB-D视频中3D重建中的一个常见问题，常常阻塞对象的完整重建，是一个持续的问题。在这篇论文中，我们提出了一种新的框架，利用2D扩散模型来填充对象隐藏部分的表面。 Specifically，我们使用预训练的扩散模型填充隐藏在2D图像中的部分。然后，我们使用这些填充后的图像来优化每个实例的神经蕴涵表示。由于创建填充面积需要很多人工参与，我们采用了人类在循环的策略，即很少需要人工参与来生成高质量的填充面积。此外，视频通常从有限的视角拍摄，因此一些对象部分可能会被完全隐藏。为了保证恢复这些隐藏的部分，我们开发了一种层次网络架构，用于预测签名距离场，利用不同频率域的位置编码，并保持整体的平滑性。此外，我们还采用了基于CLIP的语义一致损失，以导引表面从未看到的摄像头角度。实验结果表明，我们的提出的框架在SceneNet场景中实现了state-of-the-art的准确性和完整性。
</details></li>
</ul>
<hr>
<h2 id="Deep-Equilibrium-Object-Detection"><a href="#Deep-Equilibrium-Object-Detection" class="headerlink" title="Deep Equilibrium Object Detection"></a>Deep Equilibrium Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09564">http://arxiv.org/abs/2308.09564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MCG-NJU/DEQDet">https://github.com/MCG-NJU/DEQDet</a></li>
<li>paper_authors: Shuai Wang, Yao Teng, Limin Wang</li>
<li>for: 本文提出了一种新的Query-based对象检测器（DEQDet），通过设计深度平衡解码器来进行对象检测。</li>
<li>methods: 本文使用了一种叫做深度平衡解码器（DEQ），它模型了查询向量精细化为稳定有意义的表示，然后使用简单的FFN头直接预测对象的位置和类别。</li>
<li>results: 对比基eline，本文的DEQDet在MS COCO数据集上达到了$49.5$ mAP和$33.0$ AP$_s$的性能，并且需要较少的内存和训练时间。<details>
<summary>Abstract</summary>
Query-based object detectors directly decode image features into object instances with a set of learnable queries. These query vectors are progressively refined to stable meaningful representations through a sequence of decoder layers, and then used to directly predict object locations and categories with simple FFN heads. In this paper, we present a new query-based object detector (DEQDet) by designing a deep equilibrium decoder. Our DEQ decoder models the query vector refinement as the fixed point solving of an {implicit} layer and is equivalent to applying {infinite} steps of refinement. To be more specific to object decoding, we use a two-step unrolled equilibrium equation to explicitly capture the query vector refinement. Accordingly, we are able to incorporate refinement awareness into the DEQ training with the inexact gradient back-propagation (RAG). In addition, to stabilize the training of our DEQDet and improve its generalization ability, we devise the deep supervision scheme on the optimization path of DEQ with refinement-aware perturbation~(RAP). Our experiments demonstrate DEQDet converges faster, consumes less memory, and achieves better results than the baseline counterpart (AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queries achieves the $49.5$ mAP and $33.0$ AP$_s$ on the MS COCO benchmark under $2\times$ training scheme (24 epochs).
</details>
<details>
<summary>摘要</summary>
干ifen-based object detectors directly将图像特征解码成对象实例的集合，使用一组学习的查询向量。这些查询向量通过一系列解码层进行逐渐精细化，然后直接使用简单的FFN头预测对象的位置和类别。在这篇论文中，我们提出了一种新的查询基于object detector（DEQDet），通过设计深度平衡解码器来实现。我们的DEQ解码器模型查询向量精细化为 fixes point解决的{implicit}层，等于应用{无限}步的精细化。为了更加准确地捕捉对象解码中的查询向量精细化，我们采用了两步卷积的平衡方程。因此，我们可以在DEQ训练中引入精细化意识，并通过不确定梯度反propagation（RAG）进行训练。此外，为了稳定DEQDet的训练和提高其通用能力，我们提出了深度监视 schema在DEQ的优化路径上进行反制�ubble~(RAP)。我们的实验表明，DEQDet在比特笔COCO数据集上 converge faster，占用更少的内存，并 achieve better results than基eline对手（AdaMixer）。具体来说，我们的DEQDet使用ResNet50底层和300个查询可以在$2\times$ 训练方案下 achieve $49.5$ mAP和$33.0$ AP$_s$。
</details></li>
</ul>
<hr>
<h2 id="Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection"><a href="#Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection" class="headerlink" title="Decoupled conditional contrastive learning with variable metadata for prostate lesion detection"></a>Decoupled conditional contrastive learning with variable metadata for prostate lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09542">http://arxiv.org/abs/2308.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/camilleruppli/decoupled_ccl">https://github.com/camilleruppli/decoupled_ccl</a></li>
<li>paper_authors: Camille Ruppli, Pietro Gori, Roberto Ardon, Isabelle Bloch</li>
<li>For: 早期检测 próstate cancer 的重要性，以便有效的治疗。* Methods: 使用多parametric Magnetic Resonance Images (mp-MRI)  для诊断肿瘤。使用 Prostate Imaging Reporting and Data System (PI-RADS) 标准化评估肿瘤癌变性。* Results: 通过利用weak metadata 和多个标注者每个样本的 metadata confidence，定义metadata confidence，提高了lesion detection的准确率，在公共PI-CAI挑战数据集上提高了3%的AUC。Here’s the English version of the paper’s abstract, with the three key information points highlighted:* For: Early detection of prostate cancer is crucial for efficient treatment.* Methods: The paper uses multi-parametric Magnetic Resonance Images (mp-MRI) for lesion detection, and leverages the Prostate Imaging Reporting and Data System (PI-RADS) to standardize the interpretation of prostate MRI.* Results: The proposed contrastive loss function, which combines metadata of varying confidence with unannotated data, improves lesion detection accuracy by 3% on the public PI-CAI challenge dataset.I hope this helps!<details>
<summary>Abstract</summary>
Early diagnosis of prostate cancer is crucial for efficient treatment. Multi-parametric Magnetic Resonance Images (mp-MRI) are widely used for lesion detection. The Prostate Imaging Reporting and Data System (PI-RADS) has standardized interpretation of prostate MRI by defining a score for lesion malignancy. PI-RADS data is readily available from radiology reports but is subject to high inter-reports variability. We propose a new contrastive loss function that leverages weak metadata with multiple annotators per sample and takes advantage of inter-reports variability by defining metadata confidence. By combining metadata of varying confidence with unannotated data into a single conditional contrastive loss function, we report a 3% AUC increase on lesion detection on the public PI-CAI challenge dataset.   Code is available at: https://github.com/camilleruppli/decoupled_ccl
</details>
<details>
<summary>摘要</summary>
早期检测 проstate 癌的诊断是非常重要，以便进行有效的治疗。多 Parametric Magnetic Resonance Image (mp-MRI) 广泛用于肿瘤检测。 Prostate Imaging Reporting and Data System (PI-RADS) 已经标准化了 prostate MRI 的解释，并定义了肿瘤癌性的分数。PI-RADS 数据ready availability from radiology reports，但是受到高度的Inter-reports variability。我们提议一种新的对比损失函数，利用weak metadata 和多个注解者per sample，并利用Inter-reports variability by defining metadata confidence。通过将metadata of varying confidence 和无注释数据 combine into a single conditional contrastive loss function，我们在public PI-CAI challenge dataset上发现3%的AUC提升。Code is available at: https://github.com/camilleruppli/decoupled_cclNote: "prostate" is translated as " проstate" in Simplified Chinese, which is the standard translation.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI"><a href="#Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI" class="headerlink" title="Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI"></a>Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09538">http://arxiv.org/abs/2308.09538</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miagrouput/carotid-segmentation">https://github.com/miagrouput/carotid-segmentation</a></li>
<li>paper_authors: Elina Thibeau-Sutre, Dieuwertje Alblas, Sophie Buurman, Christoph Brune, Jelmer M. Wolterink</li>
<li>for: 这个论文目的是为了应用深度学习模型到大规模数据集中，并自动进行质量控制。</li>
<li>methods: 这个论文使用的方法包括自动化的某些血液MRI图像的分割，以及测试时数据增强和Monte Carlo dropout来估计模型预测结果的不确定性。</li>
<li>results: 研究发现，包括不确定性测量不会下降分割质量，不确定性指标可以用来衡量分割质量，并且可以检测参与者级别的低质量分割。这种自动质量控制工具可能会使得深度学习模型在大规模数据集中应用更加广泛。<details>
<summary>Abstract</summary>
The application of deep learning models to large-scale data sets requires means for automatic quality assurance. We have previously developed a fully automatic algorithm for carotid artery wall segmentation in black-blood MRI that we aim to apply to large-scale data sets. This method identifies nested artery walls in 3D patches centered on the carotid artery. In this study, we investigate to what extent the uncertainty in the model predictions for the contour location can serve as a surrogate for error detection and, consequently, automatic quality assurance. We express the quality of automatic segmentations using the Dice similarity coefficient. The uncertainty in the model's prediction is estimated using either Monte Carlo dropout or test-time data augmentation. We found that (1) including uncertainty measurements did not degrade the quality of the segmentations, (2) uncertainty metrics provide a good proxy of the quality of our contours if the center found during the first step is enclosed in the lumen of the carotid artery and (3) they could be used to detect low-quality segmentations at the participant level. This automatic quality assurance tool might enable the application of our model in large-scale data sets.
</details>
<details>
<summary>摘要</summary>
aplicación de modelos de aprendizaje profundo a conjuntos de datos a gran escala requiere medidas para la calidad automática. Hemos desarrollado anteriormente un algoritmo completamente automático para la segmentación de las paredes de la arteria carótida en imágenes de resonancia magnética negra, y nuestro objetivo es aplicar este método a conjuntos de datos a gran escala. Este método identifica las paredes de la arteria carótida en 3D en secciones centradas en la arteria carótida. En este estudio, investigamos hasta qué punto la incertidumbre en las predicciones del modelo sobre el lugar de la contornación puede servir como un substituto para la detección de errores y, por lo tanto, como una medida de calidad automática. Expresamos la calidad de las segmentaciones automáticas utilizando el coeficiente de semejanza de Dice. La incertidumbre en las predicciones del modelo se estima utilizando either dropout de Monte Carlo o augmentación de datos en tiempo de prueba. Encontramos que:1. Incluir mediciones de incertidumbre no degrade la calidad de las segmentaciones;2. Los métricos de incertidumbre proporcionan un buen sustituto de la calidad de nuestras contornaciones si el centro encontrado durante la primera etapa está en el lumen de la arteria carótida;3. Pueden ser utilizados para detectar segmentaciones de baja calidad en el nivel de participante.Este herramienta de calidad automática podría permitir la aplicación de nuestro modelo en conjuntos de datos a gran escala.
</details></li>
</ul>
<hr>
<h2 id="Small-Object-Detection-via-Coarse-to-fine-Proposal-Generation-and-Imitation-Learning"><a href="#Small-Object-Detection-via-Coarse-to-fine-Proposal-Generation-and-Imitation-Learning" class="headerlink" title="Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning"></a>Small Object Detection via Coarse-to-fine Proposal Generation and Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09534">http://arxiv.org/abs/2308.09534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shaunyuan22/cfinet">https://github.com/shaunyuan22/cfinet</a></li>
<li>paper_authors: Xiang Yuan, Gong Cheng, Kebing Yan, Qinghua Zeng, Junwei Han</li>
<li>for: 提高小对象检测精度</li>
<li>methods: 使用Coarse-to-fine RPN和Feature Imitation learning</li>
<li>results: 在大规模小对象检测标准 benchmark 上达到领先的性能<details>
<summary>Abstract</summary>
The past few years have witnessed the immense success of object detection, while current excellent detectors struggle on tackling size-limited instances. Concretely, the well-known challenge of low overlaps between the priors and object regions leads to a constrained sample pool for optimization, and the paucity of discriminative information further aggravates the recognition. To alleviate the aforementioned issues, we propose CFINet, a two-stage framework tailored for small object detection based on the Coarse-to-fine pipeline and Feature Imitation learning. Firstly, we introduce Coarse-to-fine RPN (CRPN) to ensure sufficient and high-quality proposals for small objects through the dynamic anchor selection strategy and cascade regression. Then, we equip the conventional detection head with a Feature Imitation (FI) branch to facilitate the region representations of size-limited instances that perplex the model in an imitation manner. Moreover, an auxiliary imitation loss following supervised contrastive learning paradigm is devised to optimize this branch. When integrated with Faster RCNN, CFINet achieves state-of-the-art performance on the large-scale small object detection benchmarks, SODA-D and SODA-A, underscoring its superiority over baseline detector and other mainstream detection approaches.
</details>
<details>
<summary>摘要</summary>
Recent years have seen tremendous success in object detection, but current state-of-the-art detectors still struggle with detecting small objects. The main challenge is that the prior knowledge and object regions have low overlap, leading to a limited sample pool for optimization, and the lack of discriminative information makes it even more difficult to recognize the objects. To address these issues, we propose CFINet, a two-stage framework tailored for small object detection based on the Coarse-to-fine pipeline and Feature Imitation learning.First, we introduce Coarse-to-fine RPN (CRPN) to ensure sufficient and high-quality proposals for small objects through dynamic anchor selection and cascade regression. Then, we add a Feature Imitation (FI) branch to the conventional detection head to enhance the region representations of size-limited instances in an imitation manner. Moreover, we design an auxiliary imitation loss based on supervised contrastive learning to optimize this branch.When integrated with Faster RCNN, CFINet achieves state-of-the-art performance on large-scale small object detection benchmarks SODA-D and SODA-A, outperforming baseline detectors and other mainstream detection approaches.
</details></li>
</ul>
<hr>
<h2 id="Improving-3D-Pose-Estimation-for-Sign-Language"><a href="#Improving-3D-Pose-Estimation-for-Sign-Language" class="headerlink" title="Improving 3D Pose Estimation for Sign Language"></a>Improving 3D Pose Estimation for Sign Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09525">http://arxiv.org/abs/2308.09525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maksym Ivashechkin, Oscar Mendez, Richard Bowden</li>
<li>for: 这篇论文目标是提出一种基于前向运动学和神经网络的3D人姿重建方法，以实现快速和准确地测量人姿。</li>
<li>methods: 该方法利用了前向运动学（FK）和神经网络，将2D图像中的关键点检测转化为3D人姿。首先，使用神经网络预测人 JOINT 的旋转和肢长，然后将这些预测结果与骨骼约束相结合，通过实现了 PyTorch 中的 FK 层，以获得准确的3D人姿。</li>
<li>results: 通过量化和质量评估，该方法与 MediaPipe 相比，在每个 JOINT 的位势误差和视觉效果上具有显著的优势。此外，该方法还能够在不同的数据集上进行泛化。 PyTorch 实现在 CPU 上运行时间为100-200毫秒（包括 CNN 检测）。<details>
<summary>Abstract</summary>
This work addresses 3D human pose reconstruction in single images. We present a method that combines Forward Kinematics (FK) with neural networks to ensure a fast and valid prediction of 3D pose. Pose is represented as a hierarchical tree/graph with nodes corresponding to human joints that model their physical limits. Given a 2D detection of keypoints in the image, we lift the skeleton to 3D using neural networks to predict both the joint rotations and bone lengths. These predictions are then combined with skeletal constraints using an FK layer implemented as a network layer in PyTorch. The result is a fast and accurate approach to the estimation of 3D skeletal pose. Through quantitative and qualitative evaluation, we demonstrate the method is significantly more accurate than MediaPipe in terms of both per joint positional error and visual appearance. Furthermore, we demonstrate generalization over different datasets. The implementation in PyTorch runs at between 100-200 milliseconds per image (including CNN detection) using CPU only.
</details>
<details>
<summary>摘要</summary>
We evaluate the method through both quantitative and qualitative means, demonstrating that it is significantly more accurate than MediaPipe in terms of both per joint positional error and visual appearance. Additionally, we show that the method generalizes well across different datasets. The implementation in PyTorch runs at between 100-200 milliseconds per image (including CNN detection) using CPU only.
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-for-3D-Hand-Pose-Estimation-from-Images"><a href="#Denoising-Diffusion-for-3D-Hand-Pose-Estimation-from-Images" class="headerlink" title="Denoising Diffusion for 3D Hand Pose Estimation from Images"></a>Denoising Diffusion for 3D Hand Pose Estimation from Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09523">http://arxiv.org/abs/2308.09523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maksym Ivashechkin, Oscar Mendez, Richard Bowden</li>
<li>for: 本研究旨在解决从单个图像或序列中提取3D手势的问题。</li>
<li>methods: 提出了一种新的端到端框架，使用了扩散模型来捕捉数据分布，并遵循了机械约束来确保生成的姿势是有效的。</li>
<li>results: 研究表明该方法可以在提取2D单手图像到3D的过程中提供状态的最佳性能，并在使用序列数据时通过添加Transformer模块来进一步提高准确性。<details>
<summary>Abstract</summary>
Hand pose estimation from a single image has many applications. However, approaches to full 3D body pose estimation are typically trained on day-to-day activities or actions. As such, detailed hand-to-hand interactions are poorly represented, especially during motion. We see this in the failure cases of techniques such as OpenPose or MediaPipe. However, accurate hand pose estimation is crucial for many applications where the global body motion is less important than accurate hand pose estimation.   This paper addresses the problem of 3D hand pose estimation from monocular images or sequences. We present a novel end-to-end framework for 3D hand regression that employs diffusion models that have shown excellent ability to capture the distribution of data for generative purposes. Moreover, we enforce kinematic constraints to ensure realistic poses are generated by incorporating an explicit forward kinematic layer as part of the network. The proposed model provides state-of-the-art performance when lifting a 2D single-hand image to 3D. However, when sequence data is available, we add a Transformer module over a temporal window of consecutive frames to refine the results, overcoming jittering and further increasing accuracy.   The method is quantitatively and qualitatively evaluated showing state-of-the-art robustness, generalization, and accuracy on several different datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduced the text into Simplified Chinese.<</SYS>>人体手势估计从单个图像中有很多应用。然而，对全息体姿估计方法通常是在日常活动或动作上训练的。因此，手部之间的细节互动被不好地表示，特别是在运动中。我们在技术如OpenPose或MediaPipe的失败案例中看到了这一点。然而，准确的手势估计对许多应用来说非常重要，特别是在全息体姿的运动不那么重要。这篇论文Addresses the problem of 3D手势估计从单个图像或序列中。我们提出了一种新的终端框架，用于3D手势回归。该框架利用了扩散模型，这些模型在生成目的上表现出了优秀的能力。此外，我们加入了显式前向几何层，以确保生成的姿势是真实的。提议的模型在将2D单手图像提升到3D的任务中提供了状态机器的性能。当序列数据可用时，我们将Transformer模块添加到 consecutives帧中，以修复结果，超越晃动，并提高准确率。方法被量测量和质量上评估，显示了状态机器的稳定性、泛化能力和准确率在多个不同的数据集上都达到了顶峰。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Intrinsic-Properties-for-Non-Rigid-Garment-Alignment"><a href="#Leveraging-Intrinsic-Properties-for-Non-Rigid-Garment-Alignment" class="headerlink" title="Leveraging Intrinsic Properties for Non-Rigid Garment Alignment"></a>Leveraging Intrinsic Properties for Non-Rigid Garment Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09519">http://arxiv.org/abs/2308.09519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyou Lin, Boyao Zhou, Zerong Zheng, Hongwen Zhang, Yebin Liu<br>for:这篇论文targets the problem of aligning real-world 3D garment data, which is beneficial for applications such as texture learning, physical parameter estimation, and generative modeling of garments.methods:The proposed method leverages intrinsic manifold properties and uses two neural deformation fields, one in 3D space and another in intrinsic space, to achieve coarse-to-fine alignment. The coarse stage performs a 3D fitting, and the refined stage uses a second neural deformation field for higher accuracy.results:The method achieves accurate wrinkle-level and texture-level alignment and works well for difficult garment types such as long coats. The project page with more information and results is available at <a target="_blank" rel="noopener" href="https://jsnln.github.io/iccv2023_intrinsic/index.html">https://jsnln.github.io/iccv2023_intrinsic/index.html</a>.<details>
<summary>Abstract</summary>
We address the problem of aligning real-world 3D data of garments, which benefits many applications such as texture learning, physical parameter estimation, generative modeling of garments, etc. Existing extrinsic methods typically perform non-rigid iterative closest point and struggle to align details due to incorrect closest matches and rigidity constraints. While intrinsic methods based on functional maps can produce high-quality correspondences, they work under isometric assumptions and become unreliable for garment deformations which are highly non-isometric. To achieve wrinkle-level as well as texture-level alignment, we present a novel coarse-to-fine two-stage method that leverages intrinsic manifold properties with two neural deformation fields, in the 3D space and the intrinsic space, respectively. The coarse stage performs a 3D fitting, where we leverage intrinsic manifold properties to define a manifold deformation field. The coarse fitting then induces a functional map that produces an alignment of intrinsic embeddings. We further refine the intrinsic alignment with a second neural deformation field for higher accuracy. We evaluate our method with our captured garment dataset, GarmCap. The method achieves accurate wrinkle-level and texture-level alignment and works for difficult garment types such as long coats. Our project page is https://jsnln.github.io/iccv2023_intrinsic/index.html.
</details>
<details>
<summary>摘要</summary>
我们解决了对真实世界3D资料的衣服对齐问题，这有助于训练文本、物理参数估计、生成衣服等应用程序。现有的外部方法通常会进行非静态迭代最近点，并很难跟踪细节，因为 incorrect closest matches 和静态约束。而内部方法基于函数对应则可以生成高品质对应关系，但它们在衣服塑形中是不可靠的，因为衣服塑形是非静态的。为了 achiev wrinkle-level 和 texture-level 对齐，我们提出了一个新的两阶段方法，利用内部构造特性，并使用两个神经塑形场，在3D空间和内部空间中进行塑形。在粗细阶段，我们利用内部构造特性定义一个构造塑形场，并将其调整为一个函数对应。我们进一步将这个函数对应进行精确调整，使用第二个神经塑形场。我们评估了我们的方法，使用我们捕捉的衣服Dataset，GarmCap。我们的方法可以实现细节级和纹理级对齐，并在困难的衣服类型，如长大袍，也能够运作。我们的项目页面是 <https://jsnln.github.io/iccv2023_intrinsic/index.html>。
</details></li>
</ul>
<hr>
<h2 id="Learnt-Contrastive-Concept-Embeddings-for-Sign-Recognition"><a href="#Learnt-Contrastive-Concept-Embeddings-for-Sign-Recognition" class="headerlink" title="Learnt Contrastive Concept Embeddings for Sign Recognition"></a>Learnt Contrastive Concept Embeddings for Sign Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09515">http://arxiv.org/abs/2308.09515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Wong, Necati Cihan Camgoz, Richard Bowden</li>
<li>for: 本研究旨在提供一种基于contrastive learning的、weakly supervised的签语嵌入方法，用于将手语语言与口语语言相互连接。</li>
<li>methods: 我们提出了一种学习框架，用于 derivation LCC（学习对应概念）嵌入 для手语语言，该方法基于手语语言的语言标签来训练词语表示。我们还开发了一种基于word embedding的概念相似损失函数，以便使用NLP方法中的word embedding来创建更好的手语嵌入。</li>
<li>results: 我们的方法可以在WLASL和BOBSL数据集上实现状态实验室的键点基于手语识别性能。<details>
<summary>Abstract</summary>
In natural language processing (NLP) of spoken languages, word embeddings have been shown to be a useful method to encode the meaning of words. Sign languages are visual languages, which require sign embeddings to capture the visual and linguistic semantics of sign. Unlike many common approaches to Sign Recognition, we focus on explicitly creating sign embeddings that bridge the gap between sign language and spoken language. We propose a learning framework to derive LCC (Learnt Contrastive Concept) embeddings for sign language, a weakly supervised contrastive approach to learning sign embeddings. We train a vocabulary of embeddings that are based on the linguistic labels for sign video. Additionally, we develop a conceptual similarity loss which is able to utilise word embeddings from NLP methods to create sign embeddings that have better sign language to spoken language correspondence. These learnt representations allow the model to automatically localise the sign in time. Our approach achieves state-of-the-art keypoint-based sign recognition performance on the WLASL and BOBSL datasets.
</details>
<details>
<summary>摘要</summary>
在自然语言处理（NLP）的 spoken语言中，词嵌入已经被证明是一种有用的方法来编码词语的含义。手语是一种视觉语言，需要手势嵌入来捕捉手语的视觉和语言 semantics。不同于许多常见的手语识别方法，我们注重于显式地创建手势嵌入，以bridging the gap between手语和口语。我们提出了一种学习框架，用于 derivation LCC（学习对应概念）嵌入 для手语，这是一种弱监督对比方法来学习手势嵌入。我们训练了一个词库，基于手语视频的语言标签来学习嵌入。此外，我们开发了一种概念相似损失，可以利用 NLP 方法中的词嵌入来创建更好地与口语相匹配的手势嵌入。这些学习的表示允许模型自动在时间上local化手语。我们的方法在 WLASL 和 BOBSL 数据集上实现了状态的键点基于手语识别性能。
</details></li>
</ul>
<hr>
<h2 id="ResQ-Residual-Quantization-for-Video-Perception"><a href="#ResQ-Residual-Quantization-for-Video-Perception" class="headerlink" title="ResQ: Residual Quantization for Video Perception"></a>ResQ: Residual Quantization for Video Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09511">http://arxiv.org/abs/2308.09511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Abati, Haitam Ben Yahia, Markus Nagel, Amirhossein Habibian</li>
<li>for: 加速视频识别，如semantic segmentation和人体 pose estimation，通过利用帧间重复性。</li>
<li>methods: 利用低位量编码，基于差异网络活动的差分 residuals 的特性，提出 Residual Quantization 模型。</li>
<li>results: 比标准quantization和现有高效视频识别模型 superior performance 在准确性和位宽之间的权衡。<details>
<summary>Abstract</summary>
This paper accelerates video perception, such as semantic segmentation and human pose estimation, by levering cross-frame redundancies. Unlike the existing approaches, which avoid redundant computations by warping the past features using optical-flow or by performing sparse convolutions on frame differences, we approach the problem from a new perspective: low-bit quantization. We observe that residuals, as the difference in network activations between two neighboring frames, exhibit properties that make them highly quantizable. Based on this observation, we propose a novel quantization scheme for video networks coined as Residual Quantization. ResQ extends the standard, frame-by-frame, quantization scheme by incorporating temporal dependencies that lead to better performance in terms of accuracy vs. bit-width. Furthermore, we extend our model to dynamically adjust the bit-width proportional to the amount of changes in the video. We demonstrate the superiority of our model, against the standard quantization and existing efficient video perception models, using various architectures on semantic segmentation and human pose estimation benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Video-Instrument-Synergistic-Network-for-Referring-Video-Instrument-Segmentation-in-Robotic-Surgery"><a href="#Video-Instrument-Synergistic-Network-for-Referring-Video-Instrument-Segmentation-in-Robotic-Surgery" class="headerlink" title="Video-Instrument Synergistic Network for Referring Video Instrument Segmentation in Robotic Surgery"></a>Video-Instrument Synergistic Network for Referring Video Instrument Segmentation in Robotic Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09475">http://arxiv.org/abs/2308.09475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongqiu Wang, Lei Zhu, Guang Yang, Yike Guo, Shichen Zhang, Bo Xu, Yueming Jin</li>
<li>for: 该研究旨在提高Robot-assisted surgery的质量，通过实时器段化来促进手术 Navigation和医学教育。</li>
<li>methods: 该研究提出了一种新的 Referring Surgical Video Instrument Segmentation (RSVIS)任务，通过自动将文本描述与视频帧相关联，以提高手术器段化的精度。为此，我们提出了一种新的 Video-Instrument Synergistic Network (VIS-Net)，以学习视频水平和工具水平的知识，从而提高性能。同时，我们还设计了一种图像基于关系意识模块 (GRM)，以模型多modal信息（文本描述和视频帧）之间的关系，以便提取工具级别信息。</li>
<li>results: 我们的方法在两个RSVIS dataset上进行验证，实验结果表明，我们的VIS-Net可以significantly outperform现有的参照分割方法。<details>
<summary>Abstract</summary>
Robot-assisted surgery has made significant progress, with instrument segmentation being a critical factor in surgical intervention quality. It serves as the building block to facilitate surgical robot navigation and surgical education for the next generation of operating intelligence. Although existing methods have achieved accurate instrument segmentation results, they simultaneously generate segmentation masks for all instruments, without the capability to specify a target object and allow an interactive experience. This work explores a new task of Referring Surgical Video Instrument Segmentation (RSVIS), which aims to automatically identify and segment the corresponding surgical instruments based on the given language expression. To achieve this, we devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both video-level and instrument-level knowledge to boost performance, while previous work only used video-level information. Meanwhile, we design a Graph-based Relation-aware Module (GRM) to model the correlation between multi-modal information (i.e., textual description and video frame) to facilitate the extraction of instrument-level information. We are also the first to produce two RSVIS datasets to promote related research. Our method is verified on these datasets, and experimental results exhibit that the VIS-Net can significantly outperform existing state-of-the-art referring segmentation methods. Our code and our datasets will be released upon the publication of this work.
</details>
<details>
<summary>摘要</summary>
机器人协助手术已经做出了重要进步， инструмен特征分割成为手术干预质量的关键因素。它作为手术机器人导航和下一代操作智能的基础阶段，对手术教育也具有重要意义。 although existing methods have achieved accurate instrument segmentation results, they simultaneously generate segmentation masks for all instruments, without the capability to specify a target object and allow an interactive experience. This work explores a new task of Referring Surgical Video Instrument Segmentation (RSVIS), which aims to automatically identify and segment the corresponding surgical instruments based on the given language expression. To achieve this, we devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both video-level and instrument-level knowledge to boost performance, while previous work only used video-level information. Meanwhile, we design a Graph-based Relation-aware Module (GRM) to model the correlation between multi-modal information (i.e., textual description and video frame) to facilitate the extraction of instrument-level information. We are also the first to produce two RSVIS datasets to promote related research. Our method is verified on these datasets, and experimental results exhibit that the VIS-Net can significantly outperform existing state-of-the-art referring segmentation methods. Our code and our datasets will be released upon the publication of this work.Here's the translation in Traditional Chinese:机器人协助手术已经做出了重要进步， instrumente特征分割成为手术干预质量的关键因素。它作为手术机器人导航和下一代操作智能的基础阶段，对手术教育也具有重要意义。although existing methods have achieved accurate instrument segmentation results, they simultaneously generate segmentation masks for all instruments, without the capability to specify a target object and allow an interactive experience. This work explores a new task of Referring Surgical Video Instrument Segmentation (RSVIS), which aims to automatically identify and segment the corresponding surgical instruments based on the given language expression. To achieve this, we devise a novel Video-Instrument Synergistic Network (VIS-Net) to learn both video-level and instrument-level knowledge to boost performance, while previous work only used video-level information. Meanwhile, we design a Graph-based Relation-aware Module (GRM) to model the correlation between multi-modal information (i.e., textual description and video frame) to facilitate the extraction of instrument-level information. We are also the first to produce two RSVIS datasets to promote related research. Our method is verified on these datasets, and experimental results exhibit that the VIS-Net can significantly outperform existing state-of-the-art referring segmentation methods. Our code and our datasets will be released upon the publication of this work.
</details></li>
</ul>
<hr>
<h2 id="Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP"><a href="#Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP" class="headerlink" title="Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)"></a>Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09467">http://arxiv.org/abs/2308.09467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuang Xiong, Yang Gao, Yin Liu, Amir Fazlollahi, Peter Nestor, Feng Liu, Hongfu Sun</li>
<li>for: 解决量子感知图像推导中不同对象的扫描参数下的偏振转换问题</li>
<li>methods: 提出了一种新的无需训练的模型基于深度学习方法，称为MoDIP（模型基于深度图像优先）</li>
<li>results: 在不同扫描参数下，MoDIP可以 excellently 普适化解决量子感知图像推导问题，并且比传统的迭代方法和深度学习方法有32%的准确率提升，同时也比传统的DIP方法33%更快速，可以在4.5分钟内完成3D高分辨率图像重建。<details>
<summary>Abstract</summary>
The data-driven approach of supervised learning methods has limited applicability in solving dipole inversion in Quantitative Susceptibility Mapping (QSM) with varying scan parameters across different objects. To address this generalization issue in supervised QSM methods, we propose a novel training-free model-based unsupervised method called MoDIP (Model-based Deep Image Prior). MoDIP comprises a small, untrained network and a Data Fidelity Optimization (DFO) module. The network converges to an interim state, acting as an implicit prior for image regularization, while the optimization process enforces the physical model of QSM dipole inversion. Experimental results demonstrate MoDIP's excellent generalizability in solving QSM dipole inversion across different scan parameters. It exhibits robustness against pathological brain QSM, achieving over 32% accuracy improvement than supervised deep learning and traditional iterative methods. It is also 33% more computationally efficient and runs 4 times faster than conventional DIP-based approaches, enabling 3D high-resolution image reconstruction in under 4.5 minutes.
</details>
<details>
<summary>摘要</summary>
supervised学习方法的数据驱动方法在量子频谱地图（QSM）中的杂项倒转问题中有限的应用。为解决这种泛化问题，我们提议一种没有训练的模型基于无supervised方法called MoDIP（模型基于深度图像先验）。MoDIP包括一个小型、未训练的网络和数据准确优化（DFO）模块。网络在进行杂项倒转时 converges to an interim state，作为图像规范化的隐藏先验，而优化过程检查QSM杂项倒转的物理模型。实验结果表明MoDIP在不同扫描参数下的QSM杂项倒转问题中具有出色的泛化性。它在异常脑QSM问题中表现稳定，比超出深度学习和传统迭代方法32%以上的准确率提高。它还比折衔DIP基于方法33%更快，可以在4.5分钟内完成3D高分辨率图像重建。
</details></li>
</ul>
<hr>
<h2 id="Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning"><a href="#Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning" class="headerlink" title="Data augmentation and explainability for bias discovery and mitigation in deep learning"></a>Data augmentation and explainability for bias discovery and mitigation in deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09464">http://arxiv.org/abs/2308.09464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła<br>for: 这个论文的目的是研究深度神经网络中的偏见问题，并提出了一些方法来减少偏见对模型性能的影响。methods: 这个论文使用了一些方法来减少偏见，包括：Explainable AI、Style Transfer Data Augmentation、Targeted Data Augmentations和Attribution Feedback。results: 这个论文的研究结果表明，通过使用这些方法可以减少偏见对模型性能的影响，并提高模型的准确率。<details>
<summary>Abstract</summary>
This dissertation explores the impact of bias in deep neural networks and presents methods for reducing its influence on model performance. The first part begins by categorizing and describing potential sources of bias and errors in data and models, with a particular focus on bias in machine learning pipelines. The next chapter outlines a taxonomy and methods of Explainable AI as a way to justify predictions and control and improve the model. Then, as an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as an alternative semi-automatic approach to manual data exploration for discovering potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of the identified biases on the model. Whereas identifying errors and bias is critical, improving the model and reducing the number of flaws in the future is an absolute priority. Hence, the second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed: Style Transfer Data Augmentation, Targeted Data Augmentations, and Attribution Feedback. Style Transfer Data Augmentation aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one. Targeted Data Augmentations randomly insert possible biases into all images in the dataset during the training, as a way to make the process random and, thus, destroy spurious correlations. Lastly, Attribution Feedback is used to fine-tune the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss. The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details>
<details>
<summary>摘要</summary>
As an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as an alternative semi-automatic approach to manual data exploration for discovering potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of the identified biases on the model.Whereas identifying errors and bias is critical, improving the model and reducing the number of flaws in the future is an absolute priority. Hence, the second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed: Style Transfer Data Augmentation, Targeted Data Augmentations, and Attribution Feedback.Style Transfer Data Augmentation aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one. Targeted Data Augmentations randomly insert possible biases into all images in the dataset during the training, as a way to make the process random and, thus, destroy spurious correlations. Lastly, Attribution Feedback is used to fine-tune the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss. The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Bayesian-imaging-by-relaxed-proximal-point-Langevin-sampling"><a href="#Accelerated-Bayesian-imaging-by-relaxed-proximal-point-Langevin-sampling" class="headerlink" title="Accelerated Bayesian imaging by relaxed proximal-point Langevin sampling"></a>Accelerated Bayesian imaging by relaxed proximal-point Langevin sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09460">http://arxiv.org/abs/2308.09460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teresa Klatzer, Paul Dobson, Yoann Altmann, Marcelo Pereyra, Jesús María Sanz-Serna, Konstantinos C. Zygalakis</li>
<li>for: 这个论文提出了一种新的加速 proximal Markov chain Monte Carlo 方法，用于在具有凸凹性的图像反问题中进行 Bayesian 推断。</li>
<li>methods: 该方法使用了一种 Stochastic Relaxed Proximal-Point 迭代法，该迭代法可以看作是一种停止推断方法，并且有两种不同的解释。对于满足某些条件的模型，该方法等价于一种停止推断方法，而对于不满足这些条件的模型，该方法等价于一种 Leimkuhler-Matthews 离散方法。</li>
<li>results: 该paper提供了一种非对称的 accelerated proximal Markov chain Monte Carlo 方法，该方法可以在凸凹性的图像反问题中获得加速的速度，并且可以避免对于非凸凹性模型的偏度。在不同的实验中，该方法都能够达到更高的速度和更好的准确性，比如图像恢复问题中的 Gaussian 和 Poisson 噪声。<details>
<summary>Abstract</summary>
This paper presents a new accelerated proximal Markov chain Monte Carlo methodology to perform Bayesian inference in imaging inverse problems with an underlying convex geometry. The proposed strategy takes the form of a stochastic relaxed proximal-point iteration that admits two complementary interpretations. For models that are smooth or regularised by Moreau-Yosida smoothing, the algorithm is equivalent to an implicit midpoint discretisation of an overdamped Langevin diffusion targeting the posterior distribution of interest. This discretisation is asymptotically unbiased for Gaussian targets and shown to converge in an accelerated manner for any target that is $\kappa$-strongly log-concave (i.e., requiring in the order of $\sqrt{\kappa}$ iterations to converge, similarly to accelerated optimisation schemes), comparing favorably to [M. Pereyra, L. Vargas Mieles, K.C. Zygalakis, SIAM J. Imaging Sciences, 13, 2 (2020), pp. 905-935] which is only provably accelerated for Gaussian targets and has bias. For models that are not smooth, the algorithm is equivalent to a Leimkuhler-Matthews discretisation of a Langevin diffusion targeting a Moreau-Yosida approximation of the posterior distribution of interest, and hence achieves a significantly lower bias than conventional unadjusted Langevin strategies based on the Euler-Maruyama discretisation. For targets that are $\kappa$-strongly log-concave, the provided non-asymptotic convergence analysis also identifies the optimal time step which maximizes the convergence speed. The proposed methodology is demonstrated through a range of experiments related to image deconvolution with Gaussian and Poisson noise, with assumption-driven and data-driven convex priors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transformer-based-Detection-of-Microorganisms-on-High-Resolution-Petri-Dish-Images"><a href="#Transformer-based-Detection-of-Microorganisms-on-High-Resolution-Petri-Dish-Images" class="headerlink" title="Transformer-based Detection of Microorganisms on High-Resolution Petri Dish Images"></a>Transformer-based Detection of Microorganisms on High-Resolution Petri Dish Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09436">http://arxiv.org/abs/2308.09436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolas Ebert, Didier Stricker, Oliver Wasenmüller</li>
<li>for: 该论文主要用于提高医疗或制药过程中的不间断清洁监测。</li>
<li>methods: 该论文使用了一种新型的变体自注意机制，称为高效全局自注意机制，以解决当前自动化检测技术面临的主要挑战。</li>
<li>results: 在公共可用的 AGAR 数据集上，该网络表现出了与当前状态艺术的超过当前最佳性能。此外，通过在 COCO 和 LIVECell 数据集上进行进一步的实验，我们证明了该方法的任务独立性。<details>
<summary>Abstract</summary>
Many medical or pharmaceutical processes have strict guidelines regarding continuous hygiene monitoring. This often involves the labor-intensive task of manually counting microorganisms in Petri dishes by trained personnel. Automation attempts often struggle due to major challenges: significant scaling differences, low separation, low contrast, etc. To address these challenges, we introduce AttnPAFPN, a high-resolution detection pipeline that leverages a novel transformer variation, the efficient-global self-attention mechanism. Our streamlined approach can be easily integrated in almost any multi-scale object detection pipeline. In a comprehensive evaluation on the publicly available AGAR dataset, we demonstrate the superior accuracy of our network over the current state-of-the-art. In order to demonstrate the task-independent performance of our approach, we perform further experiments on COCO and LIVECell datasets.
</details>
<details>
<summary>摘要</summary>
许多医疗或药品生产过程有严格的清洁监测指南。这经常包括人工计数微生物在杯中的劳动密集任务，由训练过的人员进行。自动化尝试经常遇到主要挑战，如规模差异、低分离、低对比等。为解决这些挑战，我们介绍AttnPAFPN，一种高分辨率检测管线，利用一种新型变体的 transformer 机制，即有效全球自注意机制。我们的流线型approach可以轻松地与大多数多尺度对象检测管线集成。在公共可用的 AGAR 数据集上进行了全面的评估，我们示出了我们网络比现状态的最高精度。为了证明我们的方法是任务独立的，我们在 COCO 和 LIVECell 数据集上进行了进一步的实验。
</details></li>
</ul>
<hr>
<h2 id="Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability"><a href="#Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability" class="headerlink" title="Can ultrasound confidence maps predict sonographers’ labeling variability?"></a>Can ultrasound confidence maps predict sonographers’ labeling variability?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09433">http://arxiv.org/abs/2308.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vanessa Gonzalez Duque, Leonhard Zirus, Yordanka Velikova, Nassir Navab, Diana Mateus</li>
<li>for: 这个论文的目的是提出一种新的深度学习 segmentation 方法，以便考虑医生对 ultrasound 图像的注意力和不确定性。</li>
<li>methods: 该方法使用了 ultrasound 图像中的信任度映射 (CM)，以帮助深度学习 segmentation 网络生成更加准确和多样化的预测结果。</li>
<li>results: 研究结果表明，使用 ultrasound CM 可以提高 Dice 分数、改善 Hausdorff 和平均表面距离，以及减少孤立像素预测。此外，研究还发现， ultrasound CM 可以改善真实 interpolations 的惩罚，从而提高深度学习 segmentation 的准确性。<details>
<summary>Abstract</summary>
Measuring cross-sectional areas in ultrasound images is a standard tool to evaluate disease progress or treatment response. Often addressed today with supervised deep-learning segmentation approaches, existing solutions highly depend upon the quality of experts' annotations. However, the annotation quality in ultrasound is anisotropic and position-variant due to the inherent physical imaging principles, including attenuation, shadows, and missing boundaries, commonly exacerbated with depth. This work proposes a novel approach that guides ultrasound segmentation networks to account for sonographers' uncertainties and generate predictions with variability similar to the experts. We claim that realistic variability can reduce overconfident predictions and improve physicians' acceptance of deep-learning cross-sectional segmentation solutions. Our method provides CM's certainty for each pixel for minimal computational overhead as it can be precalculated directly from the image. We show that there is a correlation between low values in the confidence maps and expert's label uncertainty. Therefore, we propose to give the confidence maps as additional information to the networks. We study the effect of the proposed use of ultrasound CMs in combination with four state-of-the-art neural networks and in two configurations: as a second input channel and as part of the loss. We evaluate our method on 3D ultrasound datasets of the thyroid and lower limb muscles. Our results show ultrasound CMs increase the Dice score, improve the Hausdorff and Average Surface Distances, and decrease the number of isolated pixel predictions. Furthermore, our findings suggest that ultrasound CMs improve the penalization of uncertain areas in the ground truth data, thereby improving problematic interpolations. Our code and example data will be made public at https://github.com/IFL-CAMP/Confidence-segmentation.
</details>
<details>
<summary>摘要</summary>
评估静止影像中的横截面面积是评估疾病进程或治疗效果的标准工具。现有的深度学习分割方法可以高度依赖专家的注释，但是现有的解决方案受到镜像质量的限制，镜像质量呈扁平不均，受到物理镜像原理的影响，包括吸收、阴影和缺失边界，这些问题通常会在深度方向加剧。本文提出了一种新的方法，使得ultrasound分割网络能够考虑医生uncertainty，生成具有类似专家预测的各种性的预测。我们认为，在静止影像中添加realistic的variability可以减少过于自信的预测，提高医生对深度学习横截面分割解决方案的acceptance。我们的方法可以在低计算开销下提供CM的确定性，直接从图像中计算。我们发现，低值在确定性图中与专家标签不确定性相吻合。因此，我们提议使用ultrasound CM来补充网络。我们在四种state-of-the-art神经网络和两种配置下测试了我们的方法：作为第二个输入通道和作为损失的一部分。我们在3D静止影像 datasets中评估了我们的方法，结果显示，ultrasound CM可以提高 dice分数、改善 Hausdorff和平均表面距离，并减少孤立像素预测。此外，我们发现，ultrasound CM可以优化问题 interpolations，因此提高了对真实值数据的惩罚。我们的代码和示例数据将在https://github.com/IFL-CAMP/Confidence-segmentation中公开。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks"><a href="#Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks" class="headerlink" title="Self-Supervised Single-Image Deconvolution with Siamese Neural Networks"></a>Self-Supervised Single-Image Deconvolution with Siamese Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09426">http://arxiv.org/abs/2308.09426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Papkov, Kaupo Palo, Leopold Parts</li>
<li>for: 这篇论文旨在提出一种基于深度学习的图像恢复方法，以提高图像的清晰度和锐度。</li>
<li>methods: 该方法使用了自适应掩模神经网络，通过直接从数据中学习雨花点质量而不需要手动设置参数。</li>
<li>results: 实验结果表明，该改进的框架可以在3D微型scopy图像恢复任务中高效地提高图像的清晰度和锐度，并且超过了之前的状态艺术恢复方法。<details>
<summary>Abstract</summary>
Inverse problems in image reconstruction are fundamentally complicated by unknown noise properties. Classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain. Deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data. Recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training. However, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized. We tackle this problem with Fast Fourier Transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks. Further, we propose to adopt a Siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches. The experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function.
</details>
<details>
<summary>摘要</summary>
“影像重建问题的逆问题是由于未知的噪音特性而困难。经典的迭代复原方法将噪音放大，需要精确地选择参数以取得最佳的对称性和粒子。深度学习方法可以灵活地设定噪音的参数，并将其直接从数据中学习。现在，自我监督的盲点神经网络在影像复原中获得了成功，但它们仅适用于2D图像在医学领域中。我们解决这个问题，使用快速傅立叶变换来提高3D显微镜复原任务的训练速度。此外，我们提议运用对称损失来复原，并评估其在神经网络中的最佳位置。实验结果显示，我们改进的框架比前一代的复原方法更高效。”Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="MonoNeRD-NeRF-like-Representations-for-Monocular-3D-Object-Detection"><a href="#MonoNeRD-NeRF-like-Representations-for-Monocular-3D-Object-Detection" class="headerlink" title="MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection"></a>MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09421">http://arxiv.org/abs/2308.09421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cskkxjk/mononerd">https://github.com/cskkxjk/mononerd</a></li>
<li>paper_authors: Junkai Xu, Liang Peng, Haoran Cheng, Hao Li, Wei Qian, Ke Li, Wenxiao Wang, Deng Cai</li>
<li>for: 提高单目3D检测器的性能，使其能够更好地检测远离和受阻物体。</li>
<li>methods: 使用Signed Distance Functions (SDF)模型场景，然后将这些表示转化为Neural Radiance Fields (NeRF)，并使用卷积渲染来恢复RGB图像和深度图。</li>
<li>results: 在KITTI-3D benchmark和Waymo Open Dataset上进行了广泛的实验，并得到了效果报告。<details>
<summary>Abstract</summary>
In the field of monocular 3D detection, it is common practice to utilize scene geometric clues to enhance the detector's performance. However, many existing works adopt these clues explicitly such as estimating a depth map and back-projecting it into 3D space. This explicit methodology induces sparsity in 3D representations due to the increased dimensionality from 2D to 3D, and leads to substantial information loss, especially for distant and occluded objects. To alleviate this issue, we propose MonoNeRD, a novel detection framework that can infer dense 3D geometry and occupancy. Specifically, we model scenes with Signed Distance Functions (SDF), facilitating the production of dense 3D representations. We treat these representations as Neural Radiance Fields (NeRF) and then employ volume rendering to recover RGB images and depth maps. To the best of our knowledge, this work is the first to introduce volume rendering for M3D, and demonstrates the potential of implicit reconstruction for image-based 3D perception. Extensive experiments conducted on the KITTI-3D benchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD. Codes are available at https://github.com/cskkxjk/MonoNeRD.
</details>
<details>
<summary>摘要</summary>
在单目3D探测领域，广泛采用场景几何准确来提高探测器的性能。然而，许多现有工作直接使用这些准确，如计算深度图并将其投影到3D空间中。这种直接方法会导致3D表示的稀疏性，特别是对于远距离和遮挡的对象。为了解决这个问题，我们提出了MonoNeRD检测框架，可以对场景进行精度的3D几何和占用率的推测。具体来说，我们使用signed distance functions（SDF）来建模场景，并将这些表示转换为神经辐射场（NeRF）。然后，我们使用卷积渲染来恢复RGB图像和深度图。我们知道，这是第一个在M3D中应用卷积渲染的工作，并证明了隐式重建的潜在优势。我们在KITTI-3D标准测试集和Waymo开放数据集进行了广泛的实验，并证明了MonoNeRD的有效性。代码可以在https://github.com/cskkxjk/MonoNeRD中找到。
</details></li>
</ul>
<hr>
<h2 id="Metadata-Improves-Segmentation-Through-Multitasking-Elicitation"><a href="#Metadata-Improves-Segmentation-Through-Multitasking-Elicitation" class="headerlink" title="Metadata Improves Segmentation Through Multitasking Elicitation"></a>Metadata Improves Segmentation Through Multitasking Elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09411">http://arxiv.org/abs/2308.09411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iaroslav Plutenko, Mikhail Papkov, Kaupo Palo, Leopold Parts, Dmytro Fishman</li>
<li>for: 这个论文主要是为了提高深度学习方法中的 semantic segmentation 性能。</li>
<li>methods: 这个论文使用了通道调制机制，将metadata作为 convolutional network 的输入，以提高 segmentation 结果。</li>
<li>results: 论文表明，通过使用metadata，可以提高 segmentation 结果，而且这种方法具有低成本和可以与现有模型结合使用的优点。<details>
<summary>Abstract</summary>
Metainformation is a common companion to biomedical images. However, this potentially powerful additional source of signal from image acquisition has had limited use in deep learning methods, for semantic segmentation in particular. Here, we incorporate metadata by employing a channel modulation mechanism in convolutional networks and study its effect on semantic segmentation tasks. We demonstrate that metadata as additional input to a convolutional network can improve segmentation results while being inexpensive in implementation as a nimble add-on to popular models. We hypothesize that this benefit of metadata can be attributed to facilitating multitask switching. This aspect of metadata-driven systems is explored and discussed in detail.
</details>
<details>
<summary>摘要</summary>
这里的metadata是对生物医学影像的常见伴侣。然而，这个潜在强大的额外讯号仍然受到深度学习方法中的有限使用，尤其是在 semantic segmentation 方面。在这里，我们通过将metadata作为构成元件加入卷积网络中，研究 metadata 在 semantic segmentation 任务中的影响。我们证明了 metadata 作为卷积网络的额外输入，可以提高分类结果，而且实现起来相对便宜。我们推测这个metadata的好处可以归因于促进多任务转换。这个metadata-驱动的系统的内部运作和讨论在详细的文章中进行了详细的探讨。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Decision-Boundaries-Dualistic-Meta-Learning-for-Open-Set-Domain-Generalization"><a href="#Generalizable-Decision-Boundaries-Dualistic-Meta-Learning-for-Open-Set-Domain-Generalization" class="headerlink" title="Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization"></a>Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09391">http://arxiv.org/abs/2308.09391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zzwdx/medic">https://github.com/zzwdx/medic</a></li>
<li>paper_authors: Xiran Wang, Jian Zhang, Lei Qi, Yinghuan Shi</li>
<li>for: 本研究旨在解决频繁出现在目标领域中的领域转换问题，特别是当源领域和目标领域有不同的类型时。</li>
<li>methods: 本研究使用多个一对一分类器来定义决策边界，并将外围样本拒绝为未知样本。然而，在许多情况下，正样本和负样本的类别数量不均衡，导致决策边界偏向正样本，从而导致已知样本在目标领域中的误分类。本研究提出了一种基于元学习的框架，即双重MEta-learning with joint DomaIn-Class matching（MEDIC），它同时考虑了多个领域和类别的梯度匹配，以找到一个总体可靠的、Balanced的决策边界。</li>
<li>results: 实验结果表明，MEDIC不仅在开集enario下超过了先前的方法，同时也保持了相对较好的闭集泛化能力。 Code可以在<a target="_blank" rel="noopener" href="https://github.com/zzwdx/MEDIC%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/zzwdx/MEDIC中找到。</a><details>
<summary>Abstract</summary>
Domain generalization (DG) is proposed to deal with the issue of domain shift, which occurs when statistical differences exist between source and target domains. However, most current methods do not account for a common realistic scenario where the source and target domains have different classes. To overcome this deficiency, open set domain generalization (OSDG) then emerges as a more practical setting to recognize unseen classes in unseen domains. An intuitive approach is to use multiple one-vs-all classifiers to define decision boundaries for each class and reject the outliers as unknown. However, the significant class imbalance between positive and negative samples often causes the boundaries biased towards positive ones, resulting in misclassification for known samples in the unseen target domain. In this paper, we propose a novel meta-learning-based framework called dualistic MEta-learning with joint DomaIn-Class matching (MEDIC), which considers gradient matching towards inter-domain and inter-class splits simultaneously to find a generalizable boundary balanced for all tasks. Experimental results demonstrate that MEDIC not only outperforms previous methods in open set scenarios, but also maintains competitive close set generalization ability at the same time. Our code is available at https://github.com/zzwdx/MEDIC.
</details>
<details>
<summary>摘要</summary>
域外泛化（DG）是为了解决域shift问题而提出的，域shift问题是指源领域和目标领域存在统计上的差异。然而，现有的大多数方法不能考虑到预测领域和目标领域中的类不同的常见情况。为了解决这个不足，开放集领域泛化（OSDG）作为更实际的设定，用于识别未经见过的类。一种直观的方法是使用多个一对一分类器来定义决策边界，并将异常值作为未知样本拒绝。然而，难以平衡的类划分常导致决策边界偏向正样本，从而导致在未经见过的目标领域中误分类已知样本。在这篇论文中，我们提出了一种基于元学习的框架，即双向MEta-learning with joint DomaIn-Class matching（MEDIC）。该框架同时考虑到域间和类间的梯度匹配，以找到一个泛化性能良好的、对所有任务均匀的决策边界。实验结果表明，MEDIC不仅在开放集领域中表现出优于前方法，还能够保持与闭set泛化能力相当的竞争力。我们的代码可以在https://github.com/zzwdx/MEDIC上找到。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-for-Image-Restoration-and-Enhancement-–-A-Comprehensive-Survey"><a href="#Diffusion-Models-for-Image-Restoration-and-Enhancement-–-A-Comprehensive-Survey" class="headerlink" title="Diffusion Models for Image Restoration and Enhancement – A Comprehensive Survey"></a>Diffusion Models for Image Restoration and Enhancement – A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09388">http://arxiv.org/abs/2308.09388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lixinustc/awesome-diffusion-model-for-image-processing">https://github.com/lixinustc/awesome-diffusion-model-for-image-processing</a></li>
<li>paper_authors: Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang, Wenjun Zeng, Xinchao Wang, Zhibo Chen</li>
<li>for: 本文旨在为图像修复（IR）领域提供一份总结，尤其是在使用扩散模型（Diffusion Model）进行图像修复方面。</li>
<li>methods: 本文总结了最新的扩散模型基于的图像修复方法，包括学习框架、条件策略、模型设计等方面，并对现有的方法进行评价。</li>
<li>results: 本文对现有的扩散模型基于的图像修复方法进行了全面的总结，并提出了五个可能的未来研究方向，包括样本效率、模型压缩、质量评价等方面。<details>
<summary>Abstract</summary>
Image restoration (IR) has been an indispensable and challenging task in the low-level vision field, which strives to improve the subjective quality of images distorted by various forms of degradation. Recently, the diffusion model has achieved significant advancements in the visual generation of AIGC, thereby raising an intuitive question, "whether diffusion model can boost image restoration". To answer this, some pioneering studies attempt to integrate diffusion models into the image restoration task, resulting in superior performances than previous GAN-based methods. Despite that, a comprehensive and enlightening survey on diffusion model-based image restoration remains scarce. In this paper, we are the first to present a comprehensive review of recent diffusion model-based methods on image restoration, encompassing the learning paradigm, conditional strategy, framework design, modeling strategy, and evaluation. Concretely, we first introduce the background of the diffusion model briefly and then present two prevalent workflows that exploit diffusion models in image restoration. Subsequently, we classify and emphasize the innovative designs using diffusion models for both IR and blind/real-world IR, intending to inspire future development. To evaluate existing methods thoroughly, we summarize the commonly-used dataset, implementation details, and evaluation metrics. Additionally, we present the objective comparison for open-sourced methods across three tasks, including image super-resolution, deblurring, and inpainting. Ultimately, informed by the limitations in existing works, we propose five potential and challenging directions for the future research of diffusion model-based IR, including sampling efficiency, model compression, distortion simulation and estimation, distortion invariant learning, and framework design.
</details>
<details>
<summary>摘要</summary>
Image restoration（IR）是低级视觉领域中不可或缺的和挑战性的任务，旨在提高受到不同类型的降低的图像主观质量。最近，扩散模型在视觉生成AIGC中取得了显著进步，因此提出了一个直观的问题：“扩散模型能否提高图像restoration”？为了回答这个问题，一些创新的研究尝试将扩散模型与图像restoration任务结合，从而实现更高的性能。然而，一份全面和激进的扩散模型基于图像restoration的评价还缺乏。在这篇论文中，我们是首次提供了一份全面的扩散模型基于图像restoration的评价，涵盖学习 парадиг，条件策略，框架设计，模型化策略和评价。具体来说，我们首先介绍了扩散模型的背景，然后介绍了在图像restoration中使用扩散模型的两种常见方法。接着，我们将扩散模型在图像restoration和匿名/实际图像restoration中的创新设计进行分类和强调，以便激励未来的发展。为了评价现有方法的全面性，我们列举了一些常用的数据集、实现细节和评价度量。此外，我们还对开源方法进行了对比评价，包括图像超解、压缩和填充等三个任务。最后，根据现有方法的局限性，我们提出了五个未来研究的挑战和可能性，包括样本效率、模型压缩、降低度量和不变学习。
</details></li>
</ul>
<hr>
<h2 id="DReg-NeRF-Deep-Registration-for-Neural-Radiance-Fields"><a href="#DReg-NeRF-Deep-Registration-for-Neural-Radiance-Fields" class="headerlink" title="DReg-NeRF: Deep Registration for Neural Radiance Fields"></a>DReg-NeRF: Deep Registration for Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09386">http://arxiv.org/abs/2308.09386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aibluefisher/dreg-nerf">https://github.com/aibluefisher/dreg-nerf</a></li>
<li>paper_authors: Yu Chen, Gim Hee Lee</li>
<li>for: 本研究的目的是解决基于物体中心的场景下多个NeRF的注册问题，而不需要人工标注关键点。</li>
<li>methods: 我们提出了一种基于transformer架构的DReg-NeRF方法，首先提取NeRF中的占据网格特征，然后使用自我关注和相关关注层来学习对应NeRF块之间的关系。与现有的点云注册方法不同，我们的方法不需要任何人工标注，并且使用表面场所supervise协调对应点云。</li>
<li>results: 我们在测试集上评估了我们的方法，与现有的点云注册方法相比，我们的方法在mean $\text{RPE}$和mean $\text{RTE}$上都有大幅度的提升，分别为9.67度和0.038。<details>
<summary>Abstract</summary>
Although Neural Radiance Fields (NeRF) is popular in the computer vision community recently, registering multiple NeRFs has yet to gain much attention. Unlike the existing work, NeRF2NeRF, which is based on traditional optimization methods and needs human annotated keypoints, we propose DReg-NeRF to solve the NeRF registration problem on object-centric scenes without human intervention. After training NeRF models, our DReg-NeRF first extracts features from the occupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformer architecture with self-attention and cross-attention layers to learn the relations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA) point cloud registration methods, the decoupled correspondences are supervised by surface fields without any ground truth overlapping labels. We construct a novel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse to train our network. When evaluated on the test set, our proposed method beats the SOTA point cloud registration methods by a large margin, with a mean $\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.   Our code is available at https://github.com/AIBluefisher/DReg-NeRF.
</details>
<details>
<summary>摘要</summary>
尽管神经辐射场（NeRF）在计算机视觉领域最近受欢迎，但是多个NeRF的注册问题还尚未得到了充分的关注。与现有的工作不同，我们的NeRF2NeRF基于传统优化方法，需要人工标注关键点，我们提出了DReg-NeRF来解决对象中心场景下NeRF注册问题。经过训练NeRF模型后，我们的DReg-NeRF首先从NeRF中提取特征。然后，我们的DReg-NeRF利用了转换器架构，包括自我注意和跨注意层，以学习NeRF块对之间的关系。与现状最佳点云注册方法相比，我们的解耦对应点不需要任何真实重合标签。我们在Objaverse上构建了一个新的视图合成数据集，并在其上训练我们的网络。在测试集上评估时，我们的提议方法与最佳点云注册方法相比， Mean RPE=9.67度和Mean RTE=0.038。 参考代码可以在https://github.com/AIBluefisher/DReg-NeRF中找到。
</details></li>
</ul>
<hr>
<h2 id="Label-Free-Event-based-Object-Recognition-via-Joint-Learning-with-Image-Reconstruction-from-Events"><a href="#Label-Free-Event-based-Object-Recognition-via-Joint-Learning-with-Image-Reconstruction-from-Events" class="headerlink" title="Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events"></a>Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09383">http://arxiv.org/abs/2308.09383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoonhee Cho, Hyeonseong Kim, Yujeong Chae, Kuk-Jin Yoon</li>
<li>for: 本研究旨在实现没有类别标签和对应图像的事件基于对象识别。</li>
<li>methods: 我们提出了一种结合对象识别和图像重建的共同形式，首先将事件重建为图像，然后通过对比语言-图像预训练（CLIP）进行对象识别，并使用类别导向吸引损失和类别无关排斥损失将文本特征与重建图像的视觉特征相吸引。我们还提出了一种可靠的数据采样策略和本地-全局重建一致性来促进两个任务的共同学习。</li>
<li>results: 我们的方法在预测和重建质量方面具有明显的优势，并且可以进行零参数对象识别。我们的项目代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/Chohoonhee/Ev-LaFOR%7D">https://github.com/Chohoonhee/Ev-LaFOR}</a> 上下载。<details>
<summary>Abstract</summary>
Recognizing objects from sparse and noisy events becomes extremely difficult when paired images and category labels do not exist. In this paper, we study label-free event-based object recognition where category labels and paired images are not available. To this end, we propose a joint formulation of object recognition and image reconstruction in a complementary manner. Our method first reconstructs images from events and performs object recognition through Contrastive Language-Image Pre-training (CLIP), enabling better recognition through a rich context of images. Since the category information is essential in reconstructing images, we propose category-guided attraction loss and category-agnostic repulsion loss to bridge the textual features of predicted categories and the visual features of reconstructed images using CLIP. Moreover, we introduce a reliable data sampling strategy and local-global reconstruction consistency to boost joint learning of two tasks. To enhance the accuracy of prediction and quality of reconstruction, we also propose a prototype-based approach using unpaired images. Extensive experiments demonstrate the superiority of our method and its extensibility for zero-shot object recognition. Our project code is available at \url{https://github.com/Chohoonhee/Ev-LaFOR}.
</details>
<details>
<summary>摘要</summary>
Recognizing objects from sparse and noisy events becomes extremely difficult when paired images and category labels do not exist. In this paper, we study label-free event-based object recognition where category labels and paired images are not available. To this end, we propose a joint formulation of object recognition and image reconstruction in a complementary manner. Our method first reconstructs images from events and performs object recognition through Contrastive Language-Image Pre-training (CLIP), enabling better recognition through a rich context of images. Since the category information is essential in reconstructing images, we propose category-guided attraction loss and category-agnostic repulsion loss to bridge the textual features of predicted categories and the visual features of reconstructed images using CLIP. Moreover, we introduce a reliable data sampling strategy and local-global reconstruction consistency to boost joint learning of two tasks. To enhance the accuracy of prediction and quality of reconstruction, we also propose a prototype-based approach using unpaired images. Extensive experiments demonstrate the superiority of our method and its extensibility for zero-shot object recognition. Our project code is available at \url{https://github.com/Chohoonhee/Ev-LaFOR}.Here's the translation in Traditional Chinese:当事件为 sparse 和噪音时，对象识别成为极其困难，当 paired images 和 category labels 不存在时。在这篇文章中，我们研究了无标签事件基于的对象识别，其中 category labels 和 paired images 都不可用。为了解决这个问题，我们提出了一个组合的形式，即对象识别和图像重建的联合运算。我们的方法首先从事件中重建图像，然后通过 Contrastive Language-Image Pre-training (CLIP) 进行对象识别，这样可以通过充分的图像背景来提高识别的精度。由于类别信息是重建图像中的重要元素，我们提出了类别导引吸引损失和类别无关排斥损失，将文本特征与重建图像中的视觉特征相连接。此外，我们还引入了可靠的抽样策略和本地-全球重建一致性，以提高两个任务之间的联合学习。为了提高预测精度和重建质量，我们还提出了一个原型基于的方法，使用无标签图像。实验结果显示了我们的方法的超越性和可扩展性。我们的项目代码可以在 \url{https://github.com/Chohoonhee/Ev-LaFOR} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package"><a href="#Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package" class="headerlink" title="Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package"></a>Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09375">http://arxiv.org/abs/2308.09375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/behnoodrasti/hysupp">https://github.com/behnoodrasti/hysupp</a></li>
<li>paper_authors: Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot</li>
<li>for: 本文提供了一个概述先进和传统混合分析方法的评 comparison，以及三种不同类型的 linear unmixing 方法的性能对比。</li>
<li>methods: 本文使用了先进的 Image processing 和机器学习技术，包括超vised、semi-supervised 和 blind  linear unmixing 方法。</li>
<li>results: 实验结果表明，不同类型的 unmixing 方法在不同的混合分析场景下有不同的优势，以及一个开源的 Python 基于包可以在 <a target="_blank" rel="noopener" href="https://github.com/BehnoodRasti/HySUPP">https://github.com/BehnoodRasti/HySUPP</a> 上下载。<details>
<summary>Abstract</summary>
Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-source Python-based package available at https://github.com/BehnoodRasti/HySUPP to reproduce the results.
</details>
<details>
<summary>摘要</summary>
干扰像素通常是Materials的纯谱混合物，由于干扰数据探针的低空间分辨率、双折射和场景中Materials的密切混合，导致干扰像素的混合。拆分分析计算机程序可以Estimate the fractional abundance of endmembers within the pixel.根据对Endmember的知识，线性拆分可以分为三个主要类别：有监督、半监督和无监督（盲目）线性拆分。图像处理和机器学习技术的进步对拆分具有重要影响。本文提供了高级和传统拆分方法的总览，并对这些方法进行了 kritische 比较。我们在三个 simulated 和两个实际数据集上测试了不同拆分方法的性能。实验结果表明不同拆分类型在不同拆分场景中具有优势。此外，我们还提供了一个开源的 Python 基于 package，可以在 https://github.com/BehnoodRasti/HySUPP 上下载。
</details></li>
</ul>
<hr>
<h2 id="Single-Frame-Semantic-Segmentation-Using-Multi-Modal-Spherical-Images"><a href="#Single-Frame-Semantic-Segmentation-Using-Multi-Modal-Spherical-Images" class="headerlink" title="Single Frame Semantic Segmentation Using Multi-Modal Spherical Images"></a>Single Frame Semantic Segmentation Using Multi-Modal Spherical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09369">http://arxiv.org/abs/2308.09369</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sguttikon/SFSS-MMSI">https://github.com/sguttikon/SFSS-MMSI</a></li>
<li>paper_authors: Suresh Guttikonda, Jason Rambach</li>
<li>for: 这篇论文旨在探讨多modal的拼接和扩展对全景场景理解的可能性。</li>
<li>methods: 该论文提出了一种基于transformer的跨Modal融合架构，以AddressExtreme object deformation和全景图像扭曲。</li>
<li>results: 该论文在三个indoor全景视图数据集上进行了广泛的测试，并达到了当前领域最佳的mIoU性能：60.60%在Stanford2D3DS（RGB-HHA）、71.97%在Structured3D（RGB-D-N）以及35.92%在Matterport3D（RGB-D）。<details>
<summary>Abstract</summary>
In recent years, the research community has shown a lot of interest to panoramic images that offer a 360-degree directional perspective. Multiple data modalities can be fed, and complimentary characteristics can be utilized for more robust and rich scene interpretation based on semantic segmentation, to fully realize the potential. Existing research, however, mostly concentrated on pinhole RGB-X semantic segmentation. In this study, we propose a transformer-based cross-modal fusion architecture to bridge the gap between multi-modal fusion and omnidirectional scene perception. We employ distortion-aware modules to address extreme object deformations and panorama distortions that result from equirectangular representation. Additionally, we conduct cross-modal interactions for feature rectification and information exchange before merging the features in order to communicate long-range contexts for bi-modal and tri-modal feature streams. In thorough tests using combinations of four different modality types in three indoor panoramic-view datasets, our technique achieved state-of-the-art mIoU performance: 60.60% on Stanford2D3DS (RGB-HHA), 71.97% Structured3D (RGB-D-N), and 35.92% Matterport3D (RGB-D). We plan to release all codes and trained models soon.
</details>
<details>
<summary>摘要</summary>
近年来，研究社区对全景图像（panoramic image）表现出了很大的兴趣，这些图像可以提供360度方向的视角。多种数据模式可以被 fed，并可以利用不同特征进行更加robust和 ricn scene解释，以实现潜在的潜力。然而，现有的研究主要集中在pinhole RGB-X semantic segmentation。在本研究中，我们提议一种基于 transformer 混合模型来bridging the gap between multi-modal fusion和全景场景理解。我们使用扭曲感知模块来处理极端对象扭曲和全景图像扭曲，并进行交叉模式交互以便Feature rectification和信息交换，以便在bi-modal和tri-modal流水线中传递长距离上下文。在使用四种不同模式类型的三个indoor panoramic-view数据集进行了经验详细测试后，我们的技术实现了状态的最佳mIoU性能：60.60%在Stanford2D3DS（RGB-HHA）、71.97%在Structured3D（RGB-D-N）以及35.92%在Matterport3D（RGB-D）。我们计划在不久将所有代码和训练模型公开。
</details></li>
</ul>
<hr>
<h2 id="Overlap-Bias-Matching-is-Necessary-for-Point-Cloud-Registration"><a href="#Overlap-Bias-Matching-is-Necessary-for-Point-Cloud-Registration" class="headerlink" title="Overlap Bias Matching is Necessary for Point Cloud Registration"></a>Overlap Bias Matching is Necessary for Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09364">http://arxiv.org/abs/2308.09364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengcheng Shi, Jie Zhang, Haozhe Cheng, Junyang Wang, Yiyang Zhou, Chenlin Zhao, Jihua Zhu</li>
<li>for: 本研究旨在提出一种基于无监督学习的点云注册方法，以解决实际中点云注册问题中的受限 overlap 问题。</li>
<li>methods: 本方法基于一个名为 Overlap Bias Matching Network (OBMNet)，它包括两个Integral component： overlap sampling module和 bias prediction module。这两个组件可以捕捉点云重叠区域的分布，并预测点云共同结构的偏置系数。然后，我们将OBMM与邻居地图匹配模块结合，以精确地挖掘对应关系。</li>
<li>results: 实验结果表明，我们的方法在各种数据集上达到了与当前注册方法相比较显著的改进。<details>
<summary>Abstract</summary>
Point cloud registration is a fundamental problem in many domains. Practically, the overlap between point clouds to be registered may be relatively small. Most unsupervised methods lack effective initial evaluation of overlap, leading to suboptimal registration accuracy. To address this issue, we propose an unsupervised network Overlap Bias Matching Network (OBMNet) for partial point cloud registration. Specifically, we propose a plug-and-play Overlap Bias Matching Module (OBMM) comprising two integral components, overlap sampling module and bias prediction module. These two components are utilized to capture the distribution of overlapping regions and predict bias coefficients of point cloud common structures, respectively. Then, we integrate OBMM with the neighbor map matching module to robustly identify correspondences by precisely merging matching scores of points within the neighborhood, which addresses the ambiguities in single-point features. OBMNet can maintain efficacy even in pair-wise registration scenarios with low overlap ratios. Experimental results on extensive datasets demonstrate that our approach's performance achieves a significant improvement compared to the state-of-the-art registration approach.
</details>
<details>
<summary>摘要</summary>
点云注册是许多领域的基本问题。实际上，注册点云的重叠部分可能很小。大多数无监督方法缺乏有效的初始评估重叠，导致注册精度下降。为解决这个问题，我们提议一种无监督网络Overlap Bias Matching Network（OBMNet），用于部分点云注册。具体来说，我们提出了一个插件式的Overlap Bias Matching Module（OBMM），包括两个基本组成部分：重叠采样模块和偏好预测模块。这两个组成部分用于捕捉重叠区域的分布和预测点云共同结构的偏好系数，分别。然后，我们将OBMM与邻居地图匹配模块集成，以robustly确定对应关系，解决单点特征之间的混淆。OBMNet可以在对点云注册方式进行对比时保持效率。实验结果表明，我们的方法在评估中达到了与当前注册方法相比较显著的提升。
</details></li>
</ul>
<hr>
<h2 id="Open-vocabulary-Video-Question-Answering-A-New-Benchmark-for-Evaluating-the-Generalizability-of-Video-Question-Answering-Models"><a href="#Open-vocabulary-Video-Question-Answering-A-New-Benchmark-for-Evaluating-the-Generalizability-of-Video-Question-Answering-Models" class="headerlink" title="Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models"></a>Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09363">http://arxiv.org/abs/2308.09363</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlvlab/ovqa">https://github.com/mlvlab/ovqa</a></li>
<li>paper_authors: Dohwan Ko, Ji Soo Lee, Miso Choi, Jaewon Chu, Jihwan Park, Hyunwoo J. Kim<br>for: 这个论文的目的是提出一个新的benchmark，以评估视频问答模型的泛化能力。methods: 这个论文使用了一种新的GNN-based soft verbalizer来提高模型对不常见答案的预测能力，同时也对现有的开放端VIDEOQA模型进行了修改，以便进一步考虑罕见和未看过的答案。results: 该论文的实验结果表明，使用GNN-based soft verbalizer可以进一步提高模型的泛化能力，特别是对罕见和未看过的答案。此外，修改了现有的开放端VIDEOQA模型也可以提高其表现。<details>
<summary>Abstract</summary>
Video Question Answering (VideoQA) is a challenging task that entails complex multi-modal reasoning. In contrast to multiple-choice VideoQA which aims to predict the answer given several options, the goal of open-ended VideoQA is to answer questions without restricting candidate answers. However, the majority of previous VideoQA models formulate open-ended VideoQA as a classification task to classify the video-question pairs into a fixed answer set, i.e., closed-vocabulary, which contains only frequent answers (e.g., top-1000 answers). This leads the model to be biased toward only frequent answers and fail to generalize on out-of-vocabulary answers. We hence propose a new benchmark, Open-vocabulary Video Question Answering (OVQA), to measure the generalizability of VideoQA models by considering rare and unseen answers. In addition, in order to improve the model's generalization power, we introduce a novel GNN-based soft verbalizer that enhances the prediction on rare and unseen answers by aggregating the information from their similar words. For evaluation, we introduce new baselines by modifying the existing (closed-vocabulary) open-ended VideoQA models and improve their performances by further taking into account rare and unseen answers. Our ablation studies and qualitative analyses demonstrate that our GNN-based soft verbalizer further improves the model performance, especially on rare and unseen answers. We hope that our benchmark OVQA can serve as a guide for evaluating the generalizability of VideoQA models and inspire future research. Code is available at https://github.com/mlvlab/OVQA.
</details>
<details>
<summary>摘要</summary>
视频问答（VideoQA）是一项复杂的多模态逻辑任务。相比多选视频问答，开放式视频问答的目标是Answer questions without restricting candidate answers。然而，大多数前一代视频问答模型将开放式视频问答视为一种分类任务，即将视频-问题对 clasified into a fixed answer set，即关闭词汇，这会导致模型偏向常见答案，而忽略非常见答案。我们因此提出了一个新的标准测试集，开放词汇视频问答（OVQA），以衡量视频问答模型的通用性。此外，为提高模型的通用性能力，我们引入了一种新的图神经网络（GNN）基于的软化词语izer，该算法可以通过聚合相似词语的信息来提高对不常见答案的预测。我们的基线和精度分析表明，我们的GNN基于的软化词语izer可以进一步提高模型的性能，特别是对于不常见答案。我们希望，我们的标准测试集OVQA可以为评估视频问答模型的通用性提供指南，并且能启发未来的研究。代码可以在https://github.com/mlvlab/OVQA 中找到。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Target-Aware-Framework-for-Constrained-Image-Splicing-Detection-and-Localization"><a href="#Multi-scale-Target-Aware-Framework-for-Constrained-Image-Splicing-Detection-and-Localization" class="headerlink" title="Multi-scale Target-Aware Framework for Constrained Image Splicing Detection and Localization"></a>Multi-scale Target-Aware Framework for Constrained Image Splicing Detection and Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09357">http://arxiv.org/abs/2308.09357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Tan, Yuanman Li, Limin Zeng, Jiaxiong Ye, Wei wang, Xia Li</li>
<li>for: 批处 multimedia 图像中的剪辑检测和定位问题，即检测两个受控图像之间的剪辑操作并定位剪辑区域在两个图像上。</li>
<li>methods: 我们提出了一种多尺度目标意识框架，将特征提取和相关比较作为一个独立的管道进行联合处理，并设计了一种目标意识注意机制，使模型可以同时学习特征和相关比较。</li>
<li>results: 我们的方法在多个标准数据集上进行测试，与现有方法相比，具有更高的检测精度和更好的灵活性，并且可以有效地处理缩放变换。<details>
<summary>Abstract</summary>
Constrained image splicing detection and localization (CISDL) is a fundamental task of multimedia forensics, which detects splicing operation between two suspected images and localizes the spliced region on both images. Recent works regard it as a deep matching problem and have made significant progress. However, existing frameworks typically perform feature extraction and correlation matching as separate processes, which may hinder the model's ability to learn discriminative features for matching and can be susceptible to interference from ambiguous background pixels. In this work, we propose a multi-scale target-aware framework to couple feature extraction and correlation matching in a unified pipeline. In contrast to previous methods, we design a target-aware attention mechanism that jointly learns features and performs correlation matching between the probe and donor images. Our approach can effectively promote the collaborative learning of related patches, and perform mutual promotion of feature learning and correlation matching. Additionally, in order to handle scale transformations, we introduce a multi-scale projection method, which can be readily integrated into our target-aware framework that enables the attention process to be conducted between tokens containing information of varying scales. Our experiments demonstrate that our model, which uses a unified pipeline, outperforms state-of-the-art methods on several benchmark datasets and is robust against scale transformations.
</details>
<details>
<summary>摘要</summary>
《受限制的图像拼接检测和地点Localization（CISDL）是 multimedia 的基本任务，检测两个可疑图像之间的拼接操作并将拼接区域分别显示在两个图像上。现有的方法通常将特征提取和相关匹配视为两个独立的过程，这可能会阻碍模型学习特征特异的匹配特征，同时也可能受到杂质背景像素的干扰。在这种情况下，我们提出了一种多尺度目标意识框架，将特征提取和相关匹配作为一个统一的管道进行处理。与先前的方法不同，我们设计了一种目标意识的注意机制，同时学习特征和相关匹配。我们的方法可以有效地促进相关块之间的协同学习，并且可以同时促进特征学习和相关匹配的协同发展。此外，为了处理缩放变换，我们引入了多尺度投影方法，可以轻松地 интеGRATE到我们的目标意识框架中，使注意过程可以在不同的尺度上进行。我们的实验表明，使用统一管道的我们模型，在多个标准数据集上表现出色，并且对缩放变换具有鲁棒性。》
</details></li>
</ul>
<hr>
<h2 id="Boosting-Few-shot-Action-Recognition-with-Graph-guided-Hybrid-Matching"><a href="#Boosting-Few-shot-Action-Recognition-with-Graph-guided-Hybrid-Matching" class="headerlink" title="Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching"></a>Boosting Few-shot Action Recognition with Graph-guided Hybrid Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09346">http://arxiv.org/abs/2308.09346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiazheng-xing/gghm">https://github.com/jiazheng-xing/gghm</a></li>
<li>paper_authors: Jiazheng Xing, Mengmeng Wang, Yudi Ruan, Bofan Chen, Yaowei Guo, Boyu Mu, Guang Dai, Jingdong Wang, Yong Liu</li>
<li>for: 本文提出了一种新的几何匹配框架（GgHM），用于解决几何捕捉异常分类问题。</li>
<li>methods: 本文使用图 neural network 引导构建任务特有的特征，并对这些特征进行内部和间部特征相关性优化。然后，本文提出了一种混合匹配策略，将帧级匹配和元组级匹配相结合，以便将视频匹配到多种样式。最后，本文还提出了一种可学习的细致时间模型，以增强视频特征的时间表示，为匹配过程建立更加坚实的基础。</li>
<li>results: GgHM 在多个几何捕捉数据集上实现了一致性好于其他挑战性基准点，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Class prototype construction and matching are core aspects of few-shot action recognition. Previous methods mainly focus on designing spatiotemporal relation modeling modules or complex temporal alignment algorithms. Despite the promising results, they ignored the value of class prototype construction and matching, leading to unsatisfactory performance in recognizing similar categories in every task. In this paper, we propose GgHM, a new framework with Graph-guided Hybrid Matching. Concretely, we learn task-oriented features by the guidance of a graph neural network during class prototype construction, optimizing the intra- and inter-class feature correlation explicitly. Next, we design a hybrid matching strategy, combining frame-level and tuple-level matching to classify videos with multivariate styles. We additionally propose a learnable dense temporal modeling module to enhance the video feature temporal representation to build a more solid foundation for the matching process. GgHM shows consistent improvements over other challenging baselines on several few-shot datasets, demonstrating the effectiveness of our method. The code will be publicly available at https://github.com/jiazheng-xing/GgHM.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>前方法主要集中在设计空间时间关系模型或复杂的时间对Alignment算法。尽管其 promise 的结果，但它们忽略了类prototype构建和匹配的价值，导致每个任务中类似类型的识别性不够。在这篇论文中，我们提出了GgHM，一个新的框架，具有图导向混合匹配。具体来说，我们在类prototype构建过程中，通过图 neural network 的引导，显式地优化内类和外类特征相关性。接着，我们设计了混合匹配策略，将帧级匹配和元组级匹配结合起来，以便在多样化风格下分类视频。此外，我们还提出了一个可学习的紧凑时间模型，以强化视频特征的时间表示，以建立更坚实的匹配基础。GgHM在多个复杂的基准下显示了稳定的改进，证明了我们的方法的有效性。代码将在https://github.com/jiazheng-xing/GgHM上公开。
</details></li>
</ul>
<hr>
<h2 id="Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations"><a href="#Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations" class="headerlink" title="Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations"></a>Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09345">http://arxiv.org/abs/2308.09345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robert-graf/readable-conditional-denoising-diffusion">https://github.com/robert-graf/readable-conditional-denoising-diffusion</a></li>
<li>paper_authors: Robert Graf, Joachim Schmitt, Sarah Schlaeger, Hendrik Kristian Möller, Vasiliki Sideri-Lampretsa, Anjany Sekuboyina, Sandro Manuel Krieg, Benedikt Wiestler, Bjoern Menze, Daniel Rueckert, Jan Stefan Kirschke<br>for: This paper aims to develop and evaluate methods for translating spinal MR images to CT images, with a focus on accurately delineating posterior spine structures.methods: The study uses a combination of landmark-based registration and image-to-image translation techniques, including paired and unpaired methods such as Pix2Pix, DDIM, and SynDiff. The authors evaluate the performance of these methods using PSNR and Dice scores.results: The study finds that paired methods and SynDiff exhibit similar translation performance and Dice scores on paired data, while DDIM image mode achieves the highest image quality. The 3D translation methods outperform the 2D approach, providing anatomically accurate segmentations with improved Dice scores and avoiding underprediction of small structures like the spinous process.<details>
<summary>Abstract</summary>
Background: Automated segmentation of spinal MR images plays a vital role both scientifically and clinically. However, accurately delineating posterior spine structures presents challenges.   Methods: This retrospective study, approved by the ethical committee, involved translating T1w and T2w MR image series into CT images in a total of n=263 pairs of CT/MR series. Landmark-based registration was performed to align image pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit models (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired translation, SynDiff) image-to-image translation using "peak signal to noise ratio" (PSNR) as quality measure. A publicly available segmentation network segmented the synthesized CT datasets, and Dice scores were evaluated on in-house test sets and the "MRSpineSeg Challenge" volumes. The 2D findings were extended to 3D Pix2Pix and DDIM.   Results: 2D paired methods and SynDiff exhibited similar translation performance and Dice scores on paired data. DDIM image mode achieved the highest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated similar Dice scores (0.77). For craniocaudal axis rotations, at least two landmarks per vertebra were required for registration. The 3D translation outperformed the 2D approach, resulting in improved Dice scores (0.80) and anatomically accurate segmentations in a higher resolution than the original MR image.   Conclusion: Two landmarks per vertebra registration enabled paired image-to-image translation from MR to CT and outperformed all unpaired approaches. The 3D techniques provided anatomically correct segmentations, avoiding underprediction of small structures like the spinous process.
</details>
<details>
<summary>摘要</summary>
Methods: 这是一个回顾性研究，由伦敦病理学会approved的审核委员会批准。该研究将T1w和T2w MR图像系列转换为CT图像系列，共计n=263对CT/MR系列图像对应。使用了landmark-based registration来对图像对对应。我们使用"peak signal to noise ratio"（PSNR）作为质量指标，并对在家测试集和"MRSpineSeg Challenge"的Volume进行评估。使用了一个公共可用的分割网络将synthesized CT数据分割，并评估了Dice分数。Results: 2D对应方法和SynDiff在对应数据上 exhibited similar translation performance和Dice分数。DDIM图像模式实现了最高的图像质量。SynDiff、Pix2Pix和DDIM图像模式在对应数据上都表现了类似的Dice分数（0.77）。对于脊梗磁旋转轴，至少需要两个附加的landmark per vertebra进行 регистраción。3D翻译超过了2D方法，导致了改进的Dice分数（0.80）和精度高于原始MR图像的正确的分割。Conclusion: 使用两个附加的landmark per vertebra进行 registration可以实现对MR图像与CT图像的对应，并且超过了所有的无对应方法。3D技术提供了正确的分割，避免了小结构的下预测，如脊梗处。
</details></li>
</ul>
<hr>
<h2 id="LSCD-A-Large-Scale-Screen-Content-Dataset-for-Video-Compression"><a href="#LSCD-A-Large-Scale-Screen-Content-Dataset-for-Video-Compression" class="headerlink" title="LSCD: A Large-Scale Screen Content Dataset for Video Compression"></a>LSCD: A Large-Scale Screen Content Dataset for Video Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09332">http://arxiv.org/abs/2308.09332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Cheng, Siru Zhang, Yiqiang Yan, Rong Chen, Yun Zhang</li>
<li>for: 提供一个大规模屏幕内容数据集(LSCD)，用于促进屏幕内容视频压缩领域的研究。</li>
<li>methods: 使用人工智能和视频压缩技术，对屏幕内容视频进行学习型压缩。</li>
<li>results: 提供了一个大规模的屏幕内容视频压缩数据集(LSCD)，并对数据集进行分析，以帮助研究人员更好地理解屏幕内容视频的特点，并提高学习型压缩算法的开发。<details>
<summary>Abstract</summary>
Multimedia compression allows us to watch videos, see pictures and hear sounds within a limited bandwidth, which helps the flourish of the internet. During the past decades, multimedia compression has achieved great success using hand-craft features and systems. With the development of artificial intelligence and video compression, there emerges a lot of research work related to using the neural network on the video compression task to get rid of the complicated system. Not only producing the advanced algorithms, but researchers also spread the compression to different content, such as User Generated Content(UGC). With the rapid development of mobile devices, screen content videos become an important part of multimedia data. In contrast, we find community lacks a large-scale dataset for screen content video compression, which impedes the fast development of the corresponding learning-based algorithms. In order to fulfill this blank and accelerate the research of this special type of videos, we propose the Large-scale Screen Content Dataset(LSCD), which contains 714 source sequences. Meanwhile, we provide the analysis of the proposed dataset to show some features of screen content videos, which will help researchers have a better understanding of how to explore new algorithms. Besides collecting and post-processing the data to organize the dataset, we also provide a benchmark containing the performance of both traditional codec and learning-based methods.
</details>
<details>
<summary>摘要</summary>
Multimedia compression allow us to watch videos, see pictures and hear sounds within a limited bandwidth, which helps the flourish of the internet. During the past decades, multimedia compression has achieved great success using hand-craft features and systems. With the development of artificial intelligence and video compression, there emerges a lot of research work related to using the neural network on the video compression task to get rid of the complicated system. Not only producing the advanced algorithms, but researchers also spread the compression to different content, such as User Generated Content(UGC). With the rapid development of mobile devices, screen content videos become an important part of multimedia data. In contrast, we find community lacks a large-scale dataset for screen content video compression, which impedes the fast development of the corresponding learning-based algorithms. In order to fulfill this blank and accelerate the research of this special type of videos, we propose the Large-scale Screen Content Dataset(LSCD), which contains 714 source sequences. Meanwhile, we provide the analysis of the proposed dataset to show some features of screen content videos, which will help researchers have a better understanding of how to explore new algorithms. Besides collecting and post-processing the data to organize the dataset, we also provide a benchmark containing the performance of both traditional codec and learning-based methods.
</details></li>
</ul>
<hr>
<h2 id="SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT"><a href="#SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT" class="headerlink" title="SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT"></a>SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09331">http://arxiv.org/abs/2308.09331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Botond Fazekas, José Morano, Dmitrii Lachinov, Guilherme Aresta, Hrvoje Bogunović<br>for: 这篇论文主要是为了评估Segment Anything Model（SAM）在 RETOUCH 挑战中的大规模公共数据集上的应用。methods: 这篇论文使用了SAM和其修改版本进行了Retinal OCT影像分割的评估，并与当前领导的Retinal fluid segmentation方法进行了比较。results: 研究发现， adapted SAM在Retinal OCT影像分割中表现出了优异的能力，但在一些情况下仍落后于当前领导的方法。这些结果表明SAM在Retinal OCT图像分析中具有适应性和稳定性，并且可以作为Retinal OCT图像分析中的一种有价值工具。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has gained significant attention in the field of image segmentation due to its impressive capabilities and prompt-based interface. While SAM has already been extensively evaluated in various domains, its adaptation to retinal OCT scans remains unexplored. To bridge this research gap, we conduct a comprehensive evaluation of SAM and its adaptations on a large-scale public dataset of OCTs from RETOUCH challenge. Our evaluation covers diverse retinal diseases, fluid compartments, and device vendors, comparing SAM against state-of-the-art retinal fluid segmentation methods. Through our analysis, we showcase adapted SAM's efficacy as a powerful segmentation model in retinal OCT scans, although still lagging behind established methods in some circumstances. The findings highlight SAM's adaptability and robustness, showcasing its utility as a valuable tool in retinal OCT image analysis and paving the way for further advancements in this domain.
</details>
<details>
<summary>摘要</summary>
segmen anything model (SAM) 在图像分割领域备受关注，因其出色的能力和提示式界面。 although SAM 在不同领域得到了广泛的评估，它在Retinal OCT 图像中的适应仍然未经探索。 To bridge this research gap, we conducted a comprehensive evaluation of SAM and its adaptations on a large-scale public dataset of OCTs from RETOUCH challenge. 我们的评估覆盖了多种Retinal diseases, fluid compartments, and device vendors，比较SAM 与现有的Retinal fluid segmentation方法。 Through our analysis, we showcased adapted SAM's efficacy as a powerful segmentation model in retinal OCT scans, although still lagging behind established methods in some circumstances. The findings highlight SAM's adaptability and robustness, showcasing its utility as a valuable tool in retinal OCT image analysis and paving the way for further advancements in this domain.
</details></li>
</ul>
<hr>
<h2 id="Unlimited-Knowledge-Distillation-for-Action-Recognition-in-the-Dark"><a href="#Unlimited-Knowledge-Distillation-for-Action-Recognition-in-the-Dark" class="headerlink" title="Unlimited Knowledge Distillation for Action Recognition in the Dark"></a>Unlimited Knowledge Distillation for Action Recognition in the Dark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09327">http://arxiv.org/abs/2308.09327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruibing Jin, Guosheng Lin, Min Wu, Jie Lin, Zhengguo Li, Xiaoli Li, Zhenghua Chen</li>
<li>for: 提高动作识别网络学习的知识。</li>
<li>methods: 提出无限知识填充（UKD）技术，不需大量GPU内存，可以有效地融合不同的知识。</li>
<li>results: 对ARID数据集进行了广泛的实验，单流网络通过UKD的填充得到了优于两流网络的表现。<details>
<summary>Abstract</summary>
Dark videos often lose essential information, which causes the knowledge learned by networks is not enough to accurately recognize actions. Existing knowledge assembling methods require massive GPU memory to distill the knowledge from multiple teacher models into a student model. In action recognition, this drawback becomes serious due to much computation required by video process. Constrained by limited computation source, these approaches are infeasible. To address this issue, we propose an unlimited knowledge distillation (UKD) in this paper. Compared with existing knowledge assembling methods, our UKD can effectively assemble different knowledge without introducing high GPU memory consumption. Thus, the number of teaching models for distillation is unlimited. With our UKD, the network's learned knowledge can be remarkably enriched. Our experiments show that the single stream network distilled with our UKD even surpasses a two-stream network. Extensive experiments are conducted on the ARID dataset.
</details>
<details>
<summary>摘要</summary>
黑色视频常常会产生重要信息的丢失，导致网络学习的知识不够准确地识别动作。现有的知识组合方法需要巨量的GPU内存来浸泡多个教师模型中的知识到学生模型中。在动作识别 tasks中，这种缺点变得非常严重，因为视频处理需要很多计算。由于计算源有限，这些方法是不可能实现的。为解决这个问题，我们在这篇论文中提出了无限知识浸泡（UKD）方法。与现有的知识组合方法相比，我们的 UKD 可以无需高GPU内存占用，因此教师模型的数量是无限的。通过我们的 UKD，网络学习的知识可以备受扩展。我们的实验表明，单流网络通过我们的 UKD 甚至超过了两流网络。我们在 ARID 数据集上进行了广泛的实验。
</details></li>
</ul>
<hr>
<h2 id="Retro-FPN-Retrospective-Feature-Pyramid-Network-for-Point-Cloud-Semantic-Segmentation"><a href="#Retro-FPN-Retrospective-Feature-Pyramid-Network-for-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud Semantic Segmentation"></a>Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09314">http://arxiv.org/abs/2308.09314</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/allenxiangx/retro-fpn">https://github.com/allenxiangx/retro-fpn</a></li>
<li>paper_authors: Peng Xiang, Xin Wen, Yu-Shen Liu, Hui Zhang, Yi Fang, Zhizhong Han</li>
<li>for: 提高点云Semantic segmentation的精度，解决过去方法中的信息损失和不明确区域特征问题。</li>
<li>methods: 提出Retro-FPN方法，将每个点的特征预测设计为一个显式和回顾的修复过程，通过所有层次结构来提取semantic features。其关键新特点是一个Retro-Transformer，用于从前一层层次结构中概括semantic context，并在当前阶段修复特征。</li>
<li>results: 与州际标准背景方法相比，Retro-FPN可以显著提高性能。经过广泛的实验证明，Retro-FPN可以在多种常用的 benchmark 上达到州际标准水平。源代码可以在<a target="_blank" rel="noopener" href="https://github.com/AllenXiangX/Retro-FPN%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/AllenXiangX/Retro-FPN上获取。</a><details>
<summary>Abstract</summary>
Learning per-point semantic features from the hierarchical feature pyramid is essential for point cloud semantic segmentation. However, most previous methods suffered from ambiguous region features or failed to refine per-point features effectively, which leads to information loss and ambiguous semantic identification. To resolve this, we propose Retro-FPN to model the per-point feature prediction as an explicit and retrospective refining process, which goes through all the pyramid layers to extract semantic features explicitly for each point. Its key novelty is a retro-transformer for summarizing semantic contexts from the previous layer and accordingly refining the features in the current stage. In this way, the categorization of each point is conditioned on its local semantic pattern. Specifically, the retro-transformer consists of a local cross-attention block and a semantic gate unit. The cross-attention serves to summarize the semantic pattern retrospectively from the previous layer. And the gate unit carefully incorporates the summarized contexts and refines the current semantic features. Retro-FPN is a pluggable neural network that applies to hierarchical decoders. By integrating Retro-FPN with three representative backbones, including both point-based and voxel-based methods, we show that Retro-FPN can significantly improve performance over state-of-the-art backbones. Comprehensive experiments on widely used benchmarks can justify the effectiveness of our design. The source is available at https://github.com/AllenXiangX/Retro-FPN
</details>
<details>
<summary>摘要</summary>
学习每个点的 semantic 特征从层次特征 пирамид是semantic segmentation of point clouds 中的关键。然而，大多数之前的方法受到不明确的区域特征或不能有效地细化每个点的特征，导致信息损失和不明确的 semantic 标识。为解决这个问题，我们提出了Retro-FPN，它模型了每个点的特征预测为明确的和Retrospective 细化过程，通过所有层次来提取semantic 特征。其关键创新是一个Retro-transformer，它在上一层的semantic 上下文中进行总结，并在当前阶段细化特征。因此，每个点的分类是基于其当前semantic 模式。具体来说，Retro-transformer包括一个local cross-attention块和一个semantic gate单元。cross-attention 用于在上一层的semantic 模式中总结，并将其细化到当前阶段。而gate单元通过细化current semantic features ，以提高分类精度。Retro-FPN是一个可插入的神经网络，可以应用于层次解码器。通过与三种代表性的背景结构相结合，包括点基的和voxel基的方法，我们表明Retro-FPN可以在state-of-the-art 背景下显著提高性能。广泛的实验证明了我们的设计的有效性。源代码可以在https://github.com/AllenXiangX/Retro-FPN 中下载。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Image-Forgery-Detection-via-Contrastive-Learning-and-Unsupervised-Clustering"><a href="#Rethinking-Image-Forgery-Detection-via-Contrastive-Learning-and-Unsupervised-Clustering" class="headerlink" title="Rethinking Image Forgery Detection via Contrastive Learning and Unsupervised Clustering"></a>Rethinking Image Forgery Detection via Contrastive Learning and Unsupervised Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09307">http://arxiv.org/abs/2308.09307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/highwaywu/focal">https://github.com/highwaywu/focal</a></li>
<li>paper_authors: Haiwei Wu, Yiming Chen, Jiantao Zhou</li>
<li>for: 本研究旨在提高图像forge检测的精度和效果，并提出一种新的方法FOCAL（Forensic ContrAstive cLustering），这种方法基于对比学习和无监督划分，能够准确地检测图像中的forge区域。</li>
<li>methods: FOCAL方法包括三个主要部分：1）使用像素级对比学习来监督高级别侦验特征提取; 2）使用在线无监督划分算法来将学习到的特征分为forge和正常两类; 3）通过简单地Feature层 concatenation来进一步提高检测性能，无需重新训练。</li>
<li>results: 实验结果表明，FOCAL方法在六个公共测试数据集上达到了与state-of-the-art竞争算法之间的大幅提升：+24.3%的覆盖率、+18.6%的哥伦比亚、+17.5%的FF++, +14.2%的MISD、+13.5%的CASIA和+10.3%的NIST，以及IoU方面。<details>
<summary>Abstract</summary>
Image forgery detection aims to detect and locate forged regions in an image. Most existing forgery detection algorithms formulate classification problems to classify pixels into forged or pristine. However, the definition of forged and pristine pixels is only relative within one single image, e.g., a forged region in image A is actually a pristine one in its source image B (splicing forgery). Such a relative definition has been severely overlooked by existing methods, which unnecessarily mix forged (pristine) regions across different images into the same category. To resolve this dilemma, we propose the FOrensic ContrAstive cLustering (FOCAL) method, a novel, simple yet very effective paradigm based on contrastive learning and unsupervised clustering for the image forgery detection. Specifically, FOCAL 1) utilizes pixel-level contrastive learning to supervise the high-level forensic feature extraction in an image-by-image manner, explicitly reflecting the above relative definition; 2) employs an on-the-fly unsupervised clustering algorithm (instead of a trained one) to cluster the learned features into forged/pristine categories, further suppressing the cross-image influence from training data; and 3) allows to further boost the detection performance via simple feature-level concatenation without the need of retraining. Extensive experimental results over six public testing datasets demonstrate that our proposed FOCAL significantly outperforms the state-of-the-art competing algorithms by big margins: +24.3% on Coverage, +18.6% on Columbia, +17.5% on FF++, +14.2% on MISD, +13.5% on CASIA and +10.3% on NIST in terms of IoU. The paradigm of FOCAL could bring fresh insights and serve as a novel benchmark for the image forgery detection task. The code is available at https://github.com/HighwayWu/FOCAL.
</details>
<details>
<summary>摘要</summary>
Image forgery detection aims to detect and locate forged regions in an image. Most existing forgery detection algorithms formulate classification problems to classify pixels into forged or pristine. However, the definition of forged and pristine pixels is only relative within one single image, e.g., a forged region in image A is actually a pristine one in its source image B (splicing forgery). Such a relative definition has been severely overlooked by existing methods, which unnecessarily mix forged (pristine) regions across different images into the same category. To resolve this dilemma, we propose the FOrensic ContrAstive cLustering (FOCAL) method, a novel, simple yet very effective paradigm based on contrastive learning and unsupervised clustering for the image forgery detection. Specifically, FOCAL 1) utilizes pixel-level contrastive learning to supervise the high-level forensic feature extraction in an image-by-image manner, explicitly reflecting the above relative definition; 2) employs an on-the-fly unsupervised clustering algorithm (instead of a trained one) to cluster the learned features into forged/pristine categories, further suppressing the cross-image influence from training data; and 3) allows to further boost the detection performance via simple feature-level concatenation without the need of retraining. Extensive experimental results over six public testing datasets demonstrate that our proposed FOCAL significantly outperforms the state-of-the-art competing algorithms by big margins: +24.3% on Coverage, +18.6% on Columbia, +17.5% on FF++, +14.2% on MISD, +13.5% on CASIA and +10.3% on NIST in terms of IoU. The paradigm of FOCAL could bring fresh insights and serve as a novel benchmark for the image forgery detection task. The code is available at https://github.com/HighwayWu/FOCAL.
</details></li>
</ul>
<hr>
<h2 id="DiffDis-Empowering-Generative-Diffusion-Model-with-Cross-Modal-Discrimination-Capability"><a href="#DiffDis-Empowering-Generative-Diffusion-Model-with-Cross-Modal-Discrimination-Capability" class="headerlink" title="DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability"></a>DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09306">http://arxiv.org/abs/2308.09306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runhui Huang, Jianhua Han, Guansong Lu, Xiaodan Liang, Yihan Zeng, Wei Zhang, Hang Xu</li>
<li>for: 本文旨在探讨将生成和检测融合到一起的可能性，以提高图像生成和图像文本检测的性能。</li>
<li>methods: 本文提出了一种名为DiffDis的散度过程模型，它将批处理的散度过程与多modal的预训练模型（如CLIP、ALIGN和FILIP）结合起来，以实现图像生成和图像文本检测的同时学习。</li>
<li>results: 实验结果表明，DiffDis可以在12个数据集上的零 shot分类任务上提高平均精度0.65%，并在图像生成和图像文本检测任务上提高FID指标0.242%。<details>
<summary>Abstract</summary>
Recently, large-scale diffusion models, e.g., Stable diffusion and DallE2, have shown remarkable results on image synthesis. On the other hand, large-scale cross-modal pre-trained models (e.g., CLIP, ALIGN, and FILIP) are competent for various downstream tasks by learning to align vision and language embeddings. In this paper, we explore the possibility of jointly modeling generation and discrimination. Specifically, we propose DiffDis to unify the cross-modal generative and discriminative pretraining into one single framework under the diffusion process. DiffDis first formulates the image-text discriminative problem as a generative diffusion process of the text embedding from the text encoder conditioned on the image. Then, we propose a novel dual-stream network architecture, which fuses the noisy text embedding with the knowledge of latent images from different scales for image-text discriminative learning. Moreover, the generative and discriminative tasks can efficiently share the image-branch network structure in the multi-modality model. Benefiting from diffusion-based unified training, DiffDis achieves both better generation ability and cross-modal semantic alignment in one architecture. Experimental results show that DiffDis outperforms single-task models on both the image generation and the image-text discriminative tasks, e.g., 1.65% improvement on average accuracy of zero-shot classification over 12 datasets and 2.42 improvement on FID of zero-shot image synthesis.
</details>
<details>
<summary>摘要</summary>
近期，大规模扩散模型，如稳定扩散和DallE2，在图像生成方面已经显示出惊人的成绩。然而，大规模的交叉模态预训练模型（如CLIP、ALIGN和FILIP）在多种下游任务上表现出色，这些模型通过学习视觉和语言嵌入的对应关系来学习。在这篇论文中，我们探讨了将生成和批判合并到一起的可能性。特别是，我们提出了DiffDis模型，它将交叉模态生成和批判预训练集成为一个散射过程中的单一框架。DiffDis首先将图像-文本识别问题定义为图像编码器生成的文本嵌入在图像条件下的散射过程。然后，我们提出了一种新的双流网络架构，它将不同缩放的图像嵌入与文本嵌入进行混合，以进行图像-文本识别学习。此外，生成和批判任务可以在多模态模型中有效共享图像分支网络结构。由于散射基于的统一训练，DiffDis可以同时实现更好的生成能力和交叉模态含义对齐。实验结果表明，DiffDis在图像生成和图像-文本识别任务上都有更好的表现，比如在12个数据集上的平均精度为1.65%，和在FID上的图像生成精度为2.42%。
</details></li>
</ul>
<hr>
<h2 id="Human-Part-wise-3D-Motion-Context-Learning-for-Sign-Language-Recognition"><a href="#Human-Part-wise-3D-Motion-Context-Learning-for-Sign-Language-Recognition" class="headerlink" title="Human Part-wise 3D Motion Context Learning for Sign Language Recognition"></a>Human Part-wise 3D Motion Context Learning for Sign Language Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09305">http://arxiv.org/abs/2308.09305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taeryung Lee, Yeonguk Oh, Kyoung Mu Lee</li>
<li>for: 提高手语识别的表现，特别是利用手部特征来提高表现。</li>
<li>methods: 提出了一种基于人体部分动作上下文学习的框架，包括使用分解变换器（PET）和整体变换器（WET）来学习手部动作上下文，以及对2D和3D姿态进行ensemble。</li>
<li>results: 在WLASL数据集上实现了比前一代方法更高的表现，具体来说是通过学习手部动作上下文来提高手语识别的精度。<details>
<summary>Abstract</summary>
In this paper, we propose P3D, the human part-wise motion context learning framework for sign language recognition. Our main contributions lie in two dimensions: learning the part-wise motion context and employing the pose ensemble to utilize 2D and 3D pose jointly. First, our empirical observation implies that part-wise context encoding benefits the performance of sign language recognition. While previous methods of sign language recognition learned motion context from the sequence of the entire pose, we argue that such methods cannot exploit part-specific motion context. In order to utilize part-wise motion context, we propose the alternating combination of a part-wise encoding Transformer (PET) and a whole-body encoding Transformer (WET). PET encodes the motion contexts from a part sequence, while WET merges them into a unified context. By learning part-wise motion context, our P3D achieves superior performance on WLASL compared to previous state-of-the-art methods. Second, our framework is the first to ensemble 2D and 3D poses for sign language recognition. Since the 3D pose holds rich motion context and depth information to distinguish the words, our P3D outperformed the previous state-of-the-art methods employing a pose ensemble.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出P3D，人体部分动作上下文学习框架，用于手语识别。我们的主要贡献在两个维度：学习部分动作上下文和利用2D和3D pose共同。首先，我们的实验观察表明，部分动作上下文编码对手语识别性能有益。而前一代方法通常从整个姿态序列学习动作上下文，我们 argue那些方法无法利用部分动作上下文。为了利用部分动作上下文，我们提议使用分解成部分编码器（PET）和整体编码器（WET）的交互式组合。PET编码部分动作上下文，而WET将其合并为一个统一的上下文。通过学习部分动作上下文，我们的P3D在WLASL上比前一代方法更高的性能。其次，我们的框架是首次将2D和3D姿态共同用于手语识别。因为3D姿态具有较多的动作上下文和深度信息，我们的P3D在使用pose ensemble时表现出了更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning"><a href="#Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning" class="headerlink" title="Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning"></a>Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09303">http://arxiv.org/abs/2308.09303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moonjunyyy/si-blurry">https://github.com/moonjunyyy/si-blurry</a></li>
<li>paper_authors: Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, Gyeong-Moon Park</li>
<li>for: 本研究旨在 Addressing the challenges of continual learning in real-world scenarios, where the number of input data and tasks is constantly changing in a statistical way.</li>
<li>methods: 本研究提出了一种新的 Stochastic incremental Blurry task boundary scenario (Si-Blurry)，以及一种名为 Mask and Visual Prompt tuning (MVP) 的方法来解决 inter-和 intra-task 忘记问题和 class imbalance 问题。MVP 包括一种新的 instance-wise logit 遮盾和 contrastive visual prompt 准则，以及一种新的 gradient similarity-based focal loss 和 adaptive feature scaling。</li>
<li>results: 对于our challenging Si-Blurry scenario, extensive experiments show that our proposed MVP significantly outperforms the existing state-of-the-art methods.<details>
<summary>Abstract</summary>
Continual learning aims to learn a model from a continuous stream of data, but it mainly assumes a fixed number of data and tasks with clear task boundaries. However, in real-world scenarios, the number of input data and tasks is constantly changing in a statistical way, not a static way. Although recently introduced incremental learning scenarios having blurry task boundaries somewhat address the above issues, they still do not fully reflect the statistical properties of real-world situations because of the fixed ratio of disjoint and blurry samples. In this paper, we propose a new Stochastic incremental Blurry task boundary scenario, called Si-Blurry, which reflects the stochastic properties of the real-world. We find that there are two major challenges in the Si-Blurry scenario: (1) inter- and intra-task forgettings and (2) class imbalance problem. To alleviate them, we introduce Mask and Visual Prompt tuning (MVP). In MVP, to address the inter- and intra-task forgetting issues, we propose a novel instance-wise logit masking and contrastive visual prompt tuning loss. Both of them help our model discern the classes to be learned in the current batch. It results in consolidating the previous knowledge. In addition, to alleviate the class imbalance problem, we introduce a new gradient similarity-based focal loss and adaptive feature scaling to ease overfitting to the major classes and underfitting to the minor classes. Extensive experiments show that our proposed MVP significantly outperforms the existing state-of-the-art methods in our challenging Si-Blurry scenario.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文。<</SYS>>持续学习目标是学习从连续流动的数据中学习模型，但是它主要假设Fixed数据量和任务数量，并且任务boundary是明确的。然而，在实际情况下，输入数据和任务的数量在统计方式上不断变化，而不是静态的方式。虽然最近引入的增量学习方式有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方有些地方
</details></li>
</ul>
<hr>
<h2 id="Inferior-Alveolar-Nerve-Segmentation-in-CBCT-images-using-Connectivity-Based-Selective-Re-training"><a href="#Inferior-Alveolar-Nerve-Segmentation-in-CBCT-images-using-Connectivity-Based-Selective-Re-training" class="headerlink" title="Inferior Alveolar Nerve Segmentation in CBCT images using Connectivity-Based Selective Re-training"></a>Inferior Alveolar Nerve Segmentation in CBCT images using Connectivity-Based Selective Re-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09298">http://arxiv.org/abs/2308.09298</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/garynico517/ssl-ian-retraining">https://github.com/garynico517/ssl-ian-retraining</a></li>
<li>paper_authors: Yusheng Liu, Rui Xin, Tao Yang, Lisheng Wang</li>
<li>for: 这个论文的目的是提高自动尖齿神经 canal  segmentation 的能力，以便在 dental 和 maxillofacial 手术中避免不可逆的神经损伤。</li>
<li>methods: 作者提出了一种基于 IAN 连接性的选择性重训练方法，以解决自动 segmentation 中 sparse labeling 的负面影响。</li>
<li>results: 作者的方法在 ToothFairy 验证集上进行了量化评估，达到了 dice similarity coefficient (DSC) 0.7956，和 95% hausdorff distance (HD95) 4.4905，并在竞赛中获得冠军。<details>
<summary>Abstract</summary>
Inferior Alveolar Nerve (IAN) canal detection in CBCT is an important step in many dental and maxillofacial surgery applications to prevent irreversible damage to the nerve during the procedure.The ToothFairy2023 Challenge aims to establish a 3D maxillofacial dataset consisting of all sparse labels and partial dense labels, and improve the ability of automatic IAN segmentation. In this work, in order to avoid the negative impact brought by sparse labeling, we transform the mixed supervised problem into a semi-supervised problem. Inspired by self-training via pseudo labeling, we propose a selective re-training framework based on IAN connectivity. Our method is quantitatively evaluated on the ToothFairy verification cases, achieving the dice similarity coefficient (DSC) of 0.7956, and 95\% hausdorff distance (HD95) of 4.4905, and wining the champion in the competition. Code is available at https://github.com/GaryNico517/SSL-IAN-Retraining.
</details>
<details>
<summary>摘要</summary>
“ inferior alveolar nerve（IAN） Canal detection in CBCT 是 dental 和 maxillofacial surgery 中重要的一步，以避免在过程中对神经造成 irreversible 的损害。ToothFairy2023 Challenge 目标是建立一个 3D maxillofacial 数据集，包括所有稀疏标签和部分杂散标签，以提高自动 IAN 分割能力。在本工作中，以免除稀疏标签的负面影响，我们将混合监督问题转化为 semi-supervised 问题。受 pseudo labeling 的启发，我们提出一种选择性重训练框架，基于 IAN 连接性。我们的方法在 ToothFairy 验证例程中被评估，达到了 dice similarity coefficient（DSC）0.7956，和 Hausdorff distance（HD95）4.4905，并在竞赛中获胜。代码可以在 https://github.com/GaryNico517/SSL-IAN-Retraining 上获取。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="NAPA-VQ-Neighborhood-Aware-Prototype-Augmentation-with-Vector-Quantization-for-Continual-Learning"><a href="#NAPA-VQ-Neighborhood-Aware-Prototype-Augmentation-with-Vector-Quantization-for-Continual-Learning" class="headerlink" title="NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization for Continual Learning"></a>NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09297">http://arxiv.org/abs/2308.09297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tamasham/napa-vq">https://github.com/tamasham/napa-vq</a></li>
<li>paper_authors: Tamasha Malepathirana, Damith Senanayake, Saman Halgamuge</li>
<li>For: 强调在深度神经网络中避免严重遗传问题，即在新知获得时不损失之前知识。* Methods: 基于非特例学习（NECIL），不使用过去的例子来学习新的类别，并且通过对邻近类别的了解来增强分类器的决策界。* Results: 与现有的State-of-the-art NECIL方法比较，NAPA-VQ方法在CIFAR-100、TinyImageNet和ImageNet-Subset上的实验结果显示，获得了5%、2%和4%的精度提升和10%、3%和9%的遗传减少。<details>
<summary>Abstract</summary>
Catastrophic forgetting; the loss of old knowledge upon acquiring new knowledge, is a pitfall faced by deep neural networks in real-world applications. Many prevailing solutions to this problem rely on storing exemplars (previously encountered data), which may not be feasible in applications with memory limitations or privacy constraints. Therefore, the recent focus has been on Non-Exemplar based Class Incremental Learning (NECIL) where a model incrementally learns about new classes without using any past exemplars. However, due to the lack of old data, NECIL methods struggle to discriminate between old and new classes causing their feature representations to overlap. We propose NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization, a framework that reduces this class overlap in NECIL. We draw inspiration from Neural Gas to learn the topological relationships in the feature space, identifying the neighboring classes that are most likely to get confused with each other. This neighborhood information is utilized to enforce strong separation between the neighboring classes as well as to generate old class representative prototypes that can better aid in obtaining a discriminative decision boundary between old and new classes. Our comprehensive experiments on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that NAPA-VQ outperforms the State-of-the-art NECIL methods by an average improvement of 5%, 2%, and 4% in accuracy and 10%, 3%, and 9% in forgetting respectively. Our code can be found in https://github.com/TamashaM/NAPA-VQ.git.
</details>
<details>
<summary>摘要</summary>
深度神经网络在实际应用中面临的一个挑战是 catastrophic forgetting，即因为新知约而失去老知识的问题。许多现有的解决方案是通过存储过去的数据（例emplars），但这可能不是应用中存储限制或隐私限制的情况下可行。因此，近期的注意点在 Non-Exemplar based Class Incremental Learning (NECIL) 方面，这种方法可以在没有过去数据的情况下，逐步学习新类。然而，由于缺乏过去数据，NECIL 方法可能会将旧类和新类的特征表示相互混淆。我们提出了 NAPA-VQ：它是一种基于 Neural Gas 学习特征空间中类之间的 topological 关系的框架，以便在 feature 空间中强制分离邻近类。此外，它还可以生成旧类代表对象，以便更好地帮助得出精准的决策边界 между旧类和新类。我们的全面实验表明，NAPA-VQ 在 CIFAR-100、TinyImageNet 和 ImageNet-Subset 上比 State-of-the-art NECIL 方法平均提高了5%, 2%, 4% 的准确率和10%, 3%, 9% 的忘记率。我们的代码可以在 https://github.com/TamashaM/NAPA-VQ.git 找到。
</details></li>
</ul>
<hr>
<h2 id="Self-Calibrated-Cross-Attention-Network-for-Few-Shot-Segmentation"><a href="#Self-Calibrated-Cross-Attention-Network-for-Few-Shot-Segmentation" class="headerlink" title="Self-Calibrated Cross Attention Network for Few-Shot Segmentation"></a>Self-Calibrated Cross Attention Network for Few-Shot Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09294">http://arxiv.org/abs/2308.09294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sam1224/sccan">https://github.com/sam1224/sccan</a></li>
<li>paper_authors: Qianxiong Xu, Wenting Zhao, Guosheng Lin, Cheng Long<br>for:This paper focuses on improving few-shot segmentation (FSS) by effectively utilizing support samples.methods:The proposed method uses a self-calibrated cross attention (SCCA) block, which splits the query and support features into patches, aligns each query patch with its most similar support patch, and fuses the query background features with matched background features from the support image.results:The proposed method achieves state-of-the-art performance on PASCAL-5^i and COCO-20^i under 5-shot setting, with a mIoU score of 5.6% better than previous state-of-the-arts on COCO-20^i.<details>
<summary>Abstract</summary>
The key to the success of few-shot segmentation (FSS) lies in how to effectively utilize support samples. Most solutions compress support foreground (FG) features into prototypes, but lose some spatial details. Instead, others use cross attention to fuse query features with uncompressed support FG. Query FG could be fused with support FG, however, query background (BG) cannot find matched BG features in support FG, yet inevitably integrates dissimilar features. Besides, as both query FG and BG are combined with support FG, they get entangled, thereby leading to ineffective segmentation. To cope with these issues, we design a self-calibrated cross attention (SCCA) block. For efficient patch-based attention, query and support features are firstly split into patches. Then, we design a patch alignment module to align each query patch with its most similar support patch for better cross attention. Specifically, SCCA takes a query patch as Q, and groups the patches from the same query image and the aligned patches from the support image as K&V. In this way, the query BG features are fused with matched BG features (from query patches), and thus the aforementioned issues will be mitigated. Moreover, when calculating SCCA, we design a scaled-cosine mechanism to better utilize the support features for similarity calculation. Extensive experiments conducted on PASCAL-5^i and COCO-20^i demonstrate the superiority of our model, e.g., the mIoU score under 5-shot setting on COCO-20^i is 5.6%+ better than previous state-of-the-arts. The code is available at https://github.com/Sam1224/SCCAN.
</details>
<details>
<summary>摘要</summary>
针对几何shot segmentation（FSS）的成功关键在于如何有效利用支持样本。大多数解决方案将支持背景（BG）特征压缩成原型，但是会产生一些空间细节的损失。其他人则使用批注注意力机制来融合查询特征和未压缩的支持BG。然而，查询BG无法在支持BG中找到匹配的BG特征，却必然混合不同的特征。此外，由于查询BG和支持BG都与支持BG进行融合，因此会导致不准确的分割。为了解决这些问题，我们设计了一个自适应批注注意力块（SCCA）。SCCA块的实现方式如下：首先，我们将查询特征和支持特征切分成小块。然后，我们设计了一个块对齐模块，用于将每个查询块与其最相似的支持块进行对齐。这样，查询BG特征可以与支持BG中的匹配BG特征进行混合，从而解决上述问题。此外，在计算SCCA时，我们设计了一个托管整数机制，以更好地利用支持特征进行相似性计算。我们在PASCAL-5^i和COCO-20^i上进行了广泛的实验，结果显示我们的模型在5架shot设定下的mIoU分数比前一个状态艺术高出5.6%。代码可以在https://github.com/Sam1224/SCCAN中下载。
</details></li>
</ul>
<hr>
<h2 id="RFDforFin-Robust-Deep-Forgery-Detection-for-GAN-generated-Fingerprint-Images"><a href="#RFDforFin-Robust-Deep-Forgery-Detection-for-GAN-generated-Fingerprint-Images" class="headerlink" title="RFDforFin: Robust Deep Forgery Detection for GAN-generated Fingerprint Images"></a>RFDforFin: Robust Deep Forgery Detection for GAN-generated Fingerprint Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09285">http://arxiv.org/abs/2308.09285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Miao, Yuanfang Guo, Yunhong Wang</li>
<li>for: 防止GAN生成的指纹图像被恶意褪别，以保护公共安全。</li>
<li>methods:  combining unique ridge features of fingerprint and generation artifacts of the GAN-generated images to detect deep forgery.</li>
<li>results: 提出了首个针对指纹图像的深度褪别检测方法，实现了低复杂度和高效果。<details>
<summary>Abstract</summary>
With the rapid development of the image generation technologies, the malicious abuses of the GAN-generated fingerprint images poses a significant threat to the public safety in certain circumstances. Although the existing universal deep forgery detection approach can be applied to detect the fake fingerprint images, they are easily attacked and have poor robustness. Meanwhile, there is no specifically designed deep forgery detection method for fingerprint images. In this paper, we propose the first deep forgery detection approach for fingerprint images, which combines unique ridge features of fingerprint and generation artifacts of the GAN-generated images, to the best of our knowledge. Specifically, we firstly construct a ridge stream, which exploits the grayscale variations along the ridges to extract unique fingerprint-specific features. Then, we construct a generation artifact stream, in which the FFT-based spectrums of the input fingerprint images are exploited, to extract more robust generation artifact features. At last, the unique ridge features and generation artifact features are fused for binary classification (\textit{i.e.}, real or fake). Comprehensive experiments demonstrate that our proposed approach is effective and robust with low complexities.
</details>
<details>
<summary>摘要</summary>
随着图像生成技术的快速发展，GAN生成的指纹图像在某些情况下可能会对公共安全构成威胁。虽然现有的通用深度伪造检测方法可以检测假指纹图像，但它们容易受到攻击并有低效率。此外，没有专门设计的深度伪造检测方法 для指纹图像。在这篇论文中，我们提出了首个深度伪造检测方法 для指纹图像，该方法结合了指纹特有的缝合特征和GAN生成图像的生成痕迹特征。具体来说，我们首先构建缝合流，利用缝合中的灰度变化来提取指纹特有的特征。然后，我们构建生成痕迹流，利用输入指纹图像的FFT基准谱来提取更加鲁棒的生成痕迹特征。最后，缝合特征和生成痕迹特征进行 binary 分类（即真实或假）。广泛的实验表明，我们提出的方法有效和可靠，并且具有低复杂性。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Cotraining-Makes-Strong-Semi-Supervised-Segmentor"><a href="#Diverse-Cotraining-Makes-Strong-Semi-Supervised-Segmentor" class="headerlink" title="Diverse Cotraining Makes Strong Semi-Supervised Segmentor"></a>Diverse Cotraining Makes Strong Semi-Supervised Segmentor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09281">http://arxiv.org/abs/2308.09281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijiang Li, Xinjiang Wang, Lihe Yang, Litong Feng, Wayne Zhang, Ying Gao</li>
<li>for: 本研究旨在探讨深度共训的工作机制，并证明多视图支持的假设不符合现实。</li>
<li>methods: 本研究使用多种方法来增加共训模型的多样性，包括输入领域、不同的扩展和网络架构。</li>
<li>results:  compared to current state-of-the-art 方法，我们的多样共训方法在不同的评估协议上取得了明显的提高，例如在 Pascal 上 achieve the best mIoU of 76.2%, 77.7% and 80.2% with only 92, 183 and 366 labeled images.<details>
<summary>Abstract</summary>
Deep co-training has been introduced to semi-supervised segmentation and achieves impressive results, yet few studies have explored the working mechanism behind it. In this work, we revisit the core assumption that supports co-training: multiple compatible and conditionally independent views. By theoretically deriving the generalization upper bound, we prove the prediction similarity between two models negatively impacts the model's generalization ability. However, most current co-training models are tightly coupled together and violate this assumption. Such coupling leads to the homogenization of networks and confirmation bias which consequently limits the performance. To this end, we explore different dimensions of co-training and systematically increase the diversity from the aspects of input domains, different augmentations and model architectures to counteract homogenization. Our Diverse Co-training outperforms the state-of-the-art (SOTA) methods by a large margin across different evaluation protocols on the Pascal and Cityscapes. For example. we achieve the best mIoU of 76.2%, 77.7% and 80.2% on Pascal with only 92, 183 and 366 labeled images, surpassing the previous best results by more than 5%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DiffLLE-Diffusion-guided-Domain-Calibration-for-Unsupervised-Low-light-Image-Enhancement"><a href="#DiffLLE-Diffusion-guided-Domain-Calibration-for-Unsupervised-Low-light-Image-Enhancement" class="headerlink" title="DiffLLE: Diffusion-guided Domain Calibration for Unsupervised Low-light Image Enhancement"></a>DiffLLE: Diffusion-guided Domain Calibration for Unsupervised Low-light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09279">http://arxiv.org/abs/2308.09279</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhou Yang, Xuanyu Zhang, Yinhuai Wang, Jiwen Yu, Yuhan Wang, Jian Zhang</li>
<li>for: 这篇论文是为了提出一种robust和有效的无监督低光照图像改善方法，即Diffusion-based domain calibration（DiffLLE）。</li>
<li>methods: 该方法采用了一种简单的无监督增强算法，并采用了两个附加的零扩展插件模块：Diffusion-guided Degradation Calibration（DDC）模块和Fine-grained Target domain Distillation（FTD）模块。</li>
<li>results: 该方法在多种实验中表现出色，甚至超过了一些监督学习方法。<details>
<summary>Abstract</summary>
Existing unsupervised low-light image enhancement methods lack enough effectiveness and generalization in practical applications. We suppose this is because of the absence of explicit supervision and the inherent gap between real-world scenarios and the training data domain. In this paper, we develop Diffusion-based domain calibration to realize more robust and effective unsupervised Low-Light Enhancement, called DiffLLE. Since the diffusion model performs impressive denoising capability and has been trained on massive clean images, we adopt it to bridge the gap between the real low-light domain and training degradation domain, while providing efficient priors of real-world content for unsupervised models. Specifically, we adopt a naive unsupervised enhancement algorithm to realize preliminary restoration and design two zero-shot plug-and-play modules based on diffusion model to improve generalization and effectiveness. The Diffusion-guided Degradation Calibration (DDC) module narrows the gap between real-world and training low-light degradation through diffusion-based domain calibration and a lightness enhancement curve, which makes the enhancement model perform robustly even in sophisticated wild degradation. Due to the limited enhancement effect of the unsupervised model, we further develop the Fine-grained Target domain Distillation (FTD) module to find a more visual-friendly solution space. It exploits the priors of the pre-trained diffusion model to generate pseudo-references, which shrinks the preliminary restored results from a coarse normal-light domain to a finer high-quality clean field, addressing the lack of strong explicit supervision for unsupervised methods. Benefiting from these, our approach even outperforms some supervised methods by using only a simple unsupervised baseline. Extensive experiments demonstrate the superior effectiveness of the proposed DiffLLE.
</details>
<details>
<summary>摘要</summary>
We adopt a diffusion model that has been trained on massive clean images, and use it to bridge the gap between the real low-light domain and the training degradation domain. This provides efficient priors of real-world content for unsupervised models. We use a naive unsupervised enhancement algorithm to realize preliminary restoration, and design two zero-shot plug-and-play modules based on the diffusion model to improve generalization and effectiveness.The Diffusion-guided Degradation Calibration (DDC) module narrows the gap between real-world and training low-light degradation through diffusion-based domain calibration and a lightness enhancement curve. This makes the enhancement model perform robustly even in sophisticated wild degradation. However, the limited enhancement effect of the unsupervised model leads to the development of the Fine-grained Target domain Distillation (FTD) module. This module exploits the priors of the pre-trained diffusion model to generate pseudo-references, which shrink the preliminary restored results from a coarse normal-light domain to a finer high-quality clean field. This addresses the lack of strong explicit supervision for unsupervised methods.Our approach outperforms some supervised methods using only a simple unsupervised baseline. Extensive experiments demonstrate the superior effectiveness of the proposed DiffLLE.
</details></li>
</ul>
<hr>
<h2 id="MATLABER-Material-Aware-Text-to-3D-via-LAtent-BRDF-auto-EncodeR"><a href="#MATLABER-Material-Aware-Text-to-3D-via-LAtent-BRDF-auto-EncodeR" class="headerlink" title="MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR"></a>MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09278">http://arxiv.org/abs/2308.09278</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SheldonTsui/Matlaber">https://github.com/SheldonTsui/Matlaber</a></li>
<li>paper_authors: Xudong Xu, Zhaoyang Lyu, Xingang Pan, Bo Dai<br>for:本研究旨在提高文本到3D图像生成中的物料质量，通过使用新的秘密BRDF自动编码器（Latent BRDF auto-EncodeR，简称MATLABER）。methods:我们使用大规模的真实世界BRDF收集来训练这个自动编码器，并确保其隐藏空间的光滑性，这些隐藏空间自然变为物料的自然分布。在文本到3D图像生成中，我们在物料预测中使用这些隐藏空间编码器，而不是直接使用BRDF参数。results:我们的方法在生成物料的实际和准确性方面表现出色，并且高质量的物料自然地启用了多个下游任务，如重新照明和物料编辑。<details>
<summary>Abstract</summary>
Based on powerful text-to-image diffusion models, text-to-3D generation has made significant progress in generating compelling geometry and appearance. However, existing methods still struggle to recover high-fidelity object materials, either only considering Lambertian reflectance, or failing to disentangle BRDF materials from the environment lights. In this work, we propose Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR (\textbf{MATLABER}) that leverages a novel latent BRDF auto-encoder for material generation. We train this auto-encoder with large-scale real-world BRDF collections and ensure the smoothness of its latent space, which implicitly acts as a natural distribution of materials. During appearance modeling in text-to-3D generation, the latent BRDF embeddings, rather than BRDF parameters, are predicted via a material network. Through exhaustive experiments, our approach demonstrates the superiority over existing ones in generating realistic and coherent object materials. Moreover, high-quality materials naturally enable multiple downstream tasks such as relighting and material editing. Code and model will be publicly available at \url{https://sheldontsui.github.io/projects/Matlaber}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Progression-Guided-Temporal-Action-Detection-in-Videos"><a href="#Progression-Guided-Temporal-Action-Detection-in-Videos" class="headerlink" title="Progression-Guided Temporal Action Detection in Videos"></a>Progression-Guided Temporal Action Detection in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09268">http://arxiv.org/abs/2308.09268</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/makecent/apn">https://github.com/makecent/apn</a></li>
<li>paper_authors: Chongkai Lu, Man-Wai Mak, Ruimin Li, Zheru Chi, Hong Fu</li>
<li>For: The paper proposes a novel framework called Action Progression Network (APN) for temporal action detection (TAD) in videos.* Methods: The APN framework uses a complete action process to encode the temporal structure of actions, and trains a neural network to recognize the action progressions. The framework detects action boundaries by detecting complete action processes in the videos.* Results: The APN achieves competitive performance and significantly surpasses its counterparts in detecting long-lasting actions, with a mean Average Precision (mAP) of 58.3% on the THUMOS14 dataset and 98.9% mAP on the DFMAD70 dataset.Here’s the same information in Simplified Chinese text:* For: 本文提出了一种名为Action Progression Network (APN)的新框架，用于视频中的时间动作检测 (TAD)。* Methods: APN框架使用完整的动作进程来编码动作的时间结构，然后使用神经网络来识别动作进程。框架通过检测视频中的完整动作进程来检测动作边界。* Results: APN达到了竞争性的性能，并在检测持续时间的动作方面表现出色，THUMOS14 dataset上的mean Average Precision (mAP)为58.3%，DFMAD70 dataset上的mAP为98.9%。<details>
<summary>Abstract</summary>
We present a novel framework, Action Progression Network (APN), for temporal action detection (TAD) in videos. The framework locates actions in videos by detecting the action evolution process. To encode the action evolution, we quantify a complete action process into 101 ordered stages (0\%, 1\%, ..., 100\%), referred to as action progressions. We then train a neural network to recognize the action progressions. The framework detects action boundaries by detecting complete action processes in the videos, e.g., a video segment with detected action progressions closely follow the sequence 0\%, 1\%, ..., 100\%. The framework offers three major advantages: (1) Our neural networks are trained end-to-end, contrasting conventional methods that optimize modules separately; (2) The APN is trained using action frames exclusively, enabling models to be trained on action classification datasets and robust to videos with temporal background styles differing from those in training; (3) Our framework effectively avoids detecting incomplete actions and excels in detecting long-lasting actions due to the fine-grained and explicit encoding of the temporal structure of actions. Leveraging these advantages, the APN achieves competitive performance and significantly surpasses its counterparts in detecting long-lasting actions. With an IoU threshold of 0.5, the APN achieves a mean Average Precision (mAP) of 58.3\% on the THUMOS14 dataset and 98.9\% mAP on the DFMAD70 dataset.
</details>
<details>
<summary>摘要</summary>
我们提出了一个新的框架，即 Action Progression Network (APN)，用于视频中的时间动作检测（TAD）。这个框架通过检测动作进程来确定动作的位置。为了编码动作进程，我们将完整的动作过程分解为101个顺序阶段（0%、1%、...、100%），称之为动作进程。然后我们将神经网络训练以认识这些动作进程。框架通过检测视频中的完整动作进程来检测动作边界，例如视频段中的检测动作进程与序列0%、1%、...、100%的顺序匹配。框架具有以下三大优点：1. 我们的神经网络是通过端到端训练的，与传统方法不同，这些方法通常将模块分割并优化。2. APN使用唯一的动作帧进行训练，因此模型可以在动作分类数据集上训练，并且对视频中的时间背景样式不同于训练时的样式具有鲁棒性。3. 我们的框架可以准确地检测长时间的动作，因为它使用细化和明确的时间结构编码器，从而避免检测不完整的动作。通过这些优点，APN在检测长时间动作方面实现了竞争性的性能，并在THUMOS14和DFMAD70数据集上达到了98.9%的mAP和58.3%的mAP。
</details></li>
</ul>
<hr>
<h2 id="SparseBEV-High-Performance-Sparse-3D-Object-Detection-from-Multi-Camera-Videos"><a href="#SparseBEV-High-Performance-Sparse-3D-Object-Detection-from-Multi-Camera-Videos" class="headerlink" title="SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos"></a>SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09244">http://arxiv.org/abs/2308.09244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mcg-nju/sparsebev">https://github.com/mcg-nju/sparsebev</a></li>
<li>paper_authors: Haisong Liu, Yao Teng, Tao Lu, Haiguang Wang, Limin Wang</li>
<li>for: This paper focuses on developing a fully sparse 3D object detector, SparseBEV, to mitigate the performance gap between sparse and dense detectors in camera-based 3D object detection.</li>
<li>methods: SparseBEV uses a query-based paradigm without explicit dense BEV feature construction, and includes three key designs: scale-adaptive self attention, adaptive spatio-temporal sampling, and adaptive mixing.</li>
<li>results: On the test split of nuScenes, SparseBEV achieves the state-of-the-art performance of 67.5 NDS. On the val split, SparseBEV achieves 55.8 NDS while maintaining a real-time inference speed of 23.5 FPS.<details>
<summary>Abstract</summary>
Camera-based 3D object detection in BEV (Bird's Eye View) space has drawn great attention over the past few years. Dense detectors typically follow a two-stage pipeline by first constructing a dense BEV feature and then performing object detection in BEV space, which suffers from complex view transformations and high computation cost. On the other side, sparse detectors follow a query-based paradigm without explicit dense BEV feature construction, but achieve worse performance than the dense counterparts. In this paper, we find that the key to mitigate this performance gap is the adaptability of the detector in both BEV and image space. To achieve this goal, we propose SparseBEV, a fully sparse 3D object detector that outperforms the dense counterparts. SparseBEV contains three key designs, which are (1) scale-adaptive self attention to aggregate features with adaptive receptive field in BEV space, (2) adaptive spatio-temporal sampling to generate sampling locations under the guidance of queries, and (3) adaptive mixing to decode the sampled features with dynamic weights from the queries. On the test split of nuScenes, SparseBEV achieves the state-of-the-art performance of 67.5 NDS. On the val split, SparseBEV achieves 55.8 NDS while maintaining a real-time inference speed of 23.5 FPS. Code is available at https://github.com/MCG-NJU/SparseBEV.
</details>
<details>
<summary>摘要</summary>
过去几年，基于 bird's eye view（BEV）空间的几何检测器在Camera-based 3D объек体检测领域中吸引了很大的注意力。紧密的检测器通常遵循一个 two-stage 管道，首先是在BEV空间建立紧密的BEV特征，然后进行 объек detection，这会受到复杂的视野转换和高计算成本的影响。另一方面，稀疏的检测器遵循一个查询基本的思想，不需要明确的紧密BEV特征建立，但是其性能较差。在这篇论文中，我们发现了关键是在BEV和图像空间中的检测器适应能力。为了实现这个目标，我们提出了SparseBEV，一个完全稀疏的3D几何检测器，它的性能比紧密检测器更高。SparseBEV包括三个关键设计，分别是（1）缩寸自适应的自我注意力，用于在BEV空间中缩寸特征的整合，（2）适应的空间-时间抽样，用于在寻找适当的抽样位置的指导，以及（3）适应的混合，用于将抽样结果与问题答案进行适应权重的混合。在nuScenes的测试分区上，SparseBEV实现了67.5 NDS的国际级别性能。在val分区上，SparseBEV实现了55.8 NDS，同时保持了实时推理速度的23.5 FPS。代码可以在https://github.com/MCG-NJU/SparseBEV上找到。
</details></li>
</ul>
<hr>
<h2 id="ASAG-Building-Strong-One-Decoder-Layer-Sparse-Detectors-via-Adaptive-Sparse-Anchor-Generation"><a href="#ASAG-Building-Strong-One-Decoder-Layer-Sparse-Detectors-via-Adaptive-Sparse-Anchor-Generation" class="headerlink" title="ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation"></a>ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09242">http://arxiv.org/abs/2308.09242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/isee-laboratory/asag">https://github.com/isee-laboratory/asag</a></li>
<li>paper_authors: Shenghao Fu, Junkai Yan, Yipeng Gao, Xiaohua Xie, Wei-Shi Zheng</li>
<li>for: 提高 object detection 的速度和准确率， bridging the performance gap between sparse and dense detectors.</li>
<li>methods: 提出 Adaptive Sparse Anchor Generator (ASAG)， dynamically predicting anchors on patches rather than grids in a sparse way to alleviate feature conflict problem, and using a simple and effective Query Weighting method to ease the training instability.</li>
<li>results: 比较 dense-initialized ones 和其他方法，实现了更好的速度-准确率平衡，并且在实验中表现出色。<details>
<summary>Abstract</summary>
Recent sparse detectors with multiple, e.g. six, decoder layers achieve promising performance but much inference time due to complex heads. Previous works have explored using dense priors as initialization and built one-decoder-layer detectors. Although they gain remarkable acceleration, their performance still lags behind their six-decoder-layer counterparts by a large margin. In this work, we aim to bridge this performance gap while retaining fast speed. We find that the architecture discrepancy between dense and sparse detectors leads to feature conflict, hampering the performance of one-decoder-layer detectors. Thus we propose Adaptive Sparse Anchor Generator (ASAG) which predicts dynamic anchors on patches rather than grids in a sparse way so that it alleviates the feature conflict problem. For each image, ASAG dynamically selects which feature maps and which locations to predict, forming a fully adaptive way to generate image-specific anchors. Further, a simple and effective Query Weighting method eases the training instability from adaptiveness. Extensive experiments show that our method outperforms dense-initialized ones and achieves a better speed-accuracy trade-off. The code is available at \url{https://github.com/iSEE-Laboratory/ASAG}.
</details>
<details>
<summary>摘要</summary>
We find that the architecture discrepancy between dense and sparse detectors leads to feature conflict, hindering the performance of one-decoder-layer detectors. To address this, we propose the Adaptive Sparse Anchor Generator (ASAG), which predicts dynamic anchors on patches in a sparse way, alleviating the feature conflict problem. For each image, ASAG dynamically selects which feature maps and which locations to predict, forming a fully adaptive way to generate image-specific anchors.Further, we propose a simple and effective Query Weighting method to ease the training instability from adaptiveness. Extensive experiments show that our method outperforms dense-initialized ones and achieves a better speed-accuracy trade-off. The code is available at \url{https://github.com/iSEE-Laboratory/ASAG}.Translation notes:* "Recent sparse detectors" is translated as "近期的 sparse detectors" (jì qī de sparse detectors)* "multiple, e.g. six, decoder layers" is translated as "多个，例如六个，decoder层" (duō gè, yǐ jǐ liù gè, decoder layer)* "achieve promising performance" is translated as "实现了可能的表现" (shí yì le kě néng de biǎo xiǎng)* "but much inference time" is translated as "但是大量的推理时间" (dàn shì dà liú de tuī lǐ shí jiān)* "due to complex heads" is translated as "由于复杂的头部" (yūn guī zhī de tóu bù)* "Previous works have explored using dense priors as initialization" is translated as "先前的工作已经探索了使用密集的初始值" (xiān qian de gōng zuò yǐ jīn tàng zhī de chū shí zhì)* "built one-decoder-layer detectors" is translated as "构建了一层decoder的检测器" (gòng jìan le yī cè decoder de jiǎn téng)* "Although they gain remarkable acceleration" is translated as "尽管它们很快" (zhòu guàn tā men hěn kuài)* "their performance still lags behind their six-decoder-layer counterparts by a large margin" is translated as "它们的表现仍然落后六层decoder的对手 by a large margin" (tā men de biǎo xiǎng zhèng rán zài liù cè decoder de duì shǒu by a large margin)* "In this work, we aim to bridge this performance gap" is translated as "在这个工作中，我们目标是填补这个表现差距" (zài zhè ge gōng zuò, wǒmen mù tiāo shì fēn chōng zhè ge biǎo xiǎng jì dao)* "while retaining fast speed" is translated as "同时保持快速" (tóng shí bǎo qiú sù)* "We find that the architecture discrepancy between dense and sparse detectors leads to feature conflict" is translated as "我们发现 dense和sparse detector的 architecture difference导致 feature conflict" (wǒmen fā xiǎng dense yào sparse detector de architecture difference dào yùn feature conflict)* "hampering the performance of one-decoder-layer detectors" is translated as "妨碍一层decoder的检测器表现" (mèi yòu yī cè decoder de jiǎn téng biǎo xiǎng)* "Thus we propose Adaptive Sparse Anchor Generator (ASAG)" is translated as "因此我们提出 Adaptive Sparse Anchor Generator (ASAG)" (yīn qí wǒmen tī shuā Adaptive Sparse Anchor Generator (ASAG))* "which predicts dynamic anchors on patches in a sparse way" is translated as "可以在缺省的方式下预测动态的锚点" (kě yǐ zài yì qiū xiāng de fāng shí zhī xiǎng yì qiū yì zhī)* "alleviating the feature conflict problem" is translated as "解决 feature conflict 问题" (jiě jī feature conflict wèn tí)* "For each image, ASAG dynamically selects which feature maps and which locations to predict" is translated as "对于每个图像，ASAG动态选择哪些特征地图和哪些位置预测" (duì yú zhè ge hú xiàng, ASAG dòng tài xūn zhè yǐ qù zhì yǐ zhòng zhì)* "forming a fully adaptive way to generate image-specific anchors" is translated as "形成一种完全适应的方式来生成图像特定的锚点" (xíng chéng yī zhī qù zhì yǐ zhòng zhì)* "Further, we propose a simple and effective Query Weighting method" is translated as "另外，我们提出了一种简单有效的查询权重方法" (yīn qí, wǒmen tī shuā yī qiū yǒu yì de chá qiǎn zhèng yì)* "to ease the training instability from adaptiveness" is translated as "以适应性导致的训练不稳定性" (yǐ shì yì qiū xiāng de tào xiǎng shì)* "Extensive experiments show that our method outperforms dense-initialized ones" is translated as "广泛的实验表明我们的方法在 dense-initialized 方面表现出色" (guǎng fāng de shí yàn bǎng mìng wǒmen de fāng shì zài dense-initialized zhōng xiàng)* "and achieves a better speed-accuracy trade-off" is translated as "并实现了更好的速度-准确性质量比" (yǔ shì yì qiū yǐ zhèng yì qiū yì zhòng zhì)* "The code is available at \url{https://github.com/iSEE-Laboratory/ASAG}" is translated as "代码可以在 \url{https://github.com/iSEE-Laboratory/ASAG} 上获取" (dài māo kě yǐ zài \url{https://github.com/iSEE-Laboratory/ASAG} shàng gòu qǔ)
</details></li>
</ul>
<hr>
<h2 id="Deep-Boosting-Multi-Modal-Ensemble-Face-Recognition-with-Sample-Level-Weighting"><a href="#Deep-Boosting-Multi-Modal-Ensemble-Face-Recognition-with-Sample-Level-Weighting" class="headerlink" title="Deep Boosting Multi-Modal Ensemble Face Recognition with Sample-Level Weighting"></a>Deep Boosting Multi-Modal Ensemble Face Recognition with Sample-Level Weighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09234">http://arxiv.org/abs/2308.09234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahar Rahimi Malakshan, Mohammad Saeed Ebrahimi Saadabadi, Nima Najafzadeh, Nasser M. Nasrabadi</li>
<li>for: 提高人脸识别（FR）模型的泛化能力，解决现有训练数据的质量不均问题。</li>
<li>methods: 使用多模型增强技术，即AdaBoost的sample-level weighting方法，使得不同模型在不同样本困难程度上拥有专家性。</li>
<li>results: 在CFP-FP、LFW、CPLFW、CALFW、AgeDB、TinyFace、IJB-B和IJB-C评估 datasets上实现了比 estado-of-the-art 性能。<details>
<summary>Abstract</summary>
Deep convolutional neural networks have achieved remarkable success in face recognition (FR), partly due to the abundant data availability. However, the current training benchmarks exhibit an imbalanced quality distribution; most images are of high quality. This poses issues for generalization on hard samples since they are underrepresented during training. In this work, we employ the multi-model boosting technique to deal with this issue. Inspired by the well-known AdaBoost, we propose a sample-level weighting approach to incorporate the importance of different samples into the FR loss. Individual models of the proposed framework are experts at distinct levels of sample hardness. Therefore, the combination of models leads to a robust feature extractor without losing the discriminability on the easy samples. Also, for incorporating the sample hardness into the training criterion, we analytically show the effect of sample mining on the important aspects of current angular margin loss functions, i.e., margin and scale. The proposed method shows superior performance in comparison with the state-of-the-art algorithms in extensive experiments on the CFP-FP, LFW, CPLFW, CALFW, AgeDB, TinyFace, IJB-B, and IJB-C evaluation datasets.
</details>
<details>
<summary>摘要</summary>
深度卷积神经网络在人脸识别（FR）领域取得了很大的成功，一部分是因为数据的充足性。然而，当前的训练标准 exhibit 一个不均衡的质量分布，大多数图像是高质量的。这会导致在训练中困难样本的代表性受到限制。在这种情况下，我们使用多模型增强技术来解决这个问题。我们提出了一种样本水平的权重方法，以便在 FR 损失中包含不同样本的重要性。具体来说，我们的框架中的各个模型是专家在不同水平上的样本困难程度。因此，将这些模型相加可以获得一个可靠的特征提取器，而不会失去易样本的把握。此外，为了在训练标准中包含样本困难程度，我们分析了现有的angular margin loss函数的重要方面，即边和缩放。我们的方法在大量的实验中表现出色，比如CFP-FP、LFW、CPLFW、CALFW、AgeDB、TinyFace、IJB-B 和 IJB-C 评估数据集。
</details></li>
</ul>
<hr>
<h2 id="CCFace-Classification-Consistency-for-Low-Resolution-Face-Recognition"><a href="#CCFace-Classification-Consistency-for-Low-Resolution-Face-Recognition" class="headerlink" title="CCFace: Classification Consistency for Low-Resolution Face Recognition"></a>CCFace: Classification Consistency for Low-Resolution Face Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09230">http://arxiv.org/abs/2308.09230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Saeed Ebrahimi Saadabadi, Sahar Rahimi Malakshan, Hossein Kashiani, Nasser M. Nasrabadi</li>
<li>for: 提高低分辨率人脸识别的性能</li>
<li>methods: 使用分类一致知识塑造和适应角度罚款，以及异形跨分辨率学习</li>
<li>results: 在低分辨率benchmark上提高三个百分点，包括tinyface和scface等In English, this would be:</li>
<li>for: Improving the performance of low-resolution face recognition</li>
<li>methods: Using classification consistency knowledge distillation and adaptive angular penalty, as well as asymmetric cross-resolution learning</li>
<li>results: Improving performance by three percent on TinyFace and other low-resolution benchmarks while maintaining performance on high-resolution benchmarks.<details>
<summary>Abstract</summary>
In recent years, deep face recognition methods have demonstrated impressive results on in-the-wild datasets. However, these methods have shown a significant decline in performance when applied to real-world low-resolution benchmarks like TinyFace or SCFace. To address this challenge, we propose a novel classification consistency knowledge distillation approach that transfers the learned classifier from a high-resolution model to a low-resolution network. This approach helps in finding discriminative representations for low-resolution instances. To further improve the performance, we designed a knowledge distillation loss using the adaptive angular penalty inspired by the success of the popular angular margin loss function. The adaptive penalty reduces overfitting on low-resolution samples and alleviates the convergence issue of the model integrated with data augmentation. Additionally, we utilize an asymmetric cross-resolution learning approach based on the state-of-the-art semi-supervised representation learning paradigm to improve discriminability on low-resolution instances and prevent them from forming a cluster. Our proposed method outperforms state-of-the-art approaches on low-resolution benchmarks, with a three percent improvement on TinyFace while maintaining performance on high-resolution benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generalized-Sum-Pooling-for-Metric-Learning"><a href="#Generalized-Sum-Pooling-for-Metric-Learning" class="headerlink" title="Generalized Sum Pooling for Metric Learning"></a>Generalized Sum Pooling for Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09228">http://arxiv.org/abs/2308.09228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yetigurbuz/generalized-sum-pooling">https://github.com/yetigurbuz/generalized-sum-pooling</a></li>
<li>paper_authors: Yeti Z. Gurbuz, Ozan Sener, A. Aydın Alatan</li>
<li>for: 该论文主要研究了深度度量学中的核心选择方法，即全局平均 pooling (GAP) 的扩展和改进。</li>
<li>methods: 该论文提出了一种名为泛化总和Pooling (GSP) 的新方法，它可以更好地选择Semantic entity，并学习每个 Semantic entity 的重要性。</li>
<li>results: 论文通过广泛的实验证明了 GSP 方法的效果，在四个Popular metric learning benchmark上表现出色，代替 GAP 方法可以更好地进行深度度量学。<details>
<summary>Abstract</summary>
A common architectural choice for deep metric learning is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different semantic entity and GAP as a convex combination of them. Following this perspective, we generalize GAP and propose a learnable generalized sum pooling method (GSP). GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effectively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct learnable replacement for GAP. We further propose a zero-shot loss to ease the learning of GSP. We show the effectiveness of our method with extensive evaluations on 4 popular metric learning benchmarks. Code is available at: GSP-DML Framework
</details>
<details>
<summary>摘要</summary>
通常的建筑设计 для深度度量学是使用卷积神经网络后跟global average pooling（GAP）。尽管简单，但GAP是一种非常有效的信息汇总方法。一种可能的解释是每个特征向量都代表着不同的semantic entity，GAP是这些entity的 convex combination。基于这种视角，我们总结GAP并提出了一种可学习的总和汇总方法（GSP）。GSP在两个方面提高了GAP：一是选择一 subset of semantic entities，有效地忽略干扰信息；二是学习每个entity的重要性的权重。我们提出了一个 Entropy-smoothed optimal transport problem，并证明它是GAP的严格泛化，即一个特定的实现问题可以得回GAP。我们还提出了一个零战损损失函数，以便学习GSP。我们通过对4个popular度量学benchmark进行广泛的评估，证明了我们的方法的效果。代码可以在：GSP-DML框架中找到。
</details></li>
</ul>
<hr>
<h2 id="DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction"><a href="#DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction" class="headerlink" title="DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction"></a>DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09223">http://arxiv.org/abs/2308.09223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hexiaoxiao-cs/dmcvr">https://github.com/hexiaoxiao-cs/dmcvr</a></li>
<li>paper_authors: Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, Dimitris N. Metaxas</li>
<li>for: 提高心脏疾病诊断和治疗规划的准确3D心脏重建</li>
<li>methods: 使用形态导航推 diffusion模型（DMCVR）Synthesize高解度2D图像和对应的3D重建体积</li>
<li>results: 比前方法高效，能生成高解度3D心脏MRI重建图像，提高心脏疾病诊断和治疗规划的准确性<details>
<summary>Abstract</summary>
Accurate 3D cardiac reconstruction from cine magnetic resonance imaging (cMRI) is crucial for improved cardiovascular disease diagnosis and understanding of the heart's motion. However, current cardiac MRI-based reconstruction technology used in clinical settings is 2D with limited through-plane resolution, resulting in low-quality reconstructed cardiac volumes. To better reconstruct 3D cardiac volumes from sparse 2D image stacks, we propose a morphology-guided diffusion model for 3D cardiac volume reconstruction, DMCVR, that synthesizes high-resolution 2D images and corresponding 3D reconstructed volumes. Our method outperforms previous approaches by conditioning the cardiac morphology on the generative model, eliminating the time-consuming iterative optimization process of the latent code, and improving generation quality. The learned latent spaces provide global semantics, local cardiac morphology and details of each 2D cMRI slice with highly interpretable value to reconstruct 3D cardiac shape. Our experiments show that DMCVR is highly effective in several aspects, such as 2D generation and 3D reconstruction performance. With DMCVR, we can produce high-resolution 3D cardiac MRI reconstructions, surpassing current techniques. Our proposed framework has great potential for improving the accuracy of cardiac disease diagnosis and treatment planning. Code can be accessed at https://github.com/hexiaoxiao-cs/DMCVR.
</details>
<details>
<summary>摘要</summary>
精确的3D心脏重建从cinematic magnetic resonance imaging（cMRI）是诊断心血管疾病和心脏运动的关键。然而，现有的心脏MRI基于的重建技术在临床设置中只有2D的限制 Through-plane 分辨率，导致低质量重建的心脏体积。为了更好地从稀疏的2D图像堆栈中重建3D心脏体积，我们提议一种基于形态指导的分子模型，DMCVR，该模型可以生成高分辨率的2D图像和对应的3D重建体积。我们的方法比前一代方法更高效，因为它们 conditioning cardiac morphology 在生成模型中，消除耗时的迭代优化过程，并提高生成质量。学习的秘密空间提供了全球 semantics，局部心脏形态和每个2D cMRI slice 的高可读性，以重建3D心脏形态。我们的实验表明，DMCVR 在多个方面表现出色，如2D生成和3D重建性能。通过 DMCVR，我们可以生成高分辨率的3D心脏MRI重建，超过当前技术。我们提出的框架具有较大的诊断心血管疾病和治疗规划的潜在优势。代码可以在https://github.com/hexiaoxiao-cs/DMCVR 中获取。
</details></li>
</ul>
<hr>
<h2 id="A-review-of-technical-factors-to-consider-when-designing-neural-networks-for-semantic-segmentation-of-Earth-Observation-imagery"><a href="#A-review-of-technical-factors-to-consider-when-designing-neural-networks-for-semantic-segmentation-of-Earth-Observation-imagery" class="headerlink" title="A review of technical factors to consider when designing neural networks for semantic segmentation of Earth Observation imagery"></a>A review of technical factors to consider when designing neural networks for semantic segmentation of Earth Observation imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09221">http://arxiv.org/abs/2308.09221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Khallaghi, J. Ronald Eastman, Lyndon D. Estes</li>
<li>for: 本文旨在提供对遥感图像semantic segmentation（分类） tasks的技术因素的全面审查，帮助研究人员和实践者更好地理解这个领域中的 neural network 设计因素。</li>
<li>methods: 本文详细介绍了 CNNs、RNNs、GANs 和 transformer 模型，并讨论了这些 ANN 家族中的显著设计特征和其对 semantic segmentation 的影响。同时，也涵盖了常见的预处理技术，包括图像normalization和chipping，以及如何处理训练样本数据不均衡的问题，以及如何使用扩展学习、转移学习和领域适应来解决有限数据的问题。</li>
<li>results: 本文提供了一个全面和最新的理解，涵盖了遥感图像semantic segmentation tasks中 neural network 设计因素的技术和数据相关因素。<details>
<summary>Abstract</summary>
Semantic segmentation (classification) of Earth Observation imagery is a crucial task in remote sensing. This paper presents a comprehensive review of technical factors to consider when designing neural networks for this purpose. The review focuses on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Adversarial Networks (GANs), and transformer models, discussing prominent design patterns for these ANN families and their implications for semantic segmentation. Common pre-processing techniques for ensuring optimal data preparation are also covered. These include methods for image normalization and chipping, as well as strategies for addressing data imbalance in training samples, and techniques for overcoming limited data, including augmentation techniques, transfer learning, and domain adaptation. By encompassing both the technical aspects of neural network design and the data-related considerations, this review provides researchers and practitioners with a comprehensive and up-to-date understanding of the factors involved in designing effective neural networks for semantic segmentation of Earth Observation imagery.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation (classification) of Earth Observation imagery 是 remote sensing 领域中的一个重要任务。本文将提供 Earth Observation imagery 中 neural networks 的设计因素的 comprehensive 综述，包括 Convolutional Neural Networks (CNNs)、Recurrent Neural Networks (RNNs)、Generative Adversarial Networks (GANs) 和 transformer 模型，并讨论这些 ANN 家族的主要设计模式以及它们在 semantic segmentation 中的应用。本文还讨论了一些常见的预processing 技术，包括图像 normalization 和扩展、训练数据均衡问题的解决方案、以及对于有限数据的处理方法，包括扩展技术、传播学习以及领域适应。本文涵盖了 neural network 设计的技术性和数据相关的考虑因素，以提供研究者和实践者一个完整和最新的理解，对 Earth Observation imagery 中 neural networks 的设计为 semantic segmentation 做出更有效的应用。
</details></li>
</ul>
<hr>
<h2 id="LibreFace-An-Open-Source-Toolkit-for-Deep-Facial-Expression-Analysis"><a href="#LibreFace-An-Open-Source-Toolkit-for-Deep-Facial-Expression-Analysis" class="headerlink" title="LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis"></a>LibreFace: An Open-Source Toolkit for Deep Facial Expression Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10713">http://arxiv.org/abs/2308.10713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Chang, Yufeng Yin, Zongjian Li, Minh Tran, Mohammad Soleymani</li>
<li>for: 这个论文主要目的是提出一个开源的人工智能工具kit，用于实时和离线的表情分析。</li>
<li>methods: 这个工具kit使用了深度学习模型，包括表情动作单元（AU）检测、AU强度估计和表情识别。具体来说，我们使用了一个大规模预训练的网络、特征知识填充和任务特定的细化调整等技术，以便准确地分析人脸表情。</li>
<li>results: 在Action Unit（AU）强度估计方面，我们在DISFA上达到了0.63的佩森相关系数（PCC），比OpenFace 2.0的性能高7%，同时保持高效的推理，运行速度两倍于OpenFace 2.0。尽管占用空间小，我们的模型也能够与当前最佳表情分析方法在AffecNet、FFHQ和RAFDB等 datasets上达到竞争性表现。<details>
<summary>Abstract</summary>
Facial expression analysis is an important tool for human-computer interaction. In this paper, we introduce LibreFace, an open-source toolkit for facial expression analysis. This open-source toolbox offers real-time and offline analysis of facial behavior through deep learning models, including facial action unit (AU) detection, AU intensity estimation, and facial expression recognition. To accomplish this, we employ several techniques, including the utilization of a large-scale pre-trained network, feature-wise knowledge distillation, and task-specific fine-tuning. These approaches are designed to effectively and accurately analyze facial expressions by leveraging visual information, thereby facilitating the implementation of real-time interactive applications. In terms of Action Unit (AU) intensity estimation, we achieve a Pearson Correlation Coefficient (PCC) of 0.63 on DISFA, which is 7% higher than the performance of OpenFace 2.0 while maintaining highly-efficient inference that runs two times faster than OpenFace 2.0. Despite being compact, our model also demonstrates competitive performance to state-of-the-art facial expression analysis methods on AffecNet, FFHQ, and RAFDB. Our code will be released at https://github.com/ihp-lab/LibreFace
</details>
<details>
<summary>摘要</summary>
Facial expression analysis is an important tool for human-computer interaction. In this paper, we introduce LibreFace, an open-source toolkit for facial expression analysis. This open-source toolbox offers real-time and offline analysis of facial behavior through deep learning models, including facial action unit (AU) detection, AU intensity estimation, and facial expression recognition. To accomplish this, we employ several techniques, including the utilization of a large-scale pre-trained network, feature-wise knowledge distillation, and task-specific fine-tuning. These approaches are designed to effectively and accurately analyze facial expressions by leveraging visual information, thereby facilitating the implementation of real-time interactive applications. In terms of Action Unit (AU) intensity estimation, we achieve a Pearson Correlation Coefficient (PCC) of 0.63 on DISFA, which is 7% higher than the performance of OpenFace 2.0 while maintaining highly-efficient inference that runs two times faster than OpenFace 2.0. Despite being compact, our model also demonstrates competitive performance to state-of-the-art facial expression analysis methods on AffecNet, FFHQ, and RAFDB. Our code will be released at https://github.com/ihp-lab/LibreFace.
</details></li>
</ul>
<hr>
<h2 id="TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning"><a href="#TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning" class="headerlink" title="TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning"></a>TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09201">http://arxiv.org/abs/2308.09201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Rüb, Daniel Maier, Daniel Mueller-Gritschneder, Axel Sikora</li>
<li>for: 这篇论文主要目的是提出一种可以在低功耗微控制器单元（MCU）上进行Device Learning或精益化的深度神经网络训练方法，以减少训练时间和计算负载。</li>
<li>methods: 这篇论文使用了一种名为TinyProp的简单传播方法，这个方法可以在MCU上进行Device Learning，并且可以在训练过程中动态地调整传播比例，以提高训练效率和精确性。</li>
<li>results: 根据论文的结果，TinyProp比非简单训练更快，并且可以保持精确性。具体来说，与非简单训练相比，TinyProp在三个数据集（MNIST、DCASE2020和CIFAR10）上的训练时间比例为5倍，并且仅受到轻微的计算过程影响。此外，TinyProp与现有的静态简单传播方法相比，在训练效率和精确性方面都有优势。<details>
<summary>Abstract</summary>
Training deep neural networks using backpropagation is very memory and computationally intensive. This makes it difficult to run on-device learning or fine-tune neural networks on tiny, embedded devices such as low-power micro-controller units (MCUs). Sparse backpropagation algorithms try to reduce the computational load of on-device learning by training only a subset of the weights and biases. Existing approaches use a static number of weights to train. A poor choice of this so-called backpropagation ratio limits either the computational gain or can lead to severe accuracy losses. In this paper we present TinyProp, the first sparse backpropagation method that dynamically adapts the back-propagation ratio during on-device training for each training step. TinyProp induces a small calculation overhead to sort the elements of the gradient, which does not significantly impact the computational gains. TinyProp works particularly well on fine-tuning trained networks on MCUs, which is a typical use case for embedded applications. For typical datasets from three datasets MNIST, DCASE2020 and CIFAR10, we are 5 times faster compared to non-sparse training with an accuracy loss of on average 1%. On average, TinyProp is 2.9 times faster than existing, static sparse backpropagation algorithms and the accuracy loss is reduced on average by 6 % compared to a typical static setting of the back-propagation ratio.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用深度神经网络进行训练是非常占用内存和计算资源的。这使得在低功耗微控制器单元（MCU）上进行设备学习或细化神经网络 become difficult。使用 sparse backpropagation 算法可以减少设备学习中的计算负担，但现有方法使用静态的 backpropagation 比率，这限制了计算减少或导致精度损失。在这篇论文中，我们提出了 TinyProp，第一个在设备学习过程中动态调整 backpropagation 比率的稀疏 backpropagation 方法。TinyProp 在MCU 上进行设备学习时具有较小的计算开销，并不会对计算减少的影响。TinyProp 在微controller 上进行 fine-tuning 神经网络时表现特别好，这是常见的嵌入式应用场景。对于 Typical datasets  MNIST、DCASE2020 和 CIFAR10，我们比非稀疏训练更快，减少了平均1%的精度损失。与现有静态稀疐 backpropagation 算法相比，TinyProp 在平均上2.9倍快，并且在平均上减少了6%的精度损失。
</details></li>
</ul>
<hr>
<h2 id="FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning"><a href="#FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning" class="headerlink" title="FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning"></a>FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09160">http://arxiv.org/abs/2308.09160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imguangyu/fedperfix">https://github.com/imguangyu/fedperfix</a></li>
<li>paper_authors: Guangyu Sun, Matias Mendieta, Jun Luo, Shandong Wu, Chen Chen</li>
<li>for: 提高分布式学习的效率和个性化性</li>
<li>methods: 研究如何在Vision Transformer（ViT）模型中部分个性化，并提出了一种基于插件的方法—FedPerfix</li>
<li>results: 在CIFAR-100、OrganAMNIST和Office-Home等 datasets上进行了实验，并证明了该方法可以与多种先进的分布式学习方法相比提高模型的性能<details>
<summary>Abstract</summary>
Personalized Federated Learning (PFL) represents a promising solution for decentralized learning in heterogeneous data environments. Partial model personalization has been proposed to improve the efficiency of PFL by selectively updating local model parameters instead of aggregating all of them. However, previous work on partial model personalization has mainly focused on Convolutional Neural Networks (CNNs), leaving a gap in understanding how it can be applied to other popular models such as Vision Transformers (ViTs). In this work, we investigate where and how to partially personalize a ViT model. Specifically, we empirically evaluate the sensitivity to data distribution of each type of layer. Based on the insights that the self-attention layer and the classification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrate its effectiveness in improving the model's performance compared to several advanced PFL methods.
</details>
<details>
<summary>摘要</summary>
个人化联合学习（PFL）表示在不同数据环境中分布式学习的有优解决方案。部分模型个性化已被提议以提高PFL的效率，但以前的工作主要集中在卷积神经网络（CNNs）上，对其他受欢迎的模型，如视觉变换器（ViTs），的应用仍然存在知识空白。在这种情况下，我们调查了在ViT模型中可以 selectively更新的部分。 Specifically, we empirically evaluated the sensitivity of each type of layer to data distribution. Based on the findings that the self-attention layer and the classification head are the most sensitive parts of a ViT, we proposed a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluated the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrated its effectiveness in improving the model's performance compared to several advanced PFL methods.
</details></li>
</ul>
<hr>
<h2 id="Semi-sparsity-Priors-for-Image-Structure-Analysis-and-Extraction"><a href="#Semi-sparsity-Priors-for-Image-Structure-Analysis-and-Extraction" class="headerlink" title="Semi-sparsity Priors for Image Structure Analysis and Extraction"></a>Semi-sparsity Priors for Image Structure Analysis and Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09141">http://arxiv.org/abs/2308.09141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junqing Huang, Haihui Wang, Michael Ruzhansky</li>
<li>for: 图像结构分割和图像分类</li>
<li>methods: 通过总体半稀疑函数框架，将图像结构与复杂的质地背景解耦开来</li>
<li>results: 能够保持图像结构，不会出现波动融合现象，同时能够处理强抗振荡的图像文化分割问题，并且可以与现代方法相比肤<details>
<summary>Abstract</summary>
Image structure-texture decomposition is a long-standing and fundamental problem in both image processing and computer vision fields. In this paper, we propose a generalized semi-sparse regularization framework for image structural analysis and extraction, which allows us to decouple the underlying image structures from complicated textural backgrounds. Combining with different textural analysis models, such a regularization receives favorable properties differing from many traditional methods. We demonstrate that it is not only capable of preserving image structures without introducing notorious staircase artifacts in polynomial-smoothing surfaces but is also applicable for decomposing image textures with strong oscillatory patterns. Moreover, we also introduce an efficient numerical solution based on an alternating direction method of multipliers (ADMM) algorithm, which gives rise to a simple and maneuverable way for image structure-texture decomposition. The versatility of the proposed method is finally verified by a series of experimental results with the capability of producing comparable or superior image decomposition results against cutting-edge methods.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:图像结构-文本分解是图像处理和计算机视觉领域的一个长期和基本问题。在这篇论文中，我们提出了一种通用半稀畴化常数执行框架，用于图像结构分析和提取。这种常数执行允许我们将图像结构从复杂的文本背景中分离出来。与不同的文本分析模型结合，这种常数执行具有不同于许多传统方法的优点。我们示示它不仅可以保留图像结构，而且可以避免引入普遍存在的梯形artefacts在多阶趋正面上。此外，我们还介绍了一种高效的数字解决方案，基于分配方法 OF 多个参数（ADMM）算法，这给出了一种简单可操作的图像结构-文本分解方法。我们的方法的多样性最终被实验证明，可以生成与当今顶尖方法相当或更好的图像分解结果。
</details></li>
</ul>
<hr>
<h2 id="The-Unreasonable-Effectiveness-of-Large-Language-Vision-Models-for-Source-free-Video-Domain-Adaptation"><a href="#The-Unreasonable-Effectiveness-of-Large-Language-Vision-Models-for-Source-free-Video-Domain-Adaptation" class="headerlink" title="The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation"></a>The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09139">http://arxiv.org/abs/2308.09139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giaczara/dallv">https://github.com/giaczara/dallv</a></li>
<li>paper_authors: Giacomo Zara, Alessandro Conti, Subhankar Roy, Stéphane Lathuilière, Paolo Rota, Elisa Ricci</li>
<li>for: 这篇论文旨在解决无监督目标数据集上的动作识别模型转移，并且不需要存取实际的源数据。</li>
<li>methods: 这篇论文使用了大型语言视觉模型（LLVM）的“网络超级vision”来推广SFVUDA任务，并且提出了一个叫做“Domain Adaptation with Large Language-Vision models”的方法（简称DALL-V），将 LLVM 的世界假设和补充性源模型信息转换为适应target的学习网络。</li>
<li>results:  despite the simplicity, DALL-V 可以实现显著的提升 compared to 目前的SFVUDA方法。<details>
<summary>Abstract</summary>
Source-Free Video Unsupervised Domain Adaptation (SFVUDA) task consists in adapting an action recognition model, trained on a labelled source dataset, to an unlabelled target dataset, without accessing the actual source data. The previous approaches have attempted to address SFVUDA by leveraging self-supervision (e.g., enforcing temporal consistency) derived from the target data itself. In this work, we take an orthogonal approach by exploiting "web-supervision" from Large Language-Vision Models (LLVMs), driven by the rationale that LLVMs contain a rich world prior surprisingly robust to domain-shift. We showcase the unreasonable effectiveness of integrating LLVMs for SFVUDA by devising an intuitive and parameter-efficient method, which we name Domain Adaptation with Large Language-Vision models (DALL-V), that distills the world prior and complementary source model information into a student network tailored for the target. Despite the simplicity, DALL-V achieves significant improvement over state-of-the-art SFVUDA methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ICAR-Image-based-Complementary-Auto-Reasoning"><a href="#ICAR-Image-based-Complementary-Auto-Reasoning" class="headerlink" title="ICAR: Image-based Complementary Auto Reasoning"></a>ICAR: Image-based Complementary Auto Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09119">http://arxiv.org/abs/2308.09119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xijun Wang, Anqi Liang, Junbang Liang, Ming Lin, Yu Lou, Shan Yang</li>
<li>for:  addresses the challenging task of scene-aware complementary item retrieval (CIR), which requires generating a set of compatible items across domains.</li>
<li>methods:  proposes a visual compatibility concept based on similarity and complementarity, and a category-aware Flexible Bidirectional Transformer (FBT) framework for visual “scene-based set compatibility reasoning” with cross-domain visual similarity input and auto-regressive complementary item generation.</li>
<li>results:  achieves up to 5.3% and 9.6% in FITB score and 22.3% and 31.8% SFID improvement on fashion and furniture, respectively, compared with state-of-the-art methods.<details>
<summary>Abstract</summary>
Scene-aware Complementary Item Retrieval (CIR) is a challenging task which requires to generate a set of compatible items across domains. Due to the subjectivity, it is difficult to set up a rigorous standard for both data collection and learning objectives. To address this challenging task, we propose a visual compatibility concept, composed of similarity (resembling in color, geometry, texture, and etc.) and complementarity (different items like table vs chair completing a group). Based on this notion, we propose a compatibility learning framework, a category-aware Flexible Bidirectional Transformer (FBT), for visual "scene-based set compatibility reasoning" with the cross-domain visual similarity input and auto-regressive complementary item generation. We introduce a "Flexible Bidirectional Transformer (FBT)" consisting of an encoder with flexible masking, a category prediction arm, and an auto-regressive visual embedding prediction arm. And the inputs for FBT are cross-domain visual similarity invariant embeddings, making this framework quite generalizable. Furthermore, our proposed FBT model learns the inter-object compatibility from a large set of scene images in a self-supervised way. Compared with the SOTA methods, this approach achieves up to 5.3% and 9.6% in FITB score and 22.3% and 31.8% SFID improvement on fashion and furniture, respectively.
</details>
<details>
<summary>摘要</summary>
Scene-aware Complementary Item Retrieval (CIR) 是一个复杂的任务，需要生成兼容性的项目领域之间。由于主观性，设置严格的数据采集标准和学习目标很难。为解决这个挑战，我们提出了视觉兼容性概念，包括相似性（颜色、形状、文本等的相似性）和补充性（如桌子和椅子完成一组）。基于这个概念，我们提议一种兼容学习框架，一种类型意识的灵活双向Transformer（FBT），用于视觉“场景基于集合兼容理解”，输入为跨领域视觉相似性无关的嵌入，并使用自动生成的补充项。我们的FBT模型包括一个灵活的面罩，一个类型预测臂，和一个自动生成视觉嵌入预测臂。我们的输入是跨领域视觉相似性无关的嵌入，使得这个框架非常普适。此外，我们的FBT模型从大量场景图像中学习了各对象之间的相互兼容性，自然地实现了Scene-aware CIR。相比 соState-of-the-art方法，我们的方法可以达到5.3%和9.6%的FITB分数提升和22.3%和31.8%的SFID提升在时尚和家具等领域。
</details></li>
</ul>
<hr>
<h2 id="JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer"><a href="#JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer" class="headerlink" title="JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer"></a>JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09110">http://arxiv.org/abs/2308.09110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Ouyang, Zhenzhong Chen</li>
<li>for: 提高JPEG压缩图像的Restoration效果，并能处理各种压缩质因子。</li>
<li>methods: 提出了DCT域的Transformer模型， dual-branch架构用于捕捉空间频率相关性，并采用量化矩阵嵌入和同步颜色卷积。</li>
<li>results: 与当前状态的JPEG噪声除法相比，提高了Restoration效果。<details>
<summary>Abstract</summary>
JPEG compression adopts the quantization of Discrete Cosine Transform (DCT) coefficients for effective bit-rate reduction, whilst the quantization could lead to a significant loss of important image details. Recovering compressed JPEG images in the frequency domain has attracted more and more attention recently, in addition to numerous restoration approaches developed in the pixel domain. However, the current DCT domain methods typically suffer from limited effectiveness in handling a wide range of compression quality factors, or fall short in recovering sparse quantized coefficients and the components across different colorspace. To address these challenges, we propose a DCT domain spatial-frequential Transformer, named as DCTransformer. Specifically, a dual-branch architecture is designed to capture both spatial and frequential correlations within the collocated DCT coefficients. Moreover, we incorporate the operation of quantization matrix embedding, which effectively allows our single model to handle a wide range of quality factors, and a luminance-chrominance alignment head that produces a unified feature map to align different-sized luminance and chrominance components. Our proposed DCTransformer outperforms the current state-of-the-art JPEG artifact removal techniques, as demonstrated by our extensive experiments.
</details>
<details>
<summary>摘要</summary>
JPEG压缩使用Discrete Cosine Transform（DCT）系数归一化来实现有效的比特率减少，但这可能导致重要的图像细节丢失。在频域中恢复压缩JPEG图像已经引起了更多的关注，同时出现了许多像素领域的恢复方法。然而，当前的DCT频域方法通常具有处理各种压缩质量因子的有限效果，或者缺乏恢复粗化quantized均值和不同色彩空间中的组件。为解决这些挑战，我们提议了DCT频域的空间-频率Transformer，称为DCTransformer。具体来说，我们设计了双支架构，以捕捉DCT系数中的空间和频率相关性。此外，我们还包含了量化矩阵嵌入操作，可以有效地让我们的单个模型处理各种质量因子，以及颜色彩空间中的一致性头，可以生成一个统一的特征图来对不同大小的颜色和灰度组件进行对齐。我们的提议的DCTransformer比当前状态的JPEG噪声去除技术高效，如我们的广泛的实验所示。
</details></li>
</ul>
<hr>
<h2 id="Hyperbolic-Face-Anti-Spoofing"><a href="#Hyperbolic-Face-Anti-Spoofing" class="headerlink" title="Hyperbolic Face Anti-Spoofing"></a>Hyperbolic Face Anti-Spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09107">http://arxiv.org/abs/2308.09107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuangpeng Han, Rizhao Cai, Yawen Cui, Zitong Yu, Yongjian Hu, Alex Kot</li>
<li>for: 本研究旨在提高面Recognition系统的安全性，通过学习泛化化脸认证防 spoofing (FAS) 模型，对于训练数据中未seen的攻击进行检测。</li>
<li>methods: 本研究提出了一种基于 hyperbolic space 的 FAS 学习方法，包括将特征编码项 проекed 到 Poincaré 球中，然后使用 hyperbolic binary logistic regression 层进行分类。为了进一步提高泛化能力，我们还实现了 hyperbolic contrastive learning 方法，并对 bonafide 进行了 relaxation 操作。此外，我们还提出了一种新的特征clipping方法，以解决 hyperbolic 空间中的衰减Gradient问题。</li>
<li>results: 实验表明，提出的方法可以与 Euclidean 基eline 相比，在未seen 攻击检测中带来显著的改善。此外，我们还发现，该方法在四个 benchmark dataset （i.e., MSU-MFSD, IDIAP REPLAY-ATTACK, CASIA-FASD, 和 OULU-NPU）上也具有良好的泛化能力。<details>
<summary>Abstract</summary>
Learning generalized face anti-spoofing (FAS) models against presentation attacks is essential for the security of face recognition systems. Previous FAS methods usually encourage models to extract discriminative features, of which the distances within the same class (bonafide or attack) are pushed close while those between bonafide and attack are pulled away. However, these methods are designed based on Euclidean distance, which lacks generalization ability for unseen attack detection due to poor hierarchy embedding ability. According to the evidence that different spoofing attacks are intrinsically hierarchical, we propose to learn richer hierarchical and discriminative spoofing cues in hyperbolic space. Specifically, for unimodal FAS learning, the feature embeddings are projected into the Poincar\'e ball, and then the hyperbolic binary logistic regression layer is cascaded for classification. To further improve generalization, we conduct hyperbolic contrastive learning for the bonafide only while relaxing the constraints on diverse spoofing attacks. To alleviate the vanishing gradient problem in hyperbolic space, a new feature clipping method is proposed to enhance the training stability of hyperbolic models. Besides, we further design a multimodal FAS framework with Euclidean multimodal feature decomposition and hyperbolic multimodal feature fusion & classification. Extensive experiments on three benchmark datasets (i.e., WMCA, PADISI-Face, and SiW-M) with diverse attack types demonstrate that the proposed method can bring significant improvement compared to the Euclidean baselines on unseen attack detection. In addition, the proposed framework is also generalized well on four benchmark datasets (i.e., MSU-MFSD, IDIAP REPLAY-ATTACK, CASIA-FASD, and OULU-NPU) with a limited number of attack types.
</details>
<details>
<summary>摘要</summary>
学习通用面部防伪模型（FAS）对于攻击推送是face recognition系统安全的基础。过去的FAS方法通常会让模型提取特征，其中同类（bonafide或攻击）之间的距离压缩，而不同类之间的距离弹性地压缩。但这些方法基于欧几丁度距离，lack generalization ability for unseen attack detection due to poor hierarchy embedding ability。根据不同的骗术攻击是内在层次结构的证据，我们提议学习更加具有层次结构和特征的骗术攻击特征在希腊空间。specifically，for unimodal FAS learning, the feature embeddings are projected into the Poincaré ball, and then the hyperbolic binary logistic regression layer is cascaded for classification. To further improve generalization, we conduct hyperbolic contrastive learning for the bonafide only while relaxing the constraints on diverse spoofing attacks. To alleviate the vanishing gradient problem in hyperbolic space, a new feature clipping method is proposed to enhance the training stability of hyperbolic models. Besides, we further design a multimodal FAS framework with Euclidean multimodal feature decomposition and hyperbolic multimodal feature fusion & classification. Extensive experiments on three benchmark datasets (i.e., WMCA, PADISI-Face, and SiW-M) with diverse attack types demonstrate that the proposed method can bring significant improvement compared to the Euclidean baselines on unseen attack detection. In addition, the proposed framework is also generalized well on four benchmark datasets (i.e., MSU-MFSD, IDIAP REPLAY-ATTACK, CASIA-FASD, and OULU-NPU) with a limited number of attack types.
</details></li>
</ul>
<hr>
<h2 id="Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation"><a href="#Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation" class="headerlink" title="Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation"></a>Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09105">http://arxiv.org/abs/2308.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengcao Cao, Mengtian Li, James Hays, Deva Ramanan, Yi-Xiong Wang, Liang-Yan Gui</li>
<li>for: 提高资源受限的视觉系统中的检测和分割精度，使用知识储存技术来提高轻量级视觉模型的性能。</li>
<li>methods: 提出了一种简单 yet 有效的顺序方法，通过将多个教师模型转化为一个学生模型来逐渐传递知识。</li>
<li>results: 成功地将Transformer基本的教师检测器知识传递到Convolution基本的学生检测器上，并在MS COCO数据集上提高了RetinaNet的 AP 分数从36.5% 提高到42.0%，以及Mask R-CNN的 AP 分数从38.2% 提高到42.5%。<details>
<summary>Abstract</summary>
Resource-constrained perception systems such as edge computing and vision-for-robotics require vision models to be both accurate and lightweight in computation and memory usage. While knowledge distillation is a proven strategy to enhance the performance of lightweight classification models, its application to structured outputs like object detection and instance segmentation remains a complicated task, due to the variability in outputs and complex internal network modules involved in the distillation process. In this paper, we propose a simple yet surprisingly effective sequential approach to knowledge distillation that progressively transfers the knowledge of a set of teacher detectors to a given lightweight student. To distill knowledge from a highly accurate but complex teacher model, we construct a sequence of teachers to help the student gradually adapt. Our progressive strategy can be easily combined with existing detection distillation mechanisms to consistently maximize student performance in various settings. To the best of our knowledge, we are the first to successfully distill knowledge from Transformer-based teacher detectors to convolution-based students, and unprecedentedly boost the performance of ResNet-50 based RetinaNet from 36.5% to 42.0% AP and Mask R-CNN from 38.2% to 42.5% AP on the MS COCO benchmark.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a simple yet effective sequential approach to knowledge distillation that progressively transfers the knowledge of a set of teacher detectors to a given lightweight student. To distill knowledge from a highly accurate but complex teacher model, we construct a sequence of teachers to help the student gradually adapt. Our progressive strategy can be easily combined with existing detection distillation mechanisms to consistently maximize student performance in various settings.As far as we know, we are the first to successfully distill knowledge from Transformer-based teacher detectors to convolution-based students, and unprecedentedly boost the performance of ResNet-50 based RetinaNet from 36.5% to 42.0% AP and Mask R-CNN from 38.2% to 42.5% AP on the MS COCO benchmark.
</details></li>
</ul>
<hr>
<h2 id="ImGeoNet-Image-induced-Geometry-aware-Voxel-Representation-for-Multi-view-3D-Object-Detection"><a href="#ImGeoNet-Image-induced-Geometry-aware-Voxel-Representation-for-Multi-view-3D-Object-Detection" class="headerlink" title="ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection"></a>ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09098">http://arxiv.org/abs/2308.09098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Tu, Shun-Po Chuang, Yu-Lun Liu, Cheng Sun, Ke Zhang, Donna Roy, Cheng-Hao Kuo, Min Sun</li>
<li>for: 提高多视图图像基于3D对象检测的精度和效率</li>
<li>methods: 使用图像引导的几何空间表示法，从多视图图像中学习几何信息，并在推理阶段只需要多视图图像进行检测</li>
<li>results: 在三个indoor数据集上（ARKitScenes、ScanNetV2、ScanNet200）实现了与当前状态艺术多视图图像基于方法ImVoxelNet相同或更高的检测精度，并且在数据效率方面也表现出了优异性。<details>
<summary>Abstract</summary>
We propose ImGeoNet, a multi-view image-based 3D object detection framework that models a 3D space by an image-induced geometry-aware voxel representation. Unlike previous methods which aggregate 2D features into 3D voxels without considering geometry, ImGeoNet learns to induce geometry from multi-view images to alleviate the confusion arising from voxels of free space, and during the inference phase, only images from multiple views are required. Besides, a powerful pre-trained 2D feature extractor can be leveraged by our representation, leading to a more robust performance. To evaluate the effectiveness of ImGeoNet, we conduct quantitative and qualitative experiments on three indoor datasets, namely ARKitScenes, ScanNetV2, and ScanNet200. The results demonstrate that ImGeoNet outperforms the current state-of-the-art multi-view image-based method, ImVoxelNet, on all three datasets in terms of detection accuracy. In addition, ImGeoNet shows great data efficiency by achieving results comparable to ImVoxelNet with 100 views while utilizing only 40 views. Furthermore, our studies indicate that our proposed image-induced geometry-aware representation can enable image-based methods to attain superior detection accuracy than the seminal point cloud-based method, VoteNet, in two practical scenarios: (1) scenarios where point clouds are sparse and noisy, such as in ARKitScenes, and (2) scenarios involve diverse object classes, particularly classes of small objects, as in the case in ScanNet200.
</details>
<details>
<summary>摘要</summary>
我们提出ImGeoNet，一种基于多视图图像的3D对象探测框架，该框架使用图像引导的geometry-aware粒子表示来模型3D空间。与前方方法不同，ImGeoNet不仅将2D特征积累到3D粒子上无论 Considering geometry, but also learn to induce geometry from multi-view images to alleviate the confusion arising from free space voxels during the inference phase. In addition, our representation can be leveraged by a powerful pre-trained 2D feature extractor, leading to a more robust performance. To evaluate the effectiveness of ImGeoNet, we conduct quantitative and qualitative experiments on three indoor datasets, namely ARKitScenes, ScanNetV2, and ScanNet200. The results show that ImGeoNet outperforms the current state-of-the-art multi-view image-based method, ImVoxelNet, on all three datasets in terms of detection accuracy. Furthermore, ImGeoNet achieves results comparable to ImVoxelNet with 100 views using only 40 views, demonstrating great data efficiency. Our studies also indicate that our proposed image-induced geometry-aware representation can enable image-based methods to achieve superior detection accuracy than the seminal point cloud-based method, VoteNet, in two practical scenarios: (1) scenarios where point clouds are sparse and noisy, such as in ARKitScenes, and (2) scenarios involve diverse object classes, particularly classes of small objects, as in the case in ScanNet200.
</details></li>
</ul>
<hr>
<h2 id="Edit-Temporal-Consistent-Videos-with-Image-Diffusion-Model"><a href="#Edit-Temporal-Consistent-Videos-with-Image-Diffusion-Model" class="headerlink" title="Edit Temporal-Consistent Videos with Image Diffusion Model"></a>Edit Temporal-Consistent Videos with Image Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09091">http://arxiv.org/abs/2308.09091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanzhi Wang, Yong Li, Xin Liu, Anbo Dai, Antoni Chan, Zhen Cui</li>
<li>for: 本文旨在提出一种能够减少视频时间不一致问题的robust文本导向视频编辑方法，以提高视频编辑效果。</li>
<li>methods: 本文提出了一种名为Temporal-Consistent Video Editing（TCVE）方法，其中使用了预训练的2D Unet进行空间内容修改，同时设计了专门用于捕捉视频序列的时间特征的Temporal Unet架构。此外，通过建立空间和时间两个方面的协同关系，提高了视频编辑效果和时间一致性。</li>
<li>results: 实验结果表明，TCVE方法在视频时间一致性和视频编辑能力两个方面均达到了领域内最佳性能，超过了现有的标准准则。<details>
<summary>Abstract</summary>
Large-scale text-to-image (T2I) diffusion models have been extended for text-guided video editing, yielding impressive zero-shot video editing performance. Nonetheless, the generated videos usually show spatial irregularities and temporal inconsistencies as the temporal characteristics of videos have not been faithfully modeled. In this paper, we propose an elegant yet effective Temporal-Consistent Video Editing (TCVE) method, to mitigate the temporal inconsistency challenge for robust text-guided video editing. In addition to the utilization of a pretrained 2D Unet for spatial content manipulation, we establish a dedicated temporal Unet architecture to faithfully capture the temporal coherence of the input video sequences. Furthermore, to establish coherence and interrelation between the spatial-focused and temporal-focused components, a cohesive joint spatial-temporal modeling unit is formulated. This unit effectively interconnects the temporal Unet with the pretrained 2D Unet, thereby enhancing the temporal consistency of the generated video output while simultaneously preserving the capacity for video content manipulation. Quantitative experimental results and visualization results demonstrate that TCVE achieves state-of-the-art performance in both video temporal consistency and video editing capability, surpassing existing benchmarks in the field.
</details>
<details>
<summary>摘要</summary>
In addition to utilizing a pretrained 2D Unet for spatial content manipulation, we establish a dedicated temporal Unet architecture to faithfully capture the temporal coherence of the input video sequences. Furthermore, to establish coherence and interrelation between the spatial-focused and temporal-focused components, we formulate a cohesive joint spatial-temporal modeling unit. This unit effectively interconnects the temporal Unet with the pretrained 2D Unet, enhancing the temporal consistency of the generated video output while simultaneously preserving the capacity for video content manipulation.Experimental results and visualization results demonstrate that TCVE achieves state-of-the-art performance in both video temporal consistency and video editing capability, surpassing existing benchmarks in the field.
</details></li>
</ul>
<hr>
<h2 id="Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries"><a href="#Bridging-High-Quality-Audio-and-Video-via-Language-for-Sound-Effects-Retrieval-from-Visual-Queries" class="headerlink" title="Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries"></a>Bridging High-Quality Audio and Video via Language for Sound Effects Retrieval from Visual Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09089">http://arxiv.org/abs/2308.09089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Wilkins, Justin Salamon, Magdalena Fuentes, Juan Pablo Bello, Oriol Nieto</li>
<li>for: 这个论文是为了找到高质量的声效（SFX），以匹配视频中的特定场景。</li>
<li>methods: 这个论文使用了大型自然语言模型和基础视觉语言模型来连接高质量的音频和视频，从而创建了一个可扩展的自动音频视频数据采集管道。它还使用了预训练的音频和视频编码器来训练一种对比学习基于的检索系统。</li>
<li>results: 这个论文的系统在对高质量音频和视频进行检索 task 上显示了显著的超越基eline的表现。它还能够在不同的数据集上保持一定的表现，并且在用户测试中，人们对系统提供的声效 preference 比基eline 67%。<details>
<summary>Abstract</summary>
Finding the right sound effects (SFX) to match moments in a video is a difficult and time-consuming task, and relies heavily on the quality and completeness of text metadata. Retrieving high-quality (HQ) SFX using a video frame directly as the query is an attractive alternative, removing the reliance on text metadata and providing a low barrier to entry for non-experts. Due to the lack of HQ audio-visual training data, previous work on audio-visual retrieval relies on YouTube (in-the-wild) videos of varied quality for training, where the audio is often noisy and the video of amateur quality. As such it is unclear whether these systems would generalize to the task of matching HQ audio to production-quality video. To address this, we propose a multimodal framework for recommending HQ SFX given a video frame by (1) leveraging large language models and foundational vision-language models to bridge HQ audio and video to create audio-visual pairs, resulting in a highly scalable automatic audio-visual data curation pipeline; and (2) using pre-trained audio and visual encoders to train a contrastive learning-based retrieval system. We show that our system, trained using our automatic data curation pipeline, significantly outperforms baselines trained on in-the-wild data on the task of HQ SFX retrieval for video. Furthermore, while the baselines fail to generalize to this task, our system generalizes well from clean to in-the-wild data, outperforming the baselines on a dataset of YouTube videos despite only being trained on the HQ audio-visual pairs. A user study confirms that people prefer SFX retrieved by our system over the baseline 67% of the time both for HQ and in-the-wild data. Finally, we present ablations to determine the impact of model and data pipeline design choices on downstream retrieval performance. Please visit our project website to listen to and view our SFX retrieval results.
</details>
<details>
<summary>摘要</summary>
找到合适的声效（SFX）以匹配录影中的时刻是一个困难和时间耗费的任务，并且取决于录影中的文本元数据质量。使用录影帧作为查询来撷取高品质（HQ）声效是一个吸引人的选择，减少了文本元数据的依赖和入门障碍。然而，由于缺乏HQ音频视觉训练数据，先前的对话音频视觉检索工作通常是使用YouTube（在野）影片的不同质量进行训练，其中的音频通常是噪音的，影片质量则是业余级。这使得是否这些系统能够通用到对HQ音频视觉匹配的任务仍然存在问题。为了解决这问题，我们提出了一个多 modal 框架，用于根据录影帧提供高品质声效的建议，包括：1. 利用大型语言模型和基础的视觉语言模型来跨度HQ音频和影片，实现高可扩展自动 audio-visual 数据库 Curate 管道。2. 使用预训的音频和视觉嵌入器来训练对比学习扩展系统。我们发现，我们的系统，通过我们自动生成的数据库管道，与基eline 相比，在HQ声效撷取任务上表现出色，并且在实际应用中具有很好的一致性。此外，我们的系统能够从清洁到在野数据中具有良好的一致性，并且在YouTube 影片上进行测试时表现出色。一个使用者研究确认，使用我们的系统比基eline 67% 的时间 prefer SFX。最后，我们提供了ablation 来测试模型和数据管道设计的影响下沿 Retrieval 性能。您可以访问我们的项目网站，listen 和观看我们的 SFX 撷取结果。
</details></li>
</ul>
<hr>
<h2 id="MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices"><a href="#MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices" class="headerlink" title="MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices"></a>MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09084">http://arxiv.org/abs/2308.09084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Haoyue Zhang, Zhirui Zhou, Wangpeng An, Yanhong Yang</li>
<li>for: 这个研究旨在提供高精度和实时性的人体姿势估测算法，特别适用于CPU型手持式移动设备。</li>
<li>methods:  MovePose使用优化的轻量级卷积神经网络，包括三种技术：deconvolution、大kernel卷积和坐标分类方法，以提高精度和速度。</li>
<li>results: MovePose在COCO验证数据集上获得了67.7的Mean Average Precision（mAP）分数，并在Intel i9-10920x CPU和NVIDIA RTX3090 GPU上显示出了高效性，其中在Android手机上的帧率超过11帧&#x2F;秒。<details>
<summary>Abstract</summary>
We present MovePose, an optimized lightweight convolutional neural network designed specifically for real-time body pose estimation on CPU-based mobile devices. The current solutions do not provide satisfactory accuracy and speed for human posture estimation, and MovePose addresses this gap. It aims to maintain real-time performance while improving the accuracy of human posture estimation for mobile devices. The network produces 17 keypoints for each individual at a rate exceeding 11 frames per second, making it suitable for real-time applications such as fitness tracking, sign language interpretation, and advanced mobile human posture estimation. Our MovePose algorithm has attained an Mean Average Precision (mAP) score of 67.7 on the COCO \cite{cocodata} validation dataset. The MovePose algorithm displayed efficiency with a performance of 69+ frames per second (fps) when run on an Intel i9-10920x CPU. Additionally, it showcased an increased performance of 452+ fps on an NVIDIA RTX3090 GPU. On an Android phone equipped with a Snapdragon 8 + 4G processor, the fps reached above 11. To enhance accuracy, we incorporated three techniques: deconvolution, large kernel convolution, and coordinate classification methods. Compared to basic upsampling, deconvolution is trainable, improves model capacity, and enhances the receptive field. Large kernel convolution strengthens these properties at a decreased computational cost. In summary, MovePose provides high accuracy and real-time performance, marking it a potential tool for a variety of applications, including those focused on mobile-side human posture estimation. The code and models for this algorithm will be made publicly accessible.
</details>
<details>
<summary>摘要</summary>
我们现在提出了 MovePose，一种优化的轻量级卷积神经网络，专门为 CPU 基于移动设备上的实时人体姿态估计设计。现有的解决方案无法提供满意的准确率和速度，MovePose 弥补了这一空隙。它目标保持实时性，同时改进移动设备上人体姿态估计的准确率。该网络每秒可生成 17 个锚点，每秒 exceeding 11 帧，适用于实时应用程序，如健身跟踪、手语理解和高级移动人体姿态估计。我们的 MovePose 算法在 COCO 验证集（\cite{cocodata}) 上获得了 67.7 的 Mean Average Precision（mAP）分数。MovePose 算法在 Intel i9-10920x CPU 上运行时显示了效率，每秒可以达到 69+ 帧/秒。此外，在 NVIDIA RTX3090 GPU 上，其性能提高了 452+ 帧/秒。在装有 Snapdragon 8 + 4G 处理器的 Android 手机上，帧率可达上限。为了提高准确率，我们采用了三种技术：减 convolution、大小 kernel convolution 和坐标分类方法。相比基本的 upsampling，减 convolution 可以学习、提高模型容量和扩展感受野。大小 kernel convolution 强化这些属性，但减少计算成本。简单来说，MovePose 提供了高准确率和实时性，使其成为许多应用程序的可能工具，包括移动端人体姿态估计。我们将代码和模型公开访问。
</details></li>
</ul>
<hr>
<h2 id="Pedestrian-Environment-Model-for-Automated-Driving"><a href="#Pedestrian-Environment-Model-for-Automated-Driving" class="headerlink" title="Pedestrian Environment Model for Automated Driving"></a>Pedestrian Environment Model for Automated Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09080">http://arxiv.org/abs/2308.09080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Holzbock, Alexander Tsaregorodtsev, Vasileios Belagiannis</li>
<li>for: 这个论文的目的是提供一种能够在自动驾驶车辆与游客之间安全交互的环境模型。</li>
<li>methods: 该论文使用了一种基于单目相机图像和车辆定位数据的人体pose估计器，以及一种简单的跟踪算法和egosynchronous抵消。</li>
<li>results: 该论文在CARLA simulate器和nuScenes数据集上测试了其人体环境模型，并达到了约16%的相对位置误差。<details>
<summary>Abstract</summary>
Besides interacting correctly with other vehicles, automated vehicles should also be able to react in a safe manner to vulnerable road users like pedestrians or cyclists. For a safe interaction between pedestrians and automated vehicles, the vehicle must be able to interpret the pedestrian's behavior. Common environment models do not contain information like body poses used to understand the pedestrian's intent. In this work, we propose an environment model that includes the position of the pedestrians as well as their pose information. We only use images from a monocular camera and the vehicle's localization data as input to our pedestrian environment model. We extract the skeletal information with a neural network human pose estimator from the image. Furthermore, we track the skeletons with a simple tracking algorithm based on the Hungarian algorithm and an ego-motion compensation. To obtain the 3D information of the position, we aggregate the data from consecutive frames in conjunction with the vehicle position. We demonstrate our pedestrian environment model on data generated with the CARLA simulator and the nuScenes dataset. Overall, we reach a relative position error of around 16% on both datasets.
</details>
<details>
<summary>摘要</summary>
besides correctly interacting with other vehicles, automated vehicles should also be able to react safely to vulnerable road users like pedestrians or cyclists. for a safe interaction between pedestrians and automated vehicles, the vehicle must be able to interpret the pedestrian's behavior. common environment models do not contain information like body poses used to understand the pedestrian's intent. in this work, we propose an environment model that includes the position of the pedestrians as well as their pose information. we only use images from a monocular camera and the vehicle's localization data as input to our pedestrian environment model. we extract the skeletal information with a neural network human pose estimator from the image. furthermore, we track the skeletons with a simple tracking algorithm based on the hungarian algorithm and an ego-motion compensation. to obtain the 3D information of the position, we aggregate the data from consecutive frames in conjunction with the vehicle position. we demonstrate our pedestrian environment model on data generated with the carla simulator and the nuscenes dataset. overall, we reach a relative position error of around 16% on both datasets.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.CV_2023_08_18/" data-id="clpztdni400j0es887m7ra8sq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.AI_2023_08_18/" class="article-date">
  <time datetime="2023-08-18T12:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.AI_2023_08_18/">cs.AI - 2023-08-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments"><a href="#VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments" class="headerlink" title="VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments"></a>VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09632">http://arxiv.org/abs/2308.09632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Grau, Korbinian Hagn</li>
<li>for: 这个论文的目的是研究深度神经网络（DNN）在城市环境中的识别性能，并开发了一种方法来验证DNN的验证方法。</li>
<li>methods: 这个论文使用了VALERIE工具管道生成了一个名为VALERIE22的synthetic数据集，该数据集包含了高度真实的摄像头 simulate的场景，并提供了丰富的元数据，可以提取特定场景和Semantic特征（如精确的遮挡率、场景中的位置和相机距离）。</li>
<li>results: 根据性能指标，这个论文比较了VALERIE22数据集与其他公开available的数据集，结果显示VALERIE22是目前开放领域中最佳的synthetic数据集之一。<details>
<summary>Abstract</summary>
The VALERIE tool pipeline is a synthetic data generator developed with the goal to contribute to the understanding of domain-specific factors that influence perception performance of DNNs (deep neural networks). This work was carried out under the German research project KI Absicherung in order to develop a methodology for the validation of DNNs in the context of pedestrian detection in urban environments for automated driving. The VALERIE22 dataset was generated with the VALERIE procedural tools pipeline providing a photorealistic sensor simulation rendered from automatically synthesized scenes. The dataset provides a uniquely rich set of metadata, allowing extraction of specific scene and semantic features (like pixel-accurate occlusion rates, positions in the scene and distance + angle to the camera). This enables a multitude of possible tests on the data and we hope to stimulate research on understanding performance of DNNs. Based on performance metric a comparison with several other publicly available datasets is provided, demonstrating that VALERIE22 is one of best performing synthetic datasets currently available in the open domain.
</details>
<details>
<summary>摘要</summary>
valeerie工具管道是一种人工数据生成工具，用于帮助理解深度神经网络（DNN）在不同领域的感知性能因素。这项工作是在德国研究项目“KI Absicherung”下进行的，旨在为自动驾驶中的人体检测中开发一种验证方法。valerie22数据集是通过 valeerie工具管道生成的，该管道提供了高度真实的感知器 simulated scenes。该数据集具有独特的元数据，例如像素精度的遮挡率、场景中的位置和相机到摄像头的距离。这些元数据可以用于进行多种测试，并且我们希望通过这些测试来探索DNN的性能。基于性能指标，我们对多个公共可用的数据集进行了比较，并证明了valerie22是目前公开领域中最佳的人工数据集之一。
</details></li>
</ul>
<hr>
<h2 id="Minimum-Coverage-Sets-for-Training-Robust-Ad-Hoc-Teamwork-Agents"><a href="#Minimum-Coverage-Sets-for-Training-Robust-Ad-Hoc-Teamwork-Agents" class="headerlink" title="Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents"></a>Minimum Coverage Sets for Training Robust Ad Hoc Teamwork Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09595">http://arxiv.org/abs/2308.09595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arrasy Rahman, Jiaxun Cui, Peter Stone</li>
<li>for: 本研究旨在提高不可见的代理和人类合作伙伴的协作robustness，因为这些合作伙伴可能采用多种不同的合作规则。</li>
<li>methods: 我们首先提出，为了提高AHT代理的Robustness，应该让代理模仿环境中任何合作伙伴策略的最小覆盖集（MCS）中的策略。然后，我们引入L-BRDiv算法，它可以生成一组合适的团队策略，使AHT代理在训练过程中模仿MCS中的策略。L-BRDiv通过解决一个具有约束的优化问题，同时让AHT代理学习MCS中的策略和团队策略。</li>
<li>results: 我们的实验表明，L-BRDiv在更多的两个玩家合作问题中生成了更加稳定和可靠的AHT代理，而不需要让对象的目标进行详细调整。我们的研究表明，L-BRDiv比基eline方法更好地发现不同的MCS成员，而不是重复找到相同的策略。<details>
<summary>Abstract</summary>
Robustly cooperating with unseen agents and human partners presents significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, these heuristic diversity metrics do not always maximize the agent's robustness in all cooperative problems. In this work, we first propose that maximizing an AHT agent's robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization problem to jointly train teammate policies for AHT training and approximating AHT agent policies that are members of the MCS. We empirically demonstrate that L-BRDiv produces more robust AHT agents than state-of-the-art methods in a broader range of two-player cooperative problems without the need for extensive hyperparameter tuning for its objectives. Our study shows that L-BRDiv outperforms the baseline methods by prioritizing discovering distinct members of the MCS instead of repeatedly finding redundant policies.
</details>
<details>
<summary>摘要</summary>
Robustly collaborating with unseen agents and human partners poses significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, these heuristic diversity metrics do not always maximize the agent's robustness in all cooperative problems. In this work, we first propose that maximizing an AHT agent's robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization problem to jointly train teammate policies for AHT training and approximating AHT agent policies that are members of the MCS. We empirically demonstrate that L-BRDiv produces more robust AHT agents than state-of-the-art methods in a broader range of two-player cooperative problems without the need for extensive hyperparameter tuning for its objectives. Our study shows that L-BRDiv outperforms the baseline methods by prioritizing discovering distinct members of the MCS instead of repeatedly finding redundant policies.Here is the translation in Traditional Chinese:和不可见的代理人和人类合作伙伴一起合作具有很大的挑战，因为这些合作伙伴可能遵循多种不同的合作传统。现有的协作团队（AHT）方法面对这个挑战，通过将一个代理人训练为一群多样化的伙伴政策，并通过最大化特定多样性度量来实现。但这些旧的多样性度量不一定能够将代理人的Robustness最大化在所有的合作问题中。在这个工作中，我们首先提出，将代理人的Robustness最大化需要它们模仿环境中的最小覆盖集（MCS）中的最佳回应策略。我们然后引入L-BRDiv算法，这个算法可以将一群伙伴政策训练为AHT训练，并且将AHT代理人的政策变成MCS中的成员。L-BRDiv算法通过解决一个受限搜索问题，以实现AHT训练和MCS中的策略匹配。我们实际地显示，L-BRDiv算法可以在更广泛的二 player合作问题中生成更Robust的AHT代理人，并且不需要进行大量的参数调整。我们的研究显示，L-BRDiv算法比基eline方法更好，因为它将优先发现MCS中的不同成员，而不是重复发现重复的策略。
</details></li>
</ul>
<hr>
<h2 id="WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct"><a href="#WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct" class="headerlink" title="WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"></a>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09583">http://arxiv.org/abs/2308.09583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlpxucan/wizardlm">https://github.com/nlpxucan/wizardlm</a></li>
<li>paper_authors: Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang</li>
<li>for: 提高大型自然语言处理（NLP）任务中的数学逻辑能力</li>
<li>methods: 应用提出的生长学习自适应反馈法（RLEIF）方法</li>
<li>results: 在两个数学逻辑评测 benchmark 上，WizardMath 表现出众，超越了所有现有的开源大型自然语言模型，并在 GSM8k 和 MATH 两个评测 benchmark 上同时超越了 ChatGPT-3.5、Claude Instant-1、PaLM-2 和 Minerva。Here’s the full translation in Simplified Chinese:</li>
<li>for: 本文旨在提高大型自然语言处理（NLP）任务中的数学逻辑能力。</li>
<li>methods: 本文提出了一种基于生长学习自适应反馈法（RLEIF）的方法，用于提高大型自然语言模型的数学逻辑能力。</li>
<li>results: 经过广泛的实验，WizardMath 在两个数学逻辑评测 benchmark 上表现出众，超越了所有现有的开源大型自然语言模型，并在 GSM8k 和 MATH 两个评测 benchmark 上同时超越了 ChatGPT-3.5、Claude Instant-1、PaLM-2 和 Minerva。Please note that the translation is based on the abstract you provided, and the full paper may contain more details and results.<details>
<summary>Abstract</summary>
Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of Llama-2, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. WizardMath surpasses all other open-source LLMs by a substantial margin. Furthermore, our model even outperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and model weights are public at https://github.com/nlpxucan/WizardLM and https://huggingface.co/WizardLM.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理（NLP）模型（如GPT-4）在数学逻辑任务中表现出色，但大多数现有的开源模型都只是通过大规模互联网数据进行预训练而未进行数学相关的优化。在这篇论文中，我们提出了增强大型自然语言处理模型的数学逻辑能力的方法——基于反馈学习的强化学习（RLEIF）方法。我们在数学逻辑 benchmark 上进行了广泛的实验，并证明了我们的模型在 GSM8k 和 MATH 上的超常表现。我们的 WizardMath 模型在 GSM8k 上超过了所有现有的开源 LLM，并同时超过了 ChatGPT-3.5、Claude Instant-1、PaLM-2 和 Minerva。此外，我们的模型还在 MATH 上超过了 Text-davinci-002、PaLM-1 和 GPT-3。更多细节和模型权重可以在 GitHub 上找到（https://github.com/nlpxucan/WizardLM）和 Hugging Face 上找到（https://huggingface.co/WizardLM）。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning"><a href="#Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning" class="headerlink" title="Investigating the Interplay between Features and Structures in Graph Learning"></a>Investigating the Interplay between Features and Structures in Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09570">http://arxiv.org/abs/2308.09570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Castellana, Federico Errica</li>
<li>for: This paper aims to investigate the relationship between node features and target labels in deep graph networks, and to develop new metrics to measure the influence of node features on target labels.</li>
<li>methods: The paper uses two generative processes to build and study ad-hoc node classification tasks, and evaluates the performance of six models, including structure-agnostic ones.</li>
<li>results: The paper finds that previously defined metrics are not adequate when the assumption of a strong correlation between node features and target labels is relaxed, and presents novel research findings that could help advance our understanding of the field.Here’s the Chinese translation of the three pieces of information:</li>
<li>for: 这篇论文的目的是 investigate深度图网络中节点特征和目标标签之间的关系，并开发新的度量来衡量节点特征对目标标签的影响。</li>
<li>methods: 这篇论文使用两种生成过程来建立和研究特定的节点分类任务，并评估了六种模型的性能，包括无结构的模型。</li>
<li>results: 这篇论文发现先前定义的度量在减弱节点特征和目标标签之间的强相关关系时不适用，并提出了新的研究成果，可能帮助我们更深入理解这个领域。<details>
<summary>Abstract</summary>
In the past, the dichotomy between homophily and heterophily has inspired research contributions toward a better understanding of Deep Graph Networks' inductive bias. In particular, it was believed that homophily strongly correlates with better node classification predictions of message-passing methods. More recently, however, researchers pointed out that such dichotomy is too simplistic as we can construct node classification tasks where graphs are completely heterophilic but the performances remain high. Most of these works have also proposed new quantitative metrics to understand when a graph structure is useful, which implicitly or explicitly assume the correlation between node features and target labels. Our work empirically investigates what happens when this strong assumption does not hold, by formalising two generative processes for node classification tasks that allow us to build and study ad-hoc problems. To quantitatively measure the influence of the node features on the target labels, we also use a metric we call Feature Informativeness. We construct six synthetic tasks and evaluate the performance of six models, including structure-agnostic ones. Our findings reveal that previously defined metrics are not adequate when we relax the above assumption. Our contribution to the workshop aims at presenting novel research findings that could help advance our understanding of the field.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在过去，豪豪和非豪豪的分类理论在深度图学网络的吸引偏好上促进了研究贡献。特别是，人们认为豪豪和非豪豪之间的差异会导致更好的节点预测结果。然而，最近的研究表明，这种分类是太过简单，因为我们可以构建节点预测任务，其中图是完全不豪豪的，却可以达到高性能。大多数这些工作还提出了新的量化指标，以评估图结构的有用性，这些指标直接或间接假设节点特征和目标标签之间的相关性。我们的工作employs two generative processes for node classification tasks，allowing us to build and study ad-hoc problems。为了量化节点特征对目标标签的影响，我们还使用一个叫做特征有用性的指标。我们构建了六个 sintetic任务，并评估了六种模型，包括结构不关注的模型。我们的发现表明，先前定义的指标不适用于放宽这种假设。我们的贡献是在工作室会议上发表新的研究发现，可以帮助我们更深入地理解这个领域。
</details></li>
</ul>
<hr>
<h2 id="Eigenvalue-based-Incremental-Spectral-Clustering"><a href="#Eigenvalue-based-Incremental-Spectral-Clustering" class="headerlink" title="Eigenvalue-based Incremental Spectral Clustering"></a>Eigenvalue-based Incremental Spectral Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10999">http://arxiv.org/abs/2308.10999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek, Bartłmiej Starosta, Sławomir T. Wierzchoń</li>
<li>for: 这个论文是用于提出一种增量 spectral clustering 方法的。</li>
<li>methods: 该方法包括将数据分割成可处理的子集，对每个子集进行归一化，然后将不同子集中的归一化结果合并，以形成整个数据集的归一化结果。</li>
<li>results: 实验表明，这种增量 spectral clustering 方法可以准确地 clustering 大量数据，并且可以避免因数据规模增长而导致的复杂性增加。<details>
<summary>Abstract</summary>
Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
</details>
<details>
<summary>摘要</summary>
我们之前的实验表明， subsets collections of (短) 文档（具有数百个数据）共享一个常数归一化的几何 Laplacian 吸引器谱。基于这一点，我们提出了一种逐步增量 spectral clustering 方法。该方法包括以下步骤：1. 将数据分割成可控制的 subset，2. 对每个 subset 进行归一化，3. 根据吸引器谱的相似性，将不同 subset 的归一化结果合并形成整个数据集的归一化结果。这种方法可以特别有用于数据集的规模快速增长的情况下，如典型的spectral clustering。我们的实验表明，将 subset 进行归一化并将归一化结果合并可以得到整个数据集的准确归一化结果。
</details></li>
</ul>
<hr>
<h2 id="Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning"><a href="#Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning" class="headerlink" title="Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning"></a>Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09544">http://arxiv.org/abs/2308.09544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filip Szatkowski, Mateusz Pyla, Marcin Przewięźlikowski, Sebastian Cygert, Bartłomiej Twardowski, Tomasz Trzciński</li>
<li>for: 这篇研究探讨了例示无法学习（Class Incremental Learning，CIL）中的知识传授（Knowledge Distillation，KD）作为调节策略，以预防忘记。</li>
<li>methods: 这篇研究使用了KD作为CIL中的调节策略，但是KD通常需要对前一任 зада的资料进行例示，这可能导致 repreentation shift 问题。这篇研究引入了 Teacher Adaptation（TA）方法，让教师网络和主要模型在增量训练中同步更新。</li>
<li>results: 这篇研究发现TA方法可以与KD-based CIL方法相互运作，并在多个例示无法学习benchmark上提供了一致的性能提升。<details>
<summary>Abstract</summary>
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main model during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们调查了无 exemplar 的类增量学习（CIL）中使用知识塑造（KD）作为准则约束，以避免忘记。KD 基本方法在 CIL 中成功应用，但它们经常无法规范模型，不管在之前任务的示例数据上进行学习。我们的分析发现，这个问题源于教师网络对异常数据的表达变化，导致 KD 损失成分中的大误差，从而导致 CIL 的性能下降。针对此，我们引入了教师适应（TA）方法，该方法在增量训练中同时更新教师网络和主模型。我们的方法顺利地融合了 KD 基本方法和 CIL 方法，并且可以在多个无 exemplar 的 CIL 基准上提供一致的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Meta-ZSDETR-Zero-shot-DETR-with-Meta-learning"><a href="#Meta-ZSDETR-Zero-shot-DETR-with-Meta-learning" class="headerlink" title="Meta-ZSDETR: Zero-shot DETR with Meta-learning"></a>Meta-ZSDETR: Zero-shot DETR with Meta-learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09540">http://arxiv.org/abs/2308.09540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lu Zhang, Chenbo Zhang, Jiajia Zhao, Jihong Guan, Shuigeng Zhou<br>for: 这篇论文的目的是为了解决零次识别 task 中的问题，特别是低精度和背景混淆问题。methods: 这篇论文使用了 DETR 和 meta-learning 技术，实现了名为 Meta-ZSDETR 的零次识别方法。这篇论文不同于使用 Faster R-CNN 的方法，首先产生无关于类别的提案，然后使用视觉 semantic 对照模组进行分类。相反地，Meta-ZSDETR 直接预测类别特定的方块坐标，使用类别特定的查询来预测类别对应的坐标，并且使用预测的准确度从分类头进行筛选。results: 这篇论文的实验结果显示，该方法在两个 benchmark 数据集 MS COCO 和 PASCAL VOC 上的表现都大幅超过了现有的 ZSD 方法。<details>
<summary>Abstract</summary>
Zero-shot object detection aims to localize and recognize objects of unseen classes. Most of existing works face two problems: the low recall of RPN in unseen classes and the confusion of unseen classes with background. In this paper, we present the first method that combines DETR and meta-learning to perform zero-shot object detection, named Meta-ZSDETR, where model training is formalized as an individual episode based meta-learning task. Different from Faster R-CNN based methods that firstly generate class-agnostic proposals, and then classify them with visual-semantic alignment module, Meta-ZSDETR directly predict class-specific boxes with class-specific queries and further filter them with the predicted accuracy from classification head. The model is optimized with meta-contrastive learning, which contains a regression head to generate the coordinates of class-specific boxes, a classification head to predict the accuracy of generated boxes, and a contrastive head that utilizes the proposed contrastive-reconstruction loss to further separate different classes in visual space. We conduct extensive experiments on two benchmark datasets MS COCO and PASCAL VOC. Experimental results show that our method outperforms the existing ZSD methods by a large margin.
</details>
<details>
<summary>摘要</summary>
Zero-shot object detection aims to localize and recognize objects of unseen classes. Most existing works face two problems: low recall of RPN in unseen classes and confusion of unseen classes with background. In this paper, we present the first method that combines DETR and meta-learning for zero-shot object detection, named Meta-ZSDETR. The model training is formalized as an individual episode-based meta-learning task. Different from Faster R-CNN based methods that first generate class-agnostic proposals and then classify them with visual-semantic alignment modules, Meta-ZSDETR directly predicts class-specific boxes with class-specific queries and further filters them with the predicted accuracy from the classification head. The model is optimized with meta-contrastive learning, which contains a regression head to generate the coordinates of class-specific boxes, a classification head to predict the accuracy of generated boxes, and a contrastive head that utilizes the proposed contrastive-reconstruction loss to further separate different classes in visual space. We conduct extensive experiments on two benchmark datasets MS COCO and PASCAL VOC. Experimental results show that our method outperforms existing ZSD methods by a large margin.
</details></li>
</ul>
<hr>
<h2 id="Proceedings-of-the-2nd-International-Workshop-on-Adaptive-Cyber-Defense"><a href="#Proceedings-of-the-2nd-International-Workshop-on-Adaptive-Cyber-Defense" class="headerlink" title="Proceedings of the 2nd International Workshop on Adaptive Cyber Defense"></a>Proceedings of the 2nd International Workshop on Adaptive Cyber Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09520">http://arxiv.org/abs/2308.09520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Carvalho, Damian Marriott, Mark Bilinski, Ahmad Ridley</li>
<li>for: 本研讨会旨在分享利用人工智能（AI）和机器学习（ML）技术创造固定的响应和适应的网络防御方法。</li>
<li>methods: 本研究使用了AI和ML技术来帮助网络防御，包括自动识别和响应网络攻击、发现和缓解网络漏洞等。</li>
<li>results: 研究人员通过实验和演示证明了AI和ML技术在网络防御中的可行性和有效性，并提出了一些可能的应用场景。<details>
<summary>Abstract</summary>
The 2nd International Workshop on Adaptive Cyber Defense was held at the Florida Institute of Technology, Florida. This workshop was organized to share research that explores unique applications of Artificial Intelligence (AI) and Machine Learning (ML) as foundational capabilities for the pursuit of adaptive cyber defense. The cyber domain cannot currently be reliably and effectively defended without extensive reliance on human experts. Skilled cyber defenders are in short supply and often cannot respond fast enough to cyber threats.   Building on recent advances in AI and ML the Cyber defense research community has been motivated to develop new dynamic and sustainable defenses through the adoption of AI and ML techniques to cyber settings. Bridging critical gaps between AI and Cyber researchers and practitioners can accelerate efforts to create semi-autonomous cyber defenses that can learn to recognize and respond to cyber attacks or discover and mitigate weaknesses in cooperation with other cyber operation systems and human experts. Furthermore, these defenses are expected to be adaptive and able to evolve over time to thwart changes in attacker behavior, changes in the system health and readiness, and natural shifts in user behavior over time.   The workshop was comprised of invited keynote talks, technical presentations and a panel discussion about how AI/ML can enable autonomous mitigation of current and future cyber attacks. Workshop submissions were peer reviewed by a panel of domain experts with a proceedings consisting of six technical articles exploring challenging problems of critical importance to national and global security. Participation in this workshop offered new opportunities to stimulate research and innovation in the emerging domain of adaptive and autonomous cyber defense.
</details>
<details>
<summary>摘要</summary>
第二届国际适应性网络防御工作坊在美国佛罗里达理工学院举行，旨在共享研究人员在使用人工智能（AI）和机器学习（ML）技术为适应性网络防御做出各种应用。现在，无法可靠地和有效地防御网络域，不可避免人才短缺和快速响应网络攻击的问题。基于最新的AI和ML技术，网络防御研究社区受到了激励，以开发新的动态和可持续的防御机制，通过与网络设备和人类专家合作，实现自动化的防御。这些防御机制预期能够适应变化的攻击者行为、系统健康和准备度以及自然的用户行为变化。工作坊包括邀请演讲、技术演示和关于如何通过AI/ML实现自主 Mitigation的panel讨论。工作坊提交经过埃评审定，汇报包括六篇技术文章，探讨了国家和全球安全中的挑战性问题。参加这个工作坊，提供了新的研究和创新机会在emerging领域中，即适应性网络防御。
</details></li>
</ul>
<hr>
<h2 id="Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning"><a href="#Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning" class="headerlink" title="Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning"></a>Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09514">http://arxiv.org/abs/2308.09514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-spatial-librispeech">https://github.com/apple/ml-spatial-librispeech</a></li>
<li>paper_authors: Miguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, Jonathan Sheaffer</li>
<li>for: 这篇论文是用于训练机器学习模型的，特别是用于三维声音定位和相关任务。</li>
<li>methods: 这篇论文使用了大量的 simulate acoustic condition 和 synthetic room 来生成了一个名为 Spatial LibriSpeech 的三维声音数据集，该数据集包含了19个通道的声音、一级散射和可选的干扰声音。</li>
<li>results: 作者使用了这个数据集训练了四个空间声音任务，并取得了 median absolute error 为6.60{\deg} 的3D声音源定位任务，0.43m 的距离任务，90.66ms 的 T30 任务和 2.74dB 的 DRR 估计任务的结果。此外，作者还证明了这些模型可以通过在广泛使用的评估数据集上进行训练来获得良好的普适性。<details>
<summary>Abstract</summary>
We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60{\deg} on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on DRR estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43{\deg} on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge.
</details>
<details>
<summary>摘要</summary>
我们现在提供的是一个空间听说数据集，名为空间听说（Spatial LibriSpeech），它包含了650小时以上的19个频道声音，以及一个首选的干扰噪声。空间听说数据集是为机器学习模型训练而设计，其中包括源位置、说话方向、室内声学和室内geometry的标签。空间听说数据集通过对LibriSpeech样本进行扩展，生成了200,000+个simulated acoustic condition，并在8,000+个synthetic room中进行了扩展。为了证明我们的数据集的实用性，我们在四个空间听说任务上训练了模型，其中包括3D源localization、距离、T30和DRR估计。我们发现这些模型在广泛使用的评估数据集上也能够良好地适应，例如在TUT Sound Events 2018中， median absolute error为12.43°，和在ACE Challenge中，T30估计中的 median absolute error为157.32ms。
</details></li>
</ul>
<hr>
<h2 id="Semantic-relatedness-in-DBpedia-A-comparative-and-experimental-assessment"><a href="#Semantic-relatedness-in-DBpedia-A-comparative-and-experimental-assessment" class="headerlink" title="Semantic relatedness in DBpedia: A comparative and experimental assessment"></a>Semantic relatedness in DBpedia: A comparative and experimental assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09502">http://arxiv.org/abs/2308.09502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Formica, Francesco Taglino</li>
<li>for: 本研究的目的是评估Web资源之间的Semantic relatedness，尤其是基于知识图的方法。</li>
<li>methods: 本研究选择了10种现有Literature中的方法，分为邻居资源、 triple patterns 和 triple weights 三种类型。这些方法都基于DBpedia作为参考RDF知识图进行实现和评估。</li>
<li>results: 根据实验结果，将RDF triplets权重化并评估所有导向链连接的资源是计算DBpedia中Semantic relatedness的最佳策略。<details>
<summary>Abstract</summary>
Evaluating semantic relatedness of Web resources is still an open challenge. This paper focuses on knowledge-based methods, which represent an alternative to corpus-based approaches, and rely in general on the availability of knowledge graphs. In particular, we have selected 10 methods from the existing literature, that have been organized according to it adjacent resources, triple patterns, and triple weights-based methods. They have been implemented and evaluated by using DBpedia as reference RDF knowledge graph. Since DBpedia is continuously evolving, the experimental results provided by these methods in the literature are not comparable. For this reason, in this work, such methods have been experimented by running them all at once on the same DBpedia release and against 14 well-known golden datasets. On the basis of the correlation values with human judgment obtained according to the experimental results, weighting the RDF triples in combination with evaluating all the directed paths linking the compared resources is the best strategy in order to compute semantic relatedness in DBpedia.
</details>
<details>
<summary>摘要</summary>
evaluating semantic relatedness of web resources is still an open challenge. this paper focuses on knowledge-based methods, which represent an alternative to corpus-based approaches, and rely in general on the availability of knowledge graphs. in particular, we have selected 10 methods from the existing literature, that have been organized according to their adjacent resources, triple patterns, and triple weights-based methods. they have been implemented and evaluated by using dbpedia as reference rdf knowledge graph. since dbpedia is continuously evolving, the experimental results provided by these methods in the literature are not comparable. for this reason, in this work, such methods have been experimented by running them all at once on the same dbpedia release and against 14 well-known golden datasets. on the basis of the correlation values with human judgment obtained according to the experimental results, weighting the rdf triples in combination with evaluating all the directed paths linking the compared resources is the best strategy in order to compute semantic relatedness in dbpedia.
</details></li>
</ul>
<hr>
<h2 id="Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication"><a href="#Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication" class="headerlink" title="Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication"></a>Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09497">http://arxiv.org/abs/2308.09497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jayralencar/pictogram_prediction_pt">https://github.com/jayralencar/pictogram_prediction_pt</a></li>
<li>paper_authors: Jayr Pereira, Rodrigo Nogueira, Cleber Zanchettin, Robson Fidalgo</li>
<li>for: This paper proposes using a BERT-like model for pictogram prediction in AAC systems to improve the efficiency of message authoring for individuals with complex communication needs.</li>
<li>methods: The authors finetune BERTimbau, a Brazilian Portuguese version of BERT, using an AAC corpus for Brazilian Portuguese, and test different approaches to representing a pictogram for prediction, including as a word, as a concept, and as a set of synonyms.</li>
<li>results: The results demonstrate that using embeddings computed from the pictograms’ captions, synonyms, or definitions have a similar performance, and using synonyms leads to lower perplexity, but using captions leads to the highest accuracies. The paper provides insight into how to represent a pictogram for prediction using a BERT-like model and the potential of using images for pictogram prediction.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文提议使用基于BERT的模型进行AAC系统中图像预测，以提高帮助受残障人群表达需求的消息排版效率。</li>
<li>methods: 作者使用了一个特定于巴西葡萄牙语的BERT版本（BERTimbau），并使用了一个特定于巴西葡萄牙语的AAC corpus进行训练。他们测试了不同的图像预测方法，包括作为单词（使用图像描述）、作为概念（使用词典定义）和作为相关词（使用相关词）。</li>
<li>results: 结果表明，使用图像描述、词典定义或相关词来计算图像预测的方法具有相似的性能。使用相关词的方法可以得到更低的混淆率，但使用描述的方法可以得到最高的准确率。这篇论文提供了使用基于BERT的模型进行图像预测的可能性和图像预测的技术。<details>
<summary>Abstract</summary>
Individuals with complex communication needs (CCN) often rely on augmentative and alternative communication (AAC) systems to have conversations and communique their wants. Such systems allow message authoring by arranging pictograms in sequence. However, the difficulty of finding the desired item to complete a sentence can increase as the user's vocabulary increases. This paper proposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram prediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus for Brazilian Portuguese to use as a training corpus. We tested different approaches to representing a pictogram for prediction: as a word (using pictogram captions), as a concept (using a dictionary definition), and as a set of synonyms (using related terms). We also evaluated the usage of images for pictogram prediction. The results demonstrate that using embeddings computed from the pictograms' caption, synonyms, or definitions have a similar performance. Using synonyms leads to lower perplexity, but using captions leads to the highest accuracies. This paper provides insight into how to represent a pictogram for prediction using a BERT-like model and the potential of using images for pictogram prediction.
</details>
<details>
<summary>摘要</summary>
人们 WITH complex communication needs (CCN) 常常使用增强型替代通信系统 (AAC) 进行交流和表达自己的需求。这些系统允许用户通过排序绘图来编写消息。然而，如果用户的词汇量增加，则查找所需的项目可能变得更加困难。这篇论文提议使用 BERTimbau，一种巴西葡萄牙语版的 BERT，来预测绘图。为了训练 BERTimbau，我们创建了一个用于巴西葡萄牙语 AAC 系统的训练集。我们测试了不同的绘图预测方法：以字符 (使用绘图描述)、以概念 (使用词典定义) 和以同义词 (使用相关词语) 三种方法。我们还评估了使用图像进行绘图预测。结果表明，使用绘图描述、同义词或词典定义生成的嵌入都有类似的表现。使用同义词可以降低噪音水平，但使用绘图描述可以达到最高的准确率。这篇论文提供了如何使用 BERT-like 模型来预测绘图，以及使用图像进行绘图预测的潜在性。
</details></li>
</ul>
<hr>
<h2 id="Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models"><a href="#Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models" class="headerlink" title="Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models"></a>Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09490">http://arxiv.org/abs/2308.09490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Hintersdorf, Lukas Struppek, Kristian Kersting</li>
<li>for: 这项研究的目的是提醒用户关于使用开源机器学习模型的隐私和安全风险。</li>
<li>methods: 该研究使用了诸如攻击分析和隐私检测等方法来描述开源模型中常见的隐私和安全风险。</li>
<li>results: 研究发现了许多开源模型中隐私和安全风险的例子，包括模型中隐藏的功能和输入模式的攻击等。这些风险可能导致服务中断、敏感用户数据泄露和更严重的物理损害等。<details>
<summary>Abstract</summary>
The field of artificial intelligence (AI) has experienced remarkable progress in recent years, driven by the widespread adoption of open-source machine learning models in both research and industry. Considering the resource-intensive nature of training on vast datasets, many applications opt for models that have already been trained. Hence, a small number of key players undertake the responsibility of training and publicly releasing large pre-trained models, providing a crucial foundation for a wide range of applications. However, the adoption of these open-source models carries inherent privacy and security risks that are often overlooked. To provide a concrete example, an inconspicuous model may conceal hidden functionalities that, when triggered by specific input patterns, can manipulate the behavior of the system, such as instructing self-driving cars to ignore the presence of other vehicles. The implications of successful privacy and security attacks encompass a broad spectrum, ranging from relatively minor damage like service interruptions to highly alarming scenarios, including physical harm or the exposure of sensitive user data. In this work, we present a comprehensive overview of common privacy and security threats associated with the use of open-source models. By raising awareness of these dangers, we strive to promote the responsible and secure use of AI systems.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）领域在最近几年内取得了很大进步，由于研究和业务中广泛采用开源机器学习模型而驱动。由于训练大量数据集需要巨量的资源，许多应用程序选择使用已经训练过的模型。因此，只有一些关键参与者在训练和公共发布大型预训练模型，为各种应用提供了重要的基础。然而，使用这些开源模型的采用带来了隐私和安全风险，这些风险frequently overlooked。为了提供一个具体的例子，一个无征的模型可能封装了隐藏的功能，当特定的输入模式触发时，可以 manipulate the behavior of the system，如 instructing self-driving cars to ignore the presence of other vehicles。成功的隐私和安全攻击的后果extremely broad，从relatively minor damage like service interruptions到highly alarming scenarios, including physical harm or the exposure of sensitive user data。在这项工作中，我们提供了对公开源模型的常见隐私和安全威胁的全面概述。通过提醒这些危险，我们希望促进AI系统的负责任和安全使用。
</details></li>
</ul>
<hr>
<h2 id="Modelling-Electricity-Consumption-in-Irish-Dairy-Farms-Using-Agent-Based-Modelling"><a href="#Modelling-Electricity-Consumption-in-Irish-Dairy-Farms-Using-Agent-Based-Modelling" class="headerlink" title="Modelling Electricity Consumption in Irish Dairy Farms Using Agent-Based Modelling"></a>Modelling Electricity Consumption in Irish Dairy Farms Using Agent-Based Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09488">http://arxiv.org/abs/2308.09488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Khaleghy, Abdul Wahid, Eoghan Clifford, Karl Mason</li>
<li>for: 这个研究报告是为了研究爱尔兰奶牛农场的电力消耗而写的。</li>
<li>methods: 该研究使用了代理模型来模拟奶牛农场的电力消耗，考虑了奶牛数量、牛奶机器数量以及时间的影响。</li>
<li>results: 研究发现代理模型可以准确地预测奶牛农场的电力消耗，并且可以提供完全可解释的输出，这是其他人工智能技术，如深度学习模型所不具备的优点。<details>
<summary>Abstract</summary>
Dairy farming can be an energy intensive form of farming. Understanding the factors affecting electricity consumption on dairy farms is crucial for farm owners and energy providers. In order to accurately estimate electricity demands in dairy farms, it is necessary to develop a model. In this research paper, an agent-based model is proposed to model the electricity consumption of Irish dairy farms. The model takes into account various factors that affect the energy consumption of dairy farms, including herd size, number of milking machines, and time of year. The outputs are validated using existing state-of-the-art dairy farm modelling frameworks. The proposed agent-based model is fully explainable, which is an advantage over other Artificial Intelligence techniques, e.g. deep learning.
</details>
<details>
<summary>摘要</summary>
牛奶农业可能是一种能源密集的农业形式。了解牛奶农场电ity消耗的因素对农场所有者和能源供应商非常重要。为了准确估算牛奶农场电ity需求，需要开发一个模型。在这篇研究论文中，我们提出了一个代理基模型，用于模拟爱尔兰牛奶农场电ity消耗。该模型考虑了各种影响牛奶农场电ity消耗的因素，包括牛群规模、数量 milking machines 和时间季节。输出被验证使用现有的国际先进的牛奶农场模型框架。我们的代理基模型具有可解释性，这是对其他人工智能技术，例如深度学习，的优势。
</details></li>
</ul>
<hr>
<h2 id="Poison-Dart-Frog-A-Clean-Label-Attack-with-Low-Poisoning-Rate-and-High-Attack-Success-Rate-in-the-Absence-of-Training-Data"><a href="#Poison-Dart-Frog-A-Clean-Label-Attack-with-Low-Poisoning-Rate-and-High-Attack-Success-Rate-in-the-Absence-of-Training-Data" class="headerlink" title="Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data"></a>Poison Dart Frog: A Clean-Label Attack with Low Poisoning Rate and High Attack Success Rate in the Absence of Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09487">http://arxiv.org/abs/2308.09487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/magic-ma-tech/poison-dart-frog">https://github.com/magic-ma-tech/poison-dart-frog</a></li>
<li>paper_authors: Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng</li>
<li>for: 防止背门的攻击 (backdoor attacks)</li>
<li>methods: 使用“Poison Dart Frog”clean-label方法，不需要攻击者有训练集的知识 (knowledge of the entire training set or a portion of it)</li>
<li>results: 在CIFAR10、Tiny-ImageNet和TSRD上，使用0.1%、0.025%和0.4%的训练集恶化率，分别达到高的攻击成功率，并与州对抗方法NARCISSUS相似的攻击成功率，而不需要任何训练集知识。<details>
<summary>Abstract</summary>
To successfully launch backdoor attacks, injected data needs to be correctly labeled; otherwise, they can be easily detected by even basic data filters. Hence, the concept of clean-label attacks was introduced, which is more dangerous as it doesn't require changing the labels of injected data. To the best of our knowledge, the existing clean-label backdoor attacks largely relies on an understanding of the entire training set or a portion of it. However, in practice, it is very difficult for attackers to have it because of training datasets often collected from multiple independent sources. Unlike all current clean-label attacks, we propose a novel clean label method called 'Poison Dart Frog'. Poison Dart Frog does not require access to any training data; it only necessitates knowledge of the target class for the attack, such as 'frog'. On CIFAR10, Tiny-ImageNet, and TSRD, with a mere 0.1\%, 0.025\%, and 0.4\% poisoning rate of the training set size, respectively, Poison Dart Frog achieves a high Attack Success Rate compared to LC, HTBA, BadNets, and Blend. Furthermore, compared to the state-of-the-art attack, NARCISSUS, Poison Dart Frog achieves similar attack success rates without any training data. Finally, we demonstrate that four typical backdoor defense algorithms struggle to counter Poison Dart Frog.
</details>
<details>
<summary>摘要</summary>
要成功发起后门攻击，注入数据需要正确地标签，否则可能被even basic数据筛选器轻易探测。因此， cleaner-label攻击被引入，这种攻击更加危险，因为它不需要改变注入数据的标签。据我们所知，现有的 cleaner-label 后门攻击大多数依据整个训练集或一部分它的理解。然而，在实践中，攻击者很难获得这些训练集，因为训练集通常来自多个独立的源头。不同于现有的 cleaner-label 攻击，我们提出了一种新的 cleaner-label 方法 called 'Poison Dart Frog'。Poison Dart Frog 不需要训练集的访问权限，只需要target类的知识，例如 '蛙'。在 CIFAR10、Tiny-ImageNet 和 TSRD 上，使用0.1%、0.025% 和 0.4% 的训练集大小杂入率，Poison Dart Frog 可以获得高度的 Attack Success Rate，比LC、HTBA、BadNets 和 Blend 高。此外，与当前状态的攻击相比，Poison Dart Frog 可以达到类似的攻击成功率，不需要任何训练数据。最后，我们示示了四种常见的后门防御算法无法防御 Poison Dart Frog。
</details></li>
</ul>
<hr>
<h2 id="RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition"><a href="#RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition" class="headerlink" title="RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition"></a>RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11029">http://arxiv.org/abs/2308.11029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luftmenscher/RBA-GCN">https://github.com/luftmenscher/RBA-GCN</a></li>
<li>paper_authors: Lin Yuan, Guoheng Huang, Fenghuan Li, Xiaochen Yuan, Chi-Man Pun, Guo Zhong</li>
<li>for: 这篇论文的目的是提出一种基于图 convolutional networks (GCNs) 的 Emotion recognition in conversation (ERC) 方法，以解决传统GCNs的节点资料重复问题和单层GCNs无法捕捉广泛的上下文资讯问题。</li>
<li>methods: 这篇论文使用了三个模组：graph generation module (GGM)、similarity-based cluster building module (SCBM) 和 bilevel aggregation module (BiAM)。GGM 将目标节点资料简化为减少节点资料重复问题，而 SCBM 计算节点与其结构邻域的相似性，删除低相似性的资讯以保留节点的决定性资讯。 Meanwhile, BiAM 是一种新的统计方法，可以在不同模式之间建立互动，并捕捉广泛的上下文资讯。</li>
<li>results: 在 IEMOCAP 和 MELD  datasets 上，RBA-GCN 的 weighted average F1 score 与最先进方法相比，有2.17%∼5.21%的提升。<details>
<summary>Abstract</summary>
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation module (BiAM). First, GGM constructs a novel graph to reduce the redundancy of target node information. Then, SCBM calculates the node similarity in the target node and its structural neighborhood, where noisy information with low similarity is filtered out to preserve the discriminant information of the node. Meanwhile, BiAM is a novel aggregation method that can preserve the information of nodes during the aggregation process. This module can construct the interaction between different modalities and capture long-range contextual information based on similarity clusters. On both the IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a 2.17$\sim$5.21\% improvement over that of the most advanced method.
</details>
<details>
<summary>摘要</summary>
很多研究者对话情感识别（ERC）的注意力增加，因为它在各种应用方面有广泛的应用空间。由于对话有自然的图形结构，许多方法使用图形傅敏网（GCNs）来模型ERC，它们已经获得了重要的成果。但是，传统GCN的聚合方法受到节点信息重复问题的影响，导致节点标识信息的损失。此外，单层GCN缺乏捕捉长距离内容关联的能力。此外，大多数方法仅基于文本模式或将不同模式组合起来，导致模式间的互动capture能力弱化。为了解决这些问题，我们提出了关系两级聚合图形傅敏网（RBA-GCN），它包括三个模组：图形生成模组（GGM）、相似度基于集群建立模组（SCBM）和两级聚合模组（BiAM）。首先，GGM将建立一个新的图形，以减少目标节点信息的重复性。接着，SCBM计算目标节点和其结构邻域中的相似度，并将低相似度的信息排除，以保留节点标识信息的 дискリ民资讯。同时，BiAM是一个新的聚合方法，可以在聚合过程中保持节点信息。这个模组可以建立不同模式之间的互动，并捕捉长距离内容关联。在IEMOCAP和MELD dataset上，RBA-GCN的复合权重平均F1分数与最先进方法相比，具有2.17%∼5.21%的提升。
</details></li>
</ul>
<hr>
<h2 id="AI-Hilbert-From-Data-and-Background-Knowledge-to-Automated-Scientific-Discovery"><a href="#AI-Hilbert-From-Data-and-Background-Knowledge-to-Automated-Scientific-Discovery" class="headerlink" title="AI Hilbert: From Data and Background Knowledge to Automated Scientific Discovery"></a>AI Hilbert: From Data and Background Knowledge to Automated Scientific Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09474">http://arxiv.org/abs/2308.09474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Cory-Wright, Bachir El Khadir, Cristina Cornelio, Sanjeeb Dash, Lior Horesh</li>
<li>for: 本研究的目的是找到一种能够简洁地解释自然现象的科学方程式，并与现有背景理论相符。</li>
<li>methods: 本研究使用了回归和理解的方法，以消除与背景理论不符的方程式。</li>
<li>results: 研究表明，使用拟合和逻辑的方法可以快速地找到一个与数据最佳匹配的科学方程式，并且可以自动证明这些发现的正确性。<details>
<summary>Abstract</summary>
The discovery of scientific formulae that parsimoniously explain natural phenomena and align with existing background theory is a key goal in science. Historically, scientists have derived natural laws by manipulating equations based on existing knowledge, forming new equations, and verifying them experimentally. In recent years, data-driven scientific discovery has emerged as a viable competitor in settings with large amounts of experimental data. Unfortunately, data-driven methods often fail to discover valid laws when data is noisy or scarce. Accordingly, recent works combine regression and reasoning to eliminate formulae inconsistent with background theory. However, the problem of searching over the space of formulae consistent with background theory to find one that fits the data best is not well solved. We propose a solution to this problem when all axioms and scientific laws are expressible via polynomial equalities and inequalities and argue that our approach is widely applicable. We further model notions of minimal complexity using binary variables and logical constraints, solve polynomial optimization problems via mixed-integer linear or semidefinite optimization, and automatically prove the validity of our scientific discoveries via Positivestellensatz certificates. Remarkably, the optimization techniques leveraged in this paper allow our approach to run in polynomial time with fully correct background theory, or non-deterministic polynomial (NP) time with partially correct background theory. We experimentally demonstrate that some famous scientific laws, including Kepler's Third Law of Planetary Motion, the Hagen-Poiseuille Equation, and the Radiated Gravitational Wave Power equation, can be automatically derived from sets of partially correct background axioms.
</details>
<details>
<summary>摘要</summary>
科学发现的目标是找到经济、与背景理论相align的自然现象的科学方程。历史上，科学家通过拟合方程、形成新方程、并通过实验验证来 derive 自然法律。在近年来，数据驱动的科学发现得到了广泛应用，但是在具有噪音或缺乏数据的情况下，数据驱动方法常常无法发现有效的法律。因此，现有的方法通常是结合回归和逻辑来排除不符合背景理论的方程。然而，搜索符合背景理论的方程空间中最佳方程的问题还没有得到妥善解决。我们提出一种解决这个问题的方法，其中所有的axioms和科学法律都可以表示为多项式等式和不等式。我们还使用二进制变量和逻辑约束来模型简洁度的概念，使用混合整数线性或半definiteProgramming来解决多项式优化问题，并使用Positivestellensatz证明来自动验证我们的科学发现。很显然，我们的优化技术可以使我们的方法在 polynomial 时间内运行，并且完全符合背景理论，或者在 partially correct 背景理论下运行在非 deterministic  polynomial 时间内。我们在实验中证明了一些著名的科学法律，包括凯撒第三法律、hagen-Poiseuille方程和释放 gravitational wave 的能量方程，可以自动从部分正确的背景axioms中 derivation。
</details></li>
</ul>
<hr>
<h2 id="Vision-Relation-Transformer-for-Unbiased-Scene-Graph-Generation"><a href="#Vision-Relation-Transformer-for-Unbiased-Scene-Graph-Generation" class="headerlink" title="Vision Relation Transformer for Unbiased Scene Graph Generation"></a>Vision Relation Transformer for Unbiased Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09472">http://arxiv.org/abs/2308.09472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visinf/veto">https://github.com/visinf/veto</a></li>
<li>paper_authors: Gopika Sudhakaran, Devendra Singh Dhami, Kristian Kersting, Stefan Roth</li>
<li>for:  scene graph generation (SGG) task, aiming to predict entity relationships using a relation encoder-decoder pipeline stacked on top of an object encoder-decoder backbone.</li>
<li>methods:  introduces the Vision rElation TransfOrmer (VETO), consisting of a novel local-level entity relation encoder, and the Mutually Exclusive ExperT (MEET) learning strategy to capture important relation features without bias towards head or tail classes.</li>
<li>results:  experimental results on the VG and GQA datasets demonstrate that VETO + MEET boosts the predictive performance by up to 47 percentage over the state of the art while being 10 times smaller.<details>
<summary>Abstract</summary>
Recent years have seen a growing interest in Scene Graph Generation (SGG), a comprehensive visual scene understanding task that aims to predict entity relationships using a relation encoder-decoder pipeline stacked on top of an object encoder-decoder backbone. Unfortunately, current SGG methods suffer from an information loss regarding the entities local-level cues during the relation encoding process. To mitigate this, we introduce the Vision rElation TransfOrmer (VETO), consisting of a novel local-level entity relation encoder. We further observe that many existing SGG methods claim to be unbiased, but are still biased towards either head or tail classes. To overcome this bias, we introduce a Mutually Exclusive ExperT (MEET) learning strategy that captures important relation features without bias towards head or tail classes. Experimental results on the VG and GQA datasets demonstrate that VETO + MEET boosts the predictive performance by up to 47 percentage over the state of the art while being 10 times smaller.
</details>
<details>
<summary>摘要</summary>
Recent years have seen a growing interest in Scene Graph Generation (SGG), a comprehensive visual scene understanding task that aims to predict entity relationships using a relation encoder-decoder pipeline stacked on top of an object encoder-decoder backbone. Unfortunately, current SGG methods suffer from an information loss regarding the entities local-level cues during the relation encoding process. To mitigate this, we introduce the Vision rElation TransfOrmer (VETO), consisting of a novel local-level entity relation encoder. We further observe that many existing SGG methods claim to be unbiased, but are still biased towards either head or tail classes. To overcome this bias, we introduce a Mutually Exclusive ExperT (MEET) learning strategy that captures important relation features without bias towards head or tail classes. Experimental results on the VG and GQA datasets demonstrate that VETO + MEET boosts the predictive performance by up to 47 percentage over the state of the art while being 10 times smaller.Here's the translation in Traditional Chinese:Recent years have seen a growing interest in Scene Graph Generation (SGG), a comprehensive visual scene understanding task that aims to predict entity relationships using a relation encoder-decoder pipeline stacked on top of an object encoder-decoder backbone. Unfortunately, current SGG methods suffer from an information loss regarding the entities local-level cues during the relation encoding process. To mitigate this, we introduce the Vision rElation TransfOrmer (VETO), consisting of a novel local-level entity relation encoder. We further observe that many existing SGG methods claim to be unbiased, but are still biased towards either head or tail classes. To overcome this bias, we introduce a Mutually Exclusive ExperT (MEET) learning strategy that captures important relation features without bias towards head or tail classes. Experimental results on the VG and GQA datasets demonstrate that VETO + MEET boosts the predictive performance by up to 47 percentage over the state of the art while being 10 times smaller.
</details></li>
</ul>
<hr>
<h2 id="Artificial-Spiking-Hierarchical-Networks-for-Vision-Language-Representation-Learning"><a href="#Artificial-Spiking-Hierarchical-Networks-for-Vision-Language-Representation-Learning" class="headerlink" title="Artificial-Spiking Hierarchical Networks for Vision-Language Representation Learning"></a>Artificial-Spiking Hierarchical Networks for Vision-Language Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09455">http://arxiv.org/abs/2308.09455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeming Chen, Siyu Zhang, Yaoru Sun, Weijian Liang, Haoran Wang</li>
<li>for: 本研究旨在提高视频语言（VL）任务中的semantic gap问题，通过引入一种新的视觉 semantic模块来提高 VL 任务的性能。</li>
<li>methods: 本研究提出了一种灵活的计算框架，即Artificial-Spiking Hierarchical Networks (ASH-Nets)，它将Artificial neural networks (ANNs)和Spiking neural networks (SNNs)的优点相结合，以增强视觉 semantic表示。具体来说，ASH-Nets 使用了一个视觉具体编码器和一个semantic抽象编码器，以学习不同类型的 kontinuous和 discrete 隐藏变量，以提高视觉表示的灵活性。此外，通过对相似样本进行对比学习，可以提高层次网络的计算效率，同时对硬样本的扩充也有助于视觉表示的学习。</li>
<li>results: 在多个Established downstream VL tasks上，提出的 STUA 预训练方法和 ASH-Nets 模型实现了竞争性的 результаados。<details>
<summary>Abstract</summary>
With the success of self-supervised learning, multimodal foundation models have rapidly adapted a wide range of downstream tasks driven by vision and language (VL) pretraining. State-of-the-art methods achieve impressive performance by pre-training on large-scale datasets. However, bridging the semantic gap between the two modalities remains a nonnegligible challenge for VL tasks. In this work, we propose an efficient computation framework for multimodal alignment by introducing a novel visual semantic module to further improve the performance of the VL tasks. Specifically, we propose a flexible model, namely Artificial-Spiking Hierarchical Networks (ASH-Nets), which combines the complementary advantages of Artificial neural networks (ANNs) and Spiking neural networks (SNNs) to enrich visual semantic representations. In particular, a visual concrete encoder and a semantic abstract encoder are constructed to learn continuous and discrete latent variables to enhance the flexibility of semantic encoding. Considering the spatio-temporal properties of SNNs modeling, we introduce a contrastive learning method to optimize the inputs of similar samples. This can improve the computational efficiency of the hierarchical network, while the augmentation of hard samples is beneficial to the learning of visual representations. Furthermore, the Spiking to Text Uni-Alignment Learning (STUA) pre-training method is proposed, which only relies on text features to enhance the encoding ability of abstract semantics. We validate the performance on multiple well-established downstream VL tasks. Experiments show that the proposed ASH-Nets achieve competitive results.
</details>
<details>
<summary>摘要</summary>
自顾精学学习的成功，多Modal基础模型快速适应了视频语言（VL）预训练驱动的广泛下游任务。状态顶尖方法在大规模数据预训练后达到了印象式的性能。然而，跨模态 semantic gap 问题仍然是 VL 任务的一个重要挑战。在这种情况下，我们提议一种高效的计算框架，即 Multimodal 对接框架，通过引入一种新的视觉semantic模块来进一步提高 VL 任务的性能。具体来说，我们提出一种灵活的模型，即人工神经网络（ANNs）和脉冲神经网络（SNNs）的组合模型，以增强视觉semantic表示。特别是，我们构建了一个视觉具体编码器和一个semantic抽象编码器，以学习不同的连续和离散的幂变量，以提高模型的灵活性。此外，我们引入了一种对比学习方法，以优化层次网络的输入，从而提高计算效率。此外，我们还提出了一种基于文本特征的预训练方法，即 STUA 预训练方法，可以增强模型的抽象semantic编码能力。我们 validate 了多个常见的下游 VL 任务的性能，实验结果表明，我们提出的 ASH-Nets 可以 achieve 竞争性的结果。
</details></li>
</ul>
<hr>
<h2 id="Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances"><a href="#Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances" class="headerlink" title="Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances"></a>Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11038">http://arxiv.org/abs/2308.11038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdul Rahman, Muhammad Aamir Basheer, Zubair Khalid, Muhammad Tahir, Momin Uppal</li>
<li>For: 优化快递总站的位置，以提高电商业务的效率和环保性。* Methods:  hybrid方法，首先使用K-Means对交通网络距离相关的配送点进行聚合，然后使用P-Median方法将总站设置在优化位置。* Results: 通过使用优化的总站位置，可以节省每个配送的10%（815米）的距离。<details>
<summary>Abstract</summary>
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&P) is used to demonstrate the effectiveness of the approach. Serving deliveries from the optimal hub locations results in the saving of 815 (10%) meters per delivery.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Delivery points are clustered using K-Means based on their spatial locations, utilizing road network distances instead of Euclidean distances to ensure accurate results.2. Hubs are located using the P-Median method, which takes into account the number of deliveries and population as weights to ensure the most efficient placement.Using real-world delivery data from Muller and Phipps (M&amp;P), we demonstrate the effectiveness of our approach. By serving deliveries from the optimal hub locations, we save 815 meters (10%) per delivery, significantly reducing the carbon footprint and improving the efficiency of the e-commerce industry.</details></li>
</ol>
<hr>
<h2 id="From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space"><a href="#From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space" class="headerlink" title="From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space"></a>From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09437">http://arxiv.org/abs/2308.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Dreyer, Frederik Pahde, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin</li>
<li>for: 这篇研究旨在减少深度神经网络中嵌入的伪 positives corrleations，以避免高风险的预测。</li>
<li>methods: 我们使用了一种新的方法，通过减少模型对伪 positives 的敏感性，以确保模型做出的预测是正确的。</li>
<li>results: 我们在控制环境和实际世界中进行了实验，在 ISIC、Bone Age、ImageNet 和 CelebA 数据集上使用 VGG、ResNet 和 EfficientNet 架构，实现了对伪 positives 的有效控制。<details>
<summary>Abstract</summary>
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures.
</details>
<details>
<summary>摘要</summary>
We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient 梯度。 When modeling biases via Concept Activation Vectors 概念活动向量, we highlight the importance of choosing robust directions 选择可靠的方向, as traditional regression-based approaches such as Support Vector Machines 支持向量机 tend to result in diverging directions 分散的方向. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures 在 ISIC, Bone Age, ImageNet 和 CelebA  dataset上使用 VGG, ResNet 和 EfficientNet 架构中有效地缓解偏见。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Agent-Communication-and-Learning-through-Action-and-Language"><a href="#Enhancing-Agent-Communication-and-Learning-through-Action-and-Language" class="headerlink" title="Enhancing Agent Communication and Learning through Action and Language"></a>Enhancing Agent Communication and Learning through Action and Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10842">http://arxiv.org/abs/2308.10842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Caselles-Dupré Hugo, Sigaud Olivier, Chetouani Mohamed</li>
<li>for: 这个论文旨在描述一种新的 GC-agent，可以同时作为教师和学生进行交互。</li>
<li>methods: 这种 GC-agent 利用了动作示例和语言指令，提高了交互效率。</li>
<li>results: 研究发现，通过 combining 动作和语言交互模式，可以提高学习效果。<details>
<summary>Abstract</summary>
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的 GC-代理人，可以作为 both 教师和学生。通过行为示例和语言指令，这些代理人可以提高交流效率。我们研究了包括教学理论和 Pragmatics 等人类交流和目标实现的关键元素，以提高代理人的教学和学习能力。此外，我们还探讨了将多种交流方式（行为和语言）结合使用的影响，并指出了多模式approach的 beneficial effects。
</details></li>
</ul>
<hr>
<h2 id="ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks"><a href="#ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks" class="headerlink" title="ICU Mortality Prediction Using Long Short-Term Memory Networks"></a>ICU Mortality Prediction Using Long Short-Term Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12800">http://arxiv.org/abs/2308.12800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manel Mili, Asma Kerkeni, Asma Ben Abdallah, Mohamed Hedi Bedoui</li>
<li>for: 这篇论文是为了提出一种自动化数据驱动系统，用于分析医疗电子健康记录（EHRs）中的大量多变量时间序列数据，并提取高级信息，以预测医院内死亡率和医疗时间（LOS）的早期预测。</li>
<li>methods: 这篇论文使用了LSTM网络，通过减少时间框架为6小时，提高临床任务效果。</li>
<li>results: 实验结果表明，LSTM模型在具有严格多变量时间序列测量的情况下，对实际世界预测建立了高效的预测机制。<details>
<summary>Abstract</summary>
Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines.
</details>
<details>
<summary>摘要</summary>
延伸床side监测在医疗急诊室（ICU）中已经导致了复杂的时间序列数据，这些数据提供了更高级别的临床数据分析的上下文。然而，在这些数据中找到时间序列模式可能提供高度预测临床事件的可能性。因此，在本研究中，我们调查了一个自动化的数据驱动系统，该系统分析大量的多变量时间序列数据，并从医疗记录（EHRs）中提取高级别的信息，以预测患者在医院内的死亡率和治疗时间（LOS）的早期预测。实际上，我们研究了使用LSTM网络，通过减少时间帧为6小时，以提高临床任务的效率。实验结果表明，LSTM模型在多变量时间序列测量下表现了高效的预测能力。
</details></li>
</ul>
<hr>
<h2 id="Multi-Level-Compositional-Reasoning-for-Interactive-Instruction-Following"><a href="#Multi-Level-Compositional-Reasoning-for-Interactive-Instruction-Following" class="headerlink" title="Multi-Level Compositional Reasoning for Interactive Instruction Following"></a>Multi-Level Compositional Reasoning for Interactive Instruction Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09387">http://arxiv.org/abs/2308.09387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suvaansh Bhambri, Byeonghwi Kim, Jonghyun Choi</li>
<li>for: 这个论文旨在提高机器人在家庭用品上进行各种任务的能力，使其能够更好地理解和完成复杂的任务指令。</li>
<li>methods: 该论文提出了一种分解任务为多个互相关联的子任务的方法，并通过多级组合政策来实现。</li>
<li>results: 该论文通过这种方法实现了2.03%的绝对提升（PLWSR）在未seen集中，而不使用规则基本规划或semantic spatial memory。<details>
<summary>Abstract</summary>
Robotic agents performing domestic chores by natural language directives are required to master the complex job of navigating environment and interacting with objects in the environments. The tasks given to the agents are often composite thus are challenging as completing them require to reason about multiple subtasks, e.g., bring a cup of coffee. To address the challenge, we propose to divide and conquer it by breaking the task into multiple subgoals and attend to them individually for better navigation and interaction. We call it Multi-level Compositional Reasoning Agent (MCR-Agent). Specifically, we learn a three-level action policy. At the highest level, we infer a sequence of human-interpretable subgoals to be executed based on language instructions by a high-level policy composition controller. At the middle level, we discriminatively control the agent's navigation by a master policy by alternating between a navigation policy and various independent interaction policies. Finally, at the lowest level, we infer manipulation actions with the corresponding object masks using the appropriate interaction policy. Our approach not only generates human interpretable subgoals but also achieves 2.03% absolute gain to comparable state of the arts in the efficiency metric (PLWSR in unseen set) without using rule-based planning or a semantic spatial memory.
</details>
<details>
<summary>摘要</summary>
机器人代理人在完成家务任务时需要掌握环境导航和对物体互动的复杂任务。给予的任务经常是复杂的，需要理解多个子任务，例如带一杯咖啡。为了解决这个挑战，我们提议分解任务为多个子目标，并对它们进行独立的处理，以更好地导航和互动。我们称之为多级组合理解代理人（MCR-Agent）。具体来说，我们学习了三级行为策略。在最高层，我们根据语言指令生成一个序列的人类可读Subgoal，并由高级组合控制器执行。在中层，我们通过alternating between a navigation policy和多种独立的互动策略来干扰导航。最后，在最低层，我们使用相应的互动策略来INFER操作。我们的方法不仅生成了人类可读的Subgoal，而且在PLWSR指标上（未seen集）实现了2.03%的绝对提升，不使用规则化计划或semantic spatial memory。
</details></li>
</ul>
<hr>
<h2 id="Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review"><a href="#Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review" class="headerlink" title="Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review"></a>Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09380">http://arxiv.org/abs/2308.09380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai</li>
<li>for: 本研究旨在提供一份关于膝关节杂合病（OA）诊断的帮助，使用可解释人工智能（XAI）技术以提高诊断的可靠性和可信度。</li>
<li>methods: 本研究使用了两种视角来描述XAI技术的应用：数据可解释性和模型可解释性。</li>
<li>results: 本研究发现XAI技术可以提高膝关节杂合病诊断的可靠性和可信度，并且可以提供有用的启示，以便促进XAI技术的应用在临床实践中。<details>
<summary>Abstract</summary>
Existing artificial intelligence (AI) models for diagnosing knee osteoarthritis (OA) have faced criticism for their lack of transparency and interpretability, despite achieving medical-expert-like performance. This opacity makes them challenging to trust in clinical practice. Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model's prediction by revealing how the prediction is derived, thus promoting the use of AI systems in healthcare. This paper presents the first survey of XAI techniques used for knee OA diagnosis. The XAI techniques are discussed from two perspectives: data interpretability and model interpretability. The aim of this paper is to provide valuable insights into XAI's potential towards a more reliable knee OA diagnosis approach and encourage its adoption in clinical practice.
</details>
<details>
<summary>摘要</summary>
现有的膝关节artoarthritis（OA）诊断模型（AI）受到了不透明性和解释性的批评，即使它们达到了医疗专业人员水平。这种透明性使得它们在临床实践中具有挑战。最近，解释性人工智能（XAI）作为一种专门的技术，可以提供对预测的信任，并透明地表明预测是如何 derivation的。这篇文章发表了膝关节OA诊断中使用XAI技术的首次报告。XAI技术从两个角度进行了讨论：数据解释性和模型解释性。文章的目的是提供XAI在膝关节OA诊断方面的可靠性的 valuable 信息，并促进XAI在临床实践中的采用。
</details></li>
</ul>
<hr>
<h2 id="Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers"><a href="#Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers" class="headerlink" title="Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers"></a>Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09372">http://arxiv.org/abs/2308.09372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tobna/whattransformertofavor">https://github.com/tobna/whattransformertofavor</a></li>
<li>paper_authors: Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel</li>
<li>for: This paper aims to provide a comprehensive evaluation of vision transformers and related architectures, focusing on their efficiency across multiple performance metrics.</li>
<li>methods: The authors use more than 30 models and consider various performance metrics to evaluate the efficiency of different architectures. They also propose a hybrid attention-CNN model that performs well with low inference memory and number of parameters.</li>
<li>results: The study finds that ViT is still Pareto optimal across multiple efficiency metrics, despite the existence of alternative approaches claiming to be more efficient. The authors also discover a strong positive correlation between the number of FLOPS and training memory, and that scaling the model size is more effective than scaling the image size. The study provides valuable insights for practitioners and researchers when selecting models for specific applications.<details>
<summary>Abstract</summary>
The growing popularity of Vision Transformers as the go-to models for image classification has led to an explosion of architectural modifications claiming to be more efficient than the original ViT. However, a wide diversity of experimental conditions prevents a fair comparison between all of them, based solely on their reported results. To address this gap in comparability, we conduct a comprehensive analysis of more than 30 models to evaluate the efficiency of vision transformers and related architectures, considering various performance metrics. Our benchmark provides a comparable baseline across the landscape of efficiency-oriented transformers, unveiling a plethora of surprising insights. For example, we discover that ViT is still Pareto optimal across multiple efficiency metrics, despite the existence of several alternative approaches claiming to be more efficient. Results also indicate that hybrid attention-CNN models fare particularly well when it comes to low inference memory and number of parameters, and also that it is better to scale the model size, than the image size. Furthermore, we uncover a strong positive correlation between the number of FLOPS and the training memory, which enables the estimation of required VRAM from theoretical measurements alone.   Thanks to our holistic evaluation, this study offers valuable insights for practitioners and researchers, facilitating informed decisions when selecting models for specific applications. We publicly release our code and data at https://github.com/tobna/WhatTransformerToFavor
</details>
<details>
<summary>摘要</summary>
“vision transformer”的快速增长 popularity 使得许多建筑修改被提出，承认这些模型更高效 чем原始的 ViT。然而，各种实验条件的多样性使得对各模型的比较变得困难，根据报告的结果来进行对比。为了解决这个问题，我们进行了对超过30个模型的全面分析，以评估视transformer和相关建筑的效率，并考虑多种性能指标。我们的基准提供了不同类型的transformer模型之间的比较基准，揭示了许多有趣的发现。例如，我们发现， despite the existence of several alternative approaches claiming to be more efficient, ViT仍然在多种效率指标上保持Pareto优化的状态。此外，我们发现在低执行 памяти和参数数量方面，混合注意力-CNN模型表现特别好，而且Scaling the model size是比Scale the image size更好的选择。此外，我们发现对于FLOPS和训练内存之间存在强正相关关系，这使得通过理论测量 alone 可以估算需要的VRAM。由于我们的彻底评估，这些研究可以为实践者和研究人员提供有价值的意见，以便在特定应用中选择合适的模型。我们将代码和数据公开发布在https://github.com/tobna/WhatTransformerToFavor上。”
</details></li>
</ul>
<hr>
<h2 id="RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training"><a href="#RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training" class="headerlink" title="RLIPv2: Fast Scaling of Relational Language-Image Pre-training"></a>RLIPv2: Fast Scaling of Relational Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09351">http://arxiv.org/abs/2308.09351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobyuan7/rlipv2">https://github.com/jacobyuan7/rlipv2</a></li>
<li>paper_authors: Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni, Yingya Zhang, Deli Zhao</li>
<li>for: 提高计算机视觉任务中的关系理解能力，通过将视觉表示与语言表示相关联。</li>
<li>methods: 提出了一种快速结合的模型RLIPv2，使得可以在大规模 Pseudo-labelled scene graph 数据上进行关系预训练。RLIPv2 引入了非对称语言图像融合 (ALIF) 机制，使得早期和深入的受阻语言编码层可以更加简洁。</li>
<li>results: 通过对 Human-Object Interaction Detection 和 Scene Graph Generation 等两个任务进行广泛的实验，RLIPv2 在三个标准 bencmarks 上达到了状态的艺术性表现，包括无需训练、几架shot 和零shot 设置下的表现。特别是，最大的 RLIPv2 在 HICO-DET 上达到了 23.29mAP 的最高分，只需要 1% 的数据进行 finituning，可以达到 32.22mAP 的表现，并且在 100% 的数据上进行 finituning 可以达到 45.09mAP。<details>
<summary>Abstract</summary>
Relational Language-Image Pre-training (RLIP) aims to align vision representations with relational texts, thereby advancing the capability of relational reasoning in computer vision tasks. However, hindered by the slow convergence of RLIPv1 architecture and the limited availability of existing scene graph data, scaling RLIPv1 is challenging. In this paper, we propose RLIPv2, a fast converging model that enables the scaling of relational pre-training to large-scale pseudo-labelled scene graph data. To enable fast scaling, RLIPv2 introduces Asymmetric Language-Image Fusion (ALIF), a mechanism that facilitates earlier and deeper gated cross-modal fusion with sparsified language encoding layers. ALIF leads to comparable or better performance than RLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtain scene graph data at scale, we extend object detection datasets with free-form relation labels by introducing a captioner (e.g., BLIP) and a designed Relation Tagger. The Relation Tagger assigns BLIP-generated relation texts to region pairs, thus enabling larger-scale relational pre-training. Through extensive experiments conducted on Human-Object Interaction Detection and Scene Graph Generation, RLIPv2 shows state-of-the-art performance on three benchmarks under fully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2 achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP with just 1% data and yields 45.09mAP with 100% data. Code and models are publicly available at https://github.com/JacobYuan7/RLIPv2.
</details>
<details>
<summary>摘要</summary>
RLIP（关系语言图前期训练）目的是将视觉表示与关系文本对齐，从而提高计算机视觉任务中的关系理解能力。然而，RLIPv1架构的慢慢涨潮和现有场景图数据的有限性，使得扩展RLIPv1的困难。在这篇论文中，我们提出RLIPv2，一种快速涨潮的模型，可以将关系预训练扩展到大规模 Pseudo-标注场景图数据。为了快速涨潮，RLIPv2引入了异常语言图像融合（ALIF）机制，使得更早、更深的隔离modal融合，并使用压缩语言编码层。ALIF使得RLIPv2在预训练和精度调整中比RLIPv1快得多。为了获得大规模场景图数据，我们将对象检测数据集扩展，并在BLIP（例如）和设计的关系标签器（Relation Tagger）的帮助下，对区域对应关系文本进行分配。这使得更大规模的关系预训练可能。通过对人物交互检测和场景图生成进行广泛的实验，RLIPv2在三个标准准则下达到了状态 искусственный智能水平，包括完全finetuning、几何shot和零shot设置。尤其是RLIPv2最大的版本在HICO-DET上达到了23.29mAP，无需任何调整，并且在1%数据上达到了32.22mAP，以及在100%数据上达到了45.09mAP。代码和模型在https://github.com/JacobYuan7/RLIPv2上公开。
</details></li>
</ul>
<hr>
<h2 id="Surprise-machines-revealing-Harvard-Art-Museums’-image-collection"><a href="#Surprise-machines-revealing-Harvard-Art-Museums’-image-collection" class="headerlink" title="Surprise machines: revealing Harvard Art Museums’ image collection"></a>Surprise machines: revealing Harvard Art Museums’ image collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09343">http://arxiv.org/abs/2308.09343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dario Rodighiero, Lins Derry, Douglas Duhaime, Jordan Kruguer, Maximilian C. Mueller, Christopher Pietsch, Jeffrey T. Schnapp, Jeff Steward</li>
<li>for: 这项研究旨在为哈佛艺术博物馆的所有图像收藏开展一项实验性的 museology 项目，以探索人工智能在显示大量图像时的限制，并为访问者带来意外的视觉体验。</li>
<li>methods: 该项目使用了一种chorographic interface，通过访问者的运动连接多个独特的图像视图，以创造意外的感受。</li>
<li>results: 该项目成功地创造了一种意外的视觉体验，使访问者能够更深入地了解和探索哈佛艺术博物馆的图像收藏。<details>
<summary>Abstract</summary>
Surprise Machines is a project of experimental museology that sets out to visualize the entire image collection of the Harvard Art Museums, intending to open up unexpected vistas on more than 200,000 objects usually inaccessible to visitors. Part of the exhibition Curatorial A(i)gents organized by metaLAB (at) Harvard, the project explores the limits of artificial intelligence to display a large set of images and create surprise among visitors. To achieve such a feeling of surprise, a choreographic interface was designed to connect the audience's movement with several unique views of the collection.
</details>
<details>
<summary>摘要</summary>
《意料之机》是哈佛艺术博物馆的一个实验 museology 项目，旨在将哈佛艺术博物馆的全部图像收藏视觉化，以开拓访问者未曾能够访问的更多 чем200,000个物品。该项目是metaLAB（@）哈佛所组织的《Curatorial A(i)gents》展览的一部分，探索人工智能是否能够显示大量图像并创造访问者的意外感。为实现此类感受，项目设计了一个chorographic User Interface，以连接访客的移动和图像收藏中的多个独特视图。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Neurodynamics-Based-Backstepping-Optimal-Control-for-Robust-Constrained-Consensus-of-Underactuated-Underwater-Vehicles-Fleet"><a href="#Distributed-Neurodynamics-Based-Backstepping-Optimal-Control-for-Robust-Constrained-Consensus-of-Underactuated-Underwater-Vehicles-Fleet" class="headerlink" title="Distributed Neurodynamics-Based Backstepping Optimal Control for Robust Constrained Consensus of Underactuated Underwater Vehicles Fleet"></a>Distributed Neurodynamics-Based Backstepping Optimal Control for Robust Constrained Consensus of Underactuated Underwater Vehicles Fleet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09326">http://arxiv.org/abs/2308.09326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Yan, Zhe Xu, Simon X. Yang, S. Andrew Gadsden</li>
<li>for: 本研究旨在提出一种robust constrained formation tracking控制方法，用于三维空间下无力 actuated underwater vehicles（UUVs）队伍的可靠性和稳定性问题。</li>
<li>methods: 本研究采用了一种协调协议和一种稳定控制器，其中约束级别采用了层次架构。在上层，通过圆拟变换来解决非拘束约束，然后开发了一种分布式优化运动协调策略。在下层控制循环中，采用了基于神经动力学的稳定控制器，以避免在传统的backstepping控制器中出现的”爆炸性”问题，并提高控制活动。</li>
<li>results: 研究发现，基于协调协议和稳定控制器的优化形态跟踪控制方法可以实现UUVs队伍的最佳形态跟踪，同时满足约束条件。此外，在不确定干扰的情况下，研究也证明了UUVs队伍的稳定性和可靠性。<details>
<summary>Abstract</summary>
Robust constrained formation tracking control of underactuated underwater vehicles (UUVs) fleet in three-dimensional space is a challenging but practical problem. To address this problem, this paper develops a novel consensus based optimal coordination protocol and a robust controller, which adopts a hierarchical architecture. On the top layer, the spherical coordinate transform is introduced to tackle the nonholonomic constraint, and then a distributed optimal motion coordination strategy is developed. As a result, the optimal formation tracking of UUVs fleet can be achieved, and the constraints are fulfilled. To realize the generated optimal commands better and, meanwhile, deal with the underactuation, at the lower-level control loop a neurodynamics based robust backstepping controller is designed, and in particular, the issue of "explosion of terms" appearing in conventional backstepping based controllers is avoided and control activities are improved. The stability of the overall UUVs formation system is established to ensure that all the states of the UUVs are uniformly ultimately bounded in the presence of unknown disturbances. Finally, extensive simulation comparisons are made to illustrate the superiority and effectiveness of the derived optimal formation tracking protocol.
</details>
<details>
<summary>摘要</summary>
Robust constrained formation tracking控制 OF underactuated underwater vehicles (UUVs) 舰队在三维空间是一个具有挑战性但实际性的问题。为解决这个问题，这篇论文开发了一种新的协调协议和一种稳定控制器，其采用了层次架构。在最高层，引入了球坐标变换来处理非整体约束，然后开发了分布式优化运动协调策略。因此，UUVs 舰队可以实现优化的形态跟踪，同时满足约束。为了实现生成的优化命令更好地，并且处理下 actuation，在下一级控制循环中设计了基于神经动力学的稳定后退控制器，其中避免了传统后退控制器中的“爆炸性”问题，提高了控制活动。最后，确立了 UUVs formation 系统的稳定性，以确保所有 UUVs 的状态在未知干扰的情况下都是 ultimately bounded。 finally，通过了EXTENSIVE 的simulation 比较，证明了获得的优化形态跟踪协议的优越性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Glance-Network-for-Efficient-Video-Recognition"><a href="#Audio-Visual-Glance-Network-for-Efficient-Video-Recognition" class="headerlink" title="Audio-Visual Glance Network for Efficient Video Recognition"></a>Audio-Visual Glance Network for Efficient Video Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09322">http://arxiv.org/abs/2308.09322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Adi Nugroho, Sangmin Woo, Sumin Lee, Changick Kim</li>
<li>for: 提高视频理解任务的效率和可扩展性，使用可用的音频和视频modalities进行效率的处理。</li>
<li>methods: 使用lightweight unimodal encoders提取全局视觉特征和音频特征，并使用Audio-Visual Temporal Saliency Transformer（AV-TeST）估计视频帧中的重要性分数。在空间维度中提高效率，使用Audio-Enhanced Spatial Patch Attention（AESPA）模块生成改进的粗细视觉特征，并使用策略网络生成重要的补做坐标。</li>
<li>results: 在多个视频认证测试准则中实现新的状态略进行多个视频认证任务，并实现更快的处理速度。<details>
<summary>Abstract</summary>
Deep learning has made significant strides in video understanding tasks, but the computation required to classify lengthy and massive videos using clip-level video classifiers remains impractical and prohibitively expensive. To address this issue, we propose Audio-Visual Glance Network (AVGN), which leverages the commonly available audio and visual modalities to efficiently process the spatio-temporally important parts of a video. AVGN firstly divides the video into snippets of image-audio clip pair and employs lightweight unimodal encoders to extract global visual features and audio features. To identify the important temporal segments, we use an Audio-Visual Temporal Saliency Transformer (AV-TeST) that estimates the saliency scores of each frame. To further increase efficiency in the spatial dimension, AVGN processes only the important patches instead of the whole images. We use an Audio-Enhanced Spatial Patch Attention (AESPA) module to produce a set of enhanced coarse visual features, which are fed to a policy network that produces the coordinates of the important patches. This approach enables us to focus only on the most important spatio-temporally parts of the video, leading to more efficient video recognition. Moreover, we incorporate various training techniques and multi-modal feature fusion to enhance the robustness and effectiveness of our AVGN. By combining these strategies, our AVGN sets new state-of-the-art performance in multiple video recognition benchmarks while achieving faster processing speed.
</details>
<details>
<summary>摘要</summary>
深度学习在视频理解任务中做出了重要的进步，但计算长期和大量视频使用clip级视频分类器仍然是不可能和过分昂贵的。为解决这个问题，我们提出了Audio-Visual Glance Network（AVGN），它利用通常可以获得的音频和视觉modalities来高效地处理视频中的空间-时间重要部分。AVGN首先将视频分割成帧和音频clip对的剪辑，然后使用轻量级单模态编码器提取全局视觉特征和音频特征。为了确定重要的时间段落，我们使用Audio-Visual Temporal Saliency Transformer（AV-TeST）来估算每帧的特征积分。然后，我们使用Audio-Enhanced Spatial Patch Attention（AESPA）模块生成一组改进后的粗糙视觉特征，并将其传递给一个策略网络，以生成视频中重要的空间-时间部分的坐标。这种方法使得我们只需要关注视频中最重要的空间-时间部分，从而提高视频识别的效率。此外，我们还 incorporatedvarious training techniques和多模态特征融合以提高我们的AVGN的 Robustness和有效性。通过组合这些策略，我们的AVGN在多个视频识别benchmark上达到了新的状态态表现，同时实现更快的处理速度。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Robust-Learning-Based-Backstepping-Control-Aided-with-Neurodynamics-for-Consensus-Formation-Tracking-of-Underwater-Vessels"><a href="#Distributed-Robust-Learning-Based-Backstepping-Control-Aided-with-Neurodynamics-for-Consensus-Formation-Tracking-of-Underwater-Vessels" class="headerlink" title="Distributed Robust Learning-Based Backstepping Control Aided with Neurodynamics for Consensus Formation Tracking of Underwater Vessels"></a>Distributed Robust Learning-Based Backstepping Control Aided with Neurodynamics for Consensus Formation Tracking of Underwater Vessels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09320">http://arxiv.org/abs/2308.09320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Yan, Zhe Xu, Simon X. Yang</li>
<li>for: 本 paper 旨在提出一种分布式Robust学习控制方法，用于多艘海上 vessel 的聚合成本 Tracking 问题，系统参数被完全 unknown 并且受到模型匹配错误、海洋干扰和噪声的影响。</li>
<li>methods: 本 paper 使用 graph theory  synthesize 分布式控制器，并提供了稳定性保证。由于参数不确定性仅出现在船舶动态模型中，因此使用了 backstepping control technique。然后，为了解决时变不确定系统的问题，提出了一种在线学习程序。此外，模型误差、环境干扰和测量噪声也被考虑并解决。</li>
<li>results: 本 paper 提出的分布式控制协议，可以在面对模型误差、海洋干扰和测量噪声等问题下，实现稳定的聚合成本 Tracking。并通过了大量的simulation experiment 来验证其效果。<details>
<summary>Abstract</summary>
This paper addresses distributed robust learning-based control for consensus formation tracking of multiple underwater vessels, in which the system parameters of the marine vessels are assumed to be entirely unknown and subject to the modeling mismatch, oceanic disturbances, and noises. Towards this end, graph theory is used to allow us to synthesize the distributed controller with a stability guarantee. Due to the fact that the parameter uncertainties only arise in the vessels' dynamic model, the backstepping control technique is then employed. Subsequently, to overcome the difficulties in handling time-varying and unknown systems, an online learning procedure is developed in the proposed distributed formation control protocol. Moreover, modeling errors, environmental disturbances, and measurement noises are considered and tackled by introducing a neurodynamics model in the controller design to obtain a robust solution. Then, the stability analysis of the overall closed-loop system under the proposed scheme is provided to ensure the robust adaptive performance at the theoretical level. Finally, extensive simulation experiments are conducted to further verify the efficacy of the presented distributed control protocol.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文关于多艘海上船舶的协同追踪共轨，即使系统参数完全不确定，并且受到模型误差、海洋干扰和噪声的影响。为此，我们使用图论来确保稳定性，并使用backstepping控制技术来处理时间变化和不确定系统。此外，我们还引入了神经动力学模型来处理模型误差、环境干扰和测量噪声。最后，我们提供了对整个关闭环系统的稳定分析，以确保在理论上的稳定适应性。此外，我们还进行了详细的仿真实验来验证我们的分布式控制协议的有效性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis"><a href="#Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis" class="headerlink" title="Towards Attack-tolerant Federated Learning via Critical Parameter Analysis"></a>Towards Attack-tolerant Federated Learning via Critical Parameter Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09318">http://arxiv.org/abs/2308.09318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sungwon-han/fedcpa">https://github.com/sungwon-han/fedcpa</a></li>
<li>paper_authors: Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha</li>
<li>for: 本研究旨在防御 federated learning 系统中的毒 poisoning 攻击，使用 Critical Parameter Analysis (FedCPA) 方法。</li>
<li>methods: 本研究使用了针对本地模型中权重参数的敏感分析，并提出了一种新的攻击忍受策略。</li>
<li>results: 实验结果表明，我们的模型在不同的攻击场景下，能够比现有的防御策略更高效地防御 poisoning 攻击。<details>
<summary>Abstract</summary>
Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffective under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Critical Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种用于共享模型的分布式训练方式，无需客户端共享私人数据。但是， federated learning 系统容易受到毒品攻击，当恶意客户端将false更新发送到中央服务器。现有的防御策略在非标一分布数据设置下无效。这篇论文提出了一种新的防御策略，即 FedCPA（ federated learning with Critical Parameter Analysis）。我们的攻击忍受聚合方法基于本地模型的涵义参数 sets 的观察，即善意本地模型的 top-k 和 bottom-k 涵义参数集相似，而毒品本地模型不同。在多个数据集上对不同的攻击场景进行了实验，我们的模型在防御毒品攻击方面表现出excel，比现有的防御策略更高效。
</details></li>
</ul>
<hr>
<h2 id="Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms"><a href="#Robust-Audio-Anti-Spoofing-with-Fusion-Reconstruction-Learning-on-Multi-Order-Spectrograms" class="headerlink" title="Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms"></a>Robust Audio Anti-Spoofing with Fusion-Reconstruction Learning on Multi-Order Spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09302">http://arxiv.org/abs/2308.09302</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ph-w2000/s2pecnet">https://github.com/ph-w2000/s2pecnet</a></li>
<li>paper_authors: Penghui Wen, Kun Hu, Wenxi Yue, Sen Zhang, Wanlei Zhou, Zhiyong Wang</li>
<li>for: 防止伪造语音（audio deepfake）攻击</li>
<li>methods: 使用多频率特征融合重建策略（S2pecNet），包括来自不同频率的spectral pattern的融合，以提高对不同伪造攻击的抗伪声识别表现。</li>
<li>results: 在ASVspoof2019 LA Challenge上取得了顶尖表现，EER为0.77%。<details>
<summary>Abstract</summary>
Robust audio anti-spoofing has been increasingly challenging due to the recent advancements on deepfake techniques. While spectrograms have demonstrated their capability for anti-spoofing, complementary information presented in multi-order spectral patterns have not been well explored, which limits their effectiveness for varying spoofing attacks. Therefore, we propose a novel deep learning method with a spectral fusion-reconstruction strategy, namely S2pecNet, to utilise multi-order spectral patterns for robust audio anti-spoofing representations. Specifically, spectral patterns up to second-order are fused in a coarse-to-fine manner and two branches are designed for the fine-level fusion from the spectral and temporal contexts. A reconstruction from the fused representation to the input spectrograms further reduces the potential fused information loss. Our method achieved the state-of-the-art performance with an EER of 0.77% on a widely used dataset: ASVspoof2019 LA Challenge.
</details>
<details>
<summary>摘要</summary>
受深圳技术的进步影响，Robust audio anti-spoofing已经变得越来越困难。虽然spectrograms表明其可以用于anti-spoofing，但是多个 spectral pattern的信息尚未得到了充分利用，这限制了它们在不同的 spoofing 攻击下的效iveness。因此，我们提出了一种基于深度学习的新方法，即S2pecNet，以利用多个 spectral pattern来获得Robust audio anti-spoofing表示。特别是，在第二顺序的spectral pattern中进行了混合，并在粗糙到细腻的manner中设置了两个分支来从spectral和temporal上下文中进行细粒度的拼接。从拼接后的表示重建回到输入spectrograms，可以避免潜在的混合信息损失。我们的方法在ASVspoof2019 LA Challenge上达到了状态级的表现，EER为0.77%。
</details></li>
</ul>
<hr>
<h2 id="V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models"><a href="#V2A-Mapper-A-Lightweight-Solution-for-Vision-to-Audio-Generation-by-Connecting-Foundation-Models" class="headerlink" title="V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models"></a>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09300">http://arxiv.org/abs/2308.09300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, Weidong Cai<br>for:This paper focuses on the problem of generating semantically-relevant sound from visual input, specifically using foundation models (FMs) to bridge the domain gap between visual and auditory modalities.methods:The proposed method uses a simple yet effective mapper mechanism (V2A-Mapper) to translate the visual input between the CLIP and CLAP spaces, and then uses pretrained audio generative FM AudioLDM to produce high-fidelity and visually-aligned sound.results:Compared to previous approaches, the proposed method achieves superior performance in both objective and subjective evaluations, with 53% and 19% improvement in fidelity and relevance, respectively, using 86% fewer parameters.<details>
<summary>Abstract</summary>
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:建立基于基础模型（FM）的人工智能系统是当前AI研究中的新方案。这些基础模型通过大量数据学习的表示和生成能力，可以方便地适应多种下游任务，无需从头开始重新训练。然而，在音频模式中使用FM进行跨模态生成仍然受到了较少的研究。在这篇论文中，我们提出了一种轻量级的解决方案，利用CLIP、CLAP和AudioLDM这些基础模型。我们首先调查了视觉CLIP和听音CLAP模型之间的领域差距。然后，我们提出了一种简单 yet有效的映射机制（V2A-Mapper），用于跨模态的映射。 conditioned on the translated CLAP embedding,我们采用了预训练的听音生成FM AudioLDM，以生成具有高准确性和视觉对齐的声音。相比之前的方法，我们的方法只需快速训练V2A-Mapper。我们进一步分析和进行了大量实验，研究V2A-Mapper的选择问题，并发现生成映射在FD中性能提高53%，CS中性能提高19%。两个V2A数据集的对象和主观评估都表明了我们提出的方法在当前状态方法中的超越性，训练参数减少86%，FD和CS中性能提高53%和19%，分别。
</details></li>
</ul>
<hr>
<h2 id="How-important-are-specialized-transforms-in-Neural-Operators"><a href="#How-important-are-specialized-transforms-in-Neural-Operators" class="headerlink" title="How important are specialized transforms in Neural Operators?"></a>How important are specialized transforms in Neural Operators?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09293">http://arxiv.org/abs/2308.09293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ritam-M/LearnableTransformsNO">https://github.com/Ritam-M/LearnableTransformsNO</a></li>
<li>paper_authors: Ritam Majumdar, Shirish Karande, Lovekesh Vig</li>
<li>for: 本研究旨在探讨transform-based neural operators中转换层的重要性，以及其对 solving partial differential equations (PDEs) 的影响。</li>
<li>methods: 本研究使用了一种简单的方法，即将所有的转换层替换为学习型线性层，以评估其影响于性能和计算时间。</li>
<li>results: 研究发现，使用学习型线性层可以提供与最佳转换层相当的性能，并且在计算时间方面也有一定的优势。这种观察可能对未来关于Neural Operators的研究有着重要的影响。<details>
<summary>Abstract</summary>
Simulating physical systems using Partial Differential Equations (PDEs) has become an indispensible part of modern industrial process optimization. Traditionally, numerical solvers have been used to solve the associated PDEs, however recently Transform-based Neural Operators such as the Fourier Neural Operator and Wavelet Neural Operator have received a lot of attention for their potential to provide fast solutions for systems of PDEs. In this work, we investigate the importance of the transform layers to the reported success of transform based neural operators. In particular, we record the cost in terms of performance, if all the transform layers are replaced by learnable linear layers. Surprisingly, we observe that linear layers suffice to provide performance comparable to the best-known transform-based layers and seem to do so with a compute time advantage as well. We believe that this observation can have significant implications for future work on Neural Operators, and might point to other sources of efficiencies for these architectures.
</details>
<details>
<summary>摘要</summary>
使用分数方程（PDEs）模拟物理系统已成为现代工业过程优化的不可或缺的一部分。传统上，数值解法被用来解决相关的PDEs，但最近，基于变换的神经操作符，如傅里叶 neural operator和wavelet neural operator，在提供快速解决系统的PDEs方面受到了广泛关注。在这项工作中，我们研究了变换层的重要性，特别是替换所有变换层为学习的线性层后的成本。我们发现了一个Counterintuitive的现象：所有变换层被替换后，使用学习的线性层可以提供与最佳变换层相当的性能，并且在计算时间方面也有一定的优势。我们认为这一观察可能对未来神经操作符的研究产生重要的影响，并可能暴露出其他优化的来源。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Alignment-and-Uniformity-for-Recommendation"><a href="#Graph-based-Alignment-and-Uniformity-for-Recommendation" class="headerlink" title="Graph-based Alignment and Uniformity for Recommendation"></a>Graph-based Alignment and Uniformity for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09292">http://arxiv.org/abs/2308.09292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangliangwei/graphau">https://github.com/yangliangwei/graphau</a></li>
<li>paper_authors: Liangwei Yang, Zhiwei Liu, Chen Wang, Mingdai Yang, Xiaolong Liu, Jing Ma, Philip S. Yu</li>
<li>for: addressing the sparsity issue in collaborative filtering-based recommender systems (RecSys)</li>
<li>methods:  proposes a novel approach called graph-based alignment and uniformity (GraphAU), which explicitly considers high-order connectivities in the user-item bipartite graph</li>
<li>results: significantly alleviates the sparsity issue and achieves state-of-the-art performance on four datasets, with the open-source code available at <a target="_blank" rel="noopener" href="https://github.com/YangLiangwei/GraphAU.Here's">https://github.com/YangLiangwei/GraphAU.Here&#39;s</a> the full Chinese text:</li>
<li>for: 本研究旨在解决collaborative filtering-based recommender systems (RecSys)中的缺乏问题。</li>
<li>methods: 提出了一种新的方法，即图基于对齐和均匀性(GraphAU)，该方法直接考虑用户-物品 биipartite图中的高阶连接度。</li>
<li>results: 在四个数据集上，GraphAU有效地解决了缺乏问题，并达到了当前最佳性能。代码可以在<a target="_blank" rel="noopener" href="https://github.com/YangLiangwei/GraphAU%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/YangLiangwei/GraphAU中下载。</a><details>
<summary>Abstract</summary>
Collaborative filtering-based recommender systems (RecSys) rely on learning representations for users and items to predict preferences accurately. Representation learning on the hypersphere is a promising approach due to its desirable properties, such as alignment and uniformity. However, the sparsity issue arises when it encounters RecSys. To address this issue, we propose a novel approach, graph-based alignment and uniformity (GraphAU), that explicitly considers high-order connectivities in the user-item bipartite graph. GraphAU aligns the user/item embedding to the dense vector representations of high-order neighbors using a neighborhood aggregator, eliminating the need to compute the burdensome alignment to high-order neighborhoods individually. To address the discrepancy in alignment losses, GraphAU includes a layer-wise alignment pooling module to integrate alignment losses layer-wise. Experiments on four datasets show that GraphAU significantly alleviates the sparsity issue and achieves state-of-the-art performance. We open-source GraphAU at https://github.com/YangLiangwei/GraphAU.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HyperLoRA-for-PDEs"><a href="#HyperLoRA-for-PDEs" class="headerlink" title="HyperLoRA for PDEs"></a>HyperLoRA for PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09290">http://arxiv.org/abs/2308.09290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritam Majumdar, Vishal Jadhav, Anirudh Deodhar, Shirish Karande, Lovekesh Vig, Venkataramana Runkana</li>
<li>for: 用于解决参数化部分 diferencial equations 的 neural surrogates 问题</li>
<li>methods: 使用 Hypernetwork 和 low-ranked adaptation (LoRA) 技术，将每层基本网络转化为低维度的tensor，并使用 hypernetworks 预测这些tensor的参数</li>
<li>results: 通过添加物理信息损失函数 HyperPINN 进行训练，可以快速学习参数化部分 differential equations 的解，如布格尔方程和奈尔-斯托克斯-科瓦斯纳流体动力学方程，并在参数量方面实现8倍减少，而不会影响准确性。<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been widely used to develop neural surrogates for solutions of Partial Differential Equations. A drawback of PINNs is that they have to be retrained with every change in initial-boundary conditions and PDE coefficients. The Hypernetwork, a model-based meta learning technique, takes in a parameterized task embedding as input and predicts the weights of PINN as output. Predicting weights of a neural network however, is a high-dimensional regression problem, and hypernetworks perform sub-optimally while predicting parameters for large base networks. To circumvent this issue, we use a low ranked adaptation (LoRA) formulation to decompose every layer of the base network into low-ranked tensors and use hypernetworks to predict the low-ranked tensors. Despite the reduced dimensionality of the resulting weight-regression problem, LoRA-based Hypernetworks violate the underlying physics of the given task. We demonstrate that the generalization capabilities of LoRA-based hypernetworks drastically improve when trained with an additional physics-informed loss component (HyperPINN) to satisfy the governing differential equations. We observe that LoRA-based HyperPINN training allows us to learn fast solutions for parameterized PDEs like Burger's equation and Navier Stokes: Kovasznay flow, while having an 8x reduction in prediction parameters on average without compromising on accuracy when compared to all other baselines.
</details>
<details>
<summary>摘要</summary>
физи学信息泛化神经网络（PINNs）已广泛应用于解决部分梯度方程的解的神经替代模型。 PINNs 的缺点是它们需要每次更改初始边界条件和微分方程系数时重新训练。 神经网络模型基于元学习技术（Hypernetwork）可以将任务嵌入作为输入，预测 PINN 的权重。 但是，预测神经网络权重是一个高维度回归问题，神经网络模型在预测基础网络参数时表现不佳。 为了解决这个问题，我们使用 low-rank adaptation（LoRA）形式划分每层基础网络中的每个层成低维度的矩阵，并使用神经网络预测这些低维度矩阵。 尽管LoRA-based Hypernetworks 的维度减少了，但是它们仍然不符合给定任务的物理学。 我们表明，通过在 LoRA-based HyperPINN 训练中添加物理学信息泛化损失函数，可以提高 LoRA-based HyperPINN 的泛化能力。 我们观察到，LoRA-based HyperPINN 训练可以快速地解决参数化的微分方程，如布尔格方程和奈尔-斯托克斯：科瓦兹纳流，而且在 average 上降低预测参数的数量约 8 倍，不会降低准确性。
</details></li>
</ul>
<hr>
<h2 id="Preference-conditioned-Pixel-based-AI-Agent-For-Game-Testing"><a href="#Preference-conditioned-Pixel-based-AI-Agent-For-Game-Testing" class="headerlink" title="Preference-conditioned Pixel-based AI Agent For Game Testing"></a>Preference-conditioned Pixel-based AI Agent For Game Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09289">http://arxiv.org/abs/2308.09289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Adrian Brown, Pushi Zhang<br>for: This paper aims to improve game testing AI agents’ ability to explore and test games with high quality and efficiency, addressing the limitations of current methods that rely on game state information and lack explicit control over exploration style.methods: The proposed agent design uses pixel-based state observations and imitation learning with self-supervised and supervised learning objectives to improve exploration coverage and test execution quality.results: The proposed agent significantly outperforms state-of-the-art pixel-based game testing agents in exploration coverage and test execution quality when evaluated on a complex open-world environment resembling many aspects of real AAA games.<details>
<summary>Abstract</summary>
The game industry is challenged to cope with increasing growth in demand and game complexity while maintaining acceptable quality standards for released games. Classic approaches solely depending on human efforts for quality assurance and game testing do not scale effectively in terms of time and cost. Game-testing AI agents that learn by interaction with the environment have the potential to mitigate these challenges with good scalability properties on time and costs. However, most recent work in this direction depends on game state information for the agent's state representation, which limits generalization across different game scenarios. Moreover, game test engineers usually prefer exploring a game in a specific style, such as exploring the golden path. However, current game testing AI agents do not provide an explicit way to satisfy such a preference. This paper addresses these limitations by proposing an agent design that mainly depends on pixel-based state observations while exploring the environment conditioned on a user's preference specified by demonstration trajectories. In addition, we propose an imitation learning method that couples self-supervised and supervised learning objectives to enhance the quality of imitation behaviors. Our agent significantly outperforms state-of-the-art pixel-based game testing agents over exploration coverage and test execution quality when evaluated on a complex open-world environment resembling many aspects of real AAA games.
</details>
<details>
<summary>摘要</summary>
游戏产业面临增长的需求和游戏复杂度的挑战，同时保持适当的质量标准 для发布的游戏。经典的方法仅凭靠人工努力来确保质量检测和游戏测试，效率不足。基于环境互动学习的游戏测试AI代理具有良好的可扩展性，但大多数最新的研究仅基于游戏状态信息来表示代理的状态，导致泛化性不足。此外，游戏测试工程师通常会按照某种特定的风格来探索游戏，如探索 golden path。但现有的游戏测试AI代理没有直接提供这种偏好的实现方式。本文解决了这些限制，提出一种基于像素状态观察的代理设计，同时采用用户 preference 提供的示范轨迹来conditioned 环境探索。此外，我们提出一种imiter learning方法，将自动学习和监督学习目标相结合，以提高模仿行为质量。我们的代理在一个复杂的开放世界环境中表现出色，与现有的像素基于游戏测试代理相比，在探索覆盖率和测试执行质量方面显著超越。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Reasoning-Capabilities-of-Large-Language-Models-A-Graph-Based-Verification-Approach"><a href="#Enhancing-Reasoning-Capabilities-of-Large-Language-Models-A-Graph-Based-Verification-Approach" class="headerlink" title="Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach"></a>Enhancing Reasoning Capabilities of Large Language Models: A Graph-Based Verification Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09267">http://arxiv.org/abs/2308.09267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lang Cao</li>
<li>for: 这个论文主要是为了提高大语言模型（LLM）的理解能力，尤其是在复杂的理解任务中，如数学问题。</li>
<li>methods: 这个论文使用了一种图像基的方法来加强LLM的理解能力，具体来说是通过将多种解决方案转化为一个图像，以便对这些解决方案进行分析和验证。</li>
<li>results: 实验结果表明，这种图像基的验证方法不仅可以显著提高LLM的理解能力，而且也可以超过现有的验证方法在提高这些模型的理解性能方面。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph Verifier (RGV) to analyze and verify the solutions generated by LLMs. By evaluating these graphs, models can yield more accurate and reliable results.Our experimental results show that our graph-based verification method not only significantly enhances the reasoning abilities of LLMs but also outperforms existing verifier methods in terms of improving these models' reasoning performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Point-Contrastive-Prediction-with-Semantic-Clustering-for-Self-Supervised-Learning-on-Point-Cloud-Videos"><a href="#Point-Contrastive-Prediction-with-Semantic-Clustering-for-Self-Supervised-Learning-on-Point-Cloud-Videos" class="headerlink" title="Point Contrastive Prediction with Semantic Clustering for Self-Supervised Learning on Point Cloud Videos"></a>Point Contrastive Prediction with Semantic Clustering for Self-Supervised Learning on Point Cloud Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09247">http://arxiv.org/abs/2308.09247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Sheng, Zhiqiang Shen, Gang Xiao, Longguang Wang, Yulan Guo, Hehe Fan</li>
<li>for: 本文提出了一种点云视频自主学习框架，用于对象和场景数据的表示学习。previous方法通常在clip或帧层进行表示学习，但这些表示往往无法捕捉细腻的 semantics。</li>
<li>methods: 本文提出了一种点云自主学习框架，通过对点进行对比学习来 capture 细腻 semantics。此外，我们还引入了一种新的预测任务，即实现 superpoints 的Semantic alignment，以便更好地捕捉多尺度的 semantics。</li>
<li>results: 实验表明，我们的方法可以比超过supervised counterparts 的表示学习方法在多种下游任务上表现出色，并且表明 learned 表示的转移性。<details>
<summary>Abstract</summary>
We propose a unified point cloud video self-supervised learning framework for object-centric and scene-centric data. Previous methods commonly conduct representation learning at the clip or frame level and cannot well capture fine-grained semantics. Instead of contrasting the representations of clips or frames, in this paper, we propose a unified self-supervised framework by conducting contrastive learning at the point level. Moreover, we introduce a new pretext task by achieving semantic alignment of superpoints, which further facilitates the representations to capture semantic cues at multiple scales. In addition, due to the high redundancy in the temporal dimension of dynamic point clouds, directly conducting contrastive learning at the point level usually leads to massive undesired negatives and insufficient modeling of positive representations. To remedy this, we propose a selection strategy to retain proper negatives and make use of high-similarity samples from other instances as positive supplements. Extensive experiments show that our method outperforms supervised counterparts on a wide range of downstream tasks and demonstrates the superior transferability of the learned representations.
</details>
<details>
<summary>摘要</summary>
我们提出了一种综合点云视频自动学习框架，用于物体和场景数据的 Representation Learning。过去的方法通常在clip或帧级进行 Representation Learning，但这些方法难以捕捉细腻的 semantics。相反，在这篇论文中，我们提出了一种综合自我超vised学习框架，通过在点级进行对比学习。此外，我们还引入了一种新的预text任务，即实现superpoint的semantic aligning，以便 representations能够捕捉多个尺度的semanticcue。此外，由于点云动态数据中的时间维度具有高的重复率，直接在点级进行对比学习通常会导致庞大的undesired negatives和Positive representations的不足。为了解决这个问题，我们提出了一种选择策略，以保留正确的负例和利用其他实例中的高相似性样本作为Positive complement。广泛的实验表明，我们的方法在多种下游任务上表现出优于supervised counterpart，并且demonstrates the superior transferability of the learned representations。
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments"><a href="#A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments" class="headerlink" title="A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments"></a>A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09734">http://arxiv.org/abs/2308.09734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Kasmarik, Jiankun Hu</li>
<li>for: 这个论文主要旨在解决多目标Markov决策过程中的多目标优化问题，这种问题涉及到随机过程的Sequential决策，同时满足Markov性的约束。</li>
<li>methods: 这篇论文使用了多目标激励学习方法，它将激励学习概念与多目标优化技术相结合，但这些方法具有缺乏适应非站点环境的缺点。</li>
<li>results: 该论文提出了一种发展优化方法，可以在在线环境中不断演化政策覆盖集，同时在定义的目标空间中探索Preferencespace。该算法在非站点环境中表现出了明显的优势，并在站点环境中达到了相对的比较Result。<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments.
</details>
<details>
<summary>摘要</summary>
本文提出了一种发展优化方法，可以在在线模式下，在定义的目标空间中探索 preference space，而不是采用优化过程。我们提出了一种新的多目标激励学习算法，可以在非站ARY环境中稳定地演化一个凸包含策略的覆盖集。我们与两种现有的多目标激励学习算法进行比较，Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments, while achieving comparable results in stationary environments.
</details></li>
</ul>
<hr>
<h2 id="Masked-Spatio-Temporal-Structure-Prediction-for-Self-supervised-Learning-on-Point-Cloud-Videos"><a href="#Masked-Spatio-Temporal-Structure-Prediction-for-Self-supervised-Learning-on-Point-Cloud-Videos" class="headerlink" title="Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos"></a>Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09245">http://arxiv.org/abs/2308.09245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/johnsonsign/mast-pre">https://github.com/johnsonsign/mast-pre</a></li>
<li>paper_authors: Zhiqiang Shen, Xiaoxiao Sheng, Hehe Fan, Longguang Wang, Yulan Guo, Qiong Liu, Hao Wen, Xi Zhou</li>
<li>for: 本研究旨在提出一种无需人工标注的点云视频理解方法，以捕捉点云视频中的空间temporal结构。</li>
<li>methods: 该方法基于点云ube掩码和两种自我supervised学习任务。首先，通过重建掩码后的点云，方法可以捕捉点云视频的外观信息。其次，为了学习运动，我们提出了一个时间卡达度差预测任务，该任务估算点云ube中点的变化。</li>
<li>results: 对于MSRAction-3D、NTU-RGBD、NvGesture和SHREC’17等数据集，我们进行了广泛的实验，并证明了提议的方法的有效性。<details>
<summary>Abstract</summary>
Recently, the community has made tremendous progress in developing effective methods for point cloud video understanding that learn from massive amounts of labeled data. However, annotating point cloud videos is usually notoriously expensive. Moreover, training via one or only a few traditional tasks (e.g., classification) may be insufficient to learn subtle details of the spatio-temporal structure existing in point cloud videos. In this paper, we propose a Masked Spatio-Temporal Structure Prediction (MaST-Pre) method to capture the structure of point cloud videos without human annotations. MaST-Pre is based on spatio-temporal point-tube masking and consists of two self-supervised learning tasks. First, by reconstructing masked point tubes, our method is able to capture the appearance information of point cloud videos. Second, to learn motion, we propose a temporal cardinality difference prediction task that estimates the change in the number of points within a point tube. In this way, MaST-Pre is forced to model the spatial and temporal structure in point cloud videos. Extensive experiments on MSRAction-3D, NTU-RGBD, NvGesture, and SHREC'17 demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)最近，社区在开发有效的点云视频理解方法上做出了很大的进步，通过大量标注数据学习。然而，标注点云视频通常非常昂贵。此外，通过一些传统任务（例如分类）训练可能不足以学习点云视频中细腻的空间-时间结构。在这篇论文中，我们提出了一种Masked Spatio-Temporal Structure Prediction（MaST-Pre）方法，可以无需人工标注，捕捉点云视频的结构。MaST-Pre基于空间-时间点管道遮盲，包括两个自我超vised学习任务。首先，通过重建遮盲点管道，我们的方法可以捕捉点云视频的外观信息。其次，为了学习运动，我们提出了一个时间 cardinality difference prediction任务，可以估计点管道中点的变化。这样，MaST-Pre是被迫模型点云视频的空间和时间结构。我们在 MSRAction-3D、NTU-RGBD、NvGesture 和 SHREC'17 上进行了广泛的实验， demonstarted 提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes"><a href="#Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes" class="headerlink" title="Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes"></a>Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09733">http://arxiv.org/abs/2308.09733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Merrick, Jiankun Hu</li>
<li>for:  solve multi-objective Markov decision processes in non-stationary environments</li>
<li>methods:  intrinsically motivated reinforcement learning with dual-phase learning</li>
<li>results:  significantly outperforms state-of-the-art multi-objective reinforcement methods in a dynamic robotics environmentHere’s the simplified Chinese text:</li>
<li>for: 解决多目标Markov决策过程中的非站点环境</li>
<li>methods: 使用内在动机导向的强化学习方法，包括双期学习</li>
<li>results: 在动力环境中显著超越了现有多目标强化方法的性能I hope this helps!<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning process. In this work, intrinsically motivated reinforcement learning has been successfully deployed to evolve generic skill sets for learning hierarchical policies to solve multi-objective Markov decision processes. We propose a novel dual-phase intrinsically motivated reinforcement learning method to address this limitation. In the first phase, a generic set of skills is learned. While in the second phase, this set is used to bootstrap policy coverage sets for each shift in the environment dynamics. We show experimentally that the proposed method significantly outperforms state-of-the-art multi-objective reinforcement methods in a dynamic robotics environment.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:多目标Markov决策过程是一种sequential decision-making问题，其中存在多个矛盾的奖励函数，无法同时优化。这类问题不可以通过单一的优化策略来解决，与传统情况不同。相反，多目标学习方法会演化一个coverage集的优化策略，以满足所有可能的偏好。然而，许多这些方法无法泛化其coverage集，以适应非站ARY environments。在这些环境中，状态转移和奖励分布的参数变化过时。这限制了演化出来的策略集的性能。为了突破这些限制，需要学习一个通用技能集，可以启动演化策略集的扩展。在这种情况下，我们提出了一种新的双相启动多目标学习方法。在第一阶段，学习一个通用技能集。在第二阶段，使用这个集来扩展策略集 для每个环境动态变化。我们实验表明，我们提出的方法在动态 роботикс环境中明显超过了当前多目标学习方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Digital-Twin-Oriented-Complex-Networked-Systems-based-on-Heterogeneous-node-features-and-interaction-rules"><a href="#Digital-Twin-Oriented-Complex-Networked-Systems-based-on-Heterogeneous-node-features-and-interaction-rules" class="headerlink" title="Digital Twin-Oriented Complex Networked Systems based on Heterogeneous node features and interaction rules"></a>Digital Twin-Oriented Complex Networked Systems based on Heterogeneous node features and interaction rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11034">http://arxiv.org/abs/2308.11034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Wen, Bogdan Gabrys, Katarzyna Musial<br>for: This paper proposes a modelling framework for Digital Twin-Oriented Complex Networked Systems (DT-CNSs) to generate networks that faithfully represent real systems.methods: The modelling process focuses on features of nodes and interaction rules for creating connections based on individual node preferences.results: The paper presents a case study on disaster resilience of social networks during an epidemic outbreak, showing how different levels of structural and dynamics complexities influence network growth and epidemic spread. The analysis reveals that mitigation policies should target nodes with preferred features as they have higher infection risks and should be the focus of epidemic control.Here is the same information in Simplified Chinese text:for: 这篇论文提出了一种用于数字双体复杂网络系统（DT-CNS）的扩展模型 Frameworks，以生成具有真实系统特点的网络。methods: 模型过程关注节点特点和连接规则的互动，以建立基于个体节点偏好的连接。results: 论文通过一个案例研究，探讨在社交网络上疫情爆发时的灾害抵御能力，发现不同水平的结构和动态复杂性对网络增长和疫情传播产生了影响。分析表明，控制疫情时应该向具有偏好特点的节点进行抗疫措施，以降低疫情传播风险。<details>
<summary>Abstract</summary>
This study proposes an extendable modelling framework for Digital Twin-Oriented Complex Networked Systems (DT-CNSs) with a goal of generating networks that faithfully represent real systems. Modelling process focuses on (i) features of nodes and (ii) interaction rules for creating connections that are built based on individual node's preferences. We conduct experiments on simulation-based DT-CNSs that incorporate various features and rules about network growth and different transmissibilities related to an epidemic spread on these networks. We present a case study on disaster resilience of social networks given an epidemic outbreak by investigating the infection occurrence within specific time and social distance. The experimental results show how different levels of the structural and dynamics complexities, concerned with feature diversity and flexibility of interaction rules respectively, influence network growth and epidemic spread. The analysis revealed that, to achieve maximum disaster resilience, mitigation policies should be targeted at nodes with preferred features as they have higher infection risks and should be the focus of the epidemic control.
</details>
<details>
<summary>摘要</summary>
The case study on disaster resilience of social networks during an epidemic outbreak shows how different levels of structural and dynamics complexities, such as feature diversity and flexibility of interaction rules, influence network growth and epidemic spread. The analysis reveals that mitigation policies should target nodes with preferred features as they have a higher risk of infection and should be the focus of epidemic control.Translation in Simplified Chinese:这个研究提出了一个可扩展的模型框架，用于数字双方向复杂网络系统（DT-CNS），以创建准确表示实际系统的网络。模型 процесс关注节点特性以及建立连接的交互规则，这些规则基于个节点的偏好。研究通过在模拟基础上进行实验，检验不同特性和规则对网络增长和疾病传播的影响。研究中的案例研究探讨了在疾病爆发时社交网络的难以恢复性，通过对特定时间和社交距离内的感染发生进行调查。实验结果表明，不同的结构和动态复杂性水平对网络增长和疾病传播产生不同的影响。分析表明，为了实现最大的灾害抵御能力，应该通过针对具有偏好特性的节点进行疫苗控制，以降低这些节点的感染风险。
</details></li>
</ul>
<hr>
<h2 id="Improving-Buoy-Detection-with-Deep-Transfer-Learning-for-Mussel-Farm-Automation"><a href="#Improving-Buoy-Detection-with-Deep-Transfer-Learning-for-Mussel-Farm-Automation" class="headerlink" title="Improving Buoy Detection with Deep Transfer Learning for Mussel Farm Automation"></a>Improving Buoy Detection with Deep Transfer Learning for Mussel Farm Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09238">http://arxiv.org/abs/2308.09238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carl McMillan, Junhong Zhao, Bing Xue, Ross Vennell, Mengjie Zhang</li>
<li>for: 提高 mussel farm 运营效率和管理 accuracy 和鲜明度</li>
<li>methods: 使用人工智能和计算机视觉技术，包括深度学习方法和对象检测</li>
<li>results: 通过适应转移学习和数据多样性，实现了对浮标的检测性能的显著提高，并且在不同的天气和照明条件下具有良好的一致性和鲜明度<details>
<summary>Abstract</summary>
The aquaculture sector in New Zealand is experiencing rapid expansion, with a particular emphasis on mussel exports. As the demands of mussel farming operations continue to evolve, the integration of artificial intelligence and computer vision techniques, such as intelligent object detection, is emerging as an effective approach to enhance operational efficiency. This study delves into advancing buoy detection by leveraging deep learning methodologies for intelligent mussel farm monitoring and management. The primary objective centers on improving accuracy and robustness in detecting buoys across a spectrum of real-world scenarios. A diverse dataset sourced from mussel farms is captured and labeled for training, encompassing imagery taken from cameras mounted on both floating platforms and traversing vessels, capturing various lighting and weather conditions. To establish an effective deep learning model for buoy detection with a limited number of labeled data, we employ transfer learning techniques. This involves adapting a pre-trained object detection model to create a specialized deep learning buoy detection model. We explore different pre-trained models, including YOLO and its variants, alongside data diversity to investigate their effects on model performance. Our investigation demonstrates a significant enhancement in buoy detection performance through deep learning, accompanied by improved generalization across diverse weather conditions, highlighting the practical effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
新西兰aquaculture业正在迅速扩张，特别是对贝壳出口有着强烈的注意力。随着贝壳养殖操作的需求不断演化，人工智能和计算机视觉技术的应用正在成为提高操作效率的有效方法。本研究探讨了基于深度学习方法的智能贝壳园监测和管理，以提高贝壳园的准确性和鲁棒性。我们使用了多种预训练模型，包括YOLO和其变体，以及不同的数据多样性来研究它们对模型性能的影响。我们的调查显示，通过深度学习实现了贝壳检测的显著提高，同时在多种天气条件下也有良好的泛化性，这说明了我们的方法的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion"><a href="#Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion" class="headerlink" title="Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion"></a>Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11720">http://arxiv.org/abs/2308.11720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yerong Li, Roxana Girju</li>
<li>for: 提高relation classification的准确率和降低相似类异类的混淆</li>
<li>methods:  integrate representative examples and through co-set expansion, context-free Hearst patterns, and contrastive examples tuning</li>
<li>results: 实验结果表明提出的方法可以显著提高relation extraction的准确率，同时降低相似类异类的混淆<details>
<summary>Abstract</summary>
Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.   Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.   Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a significant enhancement of relation classification performance. Our method achieves an observed margin of at least 1 percent improvement in accuracy in most settings, on top of existing fine-tuning approaches. To further refine our approach, we conduct an in-depth analysis that focuses on tuning contrastive examples. This strategic selection and tuning effectively reduce confusion between classes sharing similarities, leading to a more precise classification process.   Experimental results underscore the effectiveness of our proposed framework for relation extraction. The synergy between co-set expansion and context-aware prompt tuning substantially contributes to improved classification accuracy. Furthermore, the reduction in confusion between contrastive classes through contrastive examples tuning validates the robustness and reliability of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>关系提取（RE）是自动从不结构化文本中提取结构化信息的关键任务。在这篇论文中，我们提出了一种多方面的方法，它将表示例和相似扩展相结合。我们的方法的 PRIMARY GOAL 是提高关系类别的分类精度，并降低对比类别的混淆。我们的方法开始于每个关系类别中的表示例。然后，我们的相似扩展算法将训练目标包含类别之间的相似度。此外，相似扩展过程还包括一个类别排名过程，它考虑了对应类别中的表示例。 Contextual details  surrounding relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.实验结果表明，我们的相似扩展方法有效地提高了关系分类性能。我们的方法在大多数设置下达到了至少1%的提升率，并且在现有的细化方法之上进行了进一步的优化。为了进一步改进我们的方法，我们进行了深入的分析，将对于相似类别的选择和调整作为战略。这种策略性的选择和调整有效地减少了类别之间的混淆，导致更加精准的分类过程。实验结果证明了我们提出的关系提取框架的效iveness。它的同时进行相似扩展和上下文相关的提升，使得分类性能得到了进一步提高。此外，通过对相似类别进行调整，我们的方法的可靠性和可靠性得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm"><a href="#Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm" class="headerlink" title="Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm"></a>Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09732">http://arxiv.org/abs/2308.09732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengshuai Yao</li>
<li>for: 测试和比较离况学习算法的效果</li>
<li>methods: 使用Gradient TD算法和两个时间步骤测量算法</li>
<li>results: 解释TD算法在这个例子中的缓慢问题，并提供了一个可用于研究离况学习算法的 debug 技术，以及实验结果显示 Impression GTD 算法在这个例子中的快速收敛。<details>
<summary>Abstract</summary>
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).   This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in general and a fast convergence rate.
</details>
<details>
<summary>摘要</summary>
白尔德对例（Baird counterexample）于1995年由Leemon Baird提出，用以证明TD(0)算法在这个例子中崩溃。自此以后，它经常用于测试和比较不同的离政学习算法。梯度TD算法解决了TD算法在白尔德对例中的崩溃问题，但它们在这个例子上的 converges 速度非常慢，并且不很了解这种慢速度的性质。例如，参见（Sutton和Barto 2018）。本记录的目的是要更好地理解TD算法在白尔德对例上的慢速度，并提供调试分析来理解这种行为。我们的调试技术可以用来研究两个时间尺度的随机抽象算法的收敛行为。我们还提供了最近的Impression GTD算法在这个例子上的实验结果，显示其 converge 速度非常快，甚至在线性速度。我们结论是，白尔德对例已经被解决，并且有一个可靠的收敛保证和快速 converges 速度。
</details></li>
</ul>
<hr>
<h2 id="Learning-in-Cooperative-Multiagent-Systems-Using-Cognitive-and-Machine-Models"><a href="#Learning-in-Cooperative-Multiagent-Systems-Using-Cognitive-and-Machine-Models" class="headerlink" title="Learning in Cooperative Multiagent Systems Using Cognitive and Machine Models"></a>Learning in Cooperative Multiagent Systems Using Cognitive and Machine Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09219">http://arxiv.org/abs/2308.09219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddm-lab/greedy-hysteretic-lenient-maibl">https://github.com/ddm-lab/greedy-hysteretic-lenient-maibl</a></li>
<li>paper_authors: Thuy Ngoc Nguyen, Duy Nhat Phan, Cleotilde Gonzalez</li>
<li>for: 本研究旨在开发有效的多智能体系统（MAS），以满足协作和协调人类的多种应用。</li>
<li>methods: 本研究提出三种变体的多智能IBLT模型（MAIBL），结合了IBLT的认知机制和MADRL模型来处理协调MAS在随机环境中的协同学习。</li>
<li>results: 对于不同的随机奖励设定，MAIBL模型在动态CMOTP任务中表现出比现有MADRL模型更快的学习速度和更好的协调性。<details>
<summary>Abstract</summary>
Developing effective Multi-Agent Systems (MAS) is critical for many applications requiring collaboration and coordination with humans. Despite the rapid advance of Multi-Agent Deep Reinforcement Learning (MADRL) in cooperative MAS, one major challenge is the simultaneous learning and interaction of independent agents in dynamic environments in the presence of stochastic rewards. State-of-the-art MADRL models struggle to perform well in Coordinated Multi-agent Object Transportation Problems (CMOTPs), wherein agents must coordinate with each other and learn from stochastic rewards. In contrast, humans often learn rapidly to adapt to nonstationary environments that require coordination among people. In this paper, motivated by the demonstrated ability of cognitive models based on Instance-Based Learning Theory (IBLT) to capture human decisions in many dynamic decision making tasks, we propose three variants of Multi-Agent IBL models (MAIBL). The idea of these MAIBL algorithms is to combine the cognitive mechanisms of IBLT and the techniques of MADRL models to deal with coordination MAS in stochastic environments from the perspective of independent learners. We demonstrate that the MAIBL models exhibit faster learning and achieve better coordination in a dynamic CMOTP task with various settings of stochastic rewards compared to current MADRL models. We discuss the benefits of integrating cognitive insights into MADRL models.
</details>
<details>
<summary>摘要</summary>
发展有效的多智能体系统（MAS）是许多需要协作和协调的应用中的关键。虽然多智能深度学习（MADRL）在合作MAS中得到了快速的进步，但一个主要挑战是独立的智能体在动态环境中同时学习和互动，并在恒定奖励下学习。现状最先进的MADRL模型在协调多智能对象运输问题（CMOTP）中表现不佳，这种问题需要智能体之间协调和学习。然而，人类在非站台环境中很快地适应和适应非站台环境，并且能够快速地学习。在这篇论文中，我们受到了基于实例学习理论（IBLT）的认知模型的能力所启发，并提出了三种多智能IBLT模型（MAIBL）。这些MAIBL算法的想法是将IBLT的认知机制与MADRL模型的技术相结合，以面对独立学习的协调MAS在随机奖励下的问题。我们示出了MAIBL模型在动态CMOTP任务中与不同设置的随机奖励下显著更快地学习和更好地协调。我们讨论了将认知预测 integrate into MADRL模型的好处。
</details></li>
</ul>
<hr>
<h2 id="GPU-Accelerated-Color-Correction-and-Frame-Warping-for-Real-time-Video-Stitching"><a href="#GPU-Accelerated-Color-Correction-and-Frame-Warping-for-Real-time-Video-Stitching" class="headerlink" title="GPU Accelerated Color Correction and Frame Warping for Real-time Video Stitching"></a>GPU Accelerated Color Correction and Frame Warping for Real-time Video Stitching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09209">http://arxiv.org/abs/2308.09209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lu Yang, Zhenglun Kong, Ting Li, Xinyi Bai, Zhiye Lin, Hong Cheng</li>
<li>for: 实时拼接多个视频序列为全景视频</li>
<li>methods: 基于GPU加速的颜色修正和框架折叠，不需要精确的摄像机参数</li>
<li>results: 实时生成高质量全景视频<details>
<summary>Abstract</summary>
Traditional image stitching focuses on a single panorama frame without considering the spatial-temporal consistency in videos. The straightforward image stitching approach will cause temporal flicking and color inconstancy when it is applied to the video stitching task. Besides, inaccurate camera parameters will cause artifacts in the image warping. In this paper, we propose a real-time system to stitch multiple video sequences into a panoramic video, which is based on GPU accelerated color correction and frame warping without accurate camera parameters. We extend the traditional 2D-Matrix (2D-M) color correction approach and a present spatio-temporal 3D-Matrix (3D-M) color correction method for the overlap local regions with online color balancing using a piecewise function on global frames. Furthermore, we use pairwise homography matrices given by coarse camera calibration for global warping followed by accurate local warping based on the optical flow. Experimental results show that our system can generate highquality panorama videos in real time.
</details>
<details>
<summary>摘要</summary>
传统的图像融合方法集中精力于单一的панaramic帧，不考虑视频中的空间-时间一致性。直接使用图像融合方法会导致视频融合任务中的时间闪烁和颜色不稳定。此外，不准确的相机参数会导致图像扭曲。在这篇论文中，我们提出了一种基于GPU加速的实时系统，用于将多个视频序列拼接成一个投影视频。我们extend了传统的2D-矩阵（2D-M）颜色修正方法，并提出了一种基于空间-时间3D-矩阵（3D-M）颜色修正方法，用于在重叠地方进行在线颜色均衡使用分割函数。此外，我们使用粗略相机封锁得到的对角矩阵，然后使用精度地ocal warping基于运动图像。实验结果表明，我们的系统可以在实时中生成高质量的投影视频。
</details></li>
</ul>
<hr>
<h2 id="A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings"><a href="#A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings" class="headerlink" title="A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings"></a>A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09202">http://arxiv.org/abs/2308.09202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Kumar Jaiswal, Yu Xiong</li>
<li>for: 提高推荐系统的准确率和个性化程度，使其能更好地捕捉用户的兴趣和需求。</li>
<li>methods: 利用 capsule network 和 interest-aware 技术，对 item 进行强化表示，直接从用户行为中捕捉用户兴趣。</li>
<li>results: 在多种benchmark dataset上进行了广泛的实验，并证明了该方法可以帮助提高推荐系统的性能，特别是在用户兴趣方面。<details>
<summary>Abstract</summary>
Item representation holds significant importance in recommendation systems, which encompasses domains such as news, retail, and videos. Retrieval and ranking models utilise item representation to capture the user-item relationship based on user behaviours. While existing representation learning methods primarily focus on optimising item-based mechanisms, such as attention and sequential modelling. However, these methods lack a modelling mechanism to directly reflect user interests within the learned item representations. Consequently, these methods may be less effective in capturing user interests indirectly. To address this challenge, we propose a novel Interest-aware Capsule network (IaCN) recommendation model, a model-agnostic framework that directly learns interest-oriented item representations. IaCN serves as an auxiliary task, enabling the joint learning of both item-based and interest-based representations. This framework adopts existing recommendation models without requiring substantial redesign. We evaluate the proposed approach on benchmark datasets, exploring various scenarios involving different deep neural networks, behaviour sequence lengths, and joint learning ratios of interest-oriented item representations. Experimental results demonstrate significant performance enhancements across diverse recommendation models, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
“item表示具有重要 significancenin recommendation系统中，包括新闻、零售和视频等领域。 Retrieval和排名模型通过item表示来捕捉用户-item关系，基于用户行为。而现有的表示学习方法主要是通过对item-based机制进行优化，如关注和序列模型。但这些方法缺乏直接表达用户兴趣的模elling机制，因此可能不能够准确地捕捉用户兴趣。为了解决这个挑战，我们提出了一种新的用户兴趣注意力感知网络（IaCN）推荐模型，这是一种model-agnostic框架，可以直接学习用户兴趣 oriented item表示。IaCN作为auxiliary task，可以在已有的推荐模型中 jointly learn item-based和用户兴趣 oriented表示。这种框架不需要大量重新设计现有的推荐模型。我们在 benchmark datasets上进行了实验，exploring不同的深度神经网络、行为序列长度和joint learning ratio of interest-oriented item表示。实验结果表明，我们的方法可以在多种不同的推荐模型和 scenarios中提高表现，证明了我们的方法的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance"><a href="#Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance" class="headerlink" title="Regularizing Adversarial Imitation Learning Using Causal Invariance"></a>Regularizing Adversarial Imitation Learning Using Causal Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09189">http://arxiv.org/abs/2308.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Ovinnikov, Joachim M. Buhmann</li>
<li>for: 这篇论文是用来学习模型从专家示范数据集中推断策略的。</li>
<li>methods: 这篇论文使用了对抗学习方法，并使用了一个推定器作为导向信号。</li>
<li>results: 这篇论文表明了使用 causal invariance 作为regularization principle可以解决模型吸收专家数据中的假 correlation问题。<details>
<summary>Abstract</summary>
Imitation learning methods are used to infer a policy in a Markov decision process from a dataset of expert demonstrations by minimizing a divergence measure between the empirical state occupancy measures of the expert and the policy. The guiding signal to the policy is provided by the discriminator used as part of an versarial optimization procedure. We observe that this model is prone to absorbing spurious correlations present in the expert data. To alleviate this issue, we propose to use causal invariance as a regularization principle for adversarial training of these models. The regularization objective is applicable in a straightforward manner to existing adversarial imitation frameworks. We demonstrate the efficacy of the regularized formulation in an illustrative two-dimensional setting as well as a number of high-dimensional robot locomotion benchmark tasks.
</details>
<details>
<summary>摘要</summary>
模型使用依据学习方法从专家示范数据集中推导策略，以减少专家和策略之间的差异度量。导引信号被用来引导策略，并通过对抗优化过程中的探测器来提供指导信号。我们发现这种模型容易吸收专家数据中的偶极 correlations。为解决这问题，我们提议使用 causal invariance 作为对抗训练这些模型的正则化原则。这个正则化目标可以直接应用于现有的对抗依据模型中。我们在一些二维设定和高维机器人行走 benchmark 任务中证明了这种准则的效果。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT"><a href="#ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT" class="headerlink" title="ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT"></a>ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09731">http://arxiv.org/abs/2308.09731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia</li>
<li>for: 这个研究旨在应用大语言模型（LLM）在医疗决策中，特点是使用 OpenAI 的 ChatGPT，并开发了一种新的应用方法。</li>
<li>methods: 这种方法利用了域知识，从高性能可读取 ML 模型中提取了关键信息，并将其灵活地 integrate 到提问设计中。</li>
<li>results: 研究表明，使用这种方法可以在数据稀缺的情况下实现高质量的二分类任务，并且在不同的数据条件下，OpenAI 的 ChatGPT 的性能比传统的直接学习 ML 模型要好。这种方法可以在医疗决策中提供更多的洞察力和支持。<details>
<summary>Abstract</summary>
This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.   Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details>
<details>
<summary>摘要</summary>
Our research also explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. This study bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details></li>
</ul>
<hr>
<h2 id="How-Does-Pruning-Impact-Long-Tailed-Multi-Label-Medical-Image-Classifiers"><a href="#How-Does-Pruning-Impact-Long-Tailed-Multi-Label-Medical-Image-Classifiers" class="headerlink" title="How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers?"></a>How Does Pruning Impact Long-Tailed Multi-Label Medical Image Classifiers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09180">http://arxiv.org/abs/2308.09180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vita-group/prunecxr">https://github.com/vita-group/prunecxr</a></li>
<li>paper_authors: Gregory Holste, Ziyu Jiang, Ajay Jaiswal, Maria Hanna, Shlomo Minkowitz, Alan C. Legasto, Joanna G. Escalon, Sharon Steinberger, Mark Bittman, Thomas C. Shen, Ying Ding, Ronald M. Summers, George Shih, Yifan Peng, Zhangyang Wang</li>
<li>for: 本研究旨在 investigating the impact of pruning on deep neural networks  trained for thorax disease diagnosis from chest X-rays (CXRs), and understanding how pruning affects model behavior in long-tailed, multi-label datasets.</li>
<li>methods: 研究使用了 two large CXR datasets, and analyzed the effect of pruning on disease classification. The study also identified individual CXRs where uncompressed and heavily pruned models disagreed, known as pruning-identified exemplars (PIEs), and conducted a human reader study to evaluate their unifying qualities.</li>
<li>results: 研究发现，采用 pruning 技术可以减少深度神经网络的内存使用和执行时间，但是这些方法可能会对模型行为产生负面影响，特别是在长尾、多标签数据集中。研究还发现，采用 pruning 技术可以增加模型的Forgettability，并且可以通过人类读者研究来评估这些 exemplars 的特征。<details>
<summary>Abstract</summary>
Pruning has emerged as a powerful technique for compressing deep neural networks, reducing memory usage and inference time without significantly affecting overall performance. However, the nuanced ways in which pruning impacts model behavior are not well understood, particularly for long-tailed, multi-label datasets commonly found in clinical settings. This knowledge gap could have dangerous implications when deploying a pruned model for diagnosis, where unexpected model behavior could impact patient well-being. To fill this gap, we perform the first analysis of pruning's effect on neural networks trained to diagnose thorax diseases from chest X-rays (CXRs). On two large CXR datasets, we examine which diseases are most affected by pruning and characterize class "forgettability" based on disease frequency and co-occurrence behavior. Further, we identify individual CXRs where uncompressed and heavily pruned models disagree, known as pruning-identified exemplars (PIEs), and conduct a human reader study to evaluate their unifying qualities. We find that radiologists perceive PIEs as having more label noise, lower image quality, and higher diagnosis difficulty. This work represents a first step toward understanding the impact of pruning on model behavior in deep long-tailed, multi-label medical image classification. All code, model weights, and data access instructions can be found at https://github.com/VITA-Group/PruneCXR.
</details>
<details>
<summary>摘要</summary>
剪辑技术已成为深度神经网络压缩的有力的方法，可以降低计算机 memory 使用量和执行时间，而不会对总性表现产生重要的影响。然而，剪辑对模型行为的细微影响还不够了解，特别是在常见的医疗数据集上。这种知识空白可能会在部署剪辑后导致诊断错误，这可能会影响病人健康。为了填补这一空白，我们进行了首次对剪辑对神经网络诊断颈部疾病（CXR）的影响的分析。在两个大CXR数据集上，我们研究了哪些疾病受到剪辑影响，并 characterize 疾病 "忘记度" 基于疾病频率和相互出现行为。此外，我们identified 压缩后和 heavily 剪辑后模型之间的分歧，称为剪辑标识 exemplars (PIEs)，并进行了人类读者研究来评估其共同特征。我们发现， radiologists 认为 PIEs 具有更多的标签噪音、更差的图像质量和更高的诊断难度。这项工作代表了对剪辑对深度、多标签医疗图像分类模型行为的影响的首次研究。所有代码、模型权重和数据访问指南可以在 GitHub 上找到：https://github.com/VITA-Group/PruneCXR。
</details></li>
</ul>
<hr>
<h2 id="Diversifying-AI-Towards-Creative-Chess-with-AlphaZero"><a href="#Diversifying-AI-Towards-Creative-Chess-with-AlphaZero" class="headerlink" title="Diversifying AI: Towards Creative Chess with AlphaZero"></a>Diversifying AI: Towards Creative Chess with AlphaZero</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09175">http://arxiv.org/abs/2308.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad Tomasev, Lisa Schut, Demis Hassabis, Satinder Singh</li>
<li>for: 本研究探讨了人工智能（AI）系统是否可以通过创新决策机制来提高其计算能力。</li>
<li>methods: 本研究使用了AlphaZero（AZ）和其扩展版本AZ_db，通过行为多样性技术和低添加计划来让AI系统生成更多的想法，并选择最有前途的想法。</li>
<li>results: 实验表明，AZ_db在围棋游戏中表现出了多样化的做法，解决了更多的问题，并在围棋游戏中超越了更一致的团队。此外，在不同的开局中，AZ_db的成员特циализиру于不同的开局，通过低添加计划选择开局的棋手可以提高50个Elo分。研究结果表明，AI团队中的多样性贡献可以与人类团队中的多样性贡献相比，多样性是解决计算复杂问题的有价值资产。<details>
<summary>Abstract</summary>
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions. When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans and that diversity is a valuable asset in solving computationally hard problems.
</details>
<details>
<summary>摘要</summary>
近年来，人工智能（AI）系统已经超越了人类智能在多种计算任务上。然而，AI系统，如人类一样，会出现错误、盲点、幻觉和难以通过新情况泛化。这项工作探讨了AI是否可以通过创新决策机制提高其计算理性的限制。特别是，我们研究了一群多样化AI系统是否可以在复杂任务上超越单个AI，通过生成更多的想法并选择最佳的想法来增强其表现。我们在国际象棋（即人工智能的“蜞蜓”）中进行了研究，我们称之为AZ_db。我们使用了行为多样性技术来让AZ_db生成更广泛的想法，并使用下降式规划选择最佳想法。我们的实验表明，AZ_db在各种开局下玩国际象棋，每个开局都有不同的特点，并且选择每个开局的最佳棋手使用下降式规划，可以提高50个Elo分。我们的发现表明，AI团队中的多样性奖励存在，与人类团队一样，多样性是解决计算上的困难问题的有价值资产。
</details></li>
</ul>
<hr>
<h2 id="Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks"><a href="#Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks" class="headerlink" title="Forensic Data Analytics for Anomaly Detection in Evolving Networks"></a>Forensic Data Analytics for Anomaly Detection in Evolving Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09171">http://arxiv.org/abs/2308.09171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Yang, Abdallah Moubayed, Abdallah Shami, Amine Boukhtouta, Parisa Heidari, Stere Preda, Richard Brunner, Daniel Migault, Adel Larabi</li>
<li>for: 本研究旨在提供一个网络异常探测的数字分析框架，以帮助实现网络安全性。</li>
<li>methods: 本研究使用多元视角特征工程、无监督异常检测和全面结果修正过程，以探测网络中的异常行为。</li>
<li>results: 实验结果表明，提出的数字分析解决方案能够有效探测网络中的异常行为。<details>
<summary>Abstract</summary>
In the prevailing convergence of traditional infrastructure-based deployment (i.e., Telco and industry operational networks) towards evolving deployments enabled by 5G and virtualization, there is a keen interest in elaborating effective security controls to protect these deployments in-depth. By considering key enabling technologies like 5G and virtualization, evolving networks are democratized, facilitating the establishment of point presences integrating different business models ranging from media, dynamic web content, gaming, and a plethora of IoT use cases. Despite the increasing services provided by evolving networks, many cybercrimes and attacks have been launched in evolving networks to perform malicious activities. Due to the limitations of traditional security artifacts (e.g., firewalls and intrusion detection systems), the research on digital forensic data analytics has attracted more attention. Digital forensic analytics enables people to derive detailed information and comprehensive conclusions from different perspectives of cybercrimes to assist in convicting criminals and preventing future crimes. This chapter presents a digital analytics framework for network anomaly detection, including multi-perspective feature engineering, unsupervised anomaly detection, and comprehensive result correction procedures. Experiments on real-world evolving network data show the effectiveness of the proposed forensic data analytics solution.
</details>
<details>
<summary>摘要</summary>
在传统基础设施（如电信和产业运营网络）协调向5G和虚拟化的演进部署方向，有很大的兴趣在彻底保护这些部署。通过考虑关键启用技术（如5G和虚拟化），演进网络被民主化，使得不同业务模式的点 présence可以成功建立，包括媒体、动态网页内容、游戏和互联网器件多种应用场景。尽管演进网络提供了越来越多的服务，但是许多网络犯罪和攻击仍然在演进网络中进行不良活动。由于传统安全文件（如防火墙和侵入检测系统）的局限性，研究数字审计数据分析的研究吸引了更多的注意。数字审计数据分析可以帮助人们从不同角度获得详细信息和全面的结论，以帮助检察官捕捉犯罪分子和预防未来的犯罪。本章介绍了一种网络异常检测数字审计框架，包括多元角度特征工程、无监督异常检测和全面结果修正过程。实验表明，提案的数字审计数据分析解决方案在真实的演进网络数据上具有有效性。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Consistency-for-Assuring-Reliability-of-Large-Language-Models"><a href="#Semantic-Consistency-for-Assuring-Reliability-of-Large-Language-Models" class="headerlink" title="Semantic Consistency for Assuring Reliability of Large Language Models"></a>Semantic Consistency for Assuring Reliability of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09138">http://arxiv.org/abs/2308.09138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar</li>
<li>for: 这篇论文旨在提高大型自然语言模型（LLMs）的安全和可靠性，使其在不同的自然语言任务中表现更加稳定和可靠。</li>
<li>methods: 本论文提出了一个通用的对称性量表（Semantic Consistency Metric, SCM），用于评估 LLMS 在开放式文本生成任务中的对称性。此外，论文还提出了一个叫做 Ask-to-Choose (A2C) 的问题提示策略，用于增加对称性。</li>
<li>results: 实验结果显示，使用 A2C 问题提示策略可以将关注率提高 47%，并且可以将对称性指标提高 7 倍。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不同自然语言任务上表现出惊人的流畅和能力。然而，最近的研究发现，LLM在输入提示的变化对其输出有敏感性。为了在安全和可靠的方式部署LLM，其输出必须在具有同意义或目的的输入提示下保持一致性。一些现有的研究已经探讨了现代LLM如何处理这个问题，但是其评估仅限于单词或多词答案的字符串一致性，忽略了生成文本序列的一致性。为了更全面地了解LLM在开放式文本生成场景下的一致性，我们提出了一个通用的 semantic consistency 度量，并制定了多个版本的这个度量来评估不同LLM的性能。我们的提案显示在开放式文本生成场景下，我们的度量具有更高的一致性和更强的与人类评估输出一致性的相关性，而传统的基于字符串一致性的度量则显示较差的性能。最后，我们提出了一种新的提示策略，called Ask-to-Choose（A2C），以提高 semantic consistency。在基于 TruthfulQA benchmark 的关闭书问答任务上，A2C 提高了预训练和训练 LLM 的准确度指标 by up to 47%，并提高了 instruction-tuned 模型的 semantic consistency 指标 by up to 7倍。
</details></li>
</ul>
<hr>
<h2 id="EgoSchema-A-Diagnostic-Benchmark-for-Very-Long-form-Video-Language-Understanding"><a href="#EgoSchema-A-Diagnostic-Benchmark-for-Very-Long-form-Video-Language-Understanding" class="headerlink" title="EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding"></a>EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09126">http://arxiv.org/abs/2308.09126</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/egoschema/egoschema">https://github.com/egoschema/egoschema</a></li>
<li>paper_authors: Karttikeya Mangalam, Raiymbek Akshulakov, Jitendra Malik</li>
<li>for: 本研究目的是评估现代视觉和语言系统的长期视频理解能力，特别是在面对长视频clip时的能力。</li>
<li>methods: 本研究使用了Ego4D dataset，经人工精心编辑，涵盖了多种自然的人类活动和行为，并提供了5000多个多选题answer对。为每个问题，需要根据三分钟的视频clip选择正确的答案。此外，本研究还引入了时间证书集，用于捕捉视频任务中的内在时间理解长度。</li>
<li>results: 本研究发现，使用EgoSchema dataset进行评估，现代视觉和语言系统的长期视频理解能力远低于人类的76%，甚至bilions参数的模型也只能达到20%~33%的答案正确率。本研究认为，EgoSchema dataset，拥有长期内在时间结构和多样化复杂性，将成为未来开发有效长期视频理解系统的价值评估工具。<details>
<summary>Abstract</summary>
We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths over 5.7x longer than the second closest dataset and 10x to 100x longer than any other video understanding dataset. Further, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33% (random is 20%) on the EgoSchema multi-choice question answering task, while humans achieve about 76% accuracy. We posit that \name{}{}, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future. Data and Zero-shot model evaluation code are open-sourced for both public and commercial use under the Ego4D license at http://egoschema.github.io
</details>
<details>
<summary>摘要</summary>
我们介绍EGOSchema，一个非常长形式的视频问题回答数据集和benchmark，用于评估现代视频和语言系统的长期视频理解能力。从EGO4D derive，EGOSchema包含了超过5000个人类混合多个选项的问题答案对，覆盖了250小时的真实视频数据，涵盖了人类活动和行为的非常广泛领域。对每个问题，EGOSchema需要根据三分钟的视频片段选择正确的答案，而不是仅根据视频clip的长度。我们认为，仅仅根据视频clip的长度不能够真正捕捉视频任务中的时间困难程度。为了解决这个问题，我们引入了时间证书集，一个通用的概念用于捕捉视频任务中的内在时间理解长度。根据这个指标，我们发现EGOSchema的内在时间长度高于第二最近的数据集5.7倍，并且与任何其他视频理解数据集相比，EGOSchema的内在时间长度在10倍至100倍之间。此外，我们评估了一些现代视频和语言模型，发现这些模型在EGOSchema的多选问题回答任务中的答案率仅在33%（随机为20%），而人类则有约76%的答案率。我们认为，EGOSchema，拥有长期视频结构和多方面的复杂性，将成为未来发展长期视频理解系统的重要评估棒。EGOSchema的数据和零式模型评估代码在http://egoschema.github.io上公开，供公共和商业使用，欢迎各位专家和研究人员前来参与。
</details></li>
</ul>
<hr>
<h2 id="Spectral-information-criterion-for-automatic-elbow-detection"><a href="#Spectral-information-criterion-for-automatic-elbow-detection" class="headerlink" title="Spectral information criterion for automatic elbow detection"></a>Spectral information criterion for automatic elbow detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09108">http://arxiv.org/abs/2308.09108</a></li>
<li>repo_url: None</li>
<li>paper_authors: L. Martino, R. San Millan-Castillo, E. Morgado</li>
<li>for: 本文提出了一种总结了其他较知名的信息拟合 criterion（BIC和AIC）的一般化信息拟合 criterion（SIC），并且可以自动检测error curve的特点。</li>
<li>methods: 本文使用了spectral information criterion（SIC）来提取error curve的几何特征，并且不需要知道likelihood函数。</li>
<li>results: 本文的实验结果表明，SIC可以提供一个较小的子集模型，并且这些模型都是error curve的拐角点。此外，本文还提出了一个实际适用的选择模型的方法。<details>
<summary>Abstract</summary>
We introduce a generalized information criterion that contains other well-known information criteria, such as Bayesian information Criterion (BIC) and Akaike information criterion (AIC), as special cases. Furthermore, the proposed spectral information criterion (SIC) is also more general than the other information criteria, e.g., since the knowledge of a likelihood function is not strictly required. SIC extracts geometric features of the error curve and, as a consequence, it can be considered an automatic elbow detector. SIC provides a subset of all possible models, with a cardinality that often is much smaller than the total number of possible models. The elements of this subset are elbows of the error curve. A practical rule for selecting a unique model within the sets of elbows is suggested as well. Theoretical invariance properties of SIC are analyzed. Moreover, we test SIC in ideal scenarios where provides always the optimal expected results. We also test SIC in several numerical experiments: some involving synthetic data, and two experiments involving real datasets. They are all real-world applications such as clustering, variable selection, or polynomial order selection, to name a few. The results show the benefits of the proposed scheme. Matlab code related to the experiments is also provided. Possible future research lines are finally discussed.
</details>
<details>
<summary>摘要</summary>
我们介绍一个通用化信息标准（Generalized Information Criterion，GIC），它包含了bayesian信息标准（BIC）和阿凯瑞信息标准（AIC）等其他知名信息标准的特别情况。此外，我们的spectral information criterion（SIC）还比其他信息标准更加通用，例如不需要知道假设概率函数。SIC从几何特征中提取错误曲线的几何特征，因此可以被视为一个自动拱角检测器。SIC提供了可能的模型集合，其中的元素都是错误曲线的拱角。我们建议一个实用的选择模型方法，以及对SIC的理论不变性性的分析。此外，我们还进行了一些理论测试和实验测试，包括使用 sintetic 数据和实际数据。结果显示了我们的方案的优点。matlab代码相关的实验也提供。最后，我们讨论了未来的研究方向。
</details></li>
</ul>
<hr>
<h2 id="MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models"><a href="#MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models" class="headerlink" title="MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models"></a>MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09729">http://arxiv.org/abs/2308.09729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilin Wen, Zifeng Wang, Jimeng Sun</li>
<li>for: 本研究旨在探讨如何通过知识图（KG）来帮助语言模型（LLM）更好地吸收新知识，激发LLM的创造力，并让LLM的决策过程更加透明。</li>
<li>methods: 我们建立了一个启用KG输入和检索外部知识的提示管道，以便让LLM能够更好地理解KG输入和推理。此外，我们还研究了如何提取LLM的推理路径和生成答案时的心图。</li>
<li>results: 我们在三个问答 dataset上进行了实验，结果表明，使用 MindMap 提示可以带来明显的实验性提升。例如，在 GPT-3.5 上使用 MindMap 提示可以在 GPT-4 上取得战胜性的表现。此外，我们还发现，通过结构化知识从 KG 中检索，MindMap 可以超越一些使用文档检索方法的提示方法，从而获得更高的准确率、更短的检索距离和更全面的知识。<details>
<summary>Abstract</summary>
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance over GPT-4 consistently. We also demonstrate that with structured facts retrieved from KG, MindMap can outperform a series of prompting-with-document-retrieval methods, benefiting from more accurate, concise, and comprehensive knowledge from KGs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Identity-Aware-Semi-Supervised-Learning-for-Comic-Character-Re-Identification"><a href="#Identity-Aware-Semi-Supervised-Learning-for-Comic-Character-Re-Identification" class="headerlink" title="Identity-Aware Semi-Supervised Learning for Comic Character Re-Identification"></a>Identity-Aware Semi-Supervised Learning for Comic Character Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09096">http://arxiv.org/abs/2308.09096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gürkan Soykan, Deniz Yuret, Tevfik Metin Sezgin</li>
<li>for: 这个论文的目的是提出一种robust的半监督框架，用于在漫画中的人物重识别。</li>
<li>methods: 这个方法结合度量学习和一种新的’自我超vised’自我反例方法，通过对人物脸和身体的对比学习，提取人物特征。</li>
<li>results: 这个方法可以准确地重识别漫画中的人物，并且比使用脸或身体独立地重识别更有效。<details>
<summary>Abstract</summary>
Character re-identification, recognizing characters consistently across different panels in comics, presents significant challenges due to limited annotated data and complex variations in character appearances. To tackle this issue, we introduce a robust semi-supervised framework that combines metric learning with a novel 'Identity-Aware' self-supervision method by contrastive learning of face and body pairs of characters. Our approach involves processing both facial and bodily features within a unified network architecture, facilitating the extraction of identity-aligned character embeddings that capture individual identities while preserving the effectiveness of face and body features. This integrated character representation enhances feature extraction and improves character re-identification compared to re-identification by face or body independently, offering a parameter-efficient solution. By extensively validating our method using in-series and inter-series evaluation metrics, we demonstrate its effectiveness in consistently re-identifying comic characters. Compared to existing methods, our approach not only addresses the challenge of character re-identification but also serves as a foundation for downstream tasks since it can produce character embeddings without restrictions of face and body availability, enriching the comprehension of comic books. In our experiments, we leverage two newly curated datasets: the 'Comic Character Instances Dataset', comprising over a million character instances and the 'Comic Sequence Identity Dataset', containing annotations of identities within more than 3000 sets of four consecutive comic panels that we collected.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Character 重认处理，在漫画中的人物重复检测，存在很大的挑战，主要是因为有限的标注数据和人物形象的复杂变化。为解决这个问题，我们提出了一种可靠的半监督性框架，将度量学习与一种新的 '自我认知' 自监督学习方法结合，通过对人物脸和身体匹配的对比学习。我们的方法包括对人物脸和身体特征进行共同处理，使得提取人物特征的标准化 embeddings 可以捕捉到个体特征，同时保持脸和身体特征的有效性。这种一体化的人物表示提高了特征提取和人物重认处理，比起独立地使用脸或身体进行重认处理，提供了参数高效的解决方案。通过对我们方法进行 série 和 inter-série 评估指标，我们证明了它的有效性，可以在漫画中一致地重复检测人物。相比之下，我们的方法不仅解决了人物重认处理的挑战，还可以为下游任务提供基础，因为它可以不受脸和身体可用性的限制生成人物 embeddings，推动漫画的理解。在我们的实验中，我们利用了两个新收集的数据集：'Comic Character Instances Dataset'，包含了大于一百万个人物实例，以及'Comic Sequence Identity Dataset'，包含了四个连续的漫画幕后的标注，我们收集了超过3000个人物标注。
</details></li>
</ul>
<hr>
<h2 id="Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning"><a href="#Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning" class="headerlink" title="Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning"></a>Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09075">http://arxiv.org/abs/2308.09075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajit KrisshnaKumar, Jhoel Witter, Steve Paul, Hanvit Cho, Karthik Dantu, Souma Chowdhury</li>
<li>for: 提高城市和郊区交通效率、安全性和快速旅行的新领域——城市空中交通 (UAM)。</li>
<li>methods: 利用图约束学习生成决策支持策略， représenter物理位置在Vertiport的空间和被管理的交通工具为两个分开的图，通过图 convolutional neural network (GCN) 提取特征，然后通过perceptron层决定行为，如继续悬停或飞行、继续停机或起飞、或在分配给Vertiport的位置上降落。</li>
<li>results: 通过在AirSim中进行实际模拟，对减小多旋翼机的实际情况进行评估，结果表明图约束学习可以有效地解决城市空中交通——Vertiport调度管理问题，并且比基本的强化学习（图嵌入）或随机选择基eline有更好的性能， measured by delays, safety (no. of collisions) and battery consumption.<details>
<summary>Abstract</summary>
Urban Air Mobility (UAM) promises a new dimension to decongested, safe, and fast travel in urban and suburban hubs. These UAM aircraft are conceived to operate from small airports called vertiports each comprising multiple take-off/landing and battery-recharging spots. Since they might be situated in dense urban areas and need to handle many aircraft landings and take-offs each hour, managing this schedule in real-time becomes challenging for a traditional air-traffic controller but instead calls for an automated solution. This paper provides a novel approach to this problem of Urban Air Mobility - Vertiport Schedule Management (UAM-VSM), which leverages graph reinforcement learning to generate decision-support policies. Here the designated physical spots within the vertiport's airspace and the vehicles being managed are represented as two separate graphs, with feature extraction performed through a graph convolutional network (GCN). Extracted features are passed onto perceptron layers to decide actions such as continue to hover or cruise, continue idling or take-off, or land on an allocated vertiport spot. Performance is measured based on delays, safety (no. of collisions) and battery consumption. Through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, our results demonstrate the suitability of using graph reinforcement learning to solve the UAM-VSM problem and its superiority to basic reinforcement learning (with graph embeddings) or random choice baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.AI_2023_08_18/" data-id="clpztdnb5002nes8806ba28fr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.CL_2023_08_18/" class="article-date">
  <time datetime="2023-08-18T11:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.CL_2023_08_18/">cs.CL - 2023-08-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ChatHaruhi-Reviving-Anime-Character-in-Reality-via-Large-Language-Model"><a href="#ChatHaruhi-Reviving-Anime-Character-in-Reality-via-Large-Language-Model" class="headerlink" title="ChatHaruhi: Reviving Anime Character in Reality via Large Language Model"></a>ChatHaruhi: Reviving Anime Character in Reality via Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09597">http://arxiv.org/abs/2308.09597</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LC1332/Chat-Haruhi-Suzumiya">https://github.com/LC1332/Chat-Haruhi-Suzumiya</a></li>
<li>paper_authors: Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, Linkang Zhan, Yaokai Jia, Pingyu Wu, Haozhen Sun</li>
<li>for: 这篇论文旨在提出一种控制语言模型以模拟特定的虚构人物的算法，以提高角色扮演能力。</li>
<li>methods: 该算法使用改进的提示和从剧本中提取的人物记忆来控制语言模型。</li>
<li>results: 自动和人工评估都表明，该方法在比基eline进行角色扮演时表现更好。Translation:</li>
<li>for: This paper proposes an algorithm to control language models to mimic specific fictional characters, with the goal of improving role-playing ability.</li>
<li>methods: The algorithm uses an improved prompt and memories of the character extracted from scripts to control the language models.</li>
<li>results: Both automatic and human evaluations show that the proposed approach performs better than baselines in role-playing.<details>
<summary>Abstract</summary>
Role-playing chatbots built on large language models have drawn interest, but better techniques are needed to enable mimicking specific fictional characters. We propose an algorithm that controls language models via an improved prompt and memories of the character extracted from scripts. We construct ChatHaruhi, a dataset covering 32 Chinese / English TV / anime characters with over 54k simulated dialogues. Both automatic and human evaluations show our approach improves role-playing ability over baselines. Code and data are available at https://github.com/LC1332/Chat-Haruhi-Suzumiya .
</details>
<details>
<summary>摘要</summary>
大语言模型上的角色扮演聊天机器人已经吸引了注意，但更好的技术是需要实现模拟特定的虚构角色。我们提出了一个算法，可以通过改进提示和从剧本中提取的角色记忆来控制语言模型。我们建立了ChatHaruhi，一个覆盖32个中文/英文电视/动画角色的32000多个虚构对话。自动和人类评估都显示了我们的方法可以提高角色扮演能力比基eline。代码和数据可以在https://github.com/LC1332/Chat-Haruhi-Suzumiya 获取。
</details></li>
</ul>
<hr>
<h2 id="PUMGPT-A-Large-Vision-Language-Model-for-Product-Understanding"><a href="#PUMGPT-A-Large-Vision-Language-Model-for-Product-Understanding" class="headerlink" title="PUMGPT: A Large Vision-Language Model for Product Understanding"></a>PUMGPT: A Large Vision-Language Model for Product Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09568">http://arxiv.org/abs/2308.09568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhui Wu, Zengming Tang, Zongyi Guo, Weiwei Zhang, Baoliang Cui, Haihong Tang, Weiming Lu</li>
<li>for: 本研究主要针对产品理解任务进行探讨，以提高在线购物体验。产品理解任务包括多种子任务，需要模型回答基于多modal产品信息的多种问题。</li>
<li>methods: 我们提出了PUMGPT，一个大型视言语模型，旨在统一所有产品理解任务于单一模型结构。为 bridging视觉和文本表现之间的差距，我们提出了层刻拓展（LA），一种方法，通过增加 fewer 视觉 токен来提供更好的整合，并允许实际 Parameter-efficient 微调。</li>
<li>results: PUMGPT 在多种产品理解任务中表现出色，包括产品描述、类别问答、特征EXTRACTION、特征问答以及自由形问答关于产品。<details>
<summary>Abstract</summary>
Recent developments of multi-modal large language models have demonstrated its strong ability in solving vision-language tasks. In this paper, we focus on the product understanding task, which plays an essential role in enhancing online shopping experience. Product understanding task includes a variety of sub-tasks, which require models to respond diverse queries based on multi-modal product information. Traditional methods design distinct model architectures for each sub-task. On the contrary, we present PUMGPT, a large vision-language model aims at unifying all product understanding tasks under a singular model structure. To bridge the gap between vision and text representations, we propose Layer-wise Adapters (LA), an approach that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning. Moreover, the inherent parameter-efficient fine-tuning ability allows PUMGPT to be readily adapted to new product understanding tasks and emerging products. We design instruction templates to generate diverse product instruction datasets. Simultaneously, we utilize open-domain datasets during training to improve the performance of PUMGPT and its generalization ability. Through extensive evaluations, PUMGPT demonstrates its superior performance across multiple product understanding tasks, including product captioning, category question-answering, attribute extraction, attribute question-answering, and even free-form question-answering about products.
</details>
<details>
<summary>摘要</summary>
To bridge the gap between vision and text representations, we introduce Layer-wise Adapters (LA), a method that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning. PUMGPT's inherent parameter-efficient fine-tuning ability allows it to be easily adapted to new product understanding tasks and emerging products. We create instruction templates to generate diverse product instruction datasets, and we train PUMGPT using open-domain datasets to improve its performance and generalization ability.Through extensive evaluations, PUMGPT demonstrates superior performance across multiple product understanding tasks, including product captioning, category question-answering, attribute extraction, attribute question-answering, and even free-form question-answering about products.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model"><a href="#Exploring-Sampling-Techniques-for-Generating-Melodies-with-a-Transformer-Language-Model" class="headerlink" title="Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model"></a>Exploring Sampling Techniques for Generating Melodies with a Transformer Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09454">http://arxiv.org/abs/2308.09454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Rose Bjare, Stefan Lattner, Gerhard Widmer</li>
<li>for:  investigate the impact of different sampling techniques on musical qualities such as diversity and structure</li>
<li>methods: train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies, and analyze the musical qualities of the samples generated using distribution truncation sampling techniques</li>
<li>results: discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.<details>
<summary>Abstract</summary>
Research in natural language processing has demonstrated that the quality of generations from trained autoregressive language models is significantly influenced by the used sampling strategy. In this study, we investigate the impact of different sampling techniques on musical qualities such as diversity and structure. To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques. Specifically, we use nucleus sampling, the recently proposed "typical sampling", and conventional ancestral sampling. We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance. We assess the generated samples using objective and subjective evaluations. We discover that probability truncation techniques may restrict diversity and structural patterns in optimal circumstances, but may also produce more musical samples in suboptimal circumstances.
</details>
<details>
<summary>摘要</summary>
研究自然语言处理显示，训练 autoregressive 语言模型的质量生成受采样策略的影响。在这项研究中，我们调查不同采样技术对音乐质量的影响。为此，我们使用高容量 transformer 模型训练一大量高结构性的爱尔兰传统民歌旋律，并分析生成的样本中的音乐质量。特别是，我们使用核心采样、“典型采样”和传统祖先采样。我们在两种情况下评估这些采样策略的影响：优化的情况下，模型性能很好，以及受损的情况下，我们系统地降低模型性能。我们使用对象和主观评估来评估生成的样本。我们发现，概率 truncation 技术可能会在优化情况下压缩多样性和结构性特征，但在受损情况下可能会生成更多的音乐样本。
</details></li>
</ul>
<hr>
<h2 id="Scope-is-all-you-need-Transforming-LLMs-for-HPC-Code"><a href="#Scope-is-all-you-need-Transforming-LLMs-for-HPC-Code" class="headerlink" title="Scope is all you need: Transforming LLMs for HPC Code"></a>Scope is all you need: Transforming LLMs for HPC Code</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09440">http://arxiv.org/abs/2308.09440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scientific-computing-lab-nrcn/tokompiler">https://github.com/scientific-computing-lab-nrcn/tokompiler</a></li>
<li>paper_authors: Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Abdul Wasay, Nesreen Ahmed, Ted Willke, Guy Tamir, Yuval Pinter, Timothy Mattson, Gal Oren<br>for:这个论文旨在探讨大型自然语言处理（NLP）模型如何应用于编程任务，特别是高性能计算（HPC）领域的任务。methods:该论文提出了一种名为 Tokompiler 的新型编译器，用于适应编程语言和编译任务。Tokompiler 利用了语言基础知识，生成了语言相关的标记，以提供语言结构的上下文感知，而完全避免了人工含义的代码结构。results:实验结果表明，使用 Tokompiler 进行预训练，可以大幅提高代码完成率和语义理解能力，比传统的标识符更低，约为 1 个折衡指数。这些结果开启了领域特定 LLM 的发展前景，以满足特定领域的独特需求。<details>
<summary>Abstract</summary>
With easier access to powerful compute resources, there is a growing trend in the field of AI for software development to develop larger and larger language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size (e.g., billions of parameters) and demand expensive compute resources for training. We found this design choice confusing - why do we need large LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question design choices made by existing LLMs by developing smaller LLMs for specific domains - we call them domain-specific LLMs. Specifically, we start off with HPC as a domain and propose a novel tokenizer named Tokompiler, designed specifically for preprocessing code in HPC and compilation-centric tasks. Tokompiler leverages knowledge of language primitives to generate language-oriented tokens, providing a context-aware understanding of code structure while avoiding human semantics attributed to code structures completely. We applied Tokompiler to pre-train two state-of-the-art models, SPT-Code and Polycoder, for a Fortran code corpus mined from GitHub. We evaluate the performance of these models against the conventional LLMs. Results demonstrate that Tokompiler significantly enhances code completion accuracy and semantic understanding compared to traditional tokenizers in normalized-perplexity tests, down to ~1 perplexity score. This research opens avenues for further advancements in domain-specific LLMs, catering to the unique demands of HPC and compilation tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用更加可accessible的计算资源，在人工智能领域的软件开发中，正在增长一种大型语言模型（LLM）的趋势，以解决多种编程任务。即使应用于高性能计算（HPC）领域的LLM也非常大（例如，十亿个参数），需要昂贵的计算资源进行训练。我们认为这种设计选择是奇怪的——为什么需要大型LLM在自然语言和编程语言不related的HPC任务上进行训练？在这个研究中，我们想要质问现有LLM的设计选择，而是开发特定领域的LLM——我们称之为域specific LLM。 Specifically，我们开始于HPC领域，并提出了一种新的tokenizer名为Tokompiler，用于适应编译和编程任务。Tokompiler利用语言基本元素的知识来生成语言 oriented 的token，提供了代码结构上的上下文感知，而完全避免了人类语义 attributed 到代码结构。我们对HPC领域中的 Fortran 代码集进行预训练两个现有模型，SPT-Code 和 Polycoder。我们对这些模型进行评估，并与传统的 tokenizer进行比较。结果表明，Tokompiler在 норма化的复杂度测试中显著提高了代码完成率和semantic理解，相比传统的 tokenizer，下降至~1复杂度分数。这些研究开创了域specific LLM的新途径，适应HPC和编译任务的特殊需求。
</details></li>
</ul>
<hr>
<h2 id="A-Methodology-for-Generative-Spelling-Correction-via-Natural-Spelling-Errors-Emulation-across-Multiple-Domains-and-Languages"><a href="#A-Methodology-for-Generative-Spelling-Correction-via-Natural-Spelling-Errors-Emulation-across-Multiple-Domains-and-Languages" class="headerlink" title="A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages"></a>A Methodology for Generative Spelling Correction via Natural Spelling Errors Emulation across Multiple Domains and Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09435">http://arxiv.org/abs/2308.09435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Martynov, Mark Baushenko, Anastasia Kozlova, Katerina Kolomeytseva, Aleksandr Abramov, Alena Fenogenova<br>for: 本研究的目的是提出一种生成式拼写修正（SC）方法，用于更好地在文本编辑任务中 corrected spelling errors and mistypings。methods: 本研究使用了自然语言拼写错误和 mistypings 的研究，以及如何通过在正确句子中模拟这些错误来增强生成模型的预训练过程。我们还 investigate 了不同的文本领域下模型的能力。results: 我们在不同的损害策略、模型结构和大小下进行了实验，并评估了模型在单一领域和多领域测试集上的性能。此外，我们还提出了一个名为 SAGET (拼写检查 via 增强和生成分布 Emulation) 的自动生成 SC 库，包括一家生成模型的家族和内置的增强算法。<details>
<summary>Abstract</summary>
Modern large language models demonstrate impressive capabilities in text generation and generalization. However, they often struggle with solving text editing tasks, particularly when it comes to correcting spelling errors and mistypings. In this paper, we present a methodology for generative spelling correction (SC), which was tested on English and Russian languages and potentially can be extended to any language with minor changes. Our research mainly focuses on exploring natural spelling errors and mistypings in texts and studying the ways those errors can be emulated in correct sentences to effectively enrich generative models' pre-train procedure. We investigate the impact of such emulations and the models' abilities across different text domains. In this work, we investigate two spelling corruption techniques: 1) first one mimics human behavior when making a mistake through leveraging statistics of errors from particular dataset and 2) second adds the most common spelling errors, keyboard miss clicks, and some heuristics within the texts. We conducted experiments employing various corruption strategies, models' architectures and sizes on the pre-training and fine-tuning stages and evaluated the models using single-domain and multi-domain test sets. As a practical outcome of our work, we introduce SAGE (Spell checking via Augmentation and Generative distribution Emulation) is a library for automatic generative SC that includes a family of pre-trained generative models and built-in augmentation algorithms.
</details>
<details>
<summary>摘要</summary>
现代大语言模型表现出了优秀的文本生成和通用能力。然而，它们经常在文本编辑任务上遇到困难，特别是正确推理和短语输入错误。在这篇论文中，我们提出了一种生成拼写检查（SC）方法，在英文和俄文语言上进行测试，并可以适用于任何语言。我们的研究主要关注自然发生的拼写错误和输入错误在文本中的表现方式，并研究如何通过模拟这些错误来增强生成模型的预训练过程。我们 investigate了不同的损害策略、模型架构和大小在预训练和精度调整阶段的影响。在我们的实验中，我们使用了不同的损害策略、模型架构和大小，并对单domain和多domain测试集进行评估。作为实践的结果，我们介绍了一个名为SAGE（拼写检查via扩展和生成分布Emulation）的自动生成SC库，该库包括一家拼写检查模型和内置的扩展算法。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-for-DRL-Based-Anti-Jamming-Strategies-in-Zero-Touch-Networks"><a href="#Leveraging-Large-Language-Models-for-DRL-Based-Anti-Jamming-Strategies-in-Zero-Touch-Networks" class="headerlink" title="Leveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks"></a>Leveraging Large Language Models for DRL-Based Anti-Jamming Strategies in Zero Touch Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09376">http://arxiv.org/abs/2308.09376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abubakar S. Ali, Dimitrios Michael Manias, Abdallah Shami, Sami Muhaidat</li>
<li>for: 这篇论文主要是为了探讨自动化网络中的零点touch网络（ZTN）概念，以及在自动化过程中提高网络透明度和用户交互的可能性。</li>
<li>methods: 本论文使用了大语言模型（LLM）来把自动化网络过程与人类中心的界面相结合，以提高网络透明度和用户交互。</li>
<li>results: 通过一个深度强化学习（DRL）基于的防止干扰技术的实践案例，本论文示出了 LLM 可以将复杂的网络操作概念化为人类可读的报告。<details>
<summary>Abstract</summary>
As the dawn of sixth-generation (6G) networking approaches, it promises unprecedented advancements in communication and automation. Among the leading innovations of 6G is the concept of Zero Touch Networks (ZTNs), aiming to achieve fully automated, self-optimizing networks with minimal human intervention. Despite the advantages ZTNs offer in terms of efficiency and scalability, challenges surrounding transparency, adaptability, and human trust remain prevalent. Concurrently, the advent of Large Language Models (LLMs) presents an opportunity to elevate the ZTN framework by bridging the gap between automated processes and human-centric interfaces. This paper explores the integration of LLMs into ZTNs, highlighting their potential to enhance network transparency and improve user interactions. Through a comprehensive case study on deep reinforcement learning (DRL)-based anti-jamming technique, we demonstrate how LLMs can distill intricate network operations into intuitive, human-readable reports. Additionally, we address the technical and ethical intricacies of melding LLMs with ZTNs, with an emphasis on data privacy, transparency, and bias reduction. Looking ahead, we identify emerging research avenues at the nexus of LLMs and ZTNs, advocating for sustained innovation and interdisciplinary synergy in the domain of automated networks.
</details>
<details>
<summary>摘要</summary>
随着第六代网络（6G）的到来，它承诺了前所未有的通信和自动化技术。其中一项主导技术是零接触网络（ZTN），旨在实现无需人类干预的完全自动化网络。虽然ZTN具有高效率和可扩展性的优势，但是在透明度、适应性和人类信任方面仍存在许多挑战。同时，大型自然语言模型（LLM）的出现提供了一个机会，通过结合LLM和ZTN来bridging自动化过程和人类中心的界面。本文 explore了LLM在ZTN中的整合，探讨其能够提高网络透明度和改善用户互动。通过一个基于深度强化学习（DRL）的防止干扰技术的实践案例，我们示出了LLM可以将复杂的网络操作概括成易于理解的人类可读报告。此外，我们还讨论了将LLM与ZTN结合的技术和道德复杂性，强调数据隐私、透明度和偏见减少。 looking ahead，我们认为在LLM和ZTN之间的研究前景很广阔，希望能够持续推动这两个领域之间的创新和跨学科共融。
</details></li>
</ul>
<hr>
<h2 id="TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition"><a href="#TrOMR-Transformer-Based-Polyphonic-Optical-Music-Recognition" class="headerlink" title="TrOMR:Transformer-Based Polyphonic Optical Music Recognition"></a>TrOMR:Transformer-Based Polyphonic Optical Music Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09370">http://arxiv.org/abs/2308.09370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/netease/polyphonic-tromr">https://github.com/netease/polyphonic-tromr</a></li>
<li>paper_authors: Yixuan Li, Huaping Liu, Qiang Jin, Miaomiao Cai, Peng Li</li>
<li>for: 这个论文是关于音乐Recognition（OMR）技术的研究，旨在提出一种基于 transformer 的全音程识别方法，以提高recognition accuracy。</li>
<li>methods: 该方法使用 transformer 来实现全音程识别，并引入了一种新的一致性损失函数和合理的数据注释方法来提高识别精度。</li>
<li>results: 实验表明，TrOMR 方法在实际场景下比现有的 OMR 方法表现更高，特别是在识别复杂的乐谱上。此外，作者还开发了 TrOMR 系统和一个实际拍摄的乐谱场景数据集。<details>
<summary>Abstract</summary>
Optical Music Recognition (OMR) is an important technology in music and has been researched for a long time. Previous approaches for OMR are usually based on CNN for image understanding and RNN for music symbol classification. In this paper, we propose a transformer-based approach with excellent global perceptual capability for end-to-end polyphonic OMR, called TrOMR. We also introduce a novel consistency loss function and a reasonable approach for data annotation to improve recognition accuracy for complex music scores. Extensive experiments demonstrate that TrOMR outperforms current OMR methods, especially in real-world scenarios. We also develop a TrOMR system and build a camera scene dataset for full-page music scores in real-world. The code and datasets will be made available for reproducibility.
</details>
<details>
<summary>摘要</summary>
《光学音乐识别（OMR）技术在音乐领域已经被研究了很长时间。前一些OMR方法通常基于CNN для图像理解和RNN для音乐符号分类。在这篇论文中，我们提出了基于transformer的全globale感知方法，称为TrOMR，以提高端到端多重音乐OMR的准确率。我们还介绍了一种新的一致损失函数和合理的数据注释方法，以提高复杂音乐手稿的识别率。广泛的实验表明，TrOMR已经超越了当前OMR方法，特别是在实际场景中。我们还开发了TrOMR系统和一个摄像头场景数据集，用于实际全页音乐手稿识别。代码和数据集将被提供，以便重现。》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin"><a href="#A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin" class="headerlink" title="A tailored Handwritten-Text-Recognition System for Medieval Latin"></a>A tailored Handwritten-Text-Recognition System for Medieval Latin</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09368">http://arxiv.org/abs/2308.09368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Koch, Gilary Vera Nuñez, Esteban Garces Arias, Christian Heumann, Matthias Schöffel, Alexander Häberlin, Matthias Aßenmacher</li>
<li>for: 针对 medieval Latin dictionary 的数字化进行 Handwritten Text Recognition (HTR) 任务。</li>
<li>methods: 使用两个 state-of-the-art (SOTA) 图像分割模型进行数据集的准备，并运行多种组合的 transformer-based 模型和 GPT-2 解码器进行实验。</li>
<li>results: 实现了一个高度竞争力的模型，最佳设置 achievement 的 Character Error Rate (CER) 为 0.015，超过了商业 Google Cloud Vision 模型，并且表现更加稳定。<details>
<summary>Abstract</summary>
The Bavarian Academy of Sciences and Humanities aims to digitize its Medieval Latin Dictionary. This dictionary entails record cards referring to lemmas in medieval Latin, a low-resource language. A crucial step of the digitization process is the Handwritten Text Recognition (HTR) of the handwritten lemmas found on these record cards. In our work, we introduce an end-to-end pipeline, tailored to the medieval Latin dictionary, for locating, extracting, and transcribing the lemmas. We employ two state-of-the-art (SOTA) image segmentation models to prepare the initial data set for the HTR task. Furthermore, we experiment with different transformer-based models and conduct a set of experiments to explore the capabilities of different combinations of vision encoders with a GPT-2 decoder. Additionally, we also apply extensive data augmentation resulting in a highly competitive model. The best-performing setup achieved a Character Error Rate (CER) of 0.015, which is even superior to the commercial Google Cloud Vision model, and shows more stable performance.
</details>
<details>
<summary>摘要</summary>
Bavarian Academy of Sciences and Humanities 计划数字化中世纪拉丁词典。这个词典包含手写 Record cards 上的中世纪拉丁词语，这是一种低资源语言。我们的工作是设计一个端到端管道，专门为中世纪拉丁词典进行找到、提取和转录词语的任务。我们使用两个现代状态的图像分割模型来准备初始数据集 для HTR 任务。此外，我们还对不同的 transformer 模型进行了试验，并对不同的视觉编码器与 GPT-2 解码器的不同组合进行了一系列实验。此外，我们还应用了广泛的数据增强，实现了非常竞争力的模型。最佳设置达到了 Character Error Rate （CER）0.015，超过了商业 Google Cloud Vision 模型，并且表现更加稳定。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-materials-language-processing-enabled-by-GPT"><a href="#Accelerated-materials-language-processing-enabled-by-GPT" class="headerlink" title="Accelerated materials language processing enabled by GPT"></a>Accelerated materials language processing enabled by GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09354">http://arxiv.org/abs/2308.09354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaewoong Choi, Byungju Lee</li>
<li>for: 这研究的目的是提高材料科学文献中信息抽取的效率，并采用生成式预训练变换器（GPT）来替换优化的模型结构。</li>
<li>methods: 这研究使用了GPT引入的文档分类方法、命名实体识别（NER）方法和抽取问答（QA）方法，其中使用了策略性的提示工程来替换优化的模型结构。</li>
<li>results: 研究发现，使用GPT引入的方法可以实现与优化模型结构相当的准确率和可靠性，并且只需要小量的数据进行训练。此外，这些方法还可以在不同的材料科学领域中应用，以加速文献中信息抽取的过程。<details>
<summary>Abstract</summary>
Materials language processing (MLP) is one of the key facilitators of materials science research, as it enables the extraction of structured information from massive materials science literature. Prior works suggested high-performance MLP models for text classification, named entity recognition (NER), and extractive question answering (QA), which require complex model architecture, exhaustive fine-tuning and a large number of human-labelled datasets. In this study, we develop generative pretrained transformer (GPT)-enabled pipelines where the complex architectures of prior MLP models are replaced with strategic designs of prompt engineering. First, we develop a GPT-enabled document classification method for screening relevant documents, achieving comparable accuracy and reliability compared to prior models, with only small dataset. Secondly, for NER task, we design an entity-centric prompts, and learning few-shot of them improved the performance on most of entities in three open datasets. Finally, we develop an GPT-enabled extractive QA model, which provides improved performance and shows the possibility of automatically correcting annotations. While our findings confirm the potential of GPT-enabled MLP models as well as their value in terms of reliability and practicability, our scientific methods and systematic approach are applicable to any materials science domain to accelerate the information extraction of scientific literature.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models"><a href="#Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models" class="headerlink" title="Document Automation Architectures: Updated Survey in Light of Large Language Models"></a>Document Automation Architectures: Updated Survey in Light of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09341">http://arxiv.org/abs/2308.09341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Ahmadi Achachlouei, Omkar Patil, Tarun Joshi, Vijayan N. Nair</li>
<li>for: 本研究审查了当前文档自动化（DA）领域的最新状况，尤其是在法律领域的商业解决方案中的自动化文档生成。</li>
<li>methods: 本研究通过审查学术文献，为DA的定义和特征提供了更清晰的定义，并识别了学术研究中的DA架构和技术。</li>
<li>results: 本研究提供了新的DA研究机遇，基于最新的生成AI和大语言模型。<details>
<summary>Abstract</summary>
This paper surveys the current state of the art in document automation (DA). The objective of DA is to reduce the manual effort during the generation of documents by automatically creating and integrating input from different sources and assembling documents conforming to defined templates. There have been reviews of commercial solutions of DA, particularly in the legal domain, but to date there has been no comprehensive review of the academic research on DA architectures and technologies. The current survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features, identifies state-of-the-art DA architectures and technologies in academic research, and provides ideas that can lead to new research opportunities within the DA field in light of recent advances in generative AI and large language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="KESDT-knowledge-enhanced-shallow-and-deep-Transformer-for-detecting-adverse-drug-reactions"><a href="#KESDT-knowledge-enhanced-shallow-and-deep-Transformer-for-detecting-adverse-drug-reactions" class="headerlink" title="KESDT: knowledge enhanced shallow and deep Transformer for detecting adverse drug reactions"></a>KESDT: knowledge enhanced shallow and deep Transformer for detecting adverse drug reactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09329">http://arxiv.org/abs/2308.09329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunzhi Qiu, Xiaokun Zhang, Weiwei Wang, Tongxuan Zhang, Bo Xu, Hongfei Lin<br>for:The paper aims to improve the detection of adverse drug reactions (ADRs) on social media platforms by proposing a novel model called Knowledge Enhanced Shallow and Deep Transformer (KESDT).methods:The KESDT model incorporates domain keywords into the Transformer model through a shallow fusion manner and integrates synonym sets through a deep fusion manner to address the challenges of low annotated data and sample imbalance.results:The proposed KESDT model outperforms state-of-the-art baselines on three public datasets (TwiMed, Twitter, and CADEC) in terms of F1 values, with relative improvements of 4.87%, 47.83%, and 5.73%, respectively.<details>
<summary>Abstract</summary>
Adverse drug reaction (ADR) detection is an essential task in the medical field, as ADRs have a gravely detrimental impact on patients' health and the healthcare system. Due to a large number of people sharing information on social media platforms, an increasing number of efforts focus on social media data to carry out effective ADR detection. Despite having achieved impressive performance, the existing methods of ADR detection still suffer from three main challenges. Firstly, researchers have consistently ignored the interaction between domain keywords and other words in the sentence. Secondly, social media datasets suffer from the challenges of low annotated data. Thirdly, the issue of sample imbalance is commonly observed in social media datasets. To solve these challenges, we propose the Knowledge Enhanced Shallow and Deep Transformer(KESDT) model for ADR detection. Specifically, to cope with the first issue, we incorporate the domain keywords into the Transformer model through a shallow fusion manner, which enables the model to fully exploit the interactive relationships between domain keywords and other words in the sentence. To overcome the low annotated data, we integrate the synonym sets into the Transformer model through a deep fusion manner, which expands the size of the samples. To mitigate the impact of sample imbalance, we replace the standard cross entropy loss function with the focal loss function for effective model training. We conduct extensive experiments on three public datasets including TwiMed, Twitter, and CADEC. The proposed KESDT outperforms state-of-the-art baselines on F1 values, with relative improvements of 4.87%, 47.83%, and 5.73% respectively, which demonstrates the effectiveness of our proposed KESDT.
</details>
<details>
<summary>摘要</summary>
医疗领域内的药物反应检测是一项非常重要的任务，因为药物反应会对患者的健康产生严重的影响，同时也会对医疗系统产生沉重的负担。随着更多的人通过社交媒体平台分享信息，有越来越多的努力集中在社交媒体数据上进行有效的药物反应检测。尽管现有的检测方法已经取得了很好的表现，但是这些方法仍然面临着三个主要挑战。首先，研究人员一直忽略了域关键词和其他单词之间的互动关系。第二，社交媒体数据受到严重的标注数据不足的影响。第三，社交媒体数据中的样本偏度问题很常见。为解决这些挑战，我们提出了基于知识的扩展深度传播（KESDT）模型，用于药物反应检测。具体来说，为了处理第一个问题，我们将域关键词 integration到传播模型中，使得模型能够充分利用域关键词和其他单词之间的互动关系。为了解决低标注数据的问题，我们将同义词集 integrate到传播模型中，从而扩大样本的大小。为了缓解样本偏度问题，我们将标准十字 entropy损失函数替换为关注损失函数，以便更好地训练模型。我们对三个公共数据集，包括TwiMed、Twitter和CADEC进行了广泛的实验。我们的提出的KESDT比 estado-of-the-art基elines在F1值上提高4.87%、47.83%和5.73%，这表明了我们的KESDT的效果。
</details></li>
</ul>
<hr>
<h2 id="Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge"><a href="#Lip-Reading-for-Low-resource-Languages-by-Learning-and-Combining-General-Speech-Knowledge-and-Language-specific-Knowledge" class="headerlink" title="Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge"></a>Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09311">http://arxiv.org/abs/2308.09311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Jeong Hun Yeo, Jeongsoo Choi, Yong Man Ro</li>
<li>for: 本文提出了一种新的脸部说话框架，特别适用于低资源语言，过去的文献中没有充分考虑这个问题。由于低资源语言没有充分的视频文本对数据来训练模型，因此lipreading模型的开发受到了挑战。</li>
<li>methods: 我们尝试了通过Predicting speech units来学习通用语言知识，即模型嘴部运动的能力。不同语言部分共享相同的phoneme，因此可以将学习一种语言的通用语言知识扩展到其他语言。然后，我们提出了Language-specific Memory-augmented Decoder（LMDecoder）来学习语言特定的知识。LMDecoder将语言特定的声音特征存储在内存银行中，可以通过声音文本对来训练。因此，我们可以将输入的speech units转换成语言特定的声音特征，并使用学习的丰富语言知识来翻译它们。</li>
<li>results: 通过对五种语言（英语、西班牙语、法语、意大利语、葡萄牙语）的广泛实验，我们证明了提出的方法的效iveness。<details>
<summary>Abstract</summary>
This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data which is more easily accessible than video-text paired data. Therefore, with LMDecoder, we can transform the input speech units into language-specific audio features and translate them into texts by utilizing the learned rich language knowledge. Finally, by combining general speech knowledge and language-specific knowledge, we can efficiently develop lip reading models even for low-resource languages. Through extensive experiments using five languages, English, Spanish, French, Italian, and Portuguese, the effectiveness of the proposed method is evaluated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Differentiable-Retrieval-Augmentation-via-Generative-Language-Modeling-for-E-commerce-Query-Intent-Classification"><a href="#Differentiable-Retrieval-Augmentation-via-Generative-Language-Modeling-for-E-commerce-Query-Intent-Classification" class="headerlink" title="Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification"></a>Differentiable Retrieval Augmentation via Generative Language Modeling for E-commerce Query Intent Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09308">http://arxiv.org/abs/2308.09308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyu Zhao, Yunjiang Jiang, Yiming Qiu, Han Zhang, Wen-Yun Yang</li>
<li>for: 提高自然语言处理(NLP)任务中下游模型的性能，具体来说是在电商搜索中的查询意图分类任务。</li>
<li>methods: 通过知识检索器和外部词库的协同使用，实现增强下游模型的性能，而不是增加模型参数的方式。</li>
<li>results: 通过实验和减少研究，证明了我们提出的方法可以在查询意图分类任务中显著提高州网络处理(NLP)任务的性能，并且在实际应用中也能够达到良好的效果。<details>
<summary>Abstract</summary>
Retrieval augmentation, which enhances downstream models by a knowledge retriever and an external corpus instead of by merely increasing the number of model parameters, has been successfully applied to many natural language processing (NLP) tasks such as text classification, question answering and so on. However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training. In this paper, we propose Differentiable Retrieval Augmentation via Generative lANguage modeling(Dragan), to address this problem by a novel differentiable reformulation. We demonstrate the effectiveness of our proposed method on a challenging NLP task in e-commerce search, namely query intent classification. Both the experimental results and ablation study show that the proposed method significantly and reasonably improves the state-of-the-art baselines on both offline evaluation and online A/B test.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT Retrieval augmentation, which enhances downstream models by a knowledge retriever and an external corpus instead of by merely increasing the number of model parameters, has been successfully applied to many natural language processing (NLP) tasks such as text classification, question answering, and so on. However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training. In this paper, we propose Differentiable Retrieval Augmentation via Generative lANguage modeling(Dragan), to address this problem by a novel differentiable reformulation. We demonstrate the effectiveness of our proposed method on a challenging NLP task in e-commerce search, namely query intent classification. Both the experimental results and ablation study show that the proposed method significantly and reasonably improves the state-of-the-art baselines on both offline evaluation and online A/B test.TRANSLATE_END
</details></li>
</ul>
<hr>
<h2 id="Conversational-Ontology-Alignment-with-ChatGPT"><a href="#Conversational-Ontology-Alignment-with-ChatGPT" class="headerlink" title="Conversational Ontology Alignment with ChatGPT"></a>Conversational Ontology Alignment with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09217">http://arxiv.org/abs/2308.09217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanaz Saki Norouzi, Mohammad Saeid Mahdavinejad, Pascal Hitzler</li>
<li>for: 本研究evaluates the feasibility and efficiency of ChatGPT for ontology alignment using a naive approach.</li>
<li>methods: 本研究使用了一种Naive方法，使用ChatGPT的输出与 Ontology Alignment Evaluation Initiative 2022 的会议轨道 ontologies进行比较，以获得更多关于 conversational large language model 在 Naive 方法下的 ontology matching 能力的信息。</li>
<li>results: 研究发现，ChatGPT 的输出与 Ontology Alignment Evaluation Initiative 2022 的结果之间存在一定的相似性，但是还有一些差异。这表明了 conversational large language model 在 Naive 方法下的 ontology matching 能力是有限的，但也有一定的潜在优势。<details>
<summary>Abstract</summary>
This study evaluates the applicability and efficiency of ChatGPT for ontology alignment using a naive approach. ChatGPT's output is compared to the results of the Ontology Alignment Evaluation Initiative 2022 campaign using conference track ontologies. This comparison is intended to provide insights into the capabilities of a conversational large language model when used in a naive way for ontology matching, and to investigate the potential advantages and disadvantages of this approach.
</details>
<details>
<summary>摘要</summary>
这项研究评估了chatGPT在ontologyAlignment中的适用性和效率，使用了一种简单的方法。chatGPT的输出与2022年ontologyAlignment评估活动的会议轨道 ontologies 的结果进行比较，以提供 conversational large language model 在 naive 方式上的ontology匹配能力的洞察，并 investigate这种方法的优劣点。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports"><a href="#A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports" class="headerlink" title="A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports"></a>A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09193">http://arxiv.org/abs/2308.09193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/av9ash/duplicatebugdetection">https://github.com/av9ash/duplicatebugdetection</a></li>
<li>paper_authors: Avinash Patil, Kihwan Han, Sabyasachi Mukhopadhyay</li>
<li>for: 本研究旨在比较不同文本 Similarity 方法在bug report retrieval中的效果,以提高bug report的检索效率。</li>
<li>methods: 本研究使用了TF-IDF (基eline), FastText, Gensim, BERT和ADA embedding模型进行比较。</li>
<li>results: 实验结果显示BERT模型在回忆率方面表现最好，其次是ADA模型，接下来是Gensim、FastText和TF-IDF模型。<details>
<summary>Abstract</summary>
Bug reports are an essential aspect of software development, and it is crucial to identify and resolve them quickly to ensure the consistent functioning of software systems. Retrieving similar bug reports from an existing database can help reduce the time and effort required to resolve bugs. In this paper, we compared the effectiveness of semantic textual similarity methods for retrieving similar bug reports based on a similarity score. We explored several embedding models such as TF-IDF (Baseline), FastText, Gensim, BERT, and ADA. We used the Software Defects Data containing bug reports for various software projects to evaluate the performance of these models. Our experimental results showed that BERT generally outperformed the rest of the models regarding recall, followed by ADA, Gensim, FastText, and TFIDF. Our study provides insights into the effectiveness of different embedding methods for retrieving similar bug reports and highlights the impact of selecting the appropriate one for this task. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
📝 Bug 报告是软件开发中非常重要的一部分，快速标识和解决它们以确保软件系统的一致性。从现有的数据库中检索类似的bug报告可以减少解决bug所需的时间和努力。在这篇论文中，我们对用 semantic textual similarity 方法检索类似bug报告的效果进行了比较，并根据相似性分数进行评估。我们检查了TF-IDF（基准）、FastText、Gensim、BERT和ADA 等嵌入模型。我们使用了 Software Defects Data 中的 bug 报告来评估这些模型的性能。我们的实验结果表明，BERT 通常在 recall 方面表现更好，其次是 ADA，Gensim，FastText 和 TF-IDF。我们的研究提供了不同嵌入方法在检索类似bug报告的效果的视角，并 highlights 选择合适的嵌入方法对这项任务的重要性。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Is-Argument-Structure-of-Learner-Chinese-Understandable-A-Corpus-Based-Analysis"><a href="#Is-Argument-Structure-of-Learner-Chinese-Understandable-A-Corpus-Based-Analysis" class="headerlink" title="Is Argument Structure of Learner Chinese Understandable: A Corpus-Based Analysis"></a>Is Argument Structure of Learner Chinese Understandable: A Corpus-Based Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09186">http://arxiv.org/abs/2308.09186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuguang Duan, Zi Lin, Weiwei Sun</li>
<li>for: 这个论文是为了分析learner中文中的意向结构错误的。</li>
<li>methods: 这个论文使用了 sentence produced by language learners和 их corrected by native speakers的数据，并与 semantic role labeling标注。</li>
<li>results: 这个论文发现了learner中文中的意向结构错误，包括word order、word selection、lack of proposition和argument-adjunct confounding等。<details>
<summary>Abstract</summary>
This paper presents a corpus-based analysis of argument structure errors in learner Chinese. The data for analysis includes sentences produced by language learners as well as their corrections by native speakers. We couple the data with semantic role labeling annotations that are manually created by two senior students whose majors are both Applied Linguistics. The annotation procedure is guided by the Chinese PropBank specification, which is originally developed to cover first language phenomena. Nevertheless, we find that it is quite comprehensive for handling second language phenomena. The inter-annotator agreement is rather high, suggesting the understandability of learner texts to native speakers. Based on our annotations, we present a preliminary analysis of competence errors related to argument structure. In particular, speech errors related to word order, word selection, lack of proposition, and argument-adjunct confounding are discussed.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了学习中文learner的语法错误分析。数据来源包括学习者生成的句子以及Native speaker的修改。我们将数据 coupling avec manually created的semantic role labeling标注，遵循中文PropBank规范，这个规范原本是为first language phenomena而设计。然而，我们发现它对second language phenomena也非常全面。标注过程中的间接对应度较高，表明学习者的文本对Native speaker来说很容易理解。基于我们的标注，我们对语法错误进行了初步分析，包括word order错误、word selection错误、缺乏 Proposition 和 argument-adjunct杂合错误等。
</details></li>
</ul>
<hr>
<h2 id="ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse"><a href="#ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse" class="headerlink" title="ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse"></a>ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09158">http://arxiv.org/abs/2308.09158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangyikaii/lamda-zhijian">https://github.com/zhangyikaii/lamda-zhijian</a></li>
<li>paper_authors: Yi-Kai Zhang, Lu Ren, Chao Yi, Qi-Wei Wang, De-Chuan Zhan, Han-Jia Ye</li>
<li>for: 本研究旨在提供一个可用于实际应用中的模型重复使用工具箱（ZhiJian），实现了多种模型重复使用方法的整合，并提供了一个简单易用的PyTorch backend。</li>
<li>methods: 本研究使用了PyTorch backend，提出了一个称为PTM的目标架构建方法，并且提供了一个基于PTM的探索和调整方法，可以帮助深度学习专家在下游任务中探索和发现不同方法之间的补偿优点。</li>
<li>results: 本研究通过实际应用和评估，显示了ZhiJian在实际应用中的效果和可靠性，并且显示了PTM-based inference可以帮助深度学习专家在下游任务中探索和发现不同方法之间的补偿优点。<details>
<summary>Abstract</summary>
The rapid expansion of foundation pre-trained models and their fine-tuned counterparts has significantly contributed to the advancement of machine learning. Leveraging pre-trained models to extract knowledge and expedite learning in real-world tasks, known as "Model Reuse", has become crucial in various applications. Previous research focuses on reusing models within a certain aspect, including reusing model weights, structures, and hypothesis spaces. This paper introduces ZhiJian, a comprehensive and user-friendly toolbox for model reuse, utilizing the PyTorch backend. ZhiJian presents a novel paradigm that unifies diverse perspectives on model reuse, encompassing target architecture construction with PTM, tuning target model with PTM, and PTM-based inference. This empowers deep learning practitioners to explore downstream tasks and identify the complementary advantages among different methods. ZhiJian is readily accessible at https://github.com/zhangyikaii/lamda-zhijian facilitating seamless utilization of pre-trained models and streamlining the model reuse process for researchers and developers.
</details>
<details>
<summary>摘要</summary>
“快速扩展的基础模型和其精细化版本的发展，对机器学习的进步做出了重要贡献。利用预训练模型提取知识和快速学习实际任务中的概念，称为“模型重用”，在各种应用中变得非常重要。先前的研究主要关注在模型重用的特定方面，包括模型权重、结构和假设空间的重用。本文介绍了一个名为ZhiJian的通用和易用的工具箱，使用PyTorch后端。ZhiJian提出了一种新的思想，即在PTM中构建目标建筑、在PTM中调参目标模型和PTM基于的推理。这使得深度学习实践者可以更好地探索下游任务和发现不同方法之间的补做优势。ZhiJian可以很容易地在https://github.com/zhangyikaii/lamda-zhijian上获取，便于预训练模型的使用和模型重用过程中的流程，为研究者和开发者提供了便捷的使用方式。”
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Information-Seeking-Events-in-Health-Related-Social-Discourse"><a href="#Characterizing-Information-Seeking-Events-in-Health-Related-Social-Discourse" class="headerlink" title="Characterizing Information Seeking Events in Health-Related Social Discourse"></a>Characterizing Information Seeking Events in Health-Related Social Discourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09156">http://arxiv.org/abs/2308.09156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Sharif, Madhusudan Basak, Tanzia Parvin, Ava Scharfstein, Alphonso Bradham, Jacob T. Borodovsky, Sarah E. Lord, Sarah Masud Preum<br>for: This paper focuses on analyzing health-related information-seeking on social media, specifically on Reddit, to understand the treatment options and misconceptions related to Opioid Use Disorder (OUD).methods: The authors use a novel approach called event-driven analysis to categorize health-related information-seeking on social media into different events, such as treatment options, misconceptions, and knowledge gaps. They also develop a dataset called TREAT-ISE, which contains Reddit posts annotated with the type of events related to recovery from OUD.results: The authors achieve a strong performance benchmark of 77.4% F1 score for the task of classifying information-seeking events on Reddit related to OUD using machine learning and deep learning classifiers. They also investigate the performance and errors of ChatGPT on this task, providing insights into the capabilities and limitations of large language models.<details>
<summary>Abstract</summary>
Social media sites have become a popular platform for individuals to seek and share health information. Despite the progress in natural language processing for social media mining, a gap remains in analyzing health-related texts on social discourse in the context of events. Event-driven analysis can offer insights into different facets of healthcare at an individual and collective level, including treatment options, misconceptions, knowledge gaps, etc. This paper presents a paradigm to characterize health-related information-seeking in social discourse through the lens of events. Events here are board categories defined with domain experts that capture the trajectory of the treatment/medication. To illustrate the value of this approach, we analyze Reddit posts regarding medications for Opioid Use Disorder (OUD), a critical global health concern. To the best of our knowledge, this is the first attempt to define event categories for characterizing information-seeking in OUD social discourse. Guided by domain experts, we develop TREAT-ISE, a novel multilabel treatment information-seeking event dataset to analyze online discourse on an event-based framework. This dataset contains Reddit posts on information-seeking events related to recovery from OUD, where each post is annotated based on the type of events. We also establish a strong performance benchmark (77.4% F1 score) for the task by employing several machine learning and deep learning classifiers. Finally, we thoroughly investigate the performance and errors of ChatGPT on this task, providing valuable insights into the LLM's capabilities and ongoing characterization efforts.
</details>
<details>
<summary>摘要</summary>
社交媒体平台已成为个人寻求和分享健康信息的受欢迎平台。 despite the progress in自然语言处理 для社交媒体挖掘, a gap remains in analyzing health-related texts in the context of events. Event-driven analysis can offer insights into different facets of healthcare at an individual and collective level, including treatment options, misconceptions, knowledge gaps, etc. This paper presents a paradigm to characterize health-related information-seeking in social discourse through the lens of events. Events here are board categories defined with domain experts that capture the trajectory of the treatment/medication. To illustrate the value of this approach, we analyze Reddit posts regarding medications for Opioid Use Disorder (OUD), a critical global health concern. To the best of our knowledge, this is the first attempt to define event categories for characterizing information-seeking in OUD social discourse. Guided by domain experts, we develop TREAT-ISE, a novel multilabel treatment information-seeking event dataset to analyze online discourse on an event-based framework. This dataset contains Reddit posts on information-seeking events related to recovery from OUD, where each post is annotated based on the type of events. We also establish a strong performance benchmark (77.4% F1 score) for the task by employing several machine learning and deep learning classifiers. Finally, we thoroughly investigate the performance and errors of ChatGPT on this task, providing valuable insights into the LLM's capabilities and ongoing characterization efforts.
</details></li>
</ul>
<hr>
<h2 id="Linearity-of-Relation-Decoding-in-Transformer-Language-Models"><a href="#Linearity-of-Relation-Decoding-in-Transformer-Language-Models" class="headerlink" title="Linearity of Relation Decoding in Transformer Language Models"></a>Linearity of Relation Decoding in Transformer Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09124">http://arxiv.org/abs/2308.09124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau</li>
<li>for: 本研究探讨了transformer语言模型（LM）中大量知识的表达方式，即关系之间的计算。</li>
<li>methods: 研究人员使用了一种单个提示来构建一个first-order approximation的LM，并证明了某些关系可以通过单一的线性变换来表示。</li>
<li>results: 研究发现，LM的表达中存在许多不是线性地编码的关系知识，但是它们可以准确地预测关系。这些结果表明LM使用了一种简单、可读的，但是不均匀分布的知识表示策略。<details>
<summary>Abstract</summary>
Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.
</details>
<details>
<summary>摘要</summary>
许多语言模型中的知识可以表示为关系：单词和其同义词之间的关系，实体和其属性之间的关系等。我们发现，对于一些关系，这种计算可以被简单地表示为单一的线性变换在主题表示中。这种线性关系表示可以通过从单个提示构建首个预测来获得，并存在许多事实、通俗知识和语言关系中。然而，我们也发现许多情况下，LM的预测捕捉到了关系知识，但这种知识不是直接地编码在其表示中。我们的结果因此揭示了一种简单、可解释的，但受到不同应用场景的各种表示策略在transformer语言模型中。
</details></li>
</ul>
<hr>
<h2 id="MaScQA-A-Question-Answering-Dataset-for-Investigating-Materials-Science-Knowledge-of-Large-Language-Models"><a href="#MaScQA-A-Question-Answering-Dataset-for-Investigating-Materials-Science-Knowledge-of-Large-Language-Models" class="headerlink" title="MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models"></a>MaScQA: A Question Answering Dataset for Investigating Materials Science Knowledge of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09115">http://arxiv.org/abs/2308.09115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohd Zaki, Jayadeva, Mausam, N. M. Anoop Krishnan</li>
<li>for: 本研究旨在开发一个可以快速找到材料的知识库，以便更好地满足材料科学领域的研究需求。</li>
<li>methods: 本研究使用语言模型来回答材料领域的问题，并从知识库中提取信息。</li>
<li>results: GPT-4模型在解决材料领域的650个问题中表现最好（约62%的准确率），而链式思维提示对模型的性能没有显著提升。研究发现，概念错误（约64%）是LLMs表现下降的主要原因，而计算错误（约36%）则是次要原因。<details>
<summary>Abstract</summary>
Information extraction and textual comprehension from materials literature are vital for developing an exhaustive knowledge base that enables accelerated materials discovery. Language models have demonstrated their capability to answer domain-specific questions and retrieve information from knowledge bases. However, there are no benchmark datasets in the materials domain that can evaluate the understanding of the key concepts by these language models. In this work, we curate a dataset of 650 challenging questions from the materials domain that require the knowledge and skills of a materials student who has cleared their undergraduate degree. We classify these questions based on their structure and the materials science domain-based subcategories. Further, we evaluate the performance of GPT-3.5 and GPT-4 models on solving these questions via zero-shot and chain of thought prompting. It is observed that GPT-4 gives the best performance (~62% accuracy) as compared to GPT-3.5. Interestingly, in contrast to the general observation, no significant improvement in accuracy is observed with the chain of thought prompting. To evaluate the limitations, we performed an error analysis, which revealed conceptual errors (~64%) as the major contributor compared to computational errors (~36%) towards the reduced performance of LLMs. We hope that the dataset and analysis performed in this work will promote further research in developing better materials science domain-specific LLMs and strategies for information extraction.
</details>
<details>
<summary>摘要</summary>
信息抽取和文本理解从材料文献中是发展加速材料发现的关键。语言模型已经表现出其能够回答域务特定问题和从知识库中提取信息。但是，在材料领域没有 benchmark 数据集来评估这些语言模型对关键概念的理解。在这项工作中，我们积集了 650 个材料领域的复杂问题，这些问题需要Materials 学生完成本科学位课程后的知识和技能。我们将这些问题分类为结构和材料科学领域下的子类别。然后，我们使用 zero-shot 和链条提问训练 GPT-3.5 和 GPT-4 模型，并评估其性能。结果显示，GPT-4 的性能最高（约 62% 准确率），而 GPT-3.5 的性能较低。另外，与通常观察不同，链条提问不对性能的提高有显著影响。为了评估局限性，我们进行了错误分析，发现概念错误（约 64%）是 LLMS 表现不佳的主要原因，而计算错误（约 36%）则是次要原因。我们希望这些数据和分析可以促进更好的材料科学领域特定 LLMS 的开发和信息抽取策略的研究。
</details></li>
</ul>
<hr>
<h2 id="mCL-NER-Cross-Lingual-Named-Entity-Recognition-via-Multi-view-Contrastive-Learning"><a href="#mCL-NER-Cross-Lingual-Named-Entity-Recognition-via-Multi-view-Contrastive-Learning" class="headerlink" title="mCL-NER: Cross-Lingual Named Entity Recognition via Multi-view Contrastive Learning"></a>mCL-NER: Cross-Lingual Named Entity Recognition via Multi-view Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09073">http://arxiv.org/abs/2308.09073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Mo, Jian Yang, Jiahao Liu, Qifan Wang, Ruoyu Chen, Jingang Wang, Zhoujun Li</li>
<li>for: 提高 Cross-Lingual Named Entity Recognition (CrossNER) 的性能，尤其是非英语数据的 scarcity 问题。</li>
<li>methods: 提出 Multi-view Contrastive Learning for Cross-Lingual Named Entity Recognition (mCL-NER)，通过考虑 token 之间的关系来协调 semantic 和 token-level 表示之间的差异。</li>
<li>results: 在 XTREME benchmark 上进行了实验，与先前的数据驱动和模型驱动方法进行比较，显示 mCL-NER 可以大幅提高 CrossNER 的性能，在40种语言中提高了 nearly +2.0 $F_1$ 得分。<details>
<summary>Abstract</summary>
Cross-lingual named entity recognition (CrossNER) faces challenges stemming from uneven performance due to the scarcity of multilingual corpora, especially for non-English data. While prior efforts mainly focus on data-driven transfer methods, a significant aspect that has not been fully explored is aligning both semantic and token-level representations across diverse languages. In this paper, we propose Multi-view Contrastive Learning for Cross-lingual Named Entity Recognition (mCL-NER). Specifically, we reframe the CrossNER task into a problem of recognizing relationships between pairs of tokens. This approach taps into the inherent contextual nuances of token-to-token connections within entities, allowing us to align representations across different languages. A multi-view contrastive learning framework is introduced to encompass semantic contrasts between source, codeswitched, and target sentences, as well as contrasts among token-to-token relations. By enforcing agreement within both semantic and relational spaces, we minimize the gap between source sentences and their counterparts of both codeswitched and target sentences. This alignment extends to the relationships between diverse tokens, enhancing the projection of entities across languages. We further augment CrossNER by combining self-training with labeled source data and unlabeled target data. Our experiments on the XTREME benchmark, spanning 40 languages, demonstrate the superiority of mCL-NER over prior data-driven and model-based approaches. It achieves a substantial increase of nearly +2.0 $F_1$ scores across a broad spectrum and establishes itself as the new state-of-the-art performer.
</details>
<details>
<summary>摘要</summary>
cross-lingual named entity recognition (CrossNER) 面临着因语言异常缺乏多语言资料而导致的性能不均衡的挑战。现有的尝试主要集中在数据驱动的传输方法上，而一个重要的方面尚未得到完全探索是在多语言空间中对表示进行同步。在这篇论文中，我们提议一种多视角对比学习方法，称为多视角对比学习 для跨语言命名实体识别（mCL-NER）。具体来说，我们将跨语言命名实体识别任务转化为一个涉及到对实体中token之间的关系的问题。这种方法利用了实体中token之间的语言特征，从而使得表示之间进行同步。我们提出了一个多视角对比学习框架，包括源语言、codeswitched语言和目标语言之间的semantic对比和token之间的关系对比。通过在Semantic和Relational空间中强制同步，我们最小化了源语言和其相应的codeswitched语言和目标语言之间的差异。这种对应扩展到不同语言之间的关系，提高了实体的跨语言 проекing。我们还通过与标注的源数据和无标注目标数据进行自我训练，进一步提高了 CrossNER 的性能。我们在XTREME benchmark上进行了40种语言的实验，并证明了mCL-NER 在先前的数据驱动和模型基于的方法之上取得了显著的改进，增加了约 +2.0 $F_1$ 分数，并成为新的状态级表现者。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.CL_2023_08_18/" data-id="clpztdndl00aies88316ofxnv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/cs.LG_2023_08_18/" class="article-date">
  <time datetime="2023-08-18T10:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/cs.LG_2023_08_18/">cs.LG - 2023-08-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification"><a href="#Revisiting-Skin-Tone-Fairness-in-Dermatological-Lesion-Classification" class="headerlink" title="Revisiting Skin Tone Fairness in Dermatological Lesion Classification"></a>Revisiting Skin Tone Fairness in Dermatological Lesion Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09640">http://arxiv.org/abs/2308.09640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tkalbl/revisitingskintonefairness">https://github.com/tkalbl/revisitingskintonefairness</a></li>
<li>paper_authors: Thorsten Kalb, Kaisar Kushibar, Celia Cintas, Karim Lekadir, Oliver Diaz, Richard Osuala</li>
<li>for: 本研究目的是为了评估皮肤疾病分类算法的公平性，因为皮肤疾病的表现可能因皮肤颜色而异常。</li>
<li>methods: 本研究使用了Individual Typology Angle（ITA）来估计皮肤颜色，并对ISIC18数据集进行了比较和分析。</li>
<li>results: 研究发现 existed large disagreement among previously published studies demonstrating the risks of ITA-based skin tone estimation methods，并且发现ISIC18数据集的不够多样性限制了其作为公平性分析的测试平台。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Addressing fairness in lesion classification from dermatological images is crucial due to variations in how skin diseases manifest across skin tones. However, the absence of skin tone labels in public datasets hinders building a fair classifier. To date, such skin tone labels have been estimated prior to fairness analysis in independent studies using the Individual Typology Angle (ITA). Briefly, ITA calculates an angle based on pixels extracted from skin images taking into account the lightness and yellow-blue tints. These angles are then categorised into skin tones that are subsequently used to analyse fairness in skin cancer classification. In this work, we review and compare four ITA-based approaches of skin tone classification on the ISIC18 dataset, a common benchmark for assessing skin cancer classification fairness in the literature. Our analyses reveal a high disagreement among previously published studies demonstrating the risks of ITA-based skin tone estimation methods. Moreover, we investigate the causes of such large discrepancy among these approaches and find that the lack of diversity in the ISIC18 dataset limits its use as a testbed for fairness analysis. Finally, we recommend further research on robust ITA estimation and diverse dataset acquisition with skin tone annotation to facilitate conclusive fairness assessments of artificial intelligence tools in dermatology. Our code is available at https://github.com/tkalbl/RevisitingSkinToneFairness.
</details>
<details>
<summary>摘要</summary>
（Addressing fairness in lesion classification from dermatological images is crucial due to variations in how skin diseases manifest across different skin tones. However, the lack of skin tone labels in public datasets hinders the development of a fair classifier. To date, skin tone labels have been estimated prior to fairness analysis in independent studies using the Individual Typology Angle (ITA). Briefly, ITA calculates an angle based on pixels extracted from skin images, taking into account the lightness and yellow-blue tints. These angles are then categorized into skin tones that are subsequently used to analyze fairness in skin cancer classification. In this work, we review and compare four ITA-based approaches of skin tone classification on the ISIC18 dataset, a common benchmark for assessing skin cancer classification fairness in the literature. Our analyses reveal a high disagreement among previously published studies, demonstrating the risks of ITA-based skin tone estimation methods. Moreover, we investigate the causes of such large discrepancies among these approaches and find that the lack of diversity in the ISIC18 dataset limits its use as a testbed for fairness analysis. Finally, we recommend further research on robust ITA estimation and diverse dataset acquisition with skin tone annotation to facilitate conclusive fairness assessments of artificial intelligence tools in dermatology. Our code is available at https://github.com/tkalbl/RevisitingSkinToneFairness.）
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Neural-Network-based-Method-for-Improved-Imputation-of-Missing-Values-in-Time-Series-Data-by-Repurposing-DataWig"><a href="#Development-of-a-Neural-Network-based-Method-for-Improved-Imputation-of-Missing-Values-in-Time-Series-Data-by-Repurposing-DataWig" class="headerlink" title="Development of a Neural Network-based Method for Improved Imputation of Missing Values in Time Series Data by Repurposing DataWig"></a>Development of a Neural Network-based Method for Improved Imputation of Missing Values in Time Series Data by Repurposing DataWig</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09635">http://arxiv.org/abs/2308.09635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Zhang</li>
<li>for: 这个研究旨在提供一个可靠地填写时间序列资料中的缺失值的方法，以便在研究、企业和管理中做出更好的决策。</li>
<li>methods: 本研究使用了tsDataWig方法，它是修改了DataWig方法的时间序列资料填写方法，可以直接处理时间变数的值和填写时间序列资料中的缺失值。</li>
<li>results: 研究结果显示，tsDataWig方法可以在实验和实际应用中表现更好，并且可以填写更多的缺失值，而且不需要强制性地假设资料缺失机制。<details>
<summary>Abstract</summary>
Time series data are observations collected over time intervals. Successful analysis of time series data captures patterns such as trends, cyclicity and irregularity, which are crucial for decision making in research, business, and governance. However, missing values in time series data occur often and present obstacles to successful analysis, thus they need to be filled with alternative values, a process called imputation. Although various approaches have been attempted for robust imputation of time series data, even the most advanced methods still face challenges including limited scalability, poor capacity to handle heterogeneous data types and inflexibility due to requiring strong assumptions of data missing mechanisms. Moreover, the imputation accuracy of these methods still has room for improvement. In this study, I developed tsDataWig (time-series DataWig) by modifying DataWig, a neural network-based method that possesses the capacity to process large datasets and heterogeneous data types but was designed for non-time series data imputation. Unlike the original DataWig, tsDataWig can directly handle values of time variables and impute missing values in complex time series datasets. Using one simulated and three different complex real-world time series datasets, I demonstrated that tsDataWig outperforms the original DataWig and the current state-of-the-art methods for time series data imputation and potentially has broad application due to not requiring strong assumptions of data missing mechanisms. This study provides a valuable solution for robustly imputing missing values in challenging time series datasets, which often contain millions of samples, high dimensional variables, and heterogeneous data types.
</details>
<details>
<summary>摘要</summary>
时间序列数据是观测过时间间隔的观察结果。成功分析时间序列数据可以捕捉到趋势、循环和不规则性，这些特征对于研究、商业和管理决策非常重要。然而，时间序列数据中的缺失值经常发生，这些缺失值需要使用代理值来填充，这个过程被称为投入。虽然有很多方法用于适应时间序列数据的缺失值投入，但是这些方法仍然面临着限制Scalability、不能处理多种数据类型和固定的假设时间序列数据缺失机制的问题。此外，这些方法的投入准确性仍然有很大的可能性提高。在这个研究中，我修改了DataWig方法，创造了tsDataWig（时间序列DataWig），它可以直接处理时间变量的值并在复杂的时间序列数据中填充缺失值。使用一个模拟和三个不同的复杂实际时间序列数据集，我示出了tsDataWig在时间序列数据投入中的优于DataWig和当前状态艺术方法，并有广泛的应用前提，不需要强制的数据缺失机制假设。这个研究为抗强制时间序列数据投入提供了一个有价值的解决方案，这些数据经常包含百万个样本、高维度变量和多种数据类型。
</details></li>
</ul>
<hr>
<h2 id="VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments"><a href="#VALERIE22-–-A-photorealistic-richly-metadata-annotated-dataset-of-urban-environments" class="headerlink" title="VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments"></a>VALERIE22 – A photorealistic, richly metadata annotated dataset of urban environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09632">http://arxiv.org/abs/2308.09632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Grau, Korbinian Hagn</li>
<li>For: The paper aims to contribute to the understanding of domain-specific factors that influence the perception performance of deep neural networks (DNNs) in the context of pedestrian detection in urban environments for automated driving.* Methods: The paper presents the VALERIE tool pipeline, a synthetic data generator that uses procedural tools to simulate photorealistic sensor data from automatically synthesized scenes. The dataset provides a rich set of metadata, enabling a variety of tests to understand the performance of DNNs.* Results: The paper demonstrates that the VALERIE22 dataset is one of the best-performing synthetic datasets currently available in the open domain, based on performance metrics. The dataset provides a unique set of features that enable researchers to understand the performance of DNNs in various scenarios.Here’s the same information in Simplified Chinese:* For: 这篇论文目标是帮助理解深度神经网络（DNNs）在城市环境中自动驾驶中的感知性能的域特有因素。* Methods: 论文提出了VALERIE工具框架，这是一个可生成 fotorealistic感知数据的工具，通过自动生成的场景来模拟感知数据。该数据集提供了丰富的元数据，可以对 DNNs 的性能进行多种测试。* Results: 论文表明，VALERIE22 数据集是目前公开领域中最好的 sintetic 数据集之一，基于性能指标。该数据集提供了一些独特的特征，如像素精度遮挡率、场景中的位置和相机与场景之间的距离和角度，以便研究 DNNs 的性能。<details>
<summary>Abstract</summary>
The VALERIE tool pipeline is a synthetic data generator developed with the goal to contribute to the understanding of domain-specific factors that influence perception performance of DNNs (deep neural networks). This work was carried out under the German research project KI Absicherung in order to develop a methodology for the validation of DNNs in the context of pedestrian detection in urban environments for automated driving. The VALERIE22 dataset was generated with the VALERIE procedural tools pipeline providing a photorealistic sensor simulation rendered from automatically synthesized scenes. The dataset provides a uniquely rich set of metadata, allowing extraction of specific scene and semantic features (like pixel-accurate occlusion rates, positions in the scene and distance + angle to the camera). This enables a multitude of possible tests on the data and we hope to stimulate research on understanding performance of DNNs. Based on performance metric a comparison with several other publicly available datasets is provided, demonstrating that VALERIE22 is one of best performing synthetic datasets currently available in the open domain.
</details>
<details>
<summary>摘要</summary>
VALERIE工具管道是一个人工数据生成器，旨在帮助理解深度神经网络（DNN）的域特性因素对感知性能的影响。这项工作是在德国研究项目“KI Absicherung”下进行的，旨在在城市环境中自动驾驶时，对人体检测器的DNN进行验证。VALERIE22数据集是通过VALERIE的批处理工具管道生成的，提供了高度 photorealistic 感知器模拟，自动生成的场景。该数据集具有独特的metadata，允许提取特定场景和semantic特征（如像素精度遮挡率、场景中的位置和相机到摄像头的距离）。这使得可以进行多种可能的测试，并希望促进DNN性能的研究。基于表现指标，VALERIE22与其他公共可用的数据集进行了比较，示出VALERIE22是目前公开领域中最佳的人工数据集之一。
</details></li>
</ul>
<hr>
<h2 id="Learning-Computational-Efficient-Bots-with-Costly-Features"><a href="#Learning-Computational-Efficient-Bots-with-Costly-Features" class="headerlink" title="Learning Computational Efficient Bots with Costly Features"></a>Learning Computational Efficient Bots with Costly Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09629">http://arxiv.org/abs/2308.09629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Kobanda, Valliappan C. A., Joshua Romoff, Ludovic Denoyer</li>
<li>For: The paper is written for decision-making processes in various fields, particularly in real-time settings such as video games.* Methods: The paper proposes a generic offline learning approach that incorporates cost constraints to limit the computational cost of the input features. The approach is based on the Decision Transformer, but with additional budgeting constraints to dynamically choose the best input features at each timestep.* Results: The paper demonstrates the effectiveness of the proposed method on several tasks, including D4RL benchmarks and complex 3D environments similar to those found in video games. The results show that the method can achieve similar performance while using significantly fewer computational resources compared to classical approaches.Here’s the information in Simplified Chinese text:* 用途：论文用于各种领域中的决策过程，特别是在视频游戏等实时设置中。* 方法：论文提出一种通用的离线学习方法，其具有限制输入特征计算成本的功能。该方法基于决策变换器，但又增加了预算约束来动态选择最佳输入特征在每个时刻。* 结果：论文在多个任务上展示了方法的效果，包括D4RL标准准则和复杂的3D环境，并显示该方法可以与传统方法相比使用更少的计算资源来 достичь相似的性能。<details>
<summary>Abstract</summary>
Deep reinforcement learning (DRL) techniques have become increasingly used in various fields for decision-making processes. However, a challenge that often arises is the trade-off between both the computational efficiency of the decision-making process and the ability of the learned agent to solve a particular task. This is particularly critical in real-time settings such as video games where the agent needs to take relevant decisions at a very high frequency, with a very limited inference time.   In this work, we propose a generic offline learning approach where the computation cost of the input features is taken into account. We derive the Budgeted Decision Transformer as an extension of the Decision Transformer that incorporates cost constraints to limit its cost at inference. As a result, the model can dynamically choose the best input features at each timestep. We demonstrate the effectiveness of our method on several tasks, including D4RL benchmarks and complex 3D environments similar to those found in video games, and show that it can achieve similar performance while using significantly fewer computational resources compared to classical approaches.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）技术在不同领域的决策过程中得到广泛应用。然而，一个常见的挑战是在决策过程中计算效率与学习模型解决特定任务的能力之间的负担。特别是在实时设定，如游戏中，模型需要在非常高频率下做出相关决策，并且具有非常有限的推理时间。在这种情况下，我们提出了一种通用的离线学习方法，其中输入特征的计算成本被考虑在内。我们 derivate了一种受成本限制的决策变换器，即优化的决策变换器，可以在每个时间步骤中动态选择最佳的输入特征。我们在多个任务上证明了我们的方法的有效性，包括D4RL数据集和复杂的3D环境，并显示它可以在计算资源方面减少了许多比 класси方法。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Bayesian-Optimization-Using-a-Lagrange-Multiplier-Applied-to-Power-Transistor-Design"><a href="#Constrained-Bayesian-Optimization-Using-a-Lagrange-Multiplier-Applied-to-Power-Transistor-Design" class="headerlink" title="Constrained Bayesian Optimization Using a Lagrange Multiplier Applied to Power Transistor Design"></a>Constrained Bayesian Optimization Using a Lagrange Multiplier Applied to Power Transistor Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09612">http://arxiv.org/abs/2308.09612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping-Ju Chuang, Ali Saadat, Sara Ghazvini, Hal Edwards, William G. Vandenberghe</li>
<li>for: 优化LDMOS晶体管的设计过程，实现目标断裂电压(BV)。</li>
<li>methods: 将受限BO问题转化为常见BO问题，使用拉格朗日矩阵作为优化目标函数。</li>
<li>results: 自动地在设计空间中获得满足优化FOM和目标BV约束的设备，并在30-50V范围内探索设备的物理限制。<details>
<summary>Abstract</summary>
We propose a novel constrained Bayesian Optimization (BO) algorithm optimizing the design process of Laterally-Diffused Metal-Oxide-Semiconductor (LDMOS) transistors while realizing a target Breakdown Voltage (BV). We convert the constrained BO problem into a conventional BO problem using a Lagrange multiplier. Instead of directly optimizing the traditional Figure-of-Merit (FOM), we set the Lagrangian as the objective function of BO. This adaptive objective function with a changeable Lagrange multiplier can address constrained BO problems which have constraints that require costly evaluations, without the need for additional surrogate models to approximate constraints. Our algorithm enables a device designer to set the target BV in the design space, and obtain a device that satisfies the optimized FOM and the target BV constraint automatically. Utilizing this algorithm, we have also explored the physical limits of the FOM for our devices in 30 - 50 V range within the defined design space.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的受限制的整合算法（BO），用于优化LDMOS晶体管的设计过程，同时实现目标破坏电压（BV）。我们将受限制BO问题转化为一个常规BO问题，使用拉格朗日multiplier。而不是直接优化传统的图像质量指标（FOM），我们设置了拉格朗日作为算法的目标函数。这种适应目标函数的改变多余分子可以解决具有costly评估的受限制BO问题，无需额外的拟合模型来近似约束。我们的算法允许设计者在设计空间中设定目标BV，并自动获得满足优化FOM和目标BV约束的设备。通过这种算法，我们还探索了我们的设备在30-50V范围内的物理限制。
</details></li>
</ul>
<hr>
<h2 id="Solving-PDEs-on-Spheres-with-Physics-Informed-Convolutional-Neural-Networks"><a href="#Solving-PDEs-on-Spheres-with-Physics-Informed-Convolutional-Neural-Networks" class="headerlink" title="Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks"></a>Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09605">http://arxiv.org/abs/2308.09605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng, Ding-Xuan Zhou</li>
<li>for: 解决部分导数方程（PDEs）的数学理论分析</li>
<li>methods: 使用物理学信息激活的径向神经网络（PICNN）和深度卷积神经网络（CNN），以及圆形幂分析</li>
<li>results: 提供了对圆形PDE的数学分析，并证明了对 Sobolev 范数的上界误差 bound，以及快速收敛率Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on providing a mathematical analysis of the numerical performance of physics-informed convolutional neural networks (PICNNs) for solving partial differential equations (PDEs) on the sphere.</li>
<li>methods: The paper uses and improves the latest approximation results of deep convolutional neural networks and spherical harmonic analysis to establish an upper bound for the approximation error with respect to the Sobolev norm. The paper also integrates this with innovative localization complexity analysis to establish fast convergence rates for PICNN.</li>
<li>results: The paper provides theoretical results on the numerical performance of PICNN for solving PDEs on the sphere, including an upper bound for the approximation error and fast convergence rates. The results are also confirmed and supplemented by experiments.<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been demonstrated to be efficient in solving partial differential equations (PDEs) from a variety of experimental perspectives. Some recent studies have also proposed PINN algorithms for PDEs on surfaces, including spheres. However, theoretical understanding of the numerical performance of PINNs, especially PINNs on surfaces or manifolds, is still lacking. In this paper, we establish rigorous analysis of the physics-informed convolutional neural network (PICNN) for solving PDEs on the sphere. By using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis, we prove an upper bound for the approximation error with respect to the Sobolev norm. Subsequently, we integrate this with innovative localization complexity analysis to establish fast convergence rates for PICNN. Our theoretical results are also confirmed and supplemented by our experiments. In light of these findings, we explore potential strategies for circumventing the curse of dimensionality that arises when solving high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
物理学 informed 神经网络 (PINNs) 已经在多种实验方面证明其高效地解决部分 diferencial 方程 (PDEs)。一些最近的研究还提出了 PINN 算法用于表面上的 PDEs。然而，对 PINNs 的数学性能，特别是在表面或 manifold 上的数学性能，仍然缺乏理论理解。在这篇论文中，我们建立了 физи学 informed 卷积神经网络 (PICNN) 的准确分析，用于解决在球体上的 PDEs。我们使用和改进了最新的深度卷积神经网络的近似结果和球面幂分析，证明了对 Sobolev  нор的上界误差。然后，我们将这与创新的本地化复杂性分析相结合，以确定 PICNN 的快速收敛速率。我们的理论结果也被实验证明。在这些发现的基础上，我们探讨了解决高维度 PDEs 所带来的 "诅咒" 的策略。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Complexity-Barrier-in-Compositional-Minimax-Optimization"><a href="#Breaking-the-Complexity-Barrier-in-Compositional-Minimax-Optimization" class="headerlink" title="Breaking the Complexity Barrier in Compositional Minimax Optimization"></a>Breaking the Complexity Barrier in Compositional Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09604">http://arxiv.org/abs/2308.09604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Liu, Xiaokang Pan, Junwen Duan, Hongdong Li, Youqi Li, Zhe Qu</li>
<li>for: 本研究旨在解决机器学习中的复合最小化优化问题，包括分布robust训练和策略评估。</li>
<li>methods: 本文提出了嵌入STORM（NSTORM）算法，可以在$O(\kappa^3&#x2F;\epsilon^3)$样本复杂度下找到一个$\epsilon$-准确的解决方案。同时，作者还提出了适应学习率的ADA-NSTORM算法，可以实现同样的样本复杂度而且在实验中表现更加有效。</li>
<li>results: 本文的方法可以匹配下界函数的最小化优化问题，而不需要大量的批处理大小。实验结果表明，ADA-NSTORM算法在实际应用中表现更加稳定和有效。<details>
<summary>Abstract</summary>
Compositional minimax optimization is a pivotal yet under-explored challenge across machine learning, including distributionally robust training and policy evaluation for reinforcement learning. Current techniques exhibit suboptimal complexity or rely heavily on large batch sizes. This paper proposes Nested STOchastic Recursive Momentum (NSTORM), attaining the optimal sample complexity of $O(\kappa^3/\epsilon^3)$ for finding an $\epsilon$-accurate solution. However, NSTORM requires low learning rates, potentially limiting applicability. Thus we introduce ADAptive NSTORM (ADA-NSTORM) with adaptive learning rates, proving it achieves the same sample complexity while experiments demonstrate greater effectiveness. Our methods match lower bounds for minimax optimization without large batch requirements, validated through extensive experiments. This work significantly advances compositional minimax optimization, a crucial capability for distributional robustness and policy evaluation
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>机器学习中的分组最小最大优化是一项重要 yet under-explored 挑战。现有技术具有较差的复杂性或者依赖于大批量。这篇论文提出了嵌套STOchastic Recursive Momentum（NSTORM），实现了 $\epsilon $-高精度解决方案的最佳样本复杂性 $O(\kappa^3/\epsilon^3)$。但NSTORM需要低学习率，可能限制其应用。因此，我们引入了自适应NSTORM（ADA-NSTORM），其中学习率适应自适应，证明它们实现了同样的样本复杂性，而实验表明它们更有效。我们的方法与最小最大优化下界匹配，无需大批量，通过广泛的实验验证。这项工作对分组最小最大优化做出了重要进展，这对分布robustness和策略评估非常重要。
</details></li>
</ul>
<hr>
<h2 id="Disparity-Inequality-and-Accuracy-Tradeoffs-in-Graph-Neural-Networks-for-Node-Classification"><a href="#Disparity-Inequality-and-Accuracy-Tradeoffs-in-Graph-Neural-Networks-for-Node-Classification" class="headerlink" title="Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification"></a>Disparity, Inequality, and Accuracy Tradeoffs in Graph Neural Networks for Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09596">http://arxiv.org/abs/2308.09596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arpitdm/gnn_accuracy_fairness_tradeoff">https://github.com/arpitdm/gnn_accuracy_fairness_tradeoff</a></li>
<li>paper_authors: Arpit Merchant, Carlos Castillo</li>
<li>for: 这个研究旨在探讨Graph Neural Networks（GNNs）在人工智能应用中是否存在偏见，并且提出了两种GNN-agnostic干预方法（PFR-AX和PostProcess）来减少这种偏见的影响。</li>
<li>methods: 这个研究使用了四个数据集进行大量的实验，测试了两种干预方法（PFR-AX和PostProcess）和三种基eline干预方法（random dropout、weighted dropout和PGD）在三个现代GNN模型上的效果。</li>
<li>results: 研究结果显示，PFR-AX和PostProcess可以提供精确的偏见控制和改善模型在保护群体中确认正面结果的自信度，但是不同数据集和GNN模型之间存在差异，无法一个通用的偏见控制方法。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are increasingly used in critical human applications for predicting node labels in attributed graphs. Their ability to aggregate features from nodes' neighbors for accurate classification also has the capacity to exacerbate existing biases in data or to introduce new ones towards members from protected demographic groups. Thus, it is imperative to quantify how GNNs may be biased and to what extent their harmful effects may be mitigated. To this end, we propose two new GNN-agnostic interventions namely, (i) PFR-AX which decreases the separability between nodes in protected and non-protected groups, and (ii) PostProcess which updates model predictions based on a blackbox policy to minimize differences between error rates across demographic groups. Through a large set of experiments on four datasets, we frame the efficacies of our approaches (and three variants) in terms of their algorithmic fairness-accuracy tradeoff and benchmark our results against three strong baseline interventions on three state-of-the-art GNN models. Our results show that no single intervention offers a universally optimal tradeoff, but PFR-AX and PostProcess provide granular control and improve model confidence when correctly predicting positive outcomes for nodes in protected groups.
</details>
<details>
<summary>摘要</summary>
граф нейронные сети (GNNs) 在重要的人类应用中越来越普遍用于预测图中节点标签。它们可以将节点邻居特征集成到准确的分类中，同时也有可能扩大现有数据中的偏见或引入新的偏见 towards 保护类群体的成员。因此，我们需要衡量 GNNs 是否偏离和如何减轻其不良影响。为此，我们提出了两种 GNN-agnostic 举措，namely，(i) PFR-AX，减少保护和非保护群体之间节点的分离度，以及 (ii) PostProcess，根据黑盒政策更新模型预测，以最小化不同群体的错误率差异。通过大量实验 на four datasets，我们框定了我们的方法（和三个变种）的算法公正性-准确性负担和比较我们的结果与三种强基eline  intervención on three state-of-the-art GNN 模型。我们的结果表明，无论单个 intervención 都不是通用的优化选择，但PFR-AX 和 PostProcess 可以提供细化的控制和提高模型对保护群体成员正确预测的自信心。
</details></li>
</ul>
<hr>
<h2 id="WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct"><a href="#WizardMath-Empowering-Mathematical-Reasoning-for-Large-Language-Models-via-Reinforced-Evol-Instruct" class="headerlink" title="WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"></a>WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09583">http://arxiv.org/abs/2308.09583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nlpxucan/wizardlm">https://github.com/nlpxucan/wizardlm</a></li>
<li>paper_authors: Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang</li>
<li>for: 这篇论文主要目的是提高大型自然语言处理（NLP）模型的数学逻辑能力。</li>
<li>methods: 该论文提出了一种基于强化学习的 Reinforcement Learning from Evol-Instruct Feedback（RLEIF）方法，用于提高LLMs的数学逻辑能力。</li>
<li>results: 经过广泛的实验，WizardMath模型在两个数学逻辑测试 benchmark 上表现出色，与其他已有的开源模型相比，具有显著的优势。此外，WizardMath模型还可以在 GSM8k 和 MATH 两个测试 benchmark 上超越 ChatGPT-3.5、Claude Instant-1、PaLM-2 和 Minerva 等模型。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of Llama-2, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. WizardMath surpasses all other open-source LLMs by a substantial margin. Furthermore, our model even outperforms ChatGPT-3.5, Claude Instant-1, PaLM-2 and Minerva on GSM8k, simultaneously surpasses Text-davinci-002, PaLM-1 and GPT-3 on MATH. More details and model weights are public at https://github.com/nlpxucan/WizardLM and https://huggingface.co/WizardLM.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理（NLP）模型（LLM），如GPT-4，在数学逻辑任务中表现出了惊人的能力。然而，现有的开源模型大多只是在大规模互联网数据上预训练而无法进行数学相关的优化。本文提出了一种基于我们的提议的强化策略——Reinforcement Learning from Evol-Instruct Feedback（RLEIF），用于提高Llama-2模型的数学逻辑能力。经过广泛的实验，我们发现了WizardMath模型的极高能力。WizardMath在GSM8k和MATH两个数学逻辑评测 benchmark 上大幅超越了所有开源LLM，并且 même surpasses ChatGPT-3.5、Claude Instant-1、PaLM-2和Minerva在GSM8k上，同时在MATH上还超越了Text-davinci-002、PaLM-1和GPT-3。更多细节和模型权重可以在 GitHub 上找到（https://github.com/nlpxucan/WizardLM）和 Hugging Face 上找到（https://huggingface.co/WizardLM）。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Boundary-Integral-Networks-PIBI-Nets-A-Data-Driven-Approach-for-Solving-Partial-Differential-Equations"><a href="#Physics-Informed-Boundary-Integral-Networks-PIBI-Nets-A-Data-Driven-Approach-for-Solving-Partial-Differential-Equations" class="headerlink" title="Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations"></a>Physics-Informed Boundary Integral Networks (PIBI-Nets): A Data-Driven Approach for Solving Partial Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09571">http://arxiv.org/abs/2308.09571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Nagy-Huber, Volker Roth</li>
<li>for: The paper is written for solving partial differential equations (PDEs) in real-world applications, especially when there is limited information about boundary or initial conditions, or when unknown model parameters need to be identified.</li>
<li>methods: The paper proposes a data-driven approach called Physics-Informed Boundary Integral Networks (PIBI-Nets) to solve PDEs. PIBI-Nets only require collocation points at the computational domain boundary, which can reduce computational costs and achieve highly accurate results.</li>
<li>results: The paper demonstrates the excellent performance of PIBI-Nets for the Laplace and Poisson equations on both artificial data sets and a real-world application concerning the reconstruction of groundwater flows. PIBI-Nets outperform Physics-Informed Neural Networks (PINNs) in high-dimensional settings and can handle point sources in inverse problems using a principled and simple approach.<details>
<summary>Abstract</summary>
Partial differential equations (PDEs) can describe many relevant phenomena in dynamical systems. In real-world applications, we commonly need to combine formal PDE models with (potentially noisy) observations. This is especially relevant in settings where we lack information about boundary or initial conditions, or where we need to identify unknown model parameters. In recent years, Physics-informed neural networks (PINNs) have become a popular tool for problems of this kind. In high-dimensional settings, however, PINNs often suffer from computational problems because they usually require dense collocation points over the entire computational domain. To address this problem, we present Physics-Informed Boundary Integral Networks (PIBI-Nets) as a data-driven approach for solving PDEs in one dimension less than the original problem space. PIBI-Nets only need collocation points at the computational domain boundary, while still achieving highly accurate results, and in several practical settings, they clearly outperform PINNs. Exploiting elementary properties of fundamental solutions of linear differential operators, we present a principled and simple way to handle point sources in inverse problems. We demonstrate the excellent performance of PIBI-Nets for the Laplace and Poisson equations, both on artificial data sets and within a real-world application concerning the reconstruction of groundwater flows.
</details>
<details>
<summary>摘要</summary>
部分偏微分方程（PDEs）可以描述多种实际系统中的相关现象。在实际应用中，我们经常需要将正式的PDE模型与（可能具有噪声）观测结合起来。特别是在我们缺乏边界或初始条件信息时，或者需要确定未知模型参数时。在过去几年中，物理学推导的神经网络（PINNs）已成为这类问题的受欢迎工具。在高维度设置中，然而，PINNs经常因计算问题而受到限制，因为它们通常需要某些稠密的散点在计算域中。为解决这个问题，我们提出了基于物理学推导的边界积分网络（PIBI-Nets），这是一种数据驱动的方法，用于解决PDEs。PIBI-Nets只需在计算域边界上设置散点，却可以具有高度准确的结果，并在一些实际应用中表现出色，超过了PINNs。利用基本的线性微分方程的元素性质，我们提出了一种原则性的和简单的方法来处理反向问题。我们在拉普拉斯和波撤方程中进行了详细的实验，并在一个真实世界应用中对地下水流的重建中获得了出色的表现。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning"><a href="#Investigating-the-Interplay-between-Features-and-Structures-in-Graph-Learning" class="headerlink" title="Investigating the Interplay between Features and Structures in Graph Learning"></a>Investigating the Interplay between Features and Structures in Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09570">http://arxiv.org/abs/2308.09570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniele Castellana, Federico Errica</li>
<li>for: This paper aims to investigate the relationship between node features and target labels in deep graph networks, and to propose new metrics to measure the influence of node features on target labels.</li>
<li>methods: The paper uses two generative processes to build and study ad-hoc node classification tasks, and evaluates the performance of six models, including structure-agnostic ones.</li>
<li>results: The paper finds that previously defined metrics are not adequate when the assumption of a strong correlation between node features and target labels is relaxed, and proposes a new metric called Feature Informativeness to quantitatively measure the influence of node features on target labels.<details>
<summary>Abstract</summary>
In the past, the dichotomy between homophily and heterophily has inspired research contributions toward a better understanding of Deep Graph Networks' inductive bias. In particular, it was believed that homophily strongly correlates with better node classification predictions of message-passing methods. More recently, however, researchers pointed out that such dichotomy is too simplistic as we can construct node classification tasks where graphs are completely heterophilic but the performances remain high. Most of these works have also proposed new quantitative metrics to understand when a graph structure is useful, which implicitly or explicitly assume the correlation between node features and target labels. Our work empirically investigates what happens when this strong assumption does not hold, by formalising two generative processes for node classification tasks that allow us to build and study ad-hoc problems. To quantitatively measure the influence of the node features on the target labels, we also use a metric we call Feature Informativeness. We construct six synthetic tasks and evaluate the performance of six models, including structure-agnostic ones. Our findings reveal that previously defined metrics are not adequate when we relax the above assumption. Our contribution to the workshop aims at presenting novel research findings that could help advance our understanding of the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Normalization-Is-All-You-Need-Understanding-Layer-Normalized-Federated-Learning-under-Extreme-Label-Shift"><a href="#Normalization-Is-All-You-Need-Understanding-Layer-Normalized-Federated-Learning-under-Extreme-Label-Shift" class="headerlink" title="Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift"></a>Normalization Is All You Need: Understanding Layer-Normalized Federated Learning under Extreme Label Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09565">http://arxiv.org/abs/2308.09565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guojun Zhang, Mahdi Beitollahi, Alex Bie, Xi Chen</li>
<li>for: 本文探讨了层normalization（LN）在 federated learning（FL）中的作用，特别是如何在非相关数据上表现出 surprisingly 的效果。</li>
<li>methods: 本文使用了层normalization（LN）和 feature normalization（FN）来控制feature collapse和本地适应，以提高 federated learning 的 globally training。</li>
<li>results: 实验结果表明，normalization 可以在 extreme label shift 下导致 drastic 的改善，并且对 learning rate 的选择有关键作用。<details>
<summary>Abstract</summary>
Layer normalization (LN) is a widely adopted deep learning technique especially in the era of foundation models. Recently, LN has been shown to be surprisingly effective in federated learning (FL) with non-i.i.d. data. However, exactly why and how it works remains mysterious. In this work, we reveal the profound connection between layer normalization and the label shift problem in federated learning. To understand layer normalization better in FL, we identify the key contributing mechanism of normalization methods in FL, called feature normalization (FN), which applies normalization to the latent feature representation before the classifier head. Although LN and FN do not improve expressive power, they control feature collapse and local overfitting to heavily skewed datasets, and thus accelerates global training. Empirically, we show that normalization leads to drastic improvements on standard benchmarks under extreme label shift. Moreover, we conduct extensive ablation studies to understand the critical factors of layer normalization in FL. Our results verify that FN is an essential ingredient inside LN to significantly improve the convergence of FL while remaining robust to learning rate choices, especially under extreme label shift where each client has access to few classes.
</details>
<details>
<summary>摘要</summary>
层normalization（LN）是深度学习中广泛采用的技术，尤其在基础模型时代。最近，LN在非独立数据（Federated Learning，FL）中表现出意外的有效性，但具体的原因和如何工作仍然不清楚。在这项工作中，我们揭示了层normalization和FL中标签变化问题之间的深刻关系。为了更好地理解层normalization在FL中，我们识别了FL中normalization方法的关键贡献机制，称为特征normalization（FN），它在批处头前应用normalization于隐藏特征表示。虽然LN和FN不会提高表达力，但它们控制特征塌积和本地适应，从而加速全局训练。我们的实验表明，normalization在极端标签变化下带来了很大的改善。此外，我们进行了广泛的减少研究，以了解层normalization在FL中关键的因素。我们的结果表明，FN是LN中重要的组成部分，可以在极端标签变化下提高FL的凝固性，尤其是当每个客户只有几个类时。
</details></li>
</ul>
<hr>
<h2 id="Eigenvalue-based-Incremental-Spectral-Clustering"><a href="#Eigenvalue-based-Incremental-Spectral-Clustering" class="headerlink" title="Eigenvalue-based Incremental Spectral Clustering"></a>Eigenvalue-based Incremental Spectral Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10999">http://arxiv.org/abs/2308.10999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek, Bartłmiej Starosta, Sławomir T. Wierzchoń</li>
<li>for:  clustering large datasets using incremental spectral clustering</li>
<li>methods:  split the data into manageable subsets, cluster each subset, and merge clusters based on eigenvalue spectrum similarity</li>
<li>results:  clustering and merging the subsets yields clusters close to clustering the entire dataset<details>
<summary>Abstract</summary>
Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
</details>
<details>
<summary>摘要</summary>
我们之前的实验表明，子集合（短文档）的聚合spectrum（卷积 Laplacian）具有一定的共同normalized方式。基于这一点，我们提出了一种增量 spectral clustering 方法。该方法包括以下步骤：（1）将数据分割成可管理的子集，（2）对每个子集进行 clustering，（3）根据聚合spectrum的相似性，将不同子集的cluster合并形成整个数据集的cluster。这种方法可以特别有用于数据样本规模增长的情况下，如典型的spectral clustering一样。我们的实验表明，将 subsets 分割并 merge 后的 clusters 与整个数据集的 clustering 几乎相同。
</details></li>
</ul>
<hr>
<h2 id="Attesting-Distributional-Properties-of-Training-Data-for-Machine-Learning"><a href="#Attesting-Distributional-Properties-of-Training-Data-for-Machine-Learning" class="headerlink" title="Attesting Distributional Properties of Training Data for Machine Learning"></a>Attesting Distributional Properties of Training Data for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09552">http://arxiv.org/abs/2308.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasisht Duddu, Anudeep Das, Nora Khayata, Hossein Yalame, Thomas Schneider, N. Asokan</li>
<li>for:  Ensuring the trustworthiness of machine learning models by demonstrating desirable distributional properties of training data.</li>
<li>methods:  Property inference and cryptographic mechanisms for data privacy-preserving property attestation.</li>
<li>results:  An effective hybrid property attestation method for model trainers to demonstrate relevant distributional properties of training data to customers without revealing the data.<details>
<summary>Abstract</summary>
The success of machine learning (ML) has been accompanied by increased concerns about its trustworthiness. Several jurisdictions are preparing ML regulatory frameworks. One such concern is ensuring that model training data has desirable distributional properties for certain sensitive attributes. For example, draft regulations indicate that model trainers are required to show that training datasets have specific distributional properties, such as reflecting diversity of the population.   We propose the notion of property attestation allowing a prover (e.g., model trainer) to demonstrate relevant distributional properties of training data to a verifier (e.g., a customer) without revealing the data. We present an effective hybrid property attestation combining property inference with cryptographic mechanisms.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）的成功也导致了其可靠性的关注。一些司法管辖区正在制定ML规则。一个如此关注点是确保模型训练数据具有特定敏感属性的恰当分布特性。例如，草拟法规要求模型训练人员证明训练数据具有特定的分布特性，如反映人口多样性。我们提出了属性证明的想法，允许证明人（例如模型训练人员）在不披露数据的情况下，向验证人（例如客户）证明训练数据具有相关的分布特性。我们介绍了一种有效的混合属性证明方法，结合属性推理和 крип加密机制。
</details></li>
</ul>
<hr>
<h2 id="Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning"><a href="#Adapt-Your-Teacher-Improving-Knowledge-Distillation-for-Exemplar-free-Continual-Learning" class="headerlink" title="Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning"></a>Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09544">http://arxiv.org/abs/2308.09544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filip Szatkowski, Mateusz Pyla, Marcin Przewięźlikowski, Sebastian Cygert, Bartłomiej Twardowski, Tomasz Trzciński</li>
<li>for: 这篇研究文章的目的是探讨例外自由类别增量学习（CIL）中的知识传递（KD）作为调节策略，以预防忘记。</li>
<li>methods: 这篇文章使用了KD作为调节策略，但是它们经常无法调节模型不具有先前任务的例子。我们的分析表明，这问题源于教师网络对于非标准资料的重大表现变化。这会导致KD损失成分中的大量错误，导致CIL中的表现下降。</li>
<li>results: 我们引入了教师适应（TA），一种同时更新教师网络和主要模型的方法，以实现CIL中的例外自由类别增量学习。我们的方法与KD-based CIL方法相互运作，并在多个例外自由CIL标准 benchmark上提供了一致的性能提升。<details>
<summary>Abstract</summary>
In this work, we investigate exemplar-free class incremental learning (CIL) with knowledge distillation (KD) as a regularization strategy, aiming to prevent forgetting. KD-based methods are successfully used in CIL, but they often struggle to regularize the model without access to exemplars of the training data from previous tasks. Our analysis reveals that this issue originates from substantial representation shifts in the teacher network when dealing with out-of-distribution data. This causes large errors in the KD loss component, leading to performance degradation in CIL. Inspired by recent test-time adaptation methods, we introduce Teacher Adaptation (TA), a method that concurrently updates the teacher and the main model during incremental training. Our method seamlessly integrates with KD-based CIL approaches and allows for consistent enhancement of their performance across multiple exemplar-free CIL benchmarks.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究无 exemplar 的类增量学习（CIL），并用知识塑化（KD）作为规范策略，以避免忘记。 KD 基本方法在 CIL 中得到了成功，但它们经常在不同任务的数据上遇到表示转移的问题，导致模型训练不稳定。我们的分析表明，这一问题源于老师网络处理异常数据时的重大表示转移，导致 KD 损失函数中的大误差，从而导致 CIL 性能下降。鉴于这一点，我们引入教师适应（TA）方法，该方法在增量训练中同时更新老师网络和主模型。我们的方法顺利地与 KD 基本方法结合，并允许在多个无 exemplar 的 CIL benchmark上进行一致增强性表现。
</details></li>
</ul>
<hr>
<h2 id="Latent-State-Models-of-Training-Dynamics"><a href="#Latent-State-Models-of-Training-Dynamics" class="headerlink" title="Latent State Models of Training Dynamics"></a>Latent State Models of Training Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09543">http://arxiv.org/abs/2308.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Y. Hu, Angelica Chen, Naomi Saphra, Kyunghyun Cho</li>
<li>for: 本研究旨在理解Randomness对模型训练的影响，包括不同数据顺序和初始化方法对模型训练的影响，以及这些影响如何导致模型训练过程中的不同演进和结果。</li>
<li>methods: 本研究使用多个随机种子训练模型，并计算训练过程中的多个指标，如模型权重的$L_2$范数、平均值和方差。然后，使用隐马尔可夫模型（HMM）来模型训练过程，将训练过程看作一种随机过程，并从而获得低维、离散的训练动力学表示。</li>
<li>results: 本研究使用HMM表示法描述了不同训练过程的训练动力学，并发现了一些稳定演进和慢速演进的特征。此外，研究还发现了一些”拐弯”状态，这些状态可能会降低模型的收敛速率。<details>
<summary>Abstract</summary>
The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent "detour" states that slow down convergence.
</details>
<details>
<summary>摘要</summary>
“随机的影响在模型训练中仍然不够清楚。实际上，不同的数据顺序和初始化方式会如何对模型的训练造成影响，使一些训练运行比其他优化得更快或更好？此外，我们如何解释训练过程中的结果和阶段转换，以及它们对不同的训练运行所造成的影响？为了理解随机的影响，我们在不同的随机seed中训练模型多次，并计算训练过程中的一些指标，例如模型的$L_2$ нор和平均值。然后，我们使用隐藏Marker模型（HMM）来描述训练过程，将训练过程看作一种随机过程，从而获得训练过程的低维、组合表示。使用我们的方法，我们可以研究训练过程中的阶段转换和潜在的“停顿”状态，以及它们对不同的训练运行所造成的影响。”Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection"><a href="#Decoupled-conditional-contrastive-learning-with-variable-metadata-for-prostate-lesion-detection" class="headerlink" title="Decoupled conditional contrastive learning with variable metadata for prostate lesion detection"></a>Decoupled conditional contrastive learning with variable metadata for prostate lesion detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09542">http://arxiv.org/abs/2308.09542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/camilleruppli/decoupled_ccl">https://github.com/camilleruppli/decoupled_ccl</a></li>
<li>paper_authors: Camille Ruppli, Pietro Gori, Roberto Ardon, Isabelle Bloch<br>for: 这个研究的目的是提高肠癌早期诊断的精度。methods: 这篇论文使用多 parametric 磁共振成像（mp-MRI）进行肠癌涂敷识别。它还使用了报告和数据系统（PI-RADS）来标准化肠MRI的解释，并利用了多个注解者每个样本的多个注解来增强metadata的信任度。results: 这篇论文report了使用这种新的对比损失函数，在PI-CAI挑战数据集上提高了肠癌涂敷识别的准确率3%。<details>
<summary>Abstract</summary>
Early diagnosis of prostate cancer is crucial for efficient treatment. Multi-parametric Magnetic Resonance Images (mp-MRI) are widely used for lesion detection. The Prostate Imaging Reporting and Data System (PI-RADS) has standardized interpretation of prostate MRI by defining a score for lesion malignancy. PI-RADS data is readily available from radiology reports but is subject to high inter-reports variability. We propose a new contrastive loss function that leverages weak metadata with multiple annotators per sample and takes advantage of inter-reports variability by defining metadata confidence. By combining metadata of varying confidence with unannotated data into a single conditional contrastive loss function, we report a 3% AUC increase on lesion detection on the public PI-CAI challenge dataset.   Code is available at: https://github.com/camilleruppli/decoupled_ccl
</details>
<details>
<summary>摘要</summary>
早期诊断 простаতheimer 癌是非常重要，以便有效地治疗。多 Parametric Magnetic Resonance Image (mp-MRI) 广泛用于肿瘤检测。Prostate Imaging Reporting and Data System (PI-RADS) 已经标准化了 простаvat MRI 的解释，并定义了肿瘤凶猛程度的分数。PI-RADS 数据ready available from radiology reports，但是受到高度的 inter-reports 变化。我们提议一种新的对比损失函数，利用weak metadata 和多个 annotators per sample，并利用 inter-reports 变化来定义metadata confidence。通过将 metadata of varying confidence 与未注释数据合并为一个conditional contrastive loss function，我们报告了PI-CAI challenge dataset上的 lesion detection 的3% AUC 提高。代码可以在：https://github.com/camilleruppli/decoupled_ccl 中找到。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Preserving-3-Layer-Neural-Network-Training-using-Mere-Homomorphic-Encryption-Technique"><a href="#Privacy-Preserving-3-Layer-Neural-Network-Training-using-Mere-Homomorphic-Encryption-Technique" class="headerlink" title="Privacy-Preserving 3-Layer Neural Network Training using Mere Homomorphic Encryption Technique"></a>Privacy-Preserving 3-Layer Neural Network Training using Mere Homomorphic Encryption Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09531">http://arxiv.org/abs/2308.09531</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Chiang</li>
<li>for: 这篇论文处理了在单纯复制算法设定下进行隐私保护的神经网络训练问题。</li>
<li>methods: 本论文结合了多种现有技术，并将其扩展和改进，最终实现了使用单纯复制算法进行3层神经网络的训练，并解决了回归和分类问题。</li>
<li>results: 本论文通过实验证明，可以使用单纯复制算法进行隐私保护的神经网络训练，并且可以解决回归和分类问题。<details>
<summary>Abstract</summary>
In this manuscript, we consider the problem of privacy-preserving training of neural networks in the mere homomorphic encryption setting. We combine several exsiting techniques available, extend some of them, and finally enable the training of 3-layer neural networks for both the regression and classification problems using mere homomorphic encryption technique.
</details>
<details>
<summary>摘要</summary>
在这个手稿中，我们考虑了使用简单卷积Encryption（MHE）的 neural network训练 privacy保护问题。我们将现有的技术综合使用，部分扩展，最终实现了使用 MHE 技术训练三层神经网络，用于回归和分类问题。Here's a breakdown of the translation:* 在这个手稿中 (in this manuscript) - This phrase is used to indicate that the text is part of a larger document or manuscript.* 我们考虑了 (we consider) - This verb is used to indicate that the authors are thinking about or discussing a particular topic.* 使用简单卷积Encryption (MHE) (using simple homomorphic encryption) - This phrase indicates that the authors are using a specific type of encryption called "simple homomorphic encryption" (简单卷积Encryption).* 神经网络训练 (neural network training) - This phrase indicates that the authors are training a neural network.* privacy保护 (privacy protection) - This phrase indicates that the authors are concerned with protecting the privacy of the data used for training the neural network.* 问题 (problem) - This word is used to indicate that the authors are trying to solve a specific problem or challenge.* 我们将现有的技术综合使用 (we will combine existing techniques) - This phrase indicates that the authors will use a combination of existing techniques to solve the problem.* 部分扩展 (extending some of them) - This phrase indicates that the authors will extend some of the existing techniques to solve the problem.* 最终实现了 (finally enabled) - This phrase indicates that the authors were able to achieve their goal of training a neural network using the combination of techniques.* 使用 MHE 技术 (using MHE technique) - This phrase indicates that the authors used the simple homomorphic encryption technique to train the neural network.* 训练三层神经网络 (training a three-layer neural network) - This phrase indicates that the authors trained a neural network with three layers.* 用于回归和分类问题 (for regression and classification problems) - This phrase indicates that the trained neural network can be used for both regression and classification tasks.
</details></li>
</ul>
<hr>
<h2 id="Transitivity-Preserving-Graph-Representation-Learning-for-Bridging-Local-Connectivity-and-Role-based-Similarity"><a href="#Transitivity-Preserving-Graph-Representation-Learning-for-Bridging-Local-Connectivity-and-Role-based-Similarity" class="headerlink" title="Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity"></a>Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09517">http://arxiv.org/abs/2308.09517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nslab-cuk/unified-graph-transformer">https://github.com/nslab-cuk/unified-graph-transformer</a></li>
<li>paper_authors: Van Thuy Hoang, O-Joun Lee</li>
<li>for: This paper aims to improve graph representation learning methods by integrating local and global structural information into fixed-length vector representations.</li>
<li>methods: The proposed Unified Graph Transformer Networks (UGT) learn local structure by identifying local substructures and aggregating features of the $k$-hop neighborhoods of each node, and construct virtual edges to capture long-range dependencies. UGT also learns unified representations through self-attention, encoding structural distance and $p$-step transition probability between node pairs.</li>
<li>results: The proposed method significantly outperformed baselines that consist of state-of-the-art models on real-world benchmark datasets over various downstream tasks, and reached the expressive power of the third-order Weisfeiler-Lehman isomorphism test (3d-WL) in distinguishing non-isomorphic graph pairs.<details>
<summary>Abstract</summary>
Graph representation learning (GRL) methods, such as graph neural networks and graph transformer models, have been successfully used to analyze graph-structured data, mainly focusing on node classification and link prediction tasks. However, the existing studies mostly only consider local connectivity while ignoring long-range connectivity and the roles of nodes. In this paper, we propose Unified Graph Transformer Networks (UGT) that effectively integrate local and global structural information into fixed-length vector representations. First, UGT learns local structure by identifying the local substructures and aggregating features of the $k$-hop neighborhoods of each node. Second, we construct virtual edges, bridging distant nodes with structural similarity to capture the long-range dependencies. Third, UGT learns unified representations through self-attention, encoding structural distance and $p$-step transition probability between node pairs. Furthermore, we propose a self-supervised learning task that effectively learns transition probability to fuse local and global structural features, which could then be transferred to other downstream tasks. Experimental results on real-world benchmark datasets over various downstream tasks showed that UGT significantly outperformed baselines that consist of state-of-the-art models. In addition, UGT reaches the expressive power of the third-order Weisfeiler-Lehman isomorphism test (3d-WL) in distinguishing non-isomorphic graph pairs. The source code is available at https://github.com/NSLab-CUK/Unified-Graph-Transformer.
</details>
<details>
<summary>摘要</summary>
GRaph representation learning (GRL) 方法，如图神经网络和图变换模型，已经成功地分析图形结构数据，主要关注节点分类和链接预测任务。然而，现有的研究通常只考虑本地连接，而忽略远程连接和节点的角色。在这篇论文中，我们提出了统一图 transformer 网络 (UGT)，可以有效地 integrate 本地和远程结构信息到固定长度 вектор表示中。首先，UGT 通过标识本地子结构和 $k $- hop 邻居的特征汇集来学习本地结构。其次，我们通过构建虚拟边，将远程节点与结构相似的节点相连，以捕捉远程依赖关系。最后，UGT 通过自我注意力学习，编码结构距离和 $p $- step 过渡概率 между 节点对，学习统一表示。此外，我们提出了一种自适应学习任务，可以有效地学习 transition probability，并将本地和远程结构特征融合，以便在其他下游任务中转移。实验结果表明，UGT 在真实世界 benchmark 数据上显著超越基eline，并达到了第三个 Weisfeiler-Lehman 同态测试 (3d-WL) 的表达力，在分辨非同态图对中表现出优异。代码可以在 <https://github.com/NSLab-CUK/Unified-Graph-Transformer> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning"><a href="#Spatial-LibriSpeech-An-Augmented-Dataset-for-Spatial-Audio-Learning" class="headerlink" title="Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning"></a>Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09514">http://arxiv.org/abs/2308.09514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-spatial-librispeech">https://github.com/apple/ml-spatial-librispeech</a></li>
<li>paper_authors: Miguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, Jonathan Sheaffer</li>
<li>for: 用于机器学习模型训练</li>
<li>methods: 使用了LibriSpeech采样并通过200k+的 simulated acoustic conditions和8k+的合成房间来生成数据集</li>
<li>results: 训练四个空间声音任务后， median absolute error为6.60{\deg} 3D源 localization，0.43m distance，90.66ms T30，和2.74dB DRR estimation，同时模型可以通过广泛使用的评估数据集进行推广<details>
<summary>Abstract</summary>
We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60{\deg} on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on DRR estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43{\deg} on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge.
</details>
<details>
<summary>摘要</summary>
我们介绍的空间听说库SpatiLibriSpeech，包含650多小时19个渠道声音数据，首次投影束聚合和可选择干扰噪音。SpatiLibriSpeech是为机器学习模型训练而设计，其包括源位置、说话方向、室内声学和几何学标签。SpatiLibriSpeech通过对LibriSpeech样本进行扩展，生成了200000+个 simulate室内声音条件和8000+个synthetic室内。为了证明我们的数据集的实用性，我们在四个空间听说任务上训练了模型，其中 median absolute error为6.60度、0.43米、90.66毫秒和2.74dB。我们还表明了这些模型在广泛使用的评估数据集上也具有良好的泛化能力，例如在TUT Sound Events 2018中 median absolute error为12.43度、ACE Challenge中T30估计为157.32毫秒。
</details></li>
</ul>
<hr>
<h2 id="Bridged-GNN-Knowledge-Bridge-Learning-for-Effective-Knowledge-Transfer"><a href="#Bridged-GNN-Knowledge-Bridge-Learning-for-Effective-Knowledge-Transfer" class="headerlink" title="Bridged-GNN: Knowledge Bridge Learning for Effective Knowledge Transfer"></a>Bridged-GNN: Knowledge Bridge Learning for Effective Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09499">http://arxiv.org/abs/2308.09499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wendong Bi, Xueqi Cheng, Bingbing Xu, Xiaoqian Sun, Li Xu, Huawei Shen</li>
<li>for: 解决数据缺乏和低质量问题，帮助深度学习模型在Target领域中提高性能。</li>
<li>methods: 基于Graph Neural Networks (GNNs)的知识抽象技术，通过构建 Bridge-Graph 来学习Target领域的知识背景，然后通过GNNs进行样本知识传递。</li>
<li>results: 对于不同的数据领域和数据质量，Bridged-GNN 表现出显著的改善，与State-of-the-Art方法相比。<details>
<summary>Abstract</summary>
The data-hungry problem, characterized by insufficiency and low-quality of data, poses obstacles for deep learning models. Transfer learning has been a feasible way to transfer knowledge from high-quality external data of source domains to limited data of target domains, which follows a domain-level knowledge transfer to learn a shared posterior distribution. However, they are usually built on strong assumptions, e.g., the domain invariant posterior distribution, which is usually unsatisfied and may introduce noises, resulting in poor generalization ability on target domains. Inspired by Graph Neural Networks (GNNs) that aggregate information from neighboring nodes, we redefine the paradigm as learning a knowledge-enhanced posterior distribution for target domains, namely Knowledge Bridge Learning (KBL). KBL first learns the scope of knowledge transfer by constructing a Bridged-Graph that connects knowledgeable samples to each target sample and then performs sample-wise knowledge transfer via GNNs.KBL is free from strong assumptions and is robust to noises in the source data. Guided by KBL, we propose the Bridged-GNN} including an Adaptive Knowledge Retrieval module to build Bridged-Graph and a Graph Knowledge Transfer module. Comprehensive experiments on both un-relational and relational data-hungry scenarios demonstrate the significant improvements of Bridged-GNN compared with SOTA methods
</details>
<details>
<summary>摘要</summary>
“问题集”， caracterized by “欠缺和低质量的数据”， poses obstacles for deep learning models. “传入学习” has been a feasible way to transfer knowledge from high-quality external data of source domains to limited data of target domains, which follows a domain-level knowledge transfer to learn a shared posterior distribution. However, they are usually built on strong assumptions, e.g., the domain invariant posterior distribution, which is usually unsatisfied and may introduce noises, resulting in poor generalization ability on target domains.Inspired by Graph Neural Networks (GNNs) that aggregate information from neighboring nodes, we redefine the paradigm as learning a knowledge-enhanced posterior distribution for target domains, namely Knowledge Bridge Learning (KBL). KBL first learns the scope of knowledge transfer by constructing a Bridged-Graph that connects knowledgeable samples to each target sample and then performs sample-wise knowledge transfer via GNNs.KBL is free from strong assumptions and is robust to noises in the source data.Guided by KBL, we propose the Bridged-GNN, including an Adaptive Knowledge Retrieval module to build Bridged-Graph and a Graph Knowledge Transfer module. Comprehensive experiments on both un-relational and relational data-hungry scenarios demonstrate the significant improvements of Bridged-GNN compared with SOTA methods.
</details></li>
</ul>
<hr>
<h2 id="Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication"><a href="#Predictive-Authoring-for-Brazilian-Portuguese-Augmentative-and-Alternative-Communication" class="headerlink" title="Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication"></a>Predictive Authoring for Brazilian Portuguese Augmentative and Alternative Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09497">http://arxiv.org/abs/2308.09497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jayralencar/pictogram_prediction_pt">https://github.com/jayralencar/pictogram_prediction_pt</a></li>
<li>paper_authors: Jayr Pereira, Rodrigo Nogueira, Cleber Zanchettin, Robson Fidalgo</li>
<li>For: This paper proposes using a BERT-like model for pictogram prediction in AAC systems to improve the efficiency of message authoring for individuals with complex communication needs.* Methods: The authors finetune BERTimbau, a Brazilian Portuguese version of BERT, using an AAC corpus for Brazilian Portuguese, and test different approaches to representing a pictogram for prediction, including as a word, as a concept, and as a set of synonyms. They also evaluate the usage of images for pictogram prediction.* Results: The results demonstrate that using embeddings computed from the pictograms’ caption, synonyms, or definitions have a similar performance, with using synonyms leading to lower perplexity but using captions leading to the highest accuracies. The paper provides insight into how to represent a pictogram for prediction using a BERT-like model and the potential of using images for pictogram prediction.Here is the information in Simplified Chinese text:* For: 这篇论文提出使用基于BERT的模型来提高AAC系统中图文推理的效率，以便更好地满足复杂通信需求的个体需要。* Methods: 作者们使用了一个特定于巴西葡萄牙语的BERT版本（BERTimbau）进行finetuning，并使用巴西葡萄牙语AAC corpus进行训练。他们还测试了不同的图文表示方法，包括作为单词、作为概念和作为相关词的方法。此外，他们还评估了使用图像进行图文预测的可能性。* Results: 结果表明，使用图文caption、synonyms或definition中的嵌入都有类似的性能。使用synonyms导致词语精度更低，但使用caption导致最高准确率。这篇论文提供了使用基于BERT的模型来表示图文的预测方法的可能性，以及使用图像进行图文预测的潜在可能性。<details>
<summary>Abstract</summary>
Individuals with complex communication needs (CCN) often rely on augmentative and alternative communication (AAC) systems to have conversations and communique their wants. Such systems allow message authoring by arranging pictograms in sequence. However, the difficulty of finding the desired item to complete a sentence can increase as the user's vocabulary increases. This paper proposes using BERTimbau, a Brazilian Portuguese version of BERT, for pictogram prediction in AAC systems. To finetune BERTimbau, we constructed an AAC corpus for Brazilian Portuguese to use as a training corpus. We tested different approaches to representing a pictogram for prediction: as a word (using pictogram captions), as a concept (using a dictionary definition), and as a set of synonyms (using related terms). We also evaluated the usage of images for pictogram prediction. The results demonstrate that using embeddings computed from the pictograms' caption, synonyms, or definitions have a similar performance. Using synonyms leads to lower perplexity, but using captions leads to the highest accuracies. This paper provides insight into how to represent a pictogram for prediction using a BERT-like model and the potential of using images for pictogram prediction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models"><a href="#Balancing-Transparency-and-Risk-The-Security-and-Privacy-Risks-of-Open-Source-Machine-Learning-Models" class="headerlink" title="Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models"></a>Balancing Transparency and Risk: The Security and Privacy Risks of Open-Source Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09490">http://arxiv.org/abs/2308.09490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Hintersdorf, Lukas Struppek, Kristian Kersting</li>
<li>for: 本研究旨在提醒开源机器学习模型使用者关注其隐私和安全问题。</li>
<li>methods: 本文提出了一种概述开源模型常见的隐私和安全威胁的方法，以提高开源模型的安全使用。</li>
<li>results: 本研究发现了一些可能的隐私和安全攻击方法，包括模型中隐藏的功能和特定输入模式触发的攻击方法。<details>
<summary>Abstract</summary>
The field of artificial intelligence (AI) has experienced remarkable progress in recent years, driven by the widespread adoption of open-source machine learning models in both research and industry. Considering the resource-intensive nature of training on vast datasets, many applications opt for models that have already been trained. Hence, a small number of key players undertake the responsibility of training and publicly releasing large pre-trained models, providing a crucial foundation for a wide range of applications. However, the adoption of these open-source models carries inherent privacy and security risks that are often overlooked. To provide a concrete example, an inconspicuous model may conceal hidden functionalities that, when triggered by specific input patterns, can manipulate the behavior of the system, such as instructing self-driving cars to ignore the presence of other vehicles. The implications of successful privacy and security attacks encompass a broad spectrum, ranging from relatively minor damage like service interruptions to highly alarming scenarios, including physical harm or the exposure of sensitive user data. In this work, we present a comprehensive overview of common privacy and security threats associated with the use of open-source models. By raising awareness of these dangers, we strive to promote the responsible and secure use of AI systems.
</details>
<details>
<summary>摘要</summary>
artifical intelligence（AI）领域在近年来已经取得了很大的进步，这主要归功于研究和业界广泛采用开源机器学习模型。由于训练大量数据集的资源占用性很高，许多应用程序选择使用已经训练好的模型。因此，只有一些关键 иг主要负责训练和公共发布大型预训练模型，提供了广泛应用的基础。然而，使用这些开源模型的采用带来了内置的隐私和安全风险，这些风险frequently overlooked。作为一个具体的例子，一个不起眼的模型可能隐藏潜在的功能，当特定的输入模式触发时，可以 manipulate the behavior of the system，如 instructing self-driving cars to ignore the presence of other vehicles。成功的隐私和安全攻击的影响范围广泛，从relatively minor damage like service interruptions到highly alarming scenarios，包括物理伤害或披露敏感用户数据。在这种工作中，我们提供了对开源模型常见隐私和安全威胁的全面回顾。通过提高对这些危险的意识，我们希望推动AI系统的负责任和安全使用。
</details></li>
</ul>
<hr>
<h2 id="RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition"><a href="#RBA-GCN-Relational-Bilevel-Aggregation-Graph-Convolutional-Network-for-Emotion-Recognition" class="headerlink" title="RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition"></a>RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11029">http://arxiv.org/abs/2308.11029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luftmenscher/RBA-GCN">https://github.com/luftmenscher/RBA-GCN</a></li>
<li>paper_authors: Lin Yuan, Guoheng Huang, Fenghuan Li, Xiaochen Yuan, Chi-Man Pun, Guo Zhong</li>
<li>for: 本研究旨在提高对话中情感认知（ERC）的性能，通过基于图 convolutional networks（GCNs）的模型来解决传统GCNs中的节点信息重复问题，以及单层GCNs缺乏捕捉长距离上下文信息的能力。</li>
<li>methods: 本研究提出了一种基于图生成模块（GGM）、相似性基于团建模块（SCBM）和双层汇集模块（BiAM）的图 convolutional network（RBA-GCN），以解决传统GCNs中节点信息重复问题和单层GCNs缺乏捕捉长距离上下文信息的问题。</li>
<li>results: 在IEMOCAP和MELD数据集上，RBA-GCN的Weighted Average F1 score比最先进的方法提高2.17%∼5.21%。<details>
<summary>Abstract</summary>
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation module (BiAM). First, GGM constructs a novel graph to reduce the redundancy of target node information. Then, SCBM calculates the node similarity in the target node and its structural neighborhood, where noisy information with low similarity is filtered out to preserve the discriminant information of the node. Meanwhile, BiAM is a novel aggregation method that can preserve the information of nodes during the aggregation process. This module can construct the interaction between different modalities and capture long-range contextual information based on similarity clusters. On both the IEMOCAP and MELD datasets, the weighted average F1 score of RBA-GCN has a 2.17$\sim$5.21\% improvement over that of the most advanced method.
</details>
<details>
<summary>摘要</summary>
“对话情感识别”（ERC）在研究者中获得了越来越多的注意力，因为它有广泛的应用领域。由于对话有自然的图形结构，许多方法使用图形卷积网（GCNs）来模型ERC，它们已经获得了显著的成果。然而，传统GCNs的聚合方法受到节点资讯重复问题的影响，导致节点标识资讯的损失。另外，单层GCNs缺乏获取长距离Contextual信息的能力。此外，大多数方法仅基于文本模式或是将不同模式组合在一起，从而导致模式之间的互动缺乏能力。为了解决这些问题，我们提出了关系内部卷积网（RBA-GCN），它包括三个模块：图形生成模块（GGM）、相似度基于单元模块（SCBM）和两层聚合模块（BiAM）。首先，GGM使用一个新的图形来减少目标节点信息的重复性。然后，SCBM计算了目标节点和其结构邻域中的相似度，并将低相似度的信息排除以保留节点标识资讯。同时，BiAM是一个新的聚合方法，可以在聚合过程中保留节点信息。这个模块可以在不同模式之间建立互动，并以相似 clusters 来捕捉长距离Contextual信息。在IEMOCAP和MELD datasets上，RBA-GCN的Weighted average F1 score与最先进方法相比，有2.17%至5.21%的提升。
</details></li>
</ul>
<hr>
<h2 id="Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning"><a href="#Data-augmentation-and-explainability-for-bias-discovery-and-mitigation-in-deep-learning" class="headerlink" title="Data augmentation and explainability for bias discovery and mitigation in deep learning"></a>Data augmentation and explainability for bias discovery and mitigation in deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09464">http://arxiv.org/abs/2308.09464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła</li>
<li>for: 本论文探讨深度神经网络中偏见的影响和降低其影响的方法。</li>
<li>methods: 本论文提出了三种方法来降低偏见的影响：样式传输数据增强、targeted数据增强和负责任反馈。</li>
<li>results: 本论文通过实验表明，这些方法可以降低深度神经网络中偏见的影响，提高模型的准确率。<details>
<summary>Abstract</summary>
This dissertation explores the impact of bias in deep neural networks and presents methods for reducing its influence on model performance. The first part begins by categorizing and describing potential sources of bias and errors in data and models, with a particular focus on bias in machine learning pipelines. The next chapter outlines a taxonomy and methods of Explainable AI as a way to justify predictions and control and improve the model. Then, as an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as an alternative semi-automatic approach to manual data exploration for discovering potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of the identified biases on the model. Whereas identifying errors and bias is critical, improving the model and reducing the number of flaws in the future is an absolute priority. Hence, the second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed: Style Transfer Data Augmentation, Targeted Data Augmentations, and Attribution Feedback. Style Transfer Data Augmentation aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one. Targeted Data Augmentations randomly insert possible biases into all images in the dataset during the training, as a way to make the process random and, thus, destroy spurious correlations. Lastly, Attribution Feedback is used to fine-tune the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss. The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details>
<details>
<summary>摘要</summary>
Then, as an example of a laborious manual data inspection and bias discovery process, a skin lesion dataset is manually examined. A Global Explanation for the Bias Identification method is proposed as an alternative semi-automatic approach to manual data exploration for discovering potential biases in data. Relevant numerical methods and metrics are discussed for assessing the effects of the identified biases on the model.Whereas identifying errors and bias is critical, improving the model and reducing the number of flaws in the future is an absolute priority. Hence, the second part of the thesis focuses on mitigating the influence of bias on ML models. Three approaches are proposed and discussed: Style Transfer Data Augmentation, Targeted Data Augmentations, and Attribution Feedback.Style Transfer Data Augmentation aims to address shape and texture bias by merging a style of a malignant lesion with a conflicting shape of a benign one. Targeted Data Augmentations randomly insert possible biases into all images in the dataset during the training, as a way to make the process random and, thus, destroy spurious correlations. Lastly, Attribution Feedback is used to fine-tune the model to improve its accuracy by eliminating obvious mistakes and teaching it to ignore insignificant input parts via an attribution loss. The goal of these approaches is to reduce the influence of bias on machine learning models, rather than eliminate it entirely.
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-S-matrix-Phases-with-Machine-Learning"><a href="#Reconstructing-S-matrix-Phases-with-Machine-Learning" class="headerlink" title="Reconstructing $S$-matrix Phases with Machine Learning"></a>Reconstructing $S$-matrix Phases with Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09451">http://arxiv.org/abs/2308.09451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aurélien Dersy, Matthew D. Schwartz, Alexander Zhiboedov</li>
<li>for: 这篇论文是关于$S$-矩阵 bootstrap 计划中对幂谱和相位之间的关系的研究。</li>
<li>methods: 作者使用现代机器学习技术来研究单位约束。他们发现，对于给定的幂谱，当存在相位时，可以通过机器学习技术准确重建相位。此外，损失函数估算算法可以用作判断给定幂谱是否与单位约束相容的准确指标。</li>
<li>results: 作者发现，在某些情况下，多个相位可能与单个幂谱相容，并发现了一种新的相位ambiguous 解。此外，他们发现这种解可以让单位约束的已知限制得到明显改善。<details>
<summary>Abstract</summary>
An important element of the $S$-matrix bootstrap program is the relationship between the modulus of an $S$-matrix element and its phase. Unitarity relates them by an integral equation. Even in the simplest case of elastic scattering, this integral equation cannot be solved analytically and numerical approaches are required. We apply modern machine learning techniques to studying the unitarity constraint. We find that for a given modulus, when a phase exists it can generally be reconstructed to good accuracy with machine learning. Moreover, the loss of the reconstruction algorithm provides a good proxy for whether a given modulus can be consistent with unitarity at all. In addition, we study the question of whether multiple phases can be consistent with a single modulus, finding novel phase-ambiguous solutions. In particular, we find a new phase-ambiguous solution which pushes the known limit on such solutions significantly beyond the previous bound.
</details>
<details>
<summary>摘要</summary>
“$S$-matrixbootstrap程序中一个重要元素是几何和几何的关系。对几何的modulus，对它的相位也存在一个统计方程。即使是最简的反射散射案例，这个统计方程无法分析解，需要使用数值方法。我们使用现代机器学习技术来研究单位约束。我们发现，给定的几何，当一个相位存在时，通常可以使用机器学习重建它，并且损失函数提供了一个单位约束是否成立的好proxy。此外，我们研究了是否多个相位可以与单一的几何相容，发现了新的相位不确定解。特别是，我们发现了一个新的相位不确定解，将已知的限制Push beyond the previous bound。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances"><a href="#Logistics-Hub-Location-Optimization-A-K-Means-and-P-Median-Model-Hybrid-Approach-Using-Road-Network-Distances" class="headerlink" title="Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances"></a>Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11038">http://arxiv.org/abs/2308.11038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdul Rahman, Muhammad Aamir Basheer, Zubair Khalid, Muhammad Tahir, Momin Uppal</li>
<li>for: 优化快递总站的位置，以提高电商业务的效率和环保性。</li>
<li>methods: 该研究使用权重P-Median方法和K-Means clustering方法，将配送点按照其空间位置归类，然后使用P-Median方法决定最佳的快递总站位置。</li>
<li>results: 结果显示，使用优化后的快递总站位置，每个配送 distances 可以减少 815（10%）米。<details>
<summary>Abstract</summary>
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&P) is used to demonstrate the effectiveness of the approach. Serving deliveries from the optimal hub locations results in the saving of 815 (10%) meters per delivery.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Defending-Label-Inference-Attacks-in-Split-Learning-under-Regression-Setting"><a href="#Defending-Label-Inference-Attacks-in-Split-Learning-under-Regression-Setting" class="headerlink" title="Defending Label Inference Attacks in Split Learning under Regression Setting"></a>Defending Label Inference Attacks in Split Learning under Regression Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09448">http://arxiv.org/abs/2308.09448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoze Qiu, Fei Zheng, Chaochao Chen, Xiaolin Zheng</li>
<li>for: 本研究主要针对Split Learning下的标签推导攻击，即通过梯度反转方法进行标签推导。</li>
<li>methods: 我们提出了两种防御方法：Random Label Extension (RLE) 和 Model-based adaptive Label Extension (MLE)。RLE 方法通过扩展标签信息，防止攻击者通过梯度来训练攻击模型。MLE 方法保留了原始任务的标签信息，并在扩展后的标签中占据主导地位。</li>
<li>results: 我们的 эксперименталь结果表明，相比基础防御方法，我们提出的防御方法可以减少攻击模型的表现，同时保持原始任务的表现。<details>
<summary>Abstract</summary>
As a privacy-preserving method for implementing Vertical Federated Learning, Split Learning has been extensively researched. However, numerous studies have indicated that the privacy-preserving capability of Split Learning is insufficient. In this paper, we primarily focus on label inference attacks in Split Learning under regression setting, which are mainly implemented through the gradient inversion method. To defend against label inference attacks, we propose Random Label Extension (RLE), where labels are extended to obfuscate the label information contained in the gradients, thereby preventing the attacker from utilizing gradients to train an attack model that can infer the original labels. To further minimize the impact on the original task, we propose Model-based adaptive Label Extension (MLE), where original labels are preserved in the extended labels and dominate the training process. The experimental results show that compared to the basic defense methods, our proposed defense methods can significantly reduce the attack model's performance while preserving the original task's performance.
</details>
<details>
<summary>摘要</summary>
As a privacy-preserving method for implementing Vertical Federated Learning, Split Learning has been extensively studied. However, numerous studies have shown that the privacy-preserving capability of Split Learning is insufficient. In this paper, we focus primarily on label inference attacks in Split Learning under the regression setting, which are mainly implemented through the gradient inversion method. To defend against label inference attacks, we propose Random Label Extension (RLE), where labels are extended to obscure the label information contained in the gradients, thereby preventing the attacker from using gradients to train an attack model that can infer the original labels. To further minimize the impact on the original task, we propose Model-based adaptive Label Extension (MLE), where original labels are preserved in the extended labels and dominate the training process. The experimental results show that compared to the basic defense methods, our proposed defense methods can significantly reduce the attack model's performance while preserving the original task's performance.Here's the translation of each sentence into Simplified Chinese:1. 在 Vertical Federated Learning 中作为隐私保护方法，Split Learning 已经得到了广泛的研究。2. 然而，许多研究表明，Split Learning 的隐私保护能力不够。3. 在本文中，我们主要关注 Split Learning 下的批量推理攻击，它们通常通过梯度倒反方法进行实现。4. 为了防止批量推理攻击，我们提议 Random Label Extension (RLE)，labels 被扩展以隐藏在梯度中的标签信息，因此防止攻击者使用梯度训练攻击模型来推理出原始标签。5. 为了最小化影响原始任务，我们提议 Model-based adaptive Label Extension (MLE)，原始标签在扩展后保留，并且主导训练过程。6. 实验结果表明，相比基本防御方法，我们提议的防御方法可以显著降低攻击模型的性能，同时保持原始任务的性能。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-1-Iteration-Learning-Algorithm-for-Gaussian-Mixture-Model-And-Gaussian-Mixture-Embedding-For-Neural-Network"><a href="#An-Efficient-1-Iteration-Learning-Algorithm-for-Gaussian-Mixture-Model-And-Gaussian-Mixture-Embedding-For-Neural-Network" class="headerlink" title="An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network"></a>An Efficient 1 Iteration Learning Algorithm for Gaussian Mixture Model And Gaussian Mixture Embedding For Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09444">http://arxiv.org/abs/2308.09444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiguo Lu, Xuan Wu, Deng Ding, Gangnan Yuan</li>
<li>for: 这个论文是为了提出一种基于我们之前的GMM扩展思想的 Gaussian Mixture Model（GMM）学习算法。</li>
<li>methods: 这个算法使用了新的GMM扩展思想，具有更高的稳定性和简洁性，并且能够在1轮内学习。我们也提供了一个对于不同参数初始值的确定性证明。</li>
<li>results: 我们的GMM扩展方法比 классиical Expectation Maximization（EM）算法更具有鲁棒性和精度，并且能够更好地应对数据不确定性和逆问题。最后，我们测试了基于GMM的生成器，并证明了它的潜在应用于随机抽样和变量控制。<details>
<summary>Abstract</summary>
We propose an Gaussian Mixture Model (GMM) learning algorithm, based on our previous work of GMM expansion idea. The new algorithm brings more robustness and simplicity than classic Expectation Maximization (EM) algorithm. It also improves the accuracy and only take 1 iteration for learning. We theoretically proof that this new algorithm is guarantee to converge regardless the parameters initialisation. We compare our GMM expansion method with classic probability layers in neural network leads to demonstrably better capability to overcome data uncertainty and inverse problem. Finally, we test GMM based generator which shows a potential to build further application that able to utilized distribution random sampling for stochastic variation as well as variation control.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种高斯混合模型（GMM）学习算法，基于我们之前的GMM扩展思想。新算法具有更高的稳定性和简洁性，并且在1轮学习中可以提高准确性。我们 theoretically证明了这种新算法是无论初始化参数都能够 converge。我们对 классический可期望最大化（EM）算法和GMM扩展方法进行比较，发现我们的方法在面对数据不确定性和逆问题时表现更好。最后，我们测试了基于GMM的生成器，发现它具有可以利用分布随机抽样以及变量控制的潜在应用 potential。Note: Please keep in mind that the translation is done by a machine and may not be perfect. It's always a good idea to have a human proofread and verify the translation, especially for important documents.
</details></li>
</ul>
<hr>
<h2 id="From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space"><a href="#From-Hope-to-Safety-Unlearning-Biases-of-Deep-Models-by-Enforcing-the-Right-Reasons-in-Latent-Space" class="headerlink" title="From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space"></a>From Hope to Safety: Unlearning Biases of Deep Models by Enforcing the Right Reasons in Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09437">http://arxiv.org/abs/2308.09437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Dreyer, Frederik Pahde, Christopher J. Anders, Wojciech Samek, Sebastian Lapuschkin</li>
<li>for: The paper is written for deep neural networks that are prone to learning spurious correlations and biases, with a focus on high-stake decision-making applications such as medical applications.</li>
<li>methods: The paper proposes a novel method for mitigating biases in deep neural networks by reducing the model’s sensitivity towards biases through the gradient. The method uses Concept Activation Vectors to model biases and highlights the importance of choosing robust directions.</li>
<li>results: The paper demonstrates the effectiveness of the proposed method in controlling biases in controlled and real-world settings on several datasets, including ISIC, Bone Age, ImageNet, and CelebA, using VGG, ResNet, and EfficientNet architectures.<details>
<summary>Abstract</summary>
Deep Neural Networks are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures.
</details>
<details>
<summary>摘要</summary>
翻译结果：深度神经网络 Deep neural networks 深度神经网络 are prone to learning spurious correlations embedded in the training data, leading to potentially biased predictions. This poses risks when deploying these models for high-stake decision-making, such as in medical applications. Current methods for post-hoc model correction either require input-level annotations, which are only possible for spatially localized biases, or augment the latent feature space, thereby hoping to enforce the right reasons. We present a novel method ensuring the right reasons on the concept level by reducing the model's sensitivity towards biases through the gradient. When modeling biases via Concept Activation Vectors, we highlight the importance of choosing robust directions, as traditional regression-based approaches such as Support Vector Machines tend to result in diverging directions. We effectively mitigate biases in controlled and real-world settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet and EfficientNet architectures.
</details></li>
</ul>
<hr>
<h2 id="Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability"><a href="#Can-ultrasound-confidence-maps-predict-sonographers’-labeling-variability" class="headerlink" title="Can ultrasound confidence maps predict sonographers’ labeling variability?"></a>Can ultrasound confidence maps predict sonographers’ labeling variability?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09433">http://arxiv.org/abs/2308.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vanessa Gonzalez Duque, Leonhard Zirus, Yordanka Velikova, Nassir Navab, Diana Mateus</li>
<li>for: 这篇论文的目的是提出一种新的深度学习 segmentation 方法，以帮助诊断人员更准确地识别骨内部结构。</li>
<li>methods: 该方法使用了 ultrasound 图像中的 confidence map，以帮助深度学习 segmentation 网络更好地识别图像中的结构。</li>
<li>results: 研究发现，使用 confidence map 可以提高 segmentation 的准确性，降低 isolated pixel 的预测数量，并且可以更好地识别图像中的异常区域。<details>
<summary>Abstract</summary>
Measuring cross-sectional areas in ultrasound images is a standard tool to evaluate disease progress or treatment response. Often addressed today with supervised deep-learning segmentation approaches, existing solutions highly depend upon the quality of experts' annotations. However, the annotation quality in ultrasound is anisotropic and position-variant due to the inherent physical imaging principles, including attenuation, shadows, and missing boundaries, commonly exacerbated with depth. This work proposes a novel approach that guides ultrasound segmentation networks to account for sonographers' uncertainties and generate predictions with variability similar to the experts. We claim that realistic variability can reduce overconfident predictions and improve physicians' acceptance of deep-learning cross-sectional segmentation solutions. Our method provides CM's certainty for each pixel for minimal computational overhead as it can be precalculated directly from the image. We show that there is a correlation between low values in the confidence maps and expert's label uncertainty. Therefore, we propose to give the confidence maps as additional information to the networks. We study the effect of the proposed use of ultrasound CMs in combination with four state-of-the-art neural networks and in two configurations: as a second input channel and as part of the loss. We evaluate our method on 3D ultrasound datasets of the thyroid and lower limb muscles. Our results show ultrasound CMs increase the Dice score, improve the Hausdorff and Average Surface Distances, and decrease the number of isolated pixel predictions. Furthermore, our findings suggest that ultrasound CMs improve the penalization of uncertain areas in the ground truth data, thereby improving problematic interpolations. Our code and example data will be made public at https://github.com/IFL-CAMP/Confidence-segmentation.
</details>
<details>
<summary>摘要</summary>
measuring cross-sectional areas in ultrasound images 是评估疾病进展或治疗效果的标准工具。现有的深度学习分割方法常常高度依赖于专家的注释质量。然而， ultrasound 中注释质量存在各向异性和位置变化，由于物理射频原理所带来的吸收、阴影和缺失边界等问题，这些问题通常会在深度中扩大。这项工作提议一种新的方法，使 ultrasound 分割网络考虑到医生的不确定性，并生成与专家的预测类似的结果。我们认为，使用真实的变化可以减少过于自信的预测，提高医生接受深度学习横截分割解决方案的可能性。我们的方法可以在图像直接从图像中计算CM的确定性，无需较大的计算负担。我们发现，CM的低值与专家标签的不确定性之间存在相关性。因此，我们提议将CM作为额外信息传递给网络。我们在四种当前的状态艺术神经网络和两种配置下对 ultrasound CM 的使用进行了研究。我们的结果表明， ultrasound CM 可以提高 dice 分数，改善 Hausdorff 和平均表面距离，并减少孤立像素预测。此外，我们的发现表明， ultrasound CM 可以改善基于真实数据的 interpolations 问题。我们的代码和示例数据将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-topographic-networks-as-models-of-cortical-map-formation-and-human-visual-behaviour-moving-beyond-convolutions"><a href="#End-to-end-topographic-networks-as-models-of-cortical-map-formation-and-human-visual-behaviour-moving-beyond-convolutions" class="headerlink" title="End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions"></a>End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09431">http://arxiv.org/abs/2308.09431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zejin Lu, Adrien Doerig, Victoria Bosch, Bas Krahmer, Daniel Kaiser, Radoslaw M Cichy, Tim C Kietzmann</li>
<li>for: 这篇论文旨在理解震撼视系统的演化和功能，以及如何通过计算模型来理解这种排列结构。</li>
<li>methods: 这篇论文使用了All-Topographic Neural Networks（All-TNNs）来模拟视觉系统，并通过训练视觉输入数据来实现了一些猴颗粒的特征，如平滑的方向图和大脑内部的倍增。</li>
<li>results: 论文表明，All-TNNs在对人类空间偏好的对象识别任务中表现 significatively better than之前的状态对模型，这是因为All-TNNs的排列性质使得它们更能够和人类视觉系统的排列结构相匹配。<details>
<summary>Abstract</summary>
Computational models are an essential tool for understanding the origin and functions of the topographic organisation of the primate visual system. Yet, vision is most commonly modelled by convolutional neural networks that ignore topography by learning identical features across space. Here, we overcome this limitation by developing All-Topographic Neural Networks (All-TNNs). Trained on visual input, several features of primate topography emerge in All-TNNs: smooth orientation maps and cortical magnification in their first layer, and category-selective areas in their final layer. In addition, we introduce a novel dataset of human spatial biases in object recognition, which enables us to directly link models to behaviour. We demonstrate that All-TNNs significantly better align with human behaviour than previous state-of-the-art convolutional models due to their topographic nature. All-TNNs thereby mark an important step forward in understanding the spatial organisation of the visual brain and how it mediates visual behaviour.
</details>
<details>
<summary>摘要</summary>
计算模型是视觉系统起源和功能理解的重要工具。然而，大多数视觉模型使用卷积神经网络，这些神经网络忽略了空间的特征，通过学习同样的特征来学习视觉数据。在这里，我们解决了这一限制，开发了全面特征神经网络（All-TNNs）。这些神经网络在训练视觉输入数据后显示出了许多非常有趣的特征，包括顺序映射和 cortical 增强在其第一层，以及在最终层中的类选择区域。此外，我们还介绍了一个新的人类空间偏见数据集，该数据集允许我们直接将模型与行为联系起来。我们表明，All-TNNs 比前一代 convolutional 模型更好地适应人类行为，这是因为它们的特征性质。All-TNNs 因此代表了我们理解视觉脑的空间组织和如何通过视觉行为来转化的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Towards-Understanding-the-Generalizability-of-Delayed-Stochastic-Gradient-Descent"><a href="#Towards-Understanding-the-Generalizability-of-Delayed-Stochastic-Gradient-Descent" class="headerlink" title="Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent"></a>Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09430">http://arxiv.org/abs/2308.09430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoge Deng, Li Shen, Shengwei Li, Tao Sun, Dongsheng Li, Dacheng Tao</li>
<li>for: 这个论文主要研究了异步延迟随机梯度下降（ASGD）在大规模机器学习模型训练中的泛化性表现。</li>
<li>methods: 作者使用了生成函数分析工具来研究延迟梯度算法的稳定性，并基于这种稳定性提供了异步延迟随机梯度下降算法的泛化误差Upper bound。</li>
<li>results: 作者的理论研究结果表明，异步延迟可以降低延迟随机梯度下降算法的泛化误差。同时，作者还对随机延迟设置进行了相应的分析，并通过实验 validate了他们的理论结论。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) performed in an asynchronous manner plays a crucial role in training large-scale machine learning models. However, the generalization performance of asynchronous delayed SGD, which is an essential metric for assessing machine learning algorithms, has rarely been explored. Existing generalization error bounds are rather pessimistic and cannot reveal the correlation between asynchronous delays and generalization. In this paper, we investigate sharper generalization error bound for SGD with asynchronous delay $\tau$. Leveraging the generating function analysis tool, we first establish the average stability of the delayed gradient algorithm. Based on this algorithmic stability, we provide upper bounds on the generalization error of $\tilde{\mathcal{O}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}(\frac{1}{n})$ for quadratic convex and strongly convex problems, respectively, where $T$ refers to the iteration number and $n$ is the amount of training data. Our theoretical results indicate that asynchronous delays reduce the generalization error of the delayed SGD algorithm. Analogous analysis can be generalized to the random delay setting, and the experimental results validate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
（注：在这个句子中，我们使用了一些简化的中文表达，以便更好地适应中文语言的特点。例如，我们使用了“异步方式”而不是“异步调度”，以便更好地表达SGD的异步性。同时，我们还使用了“$\mathcal{O}$”来表示大O符号，以便更好地表达数学符号。）
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks"><a href="#Self-Supervised-Single-Image-Deconvolution-with-Siamese-Neural-Networks" class="headerlink" title="Self-Supervised Single-Image Deconvolution with Siamese Neural Networks"></a>Self-Supervised Single-Image Deconvolution with Siamese Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09426">http://arxiv.org/abs/2308.09426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Papkov, Kaupo Palo, Leopold Parts</li>
<li>for: Image reconstruction from noisy observations, specifically in 3D microscopy deconvolution tasks.</li>
<li>methods: Deep learning methods with a Siamese invariance loss and Fast Fourier Transform (FFT) convolutions, which improve upon previous state-of-the-art deconvolution methods with a known point spread function.</li>
<li>results: Outperformance of the improved framework compared to previous state-of-the-art deconvolution methods, with improved sharpness and reduced grain.<details>
<summary>Abstract</summary>
Inverse problems in image reconstruction are fundamentally complicated by unknown noise properties. Classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain. Deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data. Recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training. However, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized. We tackle this problem with Fast Fourier Transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks. Further, we propose to adopt a Siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches. The experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate text into Simplified Chinese文本： inverse problems in image reconstruction are fundamentally complicated by unknown noise properties. classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain. deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data. recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training. however, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized. we tackle this problem with fast fourier transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks. further, we propose to adopt a siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches. the experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function.翻译： inverse problems in 图像重建是基础上 complicated by unknown noise properties。 classical iterative deconvolution approaches amplify noise and require careful parameter selection for an optimal trade-off between sharpness and grain。 deep learning methods allow for flexible parametrization of the noise and learning its properties directly from the data。 recently, self-supervised blind-spot neural networks were successfully adopted for image deconvolution by including a known point-spread function in the end-to-end training。 however, their practical application has been limited to 2D images in the biomedical domain because it implies large kernels that are poorly optimized。 we tackle this problem with fast fourier transform convolutions that provide training speed-up in 3D microscopy deconvolution tasks。 further, we propose to adopt a siamese invariance loss for deconvolution and empirically identify its optimal position in the neural network between blind-spot and full image branches。 experimental results show that our improved framework outperforms the previous state-of-the-art deconvolution methods with a known point spread function。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Agent-Communication-and-Learning-through-Action-and-Language"><a href="#Enhancing-Agent-Communication-and-Learning-through-Action-and-Language" class="headerlink" title="Enhancing Agent Communication and Learning through Action and Language"></a>Enhancing Agent Communication and Learning through Action and Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10842">http://arxiv.org/abs/2308.10842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Caselles-Dupré Hugo, Sigaud Olivier, Chetouani Mohamed</li>
<li>for: 本研究旨在开发一种新的GC-代理人，能够同时扮演教师和学生的角色，提高交流效率。</li>
<li>methods: 该研究采用了动作示范和语言指令，并考虑了人类交流中的教学和实践元素，以提高代理人的教学和学习能力。</li>
<li>results: 研究发现，结合动作和语言交流模式可以提高学习效果，并且多模式交流具有优势。<details>
<summary>Abstract</summary>
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的GC-代理，可以同时作为教师和学生进行功能。通过动作示例和语言指令，这些代理提高了交流效率。我们研究了包括教学和实践在内的人类communication的关键元素，以提高代理的教学和学习能力。此外，我们还研究了将多种交流模式（动作和语言）结合使用的影响，并 highlighted the benefits of a multi-modal approach。
</details></li>
</ul>
<hr>
<h2 id="ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks"><a href="#ICU-Mortality-Prediction-Using-Long-Short-Term-Memory-Networks" class="headerlink" title="ICU Mortality Prediction Using Long Short-Term Memory Networks"></a>ICU Mortality Prediction Using Long Short-Term Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12800">http://arxiv.org/abs/2308.12800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manel Mili, Asma Kerkeni, Asma Ben Abdallah, Mohamed Hedi Bedoui</li>
<li>for: 预测医院死亡率和医院 lengths of stay (LOS) 的早期预测</li>
<li>methods: 使用自动化数据驱动系统，分析大量多变量时间数据，并提取高级信息以预测医院死亡率和LOS</li>
<li>results: LSTM 网络模型能够准确预测医院死亡率和LOS，并且可以减少时间帧数以提高临床任务效果<details>
<summary>Abstract</summary>
Extensive bedside monitoring in Intensive Care Units (ICUs) has resulted in complex temporal data regarding patient physiology, which presents an upscale context for clinical data analysis. In the other hand, identifying the time-series patterns within these data may provide a high aptitude to predict clinical events. Hence, we investigate, during this work, the implementation of an automatic data-driven system, which analyzes large amounts of multivariate temporal data derived from Electronic Health Records (EHRs), and extracts high-level information so as to predict in-hospital mortality and Length of Stay (LOS) early. Practically, we investigate the applicability of LSTM network by reducing the time-frame to 6-hour so as to enhance clinical tasks. The experimental results highlight the efficiency of LSTM model with rigorous multivariate time-series measurements for building real-world prediction engines.
</details>
<details>
<summary>摘要</summary>
使用了床side监测的医疗机构（ICU）已经生成了复杂的时间序列数据，这些数据具有更高的上下文级别，可以进行丰富的临床数据分析。在另一方面，可以通过时间序列模式的识别来预测临床事件。因此，在这项工作中，我们研究了自动化数据驱动系统，该系统分析了大量的多变量时间序列数据，并提取高级信息，以预测医院内死亡率和长期入院天数（LOS）的早期预测。实际上，我们调整了时间帧为6小时，以便进行临床任务。实验结果表明，LSTM模型在严格的多变量时间序列测量上可以建立实用的临床预测机器。</SYS>Here is the translation of the text into Simplified Chinese:<SYS>通过床side监测的医疗机构（ICU）已经生成了复杂的时间序列数据，这些数据具有更高的上下文级别，可以进行丰富的临床数据分析。在另一方面，可以通过时间序列模式的识别来预测临床事件。因此，在这项工作中，我们研究了自动化数据驱动系统，该系统分析了大量的多变量时间序列数据，并提取高级信息，以预测医院内死亡率和长期入院天数（LOS）的早期预测。实际上，我们调整了时间帧为6小时，以便进行临床任务。实验结果表明，LSTM模型在严格的多变量时间序列测量上可以建立实用的临床预测机器。</SYS>Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Solutions-for-the-Analysis-of-Single-Particle-Diffusion-Trajectories"><a href="#Machine-Learning-Solutions-for-the-Analysis-of-Single-Particle-Diffusion-Trajectories" class="headerlink" title="Machine-Learning Solutions for the Analysis of Single-Particle Diffusion Trajectories"></a>Machine-Learning Solutions for the Analysis of Single-Particle Diffusion Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09414">http://arxiv.org/abs/2308.09414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henrik Seckler, Janusz Szwabinski, Ralf Metzler</li>
<li>for: 本研究旨在探讨现代机器学习技术如何应用于扩散时序序中，以解释记录的动态。</li>
<li>methods: 本文综述了最近引入的机器学习方法，包括在异常扩散挑战中取得成功的方法。这些方法受到批评因为缺乏可解性，所以本文强调了包含不确定性估计和特征基于的方法，以提高解释性和提供具体的学习过程的信息。</li>
<li>results: 本文分析了不同的非标型数据集中的预测结果，并评论了未来发展的想法。<details>
<summary>Abstract</summary>
Single-particle traces of the diffusive motion of molecules, cells, or animals are by-now routinely measured, similar to stochastic records of stock prices or weather data. Deciphering the stochastic mechanism behind the recorded dynamics is vital in understanding the observed systems. Typically, the task is to decipher the exact type of diffusion and/or to determine system parameters. The tools used in this endeavor are currently revolutionized by modern machine-learning techniques. In this Perspective we provide an overview over recently introduced methods in machine-learning for diffusive time series, most notably, those successfully competing in the Anomalous-Diffusion-Challenge. As such methods are often criticized for their lack of interpretability, we focus on means to include uncertainty estimates and feature-based approaches, both improving interpretability and providing concrete insight into the learning process of the machine. We expand the discussion by examining predictions on different out-of-distribution data. We also comment on expected future developments.
</details>
<details>
<summary>摘要</summary>
单 particle跟踪记录分子、细胞或动物的扩散运动已经成为惯例，与 Stochastic 记录如股票价格或天气资料一样。解读这些动态的数学机制是理解观察系统的关键。通常，任务是确定扩散的类型和/或系统参数。现代机器学习技术已经推动这些工具的改革。在这篇 Perspective 中，我们给出了最近引入的机器学习方法 для扩散时间序列，主要是在 Anomalous-Diffusion-Challenge 中竞争成功的方法。由于这些方法经常被批评缺乏解释性，我们专注于包括不确定性估计和特征基于的方法，它们可以提高解释性和给予实际的机器学习过程的关键信息。我们进一步扩展了讨论，评估不同的外部数据类型的预测。我们也评估未来的发展。
</details></li>
</ul>
<hr>
<h2 id="Metadata-Improves-Segmentation-Through-Multitasking-Elicitation"><a href="#Metadata-Improves-Segmentation-Through-Multitasking-Elicitation" class="headerlink" title="Metadata Improves Segmentation Through Multitasking Elicitation"></a>Metadata Improves Segmentation Through Multitasking Elicitation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09411">http://arxiv.org/abs/2308.09411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iaroslav Plutenko, Mikhail Papkov, Kaupo Palo, Leopold Parts, Dmytro Fishman</li>
<li>for: 本研究用于探讨Metadata在深度学习方法中的应用，具体来说是在Semantic Segmentation任务中使用Metadata进行改进。</li>
<li>methods: 本研究使用了通道调制机制，将Metadata作为 convolutional network 的输入，以提高Semantic Segmentation的结果。</li>
<li>results: 研究结果表明，Metadata作为输入可以提高Semantic Segmentation的结果，而且这种改进只需要对现有的模型进行简单修改，不需要增加训练样本或更改网络结构。<details>
<summary>Abstract</summary>
Metainformation is a common companion to biomedical images. However, this potentially powerful additional source of signal from image acquisition has had limited use in deep learning methods, for semantic segmentation in particular. Here, we incorporate metadata by employing a channel modulation mechanism in convolutional networks and study its effect on semantic segmentation tasks. We demonstrate that metadata as additional input to a convolutional network can improve segmentation results while being inexpensive in implementation as a nimble add-on to popular models. We hypothesize that this benefit of metadata can be attributed to facilitating multitask switching. This aspect of metadata-driven systems is explored and discussed in detail.
</details>
<details>
<summary>摘要</summary>
这文本中的metadata是一个常见的伴侣，尤其是在生物医学影像的摄取中。然而，这具有潜力的额外信号仍然尚未在深度学习方法中得到了广泛的利用，尤其是在semantic segmentation中。在这里，我们将metadata incorporated into convolutional networks using a channel modulation mechanism，并评估其对于semantic segmentation task的影响。我们发现，将metadata作为 convolutional network的额外输入，可以提高分类结果，而且实现起来相对容易，可以作为受欢迎的模型的一个小改进。我们预测，metadata的帮助可以减少多任务的变换，这个方面的metadata-driven系统的特点是详细地探讨和讨论。
</details></li>
</ul>
<hr>
<h2 id="Learning-MDL-logic-programs-from-noisy-data"><a href="#Learning-MDL-logic-programs-from-noisy-data" class="headerlink" title="Learning MDL logic programs from noisy data"></a>Learning MDL logic programs from noisy data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09393">http://arxiv.org/abs/2308.09393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Céline Hocquette, Andreas Niskanen, Matti Järvisalo, Andrew Cropper</li>
<li>for: 该论文是为了解决 inductive logic programming 方法在含有噪声数据时学习程序的问题。</li>
<li>methods: 该方法使用 minimal description length 来学习从含有噪声数据中的程序，包括回归式程序。</li>
<li>results: 我们在多个领域，包括药物设计、游戏撸猫和程序生成中进行了实验，发现我们的方法可以在噪声数据中提高预测精度和扩展至一定程度的噪声。<details>
<summary>Abstract</summary>
Many inductive logic programming approaches struggle to learn programs from noisy data. To overcome this limitation, we introduce an approach that learns minimal description length programs from noisy data, including recursive programs. Our experiments on several domains, including drug design, game playing, and program synthesis, show that our approach can outperform existing approaches in terms of predictive accuracies and scale to moderate amounts of noise.
</details>
<details>
<summary>摘要</summary>
多种逻辑编程方法在含噪数据上学习程序具有困难，为了解决这一限制，我们提出了一种学习最短描述长度程序从含噪数据中学习，包括循环程序。我们在多个领域，如药物设计、游戏撸抓和程序生成等，进行了实验，结果表明我们的方法可以在适度的噪音量下超过现有方法的预测精度。
</details></li>
</ul>
<hr>
<h2 id="FunQuant-A-R-package-to-perform-quantization-in-the-context-of-rare-events-and-time-consuming-simulations"><a href="#FunQuant-A-R-package-to-perform-quantization-in-the-context-of-rare-events-and-time-consuming-simulations" class="headerlink" title="FunQuant: A R package to perform quantization in the context of rare events and time-consuming simulations"></a>FunQuant: A R package to perform quantization in the context of rare events and time-consuming simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10871">http://arxiv.org/abs/2308.10871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charlie Sire, Yann Richet, Rodolphe Le Riche, Didier Rullière, Jérémy Rohmer, Lucie Pheulpin</li>
<li>for: 这篇论文是为了描述数据量化的方法和其应用场景。</li>
<li>methods: 这篇论文使用 Lloyd’s algorithm，它将数据空间分成Voronoi细分，并基于中心和概率质量来构建离散分布。</li>
<li>results: 这篇论文发现，在数据评估是成本高、罕见事件关联的情况下，Lloyd’s algorithm可能会遇到困难，而且单个无事件集中占据大量概率质量。因此，需要使用мета模型和改进的采样方法来增加精度计算的罕见集。<details>
<summary>Abstract</summary>
Quantization summarizes continuous distributions by calculating a discrete approximation. Among the widely adopted methods for data quantization is Lloyd's algorithm, which partitions the space into Vorono\"i cells, that can be seen as clusters, and constructs a discrete distribution based on their centroids and probabilistic masses. Lloyd's algorithm estimates the optimal centroids in a minimal expected distance sense, but this approach poses significant challenges in scenarios where data evaluation is costly, and relates to rare events. Then, the single cluster associated to no event takes the majority of the probability mass. In this context, a metamodel is required and adapted sampling methods are necessary to increase the precision of the computations on the rare clusters.
</details>
<details>
<summary>摘要</summary>
量化概率分布的目的是将连续的数据转换为离散的形式，以便进行更加简单的计算和分析。 Lloyd 算法是一种广泛采用的数据量化方法，它将数据空间分割成 Voronoi 维度，可以看作是各个类别的集中点，并根据这些集中点和概率质量来构建离散分布。 Lloyd 算法会估算最优中心点，但是在数据评估是成本高、与罕见事件相关的场景下，这种方法会遇到很大的挑战。在这种情况下，需要使用多态模型，并采用适当的采样方法来提高罕见类别的计算精度。Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-Gradient-like-Explanation-under-a-Black-box-Setting-When-Black-box-Explanations-Become-as-Good-as-White-box"><a href="#On-Gradient-like-Explanation-under-a-Black-box-Setting-When-Black-box-Explanations-Become-as-Good-as-White-box" class="headerlink" title="On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box"></a>On Gradient-like Explanation under a Black-box Setting: When Black-box Explanations Become as Good as White-box</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09381">http://arxiv.org/abs/2308.09381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Cai, Gerhard Wunder</li>
<li>For: 本文旨在提供一种基于梯度估计的解释方法，以便在数据驱动方法中提高解释性。* Methods: 本文提出了一种名为GEEX的解释方法，该方法可在黑盒Setting下提供梯度类型的解释。此外，本文还将GEEX方法与路径方法集成，得到了名为iGEEX的完整解释方法。* Results: 实验表明，提出的方法可以在图像数据上超越当前黑盒方法的表现，并与完全访问的方法具有相似的性能。<details>
<summary>Abstract</summary>
Attribution methods shed light on the explainability of data-driven approaches such as deep learning models by revealing the most contributing features to decisions that have been made. A widely accepted way of deriving feature attributions is to analyze the gradients of the target function with respect to input features. Analysis of gradients requires full access to the target system, meaning that solutions of this kind treat the target system as a white-box. However, the white-box assumption may be untenable due to security and safety concerns, thus limiting their practical applications. As an answer to the limited flexibility, this paper presents GEEX (gradient-estimation-based explanation), an explanation method that delivers gradient-like explanations under a black-box setting. Furthermore, we integrate the proposed method with a path method. The resulting approach iGEEX (integrated GEEX) satisfies the four fundamental axioms of attribution methods: sensitivity, insensitivity, implementation invariance, and linearity. With a focus on image data, the exhaustive experiments empirically show that the proposed methods outperform state-of-the-art black-box methods and achieve competitive performance compared to the ones with full access.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>数据驱动方法的解释方法可以把灯光抛向数字学习模型中的决策过程中的最重要的特征。一种广泛得到的解释特征的 derivation 方法是通过输入特征的梯度分析target 函数的梯度。这种方法需要对目标系统具有全访问权限，因此这种方法被称为白盒模型。然而，白盒假设可能存在安全和安全问题，因此它们的实际应用受到限制。为了解决这些限制，本文提出了 GEEX（梯度估计基于解释），一种解释方法，可以在黑盒Setting下提供梯度类似的解释。此外，我们将该方法与路径方法集成，得到了 iGEEX（集成 GEEX）。该方法满足了解释方法的四个基本假设：敏感性、不敏感性、实现不变性和直线性。对于图像数据，我们进行了大量的实验，证明了我们的方法在黑盒设置下表现出色，并且与完全访问的情况相比，其性能几乎相同。Note: "简化中文" refers to Simplified Chinese, which is one of the two standardized Chinese languages used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review"><a href="#Deciphering-knee-osteoarthritis-diagnostic-features-with-explainable-artificial-intelligence-A-systematic-review" class="headerlink" title="Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review"></a>Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09380">http://arxiv.org/abs/2308.09380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Xin Teoh, Alice Othmani, Siew Li Goh, Juliana Usman, Khin Wee Lai</li>
<li>for: 提高膝关节炎诊断的可靠性和可读性，推广人工智能技术在医疗领域的应用。</li>
<li>methods: 本文首次对膝关节炎诊断使用可解释人工智能技术进行了评估，从数据可解释性和模型可解释性两个角度进行了分析。</li>
<li>results: 研究发现，可解释人工智能技术可以提高膝关节炎诊断的可靠性和可读性，并且可以增强医生对模型预测的信任感。<details>
<summary>Abstract</summary>
Existing artificial intelligence (AI) models for diagnosing knee osteoarthritis (OA) have faced criticism for their lack of transparency and interpretability, despite achieving medical-expert-like performance. This opacity makes them challenging to trust in clinical practice. Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model's prediction by revealing how the prediction is derived, thus promoting the use of AI systems in healthcare. This paper presents the first survey of XAI techniques used for knee OA diagnosis. The XAI techniques are discussed from two perspectives: data interpretability and model interpretability. The aim of this paper is to provide valuable insights into XAI's potential towards a more reliable knee OA diagnosis approach and encourage its adoption in clinical practice.
</details>
<details>
<summary>摘要</summary>
现有的人工智能（AI）模型用于诊断膝关节病（OA）已经受到了不透明性和解释性的批评，尽管它们达到了医疗专业人员水平。这种透明性使得它们在临床实践中具有挑战。最近，可解释的人工智能（XAI）技术在医疗领域出现了，它可以为模型预测提供信任度，并且解释预测是如何 derivation。这篇论文是首次对膝关节病诊断中使用XAI技术进行了报告。本文从数据可解释性和模型可解释性两个角度来讨论XAI技术，旨在为读者提供XAI在膝关节病诊断方面的可靠性和可信度，并促进XAI在临床实践中的应用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Techniques-in-Extreme-Weather-Events-A-Review"><a href="#Deep-Learning-Techniques-in-Extreme-Weather-Events-A-Review" class="headerlink" title="Deep Learning Techniques in Extreme Weather Events: A Review"></a>Deep Learning Techniques in Extreme Weather Events: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10995">http://arxiv.org/abs/2308.10995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikha Verma, Kuldeep Srivastava, Akhilesh Tiwari, Shekhar Verma</li>
<li>for: 本评论旨在提供气象预报领域深度学习的现状报告，探讨深度学习在气象预报中的应用和发展趋势。</li>
<li>methods: 本评论总结了各种深度学习架构在气象预报中的应用，包括风暴、雷电、降水、旱情、热波、寒波等方面的应用。</li>
<li>results: 本评论指出了深度学习在气象预报中的优势，包括能够捕捉复杂的模式和非线性关系，并且对现有方法存在一些限制。<details>
<summary>Abstract</summary>
Extreme weather events pose significant challenges, thereby demanding techniques for accurate analysis and precise forecasting to mitigate its impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. This review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures, across various aspects of weather prediction such as thunderstorm, lightning, precipitation, drought, heatwave, cold waves and tropical cyclones. We highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decisions and mitigate the impacts of extreme weather events.
</details>
<details>
<summary>摘要</summary>
极端天气事件带来重大挑战，需要精准的分析和预测方法来减轻其影响。近年来，深度学习技术在天气预测和极端天气事件动力学理解方面emerged as a promising approach。本文提供了天气预测领域的深度学习状态评估，探讨了不同天气元素的适用深度学习架构，包括雨夹、闪电、降水、旱情、热潮、冰潮和热带风暴。我们强调了深度学习的可能性，如其能够捕捉复杂的模式和非线性关系。此外，我们还讨论了当前方法的局限性，并提出了未来的发展方向，以便在天气预测领域取得更大的进步。这些系统性评估的结论对科学社区的决策具有重要意义，以减轻极端天气事件的影响。
</details></li>
</ul>
<hr>
<h2 id="Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package"><a href="#Image-Processing-and-Machine-Learning-for-Hyperspectral-Unmixing-An-Overview-and-the-HySUPP-Python-Package" class="headerlink" title="Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package"></a>Image Processing and Machine Learning for Hyperspectral Unmixing: An Overview and the HySUPP Python Package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09375">http://arxiv.org/abs/2308.09375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/behnoodrasti/hysupp">https://github.com/behnoodrasti/hysupp</a></li>
<li>paper_authors: Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot<br>for:This paper provides an overview of advanced and conventional unmixing approaches for hyperspectral image analysis, and compares their performance on various datasets.methods:The paper discusses linear unmixing techniques, including supervised, semi-supervised, and unsupervised (blind) methods, and their applications in hyperspectral image analysis.results:The experimental results show the advantages of different unmixing categories for different unmixing scenarios, and provide an open-source Python-based package for reproducing the results.<details>
<summary>Abstract</summary>
Spectral pixels are often a mixture of the pure spectra of the materials, called endmembers, due to the low spatial resolution of hyperspectral sensors, double scattering, and intimate mixtures of materials in the scenes. Unmixing estimates the fractional abundances of the endmembers within the pixel. Depending on the prior knowledge of endmembers, linear unmixing can be divided into three main groups: supervised, semi-supervised, and unsupervised (blind) linear unmixing. Advances in Image processing and machine learning substantially affected unmixing. This paper provides an overview of advanced and conventional unmixing approaches. Additionally, we draw a critical comparison between advanced and conventional techniques from the three categories. We compare the performance of the unmixing techniques on three simulated and two real datasets. The experimental results reveal the advantages of different unmixing categories for different unmixing scenarios. Moreover, we provide an open-source Python-based package available at https://github.com/BehnoodRasti/HySUPP to reproduce the results.
</details>
<details>
<summary>摘要</summary>
spectral pixels 常常是Materials的纯谱的混合物，称为Endmember，由于遥感器的低空间分辨率、双折射和场景中Materials的密切混合，导致了这种混合。混合计算Endmember内Pixel中的含量。根据Endmember的先知情况，线性混合可以分为三类：有监督、半监督和无监督（盲目）线性混合。图像处理和机器学习技术的进步对混合有很大影响。本文提供了高级和传统混合方法的总览，并对这些方法进行了严格的比较。我们在三个模拟 dataset和两个实际 dataset上进行了比较性研究，研究结果表明不同混合类型在不同混合场景中的优势。此外，我们还提供了一个可以在 <https://github.com/BehnoodRasti/HySUPP> 上下载的开源Python包，以便重现结果。
</details></li>
</ul>
<hr>
<h2 id="Noise-Sensitivity-and-Stability-of-Deep-Neural-Networks-for-Binary-Classification"><a href="#Noise-Sensitivity-and-Stability-of-Deep-Neural-Networks-for-Binary-Classification" class="headerlink" title="Noise Sensitivity and Stability of Deep Neural Networks for Binary Classification"></a>Noise Sensitivity and Stability of Deep Neural Networks for Binary Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09374">http://arxiv.org/abs/2308.09374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan Jonasson, Jeffrey E. Steif, Olof Zetterqvist</li>
<li>for: 研究深度神经网络（DNN）分类器的不稳定性现象，从布尔函数的角度出发，检查某些布尔函数表示的常见DNN模型是否对随机噪声敏感或稳定。</li>
<li>methods: 使用布尔函数理论中的随机噪声敏感和稳定性概念，对常见的批处理和卷积模型进行分析和研究。</li>
<li>results: 对于两种标准的DNN模型——批处理和卷积模型——在初始化为高斯权重情况下，研究其在随机噪声下的性质和特性。<details>
<summary>Abstract</summary>
A first step is taken towards understanding often observed non-robustness phenomena of deep neural net (DNN) classifiers. This is done from the perspective of Boolean functions by asking if certain sequences of Boolean functions represented by common DNN models are noise sensitive or noise stable, concepts defined in the Boolean function literature. Due to the natural randomness in DNN models, these concepts are extended to annealed and quenched versions. Here we sort out the relation between these definitions and investigate the properties of two standard DNN architectures, the fully connected and convolutional models, when initiated with Gaussian weights.
</details>
<details>
<summary>摘要</summary>
第一步是对深度神经网络（DNN）分类器的常见非稳定性现象进行理解。这是通过将Boolean函数的概念应用到常见DNN模型中，以查核这些模型是否对随机变量敏感或稳定的。由于DNN模型的自然随机性，这些概念被扩展到气化和固化版本。我们调整这些定义并调查两种标准DNN架构——完全连接和卷积模型——在各种初始Conditions下的性质。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that instead.
</details></li>
</ul>
<hr>
<h2 id="Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers"><a href="#Which-Transformer-to-Favor-A-Comparative-Analysis-of-Efficiency-in-Vision-Transformers" class="headerlink" title="Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers"></a>Which Transformer to Favor: A Comparative Analysis of Efficiency in Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09372">http://arxiv.org/abs/2308.09372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tobna/whattransformertofavor">https://github.com/tobna/whattransformertofavor</a></li>
<li>paper_authors: Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel</li>
<li>for: 本研究旨在对多种效率准备的视图变换器和相关架构进行全面的分析，以提供一个公共的基准，从而为实践者和研究人员提供有价值的指导。</li>
<li>methods: 本研究使用了多种性能指标来评估多种效率准备的视图变换器和相关架构，包括ViT和其他一些替代方案。</li>
<li>results: 研究发现，尽管存在多种宣称更高效的方法，但ViT仍然在多个效率指标上保持Pareto优化的状态，而且混合注意力-CNN模型在具有低执行内存和参数量的情况下表现特别好。此外，研究还发现了图像大小与模型大小之间的关系，以及计算Memory和训练内存之间的正相关关系。<details>
<summary>Abstract</summary>
The growing popularity of Vision Transformers as the go-to models for image classification has led to an explosion of architectural modifications claiming to be more efficient than the original ViT. However, a wide diversity of experimental conditions prevents a fair comparison between all of them, based solely on their reported results. To address this gap in comparability, we conduct a comprehensive analysis of more than 30 models to evaluate the efficiency of vision transformers and related architectures, considering various performance metrics. Our benchmark provides a comparable baseline across the landscape of efficiency-oriented transformers, unveiling a plethora of surprising insights. For example, we discover that ViT is still Pareto optimal across multiple efficiency metrics, despite the existence of several alternative approaches claiming to be more efficient. Results also indicate that hybrid attention-CNN models fare particularly well when it comes to low inference memory and number of parameters, and also that it is better to scale the model size, than the image size. Furthermore, we uncover a strong positive correlation between the number of FLOPS and the training memory, which enables the estimation of required VRAM from theoretical measurements alone.   Thanks to our holistic evaluation, this study offers valuable insights for practitioners and researchers, facilitating informed decisions when selecting models for specific applications. We publicly release our code and data at https://github.com/tobna/WhatTransformerToFavor
</details>
<details>
<summary>摘要</summary>
“vision transformer”的崛起使得许多架构 modifications 宣称更有效性 than the original ViT，但是实验环境的多样性使得比较这些模型的公平性很难。为了解决这个问题，我们进行了详细的30多个模型的分析，以评估视觉 трансформа器和相关的架构在不同的效率指标下的表现。我们的参考基准提供了跨多种效率对应的比较基线，揭露了许多意外的发现。例如，我们发现，即使有许多替代方案，ViT仍然在多个效率指标上是Pareto优化的。实验结果显示，混合注意力-CNN模型在内存少量和参数少量的情况下表现 particulary well，而且较好的对image size的调整，而不是模型size。此外，我们发现了许多FLOPS和训练内存之间的强正相关，这使得从理论测量 alone 可以估算需要的VRAM。这些结果将为实践者和研究人员提供有用的参考，帮助他们在特定应用中选择合适的模型。我们将代码和数据公开发布在https://github.com/tobna/WhatTransformerToFavor上。”
</details></li>
</ul>
<hr>
<h2 id="A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin"><a href="#A-tailored-Handwritten-Text-Recognition-System-for-Medieval-Latin" class="headerlink" title="A tailored Handwritten-Text-Recognition System for Medieval Latin"></a>A tailored Handwritten-Text-Recognition System for Medieval Latin</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09368">http://arxiv.org/abs/2308.09368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Koch, Gilary Vera Nuñez, Esteban Garces Arias, Christian Heumann, Matthias Schöffel, Alexander Häberlin, Matthias Aßenmacher</li>
<li>for: This paper aims to digitize the Medieval Latin Dictionary by using Handwritten Text Recognition (HTR) to transcribe handwritten lemmas on record cards.</li>
<li>methods: The authors employ two state-of-the-art image segmentation models to prepare the initial data set, and experiment with different transformer-based models and data augmentation techniques to improve the HTR performance.</li>
<li>results: The best-performing setup achieved a Character Error Rate (CER) of 0.015, which is superior to the commercial Google Cloud Vision model and shows more stable performance.<details>
<summary>Abstract</summary>
The Bavarian Academy of Sciences and Humanities aims to digitize its Medieval Latin Dictionary. This dictionary entails record cards referring to lemmas in medieval Latin, a low-resource language. A crucial step of the digitization process is the Handwritten Text Recognition (HTR) of the handwritten lemmas found on these record cards. In our work, we introduce an end-to-end pipeline, tailored to the medieval Latin dictionary, for locating, extracting, and transcribing the lemmas. We employ two state-of-the-art (SOTA) image segmentation models to prepare the initial data set for the HTR task. Furthermore, we experiment with different transformer-based models and conduct a set of experiments to explore the capabilities of different combinations of vision encoders with a GPT-2 decoder. Additionally, we also apply extensive data augmentation resulting in a highly competitive model. The best-performing setup achieved a Character Error Rate (CER) of 0.015, which is even superior to the commercial Google Cloud Vision model, and shows more stable performance.
</details>
<details>
<summary>摘要</summary>
《巴伐利亚科学院计划将中世纪拉丁词典数字化。这个词典包含手写记录卡上的中世纪拉丁词汇，这是一种低资源语言。我们的工作是设计一个端到端管道，用于在记录卡上找到、提取和转写词汇。我们使用两种最新的图像分割模型来准备初始数据集，并运用多种变换器模型和GPT-2解码器进行实验，以探索不同组合的可能性。此外，我们还进行了广泛的数据增强，实现了非常竞争力强的模型。最佳配置达到了字符错误率（CER）0.015，超越了商业Google云视觉模型，表现更稳定。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The Traditional Chinese writing system is used in Taiwan and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="On-the-Approximation-of-Bi-Lipschitz-Maps-by-Invertible-Neural-Networks"><a href="#On-the-Approximation-of-Bi-Lipschitz-Maps-by-Invertible-Neural-Networks" class="headerlink" title="On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks"></a>On the Approximation of Bi-Lipschitz Maps by Invertible Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09367">http://arxiv.org/abs/2308.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Zehui Zhou, Jun Zou</li>
<li>for: 这个论文主要是为了研究嵌入型神经网络（INNs）的表达能力和应用场景。</li>
<li>methods: 论文使用了 coupling-based INNs 来表示bi-Lipschitz连续函数的映射，并提出了一种基于模型缩放、主成分分析和 INNs 的方法来近似无穷维空间上的bi-Lipschitz映射。</li>
<li>results: 论文显示了 coupling-based INNs 可以同时良好地近似前向和反向映射，并提出了一种可以同时近似前向和反向映射的方法。此外，论文还进行了初步的数值实验，并表明了该方法的可行性。<details>
<summary>Abstract</summary>
Invertible neural networks (INNs) represent an important class of deep neural network architectures that have been widely used in several applications. The universal approximation properties of INNs have also been established recently. However, the approximation rate of INNs is largely missing. In this work, we provide an analysis of the capacity of a class of coupling-based INNs to approximate bi-Lipschitz continuous mappings on a compact domain, and the result shows that it can well approximate both forward and inverse maps simultaneously. Furthermore, we develop an approach for approximating bi-Lipschitz maps on infinite-dimensional spaces that simultaneously approximate the forward and inverse maps, by combining model reduction with principal component analysis and INNs for approximating the reduced map, and we analyze the overall approximation error of the approach. Preliminary numerical results show the feasibility of the approach for approximating the solution operator for parameterized second-order elliptic problems.
</details>
<details>
<summary>摘要</summary>
insehen neural networks (INNs) represent an important class of deep neural network architectures that have been widely used in several applications. The universal approximation properties of INNs have also been established recently. However, the approximation rate of INNs is largely missing. In this work, we provide an analysis of the capacity of a class of coupling-based INNs to approximate bi-Lipschitz continuous mappings on a compact domain, and the result shows that it can well approximate both forward and inverse maps simultaneously. Furthermore, we develop an approach for approximating bi-Lipschitz maps on infinite-dimensional spaces that simultaneously approximate the forward and inverse maps, by combining model reduction with principal component analysis and INNs for approximating the reduced map, and we analyze the overall approximation error of the approach. Preliminary numerical results show the feasibility of the approach for approximating the solution operator for parameterized second-order elliptic problems.Here's the translation in Traditional Chinese:这里的文本翻译为简化字的中文：对于深度神经网络（INNs）来说，它们是一种广泛使用的深度学习架构，而且在最近的研究中，它们的通用预测性也得到了证明。然而，INNs的预测率却受到了很大的限制。在这个研究中，我们提供了一个 coupling-based INNs 的容量分析，该分析表明这种 INNs 可以同时对内部和外部的映射进行良好的预测。此外，我们还开发了一种可以同时预测前向和反向映射的方法，这种方法基于模型简化、主成分分析和 INNs 的 combinaison，并且分析了这个方法的总预测误差。preliminary numerical results 表明这个方法可以实现解析Operator的解析。
</details></li>
</ul>
<hr>
<h2 id="Multi-feature-concatenation-and-multi-classifier-stacking-an-interpretable-and-generalizable-machine-learning-method-for-MDD-discrimination-with-rsfMRI"><a href="#Multi-feature-concatenation-and-multi-classifier-stacking-an-interpretable-and-generalizable-machine-learning-method-for-MDD-discrimination-with-rsfMRI" class="headerlink" title="Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI"></a>Multi-feature concatenation and multi-classifier stacking: an interpretable and generalizable machine learning method for MDD discrimination with rsfMRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09360">http://arxiv.org/abs/2308.09360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunsong Luo, Wenyu Chen, Ling Zhan, Jiang Qiu, Tao Jia</li>
<li>For: The paper aims to improve the accuracy of diagnosing major depressive disorder (MDD) using resting-state functional MRI (rsfMRI) and machine learning algorithms.* Methods: The paper proposes a machine learning method called Multi-Feature Multi-Classifier (MFMC) that concatenates multiple features and stacks multiple classifiers to discriminate MDD patients from normal controls. The method is tested on the REST-meta-MDD data set, which contains 2428 subjects from 25 different sites.* Results: The paper reports that MFMC achieves 96.9% MDD discrimination accuracy, outperforming existing methods. The method is also validated for its generalizability by showing good performance when the training and testing subjects are from independent sites. Additionally, the paper identifies 13 feature values related to 9 brain regions that contribute most to the classification and demonstrate significant differences at the group level.<details>
<summary>Abstract</summary>
Major depressive disorder is a serious and heterogeneous psychiatric disorder that needs accurate diagnosis. Resting-state functional MRI (rsfMRI), which captures multiple perspectives on brain structure, function, and connectivity, is increasingly applied in the diagnosis and pathological research of mental diseases. Different machine learning algorithms are then developed to exploit the rich information in rsfMRI and discriminate MDD patients from normal controls. Despite recent advances reported, the discrimination accuracy has room for further improvement. The generalizability and interpretability of the method are not sufficiently addressed either. Here, we propose a machine learning method (MFMC) for MDD discrimination by concatenating multiple features and stacking multiple classifiers. MFMC is tested on the REST-meta-MDD data set that contains 2428 subjects collected from 25 different sites. MFMC yields 96.9% MDD discrimination accuracy, demonstrating a significant improvement over existing methods. In addition, the generalizability of MFMC is validated by the good performance when the training and testing subjects are from independent sites. The use of XGBoost as the meta classifier allows us to probe the decision process of MFMC. We identify 13 feature values related to 9 brain regions including the posterior cingulate gyrus, superior frontal gyrus orbital part, and angular gyrus, which contribute most to the classification and also demonstrate significant differences at the group level. The use of these 13 feature values alone can reach 87% of MFMC's full performance when taking all feature values. These features may serve as clinically useful diagnostic and prognostic biomarkers for mental disorders in the future.
</details>
<details>
<summary>摘要</summary>
大笔迹谱精神疾病是一种严重多样的心理疾病，需要准确诊断。休息态功能磁共振成像（rsfMRI）已成为诊断和病理研究心理疾病的新技术。不同的机器学习算法被开发出来利用rsfMRI中的多种信息，以分辨抑郁症患者和正常控制人群。尽管有最新的进展报道，但iscrimination accuracy还有很大的提升空间。此外，方法的一般可行性和可解释性也没有充分考虑。我们提出了一种机器学习方法（MFMC），通过 concatenating多个特征和堆叠多个分类器来实现抑郁症分辨。MFMC在 REST-meta-MDD 数据集上进行测试，包括 2428 名参与者，来自 25 个不同的站点。MFMC 的抑郁症分辨率为 96.9%，显著高于现有方法。此外，我们还 validate了 MFMC 的一般可行性，通过在训练和测试集来自独立站点时进行测试。使用 XGBoost 作为元分类器，我们可以探索 MFMC 的决策过程。我们发现了 13 个特征值，与 9 个大脑区域相关，包括后中枢孔隙、上前叶颞部和抽象皮层，这些特征值对分类具有最大的贡献，同时在群体水平也存在显著差异。这些特征值可能在未来作为心理疾病的临床有用的诊断和预后标志。
</details></li>
</ul>
<hr>
<h2 id="RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training"><a href="#RLIPv2-Fast-Scaling-of-Relational-Language-Image-Pre-training" class="headerlink" title="RLIPv2: Fast Scaling of Relational Language-Image Pre-training"></a>RLIPv2: Fast Scaling of Relational Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09351">http://arxiv.org/abs/2308.09351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacobyuan7/rlipv2">https://github.com/jacobyuan7/rlipv2</a></li>
<li>paper_authors: Hangjie Yuan, Shiwei Zhang, Xiang Wang, Samuel Albanie, Yining Pan, Tao Feng, Jianwen Jiang, Dong Ni, Yingya Zhang, Deli Zhao</li>
<li>For: 本文提出了一种名为RLIPv2的模型，用于提高计算机视觉任务中关系逻辑能力。RLIPv2可以快速地训练和 fine-tune，并且可以在大规模 pseudo-labelled scene graph 数据上进行预训练。* Methods: RLIPv2 使用了一种名为ALIF的机制，它可以在早期和深度的干扰模式中进行快速的语言-图像 fusión。此外，RLIPv2 还使用了一种名为 Relation Tagger 的工具，用于生成大量的自由形式关系标签。* Results: 通过广泛的实验，RLIPv2 在人物-物体互动检测和场景图生成任务上达到了状态的前导性表现。RLIPv2 可以在完全训练、几何shot 和零shot 设置下达到 state-of-the-art 性能。特别是，RLIPv2 的最大模型在没有任何训练的情况下可以达到 23.29mAP 的表现，并且在 1% 数据上可以达到 32.22mAP，在 100% 数据上可以达到 45.09mAP。<details>
<summary>Abstract</summary>
Relational Language-Image Pre-training (RLIP) aims to align vision representations with relational texts, thereby advancing the capability of relational reasoning in computer vision tasks. However, hindered by the slow convergence of RLIPv1 architecture and the limited availability of existing scene graph data, scaling RLIPv1 is challenging. In this paper, we propose RLIPv2, a fast converging model that enables the scaling of relational pre-training to large-scale pseudo-labelled scene graph data. To enable fast scaling, RLIPv2 introduces Asymmetric Language-Image Fusion (ALIF), a mechanism that facilitates earlier and deeper gated cross-modal fusion with sparsified language encoding layers. ALIF leads to comparable or better performance than RLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtain scene graph data at scale, we extend object detection datasets with free-form relation labels by introducing a captioner (e.g., BLIP) and a designed Relation Tagger. The Relation Tagger assigns BLIP-generated relation texts to region pairs, thus enabling larger-scale relational pre-training. Through extensive experiments conducted on Human-Object Interaction Detection and Scene Graph Generation, RLIPv2 shows state-of-the-art performance on three benchmarks under fully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2 achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP with just 1% data and yields 45.09mAP with 100% data. Code and models are publicly available at https://github.com/JacobYuan7/RLIPv2.
</details>
<details>
<summary>摘要</summary>
RLIPv2是一种快速结合模型，旨在将视觉表示与关系文本相align，从而提高计算机视觉任务中的关系逻辑能力。然而，RLIPv1架构的慢速收敛和现有场景图数据的有限性，使得扩展RLIPv1的 scaling 困难。在这篇论文中，我们提出RLIPv2，一种快速收敛的模型，可以在大规模 pseudo-labelled 场景图数据上进行关系预训练。为了实现快速收敛，RLIPv2引入了Asymmetric Language-Image Fusion（ALIF）机制，该机制使得在执行cross-modal fusión之前，可以通过隐藏层进行早期和深度的受限缓存语言编码层。ALIF使得RLIPv2在预训练和精度调整中比RLIPv1更快收敛。为了获得大规模场景图数据，我们将对象检测dataset扩展，并在其中添加自由形态关系标签。我们引入了一个captioner（例如BLIP）和一个设计的Relation Tagger，以便为region对 assigning BLIP生成的关系文本。这种方法使得我们可以在大规模的关系预训练中获得更多的数据。通过广泛的实验，我们证明RLIPv2在人物物体互动检测和场景图生成中具有状态机器人的表现，在三个标准准则下达到了全finetuning、少shot和零shot设置下的最佳性能。特别是，RLIPv2最大化的模型在HICO-DET无需任何调整就达到了23.29mAP，并且在1%数据上达到了32.22mAP，在100%数据上达到了45.09mAP。我们的代码和模型在https://github.com/JacobYuan7/RLIPv2上公开。
</details></li>
</ul>
<hr>
<h2 id="Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations"><a href="#Denoising-diffusion-based-MR-to-CT-image-translation-enables-whole-spine-vertebral-segmentation-in-2D-and-3D-without-manual-annotations" class="headerlink" title="Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations"></a>Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09345">http://arxiv.org/abs/2308.09345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robert-graf/readable-conditional-denoising-diffusion">https://github.com/robert-graf/readable-conditional-denoising-diffusion</a></li>
<li>paper_authors: Robert Graf, Joachim Schmitt, Sarah Schlaeger, Hendrik Kristian Möller, Vasiliki Sideri-Lampretsa, Anjany Sekuboyina, Sandro Manuel Krieg, Benedikt Wiestler, Bjoern Menze, Daniel Rueckert, Jan Stefan Kirschke<br>for: 这个研究的目的是将MR图像翻译成CT图像，以便更好地分类和诊断脊梁部分的疾病。methods: 该研究使用了两种方法： paired image-to-image translation 和 unpaired image-to-image translation。 paired 方法使用了 landmark-based registration 来对图像进行对齐，而 unpaired 方法使用了 contrastive unpaired translation 和 SynDiff 来进行对齐。results: 研究发现，paired 方法和 SynDiff 在对照数据上 exhibited 相似的翻译性和 Dice 分数。 DDIM 图像模式 achieve 最高的图像质量。 SynDiff、Pix2Pix 和 DDIM 图像模式在 paired 数据上都达到了 Dice 分数的 0.77。 在沿着脊梁轴的旋转变换下，至少需要两个附加的标记来进行注册。 3D 翻译方法在 Dice 分数和图像质量上超过了 2D 方法，并提供了更高分辨率的分类结果。<details>
<summary>Abstract</summary>
Background: Automated segmentation of spinal MR images plays a vital role both scientifically and clinically. However, accurately delineating posterior spine structures presents challenges.   Methods: This retrospective study, approved by the ethical committee, involved translating T1w and T2w MR image series into CT images in a total of n=263 pairs of CT/MR series. Landmark-based registration was performed to align image pairs. We compared 2D paired (Pix2Pix, denoising diffusion implicit models (DDIM) image mode, DDIM noise mode) and unpaired (contrastive unpaired translation, SynDiff) image-to-image translation using "peak signal to noise ratio" (PSNR) as quality measure. A publicly available segmentation network segmented the synthesized CT datasets, and Dice scores were evaluated on in-house test sets and the "MRSpineSeg Challenge" volumes. The 2D findings were extended to 3D Pix2Pix and DDIM.   Results: 2D paired methods and SynDiff exhibited similar translation performance and Dice scores on paired data. DDIM image mode achieved the highest image quality. SynDiff, Pix2Pix, and DDIM image mode demonstrated similar Dice scores (0.77). For craniocaudal axis rotations, at least two landmarks per vertebra were required for registration. The 3D translation outperformed the 2D approach, resulting in improved Dice scores (0.80) and anatomically accurate segmentations in a higher resolution than the original MR image.   Conclusion: Two landmarks per vertebra registration enabled paired image-to-image translation from MR to CT and outperformed all unpaired approaches. The 3D techniques provided anatomically correct segmentations, avoiding underprediction of small structures like the spinous process.
</details>
<details>
<summary>摘要</summary>
背景：自动分割神经穿梭MR图像在科学和临床领域中发挥重要作用。然而，准确地界定后方脊梁结构具有挑战。方法：这是一项回顾性研究，得到了伦敦委员会的批准。研究通过将T1w和T2wMR图像序列翻译成CT图像序列，共计n=263对CT/MR序列进行了同步注册。我们使用了“峰峰信号响应比”（PSNR）作为质量指标，并对在家试验集和“MRSpineSeg Challenge”volumes上进行了Dice分数的评估。使用了一个公共可用的 segmentation network 将生成的Synthesized CTdataset中进行了 segmentation。结果：2D paired方法和SynDiff在对应数据上显示了类似的翻译性和Dice分数。DDIM图像模式实现了最高的图像质量。SynDiff、Pix2Pix和DDIM图像模式在对应数据上具有类似的Dice分数（0.77）。对于脊梁轴向旋转，至少需要两个附加的Landmark每个vertebra进行注册。3D翻译超过了2D方法，导致了改进的Dice分数（0.80）和高分辨率的正确分 segmentation。结论：对每个vertebra需要至少两个Landmark注册，以实现paired图像-图像翻译从MR到CT，并且超过了所有的不对应方法。3D技术提供了正确的分 segmentation，避免了小结构 like spinous process的下预测。
</details></li>
</ul>
<hr>
<h2 id="Surprise-machines-revealing-Harvard-Art-Museums’-image-collection"><a href="#Surprise-machines-revealing-Harvard-Art-Museums’-image-collection" class="headerlink" title="Surprise machines: revealing Harvard Art Museums’ image collection"></a>Surprise machines: revealing Harvard Art Museums’ image collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09343">http://arxiv.org/abs/2308.09343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dario Rodighiero, Lins Derry, Douglas Duhaime, Jordan Kruguer, Maximilian C. Mueller, Christopher Pietsch, Jeffrey T. Schnapp, Jeff Steward</li>
<li>for: 这个研究是为了使用人工智能技术来visualize哈佛艺术博物馆的全部图像收藏，以便开拓不可能 для访客访问的更多than 200,000个物品的新视野。</li>
<li>methods: 这个项目使用了人工智能技术，设计了一个舞台式界面，将评估者的移动与多个特有视角的收藏相连接，以创造对访客的惊喜。</li>
<li>results: 这个项目成功地使用了人工智能技术来显示大量图像，创造了对访客的惊喜和新的视野，开拓了不可能 для访客访问的更多than 200,000个物品的新视野。<details>
<summary>Abstract</summary>
Surprise Machines is a project of experimental museology that sets out to visualize the entire image collection of the Harvard Art Museums, intending to open up unexpected vistas on more than 200,000 objects usually inaccessible to visitors. Part of the exhibition Curatorial A(i)gents organized by metaLAB (at) Harvard, the project explores the limits of artificial intelligence to display a large set of images and create surprise among visitors. To achieve such a feeling of surprise, a choreographic interface was designed to connect the audience's movement with several unique views of the collection.
</details>
<details>
<summary>摘要</summary>
“抽象机器”是哈佛艺术博物馆的一个实验博物馆项目，旨在视觉化哈佛艺术博物馆的全幅图像收藏，以创造未知的景象，并对访客展示200,000件以上不可见的物品。这个项目是metaLAB（@）哈佛的 Curatorial A(i)gents 展览的一部分，探索人工智能可以显示大量图像的限制，并创造访客的惊喜感。为了实现这种感觉，设计了一个舞蹈式的用户界面，让访客的运动与收藏中的多个独特视野相连。
</details></li>
</ul>
<hr>
<h2 id="Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models"><a href="#Document-Automation-Architectures-Updated-Survey-in-Light-of-Large-Language-Models" class="headerlink" title="Document Automation Architectures: Updated Survey in Light of Large Language Models"></a>Document Automation Architectures: Updated Survey in Light of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09341">http://arxiv.org/abs/2308.09341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Ahmadi Achachlouei, Omkar Patil, Tarun Joshi, Vijayan N. Nair</li>
<li>for: 这篇论文主要是为了对文档自动化（DA）的当前状况进行评估和概述。</li>
<li>methods: 本论文使用了学术研究的 DA 架构和技术来对不同来源的输入自动创建和组合，并且使用了定义模板来生成符合要求的文档。</li>
<li>results: 本论文通过对学术研究的 DA 架构和技术进行评估和概述，提供了更清晰的 DA 定义和特征，并且预测了基于生成 AI 和大语言模型的新研究机遇。<details>
<summary>Abstract</summary>
This paper surveys the current state of the art in document automation (DA). The objective of DA is to reduce the manual effort during the generation of documents by automatically creating and integrating input from different sources and assembling documents conforming to defined templates. There have been reviews of commercial solutions of DA, particularly in the legal domain, but to date there has been no comprehensive review of the academic research on DA architectures and technologies. The current survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features, identifies state-of-the-art DA architectures and technologies in academic research, and provides ideas that can lead to new research opportunities within the DA field in light of recent advances in generative AI and large language models.
</details>
<details>
<summary>摘要</summary>
This survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features. It identifies state-of-the-art DA architectures and technologies in academic research and suggests new research opportunities in the field of DA, taking into account recent advances in generative AI and large language models.
</details></li>
</ul>
<hr>
<h2 id="Causal-Interpretable-Progression-Trajectory-Analysis-of-Chronic-Disease"><a href="#Causal-Interpretable-Progression-Trajectory-Analysis-of-Chronic-Disease" class="headerlink" title="Causal Interpretable Progression Trajectory Analysis of Chronic Disease"></a>Causal Interpretable Progression Trajectory Analysis of Chronic Disease</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09735">http://arxiv.org/abs/2308.09735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhoujian Sun, Wenzhuo Zhang, Zhengxing Huang, Nai Ding</li>
<li>for: 预测疾病进程轨迹和决策支持</li>
<li>methods:  combining trajectory prediction and causal discovery</li>
<li>results: 提供精度预测疾病进程轨迹和揭示 causal 关系，增强模型的解释性In English, this means:</li>
<li>for: Predicting disease progression trajectories and supporting clinical decisions</li>
<li>methods: Combining trajectory prediction and causal discovery</li>
<li>results: Providing accurate predictions of disease progression trajectories and uncovering causal relationships, enhancing the interpretability of the model.<details>
<summary>Abstract</summary>
Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment effects, even in the presence of unmeasured confounders, the CTP provides valuable insights for clinical decision-making. We evaluate the performance of the CTP using simulated and real medical datasets. Experimental results demonstrate that our model achieves satisfactory performance, highlighting its potential to assist clinical decisions.
</details>
<details>
<summary>摘要</summary>
Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment effects, even in the presence of unmeasured confounders, the CTP provides valuable insights for clinical decision-making. We evaluate the performance of the CTP using simulated and real medical datasets. Experimental results demonstrate that our model achieves satisfactory performance, highlighting its potential to assist clinical decisions.Here is the word-for-word translation of the text into Simplified Chinese: Chronic disease is the leading cause of death, emphasizing the need for accurate prediction of disease progression trajectories and informed clinical decision-making. Machine learning (ML) models have shown promise in this domain by capturing non-linear patterns within patient features. However, existing ML-based models lack the ability to provide causal interpretable predictions and estimate treatment effects, limiting their decision-assisting perspective. In this study, we propose a novel model called causal trajectory prediction (CTP) to tackle the limitation. The CTP model combines trajectory prediction and causal discovery to enable accurate prediction of disease progression trajectories and uncovering causal relationships between features. By incorporating a causal graph into the prediction process, CTP ensures that ancestor features are not influenced by treatment on descendant features, thereby enhancing the interpretability of the model. By estimating the bounds of treatment effects, even in the presence of unmeasured confounders, the CTP provides valuable insights for clinical decision-making. We evaluate the performance of the CTP using simulated and real medical datasets. Experimental results demonstrate that our model achieves satisfactory performance, highlighting its potential to assist clinical decisions.
</details></li>
</ul>
<hr>
<h2 id="Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis"><a href="#Towards-Attack-tolerant-Federated-Learning-via-Critical-Parameter-Analysis" class="headerlink" title="Towards Attack-tolerant Federated Learning via Critical Parameter Analysis"></a>Towards Attack-tolerant Federated Learning via Critical Parameter Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09318">http://arxiv.org/abs/2308.09318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sungwon-han/fedcpa">https://github.com/sungwon-han/fedcpa</a></li>
<li>paper_authors: Sungwon Han, Sungwon Park, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, Meeyoung Cha</li>
<li>for: 本研究旨在提出一种防御欺诈攻击的聚合方法，以帮助在分布式学习中训练共享模型。</li>
<li>methods: 本研究使用了批处理学习和敏感参数分析（FedCPA）来防御欺诈攻击。</li>
<li>results: 实验结果显示，FedCPA模型在不同的攻击场景下，能够更高效地防御欺诈攻击，并且在多个数据集上表现出色。<details>
<summary>Abstract</summary>
Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffective under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Critical Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种在分布式方式下训练共享模型，而不需要客户端对彼此分享私人数据。 federated learning 系统容易受到恶意客户端发送false更新给中央服务器的恶意攻击。现有的防御策略在非同分布数据设置下无效。这篇论文提出了一种新的防御策略，即 FedCPA（ Federated learning with Critical Parameter Analysis）。我们的攻击忍受聚合方法基于本地模型的惊异性和恶意模型的不同性。实验结果表明，我们的模型在不同的攻击场景下，在防御恶意攻击方面表现出色，超过了现有的防御策略。
</details></li>
</ul>
<hr>
<h2 id="Path-Signatures-for-Seizure-Forecasting"><a href="#Path-Signatures-for-Seizure-Forecasting" class="headerlink" title="Path Signatures for Seizure Forecasting"></a>Path Signatures for Seizure Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09312">http://arxiv.org/abs/2308.09312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas F. Haderlein, Andre D. H. Peterson, Parvin Zarei Eskikand, Mark J. Cook, Anthony N. Burkitt, Iven M. Y. Mareels, David B. Grayden</li>
<li>for: 预测脑动力系统的状态从观测时间序列数据中</li>
<li>methods: 使用现有和新的特征提取算法，包括路径签名，一种新的时间序列分析方法，以及统计分类算法和内置的子集选择来自动发现和评估患者特定的病理特征，以预测发作</li>
<li>results: 通过使用这些复杂、非线性特征，对发作预测进行了自动发现和评估，并与基于线性特征进行了比较<details>
<summary>Abstract</summary>
Forecasting the state of a system from an observed time series is the subject of research in many domains, such as computational neuroscience. Here, the prediction of epileptic seizures from brain measurements is an unresolved problem. There are neither complete models describing underlying brain dynamics, nor do individual patients exhibit a single seizure onset pattern, which complicates the development of a `one-size-fits-all' solution. Based on a longitudinal patient data set, we address the automated discovery and quantification of statistical features (biomarkers) that can be used to forecast seizures in a patient-specific way. We use existing and novel feature extraction algorithms, in particular the path signature, a recent development in time series analysis. Of particular interest is how this set of complex, nonlinear features performs compared to simpler, linear features on this task. Our inference is based on statistical classification algorithms with in-built subset selection to discern time series with and without an impending seizure while selecting only a small number of relevant features. This study may be seen as a step towards a generalisable pattern recognition pipeline for time series in a broader context.
</details>
<details>
<summary>摘要</summary>
预测系统的状态从观测时序列是许多领域的研究主题，如计算神经科学。在这里，从脑测量获取的癫痫病发症预测是一个未解决的问题。没有完整的模型描述下面动力学，也没有各个患者表现出单一的癫痫病发症开始模式，这使得开发一个"一Size-fits-all"解决方案变得复杂。基于长期患者数据集，我们addresses自动发现和量化统计特征（生物标志），可以用于预测患者特定的癫痫病发症。我们使用现有和新的特征提取算法，特别是路径签名，是时间序列分析的新发展。我们的推断基于统计分类算法，并包含内置的子集选择，以分辨时间序列中有无危险癫痫病发症，并且只选择一小部分相关的特征。这项研究可能被看作为时间序列普遍的模式识别管道的一步。
</details></li>
</ul>
<hr>
<h2 id="Variance-reduction-techniques-for-stochastic-proximal-point-algorithms"><a href="#Variance-reduction-techniques-for-stochastic-proximal-point-algorithms" class="headerlink" title="Variance reduction techniques for stochastic proximal point algorithms"></a>Variance reduction techniques for stochastic proximal point algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09310">http://arxiv.org/abs/2308.09310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheik Traoré, Vassilis Apidopoulos, Saverio Salzo, Silvia Villa</li>
<li>for: 针对于贝叶斯随机Gradient方法的性能提高，使用变量减少技术广泛应用。</li>
<li>methods: 提出了随机 proximal 点算法，并对其进行了随机 proximal 版本的SVRG、SAGA以及一些变体的研究。</li>
<li>results: 对iterates和目标函数值进行了许多的收敛结果，并在PL条件下获得了线性收敛率。实验表明，采用 proximal 变量减少方法比 Gradient 方法更稳定，尤其是对步长的选择。<details>
<summary>Abstract</summary>
In the context of finite sums minimization, variance reduction techniques are widely used to improve the performance of state-of-the-art stochastic gradient methods. Their practical impact is clear, as well as their theoretical properties. Stochastic proximal point algorithms have been studied as an alternative to stochastic gradient algorithms since they are more stable with respect to the choice of the stepsize but a proper variance reduced version is missing. In this work, we propose the first study of variance reduction techniques for stochastic proximal point algorithms. We introduce a stochastic proximal version of SVRG, SAGA, and some of their variants for smooth and convex functions. We provide several convergence results for the iterates and the objective function values. In addition, under the Polyak-{\L}ojasiewicz (PL) condition, we obtain linear convergence rates for the iterates and the function values. Our numerical experiments demonstrate the advantages of the proximal variance reduction methods over their gradient counterparts, especially about the stability with respect to the choice of the step size.
</details>
<details>
<summary>摘要</summary>
在finite sums minimization中， variance reduction techniques 广泛应用以提高现有的泛型梯度方法性能。其实际影响明显，同时其理论性质也很清晰。随机 proximal point algoritms 被视为梯度算法的代替方案，因为它们对步长的选择更加稳定。在这项工作中，我们提出了variance reduction techniques的首次研究 для随机 proximal point algoritms。我们引入了随机 proximal版本的SVRG、SAGA以及一些其他的变体，用于对凸函数进行优化。我们提供了许多收敛结果 для迭代值和目标函数值。此外，在Polyak-{\L}ojasiewicz（PL）条件下，我们得到了梯度值和函数值的线性收敛率。我们的numerical experiments表明， proximal variance reduction方法比其梯度对应方法更加稳定，特别是对步长的选择。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-enhanced-next-POI-recommendation-by-leveraging-check-ins-from-auxiliary-cities"><a href="#Meta-learning-enhanced-next-POI-recommendation-by-leveraging-check-ins-from-auxiliary-cities" class="headerlink" title="Meta-learning enhanced next POI recommendation by leveraging check-ins from auxiliary cities"></a>Meta-learning enhanced next POI recommendation by leveraging check-ins from auxiliary cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09309">http://arxiv.org/abs/2308.09309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oli-wang/merec">https://github.com/oli-wang/merec</a></li>
<li>paper_authors: Jinze Wang, Lu Zhang, Zhu Sun, Yew-Soon Ong</li>
<li>for: 提高城市级用户偏好学习的效果，弥补城市级用户历史检查入数据缺乏问题。</li>
<li>methods: 基于城市级用户历史检查入数据和相关城市之间的相似性关系，提出一种Meta-learning Enhanced next POI Recommendation（MERec）框架。MERec利用了城市级用户检查入行为之间的相似性，在meta-learning paradigm中帮助推断用户偏好。</li>
<li>results: 对比于现有算法，MERec表现出了显著的优势。<details>
<summary>Abstract</summary>
Most existing point-of-interest (POI) recommenders aim to capture user preference by employing city-level user historical check-ins, thus facilitating users' exploration of the city. However, the scarcity of city-level user check-ins brings a significant challenge to user preference learning. Although prior studies attempt to mitigate this challenge by exploiting various context information, e.g., spatio-temporal information, they ignore to transfer the knowledge (i.e., common behavioral pattern) from other relevant cities (i.e., auxiliary cities). In this paper, we investigate the effect of knowledge distilled from auxiliary cities and thus propose a novel Meta-learning Enhanced next POI Recommendation framework (MERec). The MERec leverages the correlation of check-in behaviors among various cities into the meta-learning paradigm to help infer user preference in the target city, by holding the principle of "paying more attention to more correlated knowledge". Particularly, a city-level correlation strategy is devised to attentively capture common patterns among cities, so as to transfer more relevant knowledge from more correlated cities. Extensive experiments verify the superiority of the proposed MERec against state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary>
现有的点 интерес (POI) 推荐器大多 aim to 捕捉用户偏好，通过使用城市级别的用户历史检查入，以便用户探索城市。然而，城市级别的用户检查入缺乏是一个重要的挑战，用户偏好学习。尽管先前的研究尝试通过利用不同的上下文信息，如空间-时间信息，来解决这个挑战，但它们忽略了将知识（即共享行为模式）从其他相关城市（即辅助城市）传递到用户。在本文中，我们研究了auxiliary cities中的知识传递对用户喜好的影响，并提出了一种基于meta-learning的下一POI推荐框架（MERec）。MERec利用城市之间的检查入行为相似性在meta-learning中进行学习，以帮助推荐用户喜好的下一POI。特别是，我们设计了一种城市级别的相似性策略，以便精准地捕捉城市之间的共享模式，从而将更加相关的知识传递到用户。经验证明，我们提出的MERec在对state-of-the-art算法进行比较时显示出了明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning"><a href="#Online-Class-Incremental-Learning-on-Stochastic-Blurry-Task-Boundary-via-Mask-and-Visual-Prompt-Tuning" class="headerlink" title="Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning"></a>Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09303">http://arxiv.org/abs/2308.09303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moonjunyyy/si-blurry">https://github.com/moonjunyyy/si-blurry</a></li>
<li>paper_authors: Jun-Yeong Moon, Keon-Hee Park, Jung Uk Kim, Gyeong-Moon Park</li>
<li>for: This paper focuses on the problem of continual learning in real-world scenarios, where the number of input data and tasks is constantly changing in a statistical way, and proposes a new scenario called Stochastic Incremental Blurry (Si-Blurry) to reflect the stochastic properties of the real-world.</li>
<li>methods: The paper introduces a novel method called Mask and Visual Prompt tuning (MVP) to alleviate the inter- and intra-task forgetting issues and class imbalance problem in the Si-Blurry scenario. MVP includes a novel instance-wise logit masking and contrastive visual prompt tuning loss, as well as a new gradient similarity-based focal loss and adaptive feature scaling.</li>
<li>results: The paper shows that MVP significantly outperforms the existing state-of-the-art methods in the challenging Si-Blurry scenario through extensive experiments.<details>
<summary>Abstract</summary>
Continual learning aims to learn a model from a continuous stream of data, but it mainly assumes a fixed number of data and tasks with clear task boundaries. However, in real-world scenarios, the number of input data and tasks is constantly changing in a statistical way, not a static way. Although recently introduced incremental learning scenarios having blurry task boundaries somewhat address the above issues, they still do not fully reflect the statistical properties of real-world situations because of the fixed ratio of disjoint and blurry samples. In this paper, we propose a new Stochastic incremental Blurry task boundary scenario, called Si-Blurry, which reflects the stochastic properties of the real-world. We find that there are two major challenges in the Si-Blurry scenario: (1) inter- and intra-task forgettings and (2) class imbalance problem. To alleviate them, we introduce Mask and Visual Prompt tuning (MVP). In MVP, to address the inter- and intra-task forgetting issues, we propose a novel instance-wise logit masking and contrastive visual prompt tuning loss. Both of them help our model discern the classes to be learned in the current batch. It results in consolidating the previous knowledge. In addition, to alleviate the class imbalance problem, we introduce a new gradient similarity-based focal loss and adaptive feature scaling to ease overfitting to the major classes and underfitting to the minor classes. Extensive experiments show that our proposed MVP significantly outperforms the existing state-of-the-art methods in our challenging Si-Blurry scenario.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Continual learning目标是从连续的数据流中学习模型，但是它假设有固定的数据量和任务数，并且任务boundary是清晰的。然而，在实际情况下，输入数据和任务的数量在统计方式上不断变化，而不是静态的。虽然最近引入的增量学习场景中的模糊任务boundary有些地址了以上问题，但是它们仍然不能完全反映实际情况中的统计性质。在这篇论文中，我们提出了一种新的随机增量模糊任务boundary场景，称为Si-Blurry。我们发现在Si-Blurry场景中有两个主要挑战：（1）间和内任务忘记，以及（2）类划分问题。为了解决这些问题，我们提出了Mask和Visual Prompt tuning（MVP）技术。在MVP中，我们提出了一种新的实例级别的logit掩码和视觉提示调整损失，以帮助我们的模型在当前批处理中学习的类。这会帮助我们的模型保持之前的知识。此外，我们还引入了一种新的类相似性基于的焦点损失和自适应特征涨幅，以避免过拟合主要类和下降不适应次要类。我们的实验表明，我们的提出的MVP在我们的挑战性Si-Blurry场景中表现出色，舒适性强。Note: The translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Learning-Reward-Machines-through-Preference-Queries-over-Sequences"><a href="#Learning-Reward-Machines-through-Preference-Queries-over-Sequences" class="headerlink" title="Learning Reward Machines through Preference Queries over Sequences"></a>Learning Reward Machines through Preference Queries over Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09301">http://arxiv.org/abs/2308.09301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Hsiung, Joydeep Biswas, Swarat Chaudhuri</li>
<li>for: 学习任务中的复杂动作序列</li>
<li>methods: 使用 preference queries 和 symbolic observation table 等技术</li>
<li>results: 有correctness和termination garanties，并且能够准确地学习奖励机制<details>
<summary>Abstract</summary>
Reward machines have shown great promise at capturing non-Markovian reward functions for learning tasks that involve complex action sequencing. However, no algorithm currently exists for learning reward machines with realistic weak feedback in the form of preferences. We contribute REMAP, a novel algorithm for learning reward machines from preferences, with correctness and termination guarantees. REMAP introduces preference queries in place of membership queries in the L* algorithm, and leverages a symbolic observation table along with unification and constraint solving to narrow the hypothesis reward machine search space. In addition to the proofs of correctness and termination for REMAP, we present empirical evidence measuring correctness: how frequently the resulting reward machine is isomorphic under a consistent yet inexact teacher, and the regret between the ground truth and learned reward machines.
</details>
<details>
<summary>摘要</summary>
奖励机器有广泛应用于复杂动作序列学习任务中捕捉非马尔可夫奖函数的承诺。然而，目前没有一种算法可以学习奖励机器 WITH 实际的弱反馈。我们提出了REMAP算法，一种学习奖励机器 FROM 偏好的新算法，具有正确性和结束保证。REMAP在L*算法中替换了成员查询，引入了偏好查询，并利用符号观察表 along with 统一和约束解决方案，将假设奖励机器搜索空间缩小。此外，我们还提供了REMAP的正确性和结束证明，以及对学习后的奖励机器和真实奖励机器之间的差异进行实验证明。
</details></li>
</ul>
<hr>
<h2 id="CARLA-A-Self-supervised-Contrastive-Representation-Learning-Approach-for-Time-Series-Anomaly-Detection"><a href="#CARLA-A-Self-supervised-Contrastive-Representation-Learning-Approach-for-Time-Series-Anomaly-Detection" class="headerlink" title="CARLA: A Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection"></a>CARLA: A Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09296">http://arxiv.org/abs/2308.09296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Zamanzadeh Darban, Geoffrey I. Webb, Shirui Pan, Mahsa Salehi<br>for: 这个研究旨在提出一个自动获取的时间序列异常检测方法，并且这个方法可以在单ivariate和多ivariate时间序列数据中检测异常。methods: 这个方法使用对照式表现学习，将时间序列窗口转换为高度相似的表现，并且透过自动获取的方法来分类正常和异常表现。results: 这个方法在7个标准的实际世界时间序列异常检测 benchmark dataset 上得到了较高的 F1 和 AU-PR 成绩，较先进的州前方法。<details>
<summary>Abstract</summary>
We introduce a Self-supervised Contrastive Representation Learning Approach for Time Series Anomaly Detection (CARLA), an innovative end-to-end self-supervised framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, We introduce an innovative end-to-end self-supervised deep learning framework carefully developed to identify anomalous patterns in both univariate and multivariate time series data. By taking advantage of contrastive representation learning, CARLA effectively generates robust representations for time series windows. It achieves this by 1) learning similar representations for temporally close windows and dissimilar representations for windows and their equivalent anomalous windows and 2) employing a self-supervised approach to classify normal/anomalous representations of windows based on their nearest/furthest neighbours in the representation space. Most of the existing models focus on learning normal behaviour. The normal boundary is often tightly defined, which can result in slight deviations being classified as anomalies, resulting in a high false positive rate and limited ability to generalise normal patterns. CARLA's contrastive learning methodology promotes the production of highly consistent and discriminative predictions, thereby empowering us to adeptly address the inherent challenges associated with anomaly detection in time series data. Through extensive experimentation on 7 standard real-world time series anomaly detection benchmark datasets, CARLA demonstrates F1 and AU-PR superior to existing state-of-the-art results. Our research highlights the immense potential of contrastive representation learning in advancing the field of time series anomaly detection, thus paving the way for novel applications and in-depth exploration in this domain.
</details>
<details>
<summary>摘要</summary>
我们介绍一个自我超vised contrastive representation learning方法（CARLA），这是一个创新的端到端自我超vised深度学习框架，用于时间序列异常探测。通过利用对比表现学习，CARLA具有生成高效的时间序列窗口表现的能力。它通过以下两个方法来实现这一点：1. 学习近似时间序列窗口的表现，并将其与其等效的异常窗口表现分开。2. 使用自我超vised的方法来分类正常/异常窗口表现，基于它们与其最近/最远邻的表现空间。现有的模型通常专注于学习正常行为，但是这会导致紧密定义正常边界，从而导致微小差异被识别为异常，导致伪阳性率高且难以扩展正常模式。CARLA的对比学习方法ologies生成高度一致和描述性的预测，因此可以有效地处理时间序列异常探测中的自然挑战。经过广泛的实验，CARLA在7个标准的真实世界时间序列异常探测 benchmark 数据集上显示 F1 和 AU-PR 的成绩superior 于现有的州�� nich 的结果。我们的研究显示了对比表现学习在时间序列异常探测领域的潜在应用和深入探索的潜力。
</details></li>
</ul>
<hr>
<h2 id="How-important-are-specialized-transforms-in-Neural-Operators"><a href="#How-important-are-specialized-transforms-in-Neural-Operators" class="headerlink" title="How important are specialized transforms in Neural Operators?"></a>How important are specialized transforms in Neural Operators?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09293">http://arxiv.org/abs/2308.09293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ritam-M/LearnableTransformsNO">https://github.com/Ritam-M/LearnableTransformsNO</a></li>
<li>paper_authors: Ritam Majumdar, Shirish Karande, Lovekesh Vig</li>
<li>for: 研究transform基于神经网络的partial differential equation（PDE）解决方法，以提高现代工业过程优化的效率。</li>
<li>methods: 使用Transform-based Neural Operators，如傅里叶 нейрон运算和wavelet нейрон运算，来解决PDEs。</li>
<li>results: 发现使用learnable线性层可以提供与最佳transform层相同的性能，并且计算时间更短。这些结果可能对未来神经网络架构的研究有重要意义，并可能暴露其他效率来源。<details>
<summary>Abstract</summary>
Simulating physical systems using Partial Differential Equations (PDEs) has become an indispensible part of modern industrial process optimization. Traditionally, numerical solvers have been used to solve the associated PDEs, however recently Transform-based Neural Operators such as the Fourier Neural Operator and Wavelet Neural Operator have received a lot of attention for their potential to provide fast solutions for systems of PDEs. In this work, we investigate the importance of the transform layers to the reported success of transform based neural operators. In particular, we record the cost in terms of performance, if all the transform layers are replaced by learnable linear layers. Surprisingly, we observe that linear layers suffice to provide performance comparable to the best-known transform-based layers and seem to do so with a compute time advantage as well. We believe that this observation can have significant implications for future work on Neural Operators, and might point to other sources of efficiencies for these architectures.
</details>
<details>
<summary>摘要</summary>
使用偏微分方程（PDE）模拟物理系统已成为现代工业过程优化的不可或缺的一部分。在过去，数值方法通常用来解决相关的PDE，但最近，基于变换的神经网络运算器，如傅ри尼尔神经网络运算器和波干神经网络运算器，受到了很多关注，因为它们可以提供解决PDE的快速方法。在这项工作中，我们调查了变换层对报道的success的重要性。 Specifically，我们记录了将所有变换层替换为学习型线性层后的成本，以及性能的影响。我们发现，使用学习型线性层可以提供与最佳变换基于层相当的性能，并且在计算时间方面也有优势。我们认为这一观察可能对未来神经网络运算器的研究产生重要的影响，并可能指向其他效率来源。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Alignment-and-Uniformity-for-Recommendation"><a href="#Graph-based-Alignment-and-Uniformity-for-Recommendation" class="headerlink" title="Graph-based Alignment and Uniformity for Recommendation"></a>Graph-based Alignment and Uniformity for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09292">http://arxiv.org/abs/2308.09292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangliangwei/graphau">https://github.com/yangliangwei/graphau</a></li>
<li>paper_authors: Liangwei Yang, Zhiwei Liu, Chen Wang, Mingdai Yang, Xiaolong Liu, Jing Ma, Philip S. Yu</li>
<li>for:  solves the sparsity issue in collaborative filtering-based recommender systems (RecSys) by using graph-based alignment and uniformity (GraphAU) to learn representations for users and items.</li>
<li>methods:  uses a novel approach called GraphAU, which explicitly considers high-order connectivities in the user-item bipartite graph to align the user&#x2F;item embedding to the dense vector representations of high-order neighbors.</li>
<li>results:  significantly alleviates the sparsity issue and achieves state-of-the-art performance on four datasets.Here is the full text in Simplified Chinese:</li>
<li>for: 这个论文是为了解决基于协同推荐系统（RecSys）的用户和项目的表示学习中的缺乏问题。</li>
<li>methods: 该论文提出了一种新的方法called GraphAU，它在用户-项目二分图中显式考虑高阶连接关系，将用户&#x2F;项目的嵌入align到高阶邻居的稀盐vector表示中。</li>
<li>results:  GraphAU可以减少缺乏问题，并在四个数据集上实现了状态机器人的表现。<details>
<summary>Abstract</summary>
Collaborative filtering-based recommender systems (RecSys) rely on learning representations for users and items to predict preferences accurately. Representation learning on the hypersphere is a promising approach due to its desirable properties, such as alignment and uniformity. However, the sparsity issue arises when it encounters RecSys. To address this issue, we propose a novel approach, graph-based alignment and uniformity (GraphAU), that explicitly considers high-order connectivities in the user-item bipartite graph. GraphAU aligns the user/item embedding to the dense vector representations of high-order neighbors using a neighborhood aggregator, eliminating the need to compute the burdensome alignment to high-order neighborhoods individually. To address the discrepancy in alignment losses, GraphAU includes a layer-wise alignment pooling module to integrate alignment losses layer-wise. Experiments on four datasets show that GraphAU significantly alleviates the sparsity issue and achieves state-of-the-art performance. We open-source GraphAU at https://github.com/YangLiangwei/GraphAU.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:共同推荐系统（RecSys）通过学习用户和物品的表示来预测用户的偏好。 表示学习在圆上是一种有利的方法，因为它具有一些恰当的性质，如对齐和均匀性。但当它遇到RecSys时，缺乏性问题出现。为解决这个问题，我们提出了一种新的方法——基于图的对齐和均匀性（GraphAU），它直接考虑用户-物品 дву价图中的高阶连接。GraphAU将用户/物品的嵌入对高阶邻居的稠密向量表示进行对齐，无需计算高阶邻居的困难对齐。为了解决层次对齐损失的差异，GraphAU包含层次对齐池化模块，将层次对齐损失集成。实验表明，GraphAU可以有效地解决缺乏性问题，并达到状态机器的性能。我们在https://github.com/YangLiangwei/GraphAU中开源GraphAU。
</details></li>
</ul>
<hr>
<h2 id="HyperLoRA-for-PDEs"><a href="#HyperLoRA-for-PDEs" class="headerlink" title="HyperLoRA for PDEs"></a>HyperLoRA for PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09290">http://arxiv.org/abs/2308.09290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritam Majumdar, Vishal Jadhav, Anirudh Deodhar, Shirish Karande, Lovekesh Vig, Venkataramana Runkana</li>
<li>For:  develop neural surrogates for solutions of Partial Differential Equations* Methods:  Hypernetworks, low-ranked adaptation (LoRA)* Results: 8x reduction in prediction parameters on average without compromising on accuracy, improved generalization capabilities for parameterized PDEs like Burger’s equation and Navier Stokes: Kovasznay flow.<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) have been widely used to develop neural surrogates for solutions of Partial Differential Equations. A drawback of PINNs is that they have to be retrained with every change in initial-boundary conditions and PDE coefficients. The Hypernetwork, a model-based meta learning technique, takes in a parameterized task embedding as input and predicts the weights of PINN as output. Predicting weights of a neural network however, is a high-dimensional regression problem, and hypernetworks perform sub-optimally while predicting parameters for large base networks. To circumvent this issue, we use a low ranked adaptation (LoRA) formulation to decompose every layer of the base network into low-ranked tensors and use hypernetworks to predict the low-ranked tensors. Despite the reduced dimensionality of the resulting weight-regression problem, LoRA-based Hypernetworks violate the underlying physics of the given task. We demonstrate that the generalization capabilities of LoRA-based hypernetworks drastically improve when trained with an additional physics-informed loss component (HyperPINN) to satisfy the governing differential equations. We observe that LoRA-based HyperPINN training allows us to learn fast solutions for parameterized PDEs like Burger's equation and Navier Stokes: Kovasznay flow, while having an 8x reduction in prediction parameters on average without compromising on accuracy when compared to all other baselines.
</details>
<details>
<summary>摘要</summary>
物理学信息泛化神经网络（PINNs）已广泛应用于解决部分偏微分方程的解的神经替换问题。PINNs的缺点是它们需要每次更改初始边界条件和偏微分方程的系数时重新训练。基于模型的元学习技术，超网络（Hypernetwork）可以将任务嵌入作为输入，预测PINN的权重。但是，预测神经网络的参数是一个高维度回归问题，超网络在预测参数时表现不佳，特别是预测大基础网络的参数。为解决这个问题，我们使用低级别适应（LoRA）形式将每层基础网络分解成低级别矩阵，并使用超网络预测低级别矩阵。尽管结果的维度减少，但LoRA基于的超网络仍然违反了给定任务的物理基础。我们表明，在增加物理学信息泛化损失组件（HyperPINN）进行训练后，LoRA基于的超网络的泛化能力得到了悉数提高。我们发现，LoRA基于的HyperPINN训练可以快速解决参数化的偏微分方程，如布尔格方程和奈尔-斯托克斯方程：kovasznay流，而无需增加预测参数的数量，平均每个基eline下降8倍，而且不会妥协准确性。
</details></li>
</ul>
<hr>
<h2 id="A-hybrid-Decoder-DeepONet-operator-regression-framework-for-unaligned-observation-data"><a href="#A-hybrid-Decoder-DeepONet-operator-regression-framework-for-unaligned-observation-data" class="headerlink" title="A hybrid Decoder-DeepONet operator regression framework for unaligned observation data"></a>A hybrid Decoder-DeepONet operator regression framework for unaligned observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09274">http://arxiv.org/abs/2308.09274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cb-sjtu/decoder_deeponet">https://github.com/cb-sjtu/decoder_deeponet</a></li>
<li>paper_authors: Bo Chen, Chenyu Wang, Weipeng Li, Haiyang Fu<br>for:这个研究的目的是解决深度神经网络（DNO）在非线性映射函数空间中的维度和计算成本问题，尤其是在面对不同观察数据时。methods:该研究提出了一种混合Decoder-DeepONet操作器 regression框架，以及一种Multi-Decoder-DeepONet，它使用了训练数据的平均场作为输入增强。这两种方法都基于操作器近似理论，并且在数学上提供了一致性。results:两个数值实验（Darcy问题和风流附近飞机翼）证明了Decoder-DeepONet和Multi-Decoder-DeepONet在面对不同观察数据时的效果和准确性。这些方法可以改善预测精度，特别是在面对高维度和复杂的数据时。<details>
<summary>Abstract</summary>
Deep neural operators (DNOs) have been utilized to approximate nonlinear mappings between function spaces. However, DNOs face the challenge of increased dimensionality and computational cost associated with unaligned observation data. In this study, we propose a hybrid Decoder-DeepONet operator regression framework to handle unaligned data effectively. Additionally, we introduce a Multi-Decoder-DeepONet, which utilizes an average field of training data as input augmentation. The consistencies of the frameworks with the operator approximation theory are provided, on the basis of the universal approximation theorem. Two numerical experiments, Darcy problem and flow-field around an airfoil, are conducted to validate the efficiency and accuracy of the proposed methods. Results illustrate the advantages of Decoder-DeepONet and Multi-Decoder-DeepONet in handling unaligned observation data and showcase their potentials in improving prediction accuracy.
</details>
<details>
<summary>摘要</summary>
深度神经运算符 (DNO) 已经用于approximate非线性映射 между函数空间。然而，DNO 面临不对采样数据进行Alignment的挑战，增加维度和计算成本。在这项研究中，我们提出了一种混合Decoder-DeepONet操作量 regression框架，以有效地处理不对采样数据。此外，我们还引入了Multi-Decoder-DeepONet，它使用训练数据的平均场为输入增强。我们提供了基于运算符approximation理论的一致性，以确保方法的正确性。在Darcy问题和流体风洞附近的飞行器上进行了两个数值实验，以验证提出的方法的效率和准确性。结果表明Decoder-DeepONet和Multi-Decoder-DeepONet在处理不对采样数据方面具有优势，并且它们在改进预测精度方面具有潜力。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Pseudo-Label-Learning-for-Non-Intrusive-Speech-Quality-Assessment-Model"><a href="#Multi-Task-Pseudo-Label-Learning-for-Non-Intrusive-Speech-Quality-Assessment-Model" class="headerlink" title="Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model"></a>Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09262">http://arxiv.org/abs/2308.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryandhimas E. Zezario, Bo-Ren Brian Bai, Chiou-Shann Fuh, Hsin-Min Wang, Yu Tsao</li>
<li>for: This paper proposes a multi-task pseudo-label (MPL) learning approach for a non-intrusive speech quality assessment model.</li>
<li>methods: The MPL approach consists of two stages: obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The model is optimized using a Huber loss function.</li>
<li>results: The proposed MPL approach outperforms training the model from scratch and using knowledge transfer mechanisms. Additionally, the use of Huber loss improves the prediction capabilities of the model.<details>
<summary>Abstract</summary>
This study introduces multi-task pseudo-label (MPL) learning for a non-intrusive speech quality assessment model. MPL consists of two stages which are obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS) are selected as the primary ground-truth labels. Additionally, the pretrained MOSA-Net model is utilized to estimate three pseudo-labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning stage of MPL is then employed to train the MTQ-Net model (multi-target speech quality assessment network). The model is optimized by incorporating Loss supervision (derived from the difference between the estimated score and the real ground-truth labels) and Loss semi-supervision (derived from the difference between the estimated score and pseudo-labels), where Huber loss is employed to calculate the loss function. Experimental results first demonstrate the advantages of MPL compared to training the model from scratch and using knowledge transfer mechanisms. Secondly, the benefits of Huber Loss in improving the prediction model of MTQ-Net are verified. Finally, the MTQ-Net with the MPL approach exhibits higher overall prediction capabilities when compared to other SSL-based speech assessment models.
</details>
<details>
<summary>摘要</summary>
In the multi-task learning stage, the MTQ-Net model (multi-target speech quality assessment network) is trained using the estimated pseudo-labels and real ground-truth labels. The model is optimized using a loss function that incorporates both Loss supervision (the difference between the estimated score and the real ground-truth labels) and Loss semi-supervision (the difference between the estimated score and pseudo-labels). The Huber loss is employed to calculate the loss function.Experimental results show that the MPL approach outperforms training the model from scratch and using knowledge transfer mechanisms. Additionally, the benefits of using the Huber loss in improving the prediction model of MTQ-Net are demonstrated. Finally, the MTQ-Net with the MPL approach exhibits higher overall prediction capabilities compared to other speech assessment models based on semi-supervised learning.
</details></li>
</ul>
<hr>
<h2 id="Distribution-shift-mitigation-at-test-time-with-performance-guarantees"><a href="#Distribution-shift-mitigation-at-test-time-with-performance-guarantees" class="headerlink" title="Distribution shift mitigation at test time with performance guarantees"></a>Distribution shift mitigation at test time with performance guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09259">http://arxiv.org/abs/2308.09259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Ding, Jielong Yang, Feng Ji, Xionghu Zhong, Linbo Xie</li>
<li>For: The paper aims to address the challenge of distribution shift in Graph Neural Networks (GNNs), which can negatively impact the test performance of the model.* Methods: The proposed framework, called FR-GNN, constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. The reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time.* Results: The paper shows that the proposed FR-GNN framework can effectively reduce the distribution shift and improve the test performance of GNNs without modifying the model structure or parameters. The experimental results demonstrate the superior performance of FR-GNN in comparison to mainstream methods on various public datasets.<details>
<summary>Abstract</summary>
Due to inappropriate sample selection and limited training data, a distribution shift often exists between the training and test sets. This shift can adversely affect the test performance of Graph Neural Networks (GNNs). Existing approaches mitigate this issue by either enhancing the robustness of GNNs to distribution shift or reducing the shift itself. However, both approaches necessitate retraining the model, which becomes unfeasible when the model structure and parameters are inaccessible. To address this challenge, we propose FR-GNN, a general framework for GNNs to conduct feature reconstruction. FRGNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed node features can be directly utilized for testing the well-trained model, effectively reducing the distribution shift and leading to improved test performance. This remarkable achievement is attained without any modifications to the model structure or parameters. We provide theoretical guarantees for the effectiveness of our framework. Furthermore, we conduct comprehensive experiments on various public datasets. The experimental results demonstrate the superior performance of FRGNN in comparison to mainstream methods.
</details>
<details>
<summary>摘要</summary>
FR-GNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed node features can be directly utilized for testing the well-trained model, effectively reducing the distribution shift and leading to improved test performance. This remarkable achievement is attained without any modifications to the model structure or parameters. We provide theoretical guarantees for the effectiveness of our framework.In addition, we conduct comprehensive experiments on various public datasets, and the experimental results demonstrate the superior performance of FR-GNN in comparison to mainstream methods.
</details></li>
</ul>
<hr>
<h2 id="Capacity-Bounds-for-Hyperbolic-Neural-Network-Representations-of-Latent-Tree-Structures"><a href="#Capacity-Bounds-for-Hyperbolic-Neural-Network-Representations-of-Latent-Tree-Structures" class="headerlink" title="Capacity Bounds for Hyperbolic Neural Network Representations of Latent Tree Structures"></a>Capacity Bounds for Hyperbolic Neural Network Representations of Latent Tree Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09250">http://arxiv.org/abs/2308.09250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasis Kratsios, Ruiyang Hong, Haitz Sáez de Ocáriz Borde</li>
<li>for: 这篇论文是研究深度征有折射函数的折射神经网络（HNN）的表示能力的。</li>
<li>methods: 这篇论文使用了ReLU activation function，并提供了首次证明HNN可以在任意权重树上实现$\varepsilon$-同构嵌入，并且可以在折射空间中嵌入树的尺寸为至少$d$，并且权重树的sectional curvature为$\kappa&lt;0$。</li>
<li>results: 这篇论文提供了关于HNN实现图像表示的网络复杂度的精确上限，并且发现网络复杂度与表示质量之间无直接关系。此外，这篇论文还提供了对任意ReLU多层感知机（MLP）在折射空间中嵌入树的下界，并证明任何ReLU MLP在嵌入树时必须夹带至少$\Omega(L^{1&#x2F;d})$的质量损失。<details>
<summary>Abstract</summary>
We study the representation capacity of deep hyperbolic neural networks (HNNs) with a ReLU activation function. We establish the first proof that HNNs can $\varepsilon$-isometrically embed any finite weighted tree into a hyperbolic space of dimension $d$ at least equal to $2$ with prescribed sectional curvature $\kappa<0$, for any $\varepsilon> 1$ (where $\varepsilon=1$ being optimal). We establish rigorous upper bounds for the network complexity on an HNN implementing the embedding. We find that the network complexity of HNN implementing the graph representation is independent of the representation fidelity/distortion. We contrast this result against our lower bounds on distortion which any ReLU multi-layer perceptron (MLP) must exert when embedding a tree with $L>2^d$ leaves into a $d$-dimensional Euclidean space, which we show at least $\Omega(L^{1/d})$; independently of the depth, width, and (possibly discontinuous) activation function defining the MLP.
</details>
<details>
<summary>摘要</summary>
我们研究深度偏折 neural network (HNN) 的表示能力，使用 ReLU 活化函数。我们证明了 HNN 可以将任意有限重量树 $\varepsilon$-同构 embedding 到半正交空间中，其维度至少为 $2$，且sectional curvature $\kappa<0$，对任何 $\varepsilon>1$（其中 $\varepsilon=1$ 是最优）。我们提供了精确的网络复杂度上限，用于表示实现。我们发现网络复杂度与表示质量/扭曲度无关。我们与这些下界对任何 ReLU 多层感知机 (MLP) 将树图 embedding 到 $d$-维欧式空间中的至少 $\Omega(L^{1/d})$ 的扭曲度进行比较，独立于深度、宽度和（可能破折）活化函数定义 MLP。
</details></li>
</ul>
<hr>
<h2 id="Active-and-Passive-Causal-Inference-Learning"><a href="#Active-and-Passive-Causal-Inference-Learning" class="headerlink" title="Active and Passive Causal Inference Learning"></a>Active and Passive Causal Inference Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09248">http://arxiv.org/abs/2308.09248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Jiwoong Im, Kyunghyun Cho</li>
<li>for: 这篇论文是为机器学习研究者、工程师和学生准备的一个入门课程，旨在介绍 causal inference 的基本概念和技术。</li>
<li>methods: 论文从一系列重要的假设出发，如交换性、正性、一致性和不干扰，然后建立了一系列重要的 causal inference 技术，分为两个杯子：活跃和被动两类。</li>
<li>results: 论文介绍了一些常见的 causal inference 技术，包括随机控制试验和短剑算法，以及经典的匹配和反射权重等方法。通过完善一些 causal inference 的缺失方面，如卷积风险、聚合风险等，论文预期能为读者提供一个多样化的入门点，进一步研究和探索 causal inference 领域。<details>
<summary>Abstract</summary>
This paper serves as a starting point for machine learning researchers, engineers and students who are interested in but not yet familiar with causal inference. We start by laying out an important set of assumptions that are collectively needed for causal identification, such as exchangeability, positivity, consistency and the absence of interference. From these assumptions, we build out a set of important causal inference techniques, which we do so by categorizing them into two buckets; active and passive approaches. We describe and discuss randomized controlled trials and bandit-based approaches from the active category. We then describe classical approaches, such as matching and inverse probability weighting, in the passive category, followed by more recent deep learning based algorithms. By finishing the paper with some of the missing aspects of causal inference from this paper, such as collider biases, we expect this paper to provide readers with a diverse set of starting points for further reading and research in causal inference and discovery.
</details>
<details>
<summary>摘要</summary>
这篇论文作为机器学习研究者、工程师和学生们的入门文献，旨在介绍 causal inference 的基本概念和方法。我们从共同需要的假设出发，包括交换性、正性、一致性和没有干扰。从这些假设中，我们构建了一组重要的 causal inference 技术，并将其分为两个托管：活动和被动approaches。我们描述了Randomized controlled trials和bandit-based approaches从活动类别，然后描述了经典方法，如匹配和逆序权重法，以及更近期的深度学习基于算法。通过结束文章中的一些 causal inference 的缺失方面，如融合偏见，我们期望这篇文章能为读者提供一个多样化的入门点，进一步阅读和研究 causal inference 和发现。
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments"><a href="#A-Robust-Policy-Bootstrapping-Algorithm-for-Multi-objective-Reinforcement-Learning-in-Non-stationary-Environments" class="headerlink" title="A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments"></a>A Robust Policy Bootstrapping Algorithm for Multi-objective Reinforcement Learning in Non-stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09734">http://arxiv.org/abs/2308.09734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Kasmarik, Jiankun Hu</li>
<li>for:  solves the problem of non-stationary dynamics in multi-objective reinforcement learning</li>
<li>methods:  developmental optimization approach, novel multi-objective reinforcement learning algorithm</li>
<li>results:  significantly outperforms state-of-the-art algorithms in non-stationary environments while achieving comparable results in stationary environments.Here’s the same information in Simplified Chinese:</li>
<li>for: 解决多bjective reinforcement learning中的非站点环境问题</li>
<li>methods: 发展优化方法，新型多bjective reinforcement learning算法</li>
<li>results: 在非站点环境下显著超越现有算法，在站点环境下达到相对的比较Result。<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are a special kind of multi-objective optimization problem that involves sequential decision making while satisfying the Markov property of stochastic processes. Multi-objective reinforcement learning methods address this problem by fusing the reinforcement learning paradigm with multi-objective optimization techniques. One major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem. This paper introduces a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments.
</details>
<details>
<summary>摘要</summary>
多目标Markov决策过程是特殊的多目标优化问题，它涉及到顺序做出决策，并满足Markov性的随机过程的属性。多目标激励学习方法 Addresses this problem by combining the reinforcement learning paradigm with multi-objective optimization techniques. However, one major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem.This paper proposes a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments.Here's the translation in Traditional Chinese:多目标Markov决策过程是特殊的多目标优化问题，它涉及到顺序做出决策，并满足Markov性的随机过程的属性。多目标激励学习方法 Addresses this problem by combining the reinforcement learning paradigm with multi-objective optimization techniques. However, one major drawback of these methods is the lack of adaptability to non-stationary dynamics in the environment. This is because they adopt optimization procedures that assume stationarity to evolve a coverage set of policies that can solve the problem.This paper proposes a developmental optimization approach that can evolve the policy coverage set while exploring the preference space over the defined objectives in an online manner. We propose a novel multi-objective reinforcement learning algorithm that can robustly evolve a convex coverage set of policies in an online manner in non-stationary environments. We compare the proposed algorithm with two state-of-the-art multi-objective reinforcement learning algorithms in stationary and non-stationary environments. Results showed that the proposed algorithm significantly outperforms the existing algorithms in non-stationary environments while achieving comparable results in stationary environments.
</details></li>
</ul>
<hr>
<h2 id="Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes"><a href="#Intrinsically-Motivated-Hierarchical-Policy-Learning-in-Multi-objective-Markov-Decision-Processes" class="headerlink" title="Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes"></a>Intrinsically Motivated Hierarchical Policy Learning in Multi-objective Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09733">http://arxiv.org/abs/2308.09733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sherif Abdelfattah, Kathryn Merrick, Jiankun Hu</li>
<li>for: 解决多目标Markov决策过程中的多个矛盾奖励函数不能同时优化，而是需要一种权衡策略来满足所有可能的偏好。</li>
<li>methods: 使用了自适应奖励学习方法，通过学习一个内在motivated的技能集来演化策略覆盖集，从而实现持续学习过程。</li>
<li>results: 在动态环境中，提出了一种新的双相自适应奖励学习方法，在第一阶段学习一个通用的技能集，然后在第二阶段使用这个集来启动策略覆盖集的演化，实验显示该方法可以significantly outperform现有的多目标奖励学习方法。<details>
<summary>Abstract</summary>
Multi-objective Markov decision processes are sequential decision-making problems that involve multiple conflicting reward functions that cannot be optimized simultaneously without a compromise. This type of problems cannot be solved by a single optimal policy as in the conventional case. Alternatively, multi-objective reinforcement learning methods evolve a coverage set of optimal policies that can satisfy all possible preferences in solving the problem. However, many of these methods cannot generalize their coverage sets to work in non-stationary environments. In these environments, the parameters of the state transition and reward distribution vary over time. This limitation results in significant performance degradation for the evolved policy sets. In order to overcome this limitation, there is a need to learn a generic skill set that can bootstrap the evolution of the policy coverage set for each shift in the environment dynamics therefore, it can facilitate a continuous learning process. In this work, intrinsically motivated reinforcement learning has been successfully deployed to evolve generic skill sets for learning hierarchical policies to solve multi-objective Markov decision processes. We propose a novel dual-phase intrinsically motivated reinforcement learning method to address this limitation. In the first phase, a generic set of skills is learned. While in the second phase, this set is used to bootstrap policy coverage sets for each shift in the environment dynamics. We show experimentally that the proposed method significantly outperforms state-of-the-art multi-objective reinforcement methods in a dynamic robotics environment.
</details>
<details>
<summary>摘要</summary>
多目标Markov决策过程是一种следова意决策问题，其涉及多个矛盾奖励函数，这些奖励函数无法同时优化。这类问题不能通过单一优化策略来解决，而是需要一组优化策略来满足所有可能的偏好。然而，许多多目标学习方法无法扩展其覆盖集来处理不稳定环境。在这些环境中，状态转移和奖励分布的参数会随时间变化，这会导致优化策略集的性能下降。为了解决这一限制，需要学习一个通用技能集，以便在环境动态变化时，使用这个技能集来演化策略覆盖集，从而实现连续学习过程。在这种情况下，我们成功地应用了内在激励学习来演化通用技能集，以解决多目标Markov决策过程中的限制。我们提出了一种新的双期内在激励学习方法，其中，在第一阶段，学习一个通用技能集；在第二阶段，使用这个技能集来演化策略覆盖集 для每个环境动态变化。我们实验ally表明，提议的方法在动态 робо特环境中显著超过了当前最佳多目标学习方法。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Sum-Pooling-for-Metric-Learning"><a href="#Generalized-Sum-Pooling-for-Metric-Learning" class="headerlink" title="Generalized Sum Pooling for Metric Learning"></a>Generalized Sum Pooling for Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09228">http://arxiv.org/abs/2308.09228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yetigurbuz/generalized-sum-pooling">https://github.com/yetigurbuz/generalized-sum-pooling</a></li>
<li>paper_authors: Yeti Z. Gurbuz, Ozan Sener, A. Aydın Alatan</li>
<li>for: 本研究旨在提出一种可学习的总和池化方法（Generalized Sum Pooling，GSP），用于深度度量学习中的核心 pooling 阶段。</li>
<li>methods: 本研究提出了一种基于信息ENTROPY的优化transport问题，用于学习核心池化方法。此外，还提出了一种零批评损函数，用于帮助学习GSP。</li>
<li>results: 实验结果表明，GSP可以提高深度度量学习中的性能，并且可以快速地学习到有用的semantic entity。Code可以在GSP-DML Framework中找到。<details>
<summary>Abstract</summary>
A common architectural choice for deep metric learning is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different semantic entity and GAP as a convex combination of them. Following this perspective, we generalize GAP and propose a learnable generalized sum pooling method (GSP). GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effectively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct learnable replacement for GAP. We further propose a zero-shot loss to ease the learning of GSP. We show the effectiveness of our method with extensive evaluations on 4 popular metric learning benchmarks. Code is available at: GSP-DML Framework
</details>
<details>
<summary>摘要</summary>
一般而言，深度度量学中常用的建筑方式是一个卷积神经网络，然后跟global average pooling（GAP）。尽管简单，但GAP是一种非常有效的信息汇总方法。一种可能的解释是每个特征向量都代表不同的semantic entity，GAP是这些entity的 convex combination。基于这个视角，我们总结GAP并提出一种学习可能的总和方法（GSP）。GSP在两个方面提高GAP：一是可以选择一 subset of semantic entity，有效地忽略干扰信息，二是学习每个entity的重要性权重。我们提出一个 entropy-smoothed optimal transport问题，并证明它是GAP的严格推广，即这个问题的特定实现可以返回GAP。我们表明这个优化问题具有分析性Gradient，可以作为直接学习替换GAP的方法。此外，我们还提出了一种零拟损优化方法，以便学习GSP。我们通过对4种常见度量学权重 benchmark进行广泛的评估，证明了我们的方法的有效性。代码可以在：GSP-DML Framework 中找到。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion"><a href="#Advancing-Relation-Extraction-through-Language-Probing-with-Exemplars-from-Set-Co-Expansion" class="headerlink" title="Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion"></a>Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11720">http://arxiv.org/abs/2308.11720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yerong Li, Roxana Girju<br>for:This paper focuses on improving relation extraction accuracy and reducing confusion between contrastive classes.methods:The proposed approach uses representative examples and co-set expansion, incorporating similarity measures between target pairs and representative pairs from the target class. Contextual details are harnessed via context-free Hearst patterns to ascertain contextual similarity.results:The co-set expansion approach significantly enhances relation classification performance, achieving an observed margin of at least 1 percent improvement in accuracy in most settings. Tuning contrastive examples further refines the approach, reducing confusion between classes sharing similarities and leading to more precise classification.<details>
<summary>Abstract</summary>
Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.   Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.   Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a significant enhancement of relation classification performance. Our method achieves an observed margin of at least 1 percent improvement in accuracy in most settings, on top of existing fine-tuning approaches. To further refine our approach, we conduct an in-depth analysis that focuses on tuning contrastive examples. This strategic selection and tuning effectively reduce confusion between classes sharing similarities, leading to a more precise classification process.   Experimental results underscore the effectiveness of our proposed framework for relation extraction. The synergy between co-set expansion and context-aware prompt tuning substantially contributes to improved classification accuracy. Furthermore, the reduction in confusion between contrastive classes through contrastive examples tuning validates the robustness and reliability of our method.
</details>
<details>
<summary>摘要</summary>
relation extraction (RE) 是自动提取结构化信息的重要任务。在这篇论文中，我们提出了一种多方面的方法，该方法通过代表性例子和相似度扩展来提高关系分类精度。我们的方法首先将每个关系类型中的代表性例子填充。然后，我们的相似度扩展算法在训练目标中添加了类似度测试。此外，相似度扩展过程还包括一个类别排名过程，该过程考虑了相似类型的表达。Contextual details surrounding relation mentions are harnessed using context-free Hearst patterns to determine contextual similarity。Empirical evaluation shows the effectiveness of our co-set expansion approach, resulting in a significant improvement in relation classification performance. Our method achieves an observed margin of at least 1 percent improvement in accuracy in most settings, on top of existing fine-tuning approaches。为了进一步改进我们的方法，我们进行了深入的分析，集中在调整对比例例子上。这种筛选和调整有效地减少了类似类型之间的混淆，从而使得分类更加精准。实验结果证明我们提出的框架对relation extraction是有效的。它的同时涵盖相似度扩展和上下文相关的提示调整，导致了改进的分类精度。此外，通过对相似类型之间的混淆进行调整，我们的方法的可靠性和可靠性得到了证明。
</details></li>
</ul>
<hr>
<h2 id="DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction"><a href="#DMCVR-Morphology-Guided-Diffusion-Model-for-3D-Cardiac-Volume-Reconstruction" class="headerlink" title="DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction"></a>DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09223">http://arxiv.org/abs/2308.09223</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hexiaoxiao-cs/dmcvr">https://github.com/hexiaoxiao-cs/dmcvr</a></li>
<li>paper_authors: Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, Dimitris N. Metaxas</li>
<li>for: 提高心血管疾病诊断和治疗规划的准确三维心脏重建</li>
<li>methods: 基于生成模型的心脏形态导导模型（DMCVR），通过合并高分辨率二维心脏图像和相应的三维重建体积来提高三维心脏重建质量</li>
<li>results: 在多个方面比前方法高效，包括二维生成和三维重建性能，并且可以生成高分辨率三维心脏MRI重建图像，超过现有技术水平<details>
<summary>Abstract</summary>
Accurate 3D cardiac reconstruction from cine magnetic resonance imaging (cMRI) is crucial for improved cardiovascular disease diagnosis and understanding of the heart's motion. However, current cardiac MRI-based reconstruction technology used in clinical settings is 2D with limited through-plane resolution, resulting in low-quality reconstructed cardiac volumes. To better reconstruct 3D cardiac volumes from sparse 2D image stacks, we propose a morphology-guided diffusion model for 3D cardiac volume reconstruction, DMCVR, that synthesizes high-resolution 2D images and corresponding 3D reconstructed volumes. Our method outperforms previous approaches by conditioning the cardiac morphology on the generative model, eliminating the time-consuming iterative optimization process of the latent code, and improving generation quality. The learned latent spaces provide global semantics, local cardiac morphology and details of each 2D cMRI slice with highly interpretable value to reconstruct 3D cardiac shape. Our experiments show that DMCVR is highly effective in several aspects, such as 2D generation and 3D reconstruction performance. With DMCVR, we can produce high-resolution 3D cardiac MRI reconstructions, surpassing current techniques. Our proposed framework has great potential for improving the accuracy of cardiac disease diagnosis and treatment planning. Code can be accessed at https://github.com/hexiaoxiao-cs/DMCVR.
</details>
<details>
<summary>摘要</summary>
精准的3D心脏重建从 cinematic magnet共振成像（cMRI）是诊断心血管疾病和心脏运动的关键。然而，现有的心脏MRI基于的重建技术在临床 Settings 中只有2D的，导致重建的心脏体积质量低下。为了更好地从稀疏的2D图像堆栈中重建3D心脏体积，我们提议一种基于几何学指导的扩散模型，称为DMCVR，该模型可以将高分辨率的2D图像和相应的3D重建体积相互conditioning。我们的方法超越先前的方法，因为它们不需要质量优化过程，提高生成质量。学习的秘密空间提供了全球 semantics，局部心脏形态和每个2D cMRI slice 的高度解释性。我们的实验表明，DMCVR 在多个方面具有高效性，例如2D生成和3D重建性能。通过DMCVR，我们可以生成高分辨率的3D心脏MRI重建，超越当前技术。我们提出的框架具有较大的诊断心血管疾病和治疗规划的潜在价值。代码可以在https://github.com/hexiaoxiao-cs/DMCVR 中下载。
</details></li>
</ul>
<hr>
<h2 id="Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm"><a href="#Baird-Counterexample-Is-Solved-with-an-example-of-How-to-Debug-a-Two-time-scale-Algorithm" class="headerlink" title="Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm"></a>Baird Counterexample Is Solved: with an example of How to Debug a Two-time-scale Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09732">http://arxiv.org/abs/2308.09732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengshuai Yao</li>
<li>for: 这个论文是为了解释帕尔德对例中的TD（0）算法的异常行为，以及对此例进行测试和比较偏离政策学习算法的性能。</li>
<li>methods: 这个论文使用了调度TD算法的分析和调试技术，以解释TD（0）算法在帕尔德对例中的慢速收敛问题。</li>
<li>results: 该论文提供了一种调度TD算法的方法，可以在帕尔德对例中实现TD解决方案的快速收敛。此外，论文还提供了实验结果，表明该算法在帕尔德对例中的收敛速率是线性的。因此，该论文结论认为，帕尔德对例已经得到了解决。<details>
<summary>Abstract</summary>
Baird counterexample was proposed by Leemon Baird in 1995, first used to show that the Temporal Difference (TD(0)) algorithm diverges on this example. Since then, it is often used to test and compare off-policy learning algorithms. Gradient TD algorithms solved the divergence issue of TD on Baird counterexample. However, their convergence on this example is still very slow, and the nature of the slowness is not well understood, e.g., see (Sutton and Barto 2018).   This note is to understand in particular, why TDC is slow on this example, and provide debugging analysis to understand this behavior. Our debugging technique can be used to study the convergence behavior of two-time-scale stochastic approximation algorithms. We also provide empirical results of the recent Impression GTD algorithm on this example, showing the convergence is very fast, in fact, in a linear rate. We conclude that Baird counterexample is solved, by an algorithm with convergence guarantee to the TD solution in general and a fast convergence rate.
</details>
<details>
<summary>摘要</summary>
白尔德Counterexample（Baird counterexample）在1995年被李姓飞（Leemon Baird）提出，用以证明TD(0)算法在这个例子中出现偏离。 desde entonces, 它经常用于测试和比较不同的离散学习算法。 梯度TD算法解决了TD算法在白尔德Counterexample中的偏离问题，但是它们在这个例子中的收敛速度非常慢，并且这种慢速度的原因还不很了解，例如参见（Sutton和Barto 2018）。 本文的目的是理解特别是TD梯度算法在这个例子中的慢收敛问题，并提供调试分析以理解这种行为。 我们的调试技术可以用来研究两个时间尺度的随机抽象算法的收敛行为。 我们还提供了最近的Impression GTD算法在这个例子中的实验结果，显示其收敛非常快，甚至在线性率。 我们 conclude that Baird counterexample已经得到解决，并且TD梯度算法在总的来说具有收敛保证和快速收敛速度。
</details></li>
</ul>
<hr>
<h2 id="A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings"><a href="#A-Model-Agnostic-Framework-for-Recommendation-via-Interest-aware-Item-Embeddings" class="headerlink" title="A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings"></a>A Model-Agnostic Framework for Recommendation via Interest-aware Item Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09202">http://arxiv.org/abs/2308.09202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Kumar Jaiswal, Yu Xiong</li>
<li>for: 本研究旨在提高推荐系统中ITEM表示的精度，以更好地捕捉用户的兴趣。</li>
<li>methods: 本文提出了一种名为Interest-aware Capsule network（IaCN）的新型推荐模型，它通过直接学习用户兴趣 oriented item表示来提高推荐效果。</li>
<li>results: 实验结果表明，对于不同的深度神经网络、行为序列长度和共同学习率，IaCN模型均显示出显著的性能提升，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Item representation holds significant importance in recommendation systems, which encompasses domains such as news, retail, and videos. Retrieval and ranking models utilise item representation to capture the user-item relationship based on user behaviours. While existing representation learning methods primarily focus on optimising item-based mechanisms, such as attention and sequential modelling. However, these methods lack a modelling mechanism to directly reflect user interests within the learned item representations. Consequently, these methods may be less effective in capturing user interests indirectly. To address this challenge, we propose a novel Interest-aware Capsule network (IaCN) recommendation model, a model-agnostic framework that directly learns interest-oriented item representations. IaCN serves as an auxiliary task, enabling the joint learning of both item-based and interest-based representations. This framework adopts existing recommendation models without requiring substantial redesign. We evaluate the proposed approach on benchmark datasets, exploring various scenarios involving different deep neural networks, behaviour sequence lengths, and joint learning ratios of interest-oriented item representations. Experimental results demonstrate significant performance enhancements across diverse recommendation models, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
“Item representation plays a crucial role in recommendation systems, which encompasses domains such as news, retail, and videos. Retrieval and ranking models use item representation to capture the user-item relationship based on user behaviors. However, existing representation learning methods primarily focus on optimizing item-based mechanisms, such as attention and sequential modeling, and lack a modeling mechanism to directly reflect user interests within the learned item representations. As a result, these methods may be less effective in capturing user interests indirectly. To address this challenge, we propose a novel Interest-aware Capsule network (IaCN) recommendation model, a model-agnostic framework that directly learns interest-oriented item representations. IaCN serves as an auxiliary task, enabling the joint learning of both item-based and interest-based representations. This framework adopts existing recommendation models without requiring substantial redesign. We evaluate the proposed approach on benchmark datasets, exploring various scenarios involving different deep neural networks, behavior sequence lengths, and joint learning ratios of interest-oriented item representations. Experimental results demonstrate significant performance enhancements across diverse recommendation models, validating the effectiveness of our approach.”Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning"><a href="#TinyProp-–-Adaptive-Sparse-Backpropagation-for-Efficient-TinyML-On-device-Learning" class="headerlink" title="TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning"></a>TinyProp – Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09201">http://arxiv.org/abs/2308.09201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcus Rüb, Daniel Maier, Daniel Mueller-Gritschneder, Axel Sikora</li>
<li>for: 这篇论文目的是提高在低功耗微控制器单元（MCU）上进行device learning或精细化神经网络的效率。</li>
<li>methods: 这篇论文使用了简略的传播算法，通过在训练步骤中适当地选择和训练价值对的方法来实现硬件上的训练。</li>
<li>results:  compared to非简略训练，这篇论文的结果显示，使用这种方法可以提高平均5倍，并且仅受到轻微的计算成本影响。此外，与现有的静态简略传播算法相比，这种方法的执行速度提高了2.9倍，并且降低了平均6%的精度损失。<details>
<summary>Abstract</summary>
Training deep neural networks using backpropagation is very memory and computationally intensive. This makes it difficult to run on-device learning or fine-tune neural networks on tiny, embedded devices such as low-power micro-controller units (MCUs). Sparse backpropagation algorithms try to reduce the computational load of on-device learning by training only a subset of the weights and biases. Existing approaches use a static number of weights to train. A poor choice of this so-called backpropagation ratio limits either the computational gain or can lead to severe accuracy losses. In this paper we present TinyProp, the first sparse backpropagation method that dynamically adapts the back-propagation ratio during on-device training for each training step. TinyProp induces a small calculation overhead to sort the elements of the gradient, which does not significantly impact the computational gains. TinyProp works particularly well on fine-tuning trained networks on MCUs, which is a typical use case for embedded applications. For typical datasets from three datasets MNIST, DCASE2020 and CIFAR10, we are 5 times faster compared to non-sparse training with an accuracy loss of on average 1%. On average, TinyProp is 2.9 times faster than existing, static sparse backpropagation algorithms and the accuracy loss is reduced on average by 6 % compared to a typical static setting of the back-propagation ratio.
</details>
<details>
<summary>摘要</summary>
训练深度神经网络使用反传播是非常占用内存和计算资源的。这使得在设备学习或精度调整神经网络在小型嵌入式设备（如低功耗微控制器）上进行困难。稀疏反传播算法尝试减少设备上学习的计算负担，只训练部分权重和偏移。现有方法使用静态的反传播比率来训练。这称为反传播比率的选择会限制计算的益处或导致严重的准确性损失。在这篇论文中，我们介绍了TinyProp，第一个动态在设备上训练时适应反传播比率的稀疏反传播方法。TinyProp添加了一些排序元素计划的小计算开销，不会对计算的益处产生很大的影响。TinyProp在MCU上精度调整已经训练的网络时特别有效，这是常见的嵌入式应用场景。对于典型的MNIST、DCASE2020和CIFAR10数据集，我们比非稀疏训练更快5倍，准确性损失平均1%。与现有静态稀疏反传播算法相比，TinyProp平均2.9倍快，并且准确性损失平均降低6%。
</details></li>
</ul>
<hr>
<h2 id="Polynomial-Bounds-for-Learning-Noisy-Optical-Physical-Unclonable-Functions-and-Connections-to-Learning-With-Errors"><a href="#Polynomial-Bounds-for-Learning-Noisy-Optical-Physical-Unclonable-Functions-and-Connections-to-Learning-With-Errors" class="headerlink" title="Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors"></a>Polynomial Bounds for Learning Noisy Optical Physical Unclonable Functions and Connections to Learning With Errors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09199">http://arxiv.org/abs/2308.09199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Apollo Albright, Boris Gelfand, Michael Dixon</li>
<li>for: 这篇论文主要研究了一种光学物理不可克隆函数（PUF）的学习问题。</li>
<li>methods: 该论文使用了一种基于多项式的批量学习方法，通过训练一个多项式回归模型来学习PUF。</li>
<li>results: 该论文显示，可以使用这种方法将PUF学习到任意精度，即使在噪声存在的情况下，只要有充足的挑战对象和计算资源。这些结果超越了2013年 Rh&quot;uramir 等人的研究，他们只处理了一个子集的PUF，并且假设了光学系统是线性的或者有轻微非线性效应。<details>
<summary>Abstract</summary>
It is shown that a class of optical physical unclonable functions (PUFs) can be learned to arbitrary precision with arbitrarily high probability, even in the presence of noise, given access to polynomially many challenge-response pairs and polynomially bounded computational power, under mild assumptions about the distributions of the noise and challenge vectors. This extends the results of Rh\"uramir et al. (2013), who showed a subset of this class of PUFs to be learnable in polynomial time in the absence of noise, under the assumption that the optics of the PUF were either linear or had negligible nonlinear effects. We derive polynomial bounds for the required number of samples and the computational complexity of a linear regression algorithm, based on size parameters of the PUF, the distributions of the challenge and noise vectors, and the probability and accuracy of the regression algorithm, with a similar analysis to one done by Bootle et al. (2018), who demonstrated a learning attack on a poorly implemented version of the Learning With Errors problem.
</details>
<details>
<summary>摘要</summary>
研究表明，一类光学物理不可克隆函数（PUF）可以在很高的准确率下学习，即使在噪声存在的情况下，只需要访问很多挑战-响应对的对话和计算能力，假设噪声和挑战向量的分布是某种很少的，并且允许使用很多的计算资源。这个结论超越了2013年rh\"uramir等人的结果，他们只处理了这个类型的PUF的一个子集，并且假设optics的影响是线性或有小非线性效应。我们 derive了基于PUF的大小参数、挑战向量和噪声向量的分布、恢复算法的概率和准确率的多项式上限，与2018年Bootle等人的一个类似的分析相同，他们展示了一种学习攻击，攻击一个不好实现的学习问题。
</details></li>
</ul>
<hr>
<h2 id="Half-Hop-A-graph-upsampling-approach-for-slowing-down-message-passing"><a href="#Half-Hop-A-graph-upsampling-approach-for-slowing-down-message-passing" class="headerlink" title="Half-Hop: A graph upsampling approach for slowing down message passing"></a>Half-Hop: A graph upsampling approach for slowing down message passing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09198">http://arxiv.org/abs/2308.09198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nerdslab/halfhop">https://github.com/nerdslab/halfhop</a></li>
<li>paper_authors: Mehdi Azabou, Venkataramana Ganesh, Shantanu Thakoor, Chi-Heng Lin, Lakshmi Sathidevi, Ran Liu, Michal Valko, Petar Veličković, Eva L. Dyer</li>
<li>for: 提高消息传递神经网络的学习效果，特别在邻居节点属于不同类型时。</li>
<li>methods: 添加”慢节点”来协调消息传递，只需修改输入图。</li>
<li>results: 在多种批处理和自动化学习 benchmark 上提高表现，特别在异质情况下，并可用于生成自适应学习的增强视图。<details>
<summary>Abstract</summary>
Message passing neural networks have shown a lot of success on graph-structured data. However, there are many instances where message passing can lead to over-smoothing or fail when neighboring nodes belong to different classes. In this work, we introduce a simple yet general framework for improving learning in message passing neural networks. Our approach essentially upsamples edges in the original graph by adding "slow nodes" at each edge that can mediate communication between a source and a target node. Our method only modifies the input graph, making it plug-and-play and easy to use with existing models. To understand the benefits of slowing down message passing, we provide theoretical and empirical analyses. We report results on several supervised and self-supervised benchmarks, and show improvements across the board, notably in heterophilic conditions where adjacent nodes are more likely to have different labels. Finally, we show how our approach can be used to generate augmentations for self-supervised learning, where slow nodes are randomly introduced into different edges in the graph to generate multi-scale views with variable path lengths.
</details>
<details>
<summary>摘要</summary>
We provide both theoretical and empirical analyses to demonstrate the benefits of slowing down message passing. Our results on several supervised and self-supervised benchmarks show improvements across the board, particularly in heterophilic conditions where adjacent nodes are more likely to have different labels. Additionally, we show how our approach can be used to generate augmentations for self-supervised learning, where slow nodes are randomly introduced into different edges in the graph to generate multi-scale views with variable path lengths.
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports"><a href="#A-Comparative-Study-of-Text-Embedding-Models-for-Semantic-Text-Similarity-in-Bug-Reports" class="headerlink" title="A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports"></a>A Comparative Study of Text Embedding Models for Semantic Text Similarity in Bug Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09193">http://arxiv.org/abs/2308.09193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/av9ash/duplicatebugdetection">https://github.com/av9ash/duplicatebugdetection</a></li>
<li>paper_authors: Avinash Patil, Kihwan Han, Sabyasachi Mukhopadhyay</li>
<li>for: 本研究旨在比较不同文本 Similarity 方法在 bug report 中的效果，以提高 bug report 的检索和分类效果。</li>
<li>methods: 本研究使用了多种嵌入模型，包括 TF-IDF (基准), FastText, Gensim, BERT, ADA 等。</li>
<li>results: 实验结果显示，BERT 在回忆性和准确率方面比其他模型都高，其次是 ADA, Gensim, FastText, TF-IDF。这些结果提供了不同嵌入方法在 bug report 检索中的效果，并指出了选择合适的嵌入方法对这种任务的重要性。<details>
<summary>Abstract</summary>
Bug reports are an essential aspect of software development, and it is crucial to identify and resolve them quickly to ensure the consistent functioning of software systems. Retrieving similar bug reports from an existing database can help reduce the time and effort required to resolve bugs. In this paper, we compared the effectiveness of semantic textual similarity methods for retrieving similar bug reports based on a similarity score. We explored several embedding models such as TF-IDF (Baseline), FastText, Gensim, BERT, and ADA. We used the Software Defects Data containing bug reports for various software projects to evaluate the performance of these models. Our experimental results showed that BERT generally outperformed the rest of the models regarding recall, followed by ADA, Gensim, FastText, and TFIDF. Our study provides insights into the effectiveness of different embedding methods for retrieving similar bug reports and highlights the impact of selecting the appropriate one for this task. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
📝 bug 报告是软件开发中非常重要的一环，快速确定和解决 bug 可以保证软件系统的一致性。从现有数据库中检索类似 bug 报告可以减少解决 bug 所需的时间和努力。在这篇论文中，我们比较了基于 semantic textual similarity 方法来检索类似 bug 报告的效果，并计算了相似性分数。我们检查了TF-IDF（基准）、FastText、Gensim、BERT 和 ADA 等嵌入模型。我们使用了 Software Defects Data 中的 bug 报告来评估这些模型的性能。我们的实验结果表明，BERT 通常在 recall 方面表现出色，其次是 ADA、Gensim、FastText 和 TF-IDF。我们的研究提供了不同嵌入方法在检索类似 bug 报告的效果的启示，并 highlights 选择合适的嵌入方法对这种任务的重要性。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance"><a href="#Regularizing-Adversarial-Imitation-Learning-Using-Causal-Invariance" class="headerlink" title="Regularizing Adversarial Imitation Learning Using Causal Invariance"></a>Regularizing Adversarial Imitation Learning Using Causal Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09189">http://arxiv.org/abs/2308.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Ovinnikov, Joachim M. Buhmann</li>
<li>for: 这篇论文的目的是使用伪函数学习方法从专家示范数据集中推导出一个策略，并使用推论器作为对抗优化过程中的指导信号。</li>
<li>methods: 这篇论文使用了伪函数学习方法，并使用了一个推论器作为对抗优化过程中的指导信号。</li>
<li>results: 论文发现这种模型容易吸收专家数据中的偶极 correlations，为了解决这个问题，提出了使用 causal invariance 作为对抗训练这些模型的正则化原则。<details>
<summary>Abstract</summary>
Imitation learning methods are used to infer a policy in a Markov decision process from a dataset of expert demonstrations by minimizing a divergence measure between the empirical state occupancy measures of the expert and the policy. The guiding signal to the policy is provided by the discriminator used as part of an versarial optimization procedure. We observe that this model is prone to absorbing spurious correlations present in the expert data. To alleviate this issue, we propose to use causal invariance as a regularization principle for adversarial training of these models. The regularization objective is applicable in a straightforward manner to existing adversarial imitation frameworks. We demonstrate the efficacy of the regularized formulation in an illustrative two-dimensional setting as well as a number of high-dimensional robot locomotion benchmark tasks.
</details>
<details>
<summary>摘要</summary>
模仿学习方法用于在Markov决策过程中推导策略，通过将专家示例集中的数据用作示例来 minimum化分割度量和策略之间的差异。导向信号将被用作抽象优化过程中的评价函数。我们发现这些模型容易吸收专家数据中存在的偶极关系。为了解决这个问题，我们提议使用 causal invariance 作为对 adversarial 训练的规则化原则。这个正则化目标可以直接应用于现有的对抗模仿框架中。我们在一个简单的二维设定中以及一些高维机器人行走 benchmark 任务中证明了这种规则化形式的效果。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Extra-gradient-with-Optimal-Complexity-and-Communication-Guarantees"><a href="#Distributed-Extra-gradient-with-Optimal-Complexity-and-Communication-Guarantees" class="headerlink" title="Distributed Extra-gradient with Optimal Complexity and Communication Guarantees"></a>Distributed Extra-gradient with Optimal Complexity and Communication Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09187">http://arxiv.org/abs/2308.09187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lions-epfl/qgenx">https://github.com/lions-epfl/qgenx</a></li>
<li>paper_authors: Ali Ramezani-Kebrya, Kimon Antonakopoulos, Igor Krawczuk, Justin Deschenaux, Volkan Cevher</li>
<li>for: 解决多处理器&#x2F;工作者&#x2F;客户端之间的分布式凸小最优化问题，包括分布式凸最小化、最大化和游戏等问题。</li>
<li>methods: 提出了一种基于量化的通用逆向梯度法（Q-GenX），该方法可以有效地减少通信量，并且适应不同的噪声 Profile。</li>
<li>results: 提出了一种自适应步长规则，可以适应各种噪声 Profiless，并且在分布式训练下显著加速了训练速度，并通过实际实验验证了这些理论结论。<details>
<summary>Abstract</summary>
We consider monotone variational inequality (VI) problems in multi-GPU settings where multiple processors/workers/clients have access to local stochastic dual vectors. This setting includes a broad range of important problems from distributed convex minimization to min-max and games. Extra-gradient, which is a de facto algorithm for monotone VI problems, has not been designed to be communication-efficient. To this end, we propose a quantized generalized extra-gradient (Q-GenX), which is an unbiased and adaptive compression method tailored to solve VIs. We provide an adaptive step-size rule, which adapts to the respective noise profiles at hand and achieve a fast rate of ${\mathcal O}(1/T)$ under relative noise, and an order-optimal ${\mathcal O}(1/\sqrt{T})$ under absolute noise and show distributed training accelerates convergence. Finally, we validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs.
</details>
<details>
<summary>摘要</summary>
我们考虑多个GPU上的 monotoneVariational inequality（VI）问题，其中多个处理器/工作者/客户端具有本地随机对应矩阵。这些设定包括分布式凸减少问题、min-max和游戏等重要问题。标准的Extra-gradient算法（de facto algorithm）未被设计来实现通信效率。因此，我们提出了量化通过矩阵（Q-GenX），它是一种适应性和无偏量的压缩方法，用于解决VI问题。我们提供了适应步长规则，可以适应不同的噪音 профи在手中，并可以在相对噪音下 achieved a fast rate of $\mathcal{O}(1/T)$，以及在绝对噪音下 achieved an order-optimal rate of $\mathcal{O}(1/\sqrt{T})$。最后，我们 validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs.Note that "随机对应矩阵" in the text can be translated as "stochastic dual vectors" or "randomized dual vectors".
</details></li>
</ul>
<hr>
<h2 id="RatGPT-Turning-online-LLMs-into-Proxies-for-Malware-Attacks"><a href="#RatGPT-Turning-online-LLMs-into-Proxies-for-Malware-Attacks" class="headerlink" title="RatGPT: Turning online LLMs into Proxies for Malware Attacks"></a>RatGPT: Turning online LLMs into Proxies for Malware Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09183">http://arxiv.org/abs/2308.09183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mika Beckerich, Laura Plein, Sergio Coronado</li>
<li>for: 这项研究旨在探讨使用开源插件和大语言模型（LLMs）在软件工程中新开的可能性，以及这些技术在网络安全方面带来的新挑战。</li>
<li>methods: 研究人员使用ChatGPT等LLMs生成攻击性内容，并通过在攻击者和受害者之间作为中间人使用这些LLMs来实现攻击。</li>
<li>results: 研究人员成功地使用ChatGPT等LLMs生成攻击性软件，并在不被检测的情况下传递命令到受害者系统。这项研究指出了使用开源插件和LLMs时存在的重要网络安全问题，需要开发安全指南、控制和缓解策略。<details>
<summary>Abstract</summary>
The evolution of Generative AI and the capabilities of the newly released Large Language Models (LLMs) open new opportunities in software engineering. However, they also lead to new challenges in cybersecurity. Recently, researchers have shown the possibilities of using LLMs such as ChatGPT to generate malicious content that can directly be exploited or guide inexperienced hackers to weaponize tools and code. Those studies covered scenarios that still require the attacker in the middle of the loop. In this study, we leverage openly available plugins and use an LLM as proxy between the attacker and the victim. We deliver a proof-of-concept where ChatGPT is used for the dissemination of malicious software while evading detection, alongside establishing the communication to a command and control (C2) server to receive commands to interact with a victim's system. Finally, we present the general approach as well as essential elements in order to stay undetected and make the attack a success. This proof-of-concept highlights significant cybersecurity issues with openly available plugins and LLMs, which require the development of security guidelines, controls, and mitigation strategies.
</details>
<details>
<summary>摘要</summary>
随着生成式人工智能的演化和新一代大语言模型（LLMs）的发布，软件工程方面开放出了新的机遇。然而，这也导致了新的网络安全挑战。最近，研究人员已经证明了使用 ChatGPT 等 LLMs 生成恶意内容，并直接利用或帮助不熟悉黑客 weaponize 工具和代码。这些研究都是在攻击者在中途的情况下进行的。在本研究中，我们利用公开available的插件，并使用 LLM 作为攻击者和受害者之间的代理。我们实现了一个证明，在使用 ChatGPT 进行恶意软件的散布时，同时避免检测，并与Command and Control（C2）服务器建立通信，以接收对受害者系统的交互命令。最后，我们提出了一种总体方法和重要元素，以确保隐蔽和成功攻击。这个证明指出了公开available的插件和 LLMS 对网络安全的潜在问题，需要开发安全指南、控制和缓解策略。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT"><a href="#ChatGPT-HealthPrompt-Harnessing-the-Power-of-XAI-in-Prompt-Based-Healthcare-Decision-Support-using-ChatGPT" class="headerlink" title="ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT"></a>ChatGPT-HealthPrompt. Harnessing the Power of XAI in Prompt-Based Healthcare Decision Support using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09731">http://arxiv.org/abs/2308.09731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Nazary, Yashar Deldjoo, Tommaso Di Noia</li>
<li>for: 本研究旨在探讨大语言模型（LLM）在医疗决策中的应用，尤其是OpenAI的ChatGPT。我们的方法涉及使用上下文提示，截然地设计了任务描述、特征描述以及针对医疗领域知识的整合。</li>
<li>methods: 我们的研究利用了医疗领域知识，从高性能可解释Machine Learning（ML）模型中提取了关键信息，并将其灵活地 интеGRATE到提示设计中。我们视这些ML模型为医疗专家，从而提取了关键的特征重要性，以帮助决策过程。</li>
<li>results: 我们的研究发现，通过使用上下文提示和医疗领域知识的整合，可以在数据缺乏情况下实现高质量的 binary 分类任务。此外，我们还探讨了LLMs在不同数据条件下的零少shot和几少shot提示学习效果，并对传统supervised ML模型进行比较。<details>
<summary>Abstract</summary>
This study presents an innovative approach to the application of large language models (LLMs) in clinical decision-making, focusing on OpenAI's ChatGPT. Our approach introduces the use of contextual prompts-strategically designed to include task description, feature description, and crucially, integration of domain knowledge-for high-quality binary classification tasks even in data-scarce scenarios. The novelty of our work lies in the utilization of domain knowledge, obtained from high-performing interpretable ML models, and its seamless incorporation into prompt design. By viewing these ML models as medical experts, we extract key insights on feature importance to aid in decision-making processes. This interplay of domain knowledge and AI holds significant promise in creating a more insightful diagnostic tool.   Additionally, our research explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. In essence, this paper bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. It highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details>
<details>
<summary>摘要</summary>
Our research also explores the dynamics of zero-shot and few-shot prompt learning based on LLMs. By comparing the performance of OpenAI's ChatGPT with traditional supervised ML models in different data conditions, we aim to provide insights into the effectiveness of prompt engineering strategies under varied data availability. This study bridges the gap between AI and healthcare, proposing a novel methodology for LLMs application in clinical decision support systems. Our approach highlights the transformative potential of effective prompt design, domain knowledge integration, and flexible learning approaches in enhancing automated decision-making.
</details></li>
</ul>
<hr>
<h2 id="Diversifying-AI-Towards-Creative-Chess-with-AlphaZero"><a href="#Diversifying-AI-Towards-Creative-Chess-with-AlphaZero" class="headerlink" title="Diversifying AI: Towards Creative Chess with AlphaZero"></a>Diversifying AI: Towards Creative Chess with AlphaZero</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09175">http://arxiv.org/abs/2308.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Zahavy, Vivek Veeriah, Shaobo Hou, Kevin Waugh, Matthew Lai, Edouard Leurent, Nenad Tomasev, Lisa Schut, Demis Hassabis, Satinder Singh</li>
<li>for: 这项研究是为了检查人工智能（AI）系统是否可以从创造性决策机制中受益，当被推到计算性理解的边缘时。</li>
<li>methods: 我们使用了多种行为多样性技术来让AI系统生成更多的想法，然后选择最有前途的想法。我们基于AlphaZero（AZ）扩展了其为一个联盟agent，并使用了隐藏条件的建筑来让AZ_db在不同的开局中选择最佳策略。</li>
<li>results: 我们的实验表明，AZ_db在困难的棋局中玩出了更多的想法，解决了更多的问题，并在多个开局中比AZ表现出色。特别是，AZ_db在解决了困难的彭罗斯位置的棋局中表现出了特别的优异。在不同的开局中，我们发现了每个player在不同的开局中特циализи得更强，并且通过使用不同的开局来选择最佳策略，可以提高50个Elo分。我们的发现表明，在AI代理中，多样性奖励存在，与人类团队中的多样性奖励相似。<details>
<summary>Abstract</summary>
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group and outperforms a more homogeneous team. Notably, AZ_db solves twice as many challenging puzzles as AZ, including the challenging Penrose positions. When playing chess from different openings, we notice that players in AZ_db specialize in different openings, and that selecting a player for each opening using sub-additive planning results in a 50 Elo improvement over AZ. Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans and that diversity is a valuable asset in solving computationally hard problems.
</details>
<details>
<summary>摘要</summary>
近年来，人工智能（AI）系统已经超越了人类智能在多种计算任务中。然而，AI系统，如人类一样，会出现错误、盲点、幻觉和适应新情况的困难。这项工作探索了AI是否可以通过创造性决策机制来提高其计算合理性。特别是，我们研究了一群多样化AI系统是否可以在复杂任务中超越单一AI，通过生成更多的想法并选择最佳的想法来解决问题。我们在国际象棋中进行了研究，国际象棋被称为AI的“蜘蛛”。我们基于AlphaZero（AZ），并将其扩展为多个代理via含有 latent 条件的架构，称为AZ_db。我们在AZ_db中使用行为多样性技术来生成更广泛的想法，并使用负添加itive  планинг选择最有前途的想法。我们的实验表明，AZ_db在各种开局下进行棋牌比赛，可以解决更多的难题，并且在某些情况下比AZ更好。特别是，AZ_db解决了AlphaZero无法解决的困难位置，包括贝叶姆位置。当AZ_db在不同的开局下进行棋牌比赛时，我们发现每个代理在不同的开局上特化，并且通过选择每个开局中的最佳代理使用负添加itive  планинг，可以提高50个Elo分。我们的发现表明，在AI代理团队中，多样性奖励 emerge，与人类团队一样，多样性是解决计算上的难题的有价值资产。
</details></li>
</ul>
<hr>
<h2 id="Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks"><a href="#Forensic-Data-Analytics-for-Anomaly-Detection-in-Evolving-Networks" class="headerlink" title="Forensic Data Analytics for Anomaly Detection in Evolving Networks"></a>Forensic Data Analytics for Anomaly Detection in Evolving Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09171">http://arxiv.org/abs/2308.09171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Yang, Abdallah Moubayed, Abdallah Shami, Amine Boukhtouta, Parisa Heidari, Stere Preda, Richard Brunner, Daniel Migault, Adel Larabi<br>for:The paper is written to elaborate effective security controls to protect evolving network deployments in-depth, specifically in the context of 5G and virtualization.methods:The paper proposes a digital forensic data analytics framework for network anomaly detection, which includes multi-perspective feature engineering, unsupervised anomaly detection, and comprehensive result correction procedures.results:Experiments on real-world evolving network data demonstrate the effectiveness of the proposed forensic data analytics solution.<details>
<summary>Abstract</summary>
In the prevailing convergence of traditional infrastructure-based deployment (i.e., Telco and industry operational networks) towards evolving deployments enabled by 5G and virtualization, there is a keen interest in elaborating effective security controls to protect these deployments in-depth. By considering key enabling technologies like 5G and virtualization, evolving networks are democratized, facilitating the establishment of point presences integrating different business models ranging from media, dynamic web content, gaming, and a plethora of IoT use cases. Despite the increasing services provided by evolving networks, many cybercrimes and attacks have been launched in evolving networks to perform malicious activities. Due to the limitations of traditional security artifacts (e.g., firewalls and intrusion detection systems), the research on digital forensic data analytics has attracted more attention. Digital forensic analytics enables people to derive detailed information and comprehensive conclusions from different perspectives of cybercrimes to assist in convicting criminals and preventing future crimes. This chapter presents a digital analytics framework for network anomaly detection, including multi-perspective feature engineering, unsupervised anomaly detection, and comprehensive result correction procedures. Experiments on real-world evolving network data show the effectiveness of the proposed forensic data analytics solution.
</details>
<details>
<summary>摘要</summary>
在传统基础设施（如telco和产业运维网络）向5G和虚拟化技术的演进部署过渡时，有很大的兴趣在深入保护这些部署。通过考虑关键实现技术（如5G和虚拟化），演进网络得到了民主化，使得不同业务模式的点 présence得到了建立，包括媒体、动态网页内容、游戏和大量物联网应用场景。然而，随着演进网络服务的增加，许多网络犯罪和攻击也在演进网络进行 malicious 活动。由于传统安全文件（如防火墙和入侵检测系统）的局限性，研究数字科学证据分析在获得更多的关注。数字科学证据分析可以帮助人们从不同角度获得细节信息，并提供完整的结论，以帮助控制犯罪和预防未来犯罪。本章介绍了一个网络异常检测数字分析框架，包括多元视角特征工程、无监督异常检测和完整结果修正过程。对实际演进网络数据进行实验，表明提议的数字科学证据分析解决方案的有效性。
</details></li>
</ul>
<hr>
<h2 id="Online-Transition-Based-Feature-Generation-for-Anomaly-Detection-in-Concurrent-Data-Streams"><a href="#Online-Transition-Based-Feature-Generation-for-Anomaly-Detection-in-Concurrent-Data-Streams" class="headerlink" title="Online Transition-Based Feature Generation for Anomaly Detection in Concurrent Data Streams"></a>Online Transition-Based Feature Generation for Anomaly Detection in Concurrent Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10893">http://arxiv.org/abs/2308.10893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzheng Zhong, Alexei Lisitsa</li>
<li>for: 这种技术是为了处理通用活动数据，包括网络流量、系统调用和监控录像等，并生成step-by-step生成的数据。</li>
<li>methods: 这种技术使用转换基于特征生成器（TFGen）技术，可以在线处理数据，并将历史数据编码到每个进来的活动中，以实现高效计算。</li>
<li>results: 这种技术可以解决域独特性、全球过程结构的发现、时间序列数据编码和在线处理能力等问题。<details>
<summary>Abstract</summary>
In this paper, we introduce the transition-based feature generator (TFGen) technique, which reads general activity data with attributes and generates step-by-step generated data. The activity data may consist of network activity from packets, system calls from processes or classified activity from surveillance cameras. TFGen processes data online and will generate data with encoded historical data for each incoming activity with high computational efficiency. The input activities may concurrently originate from distinct traces or channels. The technique aims to address issues such as domain-independent applicability, the ability to discover global process structures, the encoding of time-series data, and online processing capability.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种基于过程的特征生成技术（TFGen），它可以读取具有特征的活动数据，并生成步骤生成的数据。活动数据可以包括网络活动数据包、进程系统调用或Surveillance camera中分类的活动数据。TFGen处理数据在线，并将生成每个入参活动数据的编码历史数据，以高效处理能力进行处理。输入活动可同时来自不同的轨迹或通道。该技术的目标是解决域独立应用性、找到全局过程结构、编码时间序列数据以及在线处理能力等问题。
</details></li>
</ul>
<hr>
<h2 id="FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning"><a href="#FedPerfix-Towards-Partial-Model-Personalization-of-Vision-Transformers-in-Federated-Learning" class="headerlink" title="FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning"></a>FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09160">http://arxiv.org/abs/2308.09160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imguangyu/fedperfix">https://github.com/imguangyu/fedperfix</a></li>
<li>paper_authors: Guangyu Sun, Matias Mendieta, Jun Luo, Shandong Wu, Chen Chen</li>
<li>for: 这个研究旨在提高分散式学习中的模型个性化，并且应用于具有不同数据分布的多种模型中。</li>
<li>methods: 研究使用了半模型个性化来提高分散式学习的效率，并且对每种层次进行了调查，以了解哪些部分最需要个性化。</li>
<li>results: 研究结果显示，使用 FedPerfix 可以对 CIFAR-100、OrganAMNIST 和 Office-Home 等 dataset 进行优化，并且与多种进阶 PFL 方法进行比较，获得了更好的效果。<details>
<summary>Abstract</summary>
Personalized Federated Learning (PFL) represents a promising solution for decentralized learning in heterogeneous data environments. Partial model personalization has been proposed to improve the efficiency of PFL by selectively updating local model parameters instead of aggregating all of them. However, previous work on partial model personalization has mainly focused on Convolutional Neural Networks (CNNs), leaving a gap in understanding how it can be applied to other popular models such as Vision Transformers (ViTs). In this work, we investigate where and how to partially personalize a ViT model. Specifically, we empirically evaluate the sensitivity to data distribution of each type of layer. Based on the insights that the self-attention layer and the classification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrate its effectiveness in improving the model's performance compared to several advanced PFL methods.
</details>
<details>
<summary>摘要</summary>
personalized federated learning (PFL) 是一种有前途的解决方案，用于分布式学习不同数据环境中的模型个性化。partial model personalization 可以提高 PFL 的效率，通过选择ively更新本地模型参数而不是所有参数的汇集。然而，过去关于 partial model personalization 的研究主要集中在卷积神经网络 (CNNs) 上，尚未对其他流行的模型，如视觉转换器 (ViTs) 进行研究。在这项工作中，我们调查了如何在 ViT 模型中部分个性化。具体来说，我们employn了数据分布的敏感性测试，发现自注意层和分类头是 ViT 模型中最敏感的部分。基于这些发现，我们提出了一种名为 FedPerfix，它使用插件来将汇集模型中的信息传递给本地客户端进行个性化。最后，我们在 CIFAR-100、OrganAMNIST 和 Office-Home  datasets 上评估了我们的方法，并证明它在许多先进的 PFL 方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Data-diversity-and-virtual-imaging-in-AI-based-diagnosis-A-case-study-based-on-COVID-19"><a href="#Data-diversity-and-virtual-imaging-in-AI-based-diagnosis-A-case-study-based-on-COVID-19" class="headerlink" title="Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19"></a>Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09730">http://arxiv.org/abs/2308.09730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi, W. Paul Segars, Ehsan Samei, Joseph Y. Lo</li>
<li>for: This study aimed to evaluate the performance of deep-learning-based AI models for COVID-19 diagnosis using diverse clinical and virtually generated medical images, and to assess the impact of dataset characteristics, disease extent, and imaging modality on AI performance.</li>
<li>methods: The study used a retrospective design and developed AI models using both clinical and virtually generated medical images. A virtual imaging trial was conducted to assess the impact of patient- and physics-based factors on AI performance.</li>
<li>results: The study found that AI performance was strongly influenced by dataset characteristics, with poor generalization to new data and a 20% drop in receiver operating characteristic area under the curve. CT results were consistently superior to those from CXR, and imaging dose had negligible influence on results. The study highlighted the significance of dataset characteristics and disease extent on COVID assessment, and the potential role of virtual imaging trial techniques in developing effective AI algorithms and facilitating translation into diagnostic practice.<details>
<summary>Abstract</summary>
Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model performance on virtual CT and CXR images was comparable to overall results on clinical data. Imaging dose proved to have negligible influence on the results, but the extent of the disease had a marked affect. CT results were consistently superior to those from CXR. Overall, the study highlighted the significant impact of dataset characteristics and disease extent on COVID assessment, and the relevance and potential role of virtual imaging trial techniques on developing effective evaluation of AI algorithms and facilitating translation into diagnostic practice.
</details>
<details>
<summary>摘要</summary>
AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model performance on virtual CT and CXR images was comparable to overall results on clinical data. Imaging dose proved to have negligible influence on the results, but the extent of the disease had a marked affect. CT results were consistently superior to those from CXR. Overall, the study highlighted the significant impact of dataset characteristics and disease extent on COVID assessment, and the relevance and potential role of virtual imaging trial techniques on developing effective evaluation of AI algorithms and facilitating translation into diagnostic practice.Translation:多些研究已经 investigate deep learning 基于人工智能（AI）模型用于医疗影像诊断 COVID-19，其中许多报告显示 near-perfect 性能。然而，表现变化和下面数据偏见引起了临床可靠性的 Concerns。这项 retrospective 研究涉及开发和评估 COVID-19 诊断中使用多种临床和虚拟生成的医疗影像 AI 模型。此外，我们还进行了虚拟成像试验，以评估 AI 性能受患者和物理因素的影响，包括疾病程度、辐射剂量和成像方式。AI 性能强烈受 dataset 特点的影响，包括数量、多样性和患者发展程度，导致到 20% 的 Receiver Operating Characteristic 曲线下降。模型在虚拟 CT 和 CXR 影像上的性能与临床数据上的性能相似。辐射剂量对结果没有重要影响，但疾病程度有显著影响。 CT 结果比 CXR 结果更好。总之，这项研究强调了 COVID 诊断中数据特点和疾病程度的重要影响，以及虚拟成像试验技术在开发有效 AI 算法并实现诊断实践中的重要作用。
</details></li>
</ul>
<hr>
<h2 id="ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse"><a href="#ZhiJian-A-Unifying-and-Rapidly-Deployable-Toolbox-for-Pre-trained-Model-Reuse" class="headerlink" title="ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse"></a>ZhiJian: A Unifying and Rapidly Deployable Toolbox for Pre-trained Model Reuse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09158">http://arxiv.org/abs/2308.09158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangyikaii/lamda-zhijian">https://github.com/zhangyikaii/lamda-zhijian</a></li>
<li>paper_authors: Yi-Kai Zhang, Lu Ren, Chao Yi, Qi-Wei Wang, De-Chuan Zhan, Han-Jia Ye</li>
<li>for: This paper is written for deep learning practitioners and researchers who want to explore downstream tasks and identify the complementary advantages among different methods for model reuse.</li>
<li>methods: The paper introduces ZhiJian, a comprehensive and user-friendly toolbox for model reuse that utilizes the PyTorch backend. ZhiJian presents a novel paradigm that unifies diverse perspectives on model reuse, including target architecture construction with PTM, tuning target model with PTM, and PTM-based inference.</li>
<li>results: The paper empowers deep learning practitioners to explore downstream tasks and identify the complementary advantages among different methods for model reuse, and ZhiJian is readily accessible at <a target="_blank" rel="noopener" href="https://github.com/zhangyikaii/lamda-zhijian">https://github.com/zhangyikaii/lamda-zhijian</a> for seamless utilization of pre-trained models and streamlining the model reuse process.<details>
<summary>Abstract</summary>
The rapid expansion of foundation pre-trained models and their fine-tuned counterparts has significantly contributed to the advancement of machine learning. Leveraging pre-trained models to extract knowledge and expedite learning in real-world tasks, known as "Model Reuse", has become crucial in various applications. Previous research focuses on reusing models within a certain aspect, including reusing model weights, structures, and hypothesis spaces. This paper introduces ZhiJian, a comprehensive and user-friendly toolbox for model reuse, utilizing the PyTorch backend. ZhiJian presents a novel paradigm that unifies diverse perspectives on model reuse, encompassing target architecture construction with PTM, tuning target model with PTM, and PTM-based inference. This empowers deep learning practitioners to explore downstream tasks and identify the complementary advantages among different methods. ZhiJian is readily accessible at https://github.com/zhangyikaii/lamda-zhijian facilitating seamless utilization of pre-trained models and streamlining the model reuse process for researchers and developers.
</details>
<details>
<summary>摘要</summary>
“快速扩展基础模型和其精度调整版本的发展对机器学习进步做出了重要贡献。利用预训练模型提取知识和快速学习实际任务，称为“模型重用”，在各种应用中变得非常重要。先前的研究主要关注在某一方面进行模型重用，包括重用模型权重、结构和假设空间。本文介绍了智慧箱（ZhiJian），一个通用且易用的模型重用工具箱，使用PyTorch backend。智慧箱提出了一种新的模型重用 paradigma，整合多种视角，包括目标建筑PTM、调整目标模型PTM和PTM基于的推理。这使得深度学习专家能够更好地探索下游任务，并发现不同方法之间的补偿优势。智慧箱可以在https://github.com/zhangyikaii/lamda-zhijian中免费获取，便于预训练模型的使用和模型重用过程，为研究人员和开发者提供便捷的使用。”
</details></li>
</ul>
<hr>
<h2 id="Accurate-machine-learning-force-fields-via-experimental-and-simulation-data-fusion"><a href="#Accurate-machine-learning-force-fields-via-experimental-and-simulation-data-fusion" class="headerlink" title="Accurate machine learning force fields via experimental and simulation data fusion"></a>Accurate machine learning force fields via experimental and simulation data fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09142">http://arxiv.org/abs/2308.09142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastien Röcken, Julija Zavadlav</li>
<li>for: 这个论文的目的是开发一种基于机器学习（ML）的力场模型，以涵盖离子质量级别的分子模型。</li>
<li>methods: 该论文使用了density functional theory（DFT）计算和实验测量的机械性质和晶体结构来训练一个ML力场模型。</li>
<li>results: 研究人员发现，通过结合DFT计算和实验数据来训练ML力场模型，可以同时满足所有目标目标，而且比使用单个数据源训练的模型更准确。此外，通过这种方法，可以正确地修正DFT函数的缺陷，同时保持偏离目标 свой性的准确性。<details>
<summary>Abstract</summary>
Machine Learning (ML)-based force fields are attracting ever-increasing interest due to their capacity to span spatiotemporal scales of classical interatomic potentials at quantum-level accuracy. They can be trained based on high-fidelity simulations or experiments, the former being the common case. However, both approaches are impaired by scarce and erroneous data resulting in models that either do not agree with well-known experimental observations or are under-constrained and only reproduce some properties. Here we leverage both Density Functional Theory (DFT) calculations and experimentally measured mechanical properties and lattice parameters to train an ML potential of titanium. We demonstrate that the fused data learning strategy can concurrently satisfy all target objectives, thus resulting in a molecular model of higher accuracy compared to the models trained with a single data source. The inaccuracies of DFT functionals at target experimental properties were corrected, while the investigated off-target properties remained largely unperturbed. Our approach is applicable to any material and can serve as a general strategy to obtain highly accurate ML potentials.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RTB-Formulation-Using-Point-Process"><a href="#RTB-Formulation-Using-Point-Process" class="headerlink" title="RTB Formulation Using Point Process"></a>RTB Formulation Using Point Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09122">http://arxiv.org/abs/2308.09122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seong Jin Lee, Bumsik Kim</li>
<li>for: 这篇论文是为了模型重复拍卖（RTB）生态系统中的点过程，并提出了一种通用的杂event-driven模型。</li>
<li>methods: 该论文使用点过程模型来描述拍卖过程，并提出了一种基于Poisson点过程的approximation方法。</li>
<li>results: 该论文提出了player的优化策略以及joint分布的重要性，并指出了在不同情况下的优化策略。<details>
<summary>Abstract</summary>
We propose a general stochastic framework for modelling repeated auctions in the Real Time Bidding (RTB) ecosystem using point processes. The flexibility of the framework allows a variety of auction scenarios including configuration of information provided to player, determination of auction winner and quantification of utility gained from each auctions. We propose theoretical results on how this formulation of process can be approximated to a Poisson point process, which enables the analyzer to take advantage of well-established properties. Under this framework, we specify the player's optimal strategy under various scenarios. We also emphasize that it is critical to consider the joint distribution of utility and market condition instead of estimating the marginal distributions independently.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用的随机化框架，用于模拟RTB生态系统中的重复拍卖。这个框架的灵活性允许多种拍卖场景，包括玩家信息提供方式、拍卖赢家决定以及每次拍卖中获得的用于量。我们提出了理论结果，表明这种过程的形式可以approximerate到一个Poisson点过程，这使得分析者可以利用已有的性质。在这个框架下，我们定义了玩家的优化策略在不同场景下。我们还强调了考虑市场条件和用户性的联合分布，而不是独立地估算各自的分布。
</details></li>
</ul>
<hr>
<h2 id="Multi-fidelity-Fourier-Neural-Operator-for-Fast-Modeling-of-Large-Scale-Geological-Carbon-Storage"><a href="#Multi-fidelity-Fourier-Neural-Operator-for-Fast-Modeling-of-Large-Scale-Geological-Carbon-Storage" class="headerlink" title="Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage"></a>Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09113">http://arxiv.org/abs/2308.09113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hewei Tang, Qingkai Kong, Joseph P. Morris</li>
<li>for: 用于加速地质碳存储（GCS）问题中预测储存压力和CO2气泡迁移的深度学习基于模拟器。</li>
<li>methods: 使用多优异Fourier神经操作器解决大规模GCS问题，使用更可持有的多优异训练数据集来预测复杂物理行为。</li>
<li>results: 测试表明，使用多优异FNO模型可以在81%的数据生成成本下达到与高精度模型相同的准确率，并且在不同地球预测模型和流体 simulator生成的高精度和低精度数据集上进行了一致预测。<details>
<summary>Abstract</summary>
Deep learning-based surrogate models have been widely applied in geological carbon storage (GCS) problems to accelerate the prediction of reservoir pressure and CO2 plume migration. Large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. In practice, the available training data are always limited in large-scale 3D problems due to the high computational cost. Therefore, we propose to use a multi-fidelity Fourier Neural Operator to solve large-scale GCS problems with more affordable multi-fidelity training datasets. The Fourier Neural Operator has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. We first test the model efficacy on a GCS reservoir model being discretized into 110k grid cells. The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained with the same amount of high-fidelity data with 81% less data generation costs. We further test the generalizability of the multi-fidelity model on a same reservoir model with a finer discretization of 1 million grid cells. This case was made more challenging by employing high-fidelity and low-fidelity datasets generated by different geostatistical models and reservoir simulators. We observe that the multi-fidelity FNO model can predict pressure fields with reasonable accuracy even when the high-fidelity data are extremely limited.
</details>
<details>
<summary>摘要</summary>
深度学习基于的代理模型在地球科学中广泛应用于加速预测储存气体和二氧化碳泵迹迁移。它们需要大量的物理基础数据来训练模型，以便准确预测这些过程中的复杂物理行为。然而，在大规模3D问题中，实际可用的培训数据总是有限的，因此我们提议使用多质量Fourier neural operator来解决这些问题。Fourier neural operator具有可以简化转移学习过程的网格不变性属性，因此可以在不同的精度水平上进行数据转移。我们首先测试了模型在110k个网格细分的气体储存器模型上的效果。我们发现，使用多质量模型可以与高精度模型在同样的数据量下达到相同的准确率，但是需要81% fewer data generation costs。我们进一步测试了模型的通用性，在同一个气体储存器模型上使用不同的地OSTATISTICAL模型和气体 simulator生成的高精度和低精度数据来进行测试。我们发现，使用多质量FNO模型可以在高精度数据很少的情况下预测压力场的reasonable accuracy。
</details></li>
</ul>
<hr>
<h2 id="Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation"><a href="#Learning-Lightweight-Object-Detectors-via-Multi-Teacher-Progressive-Distillation" class="headerlink" title="Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation"></a>Learning Lightweight Object Detectors via Multi-Teacher Progressive Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09105">http://arxiv.org/abs/2308.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengcao Cao, Mengtian Li, James Hays, Deva Ramanan, Yi-Xiong Wang, Liang-Yan Gui</li>
<li>for: 提高资源受限的感知系统中的视觉模型精度和轻量级计算和存储使用。</li>
<li>methods: 提出一种简单 yet 有效的顺序方法，从多个教师检测器中逐步传递知识到一个轻量级学生模型。</li>
<li>results: 成功地从Transformer基本的教师检测器中提取知识，并使用逐步进行知识传递的方法，从而提高了ResNet-50基于RetinaNet的MS COCObenchmark中的AP分数从36.5%提高到42.0%，以及Mask R-CNN的AP分数从38.2%提高到42.5%。<details>
<summary>Abstract</summary>
Resource-constrained perception systems such as edge computing and vision-for-robotics require vision models to be both accurate and lightweight in computation and memory usage. While knowledge distillation is a proven strategy to enhance the performance of lightweight classification models, its application to structured outputs like object detection and instance segmentation remains a complicated task, due to the variability in outputs and complex internal network modules involved in the distillation process. In this paper, we propose a simple yet surprisingly effective sequential approach to knowledge distillation that progressively transfers the knowledge of a set of teacher detectors to a given lightweight student. To distill knowledge from a highly accurate but complex teacher model, we construct a sequence of teachers to help the student gradually adapt. Our progressive strategy can be easily combined with existing detection distillation mechanisms to consistently maximize student performance in various settings. To the best of our knowledge, we are the first to successfully distill knowledge from Transformer-based teacher detectors to convolution-based students, and unprecedentedly boost the performance of ResNet-50 based RetinaNet from 36.5% to 42.0% AP and Mask R-CNN from 38.2% to 42.5% AP on the MS COCO benchmark.
</details>
<details>
<summary>摘要</summary>
资源受限的感知系统，如边缘 computing 和 robotics 视觉系统，需要视觉模型具备高度精度和轻量级的计算和内存使用。而知识传递是一种证实的策略，可以提高轻量级分类模型的性能，但是对于结构化输出 like 物体探测和实例分割仍然是一个复杂的任务，因为输出的变化和内部网络模组的复杂性。在这篇文章中，我们提出了一个简单又奇怪有效的顺序式知识传递方法，可以将一群教师探测器中的知识逐渐传递给一个轻量级学生。为了将高度精度但复杂的教师探测器中的知识传递给 convolution 型学生，我们建立了一系列的教师，帮助学生逐渐适应。我们的顺序式策略可以与现有的探测传递机制整合，以确保学生在不同的设定下的表现最佳。我们知道，我们是第一个成功地将 Transformer 型教师探测器中的知识传递给 convolution 型学生，并在 MS COCO benchmark 上从 36.5% 提高到 42.0% AP 和 38.2% 提高到 42.5% AP。
</details></li>
</ul>
<hr>
<h2 id="A-comprehensive-study-of-spike-and-slab-shrinkage-priors-for-structurally-sparse-Bayesian-neural-networks"><a href="#A-comprehensive-study-of-spike-and-slab-shrinkage-priors-for-structurally-sparse-Bayesian-neural-networks" class="headerlink" title="A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks"></a>A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09104">http://arxiv.org/abs/2308.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanket Jantre, Shrijita Bhattacharya, Tapabrata Maiti</li>
<li>for: 提高深度学习的网络复杂性和计算效率，使用减少极大过 parameterized 深度神经网络来恢复稀疏表示。</li>
<li>methods: 使用架构稀疏（例如节点稀疏）压缩深度神经网络，以实现低延迟推理、高速数据传输和降低能耗。</li>
<li>results: 使用架构稀疏 Bayesian 神经网络，通过 Spike-and-Slab Group Lasso (SS-GL) 和 Spike-and-Slab Group Horseshoe (SS-GHS) 约束，实现计算可 tractable 变分推理，并在不同网络架构、层次节点数量和网络参数的约束下，确定变分 posterior 的收缩率。通过实验，比较了我们的模型与基eline模型的预测精度、模型压缩和推理延迟，并证明了我们的模型在这些方面的竞争性。<details>
<summary>Abstract</summary>
Network complexity and computational efficiency have become increasingly significant aspects of deep learning. Sparse deep learning addresses these challenges by recovering a sparse representation of the underlying target function by reducing heavily over-parameterized deep neural networks. Specifically, deep neural architectures compressed via structured sparsity (e.g. node sparsity) provide low latency inference, higher data throughput, and reduced energy consumption. In this paper, we explore two well-established shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian neural networks. To this end, we propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables. We establish the contraction rates of the variational posterior of our proposed models as a function of the network topology, layer-wise node cardinalities, and bounds on the network weights. We empirically demonstrate the competitive performance of our models compared to the baseline models in prediction accuracy, model compression, and inference latency.
</details>
<details>
<summary>摘要</summary>
网络复杂性和计算效率在深度学习中日益重要。 sparse deep learning 通过recovering 简洁表示函数来降低深度神经网络的复杂性。Specifically, 深度神经网络通过结构减少（例如节点稀疏）提供低延迟推理、高速数据传输和降低能耗。在这篇论文中，我们探讨了两种已有的减少技术，lasso 和 horseshoe，用于神经网络压缩。为此，我们提出了结构减少的 Bayesian neural networks，并使用 Spike-and-Slab Group Lasso（SS-GL）和 Spike-and-Slab Group Horseshoe（SS-GHS）假设来系统地剪除过度的节点。我们还开发了可 computationally tractable 的变换推理，包括连续放寒变量的放寒。我们证明了我们提出的模型的可信度衰减率与网络结构、层次节点数量和网络权值的约束有关。我们还通过实验表明我们的模型与基eline模型相比在预测精度、模型压缩和推理延迟方面具有竞争性。
</details></li>
</ul>
<hr>
<h2 id="MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models"><a href="#MindMap-Knowledge-Graph-Prompting-Sparks-Graph-of-Thoughts-in-Large-Language-Models" class="headerlink" title="MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models"></a>MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09729">http://arxiv.org/abs/2308.09729</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/willing510/MindMap">https://github.com/willing510/MindMap</a></li>
<li>paper_authors: Yilin Wen, Zifeng Wang, Jimeng Sun</li>
<li>for: 这个论文的目的是解决大语言模型（LLM）的限制，包括它们的知识更新、hallucination和决策过程的不透明度。</li>
<li>methods: 这篇论文使用知识图（KG）来激活LLM的知识更新和逻辑推理。具体来说，我们建立了一个提示管道，使LLM能够理解KG输入并结合内存中的隐式知识和外部知识进行推理。此外，我们还研究了询问LLM的思维路径和生成答案。</li>
<li>results: 实验结果表明，使用 MindMap 提示可以获得显著的实验性提升。例如，使用 MindMap 提示与 GPT-3.5 比较，可以在三个问答数据集上获得悬殊的性能。此外，我们还发现，使用 KG 中的结构化事实可以超过使用文档检索方法，因为 MindMap 可以提供更准确、简洁和全面的知识。<details>
<summary>Abstract</summary>
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance over GPT-4 consistently. We also demonstrate that with structured facts retrieved from KG, MindMap can outperform a series of prompting-with-document-retrieval methods, benefiting from more accurate, concise, and comprehensive knowledge from KGs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modeling-Edge-Features-with-Deep-Bayesian-Graph-Networks"><a href="#Modeling-Edge-Features-with-Deep-Bayesian-Graph-Networks" class="headerlink" title="Modeling Edge Features with Deep Bayesian Graph Networks"></a>Modeling Edge Features with Deep Bayesian Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09087">http://arxiv.org/abs/2308.09087</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diningphil/e-cgmm">https://github.com/diningphil/e-cgmm</a></li>
<li>paper_authors: Daniele Atzeni, Federico Errica, Davide Bacciu, Alessio Micheli</li>
<li>for: 这篇论文是为了扩展Contextual Graph Markov Model（一种深度和概率学习模型），以模型边Feature的分布。</li>
<li>methods: 我们提出了一种建立额外的 Bayesian网络，将边特征映射到精确的状态中，以便由原始模型使用。这种方法使得我们可以在缺乏边特征的情况下建立更加富有的图表示，并且在标准图分类benchmark上达到了性能提升。</li>
<li>results: 我们在图 regression 场景中成功应用了我们的提议，并在三个链接预测任务上实现了明显的性能提升。此外，我们还证明了我们的模型在大规模图处理中保持了线性计算复杂性。<details>
<summary>Abstract</summary>
We propose an extension of the Contextual Graph Markov Model, a deep and probabilistic machine learning model for graphs, to model the distribution of edge features. Our approach is architectural, as we introduce an additional Bayesian network mapping edge features into discrete states to be used by the original model. In doing so, we are also able to build richer graph representations even in the absence of edge features, which is confirmed by the performance improvements on standard graph classification benchmarks. Moreover, we successfully test our proposal in a graph regression scenario where edge features are of fundamental importance, and we show that the learned edge representation provides substantial performance improvements against the original model on three link prediction tasks. By keeping the computational complexity linear in the number of edges, the proposed model is amenable to large-scale graph processing.
</details>
<details>
<summary>摘要</summary>
我们提出了Contextual Graph Markov Model的扩展，一种深度和概率机器学习模型，以模型边Feature的分布。我们的方法是建立一个额外的 Bayesian 网络，将边特征映射到精确的状态中，以便由原始模型使用。这样做的好处是，我们可以在原始模型缺失edge特征时，建立更加富裕的图表示。此外，我们成功地在图回归情况下测试了我们的建议，并证明了对三个链接预测任务的性能提高。此外，我们保持了计算复杂性为边数量的线性，使得我们的模型适用于大规模图处理。
</details></li>
</ul>
<hr>
<h2 id="Embracing-assay-heterogeneity-with-neural-processes-for-markedly-improved-bioactivity-predictions"><a href="#Embracing-assay-heterogeneity-with-neural-processes-for-markedly-improved-bioactivity-predictions" class="headerlink" title="Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions"></a>Embracing assay heterogeneity with neural processes for markedly improved bioactivity predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09086">http://arxiv.org/abs/2308.09086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucian Chan, Marcel Verdonk, Carl Poelking</li>
<li>for: 预测药物的生物活性是计算辅助药物发现中最Difficult和最重要的挑战之一，尽管年月的数据收集和整理努力已经做出了很大的贡献，但生物活性数据仍然稀缺和多样化，这使得建立准确、可迁移和稳定的预测模型变得困难。</li>
<li>methods: 作者提出了一种层次meta学框架，该框架利用不同的试验方法之间的信息相互作用，成功地考虑了试验方法的多样化。</li>
<li>results: 作者展示了该模型在不同的蛋白目标和试验方法上的较好的预测性能，并且可以快速适应新的目标 Context 使用非常少的观察，因此可以实现大规模的虚拟屏选在早期药物发现阶段。<details>
<summary>Abstract</summary>
Predicting the bioactivity of a ligand is one of the hardest and most important challenges in computer-aided drug discovery. Despite years of data collection and curation efforts by research organizations worldwide, bioactivity data remains sparse and heterogeneous, thus hampering efforts to build predictive models that are accurate, transferable and robust. The intrinsic variability of the experimental data is further compounded by data aggregation practices that neglect heterogeneity to overcome sparsity. Here we discuss the limitations of these practices and present a hierarchical meta-learning framework that exploits the information synergy across disparate assays by successfully accounting for assay heterogeneity. We show that the model achieves a drastic improvement in affinity prediction across diverse protein targets and assay types compared to conventional baselines. It can quickly adapt to new target contexts using very few observations, thus enabling large-scale virtual screening in early-phase drug discovery.
</details>
<details>
<summary>摘要</summary>
预测药物的生物活性是计算辅助药物发现中最Difficult和最重要的挑战之一。尽管世界各地研究机构多年来努力收集和筛选数据，生物活性数据仍然稀缺和多样化，这使得建立准确、可传播和可靠的预测模型变得困难。实验室中的自然变化更加剑指了数据积累做法，这些做法忽略了多样性，以 overcome sparsity。我们讨论了这些实践的局限性，并提出了一种层次元学习框架，该框架利用不同测试方法之间的信息协同，成功地考虑到多样性。我们显示，该模型在不同蛋白目标和测试方法上显著改善了粘性预测，相比传统基准值，它可以快速适应新的目标上下文，使用非常少的观察数据进行大规模的虚拟屏选，以便在早期药物发现阶段进行大规模的虚拟屏选。
</details></li>
</ul>
<hr>
<h2 id="MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices"><a href="#MovePose-A-High-performance-Human-Pose-Estimation-Algorithm-on-Mobile-and-Edge-Devices" class="headerlink" title="MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices"></a>MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09084">http://arxiv.org/abs/2308.09084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyang Yu, Haoyue Zhang, Zhirui Zhou, Wangpeng An, Yanhong Yang</li>
<li>for: 这个论文的目的是为了提供高精度且实时性强的人体姿态估计模型，特别适用于 CPU 型手持式设备上的移动设备人体姿态估计。</li>
<li>methods: 这个模型使用了优化的轻量级卷积神经网组件，包括滤除、大 kernel 卷积和坐标分类方法，以提高精度和实时性。</li>
<li>results: 这个模型在 COCO 验证数据集上获得了 67.7 的 Mean Average Precision 分数，并在 Intel i9-10920x CPU 和 NVIDIA RTX3090 GPU 上显示出了高效性和实时性。<details>
<summary>Abstract</summary>
We present MovePose, an optimized lightweight convolutional neural network designed specifically for real-time body pose estimation on CPU-based mobile devices. The current solutions do not provide satisfactory accuracy and speed for human posture estimation, and MovePose addresses this gap. It aims to maintain real-time performance while improving the accuracy of human posture estimation for mobile devices. The network produces 17 keypoints for each individual at a rate exceeding 11 frames per second, making it suitable for real-time applications such as fitness tracking, sign language interpretation, and advanced mobile human posture estimation. Our MovePose algorithm has attained an Mean Average Precision (mAP) score of 67.7 on the COCO \cite{cocodata} validation dataset. The MovePose algorithm displayed efficiency with a performance of 69+ frames per second (fps) when run on an Intel i9-10920x CPU. Additionally, it showcased an increased performance of 452+ fps on an NVIDIA RTX3090 GPU. On an Android phone equipped with a Snapdragon 8 + 4G processor, the fps reached above 11. To enhance accuracy, we incorporated three techniques: deconvolution, large kernel convolution, and coordinate classification methods. Compared to basic upsampling, deconvolution is trainable, improves model capacity, and enhances the receptive field. Large kernel convolution strengthens these properties at a decreased computational cost. In summary, MovePose provides high accuracy and real-time performance, marking it a potential tool for a variety of applications, including those focused on mobile-side human posture estimation. The code and models for this algorithm will be made publicly accessible.
</details>
<details>
<summary>摘要</summary>
我们现在提出了 MovePose，一种优化的轻量级卷积神经网络，专门用于在 CPU 基于的移动设备上实时人体姿态估计。现有解决方案无法提供满意的准确率和速度 для人体姿态估计，MovePose 弥补了这一空隙。它的目标是在实时应用中维持实时性，同时提高人体姿态估计的准确率。该网络每秒钟产生17个关键点，适用于实时应用such as 健身跟踪、手语 interpret 和高级移动人体姿态估计。我们的 MovePose 算法在 COCO 验证集（）上获得了67.7的 Mean Average Precision（mAP）分数。 MovePose 算法在 Intel i9-10920x CPU 上运行时达到69+帧每秒（fps）的性能，而在 NVIDIA RTX3090 GPU 上则达到452+ fps。在配备 Snapdragon 8 + 4G 处理器的 Android 手机上，fps 超过11。为了提高准确率，我们引入了三种技术：deconvolution、大小 kernel convolution 和坐标分类方法。相比基本的 upsampling，deconvolution 可以学习、提高模型容量和感知范围。大小 kernel convolution 强化这些特性，而且降低计算成本。综上所述，MovePose 提供了高准确率和实时性，使其成为许多应用，包括移动端人体姿态估计的可能工具。我们将代码和模型公开访问。
</details></li>
</ul>
<hr>
<h2 id="Over-the-Air-Computation-Aided-Federated-Learning-with-the-Aggregation-of-Normalized-Gradient"><a href="#Over-the-Air-Computation-Aided-Federated-Learning-with-the-Aggregation-of-Normalized-Gradient" class="headerlink" title="Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient"></a>Over-the-Air Computation Aided Federated Learning with the Aggregation of Normalized Gradient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09082">http://arxiv.org/abs/2308.09082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongfei Fan, Xuming An, Shiyuan Zuo, Han Hu</li>
<li>for: 这个论文的目的是提出一种基于无线电计算的联合学习（Federated Learning，FL）方法，以提高系统的计算效率。</li>
<li>methods: 这个论文使用了一种循环进程，其中每个移动设备都会更新本地梯度，并将其扩大并传输给服务器。服务器接收所有设备的梯度，并生成并广播新的模型参数。在扩大因子选择方面，大多数相关的工作假设了本地梯度的最大 нор Always happens，这可能会降低数据的整合性。为了解决这个问题，我们提议将本地梯度转换成正规化的梯度。</li>
<li>results: 我们的提议方法可以在平滑的损失函数下达到相对较快的收敛速率，并且在平滑和强地 convex 损失函数下可以达到最小的训练损失，并且发现了收敛速率和容错度之间的负反关系。此外，我们还提出了一些优化系统参数的问题，并 derivated 优化问题的优化解决方案，其中问题的解决方案具有多项式复杂度。实验结果表明，我们的提议方法可以超越标准方法在收敛性能方面。<details>
<summary>Abstract</summary>
Over-the-air computation is a communication-efficient solution for federated learning (FL). In such a system, iterative procedure is performed: Local gradient of private loss function is updated, amplified and then transmitted by every mobile device; the server receives the aggregated gradient all-at-once, generates and then broadcasts updated model parameters to every mobile device. In terms of amplification factor selection, most related works suppose the local gradient's maximal norm always happens although it actually fluctuates over iterations, which may degrade convergence performance. To circumvent this problem, we propose to turn local gradient to be normalized one before amplifying it. Under our proposed method, when the loss function is smooth, we prove our proposed method can converge to stationary point at sub-linear rate. In case of smooth and strongly convex loss function, we prove our proposed method can achieve minimal training loss at linear rate with any small positive tolerance. Moreover, a tradeoff between convergence rate and the tolerance is discovered. To speedup convergence, problems optimizing system parameters are also formulated for above two cases. Although being non-convex, optimal solution with polynomial complexity of the formulated problems are derived. Experimental results show our proposed method can outperform benchmark methods on convergence performance.
</details>
<details>
<summary>摘要</summary>
无需网络传输的计算是一种可靠的解决方案 для联合学习（FL）。在这种系统中，每个移动设备都会进行迭代程序：每个设备都会更新、增强并将本地梯度传输给服务器，服务器会收到所有设备的汇总梯度，并生成并广播更新后的模型参数给每个设备。在扩大因子选择方面，大多数相关的研究假设了本地梯度的最大 нор Always happens，实际上，这可能会降低对称性性的性能。为解决这个问题，我们提议将本地梯度转换成归一化的梯度之前，然后扩大它。我们的提议方法可以在平滑的损失函数下 converge 到稳定点的速度下。在平滑和强地 convex 损失函数下，我们证明我们的提议方法可以在 any small positive tolerance 下 достичь最佳训练损失的速度。此外，我们发现了训练速度和误差容差之间的负面选择。为加速训练，我们还提出了一些优化系统参数的问题，并derived 优化问题的优秀解，其复杂度为多阶 polynomials。实验结果表明，我们的提议方法可以在对比方法上出现较好的融合性表现。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Sampling-of-Variational-Autoencoders-via-Iterated-Approximate-Ancestral-Sampling"><a href="#Conditional-Sampling-of-Variational-Autoencoders-via-Iterated-Approximate-Ancestral-Sampling" class="headerlink" title="Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling"></a>Conditional Sampling of Variational Autoencoders via Iterated Approximate Ancestral Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09078">http://arxiv.org/abs/2308.09078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaidotas Simkus, Michael U. Gutmann</li>
<li>for: 用于 addresses the limitations of Metropolis-within-Gibbs (MWG) sampler in variational autoencoders (VAEs) when learning a structured latent space.</li>
<li>methods: 使用 two original methods to mitigate the pitfalls of MWG sampler: (1) a novel initialization method to improve the mixing of the latent space, and (2) a adaptive temperature schedule to adjust the sampling temperature based on the current sample.</li>
<li>results: 对一系列的 sampling tasks 进行了实验，并证明了提出的方法可以提高 sampling 的性能。<details>
<summary>Abstract</summary>
Conditional sampling of variational autoencoders (VAEs) is needed in various applications, such as missing data imputation, but is computationally intractable. A principled choice for asymptotically exact conditional sampling is Metropolis-within-Gibbs (MWG). However, we observe that the tendency of VAEs to learn a structured latent space, a commonly desired property, can cause the MWG sampler to get "stuck" far from the target distribution. This paper mitigates the limitations of MWG: we systematically outline the pitfalls in the context of VAEs, propose two original methods that address these pitfalls, and demonstrate an improved performance of the proposed methods on a set of sampling tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>转换给定文本到简化中文。</SYS>> conditional sampling of variational autoencoders (VAEs) 是在各种应用中需要的，如数据缺失填充，但是计算复杂性太高。 Metropolis-within-Gibbs (MWG) 是一种理想的 asymptotically exact conditional sampling 方法，但我们发现 VAEs 往往学习一个结构化的尘肠空间，这是通常所希望的特性，可以使 MWG 抽样器受到目标分布的吸引力强化。这篇论文解决了 VAEs 中 MWG 抽样器的局限性，我们系统地描述了这些局限性在 VAEs 中的坑害，并提出了两种原创的方法来解决这些坑害，并在一组抽样任务中示出了提高的性能。
</details></li>
</ul>
<hr>
<h2 id="Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning"><a href="#Fast-Decision-Support-for-Air-Traffic-Management-at-Urban-Air-Mobility-Vertiports-using-Graph-Learning" class="headerlink" title="Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning"></a>Fast Decision Support for Air Traffic Management at Urban Air Mobility Vertiports using Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09075">http://arxiv.org/abs/2308.09075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajit KrisshnaKumar, Jhoel Witter, Steve Paul, Hanvit Cho, Karthik Dantu, Souma Chowdhury</li>
<li>for: 解决城市和郊区堵塞、安全、快速旅行的问题，提供城市空中流体力（UAM）的新一代解决方案。</li>
<li>methods: 使用图 reconstruction学习来生成决策支持策略，图 reconstruction学习是一种基于图学习的自动决策技术。</li>
<li>results: 通过在AirSim simulator上进行实际的多rotor飞行器模拟，研究发现使用图 reconstruction学习解决UAM-VSM问题的适用性和优势，比基本的再征学习（图嵌入）或随机选择基线更好。<details>
<summary>Abstract</summary>
Urban Air Mobility (UAM) promises a new dimension to decongested, safe, and fast travel in urban and suburban hubs. These UAM aircraft are conceived to operate from small airports called vertiports each comprising multiple take-off/landing and battery-recharging spots. Since they might be situated in dense urban areas and need to handle many aircraft landings and take-offs each hour, managing this schedule in real-time becomes challenging for a traditional air-traffic controller but instead calls for an automated solution. This paper provides a novel approach to this problem of Urban Air Mobility - Vertiport Schedule Management (UAM-VSM), which leverages graph reinforcement learning to generate decision-support policies. Here the designated physical spots within the vertiport's airspace and the vehicles being managed are represented as two separate graphs, with feature extraction performed through a graph convolutional network (GCN). Extracted features are passed onto perceptron layers to decide actions such as continue to hover or cruise, continue idling or take-off, or land on an allocated vertiport spot. Performance is measured based on delays, safety (no. of collisions) and battery consumption. Through realistic simulations in AirSim applied to scaled down multi-rotor vehicles, our results demonstrate the suitability of using graph reinforcement learning to solve the UAM-VSM problem and its superiority to basic reinforcement learning (with graph embeddings) or random choice baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Joint-Power-Control-and-Data-Size-Selection-for-Over-the-Air-Computation-Aided-Federated-Learning"><a href="#Joint-Power-Control-and-Data-Size-Selection-for-Over-the-Air-Computation-Aided-Federated-Learning" class="headerlink" title="Joint Power Control and Data Size Selection for Over-the-Air Computation Aided Federated Learning"></a>Joint Power Control and Data Size Selection for Over-the-Air Computation Aided Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09072">http://arxiv.org/abs/2308.09072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anxuming/fedaircomp">https://github.com/anxuming/fedaircomp</a></li>
<li>paper_authors: Xuming An, Rongfei Fan, Shiyuan Zuo, Han Hu, Hai Jiang, Ning Zhang</li>
<li>for: 这个研究旨在提高 Federated Learning (FL) 中总站（BS）与多个移动设备（Mobile Device，MD）之间的训练模型参数的整合。</li>
<li>methods: 我们提出了一种基于 Over-the-air computation 的 Spectrum-efficient 解决方案，将 MD 的参数映射信号同时传递到 BS。</li>
<li>results: 我们的方法可以对 MSE 进行最小化，并且能够提高 FL 的训练性能，比对 Benchmark 方法更好。<details>
<summary>Abstract</summary>
Federated learning (FL) has emerged as an appealing machine learning approach to deal with massive raw data generated at multiple mobile devices, {which needs to aggregate the training model parameter of every mobile device at one base station (BS) iteratively}. For parameter aggregating in FL, over-the-air computation is a spectrum-efficient solution, which allows all mobile devices to transmit their parameter-mapped signals concurrently to a BS. Due to heterogeneous channel fading and noise, there exists difference between the BS's received signal and its desired signal, measured as the mean-squared error (MSE). To minimize the MSE, we propose to jointly optimize the signal amplification factors at the BS and the mobile devices as well as the data size (the number of data samples involved in local training) at every mobile device. The formulated problem is challenging to solve due to its non-convexity. To find the optimal solution, with some simplification on cost function and variable replacement, which still preserves equivalence, we transform the changed problem to be a bi-level problem equivalently. For the lower-level problem, optimal solution is found by enumerating every candidate solution from the Karush-Kuhn-Tucker (KKT) condition. For the upper-level problem, the optimal solution is found by exploring its piecewise convexity. Numerical results show that our proposed method can greatly reduce the MSE and can help to improve the training performance of FL compared with benchmark methods.
</details>
<details>
<summary>摘要</summary>
“联合学习（FL）已经出现为处理大量 raw 数据生成多个移动设备的有吸引力机器学习方法。在 FL 中，需要在一个基站（BS）上运算多个移动设备的训练模型参数，并在每个移动设备上进行本地训练。为了在 BS 上进行参数联合，游击式计算是一种可以实现spectrum-efficient的解决方案，允许所有的移动设备同时将参数映射到 BS 上。由于频道折射和噪音的不同，BS 所接收到的信号和其所需的信号之间存在差异，表示为 Mean-Squared Error（MSE）。为了最小化 MSE，我们提议同时优化基站和移动设备中的信号增幅因子以及每个移动设备上的数据大小。这个问题具有非对称性，实际上是一个问题。为了找到最佳解决方案，我们通过将问题转换为一个类比问题，并通过 KKT 条件来找到最佳解。结果显示，我们的提议方法可以对 MSE 进行重大降低，并且可以帮助 FL 的训练性能提高。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/cs.LG_2023_08_18/" data-id="clpztdnkm00qces8881jcdxow" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/18/eess.IV_2023_08_18/" class="article-date">
  <time datetime="2023-08-18T09:00:00.000Z" itemprop="datePublished">2023-08-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/18/eess.IV_2023_08_18/">eess.IV - 2023-08-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI"><a href="#Uncertainty-based-quality-assurance-of-carotid-artery-wall-segmentation-in-black-blood-MRI" class="headerlink" title="Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI"></a>Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09538">http://arxiv.org/abs/2308.09538</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miagrouput/carotid-segmentation">https://github.com/miagrouput/carotid-segmentation</a></li>
<li>paper_authors: Elina Thibeau-Sutre, Dieuwertje Alblas, Sophie Buurman, Christoph Brune, Jelmer M. Wolterink</li>
<li>for: 这研究旨在使用深度学习模型对大规模数据集进行自动质量控制。</li>
<li>methods: 这种方法使用全自动算法对黑血栓MRI中的柏感血管壁进行分割。它可以在3D补充中心在柏感血管中找到嵌入的内部颗粒。</li>
<li>results: 这种方法可以准确地检测模型预测结果中的不确定性，并且可以用来自动检测质量问题。通过使用误差度量，可以衡量自动分割的质量。<details>
<summary>Abstract</summary>
The application of deep learning models to large-scale data sets requires means for automatic quality assurance. We have previously developed a fully automatic algorithm for carotid artery wall segmentation in black-blood MRI that we aim to apply to large-scale data sets. This method identifies nested artery walls in 3D patches centered on the carotid artery. In this study, we investigate to what extent the uncertainty in the model predictions for the contour location can serve as a surrogate for error detection and, consequently, automatic quality assurance. We express the quality of automatic segmentations using the Dice similarity coefficient. The uncertainty in the model's prediction is estimated using either Monte Carlo dropout or test-time data augmentation. We found that (1) including uncertainty measurements did not degrade the quality of the segmentations, (2) uncertainty metrics provide a good proxy of the quality of our contours if the center found during the first step is enclosed in the lumen of the carotid artery and (3) they could be used to detect low-quality segmentations at the participant level. This automatic quality assurance tool might enable the application of our model in large-scale data sets.
</details>
<details>
<summary>摘要</summary>
aplicación de modelos de aprendizaje profundo a conjuntos de datos a gran escala requiere medios para la calidad automática. hemos desarrollado anteriormente un algoritmo completamente automático para la segmentación de las paredes de la arteria carótida en imágenes de resonancia magnética negra que nos gustaría aplicar a conjuntos de datos a gran escala. este método identifica las paredes de la arteria en 3D centradas en la arteria carótida. en este estudio, investigamos hasta qué punto la incertidumbre en las predicciones del modelo para el lugar de la contornación puede servir como un substituto para la detección de errores y, por lo tanto, como una calidad automática. expresamos la calidad de las segmentaciones automáticas utilizando el coeficiente de similitud de Dice. la incertidumbre en las predicciones del modelo se estima utilizando either la técnica de dropout Monte Carlo o la augmentación de datos en tiempo de prueba. encontramos que (1) incluyendo mediciones de incertidumbre no degradó la calidad de las segmentaciones, (2) los métricas de incertidumbre proporcionan un buen sustituto de la calidad de nuestros contornos si el centro encontrado durante la primera etapa está en el lumen de la arteria carótida y (3) pueden ser utilizados para detectar segmentaciones de baja calidad en el nivel del participante. esta herramienta de calidad automática podría permitir la aplicación de nuestro modelo en conjuntos de datos a gran escala.
</details></li>
</ul>
<hr>
<h2 id="INR-LDDMM-Fluid-based-Medical-Image-Registration-Integrating-Implicit-Neural-Representation-and-Large-Deformation-Diffeomorphic-Metric-Mapping"><a href="#INR-LDDMM-Fluid-based-Medical-Image-Registration-Integrating-Implicit-Neural-Representation-and-Large-Deformation-Diffeomorphic-Metric-Mapping" class="headerlink" title="INR-LDDMM: Fluid-based Medical Image Registration Integrating Implicit Neural Representation and Large Deformation Diffeomorphic Metric Mapping"></a>INR-LDDMM: Fluid-based Medical Image Registration Integrating Implicit Neural Representation and Large Deformation Diffeomorphic Metric Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09473">http://arxiv.org/abs/2308.09473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chulong Zhang, Xiaokun Liang</li>
<li>for: 这篇论文是用于医疗影像注册的流基于含隐神经表现的框架。</li>
<li>methods: 这篇论文使用了含隐神经表现和大型弹性数学模型（LDDMM），并使用多层感知器（MLP）作为速度生成器，同时优化速度和影像相似性。</li>
<li>results: 这篇论文在一个包含50名病人的CT-CBCT dataset上进行验证，以 dice 系数作为评估指标，与现有方法相比，其方法实现了顶尖性能。<details>
<summary>Abstract</summary>
We propose a flow-based registration framework of medical images based on implicit neural representation. By integrating implicit neural representation and Large Deformable Diffeomorphic Metric Mapping (LDDMM), we employ a Multilayer Perceptron (MLP) as a velocity generator while optimizing velocity and image similarity. Moreover, we adopt a coarse-to-fine approach to address the challenge of deformable-based registration methods dropping into local optimal solutions, thus aiding the management of significant deformations in medical image registration. Our algorithm has been validated on a paired CT-CBCT dataset of 50 patients,taking the dice coefficient of transferred annotations as an evaluation metric. Compared to existing methods, our approach achieves the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
我们提出一种基于隐藏神经表示的医学图像注册框架。通过结合隐藏神经表示和大型可变拟合度量测量（LDDMM），我们使用多层感知器（MLP）作为速度生成器，同时优化速度和图像相似性。此外，我们采用一种层次递进方法，以解决医学图像注册方法中的重要形变挑战，从而帮助管理大的形变。我们的算法在一个包含50名病人的CT-CBCT对应数据集上进行了验证，并根据 transferred 注释的锥积率作为评价指标。相比现有方法，我们的方法实现了状态机器的性能。Note: "隐藏神经表示" (implicit neural representation) is a literal translation of "implicit neural network" in Chinese, but it is not a commonly used term in the field. A more common term would be "神经网络模型" (neural network model).
</details></li>
</ul>
<hr>
<h2 id="Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP"><a href="#Quantitative-Susceptibility-Mapping-through-Model-based-Deep-Image-Prior-MoDIP" class="headerlink" title="Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)"></a>Quantitative Susceptibility Mapping through Model-based Deep Image Prior (MoDIP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09467">http://arxiv.org/abs/2308.09467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuang Xiong, Yang Gao, Yin Liu, Amir Fazlollahi, Peter Nestor, Feng Liu, Hongfu Sun</li>
<li>for: 解决量子透射图像映射（QSM）中的束缚倒转问题，提高不同对象的扫描参数下QSM的普适性。</li>
<li>methods: 提议一种没有训练的模型基于深度学习方法，即MoDIP（模型基于深度图像优先），它包括一个小型未训练网络和数据准确优化（DFO）模块。</li>
<li>results: 实验结果表明，MoDIP在不同扫描参数下解决QSM束缚倒转问题时表现出色，比超vised深度学习和传统迭代方法提高了32%以上的准确率。同时，它也比传统DIP基本方法更快速，可以在4.5分钟内完成3D高分辨率图像重建。<details>
<summary>Abstract</summary>
The data-driven approach of supervised learning methods has limited applicability in solving dipole inversion in Quantitative Susceptibility Mapping (QSM) with varying scan parameters across different objects. To address this generalization issue in supervised QSM methods, we propose a novel training-free model-based unsupervised method called MoDIP (Model-based Deep Image Prior). MoDIP comprises a small, untrained network and a Data Fidelity Optimization (DFO) module. The network converges to an interim state, acting as an implicit prior for image regularization, while the optimization process enforces the physical model of QSM dipole inversion. Experimental results demonstrate MoDIP's excellent generalizability in solving QSM dipole inversion across different scan parameters. It exhibits robustness against pathological brain QSM, achieving over 32% accuracy improvement than supervised deep learning and traditional iterative methods. It is also 33% more computationally efficient and runs 4 times faster than conventional DIP-based approaches, enabling 3D high-resolution image reconstruction in under 4.5 minutes.
</details>
<details>
<summary>摘要</summary>
supervised learning方法的数据驱动方法在不同物体的扫描参数下解决量子感知地图（QSM）中的电pole反转问题具有限制。为了解决普通化问题在超级vised QSM方法中，我们提出了一种新的无需训练的模型基于方法called MoDIP（模型基于深度图像先验）。MoDIP包括一个小型、未训练的网络和数据准确优化（DFO）模块。网络会 converges到一个临时状态，作为图像REGULARIZATION的隐藏先验，而优化过程会实现QSM电pole反转的物理模型。实验结果表明MoDIP在不同扫描参数下解决QSM电pole反转问题 exhibits excellent generalizability。它对于肿瘤脑QSM具有32%的准确性提升，比超级vised深度学习和传统迭代方法更加稳定。它还比折衣DIP基本方法33%更快速，可以在4.5分钟内完成3D高分辨率图像重建。
</details></li>
</ul>
<hr>
<h2 id="Causal-SAR-ATR-with-Limited-Data-via-Dual-Invariance"><a href="#Causal-SAR-ATR-with-Limited-Data-via-Dual-Invariance" class="headerlink" title="Causal SAR ATR with Limited Data via Dual Invariance"></a>Causal SAR ATR with Limited Data via Dual Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09412">http://arxiv.org/abs/2308.09412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwwangsaratr/saratr_causal_dual_invariance">https://github.com/cwwangsaratr/saratr_causal_dual_invariance</a></li>
<li>paper_authors: Chenwei Wang, You Qin, Li Li, Siyi Luo, Yulin Huang, Jifang Pei, Yin Zhang, Jianyu Yang</li>
<li>for: 提高弱化概化的Synthetic Aperture Radar自动目标识别（SAR ATR）缺乏数据。</li>
<li>methods: 提出了一个 causal ATR 模型，解释了有限数据所导致的弱化的原因，并使用了后门调整来解除障碍。</li>
<li>results: 实验结果显示，提出的方法在三个benchmark datasets中表现出色。<details>
<summary>Abstract</summary>
Synthetic aperture radar automatic target recognition (SAR ATR) with limited data has recently been a hot research topic to enhance weak generalization. Despite many excellent methods being proposed, a fundamental theory is lacked to explain what problem the limited SAR data causes, leading to weak generalization of ATR. In this paper, we establish a causal ATR model demonstrating that noise $N$ that could be blocked with ample SAR data, becomes a confounder with limited data for recognition. As a result, it has a detrimental causal effect damaging the efficacy of feature $X$ extracted from SAR images, leading to weak generalization of SAR ATR with limited data. The effect of $N$ on feature can be estimated and eliminated by using backdoor adjustment to pursue the direct causality between $X$ and the predicted class $Y$. However, it is difficult for SAR images to precisely estimate and eliminated the effect of $N$ on $X$. The limited SAR data scarcely powers the majority of existing optimization losses based on empirical risk minimization (ERM), thus making it difficult to effectively eliminate $N$'s effect. To tackle with difficult estimation and elimination of $N$'s effect, we propose a dual invariance comprising the inner-class invariant proxy and the noise-invariance loss. Motivated by tackling change with invariance, the inner-class invariant proxy facilitates precise estimation of $N$'s effect on $X$ by obtaining accurate invariant features for each class with the limited data. The noise-invariance loss transitions the ERM's data quantity necessity into a need for noise environment annotations, effectively eliminating $N$'s effect on $X$ by cleverly applying the previous $N$'s estimation as the noise environment annotations. Experiments on three benchmark datasets indicate that the proposed method achieves superior performance.
</details>
<details>
<summary>摘要</summary>
射频 Synthetic aperture radar自动目标识别（SAR ATR）在有限数据情况下已经是最近的热点研究，以增强弱化。 despite many excellent methods being proposed, a fundamental theory is lacked to explain what problem the limited SAR data causes, leading to weak generalization of ATR. In this paper, we establish a causal ATR model demonstrating that noise $N$ that could be blocked with ample SAR data, becomes a confounder with limited data for recognition. As a result, it has a detrimental causal effect damaging the efficacy of feature $X$ extracted from SAR images, leading to weak generalization of SAR ATR with limited data. The effect of $N$ on feature can be estimated and eliminated by using backdoor adjustment to pursue the direct causality between $X$ and the predicted class $Y$. However, it is difficult for SAR images to precisely estimate and eliminated the effect of $N$ on $X$. The limited SAR data scarcely powers the majority of existing optimization losses based on empirical risk minimization (ERM), thus making it difficult to effectively eliminate $N$'s effect. To tackle with difficult estimation and elimination of $N$'s effect, we propose a dual invariance comprising the inner-class invariant proxy and the noise-invariance loss. Motivated by tackling change with invariance, the inner-class invariant proxy facilitates precise estimation of $N$'s effect on $X$ by obtaining accurate invariant features for each class with the limited data. The noise-invariance loss transitions the ERM's data quantity necessity into a need for noise environment annotations, effectively eliminating $N$'s effect on $X$ by cleverly applying the previous $N$'s estimation as the noise environment annotations. Experiments on three benchmark datasets indicate that the proposed method achieves superior performance.
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Causalities-in-SAR-ATR-A-Causal-Interventional-Approach-for-Limited-Data"><a href="#Unveiling-Causalities-in-SAR-ATR-A-Causal-Interventional-Approach-for-Limited-Data" class="headerlink" title="Unveiling Causalities in SAR ATR: A Causal Interventional Approach for Limited Data"></a>Unveiling Causalities in SAR ATR: A Causal Interventional Approach for Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09396">http://arxiv.org/abs/2308.09396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wang, Xin Chen, You Qin, Siyi Luo, Yulin Huang, Jifang Pei, Jianyu Yang</li>
<li>for: 提高Synthetic Aperture Radar自动目标识别（SAR ATR）方法的有效性，尤其是在有限SAR数据的情况下。</li>
<li>methods: 我们提出了一种 causal interventional ATR方法（CIATR），通过 causal inference 理解 SAR 图像的镜像条件对 ATR 问题的影响，并通过后门调整来解决这种隐藏的 causal effect。</li>
<li>results: 我们的方法可以在有限 SAR 数据的情况下寻找真正的 causality 关系 между SAR 图像和对应的类别，并且在 MSTAR 和 OpenSARship 数据集上进行了实验和比较，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Synthetic aperture radar automatic target recognition (SAR ATR) methods fall short with limited training data. In this letter, we propose a causal interventional ATR method (CIATR) to formulate the problem of limited SAR data which helps us uncover the ever-elusive causalities among the key factors in ATR, and thus pursue the desired causal effect without changing the imaging conditions. A structural causal model (SCM) is comprised using causal inference to help understand how imaging conditions acts as a confounder introducing spurious correlation when SAR data is limited. This spurious correlation among SAR images and the predicted classes can be fundamentally tackled with the conventional backdoor adjustments. An effective implement of backdoor adjustments is proposed by firstly using data augmentation with spatial-frequency domain hybrid transformation to estimate the potential effect of varying imaging conditions on SAR images. Then, a feature discrimination approach with hybrid similarity measurement is introduced to measure and mitigate the structural and vector angle impacts of varying imaging conditions on the extracted features from SAR images. Thus, our CIATR can pursue the true causality between SAR images and the corresponding classes even with limited SAR data. Experiments and comparisons conducted on the moving and stationary target acquisition and recognition (MSTAR) and OpenSARship datasets have shown the effectiveness of our method with limited SAR data.
</details>
<details>
<summary>摘要</summary>
Synthetic aperture radar自动目标识别（SAR ATR）方法受限于训练数据的量。在这封信中，我们提出了一个 causal interventional ATR 方法（CIATR），以解决SAR数据的限制，从而探索隐藏的 causality 中的因素。我们使用 causal inference 来理解对于 ATR 的影像条件是否导致杂散相关，并且使用常见的后门调整来解决这种杂散相关。我们首先使用数据扩展，使用空间频率域混合变换估计对 SAR 图像的可能影响。接着，我们引入了混合相似度量表示法，以衡量和抑制不同影像条件对抽出的特征之间的结构和向量角影响。因此，我们的 CIATR 可以在有限 SAR 数据的情况下，追求真正的 causality  zwischen SAR 图像和相应的类别。实验和比较在 MSTAR 和 OpenSARship 数据集上显示了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT"><a href="#SAMedOCT-Adapting-Segment-Anything-Model-SAM-for-Retinal-OCT" class="headerlink" title="SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT"></a>SAMedOCT: Adapting Segment Anything Model (SAM) for Retinal OCT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09331">http://arxiv.org/abs/2308.09331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Botond Fazekas, José Morano, Dmitrii Lachinov, Guilherme Aresta, Hrvoje Bogunović</li>
<li>for: 这项研究是为了评估Segment Anything Model（SAM）在retinal OCT扫描图像分割领域的可用性和可靠性。</li>
<li>methods: 这项研究使用了SAM模型，并对其进行了适应和调整，以适应retinal OCT扫描图像的特点。</li>
<li>results: 研究发现，适应SAM模型在大规模公共数据集上的Retouch挑战中表现出色，对多种retinal疾病、液体室和设备制造商进行了全面的评估。然而，在某些情况下，适应SAM模型仍迟于现有的标准方法。研究发现，适应SAM模型具有良好的适应性和稳定性，表明它可以作为retinal OCT图像分析中的一种有用工具。<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) has gained significant attention in the field of image segmentation due to its impressive capabilities and prompt-based interface. While SAM has already been extensively evaluated in various domains, its adaptation to retinal OCT scans remains unexplored. To bridge this research gap, we conduct a comprehensive evaluation of SAM and its adaptations on a large-scale public dataset of OCTs from RETOUCH challenge. Our evaluation covers diverse retinal diseases, fluid compartments, and device vendors, comparing SAM against state-of-the-art retinal fluid segmentation methods. Through our analysis, we showcase adapted SAM's efficacy as a powerful segmentation model in retinal OCT scans, although still lagging behind established methods in some circumstances. The findings highlight SAM's adaptability and robustness, showcasing its utility as a valuable tool in retinal OCT image analysis and paving the way for further advancements in this domain.
</details>
<details>
<summary>摘要</summary>
《Segment Anything Model（SAM）在图像分割领域已经受到了广泛关注，因为它的出色性能和提示式界面。尽管SAM已经在不同领域进行了广泛的评估，但它在Retinal OCT扫描图像中的适应性仍然未得到了评估。为了填补这个研究漏洞，我们进行了大规模的公共数据集上的SAM和其变体的全面评估。我们的评估覆盖了多种Retinal疾病、液体腔和设备生产厂商，与现有的Retinal液体分割方法进行比较。通过我们的分析，我们发现了适应SAM的效果是在Retinal OCT扫描图像中的强大分割模型，虽然在某些情况下仍落后于已知方法。这些发现抛光了SAM的适应性和稳定性，展示了它作为Retinal OCT图像分析中的有价值工具，并为此领域的进一步发展开辟了道路。》
</details></li>
</ul>
<hr>
<h2 id="Advancing-Intra-operative-Precision-Dynamic-Data-Driven-Non-Rigid-Registration-for-Enhanced-Brain-Tumor-Resection-in-Image-Guided-Neurosurgery"><a href="#Advancing-Intra-operative-Precision-Dynamic-Data-Driven-Non-Rigid-Registration-for-Enhanced-Brain-Tumor-Resection-in-Image-Guided-Neurosurgery" class="headerlink" title="Advancing Intra-operative Precision: Dynamic Data-Driven Non-Rigid Registration for Enhanced Brain Tumor Resection in Image-Guided Neurosurgery"></a>Advancing Intra-operative Precision: Dynamic Data-Driven Non-Rigid Registration for Enhanced Brain Tumor Resection in Image-Guided Neurosurgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10868">http://arxiv.org/abs/2308.10868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikos Chrisochoides, Andriy Fedorov, Fotis Drakopoulos, Andriy Kot, Yixun Liu, Panos Foteinos, Angelos Angelopoulos, Olivier Clatz, Nicholas Ayache, Peter M. Black, Alex J. Golby, Ron Kikinis</li>
<li>for: used to improve the accuracy of brain tumor removal during neurosurgery</li>
<li>methods: uses Dynamic Data-Driven Non-Rigid Registration (NRR) to adjust pre-operative image data for intra-operative brain shift</li>
<li>results: enables NRR results to be delivered within clinical time constraints while leveraging Distributed Computing and Machine Learning to enhance registration accuracy<details>
<summary>Abstract</summary>
During neurosurgery, medical images of the brain are used to locate tumors and critical structures, but brain tissue shifts make pre-operative images unreliable for accurate removal of tumors. Intra-operative imaging can track these deformations but is not a substitute for pre-operative data. To address this, we use Dynamic Data-Driven Non-Rigid Registration (NRR), a complex and time-consuming image processing operation that adjusts the pre-operative image data to account for intra-operative brain shift. Our review explores a specific NRR method for registering brain MRI during image-guided neurosurgery and examines various strategies for improving the accuracy and speed of the NRR method. We demonstrate that our implementation enables NRR results to be delivered within clinical time constraints while leveraging Distributed Computing and Machine Learning to enhance registration accuracy by identifying optimal parameters for the NRR method. Additionally, we highlight challenges associated with its use in the operating room.
</details>
<details>
<summary>摘要</summary>
在神经外科过程中，医疗图像被用来确定肿瘤和重要结构，但脑组织变化使得先前的图像数据无法准确地remove肿瘤。实时图像跟踪这些变化，但不能取代先前的数据。为解决这个问题，我们使用动态数据驱动非固定均衡（NRR），一种复杂和时间消耗的图像处理操作，用于调整先前的图像数据，以 comptefor intra-operative brain shift。我们的文章探讨了一种NRR方法，用于在图像导航神经外科中 регистрироваbrain MRI，并评估了不同的策略来提高NRR方法的准确性和速度。我们示示了我们的实现可以在临床时间限制下提供NRR结果，同时利用分布式计算和机器学习来提高注射准确性。此外，我们还 highlighted the challenges associated with its use in the operating room.
</details></li>
</ul>
<hr>
<h2 id="JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer"><a href="#JPEG-Quantized-Coefficient-Recovery-via-DCT-Domain-Spatial-Frequential-Transformer" class="headerlink" title="JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer"></a>JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09110">http://arxiv.org/abs/2308.09110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Ouyang, Zhenzhong Chen</li>
<li>for: 本研究旨在提出一种基于DCT频域的JPEG图像恢复方法，以提高恢复效果和抗质量因素能力。</li>
<li>methods: 本方法基于DCT频域的 dual-branch 架构， capture 频域和空间协同关系，并通过量化矩阵嵌入和chrominance-luminanceAlignment来扩展模型的应用范围。</li>
<li>results: 对于多种质量因素和图像Content，our proposed DCTransformer能够提供更高的恢复效果，比如现有状态的JPEG风损除法。<details>
<summary>Abstract</summary>
JPEG compression adopts the quantization of Discrete Cosine Transform (DCT) coefficients for effective bit-rate reduction, whilst the quantization could lead to a significant loss of important image details. Recovering compressed JPEG images in the frequency domain has attracted more and more attention recently, in addition to numerous restoration approaches developed in the pixel domain. However, the current DCT domain methods typically suffer from limited effectiveness in handling a wide range of compression quality factors, or fall short in recovering sparse quantized coefficients and the components across different colorspace. To address these challenges, we propose a DCT domain spatial-frequential Transformer, named as DCTransformer. Specifically, a dual-branch architecture is designed to capture both spatial and frequential correlations within the collocated DCT coefficients. Moreover, we incorporate the operation of quantization matrix embedding, which effectively allows our single model to handle a wide range of quality factors, and a luminance-chrominance alignment head that produces a unified feature map to align different-sized luminance and chrominance components. Our proposed DCTransformer outperforms the current state-of-the-art JPEG artifact removal techniques, as demonstrated by our extensive experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/18/eess.IV_2023_08_18/" data-id="clpztdnry019bes884bem5r22" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/17/cs.SD_2023_08_17/" class="article-date">
  <time datetime="2023-08-17T15:00:00.000Z" itemprop="datePublished">2023-08-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/17/cs.SD_2023_08_17/">cs.SD - 2023-08-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Severity-Classification-of-Parkinson’s-Disease-from-Speech-using-Single-Frequency-Filtering-based-Features"><a href="#Severity-Classification-of-Parkinson’s-Disease-from-Speech-using-Single-Frequency-Filtering-based-Features" class="headerlink" title="Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features"></a>Severity Classification of Parkinson’s Disease from Speech using Single Frequency Filtering-based Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09042">http://arxiv.org/abs/2308.09042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Manila Kodali, Paavo Alku</li>
<li>for: 这个研究旨在提出一种新的评估parkinson病（PD）严重程度的对象方法，以提高诊断和治疗的效果。</li>
<li>methods: 该研究使用了单频 filtering（SFF）方法，从而 derivation two sets of novel features：(1) SFF cepstral coefficients（SFFCC）和 (2) MFCCs from SFF（MFCC-SFF）。SFF 方法可以提供更高的spectro-temporal resolution，而且在三种说话任务（vowel、sentence、text reading）中都表现出了更好的效果。</li>
<li>results: 实验结果表明，提出的特征比普通的MFCC特征更高，在三种说话任务中都达到了5.8%、7.0%和2.4%的提升。<details>
<summary>Abstract</summary>
Developing objective methods for assessing the severity of Parkinson's disease (PD) is crucial for improving the diagnosis and treatment. This study proposes two sets of novel features derived from the single frequency filtering (SFF) method: (1) SFF cepstral coefficients (SFFCC) and (2) MFCCs from the SFF (MFCC-SFF) for the severity classification of PD. Prior studies have demonstrated that SFF offers greater spectro-temporal resolution compared to the short-time Fourier transform. The study uses the PC-GITA database, which includes speech of PD patients and healthy controls produced in three speaking tasks (vowels, sentences, text reading). Experiments using the SVM classifier revealed that the proposed features outperformed the conventional MFCCs in all three speaking tasks. The proposed SFFCC and MFCC-SFF features gave a relative improvement of 5.8% and 2.3% for the vowel task, 7.0% & 1.8% for the sentence task, and 2.4% and 1.1% for the read text task, in comparison to MFCC features.
</details>
<details>
<summary>摘要</summary>
发展客观的评估parkinson病（PD）严重度的方法是至关重要的，以提高诊断和治疗的效果。这项研究提出了两个新的特征集：（1）单频范围滤波（SFF）cepstral coefficient（SFFCC）和（2）基于SFF的Mel-frequency cepstral coefficients（MFCC-SFF），用于分类PD严重度。先前的研究表明，SFF比短时间傅立叶 transform（STFT）具有更高的spectro-temporal分辨率。这项研究使用了PC-GITA数据库，包括PD患者和健康控制人员在三种说话任务（元音、句子和文本读取）中的语音。实验使用SVM分类器显示，提出的特征比折衔MFCC更高，在三种说话任务中都有显著提高。SFFCC和MFCC-SFF特征在元音任务中增加了5.8%和2.3%，在句子任务中增加了7.0%和1.8%，在文本读取任务中增加了2.4%和1.1%，相比MFCC特征。
</details></li>
</ul>
<hr>
<h2 id="Home-monitoring-for-frailty-detection-through-sound-and-speaker-diarization-analysis"><a href="#Home-monitoring-for-frailty-detection-through-sound-and-speaker-diarization-analysis" class="headerlink" title="Home monitoring for frailty detection through sound and speaker diarization analysis"></a>Home monitoring for frailty detection through sound and speaker diarization analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08985">http://arxiv.org/abs/2308.08985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yannis Tevissen, Dan Istrate, Vincent Zalc, Jérôme Boudy, Gérard Chollet, Frédéric Petitpont, Sami Boutamine</li>
<li>for: 这篇论文是为了研究如何通过人类日常生活声音识别和语音存在检测来实现可靠和隐私保护的家庭监测系统。</li>
<li>methods: 这篇论文使用了最新的声音处理和 speaker diarization 技术来改进现有的嵌入式系统。</li>
<li>results: 研究发现，使用 DNN 基本方法可以提高性能约100%。<details>
<summary>Abstract</summary>
As the French, European and worldwide populations are aging, there is a strong interest for new systems that guarantee a reliable and privacy preserving home monitoring for frailty prevention. This work is a part of a global environmental audio analysis system which aims to help identification of Activities of Daily Life (ADL) through human and everyday life sounds recognition, speech presence and number of speakers detection. The focus is made on the number of speakers detection. In this article, we present how recent advances in sound processing and speaker diarization can improve the existing embedded systems. We study the performances of two new methods and discuss the benefits of DNN based approaches which improve performances by about 100%.
</details>
<details>
<summary>摘要</summary>
As the French, European, and worldwide populations are aging, there is a strong interest in new systems that can provide reliable and privacy-preserving home monitoring for frailty prevention. This work is part of a global environmental audio analysis system that aims to help identify Activities of Daily Life (ADL) through human and everyday life sounds recognition, speech presence, and number of speakers detection. The focus is on the number of speakers detection. In this article, we discuss how recent advances in sound processing and speaker diarization can improve the existing embedded systems. We evaluate the performance of two new methods and explore the benefits of deep neural network (DNN) based approaches, which can improve performances by about 100%.
</details></li>
</ul>
<hr>
<h2 id="Explicit-Estimation-of-Magnitude-and-Phase-Spectra-in-Parallel-for-High-Quality-Speech-Enhancement"><a href="#Explicit-Estimation-of-Magnitude-and-Phase-Spectra-in-Parallel-for-High-Quality-Speech-Enhancement" class="headerlink" title="Explicit Estimation of Magnitude and Phase Spectra in Parallel for High-Quality Speech Enhancement"></a>Explicit Estimation of Magnitude and Phase Spectra in Parallel for High-Quality Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08926">http://arxiv.org/abs/2308.08926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ye-Xin Lu, Yang Ai, Zhen-Hua Ling</li>
<li>for: 提高speech质量和可理解性</li>
<li>methods: 提出MP-SENet模型，通过并行地恢复大小和相位特征图像来提高speech质量</li>
<li>results: 在多个任务上实现高质量的speech提升，包括speech干声、抑护、宽频扩展等任务，并且比存在相位感知的方法更好地避免了相位与大小相互赔偿的问题<details>
<summary>Abstract</summary>
Phase information has a significant impact on speech perceptual quality and intelligibility. However, existing speech enhancement methods encounter limitations in explicit phase estimation due to the non-structural nature and wrapping characteristics of the phase, leading to a bottleneck in enhanced speech quality. To overcome the above issue, in this paper, we proposed MP-SENet, a novel Speech Enhancement Network which explicitly enhances Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by time-frequency Transformers along both time and frequency dimensions. The encoder aims to encode time-frequency representations derived from the input distorted magnitude and phase spectra. The decoder comprises dual-stream magnitude and phase decoders, directly enhancing magnitude and wrapped phase spectra by incorporating a magnitude estimation architecture and a phase parallel estimation architecture, respectively. To train the MP-SENet model effectively, we define multi-level loss functions, including mean square error and perceptual metric loss of magnitude spectra, anti-wrapping loss of phase spectra, as well as mean square error and consistency loss of short-time complex spectra. Experimental results demonstrate that our proposed MP-SENet excels in high-quality speech enhancement across multiple tasks, including speech denoising, dereverberation, and bandwidth extension. Compared to existing phase-aware speech enhancement methods, it successfully avoids the bidirectional compensation effect between the magnitude and phase, leading to a better harmonic restoration. Notably, for the speech denoising task, the MP-SENet yields a state-of-the-art performance with a PESQ of 3.60 on the public VoiceBank+DEMAND dataset.
</details>
<details>
<summary>摘要</summary>
干扰信息对语音辨识质量和可读性有着重要的影响。然而，现有的语音增强方法受到不可靠的阶段估计的限制，由于干扰信息的非结构性和包袋特性，导致增强语音质量的瓶颈。为了突破这个问题，在这篇论文中，我们提出了MP-SENet，一种新的语音增强网络，可以并行地增强干扰信息的大小和阶段 spectra。MP-SENet 采用了 codec 架构，其中编码器和解码器通过时间频率变换器连接。编码器的目的是将输入损坏的干扰信息转化为时间频率表示。解码器包括两个独立的大小和包袋解码器，直接将输入干扰信息的大小和包袋 spectra 提高，并采用了大小估计架构和包袋平行估计架构。为了训练 MP-SENet 模型，我们定义了多个层次损失函数，包括平均方差损失和听觉指标损失，反射损失、适应损失和短时间复杂 spectra 的损失。实验结果表明，我们提出的 MP-SENet 在多个任务上实现了高质量的语音增强，包括语音减雷、抑制抑震和频率扩展。相比现有的阶段意识的语音增强方法，MP-SENet 成功避免了阶段相互赔偿效果，从而更好地保持干扰信息的谱干整复。特别是在语音减雷任务上，MP-SENet 实现了公共 VoiceBank+DEMAND 数据集上的状态级表现，PESQ 为 3.60。
</details></li>
</ul>
<hr>
<h2 id="Long-frame-shift-Neural-Speech-Phase-Prediction-with-Spectral-Continuity-Enhancement-and-Interpolation-Error-Compensation"><a href="#Long-frame-shift-Neural-Speech-Phase-Prediction-with-Spectral-Continuity-Enhancement-and-Interpolation-Error-Compensation" class="headerlink" title="Long-frame-shift Neural Speech Phase Prediction with Spectral Continuity Enhancement and Interpolation Error Compensation"></a>Long-frame-shift Neural Speech Phase Prediction with Spectral Continuity Enhancement and Interpolation Error Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08850">http://arxiv.org/abs/2308.08850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangai520/lfs-nspp">https://github.com/yangai520/lfs-nspp</a></li>
<li>paper_authors: Yang Ai, Ye-Xin Lu, Zhen-Hua Ling</li>
<li>for: 提高信号处理领域内的speech phase预测精度，使得可以准确地从 amplitude-related features 中预测长框偏移 phase spectra。</li>
<li>methods: 提出了一种基于 neural network 的长框偏移 speech phase 预测方法 (LFS-NSPP)，包括三个阶段： interpolate、predict 和 decimate。首先，将 long-frame-shift log amplitude spectra 转换为 short-frame-shift log amplitude spectra  mediante frequency-by-frequency interpolation，然后使用 NSPP 模型预测 short-frame-shift phase spectra，最后，将 long-frame-shift phase spectra 得到于 short-frame-shift phase spectra  mediante frame-by-frame decimation。</li>
<li>results: 实验结果表明，提出的 LFS-NSPP 方法可以在预测 long-frame-shift phase spectra 方面达到更高的精度，比原 NSPP 模型和其他信号处理基于 phase 估计算法更好。<details>
<summary>Abstract</summary>
Speech phase prediction, which is a significant research focus in the field of signal processing, aims to recover speech phase spectra from amplitude-related features. However, existing speech phase prediction methods are constrained to recovering phase spectra with short frame shifts, which are considerably smaller than the theoretical upper bound required for exact waveform reconstruction of short-time Fourier transform (STFT). To tackle this issue, we present a novel long-frame-shift neural speech phase prediction (LFS-NSPP) method which enables precise prediction of long-frame-shift phase spectra from long-frame-shift log amplitude spectra. The proposed method consists of three stages: interpolation, prediction and decimation. The short-frame-shift log amplitude spectra are first constructed from long-frame-shift ones through frequency-by-frequency interpolation to enhance the spectral continuity, and then employed to predict short-frame-shift phase spectra using an NSPP model, thereby compensating for interpolation errors. Ultimately, the long-frame-shift phase spectra are obtained from short-frame-shift ones through frame-by-frame decimation. Experimental results show that the proposed LFS-NSPP method can yield superior quality in predicting long-frame-shift phase spectra than the original NSPP model and other signal-processing-based phase estimation algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用signal processing的研究焦点中的speech phase prediction（SPP）方法可以从振荡功率相关特征中恢复speech phase spectra。然而，现有的SPP方法只能recover short frame shift的phase spectra，这些框架偏移远小于理论最大值需要的短时傅立埃 transform（STFT）的波形重建。为解决这个问题，我们提出了一种新的long-frame-shift neural speech phase prediction（LFS-NSPP）方法，可以高精度地预测long-frame-shift phase spectra从long-frame-shift log amplitude spectra。LFS-NSPP方法包括三个阶段：interpolation、prediction和decimation。首先，通过频率域的 interpolate来从long-frame-shift amplitude spectra中提取short-frame-shift log amplitude spectra，以提高spectral continuity。然后，使用NSPP模型预测short-frame-shift phase spectra，以补偿interpolation error。最后，通过frame-by-frame decimation，从short-frame-shift phase spectra中提取long-frame-shift phase spectra。实验结果表明，提出的LFS-NSPP方法可以在预测long-frame-shift phase spectra方面比原始的NSPP模型和其他基于signal processing的phase estimation算法更高质量。Note: The translation is in Simplified Chinese, which is a standardized form of Chinese used in mainland China. The translation may vary depending on the specific dialect or region.
</details></li>
</ul>
<hr>
<h2 id="META-SELD-Meta-Learning-for-Fast-Adaptation-to-the-new-environment-in-Sound-Event-Localization-and-Detection"><a href="#META-SELD-Meta-Learning-for-Fast-Adaptation-to-the-new-environment-in-Sound-Event-Localization-and-Detection" class="headerlink" title="META-SELD: Meta-Learning for Fast Adaptation to the new environment in Sound Event Localization and Detection"></a>META-SELD: Meta-Learning for Fast Adaptation to the new environment in Sound Event Localization and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08847">http://arxiv.org/abs/2308.08847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbo Hu, Yin Cao, Ming Wu, Feiran Yang, Ziying Yu, Wenwu Wang, Mark D. Plumbley, Jun Yang</li>
<li>for: 本研究旨在提高学习型 зву响事件定位检测（SELD）方法在不同听控环境中的性能。</li>
<li>methods: 本研究使用meta学习方法，以便快速适应新环境。</li>
<li>results: 实验结果表明，Meta-SELD在适应新环境时的性能较高，比传统的 fine-tuning 方法更加灵活和高效。<details>
<summary>Abstract</summary>
For learning-based sound event localization and detection (SELD) methods, different acoustic environments in the training and test sets may result in large performance differences in the validation and evaluation stages. Different environments, such as different sizes of rooms, different reverberation times, and different background noise, may be reasons for a learning-based system to fail. On the other hand, acquiring annotated spatial sound event samples, which include onset and offset time stamps, class types of sound events, and direction-of-arrival (DOA) of sound sources is very expensive. In addition, deploying a SELD system in a new environment often poses challenges due to time-consuming training and fine-tuning processes. To address these issues, we propose Meta-SELD, which applies meta-learning methods to achieve fast adaptation to new environments. More specifically, based on Model Agnostic Meta-Learning (MAML), the proposed Meta-SELD aims to find good meta-initialized parameters to adapt to new environments with only a small number of samples and parameter updating iterations. We can then quickly adapt the meta-trained SELD model to unseen environments. Our experiments compare fine-tuning methods from pre-trained SELD models with our Meta-SELD on the Sony-TAU Realistic Spatial Soundscapes 2023 (STARSSS23) dataset. The evaluation results demonstrate the effectiveness of Meta-SELD when adapting to new environments.
</details>
<details>
<summary>摘要</summary>
为了解决学习基于 зву频事件检测和位置标注（SELD）方法中环境不同导致验证和评估阶段表现差异较大的问题，我们提出了Meta-SELD方法。这种方法利用元学习技术来实现环境适应快速。具体来说，基于Model Agnostic Meta-Learning（MAML），我们的Meta-SELD目标是在新环境中找到好的元初始化参数，使其适应新环境只需要少量样本和参数更新迭代。这样可以快速地适应元训练的SELD模型中未看过的环境。我们在STARSSS23 dataset上进行了对照研究，并证明了Meta-SELD在适应新环境方面的效果。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Network-Backend-for-Speaker-Recognition"><a href="#Graph-Neural-Network-Backend-for-Speaker-Recognition" class="headerlink" title="Graph Neural Network Backend for Speaker Recognition"></a>Graph Neural Network Backend for Speaker Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08767">http://arxiv.org/abs/2308.08767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang He, Ruida Li, Mengqi Niu</li>
<li>for: 提高 speaker recognition 精度</li>
<li>methods: 使用图 neural network (GNN) backend 挖掘嵌入在低维度空间中的 latent 关系</li>
<li>results: 在 NIST SRE14 i-vector 挑战 task 和 VoxCeleb1-O、VoxCeleb1-E、VoxCeleb1-H  dataset 上实现了显著的性能提升<details>
<summary>Abstract</summary>
Currently, most speaker recognition backends, such as cosine, linear discriminant analysis (LDA), or probabilistic linear discriminant analysis (PLDA), make decisions by calculating similarity or distance between enrollment and test embeddings which are already extracted from neural networks. However, for each embedding, the local structure of itself and its neighbor embeddings in the low-dimensional space is different, which may be helpful for the recognition but is often ignored. In order to take advantage of it, we propose a graph neural network (GNN) backend to mine latent relationships among embeddings for classification. We assume all the embeddings as nodes on a graph, and their edges are computed based on some similarity function, such as cosine, LDA+cosine, or LDA+PLDA. We study different graph settings and explore variants of GNN to find a better message passing and aggregation way to accomplish the recognition task. Experimental results on NIST SRE14 i-vector challenging, VoxCeleb1-O, VoxCeleb1-E, and VoxCeleb1-H datasets demonstrate that our proposed GNN backends significantly outperform current mainstream methods.
</details>
<details>
<summary>摘要</summary>
当前大多数 speaker recognition 后端，如cosine、线性混合分析（LDA）或 probabilistic 线性混合分析（PLDA），做出决策时通常计算投入和测试嵌入的相似性或距离。然而，每个嵌入都有自己本地结构，这些结构可能对识别有帮助，但通常被忽略。为了利用这些结构，我们提议一种基于图 neural network（GNN）的后端，以挖掘嵌入之间的隐藏关系，并用于分类。我们将所有嵌入视为图中的节点，并根据某种相似函数（如cosine、LDA+cosine或LDA+PLDA）计算它们之间的边。我们研究了不同的图设置和GNN变种，以找到更好的消息传递和聚合方式，以完成识别任务。实验结果表明，我们提议的 GNN 后端在 NIST SRE14 i-vector 挑战任务、VoxCeleb1-O、VoxCeleb1-E 和 VoxCeleb1-H 数据集上显著超越了当前主流方法。
</details></li>
</ul>
<hr>
<h2 id="The-DKU-MSXF-Speaker-Verification-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023"><a href="#The-DKU-MSXF-Speaker-Verification-System-for-the-VoxCeleb-Speaker-Recognition-Challenge-2023" class="headerlink" title="The DKU-MSXF Speaker Verification System for the VoxCeleb Speaker Recognition Challenge 2023"></a>The DKU-MSXF Speaker Verification System for the VoxCeleb Speaker Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08766">http://arxiv.org/abs/2308.08766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Li, Yuke Lin, Xiaoyi Qin, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>For: 本研究是DKU-MSXF系统的 Track1、Track2 和 Track3 的 VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）系统描述。* Methods: 我们利用基于 ResNet 网络结构的训练方法，并构建了跨年龄 QMF 训练集，从而实现了显著提高系统性能。* Results: 我们在 Track 2 中使用预训练模型，并通过将 VoxBlink-clean 数据集 incorporated 进行混合训练，相比 Track 1，包含 VoxBlink-clean 数据集的模型表现提高了 более чем 10%。在 Track 3 中，我们采用了一种新的 Pseudo-labeling 方法，并使用 triple thresholds 和 sub-center purification，实现了预测领域的适应。最终提交得到了 task1 的 mDCF 0.1243、Track 2 的 mDCF 0.1165 和 Track 3 的 EER 4.952%。<details>
<summary>Abstract</summary>
This paper is the system description of the DKU-MSXF System for the track1, track2 and track3 of the VoxCeleb Speaker Recognition Challenge 2023 (VoxSRC-23). For Track 1, we utilize a network structure based on ResNet for training. By constructing a cross-age QMF training set, we achieve a substantial improvement in system performance. For Track 2, we inherite the pre-trained model from Track 1 and conducte mixed training by incorporating the VoxBlink-clean dataset. In comparison to Track 1, the models incorporating VoxBlink-clean data exhibit a performance improvement by more than 10% relatively. For Track3, the semi-supervised domain adaptation task, a novel pseudo-labeling method based on triple thresholds and sub-center purification is adopted to make domain adaptation. The final submission achieves mDCF of 0.1243 in task1, mDCF of 0.1165 in Track 2 and EER of 4.952% in Track 3.
</details>
<details>
<summary>摘要</summary>
这份文章是DKU-MSXF系统的描述，用于VoxCeleb Speaker Recognition Challenge 2023（VoxSRC-23）的track1、track2和track3。在track1中，我们使用基于ResNet的网络结构进行训练，通过构建跨年龄QMF训练集，实现了显著提高系统性能。在track2中，我们继承了track1中的预训练模型，并通过将VoxBlink-clean数据集 incorporated进行混合训练，相比track1，包含VoxBlink-clean数据集的模型表现相对提高了 более10%。在track3中，我们采用了一种新的半有限预测方法，基于三个阈值和子中心纯化，进行预测领域适应。最终提交的结果为task1中的mDCF为0.1243，track2中的mDCF为0.1165，以及track3中的EER为4.952%。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Emotions-A-comprehensive-Multilingual-Study-of-Speech-Models-for-Speech-Emotion-Recognition"><a href="#Decoding-Emotions-A-comprehensive-Multilingual-Study-of-Speech-Models-for-Speech-Emotion-Recognition" class="headerlink" title="Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition"></a>Decoding Emotions: A comprehensive Multilingual Study of Speech Models for Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08713">http://arxiv.org/abs/2308.08713</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/95anantsingh/decoding-emotions">https://github.com/95anantsingh/decoding-emotions</a></li>
<li>paper_authors: Anant Singh, Akshat Gupta</li>
<li>for: 这个研究旨在评估基于变换器的speech表示模型在多种语言的语音情感识别 tasks 中的表现，以及这些模型内部的表示方式。</li>
<li>methods: 本文使用了八种speech表示模型和六种语言进行了一系列的比较和探索，包括 probing 实验来探究这些模型内部的工作方式。</li>
<li>results: 研究发现，使用单个最佳层的speech模型特征可以降低错误率32%的平均值，并在七个数据集中达到了德语和波斯语的国际前景。 probing 结果表明，speech模型的中间层 capture 最重要的情感信息。<details>
<summary>Abstract</summary>
Recent advancements in transformer-based speech representation models have greatly transformed speech processing. However, there has been limited research conducted on evaluating these models for speech emotion recognition (SER) across multiple languages and examining their internal representations. This article addresses these gaps by presenting a comprehensive benchmark for SER with eight speech representation models and six different languages. We conducted probing experiments to gain insights into inner workings of these models for SER. We find that using features from a single optimal layer of a speech model reduces the error rate by 32\% on average across seven datasets when compared to systems where features from all layers of speech models are used. We also achieve state-of-the-art results for German and Persian languages. Our probing results indicate that the middle layers of speech models capture the most important emotional information for speech emotion recognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/17/cs.SD_2023_08_17/" data-id="clpztdnni00y8es88006edwx6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/65/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/64/">64</a><a class="page-number" href="/page/65/">65</a><span class="page-number current">66</span><a class="page-number" href="/page/67/">67</a><a class="page-number" href="/page/68/">68</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/67/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
