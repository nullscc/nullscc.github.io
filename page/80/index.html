
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/80/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.AS_2023_07_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/15/eess.AS_2023_07_15/" class="article-date">
  <time datetime="2023-07-15T14:00:00.000Z" itemprop="datePublished">2023-07-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/15/eess.AS_2023_07_15/">eess.AS - 2023-07-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-Visual-Speech-Enhancement-Using-Self-supervised-Learning-to-Improve-Speech-Intelligibility-in-Cochlear-Implant-Simulations"><a href="#Audio-Visual-Speech-Enhancement-Using-Self-supervised-Learning-to-Improve-Speech-Intelligibility-in-Cochlear-Implant-Simulations" class="headerlink" title="Audio-Visual Speech Enhancement Using Self-supervised Learning to Improve Speech Intelligibility in Cochlear Implant Simulations"></a>Audio-Visual Speech Enhancement Using Self-supervised Learning to Improve Speech Intelligibility in Cochlear Implant Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07748">http://arxiv.org/abs/2307.07748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Lee Lai, Jen-Cheng Hou, Mandar Gogate, Kia Dashtipour, Amir Hussain, Yu Tsao</li>
<li>for: 帮助听力障碍者更好地理解对话，特别是在噪声环境中。</li>
<li>methods: 提出了一种基于深度学习的自动识别技术，combines 视频和声音信号，并使用Transformer-based SSL AV-HuBERT模型提取特征，然后使用 BLSTM-based SE 模型进行加工。</li>
<li>results: 实验结果显示，提出的方法可以成功地超越限制性的数据问题，并且在不同的噪声环境中都能够提高对话理解性。具体来说，PESQ 值从 1.43 提高到 1.67，STOI 值从 0.70 提高到 0.74。此外，还进行了对 CI 用户的评估，结果表明，在人工对话中遇到的动态噪声中，SSL-AVSE 表现出了明显的改善。NCM 值提高了 26.5% 到 87.2% 相比于噪声基线。<details>
<summary>Abstract</summary>
Individuals with hearing impairments face challenges in their ability to comprehend speech, particularly in noisy environments. The aim of this study is to explore the effectiveness of audio-visual speech enhancement (AVSE) in enhancing the intelligibility of vocoded speech in cochlear implant (CI) simulations. Notably, the study focuses on a challenged scenario where there is limited availability of training data for the AVSE task. To address this problem, we propose a novel deep neural network framework termed Self-Supervised Learning-based AVSE (SSL-AVSE). The proposed SSL-AVSE combines visual cues, such as lip and mouth movements, from the target speakers with corresponding audio signals. The contextually combined audio and visual data are then fed into a Transformer-based SSL AV-HuBERT model to extract features, which are further processed using a BLSTM-based SE model. The results demonstrate several key findings. Firstly, SSL-AVSE successfully overcomes the issue of limited data by leveraging the AV-HuBERT model. Secondly, by fine-tuning the AV-HuBERT model parameters for the target SE task, significant performance improvements are achieved. Specifically, there is a notable enhancement in PESQ (Perceptual Evaluation of Speech Quality) from 1.43 to 1.67 and in STOI (Short-Time Objective Intelligibility) from 0.70 to 0.74. Furthermore, the performance of the SSL-AVSE was evaluated using CI vocoded speech to assess the intelligibility for CI users. Comparative experimental outcomes reveal that in the presence of dynamic noises encountered during human conversations, SSL-AVSE exhibits a substantial improvement. The NCM (Normal Correlation Matrix) values indicate an increase of 26.5% to 87.2% compared to the noisy baseline.
</details>
<details>
<summary>摘要</summary>
听力障碍者面临听说能力下降的挑战，特别是在噪声环境中。本研究的目的是探讨audio-visualspeech增强（AVSE）在cochlear implant（CI）模拟中的有效性。值得注意的是，这个研究强调了一个困难的enario，即有限的培训数据 дляAVSE任务。为解决这个问题，我们提出了一种新的深度神经网络框架，称为Self-Supervised Learning-based AVSE（SSL-AVSE）。SSL-AVSE通过将目标speaker的lip和mouth运动视频信号与相应的音频信号结合，并将这些上下文混合的数据feed into一个Transformer-based SSL AV-HuBERT模型来提取特征。然后，通过一个BLSTM-based SE模型进行进一步处理。实验结果显示了以下几点：首先，SSL-AVSE成功地超越了有限的数据问题，利用了AV-HuBERT模型。其次，通过对AV-HuBERT模型参数的微调，在目标SE任务中实现了显著性能提升。具体来说，PESQ（Perceptual Evaluation of Speech Quality）从1.43提高到1.67，STOI（Short-Time Objective Intelligibility）从0.70提高到0.74。此外，SSL-AVSE的性能还被评估了使用CI vocoded speech来评估智能度对CI用户的智能度。对比实验结果表明，在人类对话中遇到的动态噪声中，SSL-AVSE表现出了显著提升。NCM（Normal Correlation Matrix）值表明，在噪声基eline比较之下，SSL-AVSE的提升为26.5%-87.2%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/15/eess.AS_2023_07_15/" data-id="clot2mhix010gx7882vamekqh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/15/cs.CV_2023_07_15/" class="article-date">
  <time datetime="2023-07-15T13:00:00.000Z" itemprop="datePublished">2023-07-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/15/cs.CV_2023_07_15/">cs.CV - 2023-07-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HQG-Net-Unpaired-Medical-Image-Enhancement-with-High-Quality-Guidance"><a href="#HQG-Net-Unpaired-Medical-Image-Enhancement-with-High-Quality-Guidance" class="headerlink" title="HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance"></a>HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07829">http://arxiv.org/abs/2307.07829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunming He, Kai Li, Guoxia Xu, Jiangpeng Yan, Longxiang Tang, Yulun Zhang, Xiu Li, Yaowei Wang</li>
<li>for: 提高医疗影像质量（UMIE），使低质量医疗影像变成高质量医疗影像，不需要使用对应的高质量影像进行训练。</li>
<li>methods: 提出了一种新的UMIE方法，利用变量ormalization模块直接在低质量影像增强过程中编码高质量灵感，并通过对抗学习和内容响应损失来导航增强过程。</li>
<li>results: 对三个医疗影像 dataset进行了实验，并证明了该方法可以在增强质量和下游任务性能之间取得平衡，并且在许多情况下超越了现有的方法。<details>
<summary>Abstract</summary>
Unpaired Medical Image Enhancement (UMIE) aims to transform a low-quality (LQ) medical image into a high-quality (HQ) one without relying on paired images for training. While most existing approaches are based on Pix2Pix/CycleGAN and are effective to some extent, they fail to explicitly use HQ information to guide the enhancement process, which can lead to undesired artifacts and structural distortions. In this paper, we propose a novel UMIE approach that avoids the above limitation of existing methods by directly encoding HQ cues into the LQ enhancement process in a variational fashion and thus model the UMIE task under the joint distribution between the LQ and HQ domains. Specifically, we extract features from an HQ image and explicitly insert the features, which are expected to encode HQ cues, into the enhancement network to guide the LQ enhancement with the variational normalization module. We train the enhancement network adversarially with a discriminator to ensure the generated HQ image falls into the HQ domain. We further propose a content-aware loss to guide the enhancement process with wavelet-based pixel-level and multi-encoder-based feature-level constraints. Additionally, as a key motivation for performing image enhancement is to make the enhanced images serve better for downstream tasks, we propose a bi-level learning scheme to optimize the UMIE task and downstream tasks cooperatively, helping generate HQ images both visually appealing and favorable for downstream tasks. Experiments on three medical datasets, including two newly collected datasets, verify that the proposed method outperforms existing techniques in terms of both enhancement quality and downstream task performance. We will make the code and the newly collected datasets publicly available for community study.
</details>
<details>
<summary>摘要</summary>
<<SYS>>医学图像提高（UMIE）的目标是将低质量（LQ）医学图像转换成高质量（HQ）图像，不需要使用对应的图像进行训练。现有的方法多数基于Pix2Pix/CycleGAN，尽管有一定的效果，但是它们不会直接使用HQ信息来导引提高过程，这可能会导致不必要的artefacts和结构性错误。在这篇论文中，我们提出了一种新的UMIE方法，避免了现有方法的这一限制，通过直接在LQ提高过程中编码HQ信息，并在变量化正则模块中进行变量化正则化。具体来说，我们从HQ图像中提取特征，并直接插入提高网络中，以便使LQ图像提高以HQ图像的特征为导引。我们通过对提高网络进行对抗训练，使得生成的HQ图像在HQ领域中。我们还提出了一种内容相关损失，以便通过波лет特-基于的像素级和多编码器-基于的特征级约束，指导提高过程。此外，作为提高图像的主要动机是为了使图像更好地服务于下游任务，我们提出了一种叠加学习方案，以便在UMIE任务和下游任务之间协同优化，使得生成的HQ图像同时具备视觉吸引力和下游任务的有利性。实验结果表明，提出的方法在三个医学 dataset 上表现出色，超过了现有技术。我们将代码和新收集的 dataset 公开发布，为社区的研究提供便利。
</details></li>
</ul>
<hr>
<h2 id="TinyTracker-Ultra-Fast-and-Ultra-Low-Power-Edge-Vision-In-Sensor-for-Gaze-Estimation"><a href="#TinyTracker-Ultra-Fast-and-Ultra-Low-Power-Edge-Vision-In-Sensor-for-Gaze-Estimation" class="headerlink" title="TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze Estimation"></a>TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07813">http://arxiv.org/abs/2307.07813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Bonazzi, Thomas Ruegg, Sizhen Bian, Yawei Li, Michele Magno</li>
<li>for: 这个论文的目的是提出一种高效低功耗的边缘视觉应用，以解决边缘设备的计算负担问题。</li>
<li>methods: 该论文使用了一种新的“AI在传感器”视觉平台，SONY的IMX500，实现了ultra-fast和ultra-low-power的边缘视觉应用。</li>
<li>results: 研究表明，IMX500比Google Coral Dev Micro和SONY Spresense更快（19ms vs 34.4ms）和更高效（4.9mJ VS 34.2mJ），并且可以实现2D gaze estimation的高精度预测（最大差错0.16cm）。<details>
<summary>Abstract</summary>
Intelligent edge vision tasks encounter the critical challenge of ensuring power and latency efficiency due to the typically heavy computational load they impose on edge platforms.This work leverages one of the first "AI in sensor" vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-power end-to-end edge vision applications. We evaluate the IMX500 and compare it to other edge platforms, such as the Google Coral Dev Micro and Sony Spresense, by exploring gaze estimation as a case study. We propose TinyTracker, a highly efficient, fully quantized model for 2D gaze estimation designed to maximize the performance of the edge vision systems considered in this study. TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1] without significant loss in gaze estimation accuracy (maximum of 0.16 cm when fully quantized). TinyTracker's deployment on the Sony IMX500 vision sensor results in end-to-end latency of around 19ms. The camera takes around 17.9ms to read, process and transmit the pixels to the accelerator. The inference time of the network is 0.86ms with an additional 0.24 ms for retrieving the results from the sensor. The overall energy consumption of the end-to-end system is 4.9 mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is 1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ VS 34.2mJ)
</details>
<details>
<summary>摘要</summary>
智能边缘视觉任务面临 crítical 挑战，因为它们通常占用边缘平台的重要计算负担。这项工作利用了首个“AI在传感器”视觉平台IMX500（由索尼制造），实现ultra-fast和ultra-low-power的边缘视觉应用。我们对IMX500进行了比较，并使用了其他边缘平台，如Google Coral Dev Micro和索尼Spresense，通过察看计算作为研究案例。我们提出了TinyTracker，一种高效、全数字化模型，用于实现2D gaze estimation。TinyTracker在IMX500视觉传感器上部署后，实现了约19ms的终端延迟时间。摄像头需要17.9ms来读取、处理和传输像素到加速器。网络执行时间为0.86ms，加上从传感器中获取结果的时间（0.24ms）。总的来说，边缘系统的能 consumption 为4.9 mJ，其中计算部分占用0.06 mJ。对照研究显示，IMX500比CoralMicro（19ms VS 34.4ms）更快，并且在能 efficiency 方面更高（4.9mJ VS 34.2mJ）。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Memory-Comparator-Transformer-for-Few-Shot-Video-Segmentation"><a href="#Multiscale-Memory-Comparator-Transformer-for-Few-Shot-Video-Segmentation" class="headerlink" title="Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation"></a>Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07812">http://arxiv.org/abs/2307.07812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msiam/mmc-multiscalememory">https://github.com/msiam/mmc-multiscalememory</a></li>
<li>paper_authors: Mennatullah Siam, Rezaul Karim, He Zhao, Richard Wildes</li>
<li>for: 这篇论文主要针对几个支持图像中的特定新类，使用少量标注图像进行分类。</li>
<li>methods: 该方法使用一种名为多尺度记忆比较器（MMC），它将信息在不同尺度级别内共享，以增强分类精度。</li>
<li>results: 该方法在几个实验中均超过了基eline，并达到了状态之arte。代码可以在<a target="_blank" rel="noopener" href="https://github.com/MSiam/MMC-MultiscaleMemory%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/MSiam/MMC-MultiscaleMemory中下载。</a><details>
<summary>Abstract</summary>
Few-shot video segmentation is the task of delineating a specific novel class in a query video using few labelled support images. Typical approaches compare support and query features while limiting comparisons to a single feature layer and thereby ignore potentially valuable information. We present a meta-learned Multiscale Memory Comparator (MMC) for few-shot video segmentation that combines information across scales within a transformer decoder. Typical multiscale transformer decoders for segmentation tasks learn a compressed representation, their queries, through information exchange across scales. Unlike previous work, we instead preserve the detailed feature maps during across scale information exchange via a multiscale memory transformer decoding to reduce confusion between the background and novel class. Integral to the approach, we investigate multiple forms of information exchange across scales in different tasks and provide insights with empirical evidence on which to use in each task. The overall comparisons among query and support features benefit from both rich semantics and precise localization. We demonstrate our approach primarily on few-shot video object segmentation and an adapted version on the fully supervised counterpart. In all cases, our approach outperforms the baseline and yields state-of-the-art performance. Our code is publicly available at https://github.com/MSiam/MMC-MultiscaleMemory.
</details>
<details>
<summary>摘要</summary>
几个批量视频分割是指将特定的新类型在查询视频中分割使用几个标注图像。通常的方法是比较支持和查询特征，并限制比较到单个特征层，因此可能会忽略有价值信息。我们提出了基于模型学习的多尺度内存比较器（MMC），用于几个批量视频分割，它将在变换器解码器中结合多尺度信息。通常的多尺度变换器解码器用于分割任务会学习压缩表示，其查询通过不同尺度之间的信息交换来学习压缩表示。与之前的工作不同，我们在交换过程中保留细节特征图，以避免背景和新类型之间的混淆。我们的方法包括多种不同任务中的信息交换方式，并提供了实验证据，以帮助选择合适的信息交换方式。通过比较查询和支持特征，我们的方法可以获得丰富的 semantics 和精确的地方化。我们在几个几个批量视频对象分割和修改后的完全监督版本中展示了我们的方法，在所有情况下，我们的方法都超越基准，并实现了状态的最佳性。我们的代码可以在 GitHub 上获取：https://github.com/MSiam/MMC-MultiscaleMemory。
</details></li>
</ul>
<hr>
<h2 id="MUVF-YOLOX-A-Multi-modal-Ultrasound-Video-Fusion-Network-for-Renal-Tumor-Diagnosis"><a href="#MUVF-YOLOX-A-Multi-modal-Ultrasound-Video-Fusion-Network-for-Renal-Tumor-Diagnosis" class="headerlink" title="MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis"></a>MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07807">http://arxiv.org/abs/2307.07807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeunyuli/muaf">https://github.com/jeunyuli/muaf</a></li>
<li>paper_authors: Junyu Li, Han Huang, Dong Ni, Wufeng Xue, Dongmei Zhu, Jun Cheng</li>
<li>for: 这个研究的目的是为了早期识别肾脏癌，以提高病人存活率。</li>
<li>methods: 这个研究使用了融合B模式和CEUS模式射频影像的多modal聚合网络，以实现多modal特征融合和影像分类。</li>
<li>results: 实验结果显示，提案的框架比单模式模型和竞争方法更高的准确性。此外，我们的OTA模组在多帧射频影像中实现了更高的分类精度。<details>
<summary>Abstract</summary>
Early diagnosis of renal cancer can greatly improve the survival rate of patients. Contrast-enhanced ultrasound (CEUS) is a cost-effective and non-invasive imaging technique and has become more and more frequently used for renal tumor diagnosis. However, the classification of benign and malignant renal tumors can still be very challenging due to the highly heterogeneous appearance of cancer and imaging artifacts. Our aim is to detect and classify renal tumors by integrating B-mode and CEUS-mode ultrasound videos. To this end, we propose a novel multi-modal ultrasound video fusion network that can effectively perform multi-modal feature fusion and video classification for renal tumor diagnosis. The attention-based multi-modal fusion module uses cross-attention and self-attention to extract modality-invariant features and modality-specific features in parallel. In addition, we design an object-level temporal aggregation (OTA) module that can automatically filter low-quality features and efficiently integrate temporal information from multiple frames to improve the accuracy of tumor diagnosis. Experimental results on a multicenter dataset show that the proposed framework outperforms the single-modal models and the competing methods. Furthermore, our OTA module achieves higher classification accuracy than the frame-level predictions. Our code is available at \url{https://github.com/JeunyuLi/MUAF}.
</details>
<details>
<summary>摘要</summary>
早期诊断肾癌可以大幅提高患者存活率。扩增噪声超声（CEUS）是一种Cost-effective和非侵入性的成像技术，在肾肿瘤诊断中越来越常用。然而，肾肿瘤的分类仍然是非常困难的，主要是因为肿瘤的外观非常异常，以及成像 artifacts。我们的目标是通过融合B模式和CEUS模式超声视频来检测和分类肾肿瘤。为此，我们提出了一种新的多模态超声视频融合网络，可以有效地执行多模态特征融合和视频分类。我们的注意力基于多模态融合模块使用交叉注意力和自注意力来提取模式不变特征和模式特定特征。此外，我们设计了一个物体级别时间聚合（OTA）模块，可以自动筛选低质量特征并高效地集成多帧中的时间信息，以提高肾肿瘤诊断的准确性。实验结果表明，我们提出的框架在多中心数据集上表现出优于单模态模型和竞争方法。此外，我们的 OTA 模块在帧级预测中实现了更高的分类精度。我们的代码可以在 \url{https://github.com/JeunyuLi/MUAF} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Joint-Adversarial-and-Collaborative-Learning-for-Self-Supervised-Action-Recognition"><a href="#Joint-Adversarial-and-Collaborative-Learning-for-Self-Supervised-Action-Recognition" class="headerlink" title="Joint Adversarial and Collaborative Learning for Self-Supervised Action Recognition"></a>Joint Adversarial and Collaborative Learning for Self-Supervised Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07791">http://arxiv.org/abs/2307.07791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/levigty/acl">https://github.com/levigty/acl</a></li>
<li>paper_authors: Tianyu Guo, Mengyuan Liu, Hong Liu, Wenhao Li, Jingwen Guo, Tao Wang, Yidi Li</li>
<li>for: 本研究的目的是使用对比学习方法，包括MoCo和SimCLR，来解决自然语言处理中的自我supervised动作识别任务。</li>
<li>methods: 本研究使用多个数据流（joint、运动和骨）进行ensemble学习，同时如何在单个流中构建一个特异性的特征空间并有效地将多个流的信息汇聚到一起仍然是一个开放的问题。</li>
<li>results: 我们首先应用一种新的对比学习方法called BYOL来学习从骨架数据，并将其формализова为一个简单 yet effective的基线方法 для自我supervised骨架动作识别。此外，我们还提出了一种联合对抗学习（ACL）框架，其结合了cross-model对抗学习（CMAL）和cross-stream合作学习（CSCL）。<details>
<summary>Abstract</summary>
Considering the instance-level discriminative ability, contrastive learning methods, including MoCo and SimCLR, have been adapted from the original image representation learning task to solve the self-supervised skeleton-based action recognition task. These methods usually use multiple data streams (i.e., joint, motion, and bone) for ensemble learning, meanwhile, how to construct a discriminative feature space within a single stream and effectively aggregate the information from multiple streams remains an open problem. To this end, we first apply a new contrastive learning method called BYOL to learn from skeleton data and formulate SkeletonBYOL as a simple yet effective baseline for self-supervised skeleton-based action recognition. Inspired by SkeletonBYOL, we further present a joint Adversarial and Collaborative Learning (ACL) framework, which combines Cross-Model Adversarial Learning (CMAL) and Cross-Stream Collaborative Learning (CSCL). Specifically, CMAL learns single-stream representation by cross-model adversarial loss to obtain more discriminative features. To aggregate and interact with multi-stream information, CSCL is designed by generating similarity pseudo label of ensemble learning as supervision and guiding feature generation for individual streams. Exhaustive experiments on three datasets verify the complementary properties between CMAL and CSCL and also verify that our method can perform favorably against state-of-the-art methods using various evaluation protocols. Our code and models are publicly available at \url{https://github.com/Levigty/ACL}.
</details>
<details>
<summary>摘要</summary>
基于实例水平的异同能力，包括MoCo和SimCLR等对比学习方法，已经从原始图像表示学习任务中适应到解决自动识别动作任务。这些方法通常使用多个数据流（即联合、运动和骨）进行ensemble学习，而在单个流中构建准确的特征空间并有效地聚合多个流的信息仍然是一个开放的问题。为此，我们首先应用一种新的对比学习方法called BYOL来学习从骨架数据，并将SkeletonBYOL作为一个简单又有效的基线 для自主学习骨架动作识别。受SkeletonBYOL的启发，我们进一步提出了一种联合对抗学习（ACL）框架，该框架将对抗学习（CMAL）和流处理学习（CSCL）相结合。具体来说，CMAL通过跨模型对抗损失来获得更准确的特征，而CSCL通过生成流处理学习中的similarity pseudo标签来引导特征生成和流处理学习。我们对三个数据集进行了详细的实验，并证明了ACL的共轭性和对比学习方法的优势。我们的代码和模型在\url{https://github.com/Levigty/ACL}上公开。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Nonlinear-Latent-Transformation-for-Conditional-Face-Editing"><a href="#Adaptive-Nonlinear-Latent-Transformation-for-Conditional-Face-Editing" class="headerlink" title="Adaptive Nonlinear Latent Transformation for Conditional Face Editing"></a>Adaptive Nonlinear Latent Transformation for Conditional Face Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07790">http://arxiv.org/abs/2307.07790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hzzone/adatrans">https://github.com/hzzone/adatrans</a></li>
<li>paper_authors: Zhizhong Huang, Siteng Ma, Junping Zhang, Hongming Shan</li>
<li>for: 本文提出了一种新的适应非线性潜在转换方法（AdaTrans），用于不混合的条件脸部编辑。</li>
<li>methods: 本方法将编辑过程分解为多个细节步骤，每个步骤的方向和大小受到面部特征和潜在码的控制。这种非线性转换路径可以操作面部到目标特征Attributes，保持其他特征不变。此外，本文还提出了一种独立学习策略，基于mutual information框架，以消除特征之间的混合。</li>
<li>results: 对于多种面部特征，AdaTrans能够实现可控的脸部编辑，具有分离、灵活性和高质量的优点。与现有方法相比，AdaTrans在面部特征之间的混合、年龄差距和少量标注数据下表现出较好的性能。代码可以从<a target="_blank" rel="noopener" href="https://github.com/Hzzone/AdaTrans%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Hzzone/AdaTrans获取。</a><details>
<summary>Abstract</summary>
Recent works for face editing usually manipulate the latent space of StyleGAN via the linear semantic directions. However, they usually suffer from the entanglement of facial attributes, need to tune the optimal editing strength, and are limited to binary attributes with strong supervision signals. This paper proposes a novel adaptive nonlinear latent transformation for disentangled and conditional face editing, termed AdaTrans. Specifically, our AdaTrans divides the manipulation process into several finer steps; i.e., the direction and size at each step are conditioned on both the facial attributes and the latent codes. In this way, AdaTrans describes an adaptive nonlinear transformation trajectory to manipulate the faces into target attributes while keeping other attributes unchanged. Then, AdaTrans leverages a predefined density model to constrain the learned trajectory in the distribution of latent codes by maximizing the likelihood of transformed latent code. Moreover, we also propose a disentangled learning strategy under a mutual information framework to eliminate the entanglement among attributes, which can further relax the need for labeled data. Consequently, AdaTrans enables a controllable face editing with the advantages of disentanglement, flexibility with non-binary attributes, and high fidelity. Extensive experimental results on various facial attributes demonstrate the qualitative and quantitative effectiveness of the proposed AdaTrans over existing state-of-the-art methods, especially in the most challenging scenarios with a large age gap and few labeled examples. The source code is available at https://github.com/Hzzone/AdaTrans.
</details>
<details>
<summary>摘要</summary>
最近的面部编辑方法通常通过 StyleGAN 的归一化空间进行 manipulate。然而，这些方法通常受到面部特征的杂化的限制，需要调整最佳编辑强度，并且只能处理二分类特征。这篇论文提出了一种新的适应非线性层次变换方法，称为 AdaTrans。具体来说，我们的 AdaTrans 将掌控过程分解成多个更细的步骤，即在每个步骤中的方向和大小受到面部特征和归一化码的Conditioning。这样，AdaTrans 可以描述一个适应非线性变换轨迹，以控制面部变换为目标特征，保持其他特征不变。然后，AdaTrans 利用一个预定义的概率模型来约束学习的轨迹在归一化码的分布中，通过最大化变换后的归一化码的概率来提升。此外，我们还提出了一种不依赖标注数据的分解学习策略，基于mutual information框架，以消除特征之间的杂化，从而更好地适应具有不同特征的面部编辑。因此，AdaTrans 可以提供可控的面部编辑，具有分解、非二分类特征和高精度的优势。我们的实验结果表明，AdaTrans 在不同的面部特征下表现出较好的效果，特别是在面部年龄差较大和标注数据少的情况下。代码可以在 <https://github.com/Hzzone/AdaTrans> 上获取。
</details></li>
</ul>
<hr>
<h2 id="SoccerKDNet-A-Knowledge-Distillation-Framework-for-Action-Recognition-in-Soccer-Videos"><a href="#SoccerKDNet-A-Knowledge-Distillation-Framework-for-Action-Recognition-in-Soccer-Videos" class="headerlink" title="SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos"></a>SoccerKDNet: A Knowledge Distillation Framework for Action Recognition in Soccer Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07768">http://arxiv.org/abs/2307.07768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarosij Bose, Saikat Sarkar, Amlan Chakrabarti</li>
<li>For: The paper is written for classifying player actions in soccer videos, which is a challenging problem in sports analytics.* Methods: The paper proposes a novel end-to-end knowledge distillation based transfer learning network pre-trained on the Kinetics400 dataset, and introduces a unique loss parameterization.* Results: The paper obtains 67.20% validation accuracy on a new dataset named SoccerDB1, which consists of 448 videos and 4 diverse classes of players playing soccer. The model also generalizes well to new datasets.Here is the same information in Simplified Chinese text:* For: 这篇论文是为了解决足球视频中玩家行为的分类问题，这是体育分析中的一个挑战。* Methods: 论文提出了一种新的端到端知识储备基于转移学习网络，并引入了一个唯一的损失参数化。* Results: 论文在新的足球DB1数据集上获得了67.20%的验证精度，该数据集包括448个视频和4种多样化的玩家类别。模型也能够轻松地适应新的数据集。<details>
<summary>Abstract</summary>
Classifying player actions from soccer videos is a challenging problem, which has become increasingly important in sports analytics over the years. Most state-of-the-art methods employ highly complex offline networks, which makes it difficult to deploy such models in resource constrained scenarios. Here, in this paper we propose a novel end-to-end knowledge distillation based transfer learning network pre-trained on the Kinetics400 dataset and then perform extensive analysis on the learned framework by introducing a unique loss parameterization. We also introduce a new dataset named SoccerDB1 containing 448 videos and consisting of 4 diverse classes each of players playing soccer. Furthermore, we introduce an unique loss parameter that help us linearly weigh the extent to which the predictions of each network are utilized. Finally, we also perform a thorough performance study using various changed hyperparameters. We also benchmark the first classification results on the new SoccerDB1 dataset obtaining 67.20% validation accuracy. Apart from outperforming prior arts significantly, our model also generalizes to new datasets easily. The dataset has been made publicly available at: https://bit.ly/soccerdb1
</details>
<details>
<summary>摘要</summary>
“足球视频中玩家行为分类是一个复杂的问题，在体育分析领域已经越来越重要。大多数当前最佳方法使用高度复杂的离线网络，这使得在资源受限的情况下难以部署这些模型。在这篇论文中，我们提出了一种新的终端到终端知识储备基于转移学习网络，并进行了广泛的分析。我们还介绍了一个新的损失参数化方法，以及一个名为足球DB1的新数据集，该数据集包含448个视频和4种多样化的玩家在足球比赛中的行为。此外，我们还引入了一个唯一的损失参数，以便将每个网络的预测值 linearly 权重。最后，我们还进行了详细的性能研究，并对不同的变量参数进行了测试。我们的模型在新的足球DB1数据集上获得了67.20%的验证精度，并且能够轻松地在新的数据集上进行扩展。数据集可以在以下链接获取：https://bit.ly/soccerdb1。”
</details></li>
</ul>
<hr>
<h2 id="Tightly-Coupled-LiDAR-Visual-SLAM-Based-on-Geometric-Features-for-Mobile-Agents"><a href="#Tightly-Coupled-LiDAR-Visual-SLAM-Based-on-Geometric-Features-for-Mobile-Agents" class="headerlink" title="Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents"></a>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07763">http://arxiv.org/abs/2307.07763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Cao, Ruiping Liu, Ze Wang, Kunyu Peng, Jiaming Zhang, Junwei Zheng, Zhifeng Teng, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 提供自动化导航和任务执行在复杂和未知环境中，以便移动机器人可以更好地操作。</li>
<li>methods: 利用LiDAR-视觉Simultaneous Localization and Mapping（SLAM）技术，包括两个子系统（LiDAR和单目视觉SLAM）和一个融合框架。融合框架将深度和 semantic的多Modal 几何特征相关，以便补充视觉线尺标和添加方向优化。</li>
<li>results: 在M2DGR公共数据集上进行评估，与当前状态的多模式方法相比，我们的系统实现了更高精度和更加稳定的姿态估计。<details>
<summary>Abstract</summary>
The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.
</details>
<details>
<summary>摘要</summary>
Mobile robot 依靠 SLAM (同时地理定位和地图生成) 实现自主导航和任务执行在复杂和未知环境中。但是开发专门的移动 robot 算法具有困难的动态和挑战性情况，如低光照条件和运动模糊。为解决这个问题，我们提议一种紧密相关的 LiDAR-视觉 SLAM，其包括两个子系统（LiDAR 和单目视觉 SLAM）以及一个融合框架。融合框架将depth 和视觉特征相关，以补偿视线标记的缺失，并在Bundle Adjustment 中添加方向优化。这使得视觉遥感更加精准。另一方面，视觉子系统检测到的整条线段超越了 LiDAR 子系统的局部计算能力，可以调整方向和过滤异常值，从而实现更高精度的遥感系统。最后，我们采用一个模块来检测子系统的运作，提供 LiDAR 子系统的补偿轨迹，而视觉子系统跟踪失败时。根据公共数据集 M2DGR，从各种室内和室外场景中收集的数据，我们的系统实现了与当前状态艺术多模式方法相比更高精度和更加稳定的姿态估计。
</details></li>
</ul>
<hr>
<h2 id="Open-Scene-Understanding-Grounded-Situation-Recognition-Meets-Segment-Anything-for-Helping-People-with-Visual-Impairments"><a href="#Open-Scene-Understanding-Grounded-Situation-Recognition-Meets-Segment-Anything-for-Helping-People-with-Visual-Impairments" class="headerlink" title="Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments"></a>Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07757">http://arxiv.org/abs/2307.07757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruipingl/opensu">https://github.com/ruipingl/opensu</a></li>
<li>paper_authors: Ruiping Liu, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ke Cao, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 帮助人们 WITH 视觉障碍（PVI）更加独立地移动，通过增强场景理解和提供有用信息。</li>
<li>methods: 基于 Grounded Situation Recognition（GSR）技术，采用高效的 Segment Anything Model（SAM）和固定纯transformer背景结构，并将所有激活函数换为GELU，以提高GSR性能。</li>
<li>results: 在 SWiG 数据集上达到了领先的表现，并通过在辅助技术数据集和应用示例中的场景测试和应用示例，证明了 OpenSU 系统的可行性和有用性。<details>
<summary>Abstract</summary>
Grounded Situation Recognition (GSR) is capable of recognizing and interpreting visual scenes in a contextually intuitive way, yielding salient activities (verbs) and the involved entities (roles) depicted in images. In this work, we focus on the application of GSR in assisting people with visual impairments (PVI). However, precise localization information of detected objects is often required to navigate their surroundings confidently and make informed decisions. For the first time, we propose an Open Scene Understanding (OpenSU) system that aims to generate pixel-wise dense segmentation masks of involved entities instead of bounding boxes. Specifically, we build our OpenSU system on top of GSR by additionally adopting an efficient Segment Anything Model (SAM). Furthermore, to enhance the feature extraction and interaction between the encoder-decoder structure, we construct our OpenSU system using a solid pure transformer backbone to improve the performance of GSR. In order to accelerate the convergence, we replace all the activation functions within the GSR decoders with GELU, thereby reducing the training duration. In quantitative analysis, our model achieves state-of-the-art performance on the SWiG dataset. Moreover, through field testing on dedicated assistive technology datasets and application demonstrations, the proposed OpenSU system can be used to enhance scene understanding and facilitate the independent mobility of people with visual impairments. Our code will be available at https://github.com/RuipingL/OpenSU.
</details>
<details>
<summary>摘要</summary>
“固定Scene理解（GSR）能够识别和解释视觉场景，将其转换为有意义的活动（动词）和参与的实体（角色）。在这个工作中，我们专注于将GSR应用于视障人士（PVI）。然而，精确的地方化信息是需要帮助PVI人士自信地穿梭环境和做出了知情的决策。为了解决这个问题，我们提出了一个开放场景理解（OpenSU）系统，旨在生成像素粒度密集的参与实体分割图案。具体来说，我们将OpenSU系统建立在GSR之上，还 adopting一个高效的填充模型（SAM）。此外，为了增强协变和协变之间的交互，我们使用一个坚固的纯transformer背景架构来提高GSR的表现。为了加速训练，我们将所有的启动函数在GSR解oder中替换为GELU，从而减少训练时间。在量值分析中，我们的模型在SWiG dataset上 achieve state-of-the-art表现。此外，通过特定助理技术dataset和应用示例的野试，我们的OpenSU系统可以增强场景理解和推动视障人士的独立 mobilility。我们的代码将在https://github.com/RuipingL/OpenSU 上available。”
</details></li>
</ul>
<hr>
<h2 id="Fast-Adaptation-with-Bradley-Terry-Preference-Models-in-Text-To-Image-Classification-and-Generation"><a href="#Fast-Adaptation-with-Bradley-Terry-Preference-Models-in-Text-To-Image-Classification-and-Generation" class="headerlink" title="Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation"></a>Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07929">http://arxiv.org/abs/2308.07929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Gallego</li>
<li>for: 这个研究是为了适应大型多modal模型（如CLIP和Stable Diffusion） towards sets of particular human preferences，使得用户能够 personnalize 这些模型 для特定任务或偏好。</li>
<li>methods: 我们使用 Bradley-Terry 偏好模型开发了一种快速适应方法，可以快速调整原始模型，只需要几个示例和Minimal computing resources。</li>
<li>results: 我们透过不同的多modal text and image understanding领域的实验，证明了这个框架的能力，包括偏好预测为赏金模型、生成任务等。<details>
<summary>Abstract</summary>
Recently, large multimodal models, such as CLIP and Stable Diffusion have experimented tremendous successes in both foundations and applications. However, as these models increase in parameter size and computational requirements, it becomes more challenging for users to personalize them for specific tasks or preferences. In this work, we address the problem of adapting the previous models towards sets of particular human preferences, aligning the retrieved or generated images with the preferences of the user. We leverage the Bradley-Terry preference model to develop a fast adaptation method that efficiently fine-tunes the original model, with few examples and with minimal computing resources. Extensive evidence of the capabilities of this framework is provided through experiments in different domains related to multimodal text and image understanding, including preference prediction as a reward model, and generation tasks.
</details>
<details>
<summary>摘要</summary>
Note:* "multimodal models" is translated as "多modal模型" (duō modal módel)* "CLIP" is translated as "CLIP" (C-L-I-P)* "Stable Diffusion" is translated as "稳定扩散" (jìng dìng kuò chuān)* "Bradley-Terry preference model" is translated as "布拉德利-特里尔喜好模型" (Bù lā dé lǐ-tè lì xǐ huān módel)* "fine-tunes" is translated as "细调" (xì diào)* "extensive evidence" is translated as "广泛的证据" (guǎng fāng de jiàn jí)
</details></li>
</ul>
<hr>
<h2 id="Prawn-Morphometrics-and-Weight-Estimation-from-Images-using-Deep-Learning-for-Landmark-Localization"><a href="#Prawn-Morphometrics-and-Weight-Estimation-from-Images-using-Deep-Learning-for-Landmark-Localization" class="headerlink" title="Prawn Morphometrics and Weight Estimation from Images using Deep Learning for Landmark Localization"></a>Prawn Morphometrics and Weight Estimation from Images using Deep Learning for Landmark Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07732">http://arxiv.org/abs/2307.07732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alzayat Saleh, Md Mehedi Hasan, Herman W Raadsma, Mehar S Khatkar, Dean R Jerry, Mostafa Rahimi Azghadi</li>
<li>for: 这个研究是为了开发一个可靠且高精度的数位图像识别技术，以便在水产业中快速和准确地取得鱼类的形态数据，包括体重和 morphometric 分析。</li>
<li>methods: 这个研究使用了一种新的深度学习（DL）方法，包括两个主要 ком成分：一个特征提取模组，使用克罗内克积分操作 effficiently 结合低级和高级特征；以及一个点位标定模组，使用这些特征来预测虾子身体中的关键 morphological 点（点标）的坐标。</li>
<li>results: 我们的实验结果显示，这个新的DL方法在精度、可靠性和效率方面都比以前的DL方法表现更好，并且在8164幅虾子图像 dataset 上进行评估。我们也使用了原始特征来Derive five important prawn traits，并使用PCA方法来找出这些点标之间的距离，发现这些距离和鱼类的体长和宽度有高度的相关性。<details>
<summary>Abstract</summary>
Accurate weight estimation and morphometric analyses are useful in aquaculture for optimizing feeding, predicting harvest yields, identifying desirable traits for selective breeding, grading processes, and monitoring the health status of production animals. However, the collection of phenotypic data through traditional manual approaches at industrial scales and in real-time is time-consuming, labour-intensive, and prone to errors. Digital imaging of individuals and subsequent training of prediction models using Deep Learning (DL) has the potential to rapidly and accurately acquire phenotypic data from aquaculture species. In this study, we applied a novel DL approach to automate weight estimation and morphometric analysis using the black tiger prawn (Penaeus monodon) as a model crustacean. The DL approach comprises two main components: a feature extraction module that efficiently combines low-level and high-level features using the Kronecker product operation; followed by a landmark localization module that then uses these features to predict the coordinates of key morphological points (landmarks) on the prawn body. Once these landmarks were extracted, weight was estimated using a weight regression module based on the extracted landmarks using a fully connected network. For morphometric analyses, we utilized the detected landmarks to derive five important prawn traits. Principal Component Analysis (PCA) was also used to identify landmark-derived distances, which were found to be highly correlated with shape features such as body length, and width. We evaluated our approach on a large dataset of 8164 images of the Black tiger prawn (Penaeus monodon) collected from Australian farms. Our experimental results demonstrate that the novel DL approach outperforms existing DL methods in terms of accuracy, robustness, and efficiency.
</details>
<details>
<summary>摘要</summary>
Accurate weight estimation和形态分析在水产业中是非常有用的，可以优化饲料、预测捕捞量、确定适应性 trait、分级处理和监测生产动物的健康状况。但是，通过传统的手动方法收集生物数据在工业规模和实时是时间consuming、劳动密集和容易出错。数字图像技术可以快速和准确地获取生物数据。在这种研究中，我们采用了一种新的深度学习（DL）方法来自动化生物量和形态分析。这种DL方法包括两个主要组成部分：一个特征提取模块，使用Kronecker乘法高效地结合低级和高级特征；然后是一个地标定位模块，使用这些特征预测虾蟹身体上关键的形态特征（地标）的坐标。一旦获得了这些地标，我们使用基于这些地标的普通连接网络来估算虾蟹的重量。为了形态分析，我们利用检测到的地标来计算五个重要的虾蟹特征。我们还使用主成分分析（PCA）来确定由地标得到的距离，这些距离与身体长度和宽度之间存在高度相关性。我们对澳大利亚营养虾蟹（Penaeus monodon）的大量数据集进行了实验，结果表明，我们的DL方法在准确性、可靠性和效率方面都高于现有的DL方法。
</details></li>
</ul>
<hr>
<h2 id="Improving-NeRF-with-Height-Data-for-Utilization-of-GIS-Data"><a href="#Improving-NeRF-with-Height-Data-for-Utilization-of-GIS-Data" class="headerlink" title="Improving NeRF with Height Data for Utilization of GIS Data"></a>Improving NeRF with Height Data for Utilization of GIS Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07729">http://arxiv.org/abs/2307.07729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hinata Aoki, Takao Yamanaka</li>
<li>for: 使用 Geographic Information System (GIS) 获取的高程数据，应用 Neural Radiance Fields (NeRF) 技术来重建大规模Scene。</li>
<li>methods: 将场景空间分成多个 объек和背景，使用高程数据来表示它们，并采用自适应采样方法。</li>
<li>results: 通过这种方法，可以提高图像渲染的准确性，同时减少训练速度。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) has been applied to various tasks related to representations of 3D scenes. Most studies based on NeRF have focused on a small object, while a few studies have tried to reconstruct large-scale scenes although these methods tend to require large computational cost. For the application of NeRF to large-scale scenes, a method based on NeRF is proposed in this paper to effectively use height data which can be obtained from GIS (Geographic Information System). For this purpose, the scene space was divided into multiple objects and a background using the height data to represent them with separate neural networks. In addition, an adaptive sampling method is also proposed by using the height data. As a result, the accuracy of image rendering was improved with faster training speed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Neural Radiance Fields (NeRF) has been applied to various tasks related to representations of 3D scenes. Most studies based on NeRF have focused on a small object, while a few studies have tried to reconstruct large-scale scenes although these methods tend to require large computational cost. For the application of NeRF to large-scale scenes, a method based on NeRF is proposed in this paper to effectively use height data which can be obtained from GIS (Geographic Information System). For this purpose, the scene space was divided into multiple objects and a background using the height data to represent them with separate neural networks. In addition, an adaptive sampling method is also proposed by using the height data. As a result, the accuracy of image rendering was improved with faster training speed." into Simplified Chinese.哦！下面是文本翻译成简化字的中文：NeRF（神经辐射场）已经应用于多种3D场景表示任务中。大多数NeRF研究都集中在小对象上，只有一些研究尝试了大规模场景重建，但这些方法通常需要大量计算成本。为了应用NeRF到大规模场景，这篇论文提出了一种基于NeRF的方法，使用GIS（地理信息系统）获取的高程数据来有效地使用它们。为此，场景空间被分解成多个对象和背景，使用高程数据来表示它们的不同神经网络。此外，还提出了一种适应采样方法。因此，图像渲染精度得到了改善，同时训练速度也得到了加速。
</details></li>
</ul>
<hr>
<h2 id="Improving-Translation-Invariance-in-Convolutional-Neural-Networks-with-Peripheral-Prediction-Padding"><a href="#Improving-Translation-Invariance-in-Convolutional-Neural-Networks-with-Peripheral-Prediction-Padding" class="headerlink" title="Improving Translation Invariance in Convolutional Neural Networks with Peripheral Prediction Padding"></a>Improving Translation Invariance in Convolutional Neural Networks with Peripheral Prediction Padding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07725">http://arxiv.org/abs/2307.07725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kensuke Mukai, Takao Yamanaka</li>
<li>for: 这个论文是为了提出一种新的填充方法，以便在卷积神经网络中进行端到端训练。</li>
<li>methods: 该方法使用一种新的填充方法，即 Peripheral Prediction Padding (PP-Pad) 方法，可以根据每个任务而不同地训练填充值。</li>
<li>results: 经过测试，该方法在 semantic segmentation 任务中实现了更高的准确率和翻译不变性，比既前一些方法更好。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Zero padding is often used in convolutional neural networks to prevent the feature map size from decreasing with each layer. However, recent studies have shown that zero padding promotes encoding of absolute positional information, which may adversely affect the performance of some tasks. In this work, a novel padding method called Peripheral Prediction Padding (PP-Pad) method is proposed, which enables end-to-end training of padding values suitable for each task instead of zero padding. Moreover, novel metrics to quantitatively evaluate the translation invariance of the model are presented. By evaluating with these metrics, it was confirmed that the proposed method achieved higher accuracy and translation invariance than the previous methods in a semantic segmentation task.
</details>
<details>
<summary>摘要</summary>
很多时候，卷积神经网络中使用零填充来防止特征图的大小随层数量递减。然而，最近的研究表明，零填充可能会导致编码绝对位置信息，这可能会影响一些任务的性能。在这种情况下，一种新的填充方法called Peripheral Prediction Padding（PP-Pad）方法被提出，该方法可以在每个任务中自动训练适合的填充值。此外，一些用于评估模型的翻译不变性的新指标也被提出。通过使用这些指标评估，确认了提议方法在 semantic segmentation 任务中实现了更高的准确率和翻译不变性，与之前的方法相比。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Spectral-Hyperspectral-Classification-based-on-Learnable-3D-Group-Convolution"><a href="#Spatial-Spectral-Hyperspectral-Classification-based-on-Learnable-3D-Group-Convolution" class="headerlink" title="Spatial-Spectral Hyperspectral Classification based on Learnable 3D Group Convolution"></a>Spatial-Spectral Hyperspectral Classification based on Learnable 3D Group Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07720">http://arxiv.org/abs/2307.07720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guandong Li, Mengxia Ye<br>for:This paper proposes a learnable group convolution network (LGCNet) to improve the performance of deep neural networks in hyperspectral image classification.methods:The LGCNet module uses a dynamic learning method for the input channels and convolution kernel grouping, which allows for flexible grouping structures and improved representation ability.results:The LGCNet achieves better inference speed and accuracy than mainstream hyperspectral image classification methods on three datasets (Indian Pines, Pavia University, and KSC).<details>
<summary>Abstract</summary>
Deep neural networks have faced many problems in hyperspectral image classification, including the ineffective utilization of spectral-spatial joint information and the problems of gradient vanishing and overfitting that arise with increasing depth. In order to accelerate the deployment of models on edge devices with strict latency requirements and limited computing power, this paper proposes a learnable group convolution network (LGCNet) based on an improved 3D-DenseNet model and a lightweight model design. The LGCNet module improves the shortcomings of group convolution by introducing a dynamic learning method for the input channels and convolution kernel grouping, enabling flexible grouping structures and generating better representation ability. Through the overall loss and gradient of the backpropagation network, the 3D group convolution is dynamically determined and updated in an end-to-end manner. The learnable number of channels and corresponding grouping can capture different complementary visual features of input images, allowing the CNN to learn richer feature representations. When extracting high-dimensional and redundant hyperspectral data, the 3D convolution kernels also contain a large amount of redundant information. The LGC module allows the 3D-DenseNet to choose channel information with more semantic features, and is very efficient, making it suitable for embedding in any deep neural network for acceleration and efficiency improvements. LGC enables the 3D-CNN to achieve sufficient feature extraction while also meeting speed and computing requirements. Furthermore, LGCNet has achieved progress in inference speed and accuracy, and outperforms mainstream hyperspectral image classification methods on the Indian Pines, Pavia University, and KSC datasets.
</details>
<details>
<summary>摘要</summary>
深度神经网络在多spectral图像分类中遇到了许多问题，包括不好地利用spectral-spatial共同信息和深度增加导致梯度消失和过拟合问题。为了加速部署模型在边缘设备上，这篇文章提出了一种可学习的群集卷积网络（LGCNet），基于改进的3D-DenseNet模型和轻量级模型设计。LGCNet模块将group卷积缺点改进，通过动态学习输入通道和卷积核组合，实现更flexible的grouping结构，并且能够更好地表示能力。通过整体损失和反向传播网络的梯度，3D group卷积在端到端方式进行动态确定和更新。学习可变通道和相应的grouping可以捕捉不同的辐射特征图像的可读性信息，使CNN学习更加丰富的特征表示。在提取高维和重复的多spectral数据时，3D卷积核也包含了大量的重复信息。LGC模块使得3D-DenseNet可以选择更多semantic的通道信息，非常高效，适用于任何深度神经网络的加速和效率提高。LGCNet在推理速度和准确率方面进步，并在印度棕榈、帕维亚大学和科学中心 datasets 上超越主流多spectral图像分类方法。
</details></li>
</ul>
<hr>
<h2 id="ExposureDiffusion-Learning-to-Expose-for-Low-light-Image-Enhancement"><a href="#ExposureDiffusion-Learning-to-Expose-for-Low-light-Image-Enhancement" class="headerlink" title="ExposureDiffusion: Learning to Expose for Low-light Image Enhancement"></a>ExposureDiffusion: Learning to Expose for Low-light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07710">http://arxiv.org/abs/2307.07710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ExposureDiffusion">https://github.com/wyf0912/ExposureDiffusion</a></li>
<li>paper_authors: Yufei Wang, Yi Yu, Wenhan Yang, Lanqing Guo, Lap-Pui Chau, Alex C. Kot, Bihan Wen</li>
<li>for: 提高图像亮度和准确率</li>
<li>methods: 结合扩散模型和物理基础模型，实现图像亮度提高和准确率提高</li>
<li>results: 实现了图像亮度提高和准确率提高，并且比传统扩散模型具有更好的一致性和更快的推理速度<details>
<summary>Abstract</summary>
Previous raw image-based low-light image enhancement methods predominantly relied on feed-forward neural networks to learn deterministic mappings from low-light to normally-exposed images. However, they failed to capture critical distribution information, leading to visually undesirable results. This work addresses the issue by seamlessly integrating a diffusion model with a physics-based exposure model. Different from a vanilla diffusion model that has to perform Gaussian denoising, with the injected physics-based exposure model, our restoration process can directly start from a noisy image instead of pure noise. As such, our method obtains significantly improved performance and reduced inference time compared with vanilla diffusion models. To make full use of the advantages of different intermediate steps, we further propose an adaptive residual layer that effectively screens out the side-effect in the iterative refinement when the intermediate results have been already well-exposed. The proposed framework can work with both real-paired datasets, SOTA noise models, and different backbone networks. Note that, the proposed framework is compatible with real-paired datasets, real/synthetic noise models, and different backbone networks. We evaluate the proposed method on various public benchmarks, achieving promising results with consistent improvements using different exposure models and backbones. Besides, the proposed method achieves better generalization capacity for unseen amplifying ratios and better performance than a larger feedforward neural model when few parameters are adopted.
</details>
<details>
<summary>摘要</summary>
previous raw image-based low-light image enhancement methods mostly relied on feed-forward neural networks to learn deterministic mappings from low-light to normally-exposed images, but they failed to capture critical distribution information, leading to visually undesirable results. this work addresses the issue by seamlessly integrating a diffusion model with a physics-based exposure model. different from a vanilla diffusion model that has to perform Gaussian denoising, with the injected physics-based exposure model, our restoration process can directly start from a noisy image instead of pure noise. as such, our method obtains significantly improved performance and reduced inference time compared with vanilla diffusion models. to make full use of the advantages of different intermediate steps, we further propose an adaptive residual layer that effectively screens out the side-effect in the iterative refinement when the intermediate results have been already well-exposed. the proposed framework can work with both real-paired datasets, SOTA noise models, and different backbone networks. note that, the proposed framework is compatible with real-paired datasets, real/synthetic noise models, and different backbone networks. we evaluate the proposed method on various public benchmarks, achieving promising results with consistent improvements using different exposure models and backbones. besides, the proposed method achieves better generalization capacity for unseen amplifying ratios and better performance than a larger feedforward neural model when few parameters are adopted.Here's the translation in Traditional Chinese:previous raw image-based low-light image enhancement methods mostly relied on feed-forward neural networks to learn deterministic mappings from low-light to normally-exposed images, but they failed to capture critical distribution information, leading to visually undesirable results. this work addresses the issue by seamlessly integrating a diffusion model with a physics-based exposure model. different from a vanilla diffusion model that has to perform Gaussian denoising, with the injected physics-based exposure model, our restoration process can directly start from a noisy image instead of pure noise. as such, our method obtains significantly improved performance and reduced inference time compared with vanilla diffusion models. to make full use of the advantages of different intermediate steps, we further propose an adaptive residual layer that effectively screens out the side-effect in the iterative refinement when the intermediate results have been already well-exposed. the proposed framework can work with both real-paired datasets, SOTA noise models, and different backbone networks. note that, the proposed framework is compatible with real-paired datasets, real/synthetic noise models, and different backbone networks. we evaluate the proposed method on various public benchmarks, achieving promising results with consistent improvements using different exposure models and backbones. besides, the proposed method achieves better generalization capacity for unseen amplifying ratios and better performance than a larger feedforward neural model when few parameters are adopted.
</details></li>
</ul>
<hr>
<h2 id="PSGformer-Enhancing-3D-Point-Cloud-Instance-Segmentation-via-Precise-Semantic-Guidance"><a href="#PSGformer-Enhancing-3D-Point-Cloud-Instance-Segmentation-via-Precise-Semantic-Guidance" class="headerlink" title="PSGformer: Enhancing 3D Point Cloud Instance Segmentation via Precise Semantic Guidance"></a>PSGformer: Enhancing 3D Point Cloud Instance Segmentation via Precise Semantic Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07708">http://arxiv.org/abs/2307.07708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Pan, Wuyang Luan, Yuan Zheng, Qiang Fu, Junhui Li</li>
<li>for: 提高3D实例分割的性能，解决现有的3D semantic segmentation模型导致的限制。</li>
<li>methods: 提出了一种新的3D实例分割网络PSGformer，包括两个关键进步：首先，我们提出了一种多级semantic aggregation module，通过对前景点 filtering和多半径聚合来有效地捕捉场景特征。其次，PSGformer引入了并行特征融合transformer模块，通过独立处理超点特征和聚合特征来实现更全面的特征表示。</li>
<li>results: 在ScanNetv2 dataset上进行了广泛的实验，并证明PSGformer可以在Scannetv2隐藏测试集上提高3D实例分割的性能，比对比的状态体系方法高2.2%。<details>
<summary>Abstract</summary>
Most existing 3D instance segmentation methods are derived from 3D semantic segmentation models. However, these indirect approaches suffer from certain limitations. They fail to fully leverage global and local semantic information for accurate prediction, which hampers the overall performance of the 3D instance segmentation framework. To address these issues, this paper presents PSGformer, a novel 3D instance segmentation network. PSGformer incorporates two key advancements to enhance the performance of 3D instance segmentation. Firstly, we propose a Multi-Level Semantic Aggregation Module, which effectively captures scene features by employing foreground point filtering and multi-radius aggregation. This module enables the acquisition of more detailed semantic information from global and local perspectives. Secondly, PSGformer introduces a Parallel Feature Fusion Transformer Module that independently processes super-point features and aggregated features using transformers. The model achieves a more comprehensive feature representation by the features which connect global and local features. We conducted extensive experiments on the ScanNetv2 dataset. Notably, PSGformer exceeds compared state-of-the-art methods by 2.2% on ScanNetv2 hidden test set in terms of mAP. Our code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
现有的大多数3D实例分割方法都是基于3Dsemantic分割模型的派生方法。然而，这些间接方法受到一定的限制。它们不能充分利用全局和局部semantic信息，导致3D实例分割框架的总性表现下降。为了解决这些问题，本文提出了PSGformer，一种新的3D实例分割网络。PSGformer包括两项关键进步，以提高3D实例分割的表现。首先，我们提出了一个多级semantic汇集模块，该模块通过对前景点滤波和多半径汇集来有效地捕捉场景特征。这使得更详细的semantic信息从全局和局部角度得到了捕捉。其次，PSGformer引入了并行特征融合 transformer模块，该模块使用transformer独立处理超点特征和汇集特征，以实现更全面的特征表示。我们在ScanNetv2 dataset上进行了广泛的实验。值得注意的是，PSGformer在ScanNetv2隐藏测试集上比相对state-of-the-art方法高出2.2%的mAP。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Neural-Deformable-Models-for-3D-Bi-Ventricular-Heart-Shape-Reconstruction-and-Modeling-from-2D-Sparse-Cardiac-Magnetic-Resonance-Imaging"><a href="#Neural-Deformable-Models-for-3D-Bi-Ventricular-Heart-Shape-Reconstruction-and-Modeling-from-2D-Sparse-Cardiac-Magnetic-Resonance-Imaging" class="headerlink" title="Neural Deformable Models for 3D Bi-Ventricular Heart Shape Reconstruction and Modeling from 2D Sparse Cardiac Magnetic Resonance Imaging"></a>Neural Deformable Models for 3D Bi-Ventricular Heart Shape Reconstruction and Modeling from 2D Sparse Cardiac Magnetic Resonance Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07693">http://arxiv.org/abs/2307.07693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Ye, Dong Yang, Mikael Kanski, Leon Axel, Dimitris Metaxas</li>
<li>for: 重建和模型三维心脏bi-射影形状从二维精 sparsecardiac magnetic resonance(CMR)成像数据</li>
<li>methods: 使用混合型抽象超quadrics模型，通过 globally和 locally抽象来模型bi-射影形状</li>
<li>results: 比 conventiomal方法高效、可以自动生成高质量三角形网格、学习 dense对应关系以进行准确心脏形态 региSTRATION<details>
<summary>Abstract</summary>
We propose a novel neural deformable model (NDM) targeting at the reconstruction and modeling of 3D bi-ventricular shape of the heart from 2D sparse cardiac magnetic resonance (CMR) imaging data. We model the bi-ventricular shape using blended deformable superquadrics, which are parameterized by a set of geometric parameter functions and are capable of deforming globally and locally. While global geometric parameter functions and deformations capture gross shape features from visual data, local deformations, parameterized as neural diffeomorphic point flows, can be learned to recover the detailed heart shape.Different from iterative optimization methods used in conventional deformable model formulations, NDMs can be trained to learn such geometric parameter functions, global and local deformations from a shape distribution manifold. Our NDM can learn to densify a sparse cardiac point cloud with arbitrary scales and generate high-quality triangular meshes automatically. It also enables the implicit learning of dense correspondences among different heart shape instances for accurate cardiac shape registration. Furthermore, the parameters of NDM are intuitive, and can be used by a physician without sophisticated post-processing. Experimental results on a large CMR dataset demonstrate the improved performance of NDM over conventional methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的神经流形模型（NDM），用于从2D稀疏卡路里影像数据中重建和模型3D双腔形心的形状。我们使用混合的流形超quadrics来模型双腔形，这些超quadrics是由一组 геометрических参数函数parameter化的，可以在全球和地方水平上进行扭曲。而全球的 геометрические参数函数和扭曲可以从视觉数据中提取大规模的形状特征，而地方的扭曲，则是通过神经Diffusion点流来学习，以回归心形的细节。与传统的可变模型形式ulation中的迭代优化方法不同，NDM可以通过形态分布拟合来学习 geometric parameter functions和全球和地方的扭曲。我们的NDM可以从稀疏卡路里点云中填充任意尺度的点云，并自动生成高质量的三角形网格。此外，NDM还可以自动学习 dense correspondences among different heart shape instances，以实现准确的心形注册。此外，NDM的参数是直观的，可以由医生 без高级后处理使用。实验结果表明，NDM在大量CMR数据集上表现得更好于传统方法。
</details></li>
</ul>
<hr>
<h2 id="DRM-IR-Task-Adaptive-Deep-Unfolding-Network-for-All-In-One-Image-Restoration"><a href="#DRM-IR-Task-Adaptive-Deep-Unfolding-Network-for-All-In-One-Image-Restoration" class="headerlink" title="DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration"></a>DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07688">http://arxiv.org/abs/2307.07688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuanshuoCheng/DRM-IR">https://github.com/YuanshuoCheng/DRM-IR</a></li>
<li>paper_authors: Yuanshuo Cheng, Mingwen Shao, Yecong Wan, Chao Wang, Wangmeng Zuo</li>
<li>for: 这篇论文是为了提出一种高效的全能性图像修复方法（DRM-IR），以提高图像修复性能。</li>
<li>methods: 该方法包括任务适应质量模型和模型基于图像修复两个子任务，它们是通过参考图像对进行最大 posteriori推断的。</li>
<li>results: 对多个标准数据集进行了广泛的实验，显示DRM-IR可以达到当今最佳的全能性图像修复性能。<details>
<summary>Abstract</summary>
Existing All-In-One image restoration (IR) methods usually lack flexible modeling on various types of degradation, thus impeding the restoration performance. To achieve All-In-One IR with higher task dexterity, this work proposes an efficient Dynamic Reference Modeling paradigm (DRM-IR), which consists of task-adaptive degradation modeling and model-based image restoring. Specifically, these two subtasks are formalized as a pair of entangled reference-based maximum a posteriori (MAP) inferences, which are optimized synchronously in an unfolding-based manner. With the two cascaded subtasks, DRM-IR first dynamically models the task-specific degradation based on a reference image pair and further restores the image with the collected degradation statistics. Besides, to bridge the semantic gap between the reference and target degraded images, we further devise a Degradation Prior Transmitter (DPT) that restrains the instance-specific feature differences. DRM-IR explicitly provides superior flexibility for All-in-One IR while being interpretable. Extensive experiments on multiple benchmark datasets show that our DRM-IR achieves state-of-the-art in All-In-One IR.
</details>
<details>
<summary>摘要</summary>
现有的全包式图像恢复（IR）方法通常缺乏不同类型的退化模型的灵活定制，因此影响了恢复性能。为了实现更高的任务技巧性，本工作提出了高效的动态参照模型 paradigma（DRM-IR），它包括任务适应性的退化模型和基于模型的图像恢复。具体来说，这两个子任务被形式化为一对推理最大 posteriori（MAP）推理，它们在一种层次结构上进行同步优化。通过两个层次的子任务，DRM-IR首先在参照图像对中动态地模型任务特定的退化，然后使用采集的退化统计来恢复图像。此外，为了跨越参照图像和目标退化图像之间的Semantic gap，我们还提出了退化先天传输器（DPT），它限制了特定的特征差异。DRM-IR显式地提供了更高灵活性和可解释性，并在多个 benchmark 数据集上进行了广泛的实验，证明了我们的 DRM-IR 在全包式IR 中实现了state-of-the-art。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Contrastive-Bootstrapping-for-Single-positive-Multi-label-Recognition"><a href="#Semantic-Contrastive-Bootstrapping-for-Single-positive-Multi-label-Recognition" class="headerlink" title="Semantic Contrastive Bootstrapping for Single-positive Multi-label Recognition"></a>Semantic Contrastive Bootstrapping for Single-positive Multi-label Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07680">http://arxiv.org/abs/2307.07680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iCVTEAM/Scob">https://github.com/iCVTEAM/Scob</a></li>
<li>paper_authors: Cheng Chen, Yifan Zhao, Jia Li</li>
<li>for: 这篇论文主要用于提出一种学习多个标签图像识别方法，以便利用部分标注数据进行训练，并且可以提高图像识别性能和减少标注工作量。</li>
<li>methods: 该方法使用semantic contrastive bootstrapping（Scob）方法，通过引入类活动作为semantic guidance来慢慢恢复交对关系，然后提出一种循环semantic masked transformer来提取图像级别的iconic对象表示，并在多个标签分类任务上进行contrastive学习。</li>
<li>results: 经过extensive的实验表明，提出的共同学习框架可以在四个公共多个标签图像识别benchmark上大幅超越现有的模型，并且可以减少可能由错误semantic guidance引起的干扰。<details>
<summary>Abstract</summary>
Learning multi-label image recognition with incomplete annotation is gaining popularity due to its superior performance and significant labor savings when compared to training with fully labeled datasets. Existing literature mainly focuses on label completion and co-occurrence learning while facing difficulties with the most common single-positive label manner. To tackle this problem, we present a semantic contrastive bootstrapping (Scob) approach to gradually recover the cross-object relationships by introducing class activation as semantic guidance. With this learning guidance, we then propose a recurrent semantic masked transformer to extract iconic object-level representations and delve into the contrastive learning problems on multi-label classification tasks. We further propose a bootstrapping framework in an Expectation-Maximization fashion that iteratively optimizes the network parameters and refines semantic guidance to alleviate possible disturbance caused by wrong semantic guidance. Extensive experimental results demonstrate that the proposed joint learning framework surpasses the state-of-the-art models by a large margin on four public multi-label image recognition benchmarks. Codes can be found at https://github.com/iCVTEAM/Scob.
</details>
<details>
<summary>摘要</summary>
学习多标签图像识别 WITH incomplete annotation 在当前研究中得到了广泛的关注，因为它在训练完全标注数据集时表现出的高性能和重要的劳动力成本减少。现有文献主要关注于标签完成和共occurrence学习，而面临着最常见的单个正样式问题。为解决这个问题，我们提出了semantic contrastive bootstrapping（Scob）方法，通过引入类活动作为semantic guidance来慢慢地恢复交对关系。然后，我们提议一种循环semantic masked transformer来提取图像级别的iconic对象表示，并探索多标签分类任务中的contrastive learning问题。我们还提出了一个Expectation-Maximization的框架，iteratively optimize网络参数和refine semantic guidance，以避免可能由错误的semantic guidance引起的干扰。实验结果表明，我们提出的 JOINT learning框架在四个公共多标签图像识别benchmark上超过了当前状态的模型，并且可以在https://github.com/iCVTEAM/Scob找到代码。
</details></li>
</ul>
<hr>
<h2 id="Both-Spatial-and-Frequency-Cues-Contribute-to-High-Fidelity-Image-Inpainting"><a href="#Both-Spatial-and-Frequency-Cues-Contribute-to-High-Fidelity-Image-Inpainting" class="headerlink" title="Both Spatial and Frequency Cues Contribute to High-Fidelity Image Inpainting"></a>Both Spatial and Frequency Cues Contribute to High-Fidelity Image Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07678">http://arxiv.org/abs/2307.07678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ze Lu, Yalei Lv, Wenqi Wang, Pengfei Xiong</li>
<li>for:  Image inpainting with deep generative approaches.</li>
<li>methods:  Proposed Frequency-Spatial Complementary Network (FSCN) with extra Frequency Branch and Frequency Loss, and Frequency-Spatial Cross-Attention Block (FSCAB) to fuse multi-domain features.</li>
<li>results:  Superior inpainting results with fewer parameters and less computation cost, outperforming previous state-of-the-art approaches.<details>
<summary>Abstract</summary>
Deep generative approaches have obtained great success in image inpainting recently. However, most generative inpainting networks suffer from either over-smooth results or aliasing artifacts. The former lacks high-frequency details, while the latter lacks semantic structure. To address this issue, we propose an effective Frequency-Spatial Complementary Network (FSCN) by exploiting rich semantic information in both spatial and frequency domains. Specifically, we introduce an extra Frequency Branch and Frequency Loss on the spatial-based network to impose direct supervision on the frequency information, and propose a Frequency-Spatial Cross-Attention Block (FSCAB) to fuse multi-domain features and combine the corresponding characteristics. With our FSCAB, the inpainting network is capable of capturing frequency information and preserving visual consistency simultaneously. Extensive quantitative and qualitative experiments demonstrate that our inpainting network can effectively achieve superior results, outperforming previous state-of-the-art approaches with significantly fewer parameters and less computation cost. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
深度生成方法在图像填充方面最近几年取得了很大的成功。然而，大多数生成填充网络都会面临高频环境的过滤问题，导致结果过滤而失去高频环境的细节，或者保留了semantic结构，但是图像中的细节失真。为了解决这个问题，我们提出了一种有效的频率空间补充网络（FSCN），通过利用图像空间和频率频谱中的丰富semantic信息来解决。特别是，我们在网络中添加了额外的频率分支和频率损失，以直接监督频率信息，并提出了频率空间协同块（FSCAB）来融合多个频谱特征并组合相应的特征。通过我们的FSCAB，填充网络可以同时捕捉频率信息和保持视觉一致性。广泛的量化和质量测试表明，我们的填充网络可以达到更高的效果，比前一代方法有更少的参数和计算成本。代码将很快地发布。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Pseudo-labeled-Segmentation-for-Multi-Class-Object-Counting"><a href="#Learning-from-Pseudo-labeled-Segmentation-for-Multi-Class-Object-Counting" class="headerlink" title="Learning from Pseudo-labeled Segmentation for Multi-Class Object Counting"></a>Learning from Pseudo-labeled Segmentation for Multi-Class Object Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07677">http://arxiv.org/abs/2307.07677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyi Xu, Hieu Le, Dimitris Samaras</li>
<li>for: 本研究的目的是为了解决现有的物件计数模型在多类图像中对象计数 task 中的挑战，即尝试使用只有一些示例来计数图像中的多个物件类型。</li>
<li>methods: 我们提议使用示例基本的分割模型来地方化对象区域，然后使用这些分割模型来计数图像中的物件。我们使用只有盒子示例和点注释来获取pseudo segmentation masks，并训练这些分割模型。</li>
<li>results: 我们在两个新的多类数据集和一个真实图像集上评估了不同方法的性能，并显示了我们的提议方法在这些数据集上显著超过了之前的物件计数方法。<details>
<summary>Abstract</summary>
Class-agnostic counting (CAC) has numerous potential applications across various domains. The goal is to count objects of an arbitrary category during testing, based on only a few annotated exemplars. In this paper, we point out that the task of counting objects of interest when there are multiple object classes in the image (namely, multi-class object counting) is particularly challenging for current object counting models. They often greedily count every object regardless of the exemplars. To address this issue, we propose localizing the area containing the objects of interest via an exemplar-based segmentation model before counting them. The key challenge here is the lack of segmentation supervision to train this model. To this end, we propose a method to obtain pseudo segmentation masks using only box exemplars and dot annotations. We show that the segmentation model trained on these pseudo-labeled masks can effectively localize objects of interest for an arbitrary multi-class image based on the exemplars. To evaluate the performance of different methods on multi-class counting, we introduce two new benchmarks, a synthetic multi-class dataset and a new test set of real images in which objects from multiple classes are present. Our proposed method shows a significant advantage over the previous CAC methods on these two benchmarks.
</details>
<details>
<summary>摘要</summary>
“类agnostic counting（CAC）具有广泛的应用前景，可以在不同领域中应用。目标是在测试过程中，基于只有一些标注的示例来计数对象。在本文中，我们指出，当图像中存在多个对象类时（即多类对象计数），现有的对象计数模型很难准确地计数对象。他们通常会吃掉所有对象，不论是否符合示例。为解决这个问题，我们提议先使用示例基于分割模型将对象区域围括出来，然后计数围括区域中的对象。关键问题在于，如何训练这个分割模型。为此，我们提议使用只有盒子示例和点标注来生成pseudo分割面积。我们发现，这种分割模型可以基于示例来有效地围括对象区域，并且可以在任意多类图像中计数对象。为评估不同方法的性能，我们创建了两个新的标准 bencmarks：一个是Synthetic多类数据集，另一个是一个新的实际图像测试集，其中对象来自多个类。我们的提议方法在这两个标准 bencmarks上表现出了明显的优势。”
</details></li>
</ul>
<hr>
<h2 id="INVE-Interactive-Neural-Video-Editing"><a href="#INVE-Interactive-Neural-Video-Editing" class="headerlink" title="INVE: Interactive Neural Video Editing"></a>INVE: Interactive Neural Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07663">http://arxiv.org/abs/2307.07663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahui Huang, Leonid Sigal, Kwang Moo Yi, Oliver Wang, Joon-Young Lee</li>
<li>for: 这个论文是为了提供一种实时视频编辑解决方案，帮助视频编辑过程中的 sparse frame 编辑propagation到整个视频clip。</li>
<li>methods: 该方法基于 recent work on Layered Neural Atlas (LNA)，但是LNA受到两大缺点：（1）方法太慢 для实时编辑，（2）不支持一些编辑用case，如直接帧编辑和固定Texture tracking。我们采用高效的网络架构，以及 hash-grids 编码，以提高处理速度。此外，我们学习 bi-directional 函数 между image-atlas 和引入 vectorized editing，这些都使得我们的 INVE 可以支持更多的编辑。</li>
<li>results: 相比 LNA，我们的 INVE 可以减少学习和推理时间，并且支持更多的视频编辑操作。我们通过对 INVE 和 LNA 进行了全面的量化和质量分析，并展示了 INVE 的优越性和改进的性能。 для视频结果，请参考 <a target="_blank" rel="noopener" href="https://gabriel-huang.github.io/inve/">https://gabriel-huang.github.io/inve/</a>。<details>
<summary>Abstract</summary>
We present Interactive Neural Video Editing (INVE), a real-time video editing solution, which can assist the video editing process by consistently propagating sparse frame edits to the entire video clip. Our method is inspired by the recent work on Layered Neural Atlas (LNA). LNA, however, suffers from two major drawbacks: (1) the method is too slow for interactive editing, and (2) it offers insufficient support for some editing use cases, including direct frame editing and rigid texture tracking. To address these challenges we leverage and adopt highly efficient network architectures, powered by hash-grids encoding, to substantially improve processing speed. In addition, we learn bi-directional functions between image-atlas and introduce vectorized editing, which collectively enables a much greater variety of edits in both the atlas and the frames directly. Compared to LNA, our INVE reduces the learning and inference time by a factor of 5, and supports various video editing operations that LNA cannot. We showcase the superiority of INVE over LNA in interactive video editing through a comprehensive quantitative and qualitative analysis, highlighting its numerous advantages and improved performance. For video results, please see https://gabriel-huang.github.io/inve/
</details>
<details>
<summary>摘要</summary>
我们介绍Interactive Neural Video Editing（INVE），一个实时影像修剪解决方案，可以帮助影像修剪过程中的叠加短缺几帧至整个影像片。我们的方法受到latest Layered Neural Atlas（LNA）的启发，但LNA有两个主要缺点：（1）方法太慢 для互动编辑，（2）它无法支持一些编辑用 caso，包括直接几帧编辑和固定的 Texture Tracking。为了解决这些挑战，我们采用高效的网络架构，并利用 Hash-Grids 编码，以提高处理速度。此外，我们学习了两向函数 между Image-Atlas和引入 вектор化编辑，这样可以实现更多的编辑在 both the atlas and the frames directly。与LNA相比，INVE可以reduces the learning and inference time by a factor of 5, and supports various video editing operations that LNA cannot。我们通过对INVE和LNA的Quantitative and qualitative analysis，强调其许多优点和改进的表现。如果您想看到影像效果，请参考 <https://gabriel-huang.github.io/inve/>。
</details></li>
</ul>
<hr>
<h2 id="RFLA-A-Stealthy-Reflected-Light-Adversarial-Attack-in-the-Physical-World"><a href="#RFLA-A-Stealthy-Reflected-Light-Adversarial-Attack-in-the-Physical-World" class="headerlink" title="RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World"></a>RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07653">http://arxiv.org/abs/2307.07653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/winterwindwang/rfla">https://github.com/winterwindwang/rfla</a></li>
<li>paper_authors: Donghua Wang, Wen Yao, Tingsong Jiang, Chao Li, Xiaoqian Chen</li>
<li>for: 本研究旨在攻击深度神经网络(DNNs)的物理攻击方法。</li>
<li>methods: 本文提出了一种新的反射光攻击(RFLA)方法，通过在镜前放置透明彩虹的彩色透明膜和纸剪形成不同颜色的几何图形来实现。</li>
<li>results: 实验结果表明，提出的方法在不同的数据集和模型上达到了99%以上的成功率，并在不同的物理环境中进行了验证。<details>
<summary>Abstract</summary>
Physical adversarial attacks against deep neural networks (DNNs) have recently gained increasing attention. The current mainstream physical attacks use printed adversarial patches or camouflage to alter the appearance of the target object. However, these approaches generate conspicuous adversarial patterns that show poor stealthiness. Another physical deployable attack is the optical attack, featuring stealthiness while exhibiting weakly in the daytime with sunlight. In this paper, we propose a novel Reflected Light Attack (RFLA), featuring effective and stealthy in both the digital and physical world, which is implemented by placing the color transparent plastic sheet and a paper cut of a specific shape in front of the mirror to create different colored geometries on the target object. To achieve these goals, we devise a general framework based on the circle to model the reflected light on the target object. Specifically, we optimize a circle (composed of a coordinate and radius) to carry various geometrical shapes determined by the optimized angle. The fill color of the geometry shape and its corresponding transparency are also optimized. We extensively evaluate the effectiveness of RFLA on different datasets and models. Experiment results suggest that the proposed method achieves over 99% success rate on different datasets and models in the digital world. Additionally, we verify the effectiveness of the proposed method in different physical environments by using sunlight or a flashlight.
</details>
<details>
<summary>摘要</summary>
Recently, physical adversarial attacks against deep neural networks (DNNs) have gained increasing attention. Current mainstream physical attacks use printed adversarial patches or camouflage to alter the appearance of the target object, but these approaches generate conspicuous adversarial patterns that show poor stealthiness. Another physical deployable attack is the optical attack, which is stealthy during the daytime with sunlight. In this paper, we propose a novel Reflected Light Attack (RFLA), which is effective and stealthy in both the digital and physical worlds. This attack is implemented by placing a color transparent plastic sheet and a paper cut of a specific shape in front of a mirror to create different colored geometries on the target object. To achieve these goals, we develop a general framework based on the circle to model the reflected light on the target object. Specifically, we optimize a circle (composed of a coordinate and radius) to carry various geometrical shapes determined by the optimized angle. The fill color of the geometry shape and its corresponding transparency are also optimized. We extensively evaluate the effectiveness of RFLA on different datasets and models. Experiment results suggest that the proposed method achieves over 99% success rate on different datasets and models in the digital world. Additionally, we verify the effectiveness of the proposed method in different physical environments by using sunlight or a flashlight.
</details></li>
</ul>
<hr>
<h2 id="ACF-Net-An-Attention-enhanced-Co-interactive-Fusion-Network-for-Automated-Structural-Condition-Assessment-in-Visual-Inspection"><a href="#ACF-Net-An-Attention-enhanced-Co-interactive-Fusion-Network-for-Automated-Structural-Condition-Assessment-in-Visual-Inspection" class="headerlink" title="ACF-Net: An Attention-enhanced Co-interactive Fusion Network for Automated Structural Condition Assessment in Visual Inspection"></a>ACF-Net: An Attention-enhanced Co-interactive Fusion Network for Automated Structural Condition Assessment in Visual Inspection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07643">http://arxiv.org/abs/2307.07643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyu Zhang, Zhaozheng Yin, Ruwen Qin<br>for: 这篇论文旨在提出一种自动化桥梁Visual化检查的方法，以提高公共工程设施的监测效率。methods: 该方法基于Attention-enhanced Co-interactive Fusion Network (ACF-Net)，可同时分析结构元素和表面缺陷，并使用两个任务特定的学习子网计算任务特定特征。results: 实验结果表明，提出的ACF-Net方法在新的Steel Bridge Condition Inspection Visual (SBCIV)测试集上达到了92.11% mIoU для元素分析和87.16% mIoU для腐蚀分割，超越当前状态的方法。<details>
<summary>Abstract</summary>
Efficiently monitoring the condition of civil infrastructures necessitates automating the structural condition assessment in visual inspection. This paper proposes an Attention-enhanced Co-interactive Fusion Network (ACF-Net) for automatic structural condition assessment in visual bridge inspection. The ACF-Net can simultaneously parse structural elements and segment surface defects on the elements in inspection images. It integrates two task-specific relearning subnets to extract task-specific features from an overall feature embedding and a co-interactive feature fusion module to capture the spatial correlation and facilitate information sharing between tasks. Experimental results demonstrate that the proposed ACF-Net outperforms the current state-of-the-art approaches, achieving promising performance with 92.11% mIoU for element parsing and 87.16% mIoU for corrosion segmentation on the new benchmark dataset Steel Bridge Condition Inspection Visual (SBCIV) testing set. An ablation study reveals the strengths of ACF-Net, and a case study showcases its capability to automate structural condition assessment. The code will be open-source after acceptance.
</details>
<details>
<summary>摘要</summary>
efficiently monitoring civil infrastructure condition requires automating structural condition assessment in visual inspection. this paper proposes an attention-enhanced co-interactive fusion network (ACF-Net) for automatic structural condition assessment in visual bridge inspection. the ACF-Net can simultaneously parse structural elements and segment surface defects on the elements in inspection images. it integrates two task-specific relearning subnets to extract task-specific features from an overall feature embedding and a co-interactive feature fusion module to capture the spatial correlation and facilitate information sharing between tasks. experimental results demonstrate that the proposed ACF-Net outperforms the current state-of-the-art approaches, achieving promising performance with 92.11% mIoU for element parsing and 87.16% mIoU for corrosion segmentation on the new benchmark dataset steel bridge condition inspection visual (SBCIV) testing set. an ablation study reveals the strengths of ACF-Net, and a case study showcases its capability to automate structural condition assessment. the code will be open-source after acceptance.Here's the text with traditional Chinese characters:efficiently monitoring civil infrastructure condition requires automating structural condition assessment in visual inspection. this paper proposes an attention-enhanced co-interactive fusion network (ACF-Net) for automatic structural condition assessment in visual bridge inspection. the ACF-Net can simultaneously parse structural elements and segment surface defects on the elements in inspection images. it integrates two task-specific relearning subnets to extract task-specific features from an overall feature embedding and a co-interactive feature fusion module to capture the spatial correlation and facilitate information sharing between tasks. experimental results demonstrate that the proposed ACF-Net outperforms the current state-of-the-art approaches, achieving promising performance with 92.11% mIoU for element parsing and 87.16% mIoU for corrosion segmentation on the new benchmark dataset steel bridge condition inspection visual (SBCIV) testing set. an ablation study reveals the strengths of ACF-Net, and a case study showcases its capability to automate structural condition assessment. the code will be open-source after acceptance.
</details></li>
</ul>
<hr>
<h2 id="CoTracker-It-is-Better-to-Track-Together"><a href="#CoTracker-It-is-Better-to-Track-Together" class="headerlink" title="CoTracker: It is Better to Track Together"></a>CoTracker: It is Better to Track Together</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07635">http://arxiv.org/abs/2307.07635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/co-tracker">https://github.com/facebookresearch/co-tracker</a></li>
<li>paper_authors: Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, Christian Rupprecht</li>
<li>for: 这个论文的目的是提出一种能够同时跟踪多个视频帧中多个点的抽象方法，以提高视频动作预测的性能。</li>
<li>methods: 这个方法使用了transformer网络，特制的注意层，iteratively更新多个轨迹的估计。</li>
<li>results: 这个方法在大多数测试 benchmark 中超越了当前state-of-the-art方法，并且可以同时跟踪一些点，还可以在视频帧中添加新的点进行跟踪。<details>
<summary>Abstract</summary>
Methods for video motion prediction either estimate jointly the instantaneous motion of all points in a given video frame using optical flow or independently track the motion of individual points throughout the video. The latter is true even for powerful deep-learning methods that can track points through occlusions. Tracking points individually ignores the strong correlation that can exist between the points, for instance, because they belong to the same physical object, potentially harming performance. In this paper, we thus propose CoTracker, an architecture that jointly tracks multiple points throughout an entire video. This architecture combines several ideas from the optical flow and tracking literature in a new, flexible and powerful design. It is based on a transformer network that models the correlation of different points in time via specialised attention layers. The transformer iteratively updates an estimate of several trajectories. It can be applied in a sliding-window manner to very long videos, for which we engineer an unrolled training loop. It can track from one to several points jointly and supports adding new points to track at any time. The result is a flexible and powerful tracking algorithm that outperforms state-of-the-art methods in almost all benchmarks.
</details>
<details>
<summary>摘要</summary>
视频动态预测方法可以同时估计视频帧中所有点的快照动态，或者独立地跟踪视频中每个点的动态。前者是true，即使使用深度学习方法，可以在遮挡中跟踪点。但是，单独跟踪点可能忽略视频中点之间的强相关性，例如因为它们属于同一物理对象，从而影响性能。为此，我们在这篇论文中提出了CoTracker，一种架构，可以在整个视频中同时跟踪多个点。这种架构结合了光流和跟踪领域的一些想法，并实现了一种flexible和强大的设计。它基于特殊注意层，以模型不同时间点之间的点相关性。特殊注意层在循环更新多个轨迹的估计，可以在很长的视频中使用滑块训练方法。它可以同时跟踪1到多个点，并且支持在任何时间添加新的点。结果是一种灵活和强大的跟踪算法，比state-of-the-art方法在大多数测试 benchMarks 上出perform。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Embeddings-with-Cross-batch-Metric-Learning"><a href="#Generalizable-Embeddings-with-Cross-batch-Metric-Learning" class="headerlink" title="Generalizable Embeddings with Cross-batch Metric Learning"></a>Generalizable Embeddings with Cross-batch Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07620">http://arxiv.org/abs/2307.07620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yetigurbuz/xml-dml">https://github.com/yetigurbuz/xml-dml</a></li>
<li>paper_authors: Yeti Z. Gurbuz, A. Aydin Alatan</li>
<li>for: 本文探讨了深度度量学中的全球平均pooling（GAP）组件，以及其在学习泛化表示的效果。</li>
<li>methods: 作者将GAP视为一种将各个特征向量作为独立的 semantic entity 进行组合的方式，并通过一种可 convex combination of learnable prototypes来表示GAP。然后，作者通过一种 recursive 过程来适应一个批处理的样本，并在每一轮中使用不同的批处理来正则化学习。</li>
<li>results: 作者在4个流行的DML benchmark上验证了他们的方法，并发现其可以提高DML模型的泛化能力。<details>
<summary>Abstract</summary>
Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Gastrointestinal-Disease-Classification-through-Explainable-and-Cost-Sensitive-Deep-Neural-Networks-with-Supervised-Contrastive-Learning"><a href="#Gastrointestinal-Disease-Classification-through-Explainable-and-Cost-Sensitive-Deep-Neural-Networks-with-Supervised-Contrastive-Learning" class="headerlink" title="Gastrointestinal Disease Classification through Explainable and Cost-Sensitive Deep Neural Networks with Supervised Contrastive Learning"></a>Gastrointestinal Disease Classification through Explainable and Cost-Sensitive Deep Neural Networks with Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07603">http://arxiv.org/abs/2307.07603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dibya404/gastrointestinal-disease-classification-through-explainable-and-cost-sensitive-dnn-with-scl">https://github.com/dibya404/gastrointestinal-disease-classification-through-explainable-and-cost-sensitive-dnn-with-scl</a></li>
<li>paper_authors: Dibya Nath, G. M. Shahariar</li>
<li>for: 这篇论文是为了开发一种基于深度卷积神经网络和可读性学习的肠胃疾病分类方法。</li>
<li>methods: 该方法利用了Cost-sensitive预训练的深度卷积神经网络架构，并采用了监督式对照学习来学习疾病相关的特征。此外，该方法还包括了Gradient-based的可读性技术，以提高模型的解释性。</li>
<li>results: 经过广泛的实验和比较，该方法能够实现高精度的肠胃疾病分类，同时具有Robustness和解释性。<details>
<summary>Abstract</summary>
Gastrointestinal diseases pose significant healthcare chall-enges as they manifest in diverse ways and can lead to potential complications. Ensuring precise and timely classification of these diseases is pivotal in guiding treatment choices and enhancing patient outcomes. This paper introduces a novel approach on classifying gastrointestinal diseases by leveraging cost-sensitive pre-trained deep convolutional neural network (CNN) architectures with supervised contrastive learning. Our approach enables the network to learn representations that capture vital disease-related features, while also considering the relationships of similarity between samples. To tackle the challenges posed by imbalanced datasets and the cost-sensitive nature of misclassification errors in healthcare, we incorporate cost-sensitive learning. By assigning distinct costs to misclassifications based on the disease class, we prioritize accurate classification of critical conditions. Furthermore, we enhance the interpretability of our model by integrating gradient-based techniques from explainable artificial intelligence (AI). This inclusion provides valuable insights into the decision-making process of the network, aiding in understanding the features that contribute to disease classification. To assess the effectiveness of our proposed approach, we perform extensive experiments on a comprehensive gastrointestinal disease dataset, such as the Hyper-Kvasir dataset. Through thorough comparisons with existing works, we demonstrate the strong classification accuracy, robustness and interpretability of our model. We have made the implementation of our proposed approach publicly available at https://github.com/dibya404/Gastrointestinal-Disease-Classification-through-Explainable-and-Cost-Sensitive-DNN-with-SCL
</details>
<details>
<summary>摘要</summary>
Gastrointestinal diseases pose significant healthcare challenges as they manifest in diverse ways and can lead to potential complications. Ensuring precise and timely classification of these diseases is crucial in guiding treatment choices and enhancing patient outcomes. This paper introduces a novel approach to classifying gastrointestinal diseases by leveraging cost-sensitive pre-trained deep convolutional neural network (CNN) architectures with supervised contrastive learning. Our approach enables the network to learn representations that capture vital disease-related features, while also considering the relationships of similarity between samples. To tackle the challenges posed by imbalanced datasets and the cost-sensitive nature of misclassification errors in healthcare, we incorporate cost-sensitive learning. By assigning distinct costs to misclassifications based on the disease class, we prioritize accurate classification of critical conditions. Furthermore, we enhance the interpretability of our model by integrating gradient-based techniques from explainable artificial intelligence (AI). This inclusion provides valuable insights into the decision-making process of the network, aiding in understanding the features that contribute to disease classification. To assess the effectiveness of our proposed approach, we perform extensive experiments on a comprehensive gastrointestinal disease dataset, such as the Hyper-Kvasir dataset. Through thorough comparisons with existing works, we demonstrate the strong classification accuracy, robustness, and interpretability of our model. We have made the implementation of our proposed approach publicly available at https://github.com/dibya404/Gastrointestinal-Disease-Classification-through-Explainable-and-Cost-Sensitive-DNN-with-SCL.
</details></li>
</ul>
<hr>
<h2 id="NIFTY-Neural-Object-Interaction-Fields-for-Guided-Human-Motion-Synthesis"><a href="#NIFTY-Neural-Object-Interaction-Fields-for-Guided-Human-Motion-Synthesis" class="headerlink" title="NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis"></a>NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07511">http://arxiv.org/abs/2307.07511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, Leonidas Guibas</li>
<li>for: 这个论文 targets the problem of generating realistic 3D human motions interacting with objects in a scene.</li>
<li>methods: 该论文提出了一个名为Neural Interaction Fields for Trajectory sYnthesis（NIFTY）的框架，它使用了神经网络生成交互场，以及基于物体的物理约束来驱动人物动作的扩散过程，以生成更加真实和可信的人物动作。</li>
<li>results: 该论文通过使用自动生成的 sintetic数据和NIFTY框架，实现了更加真实和可信的人物动作Synthesize，包括坐下和抬起几种对象的动作。这些动作质量和成功完成率都高于其他方法。<details>
<summary>Abstract</summary>
We address the problem of generating realistic 3D motions of humans interacting with objects in a scene. Our key idea is to create a neural interaction field attached to a specific object, which outputs the distance to the valid interaction manifold given a human pose as input. This interaction field guides the sampling of an object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To support interactions with scarcely available data, we propose an automated synthetic data pipeline. For this, we seed a pre-trained motion model, which has priors for the basics of human movement, with interaction-specific anchor poses extracted from limited motion capture data. Using our guided diffusion model trained on generated synthetic data, we synthesize realistic motions for sitting and lifting with several objects, outperforming alternative approaches in terms of motion quality and successful action completion. We call our framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.
</details>
<details>
<summary>摘要</summary>
我们Addressing the problem of generating realistic 3D human motions interacting with objects in a scene. Our key idea is to create a neural interaction field attached to a specific object, which outputs the distance to the valid interaction manifold given a human pose as input. This interaction field guides the sampling of an object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To support interactions with scarce data, we propose an automated synthetic data pipeline. For this, we seed a pre-trained motion model with interaction-specific anchor poses extracted from limited motion capture data. Using our guided diffusion model trained on generated synthetic data, we synthesize realistic motions for sitting and lifting with several objects, outperforming alternative approaches in terms of motion quality and successful action completion. We call our framework NIFTY: Neural Interaction Fields for Trajectory sYnthesis.Here's the translation breakdown:* 我们 (wǒmen) - we* Addressing ( Addressing) - addressing* the problem (the problem) - the problem* of generating (of generating) - of generating* realistic 3D human motions (realistic 3D human motions) - realistic 3D human motions* interacting (interacting) - interacting* with objects (with objects) - with objects* in a scene (in a scene) - in a scene* Our key idea (Our key idea) - Our key idea* is to create (is to create) - is to create* a neural interaction field (a neural interaction field) - a neural interaction field* attached to (attached to) - attached to* a specific object (a specific object) - a specific object* which outputs (which outputs) - which outputs* the distance (the distance) - the distance* to the valid interaction manifold (to the valid interaction manifold) - to the valid interaction manifold* given (given) - given* a human pose (a human pose) - a human pose* as input (as input) - as input* This interaction field (This interaction field) - This interaction field* guides (guides) - guides* the sampling (the sampling) - the sampling* of an object-conditioned (object-conditioned) - object-conditioned* human motion diffusion model (human motion diffusion model) - human motion diffusion model* so as to encourage (so as to encourage) - so as to encourage* plausible contacts (plausible contacts) - plausible contacts* and affordance semantics (and affordance semantics) - and affordance semantics* To support interactions (To support interactions) - To support interactions* with scarce data (with scarce data) - with scarce data* we propose (we propose) - we propose* an automated synthetic data pipeline (an automated synthetic data pipeline) - an automated synthetic data pipeline* For this (For this) - For this* we seed (we seed) - we seed* a pre-trained motion model (a pre-trained motion model) - a pre-trained motion model* with interaction-specific (with interaction-specific) - with interaction-specific* anchor poses (anchor poses) - anchor poses* extracted from (extracted from) - extracted from* limited motion capture data (limited motion capture data) - limited motion capture data* Using our guided diffusion model (Using our guided diffusion model) - Using our guided diffusion model* trained on generated (trained on generated) - trained on generated* synthetic data (synthetic data) - synthetic data* we synthesize (we synthesize) - we synthesize* realistic motions (realistic motions) - realistic motions* for sitting (for sitting) - for sitting* and lifting (and lifting) - and lifting* with several objects (with several objects) - with several objects* outperforming (outperforming) - outperforming* alternative approaches (alternative approaches) - alternative approaches* in terms of motion quality (in terms of motion quality) - in terms of motion quality* and successful action completion (and successful action completion) - and successful action completion* We call our framework (We call our framework) - We call our framework* NIFTY (NIFTY) - NIFTY* Neural Interaction Fields for Trajectory sYnthesis (Neural Interaction Fields for Trajectory sYnthesis) - Neural Interaction Fields for Trajectory sYnthesis
</details></li>
</ul>
<hr>
<h2 id="Brain-Tumor-Detection-using-Convolutional-Neural-Networks-with-Skip-Connections"><a href="#Brain-Tumor-Detection-using-Convolutional-Neural-Networks-with-Skip-Connections" class="headerlink" title="Brain Tumor Detection using Convolutional Neural Networks with Skip Connections"></a>Brain Tumor Detection using Convolutional Neural Networks with Skip Connections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07503">http://arxiv.org/abs/2307.07503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aupam Hamran, Marzieh Vaeztourshizi, Amirhossein Esmaili, Massoud Pedram</li>
<li>for: 用CNN分类脑肿为benign和malignant两类</li>
<li>methods: 使用MRI技术，采用不同的CNN体系结构，并应用扩展和深化网络，以及添加跳过连接等优化技术以提高网络的准确率</li>
<li>results: 结果表明，一些优化技术可以judiciously用于超越基eline CNN模型Note: “judiciously” is a bit of a tricky word to translate, but I’ve translated it as “可以judiciously用于” (can be used judiciously) to convey the idea that the optimization techniques can be selectively applied to achieve better results.<details>
<summary>Abstract</summary>
In this paper, we present different architectures of Convolutional Neural Networks (CNN) to analyze and classify the brain tumors into benign and malignant types using the Magnetic Resonance Imaging (MRI) technique. Different CNN architecture optimization techniques such as widening and deepening of the network and adding skip connections are applied to improve the accuracy of the network. Results show that a subset of these techniques can judiciously be used to outperform a baseline CNN model used for the same purpose.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了不同类型的卷积神经网络（CNN），用于通过磁共振成像（MRI）技术分类脑肿为非恶性和恶性两类。不同的CNN建立优化技术，如宽化和深化网络以及添加跳过连接，用于提高网络的准确性。结果表明，一些这些技术可以有效地用于超越基eline CNN模型，用于同一目的。Here's a word-for-word translation of the text using Traditional Chinese characters:在这篇论文中，我们介绍了不同类型的卷积神经网络（CNN），用于通过磁共振成像（MRI）技术分类脑肿为非恶性和恶性两类。不同的CNN建立优化技术，如宽化和深化网络以及添加跳过连接，用于提高网络的准确性。结果表明，一些这些技术可以有效地用于超越基eline CNN模型，用于同一目的。
</details></li>
</ul>
<hr>
<h2 id="TALL-Thumbnail-Layout-for-Deepfake-Video-Detection"><a href="#TALL-Thumbnail-Layout-for-Deepfake-Video-Detection" class="headerlink" title="TALL: Thumbnail Layout for Deepfake Video Detection"></a>TALL: Thumbnail Layout for Deepfake Video Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07494">http://arxiv.org/abs/2307.07494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuting Xu, Jian Liang, Gengyun Jia, Ziming Yang, Yanhao Zhang, Ran He</li>
<li>for: 这篇论文主要应用于侦测深增伪影片（deepfake video detection）。</li>
<li>methods: 本篇论文提出了一个简单 yet effective的策略，即幕照片（Thumbnail Layout，TALL），它可以将影片幕拍成一个预先定义的布局，以保留空间和时间相依性。TALL 是无需更改现有代码的简单方法，只需要在每幕中对继承幕拍进行干扰，然后将其转换为子图排序为预先定义的布局。</li>
<li>results: 实验结果显示，TALL 和 SOTA TALL-Swin 在标准测试集和跨测试集上均有出色的表现，TALL 的 AUC 为 90.79%，而 TALL-Swin 的 AUC 为 93.43%。<details>
<summary>Abstract</summary>
The growing threats of deepfakes to society and cybersecurity have raised enormous public concerns, and increasing efforts have been devoted to this critical topic of deepfake video detection. Existing video methods achieve good performance but are computationally intensive. This paper introduces a simple yet effective strategy named Thumbnail Layout (TALL), which transforms a video clip into a pre-defined layout to realize the preservation of spatial and temporal dependencies. Specifically, consecutive frames are masked in a fixed position in each frame to improve generalization, then resized to sub-images and rearranged into a pre-defined layout as the thumbnail. TALL is model-agnostic and extremely simple by only modifying a few lines of code. Inspired by the success of vision transformers, we incorporate TALL into Swin Transformer, forming an efficient and effective method TALL-Swin. Extensive experiments on intra-dataset and cross-dataset validate the validity and superiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\%$ AUC on the challenging cross-dataset task, FaceForensics++ $\to$ Celeb-DF. The code is available at https://github.com/rainy-xu/TALL4Deepfake.
</details>
<details>
<summary>摘要</summary>
“深圳技术”的威胁对社会和网络安全引起了巨大的公众关注，而寻找深圳视频的检测方法也在不断地努力。现有的视频方法可以达到好的性能，但是 computationally intensive。这篇文章介绍了一种简单 yet effective的策略，即 Thumbnail Layout（TALL），它将视频剪辑转换为预定的布局，以保持空间和时间相互依赖关系。 Specifically, consecutive frames 是在固定位置上填充，然后resize 为子图并重新排序成预定的布局作为缩略图。TALL 是无关模型的，只需要修改一些代码。通过 incorporate TALL 到 Swin Transformer 中，我们得到了高效和有效的方法 TALL-Swin。广泛的实验表明 TALL 和 SOTA TALL-Swin 的有效性和优势。 TALL-Swin 在艰辛的 cross-dataset 任务 FaceForensics++ $\to$ Celeb-DF 上达到了 90.79% AUC。代码可以在 GitHub 上找到：https://github.com/rainy-xu/TALL4Deepfake。
</details></li>
</ul>
<hr>
<h2 id="PseudoCal-A-Source-Free-Approach-to-Unsupervised-Uncertainty-Calibration-in-Domain-Adaptation"><a href="#PseudoCal-A-Source-Free-Approach-to-Unsupervised-Uncertainty-Calibration-in-Domain-Adaptation" class="headerlink" title="PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation"></a>PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07489">http://arxiv.org/abs/2307.07489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dapeng Hu, Jian Liang, Xinchao Wang, Chuan-Sheng Foo</li>
<li>for: 提高目标频率下模型的准确性</li>
<li>methods: 使用 PseudoCal 方法，依据无标目标数据进行准确性调整</li>
<li>results: 相比现有方法， PseudoCal 方法显示出了更低的准确性错误<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in improving the accuracy of models for unlabeled target domains. However, the calibration of predictive uncertainty in the target domain, a crucial aspect of the safe deployment of UDA models, has received limited attention. The conventional in-domain calibration method, \textit{temperature scaling} (TempScal), encounters challenges due to domain distribution shifts and the absence of labeled target domain data. Recent approaches have employed importance-weighting techniques to estimate the target-optimal temperature based on re-weighted labeled source data. Nonetheless, these methods require source data and suffer from unreliable density estimates under severe domain shifts, rendering them unsuitable for source-free UDA settings. To overcome these limitations, we propose PseudoCal, a source-free calibration method that exclusively relies on unlabeled target data. Unlike previous approaches that treat UDA calibration as a \textit{covariate shift} problem, we consider it as an unsupervised calibration problem specific to the target domain. Motivated by the factorization of the negative log-likelihood (NLL) objective in TempScal, we generate a labeled pseudo-target set that captures the structure of the real target. By doing so, we transform the unsupervised calibration problem into a supervised one, enabling us to effectively address it using widely-used in-domain methods like TempScal. Finally, we thoroughly evaluate the calibration performance of PseudoCal by conducting extensive experiments on 10 UDA methods, considering both traditional UDA settings and recent source-free UDA scenarios. The experimental results consistently demonstrate the superior performance of PseudoCal, exhibiting significantly reduced calibration error compared to existing calibration methods.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA) 在提高目标领域模型的准确性方面做出了非常出色的进步。然而，目标领域模型的预测uncertainty的准确性调整，作为模型安全部署的关键方面，受到了有限的关注。传统的域内准确度调整方法（TemperatureScaling， TempScal）在域分布转移和目标领域数据缺失问题上遇到了挑战。 latest approaches use importance-weighting techniques to estimate the target-optimal temperature based on re-weighted labeled source data. 然而，这些方法需要源数据，并且在严重的域转移情况下，density estimates 不可靠，从而无法适用于源自由 UDA 设置。为了解决这些限制，我们提出了 PseudoCal，一种源自由的准确性调整方法，不依赖于源数据。与前方法不同，我们将 UDA 准确性调整视为目标域specific的无监督准确性调整问题。 Motivated by the factorization of the negative log-likelihood (NLL) objective in TempScal, we generate a labeled pseudo-target set that captures the structure of the real target. By doing so, we transform the unsupervised calibration problem into a supervised one, enabling us to effectively address it using widely-used in-domain methods like TempScal。我们进行了广泛的实验，评估 PseudoCal 的准确性调整性能，包括传统 UDA 设置以及最近的源自由 UDA 场景。实验结果 consistently demonstrate the superior performance of PseudoCal， exhibiting significantly reduced calibration error compared to existing calibration methods。
</details></li>
</ul>
<hr>
<h2 id="DreamTeacher-Pretraining-Image-Backbones-with-Deep-Generative-Models"><a href="#DreamTeacher-Pretraining-Image-Backbones-with-Deep-Generative-Models" class="headerlink" title="DreamTeacher: Pretraining Image Backbones with Deep Generative Models"></a>DreamTeacher: Pretraining Image Backbones with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07487">http://arxiv.org/abs/2307.07487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, Sanja Fidler</li>
<li>for: 本研究提出了一种自我超vised特征表示学习框架 DreamTeacher，利用生成网络进行预训练下游图像卷积体。</li>
<li>methods: 我们提出了两种知识精炼方法：1）将生成模型学习的生成特征精炼到目标图像卷积体上，作为代替大量标注数据集such as ImageNet的预训练方法；2）将生成网络中的标签精炼到目标卷积体的幂点上。</li>
<li>results: 我们进行了多种生成模型、密集预测 benchmarks 和多种预训练方法的实验研究，发现我们的 DreamTeacher 对现有的自我超vised表示学习方法进行了显著超越。 不supervised ImageNet 预训练通过 DreamTeacher  leads to significant improvements over ImageNet classification pre-training on downstream datasets, showcasing generative models and diffusion generative models specifically, as a promising approach to representation learning on large, diverse datasets without requiring manual annotation.<details>
<summary>Abstract</summary>
In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significant improvements over ImageNet classification pre-training on downstream datasets, showcasing generative models, and diffusion generative models specifically, as a promising approach to representation learning on large, diverse datasets without requiring manual annotation.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了一个自我超vised特征表示学习框架 DreamTeacher，该框架利用生成网络进行预训练下游图像脊梁。我们提议通过将已经训练过的生成模型中的知识注入到标准图像脊梁上来，以代替使用大量标注数据集如ImageNet进行预训练。我们 investigate了两种知识注入方法：1）将生成模型中学习的特征注入到目标图像脊梁上作为替代预训练方法，2）将生成网络中的标签注入到目标脊梁的幂点上。我们进行了多种生成模型、精度预测 bencmarks 和多种预训练方式的实验研究。我们发现，我们的 DreamTeacher 在所有自我超vised表示学习方法中显著超越了其他方法。不需要人工标注，使用 DreamTeacher 进行无监督ImageNet预训练可以在下游数据集上获得显著改进，并且在多个生成模型和扩散生成模型中都有显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Distillation-for-Egocentric-Action-Recognition"><a href="#Multimodal-Distillation-for-Egocentric-Action-Recognition" class="headerlink" title="Multimodal Distillation for Egocentric Action Recognition"></a>Multimodal Distillation for Egocentric Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07483">http://arxiv.org/abs/2307.07483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gorjanradevski/multimodal-distillation">https://github.com/gorjanradevski/multimodal-distillation</a></li>
<li>paper_authors: Gorjan Radevski, Dusan Grujicic, Marie-Francine Moens, Matthew Blaschko, Tinne Tuytelaars</li>
<li>for: 本研究目的是为了模型手部物体互动，以提高 egocentric 视频理解性能。</li>
<li>methods: 该paper使用了 CNNs 和 Vision Transformers 等标准模型，并采用了多modal 输入模块，以提高模型性能。然而，这些多modal 模块增加了模型的复杂度，使其不适合实际应用。本研究目标是保留多modal 模型的性能，但只使用 RGB 帧作为输入。</li>
<li>results: 研究发现，使用 multimodal 教师进行教学，可以使学生模型更加准确和更加靠谱。此外，本研究还提出了一种原则正的多modal 知识塑造框架，以解决多modal 知识塑造中出现的问题。最后，研究发现了计算复杂度的减少，并证明了我们的方法可以保持高性能，同时减少输入视图的数量。<details>
<summary>Abstract</summary>
The focal point of egocentric video understanding is modelling hand-object interactions. Standard models, e.g. CNNs or Vision Transformers, which receive RGB frames as input perform well. However, their performance improves further by employing additional input modalities that provide complementary cues, such as object detections, optical flow, audio, etc. The added complexity of the modality-specific modules, on the other hand, makes these models impractical for deployment. The goal of this work is to retain the performance of such a multimodal approach, while using only the RGB frames as input at inference time. We demonstrate that for egocentric action recognition on the Epic-Kitchens and the Something-Something datasets, students which are taught by multimodal teachers tend to be more accurate and better calibrated than architecturally equivalent models trained on ground truth labels in a unimodal or multimodal fashion. We further adopt a principled multimodal knowledge distillation framework, allowing us to deal with issues which occur when applying multimodal knowledge distillation in a naive manner. Lastly, we demonstrate the achieved reduction in computational complexity, and show that our approach maintains higher performance with the reduction of the number of input views. We release our code at https://github.com/gorjanradevski/multimodal-distillation.
</details>
<details>
<summary>摘要</summary>
主要焦点是模型手object交互的 egocentric视频理解。标准模型，如CNNs或视Transformers，使用RGB帧作为输入表现良好。然而，通过添加补充的模式特征信息，如物体检测、流动、音频等，其性能可以进一步提高。然而，这些模式特征模块的附加复杂性使得这些模型在部署时不实用。我们的目标是保留多模式approach的性能，使用只RGB帧作为输入进行推理。我们示出在Epic-Kitchens和Something-Something数据集上进行 egocentric动作识别 tasks，使用多模式教师进行教育的学生比architectureEquivalent模型在单模式或多模式教学情况下更准确和更好地规范。我们进一步采用了一种原则正的多模式知识储存框架，以解决在naive manner中应用多模式知识储存时出现的问题。最后，我们表明实现的计算复杂度减少，并示出我们的方法可以在输入视图数量减少的情况下保持更高的性能。我们在https://github.com/gorjanradevski/multimodal-distillation上发布了代码。
</details></li>
</ul>
<hr>
<h2 id="Dual-Query-Multiple-Instance-Learning-for-Dynamic-Meta-Embedding-based-Tumor-Classification"><a href="#Dual-Query-Multiple-Instance-Learning-for-Dynamic-Meta-Embedding-based-Tumor-Classification" class="headerlink" title="Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based Tumor Classification"></a>Dual-Query Multiple Instance Learning for Dynamic Meta-Embedding based Tumor Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07482">http://arxiv.org/abs/2307.07482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Holdenried-Krafft, Peter Somers, Ivonne A. Montes-Majarro, Diana Silimon, Cristina Tarín, Falko Fend, Hendrik P. A. Lensch</li>
<li>for: 这个论文主要针对的是肿瘤诊断和治疗规划中的整幕影像评估，以高Definition Magnification进行细胞分析。</li>
<li>methods: 该论文提出了一种基于嵌入模型的多例学习（MIL）管道，包括嵌入模型和聚合步骤。在嵌入模型方面，我们explore了使用最新的自我超vised预训练模型来提高MIL的普适性。在聚合步骤方面，我们提出了一种新的MIL架构，可以将MIL-注意力与相关自注意力结合使用。</li>
<li>results: 我们在三个 histopathological 数据集上进行了实验，并证明了我们的方法可以与当前状态艺技相比提高至多10%的性能。<details>
<summary>Abstract</summary>
Whole slide image (WSI) assessment is a challenging and crucial step in cancer diagnosis and treatment planning. WSIs require high magnifications to facilitate sub-cellular analysis. Precise annotations for patch- or even pixel-level classifications in the context of gigapixel WSIs are tedious to acquire and require domain experts. Coarse-grained labels, on the other hand, are easily accessible, which makes WSI classification an ideal use case for multiple instance learning (MIL). In our work, we propose a novel embedding-based Dual-Query MIL pipeline (DQ-MIL). We contribute to both the embedding and aggregation steps. Since all-purpose visual feature representations are not yet available, embedding models are currently limited in terms of generalizability. With our work, we explore the potential of dynamic meta-embedding based on cutting-edge self-supervised pre-trained models in the context of MIL. Moreover, we propose a new MIL architecture capable of combining MIL-attention with correlated self-attention. The Dual-Query Perceiver design of our approach allows us to leverage the concept of self-distillation and to combine the advantages of a small model in the context of a low data regime with the rich feature representation of a larger model. We demonstrate the superior performance of our approach on three histopathological datasets, where we show improvement of up to 10% over state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
整幕图像（WSI）评估是癌症诊断和治疗规划中的关键步骤。WSI需要高放大以便进行细胞分析。精确的标注对patch-或甚至像素级分类在 context of gigapixel WSIs 是繁琐的和需要域专家。相比之下，粗粒标注更加容易获得，这使得WSI分类成为多例学习（MIL）的理想应用场景。在我们的工作中，我们提出了一种新的嵌入基于的双Query MIL管道（DQ-MIL）。我们对嵌入和聚合步骤做出了贡献。由于目前的视觉特征表示Model 没有通用的ALL-PURPOSE，嵌入模型因此受限于通用性。我们通过在MILCONTEXT中使用 cutting-edge self-supervised pre-trained model 的动态元 embedding来探索这一点。此外，我们还提出了一种新的MIL架构，可以结合MIL注意力和相关自注意力。我们的approach 使用 Dual-Query Perceiver 的设计，可以利用自馈采集和小模型在低数据情况下的优势，同时保留大模型的丰富特征表示。我们在三个 histopathological 数据集上进行了实验，并证明了我们的方法在 state-of-the-art 方法之上提高了10%。
</details></li>
</ul>
<hr>
<h2 id="Atlas-Based-Interpretable-Age-Prediction"><a href="#Atlas-Based-Interpretable-Age-Prediction" class="headerlink" title="Atlas-Based Interpretable Age Prediction"></a>Atlas-Based Interpretable Age Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07439">http://arxiv.org/abs/2307.07439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie Starck, Yadunandan Vivekanand Kini, Jessica Johanna Maria Ritter, Rickmer Braren, Daniel Rueckert, Tamara Mueller</li>
<li>for: 这个研究旨在提高医疗评估和研究中的年龄预测精度，以检测疾病和异常年龄变化。</li>
<li>methods: 这种研究使用整体图像进行整个身体的研究，并使用Grad-CAM解释方法确定身体部位对年龄预测的影响。</li>
<li>results: 研究发现了三个关键的身体部位，即脊梁、自生肌肉和心脏区域，这三个部位对年龄预测具有最高的重要性。<details>
<summary>Abstract</summary>
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
</details>
<details>
<summary>摘要</summary>
生长预测是医学评估和研究中非常重要的一部分。它可以帮助检测疾病以及异常年龄，并且可以预测人体各部位的年龄变化。为了更全面地了解各部位年龄变化，我们使用整体图像进行研究。我们使用Grad-CAM可读性方法来确定人体各部位年龄预测最有价值的部位。此外，我们还使用注册技术来生成人类总体可读性图。此外，我们设置了全面的整体年龄预测模型，实现了年龄差值的平均绝对误差为2.76年。我们的发现显示了三个主要的关注方向：脊梁、自生肌肉和心脏区域，这三个方向具有最高的重要性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/15/cs.CV_2023_07_15/" data-id="clot2mhbu00f7x78884znh9q7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/15/cs.AI_2023_07_15/" class="article-date">
  <time datetime="2023-07-15T12:00:00.000Z" itemprop="datePublished">2023-07-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/15/cs.AI_2023_07_15/">cs.AI - 2023-07-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MixupExplainer-Generalizing-Explanations-for-Graph-Neural-Networks-with-Data-Augmentation"><a href="#MixupExplainer-Generalizing-Explanations-for-Graph-Neural-Networks-with-Data-Augmentation" class="headerlink" title="MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation"></a>MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07832">http://arxiv.org/abs/2307.07832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jz48/mixupexplainer">https://github.com/jz48/mixupexplainer</a></li>
<li>paper_authors: Jiaxing Zhang, Dongsheng Luo, Hua Wei</li>
<li>for: 本文旨在探讨Graph Neural Networks（GNN）的预测结果是否可解释，并提出一种基于Graph Information Bottleneck（GIB）的mixup方法来解决分布Shift问题。</li>
<li>methods: 本文提出了一种基于GIB的mixup方法，称为MixupExplainer，其具有理论保证，能够解决分布Shift问题。</li>
<li>results: 经过广泛的实验 validate MixupExplainer方法的效果，并提供了分布Shift问题的解决方案。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed mixup approach over existing approaches. We also provide a detailed analysis of how our proposed approach alleviates the distribution shifting issue.
</details>
<details>
<summary>摘要</summary>
图 нейрон网络（GNN）因其能够学习图结构数据而受到了越来越多的关注。然而，它们的预测结果通常不可解释。后续的实例级别解释方法已经被提出来了解 GNN 预测结果。这些方法寻找可以解释训练过的 GNN 预测行为的子结构。在这篇文章中，我们指出了现有方法中的分布转移问题，这会影响解释质量，特别是在实际数据集上面临着紧张的决策边界。为解决这个问题，我们引入一种通用的图信息瓶颈（GIB）形式，该形式包括一个独立于标签的图变量，与普通的 GIB 相同。驱动于通用 GIB，我们提出了一种图混合方法，叫做 MixupExplainer，具有解决分布转移问题的理论保证。我们在 synthetic 和实际数据集上进行了广泛的实验，证明了我们的提议的混合方法比现有方法更有效。我们还提供了对我们的提议方法如何缓解分布转移问题的详细分析。
</details></li>
</ul>
<hr>
<h2 id="text-EFO-k-CQA-Towards-Knowledge-Graph-Complex-Query-Answering-beyond-Set-Operation"><a href="#text-EFO-k-CQA-Towards-Knowledge-Graph-Complex-Query-Answering-beyond-Set-Operation" class="headerlink" title="$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation"></a>$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13701">http://arxiv.org/abs/2307.13701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/efok-cqa">https://github.com/hkust-knowcomp/efok-cqa</a></li>
<li>paper_authors: Hang Yin, Zihao Wang, Weizhi Fei, Yangqiu Song</li>
<li>for: 提供了一个框架，用于 Answering Existential First-order Queries with multiple variables（EFO），并评估这些方法在这个框架下的性能。</li>
<li>methods: 使用了学习基本的方法来掌握不完整的知识，以应对开放世界假设下的查询。</li>
<li>results: 建立了一个具有741种查询的数据集（EFO-CQA），并通过实验证明了这些查询的难度对于查询方法的影响。<details>
<summary>Abstract</summary>
To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\text{EFO}_{k}$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\text{EFO}_{k}$-CQA, with 741 types of query for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased that hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\url{https://github.com/HKUST-KnowComp/EFOK-CQA}.
</details>
<details>
<summary>摘要</summary>
“为了回答知识图中的复杂查询，因为开放世界假设，需要逻辑推理 sobre 未完整的知识。学习型方法是必要的，因为它们可以泛化到未观察到的知识。因此，一个合适的数据集是知识检索方法的基础和评估的重要组成部分。在这篇论文中，我们提出了一个完整的框架，包括数据生成、模型训练和方法评估，对多变量Existential First-order Queries（EFO）的 combinatorial 查询空间进行覆盖。我们的框架中的查询空间比现有文献中的set操作定义的更加广泛。此外，我们还构建了741种类型的查询集，并对其进行实验评估。我们的研究结果提供了新的思路，描述了查询难度如何影响结果。此外，我们还发现了现有数据集构建过程存在系统性的偏见，这阻碍了合适的查询回答方法的发展，高亮了我们的工作的重要性。我们的代码和数据可以在https://github.com/HKUST-KnowComp/EFOK-CQA中获取。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Improving-Trace-Link-Recommendation-by-Using-Non-Isotropic-Distances-and-Combinations"><a href="#Improving-Trace-Link-Recommendation-by-Using-Non-Isotropic-Distances-and-Combinations" class="headerlink" title="Improving Trace Link Recommendation by Using Non-Isotropic Distances and Combinations"></a>Improving Trace Link Recommendation by Using Non-Isotropic Distances and Combinations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07781">http://arxiv.org/abs/2307.07781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christof Tinnes</li>
<li>for: 本研究旨在提高软件开发、维护和运维中 Trace 链的效率，尤其是通过自动计算Trace链来减少人工干预。</li>
<li>methods: 本研究使用了自然语言处理工具来自动计算Trace链，并通过 geometric viewpoint on semantic similarity 来提高 Trace 链的准确率。</li>
<li>results: 研究在四个开源项目和两个企业项目上进行了实验，结果表明， geometric viewpoint on semantic similarity 可以帮助提高 Trace 链的准确率，并且这些发现可以用于其他信息检索问题。<details>
<summary>Abstract</summary>
The existence of trace links between artifacts of the software development life cycle can improve the efficiency of many activities during software development, maintenance and operations. Unfortunately, the creation and maintenance of trace links is time-consuming and error-prone. Research efforts have been spent to automatically compute trace links and lately gained momentum, e.g., due to the availability of powerful tools in the area of natural language processing. In this paper, we report on some observations that we made during studying non-linear similarity measures for computing trace links. We argue, that taking a geometric viewpoint on semantic similarity can be helpful for future traceability research. We evaluated our observations on a dataset of four open source projects and two industrial projects. We furthermore point out that our findings are more general and can build the basis for other information retrieval problems as well.
</details>
<details>
<summary>摘要</summary>
软件开发生命周期中的trace链可以提高软件开发、维护和运维的效率。然而，创建和维护trace链却是时间consuming和容易出错的。研究者们已经投入大量时间和精力来自动计算trace链，最近又得到了新的动力，例如自然语言处理领域的强大工具的出现。本文报告了我们在非线性相似度测量中所作出的观察，我们认为从 geometric 视角来看 semantic 相似度可以对未来的traceability研究提供帮助。我们对四个开源项目和两个industrial项目进行了评估，并指出我们的发现不仅限于traceability问题，还可以应用于其他信息检索问题。
</details></li>
</ul>
<hr>
<h2 id="Explaining-and-visualizing-black-box-models-through-counterfactual-paths"><a href="#Explaining-and-visualizing-black-box-models-through-counterfactual-paths" class="headerlink" title="Explaining and visualizing black-box models through counterfactual paths"></a>Explaining and visualizing black-box models through counterfactual paths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07764">http://arxiv.org/abs/2307.07764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pievos101/cpath">https://github.com/pievos101/cpath</a></li>
<li>paper_authors: Bastian Pfeifer, Mateusz Krzyzinski, Hubert Baniecki, Anna Saranti, Andreas Holzinger, Przemyslaw Biecek</li>
<li>for: 该论文旨在提出一种基于 conditional permutation 的 Explainable AI（XAI）方法，使黑盒模型变得透明和可解释。</li>
<li>methods: 该方法使用 conditional permutation 生成的 counterfactual paths，测量特征的重要性通过Sequential permutations of features 的影响对模型预测变化。</li>
<li>results: 实验表明，该方法可以准确地解释和视觉化黑盒模型，并在 synthetic 和医疗数据上得到了实际应用。<details>
<summary>Abstract</summary>
Explainable AI (XAI) is an increasingly important area of machine learning research, which aims to make black-box models transparent and interpretable. In this paper, we propose a novel approach to XAI that uses the so-called counterfactual paths generated by conditional permutations of features. The algorithm measures feature importance by identifying sequential permutations of features that most influence changes in model predictions. It is particularly suitable for generating explanations based on counterfactual paths in knowledge graphs incorporating domain knowledge. Counterfactual paths introduce an additional graph dimension to current XAI methods in both explaining and visualizing black-box models. Experiments with synthetic and medical data demonstrate the practical applicability of our approach.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用于机器学习研究的可解释AI（XAI）是一个日益重要的领域，旨在让黑盒模型变得透明和可解释。在这篇论文中，我们提出了一种新的XAI方法，使用叫做条件 permutation的feature counterfactual paths来衡量特征重要性。这个算法可以基于知识图 incorporating domain knowledge中的counterfactual paths来生成解释。counterfactual paths增加了现有XAI方法的一个新的维度，可以对黑盒模型的解释和可视化进行更好的支持。实验结果显示了我们的方法在synthetic和医疗数据上的实际可行性。Translation notes:* "可解释AI" (XAI) is translated as "可解释AI" (XAI), which is the standard term used in Simplified Chinese.* "黑盒模型" (black-box model) is translated as "黑盒模型" (black-box model), which is the standard term used in Simplified Chinese.* "counterfactual paths" is translated as "counterfactual paths" (counterfactual paths), which is the standard term used in Simplified Chinese.* "knowledge graph" is translated as "知识图" (knowledge graph), which is the standard term used in Simplified Chinese.* "domain knowledge" is translated as "领域知识" (domain knowledge), which is the standard term used in Simplified Chinese.I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Bidirectionally-Deformable-Motion-Modulation-For-Video-based-Human-Pose-Transfer"><a href="#Bidirectionally-Deformable-Motion-Modulation-For-Video-based-Human-Pose-Transfer" class="headerlink" title="Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer"></a>Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07754">http://arxiv.org/abs/2307.07754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rocketappslab/bdmm">https://github.com/rocketappslab/bdmm</a></li>
<li>paper_authors: Wing-Yin Yu, Lai-Man Po, Ray C. C. Cheung, Yuzhi Zhao, Yu Xue, Kun Li</li>
<li>For: 动作人体图像做pose转换，即将原始图像动作转换为目标人体pose中的动作。* Methods: 提出了一种新的弹性动作修饰（DMM）方法，通过几何核OFFSET和自适应重量调整来同时实现特征对Alignment和样式传递。* Results: 与现有方法相比，提出的方法可以更好地处理衣物上的复杂结构和不连续的姿势，并且可以更好地保持图像的稳定性和视觉连续性。<details>
<summary>Abstract</summary>
Video-based human pose transfer is a video-to-video generation task that animates a plain source human image based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods often generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that utilizes geometric kernel offset with adaptive weight modulation to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adaptively reconstructs smoothed frames from style codes according to the object shape through an irregular receptive field of view. To enhance the spatio-temporal consistency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The proposed feature propagation significantly enhances the motion prediction ability by forward and backward propagation. Both quantitative and qualitative experimental results demonstrate superiority over the state-of-the-arts in terms of image fidelity and visual continuity. The source code is publicly available at github.com/rocketappslab/bdmm.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified ChineseVideo-based human pose transfer是一种视频到视频生成任务，把平板的源人像图像基于一系列目标人 pose 动作。由于衣物上的结构很复杂，以及各种异常的姿势，现有方法通常会生成不满意的结果，如扭曲的 тексту涂抹和闪烁 artifacts。为解决这些问题，我们提出了一种新的减少动作模ulation（DMM）技术，利用几何kernel偏移以及适应加权修正来同时进行特征对齐和样式传递。与普通的样式修饰用于样式传递不同，我们的修饰机制可以根据对象形状自适应重建缓和frames从样式代码中。为了提高空间-时间一致性，我们利用双向传播来提取隐藏的运动信息从扭曲的图像序列，并且通过前向和后向传播来增强运动预测能力。实验结果表明，我们的特征传播方法可以明显提高图像准确性和视觉连续性，而且在量化和质量两个方面都超过了现有技术。源代码可以在github.com/rocketappslab/bdmm 中获取。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Learning-Expressive-Priors-for-Generalization-and-Uncertainty-Estimation-in-Neural-Networks"><a href="#Learning-Expressive-Priors-for-Generalization-and-Uncertainty-Estimation-in-Neural-Networks" class="headerlink" title="Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks"></a>Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07753">http://arxiv.org/abs/2307.07753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dlr-rm/bpnn">https://github.com/dlr-rm/bpnn</a></li>
<li>paper_authors: Dominik Schnaus, Jongseok Lee, Daniel Cremers, Rudolph Triebel</li>
<li>for: 提高深度神经网络的通用化和不确定性估计</li>
<li>methods: 利用可扩展的结构 posteriors 作为帮助神经网络通用化和不确定性估计的快速学习方法，并提供可靠的泛化证明</li>
<li>results: 实验表明，该方法可以有效地提高神经网络的不确定性估计和通用化性，并且在不断学习框架中实现了良好的性能<details>
<summary>Abstract</summary>
In this work, we propose a novel prior learning method for advancing generalization and uncertainty estimation in deep neural networks. The key idea is to exploit scalable and structured posteriors of neural networks as informative priors with generalization guarantees. Our learned priors provide expressive probabilistic representations at large scale, like Bayesian counterparts of pre-trained models on ImageNet, and further produce non-vacuous generalization bounds. We also extend this idea to a continual learning framework, where the favorable properties of our priors are desirable. Major enablers are our technical contributions: (1) the sums-of-Kronecker-product computations, and (2) the derivations and optimizations of tractable objectives that lead to improved generalization bounds. Empirically, we exhaustively show the effectiveness of this method for uncertainty estimation and generalization.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的先学习方法，用于提高深度神经网络的通用化和不确定性估计。关键思想是利用可扩展和结构化的神经网络 posterior 作为有效的先学习模型，具有通用化保证。我们学习的先学习模型可以在大规模上提供表达性的概率表示，类似于 bayesian 对 ImageNet 预训练模型的counterpart，并且生成非虚无效的通用化误差 bound。我们还将这个想法扩展到 continual learning 框架中，其中我们的先学习模型具有恰当的特性。主要实现方法包括：(1)  kronecker 乘积计算，以及对这些计算的Derivation和优化，以实现改进的通用化误差 bound。我们在实验中证明了这种方法的有效性，用于 uncertainty estimation 和通用化。
</details></li>
</ul>
<hr>
<h2 id="Combining-model-predictive-control-and-predictive-reinforcement-learning-for-stable-quadrupedal-robot-locomotion"><a href="#Combining-model-predictive-control-and-predictive-reinforcement-learning-for-stable-quadrupedal-robot-locomotion" class="headerlink" title="Combining model-predictive control and predictive reinforcement learning for stable quadrupedal robot locomotion"></a>Combining model-predictive control and predictive reinforcement learning for stable quadrupedal robot locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07752">http://arxiv.org/abs/2307.07752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vyacheslav Kovalev, Anna Shkromada, Henni Ouerdane, Pavel Osinenko</li>
<li>For: 这篇论文旨在研究如何通过模型预测和预测学习控制器来获得四肢机器人稳定的步行。* Methods: 本文使用了模型预测控制（MPC）和预测学习（RL）两种控制方法来解决四肢机器人稳定步行问题。MPC是一种已知的控制方法，但是它不使用线上学习，只有一些适应型的变化。RL则是一种基于体验的学习方法，但是在高复杂的机器人中可能不太适用。本文的混合方法结合了MPC和RL，使用了成本滚动算法和一个对应的Q函数预测器，以缓解MPC的计算复杂性。* Results: 本文的实验结果显示，使用了混合控制的四肢机器人可以在短时间内获得稳定的步行，而nominal MP控制器则在较长时间内失败。此外，本文的控制器不需要前期训练，可以进行现场操作。结果显示，混合MPC和RL的控制方法可以实现四肢机器人稳定步行的平衡。<details>
<summary>Abstract</summary>
Stable gait generation is a crucial problem for legged robot locomotion as this impacts other critical performance factors such as, e.g. mobility over an uneven terrain and power consumption. Gait generation stability results from the efficient control of the interaction between the legged robot's body and the environment where it moves. Here, we study how this can be achieved by a combination of model-predictive and predictive reinforcement learning controllers. Model-predictive control (MPC) is a well-established method that does not utilize any online learning (except for some adaptive variations) as it provides a convenient interface for state constraints management. Reinforcement learning (RL), in contrast, relies on adaptation based on pure experience. In its bare-bone variants, RL is not always suitable for robots due to their high complexity and expensive simulation/experimentation. In this work, we combine both control methods to address the quadrupedal robot stable gate generation problem. The hybrid approach that we develop and apply uses a cost roll-out algorithm with a tail cost in the form of a Q-function modeled by a neural network; this allows to alleviate the computational complexity, which grows exponentially with the prediction horizon in a purely MPC approach. We demonstrate that our RL gait controller achieves stable locomotion at short horizons, where a nominal MP controller fails. Further, our controller is capable of live operation, meaning that it does not require previous training. Our results suggest that the hybridization of MPC with RL, as presented here, is beneficial to achieve a good balance between online control capabilities and computational complexity.
</details>
<details>
<summary>摘要</summary>
稳定步态生成是四肢机器人行走中的关键问题，这会影响其他重要性能因素，如覆盖不平地形和功率消耗。稳定步态生成的稳定性来自四肢机器人身体和运动环境之间的有效控制。在这里，我们研究如何通过组合模型预测和预测学习控制器来实现稳定步态生成。模型预测控制（MPC）是一种已知的方法，不使用线上学习（除了一些适应变化），它提供了一个方便的状态约束管理界面。学习控制（RL），相比之下，基于经验学习，不适用于机器人，因为它们的复杂性和临界实验/仿真成本高。在这个工作中，我们将这两种控制方法结合使用，以解决四肢机器人稳定步态生成问题。我们开发的混合方法使用一个成本滚动算法，其中的尾成本是一个模拟网络模型的Q函数；这使得计算复杂性减少，在完全MPC方法中计算复杂性呈指数增长的情况下。我们的RL步态控制器可以在短预测时间内实现稳定行走，而一个准确MP控制器则无法实现。此外，我们的控制器可以进行实时操作，不需要先期训练。我们的结果表明，在MPC和RL之间的混合，如我们所提出的，可以实现一个良好的平衡，以提高在线控制能力和计算复杂性。
</details></li>
</ul>
<hr>
<h2 id="SINC-Self-Supervised-In-Context-Learning-for-Vision-Language-Tasks"><a href="#SINC-Self-Supervised-In-Context-Learning-for-Vision-Language-Tasks" class="headerlink" title="SINC: Self-Supervised In-Context Learning for Vision-Language Tasks"></a>SINC: Self-Supervised In-Context Learning for Vision-Language Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07742">http://arxiv.org/abs/2307.07742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi-Syuan Chen, Yun-Zhu Song, Cheng Yu Yeo, Bei Liu, Jianlong Fu, Hong-Han Shuai<br>for: 这个论文的目的是探讨如何实现无 gradient 学习的情况下，大型 transformer 模型在视觉语言领域中进行增Context 学习。methods: 这个论文使用的方法是引入视觉信息到大型语言模型中，以便在输入上进行增Context 预测。results: 实验结果表明，SINC 方法在多种视觉语言任务下，在几个shot Setting 下表现出色，而且可以在实时进行增Context 预测。此外，SINC 方法的设计也帮助我们了解视觉语言领域中增Context 学习的好处，以及这种学习方式在不同任务中的发展。<details>
<summary>Abstract</summary>
Large Pre-trained Transformers exhibit an intriguing capacity for in-context learning. Without gradient updates, these models can rapidly construct new predictors from demonstrations presented in the inputs. Recent works promote this ability in the vision-language domain by incorporating visual information into large language models that can already make in-context predictions. However, these methods could inherit issues in the language domain, such as template sensitivity and hallucination. Also, the scale of these language models raises a significant demand for computations, making learning and operating these models resource-intensive. To this end, we raise a question: ``How can we enable in-context learning without relying on the intrinsic in-context ability of large language models?". To answer it, we propose a succinct and general framework, Self-supervised IN-Context learning (SINC), that introduces a meta-model to learn on self-supervised prompts consisting of tailored demonstrations. The learned models can be transferred to downstream tasks for making in-context predictions on-the-fly. Extensive experiments show that SINC outperforms gradient-based methods in various vision-language tasks under few-shot settings. Furthermore, the designs of SINC help us investigate the benefits of in-context learning across different tasks, and the analysis further reveals the essential components for the emergence of in-context learning in the vision-language domain.
</details>
<details>
<summary>摘要</summary>
大型预训Transformer显示了有趣的 Context Learning能力。无需梯度更新，这些模型可快速从输入中提取示例构建新预测器。最近的工作在视觉语言领域把视觉信息 integrate到可以在输入中进行预测的大语言模型中，以提高Context Learning能力。然而，这些方法可能会继承语言领域的问题，如模板敏感和幻觉。此外，这些语言模型的大规模需要巨量的计算资源，使学习和运行这些模型成为资源占用。因此，我们提出了问题：“如何启用Context Learning无需大语言模型的内在能力？”为回答这个问题，我们提出了一种简洁且通用的框架，Self-supervised IN-Context learning（SINC）。SINC引入了一个元模型，用于在自我超visuelle示例上学习。学习后，这些模型可以被转移到下游任务中进行实时预测。广泛的实验显示，SINC在视觉语言任务下的几个shot设定下表现出色，超过了梯度更新方法。此外，SINC的设计帮助我们调查在不同任务中Context Learning的好处，并且分析还揭示了视觉语言领域Context Learning的发展的关键组成部分。
</details></li>
</ul>
<hr>
<h2 id="Intuitive-Access-to-Smartphone-Settings-Using-Relevance-Model-Trained-by-Contrastive-Learning"><a href="#Intuitive-Access-to-Smartphone-Settings-Using-Relevance-Model-Trained-by-Contrastive-Learning" class="headerlink" title="Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning"></a>Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09177">http://arxiv.org/abs/2307.09177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joonyoung Kim, Kangwook Lee, Haebin Shin, Hurnjoo Lee, Sechun Kang, Byunguk Choi, Dong Shin, Joohyung Lee<br>for: 该论文 targets 智能手机中的功能搜索问题，即用户难以找到功能的问题。methods: 该论文提出了一种新的搜索系统，使用了对比学习来训练一个拥有Contextual relevance的相关性模型，以及应用了知识填充来压缩模型，使其在设备上运行高效。results: 测试结果显示，该系统在 contextual sentence 查询和 usual keyword-based 查询中表现出色，超过了现有的搜索基准。<details>
<summary>Abstract</summary>
The more new features that are being added to smartphones, the harder it becomes for users to find them. This is because the feature names are usually short, and there are just too many to remember. In such a case, the users may want to ask contextual queries that describe the features they are looking for, but the standard term frequency-based search cannot process them. This paper presents a novel retrieval system for mobile features that accepts intuitive and contextual search queries. We trained a relevance model via contrastive learning from a pre-trained language model to perceive the contextual relevance between query embeddings and indexed mobile features. Also, to make it run efficiently on-device using minimal resources, we applied knowledge distillation to compress the model without degrading much performance. To verify the feasibility of our method, we collected test queries and conducted comparative experiments with the currently deployed search baselines. The results show that our system outperforms the others on contextual sentence queries and even on usual keyword-based queries.
</details>
<details>
<summary>摘要</summary>
随着智能手机中新增功能的数量的增加，用户找到这些功能越来越Difficult.这是因为功能名称通常很短，而且有太多了，用户可能会想要提问 Contextual queries 描述所需的功能，但标准的 terme frequency-based search 系统无法处理这些查询。本文介绍了一种新的手机功能检索系统，该系统可以接受用户提出的Intuitive和Contextual search queries。我们通过对预先训练的语言模型进行对比学习来训练一个 relevance 模型，以便在查询embeddings中感知Contextual relevance。此外，为了在设备上运行效率高并使用 minimal resources，我们应用了知识填充技术来压缩模型。为了证明我们的方法的可行性，我们收集了测试查询并进行了相对 эксперименты。结果表明，我们的系统在Contextual sentence queries 和一般键 palab 查询中都高于其他基elines。
</details></li>
</ul>
<hr>
<h2 id="Safe-Formulas-in-the-General-Theory-of-Stable-Models"><a href="#Safe-Formulas-in-the-General-Theory-of-Stable-Models" class="headerlink" title="Safe Formulas in the General Theory of Stable Models"></a>Safe Formulas in the General Theory of Stable Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09166">http://arxiv.org/abs/2307.09166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joohyung Lee, Vladimir Lifschitz, Ravi Palla</li>
<li>for: 本研究探讨了安全首项公式的概念，它们可以视为答案集解释器的设计中的一个重要组成部分。</li>
<li>methods: 本研究使用了安全句子的概念，并证明任何安全句子都等价于其归grounding的结果 – 即将所有量词替换为多重 conjunctions 和 disjunctions 后得到的变量自由句子。</li>
<li>results: 根据本研究结果，安全句子和其归grounding结果具有相同的稳定模型，而稳定模型的描述可以用一种简单的语法形式来表示。<details>
<summary>Abstract</summary>
Safe first-order formulas generalize the concept of a safe rule, which plays an important role in the design of answer set solvers. We show that any safe sentence is equivalent, in a certain sense, to the result of its grounding -- to the variable-free sentence obtained from it by replacing all quantifiers with multiple conjunctions and disjunctions. It follows that a safe sentence and the result of its grounding have the same stable models, and that the stable models of a safe sentence can be characterized by a formula of a simple syntactic form.
</details>
<details>
<summary>摘要</summary>
安全的首项式式表示安全规则的概念，这种概念在答案集解决器的设计中发挥着重要作用。我们证明任何安全句子都等价于其归根结构 -- 将它中的全量化器替换为多个并 conjunctions 和 disjunctions 后得到的变量自由句子。因此，安全句子和它的归根结构具有相同的稳定模型，并且安全句子的稳定模型可以通过一个简单的语法结构来描述。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Perceived-Trust-in-XAI-Assisted-Decision-Making-by-Eliciting-a-Mental-Model"><a href="#Measuring-Perceived-Trust-in-XAI-Assisted-Decision-Making-by-Eliciting-a-Mental-Model" class="headerlink" title="Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model"></a>Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11765">http://arxiv.org/abs/2307.11765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Abbaspour Onari, Isel Grau, Marco S. Nobile, Yingqian Zhang</li>
<li>for: 本研究用了一种新的方法来测量用户对可解释人工智能（XAI）模型的信任感。</li>
<li>methods: 这种方法利用了可解释机器学习（ML）模型来分类可能患有 COVID-19 的病人为正或负。然后，医疗专家（ME）根据他们的知识和 XAI 模型的预测和解释进行诊断决策任务。</li>
<li>results: 研究发现，对于每个 ME，可以获得一个量化的信任值，以确定他们对 XAI 模型的信任程度。这些量化值可以判断 ME 是否对 XAI 模型有信任或不信任。此外，研究还发现，MEs 的心理主观性会影响他们对 XAI 模型的信任程度。<details>
<summary>Abstract</summary>
This empirical study proposes a novel methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model. To do so, users' mental models are elicited using Fuzzy Cognitive Maps (FCMs). First, we exploit an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients into positive or negative cases. Then, Medical Experts' (MEs) conduct a diagnostic decision-making task based on their knowledge and then prediction and interpretations provided by the XAI model. In order to evaluate the impact of interpretations on perceived trust, explanation satisfaction attributes are rated by MEs through a survey. Then, they are considered as FCM's concepts to determine their influences on each other and, ultimately, on the perceived trust. Moreover, to consider MEs' mental subjectivity, fuzzy linguistic variables are used to determine the strength of influences. After reaching the steady state of FCMs, a quantified value is obtained to measure the perceived trust of each ME. The results show that the quantified values can determine whether MEs trust or distrust the XAI model. We analyze this behavior by comparing the quantified values with MEs' performance in completing diagnostic tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Elementary-Sets-for-Logic-Programs"><a href="#Elementary-Sets-for-Logic-Programs" class="headerlink" title="Elementary Sets for Logic Programs"></a>Elementary Sets for Logic Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09168">http://arxiv.org/abs/2307.09168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Martin Gebser, Joohyung Lee, Yuliya Lierler</li>
<li>for: 本文研究了非逻辑程序的答案集和 Clark 完善的模型之间的关系。</li>
<li>methods: 本文使用了林和赵的定理，以及 Gebser 和 Schaub 的 restrict loop 方法。</li>
<li>results: 本文提出了一种更加简单和普适的 elementary set 概念，并证明了其与非逻辑程序相关的最大不充足 elementary set 是非逻辑程序的所有非空不充足集之最小集。此外，本文还提供了一种图理学方法来Characterize elementary sets for nondisjunctive programs。在 contrast to nondisjunctive programs, 本文显示了对于分配程序，确定 elementary set 是 coNP-complete。<details>
<summary>Abstract</summary>
By introducing the concepts of a loop and a loop formula, Lin and Zhao showed that the answer sets of a nondisjunctive logic program are exactly the models of its Clark's completion that satisfy the loop formulas of all loops. Recently, Gebser and Schaub showed that the Lin-Zhao theorem remains correct even if we restrict loop formulas to a special class of loops called ``elementary loops.'' In this paper, we simplify and generalize the notion of an elementary loop, and clarify its role. We propose the notion of an elementary set, which is almost equivalent to the notion of an elementary loop for nondisjunctive programs, but is simpler, and, unlike elementary loops, can be extended to disjunctive programs without producing unintuitive results. We show that the maximal unfounded elementary sets for the ``relevant'' part of a program are exactly the minimal sets among the nonempty unfounded sets. We also present a graph-theoretic characterization of elementary sets for nondisjunctive programs, which is simpler than the one proposed in (Gebser & Schaub 2005). Unlike the case of nondisjunctive programs, we show that the problem of deciding an elementary set is coNP-complete for disjunctive programs.
</details>
<details>
<summary>摘要</summary>
林和赵通过引入循环和循环公式，表明答案集合的非逻辑程序是完全 Clark 完成的模型，满足所有循环公式的循环。最近，格卜和瑞布 показа了林-赵定理仍然正确，只要限制循环公式为特殊类循环 called “元素循环”。在这篇文章中，我们简化和推广元素循环的概念，并解释其作用。我们提出了元素集的概念，它与元素循环对非逻辑程序几乎等价，但更简单，而且与元素循环不同的是可以扩展到分支程序无需生成不自然的结果。我们证明了最大不定元素集的“相关”部分的程序是非空不定集中的最小集。我们还提出了非逻辑程序的元素集的图学特征化，这比 Gebser 和 Schaub （2005）提出的特征化更简单。不同于非逻辑程序，我们证明了决定元素集的问题是 coNP-完全的 для分支程序。
</details></li>
</ul>
<hr>
<h2 id="Abstracting-Concept-Changing-Rules-for-Solving-Raven’s-Progressive-Matrix-Problems"><a href="#Abstracting-Concept-Changing-Rules-for-Solving-Raven’s-Progressive-Matrix-Problems" class="headerlink" title="Abstracting Concept-Changing Rules for Solving Raven’s Progressive Matrix Problems"></a>Abstracting Concept-Changing Rules for Solving Raven’s Progressive Matrix Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07734">http://arxiv.org/abs/2307.07734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fudanvi/generative-abstract-reasoning">https://github.com/fudanvi/generative-abstract-reasoning</a></li>
<li>paper_authors: Fan Shi, Bin Li, Xiangyang Xue</li>
<li>for: 这种研究旨在提高机器智能的抽象能力，以便在新环境中发现底层规则。</li>
<li>methods: 这种方法使用了Raven’s Progressive Matrix（RPM）测试，并使用深度隐藏变量模型来抽象概念改变规则。</li>
<li>results: 这种方法可以自动抽象全局规则，并且在无外部监督下达到了同级或更高的准确率。<details>
<summary>Abstract</summary>
The abstract visual reasoning ability in human intelligence benefits discovering underlying rules in the novel environment. Raven's Progressive Matrix (RPM) is a classic test to realize such ability in machine intelligence by selecting from candidates. Recent studies suggest that solving RPM in an answer-generation way boosts a more in-depth understanding of rules. However, existing generative solvers cannot discover the global concept-changing rules without auxiliary supervision (e.g., rule annotations and distractors in candidate sets). To this end, we propose a deep latent variable model for Concept-changing Rule ABstraction (CRAB) by learning interpretable concepts and parsing concept-changing rules in the latent space. With the iterative learning process, CRAB can automatically abstract global rules shared on the dataset on each concept and form the learnable prior knowledge of global rules. CRAB outperforms the baselines trained without auxiliary supervision in the arbitrary-position answer generation task and achieves comparable and even higher accuracy than the compared models trained with auxiliary supervision. Finally, we conduct experiments to illustrate the interpretability of CRAB in concept learning, answer selection, and global rule abstraction.
</details>
<details>
<summary>摘要</summary>
人类智能中的抽象视觉理解能力可以帮助发现新环境中的底层规则。Raven's Progressive Matrix (RPM) 是一种经典的测试，用于评测机器智能中的这种能力。Recent studies 表明，通过answer-generation的方式解决RPM可以提高对规则的更深入的理解。然而，现有的生成解决方案无法自动发现全局概念变化的规则，需要 auxiliary supervision（例如，规则注释和distractors在候选集中）。为此，我们提出了一种深入学习的秘密变量模型，即Concept-changing Rule ABstraction (CRAB)，可以学习可读的概念和解析概念变化规则在隐藏空间中。通过迭代学习过程，CRAB可以自动抽象数据集中的全局规则，并将这些规则形成可学习的先验知识。CRAB在无auxiliary supervision的arbitrary-position answer generation任务中表现出色，并与包括auxiliary supervision的比较模型相比，达到了相同或更高的准确率。最后，我们进行了实验，以示CRAB在概念学习、答案选择和全局规则抽象方面的解释性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Laws-and-Multi-Valued-Fluents"><a href="#Causal-Laws-and-Multi-Valued-Fluents" class="headerlink" title="Causal Laws and Multi-Valued Fluents"></a>Causal Laws and Multi-Valued Fluents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10227">http://arxiv.org/abs/2307.10227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Enrico Giunchiglia, Joohyung Lee, Vladimir Lifschitz, Hudson Turner</li>
<li>for: 本研究继续 investigate 非 monotonic  formalism 表示行为属性的工作，特别是 отлича между “真” 和 “被引起”，如 McCain 和 Turner 提出的 causal logic 和 Giunchiglia 和 Lifschitz 提出的 action language C。</li>
<li>methods: 本文使用 extension 方法，使得 language C+ 可以表示非空集值。此外，本文还描述了 actions 的 attribute 的描述，这对 elaboration tolerance 非常重要。</li>
<li>results: 本文显示了 causal theories 中 multi-valued constants 的 eliminating，并将 C+ 与 Pednault 提出的 action language ADL 进行比较。<details>
<summary>Abstract</summary>
This paper continues the line of work on representing properties of actions in nonmonotonic formalisms that stresses the distinction between being "true" and being "caused", as in the system of causal logic introduced by McCain and Turner and in the action language C proposed by Giunchiglia and Lifschitz. The only fluents directly representable in language C+ are truth-valued fluents, which is often inconvenient. We show that both causal logic and language C can be extended to allow values from arbitrary nonempty sets. Our extension of language C, called C+, also makes it possible to describe actions in terms of their attributes, which is important from the perspective of elaboration tolerance. We describe an embedding of C+ in causal theories with multi-valued constants, relate C+ to Pednault's action language ADL, and show how multi-valued constants can be eliminated in favor of Boolean constants.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-Loop-Formulas-with-Variables"><a href="#On-Loop-Formulas-with-Variables" class="headerlink" title="On Loop Formulas with Variables"></a>On Loop Formulas with Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10226">http://arxiv.org/abs/2307.10226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SOYJUN/FTP-implement-based-on-UDP">https://github.com/SOYJUN/FTP-implement-based-on-UDP</a></li>
<li>paper_authors: Joohyung Lee, Yunsong Meng</li>
<li>for: 这个论文是为了推广 Ferraris et al. 的稳定模型定义，不再基于固定点，可应用于任意首选论 sentences 的 syntax。</li>
<li>methods: 这篇论文使用 Chen, Lin, Wang, Zhang 的 loop formulas with variables，并将其推广到分支计划和任意首选论 sentences。它还扩展了逻辑计划的语法，允许显式Quantifier，并定义其 semantics 为 Ferraris et al. 的稳定模型语言的一个 subclass。</li>
<li>results: 这篇论文显示了这种扩展的逻辑计划可以在稳定模型 semantics 下进行非 monotonic reasoning，而且在不假设唯一名称和Domain closure 的情况下仍然能够处理非 Herbrand 稳定模型。此外，它还显示了一些语法条件，使得查询答案可以通过 first-order 逻辑推理来实现，从而可以使用 first-order 证明器进行非 Herbrand 稳定模型的推理。<details>
<summary>Abstract</summary>
Recently Ferraris, Lee and Lifschitz proposed a new definition of stable models that does not refer to grounding, which applies to the syntax of arbitrary first-order sentences. We show its relation to the idea of loop formulas with variables by Chen, Lin, Wang and Zhang, and generalize their loop formulas to disjunctive programs and to arbitrary first-order sentences. We also extend the syntax of logic programs to allow explicit quantifiers, and define its semantics as a subclass of the new language of stable models by Ferraris et al. Such programs inherit from the general language the ability to handle nonmonotonic reasoning under the stable model semantics even in the absence of the unique name and the domain closure assumptions, while yielding more succinct loop formulas than the general language due to the restricted syntax. We also show certain syntactic conditions under which query answering for an extended program can be reduced to entailment checking in first-order logic, providing a way to apply first-order theorem provers to reasoning about non-Herbrand stable models.
</details>
<details>
<summary>摘要</summary>
最近，菲律数、李和里夫斯提出了一个新的定义方式，不受地面影响，可以应用于任意首项关系文法中的语法。我们展示了这个定义与陈等人的循环式关系式的联系，并将其扩展到分类程式和任意首项关系文法中。我们还将逻辑程式的 syntax 扩展为允许显式量词，并定义其 semantics 为一个基于新的稳定模型语言的子集。这些程式继承了稳定模型语言中的非对称逻辑推理能力，但是具有更短的循环式关系式，因为其 restrictive syntax。我们还展示了一些语法条件，使得问题回答可以与首项关系逻辑推理相同，并且可以运用首项关系逻辑推理器进行非HERBRAND稳定模型的推理。
</details></li>
</ul>
<hr>
<h2 id="First-Order-Stable-Model-Semantics-with-Intensional-Functions"><a href="#First-Order-Stable-Model-Semantics-with-Intensional-Functions" class="headerlink" title="First-Order Stable Model Semantics with Intensional Functions"></a>First-Order Stable Model Semantics with Intensional Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10225">http://arxiv.org/abs/2307.10225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Bartholomew, Joohyung Lee</li>
<li>for: 该论文旨在扩展answer set programming（ASP）中的函数支持，以便在ASP中执行first-order reasoning。</li>
<li>methods: 该论文使用了 Ferraris、Lee、Lifschitz的first-order stable model semantics，并将函数与前置定义的 predicate 一样地处理。</li>
<li>results: 该论文显示了多种已知的ASP性质可以自然地扩展到该形式中，并与其他相关的方法进行比较。此外，该论文还基于这种扩展定义了Answer Set Programming Modulo Theories（ASPMT），可以在含有实数的领域中进行有效的first-orderreasoning。<details>
<summary>Abstract</summary>
In classical logic, nonBoolean fluents, such as the location of an object, can be naturally described by functions. However, this is not the case in answer set programs, where the values of functions are pre-defined, and nonmonotonicity of the semantics is related to minimizing the extents of predicates but has nothing to do with functions. We extend the first-order stable model semantics by Ferraris, Lee, and Lifschitz to allow intensional functions -- functions that are specified by a logic program just like predicates are specified. We show that many known properties of the stable model semantics are naturally extended to this formalism and compare it with other related approaches to incorporating intensional functions. Furthermore, we use this extension as a basis for defining Answer Set Programming Modulo Theories (ASPMT), analogous to the way that Satisfiability Modulo Theories (SMT) is defined, allowing for SMT-like effective first-order reasoning in the context of ASP. Using SMT solving techniques involving functions, ASPMT can be applied to domains containing real numbers and alleviates the grounding problem. We show that other approaches to integrating ASP and CSP/SMT can be related to special cases of ASPMT in which functions are limited to non-intensional ones.
</details>
<details>
<summary>摘要</summary>
在经典逻辑中，非布尔流变量，如物体的位置，可以自然地被函数描述。然而，在答案集程序中，函数的值是预先定义的，并且非 monotonicity 的 semantics 与函数没有直接关系。我们将 Ferraris、Lee 和 Lifschitz 的第一阶stable model semantics 扩展以允许内在函数 -- 函数被逻辑程序所定义，与 predicates 一样。我们表明了许多已知的 stable model semantics 的属性被自然地扩展到这种 формалиズмом，并与其他相关的方法进行比较。此外，我们使用这种扩展为基础，定义了 Answer Set Programming Modulo Theories（ASPMT），类似于 Satisfiability Modulo Theories（SMT）的定义，允许在 ASP 中进行有效的第一阶逻辑推理。使用 SMT 解决方法 involving functions，ASPMT 可以应用于含有实数的Domain中，并alleviate the grounding problem。我们还证明了其他将 ASP 和 CSP/SMT 集成的方法可以被看作 ASPMT 中函数的特殊情况。
</details></li>
</ul>
<hr>
<h2 id="RL-ViGen-A-Reinforcement-Learning-Benchmark-for-Visual-Generalization"><a href="#RL-ViGen-A-Reinforcement-Learning-Benchmark-for-Visual-Generalization" class="headerlink" title="RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization"></a>RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10224">http://arxiv.org/abs/2307.10224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gemcollector/rl-vigen">https://github.com/gemcollector/rl-vigen</a></li>
<li>paper_authors: Zhecheng Yuan, Sizhe Yang, Pu Hua, Can Chang, Kaizhe Hu, Xiaolong Wang, Huazhe Xu<br>for:* 这篇论文旨在解决视觉学习中的扩展性问题，即RL Agent在不同任务和扩展类别下的扩展性能力的评估。methods:* 本论文提出了RL-ViGen，一个新的视觉学习评价 benchmark，其包含了多种任务和扩展类别，以便更好地评估RL Agent的扩展性能力。results:* 实验结果表明，现有的视觉RL算法中没有一个 universally 适用于所有任务，RL-ViGen 可以作为一个 catalyst，促进未来创造出适用于实际场景的通用视觉RL Agent。<details>
<summary>Abstract</summary>
Visual Reinforcement Learning (Visual RL), coupled with high-dimensional observations, has consistently confronted the long-standing challenge of out-of-distribution generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel Reinforcement Learning Benchmark for Visual Generalization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that RL-ViGen will serve as a catalyst in this area, and lay a foundation for the future creation of universal visual generalization RL agents suitable for real-world scenarios. Access to our code and implemented algorithms is provided at https://gemcollector.github.io/RL-ViGen/.
</details>
<details>
<summary>摘要</summary>
visual reinforcement learning (Visual RL)  coupled with high-dimensional observations, has consistently confronted the long-standing challenge of out-of-distribution generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel Reinforcement Learning Benchmark for Visual Generalization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that RL-ViGen will serve as a catalyst in this area, and lay a foundation for the future creation of universal visual generalization RL agents suitable for real-world scenarios. Access to our code and implemented algorithms is provided at https://gemcollector.github.io/RL-ViGen/.Here's the word-for-word translation of the text into Simplified Chinese:视觉强化学习（Visual RL），结合高维度观察，一直面临着 OUT-OF-distribution 泛化挑战。尽管关注在解决视觉泛化问题上的算法，但我们认为存在的 benchmarks 是隔离任务和泛化类别的，这会妨碍对代理人的视觉泛化能力进行全面评估。为了bridging这个差距，我们介绍 RL-ViGen：一个新的强化学习 benchmark  для视觉泛化，包含多种任务和广泛的泛化类型，从而促进更可靠的结论。此外， RL-ViGen 还将 latest visual RL 泛化算法集成到一个统一的框架中，实验结果表明，无论任务，任何一个现有算法都没有在所有任务上 universal 适用。我们希望 RL-ViGen 能成为这一领域的 catalyst，并为实际场景中的 universal 视觉泛化 RL 代理人提供基础。可以在https://gemcollector.github.io/RL-ViGen/ 获取我们的代码和实现算法。
</details></li>
</ul>
<hr>
<h2 id="NeurASP-Embracing-Neural-Networks-into-Answer-Set-Programming"><a href="#NeurASP-Embracing-Neural-Networks-into-Answer-Set-Programming" class="headerlink" title="NeurASP: Embracing Neural Networks into Answer Set Programming"></a>NeurASP: Embracing Neural Networks into Answer Set Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07700">http://arxiv.org/abs/2307.07700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhun Yang, Adam Ishay, Joohyung Lee</li>
<li>for: 用于结合符号计算和低水平计算的简单扩展</li>
<li>methods: 使用神经网络输出作为答案集计算中的概率分布</li>
<li>results: 可以使用预训练神经网络进行符号计算，并通过应用符号逻辑来改善神经网络的感知结果，同时可以通过训练ASP规则来使神经网络更好地学习。<details>
<summary>Abstract</summary>
We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can be used to train a neural network better by training with ASP rules so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.
</details>
<details>
<summary>摘要</summary>
我团队今天宣布了一个简单的扩展项目，即NeurASP，它通过将神经网络输出视为答案集程序中的概率分布来实现。通过将子符号计算和符号计算结合起来，NeurASP提供了一个简单有效的方式来整合子符号计算和符号计算。我们展示了如何使用预训练神经网络进行符号计算，以及如何通过应用答案集程序的 символиック逻辑来改进神经网络的识别结果。此外，NeurASP还可以用来训练神经网络，使其不仅从数据中学习隐式相关性，还从答案集程序中表达的复杂 semantic constraints中学习明确的符号逻辑。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-to-Generate-Answer-Set-Programs"><a href="#Leveraging-Large-Language-Models-to-Generate-Answer-Set-Programs" class="headerlink" title="Leveraging Large Language Models to Generate Answer Set Programs"></a>Leveraging Large Language Models to Generate Answer Set Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07699">http://arxiv.org/abs/2307.07699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azreasoners/gpt-asp-rules">https://github.com/azreasoners/gpt-asp-rules</a></li>
<li>paper_authors: Adam Ishay, Zhun Yang, Joohyung Lee</li>
<li>for: 该论文旨在探讨大型自然语言处理模型（LLM）如何帮助创建答案集程序（Answer Set Program），以便解决复杂的逻辑问题。</li>
<li>methods: 该论文提出了一种 neuromorphic 方法，即使用 LLM 将自然语言描述转换为答案集程序。该方法首先使用 LLM 转换自然语言描述为一系列的句子，然后使用答案集程序语言来描述问题。</li>
<li>results: 研究发现，只需要几个受Context learning示例，LLM 就可以生成相对复杂的答案集程序。大多数错误都是相对简单的，可以由人类轻松 corrrect。因此，LLM 可以有效地帮助创建答案集程序。<details>
<summary>Abstract</summary>
Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated exceptional performance in various natural language processing tasks and have shown the ability to solve certain reasoning problems. However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques. In contrast, formal logic is adept at handling complex reasoning, but translating natural language descriptions into formal logic is a challenging task that non-experts struggle with. This paper proposes a neuro-symbolic method that combines the strengths of large language models and answer set programming. Specifically, we employ an LLM to transform natural language descriptions of logic puzzles into answer set programs. We carefully design prompts for an LLM to convert natural language descriptions into answer set programs in a step by step manner. Surprisingly, with just a few in-context learning examples, LLMs can generate reasonably complex answer set programs. The majority of errors made are relatively simple and can be easily corrected by humans, thus enabling LLMs to effectively assist in the creation of answer set programs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如GPT-3和GPT-4，在不同的自然语言处理任务中表现出色，并且能够解决一些推理问题。然而，它们的推理能力相对较浅，即使使用了不同的推问技巧。相比之下，正式逻辑能够处理复杂的推理，但将自然语言描述转换为正式逻辑是一个困难的任务，非专家通常难以进行。这篇论文提议了一个神经符号方法，将大型语言模型和答案集计算结合在一起。具体来说，我们使用一个LLM将自然语言描述逻辑题目转换为答案集程式。我们严格设计了对LLM的推问示例，以步骤地方式将自然语言描述转换为答案集程式。 surprisingly，仅需几个内容学习示例，LLM可以生成相对复杂的答案集程式。大多数错误都是相对简单的，可以轻松地由人类更正，因此LLM可以有效地协助创建答案集程式。
</details></li>
</ul>
<hr>
<h2 id="The-Growth-of-E-Bike-Use-A-Machine-Learning-Approach"><a href="#The-Growth-of-E-Bike-Use-A-Machine-Learning-Approach" class="headerlink" title="The Growth of E-Bike Use: A Machine Learning Approach"></a>The Growth of E-Bike Use: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02034">http://arxiv.org/abs/2308.02034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Gupta, Samarth Chitgopekar, Alexander Kim, Joseph Jiang, Megan Wang, Christopher Grattoni</li>
<li>for: 这项研究是为了帮助政策制定者更好地理解电动自行车（e-bike）的发展和影响。</li>
<li>methods: 这项研究使用了ARIMA模型和一种监管式机器学习算法来预测电动自行车销售量的增长。此外，研究还使用了Random Forest回归模型来分析电动自行车销售增长的因素。</li>
<li>results: 研究发现，电动自行车在美国的使用带来了15,737.82公斤的二氧化碳排放减少和716,630.727千卡路里的热量燃烧。此外，研究还发现了电动自行车销售增长的主要影响因素，包括可 dispose的人均收入和受欢迎程度。<details>
<summary>Abstract</summary>
We present our work on electric bicycles (e-bikes) and their implications for policymakers in the United States. E-bikes have gained significant popularity as a fast and eco-friendly transportation option. As we strive for a sustainable energy plan, understanding the growth and impact of e-bikes is crucial for policymakers. Our mathematical modeling offers insights into the value of e-bikes and their role in the future. Using an ARIMA model, a supervised machine-learning algorithm, we predicted the growth of e-bike sales in the U.S. Our model, trained on historical sales data from January 2006 to December 2022, projected sales of 1.3 million units in 2025 and 2.113 million units in 2028. To assess the factors contributing to e-bike usage, we employed a Random Forest regression model. The most significant factors influencing e-bike sales growth were disposable personal income and popularity. Furthermore, we examined the environmental and health impacts of e-bikes. Through Monte Carlo simulations, we estimated the reduction in carbon emissions due to e-bike use and the calories burned through e-biking. Our findings revealed that e-bike usage in the U.S. resulted in a reduction of 15,737.82 kilograms of CO2 emissions in 2022. Additionally, e-bike users burned approximately 716,630.727 kilocalories through their activities in the same year. Our research provides valuable insights for policymakers, emphasizing the potential of e-bikes as a sustainable transportation solution. By understanding the growth factors and quantifying the environmental and health benefits, policymakers can make informed decisions about integrating e-bikes into future energy and transportation strategies.
</details>
<details>
<summary>摘要</summary>
我们在美国的电动自行车（e-bike）方面进行了研究，并对政策制定者提供了有价值的信息。电动自行车在快速和环保的交通方式上受到了广泛的欢迎，因此理解电动自行车的增长和影响对于政策制定者是非常重要的。我们使用ARIMA模型和一种监管的机器学习算法来预测美国电动自行车销售的增长。我们的模型，基于2006年1月至2022年12月的历史销售数据，预测到2025年的销售量将达130万台，而到2028年将达2113万台。为了了解电动自行车使用的因素，我们使用Random Forest回归模型。我们发现，个人废弃收入和流行度是电动自行车销售增长的最重要因素。此外，我们还对电动自行车的环境和健康影响进行了分析。通过蒙特卡罗 simulate，我们计算了因电动自行车使用而减少的二氧化碳排放量和通过电动自行车活动烧取的卡路里。我们的发现表明，在2022年，美国的电动自行车使用已经减少了15737.82公斤的二氧化碳排放量，同时电动自行车用户通过其活动烧取了约716630.727公斤的卡路里。我们的研究为政策制定者提供了有价值的信息，证明了电动自行车的可持续性，并且可以作为未来能源和交通战略的一部分。通过理解电动自行车增长的因素和量化电动自行车对环境和健康的影响，政策制定者可以做出有知识的决策。
</details></li>
</ul>
<hr>
<h2 id="Coupling-Large-Language-Models-with-Logic-Programming-for-Robust-and-General-Reasoning-from-Text"><a href="#Coupling-Large-Language-Models-with-Logic-Programming-for-Robust-and-General-Reasoning-from-Text" class="headerlink" title="Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text"></a>Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07696">http://arxiv.org/abs/2307.07696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azreasoners/llm-asp">https://github.com/azreasoners/llm-asp</a></li>
<li>paper_authors: Zhun Yang, Adam Ishay, Joohyung Lee</li>
<li>for: 这个论文主要目标是提高大语言模型（LLM）的逻辑能力，使其能够与专门为逻辑语言理解问题训练的模型竞争。</li>
<li>methods: 该论文使用了一种基于 ASP（answer set programming）的逻辑知识表示形式，将自然语言句子转换为逻辑形式，并将这些逻辑形式作为 LLM 的输入。这种方法可以在不需要重新训练的情况下，让 LLM 适应不同的问题。</li>
<li>results: 该论文实验表明，这种方法可以在多个 NLP 评估 benchmark 上实现state-of-the-art 性能，包括 bAbI、StepGame、CLUTRR 和 gSCAN。此外，这种方法还可以成功解决了一些 LLM 无法解决的机器人规划任务。<details>
<summary>Abstract</summary>
While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM），如GPT-3，看起来具有坚固的基础和通用性，但它们的理解能力并没有与专门设计的自然语言理解问题模型相比。在这个研究中，我们发现了一种使用大型自然语言模型来实现几次例示semantic parser的方法。它可以将自然语言句子转换成逻辑形式，该逻辑形式可以作为答案集程序的输入，这种逻辑基于的知识表示形式。这种组合系统可以处理多个问题回答任务，无需为每个新任务进行重新训练。它只需要几个示例来导引LLM的适应特定任务，以及可重用的ASP知识模块，可以应用于多个任务。我们 demonstated 这种方法在多个 NLP 标准准测试上达到了现状最佳性能，包括 bAbI、StepGame、CLUTRR 和 gSCAN。此外，它还成功解决了一些 LLM 无法解决的机器人规划任务。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Change-Detection-Techniques-in-Document-Images"><a href="#A-Survey-on-Change-Detection-Techniques-in-Document-Images" class="headerlink" title="A Survey on Change Detection Techniques in Document Images"></a>A Survey on Change Detection Techniques in Document Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07691">http://arxiv.org/abs/2307.07691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhinandan Kumar Pun, Mohammed Javed, David S. Doermann</li>
<li>for: 本文主要针对文档图像中的变化检测问题，其应用于医学、Remote Sensing 等领域。</li>
<li>methods: 本文对文档图像中的变化检测方法进行了报告和分析，包括内容基于的方法和结构基于的方法。</li>
<li>results: 本文对文档图像中的变化检测方法进行了总结和评价，并报告了现有的数据集和评价指标，以及现有方法的缺点和挑战。<details>
<summary>Abstract</summary>
The problem of change detection in images finds application in different domains like diagnosis of diseases in the medical field, detecting growth patterns of cities through remote sensing, and finding changes in legal documents and contracts. However, this paper presents a survey on core techniques and rules to detect changes in different versions of a document image. Our discussions on change detection focus on two categories -- content-based and layout-based. The content-based techniques intelligently extract and analyze the image contents (text or non-text) to show the possible differences, whereas the layout-based techniques use structural information to predict document changes. We also summarize the existing datasets and evaluation metrics used in change detection experiments. The shortcomings and challenges the existing methods face are reported, along with some pointers for future research work.
</details>
<details>
<summary>摘要</summary>
该问题在不同领域都有应用，如医疗领域的疾病诊断、通过远程感知获取城市增长趋势，以及法律文档和合同中的变化检测。然而，本文主要介绍了文档版本之间的变化检测技术和规则。我们的讨论关注内容基于和布局基于的两个类别。内容基于的技术通过智能EXTRACT和分析图像内容（文本或非文本）来显示可能的差异，而布局基于的技术使用结构信息预测文档变化。我们还总结了已有的数据集和评价标准，并报告现有方法的缺陷和挑战，以及未来研究的指向。
</details></li>
</ul>
<hr>
<h2 id="Creating-a-Dataset-for-High-Performance-Computing-Code-Translation-A-Bridge-Between-HPC-Fortran-and-C"><a href="#Creating-a-Dataset-for-High-Performance-Computing-Code-Translation-A-Bridge-Between-HPC-Fortran-and-C" class="headerlink" title="Creating a Dataset for High-Performance Computing Code Translation: A Bridge Between HPC Fortran and C++"></a>Creating a Dataset for High-Performance Computing Code Translation: A Bridge Between HPC Fortran and C++</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07686">http://arxiv.org/abs/2307.07686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bin123apple/fortran-cpp-hpc-code-translation-dataset">https://github.com/bin123apple/fortran-cpp-hpc-code-translation-dataset</a></li>
<li>paper_authors: Bin Lei, Caiwen Ding, Le Chen, Pei-Hung Lin, Chunhua Liao</li>
<li>for: 这个论文是为了提供一个用于训练机器学习模型翻译OpenMP Fortran和C++代码的新数据集而写的。</li>
<li>methods: 该论文使用了一种精心制定的代码相似性测试来初步准备数据集，以确保其可靠性和可应用性。然后，通过量化（CodeBLEU）和质量（人员评估）方法评估数据集的效果。</li>
<li>results: 研究表明，使用该数据集可以大幅提高大规模语言模型的翻译能力，具体提高$\times 5.1$（无前期编程知识）和$\times 9.9$（有些编程familiarity）。这种数据集的出现有助于提高高性计算代码翻译的领域进步。数据集可以在<a target="_blank" rel="noopener" href="https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset中下载。</a><details>
<summary>Abstract</summary>
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of $\mathbf{\times 5.1}$ for models with no prior coding knowledge and $\mathbf{\times 9.9}$ for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing. The dataset is available at https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提供了一个新的代码集合用于训练机器学习模型翻译OpenMP Fortran和C++代码。为确保可靠性和可应用性，我们首先使用仔细的代码相似性测试进行初步约束。我们使用代码BLEU和人类评估方法进行评估效果，并证明这些数据可以帮助大规模语言模型提高翻译能力，其中模型没有编程知识时提高$\times 5.1$，而具有一定编程经验时提高$\times 9.9$。我们的工作表明这个数据集可以推动高性能计算领域代码翻译的发展。这个数据集可以在https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset中下载。
</details></li>
</ul>
<hr>
<h2 id="Bound-by-the-Bounty-Collaboratively-Shaping-Evaluation-Processes-for-Queer-AI-Harms"><a href="#Bound-by-the-Bounty-Collaboratively-Shaping-Evaluation-Processes-for-Queer-AI-Harms" class="headerlink" title="Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms"></a>Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10223">http://arxiv.org/abs/2307.10223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Organizers of QueerInAI, Nathan Dennler, Anaelia Ovalle, Ashwin Singh, Luca Soldaini, Arjun Subramonian, Huy Tu, William Agnew, Avijit Ghosh, Kyra Yee, Irene Font Peradejordi, Zeerak Talat, Mayra Russo, Jess de Jesus de Pinho Pinhal</li>
<li>for: This paper aims to understand the perspectives of queer communities on bias evaluation benchmarks and dataset and model documentation for AI systems, and to redesign these processes from queer perspectives.</li>
<li>methods: The paper uses a participatory workshop to gather feedback from queer communities on bias bounties and to critique and redesign these processes.</li>
<li>results: The paper finds that queer communities have concerns about the ownership, incentives, and efficacy of bias bounties, and advocates for community ownership of bounties and the use of participatory processes (e.g., co-creation) to complement bias bounties.<details>
<summary>Abstract</summary>
Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants questioning the ownership, incentives, and efficacy of bounties. We conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).
</details>
<details>
<summary>摘要</summary>
人工智能（AI）系统的偏见和伤害评估过程和数据集已成为评估AI系统偏见和伤害的中心过程。然而，这些审核过程受到了社会弱势群体知识不包括和审核人员与社区力量不均衡的批评。因此，一些偏见评估模式被提出，通过与受影响社区合作来识别和评估AI系统的伤害（例如，偏见报酬）。然而，寻求社会弱势群体想要从审核过程中获得的问题仍然被忽略。在这篇论文中，我们问到了LGBTQ+社区对审核过程的看法和期望。为此，我们组织了参与式工作坊，批判和重新设计偏见报酬从Queer perspective。我们发现，当给予参与者空间时，参与者的反馈范围超出了偏见报酬的范围，参与者质疑报酬的所有权、动机和有效性。我们 conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).
</details></li>
</ul>
<hr>
<h2 id="Efficient-Adversarial-Attacks-on-Online-Multi-agent-Reinforcement-Learning"><a href="#Efficient-Adversarial-Attacks-on-Online-Multi-agent-Reinforcement-Learning" class="headerlink" title="Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning"></a>Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07670">http://arxiv.org/abs/2307.07670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanlin Liu, Lifeng Lai</li>
<li>For: 本研究探讨了多智能体强化学习（MARL）模型面临恶意攻击的影响。* Methods: 我们 investigate the impact of adversarial attacks on MARL models, including action poisoning and reward poisoning attacks, as well as a mixed attack strategy that combines both.* Results: 我们发现，混合攻击策略可以高效地攻击 MARL 模型，即使攻击者没有对环境和代理人算法的任何先前信息。<details>
<summary>Abstract</summary>
Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and the reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior information about the underlying environment and the agents' algorithms.
</details>
<details>
<summary>摘要</summary>
We first show the limitations of action poisoning only attacks and reward poisoning only attacks. We then introduce a mixed attack strategy that combines both action poisoning and reward poisoning. We demonstrate that the mixed attack strategy can effectively attack MARL agents even if the attacker has no prior knowledge of the underlying environment and the agents' algorithms.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Action-Robust-Reinforcement-Learning-with-Probabilistic-Policy-Execution-Uncertainty"><a href="#Efficient-Action-Robust-Reinforcement-Learning-with-Probabilistic-Policy-Execution-Uncertainty" class="headerlink" title="Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty"></a>Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07666">http://arxiv.org/abs/2307.07666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanlin Liu, Zhihan Zhou, Han Liu, Lifeng Lai</li>
<li>for: 这 paper 的目的是提出一种可靠的 reinforcement learning 算法，可以在行动不确定性的情况下优化最坏情况性能。</li>
<li>methods: 这 paper 使用了 probablistic policy execution uncertainty，并提出了 action robust Bellman optimality equation 来解决这类 MDP 中的优化问题。</li>
<li>results: 该 paper 的 Action Robust Reinforcement Learning with Certificates (ARRLC) 算法可以 дости到 minimax 优化的 regret 和样本复杂度，并且在实验中证明了其在行动偏移情况下的稳定性和更快的 converges 速度。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm in the presence of action perturbations.
</details>
<details>
<summary>摘要</summary>
中文简体版Robust reinforcement learning（RL）目的是找到面对不确定性时的最佳策略。在这篇论文中，我们关注行动稳健RL，即在执行策略时存在可能性的情况下，agent将按照策略指定的行动执行的可能性为$1-\rho$，而剩下的可能性为$\rho$。我们证明了action robust Markov decision process（MDP）中的优化策略的存在，并提供了action robust Bellman优化方程的解。此外，我们开发了Action Robust Reinforcement Learning with Certificates（ARRLC）算法，实现了最小最大 regret和样本复杂度的优化。此外，我们进行了数值实验，证明了ARRLC在行动偏移下的 robustness，并证明它在存在行动偏移时比非稳健RL算法和robust TD算法更快 converges。Traditional Chinese versionRobust reinforcement learning（RL）的目的是找到面对不确定性时的最佳策略。在这篇论文中，我们关注行动稳健RL，即在执行策略时存在可能性的情况下，agent将按照策略指定的行动执行的可能性为$1-\rho$，而剩下的可能性为$\rho$。我们证明了action robust Markov decision process（MDP）中的优化策略的存在，并提供了action robust Bellman优化方程的解。此外，我们开发了Action Robust Reinforcement Learning with Certificates（ARRLC）算法，实现了最小最大 regret和样本复杂度的优化。此外，我们进行了数值实验，证明了ARRLC在行动偏移下的 robustness，并证明它在存在行动偏移时比非稳健RL算法和robust TD算法更快 converges。
</details></li>
</ul>
<hr>
<h2 id="MPDIoU-A-Loss-for-Efficient-and-Accurate-Bounding-Box-Regression"><a href="#MPDIoU-A-Loss-for-Efficient-and-Accurate-Bounding-Box-Regression" class="headerlink" title="MPDIoU: A Loss for Efficient and Accurate Bounding Box Regression"></a>MPDIoU: A Loss for Efficient and Accurate Bounding Box Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07662">http://arxiv.org/abs/2307.07662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ma Siliang, Xu Yong</li>
<li>for: 本研究旨在解决现有 bounding box regression loss function 无法优化 predicted box 和 groundtruth box 尺寸相同但是宽高值不同的问题。</li>
<li>methods: 本研究提出了一种基于 minimum point distance 的 bounding box similarity comparison metric MPDIoU，包括了现有 loss functions 中考虑的所有相关因素，如 overlap 或 non-overlapping 面积、中心点距离、宽高差异，而简化计算过程。基于 MPDIoU 的 bounding box regression loss function 被称为 LMPDIoU。</li>
<li>results: 实验结果表明，基于 MPDIoU 的 loss function 在 state-of-the-art instance segmentation 和 object detection 模型（例如 YOLACT 和 YOLOv7）上，对 PASCAL VOC、MS COCO 和 IIIT5k 进行训练后，与现有 loss functions 相比，具有更高的精度和效果。<details>
<summary>Abstract</summary>
Bounding box regression (BBR) has been widely used in object detection and instance segmentation, which is an important step in object localization. However, most of the existing loss functions for bounding box regression cannot be optimized when the predicted box has the same aspect ratio as the groundtruth box, but the width and height values are exactly different. In order to tackle the issues mentioned above, we fully explore the geometric features of horizontal rectangle and propose a novel bounding box similarity comparison metric MPDIoU based on minimum point distance, which contains all of the relevant factors considered in the existing loss functions, namely overlapping or non-overlapping area, central points distance, and deviation of width and height, while simplifying the calculation process. On this basis, we propose a bounding box regression loss function based on MPDIoU, called LMPDIoU . Experimental results show that the MPDIoU loss function is applied to state-of-the-art instance segmentation (e.g., YOLACT) and object detection (e.g., YOLOv7) model trained on PASCAL VOC, MS COCO, and IIIT5k outperforms existing loss functions.
</details>
<details>
<summary>摘要</summary>
bounding box regression (BBR) 广泛应用于物体检测和实例分割，是物体Localization的重要步骤。然而，现有的 bounding box regression 损失函数无法优化预测框与实际框的尺寸值不同，但宽高比相同的情况。为解决以上问题，我们彻底探讨直方框的几何特征，并提出一种基于最小点距离的 bounding box 相似比较度量 MPDIoU，该度量包含现有损失函数中考虑的所有因素，包括重叠或非重叠区域、中心点距离、宽高差异，而简化计算过程。基于 MPDIoU 度量，我们提出了一种基于 MPDIoU 的 bounding box regression 损失函数 LMPDIoU。实验结果表明，MPDIoU 损失函数在 state-of-the-art 实例分割（如 YOLACT）和物体检测（如 YOLOv7）模型在 PASCAL VOC、MS COCO 和 IIIT5k 上训练后，与现有损失函数相比，具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="SALC-Skeleton-Assisted-Learning-Based-Clustering-for-Time-Varying-Indoor-Localization"><a href="#SALC-Skeleton-Assisted-Learning-Based-Clustering-for-Time-Varying-Indoor-Localization" class="headerlink" title="SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization"></a>SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07650">http://arxiv.org/abs/2307.07650</a></li>
<li>repo_url: None</li>
<li>paper_authors: An-Hung Hsiao, Li-Hsiang Shen, Chen-Yi Chang, Chun-Jie Chiu, Kai-Ten Feng<br>for: 本研究旨在提高室内地位系统的精度和可靠性，透过对室内 WiFi 接点点 (AP) 的接收信号强度 (RSS) 进行建立 fingerprinting 数据库。methods: 本研究提出了一个基于 skeleton-assisted learning-based clustering (SALC) 系统，包括 RSS-oriented map-assisted clustering (ROMAC)、cluster-based online database establishment (CODE) 和 cluster-scaled location estimation (CsLE)。SALC 系统结合了骨架基于最短路 (SSP) 的相似性和时间变化的 RSS 测量 across reference points (RPs)。results:  simulation 和实验结果表明，提出的 SALC 系统可以有效地重建 fingerprint 数据库，提高地位估计精度，比较出色于现有Literature中的其他方法。<details>
<summary>Abstract</summary>
Wireless indoor localization has attracted significant amount of attention in recent years. Using received signal strength (RSS) obtained from WiFi access points (APs) for establishing fingerprinting database is a widely utilized method in indoor localization. However, the time-variant problem for indoor positioning systems is not well-investigated in existing literature. Compared to conventional static fingerprinting, the dynamicallyreconstructed database can adapt to a highly-changing environment, which achieves sustainability of localization accuracy. To deal with the time-varying issue, we propose a skeleton-assisted learning-based clustering localization (SALC) system, including RSS-oriented map-assisted clustering (ROMAC), cluster-based online database establishment (CODE), and cluster-scaled location estimation (CsLE). The SALC scheme jointly considers similarities from the skeleton-based shortest path (SSP) and the time-varying RSS measurements across the reference points (RPs). ROMAC clusters RPs into different feature sets and therefore selects suitable monitor points (MPs) for enhancing location estimation. Moreover, the CODE algorithm aims for establishing adaptive fingerprint database to alleviate the timevarying problem. Finally, CsLE is adopted to acquire the target position by leveraging the benefits of clustering information and estimated signal variations in order to rescale the weights fromweighted k-nearest neighbors (WkNN) method. Both simulation and experimental results demonstrate that the proposed SALC system can effectively reconstruct the fingerprint database with an enhanced location estimation accuracy, which outperforms the other existing schemes in the open literature.
</details>
<details>
<summary>摘要</summary>
无线内部位置已引起过去几年的广泛关注。使用WiFi接入点（AP）接收信号强度（RSS）建立指本库是内部位置系统中广泛使用的方法。然而，现有文献中对内部位置系统的时间变化问题并未得到充分研究。与传统静止指本相比，动态重建库可以适应高度变化的环境，实现位置准确性的持续稳定。为解决时间变化问题，我们提出了骨架帮助学习基本的集成位置系统（SALC），包括RSS方向的地图帮助集成（ROMAC）、集成在线数据库建立（CODE）和集群扩大位置估计（CsLE）。SALC方案同时考虑骨架基于最短路（SSP）的相似性和时间变化的RSS测量值。ROMAC将RP集成到不同的特征集中，选择合适的监测点（MP）以提高位置估计。此外，CODE算法旨在建立适应时间变化的指本库，以解决时间变化问题。最后，CsLE方法通过利用集群信息和估计信号变化来重新调整WkNN方法中的权重，以获得更高的位置估计精度。实验和 simulations结果表明，提出的SALC系统可以有效地重建指本库，并提高位置估计精度，超过现有文献中的其他方案。
</details></li>
</ul>
<hr>
<h2 id="Othering-and-low-prestige-framing-of-immigrant-cuisines-in-US-restaurant-reviews-and-large-language-models"><a href="#Othering-and-low-prestige-framing-of-immigrant-cuisines-in-US-restaurant-reviews-and-large-language-models" class="headerlink" title="Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models"></a>Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07645">http://arxiv.org/abs/2307.07645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiweiluo/immigrant-food-framing">https://github.com/yiweiluo/immigrant-food-framing</a></li>
<li>paper_authors: Yiwei Luo, Kristina Gligorić, Dan Jurafsky</li>
<li>for: This paper aims to understand implicit attitudes toward food and how they can perpetuate social prejudice, specifically in the context of immigrant cuisines.</li>
<li>methods: The authors use linguistic analyses of over 2.1 million English language Yelp reviews of restaurants in 14 US states to evaluate social theories about attitudes toward immigrant cuisine. They control for factors such as restaurant price and neighborhood racial diversity.</li>
<li>results: The authors find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity, exoticism, and prototypicality, and that non-Western immigrant cuisines receive more othering than European cuisines. Additionally, they find that non-Western immigrant cuisines are framed less positively and as lower status, being evaluated in terms of affordability and hygiene. Finally, they show that reviews generated by large language models (LLMs) reproduce many of the same framing tendencies.<details>
<summary>Abstract</summary>
Identifying and understanding implicit attitudes toward food can help efforts to mitigate social prejudice due to food's pervasive role as a marker of cultural and ethnic identity. Stereotypes about food are a form of microaggression that contribute to harmful public discourse that may in turn perpetuate prejudice toward ethnic groups and negatively impact economic outcomes for restaurants. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews of restaurants in 14 US states. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be framed in objectifying and othering terms of authenticity (e.g., authentic, traditional), exoticism (e.g., exotic, different), and prototypicality (e.g., typical, usual), but that non-Western immigrant cuisines (e.g., Indian, Mexican) receive more othering than European cuisines (e.g., French, Italian). We further find that non-Western immigrant cuisines are framed less positively and as lower status, being evaluated in terms of affordability and hygiene. Finally, we show that reviews generated by large language models (LLMs) reproduce many of the same framing tendencies. Our results empirically corroborate social theories of taste and gastronomic stereotyping, and reveal linguistic processes by which such attitudes are reified.
</details>
<details>
<summary>摘要</summary>
认识和理解食物的隐式态度可以帮助减少基于食物的文化和民族身份 marker 的社会偏见。食物的刻板印象是一种微侵略，它们可能在社会公共讨论中产生有害的影响，从而导致对少数民族的偏见和不良经济效益。通过精心的语言分析，我们在大规模的英语 Yelp 评论数据集中评估社会理论对移民菜系的态度。控制因素包括餐厅价格和邻里种族多样性，我们发现移民菜系更有可能被归类为真实、传统、特色、外国、不同、常见等词汇，但非西方移民菜系（如印度、墨西哥）被其他化比欧洲菜系（如法国、意大利）更多。此外，我们发现非西方移民菜系在评价中受到负面评价，被评价为便宜和卫生。最后，我们发现由大语言模型（LLMs）生成的评论也存在类似的归类倾向。我们的结果经验证了社会理论的味蕾和文化刻板印象，并揭示了语言过程如何固化这些态度。
</details></li>
</ul>
<hr>
<h2 id="It-is-currently-hodgepodge’’-Examining-AI-ML-Practitioners’-Challenges-during-Co-production-of-Responsible-AI-Values"><a href="#It-is-currently-hodgepodge’’-Examining-AI-ML-Practitioners’-Challenges-during-Co-production-of-Responsible-AI-Values" class="headerlink" title="&#96;It is currently hodgepodge’’: Examining AI&#x2F;ML Practitioners’ Challenges during Co-production of Responsible AI Values"></a>&#96;It is currently hodgepodge’’: Examining AI&#x2F;ML Practitioners’ Challenges during Co-production of Responsible AI Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10221">http://arxiv.org/abs/2307.10221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rama Adithya Varanasi, Nitesh Goyal</li>
<li>for: 本研究旨在探讨AI&#x2F;ML实践者在实施责任AI（RAI）价值观时遇到的挑战，以及这些挑战如何影响实践者的工作。</li>
<li>methods: 本研究采用了采访方法，问问23名参与者，来自10家组织，他们在实施AI&#x2F;ML产品时如何保持RAI价值观。</li>
<li>results: 研究发现，实施RAI价值观会由于组织结构和价值观念的冲突而带来挑战，这些挑战会影响实践者的工作。研究还发现了多种解决这些挑战的策略，包括在组织结构和价值观念方面进行调整。<details>
<summary>Abstract</summary>
Recently, the AI/ML research community has indicated an urgent need to establish Responsible AI (RAI) values and practices as part of the AI/ML lifecycle. Several organizations and communities are responding to this call by sharing RAI guidelines. However, there are gaps in awareness, deliberation, and execution of such practices for multi-disciplinary ML practitioners. This work contributes to the discussion by unpacking co-production challenges faced by practitioners as they align their RAI values. We interviewed 23 individuals, across 10 organizations, tasked to ship AI/ML based products while upholding RAI norms and found that both top-down and bottom-up institutional structures create burden for different roles preventing them from upholding RAI values, a challenge that is further exacerbated when executing conflicted values. We share multiple value levers used as strategies by the practitioners to resolve their challenges. We end our paper with recommendations for inclusive and equitable RAI value-practices, creating supportive organizational structures and opportunities to further aid practitioners.
</details>
<details>
<summary>摘要</summary>
We interviewed 23 individuals from 10 organizations that are tasked with shipping AI/ML-based products while upholding RAI norms. We found that both top-down and bottom-up institutional structures create burdens for different roles, preventing them from upholding RAI values. This challenge is further exacerbated when executing conflicting values.To address these challenges, we share multiple value levers used by practitioners as strategies to resolve their challenges. These include:1. Inclusive and equitable value-practices: Practitioners must prioritize inclusive and equitable value-practices that take into account the needs and perspectives of diverse stakeholders.2. Supportive organizational structures: Organizations must create supportive structures and opportunities to further aid practitioners in upholding RAI values.3. Ongoing education and training: Practitioners must receive ongoing education and training on RAI values and practices to ensure they are equipped to handle the challenges they face.We end our paper with recommendations for inclusive and equitable RAI value-practices, creating supportive organizational structures, and providing ongoing education and training to practitioners. By addressing these challenges, we can ensure that RAI values are upheld in the development and deployment of AI/ML-based products.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Link-Prediction-over-Hyper-Relational-Temporal-Knowledge-Graphs-Enhanced-with-Time-Invariant-Relational-Knowledge"><a href="#Exploring-Link-Prediction-over-Hyper-Relational-Temporal-Knowledge-Graphs-Enhanced-with-Time-Invariant-Relational-Knowledge" class="headerlink" title="Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge"></a>Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10219">http://arxiv.org/abs/2307.10219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifeng Ding, Jingcheng Wu, Jingpei Wu, Yan Xia, Volker Tresp</li>
<li>for: This paper focuses on filling the gap between temporal knowledge graph (TKG) reasoning and hyper-relational knowledge graph (HKG) reasoning, by developing a new benchmark dataset and a reasoning model that can efficiently handle both temporal and qualifier information.</li>
<li>methods: The proposed reasoning model leverages both temporal and time-invariant relational knowledge from the Wikidata knowledge base to improve the performance of HTKG reasoning.</li>
<li>results: The experimental results show that the proposed model outperforms previous related methods on HTKG link prediction, and can be further enhanced by jointly leveraging both temporal and time-invariant relational knowledge.Here’s the simplified Chinese text version of the three information points:</li>
<li>for: 这篇论文主要是填补知识图论理和超 relate 知识图论理之间的空白，通过开发新的benchmark数据集和一种能够有效处理时间和资格信息的reasoning模型。</li>
<li>methods: 提出的reasoning模型利用知识图中的时间不变的关系知识和Wikidata知识库中的时间不变关系知识来提高HTKG论理的性能。</li>
<li>results: 实验结果表明，提出的模型在HTKG链接预测 task上表现出色，并且可以通过共同利用时间不变和资格信息来进一步提高性能。<details>
<summary>Abstract</summary>
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and YAGO-hy, and propose a HTKG reasoning model that efficiently models both temporal facts and qualifiers. We further exploit additional time-invariant relational knowledge from the Wikidata knowledge base and study its effectiveness in HTKG reasoning. Time-invariant relational knowledge serves as the knowledge that remains unchanged in time (e.g., Sasha Obama is the child of Barack Obama), and it has never been fully explored in previous TKG reasoning benchmarks and approaches. Experimental results show that our model substantially outperforms previous related methods on HTKG link prediction and can be enhanced by jointly leveraging both temporal and time-invariant relational knowledge.
</details>
<details>
<summary>摘要</summary>
traditional知识 graphs（KGs）的扩展，hyper-relational知识 graphs（HKGs）提供每个KG事实的额外关键值对（i.e., 资格），以更好地限定事实的有效性。近年来，研究graph reasoning over HKGs的兴趣在增长。同时，由于世界知识的不断演进，大量并发的工作在研究temporal知识 graphs（TKGs）上进行reasoning，每个TKG事实可以视为KG事实加上时间戳（或时间段），指定其时间有效性。现有的HKGreasoning方法不考虑时间信息，因为之前的benchmark dataset不Explicitly specified。此外，所有前一些TKGreasoning方法只是强调时间reasoning，没有考虑学习资格。为此，我们希望填补HKGreasoning和TKGreasoning之间的空白。我们开发了两个新的benchmark hyper-relational TKG（HTKG）数据集，即Wiki-hy和YAGO-hy，并提出了一种HTKGreasoning模型，可以效率地模型时间事实和资格。此外，我们还利用Wikidata知识库中的静止关系知识，并研究其在HTKGreasoning中的效果。静止关系知识是指不变于时间的知识（例如，萨沙·奥巴马是巴拉克·奥巴马的孩子），在前一些TKGreasoningbenchmark和方法中从未得到过全面的探索。实验结果表明，我们的模型在HTKGlink prediction中具有明显的优势，并可以通过结合时间和静止关系知识来进一步提高性能。
</details></li>
</ul>
<hr>
<h2 id="Dissenting-Explanations-Leveraging-Disagreement-to-Reduce-Model-Overreliance"><a href="#Dissenting-Explanations-Leveraging-Disagreement-to-Reduce-Model-Overreliance" class="headerlink" title="Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance"></a>Dissenting Explanations: Leveraging Disagreement to Reduce Model Overreliance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07636">http://arxiv.org/abs/2307.07636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omer Reingold, Judy Hanwen Shen, Aditi Talati</li>
<li>for: 该文章的目的是提出了一种新的解释方法，即“分裂解释”，以帮助人们从解释中获得更多的启示，而不仅仅是依靠模型的预测。</li>
<li>methods: 该文章提出了一种基于多模型的分裂解释方法，包括全局和本地的方法。这些方法可以在模型之间的不同预测时提供不同的解释，以便帮助人们更好地理解模型的决策过程。</li>
<li>results: 经过一个小样本研究，authors发现，通过提供分裂解释，可以减少人们对模型预测的过重依赖，同时不会降低总准确率。这表明，分裂解释可以帮助人们更好地理解模型的决策过程，并减少模型预测的不确定性。<details>
<summary>Abstract</summary>
While explainability is a desirable characteristic of increasingly complex black-box models, modern explanation methods have been shown to be inconsistent and contradictory. The semantics of explanations is not always fully understood - to what extent do explanations "explain" a decision and to what extent do they merely advocate for a decision? Can we help humans gain insights from explanations accompanying correct predictions and not over-rely on incorrect predictions advocated for by explanations? With this perspective in mind, we introduce the notion of dissenting explanations: conflicting predictions with accompanying explanations. We first explore the advantage of dissenting explanations in the setting of model multiplicity, where multiple models with similar performance may have different predictions. In such cases, providing dissenting explanations could be done by invoking the explanations of disagreeing models. Through a pilot study, we demonstrate that dissenting explanations reduce overreliance on model predictions, without reducing overall accuracy. Motivated by the utility of dissenting explanations we present both global and local methods for their generation.
</details>
<details>
<summary>摘要</summary>
“ explainnability 是复杂黑盒模型的一个欲具备的特点，但现代解释方法有时会被视为不一致和矛盾。解释的 semantics 不 sempre fully understood - 解释是否真的解释了一个决策，或者仅仅是支持一个决策？我们可以帮助人们从解释中获得启发，而不是仅仅依赖错误的预测和解释？以此角度，我们引入了不同的解释： conflicting predictions with accompanying explanations。在模型多样性的设定中，多个模型具有相似的表现可能会有不同的预测。在这种情况下，提供不同的解释可以通过邀请不同的模型的解释。我们通过一个小试验示出，提供不同的解释可以对抗预测的过度依赖，而不减少整体准确性。驱动了不同解释的 utility，我们提出了全球和本地的生成方法。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Value-based-Fast-and-Slow-AI-Nudging"><a href="#Value-based-Fast-and-Slow-AI-Nudging" class="headerlink" title="Value-based Fast and Slow AI Nudging"></a>Value-based Fast and Slow AI Nudging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07628">http://arxiv.org/abs/2307.07628</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marianna B. Ganapini, Francesco Fabiano, Lior Horesh, Andrea Loreggia, Nicholas Mattei, Keerthiram Murugesan, Vishal Pallagani, Francesca Rossi, Biplav Srivastava, Brent Venable<br>for: 这项研究旨在开发一个基于人工智能和人类协作的决策框架，该框架通过提供决策建议来引导人类做出决策。methods: 该研究使用了三种决策激励模式，这些模式根据决策建议是在什么时候提供给人类，以便激励人类快速思考、慢速思考或自reflective思考。results: 研究人员通过使用不同的价值来决定何时和如何使用各种决策激励模式，以实现更好的决策效果。例如，在做出决策时，可以考虑decision quality、speed、human upskilling和learning、human agency和隐私等价值。<details>
<summary>Abstract</summary>
Nudging is a behavioral strategy aimed at influencing people's thoughts and actions. Nudging techniques can be found in many situations in our daily lives, and these nudging techniques can targeted at human fast and unconscious thinking, e.g., by using images to generate fear or the more careful and effortful slow thinking, e.g., by releasing information that makes us reflect on our choices. In this paper, we propose and discuss a value-based AI-human collaborative framework where AI systems nudge humans by proposing decision recommendations. Three different nudging modalities, based on when recommendations are presented to the human, are intended to stimulate human fast thinking, slow thinking, or meta-cognition. Values that are relevant to a specific decision scenario are used to decide when and how to use each of these nudging modalities. Examples of values are decision quality, speed, human upskilling and learning, human agency, and privacy. Several values can be present at the same time, and their priorities can vary over time. The framework treats values as parameters to be instantiated in a specific decision environment.
</details>
<details>
<summary>摘要</summary>
推动（nudging）是一种行为战略，旨在影响人们的思想和行为。推动技巧可以在我们日常生活中找到很多的应用，这些推动技巧可以 targets 人们的快速和不自觉的思维，例如使用图像引发恐惧或更加细致和努力的慢思考。在这篇论文中，我们提出了一种基于人工智能和人类合作的价值基于推动框架。这个框架中的三种推动模式，基于建议给人时的 WHEN 和 HOW，用于刺激人们的快速思维、慢思考或者元认知。在具体的决策场景中，根据相关的价值来决定使用哪种推动模式。例如，决策质量、快速响应、人类技能和学习、人类自主权和隐私等价值。在这个框架中，价值被视为实例化在特定决策环境中的参数。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Spatiotemporal-Token-Attention-Network-for-Skeleton-based-General-Interactive-Action-Recognition"><a href="#Interactive-Spatiotemporal-Token-Attention-Network-for-Skeleton-based-General-Interactive-Action-Recognition" class="headerlink" title="Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition"></a>Interactive Spatiotemporal Token Attention Network for Skeleton-based General Interactive Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07469">http://arxiv.org/abs/2307.07469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Necolizer/ISTA-Net">https://github.com/Necolizer/ISTA-Net</a></li>
<li>paper_authors: Yuhang Wen, Zixuan Tang, Yunsheng Pang, Beichen Ding, Mengyuan Liu</li>
<li>for: 本研究旨在提高人机交互行为识别的精度和效率，以便更好地实现人机合作。</li>
<li>methods: 本文提出了一种Interactive Spatiotemporal Token Attention Network（ISTA-Net），该网络同时模型了空间、时间和交互关系。具体来说，ISTA-Net使用了Tokenizer将Interactive Spatiotemporal Tokens（IST）分割成多个多样化实体的动作。通过扩展实体维度，IST提供了更好的交互表示。为了同时学习三个维度，ISTA-Net使用了多头自注意блоック和3D卷积来捕捉间Token的相关性。</li>
<li>results: EXTensive experiments on four datasets show that ISTA-Net outperforms state-of-the-art methods in recognizing interactive actions, demonstrating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously model spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net
</details>
<details>
<summary>摘要</summary>
Recognizing interactive action plays an important role in human-robot interaction and collaboration. Previous methods use late fusion and co-attention mechanism to capture interactive relations, which have limited learning capability or inefficiency to adapt to more interacting entities. With assumption that priors of each entity are already known, they also lack evaluations on a more general setting addressing the diversity of subjects. To address these problems, we propose an Interactive Spatiotemporal Token Attention Network (ISTA-Net), which simultaneously models spatial, temporal, and interactive relations. Specifically, our network contains a tokenizer to partition Interactive Spatiotemporal Tokens (ISTs), which is a unified way to represent motions of multiple diverse entities. By extending the entity dimension, ISTs provide better interactive representations. To jointly learn along three dimensions in ISTs, multi-head self-attention blocks integrated with 3D convolutions are designed to capture inter-token correlations. When modeling correlations, a strict entity ordering is usually irrelevant for recognizing interactive actions. To this end, Entity Rearrangement is proposed to eliminate the orderliness in ISTs for interchangeable entities. Extensive experiments on four datasets verify the effectiveness of ISTA-Net by outperforming state-of-the-art methods. Our code is publicly available at https://github.com/Necolizer/ISTA-Net.Here's the text with some notes on the translation:* "Recognizing interactive action" is translated as "认识人机交互行为" (règng shí yǔ jì xìng zhì xíng)* "Interactive Spatiotemporal Token Attention Network" is translated as "交互空时token注意网络" (jiāo xì kōng shí zhōng zhì wǎng wǎn)* "Interactive Spatiotemporal Tokens" is translated as "交互空时token" (jiāo xì kōng shí zhōng zhì)* "Entity Rearrangement" is translated as "实体重新排序" (shí tǐ zhòng xīn pinyīn)Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Structured-Pruning-of-Neural-Networks-for-Constraints-Learning"><a href="#Structured-Pruning-of-Neural-Networks-for-Constraints-Learning" class="headerlink" title="Structured Pruning of Neural Networks for Constraints Learning"></a>Structured Pruning of Neural Networks for Constraints Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07457">http://arxiv.org/abs/2307.07457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Cacciola, Antonio Frangioni, Andrea Lodi<br>for: 这篇论文主要关注在机器学习（ML）模型与运筐学（OR）工具的集成方面，具体来说是使用混合整数编程（MIP）表述ML模型输出的问题。methods: 本论文使用了束缚（pruning）技术来缩减人工神经网络（ANNs）中的参数数量，从而提高MIP表述的效率。results:  experiments 表明，使用束缚技术可以在ML模型的解决过程中提供显著的加速，而无需妥协解决质量。<details>
<summary>Abstract</summary>
In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity across diverse applications, including cancer treatment, algorithmic configuration, and chemical process optimization. In this domain, the combination of ML and OR often relies on representing the ML model output using Mixed Integer Programming (MIP) formulations. Numerous studies in the literature have developed such formulations for many ML predictors, with a particular emphasis on Artificial Neural Networks (ANNs) due to their significant interest in many applications. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations that are impractical to solve, thereby impeding scalability. In fact, the ML community has already introduced several techniques to reduce the parameter count of ANNs without compromising their performance, since the substantial size of modern ANNs presents challenges for ML applications as it significantly impacts computational efforts during training and necessitates significant memory resources for storage. In this paper, we showcase the effectiveness of pruning, one of these techniques, when applied to ANNs prior to their integration into MIPs. By pruning the ANN, we achieve significant improvements in the speed of the solution process. We discuss why pruning is more suitable in this context compared to other ML compression techniques, and we identify the most appropriate pruning strategies. To highlight the potential of this approach, we conduct experiments using feed-forward neural networks with multiple layers to construct adversarial examples. Our results demonstrate that pruning offers remarkable reductions in solution times without hindering the quality of the final decision, enabling the resolution of previously unsolvable instances.
</details>
<details>
<summary>摘要</summary>
近年来，机器学习（ML）模型与运筐学（OR）工具的集成在多种应用中得到了广泛的推广，包括肿瘤治疗、算法配置和化学过程优化。在这个领域，ML和OR的结合常常通过表示ML模型输出的混合整数编程（MIP）形式来实现。文献中有许多研究发展了这种形式，尤其是人工神经网络（ANNs），因为它们在许多应用中具有广泛的 интерес。然而，ANNs经常具有较大的参数数量，导致MIP形式成为实际不可解决的，从而阻碍了扩展性。事实上，ML社区已经开发了许多技术来减少ANNs中参数的数量，以避免降低性能。在这篇论文中，我们展示了对ANNs进行剪裁后，在MIP中的速度解决过程中的显著改善。我们解释了为什么剪裁在这种上下文中比其他ML压缩技术更适合，并确定了最佳剪裁策略。为了强调这种方法的潜力，我们在多层扩散神经网络中构建了反对例。我们的结果表明，剪裁可以在解决之前不可解决的实例中提供了很大的改善，而不会影响最终决策的质量。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Empower-Molecular-Property-Prediction"><a href="#Can-Large-Language-Models-Empower-Molecular-Property-Prediction" class="headerlink" title="Can Large Language Models Empower Molecular Property Prediction?"></a>Can Large Language Models Empower Molecular Property Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07443">http://arxiv.org/abs/2307.07443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chnq/llm4mol">https://github.com/chnq/llm4mol</a></li>
<li>paper_authors: Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）在分子性质预测中的应用。</li>
<li>methods: 作者采用了两个视角：零&#x2F;几 shot分子分类和使用 LL.M 生成的新解释作为分子表示。</li>
<li>results: 实验结果表明，文本解释作为分子表示在多个 benchmark 数据集上具有优势，并证明 LL.M 在分子性质预测任务中具有潜在的潜力。<details>
<summary>Abstract</summary>
Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at \url{https://github.com/ChnQ/LLM4Mol}.
</details>
<details>
<summary>摘要</summary>
摩尔电子性预测已经受到了广泛关注，因为它在多种科学领域的变革性很大。传统上，摩尔图可以被表示为图structured data或SMILES文本。在最近的几年中，大型自然语言模型（LLMs）的快速发展对涉及到NLP领域的研究带来了革命性的变革。虽然可以使用LLMs来帮助理解表示by SMILES的分子，但是研究如何使用LLMs进行分子性预测的阶段仍处于初期阶段。在这项工作中，我们通过两个视角进行推进：零/几个shot分子分类，和使用LLMs生成的新的解释来代表分子。具体来说，我们首先让LLMs在上下文中进行分子分类，并评估其性能。然后，我们使用LLMs生成Semantically enriched explanations for the original SMILES，并使用该解释来精化一个小规模LM模型以进行多个下游任务。实验结果表明，文本解释作为分子表示的 superiority across multiple benchmark datasets，并证明了LLMs在分子性预测任务中的巨大潜力。代码可以在 \url{https://github.com/ChnQ/LLM4Mol} 上获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/15/cs.AI_2023_07_15/" data-id="clot2mh7i000px7886an1a6hy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/15/cs.CL_2023_07_15/" class="article-date">
  <time datetime="2023-07-15T11:00:00.000Z" itemprop="datePublished">2023-07-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/15/cs.CL_2023_07_15/">cs.CL - 2023-07-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Three-way-Decisions-with-Evaluative-Linguistic-Expressions"><a href="#Three-way-Decisions-with-Evaluative-Linguistic-Expressions" class="headerlink" title="Three-way Decisions with Evaluative Linguistic Expressions"></a>Three-way Decisions with Evaluative Linguistic Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11766">http://arxiv.org/abs/2307.11766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefania Boffa, Davide Ciucci</li>
<li>for: 这篇论文探讨了三选一的决策问题，以及关于评价语言表达的理论。</li>
<li>methods: 该论文使用了评价语言表达来构建决策的区域，包括接受、拒绝和不确定的区域。</li>
<li>results: 研究发现了两个不同领域之间的新连接：三选一决策和评价语言表达理论。<details>
<summary>Abstract</summary>
We propose a linguistic interpretation of three-way decisions, where the regions of acceptance, rejection, and non-commitment are constructed by using the so-called evaluative linguistic expressions, which are expressions of natural language such as small, medium, very short, quite roughly strong, extremely good, etc. Our results highlight new connections between two different research areas: three-way decisions and the theory of evaluative linguistic expressions.
</details>
<details>
<summary>摘要</summary>
我们提出了一种语言解释三元决策，其中acceptance、rejection和non-commitment的区域由使用所谓的评价语言表达来构建，这些表达包括自然语言中的小、中、很短、很强、非常好等等。我们的研究结果揭示了两个不同的研究领域之间的新连接：三元决策和评价语言表达理论。
</details></li>
</ul>
<hr>
<h2 id="Opinion-mining-using-Double-Channel-CNN-for-Recommender-System"><a href="#Opinion-mining-using-Double-Channel-CNN-for-Recommender-System" class="headerlink" title="Opinion mining using Double Channel CNN for Recommender System"></a>Opinion mining using Double Channel CNN for Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07798">http://arxiv.org/abs/2307.07798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minoo Sayyadpour, Ali Nazarizadeh</li>
<li>for: 这个论文是为了提出一种基于深度学习的 sentiment analysis 方法，用于评价用户对产品的看法，并在 recommender system 中应用。</li>
<li>methods: 该方法使用了一个两核神经网络模型，包括五层核心网络和一个矩阵分解算法，以提取数据中的关键特征。 初始数据集被SMOTE算法扩展，并将数据集均衡化。然后，对各个方面进行聚合，并为每个方面分配一个权重。</li>
<li>results: 该方法在评价用户对产品的看法方面达到了 91.6% 的准确率，与之前的方面基于的方法相比有显著提高。<details>
<summary>Abstract</summary>
Much unstructured data has been produced with the growth of the Internet and social media. A significant volume of textual data includes users' opinions about products in online stores and social media. By exploring and categorizing them, helpful information can be acquired, including customer satisfaction, user feedback about a particular event, predicting the sale of a specific product, and other similar cases. In this paper, we present an approach for sentiment analysis with a deep learning model and use it to recommend products. A two-channel convolutional neural network model has been used for opinion mining, which has five layers and extracts essential features from the data. We increased the number of comments by applying the SMOTE algorithm to the initial dataset and balanced the data. Then we proceed to cluster the aspects. We also assign a weight to each cluster using tensor decomposition algorithms that improve the recommender system's performance. Our proposed method has reached 91.6% accuracy, significantly improved compared to previous aspect-based approaches.
</details>
<details>
<summary>摘要</summary>
“随着互联网和社交媒体的发展，大量的未结构化数据被生成。这些文本数据中包含用户对产品的评价，可以从中获得有益信息，如客户满意度、用户对某个事件的反馈、预测特定产品的销售等。在这篇论文中，我们提出了一种基于深度学习模型的情感分析方法，并使其用于产品推荐。我们使用了两个通道卷积神经网络模型进行意见挖掘，该模型有五层，可以从数据中提取重要特征。我们首先应用了SMOTE算法来增加数据量，然后对特征进行分类。此外，我们还使用了矩阵分解算法来赋予每个分类器一个权重，以提高推荐系统的性能。我们的提议方法已达91.6%的准确率，与之前的方面基于的方法相比有显著提高。”
</details></li>
</ul>
<hr>
<h2 id="Political-Sentiment-Analysis-of-Persian-Tweets-Using-CNN-LSTM-Model"><a href="#Political-Sentiment-Analysis-of-Persian-Tweets-Using-CNN-LSTM-Model" class="headerlink" title="Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model"></a>Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07740">http://arxiv.org/abs/2307.07740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Dehghani, Zahra Yazdanparast<br>for: 本研究的目的是使用机器学习和深度学习模型来分析波斯语政治微博上的情感。methods: 本研究使用Bag of Words和ParsBERT来表示字词，并应用 Gaussian Naive Bayes、Gradient Boosting、Logistic Regression、Decision Trees、Random Forests，以及一种 combinaison of CNN和LSTM 来类别 tweet 的方向。results: 研究结果显示，使用 ParsBERT 嵌入深度学习模型可以更好地分析波斯语政治微博上的情感，CNN-LSTM 模型在第一个数据集上取得了89%的分类精度，在第二个数据集上取得了71%的分类精度。<details>
<summary>Abstract</summary>
Sentiment analysis is the process of identifying and categorizing people's emotions or opinions regarding various topics. The analysis of Twitter sentiment has become an increasingly popular topic in recent years. In this paper, we present several machine learning and a deep learning model to analysis sentiment of Persian political tweets. Our analysis was conducted using Bag of Words and ParsBERT for word representation. We applied Gaussian Naive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random Forests, as well as a combination of CNN and LSTM to classify the polarities of tweets. The results of this study indicate that deep learning with ParsBERT embedding performs better than machine learning. The CNN-LSTM model had the highest classification accuracy with 89 percent on the first dataset with three classes and 71 percent on the second dataset with seven classes. Due to the complexity of Persian, it was a difficult task to achieve this level of efficiency.
</details>
<details>
<summary>摘要</summary>
sentiment分析是指 identificifying和 categorizing人们对不同话题的情感或意见。在最近几年，Twitter sentiment的分析已成为一个越来越流行的话题。在这篇论文中，我们提出了一些机器学习和深度学习模型，用于分析波斯政治微博的情感。我们的分析使用了Bag of Words和ParsBERT来表示单词。我们应用了 Gaussian Naive Bayes、Gradient Boosting、Logistic Regression、Decision Trees、Random Forests，以及一个组合的CNN和LSTM来分类微博的褒贬性。研究结果表明，使用ParsBERT embedding的深度学习模型在波斯政治微博中的情感分类任务中表现较好，其中CNN-LSTM模型在第一个数据集中的三类分类任务中取得了89%的分类精度，在第二个数据集中的七类分类任务中取得了71%的分类精度。由于波斯语的复杂性，这是一项具有挑战性的任务。
</details></li>
</ul>
<hr>
<h2 id="CPET-Effective-Parameter-Efficient-Tuning-for-Compressed-Large-Language-Models"><a href="#CPET-Effective-Parameter-Efficient-Tuning-for-Compressed-Large-Language-Models" class="headerlink" title="CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models"></a>CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07705">http://arxiv.org/abs/2307.07705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Maosong Sun</li>
<li>for: 这篇论文主要关注 Parameter-efficient tuning (PET) 的问题，特别是在多任务情况下如何实现优化。</li>
<li>methods: 论文使用了各种主流 LLM 压缩技术来提高 PET 性能，并引入了知识继承和恢复策略来补偿压缩技术导致的知识损失。</li>
<li>results: 论文的实验结果显示，与原始压缩 LLM 相比，使用 CPET 框架可以实现更好的性能，并且在多任务情况下与直接运用普通 PET 方法相比 OUTperform。<details>
<summary>Abstract</summary>
Parameter-efficient tuning (PET) has been widely explored in recent years because it tunes much fewer parameters (PET modules) than full-parameter fine-tuning (FT) while still stimulating sufficient knowledge from large language models (LLMs) for downstream tasks. Moreover, when PET is employed to serve multiple tasks, different task-specific PET modules can be built on a frozen LLM, avoiding redundant LLM deployments. Although PET significantly reduces the cost of tuning and deploying LLMs, its inference still suffers from the computational bottleneck of LLMs. To address the above issue, we propose an effective PET framework based on compressed LLMs, named "CPET". In CPET, we evaluate the impact of mainstream LLM compression techniques on PET performance and then introduce knowledge inheritance and recovery strategies to restore the knowledge loss caused by these compression techniques. Our experimental results demonstrate that, owing to the restoring strategies of CPET, collaborating task-specific PET modules with a compressed LLM can achieve comparable performance to collaborating PET modules with the original version of the compressed LLM and outperform directly applying vanilla PET methods to the compressed LLM.
</details>
<details>
<summary>摘要</summary>
减少参数调参 (PET) 在最近几年内得到了广泛的探索，因为它在调参 fewer parameters (PET modules) 时仍然可以从大语言模型 (LLMs) 中继承足够的知识，用于下游任务。此外，当使用 PET 服务多个任务时，可以在冻结 LLM 上建立不同任务专门的 PET modules，以避免重复的 LLM 部署。虽然 PET 可以减少调试和部署 LLMS 的成本，但其推理仍然受到 LLMS 的计算瓶颈的限制。为解决以上问题，我们提出了一个有效的 PET 框架，名为 "CPET"。在 CPET 中，我们评估了主流 LLMS 压缩技术对 PET 性能的影响，然后引入了知识继承和恢复策略，以弥补压缩技术所导致的知识损失。我们的实验结果表明， CPET 可以在压缩 LLMS 上与原始版本的压缩 LLMS 相比，并且可以超越直接应用 vanilla PET 方法。
</details></li>
</ul>
<hr>
<h2 id="Think-on-Graph-Deep-and-Responsible-Reasoning-of-Large-Language-Model-with-Knowledge-Graph"><a href="#Think-on-Graph-Deep-and-Responsible-Reasoning-of-Large-Language-Model-with-Knowledge-Graph" class="headerlink" title="Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph"></a>Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07697">http://arxiv.org/abs/2307.07697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, Jian Guo</li>
<li>for: 提高大语言模型（LLM）在多种任务中表现，但它们在复杂的推理和精准性方面经常遇到限制。</li>
<li>methods: 我们提出了 Think-on-Graph（ToG）框架，利用知识图来提升 LLM 的深入负责推理能力。 ToG 可以识别问题中关键的实体，并从外部知识库中检索相关的 triplet，进行循环的推理和检索。</li>
<li>results: 通过对复杂多层推理问答 зада务进行实验，我们表明 ToG 可以有效地解决 LLM 的上述限制，不需要额外训练成本。<details>
<summary>Abstract</summary>
Large language models (LLMs) have made significant strides in various tasks, yet they often struggle with complex reasoning and exhibit poor performance in scenarios where knowledge traceability, timeliness, and accuracy are crucial. To address these limitations, we present Think-on-Graph (ToG), a novel framework that leverages knowledge graphs to enhance LLMs' ability for deep and responsible reasoning. By employing ToG, we can identify entities relevant to a given question and conduct exploration and reasoning to retrieve related triples from an external knowledge database. This iterative procedure generates multiple reasoning pathways consisting of sequentially connected triplets until sufficient information is gathered to answer the question or the maximum depth is reached. Through experiments on complex multi-hop reasoning question-answering tasks, we demonstrate that ToG outperforms existing methods, effectively addressing the aforementioned limitations of LLMs without incurring additional training costs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Single-and-Multi-Speaker-Cloned-Voice-Detection-From-Perceptual-to-Learned-Features"><a href="#Single-and-Multi-Speaker-Cloned-Voice-Detection-From-Perceptual-to-Learned-Features" class="headerlink" title="Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features"></a>Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07683">http://arxiv.org/abs/2307.07683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-df-ucb/clonedvoicedetection">https://github.com/audio-df-ucb/clonedvoicedetection</a></li>
<li>paper_authors: Sarah Barrington, Romit Barua, Gautham Koorma, Hany Farid</li>
<li>for: 防止伪造声音，尤其是用于诈骗和伪信息运动。</li>
<li>methods: 描述三种方法来分辨真实声音和伪造声音，包括低维度感知特征、通用频率特征和端到端学习特征。</li>
<li>results: 这三种方法在训练在单一话者的声音和多话者的声音下都显示了良好的效果，并且对对抗过滤攻击也表现了良好的Robustness。<details>
<summary>Abstract</summary>
Synthetic-voice cloning technologies have seen significant advances in recent years, giving rise to a range of potential harms. From small- and large-scale financial fraud to disinformation campaigns, the need for reliable methods to differentiate real and synthesized voices is imperative. We describe three techniques for differentiating a real from a cloned voice designed to impersonate a specific person. These three approaches differ in their feature extraction stage with low-dimensional perceptual features offering high interpretability but lower accuracy, to generic spectral features, and end-to-end learned features offering less interpretability but higher accuracy. We show the efficacy of these approaches when trained on a single speaker's voice and when trained on multiple voices. The learned features consistently yield an equal error rate between $0\%$ and $4\%$, and are reasonably robust to adversarial laundering.
</details>
<details>
<summary>摘要</summary>
artifical voice 技术在过去几年内得到了 significiant advances，带来了一系列可能的害。从小规模到大规模的金融诈骗到假信息运动，需要可靠的方法来分辨真正的声音和假声音。我们描述了三种方法来分辨真正的声音和假声音，这三种方法在特征提取阶段有低维度的感知特征、通用频谱特征和终端学习的特征。我们表明这些方法在单个 speaker 的声音和多个声音上都具有较高的准确率，并且可以抵抗假钱包洗涤。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-Detection-of-Urgency-of-Discussion-Forum-Posts"><a href="#Towards-Generalizable-Detection-of-Urgency-of-Discussion-Forum-Posts" class="headerlink" title="Towards Generalizable Detection of Urgency of Discussion Forum Posts"></a>Towards Generalizable Detection of Urgency of Discussion Forum Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07614">http://arxiv.org/abs/2307.07614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pcla-code/forum-posts-urgency">https://github.com/pcla-code/forum-posts-urgency</a></li>
<li>paper_authors: Valdemar Švábenský, Ryan S. Baker, Andrés Zambrano, Yishan Zou, Stefan Slater<br>for: This paper aims to help instructors in online courses, such as MOOCs, better support student learning by automatically determining the urgency of forum posts.methods: The authors use machine learning techniques to build predictive models that determine the urgency of forum posts on a 7-point scale. They train and cross-validate several models on an original data set of 3,503 posts from MOOCs at the University of Pennsylvania, and test their performance on a separate data set of 29,604 posts from MOOCs at Stanford University.results: The best-performing model was a support vector regressor trained on the Universal Sentence Encoder embeddings of the posts, achieving an RMSE of 1.1 on the training set and 1.4 on the test set. This suggests that the model is effective in predicting the urgency of forum posts and could be used to help instructors focus their time more effectively and better support student learning.<details>
<summary>Abstract</summary>
Students who take an online course, such as a MOOC, use the course's discussion forum to ask questions or reach out to instructors when encountering an issue. However, reading and responding to students' questions is difficult to scale because of the time needed to consider each message. As a result, critical issues may be left unresolved, and students may lose the motivation to continue in the course. To help address this problem, we build predictive models that automatically determine the urgency of each forum post, so that these posts can be brought to instructors' attention. This paper goes beyond previous work by predicting not just a binary decision cut-off but a post's level of urgency on a 7-point scale. First, we train and cross-validate several models on an original data set of 3,503 posts from MOOCs at University of Pennsylvania. Second, to determine the generalizability of our models, we test their performance on a separate, previously published data set of 29,604 posts from MOOCs at Stanford University. While the previous work on post urgency used only one data set, we evaluated the prediction across different data sets and courses. The best-performing model was a support vector regressor trained on the Universal Sentence Encoder embeddings of the posts, achieving an RMSE of 1.1 on the training set and 1.4 on the test set. Understanding the urgency of forum posts enables instructors to focus their time more effectively and, as a result, better support student learning.
</details>
<details>
<summary>摘要</summary>
在线学生们，如果他们参加了MOOC课程，通常会使用课程的讨论 форум来提问或向教师们寻求帮助当遇到问题。然而，阅读和回答学生的问题需要一定的时间，因此可能会有一些重要的问题被忽略。为了解决这个问题，我们建立了一些预测模型，以便自动确定讨论 форум的帖子的紧急程度，以便将其带到教师的注意力中。这篇论文超过了之前的工作，因为它不仅预测了一个二分类决策门槛，而且预测了帖子的紧急程度在7个级别上。我们首先训练了多个模型，并对其进行跨验证。其中，我们使用大学 Pennsylvania 的 MOOC 课程数据集，训练了多个模型，并对其进行跨验证。为了证明我们的模型的一致性，我们对其进行了测试，并与之前发表的 Stanford University 的 MOOC 课程数据集进行了比较。而之前的帖子紧急性预测工作只使用了一个数据集，我们的模型则在不同的数据集和课程上进行了预测。我们最佳的模型是使用 Universal Sentence Encoder  embedding 训练的支持向量回归模型，其在训练集上的 RMSE 为1.1，测试集上的 RMSE 为1.4。理解讨论帖子的紧急程度可以帮助教师更好地利用时间，从而更好地支持学生的学习。
</details></li>
</ul>
<hr>
<h2 id="QontSum-On-Contrasting-Salient-Content-for-Query-focused-Summarization"><a href="#QontSum-On-Contrasting-Salient-Content-for-Query-focused-Summarization" class="headerlink" title="QontSum: On Contrasting Salient Content for Query-focused Summarization"></a>QontSum: On Contrasting Salient Content for Query-focused Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07586">http://arxiv.org/abs/2307.07586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajad Sotudeh, Nazli Goharian</li>
<li>for: 本研究主要探讨了Query-focused summarization（QFS）在生成文本检索中的角色，以及如何使用生成方法提高文本检索的精度和效率。</li>
<li>methods: 本研究提出了一种新的QFS方法，即QontSum，它利用对比学习方法让模型更好地关注输入文档中最相关的部分。</li>
<li>results: 对于一些 benchmark 数据集，QontSum  Either outperforms 现有状态的艺术 или 具有较低的计算成本，而不是通过大规模预训练实验来实现。此外，人工研究表明，QontSum 生成的摘要更加与问题相关，而不会产生流利性的损失。<details>
<summary>Abstract</summary>
Query-focused summarization (QFS) is a challenging task in natural language processing that generates summaries to address specific queries. The broader field of Generative Information Retrieval (Gen-IR) aims to revolutionize information extraction from vast document corpora through generative approaches, encompassing Generative Document Retrieval (GDR) and Grounded Answer Retrieval (GAR). This paper highlights the role of QFS in Grounded Answer Generation (GAR), a key subdomain of Gen-IR that produces human-readable answers in direct correspondence with queries, grounded in relevant documents. In this study, we propose QontSum, a novel approach for QFS that leverages contrastive learning to help the model attend to the most relevant regions of the input document. We evaluate our approach on a couple of benchmark datasets for QFS and demonstrate that it either outperforms existing state-of-the-art or exhibits a comparable performance with considerably reduced computational cost through enhancements in the fine-tuning stage, rather than relying on large-scale pre-training experiments, which is the focus of current SOTA. Moreover, we conducted a human study and identified improvements in the relevance of generated summaries to the posed queries without compromising fluency. We further conduct an error analysis study to understand our model's limitations and propose avenues for future research.
</details>
<details>
<summary>摘要</summary>
问题集中摘要（QFS）是自然语言处理中的一个挑战任务，它生成摘要以回答特定问题。更广泛的生成信息抽取（Gen-IR）领域希望通过生成方法来从巨大的文档库中抽取信息，包括生成文档搜寻（GDR）和固定答案搜寻（GAR）。本文强调QFS在GAR中的角色，GAR是Gen-IR的一个子领域，它生成基于问题的人阅读性的答案，并将答案与问题相对应。在这篇研究中，我们提出了一种新的QFS方法，叫做QontSum，它利用对比学习来帮助模型对输入文档中最相关的区域进行专注。我们在一些QFS的 benchmarck datasets 进行评估，并证明了QontSum Either outperforms 现有的State-of-the-art（SOTA）或与现有的SOTA相比，具有许多reduced computational cost，而不是通过大规模的预训学习实验。此外，我们进行了人类研究，并发现了生成摘要与问题之间的相关性得到了改善，而不会妥协于流畅性。 finally, we conducted an error analysis study to understand our model's limitations and proposed avenues for future research.
</details></li>
</ul>
<hr>
<h2 id="Sensi-BERT-Towards-Sensitivity-Driven-Fine-Tuning-for-Parameter-Efficient-BERT"><a href="#Sensi-BERT-Towards-Sensitivity-Driven-Fine-Tuning-for-Parameter-Efficient-BERT" class="headerlink" title="Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT"></a>Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11764">http://arxiv.org/abs/2307.11764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Souvik Kundu, Sharath Sridhar Nittur, Maciej Szankin, Sairam Sundaresan</li>
<li>for: 这篇论文的目的是提出一种高效的BERT模型 fine-tuning方法，以便在资源受限的端device上应用。</li>
<li>methods: 这篇论文使用了敏感度验证来分析BERT模型中各个参数的重要性，然后根据这些参数的重要性进行紧凑和调整。</li>
<li>results: 这篇论文的实验结果显示，Sensi-BERT可以在不同的下游任务上（包括MNLI、QQP、QNLI和SST-2）实现更好的性能，而且在相同或更小的参数预算下。<details>
<summary>Abstract</summary>
Large pre-trained language models have recently gained significant traction due to their improved performance on various down-stream tasks like text classification and question answering, requiring only few epochs of fine-tuning. However, their large model sizes often prohibit their applications on resource-constrained edge devices. Existing solutions of yielding parameter-efficient BERT models largely rely on compute-exhaustive training and fine-tuning. Moreover, they often rely on additional compute heavy models to mitigate the performance gap. In this paper, we present Sensi-BERT, a sensitivity driven efficient fine-tuning of BERT models that can take an off-the-shelf pre-trained BERT model and yield highly parameter-efficient models for downstream tasks. In particular, we perform sensitivity analysis to rank each individual parameter tensor, that then is used to trim them accordingly during fine-tuning for a given parameter or FLOPs budget. Our experiments show the efficacy of Sensi-BERT across different downstream tasks including MNLI, QQP, QNLI, and SST-2, demonstrating better performance at similar or smaller parameter budget compared to various existing alternatives.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose Sensi-BERT, a sensitivity-driven efficient fine-tuning method for BERT models that can take an off-the-shelf pre-trained BERT model and generate highly parameter-efficient models for downstream tasks. Specifically, we perform sensitivity analysis to rank each individual parameter tensor, and then trim them accordingly during fine-tuning based on a given parameter or FLOPs budget. Our experiments show the effectiveness of Sensi-BERT across different downstream tasks, including MNLI, QQP, QNLI, and SST-2, demonstrating better performance at a similar or smaller parameter budget compared to various existing alternatives.
</details></li>
</ul>
<hr>
<h2 id="Population-Expansion-for-Training-Language-Models-with-Private-Federated-Learning"><a href="#Population-Expansion-for-Training-Language-Models-with-Private-Federated-Learning" class="headerlink" title="Population Expansion for Training Language Models with Private Federated Learning"></a>Population Expansion for Training Language Models with Private Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07477">http://arxiv.org/abs/2307.07477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuki Koga, Congzheng Song, Martin Pelikan, Mona Chitnis</li>
<li>for: 提高小规模训练集的模型质量和训练效率</li>
<li>methods: 使用域 adaptive 技术扩大训练集大小，提高模型质量和训练效率</li>
<li>results: 在实际语言模型 datasets 上，提高模型质量约 13%-30%，训练效率也得到了提高。<details>
<summary>Abstract</summary>
Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 与差异隐私 (DP) 结合可以实现分布式设备上的机器学习 (ML) 训练，并且具有正式的隐私保证。在大规模设备人口中，FL 与 DP 生成的模型在时间上具有良好的性能，但是在小规模应用中，模型的性能会降低，而且等待来自小池中的足够客户端的可用性需要更长时间。为了解决这个问题，我们提议通过域 adaptation 技术来扩大人口，以加速训练和提高最终模型质量。我们通过实验表明，我们的技术可以在实际语言模型 dataset 上提高utilities 13% 到 30%。
</details></li>
</ul>
<hr>
<h2 id="Towards-spoken-dialect-identification-of-Irish"><a href="#Towards-spoken-dialect-identification-of-Irish" class="headerlink" title="Towards spoken dialect identification of Irish"></a>Towards spoken dialect identification of Irish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07436">http://arxiv.org/abs/2307.07436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Lonergan, Mengjie Qian, Neasa Ní Chiaráin, Christer Gobl, Ailbhe Ní Chasaide</li>
<li>for: 这个研究的目的是为了开发一个用于识别爱尔兰语言的语音识别系统，以解决爱尔兰语言的丰富多样性和限定的训练数据问题。</li>
<li>methods: 这个研究使用了两种音频分类模型：XLS-R和ECAPA-TDNN，以及一个基于爱尔兰语言BERT模型的文本基类器。ECAPA-TDNN模型在语言标识任务上表现最佳，具体的是在VoxLingua107数据集上预训练的ECAPA-TDNN模型的准确率达到73%。</li>
<li>results: 研究发现，ECAPA-TDNN模型在识别爱尔兰语言的不同 диалект方面表现最佳，特别是在爱尔兰北部的伦敦达利尔 диалект方面具有94%的准确率。然而，模型在康诺特和明斯特两个 диалект之间的差异化方面存在问题，建议可能需要采用更加细致的方法来稳定地分辨这两个 диаLECT。<details>
<summary>Abstract</summary>
The Irish language is rich in its diversity of dialects and accents. This compounds the difficulty of creating a speech recognition system for the low-resource language, as such a system must contend with a high degree of variability with limited corpora. A recent study investigating dialect bias in Irish ASR found that balanced training corpora gave rise to unequal dialect performance, with performance for the Ulster dialect being consistently worse than for the Connacht or Munster dialects. Motivated by this, the present experiments investigate spoken dialect identification of Irish, with a view to incorporating such a system into the speech recognition pipeline. Two acoustic classification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a text-based classifier using a pretrained Irish-language BERT model. The ECAPA-TDNN, particularly a model pretrained for language identification on the VoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was further improved to 76% by fusing the model's outputs with the text-based model. The Ulster dialect was most accurately identified, with an accuracy of 94%, however the model struggled to disambiguate between the Connacht and Munster dialects, suggesting a more nuanced approach may be necessary to robustly distinguish between the dialects of Irish.
</details>
<details>
<summary>摘要</summary>
爱尔兰语言具有多样化的方言和口音，这使得为低资源语言的语音识别系统设计更加困难，因为系统需要面对各种方言和口音的变化。一项latest study发现，在爱尔兰语音识别中，具有平衡训练数据集的系统会导致不同方言的表现不均匀，特别是北爱尔兰方言表现较差。为了解决这个问题，当前实验探索爱尔兰口音的识别，并计划将其 integrate into speech recognition pipeline。两种音频分类模型被测试，即XLS-R和ECAPA-TDNN，并与一个基于爱尔兰语言BERT模型的文本分类器进行结合。ECAPA-TDNN模型，尤其是在VoxLingua107数据集上进行语言标识训练的模型，在整体上表现最佳，准确率达73%。通过将模型的输出与文本分类器结合，准确率得到了进一步提高，达76%。北爱尔兰方言的识别率最高，达94%，但是系统在connacht和munster方言之间的干扰仍然存在， suggesting a more nuanced approach may be necessary to robustly distinguish between the dialects of Irish.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/15/cs.CL_2023_07_15/" data-id="clot2mh9l007zx78874wn43fh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/15/cs.LG_2023_07_15/" class="article-date">
  <time datetime="2023-07-15T10:00:00.000Z" itemprop="datePublished">2023-07-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/15/cs.LG_2023_07_15/">cs.LG - 2023-07-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MixupExplainer-Generalizing-Explanations-for-Graph-Neural-Networks-with-Data-Augmentation"><a href="#MixupExplainer-Generalizing-Explanations-for-Graph-Neural-Networks-with-Data-Augmentation" class="headerlink" title="MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation"></a>MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07832">http://arxiv.org/abs/2307.07832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jz48/mixupexplainer">https://github.com/jz48/mixupexplainer</a></li>
<li>paper_authors: Jiaxing Zhang, Dongsheng Luo, Hua Wei</li>
<li>for: This paper aims to address the issue of distribution shifting in post-hoc instance-level explanation methods for Graph Neural Networks (GNNs), which can lead to poor explanation quality in real-world applications with tight decision boundaries.</li>
<li>methods: The proposed approach is based on a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. The approach also uses a graph mixup method called MixupExplainer, which has a theoretical guarantee to resolve the distribution shifting issue.</li>
<li>results: The proposed MixupExplainer approach is validated through extensive experiments on both synthetic and real-world datasets, and is shown to be effective in addressing the distribution shifting issue and improving explanation quality. Additionally, the paper provides a detailed analysis of how the proposed approach alleviates the distribution shifting issue.Here is the result in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是解决图神经网络（GNNs）的后期实例级解释方法中的分布shift问题，以提高实际应用中的决策边界。</li>
<li>methods: 该方法基于一种泛化的图信息瓶颈（GIB）形式，该形式包括一个独立于标签的图变量，与普通的GIB相等。该方法还使用一种图mixup方法called MixupExplainer，该方法具有解决分布shift问题的理论保证。</li>
<li>results: 该方法通过对 sintetic和实际数据集进行了广泛的实验 validate，并证明了其能够有效地解决分布shift问题，提高解释质量。此外，论文还提供了对该方法如何缓解分布shift问题的详细分析。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. However, their predictions are often not interpretable. Post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we shed light on the existence of the distribution shifting issue in existing methods, which affects explanation quality, particularly in applications on real-life datasets with tight decision boundaries. To address this issue, we introduce a generalized Graph Information Bottleneck (GIB) form that includes a label-independent graph variable, which is equivalent to the vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method, MixupExplainer, with a theoretical guarantee to resolve the distribution shifting issue. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed mixup approach over existing approaches. We also provide a detailed analysis of how our proposed approach alleviates the distribution shifting issue.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经收到了越来越多的关注，因为它们可以从图结构数据中学习。然而，它们的预测通常不是可解释的。post-hoc实例级解释方法已经被提出，以解释训练好的 GNN 的预测行为。在这篇论文中，我们探讨了现有方法中的分布转移问题，该问题影响解释质量，特别是在实际数据集上 with tight decision boundaries 上。为解决这个问题，我们引入一种通用的图信息瓶颈（GIB）形式，该形式包括一个独立于标签的图变量，与普通的 GIB 相等。驱动于通用 GIB，我们提议一种图mixup方法，MixupExplainer，具有解决分布转移问题的理论保证。我们在 both synthetic 和实际数据集上进行了广泛的实验，以验证我们的提议的混合方法的效iveness。我们还提供了详细的分析，解释我们的提议如何缓解分布转移问题。
</details></li>
</ul>
<hr>
<h2 id="Minimal-Random-Code-Learning-with-Mean-KL-Parameterization"><a href="#Minimal-Random-Code-Learning-with-Mean-KL-Parameterization" class="headerlink" title="Minimal Random Code Learning with Mean-KL Parameterization"></a>Minimal Random Code Learning with Mean-KL Parameterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07816">http://arxiv.org/abs/2307.07816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihao Andreas Lin, Gergely Flamich, José Miguel Hernández-Lobato</li>
<li>for: 这个论文研究了两种基于Minimal Random Code Learning（MIRACLE）的变分 Bayesian neural networks的质量行为和稳定性。</li>
<li>methods: 论文使用了一种强大的、conditionally Gaussian变分approximation来 aproximate the weight posterior $Q_{\mathbf{w}$，并使用relative entropy coding来压缩一个weight sample从 posterior中使用 Gaussian coding distribution $P_{\mathbf{w}$。</li>
<li>results: 作者们发现，使用 Mean-KL 参数化可以更快 converges 并保持预测性能，并且 Mean-KL 导致了更有意义的变分分布和压缩weight sample，这些sample更易受到截彩处理。<details>
<summary>Abstract</summary>
This paper studies the qualitative behavior and robustness of two variants of Minimal Random Code Learning (MIRACLE) used to compress variational Bayesian neural networks. MIRACLE implements a powerful, conditionally Gaussian variational approximation for the weight posterior $Q_{\mathbf{w}$ and uses relative entropy coding to compress a weight sample from the posterior using a Gaussian coding distribution $P_{\mathbf{w}$. To achieve the desired compression rate, $D_{\mathrm{KL}[Q_{\mathbf{w} \Vert P_{\mathbf{w}]$ must be constrained, which requires a computationally expensive annealing procedure under the conventional mean-variance (Mean-Var) parameterization for $Q_{\mathbf{w}$. Instead, we parameterize $Q_{\mathbf{w}$ by its mean and KL divergence from $P_{\mathbf{w}$ to constrain the compression cost to the desired value by construction. We demonstrate that variational training with Mean-KL parameterization converges twice as fast and maintains predictive performance after compression. Furthermore, we show that Mean-KL leads to more meaningful variational distributions with heavier tails and compressed weight samples which are more robust to pruning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-Learning-Meets-Mental-Training-–-A-Proof-of-Concept-Applied-to-Memory-Sports"><a href="#Machine-Learning-Meets-Mental-Training-–-A-Proof-of-Concept-Applied-to-Memory-Sports" class="headerlink" title="Machine Learning Meets Mental Training – A Proof of Concept Applied to Memory Sports"></a>Machine Learning Meets Mental Training – A Proof of Concept Applied to Memory Sports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08712">http://arxiv.org/abs/2307.08712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuele Regnani</li>
<li>for: 这个研究旨在结合机器学习和记忆运动两个领域，以实现一种实用的机器学习应用于记忆运动的实践。</li>
<li>methods: 该研究使用了机器学习算法，包括支持向量机和归一化树，来分析记忆运动中的数据。</li>
<li>results: 研究发现，通过使用机器学习算法，可以提高记忆运动的效果和精度，并且可以预测记忆运动的成绩。<details>
<summary>Abstract</summary>
This work aims to combine these two fields together by presenting a practical implementation of machine learning to the particular form of mental training that is the art of memory, taken in its competitive version called "Memory Sports". Such a fusion, on the one hand, strives to raise awareness about both realms, while on the other it seeks to encourage research in this mixed field as a way to, ultimately, drive forward the development of this seemingly underestimated sport.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)这项工作 aimsto combine these two fields together by presenting a practical implementation of machine learning to the particular form of mental training that is the art of memory, taken in its competitive version called "Memory Sports". Such a fusion, on the one hand, strives to raise awareness about both realms, while on the other it seeks to encourage research in this mixed field as a way to, ultimately, drive forward the development of this seemingly underestimated sport.Note: The word " Memory Sports" is not a direct translation of "Memory Sports" in Chinese, but it is a commonly used term in the field to refer to competitive memory training.
</details></li>
</ul>
<hr>
<h2 id="Graph-Automorphism-Group-Equivariant-Neural-Networks"><a href="#Graph-Automorphism-Group-Equivariant-Neural-Networks" class="headerlink" title="Graph Automorphism Group Equivariant Neural Networks"></a>Graph Automorphism Group Equivariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07810">http://arxiv.org/abs/2307.07810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Pearce-Crump</li>
<li>for: 这种研究的目的是对任意有 $n$ 个顶点的图 $G$ 和其自动同态群 $\textrm{Aut}(G)$ 进行全面的 caracterization，即确定所有可能的 $\textrm{Aut}(G)$-equivariant neural network 的层次结构，其层次空间是 $\mathbb{R}^{n}$ 的 tensor power。</li>
<li>methods: 这种研究使用了learnable、线性、$\textrm{Aut}(G)$-equivariant层函数的 span set 来 characterize 所有可能的层次结构。</li>
<li>results: 研究发现，对于任意的图 $G$ 和 $\textrm{Aut}(G)$,存在一个 span set of matrices 表示所有可能的 learnable、线性、$\textrm{Aut}(G)$-equivariant层函数，并且这些层函数可以在标准基底上表示 $\mathbb{R}^{n}$ 中的所有 tensor power。<details>
<summary>Abstract</summary>
For any graph $G$ having $n$ vertices and its automorphism group $\textrm{Aut}(G)$, we provide a full characterisation of all of the possible $\textrm{Aut}(G)$-equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. In particular, we find a spanning set of matrices for the learnable, linear, $\textrm{Aut}(G)$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:For any graph $G$ with $n$ vertices, we provide a full characterization of all possible $\textrm{Aut}(G)$-equivariant neural networks whose layers are some tensor power of $\mathbb{R}^{n}$. Specifically, we find a spanning set of matrices for the learnable, linear, $\textrm{Aut}(G)$-equivariant layer functions between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$.Note: "tensor power" is not a standard term in Simplified Chinese, so I used the phrase "some tensor power" to convey the same meaning.
</details></li>
</ul>
<hr>
<h2 id="text-EFO-k-CQA-Towards-Knowledge-Graph-Complex-Query-Answering-beyond-Set-Operation"><a href="#text-EFO-k-CQA-Towards-Knowledge-Graph-Complex-Query-Answering-beyond-Set-Operation" class="headerlink" title="$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation"></a>$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13701">http://arxiv.org/abs/2307.13701</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/efok-cqa">https://github.com/hkust-knowcomp/efok-cqa</a></li>
<li>paper_authors: Hang Yin, Zihao Wang, Weizhi Fei, Yangqiu Song</li>
<li>for: 本研究的目的是提供一个涵盖多变量existential first-order queries（EFO）的完整框架，并评估这些方法在这个框架下的性能。</li>
<li>methods: 本研究使用了一些学习基本的方法，以扩展现有的知识 гра图学习方法，并将其应用到EFO queries中。</li>
<li>results: 本研究提出了一个名为 $\text{EFO}_{k}$-CQA的新数据集，并通过实验评估了这些方法在不同的查询难度下的性能。results also show that the existing dataset construction process is biased, highlighting the importance of the proposed framework.<details>
<summary>Abstract</summary>
To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\text{EFO}_{k}$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\text{EFO}_{k}$-CQA, with 741 types of query for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased that hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\url{https://github.com/HKUST-KnowComp/EFOK-CQA}.
</details>
<details>
<summary>摘要</summary>
“为回答知识图中复杂的查询，因为开放世界假设，需要逻辑推理 sobre 不完全的知识。学习基于方法是必要的，因为它们可以对未观察到的知识进行泛化。因此，一个适当的数据集是知识推理方法的基础，以及评估这些方法的基础。在这篇论文中，我们提出了一个完整的框架，包括数据生成、模型训练和方法评估，覆盖了多变量($\text{EFO}_{k}$)的组合空间。我们的框架中的组合查询空间significantly extends those defined by set operations in the existing literature。此外，我们构建了741种类型的查询集，并提供了empirical evaluation，我们的研究结果提供了新的视角，描述了查询困难度对结果的影响。此外，我们还发现了现有数据集构建过程存在系统性的偏见，这阻碍了适当的查询答案方法的发展，强调了我们的工作的重要性。我们的代码和数据可以在\url{https://github.com/HKUST-KnowComp/EFOK-CQA}中找到。”
</details></li>
</ul>
<hr>
<h2 id="The-Interpolating-Information-Criterion-for-Overparameterized-Models"><a href="#The-Interpolating-Information-Criterion-for-Overparameterized-Models" class="headerlink" title="The Interpolating Information Criterion for Overparameterized Models"></a>The Interpolating Information Criterion for Overparameterized Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07785">http://arxiv.org/abs/2307.07785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Hodgkinson, Chris van der Heide, Robert Salomone, Fred Roosta, Michael W. Mahoney</li>
<li>for:  interpolating estimators with overparameterized models</li>
<li>methods:  using classical information criteria, Bayesian duality, and prior misspecification</li>
<li>results:  a new information criterion called Interpolating Information Criterion (IIC) that accounts for prior misspecification, geometric and spectral properties of the model, and is numerically consistent with known empirical and theoretical behavior in the overparameterized setting<details>
<summary>Abstract</summary>
The problem of model selection is considered for the setting of interpolating estimators, where the number of model parameters exceeds the size of the dataset. Classical information criteria typically consider the large-data limit, penalizing model size. However, these criteria are not appropriate in modern settings where overparameterized models tend to perform well. For any overparameterized model, we show that there exists a dual underparameterized model that possesses the same marginal likelihood, thus establishing a form of Bayesian duality. This enables more classical methods to be used in the overparameterized setting, revealing the Interpolating Information Criterion, a measure of model quality that naturally incorporates the choice of prior into the model selection. Our new information criterion accounts for prior misspecification, geometric and spectral properties of the model, and is numerically consistent with known empirical and theoretical behavior in this regime.
</details>
<details>
<summary>摘要</summary>
“模型选择问题在 interpolating estimators 的设置下被考虑，其中模型参数的数量超出数据集的大小。经典信息critérium通常在大数据 limit 下考虑模型大小，但这些 критериion 不适用于现代设置， где过参数化模型往往表现良好。我们证明，任何过参数化模型都存在一个对应的 dual underparameterized model，这两个模型具有同样的边缘分布，从而建立了一种 Bayesian duality。这使得更 classical methods 可以在过参数化 Setting 中使用，揭示了 interpolating information criterion，一种评价模型质量的指标，这个指标自然地包括先验选择的选择。我们的新信息 критериion 考虑了先验错误、模型的几何和спектраль性质，与已知的 empirical 和理论行为相一致。”
</details></li>
</ul>
<hr>
<h2 id="CatBoost-Versus-XGBoost-and-LightGBM-Developing-Enhanced-Predictive-Models-for-Zero-Inflated-Insurance-Claim-Data"><a href="#CatBoost-Versus-XGBoost-and-LightGBM-Developing-Enhanced-Predictive-Models-for-Zero-Inflated-Insurance-Claim-Data" class="headerlink" title="CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data"></a>CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07771">http://arxiv.org/abs/2307.07771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Banghee So</li>
<li>for: 这 paper 是为了构建投保laim predictive模型而写的，面临着高度右偏度分布的正确laims 和过多的 zeros 的挑战。</li>
<li>methods: 这 paper 使用了 zero-inflated 模型，将 traditional count model 和 binary model 结合起来，更有效地处理投保laim 数据。</li>
<li>results: 经过对两个不同的数据集的分析和比较， CatBoost 库在建立汽车投保laim frequency 模型方面表现最佳，并且发现 zero-inflated Poisson 树模型在不同数据特点下的假设对于relation between inflation probability and distribution mean 的变化会影响其性能。 I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
In the property and casualty insurance industry, some challenges are presented in constructing claim predictive models due to a highly right-skewed distribution of positive claims with excess zeros. Traditional models, such as Poisson or negative binomial Generalized Linear Models(GLMs), frequently struggle with inflated zeros. In response to this, researchers in actuarial science have employed ``zero-inflated" models that merge a traditional count model and a binary model to address these datasets more effectively. This paper uses boosting algorithms to process insurance claim data, including zero-inflated telematics data, in order to construct claim frequency models. We evaluated and compared three popular gradient boosting libraries - XGBoost, LightGBM, and CatBoost - with the aim of identifying the most suitable library for training insurance claim data and fitting actuarial frequency models. Through a rigorous analysis of two distinct datasets, we demonstrated that CatBoost is superior in developing auto claim frequency models based on predictive performance. We also found that Zero-inflated Poisson boosted tree models, with variations in their assumptions about the relationship between inflation probability and distribution mean, outperformed others depending on data characteristics. Furthermore, by using a specific CatBoost tool, we explored the effects and interactions of different risk features on the frequency model when using telematics data.
</details>
<details>
<summary>摘要</summary>
在财产和责任保险业务中，建立投保模型时会遇到一些挑战，主要是因为投保金额呈右skewed分布，具有过多的零值。传统模型，如波尔tz或非正态泛化模型（GLM），经常遇到膨胀零值问题。为了解决这个问题， actuarial science 研究人员使用了“zero-inflated”模型，这种模型结合了传统的计数模型和二分模型，可以更有效地处理这些数据。本文使用了扩大算法来处理投保laim data，包括零Inflated telematics data，以建立投保频率模型。我们对三种popular gradient boosting库（XGBoost、LightGBM、CatBoost）进行了评估和比较，以确定最适合训练投保laim数据和适应保险频率模型的库。经过对两个不同的数据集的严格分析，我们发现CatBoost在开发汽车投保频率模型方面表现出色，并且对数据特点进行了深入的探索和分析。此外，我们还使用了CatBoost工具来探索不同风险特征对频率模型的影响，并对telematics数据进行了深入的分析。
</details></li>
</ul>
<hr>
<h2 id="randomHAR-Improving-Ensemble-Deep-Learners-for-Human-Activity-Recognition-with-Sensor-Selection-and-Reinforcement-Learning"><a href="#randomHAR-Improving-Ensemble-Deep-Learners-for-Human-Activity-Recognition-with-Sensor-Selection-and-Reinforcement-Learning" class="headerlink" title="randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition with Sensor Selection and Reinforcement Learning"></a>randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition with Sensor Selection and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07770">http://arxiv.org/abs/2307.07770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiran Huang, Yexu Zhou, Till Riedel, Likun Fang, Michael Beigl</li>
<li>for: 提高人体动作识别（HAR）领域中的表现，并且超越其他需要手动工程Feature的建筑。</li>
<li>methods: 使用随机选择数据集中的感知器数据来训练多个深度学习模型，并使用强化学习算法来选择最佳的模型 subsets 用于运行预测。</li>
<li>results: 对六个HAR数据集进行比较，结果表明提议的方法可以超越当前状态的各种方法，包括ensembleLSTM。<details>
<summary>Abstract</summary>
Deep learning has proven to be an effective approach in the field of Human activity recognition (HAR), outperforming other architectures that require manual feature engineering. Despite recent advancements, challenges inherent to HAR data, such as noisy data, intra-class variability and inter-class similarity, remain. To address these challenges, we propose an ensemble method, called randomHAR. The general idea behind randomHAR is training a series of deep learning models with the same architecture on randomly selected sensor data from the given dataset. Besides, an agent is trained with the reinforcement learning algorithm to identify the optimal subset of the trained models that are utilized for runtime prediction. In contrast to existing work, this approach optimizes the ensemble process rather than the architecture of the constituent models. To assess the performance of the approach, we compare it against two HAR algorithms, including the current state of the art, on six HAR benchmark datasets. The result of the experiment demonstrates that the proposed approach outperforms the state-of-the-art method, ensembleLSTM.
</details>
<details>
<summary>摘要</summary>
深度学习在人动识别（HAR）领域已经证明是一种有效的方法，超过了需要人工特征工程的其他架构。 DESPITE recent advancements, HAR数据中的挑战，如噪音数据、内类变化和间类相似性，仍然存在。 To address these challenges, we propose an ensemble method, called randomHAR. The general idea behind randomHAR is to train a series of deep learning models with the same architecture on randomly selected sensor data from the given dataset. Besides, an agent is trained with the reinforcement learning algorithm to identify the optimal subset of the trained models that are utilized for runtime prediction. In contrast to existing work, this approach optimizes the ensemble process rather than the architecture of the constituent models. To assess the performance of the approach, we compare it against two HAR algorithms, including the current state of the art, on six HAR benchmark datasets. The result of the experiment demonstrates that the proposed approach outperforms the state-of-the-art method, ensembleLSTM.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Variational-Monte-Carlo-on-a-Budget-–-Fine-tuning-pre-trained-Neural-Wavefunctions"><a href="#Variational-Monte-Carlo-on-a-Budget-–-Fine-tuning-pre-trained-Neural-Wavefunctions" class="headerlink" title="Variational Monte Carlo on a Budget – Fine-tuning pre-trained Neural Wavefunctions"></a>Variational Monte Carlo on a Budget – Fine-tuning pre-trained Neural Wavefunctions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09337">http://arxiv.org/abs/2307.09337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mdsunivie/deeperwin">https://github.com/mdsunivie/deeperwin</a></li>
<li>paper_authors: Michael Scherbela, Leon Gerard, Philipp Grohs</li>
<li>for: 这 paper 的目的是提出一种基于深度学习的变量 Monte Carlo（DL-VMC）方法，以提高计算量化化学中的精度。</li>
<li>methods: 这 paper 使用了自我超vised wavefunction optimization 来预训练 DL-VMC 模型，并在新的分子实例上应用这个模型来获得更高的精度。</li>
<li>results:  compared to established methods such as CCSD(T)-2Z, 这 paper 的方法可以获得更高的精度和更好的相对能量。 In addition, the method can be applied to a wide variety of test systems and shows good scalability.<details>
<summary>Abstract</summary>
Obtaining accurate solutions to the Schr\"odinger equation is the key challenge in computational quantum chemistry. Deep-learning-based Variational Monte Carlo (DL-VMC) has recently outperformed conventional approaches in terms of accuracy, but only at large computational cost. Whereas in many domains models are trained once and subsequently applied for inference, accurate DL-VMC so far requires a full optimization for every new problem instance, consuming thousands of GPUhs even for small molecules. We instead propose a DL-VMC model which has been pre-trained using self-supervised wavefunction optimization on a large and chemically diverse set of molecules. Applying this model to new molecules without any optimization, yields wavefunctions and absolute energies that outperform established methods such as CCSD(T)-2Z. To obtain accurate relative energies, only few fine-tuning steps of this base model are required. We accomplish this with a fully end-to-end machine-learned model, consisting of an improved geometry embedding architecture and an existing SE(3)-equivariant model to represent molecular orbitals. Combining this architecture with continuous sampling of geometries, we improve zero-shot accuracy by two orders of magnitude compared to the state of the art. We extensively evaluate the accuracy, scalability and limitations of our base model on a wide variety of test systems.
</details>
<details>
<summary>摘要</summary>
computational quantum chemistry中的主要挑战是获取准确的Schrödinger方程解。深度学习基于变量 Monte Carlo（DL-VMC）在过去几年内已经超越了传统方法，但是它们的计算成本很大。在许多领域中，模型会被训练一次并用于推理，而DL-VMC则需要每个新问题都进行全局优化，消耗了千个GPUhs甚至对于小分子来说。我们提议一种已经预训练过的DL-VMC模型，使用自动优化的自我适应波函数优化算法来训练。对于新的分子，只需要几个精度调整步骤，就可以获得比CCSD(T)-2Z更高的精度。为了获取准确的相对能量，我们使用一个完整的端到端机器学习模型，包括改进的几何嵌入体系和现有的SE(3)-可变模型来表示分子轨道函数。将这种体系与连续样本的几何描述相结合，我们提高了零shot精度至少两个数量级比前state of the art。我们对各种测试系统进行了广泛的评估，包括准确度、可扩展性和限制。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Traffic-Classification-for-5G-NSA-Encrypted-Data-Flows-With-Physical-Channel-Records"><a href="#Real-time-Traffic-Classification-for-5G-NSA-Encrypted-Data-Flows-With-Physical-Channel-Records" class="headerlink" title="Real-time Traffic Classification for 5G NSA Encrypted Data Flows With Physical Channel Records"></a>Real-time Traffic Classification for 5G NSA Encrypted Data Flows With Physical Channel Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07756">http://arxiv.org/abs/2307.07756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Fei, Philippe Martins, Jialiang Lu</li>
<li>for: 5G-NR mobile network traffic classification for QoS management and dynamic resource allocation</li>
<li>methods: real-time encrypted traffic classification using physical channel records and decision-tree-based gradient boosting algorithms</li>
<li>results: 95% accuracy with state-of-the-art response time of 10ms using Light Gradient Boosting Machine (LGBM)<details>
<summary>Abstract</summary>
The classification of fifth-generation New-Radio (5G-NR) mobile network traffic is an emerging topic in the field of telecommunications. It can be utilized for quality of service (QoS) management and dynamic resource allocation. However, traditional approaches such as Deep Packet Inspection (DPI) can not be directly applied to encrypted data flows. Therefore, new real-time encrypted traffic classification algorithms need to be investigated to handle dynamic transmission. In this study, we examine the real-time encrypted 5G Non-Standalone (NSA) application-level traffic classification using physical channel records. Due to the vastness of their features, decision-tree-based gradient boosting algorithms are a viable approach for classification. We generate a noise-limited 5G NSA trace dataset with traffic from multiple applications. We develop a new pipeline to convert sequences of physical channel records into numerical vectors. A set of machine learning models are tested, and we propose our solution based on Light Gradient Boosting Machine (LGBM) due to its advantages in fast parallel training and low computational burden in practical scenarios. Our experiments demonstrate that our algorithm can achieve 95% accuracy on the classification task with a state-of-the-art response time as quick as 10ms.
</details>
<details>
<summary>摘要</summary>
fifth-generation New-Radio (5G-NR) 移动网络流量的分类是当前 телеcommunications 领域的一个热点话题。它可以用于质量服务（QoS）管理和动态资源分配。然而，传统的方法，如深度包检查（DPI），无法直接应用于加密数据流。因此，新的实时加密交通分类算法需要被研究以处理动态传输。在本研究中，我们研究了实时加密5G非标准应用级别（NSA）的应用级别流量分类，使用物理通道记录。由于它们的特征很多，决策树基本的泵浦搅拌算法是一种可行的方法。我们生成了5G NSA的噪声限定数据集，包括多个应用程序的流量。我们开发了一个新的管道，将物理通道记录序列转换为数字矢量。一系列机器学习模型被测试，我们提议使用光 Gradient Boosting Machine（LGBM），因为它在实际应用中具有快速并行训练和低计算负担的优点。我们的实验表明，我们的算法可以在分类任务上达到95%的准确率，并且响应时间只需10ms。
</details></li>
</ul>
<hr>
<h2 id="Learning-Expressive-Priors-for-Generalization-and-Uncertainty-Estimation-in-Neural-Networks"><a href="#Learning-Expressive-Priors-for-Generalization-and-Uncertainty-Estimation-in-Neural-Networks" class="headerlink" title="Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks"></a>Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07753">http://arxiv.org/abs/2307.07753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dlr-rm/bpnn">https://github.com/dlr-rm/bpnn</a></li>
<li>paper_authors: Dominik Schnaus, Jongseok Lee, Daniel Cremers, Rudolph Triebel</li>
<li>for: 本文提出了一种新的先学习方法，用于提高深度神经网络的通用化和不确定性估计。</li>
<li>methods: 本文使用了可扩展的结构化 posterior 方法，以获得具有普遍保证的通用化表达。我们的学习的先验提供了具有表达能力的概率表示，类似于 Bayesian 对 ImageNet 预训练模型的Counterparts，并且生成了非虚无的泛化 bound。</li>
<li>results: 我们通过实验证明了这种方法的效果，包括不确定性估计和通用化。<details>
<summary>Abstract</summary>
In this work, we propose a novel prior learning method for advancing generalization and uncertainty estimation in deep neural networks. The key idea is to exploit scalable and structured posteriors of neural networks as informative priors with generalization guarantees. Our learned priors provide expressive probabilistic representations at large scale, like Bayesian counterparts of pre-trained models on ImageNet, and further produce non-vacuous generalization bounds. We also extend this idea to a continual learning framework, where the favorable properties of our priors are desirable. Major enablers are our technical contributions: (1) the sums-of-Kronecker-product computations, and (2) the derivations and optimizations of tractable objectives that lead to improved generalization bounds. Empirically, we exhaustively show the effectiveness of this method for uncertainty estimation and generalization.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的先学习方法，用于提高深度神经网络的泛化和不确定性估计。关键思想是利用可扩展和结构化的神经网络 posterior 作为有用的先学习模型，具有泛化保证。我们学习的先学习模型可以在大规模上表达可信度，类似于 Bayesian 对 ImageNet 预训练模型的Counterpart，并且生成非虚无效的泛化误差 bound。我们还将这个想法应用于连续学习框架，其中我们的先学习模型具有恰当的性质。主要推动因素是我们的技术贡献：（1） Kronecker 乘积计算，以及（2）对可迭代目标函数的 derivation 和优化，导致改进的泛化误差 bound。在实验中，我们详细展示了这种方法的效果，包括不确定性估计和泛化。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Black-Box-Checking-via-Active-MDP-Learning"><a href="#Probabilistic-Black-Box-Checking-via-Active-MDP-Learning" class="headerlink" title="Probabilistic Black-Box Checking via Active MDP Learning"></a>Probabilistic Black-Box Checking via Active MDP Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07930">http://arxiv.org/abs/2308.07930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junya Shijubo, Masaki Waga, Kohei Suenaga</li>
<li>for: 测试黑盒系统的概率性行为</li>
<li>methods: 使用活动Markov决策过程学习、概率模型检查和统计假设测试</li>
<li>results: ProbBBC比现有方法更高效，特别是对具有有限观察的系统。<details>
<summary>Abstract</summary>
We introduce a novel methodology for testing stochastic black-box systems, frequently encountered in embedded systems. Our approach enhances the established black-box checking (BBC) technique to address stochastic behavior. Traditional BBC primarily involves iteratively identifying an input that breaches the system's specifications by executing the following three phases: the learning phase to construct an automaton approximating the black box's behavior, the synthesis phase to identify a candidate counterexample from the learned automaton, and the validation phase to validate the obtained candidate counterexample and the learned automaton against the original black-box system. Our method, ProbBBC, refines the conventional BBC approach by (1) employing an active Markov Decision Process (MDP) learning method during the learning phase, (2) incorporating probabilistic model checking in the synthesis phase, and (3) applying statistical hypothesis testing in the validation phase. ProbBBC uniquely integrates these techniques rather than merely substituting each method in the traditional BBC; for instance, the statistical hypothesis testing and the MDP learning procedure exchange information regarding the black-box system's observation with one another. The experiment results suggest that ProbBBC outperforms an existing method, especially for systems with limited observation.
</details>
<details>
<summary>摘要</summary>
我们介绍一种新的黑盒系统测试方法，这种方法可以更好地捕捉黑盒系统中的随机行为。我们的方法基于传统的黑盒检查（BBC）技术，但它具有以下三个特点：1. 在学习阶段使用活动马尔可夫遇处理（MDP）学习方法，以更好地模拟黑盒系统的行为。2. 在合成阶段使用概率模型检查，以更好地找到黑盒系统的错误。3. 在验证阶段使用统计假设检测，以验证获得的候选反例和学习的黑盒系统是否符合原始黑盒系统。我们的方法不同于传统的 BBC 方法，不仅是将每种方法简单地替换成另一种。例如，统计假设检测和 MDP 学习过程之间会互相交换黑盒系统的观察信息。我们的实验结果表明，ProbBBC 比现有的方法更高效，特别是针对具有有限观察的系统。
</details></li>
</ul>
<hr>
<h2 id="On-the-Utility-Gain-of-Iterative-Bayesian-Update-for-Locally-Differentially-Private-Mechanisms"><a href="#On-the-Utility-Gain-of-Iterative-Bayesian-Update-for-Locally-Differentially-Private-Mechanisms" class="headerlink" title="On the Utility Gain of Iterative Bayesian Update for Locally Differentially Private Mechanisms"></a>On the Utility Gain of Iterative Bayesian Update for Locally Differentially Private Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07744">http://arxiv.org/abs/2307.07744</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hharcolezi/multi-freq-ldpy">https://github.com/hharcolezi/multi-freq-ldpy</a></li>
<li>paper_authors: Héber H. Arcolezi, Selene Cerna, Catuscia Palamidessi</li>
<li>for: 本研究 investigate了使用 Iterative Bayesian Update (IBU) 提高 private discrete distribution 估计中的实用性，使用受到 Locally Differentially Private (LDP) 机制的数据干扰。</li>
<li>methods: 我们比较了 IBU 和 Matrix Inversion (MI) 两种估计技术的性能，对七种LDP机制进行了一次数据收集和多次数据收集的比较（如 RAPPOR）。我们还在不同的实用环境下（包括 synthetic 数据和实际数据）进行了参数调整（包括 utility 度量、用户数 n、领域大小 k 和隐私参数 {\epsilon}）。</li>
<li>results: 我们的结果表明，IBU 可以在不同的场景下提高 LDP 机制的实用性，而不需要额外的隐私成本。例如，在高隐私 режи（即 {\epsilon} 小）下，IBU 可以提供更好的实用性比 MI。我们的研究为实践者提供了使用 IBU 和现有 LDP 机制进行更准确和隐私保护的数据分析的指导。此外，我们将 IBU 实现到了 state-of-the-art multi-freq-ldpy Python 包（<a target="_blank" rel="noopener" href="https://pypi.org/project/multi-freq-ldpy/%EF%BC%89%E4%B8%AD%EF%BC%8C%E5%B9%B6%E5%B0%86%E6%89%80%E6%9C%89%E6%88%91%E4%BB%AC%E7%94%A8%E4%BA%8E%E5%AE%9E%E9%AA%8C%E7%9A%84%E4%BB%A3%E7%A0%81%E5%BC%80%E6%BA%90%E4%BA%86%E4%B8%BA">https://pypi.org/project/multi-freq-ldpy/）中，并将所有我们用于实验的代码开源了为</a> tutorials。<details>
<summary>Abstract</summary>
This paper investigates the utility gain of using Iterative Bayesian Update (IBU) for private discrete distribution estimation using data obfuscated with Locally Differentially Private (LDP) mechanisms. We compare the performance of IBU to Matrix Inversion (MI), a standard estimation technique, for seven LDP mechanisms designed for one-time data collection and for other seven LDP mechanisms designed for multiple data collections (e.g., RAPPOR). To broaden the scope of our study, we also varied the utility metric, the number of users n, the domain size k, and the privacy parameter {\epsilon}, using both synthetic and real-world data. Our results suggest that IBU can be a useful post-processing tool for improving the utility of LDP mechanisms in different scenarios without any additional privacy cost. For instance, our experiments show that IBU can provide better utility than MI, especially in high privacy regimes (i.e., when {\epsilon} is small). Our paper provides insights for practitioners to use IBU in conjunction with existing LDP mechanisms for more accurate and privacy-preserving data analysis. Finally, we implemented IBU for all fourteen LDP mechanisms into the state-of-the-art multi-freq-ldpy Python package (https://pypi.org/project/multi-freq-ldpy/) and open-sourced all our code used for the experiments as tutorials.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Knowledge-Graph-Enhanced-Intelligent-Tutoring-System-Based-on-Exercise-Representativeness-and-Informativeness"><a href="#Knowledge-Graph-Enhanced-Intelligent-Tutoring-System-Based-on-Exercise-Representativeness-and-Informativeness" class="headerlink" title="Knowledge Graph Enhanced Intelligent Tutoring System Based on Exercise Representativeness and Informativeness"></a>Knowledge Graph Enhanced Intelligent Tutoring System Based on Exercise Representativeness and Informativeness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15076">http://arxiv.org/abs/2307.15076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linqing Li, Zhifeng Wang</li>
<li>for: 提高学生的性能，适应不同学生的学习需求</li>
<li>methods: 基于知识图建立一个权重计算模型，考虑了知识图中的多种关系，并使用了新型的神经网络诊断模型</li>
<li>results: 对两个公共教育数据集进行了广泛的实验，结果表明，该 framwork 可以更好地推荐适合学生的练习题，提高学生的性能<details>
<summary>Abstract</summary>
Presently, knowledge graph-based recommendation algorithms have garnered considerable attention among researchers. However, these algorithms solely consider knowledge graphs with single relationships and do not effectively model exercise-rich features, such as exercise representativeness and informativeness. Consequently, this paper proposes a framework, namely the Knowledge-Graph-Exercise Representativeness and Informativeness Framework, to address these two issues. The framework consists of four intricate components and a novel cognitive diagnosis model called the Neural Attentive cognitive diagnosis model. These components encompass the informativeness component, exercise representation component, knowledge importance component, and exercise representativeness component. The informativeness component evaluates the informational value of each question and identifies the candidate question set that exhibits the highest exercise informativeness. Furthermore, the skill embeddings are employed as input for the knowledge importance component. This component transforms a one-dimensional knowledge graph into a multi-dimensional one through four class relations and calculates skill importance weights based on novelty and popularity. Subsequently, the exercise representativeness component incorporates exercise weight knowledge coverage to select questions from the candidate question set for the tested question set. Lastly, the cognitive diagnosis model leverages exercise representation and skill importance weights to predict student performance on the test set and estimate their knowledge state. To evaluate the effectiveness of our selection strategy, extensive experiments were conducted on two publicly available educational datasets. The experimental results demonstrate that our framework can recommend appropriate exercises to students, leading to improved student performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:当前，基于知识图的推荐算法已经吸引了研究人员的广泛关注。然而，这些算法只考虑单 relate 知识图，并不能有效地模型运动rich feature，如运动 representativeness 和 informativeness。因此，本文提出了一个框架，即知识图运动 representativeness 和 informativeness 框架，以解决这两个问题。该框架包括四个复杂的组件和一个新的认知诊断模型called Neural Attentive cognitive diagnosis model。这些组件包括 informativeness 组件、运动表现组件、知识重要性组件和运动 representativeness 组件。informativeness 组件评估每个问题的信息价值，并将候选问题集定为展示最高运动 informativeness。此外，技能嵌入被用作知识重要性组件的输入。这个组件通过四种类关系将一维知识图转换为多维知识图，并计算技能重要性 weights 基于新鲜度和流行度。然后，运动 representativeness 组件将运动权重知识覆盖纳入选择候选问题集的 tested question set。最后，认知诊断模型通过运动表现和技能重要性 weights 预测学生在测试集上的表现和知识状态。为评估我们的选择策略的效果，我们在两个公共可用的教育数据集上进行了广泛的实验。实验结果表明，我们的框架可以为学生推荐适合的运动，从而提高学生的表现。
</details></li>
</ul>
<hr>
<h2 id="Promotion-Inhibition-Effects-in-Networks-A-Model-with-Negative-Probabilities"><a href="#Promotion-Inhibition-Effects-in-Networks-A-Model-with-Negative-Probabilities" class="headerlink" title="Promotion&#x2F;Inhibition Effects in Networks: A Model with Negative Probabilities"></a>Promotion&#x2F;Inhibition Effects in Networks: A Model with Negative Probabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07738">http://arxiv.org/abs/2307.07738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anqi Dong, Tryphon T. Georgiou, Allen Tannenbaum</li>
<li>for: 本研究旨在解决基因网络中Edge-weight的 inverse problem，即根据签入式互连矩阵和表达水平确定Edge-weight。</li>
<li>methods: 本研究采用了P。Dirac和R。Feynman提出的“负概率”框架，并设立了可能性形式来获得Edge-weight的值。 solve this problem, the proposed optimization problem can be solved via a generalization of the well-known Sinkhorn algorithm.</li>
<li>results: 本研究得到了一种基于“负概率”框架的方法，可以在基因网络中确定Edge-weight，并且这种方法可以通过一种扩展的Sinkhorn算法来解决。<details>
<summary>Abstract</summary>
Biological networks often encapsulate promotion/inhibition as signed edge-weights of a graph. Nodes may correspond to genes assigned expression levels (mass) of respective proteins. The promotion/inhibition nature of co-expression between nodes is encoded in the sign of the corresponding entry of a sign-indefinite adjacency matrix, though the strength of such co-expression (i.e., the precise value of edge weights) cannot typically be directly measured. Herein we address the inverse problem to determine network edge-weights based on a sign-indefinite adjacency and expression levels at the nodes. While our motivation originates in gene networks, the framework applies to networks where promotion/inhibition dictates a stationary mass distribution at the nodes. In order to identify suitable edge-weights we adopt a framework of ``negative probabilities,'' advocated by P.\ Dirac and R.\ Feynman, and we set up a likelihood formalism to obtain values for the sought edge-weights. The proposed optimization problem can be solved via a generalization of the well-known Sinkhorn algorithm; in our setting the Sinkhorn-type ``diagonal scalings'' are multiplicative or inverse-multiplicative, depending on the sign of the respective entries in the adjacency matrix, with value computed as the positive root of a quadratic polynomial.
</details>
<details>
<summary>摘要</summary>
To identify suitable edge weights, we adopt a framework of "negative probabilities" advocated by P. Dirac and R. Feynman. We set up a likelihood formalism to obtain values for the sought edge weights. The proposed optimization problem can be solved using a generalization of the well-known Sinkhorn algorithm; in our setting, the Sinkhorn-type "diagonal scalings" are multiplicative or inverse-multiplicative, depending on the sign of the respective entries in the adjacency matrix, with values computed as the positive root of a quadratic polynomial.
</details></li>
</ul>
<hr>
<h2 id="Measuring-Perceived-Trust-in-XAI-Assisted-Decision-Making-by-Eliciting-a-Mental-Model"><a href="#Measuring-Perceived-Trust-in-XAI-Assisted-Decision-Making-by-Eliciting-a-Mental-Model" class="headerlink" title="Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model"></a>Measuring Perceived Trust in XAI-Assisted Decision-Making by Eliciting a Mental Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11765">http://arxiv.org/abs/2307.11765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Abbaspour Onari, Isel Grau, Marco S. Nobile, Yingqian Zhang</li>
<li>for: This paper aims to measure users’ perceived trust in an Explainable Artificial Intelligence (XAI) model by eliciting their mental models using Fuzzy Cognitive Maps (FCMs).</li>
<li>methods: The paper uses an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients and then evaluates the impact of interpretations on perceived trust through a survey of Medical Experts’ (MEs) explanation satisfaction attributes. Fuzzy linguistic variables are used to determine the strength of influences in MEs’ mental subjectivity.</li>
<li>results: The paper obtains quantified values to measure the perceived trust of each ME and analyzes the behavior of MEs in completing diagnostic tasks based on the quantified values. The results show that the quantified values can determine whether MEs trust or distrust the XAI model.<details>
<summary>Abstract</summary>
This empirical study proposes a novel methodology to measure users' perceived trust in an Explainable Artificial Intelligence (XAI) model. To do so, users' mental models are elicited using Fuzzy Cognitive Maps (FCMs). First, we exploit an interpretable Machine Learning (ML) model to classify suspected COVID-19 patients into positive or negative cases. Then, Medical Experts' (MEs) conduct a diagnostic decision-making task based on their knowledge and then prediction and interpretations provided by the XAI model. In order to evaluate the impact of interpretations on perceived trust, explanation satisfaction attributes are rated by MEs through a survey. Then, they are considered as FCM's concepts to determine their influences on each other and, ultimately, on the perceived trust. Moreover, to consider MEs' mental subjectivity, fuzzy linguistic variables are used to determine the strength of influences. After reaching the steady state of FCMs, a quantified value is obtained to measure the perceived trust of each ME. The results show that the quantified values can determine whether MEs trust or distrust the XAI model. We analyze this behavior by comparing the quantified values with MEs' performance in completing diagnostic tasks.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "empirical study" is translated as "实验研究" (shí yàn yán jí)* "perceived trust" is translated as "感知的信任" (gǎn zhī de xìn ràng)* "Fuzzy Cognitive Maps" is translated as "模糊认知地图" (mó huang gòu zhī dì tú)* "Medical Experts" is translated as "医学专家" (yī xué zhù jià)* "diagnostic decision-making task" is translated as "诊断决策任务" (shòu yán jì suī zhèng yì)* "explanation satisfaction attributes" is translated as "解释满意属性" (jiě jie cháng zhì fù xìng)* "fuzzy linguistic variables" is translated as "模糊语言变量" (mó huang yǔ yán biàn zhì)* "quantified value" is translated as "量化值" (liàng zhì yù)* "perceived trust of each ME" is translated as "每位ME的感知信任" (mēi zhì ME de gǎn zhī xìn ràng)
</details></li>
</ul>
<hr>
<h2 id="Fast-Adaptation-with-Bradley-Terry-Preference-Models-in-Text-To-Image-Classification-and-Generation"><a href="#Fast-Adaptation-with-Bradley-Terry-Preference-Models-in-Text-To-Image-Classification-and-Generation" class="headerlink" title="Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation"></a>Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07929">http://arxiv.org/abs/2308.07929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Gallego</li>
<li>for: 这篇论文的目的是如何将大型多modal模型（如CLIP和Stable Diffusion）进行特定任务或偏好的个性化。</li>
<li>methods: 本研究使用布莱德利-泰勒喜好模型（Bradley-Terry preference model）开发了一种快速适应方法，将原始模型迅速微调，只需少量的示例和计算资源。</li>
<li>results: 实验结果显示了这个框架在不同的多modal文本和图像理解领域中的能力，包括喜好预测和生成任务。<details>
<summary>Abstract</summary>
Recently, large multimodal models, such as CLIP and Stable Diffusion have experimented tremendous successes in both foundations and applications. However, as these models increase in parameter size and computational requirements, it becomes more challenging for users to personalize them for specific tasks or preferences. In this work, we address the problem of adapting the previous models towards sets of particular human preferences, aligning the retrieved or generated images with the preferences of the user. We leverage the Bradley-Terry preference model to develop a fast adaptation method that efficiently fine-tunes the original model, with few examples and with minimal computing resources. Extensive evidence of the capabilities of this framework is provided through experiments in different domains related to multimodal text and image understanding, including preference prediction as a reward model, and generation tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Nearly-Linear-Time-Algorithm-for-Structured-Support-Vector-Machines"><a href="#A-Nearly-Linear-Time-Algorithm-for-Structured-Support-Vector-Machines" class="headerlink" title="A Nearly-Linear Time Algorithm for Structured Support Vector Machines"></a>A Nearly-Linear Time Algorithm for Structured Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07735">http://arxiv.org/abs/2307.07735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljinstat/Structured_Data_Random_Features_for_Large-Scale_Kernel_Machines">https://github.com/ljinstat/Structured_Data_Random_Features_for_Large-Scale_Kernel_Machines</a></li>
<li>paper_authors: Yuzhou Gu, Zhao Song, Lichen Zhang</li>
<li>for:  quadratic programming with low-rank factorization or low-treewidth, and a small number of linear constraints</li>
<li>methods:  nearly-linear time algorithm</li>
<li>results:  nearly-linear time algorithms for low-treewidth or low-rank SVMs<details>
<summary>Abstract</summary>
Quadratic programming is a fundamental problem in the field of convex optimization. Many practical tasks can be formulated as quadratic programming, for example, the support vector machine (SVM). Linear SVM is one of the most popular tools over the last three decades in machine learning before deep learning method dominating.   In general, a quadratic program has input size $\Theta(n^2)$ (where $n$ is the number of variables), thus takes $\Omega(n^2)$ time to solve. Nevertheless, quadratic programs coming from SVMs has input size $O(n)$, allowing the possibility of designing nearly-linear time algorithms. Two important classes of SVMs are programs admitting low-rank kernel factorizations and low-treewidth programs. Low-treewidth convex optimization has gained increasing interest in the past few years (e.g.~linear programming [Dong, Lee and Ye 2021] and semidefinite programming [Gu and Song 2022]). Therefore, an important open question is whether there exist nearly-linear time algorithms for quadratic programs with these nice structures.   In this work, we provide the first nearly-linear time algorithm for solving quadratic programming with low-rank factorization or low-treewidth, and a small number of linear constraints. Our results imply nearly-linear time algorithms for low-treewidth or low-rank SVMs.
</details>
<details>
<summary>摘要</summary>
quadratic programming 是 convex optimization 领域中的基本问题。许多实际任务可以被формализова为quadratic programming，例如支持向量机器（SVM）。线性SVM 是过去三十年最受欢迎的机器学习工具之一，直到深度学习方法成为主流。  在一般情况下，quadratic program 的输入大小为 $\Theta(n^2)$（where $n$ 是变数的数量），因此需要 $\Omega(n^2)$ 时间来解决。然而，从 SVM 中获得的quadratic program 的输入大小为 $O(n)$，这使得可能设计近似线性时间的算法。两个重要的 SVM 类别是允许低矩阵kernel factorization 和低树几何 programme。低树几何 convex optimization 在过去几年内（例如线性程度 [Dong, Lee 和 Ye 2021] 和对偶定理程度 [Gu 和 Song 2022]）获得了增加的关注。因此，一个重要的开问是是否存在近似线性时间的算法 для quadratic program  WITH low-rank factorization 或 low-treewidth。在这个工作中，我们提供了第一个 near-linear time algorithm for solving quadratic programming with low-rank factorization or low-treewidth, 和一小数量的线性几何。我们的结果意味着 near-linear time algorithms for low-treewidth 或 low-rank SVMs.
</details></li>
</ul>
<hr>
<h2 id="Towards-Optimal-Neural-Networks-the-Role-of-Sample-Splitting-in-Hyperparameter-Selection"><a href="#Towards-Optimal-Neural-Networks-the-Role-of-Sample-Splitting-in-Hyperparameter-Selection" class="headerlink" title="Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection"></a>Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07726">http://arxiv.org/abs/2307.07726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijin Gong, Xinyu Zhang</li>
<li>for: 理解神经网络模型的效iveness</li>
<li>methods: 通过sample splitting的实践来找到优化hyperparameters的方法</li>
<li>results: 实验结果证明了这种方法可以使神经网络模型的预测风险下降到最低Translation:</li>
<li>for: Understanding the effectiveness of neural network models</li>
<li>methods: By practicing sample splitting to optimize hyperparameters</li>
<li>results: Experimental results prove that this method can minimize the prediction risk of neural network models<details>
<summary>Abstract</summary>
When artificial neural networks have demonstrated exceptional practical success in a variety of domains, investigations into their theoretical characteristics, such as their approximation power, statistical properties, and generalization performance, have made significant strides. In this paper, we construct a novel theory for understanding the effectiveness of neural networks by discovering the mystery underlying a common practice during neural network model construction: sample splitting. Our theory demonstrates that, the optimal hyperparameters derived from sample splitting can enable a neural network model that asymptotically minimizes the prediction risk. We conduct extensive experiments across different application scenarios and network architectures, and the results manifest our theory's effectiveness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Visual-Analytics-For-Machine-Learning-A-Data-Perspective-Survey"><a href="#Visual-Analytics-For-Machine-Learning-A-Data-Perspective-Survey" class="headerlink" title="Visual Analytics For Machine Learning: A Data Perspective Survey"></a>Visual Analytics For Machine Learning: A Data Perspective Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07712">http://arxiv.org/abs/2307.07712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junpeng Wang, Shixia Liu, Wei Zhang</li>
<li>for: 本文是一份系统性的回顾，探讨过去十年内关于机器学习（ML）模型的可视化（VIS）研究。</li>
<li>methods: 本文分类了常见的机器学习模型处理的数据类型为五种，解释每种类型的特点，并提及对其学习适应的机器学习模型。</li>
<li>results: 对143篇评估的论文进行分析，发现这些论文在不同的ML管道阶段和数据类型上进行了六种任务，并对未来研究方向做出预测。<details>
<summary>Abstract</summary>
The past decade has witnessed a plethora of works that leverage the power of visualization (VIS) to interpret machine learning (ML) models. The corresponding research topic, VIS4ML, keeps growing at a fast pace. To better organize the enormous works and shed light on the developing trend of VIS4ML, we provide a systematic review of these works through this survey. Since data quality greatly impacts the performance of ML models, our survey focuses specifically on summarizing VIS4ML works from the data perspective. First, we categorize the common data handled by ML models into five types, explain the unique features of each type, and highlight the corresponding ML models that are good at learning from them. Second, from the large number of VIS4ML works, we tease out six tasks that operate on these types of data (i.e., data-centric tasks) at different stages of the ML pipeline to understand, diagnose, and refine ML models. Lastly, by studying the distribution of 143 surveyed papers across the five data types, six data-centric tasks, and their intersections, we analyze the prospective research directions and envision future research trends.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie  hath witnessed a plethora of works that leveraged the power of visualization (VIS) to interpret machine learning (ML) models. The corresponding research topic, VIS4ML, hath been growing at a fast pace. To better organize the enormous works and shed light on the developing trend of VIS4ML, we provide a systematic review of these works through this survey. Since data quality greatly impacts the performance of ML models, our survey focuses specifically on summarizing VIS4ML works from the data perspective. First, we categorize the common data handled by ML models into five types, explain the unique features of each type, and highlight the corresponding ML models that are good at learning from them. Second, from the large number of VIS4ML works, we tease out six tasks that operate on these types of data (i.e., data-centric tasks) at different stages of the ML pipeline to understand, diagnose, and refine ML models. Lastly, by studying the distribution of 143 surveyed papers across the five data types, six data-centric tasks, and their intersections, we analyze the prospective research directions and envision future research trends.
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Stochasticity-by-Matrix-decomposition-Applied-on-Black-Hole-Data"><a href="#Identification-of-Stochasticity-by-Matrix-decomposition-Applied-on-Black-Hole-Data" class="headerlink" title="Identification of Stochasticity by Matrix-decomposition: Applied on Black Hole Data"></a>Identification of Stochasticity by Matrix-decomposition: Applied on Black Hole Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07703">http://arxiv.org/abs/2307.07703</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunilvengalil/ts_analysis_pca_eig">https://github.com/sunilvengalil/ts_analysis_pca_eig</a></li>
<li>paper_authors: Sai Pradeep Chakka, Sunil Kumar Vengalil, Neelam Sinha</li>
<li>for: 本研究旨在提出一种两路矩阵分解法，用于分类时间序列数据。</li>
<li>methods: 该算法使用了两种不同的技术：单值分解（SVD）和主成分分析（PCA）。</li>
<li>results: 对synthetic数据进行了分析，并在实验中使用了SVM进行分类。结果显示，在12个时间类中，SVD-label和PCA-label之间存在高度的一致性。<details>
<summary>Abstract</summary>
Timeseries classification as stochastic (noise-like) or non-stochastic (structured), helps understand the underlying dynamics, in several domains. Here we propose a two-legged matrix decomposition-based algorithm utilizing two complementary techniques for classification. In Singular Value Decomposition (SVD) based analysis leg, we perform topological analysis (Betti numbers) on singular vectors containing temporal information, leading to SVD-label. Parallely, temporal-ordering agnostic Principal Component Analysis (PCA) is performed, and the proposed PCA-derived features are computed. These features, extracted from synthetic timeseries of the two labels, are observed to map the timeseries to a linearly separable feature space. Support Vector Machine (SVM) is used to produce PCA-label. The proposed methods have been applied to synthetic data, comprising 41 realisations of white-noise, pink-noise (stochastic), Logistic-map at growth-rate 4 and Lorentz-system (non-stochastic), as proof-of-concept. Proposed algorithm is applied on astronomical data: 12 temporal-classes of timeseries of black hole GRS 1915+105, obtained from RXTE satellite with average length 25000. For a given timeseries, if SVD-label and PCA-label concur, then the label is retained; else deemed "Uncertain". Comparison of obtained results with those in literature are presented. It's found that out of 12 temporal classes of GRS 1915+105, concurrence between SVD-label and PCA-label is obtained on 11 of them.
</details>
<details>
<summary>摘要</summary>
时间序列分类为随机（噪声如的）或非随机（结构化），可以帮助我们理解时间序列的下面动力学。我们提出了一种基于两个脚本的矩阵分解算法，利用两种 complementary 技术进行分类。在 Singular Value Decomposition（SVD）基础分析脚本中，我们进行了 topological 分析（Betti 数）于时间信号中的特征向量，从而获得 SVD-标签。同时，无关于时间顺序的 Principal Component Analysis（PCA）被应用，并计算了提案的 PCA-derived 特征。这些特征从 synthetic 时间序列中提取出来，并在线性分离特征空间中映射时间序列。使用 Support Vector Machine（SVM）生成 PCA-标签。我们对 synthetic 数据进行了证明，包括41个实现 white-noise、pink-noise（随机）、Logistic-map 增长率4和 Lorentz-system（非随机）。我们还应用了这种方法于天文数据：RXTE 卫星上的 12 个 temporal 类时间序列，每个时间序列的平均长度为 25000。对于每个时间序列，如果 SVD-标签和 PCA-标签协调，则保留标签；否则被称为 "Uncertain"。我们对得到的结果与文献中的结果进行了比较，发现 GRS 1915+105 黑洞的 11 个 temporal 类时间序列中，SVD-标签和 PCA-标签协调。
</details></li>
</ul>
<hr>
<h2 id="NeurASP-Embracing-Neural-Networks-into-Answer-Set-Programming"><a href="#NeurASP-Embracing-Neural-Networks-into-Answer-Set-Programming" class="headerlink" title="NeurASP: Embracing Neural Networks into Answer Set Programming"></a>NeurASP: Embracing Neural Networks into Answer Set Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07700">http://arxiv.org/abs/2307.07700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhun Yang, Adam Ishay, Joohyung Lee</li>
<li>for: 该论文是为了推动Answer Set Programming（ASP）和神经网络之间的 integración，提供了一种简单扩展的Answer Set Programming（NeurASP）。</li>
<li>methods: 该论文使用神经网络输出作为Answer Set Programming中的概率分布，从而实现了sub-symbolic和symbolic计算的集成。它还展示了如何使用预训练神经网络在符号计算中使用ASP规则，以及如何使用ASP规则来训练神经网络。</li>
<li>results: NeurASP可以使用预训练神经网络来改善神经网络的识别结果，并且可以使用ASP规则来帮助神经网络学习从数据中的隐式相关性和Explicit complex semantic constraints。<details>
<summary>Abstract</summary>
We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network's perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can be used to train a neural network better by training with ASP rules so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.
</details>
<details>
<summary>摘要</summary>
我们介绍NeurASP，一个简单扩展Answer Set Programs（ASP）的方法，通过将神经网络输出视为Answer Set Programs中的原子事实的概率分布。NeurASP提供了一个简单而有效的方式将子符号 computations和符号 computations融合。我们显示了NeurASP如何使用预训练的神经网络在符号计算中使用，以及如何运用符号推理来改善神经网络的认知结果。此外，NeurASP还可以用来训练神经网络，使其不仅从数据中学习隐含的相互关联，而且还从ASP规则中获得明确的复杂 semantic constraint。
</details></li>
</ul>
<hr>
<h2 id="The-Growth-of-E-Bike-Use-A-Machine-Learning-Approach"><a href="#The-Growth-of-E-Bike-Use-A-Machine-Learning-Approach" class="headerlink" title="The Growth of E-Bike Use: A Machine Learning Approach"></a>The Growth of E-Bike Use: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02034">http://arxiv.org/abs/2308.02034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Gupta, Samarth Chitgopekar, Alexander Kim, Joseph Jiang, Megan Wang, Christopher Grattoni<br>for: 这个研究的目的是为美国政策制定者提供关于电动自行车（e-bike）的信息，以便他们能够更好地了解电动自行车的增长和影响，并在制定可持续能源计划时做出更 Informed decisions。methods: 这个研究使用了ARIMA模型和一种监管机器学习算法来预测电动自行车销售量的增长。此外，研究还使用Random Forest回归模型来分析电动自行车销售增长的因素。results: 研究发现，电动自行车在美国的销售量将在2025年和2028年分别达到130万和2113万个单位。此外，研究还发现，电动自行车的使用会减少碳排放和提高体能消耗。在2022年，电动自行车的使用已经减少了15737.82吨碳排放和716630.727千卡ло里。<details>
<summary>Abstract</summary>
We present our work on electric bicycles (e-bikes) and their implications for policymakers in the United States. E-bikes have gained significant popularity as a fast and eco-friendly transportation option. As we strive for a sustainable energy plan, understanding the growth and impact of e-bikes is crucial for policymakers. Our mathematical modeling offers insights into the value of e-bikes and their role in the future. Using an ARIMA model, a supervised machine-learning algorithm, we predicted the growth of e-bike sales in the U.S. Our model, trained on historical sales data from January 2006 to December 2022, projected sales of 1.3 million units in 2025 and 2.113 million units in 2028. To assess the factors contributing to e-bike usage, we employed a Random Forest regression model. The most significant factors influencing e-bike sales growth were disposable personal income and popularity. Furthermore, we examined the environmental and health impacts of e-bikes. Through Monte Carlo simulations, we estimated the reduction in carbon emissions due to e-bike use and the calories burned through e-biking. Our findings revealed that e-bike usage in the U.S. resulted in a reduction of 15,737.82 kilograms of CO2 emissions in 2022. Additionally, e-bike users burned approximately 716,630.727 kilocalories through their activities in the same year. Our research provides valuable insights for policymakers, emphasizing the potential of e-bikes as a sustainable transportation solution. By understanding the growth factors and quantifying the environmental and health benefits, policymakers can make informed decisions about integrating e-bikes into future energy and transportation strategies.
</details>
<details>
<summary>摘要</summary>
我们对电动自行车（e-bike）的研究和其对政策 makers 在美国的影响进行了报告。电动自行车在快速和环保交通方面受到了广泛的欢迎，随着我们努力实现可持续能源规划，理解电动自行车的增长和影响非常重要。我们使用 ARIMA 模型和一种监管机器学习算法来预测电动自行车销售在美国的增长。我们的模型，基于2006年1月至2022年12月的历史销售数据，预测在2025年销售130万部电动自行车，在2028年销售2113万部。为了评估电动自行车使用的因素，我们使用Random Forest回归模型。最主要影响电动自行车销售增长的因素是可 dispose 个人收入和流行度。此外，我们还研究了电动自行车对环境和健康的影响。通过蒙地卡罗模拟，我们估算了电动自行车使用在美国的碳排放减少和热量燃烧。我们的发现表明，在2022年，电动自行车在美国的使用已经减少了15737.82公斤的碳排放，同时电动自行车用户通过其活动燃烧了约716630.727公利 kalories。我们的研究为政策 makers 提供了有价值的见解，强调电动自行车作为可持续交通解决方案的潜在价值。通过理解电动自行车增长因素和评估环境和健康的影响，政策 makers 可以做出 Informed 的决策，将电动自行车纳入未来能源和交通战略中。
</details></li>
</ul>
<hr>
<h2 id="Reducing-operator-complexity-in-Algebraic-Multigrid-with-Machine-Learning-Approaches"><a href="#Reducing-operator-complexity-in-Algebraic-Multigrid-with-Machine-Learning-Approaches" class="headerlink" title="Reducing operator complexity in Algebraic Multigrid with Machine Learning Approaches"></a>Reducing operator complexity in Algebraic Multigrid with Machine Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07695">http://arxiv.org/abs/2307.07695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ru Huang, Kai Chang, Huan He, Ruipeng Li, Yuanzhe Xi</li>
<li>for:  solves parametric partial differential equation (PDE) problems with increasing operator complexity.</li>
<li>methods:  utilizes neural networks (NNs) combined with smooth test vectors from multigrid eigenvalue problems.</li>
<li>results:  reduces the complexity of coarse-grid operators while maintaining overall AMG convergence.Here’s the simplified Chinese text:</li>
<li>for: 用于解决参数部分 diferencial equation (PDE) 问题中增加运算 complexity.</li>
<li>methods: 利用神经网络 (NNs) 与多普逊值问题中的畅通测试向量结合.</li>
<li>results: 降低粗网操作符的复杂性，保持总的 AMG  converges.<details>
<summary>Abstract</summary>
We propose a data-driven and machine-learning-based approach to compute non-Galerkin coarse-grid operators in algebraic multigrid (AMG) methods, addressing the well-known issue of increasing operator complexity. Guided by the AMG theory on spectrally equivalent coarse-grid operators, we have developed novel ML algorithms that utilize neural networks (NNs) combined with smooth test vectors from multigrid eigenvalue problems. The proposed method demonstrates promise in reducing the complexity of coarse-grid operators while maintaining overall AMG convergence for solving parametric partial differential equation (PDE) problems. Numerical experiments on anisotropic rotated Laplacian and linear elasticity problems are provided to showcase the performance and compare with existing methods for computing non-Galerkin coarse-grid operators.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于数据驱动和机器学习的方法，用于在数学多普逊（AMG）方法中计算非加尔erkin粗积算子，解决了常见的算子复杂性问题。我们根据AMG理论中的特征相似粗积算子，开发了一种新的机器学习算法，利用神经网络（NN）和多普逊域值问题中的平滑测试向量。我们的方法可以减少粗积算子的复杂性，同时保持AMG方法的总体收敛性，用于解决参数化partial differential equation（PDE）问题。我们在不同的旋转卷积 Laplacian 和线性塑性问题上进行了数值实验，以示出我们的方法的性能和与现有方法相比。
</details></li>
</ul>
<hr>
<h2 id="Creating-a-Dataset-for-High-Performance-Computing-Code-Translation-A-Bridge-Between-HPC-Fortran-and-C"><a href="#Creating-a-Dataset-for-High-Performance-Computing-Code-Translation-A-Bridge-Between-HPC-Fortran-and-C" class="headerlink" title="Creating a Dataset for High-Performance Computing Code Translation: A Bridge Between HPC Fortran and C++"></a>Creating a Dataset for High-Performance Computing Code Translation: A Bridge Between HPC Fortran and C++</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07686">http://arxiv.org/abs/2307.07686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bin123apple/fortran-cpp-hpc-code-translation-dataset">https://github.com/bin123apple/fortran-cpp-hpc-code-translation-dataset</a></li>
<li>paper_authors: Bin Lei, Caiwen Ding, Le Chen, Pei-Hung Lin, Chunhua Liao</li>
<li>for: 本研究准备了一个新的机器学习模型训练集，用于翻译OpenMP Fortran和C++代码。</li>
<li>methods: 为确保可靠性和实用性，该集 initially refined 使用仔细的代码相似性测试。</li>
<li>results: 我们使用量化(CodeBLEU)和质量(人类评估)方法评估该集的有效性，并发现该集可以提高大规模语言模型的翻译能力，比如无编程知识下的提升为$\mathbf{\times 5.1}$，有编程知识下的提升为$\mathbf{\times 9.9}$。这种dataset的存在可能推动高性能计算领域中的代码翻译技术的发展。该集可以在<a target="_blank" rel="noopener" href="https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset上下载。</a><details>
<summary>Abstract</summary>
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of $\mathbf{\times 5.1}$ for models with no prior coding knowledge and $\mathbf{\times 9.9}$ for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing. The dataset is available at https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提供了一个新的数据集用于训练机器学习模型在OpenMP Fortran和C++代码之间翻译。为确保可靠性和实用性，我们首先使用精细的代码相似性测试进行初步纤细。我们使用代码BLEU和人类评估方法进行评估数据集的效果，并证明了该数据集可以大幅提高大规模语言模型的翻译能力，具体是$\times 5.1$ для没有编程知识的模型和$\times 9.9$ для具有一定编程经验的模型。我们的工作展示了该数据集在高性能计算领域的代码翻译技术的前进。数据集可以在https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset中下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-Subjective-Time-Series-Data-via-Utopia-Label-Distribution-Approximation"><a href="#Learning-Subjective-Time-Series-Data-via-Utopia-Label-Distribution-Approximation" class="headerlink" title="Learning Subjective Time-Series Data via Utopia Label Distribution Approximation"></a>Learning Subjective Time-Series Data via Utopia Label Distribution Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07682">http://arxiv.org/abs/2307.07682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxin Xu, Hexin Jiang, Xuefeng Liang, Ying Zhou, Yin Zhao, Jie Zhang<br>for:STR tasks (Subjective time-series regression)methods:ULDA (Utopia Label Distribution Approximation)TNS (Time-slice Normal Sampling)CWL (Convolutional Weighted Loss)results:lifts the state-of-the-art performance on two STR tasks and three benchmark datasets.<details>
<summary>Abstract</summary>
Subjective time-series regression (STR) tasks have gained increasing attention recently. However, most existing methods overlook the label distribution bias in STR data, which results in biased models. Emerging studies on imbalanced regression tasks, such as age estimation and depth estimation, hypothesize that the prior label distribution of the dataset is uniform. However, we observe that the label distributions of training and test sets in STR tasks are likely to be neither uniform nor identical. This distinct feature calls for new approaches that estimate more reasonable distributions to train a fair model. In this work, we propose Utopia Label Distribution Approximation (ULDA) for time-series data, which makes the training label distribution closer to real-world but unknown (utopia) label distribution. This would enhance the model's fairness. Specifically, ULDA first convolves the training label distribution by a Gaussian kernel. After convolution, the required sample quantity at each regression label may change. We further devise the Time-slice Normal Sampling (TNS) to generate new samples when the required sample quantity is greater than the initial sample quantity, and the Convolutional Weighted Loss (CWL) to lower the sample weight when the required sample quantity is less than the initial quantity. These two modules not only assist the model training on the approximated utopia label distribution, but also maintain the sample continuity in temporal context space. To the best of our knowledge, ULDA is the first method to address the label distribution bias in time-series data. Extensive experiments demonstrate that ULDA lifts the state-of-the-art performance on two STR tasks and three benchmark datasets.
</details>
<details>
<summary>摘要</summary>
受到媒体关注的主观时序回归（STR）任务在最近几年来得到了越来越多的关注。然而，大多数现有方法忽略了STR数据中标签分布偏见，导致模型偏向。新诞听学者认为STR任务中的标签分布是均匀的，但我们发现STR任务中的训练和测试集标签分布很可能不均匀，也不是完全相同的。这种特殊特点需要新的方法来训练公正的模型。在这种情况下，我们提出了UTopia标签分布近似（ULDA）方法，用于在时序数据上训练公正的模型。ULDA方法首先将训练标签分布通过 Gaussian 核函数进行混合。在混合后，每个回归标签的样本数量可能会改变。我们还提出了时间扁平分布（TNS）和卷积权重损失（CWL）两个模块，用于生成新的样本和更正模型的训练。这两个模块不仅帮助模型在训练中使用更加公正的标签分布，还保持了样本在时间上的连续性。到目前为止，ULDA方法是首个强调STR任务中标签分布偏见的方法。我们对 STR 任务中的三个标准数据集进行了广泛的实验，结果表明ULDA方法可以超越当前的状态势。
</details></li>
</ul>
<hr>
<h2 id="Data-centric-Operational-Design-Domain-Characterization-for-Machine-Learning-based-Aeronautical-Products"><a href="#Data-centric-Operational-Design-Domain-Characterization-for-Machine-Learning-based-Aeronautical-Products" class="headerlink" title="Data-centric Operational Design Domain Characterization for Machine Learning-based Aeronautical Products"></a>Data-centric Operational Design Domain Characterization for Machine Learning-based Aeronautical Products</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07681">http://arxiv.org/abs/2307.07681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fateh Kaakai, Shridhar “Shreeder” Adibhatla, Ganesh Pai, Emmanuelle Escorihuela</li>
<li>for: 这个论文是为了提供一种初次准确地定义机器学习（ML）基于飞行器产品的操作设计域（ODD）的方法。</li>
<li>methods: 该方法是基于数据而不是场景而定义ODD，并提出了将定义ODD的参数维度和 ML 应用可能遇到的数据类型进行明确表述，以及这些数据类型对 ML 模型和系统层次结构的影响。</li>
<li>results: 该论文指出，通过这种方法可以确定 ML 模型的需求，以及系统层次结构中 ML 模型和高级系统的可能的影响，以及可能需要进行学习保障过程和系统体系设计考虑。 例如，通过使用飞行器飞行范围来说明这些概念。<details>
<summary>Abstract</summary>
We give a first rigorous characterization of Operational Design Domains (ODDs) for Machine Learning (ML)-based aeronautical products. Unlike in other application sectors (such as self-driving road vehicles) where ODD development is scenario-based, our approach is data-centric: we propose the dimensions along which the parameters that define an ODD can be explicitly captured, together with a categorization of the data that ML-based applications can encounter in operation, whilst identifying their system-level relevance and impact. Specifically, we discuss how those data categories are useful to determine: the requirements necessary to drive the design of ML Models (MLMs); the potential effects on MLMs and higher levels of the system hierarchy; the learning assurance processes that may be needed, and system architectural considerations. We illustrate the underlying concepts with an example of an aircraft flight envelope.
</details>
<details>
<summary>摘要</summary>
我们给出了机器学习（ML）基于航空产品的操作设计领域（ODD）的首次正式定义。与其他应用领域（如自动驾驶道路车辆）的ODD开发不同，我们的方法是数据中心：我们提议定义ODD参数的维度，并将ML基于应用中可能遇到的数据分类，以及这些数据的系统水平重要性和影响。specifically， we discuss how these data categories can be used to determine: the requirements needed to drive the design of ML models（MLMs）; the potential effects on MLMs and higher levels of the system hierarchy; the learning assurance processes that may be needed, and system architectural considerations. We illustrate the underlying concepts with an example of an aircraft flight envelope.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Sequence-Based-Nanobody-Antigen-Binding-Prediction"><a href="#Sequence-Based-Nanobody-Antigen-Binding-Prediction" class="headerlink" title="Sequence-Based Nanobody-Antigen Binding Prediction"></a>Sequence-Based Nanobody-Antigen Binding Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01920">http://arxiv.org/abs/2308.01920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usama Sardar, Sarwan Ali, Muhammad Sohaib Ayub, Muhammad Shoaib, Khurram Bashir, Imdad Ullah Khan, Murray Patterson<br>for: This paper aims to develop a machine-learning method to predict the binding of nanobodies (Nb) to antigens based solely on sequence data.methods: The authors curated a comprehensive dataset of Nb-Antigen binding and nonbinding data and devised an embedding method based on gapped k-mers to predict binding based only on sequences of Nb and Antigen.results: The approach achieved up to 90% accuracy in binding prediction and was significantly more efficient compared to the widely-used computational docking technique.<details>
<summary>Abstract</summary>
Nanobodies (Nb) are monomeric heavy-chain fragments derived from heavy-chain only antibodies naturally found in Camelids and Sharks. Their considerably small size (~3-4 nm; 13 kDa) and favorable biophysical properties make them attractive targets for recombinant production. Furthermore, their unique ability to bind selectively to specific antigens, such as toxins, chemicals, bacteria, and viruses, makes them powerful tools in cell biology, structural biology, medical diagnostics, and future therapeutic agents in treating cancer and other serious illnesses. However, a critical challenge in nanobodies production is the unavailability of nanobodies for a majority of antigens. Although some computational methods have been proposed to screen potential nanobodies for given target antigens, their practical application is highly restricted due to their reliance on 3D structures. Moreover, predicting nanobodyantigen interactions (binding) is a time-consuming and labor-intensive task. This study aims to develop a machine-learning method to predict Nanobody-Antigen binding solely based on the sequence data. We curated a comprehensive dataset of Nanobody-Antigen binding and nonbinding data and devised an embedding method based on gapped k-mers to predict binding based only on sequences of nanobody and antigen. Our approach achieves up to 90% accuracy in binding prediction and is significantly more efficient compared to the widely-used computational docking technique.
</details>
<details>
<summary>摘要</summary>
纳诺体（Nb）是含有重链只的轻链抗体的自然存在的哺乳动物和鲨鱼中的蛋白质。它们的非常小的大小（约3-4奈米，13 kDa）和有利的生物物理性质使其成为了重点生产的目标。此外，它们可以特异性地绑定到特定抗原，如毒素、化学物质、细菌和病毒，使其成为了细胞生物、结构生物、医学诊断和未来的疾病治疗的有力工具。然而，纳诺体生产中的主要挑战是缺乏纳诺体对大多数抗原的可用性。虽然一些计算方法已经被提出来屏选纳诺体对给定抗原的可能性，但它们的实际应用受到了三维结构的限制，而且预测纳诺体-抗原交互（绑定）是一项时间consuming和劳动密集的任务。本研究旨在开发一种基于序列数据的机器学习方法，以预测纳诺体-抗原绑定。我们收集了一个完整的纳诺体-抗原绑定和非绑定数据集，并采用基于异常词的嵌入方法来预测绑定基于纳诺体和抗原的序列数据。我们的方法可以达到90%的准确率，与广泛使用的计算协同技术相比，效率明显高于。
</details></li>
</ul>
<hr>
<h2 id="Sharp-Convergence-Rates-for-Matching-Pursuit"><a href="#Sharp-Convergence-Rates-for-Matching-Pursuit" class="headerlink" title="Sharp Convergence Rates for Matching Pursuit"></a>Sharp Convergence Rates for Matching Pursuit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07679">http://arxiv.org/abs/2307.07679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason M. Klusowski, Jonathan W. Siegel</li>
<li>for: 本文研究了matching pursuit的基本限制，即用一个 слова库中的元素组成一个稀疏的线性组合来近似目标函数。当目标函数在字典的变化空间中时，过去几十年有很多卓越的工作获得了上下限 bounds on error of matching pursuit，但它们并不匹配。本文的主要贡献是将这个差异关系closed和获得了准确的衰减率特征。</li>
<li>methods: 本文使用了一个最差情况的字典来构建，该字典显示出了现有最佳上限 bound cannot be significantly improved。结果是，与其他greedy algorithm variants不同，matching pursuit的 converges rate是非优的并由一个certain non-linear equation的解决决定。这使得我们可以结论出任何Amount of shrinkage improve matching pursuit in the worst case.</li>
<li>results: 本文的结果是，任何Amount of shrinkage improve matching pursuit in the worst case。这意味着，无论如何选择 слова库，matching pursuit都会在最差情况下出现衰减。这与之前的研究不同，因为它们通常认为matching pursuit在某些情况下是optimal的。<details>
<summary>Abstract</summary>
We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the error of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the decay rate of matching pursuit. Specifically, we construct a worst case dictionary which shows that the existing best upper bound cannot be significantly improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pursuit in the worst case.
</details>
<details>
<summary>摘要</summary>
我们研究基本限制的匹配追求（也称为纯格列批处理），用一个简单的线性组合来近似目标函数。当目标函数在字典的变换空间中存在时，过去几十年有很多出色的成果，得到了误差的上下限，但是它们不匹配。本文的主要贡献是关于匹配追求的衰减率的锐化特征化。我们构建了最坏情况的字典，显示现有的最佳上限不能得到显著改进。结果表明，与其他格列算法变体不同，匹配追求的 converges率是不优的，并且取决于一个非线性方程的解。这使得我们能够 conclued 任何Amount of shrinkage 都会提高匹配追求的性能在最坏情况下。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Epoch-Greedy-in-Multi-Agent-Contextual-Bandit-Mechanisms"><a href="#On-the-Robustness-of-Epoch-Greedy-in-Multi-Agent-Contextual-Bandit-Mechanisms" class="headerlink" title="On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit Mechanisms"></a>On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07675">http://arxiv.org/abs/2307.07675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinglun Xu, Bhuvesh Kumar, Jacob Abernethy</li>
<li>for: 这篇论文主要关注在多重投机机制中的学习问题，特别是面临三大挑战：吸引真实投标行为、使用用户个性化、以及抵御 manipulate click 模式。</li>
<li>methods: 这篇论文使用了多种方法来解决这些挑战，包括 truthful multi-armed bandit mechanisms、contextual bandit algorithms 和 bandits with adversarial corruptions。</li>
<li>results: 研究发现，可以通过扩展 $\epsilon$-greedy 算法来处理这些挑战，并且这种扩展具有对 adversarial data corruption attacks 的 innate robustness，并且性能会随损害的Amount decay linearly。<details>
<summary>Abstract</summary>
Efficient learning in multi-armed bandit mechanisms such as pay-per-click (PPC) auctions typically involves three challenges: 1) inducing truthful bidding behavior (incentives), 2) using personalization in the users (context), and 3) circumventing manipulations in click patterns (corruptions). Each of these challenges has been studied orthogonally in the literature; incentives have been addressed by a line of work on truthful multi-armed bandit mechanisms, context has been extensively tackled by contextual bandit algorithms, while corruptions have been discussed via a recent line of work on bandits with adversarial corruptions. Since these challenges co-exist, it is important to understand the robustness of each of these approaches in addressing the other challenges, provide algorithms that can handle all simultaneously, and highlight inherent limitations in this combination. In this work, we show that the most prominent contextual bandit algorithm, $\epsilon$-greedy can be extended to handle the challenges introduced by strategic arms in the contextual multi-arm bandit mechanism setting. We further show that $\epsilon$-greedy is inherently robust to adversarial data corruption attacks and achieves performance that degrades linearly with the amount of corruption.
</details>
<details>
<summary>摘要</summary>
efficient learning in multi-armed bandit mechanisms such as pay-per-click (PPC) auctions typically involves three challenges: 1) inducing truthful bidding behavior (incentives), 2) using personalization in the users (context), and 3) circumventing manipulations in click patterns (corruptions). each of these challenges has been studied orthogonally in the literature; incentives have been addressed by a line of work on truthful multi-armed bandit mechanisms, context has been extensively tackled by contextual bandit algorithms, while corruptions have been discussed via a recent line of work on bandits with adversarial corruptions. since these challenges co-exist, it is important to understand the robustness of each of these approaches in addressing the other challenges, provide algorithms that can handle all simultaneously, and highlight inherent limitations in this combination. in this work, we show that the most prominent contextual bandit algorithm, $\epsilon$-greedy can be extended to handle the challenges introduced by strategic arms in the contextual multi-arm bandit mechanism setting. we further show that $\epsilon$-greedy is inherently robust to adversarial data corruption attacks and achieves performance that degrades linearly with the amount of corruption.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-the-Effectiveness-of-Using-a-Replay-Buffer-on-Mode-Discovery-in-GFlowNets"><a href="#An-Empirical-Study-of-the-Effectiveness-of-Using-a-Replay-Buffer-on-Mode-Discovery-in-GFlowNets" class="headerlink" title="An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets"></a>An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07674">http://arxiv.org/abs/2307.07674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Vemgal, Elaine Lau, Doina Precup</li>
<li>for: 本文研究了如何使用储存缓存（replay buffer）来加速GFlowNets模式发现。</li>
<li>methods: 本文employs empirical studies to explore various replay buffer sampling techniques and evaluates their impact on the speed of mode discovery and the quality of the discovered modes.</li>
<li>results: 实验结果表明，在Hypergrid即地域和分子合成环境中，使用储存缓存可以significantly improve模式发现速度和模式质量。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) algorithms aim to learn an optimal policy by iteratively sampling actions to learn how to maximize the total expected return, $R(x)$. GFlowNets are a special class of algorithms designed to generate diverse candidates, $x$, from a discrete set, by learning a policy that approximates the proportional sampling of $R(x)$. GFlowNets exhibit improved mode discovery compared to conventional RL algorithms, which is very useful for applications such as drug discovery and combinatorial search. However, since GFlowNets are a relatively recent class of algorithms, many techniques which are useful in RL have not yet been associated with them. In this paper, we study the utilization of a replay buffer for GFlowNets. We explore empirically various replay buffer sampling techniques and assess the impact on the speed of mode discovery and the quality of the modes discovered. Our experimental results in the Hypergrid toy domain and a molecule synthesis environment demonstrate significant improvements in mode discovery when training with a replay buffer, compared to training only with trajectories generated on-policy.
</details>
<details>
<summary>摘要</summary>
强化学习（RL）算法的目标是通过反复样本动作来学习最佳策略，以 maximize the total expected return, $R(x)$. GFlowNets 是一种特殊的算法，用于生成自 discrete 集合中的多个候选者，$x$, 通过学习一个策略，来近似 proportional sampling of $R(x)$. GFlowNets 在模式发现方面表现出了改善，这对于应用如药物发现和 combinatorial search 非常有用。然而，由于 GFlowNets 是一种相对较新的算法，许多RL中的技巧还没有与其相关。在这篇论文中，我们研究了 GFlowNets 中使用 replay buffer 的利用。我们通过 empirical 方式研究了不同的 replay buffer 采样技术的影响，以及它们对速度模式发现和模式质量的影响。我们的实验结果在 Hypergrid 玩家领域和一个分子合成环境中表明，在训练中使用 replay buffer 可以比训练只使用在政策上的 trajectories 更快地发现模式，并且模式质量也更高。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Adversarial-Attacks-on-Online-Multi-agent-Reinforcement-Learning"><a href="#Efficient-Adversarial-Attacks-on-Online-Multi-agent-Reinforcement-Learning" class="headerlink" title="Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning"></a>Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07670">http://arxiv.org/abs/2307.07670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanlin Liu, Lifeng Lai</li>
<li>for:  investigate the impact of adversarial attacks on MARL</li>
<li>methods:  action poisoning, reward poisoning, mixed attack strategy</li>
<li>results:  efficient attack on MARL agents even with no prior information about the environment and agents’ algorithms<details>
<summary>Abstract</summary>
Due to the broad range of applications of multi-agent reinforcement learning (MARL), understanding the effects of adversarial attacks against MARL model is essential for the safe applications of this model. Motivated by this, we investigate the impact of adversarial attacks on MARL. In the considered setup, there is an exogenous attacker who is able to modify the rewards before the agents receive them or manipulate the actions before the environment receives them. The attacker aims to guide each agent into a target policy or maximize the cumulative rewards under some specific reward function chosen by the attacker, while minimizing the amount of manipulation on feedback and action. We first show the limitations of the action poisoning only attacks and the reward poisoning only attacks. We then introduce a mixed attack strategy with both the action poisoning and the reward poisoning. We show that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior information about the underlying environment and the agents' algorithms.
</details>
<details>
<summary>摘要</summary>
We first show the limitations of action poisoning only attacks and reward poisoning only attacks. We then introduce a mixed attack strategy that combines both action poisoning and reward poisoning. We demonstrate that the mixed attack strategy can efficiently attack MARL agents even if the attacker has no prior knowledge of the underlying environment and the agents' algorithms.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Action-Robust-Reinforcement-Learning-with-Probabilistic-Policy-Execution-Uncertainty"><a href="#Efficient-Action-Robust-Reinforcement-Learning-with-Probabilistic-Policy-Execution-Uncertainty" class="headerlink" title="Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty"></a>Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07666">http://arxiv.org/abs/2307.07666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanlin Liu, Zhihan Zhou, Han Liu, Lifeng Lai</li>
<li>for: 本研究目的是找到面对不确定性时的最佳策略，以优化最差情况性能。</li>
<li>methods: 本研究使用了可靠性执行不确定性，即策略中指定的动作会被执行的概率是1-ρ，而冲击动作会被执行的概率是ρ。我们提出了动作稳健MDP的优化方法，并开发了Action Robust Reinforcement Learning with Certificates（ARRLC）算法，可以实现最小最大偏差和样本复杂度。</li>
<li>results: 我们通过数值实验 validate了我们的方法的稳健性，并证明了ARRLC在动作冲击下比非稳健RL算法表现更好，并且 faster than robust TD算法在存在动作冲击时 converge。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with the probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm in the presence of action perturbations.
</details>
<details>
<summary>摘要</summary>
Robust reinforcement learning (RL) aims to find a policy that optimizes the worst-case performance in the face of uncertainties. In this paper, we focus on action robust RL with probabilistic policy execution uncertainty, in which, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We establish the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. Furthermore, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Furthermore, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm in the presence of action perturbations.Here's the translation in Simplified Chinese:robust reinforcement learning (RL) 目标是找到面临不确定性时的政策优化策略，在这篇论文中，我们关注action robust RL中的抽象uncertainty， Specifically, instead of always carrying out the action specified by the policy, the agent will take the action specified by the policy with probability $1-\rho$ and an alternative adversarial action with probability $\rho$. We prove the existence of an optimal policy on the action robust MDPs with probabilistic policy execution uncertainty and provide the action robust Bellman optimality equation for its solution. In addition, we develop Action Robust Reinforcement Learning with Certificates (ARRLC) algorithm that achieves minimax optimal regret and sample complexity. Finally, we conduct numerical experiments to validate our approach's robustness, demonstrating that ARRLC outperforms non-robust RL algorithms and converges faster than the robust TD algorithm in the presence of action perturbations.
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-for-option-pricing-an-empirical-investigation-of-network-architectures"><a href="#Machine-learning-for-option-pricing-an-empirical-investigation-of-network-architectures" class="headerlink" title="Machine learning for option pricing: an empirical investigation of network architectures"></a>Machine learning for option pricing: an empirical investigation of network architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07657">http://arxiv.org/abs/2307.07657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurens Van Mieghem, Antonis Papapantoleon, Jonas Papazoglou-Hennig</li>
<li>for: 学习选取OPTION价格或附加价值，给出相应的输入数据（模型参数）和输出数据（选取价格或附加价值）。</li>
<li>methods: 使用激活函数网络架构，包括普通的推进网络和图像分类方法中的通用高速公路网络，以及最新的机器学习方法 дляPDEs。</li>
<li>results: 通过实验发现，对选取价格问题，使用通用高速公路网络架构可以得到最佳性能，其中误差和训练时间都是最佳。而在计算附加价值时，经过必要的转换后，DGM架构的变体可以获得最佳性能。<details>
<summary>Abstract</summary>
We consider the supervised learning problem of learning the price of an option or the implied volatility given appropriate input data (model parameters) and corresponding output data (option prices or implied volatilities). The majority of articles in this literature considers a (plain) feed forward neural network architecture in order to connect the neurons used for learning the function mapping inputs to outputs. In this article, motivated by methods in image classification and recent advances in machine learning methods for PDEs, we investigate empirically whether and how the choice of network architecture affects the accuracy and training time of a machine learning algorithm. We find that for option pricing problems, where we focus on the Black--Scholes and the Heston model, the generalized highway network architecture outperforms all other variants, when considering the mean squared error and the training time as criteria. Moreover, for the computation of the implied volatility, after a necessary transformation, a variant of the DGM architecture outperforms all other variants, when considering again the mean squared error and the training time as criteria.
</details>
<details>
<summary>摘要</summary>
我们考虑了超级vised学习问题，即通过适当的输入数据（模型参数）和对应的输出数据（选项价格或预测volatility）来学习函数映射。大多数文章在这个文献中使用（普通）径向神经网络架构来连接学习神经元。在这篇文章中，我们受到图像分类方法和最近的机器学习方法 дляPDE的影响，我们在option价格问题上进行了实验，以评估不同网络架构对精度和训练时间的影响。我们发现，对黑-谢尔斯和哈斯顿模型的option价格问题，通用高速公路网络架构在评估 Mean Squared Error 和训练时间作为标准对比下，其表现比其他所有变体更好。此外，对计算预测volatility问题，经过必要的变换，一种DGM架构的变体在评估 Mean Squared Error 和训练时间作为标准对比下，表现比其他所有变体更好。
</details></li>
</ul>
<hr>
<h2 id="DIGEST-Fast-and-Communication-Efficient-Decentralized-Learning-with-Local-Updates"><a href="#DIGEST-Fast-and-Communication-Efficient-Decentralized-Learning-with-Local-Updates" class="headerlink" title="DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates"></a>DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07652">http://arxiv.org/abs/2307.07652</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous404404/digestcode">https://github.com/anonymous404404/digestcode</a></li>
<li>paper_authors: Peyman Gholami, Hulya Seferoglu</li>
<li>for: 这个论文主要针对的是异构分布数据的归一化学习问题，并提出了一种异步分布式学习机制DIGEST，以提高通信效率和速度。</li>
<li>methods: 该论文基于Gossip算法和随机漫步算法的想法，并且关注了Stochastic Gradient Descent（SGD）算法。DIGEST机制是一种异步分布式算法，基于本地SGD算法，可以提高通信效率和速度。</li>
<li>results: 论文通过分析单流和多流DIGEST机制的渐进性和通信开销，证明了两者都可以 approached到优化解决方案。在ilogistic回归和深度神经网络ResNet20上进行了实验，结果表明，多流DIGEST在iid设定下的渐进性比基eline更好，而在非iid设定下则超越基eline。<details>
<summary>Abstract</summary>
Two widely considered decentralized learning algorithms are Gossip and random walk-based learning. Gossip algorithms (both synchronous and asynchronous versions) suffer from high communication cost, while random-walk based learning experiences increased convergence time. In this paper, we design a fast and communication-efficient asynchronous decentralized learning mechanism DIGEST by taking advantage of both Gossip and random-walk ideas, and focusing on stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized algorithm building on local-SGD algorithms, which are originally designed for communication efficient centralized learning. We design both single-stream and multi-stream DIGEST, where the communication overhead may increase when the number of streams increases, and there is a convergence and communication overhead trade-off which can be leveraged. We analyze the convergence of single- and multi-stream DIGEST, and prove that both algorithms approach to the optimal solution asymptotically for both iid and non-iid data distributions. We evaluate the performance of single- and multi-stream DIGEST for logistic regression and a deep neural network ResNet20. The simulation results confirm that multi-stream DIGEST has nice convergence properties; i.e., its convergence time is better than or comparable to the baselines in iid setting, and outperforms the baselines in non-iid setting.
</details>
<details>
<summary>摘要</summary>
“两种广泛被考虑的分布式学习算法是聊天和随机游走学习。聊天算法（同步和异步版本）具有高通信成本，而随机游走学习则具有增长的收敛时间。在这篇论文中，我们设计了一种快速和通信效率高的异步分布式学习机制DIGEST，通过融合聊天和随机游走的想法，专注于随机梯度下降（SGD）。DIGEST是一种异步分布式算法，基于本地SGD算法，原本设计用于通信效率高的中央化学习。我们设计了单流和多流DIGEST，其通信开销随着流数增加，并且存在一种收敛和通信开销贸易，可以利用。我们分析了单流和多流DIGEST的收敛，并证明它们在iid和非iid数据分布下都能够向优化解决方案 asymptotically。我们对单流和多流DIGEST进行了逻辑回归和深度神经网络ResNet20的性能评估。实验结果表明，多流DIGEST具有良好的收敛性质，即其收敛时间在iid设定下比基eline更快，并在非iid设定下超过基eline。”
</details></li>
</ul>
<hr>
<h2 id="SALC-Skeleton-Assisted-Learning-Based-Clustering-for-Time-Varying-Indoor-Localization"><a href="#SALC-Skeleton-Assisted-Learning-Based-Clustering-for-Time-Varying-Indoor-Localization" class="headerlink" title="SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization"></a>SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07650">http://arxiv.org/abs/2307.07650</a></li>
<li>repo_url: None</li>
<li>paper_authors: An-Hung Hsiao, Li-Hsiang Shen, Chen-Yi Chang, Chun-Jie Chiu, Kai-Ten Feng</li>
<li>For: The paper is written for establishing a sustainable and accurate indoor localization system that can adapt to highly-changing environments.* Methods: The paper proposes a skeleton-assisted learning-based clustering localization (SALC) system that jointly considers similarities from the skeleton-based shortest path (SSP) and time-varying RSS measurements across reference points (RPs). The system includes RSS-oriented map-assisted clustering (ROMAC), cluster-based online database establishment (CODE), and cluster-scaled location estimation (CsLE).* Results: The proposed SALC system can effectively reconstruct the fingerprint database with an enhanced location estimation accuracy, outperforming other existing schemes in the open literature. Both simulation and experimental results demonstrate the effectiveness of the proposed system.<details>
<summary>Abstract</summary>
Wireless indoor localization has attracted significant amount of attention in recent years. Using received signal strength (RSS) obtained from WiFi access points (APs) for establishing fingerprinting database is a widely utilized method in indoor localization. However, the time-variant problem for indoor positioning systems is not well-investigated in existing literature. Compared to conventional static fingerprinting, the dynamicallyreconstructed database can adapt to a highly-changing environment, which achieves sustainability of localization accuracy. To deal with the time-varying issue, we propose a skeleton-assisted learning-based clustering localization (SALC) system, including RSS-oriented map-assisted clustering (ROMAC), cluster-based online database establishment (CODE), and cluster-scaled location estimation (CsLE). The SALC scheme jointly considers similarities from the skeleton-based shortest path (SSP) and the time-varying RSS measurements across the reference points (RPs). ROMAC clusters RPs into different feature sets and therefore selects suitable monitor points (MPs) for enhancing location estimation. Moreover, the CODE algorithm aims for establishing adaptive fingerprint database to alleviate the timevarying problem. Finally, CsLE is adopted to acquire the target position by leveraging the benefits of clustering information and estimated signal variations in order to rescale the weights fromweighted k-nearest neighbors (WkNN) method. Both simulation and experimental results demonstrate that the proposed SALC system can effectively reconstruct the fingerprint database with an enhanced location estimation accuracy, which outperforms the other existing schemes in the open literature.
</details>
<details>
<summary>摘要</summary>
sans serif;">无线内部位置系统在过去几年内吸引了广泛的关注。使用WiFi接入点（AP）获得的接收信号强度（RSS）来建立指本库是内部位置系统中广泛使用的方法。然而，现有文献中对indoor位置系统中的时间变化问题的研究不够。相比于传统的静止指本，动态重建库可以适应高度变化的环境，实现地位测定精度的持续性。为解决时间变化问题，我们提议一种骨架协助学习基于扩展的分布式位置估计系统（SALC），包括RSS导向的地图帮助分组（ROMAC）、群集基本在线数据建立（CODE）和群集缩放位置估计（CsLE）。SALC方案同时考虑骨架基于最短路（SSP）的相似性和时间变化的RSS测量值 across reference points（RPs）。ROMAC将RPs分为不同的特征集并因此选择了改进地位估计的适用点（MPs）。此外，CODE算法目的是建立适应时间变化的指本库，以解决时间变化问题。最后，CsLE方法使用分组信息和估计信号变化来重新衡量weighted k-nearest neighbors（WkNN）方法中的权重，以实现更高的地位估计精度。在实验和 simulations中，我们发现，提议的SALC系统可以更好地重建指本库，并在开 literature中的其他方案中表现出更高的地位估计精度。
</details></li>
</ul>
<hr>
<h2 id="DistTGL-Distributed-Memory-Based-Temporal-Graph-Neural-Network-Training"><a href="#DistTGL-Distributed-Memory-Based-Temporal-Graph-Neural-Network-Training" class="headerlink" title="DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training"></a>DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07649">http://arxiv.org/abs/2307.07649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongkuan Zhou, Da Zheng, Xiang Song, George Karypis, Viktor Prasanna</li>
<li>for: 这个论文主要用于提出一种可Scalable的 distributed GPU clusters 上进行 memory-based Temporal Graph Neural Networks 的训练方法，以提高训练效率和精度。</li>
<li>methods: 该论文提出了三个改进方法：1) 提高 TGNN 模型，2) 开发了一种新的训练算法，3) 优化系统。</li>
<li>results: 在实验中，DistTGL 实现了近线性的速度增长，相比单机方法，准确率提高 14.5%，训练 durchput 提高 10.17倍。<details>
<summary>Abstract</summary>
Memory-based Temporal Graph Neural Networks are powerful tools in dynamic graph representation learning and have demonstrated superior performance in many real-world applications. However, their node memory favors smaller batch sizes to capture more dependencies in graph events and needs to be maintained synchronously across all trainers. As a result, existing frameworks suffer from accuracy loss when scaling to multiple GPUs. Evenworse, the tremendous overhead to synchronize the node memory make it impractical to be deployed to distributed GPU clusters. In this work, we propose DistTGL -- an efficient and scalable solution to train memory-based TGNNs on distributed GPU clusters. DistTGL has three improvements over existing solutions: an enhanced TGNN model, a novel training algorithm, and an optimized system. In experiments, DistTGL achieves near-linear convergence speedup, outperforming state-of-the-art single-machine method by 14.5% in accuracy and 10.17x in training throughput.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Memory-based Temporal Graph Neural Networks are powerful tools in dynamic graph representation learning and have demonstrated superior performance in many real-world applications. However, their node memory favors smaller batch sizes to capture more dependencies in graph events and needs to be maintained synchronously across all trainers. As a result, existing frameworks suffer from accuracy loss when scaling to multiple GPUs. Even worse, the tremendous overhead to synchronize the node memory make it impractical to be deployed to distributed GPU clusters. In this work, we propose DistTGL -- an efficient and scalable solution to train memory-based TGNNs on distributed GPU clusters. DistTGL has three improvements over existing solutions: an enhanced TGNN model, a novel training algorithm, and an optimized system. In experiments, DistTGL achieves near-linear convergence speedup, outperforming state-of-the-art single-machine method by 14.5% in accuracy and 10.17x in training throughput." into Simplified Chinese.<<SYS>>室内Memery-based Temporal Graph Neural Networks是动态图表示学习中的 poderful工具，在多个实际应用中表现出了superior的性能。然而，它们的节点记忆偏好 smaller batch size以捕捉更多的图事件依赖关系，并需要在所有训练器上同步保持。因此，现有的框架会导致精度损失when scaling to multiple GPUs。worse, synchronizing the node memory leads to significant overhead, making it impractical to deploy to distributed GPU clusters.在这种情况下，我们提出了DistTGL——一种高效可扩展的解决方案，用于在分布式GPU集群上训练 memory-based TGNNs。DistTGL有三个改进：一种改进的TGNN模型，一种新的训练算法，以及一种优化的系统。在实验中，DistTGL实现了近线性的速度增长，相比单机方法的14.5%的精度提升和10.17x的训练吞吐量。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Link-Prediction-over-Hyper-Relational-Temporal-Knowledge-Graphs-Enhanced-with-Time-Invariant-Relational-Knowledge"><a href="#Exploring-Link-Prediction-over-Hyper-Relational-Temporal-Knowledge-Graphs-Enhanced-with-Time-Invariant-Relational-Knowledge" class="headerlink" title="Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge"></a>Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10219">http://arxiv.org/abs/2307.10219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifeng Ding, Jingcheng Wu, Jingpei Wu, Yan Xia, Volker Tresp</li>
<li>for: 这篇论文主要针对的是hyper-relational知识 graphs（HKGs）和temporal知识 graphs（TKGs）的理解和推理。</li>
<li>methods: 作者提出了两个新的benchmark datasets（Wiki-hy和YAGO-hy）和一种HTKG理解模型，该模型可以有效地处理时间信息和资料信息。</li>
<li>results: 实验结果表明，作者的模型在HTKG连接预测任务上显著超过了之前相关方法，并且可以通过同时利用时间不变的关系知识和时间信息来进一步提高表现。<details>
<summary>Abstract</summary>
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and YAGO-hy, and propose a HTKG reasoning model that efficiently models both temporal facts and qualifiers. We further exploit additional time-invariant relational knowledge from the Wikidata knowledge base and study its effectiveness in HTKG reasoning. Time-invariant relational knowledge serves as the knowledge that remains unchanged in time (e.g., Sasha Obama is the child of Barack Obama), and it has never been fully explored in previous TKG reasoning benchmarks and approaches. Experimental results show that our model substantially outperforms previous related methods on HTKG link prediction and can be enhanced by jointly leveraging both temporal and time-invariant relational knowledge.
</details>
<details>
<summary>摘要</summary>
traditional知识 graphs (KGs)的核心思想，hyper-relational知识 graphs (HKGs)提供每个KG事实的额外键值对（即资格），以更好地限定事实的有效性。近年来，研究图像理解在HKGs上有增加的兴趣。同时，由于世界知识的演化性，广泛的平行工作在图像理解过程中强调时间因素。现有的HKG理解方法不考虑时间信息，而且所有以前的TKG理解方法只是强调时间理解，没有考虑资格。为了填补这一空白，我们的目标是将HKG理解和TKG理解联系起来。我们开发了两个新的Benchmark hyper-relational TKG（HTKG）数据集，即Wiki-hy和YAGO-hy，并提出了一种HTKG理解模型，该模型能够有效地处理时间因素和资格。此外，我们还利用Wikidata知识库中的时间不变的关系知识，并研究其在HTKG理解中的效果。时间不变的关系知识是指不会随着时间的变化（例如萨沙·奥巴马是巴拉克·奥巴马的孩子），这种知识从未在过去的TKG理解benchmark和方法中被完全探索。实验结果表明，我们的模型在HTKG链接预测任务上显著超越了相关方法，并且可以通过同时利用时间因素和时间不变的关系知识来进一步提高性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Model-Size-Agnostic-Compute-Free-Memorization-based-Inference-of-Deep-Learning"><a href="#Towards-Model-Size-Agnostic-Compute-Free-Memorization-based-Inference-of-Deep-Learning" class="headerlink" title="Towards Model-Size Agnostic, Compute-Free, Memorization-based Inference of Deep Learning"></a>Towards Model-Size Agnostic, Compute-Free, Memorization-based Inference of Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07631">http://arxiv.org/abs/2307.07631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Giacomini, Maeesha Binte Hashem, Jeremiah Suarez, Swarup Bhunia, Amit Ranjan Trivedi</li>
<li>for: 提高资源受限设备上深度神经网络模型的部署</li>
<li>methods: 使用记忆搜索（MBI），具有计算免卷和只需查找的特点，通过缓存中存储键值对来实现计算免卷的推理</li>
<li>results: 相比较现有的计算在内存（CIM）方法，MBI在MNIST字符识别任务上提高了能效率，相对于多层感知（MLP）-CIM和ResNet20-CIM方法，MBI的能效率提高了大约2.7倍和83倍Here’s the full translation of the abstract in Simplified Chinese:随着深度神经网络的快速发展，它们在各种任务上的表现得到了大幅提高，如图像和语音识别等。然而，随着模型的复杂度增加，计算成本和参数数量也随之增加，使得在资源受限设备上部署这些模型变得更加困难。本文提出了一种新的记忆搜索（MBI）方法，它具有计算免卷和只需查找的特点。通过缓存中存储键值对来实现计算免卷的推理。我们利用了隐藏向量来组合多个扫描结果，以实现问题的总分类输出。通过bayesian优化和归一化，减少了必要的查找数量，提高了准确率。此外，我们还提出了内存计算电路来快速查找输入查询匹配的关键vector。相比较现有的计算在内存（CIM）方法，MBI在MNIST字符识别任务上提高了能效率，相对于多层感知（MLP）-CIM和ResNet20-CIM方法，MBI的能效率提高了大约2.7倍和83倍。<details>
<summary>Abstract</summary>
The rapid advancement of deep neural networks has significantly improved various tasks, such as image and speech recognition. However, as the complexity of these models increases, so does the computational cost and the number of parameters, making it difficult to deploy them on resource-constrained devices. This paper proposes a novel memorization-based inference (MBI) that is compute free and only requires lookups. Specifically, our work capitalizes on the inference mechanism of the recurrent attention model (RAM), where only a small window of input domain (glimpse) is processed in a one time step, and the outputs from multiple glimpses are combined through a hidden vector to determine the overall classification output of the problem. By leveraging the low-dimensionality of glimpse, our inference procedure stores key value pairs comprising of glimpse location, patch vector, etc. in a table. The computations are obviated during inference by utilizing the table to read out key-value pairs and performing compute-free inference by memorization. By exploiting Bayesian optimization and clustering, the necessary lookups are reduced, and accuracy is improved. We also present in-memory computing circuits to quickly look up the matching key vector to an input query. Compared to competitive compute-in-memory (CIM) approaches, MBI improves energy efficiency by almost 2.7 times than multilayer perceptions (MLP)-CIM and by almost 83 times than ResNet20-CIM for MNIST character recognition.
</details>
<details>
<summary>摘要</summary>
深度神经网络的快速进步大大提高了各种任务，如图像和语音识别。然而，随着模型的复杂度增加，计算成本和参数数量也在增加，使得在有限资源的设备上部署变得困难。这篇论文提出了一种新的记忆化推理（MBI），它是计算免的，只需要lookups。我们的工作利用回卷注意力模型（RAM）的推理机制，只处理一次步骤中的小窗口输入领域（印象），并将多个印象的输出组合到一个隐藏向量中，以确定问题的总分类输出。我们利用印象的低维度，将推理过程中的关键值对存储在一个表中。在推理过程中，通过利用表来读取关键值对和计算免的推理。通过对搜索和分区进行优化，减少了必要的lookups，提高了准确率。我们还提出了内存计算电路，快速查找输入查询对应的匹配键向量。与与计算在内存（CIM）方法相比，MBI提高了能效率，相对于多层感知（MLP）-CIM的2.7倍，相对于ResNet20-CIM的83倍。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Embeddings-with-Cross-batch-Metric-Learning"><a href="#Generalizable-Embeddings-with-Cross-batch-Metric-Learning" class="headerlink" title="Generalizable Embeddings with Cross-batch Metric Learning"></a>Generalizable Embeddings with Cross-batch Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07620">http://arxiv.org/abs/2307.07620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yetigurbuz/xml-dml">https://github.com/yetigurbuz/xml-dml</a></li>
<li>paper_authors: Yeti Z. Gurbuz, A. Aydin Alatan</li>
<li>for: 本文研究了深度度量学中的全球平均池化（GAP）组件，以及如何使其更好地捕捉Semantic Entity。</li>
<li>methods: 本文使用了学习可迁移的原型来表示GAP，并表明了这种方法可以在不同的批处理中进行可靠的学习。</li>
<li>results: 本文在4个深度度量学benchmark上验证了这种方法的效果，并达到了比较好的结果。In English, this means:</li>
<li>for: The paper studies the Global Average Pooling (GAP) component in deep metric learning (DML) and how it can better capture Semantic Entity.</li>
<li>methods: The paper uses learnable prototypes to represent GAP, and shows that this method can be reliably learned across different batches.</li>
<li>results: The paper verifies the effectiveness of this method on four popular DML benchmarks, achieving good results.<details>
<summary>Abstract</summary>
Global average pooling (GAP) is a popular component in deep metric learning (DML) for aggregating features. Its effectiveness is often attributed to treating each feature vector as a distinct semantic entity and GAP as a combination of them. Albeit substantiated, such an explanation's algorithmic implications to learn generalizable entities to represent unseen classes, a crucial DML goal, remain unclear. To address this, we formulate GAP as a convex combination of learnable prototypes. We then show that the prototype learning can be expressed as a recursive process fitting a linear predictor to a batch of samples. Building on that perspective, we consider two batches of disjoint classes at each iteration and regularize the learning by expressing the samples of a batch with the prototypes that are fitted to the other batch. We validate our approach on 4 popular DML benchmarks.
</details>
<details>
<summary>摘要</summary>
全球平均池化（GAP）是深度度量学（DML）中常用的一个组件，用于Feature集合。其效果通常被归结到对每个特征向量视为不同的semantic实体，并将GAP视为它们的组合。虽然这种解释得到了证明，但是它的算法逻辑来学习可 generalized Entities来表示未经看过的类，深度度量学的重要目标，仍然不清楚。为此，我们将GAP表示为可学习的原型的吞合权重的 convex combination。我们然后证明了这种原型学习可以表示为一个递归过程，对一个批处理样本适应一个线性预测器。从这个角度出发，我们考虑了两个不同的批处理，并在每个迭代阶段对学习进行正则化，使用这些批处理中的样本表示另一个批处理中的原型。我们验证了我们的方法在4个深度度量学标准测试集上。
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Factorizing-Boolean-Matrices-using-Proximal-Gradient-Descent"><a href="#Efficiently-Factorizing-Boolean-Matrices-using-Proximal-Gradient-Descent" class="headerlink" title="Efficiently Factorizing Boolean Matrices using Proximal Gradient Descent"></a>Efficiently Factorizing Boolean Matrices using Proximal Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07615">http://arxiv.org/abs/2307.07615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdall/elbmf-python">https://github.com/sdall/elbmf-python</a></li>
<li>paper_authors: Sebastian Dalleiger, Jilles Vreeken</li>
<li>for:  addresses the interpretability problem of NMF on Boolean data</li>
<li>methods:  uses Boolean algebra to decompose the input into low-rank Boolean factor matrices, with a novel elastic-binary regularizer and proximal gradient algorithm</li>
<li>results:  demonstrates good performance in practice, with quick convergence, precise recovery of ground truth, and exact estimation of simulated rank; improves upon the state of the art in recall, loss, and runtime, and provides easily interpretable and semantically meaningful results on real-world data.Here’s the full text in Simplified Chinese:</li>
<li>for: addresses the interpretability problem of NMF on Boolean data</li>
<li>methods: 使用Boolean代数划分输入为低级Boolean分解矩阵，这些矩阵具有高可解释性和实际上非常有用，但是需要解决NP困难的 combinatorial优化问题; 我们提议使用一种新的灵活二进制正则化，从而 derivate一种 proximal 梯度算法</li>
<li>results: 通过广泛的实验表明，我们的方法在实际中工作良好：在 sintetic 数据上，它快速收敛，准确地回归真实值，并且正确地估算预设的rank; 在实际数据上，它超越了现有的状态，在回归、损失和运行时间上均有所提高，并且一个医疗领域的案例研究表明，我们的结果易于理解和具有Semantically Meaningful。<details>
<summary>Abstract</summary>
Addressing the interpretability problem of NMF on Boolean data, Boolean Matrix Factorization (BMF) uses Boolean algebra to decompose the input into low-rank Boolean factor matrices. These matrices are highly interpretable and very useful in practice, but they come at the high computational cost of solving an NP-hard combinatorial optimization problem. To reduce the computational burden, we propose to relax BMF continuously using a novel elastic-binary regularizer, from which we derive a proximal gradient algorithm. Through an extensive set of experiments, we demonstrate that our method works well in practice: On synthetic data, we show that it converges quickly, recovers the ground truth precisely, and estimates the simulated rank exactly. On real-world data, we improve upon the state of the art in recall, loss, and runtime, and a case study from the medical domain confirms that our results are easily interpretable and semantically meaningful.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:Addressing the interpretability problem of NMF on Boolean data, Boolean Matrix Factorization (BMF) uses Boolean algebra to decompose the input into low-rank Boolean factor matrices. These matrices are highly interpretable and very useful in practice, but they come at the high computational cost of solving an NP-hard combinatorial optimization problem. To reduce the computational burden, we propose to relax BMF continuously using a novel elastic-binary regularizer, from which we derive a proximal gradient algorithm. Through an extensive set of experiments, we demonstrate that our method works well in practice: On synthetic data, we show that it converges quickly, recovers the ground truth precisely, and estimates the simulated rank exactly. On real-world data, we improve upon the state of the art in recall, loss, and runtime, and a case study from the medical domain confirms that our results are easily interpretable and semantically meaningful.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-Detection-of-Urgency-of-Discussion-Forum-Posts"><a href="#Towards-Generalizable-Detection-of-Urgency-of-Discussion-Forum-Posts" class="headerlink" title="Towards Generalizable Detection of Urgency of Discussion Forum Posts"></a>Towards Generalizable Detection of Urgency of Discussion Forum Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07614">http://arxiv.org/abs/2307.07614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pcla-code/forum-posts-urgency">https://github.com/pcla-code/forum-posts-urgency</a></li>
<li>paper_authors: Valdemar Švábenský, Ryan S. Baker, Andrés Zambrano, Yishan Zou, Stefan Slater</li>
<li>for: 提高在线课程教学质量，帮助 instruктор更好地支持学生学习</li>
<li>methods: 使用predictive模型自动判断讨论区帖子的优先级，以便 instructor 更有效地响应学生问题</li>
<li>results: 使用支持向量回归算法和Universal Sentence Encoder嵌入式，实现了对讨论区帖子的优先级预测，可以帮助 instructor 更好地利用时间，提高学生学习质量<details>
<summary>Abstract</summary>
Students who take an online course, such as a MOOC, use the course's discussion forum to ask questions or reach out to instructors when encountering an issue. However, reading and responding to students' questions is difficult to scale because of the time needed to consider each message. As a result, critical issues may be left unresolved, and students may lose the motivation to continue in the course. To help address this problem, we build predictive models that automatically determine the urgency of each forum post, so that these posts can be brought to instructors' attention. This paper goes beyond previous work by predicting not just a binary decision cut-off but a post's level of urgency on a 7-point scale. First, we train and cross-validate several models on an original data set of 3,503 posts from MOOCs at University of Pennsylvania. Second, to determine the generalizability of our models, we test their performance on a separate, previously published data set of 29,604 posts from MOOCs at Stanford University. While the previous work on post urgency used only one data set, we evaluated the prediction across different data sets and courses. The best-performing model was a support vector regressor trained on the Universal Sentence Encoder embeddings of the posts, achieving an RMSE of 1.1 on the training set and 1.4 on the test set. Understanding the urgency of forum posts enables instructors to focus their time more effectively and, as a result, better support student learning.
</details>
<details>
<summary>摘要</summary>
在线学习者，如MOOC课程的学生，通常会使用课程的讨论 форуم来提问或与教师联系，当遇到问题时。然而，为了考虑每条消息，评估每个消息的时间成本很高，因此可能会有重要的问题被忽略。为了解决这个问题，我们构建了预测模型，以自动确定讨论 форум的优先级，以便将这些消息引导给教师的注意。这篇论文超过了之前的工作，不仅预测了一个二分类决策阈值，而且预测了每条消息的优先级水平，从1到7的7个级别。首先，我们训练和十分之检验了多种模型，使用大学 Pennsylvania的MOOC课程的原始数据集3,503条消息。其次，为了证明我们的模型的一致性，我们测试了它们的性能在另一个，已经发表的数据集29,604条消息中。而之前的帖子优先级预测工作只使用了一个数据集，我们在不同的数据集和课程之间评估预测。最佳性能的模型是使用Universe Sentence Encoder嵌入的支持向量回归模型，在训练集上的RMSE为1.1，测试集上的RMSE为1.4。了解讨论 форум中的帖子优先级，可以帮助教师更有效地利用时间，从而更好地支持学生学习。
</details></li>
</ul>
<hr>
<h2 id="First-order-Methods-for-Affinely-Constrained-Composite-Non-convex-Non-smooth-Problems-Lower-Complexity-Bound-and-Near-optimal-Methods"><a href="#First-order-Methods-for-Affinely-Constrained-Composite-Non-convex-Non-smooth-Problems-Lower-Complexity-Bound-and-Near-optimal-Methods" class="headerlink" title="First-order Methods for Affinely Constrained Composite Non-convex Non-smooth Problems: Lower Complexity Bound and Near-optimal Methods"></a>First-order Methods for Affinely Constrained Composite Non-convex Non-smooth Problems: Lower Complexity Bound and Near-optimal Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07605">http://arxiv.org/abs/2307.07605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Liu, Qihang Lin, Yangyang Xu</li>
<li>for: 这个论文主要针对 composite non-convex non-smooth 优化问题，具有线性和&#x2F;或非线性函数约束。</li>
<li>methods: 本论文使用 first-order method (FOM) 来解决上述问题，并提供了lower complexity bound的确定。</li>
<li>results: 本论文首次为 composite non-convex non-smooth 优化问题提供了lower complexity bound，并采用了一种名为减小距离梯度（IPG）方法来实现这个目标。该方法具有 oracle complexity 与 lower bound 几乎相同的性质。<details>
<summary>Abstract</summary>
Many recent studies on first-order methods (FOMs) focus on \emph{composite non-convex non-smooth} optimization with linear and/or nonlinear function constraints. Upper (or worst-case) complexity bounds have been established for these methods. However, little can be claimed about their optimality as no lower bound is known, except for a few special \emph{smooth non-convex} cases. In this paper, we make the first attempt to establish lower complexity bounds of FOMs for solving a class of composite non-convex non-smooth optimization with linear constraints. Assuming two different first-order oracles, we establish lower complexity bounds of FOMs to produce a (near) $\epsilon$-stationary point of a problem (and its reformulation) in the considered problem class, for any given tolerance $\epsilon>0$. In addition, we present an inexact proximal gradient (IPG) method by using the more relaxed one of the two assumed first-order oracles. The oracle complexity of the proposed IPG, to find a (near) $\epsilon$-stationary point of the considered problem and its reformulation, matches our established lower bounds up to a logarithmic factor. Therefore, our lower complexity bounds and the proposed IPG method are almost non-improvable.
</details>
<details>
<summary>摘要</summary>
很多最近的研究对于首项方法（FOMs）强调 composite non-convex non-smooth 优化问题，包括线性和/或非线性函数约束。然而，对于这些方法的优化性没有很多研究，只有一些特殊的平滑非几何优化问题例外。在这篇论文中，我们首次尝试确定 FOMs 对于解决 composite non-convex non-smooth 优化问题的类型的下界复杂度。我们假设两种不同的首项或acles，并确定 FOMs 的下界复杂度，以便在任何给定的 tolerance ε > 0 下，生成 (near) ε-站点。此外，我们还提出了一种不准确的 proximal Gradient（IPG）方法，使用更松的一个首项或acles。我们的 IPG 方法的 oracle 复杂度与我们确定的下界复杂度几乎相同，只有一个对数性logarithmic factor。因此，我们的下界复杂度和提出的 IPG 方法在优化性方面几乎不可改进。
</details></li>
</ul>
<hr>
<h2 id="Smooth-Lower-Bounds-for-Differentially-Private-Algorithms-via-Padding-and-Permuting-Fingerprinting-Codes"><a href="#Smooth-Lower-Bounds-for-Differentially-Private-Algorithms-via-Padding-and-Permuting-Fingerprinting-Codes" class="headerlink" title="Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes"></a>Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07604">http://arxiv.org/abs/2307.07604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naty Peter, Eliad Tsfadia, Jonathan Ullman</li>
<li>for: 这个论文是为了提供一种简单的方法来生成硬例子，以便为 differentially private（DP）算法的下界建立更加精细的lower bound。</li>
<li>methods: 这个论文使用了一种called “padding-and-permuting”的转换来生成硬例子，并使用了一个新的指纹代码构造方法来提供更加精细的下界。</li>
<li>results: 这个论文提供了新的下界在不同的设置下，包括DP averaging、approximate k-means clustering和DP subspace estimation等。这些下界是基于一种新的指纹lemmata，它比之前的指纹lemmata更加强大，并且可以直接从lemmata来证明下界。<details>
<summary>Abstract</summary>
Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.   In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:   1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).   2. A lower bound on the additive error of DP algorithms for approximate k-means clustering, as a function of the multiplicative error, which is tight for a constant multiplication error.   3. A lower bound for estimating the top singular vector of a matrix under DP in low-accuracy regimes, which is a special case of DP subspace estimation studied by Singhal and Steinke (NeurIPS 2021).   Our main technique is to apply a padding-and-permuting transformation to a fingerprinting code. However, rather than proving our results using a black-box access to an existing fingerprinting code (e.g., Tardos' code), we develop a new fingerprinting lemma that is stronger than those of Dwork et al. (FOCS 2015) and Bun et al. (SODA 2017), and prove our lower bounds directly from the lemma. Our lemma, in particular, gives a simpler fingerprinting code construction with optimal rate (up to polylogarithmic factors) that is of independent interest.
</details>
<details>
<summary>摘要</summary>
“指纹Argument”，最早由布恩、奥尔曼和 вадан（STOC 2014）引入，是最广泛使用的方法来确定下界或错误率的约 differentially private（DP）算法的下界。然而，有很多 differential privacy 问题，我们还没有知道合适的下界，而且甚至对已知的问题，下界不是平滑的，通常在误差大于某个阈值时变得无效。在这项工作中，我们提出了一种简单的方法，通过对指纹编码进行补充和排序转换来生成困难实例。我们通过以下几个方面证明了这种方法的应用性：1. 对DP抽象平均在低精度 régime中的下界，具体是对Nissim、Stemmer和 вадан（PODS 2016）所引入的私人1-集问题的新下界。2. DP算法对 Approximate k-means 集群化的添加性误差下界，其中multiplicative error是常数多项式的。3. DP算法对矩阵 top singular vector 的估计在低精度 régime中的下界，这是特殊的DP subspace estimation问题，与Singhal和Steinke（NeurIPS 2021）的研究相关。我们的主要技巧是对指纹编码进行补充和排序转换。而不是通过黑盒访问现有的指纹编码（例如Tardos的代码）来证明我们的结果（例如Dwork等人（FOCS 2015）和布恩等人（SODA 2017）的结果），我们开发了一个新的指纹 lemmatheorem，该lemmatheorem是Dwork等人（FOCS 2015）和布恩等人（SODA 2017）的lemmatheorem更强，并直接从lemmatheorem prove我们的下界。具体来说，我们的lemmatheorem提供了一种更简单的指纹编码建构，具有最佳率（即polylogarithmic factor），这是独立有价值的。
</details></li>
</ul>
<hr>
<h2 id="Training-Discrete-Energy-Based-Models-with-Energy-Discrepancy"><a href="#Training-Discrete-Energy-Based-Models-with-Energy-Discrepancy" class="headerlink" title="Training Discrete Energy-Based Models with Energy Discrepancy"></a>Training Discrete Energy-Based Models with Energy Discrepancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07595">http://arxiv.org/abs/2307.07595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Schröder, Zijing Ou, Yingzhen Li, Andrew B. Duncan</li>
<li>for: 本研究旨在提出一种新的对照损失函数，以便在离散空间上训练能量基模型（EBM）。</li>
<li>methods: 本研究使用了能量差（ED），一种新的对照损失函数，只需评估能量函数在数据点和其扰动版本之间的差异，无需采用MCMC样本抽取策略。</li>
<li>results: 研究人员通过对三种扰动过程（bernoulli噪声、杜特推论变换和邻域结构）的性能进行比较，并在离散链模型、二进制 sintetic 数据和离散图像数据集上进行了实验，证明了ED的效果。<details>
<summary>Abstract</summary>
Training energy-based models (EBMs) on discrete spaces is challenging because sampling over such spaces can be difficult. We propose to train discrete EBMs with energy discrepancy (ED), a novel type of contrastive loss functional which only requires the evaluation of the energy function at data points and their perturbed counter parts, thus not relying on sampling strategies like Markov chain Monte Carlo (MCMC). Energy discrepancy offers theoretical guarantees for a broad class of perturbation processes of which we investigate three types: perturbations based on Bernoulli noise, based on deterministic transforms, and based on neighbourhood structures. We demonstrate their relative performance on lattice Ising models, binary synthetic data, and discrete image data sets.
</details>
<details>
<summary>摘要</summary>
培训能量基于模型（EBM）在极性空间上是具有挑战性的，因为抽样这些空间可能困难。我们提议使用能量差（ED），一种新的对比损失函数，只需评估能量函数在数据点和其扰动版本之间，因此不需要采用样本策略如Markov链 Monte Carlo（MCMC）。能量差提供了对广泛类型扰动过程的理论保证，我们investigate三种类型的扰动过程：基于 Bernoulli 噪声、基于 deterministic transforms 和基于 neighbor structure。我们在邻居 Ising 模型、二进制 synthetic 数据和极性图像数据集上证明了它们的相对性能。
</details></li>
</ul>
<hr>
<h2 id="A-Quantitative-Approach-to-Predicting-Representational-Learning-and-Performance-in-Neural-Networks"><a href="#A-Quantitative-Approach-to-Predicting-Representational-Learning-and-Performance-in-Neural-Networks" class="headerlink" title="A Quantitative Approach to Predicting Representational Learning and Performance in Neural Networks"></a>A Quantitative Approach to Predicting Representational Learning and Performance in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07575">http://arxiv.org/abs/2307.07575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Pyle, Sebastian Musslick, Jonathan D. Cohen, Ankit B. Patel</li>
<li>for: 本研究旨在探讨神经网络（生物和人工）如何学习表示和处理输入信息，以解决任务。不同类型的表示可能适用于不同类型的任务，因此理解和设计有用的网络需要了解学习的表示。</li>
<li>methods: 本研究提出了一种新的 Pseudo-kernel 基于工具，用于分析和预测神经网络学习的表示。该工具基于网络的初始条件和训练课程，并且可以预测表示学习对顺序单任务和并行多任务性能的影响。</li>
<li>results: 研究人员使用了一个简单的测试案例，然后使用该工具对一个关于表示学习对顺序单任务和并行多任务性能的问题进行预测。结果显示，该工具可以预测表示学习的规模初始化和训练课程对下游同时多任务性能的影响。<details>
<summary>Abstract</summary>
A key property of neural networks (both biological and artificial) is how they learn to represent and manipulate input information in order to solve a task. Different types of representations may be suited to different types of tasks, making identifying and understanding learned representations a critical part of understanding and designing useful networks. In this paper, we introduce a new pseudo-kernel based tool for analyzing and predicting learned representations, based only on the initial conditions of the network and the training curriculum. We validate the method on a simple test case, before demonstrating its use on a question about the effects of representational learning on sequential single versus concurrent multitask performance. We show that our method can be used to predict the effects of the scale of weight initialization and training curriculum on representational learning and downstream concurrent multitasking performance.
</details>
<details>
<summary>摘要</summary>
neuronal networks（生物和人工的）的一个关键性能是如何学习表示和处理输入信息以解决任务。不同类型的表示可能适用于不同类型的任务，因此识别和理解学习的表示是设计有用网络的关键部分。在这篇论文中，我们介绍了一种新的 Pseudo-kernel 基于工具 для分析和预测学习的表示，只基于网络的初始条件和训练课程。我们验证了这种方法在一个简单的测试场景中，然后示cases the use of this method to predict the effects of representational learning on sequential single versus concurrent multitask performance. We show that our method can be used to predict the effects of the scale of weight initialization and training curriculum on representational learning and downstream concurrent multitasking performance.Here's the translation in Traditional Chinese: neuronal networks（生物和人工的）的一个关键性能是如何学习表示和处理输入信息以解决任务。不同的类型的表示可能适用于不同的任务，因此识别和理解学习的表示是设计有用网络的关键部分。在这篇论文中，我们介绍了一种新的 Pseudo-kernel 基于工具 для分析和预测学习的表示，只基于网络的初始条件和训练课程。我们验证了这种方法在一个简单的测试场景中，然后示cases the use of this method to predict the effects of representational learning on sequential single versus concurrent multitask performance. We show that our method can be used to predict the effects of the scale of weight initialization and training curriculum on representational learning and downstream concurrent multitasking performance.
</details></li>
</ul>
<hr>
<h2 id="Harpa-High-Rate-Phase-Association-with-Travel-Time-Neural-Fields"><a href="#Harpa-High-Rate-Phase-Association-with-Travel-Time-Neural-Fields" class="headerlink" title="Harpa: High-Rate Phase Association with Travel Time Neural Fields"></a>Harpa: High-Rate Phase Association with Travel Time Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07572">http://arxiv.org/abs/2307.07572</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dadacheng/phase_association">https://github.com/dadacheng/phase_association</a></li>
<li>paper_authors: Cheng Shi, Maarten V. de Hoop, Ivan Dokmanić</li>
<li>for: 这个论文是为了处理小型、高频地震事件所写的，以获取地震动力学特性的信息。</li>
<li>methods: 这个论文使用了深度神经场来建立波速和相关时间的生成模型，并解决了空间-时间源local化和波速恢复问题。</li>
<li>results: 这个论文表明可以在高速度下进行相关性分组，并且可以 efficiently处理不确定的波速。 numercial experiments表明，\harpa可以 efficiently associates high-rate seismicity clouds over complex, unknown wave speeds and graciously handles noisy and missing picks.<details>
<summary>Abstract</summary>
Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by association. We obviate the need for associated phases by interpreting arrival time data as probability measures and using an optimal transport loss to enforce data fidelity. The joint recovery problem is known to admit a unique solution under certain conditions but due to the non-convexity of the corresponding loss a simple gradient scheme converges to poor local minima. We show that this is effectively mitigated by stochastic gradient Langevin dynamics (SGLD). Numerical experiments show that \harpa~efficiently associates high-rate seismicity clouds over complex, unknown wave speeds and graciously handles noisy and missing picks.
</details>
<details>
<summary>摘要</summary>
phasic association groups seismic wave arrivals based on their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by association. We obviate the need for associated phases by interpreting arrival time data as probability measures and using an optimal transport loss to enforce data fidelity. The joint recovery problem is known to admit a unique solution under certain conditions but due to the non-convexity of the corresponding loss a simple gradient scheme converges to poor local minima. We show that this is effectively mitigated by stochastic gradient Langevin dynamics (SGLD). Numerical experiments show that \harpa~efficiently associates high-rate seismicity clouds over complex, unknown wave speeds and graciously handles noisy and missing picks.
</details></li>
</ul>
<hr>
<h2 id="Variational-Prediction"><a href="#Variational-Prediction" class="headerlink" title="Variational Prediction"></a>Variational Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07568">http://arxiv.org/abs/2307.07568</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/piyushpathak03/Recommendation-systems">https://github.com/piyushpathak03/Recommendation-systems</a></li>
<li>paper_authors: Alexander A. Alemi, Ben Poole</li>
<li>for: 这个论文是为了探讨 bayesian inference 的优势以及其计算成本问题。</li>
<li>methods: 这篇论文使用了一种名为 Variational Prediction 的技术，即直接学习一种 variational approximation 来 aproximate posterior predictive distribution。</li>
<li>results: 这篇论文通过使用 Variational Prediction 技术，可以提供良好的预测分布，而无需在测试时进行 marginalization 成本。<details>
<summary>Abstract</summary>
Bayesian inference offers benefits over maximum likelihood, but it also comes with computational costs. Computing the posterior is typically intractable, as is marginalizing that posterior to form the posterior predictive distribution. In this paper, we present variational prediction, a technique for directly learning a variational approximation to the posterior predictive distribution using a variational bound. This approach can provide good predictive distributions without test time marginalization costs. We demonstrate Variational Prediction on an illustrative toy example.
</details>
<details>
<summary>摘要</summary>
Note:* "Bayesian inference"  bayesian inference (悖论推理)* "maximum likelihood"  maximum likelihood (最大可能性)* "posterior"  posterior (后期)* "posterior predictive distribution"  posterior predictive distribution (后期预测分布)* "variational bound"  variational bound (可能性范围)* "variational prediction"  variational prediction (可能性预测)
</details></li>
</ul>
<hr>
<h2 id="Reconstruction-of-3-Axis-Seismocardiogram-from-Right-to-left-and-Head-to-foot-Components-Using-A-Long-Short-Term-Memory-Network"><a href="#Reconstruction-of-3-Axis-Seismocardiogram-from-Right-to-left-and-Head-to-foot-Components-Using-A-Long-Short-Term-Memory-Network" class="headerlink" title="Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network"></a>Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07566">http://arxiv.org/abs/2307.07566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Muntasir Rahman, Amirtahà Taebi</li>
<li>for: 这个研究旨在开发一个深度学习模型，用于预测心脏电压信号（SCG）的dorsoventral方向。</li>
<li>methods: 使用了15名健康成人的数据集来训练和验证模型，使用了三轴加速计 recording SCG信号，并使用了电cardiogram R波 Segmentation，将信号下推、 нормаLIZATION、中心化。</li>
<li>results: 研究获得了一个LSTM网络，可以将一个心脏周期中的100个时间步骤的SCG信号转换为dorsoventral方向的SCG信号，mean square error为0.09。这项研究显示了深度学习模型可以将 dual-axis加速计读取的数据转换为三轴SCG信号。<details>
<summary>Abstract</summary>
This pilot study aims to develop a deep learning model for predicting seismocardiogram (SCG) signals in the dorsoventral direction from the SCG signals in the right-to-left and head-to-foot directions ($\textrm{SCG}_x$ and $\textrm{SCG}_y$). The dataset used for the training and validation of the model was obtained from 15 healthy adult subjects. The SCG signals were recorded using tri-axial accelerometers placed on the chest of each subject. The signals were then segmented using electrocardiogram R waves, and the segments were downsampled, normalized, and centered around zero. The resulting dataset was used to train and validate a long short-term memory (LSTM) network with two layers and a dropout layer to prevent overfitting. The network took as input 100-time steps of $\textrm{SCG}_x$ and $\textrm{SCG}_y$, representing one cardiac cycle, and outputted a vector that mapped to the target variable being predicted. The results showed that the LSTM model had a mean square error of 0.09 between the predicted and actual SCG segments in the dorsoventral direction. The study demonstrates the potential of deep learning models for reconstructing 3-axis SCG signals using the data obtained from dual-axis accelerometers.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这项试验旨在开发一个深度学习模型，用于预测心电幂量信号（SCG）的dorsoventral方向。试验使用15名健康成人的SCG信号，通过三轴加速度计记录在胸部。信号被电cardiogram R波分割，下amples， норmalize和减少中心在零点。结果显示，使用LSTM网络（两层）和dropout层预防过拟合。网络输入100个时间步长的$SCG_x$和$SCG_y$，表示一个心脏频率征，输出一个向量，将目标变量映射到。结果显示LSTM模型与实际SCG段的平均方差为0.09。这项研究表明，深度学习模型可以使用双轴加速度计记录的数据来重建3轴SCG信号。
</details></li>
</ul>
<hr>
<h2 id="Expressive-Monotonic-Neural-Networks"><a href="#Expressive-Monotonic-Neural-Networks" class="headerlink" title="Expressive Monotonic Neural Networks"></a>Expressive Monotonic Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07512">http://arxiv.org/abs/2307.07512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niklasnolte/hlt_2track">https://github.com/niklasnolte/hlt_2track</a></li>
<li>paper_authors: Ouail Kitouni, Niklas Nolte, Michael Williams</li>
<li>for: 这个论文的目的是建立一种能够确保神经网络输出具有约束依赖性的权重建立方法，以便在各种应用场景中提高神经网络的可解释性和公平性。</li>
<li>methods: 该论文提出了一种基于权重约束的神经网络架构，通过单个差分连接来实现精确的依赖性。该方法直接控制神经网络的李普希茨常数，从而提供了额外的稳定性 benefit。</li>
<li>results: 该论文通过训练多种应用场景中的强大、稳定、可解释的探测器，达到了与当前状态艺术法的竞争性性能。<details>
<summary>Abstract</summary>
The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. We show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider.
</details>
<details>
<summary>摘要</summary>
很多情况下，神经网络的输出对某些输入的 monotonic dependence 是一种重要的推导假设。这种假设在解释性和公平性方面具有重要意义。在更广泛的上下文中， monotonicity 在金融、医学、物理和其他领域都具有重要的意义。因此，建立能够实现这种假设的神经网络架构是非常感兴趣的。在这种情况下，我们提出了一种带有单个差异连接的权重约束架构，可以实现任意输入子集的精确 monotonic dependence。这种约束方案直接控制神经网络的 Lipschitz 常数，从而提供了额外的robustness  benefit。与现有的 monotonicity 实现技术相比，我们的方法更简单，更有理论基础，计算开销几乎可以忽略不计，可以保证 monotonic dependence，并且具有很高的表达能力。我们显示了如何使用这种算法来训练高效、Robust、可解释的分类器，在社会应用和辐射子粒子在 CERN 大弹性粒子加速器中的分类方面达到了竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="MGit-A-Model-Versioning-and-Management-System"><a href="#MGit-A-Model-Versioning-and-Management-System" class="headerlink" title="MGit: A Model Versioning and Management System"></a>MGit: A Model Versioning and Management System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07507">http://arxiv.org/abs/2307.07507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Hao, Daniel Mendoza, Rafael da Silva, Deepak Narayanan, Amar Phanishaye</li>
<li>for: 这篇论文是关于机器学习（ML）中模型 derivation的管理系统，帮助用户更好地存储、测试、更新和合作模型Derivative。</li>
<li>methods: 该系统使用线aje graph来记录模型之间的 provinance和版本信息，并实现了高效存储模型参数的优化和相关的测试、更新和合作功能。</li>
<li>results: 该系统可以减少线aje graph的存储占用量，并自动将下游模型更新对应的上游模型的更新。<details>
<summary>Abstract</summary>
Models derived from other models are extremely common in machine learning (ML) today. For example, transfer learning is used to create task-specific models from "pre-trained" models through finetuning. This has led to an ecosystem where models are related to each other, sharing structure and often even parameter values. However, it is hard to manage these model derivatives: the storage overhead of storing all derived models quickly becomes onerous, prompting users to get rid of intermediate models that might be useful for further analysis. Additionally, undesired behaviors in models are hard to track down (e.g., is a bug inherited from an upstream model?). In this paper, we propose a model versioning and management system called MGit that makes it easier to store, test, update, and collaborate on model derivatives. MGit introduces a lineage graph that records provenance and versioning information between models, optimizations to efficiently store model parameters, as well as abstractions over this lineage graph that facilitate relevant testing, updating and collaboration functionality. MGit is able to reduce the lineage graph's storage footprint by up to 7x and automatically update downstream models in response to updates to upstream models.
</details>
<details>
<summary>摘要</summary>
现在机器学习（ML）中，基于其他模型 derivated 的模型非常常见。例如，通过 fine-tuning 来创建任务特定模型从 "预训练" 模型。这导致了一个模型之间相互关联，共享结构，甚至参数值的生态系统。然而，管理这些模型Derivative 很困难：存储所有 derivated 模型的存储开销很快就变得压力很大，让用户放弃 intermediate 模型，可能用于进一步分析。此外，模型中的不良行为困难跟踪（例如，一个 bug 是从上游模型继承吗？）。在这篇论文中，我们提出一个名为 MGit 的模型版本管理系统，使得更加容易存储、测试、更新和合作模型Derivative。MGit 引入了模型家族图，记录模型的 происхождение和版本信息，并且提供了 Parameters 的优化，以及基于这个家族图的抽象，使得更加方便地进行相关的测试、更新和合作功能。MGit 能够将模型家族图的存储占用量减少至最多 7 倍，并自动将下游模型更新响应上游模型的更新。
</details></li>
</ul>
<hr>
<h2 id="Brain-Tumor-Detection-using-Convolutional-Neural-Networks-with-Skip-Connections"><a href="#Brain-Tumor-Detection-using-Convolutional-Neural-Networks-with-Skip-Connections" class="headerlink" title="Brain Tumor Detection using Convolutional Neural Networks with Skip Connections"></a>Brain Tumor Detection using Convolutional Neural Networks with Skip Connections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07503">http://arxiv.org/abs/2307.07503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aupam Hamran, Marzieh Vaeztourshizi, Amirhossein Esmaili, Massoud Pedram</li>
<li>for: 用CNN分类脑肿为良性和恶性类型</li>
<li>methods: 使用MRI技术，采用不同的CNN建立方案进行分类</li>
<li>results: 结果显示，一些优化技术可以致使CNN模型在这个目标上表现出色<details>
<summary>Abstract</summary>
In this paper, we present different architectures of Convolutional Neural Networks (CNN) to analyze and classify the brain tumors into benign and malignant types using the Magnetic Resonance Imaging (MRI) technique. Different CNN architecture optimization techniques such as widening and deepening of the network and adding skip connections are applied to improve the accuracy of the network. Results show that a subset of these techniques can judiciously be used to outperform a baseline CNN model used for the same purpose.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了不同类型的卷积神经网络（CNN）来分类大脑肿瘤为良性和有害两类使用Magnetic Resonance Imaging（MRI）技术。不同的CNN结构优化技术 such as 宽化和深化网络以及添加跳过连接被应用以提高网络的准确率。结果显示，一个子集这些技术可以有效地使用以超越基eline CNN模型。Here's the word-for-word translation:在这篇论文中，我们介绍了不同类型的卷积神经网络（CNN）来分类大脑肿瘤为良性和有害两类使用Magnetic Resonance Imaging（MRI）技术。不同的CNN结构优化技术such as 宽化和深化网络以及添加跳过连接被应用以提高网络的准确率。结果显示，一个子集这些技术可以有效地使用以超越基eline CNN模型。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Photonic-Component-Design"><a href="#Reinforcement-Learning-for-Photonic-Component-Design" class="headerlink" title="Reinforcement Learning for Photonic Component Design"></a>Reinforcement Learning for Photonic Component Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11075">http://arxiv.org/abs/2307.11075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donald Witt, Jeff Young, Lukas Chrostowski</li>
<li>for: 本研究旨在开发一种含有异常处理的fab-in-the-loop算法，用于设计尺度在220nm的尺度较小的光子学组件。</li>
<li>methods: 该算法利用了异常处理机制，以抵消尺度较小的光子学组件 fabrication process中的异常。</li>
<li>results: 该算法可以提高插入损耗从8.8dB降至3.24dB，并且可以生成具有150nm宽扩展带width的设计，其最低点loss不超过10.2dB。<details>
<summary>Abstract</summary>
We present a new fab-in-the-loop reinforcement learning algorithm for the design of nano-photonic components that accounts for the imperfections present in nanofabrication processes. As a demonstration of the potential of this technique, we apply it to the design of photonic crystal grating couplers (PhCGC) fabricated on a 220nm silicon on insulator (SOI) single etch platform. This fab-in-the-loop algorithm improves the insertion loss from 8.8 dB to 3.24 dB. The widest bandwidth designs produced using our fab-in-the-loop algorithm are able to cover a 150nm bandwidth with less than 10.2 dB of loss at their lowest point.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的 fab-in-the-loop 束缚学习算法，用于 nanophotonic 组件的设计，考虑到 nanofabrication 过程中存在的不确定性。作为这种技术的演示，我们应用它于 SOI 单刻平台上的 photonic crystal grating couplers (PhCGC) 的设计。这种 fab-in-the-loop 算法改善了插入损耗从 8.8 dB 降低至 3.24 dB。我们使用这种算法生成的设计可以覆盖 150nm 的频谱宽度，且损耗在最低点下不超过 10.2 dB。
</details></li>
</ul>
<hr>
<h2 id="PseudoCal-A-Source-Free-Approach-to-Unsupervised-Uncertainty-Calibration-in-Domain-Adaptation"><a href="#PseudoCal-A-Source-Free-Approach-to-Unsupervised-Uncertainty-Calibration-in-Domain-Adaptation" class="headerlink" title="PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation"></a>PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07489">http://arxiv.org/abs/2307.07489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dapeng Hu, Jian Liang, Xinchao Wang, Chuan-Sheng Foo<br>for:This paper focuses on improving the calibration of predictive uncertainty in unsupervised domain adaptation (UDA) models, specifically in source-free UDA settings.methods:The proposed method, PseudoCal, relies exclusively on unlabeled target data to calibrate UDA models. It transforms the unsupervised calibration problem into a supervised one by generating a labeled pseudo-target set that captures the structure of the real target.results:Extensive experiments on 10 UDA methods show that PseudoCal consistently exhibits significantly reduced calibration error compared to existing calibration methods, both in traditional UDA settings and recent source-free UDA scenarios.<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in improving the accuracy of models for unlabeled target domains. However, the calibration of predictive uncertainty in the target domain, a crucial aspect of the safe deployment of UDA models, has received limited attention. The conventional in-domain calibration method, \textit{temperature scaling} (TempScal), encounters challenges due to domain distribution shifts and the absence of labeled target domain data. Recent approaches have employed importance-weighting techniques to estimate the target-optimal temperature based on re-weighted labeled source data. Nonetheless, these methods require source data and suffer from unreliable density estimates under severe domain shifts, rendering them unsuitable for source-free UDA settings. To overcome these limitations, we propose PseudoCal, a source-free calibration method that exclusively relies on unlabeled target data. Unlike previous approaches that treat UDA calibration as a \textit{covariate shift} problem, we consider it as an unsupervised calibration problem specific to the target domain. Motivated by the factorization of the negative log-likelihood (NLL) objective in TempScal, we generate a labeled pseudo-target set that captures the structure of the real target. By doing so, we transform the unsupervised calibration problem into a supervised one, enabling us to effectively address it using widely-used in-domain methods like TempScal. Finally, we thoroughly evaluate the calibration performance of PseudoCal by conducting extensive experiments on 10 UDA methods, considering both traditional UDA settings and recent source-free UDA scenarios. The experimental results consistently demonstrate the superior performance of PseudoCal, exhibiting significantly reduced calibration error compared to existing calibration methods.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA) 技术在目标频道中的准确性方面做出了很多突出的进步，但是目标频道中的预测 uncertainty 的准确性却受到了有限的关注。传统的域内准则（TempScal）方法在域 Distribution 的转移和目标频道没有标注数据的情况下遇到了挑战。现有的方法使用重要性评估技术来估算目标频道优化的温度，但是这些方法需要源数据，而且在严重的域转移情况下，概率估计不可靠，因此不适用于源自由 UDA 设置。为了解决这些局限性，我们提出了 PseudoCal，一种源自由的准则调整方法，不需要源数据。与前期方法不同，我们将 UDA 准则调整视为目标频道特有的无监督准则调整问题，而不是 covariate shift 问题。受 TempScal 的负逻辑 log-likelihood（NLL） objective 的因子化启发，我们生成了一个 Pseudo-target 集，这个集合捕捉了真实target 的结构。通过这种方式，我们将无监督准则调整问题转化为监督的一个，可以使用现有的域内方法，如 TempScal，进行有效地处理。最后，我们进行了广泛的实验，评估了 10 种 UDA 方法，包括传统的 UDA 设置以及 recent source-free UDA 情况。实验结果表明，PseudoCal 的准则调整性能明显高于现有的准则调整方法，显示它在 calibration error 方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="DreamTeacher-Pretraining-Image-Backbones-with-Deep-Generative-Models"><a href="#DreamTeacher-Pretraining-Image-Backbones-with-Deep-Generative-Models" class="headerlink" title="DreamTeacher: Pretraining Image Backbones with Deep Generative Models"></a>DreamTeacher: Pretraining Image Backbones with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07487">http://arxiv.org/abs/2307.07487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, Sanja Fidler</li>
<li>for: 本研究旨在提出一种自然语言处理框架，即梦教师，该框架利用生成网络进行预训练下游图像背景。</li>
<li>methods: 我们提出了两种知识填充方法：1）将生成网络学习的生成特征填充到目标图像背景上，作为对ImageNet大型标注数据集的预训练；2）将生成网络获得的标签填充到目标背景上的Logits上。</li>
<li>results: 我们进行了多种生成模型、精密预测benchmark和预训练策略的实验研究，并观察到我们的梦教师在所有自我超越现有自然语言处理方法。不需要手动标注，使用梦教师进行无监督图像预训练，可以获得显著改善。<details>
<summary>Abstract</summary>
In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significant improvements over ImageNet classification pre-training on downstream datasets, showcasing generative models, and diffusion generative models specifically, as a promising approach to representation learning on large, diverse datasets without requiring manual annotation.
</details>
<details>
<summary>摘要</summary>
“在这个研究中，我们介绍了一个自我超vised特征表示学习框架DreamTeacher，该框架利用生成网络进行预训练下游图像脑筋。我们提议通过将生成模型已经学习的特征知识融入到标准图像脑筋中来，而不是使用大量标注数据集如ImageNet进行预训练。我们研究了两种知识融入方法：1）将生成模型学习的特征知识直接融入目标图像脑筋中，2）将生成网络中的标签融入到目标脑筋的响应值中。我们对多种生成模型、粘密预测benchmark和预训练策略进行了广泛的分析。我们发现，我们的DreamTeacher在所有自我超vised表示学习方法之上表现出优异的成绩。不需要手动标注，使用DreamTeacher进行无监督ImageNet预训练可以在下游数据集上获得显著改进，特别是使用扩散生成模型。”
</details></li>
</ul>
<hr>
<h2 id="Population-Expansion-for-Training-Language-Models-with-Private-Federated-Learning"><a href="#Population-Expansion-for-Training-Language-Models-with-Private-Federated-Learning" class="headerlink" title="Population Expansion for Training Language Models with Private Federated Learning"></a>Population Expansion for Training Language Models with Private Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07477">http://arxiv.org/abs/2307.07477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tatsuki Koga, Congzheng Song, Martin Pelikan, Mona Chitnis</li>
<li>for: 这个研究旨在提高 federated learning（FL） combined with differential privacy（DP）的机器学习（ML）训练效率和形式化隐私保证，尤其是在小型人口的情况下。</li>
<li>methods: 这个研究使用了域适应技术来扩展人口，以加快训练和提高最终模型质量。</li>
<li>results: 研究表明，使用这些技术可以提高模型的使用価价（ Utility），在实际的语言模型化数据集上提高13%到30%。<details>
<summary>Abstract</summary>
Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.
</details>
<details>
<summary>摘要</summary>
联合 federated learning (FL) 和差异隐私 (DP) 可以为分布式设备进行机器学习 (ML) 训练，并且提供正式的隐私保证。通过大量的设备人口，FL 与 DP 可以生成高性能的模型，但是在小规模应用中，模型的Utility 会逐渐下降，而且训练时间会增加，因为等待足够的客户端可用于训练的池子中 slower。为了解决这个问题，我们提议通过领域适应技术扩大人口，以加速训练和提高最终模型质量。我们经验表明，我们的技术可以提高实际语言模型集成的Utility 13% 到 30%。
</details></li>
</ul>
<hr>
<h2 id="Structured-Pruning-of-Neural-Networks-for-Constraints-Learning"><a href="#Structured-Pruning-of-Neural-Networks-for-Constraints-Learning" class="headerlink" title="Structured Pruning of Neural Networks for Constraints Learning"></a>Structured Pruning of Neural Networks for Constraints Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07457">http://arxiv.org/abs/2307.07457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Cacciola, Antonio Frangioni, Andrea Lodi</li>
<li>for: 这篇论文主要探讨了Machine Learning（ML）模型与Operation Research（OR）工具的组合，尤其是在肝癌治疗、算法配置和化学处理优化等领域。</li>
<li>methods: 本研究使用了删除（pruning）技术来将人工神经网络（ANNs）裁短，以提高Mixed Integer Programming（MIP）表达的解决速度。</li>
<li>results: 实验结果显示，删除可以对多层 feed-forward neural networks 建立反例，并且可以实现很大的解决速度提高，而不会对最终决策的质量产生影响。<details>
<summary>Abstract</summary>
In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity across diverse applications, including cancer treatment, algorithmic configuration, and chemical process optimization. In this domain, the combination of ML and OR often relies on representing the ML model output using Mixed Integer Programming (MIP) formulations. Numerous studies in the literature have developed such formulations for many ML predictors, with a particular emphasis on Artificial Neural Networks (ANNs) due to their significant interest in many applications. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations that are impractical to solve, thereby impeding scalability. In fact, the ML community has already introduced several techniques to reduce the parameter count of ANNs without compromising their performance, since the substantial size of modern ANNs presents challenges for ML applications as it significantly impacts computational efforts during training and necessitates significant memory resources for storage. In this paper, we showcase the effectiveness of pruning, one of these techniques, when applied to ANNs prior to their integration into MIPs. By pruning the ANN, we achieve significant improvements in the speed of the solution process. We discuss why pruning is more suitable in this context compared to other ML compression techniques, and we identify the most appropriate pruning strategies. To highlight the potential of this approach, we conduct experiments using feed-forward neural networks with multiple layers to construct adversarial examples. Our results demonstrate that pruning offers remarkable reductions in solution times without hindering the quality of the final decision, enabling the resolution of previously unsolvable instances.
</details>
<details>
<summary>摘要</summary>
近年来，机器学习（ML）模型与运筹学（OR）工具的集成在多种应用中得到了普遍的推广，包括肿瘤治疗、算法配置和化学过程优化。在这个领域，ML和OR的结合经常通过使用混合整数编程（MIP）表述来实现。文献中有很多研究对多种ML预测器进行了MIP表述，特别是人工神经网络（ANNs），因为它们在许多应用中具有极高的 интерес。然而，ANNs经常具有很多参数，导致MIP表述变得不实现，从而降低了扩展性。事实上，ML社区已经开发出了许多技术来减少ANNs中参数的数量，以避免降低性能。在这篇论文中，我们展示了采用剪枝（pruning）这一技术可以在ANNs之前进行剪枝，从而实现显著提高解决速度的效果。我们解释了为什么剪枝在这个上下文中比其他ML压缩技术更适用，并标识了最佳剪枝策略。为了强调这种方法的潜力，我们通过使用多层感知网络构建了反对抗例。我们的结果表明，剪枝可以很有效地减少解决时间，而无需妨碍最终决策的质量，从而解决了之前不可解决的实例。
</details></li>
</ul>
<hr>
<h2 id="Generative-adversarial-networks-for-data-scarce-spectral-applications"><a href="#Generative-adversarial-networks-for-data-scarce-spectral-applications" class="headerlink" title="Generative adversarial networks for data-scarce spectral applications"></a>Generative adversarial networks for data-scarce spectral applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07454">http://arxiv.org/abs/2307.07454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan José García-Esteban, Juan Carlos Cuevas, Jorge Bravo-Abad</li>
<li>for: 本研究应用生成数学智慧（GANs）在科学领域中，解决不同科学 context 中的数据短缺问题。</li>
<li>methods: 本研究使用了 Wasserstein GANs (WGANs) 和条件 WGANs (CWGANs)，并与单元 feed-forward neural network (FFNN) 进行联合使用，以增强模型的性能。</li>
<li>results: 研究发现，使用 CWGAN 进行数据增强，可以提高 FFNN 的表现，特别是在有限数据情况下。此外，CWGAN 可以作为低数据情况下的代理模型，表现较好。<details>
<summary>Abstract</summary>
Generative adversarial networks (GANs) are one of the most robust and versatile techniques in the field of generative artificial intelligence. In this work, we report on an application of GANs in the domain of synthetic spectral data generation, offering a solution to the scarcity of data found in various scientific contexts. We demonstrate the proposed approach by applying it to an illustrative problem within the realm of near-field radiative heat transfer involving a multilayered hyperbolic metamaterial. We find that a successful generation of spectral data requires two modifications to conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels for the generated data. We show that a simple feed-forward neural network (FFNN), when augmented with data generated by a CWGAN, enhances significantly its performance under conditions of limited data availability, demonstrating the intrinsic value of CWGAN data augmentation beyond simply providing larger datasets. In addition, we show that CWGANs can act as a surrogate model with improved performance in the low-data regime with respect to simple FFNNs. Overall, this work highlights the potential of generative machine learning algorithms in scientific applications beyond image generation and optimization.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GAN）是生成人工智能领域最为稳健和多样化的技术之一。在这项工作中，我们报告了GAN在spectral数据生成领域的应用，提供了数据缺乏问题的解决方案。我们通过在多层赫普力元件中的近场辐射热传输问题中应用提议方法来示例。我们发现，成功生成spectral数据需要两个修改：（i）引入Wasserstein GANs（WGANs）以避免模式塌溃，以及（ii）使WGANs Conditioned以获取准确的标签 для生成数据。我们表明，在有限数据情况下，一个简单的Feed-Forward Neural Network（FFNN），当其被补充了由CWGAN生成的数据后，显著提高了其性能。此外，我们还示出了CWGAN可以作为低数据情况下的代理模型，其性能比简单的FFNN更高。总的来说，这项工作强调了生成机器学习算法在科学应用之外的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Clustering-in-Data-Streams"><a href="#Differentially-Private-Clustering-in-Data-Streams" class="headerlink" title="Differentially Private Clustering in Data Streams"></a>Differentially Private Clustering in Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07449">http://arxiv.org/abs/2307.07449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Epasto, Tamalika Mukherjee, Peilin Zhong</li>
<li>for: 这个论文关注的问题是如何在流处理中实现隐私保护的分 clustering算法，以满足现实世界中数据隐私的要求。</li>
<li>methods: 这个论文提出了一种基于流处理的差分隐私 clustering算法，使用了流处理模型来处理大规模的数据流。该算法只需一个通过数据流的一次扫描，并且可以在流处理中实现分 clustering。</li>
<li>results: 该论文提出了一种可以实现$(1+\gamma)$-倍增加的差分隐私 clustering算法，使用了流处理模型和差分隐私技术。该算法的空间复杂度为$poly(k,d,\log(T))$,并且可以保证对于任意的$\gamma&gt;0$，扩展系数是$(1+\gamma)$，增加系数是$poly(k,d,\log(T))$.<details>
<summary>Abstract</summary>
The streaming model is an abstraction of computing over massive data streams, which is a popular way of dealing with large-scale modern data analysis. In this model, there is a stream of data points, one after the other. A streaming algorithm is only allowed one pass over the data stream, and the goal is to perform some analysis during the stream while using as small space as possible.   Clustering problems (such as $k$-means and $k$-median) are fundamental unsupervised machine learning primitives, and streaming clustering algorithms have been extensively studied in the past. However, since data privacy becomes a central concern in many real-world applications, non-private clustering algorithms are not applicable in many scenarios.   In this work, we provide the first differentially private streaming algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean data points over a stream with length at most $T$ using $poly(k,d,\log(T))$ space to achieve a {\it constant} multiplicative error and a $poly(k,d,\log(T))$ additive error. In particular, we present a differentially private streaming clustering framework which only requires an offline DP coreset algorithm as a blackbox. By plugging in existing DP coreset results via Ghazi, Kumar, Manurangsi 2020 and Kaplan, Stemmer 2018, we achieve (1) a $(1+\gamma)$-multiplicative approximation with $\tilde{O}_\gamma(poly(k,d,\log(T)))$ space for any $\gamma>0$, and the additive error is $poly(k,d,\log(T))$ or (2) an $O(1)$-multiplicative approximation with $\tilde{O}(k \cdot poly(d,\log(T)))$ space and $poly(k,d,\log(T))$ additive error.   In addition, our algorithmic framework is also differentially private under the continual release setting, i.e., the union of outputs of our algorithms at every timestamp is always differentially private.
</details>
<details>
<summary>摘要</summary>
“流处理模型是大规模数据流处理的抽象，是现代数据分析中受欢迎的方法。在这个模型中，有一串数据点，一个接一个地进行处理。流处理算法只有一次可以访问数据流，目标是在流中进行分析，使用最小的空间。归类问题（如$k$-means和$k$- median）是现代无监督机器学习的基本 primitives，流处理归类算法已经得到了广泛的研究。然而，由于数据隐私问题的关注，非私有的归类算法不适用于许多场景。在这种情况下，我们提供了首个具有常量多元因子错误和$poly(k,d,\log(T))$空间的扩展隐私流处理归类算法。特别是，我们提供了一个具有隐私性的流处理归类框架，只需要一个私有DP核心算法作为黑盒。通过插入现有的DP核心结果，我们实现了以下两个目标：1. $(1+\gamma)$-多元因子近似， $\tilde{O}_\gamma(poly(k,d,\log(T)))$ 空间，对于任何 $\gamma>0$。错误是 $poly(k,d,\log(T))$。2. $O(1)$-多元因子近似， $\tilde{O}(k \cdot poly(d,\log(T)))$ 空间，错误是 $poly(k,d,\log(T))$。此外，我们的算法框架还是隐私的，即将流处理算法的输出集合在每个时间戳都是隐私的。”
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Empower-Molecular-Property-Prediction"><a href="#Can-Large-Language-Models-Empower-Molecular-Property-Prediction" class="headerlink" title="Can Large Language Models Empower Molecular Property Prediction?"></a>Can Large Language Models Empower Molecular Property Prediction?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07443">http://arxiv.org/abs/2307.07443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chnq/llm4mol">https://github.com/chnq/llm4mol</a></li>
<li>paper_authors: Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu</li>
<li>for: 本研究旨在利用大型自然语言模型（LLM）提高分子物理性能预测。</li>
<li>methods: 本研究采用两个视角：零&#x2F;几次分子类型化和使用LLM生成的新解释作为分子表示。</li>
<li>results: 实验结果表明，使用文本解释作为分子表示可以在多个benchmark数据集上实现优越性，并证明LLM在分子物理性能预测任务中具有极大的潜力。<details>
<summary>Abstract</summary>
Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at \url{https://github.com/ChnQ/LLM4Mol}.
</details>
<details>
<summary>摘要</summary>
молекулярная свойство предсказание 已经吸引了广泛关注，因为它在多种科学领域中可能产生很大的转变。通常，分子图可以表示为图structured data或SMILES文本。在最近几年，大型自然语言模型（LLMs）的快速发展已经革命化了自然语言处理（NLP）领域。虽然可以使用LLMs来帮助理解表示分子的SMILES，但是研究如何使用LLMs进行分子性质预测的阶段还处于早期。在这种工作中，我们通过两个视角提前这个目标：零/几个分子类别和使用LLMs生成的新解释来代表分子。具体来说，我们首先请求LLMs在上下文中进行分子分类，并评估其表现。然后，我们使用LLMs生成semantically Rich explanation for the original SMILES，并使用这些解释来精细调整一个小规模LM模型 для多个下游任务。实验结果表明文本解释作为分子表示的超越多个benchmark dataset，并证明LLMs在分子性质预测任务中的极大潜力。代码可以在 \url{https://github.com/ChnQ/LLM4Mol} 中找到。
</details></li>
</ul>
<hr>
<h2 id="Atlas-Based-Interpretable-Age-Prediction"><a href="#Atlas-Based-Interpretable-Age-Prediction" class="headerlink" title="Atlas-Based Interpretable Age Prediction"></a>Atlas-Based Interpretable Age Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07439">http://arxiv.org/abs/2307.07439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie Starck, Yadunandan Vivekanand Kini, Jessica Johanna Maria Ritter, Rickmer Braren, Daniel Rueckert, Tamara Mueller</li>
<li>for: 该研究旨在提高医学评估和研究中的年龄预测精度，以检测疾病和异常年龄衰老。</li>
<li>methods: 该研究使用整体身体图像进行研究，并使用Grad-CAM解释方法确定人体不同部位对年龄预测的影响。通过注册技术生成人口范围内的解释地图，以扩展分析范围。</li>
<li>results: 研究发现三个主要预测年龄关键部位：脊梁、自生背肌和心脏区，其中心脏区的重要性最高。该模型在整体身体图像上实现了state-of-the-art的年龄预测精度，年龄差异平均值为2.76年。<details>
<summary>Abstract</summary>
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
</details>
<details>
<summary>摘要</summary>
预测年龄是医学评估和研究中的一个重要部分。它可以帮助检测疾病以及异常年龄的趋势，并且通过显示生物年龄与 cronological age 之间的差异来提供有价值的信息。为了更全面地了解不同部位的年龄相关变化，我们使用整体图像进行研究。我们使用 Grad-CAM 可读性方法来确定人体各部位最有predictive value的地方。此外，我们还使用注册技术来生成全 популяцион的可读性地图，以扩展我们的分析范围。此外，我们实现了全身年龄预测的state-of-the-art模型，其 сред平均绝对误差为2.76年。我们的发现表明了三个主要领域：脊梁、自生肌肉和心脏区域，这三个领域具有最高的重要性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/15/cs.LG_2023_07_15/" data-id="clot2mhe700mex7889yha66wt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/15/eess.IV_2023_07_15/" class="article-date">
  <time datetime="2023-07-15T09:00:00.000Z" itemprop="datePublished">2023-07-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/15/eess.IV_2023_07_15/">eess.IV - 2023-07-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HQG-Net-Unpaired-Medical-Image-Enhancement-with-High-Quality-Guidance"><a href="#HQG-Net-Unpaired-Medical-Image-Enhancement-with-High-Quality-Guidance" class="headerlink" title="HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance"></a>HQG-Net: Unpaired Medical Image Enhancement with High-Quality Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07829">http://arxiv.org/abs/2307.07829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunming He, Kai Li, Guoxia Xu, Jiangpeng Yan, Longxiang Tang, Yulun Zhang, Xiu Li, Yaowei Wang</li>
<li>for: 本研究旨在将低品质医疗影像（LQ）转换为高品质医疗影像（HQ），不需要靠据双对边的图像进行训练。</li>
<li>methods: 我们提出了一种新的UMIE方法，通过直接将高品质图像的特征（例如特征提取）转换为低品质图像的增强过程中，以确保增强过程中的资讯是高品质图像的资讯。</li>
<li>results: 我们的方法在三个医疗图像 dataset 上进行实验，比较了旧有的方法，结果显示我们的方法可以提高增强效果和下游任务的表现。<details>
<summary>Abstract</summary>
Unpaired Medical Image Enhancement (UMIE) aims to transform a low-quality (LQ) medical image into a high-quality (HQ) one without relying on paired images for training. While most existing approaches are based on Pix2Pix/CycleGAN and are effective to some extent, they fail to explicitly use HQ information to guide the enhancement process, which can lead to undesired artifacts and structural distortions. In this paper, we propose a novel UMIE approach that avoids the above limitation of existing methods by directly encoding HQ cues into the LQ enhancement process in a variational fashion and thus model the UMIE task under the joint distribution between the LQ and HQ domains. Specifically, we extract features from an HQ image and explicitly insert the features, which are expected to encode HQ cues, into the enhancement network to guide the LQ enhancement with the variational normalization module. We train the enhancement network adversarially with a discriminator to ensure the generated HQ image falls into the HQ domain. We further propose a content-aware loss to guide the enhancement process with wavelet-based pixel-level and multi-encoder-based feature-level constraints. Additionally, as a key motivation for performing image enhancement is to make the enhanced images serve better for downstream tasks, we propose a bi-level learning scheme to optimize the UMIE task and downstream tasks cooperatively, helping generate HQ images both visually appealing and favorable for downstream tasks. Experiments on three medical datasets, including two newly collected datasets, verify that the proposed method outperforms existing techniques in terms of both enhancement quality and downstream task performance. We will make the code and the newly collected datasets publicly available for community study.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate(umie)Unsupervised Medical Image Enhancement (UMIE) 的目标是将低质量（LQ）医疗图像转化为高质量（HQ）图像，而不依赖于对训练图像的对应。现有的方法多数基于 Pix2Pix/CycleGAN，虽然有一定的效果，但是它们不会直接使用 HQ 信息来导引增强过程，这可能会导致不 DESIRED 的artefacts 和结构扭曲。在这篇论文中，我们提出了一种新的 UMIE 方法，通过直接在 LQ 增强过程中编码 HQ 信息来避免上述 limitation。 Specifically，我们从 HQ 图像中提取特征，并将这些特征直接插入增强网络中，以帮助 LQ 图像增强。我们使用变量 норmalization 模块来确保生成的 HQ 图像处于 HQ 领域内。我们还提出了一种内容相关的损失函数，用于引导增强过程，并且使用 wavelet 基于像素级和多个 encoder 基于特征级的约束来限制增强过程。此外，作为增强图像的主要目的是为了使其更适合下游任务，我们提出了一种两级学习方案，通过将 UMIE 任务和下游任务相互协同学习，以生成高质量的图像，同时也能够满足下游任务的需求。实验结果表明，提出的方法在三个医疗数据集上都有较高的增强质量和下游任务性能。我们将代码和新收集的数据集公开发布，以便社区进行研究。Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="MUVF-YOLOX-A-Multi-modal-Ultrasound-Video-Fusion-Network-for-Renal-Tumor-Diagnosis"><a href="#MUVF-YOLOX-A-Multi-modal-Ultrasound-Video-Fusion-Network-for-Renal-Tumor-Diagnosis" class="headerlink" title="MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis"></a>MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07807">http://arxiv.org/abs/2307.07807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeunyuli/muaf">https://github.com/jeunyuli/muaf</a></li>
<li>paper_authors: Junyu Li, Han Huang, Dong Ni, Wufeng Xue, Dongmei Zhu, Jun Cheng<br>for:这个论文的目的是检测和分类肾脏癌，以提高患者存活率。methods:这个论文使用了多模态超声影像视频融合网络，将B模式和CEUS模式超声影像视频融合到一起，以提高肾脏癌诊断的准确性。results:实验结果表明，提案的框架在多中心数据集上表现出色，超过单模态模型和竞争方法。此外，我们的OTA模块在分类任务中获得了更高的准确率。代码可以在GitHub上获取：<a target="_blank" rel="noopener" href="https://github.com/JeunyuLi/MUAF%E3%80%82">https://github.com/JeunyuLi/MUAF。</a><details>
<summary>Abstract</summary>
Early diagnosis of renal cancer can greatly improve the survival rate of patients. Contrast-enhanced ultrasound (CEUS) is a cost-effective and non-invasive imaging technique and has become more and more frequently used for renal tumor diagnosis. However, the classification of benign and malignant renal tumors can still be very challenging due to the highly heterogeneous appearance of cancer and imaging artifacts. Our aim is to detect and classify renal tumors by integrating B-mode and CEUS-mode ultrasound videos. To this end, we propose a novel multi-modal ultrasound video fusion network that can effectively perform multi-modal feature fusion and video classification for renal tumor diagnosis. The attention-based multi-modal fusion module uses cross-attention and self-attention to extract modality-invariant features and modality-specific features in parallel. In addition, we design an object-level temporal aggregation (OTA) module that can automatically filter low-quality features and efficiently integrate temporal information from multiple frames to improve the accuracy of tumor diagnosis. Experimental results on a multicenter dataset show that the proposed framework outperforms the single-modal models and the competing methods. Furthermore, our OTA module achieves higher classification accuracy than the frame-level predictions. Our code is available at \url{https://github.com/JeunyuLi/MUAF}.
</details>
<details>
<summary>摘要</summary>
早期诊断reno肿瘤可以大大提高患者存活率。对比增强超声（CEUS）是一种 Cost-effective 和非侵入的成像技术，在reno肿瘤诊断中越来越常用。然而，分类benign和malignantreno肿瘤仍然是非常困难的，这是因为肿瘤的高度多样性和成像 artifacts。我们的目标是通过 integrate B-mode和CEUS-mode超声视频来检测和分类reno肿瘤。为此，我们提出了一种 novel 多模态超声视频融合网络，可以有效地执行多模态特征融合和视频分类。我们的注意力基于多模态融合模块使用 Cross-attention 和自注意力来提取模式不变特征和模式特征。此外，我们设计了一个 object-level temporal aggregation（OTA）模块，可以自动筛选低质量特征并有效地集成多帧中的时间信息，以提高肿瘤诊断的准确性。实验结果表明，我们提出的框架在多中心数据集上超过单模态模型和竞争方法。此外，我们的 OTA 模块在 Frame-level 预测中实现了更高的分类精度。我们的代码可以在 <https://github.com/JeunyuLi/MUAF> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Theoretical-Analysis-of-Binary-Masks-in-Snapshot-Compressive-Imaging-Systems"><a href="#Theoretical-Analysis-of-Binary-Masks-in-Snapshot-Compressive-Imaging-Systems" class="headerlink" title="Theoretical Analysis of Binary Masks in Snapshot Compressive Imaging Systems"></a>Theoretical Analysis of Binary Masks in Snapshot Compressive Imaging Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07796">http://arxiv.org/abs/2307.07796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyu Zhao, Shirin Jalali</li>
<li>for: 这 paper 主要研究了 binary 面Mask 在 compressive imaging 系统中的影响。</li>
<li>methods: 该 paper 使用了 teoretic 分析方法来 investigate  binary 面Mask 的影响。</li>
<li>results: 研究发现，最佳的 binary 面Mask 的概率非零元素小于 0.5，这提供了设计和优化 binary 面Mask 的 valuable 信息。<details>
<summary>Abstract</summary>
Snapshot compressive imaging (SCI) systems have gained significant attention in recent years. While previous theoretical studies have primarily focused on the performance analysis of Gaussian masks, practical SCI systems often employ binary-valued masks. Furthermore, recent research has demonstrated that optimized binary masks can significantly enhance system performance. In this paper, we present a comprehensive theoretical characterization of binary masks and their impact on SCI system performance. Initially, we investigate the scenario where the masks are binary and independently identically distributed (iid), revealing a noteworthy finding that aligns with prior numerical results. Specifically, we show that the optimal probability of non-zero elements in the masks is smaller than 0.5. This result provides valuable insights into the design and optimization of binary masks for SCI systems, facilitating further advancements in the field. Additionally, we extend our analysis to characterize the performance of SCI systems where the mask entries are not independent but are generated based on a stationary first-order Markov process. Overall, our theoretical framework offers a comprehensive understanding of the performance implications associated with binary masks in SCI systems.
</details>
<details>
<summary>摘要</summary>
快照压缩成像（SCI）系统在最近几年内获得了广泛关注。而在理论研究中，既前面的研究主要集中在 Gaussian 面积上的性能分析，实际的 SCI 系统却常常使用二值面积。此外，最近的研究表明，优化的二值面积可以显著提高系统性能。在这篇论文中，我们提供了 SCi 系统中二值面积的完整理论Characterization，并对其对系统性能的影响进行了深入分析。首先，我们研究了面积为二值独立相同分布（iid）的情况，发现一个值得注意的结论，与之前的数值结果相符。具体来说，我们证明了最佳非零元素概率在面积中小于 0.5。这个结论为 SCi 系统中 binary 面积的设计和优化提供了有价值的准则，推动了领域的进一步发展。此外，我们将分析推广到 SCi 系统中面积条件不独立，而是基于一个站立的首阶Markov 过程生成的情况。总的来说，我们的理论框架为 SCi 系统中二值面积的性能影响提供了全面的理解。
</details></li>
</ul>
<hr>
<h2 id="Tightly-Coupled-LiDAR-Visual-SLAM-Based-on-Geometric-Features-for-Mobile-Agents"><a href="#Tightly-Coupled-LiDAR-Visual-SLAM-Based-on-Geometric-Features-for-Mobile-Agents" class="headerlink" title="Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents"></a>Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07763">http://arxiv.org/abs/2307.07763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Cao, Ruiping Liu, Ze Wang, Kunyu Peng, Jiaming Zhang, Junwei Zheng, Zhifeng Teng, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 提供自主导航和任务执行的基础，帮助移动机器人在复杂和未知环境中运行。</li>
<li>methods: 利用光梯雷达-视觉SLAM基于几何特征，包括两个子系统（光梯雷达和单目视觉SLAM）以及一个融合框架。融合框架将深度和 semantic 特征相关，以补做视觉线底标记，并在Bundle Adjustment 中添加方向优化。</li>
<li>results: 在公共数据集 M2DGR 上进行评估，与当前状态的多模式方法相比，我们的系统实现了更高精度和更加稳定的姿态估计。<details>
<summary>Abstract</summary>
The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to provide autonomous navigation and task execution in complex and unknown environments. However, it is hard to develop a dedicated algorithm for mobile robots due to dynamic and challenging situations, such as poor lighting conditions and motion blur. To tackle this issue, we propose a tightly-coupled LiDAR-visual SLAM based on geometric features, which includes two sub-systems (LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework associates the depth and semantics of the multi-modal geometric features to complement the visual line landmarks and to add direction optimization in Bundle Adjustment (BA). This further constrains visual odometry. On the other hand, the entire line segment detected by the visual subsystem overcomes the limitation of the LiDAR subsystem, which can only perform the local calculation for geometric features. It adjusts the direction of linear feature points and filters out outliers, leading to a higher accurate odometry system. Finally, we employ a module to detect the subsystem's operation, providing the LiDAR subsystem's output as a complementary trajectory to our system while visual subsystem tracking fails. The evaluation results on the public dataset M2DGR, gathered from ground robots across various indoor and outdoor scenarios, show that our system achieves more accurate and robust pose estimation compared to current state-of-the-art multi-modal methods.
</details>
<details>
<summary>摘要</summary>
Mobile robot靠SLAM（同时地址和地图生成）提供自主导航和任务执行在复杂和未知环境中。然而，为手动机器人开发专门的算法很难，因为有动态和挑战性的情况，如亮度不足和运动模糊。为解决这个问题，我们提议一种紧密相互关联的LiDAR-视觉SLAM，包括两个子系统（LiDAR和单目视觉SLAM）和一个融合框架。融合框架将LiDAR和视觉的多模态几何特征相关，以增强视觉线坐标的精度和 semantics，并在Bundle Adjustment（BA）中添加方向优化。这会进一步约束视觉odoometry。另一方面，视觉子系统检测到的整条视觉线将LiDAR子系统的局部计算限制，并且可以调整视觉线的方向和过滤异常值，从而实现更高精度的odoometry系统。最后，我们采用一个模块来检测子系统的操作，提供LiDAR子系统的补做轨迹，而视觉子系统tracking失败时。根据M2DGR公共数据集，评估结果显示，我们的系统在多模态方法中实现了更高精度和robust的pose估计。
</details></li>
</ul>
<hr>
<h2 id="Open-Scene-Understanding-Grounded-Situation-Recognition-Meets-Segment-Anything-for-Helping-People-with-Visual-Impairments"><a href="#Open-Scene-Understanding-Grounded-Situation-Recognition-Meets-Segment-Anything-for-Helping-People-with-Visual-Impairments" class="headerlink" title="Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments"></a>Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07757">http://arxiv.org/abs/2307.07757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ruipingl/opensu">https://github.com/ruipingl/opensu</a></li>
<li>paper_authors: Ruiping Liu, Jiaming Zhang, Kunyu Peng, Junwei Zheng, Ke Cao, Yufan Chen, Kailun Yang, Rainer Stiefelhagen</li>
<li>for: 协助人们 WITH 视觉障碍（PVI）获得精确的场景理解和独立移动。</li>
<li>methods: 使用 Grounded Situation Recognition (GSR) 技术，并将其扩展为 Open Scene Understanding (OpenSU) 系统，包括实现 pixel-wise dense segmentation masks 以及增强特征提取和Encoder-Decoder 结构之间的互动。</li>
<li>results: 在 SWiG 数据集上取得了最佳性能，并在实际应用中显示了对 PVI 人群的独立移动能力提高。<details>
<summary>Abstract</summary>
Grounded Situation Recognition (GSR) is capable of recognizing and interpreting visual scenes in a contextually intuitive way, yielding salient activities (verbs) and the involved entities (roles) depicted in images. In this work, we focus on the application of GSR in assisting people with visual impairments (PVI). However, precise localization information of detected objects is often required to navigate their surroundings confidently and make informed decisions. For the first time, we propose an Open Scene Understanding (OpenSU) system that aims to generate pixel-wise dense segmentation masks of involved entities instead of bounding boxes. Specifically, we build our OpenSU system on top of GSR by additionally adopting an efficient Segment Anything Model (SAM). Furthermore, to enhance the feature extraction and interaction between the encoder-decoder structure, we construct our OpenSU system using a solid pure transformer backbone to improve the performance of GSR. In order to accelerate the convergence, we replace all the activation functions within the GSR decoders with GELU, thereby reducing the training duration. In quantitative analysis, our model achieves state-of-the-art performance on the SWiG dataset. Moreover, through field testing on dedicated assistive technology datasets and application demonstrations, the proposed OpenSU system can be used to enhance scene understanding and facilitate the independent mobility of people with visual impairments. Our code will be available at https://github.com/RuipingL/OpenSU.
</details>
<details>
<summary>摘要</summary>
“固定场景认知（GSR）能够理解和解释视觉场景，生成出明确的活动（动词）和参与者（角色）。在这项工作中，我们关注使用GSR进行辅助视障人群（PVI）。然而，精确的本地化信息可以帮助人们自信地导航周围环境和做出 Informed 决策。为了实现这一目标，我们第一次提出了一个开放场景理解（OpenSU）系统，旨在生成像素粒度的精密分割mask，而不是 bounding box。具体来说，我们基于GSR构建了OpenSU系统，并采用高效的Segment Anything Model（SAM）。此外，为了提高Encoder-Decoder结构中的特征提取和交互，我们使用了坚实的纯transformer背景。为了加速训练，我们在GSR解码器中replace所有活动函数，使得训练时间缩短。在量化分析中，我们的模型在SWiG数据集上达到了领先的性能。此外，通过特定辅助技术数据集和应用示例测试，我们的OpenSU系统可以增强场景理解和推动视障人群的独立行动。我们的代码将在https://github.com/RuipingL/OpenSU上公开。”
</details></li>
</ul>
<hr>
<h2 id="ExposureDiffusion-Learning-to-Expose-for-Low-light-Image-Enhancement"><a href="#ExposureDiffusion-Learning-to-Expose-for-Low-light-Image-Enhancement" class="headerlink" title="ExposureDiffusion: Learning to Expose for Low-light Image Enhancement"></a>ExposureDiffusion: Learning to Expose for Low-light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07710">http://arxiv.org/abs/2307.07710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyf0912/ExposureDiffusion">https://github.com/wyf0912/ExposureDiffusion</a></li>
<li>paper_authors: Yufei Wang, Yi Yu, Wenhan Yang, Lanqing Guo, Lap-Pui Chau, Alex C. Kot, Bihan Wen</li>
<li>for: 提高图像的亮度和细节，并且可以处理不同的扬光率和噪声模型</li>
<li>methods: 结合扩散模型和物理基础模型，实现从噪声图像直接 restore 到正常曝光图像，而不需要先处理噪声</li>
<li>results: 比传统扩散模型具有更好的性能和更快的执行时间，并且可以与不同的背景网络和实际对照数据集一起使用<details>
<summary>Abstract</summary>
Previous raw image-based low-light image enhancement methods predominantly relied on feed-forward neural networks to learn deterministic mappings from low-light to normally-exposed images. However, they failed to capture critical distribution information, leading to visually undesirable results. This work addresses the issue by seamlessly integrating a diffusion model with a physics-based exposure model. Different from a vanilla diffusion model that has to perform Gaussian denoising, with the injected physics-based exposure model, our restoration process can directly start from a noisy image instead of pure noise. As such, our method obtains significantly improved performance and reduced inference time compared with vanilla diffusion models. To make full use of the advantages of different intermediate steps, we further propose an adaptive residual layer that effectively screens out the side-effect in the iterative refinement when the intermediate results have been already well-exposed. The proposed framework can work with both real-paired datasets, SOTA noise models, and different backbone networks. Note that, the proposed framework is compatible with real-paired datasets, real/synthetic noise models, and different backbone networks. We evaluate the proposed method on various public benchmarks, achieving promising results with consistent improvements using different exposure models and backbones. Besides, the proposed method achieves better generalization capacity for unseen amplifying ratios and better performance than a larger feedforward neural model when few parameters are adopted.
</details>
<details>
<summary>摘要</summary>
以前的Raw图像基于低光照图像增强方法主要采用了Feedforward神经网络来学习确定性的映射从低光照图像到正常曝光图像。然而，它们没有捕捉到重要的分布信息，导致视觉不满意的结果。这个工作解决这个问题，通过将扩散模型与物理基础曝光模型相结合。与普通的扩散模型不同，我们的恢复过程可以直接从噪声图像开始，而不需要纯噪声。因此，我们的方法可以获得显著改进的性能和减少推理时间，相比普通的扩散模型。为了充分利用不同的中间结果的优势，我们还提出了适应性的剩余层，可以有效地排除中间结果的副作用在迭代纠正过程中。我们的框架可以与真实对应的数据集、前进推理模型和不同的背景网络一起工作。注意，我们的框架与真实对应的数据集、真实/生成噪声模型和不同的背景网络兼容。我们在各种公共测试benchmark上评估了我们的方法，实现了优秀的结果，并在不同的曝光模型和背景网络上获得了适应性和性能优势。此外，我们的方法在未见扩大比率上也有更好的总体适应能力和性能优势。
</details></li>
</ul>
<hr>
<h2 id="DRM-IR-Task-Adaptive-Deep-Unfolding-Network-for-All-In-One-Image-Restoration"><a href="#DRM-IR-Task-Adaptive-Deep-Unfolding-Network-for-All-In-One-Image-Restoration" class="headerlink" title="DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration"></a>DRM-IR: Task-Adaptive Deep Unfolding Network for All-In-One Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07688">http://arxiv.org/abs/2307.07688</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuanshuoCheng/DRM-IR">https://github.com/YuanshuoCheng/DRM-IR</a></li>
<li>paper_authors: Yuanshuo Cheng, Mingwen Shao, Yecong Wan, Chao Wang, Wangmeng Zuo</li>
<li>for: 这个论文主要针对的是多种噪声和损害的图像修复问题，以实现所有在一个的图像修复方法。</li>
<li>methods: 该论文提出了一种高效的动态参照模型方法（DRM-IR），它包括任务适应型噪声模型和图像修复模型。具体来说，这两个子任务是通过一对参照图像对MAP推断来形式化，并在不断嵌套的方式进行优化。</li>
<li>results: 对多个标准 benchmark 数据集进行了广泛的实验，结果显示，我们的 DRM-IR 方法可以在 All-In-One 图像修复中达到状态的前者。<details>
<summary>Abstract</summary>
Existing All-In-One image restoration (IR) methods usually lack flexible modeling on various types of degradation, thus impeding the restoration performance. To achieve All-In-One IR with higher task dexterity, this work proposes an efficient Dynamic Reference Modeling paradigm (DRM-IR), which consists of task-adaptive degradation modeling and model-based image restoring. Specifically, these two subtasks are formalized as a pair of entangled reference-based maximum a posteriori (MAP) inferences, which are optimized synchronously in an unfolding-based manner. With the two cascaded subtasks, DRM-IR first dynamically models the task-specific degradation based on a reference image pair and further restores the image with the collected degradation statistics. Besides, to bridge the semantic gap between the reference and target degraded images, we further devise a Degradation Prior Transmitter (DPT) that restrains the instance-specific feature differences. DRM-IR explicitly provides superior flexibility for All-in-One IR while being interpretable. Extensive experiments on multiple benchmark datasets show that our DRM-IR achieves state-of-the-art in All-In-One IR.
</details>
<details>
<summary>摘要</summary>
通常的全面修复（IR）方法通常缺乏多种降低的灵活模型化，因此影响了修复性能。为了实现更高的任务敏捷度，这项工作提出了一种高效的动态参照模型（DRM-IR），它包括任务适应型的降低模型和基于模型的图像修复。具体来说，这两个子任务被формализова为一对推理最大 posteriori（MAP）推理，它们在一个层次结构中被同步优化。通过这两个相顺序的子任务，DRM-IR首先在参照图像对的基础上动态模型任务特定的降低，然后使用收集的降低统计来修复图像。此外，为了跨越参照图像和目标降低图像之间的semantic gap，我们还开发了一种降低先验（DPT），它限制了特定的特征差异。DRM-IR通过显式提供多种降低类型的灵活性，而且可读性高。广泛的实验表明，我们的DRM-IR在全面修复中实现了state-of-the-art。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/15/eess.IV_2023_07_15/" data-id="clot2mhk90143x788be6e22wt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/14/cs.SD_2023_07_14/" class="article-date">
  <time datetime="2023-07-14T15:00:00.000Z" itemprop="datePublished">2023-07-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/14/cs.SD_2023_07_14/">cs.SD - 2023-07-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-dialect-inclusive-recognition-in-a-low-resource-language-are-balanced-corpora-the-answer"><a href="#Towards-dialect-inclusive-recognition-in-a-low-resource-language-are-balanced-corpora-the-answer" class="headerlink" title="Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?"></a>Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07295">http://arxiv.org/abs/2307.07295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Lonergan, Mengjie Qian, Neasa Ní Chiaráin, Christer Gobl, Ailbhe Ní Chasaide</li>
<li>for: 本研究旨在描述如何使语音识别系统在不同 диалект的语言中表现准确。</li>
<li>methods: 研究人员使用12个语音识别系统，首先使用基线dialect-balanced训练数据集，然后使用基线数据集中dialect-specific材料的修改版本。</li>
<li>results: 结果显示，dialect-balanced数据集不会在不同的 диалект中产生相同的表现， UlDIialeкти consistently underperforms，而 Mu диалект则具有最低的wer。Co和Mu диалект之间存在密切的关系，但这种关系不是对称的。这些结果将导向未来的数据集收集和系统建立策略，以优化在不同 диаLECT中的表现准确性。<details>
<summary>Abstract</summary>
ASR systems are generally built for the spoken 'standard', and their performance declines for non-standard dialects/varieties. This is a problem for a language like Irish, where there is no single spoken standard, but rather three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a diagnostic to quantify the effect of the speaker's dialect on recognition performance, 12 ASR systems were trained, firstly using baseline dialect-balanced training corpora, and then using modified versions of the baseline corpora, where dialect-specific materials were either subtracted or added. Results indicate that dialect-balanced corpora do not yield a similar performance across the dialects: the Ul dialect consistently underperforms, whereas Mu yields lowest WERs. There is a close relationship between Co and Mu dialects, but one that is not symmetrical. These results will guide future corpus collection and system building strategies to optimise for cross-dialect performance equity.
</details>
<details>
<summary>摘要</summary>
听说系统通常是为口语标准建立的，其表现在非标准方言下降。这是一个问题，因为如爱尔兰语言中没有单一的口语标准，而是有三大方言： Ulster（Ul）、Connacht（Co）和Munster（Mu）。为了评估说话人的方言对识别表现的影响，12个听说系统在基础的方言均衡训练集上进行了训练，然后使用基础集的修改版本，其中方言特有的材料被 subtracted 或 added。结果表明，不同方言的表现不具有相似性：Ul方言一直表现不佳，而 Mu 方言具有最低 WERs。Co 和 Mu 方言之间存在密切的关系，但这种关系不是对称的。这些结果将导引未来的资料采集和系统建设策略，以优化在不同方言之间的表现 equity。
</details></li>
</ul>
<hr>
<h2 id="Replay-to-Remember-Continual-Layer-Specific-Fine-tuning-for-German-Speech-Recognition"><a href="#Replay-to-Remember-Continual-Layer-Specific-Fine-tuning-for-German-Speech-Recognition" class="headerlink" title="Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition"></a>Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07280">http://arxiv.org/abs/2307.07280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theresa Pekarek Rosin, Stefan Wermter</li>
<li>for: 这 paper 是为了研究大规模自动语音识别（ASR）模型在更小的频谱上的表现和稳定性。</li>
<li>methods: 作者使用了大规模 multilingual 模型，通过选择性冻结部分模型参数进行适应更小的频谱，并应用经验回放来实现 kontinual learning。</li>
<li>results: 研究发现，通过添加原频谱的一部分数据，可以在新频谱上达到 Word-Error-Rates（WER）低于5%，同时稳定总语音识别性能。<details>
<summary>Abstract</summary>
While Automatic Speech Recognition (ASR) models have shown significant advances with the introduction of unsupervised or self-supervised training techniques, these improvements are still only limited to a subsection of languages and speakers. Transfer learning enables the adaptation of large-scale multilingual models to not only low-resource languages but also to more specific speaker groups. However, fine-tuning on data from new domains is usually accompanied by a decrease in performance on the original domain. Therefore, in our experiments, we examine how well the performance of large-scale ASR models can be approximated for smaller domains, with our own dataset of German Senior Voice Commands (SVC-de), and how much of the general speech recognition performance can be preserved by selectively freezing parts of the model during training. To further increase the robustness of the ASR model to vocabulary and speakers outside of the fine-tuned domain, we apply Experience Replay for continual learning. By adding only a fraction of data from the original domain, we are able to reach Word-Error-Rates (WERs) below 5\% on the new domain, while stabilizing performance for general speech recognition at acceptable WERs.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）模型在无监督或自监督训练技术的引入后已经表现出了显著的进步，但这些进步仅限于一些语言和发音人群。传输学习可以使大规模多语言模型适应不仅低资源语言，还可以适应更特定的发音人群。然而，在新领域数据进行精细调整通常会导致原领域性能下降。因此，我们在实验中检查了大规模ASR模型在更小的领域上的表现如何，以及如何在 selectively 冻结模型部分 During training 中保持一定的总体语音识别性能。进一步增加ASR模型对词汇和发音人群外的Robustness，我们应用经验回放 для持续学习。只添加原领域数据的一小部分，我们可以在新领域下达 Word-Error-Rates（WER）低于5%，而同时稳定总体语音识别性能。
</details></li>
</ul>
<hr>
<h2 id="AudioInceptionNeXt-TCL-AI-LAB-Submission-to-EPIC-SOUND-Audio-Based-Interaction-Recognition-Challenge-2023"><a href="#AudioInceptionNeXt-TCL-AI-LAB-Submission-to-EPIC-SOUND-Audio-Based-Interaction-Recognition-Challenge-2023" class="headerlink" title="AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023"></a>AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07265">http://arxiv.org/abs/2307.07265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenlauhkhk/audioinceptionnext">https://github.com/stevenlauhkhk/audioinceptionnext</a></li>
<li>paper_authors: Kin Wai Lau, Yasar Abbas Ur Rehman, Yuyang Xie, Lan Ma</li>
<li>for: 本研究旨在提出一种用于2023年 Epic-Kitchen EPIC-SOUNDS音频基于互动识别挑战的提交。目标是学习音频样本与其相应的动作标签之间的映射。</li>
<li>methods: 我们提出了一种简单 yet effective的单流Convolutional Neural Network（CNN）架构 AudioInceptionNeXt，该架构在时域-频谱增幅log-mel-spectrogram上运行。以启发了InceptionNeXt的设计为基础，我们提议在AudioInceptionNeXt块中使用平行多缘分割卷积kernel，这使得模型更好地学习时间和频率信息。</li>
<li>results: 我们的方法在挑战测试集上实现55.43%的top-1准确率，在公共领先榜上排名第一。代码可以在<a target="_blank" rel="noopener" href="https://github.com/StevenLauHKHK/AudioInceptionNeXt.git%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/StevenLauHKHK/AudioInceptionNeXt.git上获取。</a><details>
<summary>Abstract</summary>
This report presents the technical details of our submission to the 2023 Epic-Kitchen EPIC-SOUNDS Audio-Based Interaction Recognition Challenge. The task is to learn the mapping from audio samples to their corresponding action labels. To achieve this goal, we propose a simple yet effective single-stream CNN-based architecture called AudioInceptionNeXt that operates on the time-frequency log-mel-spectrogram of the audio samples. Motivated by the design of the InceptionNeXt, we propose parallel multi-scale depthwise separable convolutional kernels in the AudioInceptionNeXt block, which enable the model to learn the time and frequency information more effectively. The large-scale separable kernels capture the long duration of activities and the global frequency semantic information, while the small-scale separable kernels capture the short duration of activities and local details of frequency information. Our approach achieved 55.43% of top-1 accuracy on the challenge test set, ranked as 1st on the public leaderboard. Codes are available anonymously at https://github.com/StevenLauHKHK/AudioInceptionNeXt.git.
</details>
<details>
<summary>摘要</summary>
这份报告介绍我们在2023年 Epic-Kitchen EPIC-SOUNDS 音频基于交互认知挑战中的技术细节。任务是学习音频示例与其对应的动作标签之间的映射。为了实现这个目标，我们提议一种简单 yet 高效的单流 CNN 建 architecture  AudioInceptionNeXt，该架构在时域-频谱响应的 Log-Mel спектрограм中运行。受 InceptionNeXt 的设计启发，我们提议在 AudioInceptionNeXt 块中使用并行多级分割 convolutional 核，这些核 enable 模型更好地学习时间和频谱信息。大规模分割核捕捉活动的长时间和全局频谱 semantic 信息，而小规模分割核捕捉活动的短时间和局部频谱信息。我们的方法在挑战测试集上达到了 55.43% 的 top-1 精度，排名公共排行板上第一名。代码可以在 https://github.com/StevenLauHKHK/AudioInceptionNeXt.git 上anonymous 获取。
</details></li>
</ul>
<hr>
<h2 id="Mega-TTS-2-Zero-Shot-Text-to-Speech-with-Arbitrary-Length-Speech-Prompts"><a href="#Mega-TTS-2-Zero-Shot-Text-to-Speech-with-Arbitrary-Length-Speech-Prompts" class="headerlink" title="Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts"></a>Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07218">http://arxiv.org/abs/2307.07218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao</li>
<li>for: 可以Synthesize unseen speaker的speech with arbitrary-length prompts</li>
<li>methods: 使用 multi-reference timbre encoder和prosody language model，并 introduce arbitrary-source prompts and phoneme-level auto-regressive duration model</li>
<li>results: 可以 achieve improved performance with longer speech prompts and synthesize identity-preserving speech with a short prompt of an unseen speaker<details>
<summary>Abstract</summary>
Zero-shot text-to-speech aims at synthesizing voices with unseen speech prompts. Previous large-scale multispeaker TTS models have successfully achieved this goal with an enrolled recording within 10 seconds. However, most of them are designed to utilize only short speech prompts. The limited information in short speech prompts significantly hinders the performance of fine-grained identity imitation. In this paper, we introduce Mega-TTS 2, a generic zero-shot multispeaker TTS model that is capable of synthesizing speech for unseen speakers with arbitrary-length prompts. Specifically, we 1) design a multi-reference timbre encoder to extract timbre information from multiple reference speeches; 2) and train a prosody language model with arbitrary-length speech prompts; With these designs, our model is suitable for prompts of different lengths, which extends the upper bound of speech quality for zero-shot text-to-speech. Besides arbitrary-length prompts, we introduce arbitrary-source prompts, which leverages the probabilities derived from multiple P-LLM outputs to produce expressive and controlled prosody. Furthermore, we propose a phoneme-level auto-regressive duration model to introduce in-context learning capabilities to duration modeling. Experiments demonstrate that our method could not only synthesize identity-preserving speech with a short prompt of an unseen speaker but also achieve improved performance with longer speech prompts. Audio samples can be found in https://mega-tts.github.io/mega2_demo/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>零批 Text-to-Speech 目标是synthesize voice with unseen speech prompts。前一代大规模多 speaker TTS 模型已经成功实现了这个目标，但是大多数它们只能使用短的 speech prompts。短 speech prompts 的有限信息使得 fine-grained identity imitation 的性能受到了很大的限制。在这篇论文中，我们介绍 Mega-TTS 2，一种可以 synthesize speech for unseen speakers with arbitrary-length prompts 的通用零批多 speaker TTS 模型。具体来说，我们：1. 设计了多 references timbre encoder，以EXTRACT timbre information from multiple reference speeches。2. 并使用 arbitrary-length speech prompts 进行训练 prosody language model。这些设计使得我们的模型适用于不同的提示长度，从而扩展了 speech quality 的Upper bound for zero-shot text-to-speech。此外，我们还引入了arbitrary-source prompts，这里利用了多个 P-LLM 输出的概率来生成表达性和控制的 prosody。此外，我们还提出了一种phoneme-level auto-regressive duration model，以INTRODUCE in-context learning capabilities to duration modeling。实验表明，我们的方法可以不仅synthesize identity-preserving speech with a short prompt of an unseen speaker，还可以 achieved improved performance with longer speech prompts。Audio samples can be found in <https://mega-tts.github.io/mega2_demo/>.
</details></li>
</ul>
<hr>
<h2 id="Low-Rank-Properties-for-Estimating-Microphones-Start-Time-and-Sources-Emission-Time"><a href="#Low-Rank-Properties-for-Estimating-Microphones-Start-Time-and-Sources-Emission-Time" class="headerlink" title="Low Rank Properties for Estimating Microphones Start Time and Sources Emission Time"></a>Low Rank Properties for Estimating Microphones Start Time and Sources Emission Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07096">http://arxiv.org/abs/2307.07096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faxian Cao, Yongqiang Cheng, Adil Mehmood Khan, Zhijing Yang, S. M. Ahsan Kazmiand Yingxiu Chang</li>
<li>for: 这 paper 是为了解决不精确的时间信息问题，例如麦克风和源localization 而写的。</li>
<li>methods: 这 paper 使用了一种基于low-rank property (LRP)的方法，具体来说是利用LRP的低级结构来形成linear constraint，从而解决UTIm的不确定性问题。</li>
<li>results: 实验结果表明，这 paper 的方法在比较 existed state-of-the-art 方法时表现出了更高的性能， measured 通过Recovery number 和 reduced estimation errors of UTIm。<details>
<summary>Abstract</summary>
Uncertainty in timing information pertaining to the start time of microphone recordings and sources' emission time pose significant challenges in various applications, such as joint microphones and sources localization. Traditional optimization methods, which directly estimate this unknown timing information (UTIm), often fall short compared to approaches exploiting the low-rank property (LRP). LRP encompasses an additional low-rank structure, facilitating a linear constraint on UTIm to help formulate related low-rank structure information. This method allows us to attain globally optimal solutions for UTIm, given proper initialization. However, the initialization process often involves randomness, leading to suboptimal, local minimum values. This paper presents a novel, combined low-rank approximation (CLRA) method designed to mitigate the effects of this random initialization. We introduce three new LRP variants, underpinned by mathematical proof, which allow the UTIm to draw on a richer pool of low-rank structural information. Utilizing this augmented low-rank structural information from both LRP and the proposed variants, we formulate four linear constraints on the UTIm. Employing the proposed CLRA algorithm, we derive global optimal solutions for the UTIm via these four linear constraints.Experimental results highlight the superior performance of our method over existing state-of-the-art approaches, measured in terms of both the recovery number and reduced estimation errors of UTIm.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传感器记录的开始时间和发源时间的不确定性在各种应用中具有重要挑战性，如共同扬声器和发源器localization。传统优化方法，直接估算这些未知时间信息（UTIm），经常与LRP方法相比，表现不足。LRP包含额外的低级结构，使得可以在UTIm中增加直线约束，以帮助形式化相关的低级结构信息。这种方法使得我们可以在初始化过程中获得全球最优解。然而，初始化过程通常含有Randomness，导致获得局部最优解。本文提出了一种新的combined low-rank approximation（CLRA）方法，旨在 mitigate这种随机初始化的影响。我们提出了三种新的LRP变体，基于数学证明，使得UTIm可以借鉴更加丰富的低级结构信息。通过这些增强的低级结构信息，我们将UTIm转化为四个直线约束。采用我们提出的CLRA算法，我们可以从这些四个直线约束中获得全球最优解。实验结果表明，我们的方法在与现有状态的方法相比，具有更好的性能， measured in terms of both the recovery number and reduced estimation errors of UTIm。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Pretrained-ASR-Encoders-for-Effective-and-Efficient-End-to-End-Speech-Intent-Classification-and-Slot-Filling"><a href="#Leveraging-Pretrained-ASR-Encoders-for-Effective-and-Efficient-End-to-End-Speech-Intent-Classification-and-Slot-Filling" class="headerlink" title="Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling"></a>Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07057">http://arxiv.org/abs/2307.07057</a></li>
<li>repo_url: None</li>
<li>paper_authors: He Huang, Jagadeesh Balam, Boris Ginsburg</li>
<li>for: 本研究旨在提出一种使用ASR预训练的encoder初始化一个端到端Conformer-Transformer模型，以实现新的状态对SLURP数据集的Intent分类和槽填充（SICSF）。</li>
<li>methods: 我们提出了一种使用ASR预训练的encoder初始化一个端到端Conformer-Transformer模型，并对SLURP数据集进行训练。我们还对自我学习预训练（SSL）和ASR预训练进行比较，并证明ASR预训练是更有效的。为了探索参数效率，我们冻结encoder并添加Adapter模块，并证明只有ASR预训练的encoder可以保持参数效率。</li>
<li>results: 我们的模型在SLURP数据集上实现了新的状态对Intent分类和槽填充的最佳Result，即90.14% Intent准确率和82.27% SLURP-F1。此外，我们还对端到端模型与分解模型（ASR+NLU）进行了深入比较，并证明端到端模型在参数效率和性能之间具有优势。最后，我们的模型成为了首个实现与分解模型相同性的E2E模型。<details>
<summary>Abstract</summary>
We study speech intent classification and slot filling (SICSF) by proposing to use an encoder pretrained on speech recognition (ASR) to initialize an end-to-end (E2E) Conformer-Transformer model, which achieves the new state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and 82.27% SLURP-F1. We compare our model with encoders pretrained on self-supervised learning (SSL), and show that ASR pretraining is much more effective than SSL for SICSF. To explore parameter efficiency, we freeze the encoder and add Adapter modules, and show that parameter efficiency is only achievable with an ASR-pretrained encoder, while the SSL encoder needs full finetuning to achieve comparable results. In addition, we provide an in-depth comparison on end-to-end models versus cascading models (ASR+NLU), and show that E2E models are better than cascaded models unless an oracle ASR model is provided. Last but not least, our model is the first E2E model that achieves the same performance as cascading models with oracle ASR. Code, checkpoints and configs are available.
</details>
<details>
<summary>摘要</summary>
我们研究了speech意图分类和插槽填充（SICSF），我们提议使用已经预训练的语音识别（ASR）Encoder来初始化一个端到端（E2E）Conformer-Transformer模型，这些模型在SLURP数据集上达到了新的州OF-the-art结果，具有90.14%的意图精度和82.27%的SLURP-F1。我们与self-supervised learning（SSL）预训练器进行比较，并发现ASR预训练是对SICSF的 much more effective than SSL。为了探索参数效率，我们冻结Encoder并添加Adapter模块，并发现只有ASR预训练的Encoder可以保持参数效率，而SSL预训练的Encoder需要全部finetuning才能达到相似的结果。此外，我们还提供了端到端模型与杂合模型（ASR+NLU）的深入比较，并发现E2E模型比杂合模型更好，除非提供了oracle ASR模型。最后，我们的模型是第一个E2E模型，可以与杂合模型具有oracle ASR模型的性能相同。代码、checkpoints和配置都可以获得。
</details></li>
</ul>
<hr>
<h2 id="Adapting-an-ASR-Foundation-Model-for-Spoken-Language-Assessment"><a href="#Adapting-an-ASR-Foundation-Model-for-Spoken-Language-Assessment" class="headerlink" title="Adapting an ASR Foundation Model for Spoken Language Assessment"></a>Adapting an ASR Foundation Model for Spoken Language Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09378">http://arxiv.org/abs/2307.09378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Mengjie Qian, Mark J. F. Gales, Kate M. Knill</li>
<li>for: 本研究旨在改善大规模预训练ASR模型的输出，以提供准确的候选者评估和反馈。</li>
<li>methods: 本文提出了两种解决方案：一是精度地练习，二是软提示调整。两种方法都在公共演讲数据集和英语学习数据集上进行了实验。</li>
<li>results: 实验结果显示，通过精度地练习和软提示调整，可以有效地改变Whisper的解码行为，以生成候选者实际上说的话。<details>
<summary>Abstract</summary>
A crucial part of an accurate and reliable spoken language assessment system is the underlying ASR model. Recently, large-scale pre-trained ASR foundation models such as Whisper have been made available. As the output of these models is designed to be human readable, punctuation is added, numbers are presented in Arabic numeric form and abbreviations are included. Additionally, these models have a tendency to skip disfluencies and hesitations in the output. Though useful for readability, these attributes are not helpful for assessing the ability of a candidate and providing feedback. Here a precise transcription of what a candidate said is needed. In this paper, we give a detailed analysis of Whisper outputs and propose two solutions: fine-tuning and soft prompt tuning. Experiments are conducted on both public speech corpora and an English learner dataset. Results show that we can effectively alter the decoding behaviour of Whisper to generate the exact words spoken in the response.
</details>
<details>
<summary>摘要</summary>
‪《一个精准和可靠的口语评估系统中的关键部分是底层ASR模型。最近，大规模预训练ASR基础模型如Whisper已经被提供。这些模型的输出设计为人类可读，包括括号、阿拉伯数字形式的数字和缩写。然而，这些特征不实用于评估候选人的能力和提供反馈。我们需要准确转录候选人所说的话。在这篇论文中，我们对Whisper输出进行了详细分析，并提出了两种解决方案：细化和软提示调整。我们在公共演讲 corpora 和英语学习 dataset 上进行了实验，结果表明我们可以有效地改变Whisper的解码行为，以产生候选人实际上说的话。‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬‬
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/14/cs.SD_2023_07_14/" data-id="clot2mhgj00ttx78814hr08ff" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/14/eess.AS_2023_07_14/" class="article-date">
  <time datetime="2023-07-14T14:00:00.000Z" itemprop="datePublished">2023-07-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/14/eess.AS_2023_07_14/">eess.AS - 2023-07-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Reproducing-the-Velocity-Vectors-in-the-Listening-Region"><a href="#Reproducing-the-Velocity-Vectors-in-the-Listening-Region" class="headerlink" title="Reproducing the Velocity Vectors in the Listening Region"></a>Reproducing the Velocity Vectors in the Listening Region</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07200">http://arxiv.org/abs/2307.07200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarui Wang, Thushara Abhayapala, Jihui Aimee Zhang, Prasanga Samarasinghe</li>
<li>for: 这个论文是为了实现声场重现算法，特别是在低频下提高声场重现的效果。</li>
<li>methods: 该论文提出了一种基于声速 вектор匹配的声场重现算法，其中使用了声场翻译的概念，从要求压力分布中获取声速 вектор的球面傅里叶约束。可以使用扩散式声麦数组进行测量，并将声速 веctor直接控制在听区域中，以提高声场重现的效果。</li>
<li>results: 该论文预计可以在低频下提高声场重现的效果，因为直接控制声速 веctor在听区域中，可以更好地实现声场的重现。<details>
<summary>Abstract</summary>
This paper proposes a sound field reproduction algorithm based on matching the velocity vectors in a spherical listening region. Using the concept of sound field translation, the spherical harmonic coefficients of the velocity vectors in a spherical region are derived from the desired pressure distribution. The desired pressure distribution can either correspond to sources such as plane waves and point sources, or be obtained from measurements using a spherical microphone array. Unlike previous work in which the velocity vectors are only controlled on the boundary of the listening region or at discrete sweet spots, this work directly manipulates the velocity vectors in the whole listening region, which is expected to improve the perception of the desired sound field at low frequencies.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这篇论文提出了一种基于匀速场匀速场匀速场的声场重建算法。通过声场翻译的概念，这篇论文从需要的压力分布中获得了匀速场中的匀速Vector的圆锥幂系数。需要的压力分布可以来自平面波、点源等源，或者通过圆形 Mikrofon 阵列进行测量。与先前的工作不同，这篇论文直接控制整个听区域内的匀速Vector，而不是仅在听区域边缘或 discrete sweet spots 上控制匀速Vector，这可能会在低频段提高所需的声场的感知。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/14/eess.AS_2023_07_14/" data-id="clot2mhiv010cx7883calgglj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/14/cs.CV_2023_07_14/" class="article-date">
  <time datetime="2023-07-14T13:00:00.000Z" itemprop="datePublished">2023-07-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/14/cs.CV_2023_07_14/">cs.CV - 2023-07-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Combining-multitemporal-optical-and-SAR-data-for-LAI-imputation-with-BiLSTM-network"><a href="#Combining-multitemporal-optical-and-SAR-data-for-LAI-imputation-with-BiLSTM-network" class="headerlink" title="Combining multitemporal optical and SAR data for LAI imputation with BiLSTM network"></a>Combining multitemporal optical and SAR data for LAI imputation with BiLSTM network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07434">http://arxiv.org/abs/2307.07434</a></li>
<li>repo_url: None</li>
<li>paper_authors: W. Zhao, F. Yin, H. Ma, Q. Wu, J. Gomez-Dans, P. Lewis</li>
<li>for: 这个论文是为了提高冬小麦产量预测的基础数据，即叶面指数（LAI）的补充。</li>
<li>methods: 这个论文使用了时序序列Sentinel-1 VH&#x2F;VV的数据，通过bidirectional LSTM（BiLSTM）网络进行LAI补充。</li>
<li>results: 实验结果表明，BiLSTM比传统回归方法更高效，能够捕捉多个时序序列之间的非线性动态。它在不同的生长条件下表现良好，并且可以使用有限的Sentinel-2图像来进行补充。BiLSTM在衰黄期表现特别出色。因此，BiLSTM可以用于时序序列Sentinel-1 VH&#x2F;VV和Sentinel-2数据中的LAI补充，这种方法可以应用于其他时序序列补充问题。<details>
<summary>Abstract</summary>
The Leaf Area Index (LAI) is vital for predicting winter wheat yield. Acquisition of crop conditions via Sentinel-2 remote sensing images can be hindered by persistent clouds, affecting yield predictions. Synthetic Aperture Radar (SAR) provides all-weather imagery, and the ratio between its cross- and co-polarized channels (C-band) shows a high correlation with time series LAI over winter wheat regions. This study evaluates the use of time series Sentinel-1 VH/VV for LAI imputation, aiming to increase spatial-temporal density. We utilize a bidirectional LSTM (BiLSTM) network to impute time series LAI and use half mean squared error for each time step as the loss function. We trained models on data from southern Germany and the North China Plain using only LAI data generated by Sentinel-1 VH/VV and Sentinel-2. Experimental results show BiLSTM outperforms traditional regression methods, capturing nonlinear dynamics between multiple time series. It proves robust in various growing conditions and is effective even with limited Sentinel-2 images. BiLSTM's performance surpasses that of LSTM, particularly over the senescence period. Therefore, BiLSTM can be used to impute LAI with time-series Sentinel-1 VH/VV and Sentinel-2 data, and this method could be applied to other time-series imputation issues.
</details>
<details>
<summary>摘要</summary>
“叶面指数（LAI）是预测冬小麦收获的关键因素。但是，由于可视光Remote Sensing像素可能会受到持续云层遮挡，对收获预测造成影响。Synthetic Aperture Radar（SAR）提供了不受天气阻据的影像，其中横极和衡极通道（C-band）的比率与时间序列LAI之间存在高度的相关性。本研究探讨使用时间序列Sentinel-1 VH/VV来替代LAI数据，以增加空间时间的密度。我们使用了 bidirectional LSTM（BiLSTM）网络来替代时间序列LAI，并使用每个时间步的平均方差作为损失函数。我们使用了德国南部和中国北方平原的数据，只使用Sentinel-1 VH/VV和Sentinel-2的数据来训练模型。实验结果显示，BiLSTM比传统回归方法高效，能够捕捉时间序列之间的非线性动态。它在不同生长条件下表现良好，并且具有限制Sentinel-2像素的可行性。BiLSTM在衰老期表现更好，因此可以用时间序列Sentinel-1 VH/VV和Sentinel-2数据来替代LAI数据，这种方法可以应用于其他时间序列替代问题。”
</details></li>
</ul>
<hr>
<h2 id="Transient-Neural-Radiance-Fields-for-Lidar-View-Synthesis-and-3D-Reconstruction"><a href="#Transient-Neural-Radiance-Fields-for-Lidar-View-Synthesis-and-3D-Reconstruction" class="headerlink" title="Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction"></a>Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09555">http://arxiv.org/abs/2307.09555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anagh Malik, Parsa Mirdehghan, Sotiris Nousias, Kiriakos N. Kutulakos, David B. Lindell</li>
<li>for: 这 paper 的目的是用 NeRFs 模型来描述 Multiview 图像中的场景外观和几何结构，同时使用雷达测量数据作为额外监督。</li>
<li>methods: 这 paper 使用了一种新的时间分辨率版本的卷积渲染方程来渲染雷达测量数据，以捕捉在纳秒级别上的光学传输现象。</li>
<li>results: 作者们在一个具有 simulated 和捕捉的多视图扫描数据的 dataset 上评估了他们的方法，并发现其在缺少输入视图点的情况下比点云基础监督更好地恢复场景的几何和外观。这种方法可能对于自动驾驶、 робо控和远程感知等应用有很大的应用前景。<details>
<summary>Abstract</summary>
Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling scene appearance and geometry from multiview imagery. Recent work has also begun to explore how to use additional supervision from lidar or depth sensor measurements in the NeRF framework. However, previous lidar-supervised NeRFs focus on rendering conventional camera imagery and use lidar-derived point cloud data as auxiliary supervision; thus, they fail to incorporate the underlying image formation model of the lidar. Here, we propose a novel method for rendering transient NeRFs that take as input the raw, time-resolved photon count histograms measured by a single-photon lidar system, and we seek to render such histograms from novel views. Different from conventional NeRFs, the approach relies on a time-resolved version of the volume rendering equation to render the lidar measurements and capture transient light transport phenomena at picosecond timescales. We evaluate our method on a first-of-its-kind dataset of simulated and captured transient multiview scans from a prototype single-photon lidar. Overall, our work brings NeRFs to a new dimension of imaging at transient timescales, newly enabling rendering of transient imagery from novel views. Additionally, we show that our approach recovers improved geometry and conventional appearance compared to point cloud-based supervision when training on few input viewpoints. Transient NeRFs may be especially useful for applications which seek to simulate raw lidar measurements for downstream tasks in autonomous driving, robotics, and remote sensing.
</details>
<details>
<summary>摘要</summary>
《神经辐射场（NeRFs）已成为多视图图像和几何的模型化工具。最近的研究也开始尝试将额外的超级视差从激光或深度测量设备的测量数据加入NeRF框架中。然而，先前的激光超级视差NeRF都是用来渲染传统的摄像头图像，并使用激光测量得到的点云数据作为辅助监督；因此，它们不能充分利用激光测量数据中的图像形成模型。在这里，我们提出了一种新的方法，可以将单 photon激光系统测量的时间分辨率历史曲线作为输入，并尝试从新视角渲染这些历史曲线。与传统的NeRF不同，我们的方法基于时间分辨率版本的卷积渲染方程来渲染激光测量和捕捉时钟秒级的辐射运输现象。我们对一个具有历史扫描和捕捉功能的单 photon激光系统进行了首次实验和捕捉。总的来说，我们的方法可以将NeRF引入新的时间级别的成像，并能够从新视角渲染动态图像。此外，我们还证明了我们的方法在几个输入视图点的监督下可以提取更好的几何和传统的外观特征，比起基于点云的监督来说更好。动态NeRF可能会在自动驾驶、机器人和远程感知等领域中发挥作用。》
</details></li>
</ul>
<hr>
<h2 id="Improving-Zero-Shot-Generalization-for-CLIP-with-Synthesized-Prompts"><a href="#Improving-Zero-Shot-Generalization-for-CLIP-with-Synthesized-Prompts" class="headerlink" title="Improving Zero-Shot Generalization for CLIP with Synthesized Prompts"></a>Improving Zero-Shot Generalization for CLIP with Synthesized Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07397">http://arxiv.org/abs/2307.07397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrflogs/SHIP">https://github.com/mrflogs/SHIP</a></li>
<li>paper_authors: Zhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang, Tieniu Tan</li>
<li>for: 提高现有的 fine-tuning 方法的性能，使其能够更好地适应实际应用中的长尾和Zipf’s law问题。</li>
<li>methods: 提出了一种插件化生成方法 called \textbf{S}ynt\textbf{H}es\textbf{I}zed \textbf{P}rompts~(\textbf{SHIP})，通过变量自动编码器引入生成器，使用文本编码器来重建视觉特征。</li>
<li>results: 通过基本到新的泛化、跨数据集转移学习和总体零shot学习，实验证明了我们的方法的超越性。<details>
<summary>Abstract</summary>
With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed \textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and synthesized features. Extensive experiments on base-to-new generalization, cross-dataset transfer learning, and generalized zero-shot learning demonstrate the superiority of our approach. The code is available at \url{https://github.com/mrflogs/SHIP}.
</details>
<details>
<summary>摘要</summary>
随着CLIP类型预训练语义视觉模型的兴趣增长，当前研究的焦点在于将这些模型适应下渠道任务。虽然实现了有前途的结果，但大多数现有方法需要所有类别的标注数据，这在实际应用中可能不存在，因为Zipf的法则和长尾问题。例如，某些类别可能完全缺乏标注数据，如出现的概念。为解决这个问题，我们提议一种插件式生成方法called \textbf{S}ynt\textbf{H}es\textbf{I}zed \textbf{P}rompts~(\textbf{SHIP})，以改进现有的精度训练方法。具体来说，我们采用变量自动编码器引入一个生成器，通过输入生成的提示和相应的类别名称，将视觉特征重构回CLIP的文本编码器中。这样，我们可以轻松地获得标注-只有的类别的生成特征。然后，我们将CLIP通过市场上可获得的方法进行精度训练，并将标注和生成特征相结合。我们进行了基于新基础上的泛化、跨数据集转移学习和通用零shot学习的广泛实验，结果表明我们的方法有所优势。代码可以在\url{https://github.com/mrflogs/SHIP}中找到。
</details></li>
</ul>
<hr>
<h2 id="L-DAWA-Layer-wise-Divergence-Aware-Weight-Aggregation-in-Federated-Self-Supervised-Visual-Representation-Learning"><a href="#L-DAWA-Layer-wise-Divergence-Aware-Weight-Aggregation-in-Federated-Self-Supervised-Visual-Representation-Learning" class="headerlink" title="L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning"></a>L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07393">http://arxiv.org/abs/2307.07393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasar Abbas Ur Rehman, Yan Gao, Pedro Porto Buarque de Gusmão, Mina Alibeigi, Jiajun Shen, Nicholas D. Lane</li>
<li>for: 提高edge设备上的自适应学习和联合学习的数据隐私保证和模型质量提升</li>
<li>methods: 层分别积分法（Layer-wise Divergence Aware Weight Aggregation，L-DAWA）用于缓解客户端偏见和分化导致的联合学习整合问题</li>
<li>results: 在cross-silo和cross-device设置下，对CIFAR-10&#x2F;100和Tiny ImageNet datasets进行了广泛的实验，并实现了新的SOTA性能在对照和非对照自适应学习方法上<details>
<summary>Abstract</summary>
The ubiquity of camera-enabled devices has led to large amounts of unlabeled image data being produced at the edge. The integration of self-supervised learning (SSL) and federated learning (FL) into one coherent system can potentially offer data privacy guarantees while also advancing the quality and robustness of the learned visual representations without needing to move data around. However, client bias and divergence during FL aggregation caused by data heterogeneity limits the performance of learned visual representations on downstream tasks. In this paper, we propose a new aggregation strategy termed Layer-wise Divergence Aware Weight Aggregation (L-DAWA) to mitigate the influence of client bias and divergence during FL aggregation. The proposed method aggregates weights at the layer-level according to the measure of angular divergence between the clients' model and the global model. Extensive experiments with cross-silo and cross-device settings on CIFAR-10/100 and Tiny ImageNet datasets demonstrate that our methods are effective and obtain new SOTA performance on both contrastive and non-contrastive SSL approaches.
</details>
<details>
<summary>摘要</summary>
随着摄像头设备的普遍使用，生成大量未标注图像数据的情况变得越来越普遍。通过结合自我超级vised learning（SSL）和联合学习（FL），可以实现数据隐私保证，同时提高学习的视觉表示质量和可靠性，不需要将数据传输。然而，客户端偏见和分布导致FL聚合中的客户端偏见和分布限制了下游任务的性能。在本文中，我们提出了一种新的聚合策略，即层次分解差异抑制Weight聚合（L-DAWA），以mitigate客户端偏见和分布的影响。该方法在客户端和全球模型之间进行层次分解，根据客户端模型和全球模型之间的角度差来权衡聚合。我们在跨积silod和跨设备的实验中，使用CIFAR-10/100和Tiny ImageNet datasets，证明了我们的方法的有效性，并实现了新的SOTA性能在对照和非对照SSL方法上。
</details></li>
</ul>
<hr>
<h2 id="Defect-Classification-in-Additive-Manufacturing-Using-CNN-Based-Vision-Processing"><a href="#Defect-Classification-in-Additive-Manufacturing-Using-CNN-Based-Vision-Processing" class="headerlink" title="Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing"></a>Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07378">http://arxiv.org/abs/2307.07378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Liu, Alessandra Mileo, Alan F. Smeaton</li>
<li>for: 这篇论文旨在用计算机视觉和在位MONITORING技术收集Additive manufacturing（AM）过程中的大量数据，并使用机器学习技术提高AM的质量。</li>
<li>methods: 本文使用卷积神经网络（CNNs）准确地分类AM过程中的图像数据中的缺陷，并应用活动学习技术来开发分类模型。</li>
<li>results: 本文通过将人类Loop mechanism integrate到分类模型中，以减少需要用于训练和生成训练数据的数据量。<details>
<summary>Abstract</summary>
The development of computer vision and in-situ monitoring using visual sensors allows the collection of large datasets from the additive manufacturing (AM) process. Such datasets could be used with machine learning techniques to improve the quality of AM. This paper examines two scenarios: first, using convolutional neural networks (CNNs) to accurately classify defects in an image dataset from AM and second, applying active learning techniques to the developed classification model. This allows the construction of a human-in-the-loop mechanism to reduce the size of the data required to train and generate training data.
</details>
<details>
<summary>摘要</summary>
通过计算机视觉和在位MONITORING使用视觉传感器，可以收集Additive Manufacturing（AM）过程中大量的数据集。这些数据集可以使用机器学习技术来提高AM的质量。本文研究了两个场景：首先，使用卷积神经网络（CNNs）准确地分类AM图像集中的缺陷，其次，应用活动学习技术到已经开发的分类模型，以建立人类在Loop机制，以降低需要训练和生成训练数据的大小。
</details></li>
</ul>
<hr>
<h2 id="ConTrack-Contextual-Transformer-for-Device-Tracking-in-X-ray"><a href="#ConTrack-Contextual-Transformer-for-Device-Tracking-in-X-ray" class="headerlink" title="ConTrack: Contextual Transformer for Device Tracking in X-ray"></a>ConTrack: Contextual Transformer for Device Tracking in X-ray</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07541">http://arxiv.org/abs/2307.07541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Demoustier, Yue Zhang, Venkatesh Narasimha Murthy, Florin C. Ghesu, Dorin Comaniciu</li>
<li>for: 这篇研究是设计来解决静脉手术中策略追踪的问题，特别是在心脏手术中，需要精确地探测和追踪干道器的位置。</li>
<li>methods: 这篇研究使用了一个名为ConTrack的当代化网络，该网络使用了空间和时间上下文信息来精确地探测和追踪干道器的位置。</li>
<li>results: 实验结果显示，ConTrack方法在与现有追踪模型比较时，具有45%或更高的准确率。<details>
<summary>Abstract</summary>
Device tracking is an important prerequisite for guidance during endovascular procedures. Especially during cardiac interventions, detection and tracking of guiding the catheter tip in 2D fluoroscopic images is important for applications such as mapping vessels from angiography (high dose with contrast) to fluoroscopy (low dose without contrast). Tracking the catheter tip poses different challenges: the tip can be occluded by contrast during angiography or interventional devices; and it is always in continuous movement due to the cardiac and respiratory motions. To overcome these challenges, we propose ConTrack, a transformer-based network that uses both spatial and temporal contextual information for accurate device detection and tracking in both X-ray fluoroscopy and angiography. The spatial information comes from the template frames and the segmentation module: the template frames define the surroundings of the device, whereas the segmentation module detects the entire device to bring more context for the tip prediction. Using multiple templates makes the model more robust to the change in appearance of the device when it is occluded by the contrast agent. The flow information computed on the segmented catheter mask between the current and the previous frame helps in further refining the prediction by compensating for the respiratory and cardiac motions. The experiments show that our method achieves 45% or higher accuracy in detection and tracking when compared to state-of-the-art tracking models.
</details>
<details>
<summary>摘要</summary>
Device tracking是endo vasculature процедуры的重要前提。特别是在心血管 intervención中，探测和跟踪导引器的扫描器板准确性非常重要，用于映射血管from angiography (高剂量矿物质) to fluoroscopy (低剂量无矿物质)。跟踪导引器的挑战包括：扫描器板中的导引器可能会被矿物质 occlude during angiography or interventional devices; 并且它们都在心动和呼吸运动中不断移动。为了解决这些挑战，我们提出了ConTrack，一种基于transformer网络的方法，利用扫描器板中的空间和时间上下文信息进行准确的设备探测和跟踪。空间信息来自于模板帧和分 segmentation模块：模板帧定义了设备周围的环境，而分 segmentation模块检测了整个设备，以提供更多的上下文信息 для tip prediction。使用多个模板使得模型更加鲁凤于设备的外观变化，当它被矿物质 occlude 时。流动信息在分 segmentation模块中计算的扫描器板中的�atheter mask between the current and the previous frame帮助进一步细化预测，以补做心动和呼吸运动。实验显示，我们的方法可以在与状态方法进行比较时达到45%或更高的检测和跟踪精度。
</details></li>
</ul>
<hr>
<h2 id="Flow-Guided-Controllable-Line-Drawing-Generation"><a href="#Flow-Guided-Controllable-Line-Drawing-Generation" class="headerlink" title="Flow-Guided Controllable Line Drawing Generation"></a>Flow-Guided Controllable Line Drawing Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07540">http://arxiv.org/abs/2307.07540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyu Fang, Xianfeng Han</li>
<li>for: 本研究旨在 automatizethe generation of artistic character line drawings from photographs, 以探讨 Vector Flow Aware and Line Controllable Image-to-Image Translation 架构的可能性。</li>
<li>methods: 我们首先提出了 Image-to-Flow 网络（I2FNet），用于有效地和稳定地生成 vector flow field，并提供了一个方向导航 для绘制线条。然后，我们引入了 Double Flow Generator（DFG）框架，用于融合 vector flow 和输入图像流的特征，以保证流程中的空间一致性。此外，我们还添加了一个 Line Control Matrix（LCM），以便控制绘制的线条的粗细、 глад度和连续性。</li>
<li>results: 我们的方法可以在高分辨率的图像中生成高质量的人物线 Drawing 图像，并且可以控制绘制的线条的特征。我们的实验结果表明，我们的方法可以在量和质量两个方面具有优于其他方法。<details>
<summary>Abstract</summary>
In this paper, we investigate the problem of automatically controllable artistic character line drawing generation from photographs by proposing a Vector Flow Aware and Line Controllable Image-to-Image Translation architecture, which can be viewed as an appealing intersection between Artificial Intelligence and Arts. Specifically, we first present an Image-to-Flow network (I2FNet) to efficiently and robustly create the vector flow field in a learning-based manner, which can provide a direction guide for drawing lines. Then, we introduce our well-designed Double Flow Generator (DFG) framework to fuse features from learned vector flow and input image flow guaranteeing the spatial coherence of lines. Meanwhile, in order to allow for controllable character line drawing generation, we integrate a Line Control Matrix (LCM) into DFG and train a Line Control Regressor (LCR) to synthesize drawings with different styles by elaborately controlling the level of details, such as thickness, smoothness, and continuity, of lines. Finally, we design a Fourier Transformation Loss to further constrain the character line generation from the frequency domain view of the point. Quantitative and qualitative experiments demonstrate that our approach can obtain superior performance in producing high-resolution character line-drawing images with perceptually realistic characteristics.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了自动控制的艺术人物线 drawing 生成从照片的问题，我们提出了一种 Vector Flow Aware 和 Line Controllable Image-to-Image Translation 架构，可以视为人工智能和艺术之间的吸引人交叉点。 Specifically，我们首先提出了一个 Image-to-Flow 网络 (I2FNet)，可以高效和稳定地生成学习基于的vector flow场，这可以提供笔画的方向指南。然后，我们介绍了我们妙心设计的 Double Flow Generator (DFG) 框架，将学习的vector flow和输入图像流集成，以保证图像线的空间一致性。此外，为了允许可控制的人物线 drawing 生成，我们将一个 Line Control Matrix (LCM)  интегрирован到 DFG 中，并训练一个 Line Control Regressor (LCR)，通过精心控制线的细粒度、平滑度和连续性来 synthesize 不同风格的笔画。最后，我们设计了一种 Fourier Transformation Loss，从frequency domain的角度来约束 character line generation。量化和质量实验表明，我们的方法可以在生成高分辨率的人物线 drawing 图像时获得优秀的表现，具有人工智能和艺术的实际特征。
</details></li>
</ul>
<hr>
<h2 id="LEST-Large-scale-LiDAR-Semantic-Segmentation-with-Transformer"><a href="#LEST-Large-scale-LiDAR-Semantic-Segmentation-with-Transformer" class="headerlink" title="LEST: Large-scale LiDAR Semantic Segmentation with Transformer"></a>LEST: Large-scale LiDAR Semantic Segmentation with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09367">http://arxiv.org/abs/2307.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuanyu Luo, Nuo Cheng, Sikun Ma, Han Li, Xiaohan Li, Shengguang Lei, Pu Li</li>
<li>for: 这篇论文主要用于大规模LiDAR点云semantic segmentation，即自动驾驶感知中的关键任务。</li>
<li>methods: 该论文提出了一种基于Transformer架构的LiDARsemantic Segmentation方法，包括两个新组件：Space Filling Curve（SFC）Grouping策略和Distance-based Cosine Linear Transformer（DISCO）。</li>
<li>results: 在公共nuScenes semantic segmentation验证集和SemanticKITTI测试集上，该模型超过了所有其他状态时的方法。<details>
<summary>Abstract</summary>
Large-scale LiDAR-based point cloud semantic segmentation is a critical task in autonomous driving perception. Almost all of the previous state-of-the-art LiDAR semantic segmentation methods are variants of sparse 3D convolution. Although the Transformer architecture is becoming popular in the field of natural language processing and 2D computer vision, its application to large-scale point cloud semantic segmentation is still limited. In this paper, we propose a LiDAR sEmantic Segmentation architecture with pure Transformer, LEST. LEST comprises two novel components: a Space Filling Curve (SFC) Grouping strategy and a Distance-based Cosine Linear Transformer, DISCO. On the public nuScenes semantic segmentation validation set and SemanticKITTI test set, our model outperforms all the other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
大规模 LiDAR 基于点云Semantic segmentation 是自动驾驶感知中的关键任务。大多数前一代状态对 LiDAR semantic segmentation 方法都是稀疏 3D 卷积的变种。虽然 Transformer 架构在自然语言处理和 2D 计算机视觉领域变得越来越流行，但它在大规模点云Semantic segmentation 领域的应用仍然很有限。在本文中，我们提出了 LiDAR Semantic Segmentation 架构，即 LEST，其包括两个新的组成部分：空间填充曲线（SFC）分组策略和距离基于 косину斯线性变换（DISCO）。在公共 nuScenes semantic segmentation 验证集和 SemanticKITTI 测试集上，我们的模型超过所有其他前一代方法的性能。
</details></li>
</ul>
<hr>
<h2 id="PiTL-Cross-modal-Retrieval-with-Weakly-supervised-Vision-language-Pre-training-via-Prompting"><a href="#PiTL-Cross-modal-Retrieval-with-Weakly-supervised-Vision-language-Pre-training-via-Prompting" class="headerlink" title="PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting"></a>PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07341">http://arxiv.org/abs/2307.07341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixin Guo, Tzu-Jui Julius Wang, Selen Pehlivan, Abduljalil Radman, Jorma Laaksonen</li>
<li>for: 这个研究的目的是发展一种具有更少超vision的语言预训练（W-VLP）方法，以减少需要大量的图像和文本对象标签的烦琐和成本。</li>
<li>methods: 这个研究使用了一个叫做Prompts-in-The-Loop（PiTL）的方法，将知识从大型自然语言模型（LLM）中提取出来，用于描述图像。具体来说，给定一个图像的类别标签，例如制油厂，LLM可以提取出这个图像可能包含的大量储存罐、管线和…等资讯。这些知识可以补充图像中可能出现的常见关系。</li>
<li>results: 这个研究发现，使用PiTL生成的对话 pairs可以对VLP进行强化，并且需要更少的超vision。实验结果显示，使用PiTL-生成的对话 pairs，VLP模型在图像到文本（I2T）和文本到图像（T2I）搜寻任务上表现更好，并且需要更少的超vision。这些结果显示PiTL-生成的对话 pairs 具有优秀的效用性。<details>
<summary>Abstract</summary>
Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD.   To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g. refinery, the knowledge, e.g. a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of 14K categories from ImageNet21K with PiTL. Empirically, the VL models pre-trained with PiTL-generated pairs are strongly favored over other W-VLP works on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with less supervision. The results reveal the effectiveness of PiTL-generated pairs for VLP.
</details>
<details>
<summary>摘要</summary>
视觉语言（VL）预训练（VLP）已经表现出在各种视觉下渠道任务上广泛适用，特别是跨Modal Retrieval。然而，它需要大量的图像文本对，需要辛苦和成本的筹集。相反，弱类视觉预训练（W-VLP）探索了通过图像检测器（OD）生成的对象标签来预训练VL模型。然而，它们仍然需要对的图像和对象级别注释来训练OD。  为了进一步减少监督，我们提出了Prompts-in-The-Loop（PiTL），它使用大型语言模型（LLM）来描述图像。具体来说，给定一个图像的类别标签，例如“炼厂”，我们使用LLM提取的知识，例如“炼厂可能出现的大容器、管道和...”，作为语言对应。这种知识补充了图像中可能出现的常见关系，例如场景中的实体之间的共同关系。我们创建了IN14K dataset，包含900万个图像和100万个描述，分别来自ImageNet21K dataset。我们的实验表明，使用PiTL生成的对应对在图像到文本（I2T）和文本到图像（T2I）检索任务上具有更高的性能，并且需要更少的监督。这些结果表明PiTL生成对的有效性。
</details></li>
</ul>
<hr>
<h2 id="Risk-Controlled-Image-Retrieval"><a href="#Risk-Controlled-Image-Retrieval" class="headerlink" title="Risk Controlled Image Retrieval"></a>Risk Controlled Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07336">http://arxiv.org/abs/2307.07336</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Kaiwen Cai, Chris Xiaoxuan Lu, Xingyu Zhao, Xiaowei Huang</li>
<li>for: 提高图像检索精度和可靠性</li>
<li>methods: 应用不确定性评估技术，提供可靠性 guarantees</li>
<li>results: 实现图像检索集的覆盖保证，提高检索精度和可靠性<details>
<summary>Abstract</summary>
Most image retrieval research focuses on improving predictive performance, ignoring scenarios where the reliability of the prediction is also crucial. Uncertainty quantification technique can be applied to mitigate this issue by assessing uncertainty for retrieval sets, but it can provide only a heuristic estimate of uncertainty rather than a guarantee. To address these limitations, we present Risk Controlled Image Retrieval (RCIR), which generates retrieval sets with coverage guarantee, i.e., retrieval sets that are guaranteed to contain the true nearest neighbors with a predefined probability. RCIR can be easily integrated with existing uncertainty-aware image retrieval systems, agnostic to data distribution and model selection. To the best of our knowledge, this is the first work that provides coverage guarantees for image retrieval. The validity and efficiency of RCIR are demonstrated on four real-world image retrieval datasets: Stanford CAR-196, CUB-200, Pittsburgh and ChestX-Det.
</details>
<details>
<summary>摘要</summary>
大多数图像检索研究强调改进预测性能，忽略了预测可靠性的场景。不精准量化技术可以应用于 mitigate这一问题，评估检索集的不确定性，但它只能提供一个启示性的不确定性估计，而不是一个保证。为解决这些局限性，我们提出了风险控制图像检索（RCIR），生成具有覆盖保证的检索集，即在先定概率下包含真正的最近邻居。RCIR可以与现有的不确定性感知图像检索系统集成，不受数据分布和模型选择的影响。据我们所知，这是首次为图像检索提供了覆盖保证。我们在四个真实世界图像检索数据集上 validate和demonstrate RCIR 的有效性和高效性：Stanford CAR-196、CUB-200、Pittsburgh和ChestX-Det。
</details></li>
</ul>
<hr>
<h2 id="Capsule-network-with-shortcut-routing"><a href="#Capsule-network-with-shortcut-routing" class="headerlink" title="Capsule network with shortcut routing"></a>Capsule network with shortcut routing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10212">http://arxiv.org/abs/2307.10212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dang Thanh Vu, Vo Hoang Trong, Yu Gwang-Hyun, Kim Jin-Young</li>
<li>for: 这项研究旨在提出一种新的 routing 机制，以解决卷积网络中的计算不灵活性问题，直接从地方卷积到全局卷积进行活化。</li>
<li>methods: 这项研究使用了一种注意力采用权值的方法，以提高效率。</li>
<li>results: 实验结果表明，使用提议的粗略路由方法可以在 Mnist、smallnorb 和 affNist  datasets 上达到类比性的分类性能，即准确率为 99.52%、93.91% 和 89.02%  соответственно。此外，使用注意力采用权值的方法可以减少计算量，比EM routing 减少计算量1.42倍和2.5倍。这些发现有助于提高高效精度的层次模式表示模型。<details>
<summary>Abstract</summary>
This study introduces "shortcut routing," a novel routing mechanism in capsule networks that addresses computational inefficiencies by directly activating global capsules from local capsules, eliminating intermediate layers. An attention-based approach with fuzzy coefficients is also explored for improved efficiency. Experimental results on Mnist, smallnorb, and affNist datasets show comparable classification performance, achieving accuracies of 99.52%, 93.91%, and 89.02% respectively. The proposed fuzzy-based and attention-based routing methods significantly reduce the number of calculations by 1.42 and 2.5 times compared to EM routing, highlighting their computational advantages in capsule networks. These findings contribute to the advancement of efficient and accurate hierarchical pattern representation models.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这个研究提出了一种名为"快捷路由"的新的路由机制，用于减少卷积网络中的计算复杂性。这种机制直接从本地卷积到全局卷积的方式活动，从中间层除去了。此外，还探索了一种基于注意力的方法，使用杂度系数进行补做。实验结果表明，在Mnist、smallnorb和affNist dataset上，这种方法可以达到99.52%、93.91%和89.02%的分类精度，并且与EM路由相比，具有1.42和2.5倍的计算优势。这些发现将对快速和准确的层次特征表示模型的发展做出贡献。
</details></li>
</ul>
<hr>
<h2 id="SynTable-A-Synthetic-Data-Generation-Pipeline-for-Unseen-Object-Amodal-Instance-Segmentation-of-Cluttered-Tabletop-Scenes"><a href="#SynTable-A-Synthetic-Data-Generation-Pipeline-for-Unseen-Object-Amodal-Instance-Segmentation-of-Cluttered-Tabletop-Scenes" class="headerlink" title="SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes"></a>SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07333">http://arxiv.org/abs/2307.07333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhili Ng, Haozhe Wang, Zhengshen Zhang, Francis Tay Eng Hock, Marcelo H. Ang Jr</li>
<li>for: 这篇论文是为了描述一个用于生成高质量的人工数据集的Python基于的数据生成工具 SynTable，用于进行未经见过的物体模式分割。</li>
<li>methods: 这篇论文使用了NVIDIA Isaac Sim Replicator Composer生成高质量的人工数据集，并可以根据用户的需求自动生成metadata，如模式和无模式实例分割masks，遮盲masks，深度地图， bounding box 和材料属性。</li>
<li>results: 这篇论文的实验结果表明，使用SynTable生成的数据集可以在Sim-to-Real转移中提高模型的性能，并且与OSD-Amodal数据集的评价相当。<details>
<summary>Abstract</summary>
In this work, we present SynTable, a unified and flexible Python-based dataset generator built using NVIDIA's Isaac Sim Replicator Composer for generating high-quality synthetic datasets for unseen object amodal instance segmentation of cluttered tabletop scenes. Our dataset generation tool can render a complex 3D scene containing object meshes, materials, textures, lighting, and backgrounds. Metadata, such as modal and amodal instance segmentation masks, occlusion masks, depth maps, bounding boxes, and material properties, can be generated to automatically annotate the scene according to the users' requirements. Our tool eliminates the need for manual labeling in the dataset generation process while ensuring the quality and accuracy of the dataset. In this work, we discuss our design goals, framework architecture, and the performance of our tool. We demonstrate the use of a sample dataset generated using SynTable by ray tracing for training a state-of-the-art model, UOAIS-Net. The results show significantly improved performance in Sim-to-Real transfer when evaluated on the OSD-Amodal dataset. We offer this tool as an open-source, easy-to-use, photorealistic dataset generator for advancing research in deep learning and synthetic data generation.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍SynTable，一个统一和灵活的Python基础的数据生成工具，使用NVIDIA的Isaac Sim Replicator Composer生成高质量的synthetic数据集 для未经看过的对象模式分割。我们的数据生成工具可以渲染复杂的3D场景，包括物体模型、材料、Texture、照明和背景。用户可以根据需要生成metadata，如Modal和Amodal实例分割mask、 occlusion mask、深度地图、 bounding box 和材料属性，以自动标注场景。我们的工具消除了手动标注数据生成过程中的需求，同时保证数据的质量和准确性。在这项工作中，我们讨论我们的设计目标、框架体系和工具的性能。我们示例了使用SynTable生成的样本数据集，通过投影法训练State-of-the-art模型UOAIS-Net。结果显示在Sim-to-Real传输中，SynTable生成的数据集在OSD-Amodal数据集上得到了显著改善的性能。我们将这个工具作为开源、易用、 photorealistic数据生成工具，为深度学习和synthetic数据生成的研究提供支持。
</details></li>
</ul>
<hr>
<h2 id="HEAL-SWIN-A-Vision-Transformer-On-The-Sphere"><a href="#HEAL-SWIN-A-Vision-Transformer-On-The-Sphere" class="headerlink" title="HEAL-SWIN: A Vision Transformer On The Sphere"></a>HEAL-SWIN: A Vision Transformer On The Sphere</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07313">http://arxiv.org/abs/2307.07313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/janegerken/heal-swin">https://github.com/janegerken/heal-swin</a></li>
<li>paper_authors: Oscar Carlsson, Jan E. Gerken, Hampus Linander, Heiner Spieß, Fredrik Ohlsson, Christoffer Petersson, Daniel Persson</li>
<li>for: 高分辨率宽角鱼眼图像在自动驾驶等机器人应用中日益重要，但使用普通的卷积神经网络或视transformer在这些数据上是问题，因为将投影到平面上引入了投影和扭曲的损失。</li>
<li>methods: 我们引入了HEAL-SWIN transformer，它将astrophysics和cosmology中使用的高度均匀的Hierarchical Equal Area iso-Latitude Pixelation（HEALPix）网格和层次Shifted-Window（SWIN）变换器结合，实现了高效和灵活的模型，能够在高分辨率、扭曲无的球面数据上训练。在HEAL-SWIN中，HEALPix网格的嵌套结构用于Swin transformer的覆盖和窗口操作，导致一个具有最小计算开销的一维表示方式。</li>
<li>results: 我们在semantic segmentation和depth regression任务上使用HEAL-SWIN模型在真实和 sintetic汽车数据集上达到了superior性能。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/JanEGerken/HEAL-SWIN%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/JanEGerken/HEAL-SWIN中下载。</a><details>
<summary>Abstract</summary>
High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, resulting in a one-dimensional representation of the spherical data with minimal computational overhead. We demonstrate the superior performance of our model for semantic segmentation and depth regression tasks on both synthetic and real automotive datasets. Our code is available at https://github.com/JanEGerken/HEAL-SWIN.
</details>
<details>
<summary>摘要</summary>
高分辨率宽角鱼眼图像在自动驾驶等机器人应用中日益重要。然而，使用普通的卷积神经网络或视Transformer在这些数据上进行训练存在投影和扭曲损失的问题。我们介绍了HEAL-SWIN transformer，它将astrophysics和cosmology中使用的高度均匀的 Hierarchical Equal Area iso-Latitude Pixelation（HEALPix）网格与层次分割（SWIN）转换器结合，实现高效可扩展的模型，能够在高分辨率、扭曲free的球面数据上进行训练。在HEAL-SWIN中，HEALPix网格的嵌套结构用于批处和窗口操作，从而实现了一个具有最小计算开销的一维表示方式。我们在 sintetic和实际汽车数据集上进行Semantic segmentation和深度回归任务的实验，并证明了我们的模型的优秀表现。代码可以在https://github.com/JanEGerken/HEAL-SWIN中找到。
</details></li>
</ul>
<hr>
<h2 id="3D-Shape-Based-Myocardial-Infarction-Prediction-Using-Point-Cloud-Classification-Networks"><a href="#3D-Shape-Based-Myocardial-Infarction-Prediction-Using-Point-Cloud-Classification-Networks" class="headerlink" title="3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks"></a>3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07298">http://arxiv.org/abs/2307.07298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Beetz, Yilong Yang, Abhirup Banerjee, Lei Li, Vicente Grau</li>
<li>for: 这个论文的目的是提高心肺疾病诊断，使用完整的3D卡丁形态来更好地理解和预测心肺疾病的结果。</li>
<li>methods: 该论文提出了一种完全自动化的多步骤管道，包括3D卡丁表面重建步骤和点云分类网络。该方法利用了点云深度学习的最新进展，以实现直接和高效地多尺度学习高分辨率卡丁形态模型。</li>
<li>results: 研究人员对UK Biobank数据集中的1068名参与者进行了预现性心肺疾病检测和新生心肺疾病预测任务，并发现了与临床标准相比的 ~13% 和 ~5% 的提升。此外，他们还分析了每个肺脏和卡丁阶段对3D形态基于的心肺疾病检测的作用，并进行了心肺疾病结果的视觉分析。<details>
<summary>Abstract</summary>
Myocardial infarction (MI) is one of the most prevalent cardiovascular diseases with associated clinical decision-making typically based on single-valued imaging biomarkers. However, such metrics only approximate the complex 3D structure and physiology of the heart and hence hinder a better understanding and prediction of MI outcomes. In this work, we investigate the utility of complete 3D cardiac shapes in the form of point clouds for an improved detection of MI events. To this end, we propose a fully automatic multi-step pipeline consisting of a 3D cardiac surface reconstruction step followed by a point cloud classification network. Our method utilizes recent advances in geometric deep learning on point clouds to enable direct and efficient multi-scale learning on high-resolution surface models of the cardiac anatomy. We evaluate our approach on 1068 UK Biobank subjects for the tasks of prevalent MI detection and incident MI prediction and find improvements of ~13% and ~5% respectively over clinical benchmarks. Furthermore, we analyze the role of each ventricle and cardiac phase for 3D shape-based MI detection and conduct a visual analysis of the morphological and physiological patterns typically associated with MI outcomes.
</details>
<details>
<summary>摘要</summary>
乳心细胞损伤（MI）是冠arca疾病中最常见的一种，临床决策通常基于单一的影像生物标志物。然而，这些指标只是心脏三维结构和生理的一种简化表达，因此难以更好地理解和预测MI结果。在这个工作中，我们研究了基于完整的三维心脏形状的点云的可用性，以提高MI事件的检测。我们提出了一种完全自动的多步骤管道，包括三维心脏表面重建步骤和点云分类网络。我们的方法利用了最新的点云深度学习的进步，以实现直接和高效地多尺度学习高分辨率的心脏 анатомиче模型。我们对UK Biobank数据集上的1068名参与者进行了MI检测和未来MI预测两个任务，并发现我们的方法与临床标准准确率相比提高了约13%和5%。此外，我们还分析了每个肺和卡ди亚阶段对于三维形状基于MI检测的作用，并进行了MI结果常见的 morphological和生理特征的视觉分析。
</details></li>
</ul>
<hr>
<h2 id="Sampling-Priors-Augmented-Deep-Unfolding-Network-for-Robust-Video-Compressive-Sensing"><a href="#Sampling-Priors-Augmented-Deep-Unfolding-Network-for-Robust-Video-Compressive-Sensing" class="headerlink" title="Sampling-Priors-Augmented Deep Unfolding Network for Robust Video Compressive Sensing"></a>Sampling-Priors-Augmented Deep Unfolding Network for Robust Video Compressive Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07291">http://arxiv.org/abs/2307.07291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuhaoo00/SPA-DUN">https://github.com/yuhaoo00/SPA-DUN</a></li>
<li>paper_authors: Yuhao Huang, Gangrong Qu, Youran Ge</li>
<li>For: 高速场景记录和低帧率传感器* Methods: 使用Sampling-Priors-Augmented Deep Unfolding Network (SPA-DUN) 进行高效和可靠的 видео压缩感知重建* Results: 在 simulations 和实际数据集上，SPA-DUN 可以处理多种采样设置，并 achieve state-of-the-art 性能与极高效率。<details>
<summary>Abstract</summary>
Video Compressed Sensing (VCS) aims to reconstruct multiple frames from one single captured measurement, thus achieving high-speed scene recording with a low-frame-rate sensor. Although there have been impressive advances in VCS recently, those state-of-the-art (SOTA) methods also significantly increase model complexity and suffer from poor generality and robustness, which means that those networks need to be retrained to accommodate the new system. Such limitations hinder the real-time imaging and practical deployment of models. In this work, we propose a Sampling-Priors-Augmented Deep Unfolding Network (SPA-DUN) for efficient and robust VCS reconstruction. Under the optimization-inspired deep unfolding framework, a lightweight and efficient U-net is exploited to downsize the model while improving overall performance. Moreover, the prior knowledge from the sampling model is utilized to dynamically modulate the network features to enable single SPA-DUN to handle arbitrary sampling settings, augmenting interpretability and generality. Extensive experiments on both simulation and real datasets demonstrate that SPA-DUN is not only applicable for various sampling settings with one single model but also achieves SOTA performance with incredible efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Implicit-Neural-Feature-Fusion-Function-for-Multispectral-and-Hyperspectral-Image-Fusion"><a href="#Implicit-Neural-Feature-Fusion-Function-for-Multispectral-and-Hyperspectral-Image-Fusion" class="headerlink" title="Implicit Neural Feature Fusion Function for Multispectral and Hyperspectral Image Fusion"></a>Implicit Neural Feature Fusion Function for Multispectral and Hyperspectral Image Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07288">http://arxiv.org/abs/2307.07288</a></li>
<li>repo_url: None</li>
<li>paper_authors: ShangQi Deng, RuoCheng Wu, Liang-Jian Deng, Ran Ran, Tai-Xiang Jiang<br>for: 这个论文的目的是解决多spectral和 hyperspectral图像融合问题，即将高分辨率多spectral图像（HR-MSI）和低分辨率 hyperspectral图像（LR-HSI）融合成高分辨率 hyperspectral图像（HR-HSI）。methods: 本论文提出了一种基于Implicit Neural Representation（INR）的方法，称为Implicit Neural Feature Fusion Function（INF），它利用HR-MSI作为高频率细节辅助输入，并通过 dual high-frequency fusion（DHFF）结构和cosine similarity（INR-CS）来协调多modal特征。results: 根据实验结果，提出的INFN网络在两个公共数据集上（CAVE和Harvard）上达到了当前最佳性能。<details>
<summary>Abstract</summary>
Multispectral and Hyperspectral Image Fusion (MHIF) is a practical task that aims to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI) of the same scene to obtain a high-resolution hyperspectral image (HR-HSI). Benefiting from powerful inductive bias capability, CNN-based methods have achieved great success in the MHIF task. However, they lack certain interpretability and require convolution structures be stacked to enhance performance. Recently, Implicit Neural Representation (INR) has achieved good performance and interpretability in 2D tasks due to its ability to locally interpolate samples and utilize multimodal content such as pixels and coordinates. Although INR-based approaches show promise, they require extra construction of high-frequency information (\emph{e.g.,} positional encoding). In this paper, inspired by previous work of MHIF task, we realize that HR-MSI could serve as a high-frequency detail auxiliary input, leading us to propose a novel INR-based hyperspectral fusion function named Implicit Neural Feature Fusion Function (INF). As an elaborate structure, it solves the MHIF task and addresses deficiencies in the INR-based approaches. Specifically, our INF designs a Dual High-Frequency Fusion (DHFF) structure that obtains high-frequency information twice from HR-MSI and LR-HSI, then subtly fuses them with coordinate information. Moreover, the proposed INF incorporates a parameter-free method named INR with cosine similarity (INR-CS) that uses cosine similarity to generate local weights through feature vectors. Based on INF, we construct an Implicit Neural Fusion Network (INFN) that achieves state-of-the-art performance for MHIF tasks of two public datasets, \emph{i.e.,} CAVE and Harvard. The code will soon be made available on GitHub.
</details>
<details>
<summary>摘要</summary>
多спектраль和高spectral图像融合（MHIF）是一个实用的任务，旨在将高分辨率多спектраль图像（HR-MSI）和低分辨率高spectral图像（LR-HSI）的同一场景图像融合为获得高分辨率高spectral图像（HR-HSI）。由于强大的推导偏好能力，基于CNN的方法在MHIF任务中取得了很大的成功。然而，它们缺乏一定的解释性和需要堆叠 convolution 结构来提高性能。在最近的几年，基于INR的方法在2D任务中取得了良好的性能和解释性，因为它可以地方 interpolate samples和利用多Modal的内容，如像素和坐标。虽然INR基于的方法表现良好，但它们需要额外建立高频信息（例如， pozitional encoding）。在这篇论文中，我们受到MHIF任务的启发，认为HR-MSI可以作为高频细节助手输入，这引导我们提出了一种基于INR的新的高spectral融合函数，即偏导内在特征融合函数（INF）。作为一种复杂的结构，INF解决了MHIF任务和INR基于方法的缺陷。具体来说，我们的INF实现了双高频融合（DHFF）结构，从HR-MSI和LR-HSI中获取高频信息两次，然后细致地融合它们与坐标信息。此外，我们的INF还包括一种无参数的方法，即INR WITH cosine similarity（INR-CS），使用cosine similarity来生成本地权重通过特征向量。基于INF，我们建立了一个偏导内在融合网络（INFN），实现了MHIF任务的最新状态。代码即将在GitHub上公布。
</details></li>
</ul>
<hr>
<h2 id="Cloud-Detection-in-Multispectral-Satellite-Images-Using-Support-Vector-Machines-With-Quantum-Kernels"><a href="#Cloud-Detection-in-Multispectral-Satellite-Images-Using-Support-Vector-Machines-With-Quantum-Kernels" class="headerlink" title="Cloud Detection in Multispectral Satellite Images Using Support Vector Machines With Quantum Kernels"></a>Cloud Detection in Multispectral Satellite Images Using Support Vector Machines With Quantum Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07281">http://arxiv.org/abs/2307.07281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artur Miroszewski, Jakub Mielczarek, Filip Szczepanek, Grzegorz Czelusta, Bartosz Grabowski, Bertrand Le Saux, Jakub Nalepa<br>for:* 这个论文是为了扩展 классические支持向量机(SVM)，使其能够更好地处理卫星数据分类 зада务。methods:* 这个论文使用了量子kernel（QKE）过程和经典SVM训练方法结合，将像素数据映射到希尔伯特空间中。* 使用ZZ-特征地图将参数化 ansatz 状态映射到希尔伯特空间中，并优化参数以最大化kernel目标对齐。results:* 实验结果表明，使用模拟的hybrid SVM可以成功地分类卫星图像，并且与经典SVM的准确率相当。<details>
<summary>Abstract</summary>
Support vector machines (SVMs) are a well-established classifier effectively deployed in an array of pattern recognition and classification tasks. In this work, we consider extending classic SVMs with quantum kernels and applying them to satellite data analysis. The design and implementation of SVMs with quantum kernels (hybrid SVMs) is presented. It consists of the Quantum Kernel Estimation (QKE) procedure combined with a classic SVM training routine. The pixel data are mapped to the Hilbert space using ZZ-feature maps acting on the parameterized ansatz state. The parameters are optimized to maximize the kernel target alignment. We approach the problem of cloud detection in satellite image data, which is one of the pivotal steps in both on-the-ground and on-board satellite image analysis processing chains. The experiments performed over the benchmark Landsat-8 multispectral dataset revealed that the simulated hybrid SVM successfully classifies satellite images with accuracy on par with classic SVMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Frequency-Domain-Adversarial-Training-for-Robust-Volumetric-Medical-Segmentation"><a href="#Frequency-Domain-Adversarial-Training-for-Robust-Volumetric-Medical-Segmentation" class="headerlink" title="Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation"></a>Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07269">http://arxiv.org/abs/2307.07269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asif-hanif/vafa">https://github.com/asif-hanif/vafa</a></li>
<li>paper_authors: Asif Hanif, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</li>
<li>for: 这篇论文旨在提高深度学习模型在医疗领域中的可靠性，因为这些模型在抗击测试中很容易受到攻击。</li>
<li>methods: 该论文使用3D频域对抗攻击来攻击深度学习模型，并且提出了一种基于频域对抗训练的方法来优化模型的可靠性。</li>
<li>results: 该论文通过实验表明，使用频域对抗训练可以提高模型对于不同类型的攻击的抗击能力，并且可以保持模型在清洁和攻击样本上的性能之间取得一个更好的平衡。<details>
<summary>Abstract</summary>
It is imperative to ensure the robustness of deep learning models in critical applications such as, healthcare. While recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. We present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. Using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. Code is publicly available at https://github.com/asif-hanif/vafa.
</details>
<details>
<summary>摘要</summary>
必须确保深度学习模型在重要应用领域如医疗中的稳定性。 although recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. we present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. code is publicly available at https://github.com/asif-hanif/vafa.Here's the text in Traditional Chinese:必须确保深度学习模型在重要应用领域如医疗中的稳定性。 although recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. we present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. code is publicly available at https://github.com/asif-hanif/vafa.
</details></li>
</ul>
<hr>
<h2 id="cOOpD-Reformulating-COPD-classification-on-chest-CT-scans-as-anomaly-detection-using-contrastive-representations"><a href="#cOOpD-Reformulating-COPD-classification-on-chest-CT-scans-as-anomaly-detection-using-contrastive-representations" class="headerlink" title="cOOpD: Reformulating COPD classification on chest CT scans as anomaly detection using contrastive representations"></a>cOOpD: Reformulating COPD classification on chest CT scans as anomaly detection using contrastive representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07254">http://arxiv.org/abs/2307.07254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silvia D. Almeida, Carsten T. Lüth, Tobias Norajitra, Tassilo Wald, Marco Nolden, Paul F. Jaeger, Claus P. Heussel, Jürgen Biederer, Oliver Weinheimer, Klaus Maier-Hein</li>
<li>for: 这个研究的目的是为了提高肺病诊断的精确性和效率，特别是对于肺病多形变异的诊断。</li>
<li>methods: 这个研究使用了一种自适应对应模型，通过学习无标示肺部频谱像的表现，实现肺病分类的准确性和效率。</li>
<li>results: 研究结果显示，这个方法可以实现肺病分类的高精确性和高效率，并且可以提供明确的肺部频谱像 anomaly map 和病人级别得分，对于肺病的早期诊断和监测提供了有价的帮助。<details>
<summary>Abstract</summary>
Classification of heterogeneous diseases is challenging due to their complexity, variability of symptoms and imaging findings. Chronic Obstructive Pulmonary Disease (COPD) is a prime example, being underdiagnosed despite being the third leading cause of death. Its sparse, diffuse and heterogeneous appearance on computed tomography challenges supervised binary classification. We reformulate COPD binary classification as an anomaly detection task, proposing cOOpD: heterogeneous pathological regions are detected as Out-of-Distribution (OOD) from normal homogeneous lung regions. To this end, we learn representations of unlabeled lung regions employing a self-supervised contrastive pretext model, potentially capturing specific characteristics of diseased and healthy unlabeled regions. A generative model then learns the distribution of healthy representations and identifies abnormalities (stemming from COPD) as deviations. Patient-level scores are obtained by aggregating region OOD scores. We show that cOOpD achieves the best performance on two public datasets, with an increase of 8.2% and 7.7% in terms of AUROC compared to the previous supervised state-of-the-art. Additionally, cOOpD yields well-interpretable spatial anomaly maps and patient-level scores which we show to be of additional value in identifying individuals in the early stage of progression. Experiments in artificially designed real-world prevalence settings further support that anomaly detection is a powerful way of tackling COPD classification.
</details>
<details>
<summary>摘要</summary>
临床疾病分类是一项复杂的任务，因为疾病的复杂性、症状的多样性以及图像观察结果的不确定性。 chronic obstructive pulmonary disease (COPD) 是一个典型的例子，它 Despite being the third leading cause of death, it is often underdiagnosed due to its sparse, diffuse, and heterogeneous appearance on computed tomography (CT) scans. To address this challenge, we propose a novel approach called cOOpD, which reforms COPD binary classification as an anomaly detection task.In cOOpD, we first learn representations of unlabeled lung regions using a self-supervised contrastive pretext task, which captures specific characteristics of both healthy and diseased regions. We then use a generative model to learn the distribution of healthy representations and identify abnormalities (stemming from COPD) as deviations from this distribution. Patient-level scores are obtained by aggregating region-level out-of-distribution (OOD) scores.We evaluate cOOpD on two public datasets and show that it achieves the best performance compared to previous supervised state-of-the-art methods, with an increase of 8.2% and 7.7% in terms of area under the receiver operating characteristic curve (AUROC). Additionally, cOOpD provides well-interpretable spatial anomaly maps and patient-level scores, which can be used to identify individuals in the early stage of progression.In artificially designed real-world prevalence settings, we also demonstrate that anomaly detection is a powerful way of tackling COPD classification.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Boosting-Rethinking-Medical-Contrastive-Vision-Language-Pre-Training"><a href="#Knowledge-Boosting-Rethinking-Medical-Contrastive-Vision-Language-Pre-Training" class="headerlink" title="Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training"></a>Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07246">http://arxiv.org/abs/2307.07246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaofei Chen, Yuting He, Cheng Xue, Rongjun Ge, Shuo Li, Guanyu Yang</li>
<li>for: 提高计算机助け诊断的可行性，解决医疗领域大规模semantic overlap和shift问题</li>
<li>methods: 基于医疗对话语言预训练，不需人工标注，通过描述信息学习描述信息的表示学习</li>
<li>results: 在8个任务中，包括分类、 segmentation、检索和semantic relatedness等，与零shot或几shot设置相当或更好的表现<details>
<summary>Abstract</summary>
The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowledge. Extensive experiments validate the effect of our framework on eight tasks including classification, segmentation, retrieval, and semantic relatedness, achieving comparable or better performance with the zero-shot or few-shot settings. Our code is open on https://github.com/ChenXiaoFei-CS/KoBo.
</details>
<details>
<summary>摘要</summary>
基于预训技术的基金会模型已经有效提高了人工智能的理论和实践应用。这些模型使得计算机辅助诊断的广泛应用变得可能。医疗对视语言预训，不需要人工注释，是一种有效的方法来引导表示学习使用描述信息。然而，预训的效果受到医疗领域的大规模semantic overlap和shift问题的限制。为了解决这些问题，我们提出了知识扩充对视语言预训框架(KoBo)，该框架将临床知识integrated到视语言semantic consistency的学习中。该框架使用无偏点、开放集样本知识表示来度量负样本噪声，并补充视语言共 informations和临床知识之间的对应关系。我们进行了广泛的实验，证明了我们的框架在八个任务中，包括分类、 segmentation、retrieval和semantic relatedness等，可以达到零 instances或少 instances的设置下的相当或更好的性能。我们的代码可以在https://github.com/ChenXiaoFei-CS/KoBo 中找到。
</details></li>
</ul>
<hr>
<h2 id="FreeCOS-Self-Supervised-Learning-from-Fractals-and-Unlabeled-Images-for-Curvilinear-Object-Segmentation"><a href="#FreeCOS-Self-Supervised-Learning-from-Fractals-and-Unlabeled-Images-for-Curvilinear-Object-Segmentation" class="headerlink" title="FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation"></a>FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07245">http://arxiv.org/abs/2307.07245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ty-shi/freecos">https://github.com/ty-shi/freecos</a></li>
<li>paper_authors: Tianyi Shi, Xiaohuan Ding, Liang Zhang, Xin Yang<br>for:自动 segmentation of curvilinear objects methods:使用 fractal-based synthesis 和 geometric information alignmentresults:超越现有的无监督方法和领域适应方法，在四个公共数据集上（XCAD、DRIVE、STARE和CrackTree）实现了高度的性能提升。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Curvilinear object segmentation is critical for many applications. However, manually annotating curvilinear objects is very time-consuming and error-prone, yielding insufficiently available annotated datasets for existing supervised methods and domain adaptation methods. This paper proposes a self-supervised curvilinear object segmentation method that learns robust and distinctive features from fractals and unlabeled images (FreeCOS). The key contributions include a novel Fractal-FDA synthesis (FFS) module and a geometric information alignment (GIA) approach. FFS generates curvilinear structures based on the parametric Fractal L-system and integrates the generated structures into unlabeled images to obtain synthetic training images via Fourier Domain Adaptation. GIA reduces the intensity differences between the synthetic and unlabeled images by comparing the intensity order of a given pixel to the values of its nearby neighbors. Such image alignment can explicitly remove the dependency on absolute intensity values and enhance the inherent geometric characteristics which are common in both synthetic and real images. In addition, GIA aligns features of synthetic and real images via the prediction space adaptation loss (PSAL) and the curvilinear mask contrastive loss (CMCL). Extensive experimental results on four public datasets, i.e., XCAD, DRIVE, STARE and CrackTree demonstrate that our method outperforms the state-of-the-art unsupervised methods, self-supervised methods and traditional methods by a large margin. The source code of this work is available at https://github.com/TY-Shi/FreeCOS.
</details>
<details>
<summary>摘要</summary>
Curvilinear对象 segmentation是许多应用程序的关键。然而，手动标注curvilinear对象是非常时间consuming和容易出错，导致现有的超级vised方法和领域适应方法的可用数据不足。这篇论文提出了一种无监督的curvilinear对象 segmentation方法，该方法可以从自然语言和无标签图像中学习抗衡性和特征特征（FreeCOS）。本论文的主要贡献包括一种新的FFS模块和一种几何信息对接（GIA）方法。FFS使用 Parametric Fractal L-system生成curvilinear结构，并将生成的结构与无标签图像集成，以获得synthetic训练图像via Fourier Domain Adaptation。GIA通过比较给定像素的INTENSITY值与其邻近 neighbors的INTENSITY值，来降低synthetic和真实图像之间的INTENSITY值差异。这种图像对接可以明确地去除绝对INTENSITY值的依赖关系，并强调图像的内在几何特征。此外，GIA还使用PSAL和CMCL来对实际和synthetic图像的特征进行对接。我们在四个公共数据集（XCAD、DRIVE、STARE和CrackTree）进行了广泛的实验，并证明了我们的方法在无监督方法、自监督方法和传统方法之上具有显著的优势。 FreeCOS代码可以在https://github.com/TY-Shi/FreeCOS上获取。
</details></li>
</ul>
<hr>
<h2 id="MaxSR-Image-Super-Resolution-Using-Improved-MaxViT"><a href="#MaxSR-Image-Super-Resolution-Using-Improved-MaxViT" class="headerlink" title="MaxSR: Image Super-Resolution Using Improved MaxViT"></a>MaxSR: Image Super-Resolution Using Improved MaxViT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07240">http://arxiv.org/abs/2307.07240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bincheng Yang, Gangshan Wu</li>
<li>for: 该文章的目的是提出一种基于最近的混合视觉变换器模型MaxViT的单张图像超解模型，以提高单张图像超解的性能。</li>
<li>methods: 该模型包括四部分：浅层特征提取块、多个层次适应MaxViT块来提取深层次特征和模型自身相似性，层次特征融合块、重建块。关键组件是适应MaxViT块，它将MBConv混合 с缩放-激活、块注意力和网格注意力。为了更好地全局模型输入低分辨率图像的自相似性，我们改进了块注意力和网格注意力，使之成为适应块注意力和适应网格注意力。</li>
<li>results: 我们实现了两种不同的模型：классиical单张图像超解模型（MaxSR）和轻量级单张图像超解模型（MaxSR-light）。实验显示，我们的MaxSR和MaxSR-light在效率上创造了新的状态对。<details>
<summary>Abstract</summary>
While transformer models have been demonstrated to be effective for natural language processing tasks and high-level vision tasks, only a few attempts have been made to use powerful transformer models for single image super-resolution. Because transformer models have powerful representation capacity and the in-built self-attention mechanisms in transformer models help to leverage self-similarity prior in input low-resolution image to improve performance for single image super-resolution, we present a single image super-resolution model based on recent hybrid vision transformer of MaxViT, named as MaxSR. MaxSR consists of four parts, a shallow feature extraction block, multiple cascaded adaptive MaxViT blocks to extract deep hierarchical features and model global self-similarity from low-level features efficiently, a hierarchical feature fusion block, and finally a reconstruction block. The key component of MaxSR, i.e., adaptive MaxViT block, is based on MaxViT block which mixes MBConv with squeeze-and-excitation, block attention and grid attention. In order to achieve better global modelling of self-similarity in input low-resolution image, we improve block attention and grid attention in MaxViT block to adaptive block attention and adaptive grid attention which do self-attention inside each window across all grids and each grid across all windows respectively in the most efficient way. We instantiate proposed model for classical single image super-resolution (MaxSR) and lightweight single image super-resolution (MaxSR-light). Experiments show that our MaxSR and MaxSR-light establish new state-of-the-art performance efficiently.
</details>
<details>
<summary>摘要</summary>
transformer模型已经在自然语言处理任务和高级视觉任务上得到了证明，但只有一些尝试使用强大的transformer模型进行单图超解像。因为transformer模型具有强大的表示能力和内置的自注意机制，可以借助输入低分辨率图像中的自相似性前来提高单图超解像性能。为此，我们提出了基于最近的hybrid视觉transformer（MaxViT）模型的单图超解像模型，名为MaxSR。MaxSR包括四部分：浅层特征提取块、多个彩色分割的adaptive MaxViT块、层次特征融合块和最后的重建块。MaxSR的关键组件是adaptive MaxViT块，它将MBConv与压缩激发、块注意力和网格注意力相结合。为了更好地全局地模型输入低分辨率图像中的自相似性，我们在MaxViT块中进行了改进，使其成为adaptive块注意力和adaptive网格注意力，可以在最有效的方式内进行自注意。我们实现了提议的模型，并对 классиical单图超解像（MaxSR）和lightweight单图超解像（MaxSR-light）进行实现。实验结果显示，我们的MaxSR和MaxSR-light都达到了高效的新状态码。
</details></li>
</ul>
<hr>
<h2 id="Source-Free-Domain-Adaptive-Fundus-Image-Segmentation-with-Class-Balanced-Mean-Teacher"><a href="#Source-Free-Domain-Adaptive-Fundus-Image-Segmentation-with-Class-Balanced-Mean-Teacher" class="headerlink" title="Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher"></a>Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09973">http://arxiv.org/abs/2307.09973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lloongx/sfda-cbmt">https://github.com/lloongx/sfda-cbmt</a></li>
<li>paper_authors: Longxiang Tang, Kai Li, Chunming He, Yulun Zhang, Xiu Li</li>
<li>for: 本文研究了无源领域适应性肠图像分割，目的是使用无标注图像适应一个目标领域的基于预训练的肠图像分割模型。</li>
<li>methods: 本文提出了一种基于弱强协同学习的分类平衡老师模型（CBMT），以解决无标注数据适应中的两个主要问题：一是不稳定性问题，二是类别不平衡问题。</li>
<li>results: 实验表明，CBMT可以有效地解决这两个问题，并在多个标准推广上超越现有方法。<details>
<summary>Abstract</summary>
This paper studies source-free domain adaptive fundus image segmentation which aims to adapt a pretrained fundus segmentation model to a target domain using unlabeled images. This is a challenging task because it is highly risky to adapt a model only using unlabeled data. Most existing methods tackle this task mainly by designing techniques to carefully generate pseudo labels from the model's predictions and use the pseudo labels to train the model. While often obtaining positive adaption effects, these methods suffer from two major issues. First, they tend to be fairly unstable - incorrect pseudo labels abruptly emerged may cause a catastrophic impact on the model. Second, they fail to consider the severe class imbalance of fundus images where the foreground (e.g., cup) region is usually very small. This paper aims to address these two issues by proposing the Class-Balanced Mean Teacher (CBMT) model. CBMT addresses the unstable issue by proposing a weak-strong augmented mean teacher learning scheme where only the teacher model generates pseudo labels from weakly augmented images to train a student model that takes strongly augmented images as input. The teacher is updated as the moving average of the instantly trained student, which could be noisy. This prevents the teacher model from being abruptly impacted by incorrect pseudo-labels. For the class imbalance issue, CBMT proposes a novel loss calibration approach to highlight foreground classes according to global statistics. Experiments show that CBMT well addresses these two issues and outperforms existing methods on multiple benchmarks.
</details>
<details>
<summary>摘要</summary>
This paper proposes the Class-Balanced Mean Teacher (CBMT) model to address these issues. CBMT uses a weak-strong augmented mean teacher learning scheme, where the teacher model generates pseudo labels from weakly augmented images to train a student model that takes strongly augmented images as input. The teacher is updated as the moving average of the instantly trained student, which helps prevent the teacher model from being impacted by incorrect pseudo-labels. Additionally, CBMT proposes a novel loss calibration approach to highlight foreground classes according to global statistics, addressing the class imbalance issue.Experiments show that CBMT effectively addresses these issues and outperforms existing methods on multiple benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Masked-Autoencoders-for-Unsupervised-Anomaly-Detection-in-Medical-Images"><a href="#Masked-Autoencoders-for-Unsupervised-Anomaly-Detection-in-Medical-Images" class="headerlink" title="Masked Autoencoders for Unsupervised Anomaly Detection in Medical Images"></a>Masked Autoencoders for Unsupervised Anomaly Detection in Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07534">http://arxiv.org/abs/2307.07534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lilygeorgescu/mae-medical-anomaly-detection">https://github.com/lilygeorgescu/mae-medical-anomaly-detection</a></li>
<li>paper_authors: Mariana-Iuliana Georgescu</li>
<li>For: 本研究旨在检测医学成像中的异常现象，但不使用病理样本进行训练。* Methods: 我们提出使用伪装自动编码器模型来学习正常样本的结构，然后在伪装编码器的差分上训练一个异常分类器。我们使用正常扫描图像的重建结果作为负样本，而伪装模块修改正常扫描图像的一些区域的INTENSITY，作为正样本。* Results: 我们在BRATS2020和LUNA16两个医学成像数据集上进行了实验，并与四种 state-of-the-art 异常检测框架进行比较，分别是 AST、RD4AD、AnoVAEGAN 和 f-AnoGAN。<details>
<summary>Abstract</summary>
Pathological anomalies exhibit diverse appearances in medical imaging, making it difficult to collect and annotate a representative amount of data required to train deep learning models in a supervised setting. Therefore, in this work, we tackle anomaly detection in medical images training our framework using only healthy samples. We propose to use the Masked Autoencoder model to learn the structure of the normal samples, then train an anomaly classifier on top of the difference between the original image and the reconstruction provided by the masked autoencoder. We train the anomaly classifier in a supervised manner using as negative samples the reconstruction of the healthy scans, while as positive samples, we use pseudo-abnormal scans obtained via our novel pseudo-abnormal module. The pseudo-abnormal module alters the reconstruction of the normal samples by changing the intensity of several regions. We conduct experiments on two medical image data sets, namely BRATS2020 and LUNA16 and compare our method with four state-of-the-art anomaly detection frameworks, namely AST, RD4AD, AnoVAEGAN and f-AnoGAN.
</details>
<details>
<summary>摘要</summary>
医学影像中的疾病异常现象多样化，使得收集和标注充足数据来训练深度学习模型在指导下的情况困难。因此，在这项工作中，我们采用不同的方法来检测医学影像中的异常。我们提议使用Masked Autoencoder模型来学习正常样本的结构，然后在Masked Autoencoder的差异上训练异常分类器。我们在指导下训练异常分类器，使用正样本的重建为负样本，而使用我们新提出的假异常模块生成的 pseudo-异常样本作为正样本。假异常模块对正常样本的重建进行了一些区域的INTENSITY变化。我们在 BRATS2020和LUNA16两个医学影像数据集上进行了实验，并与四种现代异常检测框架进行了比较，namely AST, RD4AD, AnoVAEGAN和f-AnoGAN。
</details></li>
</ul>
<hr>
<h2 id="Challenge-Results-Are-Not-Reproducible"><a href="#Challenge-Results-Are-Not-Reproducible" class="headerlink" title="Challenge Results Are Not Reproducible"></a>Challenge Results Are Not Reproducible</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07226">http://arxiv.org/abs/2307.07226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Annika Reinke, Georg Grab, Lena Maier-Hein</li>
<li>for: 本研究旨在评估医疗影像分析挑战的可重现性。</li>
<li>methods: 研究使用了2019年度稳定医疗影像分类挑战（ROBUST-MIS）中提交的算法的重现。</li>
<li>results: 重现后，挑战排名显著不同于原来的挑战，这表明挑战排名可能不具可重现性。<details>
<summary>Abstract</summary>
While clinical trials are the state-of-the-art methods to assess the effect of new medication in a comparative manner, benchmarking in the field of medical image analysis is performed by so-called challenges. Recently, comprehensive analysis of multiple biomedical image analysis challenges revealed large discrepancies between the impact of challenges and quality control of the design and reporting standard. This work aims to follow up on these results and attempts to address the specific question of the reproducibility of the participants methods. In an effort to determine whether alternative interpretations of the method description may change the challenge ranking, we reproduced the algorithms submitted to the 2019 Robust Medical Image Segmentation Challenge (ROBUST-MIS). The leaderboard differed substantially between the original challenge and reimplementation, indicating that challenge rankings may not be sufficiently reproducible.
</details>
<details>
<summary>摘要</summary>
在药物开发中，临床试验是当今最佳方法来评估新药的效果，而在医学影像分析领域，则通过 так称的挑战来进行比较性评估。最近，多个生物医学影像分析挑战的全面分析发现，挑战的影响和设计标准的质量控制存在大差异。这项工作的目标是跟进这些结果，并尝试解决特定问题，即参与者的方法是否可重复。为了确定参与者的方法是否具有可重复性，我们对2019年的稳定医学影像分类挑战（ROBUST-MIS）中提交的算法进行重新实现。结果发现，挑战排名存在很大的差异，这表明挑战排名可能不具有足够的可重复性。
</details></li>
</ul>
<hr>
<h2 id="Complementary-Frequency-Varying-Awareness-Network-for-Open-Set-Fine-Grained-Image-Recognition"><a href="#Complementary-Frequency-Varying-Awareness-Network-for-Open-Set-Fine-Grained-Image-Recognition" class="headerlink" title="Complementary Frequency-Varying Awareness Network for Open-Set Fine-Grained Image Recognition"></a>Complementary Frequency-Varying Awareness Network for Open-Set Fine-Grained Image Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07214">http://arxiv.org/abs/2307.07214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayin Sun, Hong Wang, Qiulei Dong</li>
<li>for: 提高开放集合图像识别精度</li>
<li>methods: 提出了一种Complementary Frequency-varying Awareness Network（CFAN），包括三个模块：（i）特征提取模块，（ii）频率变化过滤模块，（iii） complementary temporal aggregation module。</li>
<li>results: 对于3种细致图像 dataset和2种粗致图像 dataset的实验结果表明，CFAN-OSFGR方法在大多数情况下与9种现有方法进行比较，表现出了显著的优势。<details>
<summary>Abstract</summary>
Open-set image recognition is a challenging topic in computer vision. Most of the existing works in literature focus on learning more discriminative features from the input images, however, they are usually insensitive to the high- or low-frequency components in features, resulting in a decreasing performance on fine-grained image recognition. To address this problem, we propose a Complementary Frequency-varying Awareness Network that could better capture both high-frequency and low-frequency information, called CFAN. The proposed CFAN consists of three sequential modules: (i) a feature extraction module is introduced for learning preliminary features from the input images; (ii) a frequency-varying filtering module is designed to separate out both high- and low-frequency components from the preliminary features in the frequency domain via a frequency-adjustable filter; (iii) a complementary temporal aggregation module is designed for aggregating the high- and low-frequency components via two Long Short-Term Memory networks into discriminative features. Based on CFAN, we further propose an open-set fine-grained image recognition method, called CFAN-OSFGR, which learns image features via CFAN and classifies them via a linear classifier. Experimental results on 3 fine-grained datasets and 2 coarse-grained datasets demonstrate that CFAN-OSFGR performs significantly better than 9 state-of-the-art methods in most cases.
</details>
<details>
<summary>摘要</summary>
“开放集image认识是计算机视觉领域中的一个挑战。大多数现有的文献works都是学习从输入图像中学习更有吸引力的特征，然而，它们通常对高频或低频组件不敏感，导致图像细化认识性能下降。为解决这个问题，我们提出了一种Complementary Frequency-varying Awareness Network（CFAN），可以更好地捕捉高频和低频信息。CFAN包括三个顺序模块：（i）一个特征提取模块，用于从输入图像中学习初步特征；（ii）一个频率域中的频率变化滤波器，用于在频率域中分离出高频和低频组件；（iii）一个 complementary temporal aggregation模块，用于在时间域中聚合高频和低频组件，并使用两个Long Short-Term Memory网络（LSTM）进行聚合。基于CFAN，我们进一步提出了一种开放集细化图像认识方法（CFAN-OSFGR），它通过CFAN学习图像特征，并使用一个线性分类器进行分类。实验结果表明，CFAN-OSFGR在3个细化图像 dataset和2个粗化图像 dataset上表现出色，在大多数情况下与9种现有方法进行比较，表现出 significatively better 的性能。”
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Motion-Conditioned-Diffusion-Model-for-Skeleton-based-Video-Anomaly-Detection"><a href="#Multimodal-Motion-Conditioned-Diffusion-Model-for-Skeleton-based-Video-Anomaly-Detection" class="headerlink" title="Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection"></a>Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07205">http://arxiv.org/abs/2307.07205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aleflabo/MoCoDAD">https://github.com/aleflabo/MoCoDAD</a></li>
<li>paper_authors: Alessandro Flaborea, Luca Collorone, Guido D’Amely, Stefano D’Arrigo, Bardh Prenkaj, Fabio Galasso</li>
<li>for: 视频异常检测（VAD）</li>
<li>methods: 使用生成模型和扩散概率模型生成多模态人体姿态，并通过统计聚合来检测异常</li>
<li>results: 在4个 benchmark 上表现出色，超过了现有的最佳Result<details>
<summary>Abstract</summary>
Anomalies are rare and anomaly detection is often therefore framed as One-Class Classification (OCC), i.e. trained solely on normalcy. Leading OCC techniques constrain the latent representations of normal motions to limited volumes and detect as abnormal anything outside, which accounts satisfactorily for the openset'ness of anomalies. But normalcy shares the same openset'ness property since humans can perform the same action in several ways, which the leading techniques neglect. We propose a novel generative model for video anomaly detection (VAD), which assumes that both normality and abnormality are multimodal. We consider skeletal representations and leverage state-of-the-art diffusion probabilistic models to generate multimodal future human poses. We contribute a novel conditioning on the past motion of people and exploit the improved mode coverage capabilities of diffusion processes to generate different-but-plausible future motions. Upon the statistical aggregation of future modes, an anomaly is detected when the generated set of motions is not pertinent to the actual future. We validate our model on 4 established benchmarks: UBnormal, HR-UBnormal, HR-STC, and HR-Avenue, with extensive experiments surpassing state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
异常现象是罕见的，因此异常检测通常被框架为一类分类（OCC），即只在正常情况下进行训练。现leading OCC技术限制了正常动作的latent表示形式为有限体积，检测其外部为异常。然而，正常情况也具有同样的开放性属性，因为人们可以通过不同的方式执行同一个动作，这些技术忽略了。我们提出了一种新的生成模型 для视频异常检测（VAD），它假设了正常和异常都是多模的。我们考虑了骨骼表示形式，并利用当今最佳的扩散概率模型来生成多模未来人体姿势。我们采用了一种新的 conditioning 方法，基于过去人体运动的统计聚合，并利用扩散过程的改进Mode覆盖能力来生成不同 yet plausible 的未来姿势。当生成的集合不符合实际未来时，我们识别出异常。我们验证了我们的模型在 4 个确立的标准准obenchmark 上，包括 UBnormal、HR-UBnormal、HR-STC 和 HR-Avenue，并进行了广泛的实验，超过了当前最佳的成果。
</details></li>
</ul>
<hr>
<h2 id="Volumetric-Wireframe-Parsing-from-Neural-Attraction-Fields"><a href="#Volumetric-Wireframe-Parsing-from-Neural-Attraction-Fields" class="headerlink" title="Volumetric Wireframe Parsing from Neural Attraction Fields"></a>Volumetric Wireframe Parsing from Neural Attraction Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10206">http://arxiv.org/abs/2307.10206</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cherubicxn/neat">https://github.com/cherubicxn/neat</a></li>
<li>paper_authors: Nan Xue, Bin Tan, Yuxi Xiao, Liang Dong, Gui-Song Xia, Tianfu Wu</li>
<li>for: 这篇论文旨在实现高精度的3D线框解析，无需Explicit匹配。</li>
<li>methods: 该方法首先提出一种基于神经网络的Attraction Fields，用于学习3D线段坐标。然后，它们提出一种全球缝隙感知（GJP）模块，用于从Attraction Fields中感知有意义的3D缝隙。最后，它们计算3D线框的原始笔记，通过吸引查询的3D线段到3D缝隙上。</li>
<li>results: 在DTU和BlendedMVS数据集上进行了实验，取得了出色的表现。<details>
<summary>Abstract</summary>
The primal sketch is a fundamental representation in Marr's vision theory, which allows for parsimonious image-level processing from 2D to 2.5D perception. This paper takes a further step by computing 3D primal sketch of wireframes from a set of images with known camera poses, in which we take the 2D wireframes in multi-view images as the basis to compute 3D wireframes in a volumetric rendering formulation. In our method, we first propose a NEural Attraction (NEAT) Fields that parameterizes the 3D line segments with coordinate Multi-Layer Perceptrons (MLPs), enabling us to learn the 3D line segments from 2D observation without incurring any explicit feature correspondences across views. We then present a novel Global Junction Perceiving (GJP) module to perceive meaningful 3D junctions from the NEAT Fields of 3D line segments by optimizing a randomly initialized high-dimensional latent array and a lightweight decoding MLP. Benefitting from our explicit modeling of 3D junctions, we finally compute the primal sketch of 3D wireframes by attracting the queried 3D line segments to the 3D junctions, significantly simplifying the computation paradigm of 3D wireframe parsing. In experiments, we evaluate our approach on the DTU and BlendedMVS datasets with promising performance obtained. As far as we know, our method is the first approach to achieve high-fidelity 3D wireframe parsing without requiring explicit matching.
</details>
<details>
<summary>摘要</summary>
primal 绘制是马尔普视觉理论中的基本表示，允许从2D到2.5D的简洁图像处理。本文进一步计算基于多视图图像的知道摄像机位置的3D primal 绘制，其中我们将2D 绘制图像作为基础，计算3D 绘制图像在三维渲染中。我们首先提出了一种基于多层感知神经网络（MLP）的神经吸引（NEAT）场，以学习2D 绘制图像中的3D 线段。然后，我们提出了一种全球缝合（GJP）模块，用于从 NEAT 场中提取有意义的3D 缝合。由于我们明确地模型了3D 缝合，因此我们最终计算了3D 绘制图像的 primal 绘制，从而大大简化了3D 绘制图像的计算模式。在实验中，我们对DTU和BlendedMVS数据集进行了评估，并取得了出色的性能。据我们所知，我们的方法是首次实现高精度3D 绘制 parsing 无需显式匹配。
</details></li>
</ul>
<hr>
<h2 id="Omnipotent-Adversarial-Training-for-Unknown-Label-noisy-and-Imbalanced-Datasets"><a href="#Omnipotent-Adversarial-Training-for-Unknown-Label-noisy-and-Imbalanced-Datasets" class="headerlink" title="Omnipotent Adversarial Training for Unknown Label-noisy and Imbalanced Datasets"></a>Omnipotent Adversarial Training for Unknown Label-noisy and Imbalanced Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08596">http://arxiv.org/abs/2307.08596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanlinlee/oat">https://github.com/guanlinlee/oat</a></li>
<li>paper_authors: Guanlin Li, Kangjie Chen, Yuan Xu, Han Qiu, Tianwei Zhang</li>
<li>for: 本研究的目的是解决实际应用中的一个挑战，即在受损和噪声 dataset 上训练一个模型，以达到高度清洁率和鲁棒性。</li>
<li>methods: 我们提出了两种创新的方法来解决训练集中的标签噪声和数据不均匀性问题。首先，我们引入了一个 oracle 到 adversarial training  processe，以帮助模型学习正确的数据-标签 conditional distribution。其次，我们提出了 logits 调整 adversarial training，以解决数据不均匀性挑战，这可以帮助模型学习bayes-优化的分布。</li>
<li>results: 我们的全面评估结果显示，OAT 在复杂的数据不均匀和标签噪声场景下出现较大的改善，clean accuracy 提高了More than 20%，robust accuracy 提高了More than 10%。<details>
<summary>Abstract</summary>
Adversarial training is an important topic in robust deep learning, but the community lacks attention to its practical usage. In this paper, we aim to resolve a real-world application challenge, i.e., training a model on an imbalanced and noisy dataset to achieve high clean accuracy and robustness, with our proposed Omnipotent Adversarial Training (OAT). Our strategy consists of two innovative methodologies to address the label noise and data imbalance in the training set. We first introduce an oracle into the adversarial training process to help the model learn a correct data-label conditional distribution. This carefully-designed oracle can provide correct label annotations for adversarial training. We further propose logits adjustment adversarial training to overcome the data imbalance challenge, which can help the model learn a Bayes-optimal distribution. Our comprehensive evaluation results show that OAT outperforms other baselines by more than 20% clean accuracy improvement and 10% robust accuracy improvement under the complex combinations of data imbalance and label noise scenarios. The code can be found in https://github.com/GuanlinLee/OAT.
</details>
<details>
<summary>摘要</summary>
<sup>1</sup> adversarial training是深度学习中重要的话题，但社区对其实用方面缺乏关注。在这篇论文中，我们想要解决一个实际应用挑战，即在受损和噪声 dataset 上训练模型，以 достичь高度的清洁精度和Robustness。我们提出了两种创新的方法来解决训练集中的标签噪声和数据不均衡问题。我们首先引入了一个恰当的oracle到对抗训练过程中，帮助模型学习正确的数据-标签 conditional distribution。这个特制的oracle可以提供正确的标签注释 для对抗训练。我们还提出了Logits调整对抗训练，以解决数据不均衡问题，帮助模型学习bayes优化的分布。我们的全面评估结果表明，OAT在复杂的数据不均衡和标签噪声搭配下出perform得超过20%的清洁精度提升和10%的Robustness提升。代码可以在https://github.com/GuanlinLee/OAT中找到。
</details></li>
</ul>
<hr>
<h2 id="LightFormer-An-End-to-End-Model-for-Intersection-Right-of-Way-Recognition-Using-Traffic-Light-Signals-and-an-Attention-Mechanism"><a href="#LightFormer-An-End-to-End-Model-for-Intersection-Right-of-Way-Recognition-Using-Traffic-Light-Signals-and-an-Attention-Mechanism" class="headerlink" title="LightFormer: An End-to-End Model for Intersection Right-of-Way Recognition Using Traffic Light Signals and an Attention Mechanism"></a>LightFormer: An End-to-End Model for Intersection Right-of-Way Recognition Using Traffic Light Signals and an Attention Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07196">http://arxiv.org/abs/2307.07196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danielming123/lightformer">https://github.com/danielming123/lightformer</a></li>
<li>paper_authors: Zhenxing Ming, Julie Stephany Berrio, Mao Shan, Eduardo Nebot, Stewart Worrall</li>
<li>for: 本研究旨在掌握智能汽车在交通灯光控制下的行驶权。</li>
<li>methods: 该研究提出了一种基于摄像头的交通灯光识别模型，名为LightFormer，用于生成可行的行驶方向的权限状态。模型包括一个空间时间内部结构和一个注意机制，以使用过去图像特征来确定当前帧的权限状态。此外，还提出了一种修改后的多重权重arcface损失函数，以提高模型的分类性能。</li>
<li>results: 经过训练并测试两个公共交通灯光数据集，LightFormer模型能够准确地识别交通灯光的权限状态。<details>
<summary>Abstract</summary>
For smart vehicles driving through signalised intersections, it is crucial to determine whether the vehicle has right of way given the state of the traffic lights. To address this issue, camera based sensors can be used to determine whether the vehicle has permission to proceed straight, turn left or turn right. This paper proposes a novel end to end intersection right of way recognition model called LightFormer to generate right of way status for available driving directions in complex urban intersections. The model includes a spatial temporal inner structure with an attention mechanism, which incorporates features from past image to contribute to the classification of the current frame right of way status. In addition, a modified, multi weight arcface loss is introduced to enhance the model classification performance. Finally, the proposed LightFormer is trained and tested on two public traffic light datasets with manually augmented labels to demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
For smart vehicles driving through signalized intersections, it is crucial to determine whether the vehicle has the right of way given the state of the traffic lights. To address this issue, camera-based sensors can be used to determine whether the vehicle has permission to proceed straight, turn left, or turn right. This paper proposes a novel end-to-end intersection right-of-way recognition model called LightFormer to generate right-of-way status for available driving directions in complex urban intersections. The model includes a spatial-temporal inner structure with an attention mechanism, which incorporates features from past images to contribute to the classification of the current frame right-of-way status. In addition, a modified, multi-weight arcface loss is introduced to enhance the model classification performance. Finally, the proposed LightFormer is trained and tested on two public traffic light datasets with manually augmented labels to demonstrate its effectiveness.Here's the translation in Traditional Chinese:For smart vehicles driving through signalized intersections, it is crucial to determine whether the vehicle has the right of way given the state of the traffic lights. To address this issue, camera-based sensors can be used to determine whether the vehicle has permission to proceed straight, turn left, or turn right. This paper proposes a novel end-to-end intersection right-of-way recognition model called LightFormer to generate right-of-way status for available driving directions in complex urban intersections. The model includes a spatial-temporal inner structure with an attention mechanism, which incorporates features from past images to contribute to the classification of the current frame right-of-way status. In addition, a modified, multi-weight arcface loss is introduced to enhance the model classification performance. Finally, the proposed LightFormer is trained and tested on two public traffic light datasets with manually augmented labels to demonstrate its effectiveness.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-Over-Long-Tailed-Distribution"><a href="#Adversarial-Training-Over-Long-Tailed-Distribution" class="headerlink" title="Adversarial Training Over Long-Tailed Distribution"></a>Adversarial Training Over Long-Tailed Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10205">http://arxiv.org/abs/2307.10205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanlinlee/reat">https://github.com/guanlinlee/reat</a></li>
<li>paper_authors: Guanlin Li, Guowen Xu, Tianwei Zhang</li>
<li>for: 本 paper 探讨了对具有长尾分布的数据进行 adversarial 训练，这种情况在前期工作中很少被探讨。与 convent ional adversarial 训练相比，这种过程会陷入生成不均匀的 adversarial 例子 (AE) 和不均匀的特征空间，导致模型在尾部数据上表现出低 robustness 和准确率。</li>
<li>methods: 为解决这个问题，我们提出了一个新的 adversarial 训练框架 – Re-balancing Adversarial Training (REAT)。该框架包括两个组成部分：(1) 一种基于有效数量的新训练策略，以导引模型生成更加均匀和有用的 AE; (2) 一个特殊构造的罚函数，以强制模型的特征空间达到一定的质量标准。</li>
<li>results: 对不同的数据集和模型结构进行评估，我们发现 REAT 可以有效地提高模型的Robustness 和保持模型的清洁准确率。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/GuanlinLee/REAT">https://github.com/GuanlinLee/REAT</a> 找到。<details>
<summary>Abstract</summary>
In this paper, we study adversarial training on datasets that obey the long-tailed distribution, which is practical but rarely explored in previous works. Compared with conventional adversarial training on balanced datasets, this process falls into the dilemma of generating uneven adversarial examples (AEs) and an unbalanced feature embedding space, causing the resulting model to exhibit low robustness and accuracy on tail data. To combat that, we propose a new adversarial training framework -- Re-balancing Adversarial Training (REAT). This framework consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs; (2) a carefully constructed penalty function to force a satisfactory feature space. Evaluation results on different datasets and model structures prove that REAT can effectively enhance the model's robustness and preserve the model's clean accuracy. The code can be found in https://github.com/GuanlinLee/REAT.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了对长条度分布的数据集进行对抗训练，这种情况在前一些研究中很少被考虑。与传统的对抗训练在均衡数据集上进行的情况相比，这个过程会陷入生成不均匀的对抗示例（AE）以及不均匀的特征空间的困境，导致模型在尾部数据上表现出低的鲁棒性和准确率。为解决这个问题，我们提出了一个新的对抗训练框架——重新均衡对抗训练（REAT）。这个框架包括两个组成部分：（1）一种基于有效数量的新训练策略，以导引模型生成更加均匀和有用的AE;（2）一个特殊构造的罚函数，以强制模型在特征空间中达到满意的结果。经过不同数据集和模型结构的评估，我们发现REAT可以有效地提高模型的鲁棒性，同时保持模型的净精度。代码可以在https://github.com/GuanlinLee/REAT找到。
</details></li>
</ul>
<hr>
<h2 id="Erasing-Transforming-and-Noising-Defense-Network-for-Occluded-Person-Re-Identification"><a href="#Erasing-Transforming-and-Noising-Defense-Network-for-Occluded-Person-Re-Identification" class="headerlink" title="Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification"></a>Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07187">http://arxiv.org/abs/2307.07187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neng Dong, Liyan Zhang, Shuanglin Yan, Hao Tang, Jinhui Tang</li>
<li>for:  occluded person re-identification</li>
<li>methods:  adversarial defense, random erasure, random transformations, noise perturbation</li>
<li>results:  effective handling of occlusion issues, no need for external modules, superior performance on five public datasets<details>
<summary>Abstract</summary>
Occlusion perturbation presents a significant challenge in person re-identification (re-ID), and existing methods that rely on external visual cues require additional computational resources and only consider the issue of missing information caused by occlusion. In this paper, we propose a simple yet effective framework, termed Erasing, Transforming, and Noising Defense Network (ETNDNet), which treats occlusion as a noise disturbance and solves occluded person re-ID from the perspective of adversarial defense. In the proposed ETNDNet, we introduce three strategies: Firstly, we randomly erase the feature map to create an adversarial representation with incomplete information, enabling adversarial learning of identity loss to protect the re-ID system from the disturbance of missing information. Secondly, we introduce random transformations to simulate the position misalignment caused by occlusion, training the extractor and classifier adversarially to learn robust representations immune to misaligned information. Thirdly, we perturb the feature map with random values to address noisy information introduced by obstacles and non-target pedestrians, and employ adversarial gaming in the re-ID system to enhance its resistance to occlusion noise. Without bells and whistles, ETNDNet has three key highlights: (i) it does not require any external modules with parameters, (ii) it effectively handles various issues caused by occlusion from obstacles and non-target pedestrians, and (iii) it designs the first GAN-based adversarial defense paradigm for occluded person re-ID. Extensive experiments on five public datasets fully demonstrate the effectiveness, superiority, and practicality of the proposed ETNDNet. The code will be released at \url{https://github.com/nengdong96/ETNDNet}.
</details>
<details>
<summary>摘要</summary>
干扰对人重新标识（re-ID）提出了 significativ challenge，现有的方法通常需要额外的计算资源，并只考虑 occlusion 引起的信息缺失问题。在这篇论文中，我们提出了一种简单 yet effective的框架，称为Erasing, Transforming, and Noising Defense Network（ETNDNet），它将 occlusion 视为干扰，通过对抗防御的方式解决 occluded person re-ID 问题。在我们提出的 ETNDNet 中，我们引入了三种策略：首先，我们随机将特征图抹除，创建一个干扰表示，使得抗护理学习损失，以保护重新标识系统免受缺失信息的影响。其次，我们引入了随机变换，模拟了障碍物和非目标人员的位置偏移，使得抽取器和分类器通过对抗学习，学习具有抗性的表示。最后，我们在特征图中添加了随机值，对干扰引入的噪音进行处理，并通过对抗游戏，提高重新标识系统对 occlusion 的抗性。ETNDNet 的三个关键亮点是：一、它不需要任何外部模块和参数；二、它能有效地处理各种由障碍物和非目标人员引起的 occlusion 问题；三、它是首次在 occluded person re-ID 中应用 GAN 基于对抗防御的方法。我们的实验在五个公共数据集上进行了广泛的证明和评估，并证明了 ETNDNet 的有效性、优势和实用性。代码将在 \url{https://github.com/nengdong96/ETNDNet} 上发布。
</details></li>
</ul>
<hr>
<h2 id="TVPR-Text-to-Video-Person-Retrieval-and-a-New-Benchmark"><a href="#TVPR-Text-to-Video-Person-Retrieval-and-a-New-Benchmark" class="headerlink" title="TVPR: Text-to-Video Person Retrieval and a New Benchmark"></a>TVPR: Text-to-Video Person Retrieval and a New Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07184">http://arxiv.org/abs/2307.07184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Ni, Xu Zhang, Jianhui Wu, Guan-Nan Dong, Aichun Zhu, Hui Liu, Yue Zhang</li>
<li>for: 提高文本基于人脸 Retrieval的性能，解决隐藏在孤立帧中的人脸和变量动作细节的问题。</li>
<li>methods: 提出了一种新的 Text-to-Video Person Retrieval（TVPR）任务，使用文本描述和视频数据交互进行人脸 Retrieval。同时，构建了大规模的 across-modal人员视频数据集（TVPReid），包括人脸、动作和环境交互等详细自然语言描述。</li>
<li>results: 提出了 Text-to-Video Person Retrieval Network（TVPRN），使用视觉和动作特征进行人脸视频表示，并使用预训练的 BERT 获取描述文本的表示，以便找出最相关的人脸视频。经过广泛的实验，TVPRN 在 TVPReid 数据集上达到了state-of-the-art表现。<details>
<summary>Abstract</summary>
Most existing methods for text-based person retrieval focus on text-to-image person retrieval. Nevertheless, due to the lack of dynamic information provided by isolated frames, the performance is hampered when the person is obscured in isolated frames or variable motion details are given in the textual description. In this paper, we propose a new task called Text-to-Video Person Retrieval(TVPR) which aims to effectively overcome the limitations of isolated frames. Since there is no dataset or benchmark that describes person videos with natural language, we construct a large-scale cross-modal person video dataset containing detailed natural language annotations, such as person's appearance, actions and interactions with environment, etc., termed as Text-to-Video Person Re-identification (TVPReid) dataset, which will be publicly available. To this end, a Text-to-Video Person Retrieval Network (TVPRN) is proposed. Specifically, TVPRN acquires video representations by fusing visual and motion representations of person videos, which can deal with temporal occlusion and the absence of variable motion details in isolated frames. Meanwhile, we employ the pre-trained BERT to obtain caption representations and the relationship between caption and video representations to reveal the most relevant person videos. To evaluate the effectiveness of the proposed TVPRN, extensive experiments have been conducted on TVPReid dataset. To the best of our knowledge, TVPRN is the first successful attempt to use video for text-based person retrieval task and has achieved state-of-the-art performance on TVPReid dataset. The TVPReid dataset will be publicly available to benefit future research.
</details>
<details>
<summary>摘要</summary>
现有的文本基于人识别方法多数采用文本到图像人识别。然而，由于隔ilder的缺乏动态信息，在文本描述中人脸遮盖或变化的运动细节时，性能受到了限制。在这篇论文中，我们提出了一个新任务：文本到视频人识别（TVPR），旨在超越隔ilder的局限性。由于没有任何人视频与自然语言描述的数据集或标准，我们构建了一个大规模的跨模态人视频数据集，包括人物的出现、行为和环境交互等细节的自然语言描述，称为文本到视频人重识别（TVPReid）数据集。为此，我们提出了文本到视频人 Retrieval网络（TVPRN）。具体来说，TVPRN使用人视频的视觉和运动表示 fusion 来处理时间遮盖和隔ilder中缺乏变化运动细节。同时，我们使用预训练的 BERT 获取caption表示，并通过caption和视频表示之间的关系，找出最相关的人视频。为证明提出的 TVPRN 的有效性，我们进行了广泛的实验，并在 TVPReid 数据集上达到了当前最佳性能。据我们知道，TVPRN 是首次使用视频来解决文本基于人识别任务，并在 TVPReid 数据集上实现了状态畅的性能。TVPReid 数据集将于未来的研究中公开，以便推动未来的研究。
</details></li>
</ul>
<hr>
<h2 id="DISPEL-Domain-Generalization-via-Domain-Specific-Liberating"><a href="#DISPEL-Domain-Generalization-via-Domain-Specific-Liberating" class="headerlink" title="DISPEL: Domain Generalization via Domain-Specific Liberating"></a>DISPEL: Domain Generalization via Domain-Specific Liberating</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07181">http://arxiv.org/abs/2307.07181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chia-Yuan Chang, Yu-Neng Chuang, Guanchu Wang, Mengnan Du, Na Zou</li>
<li>for: 本研究旨在提出一种基于特征组分的领域总结方法，以便在只使用有限源领域训练后，在未经见过的测试领域上实现良好的性能。</li>
<li>methods: 我们提出了一种基于特征组分的领域总结方法，其中将基础特征分为领域共享特征和领域特定特征。而领域特定特征却难以从输入数据中分辨和隔离。为此，我们提出了一种叫做 DISPEL 的后处理精细掩蔽方法，该方法可以在 embedding 空间中过滤出不定义和难以分辨的领域特定特征。</li>
<li>results: 我们的实验结果表明，DISPEL 可以与现有方法相比，在五个 benchmark 上表现出色，并且可以进一步普适多种算法。此外，我们还 derivated 一个泛化误差 bounds，以保证泛化性能。<details>
<summary>Abstract</summary>
Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fine-tuned models. We derive a generalization error bound to guarantee the generalization performance by optimizing a designed objective loss. The experimental results on five benchmarks demonstrate DISPEL outperforms existing methods and can further generalize various algorithms.
</details>
<details>
<summary>摘要</summary>
领域总则目标是学习一个通用模型，可以在未经见过的测试领域上表现出色，只需在有限的源领域上训练。然而，现有的领域总则方法经常带来预测无关的噪声或需要收集领域标签。为解决这些挑战，我们从不同的角度看待领域总则问题，将下面的特征分为领域共享特征和领域特定特征。然而，领域特定特征很难被识别出来并分离于输入数据中。在这种情况下，我们提出了DISPEL方法，一种后处细化的面积掩码方法，可以在嵌入空间中过滤无法定义或不可分辨的领域特定特征。DISPEL方法使用一个生成器生成特有的掩码，用于过滤领域特定特征。DISPEL框架高度灵活，可以应用于任何精度调整的模型。我们 derivate一个总则错误范围，以保证总则性能。实验结果在五个benchmark上表明，DISPEL方法超过了现有方法，并可以进一步普适多种算法。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Region-Selection-for-Active-Learning-in-Whole-Slide-Image-Semantic-Segmentation"><a href="#Adaptive-Region-Selection-for-Active-Learning-in-Whole-Slide-Image-Semantic-Segmentation" class="headerlink" title="Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation"></a>Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07168">http://arxiv.org/abs/2307.07168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepmicroscopy/adaptiveregionselection">https://github.com/deepmicroscopy/adaptiveregionselection</a></li>
<li>paper_authors: Jingna Qiu, Frauke Wilm, Mathias Öttl, Maja Schlereth, Chang Liu, Tobias Heimann, Marc Aubreville, Katharina Breininger<br>for:这篇论文是为了提高 Histological gigapixel-sized whole slide images (WSIs) 的分类训练模型而做的准备工作，特别是在 Region-based active learning (AL) 中，即通过选择一小部分 annotated 图像区域来训练模型，而不是全图像。methods:这篇论文提出了一种新的技术，即动态选择 annotated 区域，以避免 AL Step size 的选择问题。这种技术首先会找到一个有用的区域，然后确定该区域的最佳包围框，而不是选择固定形状和大小的 rectangular 区域，如标准方法所做。results:这篇论文使用了 breast cancer metastases segmentation 任务，在 CAMELYON16  dataset 上进行了评估。结果显示，这种新技术可以在不同的 AL Step size 下 consistently 实现更高的 sampling efficiency，并且只需要 annotate 2.6% 的组织质量。这意味着可以大幅减少 annotate 图像集的成本。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/DeepMicroscopy/AdaptiveRegionSelection">https://github.com/DeepMicroscopy/AdaptiveRegionSelection</a> 上下载。<details>
<summary>Abstract</summary>
The process of annotating histological gigapixel-sized whole slide images (WSIs) at the pixel level for the purpose of training a supervised segmentation model is time-consuming. Region-based active learning (AL) involves training the model on a limited number of annotated image regions instead of requesting annotations of the entire images. These annotation regions are iteratively selected, with the goal of optimizing model performance while minimizing the annotated area. The standard method for region selection evaluates the informativeness of all square regions of a specified size and then selects a specific quantity of the most informative regions. We find that the efficiency of this method highly depends on the choice of AL step size (i.e., the combination of region size and the number of selected regions per WSI), and a suboptimal AL step size can result in redundant annotation requests or inflated computation costs. This paper introduces a novel technique for selecting annotation regions adaptively, mitigating the reliance on this AL hyperparameter. Specifically, we dynamically determine each region by first identifying an informative area and then detecting its optimal bounding box, as opposed to selecting regions of a uniform predefined shape and size as in the standard method. We evaluate our method using the task of breast cancer metastases segmentation on the public CAMELYON16 dataset and show that it consistently achieves higher sampling efficiency than the standard method across various AL step sizes. With only 2.6\% of tissue area annotated, we achieve full annotation performance and thereby substantially reduce the costs of annotating a WSI dataset. The source code is available at https://github.com/DeepMicroscopy/AdaptiveRegionSelection.
</details>
<details>
<summary>摘要</summary>
“ annotating histological gigapixel-sized whole slide images (WSIs) at the pixel level for the purpose of training a supervised segmentation model is time-consuming. Region-based active learning (AL) involves training the model on a limited number of annotated image regions instead of requesting annotations of the entire images. These annotation regions are iteratively selected, with the goal of optimizing model performance while minimizing the annotated area. The standard method for region selection evaluates the informativeness of all square regions of a specified size and then selects a specific quantity of the most informative regions. We find that the efficiency of this method highly depends on the choice of AL step size (i.e., the combination of region size and the number of selected regions per WSI), and a suboptimal AL step size can result in redundant annotation requests or inflated computation costs. This paper introduces a novel technique for selecting annotation regions adaptively, mitigating the reliance on this AL hyperparameter. Specifically, we dynamically determine each region by first identifying an informative area and then detecting its optimal bounding box, as opposed to selecting regions of a uniform predefined shape and size as in the standard method. We evaluate our method using the task of breast cancer metastases segmentation on the public CAMELYON16 dataset and show that it consistently achieves higher sampling efficiency than the standard method across various AL step sizes. With only 2.6% of tissue area annotated, we achieve full annotation performance and thereby substantially reduce the costs of annotating a WSI dataset. The source code is available at https://github.com/DeepMicroscopy/AdaptiveRegionSelection.”Here's the translation in Traditional Chinese:“ annotating histological gigapixel-sized whole slide images (WSIs) at the pixel level for the purpose of training a supervised segmentation model is time-consuming. Region-based active learning (AL) involves training the model on a limited number of annotated image regions instead of requesting annotations of the entire images. These annotation regions are iteratively selected, with the goal of optimizing model performance while minimizing the annotated area. The standard method for region selection evaluates the informativeness of all square regions of a specified size and then selects a specific quantity of the most informative regions. We find that the efficiency of this method highly depends on the choice of AL step size (i.e., the combination of region size and the number of selected regions per WSI), and a suboptimal AL step size can result in redundant annotation requests or inflated computation costs. This paper introduces a novel technique for selecting annotation regions adaptively, mitigating the reliance on this AL hyperparameter. Specifically, we dynamically determine each region by first identifying an informative area and then detecting its optimal bounding box, as opposed to selecting regions of a uniform predefined shape and size as in the standard method. We evaluate our method using the task of breast cancer metastases segmentation on the public CAMELYON16 dataset and show that it consistently achieves higher sampling efficiency than the standard method across various AL step sizes. With only 2.6% of tissue area annotated, we achieve full annotation performance and thereby substantially reduce the costs of annotating a WSI dataset. The source code is available at https://github.com/DeepMicroscopy/AdaptiveRegionSelection.”
</details></li>
</ul>
<hr>
<h2 id="Linking-vision-and-motion-for-self-supervised-object-centric-perception"><a href="#Linking-vision-and-motion-for-self-supervised-object-centric-perception" class="headerlink" title="Linking vision and motion for self-supervised object-centric perception"></a>Linking vision and motion for self-supervised object-centric perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07147">http://arxiv.org/abs/2307.07147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wayveai/socs">https://github.com/wayveai/socs</a></li>
<li>paper_authors: Kaylene C. Stocking, Zak Murez, Vijay Badrinarayanan, Jamie Shotton, Alex Kendall, Claire Tomlin, Christopher P. Burgess</li>
<li>for: 这篇论文目的是提出一种基于自主驾驶算法的对象中心表示方法，以便更好地理解环境中多个独立的代理人和场景特征之间的互动。</li>
<li>methods: 这篇论文使用了一种自我超视的对象中心视觉模型，只使用了RGB视频和车辆的pose作为输入，进行对象分解。</li>
<li>results: 研究人员通过使用这种方法在Waymo开放感知数据集上获得了可接受的结果，虽然对象层质不如supervised方法或其他使用更特权信息的方法，但模型能够学习一种能够融合多个相机视角的时间序列表示，并成功跟踪了 dataset 中的许多车辆和行人。<details>
<summary>Abstract</summary>
Object-centric representations enable autonomous driving algorithms to reason about interactions between many independent agents and scene features. Traditionally these representations have been obtained via supervised learning, but this decouples perception from the downstream driving task and could harm generalization. In this work we adapt a self-supervised object-centric vision model to perform object decomposition using only RGB video and the pose of the vehicle as inputs. We demonstrate that our method obtains promising results on the Waymo Open perception dataset. While object mask quality lags behind supervised methods or alternatives that use more privileged information, we find that our model is capable of learning a representation that fuses multiple camera viewpoints over time and successfully tracks many vehicles and pedestrians in the dataset. Code for our model is available at https://github.com/wayveai/SOCS.
</details>
<details>
<summary>摘要</summary>
object-centric表示法可以让自主驾驶算法对多个独立的Agent和场景特征进行理解。传统上，这些表示方法通过监督学习获得，但这会分离感知和下游驾驶任务，可能会对泛化造成负面影响。在这项工作中，我们适应了基于自动学习的对象分解模型，只使用RGB视频和车辆的pose作为输入。我们示示了我们的方法在 Waymo 开放感知数据集上获得了有前途的结果。虽然对象层面质量落后于监督方法或其他使用更特权信息的方法，但我们发现我们的模型可以学习一个汇集多个相机视点的时间序列，并成功跟踪多辆车和人行进在数据集中。代码可以在https://github.com/wayveai/SOCS上获取。
</details></li>
</ul>
<hr>
<h2 id="Deteksi-Sampah-di-Permukaan-dan-Dalam-Perairan-pada-Objek-Video-dengan-Metode-Robust-and-Efficient-Post-Processing-dan-Tubelet-Level-Bounding-Box-Linking"><a href="#Deteksi-Sampah-di-Permukaan-dan-Dalam-Perairan-pada-Objek-Video-dengan-Metode-Robust-and-Efficient-Post-Processing-dan-Tubelet-Level-Bounding-Box-Linking" class="headerlink" title="Deteksi Sampah di Permukaan dan Dalam Perairan pada Objek Video dengan Metode Robust and Efficient Post-Processing dan Tubelet-Level Bounding Box Linking"></a>Deteksi Sampah di Permukaan dan Dalam Perairan pada Objek Video dengan Metode Robust and Efficient Post-Processing dan Tubelet-Level Bounding Box Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10039">http://arxiv.org/abs/2307.10039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryan Tjandra, Made S. N. Negara, Nyoo S. C. Handoko</li>
<li>for: 本研究旨在开发一种自动化垃圾收集机器人，以解决印度尼西亚水域中垃圾的问题。</li>
<li>methods: 本研究使用了YOLOv5模型和Robust &amp; Efficient Post Processing（REPP）方法，以及Tubelet-level bounding box linking在FloW和Roboflow数据集上。这些方法可以提高原生Object Detection的性能，并考虑邻帧检测结果。</li>
<li>results: 研究结果表明，后处理阶段和Tubelet-level bounding box linking可以提高检测质量，相比YOLOv5 alone提高约3%。这些方法可以检测表面和水下垃圾，并可以应用于实时图像基于垃圾收集机器人。<details>
<summary>Abstract</summary>
Indonesia, as a maritime country, has a significant portion of its territory covered by water. Ineffective waste management has resulted in a considerable amount of trash in Indonesian waters, leading to various issues. The development of an automated trash-collecting robot can be a solution to address this problem. The robot requires a system capable of detecting objects in motion, such as in videos. However, using naive object detection methods in videos has limitations, particularly when image focus is reduced and the target object is obstructed by other objects. This paper's contribution provides an explanation of the methods that can be applied to perform video object detection in an automated trash-collecting robot. The study utilizes the YOLOv5 model and the Robust & Efficient Post Processing (REPP) method, along with tubelet-level bounding box linking on the FloW and Roboflow datasets. The combination of these methods enhances the performance of naive object detection from YOLOv5 by considering the detection results in adjacent frames. The results show that the post-processing stage and tubelet-level bounding box linking can improve the quality of detection, achieving approximately 3% better performance compared to YOLOv5 alone. The use of these methods has the potential to detect surface and underwater trash and can be applied to a real-time image-based trash-collecting robot. Implementing this system is expected to mitigate the damage caused by trash in the past and improve Indonesia's waste management system in the future.
</details>
<details>
<summary>摘要</summary>
印度尼西亚，作为一个海上国家，有很大一部分领土被水覆盖。不效的垃圾管理导致了印度尼西亚水域中的废弃物堆积，引起了各种问题。开发自动垃圾收集机器人可以解决这个问题。这个机器人需要一个能够检测运动 объек的系统，例如在视频中。但是使用直观的对象检测方法在视频中有限制，特别是当图像焦点减少和目标对象受到其他对象干扰时。本文的贡献是对视频对象检测方法的解释，并使用YOLOv5模型和Robust & Efficient Post Processing（REPP）方法，以及Tubelet-level bounding box linking在Flow和Roboflow数据集上。将这些方法结合使用可以提高YOLOv5直观对象检测的性能，并考虑邻帧检测结果。结果表明，后处理阶段和Tubelet-level bounding box linking可以提高检测质量，相比YOLOv5 alone，提高约3%。这些方法可以检测表面和水下垃圾，并可以应用于实时图像基于垃圾收集机器人。实施这种系统，预计可以改善过去垃圾的损害，并提高未来印度尼西亚的垃圾管理系统。
</details></li>
</ul>
<hr>
<h2 id="CFI2P-Coarse-to-Fine-Cross-Modal-Correspondence-Learning-for-Image-to-Point-Cloud-Registration"><a href="#CFI2P-Coarse-to-Fine-Cross-Modal-Correspondence-Learning-for-Image-to-Point-Cloud-Registration" class="headerlink" title="CFI2P: Coarse-to-Fine Cross-Modal Correspondence Learning for Image-to-Point Cloud Registration"></a>CFI2P: Coarse-to-Fine Cross-Modal Correspondence Learning for Image-to-Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07142">http://arxiv.org/abs/2307.07142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongxin Yao, Yixin Xuan, Yiwei Chen, Yu Pan</li>
<li>For: 本研究主要针对image-to-point cloud registration问题，实现点云和图像之间的匹配。* Methods: 我们提出了一个具有粗细对称的框架，从本地角度出发，首先建立点云和图像之间的匹配，然后透过精确的搜寻、注意力学习和精确匹配，从细腻搜寻空间中获取高品质的匹配。* Results: 我们在大规模的 OUTDOOR 实验中证明了我们的方法的优越性，并且在EPnP算法下进行了匹配。<details>
<summary>Abstract</summary>
In the context of image-to-point cloud registration, acquiring point-to-pixel correspondences presents a challenging task since the similarity between individual points and pixels is ambiguous due to the visual differences in data modalities. Nevertheless, the same object present in the two data formats can be readily identified from the local perspective of point sets and pixel patches. Motivated by this intuition, we propose a coarse-to-fine framework that emphasizes the establishment of correspondences between local point sets and pixel patches, followed by the refinement of results at both the point and pixel levels. On a coarse scale, we mimic the classic Visual Transformer to translate both image and point cloud into two sequences of local representations, namely point and pixel proxies, and employ attention to capture global and cross-modal contexts. To supervise the coarse matching, we propose a novel projected point proportion loss, which guides to match point sets with pixel patches where more points can be projected into. On a finer scale, point-to-pixel correspondences are then refined from a smaller search space (i.e., the coarsely matched sets and patches) via well-designed sampling, attentional learning and fine matching, where sampling masks are embedded in the last two steps to mitigate the negative effect of sampling. With the high-quality correspondences, the registration problem is then resolved by EPnP algorithm within RANSAC. Experimental results on large-scale outdoor benchmarks demonstrate our superiority over existing methods.
</details>
<details>
<summary>摘要</summary>
在图像与点云注册中，获取点对像像点对应存在挑战，因为图像和点云数据模式之间的视觉差异使得个别点和像素之间的相似性具有很大的歧义。然而，同一个物体在两种数据格式中可以轻松地从地方视角上识别。基于这种感知，我们提出了一个卷积框架，强调在点集和像素块之间建立对应关系，然后对点和像素水平进行重finement。在粗略层次，我们模仿了经典的视觉 трансформа器，将图像和点云转换成两个Sequence of local representation，即点和像素代理，并使用注意力来捕捉全局和交叉模式上的信息。为了监督粗略匹配，我们提出了一种新的 projeted point proportion loss，它引导将点集与像素块匹配，其中更多的点可以被投影到像素块中。在细化层次，点对像像点对应从粗略匹配的小搜索空间（即粗略匹配的集和块）进行重新匹配，使用特制的采样、注意力学习和细化匹配，其中采样面被嵌入最后两步以避免采样的负面影响。与高质量对应关系，则可以通过EPnP算法在RANSAC中解决注册问题。实验结果表明我们在大规模的户外 benchmark 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Text-Video-Retrieval-with-Frozen-Image-Encoders"><a href="#Fine-grained-Text-Video-Retrieval-with-Frozen-Image-Encoders" class="headerlink" title="Fine-grained Text-Video Retrieval with Frozen Image Encoders"></a>Fine-grained Text-Video Retrieval with Frozen Image Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09972">http://arxiv.org/abs/2307.09972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zuozhuo Dai, Fangtao Shao, Qingkun Su, Zilong Dong, Siyu Zhu</li>
<li>for: 文章主要针对文本视频检索（TVR）领域的问题，即如何有效地将文本与视频进行关联。</li>
<li>methods: 文章提出了一种两stage的文本视频检索建模，首先使用现有的 TVR 方法和cosine similarity网络进行高效的文本&#x2F;视频候选选择，然后提出了一种新的视频文本跨模块，以便在空间和时间维度上捕捉细腻的多模式信息。</li>
<li>results: 实验结果表明， compared to 状态空间的方法，我们的提出的 CrossTVR 方法可以更好地提高文本视频检索性能。<details>
<summary>Abstract</summary>
State-of-the-art text-video retrieval (TVR) methods typically utilize CLIP and cosine similarity for efficient retrieval. Meanwhile, cross attention methods, which employ a transformer decoder to compute attention between each text query and all frames in a video, offer a more comprehensive interaction between text and videos. However, these methods lack important fine-grained spatial information as they directly compute attention between text and video-level tokens. To address this issue, we propose CrossTVR, a two-stage text-video retrieval architecture. In the first stage, we leverage existing TVR methods with cosine similarity network for efficient text/video candidate selection. In the second stage, we propose a novel decoupled video text cross attention module to capture fine-grained multimodal information in spatial and temporal dimensions. Additionally, we employ the frozen CLIP model strategy in fine-grained retrieval, enabling scalability to larger pre-trained vision models like ViT-G, resulting in improved retrieval performance. Experiments on text video retrieval datasets demonstrate the effectiveness and scalability of our proposed CrossTVR compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
现代文本视频检索（TVR）方法通常使用CLIP和cosine相似性来实现高效的检索。而cross attention方法，它使用变换器解码器计算文本查询和视频帧之间的关注，可以更好地考虑文本和视频之间的互动。然而，这些方法缺乏细致的空间信息，因为直接计算文本和视频级别的токен之间的关注。为解决这个问题，我们提出了CrossTVR，一种两stage文本视频检索架构。在第一个阶段，我们利用现有的 TVR 方法和cosine相似性网络来高效地选择文本/视频候选者。在第二个阶段，我们提出了一种新的解除视频文本交叉注意模块，以捕捉视频和文本之间的细致的多Modal信息。此外，我们采用冻结 CLIP 模型策略，使得可以扩展到更大的预训练视觉模型 like ViT-G，从而提高检索性能。实验表明，我们提出的 CrossTVR 比对现有的方法更有效和可扩展。
</details></li>
</ul>
<hr>
<h2 id="CeRF-Convolutional-Neural-Radiance-Fields-for-New-View-Synthesis-with-Derivatives-of-Ray-Modeling"><a href="#CeRF-Convolutional-Neural-Radiance-Fields-for-New-View-Synthesis-with-Derivatives-of-Ray-Modeling" class="headerlink" title="CeRF: Convolutional Neural Radiance Fields for New View Synthesis with Derivatives of Ray Modeling"></a>CeRF: Convolutional Neural Radiance Fields for New View Synthesis with Derivatives of Ray Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07125">http://arxiv.org/abs/2307.07125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyan Yang, Dingbo Lu, Yang Li, Chenhui Li, Changbo Wang</li>
<li>for: novel view synthesis of high-fidelity images</li>
<li>methods: Convolutional Neural Radiance Fields with 1D convolutional operations and structured neural network architecture, and a proposed recurrent module to solve geometric ambiguity</li>
<li>results: promising results compared with existing state-of-the-art methodsHere’s the full text in Simplified Chinese:</li>
<li>for: 高效图像新视图合成</li>
<li>methods: 基于1D卷积操作的卷积神经场，以及用于解决几何杂乱的循环模块</li>
<li>results: 与现有状态艺术方法相比，显示出优秀的结果<details>
<summary>Abstract</summary>
In recent years, novel view synthesis has gained popularity in generating high-fidelity images. While demonstrating superior performance in the task of synthesizing novel views, the majority of these methods are still based on the conventional multi-layer perceptron for scene embedding. Furthermore, light field models suffer from geometric blurring during pixel rendering, while radiance field-based volume rendering methods have multiple solutions for a certain target of density distribution integration. To address these issues, we introduce the Convolutional Neural Radiance Fields to model the derivatives of radiance along rays. Based on 1D convolutional operations, our proposed method effectively extracts potential ray representations through a structured neural network architecture. Besides, with the proposed ray modeling, a proposed recurrent module is employed to solve geometric ambiguity in the fully neural rendering process. Extensive experiments demonstrate the promising results of our proposed model compared with existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
近年来，新视图合成技术受到广泛关注，并在生成高效图像方面达到了显著提高。虽然大多数这些方法仍然基于传统的多层感知器进行场景嵌入，但是它们在新视图合成任务中表现出色。然而，光场模型在像素渲染过程中受到光学模糊的影响，而基于辐射场的Volume渲染方法具有多个解的涂抹积分问题。为解决这些问题，我们介绍了卷积神经场的激素场，通过1D卷积操作来有效地提取潜在的光线表示。此外，我们还提出了一种循环模块，用于解决全神经渲染过程中的 геометрические抽象问题。广泛的实验证明了我们提出的模型与现有状态艺技术相比，具有出色的表现。
</details></li>
</ul>
<hr>
<h2 id="Improved-Flood-Insights-Diffusion-Based-SAR-to-EO-Image-Translation"><a href="#Improved-Flood-Insights-Diffusion-Based-SAR-to-EO-Image-Translation" class="headerlink" title="Improved Flood Insights: Diffusion-Based SAR to EO Image Translation"></a>Improved Flood Insights: Diffusion-Based SAR to EO Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07123">http://arxiv.org/abs/2307.07123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minseok Seo, Youngtack Oh, Doyi Kim, Dongmin Kang, Yeji Choi</li>
<li>For: 该论文目的是提高洪水灾害评估的可 interpretability，通过将Synthetic Aperture Radar（SAR）图像转换成Electro-Optical（EO）图像，提高人类分析者对洪水危机的理解。* Methods: 该论文提出了一种新的Diffusion-Based SAR to EO Image Translation（DSE）框架，用于将SAR图像转换成EO图像，以提高洪水灾害评估的可 interpretability。* Results: 实验结果表明，DSE框架不仅可以提高洪水灾害评估的可读性，还可以提高所有测试的洪水分割基准的性能。<details>
<summary>Abstract</summary>
Driven by rapid climate change, the frequency and intensity of flood events are increasing. Electro-Optical (EO) satellite imagery is commonly utilized for rapid response. However, its utilities in flood situations are hampered by issues such as cloud cover and limitations during nighttime, making accurate assessment of damage challenging. Several alternative flood detection techniques utilizing Synthetic Aperture Radar (SAR) data have been proposed. Despite the advantages of SAR over EO in the aforementioned situations, SAR presents a distinct drawback: human analysts often struggle with data interpretation. To tackle this issue, this paper introduces a novel framework, Diffusion-Based SAR to EO Image Translation (DSE). The DSE framework converts SAR images into EO images, thereby enhancing the interpretability of flood insights for humans. Experimental results on the Sen1Floods11 and SEN12-FLOOD datasets confirm that the DSE framework not only delivers enhanced visual information but also improves performance across all tested flood segmentation baselines.
</details>
<details>
<summary>摘要</summary>
随着气候变化的加速，洪水事件的频率和 INTENSITY 都在增加。电子优化（EO）卫星成像通常用于快速应对。然而，它在洪水情况下面临着云覆盖和夜间限制，增加了评估损害的困难。一些使用 Synthetic Aperture Radar（SAR）数据的洪水探测技术已经被提议。虽然 SAR 在上述情况下具有优势，但它具有一个缺点：人工分析员经常遇到数据解释的困难。为解决这个问题，本文提出了一个新的框架：傅尔基于 SAR 到 EO 图像翻译（DSE）。DSE 框架将 SAR 图像转换为 EO 图像，从而提高了洪水启示的可读性 для 人类。实验结果表明，在 Sen1Floods11 和 SEN12-FLOOD 数据集上，DSE 框架不仅提供了加强的视觉信息，还提高了所有测试的洪水分割基elines 的性能。
</details></li>
</ul>
<hr>
<h2 id="Achelous-A-Fast-Unified-Water-surface-Panoptic-Perception-Framework-based-on-Fusion-of-Monocular-Camera-and-4D-mmWave-Radar"><a href="#Achelous-A-Fast-Unified-Water-surface-Panoptic-Perception-Framework-based-on-Fusion-of-Monocular-Camera-and-4D-mmWave-Radar" class="headerlink" title="Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar"></a>Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07102">http://arxiv.org/abs/2307.07102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GuanRunwei/Achelous">https://github.com/GuanRunwei/Achelous</a></li>
<li>paper_authors: Runwei Guan, Shanliang Yao, Xiaohui Zhu, Ka Lok Man, Eng Gee Lim, Jeremy Smith, Yong Yue, Yutao Yue</li>
<li>for: 提高水面自动驾驶系统的智能化水平，提供一种低成本、快速的涵义束合抗函数。</li>
<li>methods: 基于单光相机和4D mmWave雷达的约束函数整合，同时执行五个任务：视觉目标检测和分割、可驾泊区分 segmentation、水线分割和雷达点云分割。</li>
<li>results: 在一个收集的数据集上，Achelous family模型比 HybridNets 快11 FPS，并在5 mAP$_{\text{50-95}$ 和 0.7 mIoU 上超越 YOLOX-Tiny 和 Segformer-B0，特别是在恶势卷云、黑暗环境和相机失效情况下表现出色。<details>
<summary>Abstract</summary>
Current perception models for different tasks usually exist in modular forms on Unmanned Surface Vehicles (USVs), which infer extremely slowly in parallel on edge devices, causing the asynchrony between perception results and USV position, and leading to error decisions of autonomous navigation. Compared with Unmanned Ground Vehicles (UGVs), the robust perception of USVs develops relatively slowly. Moreover, most current multi-task perception models are huge in parameters, slow in inference and not scalable. Oriented on this, we propose Achelous, a low-cost and fast unified panoptic perception framework for water-surface perception based on the fusion of a monocular camera and 4D mmWave radar. Achelous can simultaneously perform five tasks, detection and segmentation of visual targets, drivable-area segmentation, waterline segmentation and radar point cloud segmentation. Besides, models in Achelous family, with less than around 5 million parameters, achieve about 18 FPS on an NVIDIA Jetson AGX Xavier, 11 FPS faster than HybridNets, and exceed YOLOX-Tiny and Segformer-B0 on our collected dataset about 5 mAP$_{\text{50-95}$ and 0.7 mIoU, especially under situations of adverse weather, dark environments and camera failure. To our knowledge, Achelous is the first comprehensive panoptic perception framework combining vision-level and point-cloud-level tasks for water-surface perception. To promote the development of the intelligent transportation community, we release our codes in \url{https://github.com/GuanRunwei/Achelous}.
</details>
<details>
<summary>摘要</summary>
现有的感知模型通常存在模块化的形式在无人水面车（USV）上，这些模型在边缘设备上并行执行，导致感知结果与USV位置的偏差，并导致自动导航错误决策。相比于无人地面车（UGV），USV的感知模型发展相对较慢。另外，大多数当前的多任务感知模型具有庞大的参数量、慢速推理和不可扩展的问题。基于这一点，我们提出了 Achelous，一个低成本、快速的综合杂化感知框架，用于水面感知。Achelous可同时完成五个任务，包括视觉目标检测和分割、可行区域分割、水面分割和4D mmWave雷达点云分割。此外，Achelous家族中的模型，具有 Less than 5000万参数，在 NVIDIA Jetson AGX Xavier 上达到约 18 FPS，比 HybridNets 快 11 FPS，并在我们收集的数据集上 exceed YOLOX-Tiny 和 Segformer-B0 的 5 mAP$_{\text{50-95}$ 和 0.7 mIoU，特别是在恶劣天气、黑暗环境和摄像头故障等情况下。我们知道，Achelous 是首个对水面感知任务进行综合杂化感知框架的研究。为推动智能交通社区的发展，我们在 \url{https://github.com/GuanRunwei/Achelous} 上发布了代码。
</details></li>
</ul>
<hr>
<h2 id="Bootstrapping-Vision-Language-Learning-with-Decoupled-Language-Pre-training"><a href="#Bootstrapping-Vision-Language-Learning-with-Decoupled-Language-Pre-training" class="headerlink" title="Bootstrapping Vision-Language Learning with Decoupled Language Pre-training"></a>Bootstrapping Vision-Language Learning with Decoupled Language Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07063">http://arxiv.org/abs/2307.07063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiren-jian/blitext">https://github.com/yiren-jian/blitext</a></li>
<li>paper_authors: Yiren Jian, Chongyang Gao, Soroush Vosoughi</li>
<li>for: 本研究旨在优化冰冻大语言模型（LLM）的应用，以提高资源充足的视频语言（VL）预训练。</li>
<li>methods: 我们提出了一种新的方法，即通过预测理想的提示来匹配视觉特征。我们引入了一种名为Prompt-Transformer（P-Former）的模型，该模型通过仅仅使用语言数据进行训练，而不需要图像文本对应。这种方法使得VL训练过程中分解成了一个额外的阶段。</li>
<li>results: 我们的实验表明，我们的框架可以显著提高一个基本图像文本基线（BLIP-2）的性能，并有效地缩小基于4M或129M图像文本对应的模型性能差距。此外，我们的框架可以在不同的基模块上进行模块化和灵活的应用，并在视频学习任务中得到了成功应用。代码可以在<a target="_blank" rel="noopener" href="https://github.com/yiren-jian/BLIText%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/yiren-jian/BLIText上获取。</a><details>
<summary>Abstract</summary>
We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importantly, our framework is modality-agnostic and flexible in terms of architectural design, as validated by its successful application in a video learning task using varied base modules. The code is available at https://github.com/yiren-jian/BLIText
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，旨在优化冻结大型语言模型（LLM）在资源占用量大的视频语言（VL）预训练中的应用。现有的方法使用视觉特征作为提示来导引语言模型，主要是确定与文本相对应的最 relevante 视觉特征。我们的方法则注重语言组件，具体来说是确定最佳提示，以对应视觉特征进行对齐。我们提出了Prompt-Transformer（P-Former）模型，该模型预测最佳提示，并且专门在语言数据上进行训练，不需要图像文本对应。这种策略将结束到终端VL训练过程中的整个过程分解为一个额外的、独立的阶段。我们的实验表明，我们的框架可以显著提高一种稳定的图像文本基线（BLIP-2）的性能，并有效缩小使用4M或129M图像文本对应的模型性能差距。此外，我们的框架是模块化的，可以在不同的基础模块上进行成功应用，如视频学习任务中。代码可以在https://github.com/yiren-jian/BLIText 上获取。
</details></li>
</ul>
<hr>
<h2 id="AnyStar-Domain-randomized-universal-star-convex-3D-instance-segmentation"><a href="#AnyStar-Domain-randomized-universal-star-convex-3D-instance-segmentation" class="headerlink" title="AnyStar: Domain randomized universal star-convex 3D instance segmentation"></a>AnyStar: Domain randomized universal star-convex 3D instance segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07044">http://arxiv.org/abs/2307.07044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neel-dey/anystar">https://github.com/neel-dey/anystar</a></li>
<li>paper_authors: Neel Dey, S. Mazdak Abulnaga, Benjamin Billot, Esra Abaci Turk, P. Ellen Grant, Adrian V. Dalca, Polina Golland</li>
<li>for: 这篇论文是为了解决 bio-微型视觉和放射学中的星形物体实例分割问题，而不需要大量的手动标注。</li>
<li>methods: 该论文提出了一种域随机生成模型，可以生成星形物体的 synthetic 训练数据，以train 一个通用的星形实例分割网络。</li>
<li>results: 根据论文的描述，使用该方法可以训练一个通用的星形实例分割网络，可以在不同的 dataset 和模式下进行高精度的3D分割，无需进行任何的重新引入、微调或域适应。<details>
<summary>Abstract</summary>
Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, finetuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar.
</details>
<details>
<summary>摘要</summary>
星形对象在生物微scopie和放射学中出现，包括核体、肿块、肿肿转移和其他单元。现有的星形实例分割网络需要大量的手动标注实例，这需要巨大的人工标注努力。此外，对于新的数据集和成像模式来说，需要重大的重工程化或者赋值，这是因为对比、形状、方向、分辨率和密度的变化。我们提出了AnyStar，一种随机生成模型，用于生成星形对象的 sintetic 训练数据，用于训练通用的星形实例分割网络。因此，使用我们的生成模型训练的网络不需要未看过的数据集的注释。我们的网络可以高精度地3D分割C. elegans和P. dumerilii核体在 fluorescence 微scopie中，mouse cortical核体在 micro-CT 中，zebrafish brain核体在 EM 中，以及人类胎儿 Placental cotyledons 在人类胎儿 MRI 中，无需任何再训练、赋值、传输学习或领域适应。代码可以在https://github.com/neel-dey/AnyStar 中找到。
</details></li>
</ul>
<hr>
<h2 id="Deepfake-Video-Detection-Using-Generative-Convolutional-Vision-Transformer"><a href="#Deepfake-Video-Detection-Using-Generative-Convolutional-Vision-Transformer" class="headerlink" title="Deepfake Video Detection Using Generative Convolutional Vision Transformer"></a>Deepfake Video Detection Using Generative Convolutional Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07036">http://arxiv.org/abs/2307.07036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/erprogs/genconvit">https://github.com/erprogs/genconvit</a></li>
<li>paper_authors: Deressa Wodajo, Solomon Atnafu, Zahid Akhtar</li>
<li>For: The paper is written for detecting deepfake videos, which have become a significant concern due to their potential to spread false information and compromise digital media integrity.* Methods: The proposed model, called Generative Convolutional Vision Transformer (GenConViT), combines ConvNeXt and Swin Transformer models for feature extraction, and utilizes Autoencoder and Variational Autoencoder to learn from the latent data distribution.* Results: The model achieves improved performance in detecting a wide range of deepfake videos, with an average accuracy of 95.8% and an AUC value of 99.3% across the tested datasets. The model demonstrates robust performance in deepfake video detection and provides an effective solution for identifying fake videos while preserving media integrity.Here are the three points in Simplified Chinese text:* For: 这篇论文是为检测深伪视频而写的，深伪视频已经成为 False Information 的一种可能性，并且可能会对数字媒体的完整性产生负面影响。* Methods: 该论文提出的模型是 Generative Convolutional Vision Transformer (GenConViT)，它结合 ConvNeXt 和 Swin Transformer 模型来提取特征，并使用 Autoencoder 和 Variational Autoencoder 来学习 latent 数据分布。* Results: 模型在多种深伪视频检测中表现出色，具有高精度和高 F1 分数，以及 AUC 值。模型在检测深伪视频方面达到了robust性，并提供了一种有效的方法来识别假视频，保持数字媒体完整性。<details>
<summary>Abstract</summary>
Deepfakes have raised significant concerns due to their potential to spread false information and compromise digital media integrity. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes Autoencoder and Variational Autoencoder to learn from the latent data distribution. By learning from the visual artifacts and latent data distribution, GenConViT achieves improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, DeepfakeTIMIT, and Celeb-DF v2 datasets, achieving high classification accuracy, F1 scores, and AUC values. The proposed GenConViT model demonstrates robust performance in deepfake video detection, with an average accuracy of 95.8% and an AUC value of 99.3% across the tested datasets. Our proposed model addresses the challenge of generalizability in deepfake detection by leveraging visual and latent features and providing an effective solution for identifying a wide range of fake videos while preserving media integrity. The code for GenConViT is available at https://github.com/erprogs/GenConViT.
</details>
<details>
<summary>摘要</summary>
深层复制（Deepfake）技术已经引起了广泛的关注，因为它们的潜在能力可能导致假信息的传播和数字媒体的完整性的威胁。在这项工作中，我们提出了一种基于Generative Convolutional Vision Transformer（GenConViT）的深层复制视频检测模型。我们的模型结合了ConvNeXt和Swin Transformer模型，用于特征提取，并使用Autoencoder和Variational Autoencoder来学习从隐藏数据分布中学习。通过学习视觉特征和隐藏数据分布，GenConViT实现了对各种深层复制视频的广泛检测性能的改进。我们的模型在DFDC、FF++、DeepfakeTIMIT和Celeb-DF v2等数据集上进行训练和评估，实现了高的分类精度、F1分数和AUC值。我们提出的GenConViT模型在深层复制视频检测中表现了强大的一致性，其平均准确率为95.8%，AUC值为99.3%。我们的提出的模型通过利用视觉和隐藏特征，为检测各种假视频而提供了有效的解决方案，保护数字媒体完整性。GenConViT模型的代码可以在https://github.com/erprogs/GenConViT中下载。
</details></li>
</ul>
<hr>
<h2 id="Tapestry-of-Time-and-Actions-Modeling-Human-Activity-Sequences-using-Temporal-Point-Process-Flows"><a href="#Tapestry-of-Time-and-Actions-Modeling-Human-Activity-Sequences-using-Temporal-Point-Process-Flows" class="headerlink" title="Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows"></a>Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10305">http://arxiv.org/abs/2307.10305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinayak Gupta, Srikanta Bedathur</li>
<li>for: 本研究旨在理解人类活动序列中的动作时间分布，并同时解决下流作业预测、序列目标预测和端到端序列生成等高含量问题。</li>
<li>methods: 本文提出了一种基于带 markers的时间点过程（MTPP）框架，使用自注意模块和时间正常化流来模型动作序列中动作之间的影响和间隔时间。此外，提出了一种新的添加，可以处理动作序列中动作顺序的变化。</li>
<li>results: 经过广泛的实验，表明ProActive模型在动作和目标预测方面具有显著的准确率提升，并且实现了端到端动作序列生成的首次应用。<details>
<summary>Abstract</summary>
Human beings always engage in a vast range of activities and tasks that demonstrate their ability to adapt to different scenarios. Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike the time series datasets extracted from electronics or machines, these action sequences are highly disparate in their nature -- the time to finish a sequence of actions can vary between different persons. Therefore, understanding the dynamics of these sequences is essential for many downstream tasks such as activity length prediction, goal prediction, next action recommendation, etc. Existing neural network-based approaches that learn a continuous-time activity sequence (or CTAS) are limited to the presence of only visual data or are designed specifically for a particular task, i.e., limited to next action or goal prediction. In this paper, we present ProActive, a neural marked temporal point process (MTPP) framework for modeling the continuous-time distribution of actions in an activity sequence while simultaneously addressing three high-impact problems -- next action prediction, sequence-goal prediction, and end-to-end sequence generation. Specifically, we utilize a self-attention module with temporal normalizing flows to model the influence and the inter-arrival times between actions in a sequence. In addition, we propose a novel addition over the ProActive model that can handle variations in the order of actions, i.e., different methods of achieving a given goal. We demonstrate that this variant can learn the order in which the person or actor prefers to do their actions. Extensive experiments on sequences derived from three activity recognition datasets show the significant accuracy boost of ProActive over the state-of-the-art in terms of action and goal prediction, and the first-ever application of end-to-end action sequence generation.
</details>
<details>
<summary>摘要</summary>
人类总是在各种各样的活动和任务中展现出非常强的适应能力，这些活动和任务可以看作是一个时间序列的动作。与电子设备或机器的时间序列数据不同，这些动作序列具有非常不同的特点，因此理解这些序列的动态是许多下游任务的关键，如行动长度预测、目标预测、下一个动作建议等。现有的基于神经网络的方法，只能学习视觉数据的连续时间动作序列（CTAS），而且一些方法只能进行特定任务，如下一个动作或目标预测。在这篇论文中，我们提出了ProActive，一种基于marked temporal point process（MTPP）的神经网络框架，用于模型连续时间动作序列中的动作的分布，同时解决三个高度影响的问题：下一个动作预测、序列目标预测和终端动作生成。我们使用自注意模块和时间正常化流来模型动作序列中动作之间的影响和间隔时间。此外，我们还提出了一种新的ProActive变体，可以处理动作序列中动作的不同顺序，即不同的方法来完成同一个目标。我们的实验表明，这种变体可以学习actor或人类的偏好顺序。与现状的最佳实践相比，ProActive在动作和目标预测方面具有显著的准确率提升，并且是继承动作序列生成的首次应用。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-Heterogeneous-Face-Recognition-with-Conditional-Adaptive-Instance-Modulation"><a href="#Bridging-the-Gap-Heterogeneous-Face-Recognition-with-Conditional-Adaptive-Instance-Modulation" class="headerlink" title="Bridging the Gap: Heterogeneous Face Recognition with Conditional Adaptive Instance Modulation"></a>Bridging the Gap: Heterogeneous Face Recognition with Conditional Adaptive Instance Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07032">http://arxiv.org/abs/2307.07032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anjith George, Sebastien Marcel<br>for:本研究旨在提高Face Recognition（FR）系统的可用性，通过匹配不同频谱的脸像，包括可见和热成像频谱。methods:我们对不同频谱的脸像进行了分类，并将不同频谱视为不同的样式。我们还提出了一种 Conditional Adaptive Instance Modulation（CAIM）模块，可以将这些样式引入到预训练FR网络中，以适应目标频谱。results:我们在多个具有挑战性的测试集上进行了广泛的评估，并证明了我们的方法在与现有方法相比具有更高的性能。我们将源代码和实验室协议公开发布，以便重复我们的发现。<details>
<summary>Abstract</summary>
Heterogeneous Face Recognition (HFR) aims to match face images across different domains, such as thermal and visible spectra, expanding the applicability of Face Recognition (FR) systems to challenging scenarios. However, the domain gap and limited availability of large-scale datasets in the target domain make training robust and invariant HFR models from scratch difficult. In this work, we treat different modalities as distinct styles and propose a framework to adapt feature maps, bridging the domain gap. We introduce a novel Conditional Adaptive Instance Modulation (CAIM) module that can be integrated into pre-trained FR networks, transforming them into HFR networks. The CAIM block modulates intermediate feature maps, to adapt the style of the target modality effectively bridging the domain gap. Our proposed method allows for end-to-end training with a minimal number of paired samples. We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods. The source code and protocols for reproducing the findings will be made publicly available.
</details>
<details>
<summary>摘要</summary>
异构面Recognition（HFR）目的是匹配不同频谱的面图像，扩展面认知（FR）系统的应用场景。然而，频谱差距和目标频谱大规模数据的有限性使得从头文件训练Robust和抗变异HFR模型困难。在这种情况下，我们将不同频谱视为不同风格，并提议一种框架来适应特征地图。我们引入了一种新的Conditional Adaptive Instance Modulation（CAIM）模块，可以与预训练FR网络集成，将其转化为HFR网络。CAIM块在中间特征地图中进行调整，以有效地桥接频谱差距。我们提出的方法允许终端培训，只需要 minimal number of paired samples。我们对多个复杂的标准架进行了广泛的评估，并示出了与当前方法相比的优秀性能。源代码和 reproduce 的协议将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Self-regulating-Prompts-Foundational-Model-Adaptation-without-Forgetting"><a href="#Self-regulating-Prompts-Foundational-Model-Adaptation-without-Forgetting" class="headerlink" title="Self-regulating Prompts: Foundational Model Adaptation without Forgetting"></a>Self-regulating Prompts: Foundational Model Adaptation without Forgetting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06948">http://arxiv.org/abs/2307.06948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/muzairkhattak/promptsrc">https://github.com/muzairkhattak/promptsrc</a></li>
<li>paper_authors: Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan<br>for:PromptSRC is designed to improve the performance of prompt learning for downstream tasks while maintaining the generalization ability of the pre-trained CLIP model.methods:PromptSRC uses a self-regularization framework that includes mutual agreement maximization, self-ensemble of prompts, and textual diversity to guide the prompts to optimize for both task-specific and task-agnostic general representations.results:PromptSRC outperforms existing methods on 4 benchmarks and demonstrates better performance on downstream tasks while maintaining the generalization ability of the pre-trained CLIP model.Here is the answer in Simplified Chinese text:for: PromptSRC 是为提高下游任务的表现而设计的提示学习方法，同时保持预训练 CLIP 模型的通用能力。methods: PromptSRC 使用自我 regulatory 框架，包括协调最大化、自我集成和文本多样性来引导提示来优化任务特定和通用特征表示。results: PromptSRC 在 4 个 benchmark 上表现出色，在下游任务上表现更好，同时保持预训练 CLIP 模型的通用能力。<details>
<summary>Abstract</summary>
Prompt learning has emerged as an efficient alternative for fine-tuning foundational models, such as CLIP, for various downstream tasks. Conventionally trained using the task-specific objective, i.e., cross-entropy loss, prompts tend to overfit downstream data distributions and find it challenging to capture task-agnostic general features from the frozen CLIP. This leads to the loss of the model's original generalization capability. To address this issue, our work introduces a self-regularization framework for prompting called PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the prompts to optimize for both task-specific and task-agnostic general representations using a three-pronged approach by: (a) regulating prompted representations via mutual agreement maximization with the frozen model, (b) regulating with self-ensemble of prompts over the training trajectory to encode their complementary strengths, and (c) regulating with textual diversity to mitigate sample diversity imbalance with the visual branch. To the best of our knowledge, this is the first regularization framework for prompt learning that avoids overfitting by jointly attending to pre-trained model features, the training trajectory during prompting, and the textual diversity. PromptSRC explicitly steers the prompts to learn a representation space that maximizes performance on downstream tasks without compromising CLIP generalization. We perform extensive experiments on 4 benchmarks where PromptSRC overall performs favorably well compared to the existing methods. Our code and pre-trained models are publicly available at: https://github.com/muzairkhattak/PromptSRC.
</details>
<details>
<summary>摘要</summary>
它们发现了一种高效的替代方案，即使用适应性训练来练习基础模型，如CLIP，以适应不同的下游任务。通常通过任务特定的目标函数，即十字矩阵损失函数，来训练提示。然而，这会导致提示过拟合下游数据分布，难以捕捉基于冻结的CLIP的任务不可预测的通用特征。这会导致模型的原始泛化能力丢失。为解决这个问题，我们的工作提出了一个自regularization框架，即PromptSRC（提示自regularization）。PromptSRC使提示优化任务特定和任务无关通用表示，通过以下三个方法：（a）通过与冻结模型的共识最大化来规范提示表示，（b）通过自我集成提示训练轨迹中的提示来编码其优势，（c）通过文本多样性来抑制采样不均衡问题。根据我们所知，这是首个避免过拟合的提示学习规则框架，可以同时关注预训练模型特征、训练轨迹和文本多样性。PromptSRC显式地使提示学习一个表示空间，以最大化下游任务性能而无需妥协CLIP泛化。我们在4个标准测试集上进行了广泛的实验，并证明PromptSRC在现有方法中表现出色。我们的代码和预训练模型可以在https://github.com/muzairkhattak/PromptSRC上获取。
</details></li>
</ul>
<hr>
<h2 id="InternVid-A-Large-scale-Video-Text-Dataset-for-Multimodal-Understanding-and-Generation"><a href="#InternVid-A-Large-scale-Video-Text-Dataset-for-Multimodal-Understanding-and-Generation" class="headerlink" title="InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"></a>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06942">http://arxiv.org/abs/2307.06942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/internvideo">https://github.com/opengvlab/internvideo</a></li>
<li>paper_authors: Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao</li>
<li>for: 本研究旨在开发一个大规模的视频-文本多模态数据集，以便学习高效和可迁移的视频-文本表示，以掌握多modal理解和生成。</li>
<li>methods: 我们采用了一种可扩展的方法，利用大型自然语言模型（LLM）来自动建立高质量的视频-文本数据集，并在这个数据集上进行对比学习，以学习视频-文本表示。我们还提出了一种多尺度方法，用于生成视频相关的文本描述。</li>
<li>results: 我们的模型在视频识别和检索任务上表现出色，并且在视频-文本生成和对话系统等多modal应用中也具有广泛的应用前景。<details>
<summary>Abstract</summary>
This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce ViCLIP, a video-text representation learning model based on ViT-L. Learned on InternVid via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了InternVid，一个大规模的视频-中心多模态数据集，允许学习强大并可传递的视频-文本表示，以便多Modal理解和生成。InternVid数据集包含超过700万个视频，持续时间约为760000小时，共有23400万个视频clip， accompanied by detailed descriptions of total 4.1 billion words。我们的核心贡献是开发一种可扩展的方法，通过大语言模型（LLM）来自动建立高质量的视频-文本数据集，并在这个数据集上学习视频-语言表示。我们使用多尺度方法生成视频相关的描述。此外，我们介绍了ViCLIP，基于ViT-L的视频-文本表示学习模型。通过对InternVid进行对比学习，这个模型在零shot动作识别和视频检索方面表现出了领先的性能。除了基本的视频理解任务之外，我们的数据集和模型在广泛的应用领域。它们特别有用于生成交叠的视频-文本数据，以便学习基于视频的对话系统，以及进一步的视频-文本和文本-视频生成研究。这些提出的资源为关注多Modal视频理解和生成的研究人员和实践者提供了一个工具。
</details></li>
</ul>
<hr>
<h2 id="Animate-A-Story-Storytelling-with-Retrieval-Augmented-Video-Generation"><a href="#Animate-A-Story-Storytelling-with-Retrieval-Augmented-Video-Generation" class="headerlink" title="Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation"></a>Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06940">http://arxiv.org/abs/2307.06940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/videocrafter/animate-a-story">https://github.com/videocrafter/animate-a-story</a></li>
<li>paper_authors: Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen</li>
<li>for: 用于生成视觉故事，减少实际拍摄或动画渲染的复杂性。</li>
<li>methods: 基于现有视频clip的整合，提供可控的场景和动作Context，以及文本提示导航的 видео生成模型。</li>
<li>results: 比较现有基eline的优势，能够生成具有可控的场景和动作，以及文本提示导航的视觉故事视频。<details>
<summary>Abstract</summary>
Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts. For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure. For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters. The videos are synthesized by following the structural guidance and appearance instruction. To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts. Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines.
</details>
<details>
<summary>摘要</summary>
生成视频 для视觉Storytelling可以是一个劳碌和复杂的过程，通常需要live-action拍摄或图形动画渲染。为了绕过这些挑战，我们的关键想法是利用现有的视频剪辑和synthesize一个coherent的Storytelling视频，并通过自定义视频的场景和动作来满足用户的需求。我们的框架包括两个功能模块：1. 动作结构检索（Motion Structure Retrieval）：通过提供用户需要的场景或动作上下文的查询文本，检索可用的视频clip。2. 结构引导文本到视频synthesizer（Structure-Guided Text-to-Video Synthesis）：根据动作结构和文本提示，生成具有结构和人物控制的视频。为了实现第一个模块，我们利用了一个off-the-shelf的视频检索系统，并提取视频的深度作为动作结构。为了实现第二个模块，我们提议一种可控的视频生成模型，具有灵活的结构和人物控制。通过跟随结构指导和外观指令，我们可以生成具有视觉一致性的视频。为保证视频之间的一致性，我们提议一种有效的人物个性化方法，允许通过文本提示来定义愿望的人物标识。我们的方法在多个存在baseline的实验中表现出了显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Domain-Agnostic-Tuning-Encoder-for-Fast-Personalization-of-Text-To-Image-Models"><a href="#Domain-Agnostic-Tuning-Encoder-for-Fast-Personalization-of-Text-To-Image-Models" class="headerlink" title="Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models"></a>Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06925">http://arxiv.org/abs/2307.06925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, Amit H. Bermano</li>
<li>for: This paper is written for personalizing text-to-image (T2I) generation, allowing users to guide the creative image generation process by combining their own visual concepts in natural language prompts.</li>
<li>methods: The paper proposes a domain-agnostic method for T2I personalization that does not require any specialized dataset or prior information about the personalized concepts. The method uses a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space.</li>
<li>results: The experimental results demonstrate the effectiveness of the proposed approach, showing that the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods.Here’s the Chinese translation of the three information points:</li>
<li>for: 这篇论文是为了个性化文本到图像（T2I）生成，让用户通过将自己的视觉概念合并到自然语言提示中来导引创意图像生成过程。</li>
<li>methods: 论文提出了一种不需要特殊数据集或个性化概念知识的领域不依赖的方法，使用了一种新的对比基于的正则化技术来保持目标概念特征的高准确性，同时将预测的元素靠近 CLIP Token 的编辑区域。</li>
<li>results: 实验结果表明提出的方法的有效性，预测的元素比不正则化模型预测的元素更加 semantic，导致一个更好的表示，实现了状态的最佳性能，同时比前方法更加灵活。<details>
<summary>Abstract</summary>
Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LVLane-Deep-Learning-for-Lane-Detection-and-Classification-in-Challenging-Conditions"><a href="#LVLane-Deep-Learning-for-Lane-Detection-and-Classification-in-Challenging-Conditions" class="headerlink" title="LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions"></a>LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06853">http://arxiv.org/abs/2307.06853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zillur-av/LVLane">https://github.com/zillur-av/LVLane</a></li>
<li>paper_authors: Zillur Rahman, Brendan Tran Morris</li>
<li>for: 该论文的目的是提出一种基于深度学习的末端车道检测和分类系统，以提高自动驾驶和高级驾驶支持系统的性能。</li>
<li>methods: 该系统使用了深度学习方法，包括一个特有的数据集，以捕捉具有特殊挑战的车道检测场景。此外，该系统还提出了一种基于Convolutional Neural Networks (CNN)的分类分支，以便识别不同的车道类型。</li>
<li>results: 实验结果表明，该系统在TuSimple数据集、Caltech Lane数据集和LVLane数据集等多个数据集上具有优秀的车道检测和分类能力，特别是在面临特殊挑战的场景下表现出色。<details>
<summary>Abstract</summary>
Lane detection plays a pivotal role in the field of autonomous vehicles and advanced driving assistant systems (ADAS). Despite advances from image processing to deep learning based models, algorithm performance is highly dependent on training data matching the local challenges such as extreme lighting conditions, partially visible lane markings, and sparse lane markings like Botts' dots. To address this, we present an end-to-end lane detection and classification system based on deep learning methodologies. In our study, we introduce a unique dataset meticulously curated to encompass scenarios that pose significant challenges for state-of-the-art (SOTA) lane localization models. Moreover, we propose a CNN-based classification branch, seamlessly integrated with the detector, facilitating the identification of distinct lane types. This architecture enables informed lane-changing decisions and empowers more resilient ADAS capabilities. We also investigate the effect of using mixed precision training and testing on different models and batch sizes. Experimental evaluations conducted on the widely-used TuSimple dataset, Caltech Lane dataset, and our LVLane dataset demonstrate the effectiveness of our model in accurately detecting and classifying lanes amidst challenging scenarios. Our method achieves state-of-the-art classification results on the TuSimple dataset. The code of the work can be found on www.github.com/zillur-av/LVLane.
</details>
<details>
<summary>摘要</summary>
lane detection 在自动驾驶和高级驾驶助手系统（ADAS）中扮演着关键的角色。尽管从图像处理演化到深度学习基于模型，但算法性能仍然受到本地挑战的影响，如极端光照条件、部分可见的车道标记和罕见的车道标记如Botts的点。为解决这一问题，我们提出了一个端到端的车道检测和分类系统，基于深度学习方法。在我们的研究中，我们提供了一个独特的数据集，仔细筛选了表现出特殊挑战的场景，以便为现有的SOTA车道local化模型提供更加准确的挑战。此外，我们还提出了一种基于CNN的分类分支，与检测器紧密集成，以便识别不同的车道类型。这种架构允许更加有知识的车道更改决策，激发更加可靠的ADAS功能。我们还进行了不同模型和批处理大小的混合精度训练和测试的研究。实验结果在广泛使用的TuSimple数据集、Caltech Lane数据集和我们的LVLane数据集上展示了我们的模型在面临挑战场景下准确检测和分类车道的能力。我们的方法在TuSimple数据集上实现了SOTA的分类结果。代码可以在www.github.com/zillur-av/LVLane中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/14/cs.CV_2023_07_14/" data-id="clot2mhbu00f5x788fevhf66e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_14" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/14/cs.AI_2023_07_14/" class="article-date">
  <time datetime="2023-07-14T12:00:00.000Z" itemprop="datePublished">2023-07-14</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/14/cs.AI_2023_07_14/">cs.AI - 2023-07-14</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Exploiting-Counter-Examples-for-Active-Learning-with-Partial-labels"><a href="#Exploiting-Counter-Examples-for-Active-Learning-with-Partial-labels" class="headerlink" title="Exploiting Counter-Examples for Active Learning with Partial labels"></a>Exploiting Counter-Examples for Active Learning with Partial labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07413">http://arxiv.org/abs/2307.07413</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ferenas/APLL">https://github.com/Ferenas/APLL</a></li>
<li>paper_authors: Fei Zhang, Yunjie Ye, Lei Feng, Zhongwen Rao, Jieming Zhu, Marcus Kalander, Chen Gong, Jianye Hao, Bo Han</li>
<li>for: 这个论文研究了一个新的问题，即活动学习 WITH 部分标签（ALPL）。在这种设定下，一个oracle对查询样本提供部分标签，从而放宽oracle的准确标签过程。</li>
<li>methods: 我们首先构建了一个直观的基线，可以轻松地整合到现有的AL框架中。虽然有效，但这个基eline仍然受到过拟合的影响，并且在选择代表样本时缺乏表达partial-label-based samples的能力。我们从人类的认知科学中灵感，where accurate inferences can be explicitly derived from counter-examples (CEs)，我们的目标是使用这种人类学习模式来解决过拟合问题，同时提高选择代表样本的过程。</li>
<li>results: 我们的方法在五个实际数据集和四个benchmark数据集上实现了全面的改进，超过了十个代表性的AL框架。这些实验结果表明，我们提议的方法可以增强predictor的性能和选择代表样本的过程，使predictor能够更准确地捕捉数据中的征性 Patterns。<details>
<summary>Abstract</summary>
This paper studies a new problem, \emph{active learning with partial labels} (ALPL). In this setting, an oracle annotates the query samples with partial labels, relaxing the oracle from the demanding accurate labeling process. To address ALPL, we first build an intuitive baseline that can be seamlessly incorporated into existing AL frameworks. Though effective, this baseline is still susceptible to the \emph{overfitting}, and falls short of the representative partial-label-based samples during the query process. Drawing inspiration from human inference in cognitive science, where accurate inferences can be explicitly derived from \emph{counter-examples} (CEs), our objective is to leverage this human-like learning pattern to tackle the \emph{overfitting} while enhancing the process of selecting representative samples in ALPL. Specifically, we construct CEs by reversing the partial labels for each instance, and then we propose a simple but effective WorseNet to directly learn from this complementary pattern. By leveraging the distribution gap between WorseNet and the predictor, this adversarial evaluation manner could enhance both the performance of the predictor itself and the sample selection process, allowing the predictor to capture more accurate patterns in the data. Experimental results on five real-world datasets and four benchmark datasets show that our proposed method achieves comprehensive improvements over ten representative AL frameworks, highlighting the superiority of WorseNet. The source code will be available at \url{https://github.com/Ferenas/APLL}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CAMP-A-Context-Aware-Cricket-Players-Performance-Metric"><a href="#CAMP-A-Context-Aware-Cricket-Players-Performance-Metric" class="headerlink" title="CAMP: A Context-Aware Cricket Players Performance Metric"></a>CAMP: A Context-Aware Cricket Players Performance Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13700">http://arxiv.org/abs/2307.13700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sohaibayub/camp">https://github.com/sohaibayub/camp</a></li>
<li>paper_authors: Muhammad Sohaib Ayub, Naimat Ullah, Sarwan Ali, Imdad Ullah Khan, Mian Muhammad Awais, Muhammad Asad Khan, Safiullah Faizullah</li>
<li>for: 本研究旨在提供一个 Context-Aware Metric of player Performance (CAMP)，用于评估印度式板球运动员的个人表现。</li>
<li>methods: 本研究使用数据挖掘技术，包括对限定的一对板球比赛数据进行分析，以提取有关运动员表现的特征。</li>
<li>results: 研究发现，CAMP评估结果与专家委员会宣布的最佳球员（Man of the Match，MoM）相closely match，在961场比赛中，CAMP评估的top两名球员与MoM一致的比例为83%。此外，CAMP的评估结果也超过了现有的最佳球员贡献度量方法（基于Duckworth-Lewis-Stern方法）。<details>
<summary>Abstract</summary>
Cricket is the second most popular sport after soccer in terms of viewership. However, the assessment of individual player performance, a fundamental task in team sports, is currently primarily based on aggregate performance statistics, including average runs and wickets taken. We propose Context-Aware Metric of player Performance, CAMP, to quantify individual players' contributions toward a cricket match outcome. CAMP employs data mining methods and enables effective data-driven decision-making for selection and drafting, coaching and training, team line-ups, and strategy development. CAMP incorporates the exact context of performance, such as opponents' strengths and specific circumstances of games, such as pressure situations. We empirically evaluate CAMP on data of limited-over cricket matches between 2001 and 2019. In every match, a committee of experts declares one player as the best player, called Man of the M}atch (MoM). The top two rated players by CAMP match with MoM in 83\% of the 961 games. Thus, the CAMP rating of the best player closely matches that of the domain experts. By this measure, CAMP significantly outperforms the current best-known players' contribution measure based on the Duckworth-Lewis-Stern (DLS) method.
</details>
<details>
<summary>摘要</summary>
крикет是以往最受欢迎的运动之一，仅次于足球。然而，评估个体运动员的表现，是现代团队运动中的基本任务，目前主要基于汇总性统计，包括平均得分和帮助取下。我们提出了 Context-Aware Metric of player Performance（CAMP），用于衡量 крикет运动员的贡献。CAMP 利用数据挖掘技术，实现了有效的数据驱动决策，包括选择和招募、教练和训练、队列和战略开发。CAMP 包含了特定场景的表现，如对手的优势和特定游戏情况，如压力情况。我们对数据库中的限制时间 крикет比赛数据进行了Empirical评估。每场比赛中，专家委员会选择一名最佳运动员，称为 Man of the Match（MoM）。CAMP 中的前两名与 MoM 匹配在 83% 的 961 场比赛中。因此，CAMP 评分的最佳运动员与领域专家的评价相当接近。根据这个标准，CAMP 明显超过了目前最佳运动员贡献的度量方法，基于 Duckworth-Lewis-Stern（DLS）方法。
</details></li>
</ul>
<hr>
<h2 id="Rank-Your-Summaries-Enhancing-Bengali-Text-Summarization-via-Ranking-based-Approach"><a href="#Rank-Your-Summaries-Enhancing-Bengali-Text-Summarization-via-Ranking-based-Approach" class="headerlink" title="Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach"></a>Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07392">http://arxiv.org/abs/2307.07392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonmoytalukder/rank-your-summaries-enhancing-bengali-text-summarization-via-ranking-based-approach">https://github.com/tonmoytalukder/rank-your-summaries-enhancing-bengali-text-summarization-via-ranking-based-approach</a></li>
<li>paper_authors: G. M. Shahariar, Tonmoy Talukder, Rafin Alam Khan Sotez, Md. Tanvir Rouf Shawon</li>
<li>for: 本研究旨在提高 Bengali 文本摘要的效果和准确性，通过扩展和改进现有的预训练模型。</li>
<li>methods: 本研究使用四种预训练 Bengali 文本摘要模型，并应用文本排名算法来选择最佳摘要。</li>
<li>results: 实验结果表明，通过 combinig 每种预训练模型的优点并使用排名算法，本方法可以显著提高 Bengali 文本摘要的准确性和效果。<details>
<summary>Abstract</summary>
With the increasing need for text summarization techniques that are both efficient and accurate, it becomes crucial to explore avenues that enhance the quality and precision of pre-trained models specifically tailored for summarizing Bengali texts. When it comes to text summarization tasks, there are numerous pre-trained transformer models at one's disposal. Consequently, it becomes quite a challenge to discern the most informative and relevant summary for a given text among the various options generated by these pre-trained summarization models. This paper aims to identify the most accurate and informative summary for a given text by utilizing a simple but effective ranking-based approach that compares the output of four different pre-trained Bengali text summarization models. The process begins by carrying out preprocessing of the input text that involves eliminating unnecessary elements such as special characters and punctuation marks. Next, we utilize four pre-trained summarization models to generate summaries, followed by applying a text ranking algorithm to identify the most suitable summary. Ultimately, the summary with the highest ranking score is chosen as the final one. To evaluate the effectiveness of this approach, the generated summaries are compared against human-annotated summaries using standard NLG metrics such as BLEU, ROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that by leveraging the strengths of each pre-trained transformer model and combining them using a ranking-based approach, our methodology significantly improves the accuracy and effectiveness of the Bengali text summarization.
</details>
<details>
<summary>摘要</summary>
随着需要高效精准的文本摘要技术的增加，对适应 Bengali 文本摘要模型进行特点化成本的探索变得非常重要。在文本摘要任务中，有许多预训练的转换器模型可供选择。因此，对于给定的文本，从多个预训练摘要模型中选择最佳的摘要成为了一项挑战。本文提出了一种简单 yet effective 的排名基于方法，该方法通过对四个预训练 Bengali 文本摘要模型的输出进行比较，以确定最佳的摘要。该过程包括对输入文本进行简化处理，从而消除不必要的元素，如特殊字符和标点符号。接着，我们利用四个预训练摘要模型生成摘要，然后应用文本排名算法来确定最佳摘要。最后，根据排名得分，选择最高分的摘要作为最终结果。为了评估该方法的有效性，我们将生成的摘要与人工标注的摘要进行比较，使用标准的NLG指标，如 BLEU、ROUGE、BERTScore、WIL、WER 和 METEOR。实验结果表明，通过利用每个预训练转换器模型的优点并将其结合使用排名基于方法，我们的方法可以显著提高 Bengali 文本摘要的准确性和效iveness。
</details></li>
</ul>
<hr>
<h2 id="AIC-AB-NET-A-Neural-Network-for-Image-Captioning-with-Spatial-Attention-and-Text-Attributes"><a href="#AIC-AB-NET-A-Neural-Network-for-Image-Captioning-with-Spatial-Attention-and-Text-Attributes" class="headerlink" title="AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes"></a>AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07370">http://arxiv.org/abs/2307.07370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoyun Tu, Ying Liu, Vladimir Vlassov</li>
<li>For: 本研究旨在提出和实现一种Attribute-Information-Combined Attention-Based Network（AIC-AB NET），用于图像描述 generation。* Methods: 该模型结合了空间注意力架构和文本特征信息，并在encoder-decoder结构中进行了可适应的注意力调整。* Results: 对于 MS COCO 数据集和我们新提出的 Fashion 数据集，我们的 AIC-AB NET 与基eline模型和减少模型进行了比较，并取得了更高的性能水平。<details>
<summary>Abstract</summary>
Image captioning is a significant field across computer vision and natural language processing. We propose and present AIC-AB NET, a novel Attribute-Information-Combined Attention-Based Network that combines spatial attention architecture and text attributes in an encoder-decoder. For caption generation, adaptive spatial attention determines which image region best represents the image and whether to attend to the visual features or the visual sentinel. Text attribute information is synchronously fed into the decoder to help image recognition and reduce uncertainty. We have tested and evaluated our AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The Fashion dataset is employed as a benchmark of single-object images. The results show the superior performance of the proposed model compared to the state-of-the-art baseline and ablated models on both the images from MSCOCO and our single-object images. Our AIC-AB NET outperforms the baseline adaptive attention network by 0.017 (CIDEr score) on the MS COCO dataset and 0.095 (CIDEr score) on the Fashion dataset.
</details>
<details>
<summary>摘要</summary>
“图像描述是计算机视觉和自然语言处理领域中的一个重要领域。我们提议并提出了一种新的 Attribute-Information-Combined Attention-Based Network（AIC-AB NET），该模型结合了空间注意架构和文本特征在encoder-decoder中组合。在图像描述中，适应的空间注意可以确定图像中最好代表图像的区域，以及是否需要关注视觉特征或视觉特征。文本特征信息同时被 fed into decoder，以帮助图像识别和减少不确定性。我们在 MS COCO 数据集和我们提出的新的时尚数据集上测试和评估了我们的 AICAB NET。时尚数据集被用作单个物体图像的标准 benchmark。结果表明我们的提议模型与状态之前的基eline和剥离模型在 MS COCO 数据集和时尚数据集上的表现都有所提高，分别提高了0.017（CIDEr 分数）和0.095（CIDEr 分数）。”
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-a-Threat-to-Digital-Public-Goods-Evidence-from-Activity-on-Stack-Overflow"><a href="#Are-Large-Language-Models-a-Threat-to-Digital-Public-Goods-Evidence-from-Activity-on-Stack-Overflow" class="headerlink" title="Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow"></a>Are Large Language Models a Threat to Digital Public Goods? Evidence from Activity on Stack Overflow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07367">http://arxiv.org/abs/2307.07367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria del Rio-Chanona, Nadzeya Laurentsyeva, Johannes Wachs</li>
<li>for:  investigate how the release of ChatGPT affects human-generated open data on the web, specifically on Stack Overflow</li>
<li>methods: analyze activity on Stack Overflow, use difference-in-differences model to estimate the impact of ChatGPT</li>
<li>results: find a 16% decrease in weekly posts on Stack Overflow after the release of ChatGPT, with a greater impact on posts related to the most widely used programming languages, and no significant change in voting scores.<details>
<summary>Abstract</summary>
Large language models like ChatGPT efficiently provide users with information about various topics, presenting a potential substitute for searching the web and asking people for help online. But since users interact privately with the model, these models may drastically reduce the amount of publicly available human-generated data and knowledge resources. This substitution can present a significant problem in securing training data for future models. In this work, we investigate how the release of ChatGPT changed human-generated open data on the web by analyzing the activity on Stack Overflow, the leading online Q\&A platform for computer programming. We find that relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable, activity on Stack Overflow significantly decreased. A difference-in-differences model estimates a 16\% decrease in weekly posts on Stack Overflow. This effect increases in magnitude over time, and is larger for posts related to the most widely used programming languages. Posts made after ChatGPT get similar voting scores than before, suggesting that ChatGPT is not merely displacing duplicate or low-quality content. These results suggest that more users are adopting large language models to answer questions and they are better substitutes for Stack Overflow for languages for which they have more training data. Using models like ChatGPT may be more efficient for solving certain programming problems, but its widespread adoption and the resulting shift away from public exchange on the web will limit the open data people and models can learn from in the future.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型如ChatGPT有效地为用户提供了关于不同话题的信息，可能成为搜索网络和在线问答的替代方案。但是由于用户与模型进行私人交互，这些模型可能会减少公开可用的人类生成的数据和知识资源。这种替换可能会对未来模型的训练数据产生重大问题。在这项工作中，我们研究了ChatGPT的发布后对网络上的人类生成开放数据的影响，通过分析Stack Overflow网站的活动。我们发现，相比于其俄罗斯和中国的对手， Stack Overflow上的活动呈现下降趋势。此外，与数学相关的讨论forum不同，Stack Overflow上的活动呈现更大的下降趋势。使用差异分析模型，我们估算Stack Overflow每周的帖子数减少16%。这个效果随时间的推移而增强，并且对于使用最广泛的编程语言的帖子更大。帖子发布后得到了与之前相同的投票分数，表明ChatGPT不仅不是替换低质量或 duplicates的内容，而且是更好的解决某些编程问题的模型。这些结果表明，更多的用户正在使用大语言模型来回答问题，而且它们对于某些编程语言来说是更好的替代方案。使用模型如ChatGPT可能更高效地解决某些编程问题，但是其广泛的采用和相应的减少公开数据将限制未来模型和人类可以学习的开放数据。
</details></li>
</ul>
<hr>
<h2 id="Source-Free-Domain-Adaptation-with-Temporal-Imputation-for-Time-Series-Data"><a href="#Source-Free-Domain-Adaptation-with-Temporal-Imputation-for-Time-Series-Data" class="headerlink" title="Source-Free Domain Adaptation with Temporal Imputation for Time Series Data"></a>Source-Free Domain Adaptation with Temporal Imputation for Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07542">http://arxiv.org/abs/2307.07542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mohamedr002/mapu_sfda_ts">https://github.com/mohamedr002/mapu_sfda_ts</a></li>
<li>paper_authors: Mohamed Ragab, Emadeldeen Eldele, Min Wu, Chuan-Sheng Foo, Xiaoli Li, Zhenghua Chen</li>
<li>for: 这个研究的目的是为时间序列资料进行源自由领域适应（SFDA），以保持源领域资料的隐私。</li>
<li>methods: 本研究使用的方法是MAsk和imPUte（MAPU），具有以下两个主要 componenets：首先，使用随机填写来捕捉时间序列资料的时间信息，然后使用一个新的时间填写器来将原始信号从填写后的嵌入空间中恢复原始信号。其次，在适应步骤中，使用填写器网络导引目标模型生成目标特征，以确保时间相依性。</li>
<li>results: 实验结果显示， compared to现有的方法，MAPU在三个真实世界时间序列数据集上 achieves significant performance gain。<details>
<summary>Abstract</summary>
Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a labeled source domain to an unlabeled target domain without access to the source domain data, preserving source domain privacy. Despite its prevalence in visual applications, SFDA is largely unexplored in time series applications. The existing SFDA methods that are mainly designed for visual applications may fail to handle the temporal dynamics in time series, leading to impaired adaptation performance. To address this challenge, this paper presents a simple yet effective approach for source-free domain adaptation on time series data, namely MAsk and imPUte (MAPU). First, to capture temporal information of the source domain, our method performs random masking on the time series signals while leveraging a novel temporal imputer to recover the original signal from a masked version in the embedding space. Second, in the adaptation step, the imputer network is leveraged to guide the target model to produce target features that are temporally consistent with the source features. To this end, our MAPU can explicitly account for temporal dependency during the adaptation while avoiding the imputation in the noisy input space. Our method is the first to handle temporal consistency in SFDA for time series data and can be seamlessly equipped with other existing SFDA methods. Extensive experiments conducted on three real-world time series datasets demonstrate that our MAPU achieves significant performance gain over existing methods. Our code is available at \url{https://github.com/mohamedr002/MAPU_SFDA_TS}.
</details>
<details>
<summary>摘要</summary>
源自由领域适应 (SFDA) 目标是将预训练的源频率模型适应到无标记目标频率频率上，保持源频率隐私。尽管 SFDA 在视觉应用中广泛存在，但在时间序列应用中它尚未得到充分研究。现有的 SFDA 方法主要是为视觉应用设计，可能无法处理时间序列中的时间动态，导致适应性下降。为解决这个挑战，本文提出了一种简单 yet 有效的时间序列 SFDA 方法，即 Maske 和 imPUte (MAPU)。首先，为了捕捉源频率频率中的时间信息，我们的方法在时间序列信号上随机填充，并利用一种新的时间填充器来在嵌入空间中恢复原始信号。其次，在适应步骤中，填充器网络被利用来引导目标模型生成目标特征，使其与源特征在时间上具有一致性。这样，我们的 MAPU 可以在适应过程中考虑时间相互关系，而不是在噪音输入空间中进行填充。我们的方法是时间序列 SFDA 中首次考虑时间一致性的方法，可以顺利地与其他现有的 SFDA 方法结合使用。我们的实验结果表明，使用 MAPU 可以在三个实际的时间序列数据集上实现显著的性能提升。我们的代码可以在 \url{https://github.com/mohamedr002/MAPU_SFDA_TS} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Trust-Repair-in-Human-Robot-Interaction"><a href="#Rethinking-Trust-Repair-in-Human-Robot-Interaction" class="headerlink" title="Rethinking Trust Repair in Human-Robot Interaction"></a>Rethinking Trust Repair in Human-Robot Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11763">http://arxiv.org/abs/2307.11763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Connor Esterwood</li>
<li>for: 本研究旨在发现人机合作中机器人的信任维护策略，以及这些策略的成功机制。</li>
<li>methods: 本研究使用了人机交互中的信任修复策略，并探讨了这些策略的基本原理和成功因素。</li>
<li>results: 本研究提供了人机交互中信任修复策略的概念和关键组件，以及现有的研究成果。未来的研究将围绕着这些研究问题进行发展。<details>
<summary>Abstract</summary>
As robots become increasingly prevalent in work-oriented collaborations, trust has emerged as a critical factor in their acceptance and effectiveness. However, trust is dynamic and can erode when mistakes are made. Despite emerging research on trust repair in human-robot interaction, significant questions remain about identifying reliable approaches to restoring trust in robots after trust violations occur. To address this problem, my research aims to identify effective strategies for designing robots capable of trust repair in human-robot interaction (HRI) and to explore the underlying mechanisms that make these strategies successful. This paper provides an overview of the fundamental concepts and key components of the trust repair process in HRI, as well as a summary of my current published work in this area. Additionally, I discuss the research questions that will guide my future work and the potential contributions that this research could make to the field.
</details>
<details>
<summary>摘要</summary>
As robots become increasingly prevalent in work-oriented collaborations, trust has emerged as a critical factor in their acceptance and effectiveness. However, trust is dynamic and can erode when mistakes are made. Despite emerging research on trust repair in human-robot interaction, significant questions remain about identifying reliable approaches to restoring trust in robots after trust violations occur. To address this problem, my research aims to identify effective strategies for designing robots capable of trust repair in human-robot interaction (HRI) and to explore the underlying mechanisms that make these strategies successful. This paper provides an overview of the fundamental concepts and key components of the trust repair process in HRI, as well as a summary of my current published work in this area. Additionally, I discuss the research questions that will guide my future work and the potential contributions that this research could make to the field.Here's the translation in Traditional Chinese:随着机器人在工作合作中的普及，信任已成为 kritical factor 的 acceptance 和 efficiency。然而，信任是动态的，可以在错误时被损坏。尽管人机交互中的信任修复研究已经出现，但仍有很多问题需要解决，包括如何确定可靠的方法来重建机器人信任。为了解决这个问题，我的研究目标是发现可靠的机器人信任修复策略，并探索这些策略的成功关键。这篇文章提供了人机交互中信任修复过程的基本概念和关键组件，以及我的现有发表作品。此外，我还讨论了未来研究的问题和这些研究对领域的潜在贡献。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Bias-in-Conversations-A-Hate-Speech-Classifier-and-Debiaser-with-Prompts"><a href="#Mitigating-Bias-in-Conversations-A-Hate-Speech-Classifier-and-Debiaser-with-Prompts" class="headerlink" title="Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts"></a>Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10213">http://arxiv.org/abs/2307.10213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaina Raza, Chen Ding, Deval Pandya</li>
<li>for: 降低在对话中存在的歧视语言和偏见，以避免对 Target 群体（如种族、性别和宗教）产生负面影响。</li>
<li>methods: 提议一种两步方法：首先，使用分类器检测 hate speech；然后，通过提示生成更不偏见或不偏见的替代语言。</li>
<li>results: 对一个标准数据集进行评估，观察到 hate speech 的负面效果减少。 这种方法可以帮助在线对话中减少偏见，创造更公正和包容的沟通环境。<details>
<summary>Abstract</summary>
Discriminatory language and biases are often present in hate speech during conversations, which usually lead to negative impacts on targeted groups such as those based on race, gender, and religion. To tackle this issue, we propose an approach that involves a two-step process: first, detecting hate speech using a classifier, and then utilizing a debiasing component that generates less biased or unbiased alternatives through prompts. We evaluated our approach on a benchmark dataset and observed reduction in negativity due to hate speech comments. The proposed method contributes to the ongoing efforts to reduce biases in online discourse and promote a more inclusive and fair environment for communication.
</details>
<details>
<summary>摘要</summary>
【文本】恐Speech hate speech在对话中经常带有歧视性语言和偏见，这通常会对targeted groups造成负面影响，如基于种族、性别和宗教的群体。为解决这个问题，我们提出了一种两步方法：首先，使用分类器探测 hate speech，然后使用debiasing组件生成less biased或无偏见的 altenatives through prompts。我们对 benchmark dataset进行了评估，并观察到了对 hate speech负面评论的减少。该方法对在线对话中减少偏见和促进更加包容和公正的环境做出了贡献。Note: "恐Speech" is a combination of "恐怖" (terror) and "Speech" (speech), which is a common term used to refer to hate speech in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-With-Hidden-Unit-Clustering-For-Low-Resource-Speech-Applications"><a href="#Representation-Learning-With-Hidden-Unit-Clustering-For-Low-Resource-Speech-Applications" class="headerlink" title="Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications"></a>Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07325">http://arxiv.org/abs/2307.07325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varun Krishna, Tarun Sai, Sriram Ganapathy</li>
<li>for: 这 paper 是关于无文本资源的语音表征学研究，具体是用 hidden unit clustering (HUC) 框架来实现自我超参的 speech 表征学习。</li>
<li>methods: 输入 audio 样本经 windowing 和 1-D convolutional layers 处理，然后使用 long short term memory (LSTM) 层生成每个窗口段的上下文向量表示。HUC 框架用于训练模型学习具有含义的 speech 表示。</li>
<li>results: 在 ZeroSpeech 2021 挑战中 Completely Unsupervised 语音应用中，以及 TIMIT 数据集和 GramVaani Hindi 数据集上的 semi-supervised automatic speech recognition (ASR) 应用中，模型 achieved state-of-art 结果。另外，HUC 表示在 ASR 实验中显著提高了对比 Wav2vec、HuBERT 和 Best-RQ 的成果。<details>
<summary>Abstract</summary>
The representation learning of speech, without textual resources, is an area of significant interest for many low resource speech applications. In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned "time-frequency" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative k-means algorithm. We explore techniques that improve the speaker invariance of the learned representations and illustrate the effectiveness of the proposed approach on two settings, i) completely unsupervised speech applications on the sub-tasks described as part of the ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition (ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi dataset. In these experiments, we achieve state-of-art results for various ZeroSpeech tasks. Further, on the ASR experiments, the HUC representations are shown to improve significantly over other established benchmarks based on Wav2vec, HuBERT and Best-RQ.
</details>
<details>
<summary>摘要</summary>
“对于无文本资源的语音识别，是一个具有很大的研究 интерес的领域。在这篇文章中，我们描述了一种对于类似时间频率的自我监督学习方法，使用隐藏单元聚合（HUC）框架。输入模型包含了对于每个时间频率范围的窗口处理和1-D卷积层。从卷积层获得的“时间频率”表现被进一步处理，并使用长期内存（LSTM）层生成每个窗口段的内容vector表现。HUC框架允许将表现分为一小数量的语音单元，并在这些单元上进行训练，以学习具有 semantic richness 的语音表现。目标包括每个音频段的语音单元 pseudo-标签，这些标签是使用迭代k-means算法生成的。我们探索了提高话者不变的技术，并证明了我们的方法在两个设定下具有优秀的效果：完全无监督语音应用程序中的ZeroSpeech 2021挑战和半监督自动语音识别（ASR）应用程序中的TIMITdataset和GramVaani挑战Hindi dataset。在这些实验中，我们取得了顶尖的成绩，并在不同的 ZeroSpeech 任务中获得了州内最佳的成绩。此外，在 ASR 实验中，HUC 表现优化了与其他已知的参考标准Wav2vec、HuBERT和Best-RQ相比。”
</details></li>
</ul>
<hr>
<h2 id="C3-Zero-shot-Text-to-SQL-with-ChatGPT"><a href="#C3-Zero-shot-Text-to-SQL-with-ChatGPT" class="headerlink" title="C3: Zero-shot Text-to-SQL with ChatGPT"></a>C3: Zero-shot Text-to-SQL with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07306">http://arxiv.org/abs/2307.07306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bigbigwatermalon/c3sql">https://github.com/bigbigwatermalon/c3sql</a></li>
<li>paper_authors: Xuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, lu Chen, Jinshu Lin, Dongfang Lou</li>
<li>for: 提出了一种基于ChatGPT的零shot Text-to-SQL方法，以提高Text-to-SQL的执行精度。</li>
<li>methods: 方法包括三个关键组件：Clear Prompting (CP)、Calibration with Hints (CH)和Consistent Output (CO)，用于处理模型输入、模型偏见和模型输出。</li>
<li>results: 在Spider Challenge的备用测试集上达到了82.3%的执行精度，成为零shot Text-to-SQL领域的状态精度。<details>
<summary>Abstract</summary>
This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3, which achieves 82.3\% in terms of execution accuracy on the holdout test set of Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the Spider Challenge. C3 consists of three key components: Clear Prompting (CP), Calibration with Hints (CH), and Consistent Output (CO), which are corresponding to the model input, model bias and model output respectively. It provides a systematic treatment for zero-shot Text-to-SQL. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposed method.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了基于ChatGPT的零次 Text-to-SQL方法，名为C3，其在Spider挑战的备用测试集上达到了82.3%的执行精度，成为零次 Text-to-SQL领域的状态之一。C3包括三个关键组件： Clear Prompting（CP）、Calibration with Hints（CH）和Consistent Output（CO），它们分别对应模型输入、模型偏好和模型输出。它提供了零次 Text-to-SQL的系统性处理方法。我们进行了广泛的实验来证明我们提出的方法的有效性和效率。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Action-Recognition-via-Multi-Scale-Spatial-Temporal-Skeleton-Matching"><a href="#One-Shot-Action-Recognition-via-Multi-Scale-Spatial-Temporal-Skeleton-Matching" class="headerlink" title="One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching"></a>One-Shot Action Recognition via Multi-Scale Spatial-Temporal Skeleton Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07286">http://arxiv.org/abs/2307.07286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Yang, Jun Liu, Shijian Lu, Er Meng Hwa, Alex C. Kot</li>
<li>for: 一shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample.</li>
<li>methods: 使用多scale spatial-temporal feature matching to handle skeleton action recognition, representing skeleton data at multiple spatial and temporal scales and achieving optimal feature matching from two perspectives.</li>
<li>results: 在三个大规模数据集（NTU RGB+D、NTU RGB+D 120、PKU-MMD）上进行了广泛的实验，得到了superior的一shot skeleton action recognition结果，并与状态对比大幅提高了性能。<details>
<summary>Abstract</summary>
One-shot skeleton action recognition, which aims to learn a skeleton action recognition model with a single training sample, has attracted increasing interest due to the challenge of collecting and annotating large-scale skeleton action data. However, most existing studies match skeleton sequences by comparing their feature vectors directly which neglects spatial structures and temporal orders of skeleton data. This paper presents a novel one-shot skeleton action recognition technique that handles skeleton action recognition via multi-scale spatial-temporal feature matching. We represent skeleton data at multiple spatial and temporal scales and achieve optimal feature matching from two perspectives. The first is multi-scale matching which captures the scale-wise semantic relevance of skeleton data at multiple spatial and temporal scales simultaneously. The second is cross-scale matching which handles different motion magnitudes and speeds by capturing sample-wise relevance across multiple scales. Extensive experiments over three large-scale datasets (NTU RGB+D, NTU RGB+D 120, and PKU-MMD) show that our method achieves superior one-shot skeleton action recognition, and it outperforms the state-of-the-art consistently by large margins.
</details>
<details>
<summary>摘要</summary>
一shot skeleton action recognition技术，即通过单个训练样本学习skeleton action recognition模型，已经引起了越来越多的关注，因为收集和标注大规模skeleton action数据是一项挑战。然而，大多数现有研究都是通过直接比较skeleton序列的特征向量来匹配skeleton数据，这种方法忽略了skeleton数据的空间结构和时间顺序。本文提出了一种新的一shot skeleton action recognition技术，通过多级空间-时间特征匹配来处理skeleton action recognition。我们将skeleton数据表示在多个空间和时间级别，并实现最佳的特征匹配从两个方面。第一是多级匹配，同时捕捉多个空间和时间级别的scale-wise semantic relevance。第二是交叉级别匹配，处理不同的动作幅度和速度，通过捕捉不同级别的sample-wise relevance。我们在NTU RGB+D、NTU RGB+D 120和PKU-MMD三个大规模数据集上进行了广泛的实验，结果表明，我们的方法可以实现superior的一shot skeleton action recognition，并在相对评价中减少了state-of-the-art的差距。
</details></li>
</ul>
<hr>
<h2 id="AudioInceptionNeXt-TCL-AI-LAB-Submission-to-EPIC-SOUND-Audio-Based-Interaction-Recognition-Challenge-2023"><a href="#AudioInceptionNeXt-TCL-AI-LAB-Submission-to-EPIC-SOUND-Audio-Based-Interaction-Recognition-Challenge-2023" class="headerlink" title="AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023"></a>AudioInceptionNeXt: TCL AI LAB Submission to EPIC-SOUND Audio-Based-Interaction-Recognition Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07265">http://arxiv.org/abs/2307.07265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevenlauhkhk/audioinceptionnext">https://github.com/stevenlauhkhk/audioinceptionnext</a></li>
<li>paper_authors: Kin Wai Lau, Yasar Abbas Ur Rehman, Yuyang Xie, Lan Ma</li>
<li>for: 本研究投稿2023年Epic-Kitchen EPIC-SOUNDS音频基于交互识别挑战，目标是学习听取样本与其相应的动作标签之间的映射。</li>
<li>methods: 我们提出了一种简单 yet有效的单流Convolutional Neural Network（CNN）建模方法，即AudioInceptionNeXt，该方法在时域-频谱径Log-Mel spectrogram上运行。我们受InceptionNeXt的设计启发，提出了并行多级深度分解核心，以便模型更好地学习时间和频率信息。</li>
<li>results: 我们的方法在挑战测试集上达到55.43%的排名第一的top-1准确率，代码可以匿名获取于<a target="_blank" rel="noopener" href="https://github.com/StevenLauHKHK/AudioInceptionNeXt.git%E3%80%82">https://github.com/StevenLauHKHK/AudioInceptionNeXt.git。</a><details>
<summary>Abstract</summary>
This report presents the technical details of our submission to the 2023 Epic-Kitchen EPIC-SOUNDS Audio-Based Interaction Recognition Challenge. The task is to learn the mapping from audio samples to their corresponding action labels. To achieve this goal, we propose a simple yet effective single-stream CNN-based architecture called AudioInceptionNeXt that operates on the time-frequency log-mel-spectrogram of the audio samples. Motivated by the design of the InceptionNeXt, we propose parallel multi-scale depthwise separable convolutional kernels in the AudioInceptionNeXt block, which enable the model to learn the time and frequency information more effectively. The large-scale separable kernels capture the long duration of activities and the global frequency semantic information, while the small-scale separable kernels capture the short duration of activities and local details of frequency information. Our approach achieved 55.43% of top-1 accuracy on the challenge test set, ranked as 1st on the public leaderboard. Codes are available anonymously at https://github.com/StevenLauHKHK/AudioInceptionNeXt.git.
</details>
<details>
<summary>摘要</summary>
这份报告介绍我们在2023年 Epic-Kitchen EPIC-SOUNDS 音频基于交互识别挑战中的提交技术细节。任务是学习音频示例与其相应的动作标签之间的映射。为达到这个目标，我们提议一种简单 yet effective的单流Convolutional Neural Network（CNN）架构 AudioInceptionNeXt，该架构在时域-频谱幅的Log-Mel spectrogram上运行。受inceptionNeXt的设计启发，我们提议在AudioInceptionNeXt块中并行执行多尺度分割的深度独立 convolutional 核，这使得模型能够更有效地学习时间和频谱信息。大规模分割核捕捉活动的长期持续和全球频谱 semantic 信息，而小规模分割核捕捉活动的短期持续和本地频谱信息。我们的方法在挑战测试集上达到55.43%的 top-1 准确率，排名公共排行板上第一名。代码可以在https://github.com/StevenLauHKHK/AudioInceptionNeXt.git anonymous 上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Dynamic-Points-Removal-Benchmark-in-Point-Cloud-Maps"><a href="#A-Dynamic-Points-Removal-Benchmark-in-Point-Cloud-Maps" class="headerlink" title="A Dynamic Points Removal Benchmark in Point Cloud Maps"></a>A Dynamic Points Removal Benchmark in Point Cloud Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07260">http://arxiv.org/abs/2307.07260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kth-rpl/dynamicmap_benchmark">https://github.com/kth-rpl/dynamicmap_benchmark</a></li>
<li>paper_authors: Qingwen Zhang, Daniel Duberg, Ruoyu Geng, Mingkai Jia, Lujia Wang, Patric Jensfelt</li>
<li>for: 本研究旨在提供一个易扩展的单一评估框架，用于评估对Point Cloud中的动态点进行去除的技术。</li>
<li>methods: 本研究使用了现有的State-of-the-art方法，以及一些新的衡量指标，以分析这些方法的限制。</li>
<li>results: 本研究使用了多个不同感应器类型的数据集，并提供了一个公开可用的代码和数据集，以便进一步的发展和应用。<details>
<summary>Abstract</summary>
In the field of robotics, the point cloud has become an essential map representation. From the perspective of downstream tasks like localization and global path planning, points corresponding to dynamic objects will adversely affect their performance. Existing methods for removing dynamic points in point clouds often lack clarity in comparative evaluations and comprehensive analysis. Therefore, we propose an easy-to-extend unified benchmarking framework for evaluating techniques for removing dynamic points in maps. It includes refactored state-of-art methods and novel metrics to analyze the limitations of these approaches. This enables researchers to dive deep into the underlying reasons behind these limitations. The benchmark makes use of several datasets with different sensor types. All the code and datasets related to our study are publicly available for further development and utilization.
</details>
<details>
<summary>摘要</summary>
在机器人学中，点云已成为重要的地图表示方式。从本地化和全球规划任务的视角来看，对动态对象的点会有负面影响性能。现有的点云中动态点除法方法经常缺乏明确的比较评价和全面分析。因此，我们提议一个易扩展的统一评价框架，用于评估地图中动态点除法的技术。该框架包括 refactoring 当前领先的方法和新的评价指标，以分析这些方法的局限性。这使研究人员能够深入探究这些局限性的根本原因。我们的benchmark使用了不同感知器类型的数据集。所有与我们的研究相关的代码和数据集都公开可用于进一步开发和应用。
</details></li>
</ul>
<hr>
<h2 id="Dialogue-Agents-101-A-Beginner’s-Guide-to-Critical-Ingredients-for-Designing-Effective-Conversational-Systems"><a href="#Dialogue-Agents-101-A-Beginner’s-Guide-to-Critical-Ingredients-for-Designing-Effective-Conversational-Systems" class="headerlink" title="Dialogue Agents 101: A Beginner’s Guide to Critical Ingredients for Designing Effective Conversational Systems"></a>Dialogue Agents 101: A Beginner’s Guide to Critical Ingredients for Designing Effective Conversational Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07255">http://arxiv.org/abs/2307.07255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivani Kumar, Sumit Bhatia, Milan Aggarwal, Tanmoy Chakraborty</li>
<li>for: 本研究旨在提供对对话代理系统的设计方法的全面概述，以帮助实践者快速开发高质量对话代理系统。</li>
<li>methods: 本研究使用了多种方法来解决不同的对话任务，包括基于对话的数据集和评价策略。</li>
<li>results: 研究发现，使用不同的方法解决不同的对话任务可能会带来高成本和不充分利用对话任务之间的相互关系。因此，现在的趋势是向建立统一基础模型。本研究还提出了一个统一对话数据集（UNIT），用于检验对话代理系统的性能。<details>
<summary>Abstract</summary>
Sharing ideas through communication with peers is the primary mode of human interaction. Consequently, extensive research has been conducted in the area of conversational AI, leading to an increase in the availability and diversity of conversational tasks, datasets, and methods. However, with numerous tasks being explored simultaneously, the current landscape of conversational AI becomes fragmented. Therefore, initiating a well-thought-out model for a dialogue agent can pose significant challenges for a practitioner. Towards highlighting the critical ingredients needed for a practitioner to design a dialogue agent from scratch, the current study provides a comprehensive overview of the primary characteristics of a dialogue agent, the supporting tasks, their corresponding open-domain datasets, and the methods used to benchmark these datasets. We observe that different methods have been used to tackle distinct dialogue tasks. However, building separate models for each task is costly and does not leverage the correlation among the several tasks of a dialogue agent. As a result, recent trends suggest a shift towards building unified foundation models. To this end, we propose UNIT, a UNified dIalogue dataseT constructed from conversations of existing datasets for different dialogue tasks capturing the nuances for each of them. We also examine the evaluation strategies used to measure the performance of dialogue agents and highlight the scope for future research in the area of conversational AI.
</details>
<details>
<summary>摘要</summary>
人类交流的主要方式是通过与同伴的交流，因此很多研究在对话AI方面进行了探索。这导致了对话任务的多样性和可用性的提高，但是由于同时探索多个任务，现在的对话AI领域就变得分散化。因此，设计一个从头开始的对话代理模型可能会对实践者提出 significiant challenges。为了帮助实践者设计对话代理模型，本研究提供了对对话代理模型的主要特征、支持任务、相关的开放领域数据集以及用于评估这些数据集的方法的全面回顾。我们发现不同的方法在解决不同的对话任务时都有用，但是建立每个任务的单独模型是昂贵的，并且不利用对话任务之间的相互关系。因此，现在的趋势是建立统一基础模型。为此，我们提出了UNIT，一个基于对话数据集的统一基础模型， capture了每个任务的细节。我们还检查了用于评估对话代理模型的评价策略，并 highlighted the scope for future research in the area of conversational AI.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Adversarial-Vulnerability-through-Causal-Parameter-Estimation-by-Adversarial-Double-Machine-Learning"><a href="#Mitigating-Adversarial-Vulnerability-through-Causal-Parameter-Estimation-by-Adversarial-Double-Machine-Learning" class="headerlink" title="Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning"></a>Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07250">http://arxiv.org/abs/2307.07250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ByungKwanLee/Double-Debiased-Adversary">https://github.com/ByungKwanLee/Double-Debiased-Adversary</a></li>
<li>paper_authors: Byung-Kwan Lee, Junho Kim, Yong Man Ro</li>
<li>For: This paper aims to improve the adversarial robustness of deep neural networks by introducing a causal approach called Adversarial Double Machine Learning (ADML).* Methods: The ADML method uses a causal perspective to quantify the degree of adversarial vulnerability and capture the effect of treatments on the outcome of interests.* Results: The paper demonstrates through extensive experiments on various CNN and Transformer architectures that ADML improves adversarial robustness with large margins and relieves the empirical observation of vulnerability.<details>
<summary>Abstract</summary>
Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridging a causal perspective into the adversarial vulnerability. Through extensive experiments on various CNN and Transformer architectures, we corroborate that ADML improves adversarial robustness with large margins and relieve the empirical observation.
</details>
<details>
<summary>摘要</summary>
“深度神经网络的决策过程可以轻易受到来自故意设计的扰动的攻击，这些扰动可以轻易地让深度神经网络的决策过程受到损害。为了防止这些潜在的威胁，各种基于对抗训练的防御方法在深度神经网络中迅速增长，并成为了惯例的标准方法。尽管最近的竞赛成绩表明了这些防御方法的竞争力，但我们发现，对于不同的目标，攻击性的扰动的敏感性会异常变化，而且某些敏感性甚至无法被深度神经网络的更深度结构和高级防御方法缓解。为了解决这个问题，在这篇论文中，我们提出了一种名为对抗双机器学习（ADML）的 causal 方法，可以评估深度神经网络的预测中的攻击性扰动的度量，并且可以直接评估对于结果的影响。ADML可以直接估计攻击性扰动的 causal 参数，并且可以避免对 robustness 的负面影响，从而将 causal 视角引入到攻击性扰动中。通过对各种 CNN 和 Transformer 架构进行广泛的实验，我们证明了 ADML 可以提高对抗鲁棒性，并且可以解除实际观察中的异常现象。”
</details></li>
</ul>
<hr>
<h2 id="Rigorous-Runtime-Analysis-of-Diversity-Optimization-with-GSEMO-on-OneMinMax"><a href="#Rigorous-Runtime-Analysis-of-Diversity-Optimization-with-GSEMO-on-OneMinMax" class="headerlink" title="Rigorous Runtime Analysis of Diversity Optimization with GSEMO on OneMinMax"></a>Rigorous Runtime Analysis of Diversity Optimization with GSEMO on OneMinMax</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07248">http://arxiv.org/abs/2307.07248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Antipov, Aneta Neumann, Frank Neumann</li>
<li>for: 这个论文目的是研究如何使用GSEMO算法提高多目标优化问题中的多样化。</li>
<li>methods: 这个论文使用了GSEMO算法和多样化优化策略来优化一个多目标问题OneMinMax，该问题中所有解都是Pareto优化的。</li>
<li>results: 论文证明了GSEMO算法在 Population 中拥有第二最佳多样化时，预期时间 $O(n^2)$ 内找到优化多样化的解。这个结论基于Population 的随机漫步分析，该分析反映了解集的变化频率和结果。<details>
<summary>Abstract</summary>
The evolutionary diversity optimization aims at finding a diverse set of solutions which satisfy some constraint on their fitness. In the context of multi-objective optimization this constraint can require solutions to be Pareto-optimal. In this paper we study how the GSEMO algorithm with additional diversity-enhancing heuristic optimizes a diversity of its population on a bi-objective benchmark problem OneMinMax, for which all solutions are Pareto-optimal.   We provide a rigorous runtime analysis of the last step of the optimization, when the algorithm starts with a population with a second-best diversity, and prove that it finds a population with optimal diversity in expected time $O(n^2)$, when the problem size $n$ is odd. For reaching our goal, we analyse the random walk of the population, which reflects the frequency of changes in the population and their outcomes.
</details>
<details>
<summary>摘要</summary>
进化多标的优化目标是找到一个多元化的解集，满足一些适当的健康指标。在多目标优化情况下，这个限制可能需要解是Pareto优的。在这篇论文中，我们研究了GSEMO算法，具有额外的多标化增强规律，在二元最大最小问题OneMinMax上进行了多标化优化。我们提供了严谨的时间分析，当算法从一个第二最佳多标的人口开始时，证明它可以在预期时间O(n^2)内找到一个最佳多标的人口，当问题大小n是奇数时。为了实现我们的目标，我们分析了人口的随机步行，这反映了人口中的变化频率和其结果。
</details></li>
</ul>
<hr>
<h2 id="Fairness-of-ChatGPT-and-the-Role-Of-Explainable-Guided-Prompts"><a href="#Fairness-of-ChatGPT-and-the-Role-Of-Explainable-Guided-Prompts" class="headerlink" title="Fairness of ChatGPT and the Role Of Explainable-Guided Prompts"></a>Fairness of ChatGPT and the Role Of Explainable-Guided Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11761">http://arxiv.org/abs/2307.11761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yashar Deldjoo</li>
<li>for: 这个研究探索了大规模语言模型（LLMs）在信贷风险评估中的潜力， Specifically OpenAI的GPT。</li>
<li>methods: 研究使用了judiciously designed prompts和domain-specific knowledge来导向LLMs，并与传统机器学习（ML）模型进行比较。</li>
<li>results: 研究发现LLMs可以与传统ML模型相似的性能，但使用了40倍少的数据（20个数据点，相比800个数据点），并优化错误率和公平性，这些都是风险分析中重要的层面。<details>
<summary>Abstract</summary>
Our research investigates the potential of Large-scale Language Models (LLMs), specifically OpenAI's GPT, in credit risk assessment-a binary classification task. Our findings suggest that LLMs, when directed by judiciously designed prompts and supplemented with domain-specific knowledge, can parallel the performance of traditional Machine Learning (ML) models. Intriguingly, they achieve this with significantly less data-40 times less, utilizing merely 20 data points compared to the ML's 800. LLMs particularly excel in minimizing false positives and enhancing fairness, both being vital aspects of risk analysis. While our results did not surpass those of classical ML models, they underscore the potential of LLMs in analogous tasks, laying a groundwork for future explorations into harnessing the capabilities of LLMs in diverse ML tasks.
</details>
<details>
<summary>摘要</summary>
我们的研究探讨了大规模语言模型（LLMs），具体是OpenAI的GPT，在信用风险评估中的潜力。我们发现，当用judiciously设计的提示和域pecific知识支持时，LLMs可以与传统机器学习（ML）模型相当。这些LLMs可以在使用merely 20个数据点时达到800个数据点的性能，并且特别是减少假阳性和提高公平性，这两者都是重要的风险分析方面。虽然我们的结果没有超过传统ML模型的表现，但它们表明LLMs在类似任务中具有潜力，为未来在多种ML任务中利用LLMs的可能性奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Multiplicative-update-rules-for-accelerating-deep-learning-training-and-increasing-robustness"><a href="#Multiplicative-update-rules-for-accelerating-deep-learning-training-and-increasing-robustness" class="headerlink" title="Multiplicative update rules for accelerating deep learning training and increasing robustness"></a>Multiplicative update rules for accelerating deep learning training and increasing robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07189">http://arxiv.org/abs/2307.07189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manos Kirtas, Nikolaos Passalis, Anastasios Tefas</li>
<li>for: 这个论文的目的是加速深度学习（DL）训练和建立更加稳定的DL模型。</li>
<li>methods: 这个论文使用了一种新的多元更新规则，这种规则可以与传统的加法更新规则相结合，以实现更加快速和稳定的DL训练。</li>
<li>results: 实验表明，使用这种新的多元更新规则可以在多种优化方法和深度神经网络（DNN）架构下加速DL训练，并且导致模型更加稳定和robust。<details>
<summary>Abstract</summary>
Even nowadays, where Deep Learning (DL) has achieved state-of-the-art performance in a wide range of research domains, accelerating training and building robust DL models remains a challenging task. To this end, generations of researchers have pursued to develop robust methods for training DL architectures that can be less sensitive to weight distributions, model architectures and loss landscapes. However, such methods are limited to adaptive learning rate optimizers, initialization schemes, and clipping gradients without investigating the fundamental rule of parameters update. Although multiplicative updates have contributed significantly to the early development of machine learning and hold strong theoretical claims, to best of our knowledge, this is the first work that investigate them in context of DL training acceleration and robustness. In this work, we propose an optimization framework that fits to a wide range of optimization algorithms and enables one to apply alternative update rules. To this end, we propose a novel multiplicative update rule and we extend their capabilities by combining it with a traditional additive update term, under a novel hybrid update method. We claim that the proposed framework accelerates training, while leading to more robust models in contrast to traditionally used additive update rule and we experimentally demonstrate their effectiveness in a wide range of task and optimization methods. Such tasks ranging from convex and non-convex optimization to difficult image classification benchmarks applying a wide range of traditionally used optimization methods and Deep Neural Network (DNN) architectures.
</details>
<details>
<summary>摘要</summary>
In this work, we propose an optimization framework that fits to a wide range of optimization algorithms and enables one to apply alternative update rules. To this end, we propose a novel multiplicative update rule and we extend their capabilities by combining it with a traditional additive update term, under a novel hybrid update method. We claim that the proposed framework accelerates training, while leading to more robust models in contrast to traditionally used additive update rule and we experimentally demonstrate their effectiveness in a wide range of tasks and optimization methods. Such tasks ranging from convex and non-convex optimization to difficult image classification benchmarks applying a wide range of traditionally used optimization methods and Deep Neural Network (DNN) architectures.Translated into Simplified Chinese:即使现在，深度学习（DL）已经在许多研究领域达到了状态机器的表现，加速训练和建立robust DL模型仍然是一项挑战。为此，不同的研究者已经努力开发了robust DL模型训练方法，以降低参数分布、模型架构和损失landscape的敏感性。然而，这些方法受限于 adaptive learning rate optimizer、初始化方案和clipping gradients，而没有探究参数更新的基本规则。 although multiplicative updates have made significant contributions to the early development of machine learning and have strong theoretical foundations, to the best of our knowledge, this is the first work that investigates them in the context of DL training acceleration and robustness.在这个工作中，我们提出了一个优化框架，可以适应多种优化算法，并允许使用alternative update rule。为此，我们提出了一个新的乘数更新规则，并通过将其与传统的加itive更新项结合，实现了一种新的hybrid更新方法。我们声称，该 frameworks可以加速训练，同时导致模型更加稳定和robust，相比于传统使用的加itive更新规则，并通过实验证明其效果。这些任务包括从 convex和非convex优化到难度图像分类benchmark，并通过多种传统使用的优化方法和深度神经网络（DNN）架构。
</details></li>
</ul>
<hr>
<h2 id="TriFormer-A-Multi-modal-Transformer-Framework-For-Mild-Cognitive-Impairment-Conversion-Prediction"><a href="#TriFormer-A-Multi-modal-Transformer-Framework-For-Mild-Cognitive-Impairment-Conversion-Prediction" class="headerlink" title="TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction"></a>TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07177">http://arxiv.org/abs/2307.07177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linfeng Liu, Junyan Lyu, Siyu Liu, Xiaoying Tang, Shekhar S. Chandra, Fatima A. Nasrallah</li>
<li>for: 预测慢性脑功能障碍（MCI）转化为阿尔茨heimer病（AD），以便在早期治疗以避免或减慢病程进展。</li>
<li>methods: 我们提出了Triformer，一种基于转换器的新框架，包括三种特殊的转换器，以整合多种数据。Triformer使用I)一个图像转换器，将医疗扫描图像数据提取出多视图特征，II)一个临床转换器，将多modal临床数据编码并相关联，III)一个模式融合转换器，基于多种数据融合的输出来生成准确的预测。</li>
<li>results: Triformer在Alzheimer’s Disease Neuroimaging Initiative（ANDI）1和ADNI2数据集上评估，与之前的单模和多模态方法相比，显示出更高的准确率。<details>
<summary>Abstract</summary>
The prediction of mild cognitive impairment (MCI) conversion to Alzheimer's disease (AD) is important for early treatment to prevent or slow the progression of AD. To accurately predict the MCI conversion to stable MCI or progressive MCI, we propose Triformer, a novel transformer-based framework with three specialized transformers to incorporate multi-model data. Triformer uses I) an image transformer to extract multi-view image features from medical scans, II) a clinical transformer to embed and correlate multi-modal clinical data, and III) a modality fusion transformer that produces an accurate prediction based on fusing the outputs from the image and clinical transformers. Triformer is evaluated on the Alzheimer's Disease Neuroimaging Initiative (ANDI)1 and ADNI2 datasets and outperforms previous state-of-the-art single and multi-modal methods.
</details>
<details>
<summary>摘要</summary>
预测轻度认知障碍（MCI）转化为阿尔茨海默病（AD）的预测非常重要，以便在早期发现并采取措施来防止或减慢AD的进程。为了准确预测MCI转化为稳定MCI或进展MCI，我们提议Triformer，一种新的转换器基础框架，其特点是使用三种特殊的转换器来汇合多种数据。Triformer使用以下三种转换器：1. 图像转换器，用于从医疗扫描中提取多视图图像特征。2. 临床转换器，用于将多modal临床数据进行嵌入和相关计算。3. 模式融合转换器，用于基于多种数据融合的准确预测。Triformer在Alzheimer's Disease Neuroimaging Initiative（ANDI）1和ADNI2 datasets上进行评估，并与之前的单模和多模态方法相比，显示出更高的预测精度。
</details></li>
</ul>
<hr>
<h2 id="Safe-DreamerV3-Safe-Reinforcement-Learning-with-World-Models"><a href="#Safe-DreamerV3-Safe-Reinforcement-Learning-with-World-Models" class="headerlink" title="Safe DreamerV3: Safe Reinforcement Learning with World Models"></a>Safe DreamerV3: Safe Reinforcement Learning with World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07176">http://arxiv.org/abs/2307.07176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weidong Huang, Jiaming Ji, Borong Zhang, Chunhe Xia, Yaodong Yang</li>
<li>for: 本研究旨在解决现有的强化学习（Reinforcement Learning，RL）方法在真实世界中的应用受限，主要是因为它们无法满足实际系统的安全要求。</li>
<li>methods: 我们引入了一种新的Safe DreamerV3算法，该算法结合了拉格朗日-based和规划-based方法，并在世界模型中实现。这种方法在低维度和视觉任务中达到了几乎零成本。</li>
<li>results: 我们的实验结果显示，Safe DreamerV3算法在Safety-Gymnasium benchmark中的低维度和视觉任务中可以达到几乎零成本，而 existing SafeRL 方法则无法达到这个目标。<details>
<summary>Abstract</summary>
The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
</details>
<details>
<summary>摘要</summary>
RL在实际应用中的普及尚未实现，主要是因为它无法满足实际系统的安全需求。现有的安全再强化学习（SafeRL）方法，通过提高安全成本来增强安全性，在复杂的场景中，包括视觉任务，甚至 WITH comprehensive data sampling和训练，都无法实现零成本。为解决这一问题，我们提出了Safe DreamerV3算法，该算法结合了Lagrangian-based和规划基于的方法，并在世界模型中实现了这些方法。我们的方法代表了安全RL的一大突破，是首个在低维度和视觉任务中的 Safety-Gymnasium bencmark中实现零成本的算法。您可以在以下网站上找到我们的项目网站：https://sites.google.com/view/safedreamerv3。
</details></li>
</ul>
<hr>
<h2 id="HYTREL-Hypergraph-enhanced-Tabular-Data-Representation-Learning"><a href="#HYTREL-Hypergraph-enhanced-Tabular-Data-Representation-Learning" class="headerlink" title="HYTREL: Hypergraph-enhanced Tabular Data Representation Learning"></a>HYTREL: Hypergraph-enhanced Tabular Data Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.08623">http://arxiv.org/abs/2307.08623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang, George Karypis</li>
<li>for: 提高 tabular 数据中的语言模型表现，特别是尝试级联结的表达和表格结构。</li>
<li>methods: 使用 hypergraphs 来捕捉表格数据的 permutation 不变性和三种结构性质，包括行列Permutation 不变性、父子结构和总表结构。</li>
<li>results: 在四个下游任务中，HYTREL 表现出了与其他竞争对手相比的稳定和高效表现，只需要最小的预训练。此外，qualitative 分析表明 HYTREL 可以快速吸收表格结构，生成稳定的表格单元表示。<details>
<summary>Abstract</summary>
Language models pretrained on large collections of tabular data have demonstrated their effectiveness in several downstream tasks. However, many of these models do not take into account the row/column permutation invariances, hierarchical structure, etc. that exist in tabular data. To alleviate these limitations, we propose HYTREL, a tabular language model, that captures the permutation invariances and three more structural properties of tabular data by using hypergraphs - where the table cells make up the nodes and the cells occurring jointly together in each row, column, and the entire table are used to form three different types of hyperedges. We show that HYTREL is maximally invariant under certain conditions for tabular data, i.e., two tables obtain the same representations via HYTREL iff the two tables are identical up to permutations. Our empirical results demonstrate that HYTREL consistently outperforms other competitive baselines on four downstream tasks with minimal pretraining, illustrating the advantages of incorporating the inductive biases associated with tabular data into the representations. Finally, our qualitative analyses showcase that HYTREL can assimilate the table structures to generate robust representations for the cells, rows, columns, and the entire table.
</details>
<details>
<summary>摘要</summary>
language models 预训练在大量表格数据上有显示其效iveness 在多个下游任务中。然而，许多这些模型不会考虑表格数据中的列/行Permutation invariances，层次结构等属性。为了解决这些限制，我们提出了 HYTREL，一种表格语言模型，该模型通过使用 hypergraphs  capture 表格数据中的 Permutation invariances 和三种结构属性。我们证明 HYTREL 在某些条件下对 tabular data 是最大 invariant，即两个表格通过 HYTREL 生成的表示相同，只要两个表格在排序上相同。我们的实验结果表明 HYTREL 在四个下游任务中具有明显的优势，只需要 minimal pretraining，这说明了在表格数据中包含表格数据的预测性假设可以为表格数据的表示增加优势。最后，我们的质量分析表明 HYTREL 可以吸收表格结构，为细胞、行、列和整个表格生成强健的表示。
</details></li>
</ul>
<hr>
<h2 id="Vulnerability-Aware-Instance-Reweighting-For-Adversarial-Training"><a href="#Vulnerability-Aware-Instance-Reweighting-For-Adversarial-Training" class="headerlink" title="Vulnerability-Aware Instance Reweighting For Adversarial Training"></a>Vulnerability-Aware Instance Reweighting For Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07167">http://arxiv.org/abs/2307.07167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olukorede Fakorede, Ashutosh Kumar Nirala, Modeste Atsague, Jin Tian</li>
<li>for: 本研究旨在提高深度学习分类器对攻击性示例的Robustness，通过包括攻击示例在训练过程中。</li>
<li>methods: 本研究使用了不同的权重调整方法，以优化AT算法的性能。这些方法包括实例级权重调整和示例级权重调整。</li>
<li>results: 对比 existed 的权重调整方法，本研究提出了一种新的实例级权重调整方法，可以减少对攻击示例的依赖度和信息损失。实验结果表明，该方法可以显著提高对攻击示例的Robustness，特别是面对强大的白盒和黑盒攻击。<details>
<summary>Abstract</summary>
Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiments, we show that our proposed method significantly improves over existing reweighting schemes, especially against strong white and black-box attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Looking-deeper-into-interpretable-deep-learning-in-neuroimaging-a-comprehensive-survey"><a href="#Looking-deeper-into-interpretable-deep-learning-in-neuroimaging-a-comprehensive-survey" class="headerlink" title="Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey"></a>Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09615">http://arxiv.org/abs/2307.09615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Mahfuzur Rahman, Vince D. Calhoun, Sergey M. Plis</li>
<li>for: 本研究旨在探讨深度学习模型在神经成像领域中的可解释性，以便更好地理解模型做出的决策。</li>
<li>methods: 本研究使用了多种可解释深度学习模型，包括LRP、SHAP、DeepLIFT等方法，以提高模型的可解释性。</li>
<li>results: 研究发现，使用可解释深度学习模型可以更好地捕捉神经成像数据中关键的脑变化，并且可以提高模型的预测性能。但是，当前的实践还存在一些限制和挑战，需要进一步的研究和改进。<details>
<summary>Abstract</summary>
Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep learning models in the neuroimaging domain. Firstly, we summarize the current status of interpretability resources in general, focusing on the progression of methods, associated challenges, and opinions. Secondly, we discuss how multiple recent neuroimaging studies leveraged model interpretability to capture anatomical and functional brain alterations most relevant to model predictions. Finally, we discuss the limitations of the current practices and offer some valuable insights and guidance on how we can steer our future research directions to make deep learning models substantially interpretable and thus advance scientific understanding of brain disorders.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型在过去几年中得到了广泛应用，主要是因为它们可以直接从原始数据中学习，而不需要额外的错误产生的特征提取阶段。现在的DL基于的脑成像研究也经历了明显的性能提高，但DL模型的挑战仍然存在，主要是因为这些模型的透明性不够，使得它们在实际应用中不可靠。在过去几年，可解释AI（XAI）技术得到了广泛发展，以获取模型做出决策的 intuitions，这对于安全关键领域如医疗、金融和宪政机构来说是非常重要。虽然解释领域在发展中，但研究人员仍然不清楚哪些方面的模型学习post hoc方法揭示出来，以及如何验证其可靠性。这篇评论综述了在脑成像领域中可解释深度学习模型的发展。首先，我们概括了目前可解释资源的状况，包括方法的进步、相关挑战和观点。其次，我们讲述了一些最近的脑成像研究如何通过模型解释来捕捉最相关的脑结构和功能变化，以及对于模型预测的影响。最后，我们讨论了当前实践中的限制，并提供了一些有价值的指导和建议，以便我们可以在未来减少DL模型的不可靠性，并提高我们对脑疾病的科学理解。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Empowered-AI-Generated-Content-in-Wireless-Networks"><a href="#Federated-Learning-Empowered-AI-Generated-Content-in-Wireless-Networks" class="headerlink" title="Federated Learning-Empowered AI-Generated Content in Wireless Networks"></a>Federated Learning-Empowered AI-Generated Content in Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07146">http://arxiv.org/abs/2307.07146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xumin Huang, Peichun Li, Hongyang Du, Jiawen Kang, Dusit Niyato, Dong In Kim, Yuan Wu</li>
<li>for: 提高内容创作效率、质量、多样性和灵活性</li>
<li>methods: 采用分布式学习框架，协同数据所有者进行模型训练，保护用户隐私</li>
<li>results: 降低通信成本和训练延迟，同时保护用户隐私<details>
<summary>Abstract</summary>
Artificial intelligence generated content (AIGC) has emerged as a promising technology to improve the efficiency, quality, diversity and flexibility of the content creation process by adopting a variety of generative AI models. Deploying AIGC services in wireless networks has been expected to enhance the user experience. However, the existing AIGC service provision suffers from several limitations, e.g., the centralized training in the pre-training, fine-tuning and inference processes, especially their implementations in wireless networks with privacy preservation. Federated learning (FL), as a collaborative learning framework where the model training is distributed to cooperative data owners without the need for data sharing, can be leveraged to simultaneously improve learning efficiency and achieve privacy protection for AIGC. To this end, we present FL-based techniques for empowering AIGC, and aim to enable users to generate diverse, personalized, and high-quality content. Furthermore, we conduct a case study of FL-aided AIGC fine-tuning by using the state-of-the-art AIGC model, i.e., stable diffusion model. Numerical results show that our scheme achieves advantages in effectively reducing the communication cost and training latency and privacy protection. Finally, we highlight several major research directions and open issues for the convergence of FL and AIGC.
</details>
<details>
<summary>摘要</summary>
人工智能生成内容（AIGC）技术已经出现为改善内容创作过程的效率、质量、多样性和灵活性的优秀选择。在无线网络中部署AIGC服务可以提高用户体验。然而，现有的AIGC服务提供方式受到一些限制，例如中央化训练在预训练、精度调整和推理过程中，特别是在保护隐私的无线网络中。 Federated learning（FL），作为一种分布式学习框架，可以同时提高学习效率和实现隐私保护。为此，我们提出了基于FL的AIGC技术，并 Hoping to enable users to generate diverse, personalized, and high-quality content.在我们的实验中，我们使用当前的AIGC模型，即稳定扩散模型进行FL-帮助的精度调整。 numerically results show that our scheme achieves advantages in effectively reducing the communication cost and training latency, as well as privacy protection. Finally, we highlight several major research directions and open issues for the convergence of FL and AIGC.
</details></li>
</ul>
<hr>
<h2 id="Understanding-Multi-Turn-Toxic-Behaviors-in-Open-Domain-Chatbots"><a href="#Understanding-Multi-Turn-Toxic-Behaviors-in-Open-Domain-Chatbots" class="headerlink" title="Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots"></a>Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09579">http://arxiv.org/abs/2307.09579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bocheng Chen, Guangjing Wang, Hanqing Guo, Yuanda Wang, Qiben Yan</li>
<li>for: 研究探讨了 chatbot 模型在不同的对话场景下是否可能产生攻击性的问题。</li>
<li>methods:  authors 使用了一种新的攻击方法，即 fine-tuning 一个 chatbot 以便与目标对话 chatbot 进行对话。</li>
<li>results: 研究发现，open-domain chatbot 模型在多Turn对话中可以被触发出攻击性的回答，最好的情况下，\toxicbot 的激活率达到 67%。<details>
<summary>Abstract</summary>
Recent advances in natural language processing and machine learning have led to the development of chatbot models, such as ChatGPT, that can engage in conversational dialogue with human users. However, the ability of these models to generate toxic or harmful responses during a non-toxic multi-turn conversation remains an open research question. Existing research focuses on single-turn sentence testing, while we find that 82\% of the individual non-toxic sentences that elicit toxic behaviors in a conversation are considered safe by existing tools. In this paper, we design a new attack, \toxicbot, by fine-tuning a chatbot to engage in conversation with a target open-domain chatbot. The chatbot is fine-tuned with a collection of crafted conversation sequences. Particularly, each conversation begins with a sentence from a crafted prompt sentences dataset. Our extensive evaluation shows that open-domain chatbot models can be triggered to generate toxic responses in a multi-turn conversation. In the best scenario, \toxicbot achieves a 67\% activation rate. The conversation sequences in the fine-tuning stage help trigger the toxicity in a conversation, which allows the attack to bypass two defense methods. Our findings suggest that further research is needed to address chatbot toxicity in a dynamic interactive environment. The proposed \toxicbot can be used by both industry and researchers to develop methods for detecting and mitigating toxic responses in conversational dialogue and improve the robustness of chatbots for end users.
</details>
<details>
<summary>摘要</summary>
近年，自然语言处理和机器学习的进步使得聊天机器人模型，如ChatGPT，能够与人类用户进行对话交流。然而，这些模型在不恶意多轮对话中产生恶意或危险回复的能力仍然是一个开放的研究问题。现有研究主要集中在单Turn句子测试上，而我们发现82%的个人不恶意句子在对话中可以让chatbot产生恶意行为。在这篇论文中，我们设计了一种新的攻击，\toxicbot，通过细化一个目标的开放领域聊天机器人。这个聊天机器人通过一个采集的对话序列进行细化。具体来说，每个对话都开始于一个采集的提示句子集中的一句话。我们的广泛评估表明，开放领域聊天机器人模型可以在多轮对话中产生恶意回复。最好的情况下，\toxicbot达到67%的活动率。对话序列在细化阶段帮助诱发对话中的恶意，这使得攻击能够绕过两种防御方法。我们的发现表明，进一步的研究是必要的，以解决聊天机器人中的恶意对话在动态交互环境中的问题。我们提出的\toxicbot可以由行业和研究人员使用，以开发检测和mitigate聊天机器人中的恶意回复的方法，并提高聊天机器人的用户端Robustness。
</details></li>
</ul>
<hr>
<h2 id="Multi-Dimensional-Ability-Diagnosis-for-Machine-Learning-Algorithms"><a href="#Multi-Dimensional-Ability-Diagnosis-for-Machine-Learning-Algorithms" class="headerlink" title="Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms"></a>Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07134">http://arxiv.org/abs/2307.07134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kellygong/camilla">https://github.com/kellygong/camilla</a></li>
<li>paper_authors: Qi Liu, Zheng Gong, Zhenya Huang, Chuanren Liu, Hengshu Zhu, Zhi Li, Enhong Chen, Hui Xiong</li>
<li>for: 评估机器学习算法的多方面能力和样本因素的影响</li>
<li>methods: 基于心理测量理论和神经网络，对不同算法对数据示例的响应 logs 进行学习，捕捉算法、样本和技能之间的复杂交互</li>
<li>results: 对四个公共数据集进行了广泛的实验，结果显示 Camilla 可以更准确地捕捉每个算法的优缺点，并在metric可靠性、排名一致性和排名稳定性上表现出色<details>
<summary>Abstract</summary>
Machine learning algorithms have become ubiquitous in a number of applications (e.g. image classification). However, due to the insufficient measurement of traditional metrics (e.g. the coarse-grained Accuracy of each classifier), substantial gaps are usually observed between the real-world performance of these algorithms and their scores in standardized evaluations. In this paper, inspired by the psychometric theories from human measurement, we propose a task-agnostic evaluation framework Camilla, where a multi-dimensional diagnostic metric Ability is defined for collaboratively measuring the multifaceted strength of each machine learning algorithm. Specifically, given the response logs from different algorithms to data samples, we leverage cognitive diagnosis assumptions and neural networks to learn the complex interactions among algorithms, samples and the skills (explicitly or implicitly pre-defined) of each sample. In this way, both the abilities of each algorithm on multiple skills and some of the sample factors (e.g. sample difficulty) can be simultaneously quantified. We conduct extensive experiments with hundreds of machine learning algorithms on four public datasets, and our experimental results demonstrate that Camilla not only can capture the pros and cons of each algorithm more precisely, but also outperforms state-of-the-art baselines on the metric reliability, rank consistency and rank stability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Ethics-in-the-Age-of-AI-An-Analysis-of-AI-Practitioners’-Awareness-and-Challenges"><a href="#Ethics-in-the-Age-of-AI-An-Analysis-of-AI-Practitioners’-Awareness-and-Challenges" class="headerlink" title="Ethics in the Age of AI: An Analysis of AI Practitioners’ Awareness and Challenges"></a>Ethics in the Age of AI: An Analysis of AI Practitioners’ Awareness and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.10057">http://arxiv.org/abs/2307.10057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aastha Pant, Rashina Hoda, Simone V. Spiegler, Chakkrit Tantithamthavorn, Burak Turhan</li>
<li>for: 这项研究的目的是了解AI实践者对AI伦理的认识和在开发AI系统时遇到的挑战。</li>
<li>methods: 这项研究使用了一份问卷调查100名AI实践者的意见和经验，以了解他们对AI伦理的认识和在开发AI系统时遇到的挑战。</li>
<li>results: 研究发现大多数AI实践者有一定的认识度对AI伦理，主要归功于工作场所的规则和政策。他们主要关注隐私保护和安全问题。正式的教育和培训被认为有一定帮助作用。AI实践者在开发伦理AI系统时遇到的挑战包括一般挑战、技术相关挑战和人类相关挑战。<details>
<summary>Abstract</summary>
Ethics in AI has become a debated topic of public and expert discourse in recent years. But what do people who build AI - AI practitioners - have to say about their understanding of AI ethics and the challenges associated with incorporating it in the AI-based systems they develop? Understanding AI practitioners' views on AI ethics is important as they are the ones closest to the AI systems and can bring about changes and improvements. We conducted a survey aimed at understanding AI practitioners' awareness of AI ethics and their challenges in incorporating ethics. Based on 100 AI practitioners' responses, our findings indicate that majority of AI practitioners had a reasonable familiarity with the concept of AI ethics, primarily due to workplace rules and policies. Privacy protection and security was the ethical principle that majority of them were aware of. Formal education/training was considered somewhat helpful in preparing practitioners to incorporate AI ethics. The challenges that AI practitioners faced in the development of ethical AI-based systems included (i) general challenges, (ii) technology-related challenges and (iii) human-related challenges. We also identified areas needing further investigation and provided recommendations to assist AI practitioners and companies in incorporating ethics into AI development.
</details>
<details>
<summary>摘要</summary>
调查结果表明，大多数 AI practitioners 对 AI 伦理概念有一定的了解，主要归功于工作场所的规则和政策。隐私保护和安全是他们意识到的伦理原则中的主要内容。有些人认为，正式的教育和训练有所帮助，以准备 practitioners 在实践中采用 AI 伦理。在开发伦理 AI 系统时，AI practitioners 面临着一些挑战，包括（i）总体挑战、（ii）技术相关的挑战和（iii）人类相关的挑战。我们还发现了需要进一步调查的领域和提出了建议，以帮助 AI practitioners 和公司在 AI 开发中更好地 integrating ethics。
</details></li>
</ul>
<hr>
<h2 id="DataAssist-A-Machine-Learning-Approach-to-Data-Cleaning-and-Preparation"><a href="#DataAssist-A-Machine-Learning-Approach-to-Data-Cleaning-and-Preparation" class="headerlink" title="DataAssist: A Machine Learning Approach to Data Cleaning and Preparation"></a>DataAssist: A Machine Learning Approach to Data Cleaning and Preparation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07119">http://arxiv.org/abs/2307.07119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kartikay Goyle, Quin Xie, Vakul Goyle</li>
<li>for: 提高数据分析效率，减少数据整理和整合的时间</li>
<li>methods: 使用机器学习 Informed 方法自动整理数据，包括生成变量视觉化、统一数据注释、异常值 removals 和数据预处理</li>
<li>results: 可以大幅减少数据整理和整合的时间，为经济、商业和预测应用等领域提供高质量数据集Here’s the breakdown of each point:</li>
<li>for: The paper is written for people who want to improve the efficiency of data analysis and reduce the time spent on data cleaning and preparation.</li>
<li>methods: The paper uses machine learning-informed methods to automate data preparation, including generating visualizations for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data.</li>
<li>results: The paper shows that using DataAssist can significantly reduce the time spent on data cleaning and preparation, providing high-quality data sets for a variety of fields, including economics, business, and forecasting applications.<details>
<summary>Abstract</summary>
Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50% time of the time spent on data cleansing and preparation.
</details>
<details>
<summary>摘要</summary>
现有的自动化机器学习（ML）工具都是模型中心的，强调模型选择和参数优化。然而，数据分析中大部分时间都是投入到数据整理和整理中，而现有的工具却有限。我们现在提出了DataAssist，一个自动化数据准备和整理平台，使用机器学习 Informed 方法提高数据集质量。我们显示了DataAssist 提供了探索数据分析和数据整理的管道，包括生成用户选择变量的Visualization，统一数据注释、建议异常移除和数据预处理。导出的数据集可以轻松地与其他自动ML工具或用户指定的模型进行下游分析。我们的数据集中心工具适用于多个领域，包括经济、商业和预测应用，可以节省大约50%的时间用于数据整理和准备。
</details></li>
</ul>
<hr>
<h2 id="EmotionPrompt-Leveraging-Psychology-for-Large-Language-Models-Enhancement-via-Emotional-Stimulus"><a href="#EmotionPrompt-Leveraging-Psychology-for-Large-Language-Models-Enhancement-via-Emotional-Stimulus" class="headerlink" title="EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus"></a>EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11760">http://arxiv.org/abs/2307.11760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Li, Jindong Wang, Kaijie Zhu, Yixuan Zhang, Wenxin Hou, Jianxun Lian, Xing Xie</li>
<li>for: 提高大型自然语言模型（LLMs）的日常应用性，通过增强 LLMS 对提示的敏感性。</li>
<li>methods: 基于心理学的思想，提出 EmotionPrompt 技术，通过情感刺激提高 LLMS 的表现。</li>
<li>results: 对8种不同任务和4种模型（ChatGPT、Vicuna-13b、Bloom、T5）进行实验，EmotionPrompt 显著超越原始的零例示 prompt 和 Zero-shot-CoT，同时提高了真实性和信息性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our EmotionPrompt, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for humans-LLMs interaction.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）在多个领域 such as 理解、语言理解和数学问题解决方面已经达到了显著的性能，并被视为人工通用智能（AGI）的关键一步。然而，LLM的提示敏感性仍然是日常使用的主要瓶颈。在这篇论文中，我们启发自心理学，并提出了情感提示（EmotionPrompt），以增强 LLM 的性能。情感提示的工作原理很简单：在提示中添加情感刺激。实验结果表明，我们的 EmotionPrompt，使用同一个单一提示模板，在8个任务上 WITH 多种模型（ChatGPT、Vicuna-13b、Bloom 和 T5）显著超过原始 zero-shot 提示和 Zero-shot-CoT。此外，EmotionPrompt 还被观察到改善了真实性和信息性。我们认为，EmotionPrompt 开启了一条新的人类-LLM交互知识探索的道路。
</details></li>
</ul>
<hr>
<h2 id="Robotic-Manipulation-Datasets-for-Offline-Compositional-Reinforcement-Learning"><a href="#Robotic-Manipulation-Datasets-for-Offline-Compositional-Reinforcement-Learning" class="headerlink" title="Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning"></a>Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07091">http://arxiv.org/abs/2307.07091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lifelong-ml/offline-compositional-rl-datasets">https://github.com/lifelong-ml/offline-compositional-rl-datasets</a></li>
<li>paper_authors: Marcel Hussing, Jorge A. Mendez, Anisha Singrodia, Cassandra Kent, Eric Eaton</li>
<li>for: 这篇论文的目的是提高大规模数据集生成的OFFLINE强化学习（RL）方法。</li>
<li>methods: 论文使用了CompoSuite中的256个任务来生成4个模拟 робо臂操作的OFFLINE RL数据集，每个数据集包含256亿个转移。</li>
<li>results: 论文的实验表明，当前的OFFLINE RL方法可以在一定程度上学习训练任务，而 compose 方法在不同任务结构下表现 significatively  better than non-compose 方法。但是，当前的方法还无法EXTRACT 任务的compositional结构，以便在未看到的任务上掌握。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that current offline RL methods can learn the training tasks to some extent and that compositional methods significantly outperform non-compositional methods. However, current methods are still unable to extract the tasks' compositional structure to generalize to unseen tasks, showing a need for further research in offline compositional RL.
</details>
<details>
<summary>摘要</summary>
减少数据采集成本的离线强化学习（RL）是一个有前途的方向，允许RL Agent在大量数据上预训练，避免重复的数据采集成本。为了推动这一领域，生成大规模数据是非常重要的。 compositional RL 特别有利于生成大规模数据，因为它允许创建多个任务从少量组成部件，并且任务结构可能使得训练过的 Agent 能够解决新任务，并将相关学习的组成部件组合起来。这篇文章提供了基于 CompoSuite 的 256 个任务的四个离线 RL 数据集，每个数据集由一个不同的表现度作为 Agent 生成的 256 亿个转移。我们提供了训练和评估设置，以评估 Agent 是否能够学习复杂的任务策略。我们的检验结果表明，当前的离线 RL 方法可以在一定程度上学习训练任务，而 compositional 方法在不同任务之间具有显著的优势。然而，当前的方法仍然无法提取任务的复杂结构，以普适化到未看到的任务，表明需要进一步的研究在离线 compositional RL 方面。
</details></li>
</ul>
<hr>
<h2 id="Espaloma-0-3-0-Machine-learned-molecular-mechanics-force-field-for-the-simulation-of-protein-ligand-systems-and-beyond"><a href="#Espaloma-0-3-0-Machine-learned-molecular-mechanics-force-field-for-the-simulation-of-protein-ligand-systems-and-beyond" class="headerlink" title="Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond"></a>Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07085">http://arxiv.org/abs/2307.07085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenichiro Takaba, Iván Pulido, Mike Henry, Hugo MacDermott-Opeskin, John D. Chodera, Yuanqing Wang</li>
<li>For: The paper aims to develop a new approach to building molecular mechanics (MM) force fields that can be learned directly from quantum chemical calculations or condensed-phase data, using graph neural networks.* Methods: The approach uses an end-to-end differentiable force field construction method called Espaloma, which incorporates both energy and force fitting directly to quantum chemical data. The method is trained on a dataset of chemical spaces relevant to biomolecular modeling, including small molecules, proteins, and RNA.* Results: The resulting force field, espaloma 0.3.0, accurately predicts quantum chemical energies and forces, and maintains stable quantum chemical energy-minimized geometries. The approach also produces highly accurate protein-ligand binding free energies when self-consistently parametrizing protein and ligand. The method shows promise as a path forward for building systematically more accurate force fields that can be easily extended to new chemical domains of interest.Here are the three key points in Simplified Chinese text:* For: 这篇论文目标是开发一种基于量子化学计算或固体数据的分子机械力场模型，使用图 neural network 来取代人工专家审核的、不可变的化学参数分配规则。* Methods: 该方法使用一种终端可微分的力场建模方法，称为 Espaloma，直接从量子化学计算或固体数据中学习力场模型。该方法在包括小分子、蛋白质和 RNA 等化学空间中进行了训练。* Results: 所得到的力场模型，即 espaloma 0.3.0，能够准确预测量子化学能量和力，并稳定地保持量子化学能量最小化的结构。此外，该方法还能够高精度地预测蛋白质-抗体复合物的绑定自由能。该方法显示出了在新的化学领域中建立更加准确的力场模型的潜在优势。<details>
<summary>Abstract</summary>
Molecular mechanics (MM) force fields -- the models that characterize the energy landscape of molecular systems via simple pairwise and polynomial terms -- have traditionally relied on human expert-curated, inflexible, and poorly extensible discrete chemical parameter assignment rules, namely atom or valence types. Recently, there has been significant interest in using graph neural networks to replace this process, while enabling the parametrization scheme to be learned in an end-to-end differentiable manner directly from quantum chemical calculations or condensed-phase data. In this paper, we extend the Espaloma end-to-end differentiable force field construction approach by incorporating both energy and force fitting directly to quantum chemical data into the training process. Building on the OpenMM SPICE dataset, we curate a dataset containing chemical spaces highly relevant to the broad interest of biomolecular modeling, covering small molecules, proteins, and RNA. The resulting force field, espaloma 0.3.0, self-consistently parametrizes these diverse biomolecular species, accurately predicts quantum chemical energies and forces, and maintains stable quantum chemical energy-minimized geometries. Surprisingly, this simple approach produces highly accurate protein-ligand binding free energies when self-consistently parametrizing protein and ligand. This approach -- capable of fitting new force fields to large quantum chemical datasets in one GPU-day -- shows significant promise as a path forward for building systematically more accurate force fields that can be easily extended to new chemical domains of interest.
</details>
<details>
<summary>摘要</summary>
分子机械力场（MM）力场模型 -- 用简单的对拟和多项式关系来描述分子系统的能量景观的模型 -- 传统上依赖人工专家 manually curated 和不可靠的化学参数赋值规则，即原子或Valence 类型。在最近几年，有一个重要的兴趣是使用图 neural networks 取代这个过程，以使 parametrization 算法可以在一个端到端可微分的方式直接从量子化学计算或压缩物理数据中学习。在这篇文章中，我们扩展了Espaloma 端到端可微分力场建构方法，并将能量和力数据直接适应到量子化学计算中。基于 OpenMM SPICE 数据集，我们筹集了一个包含高度相关的生物分子模型化学species的数据集，包括小分子、蛋白质和 RNA。得到的力场，Espaloma 0.3.0，自 consistently 参数化这些多样化的生物分子物种，准确预测量子化学能量和力，并维持稳定的量子化学能量最小化几何。Surprisingly,这种简单的方法可以高度准确地预测蛋白质- ligand 绑定自由能量，当自 consistently 参数化蛋白质和 ligand 时。这种方法 -- 可以在一个 GPU-day 内适应新的量子化学数据集 -- 显示出了重要的承诺，作为在新化学领域中建立更准确的力场的可行之路。
</details></li>
</ul>
<hr>
<h2 id="Safe-Reinforcement-Learning-as-Wasserstein-Variational-Inference-Formal-Methods-for-Interpretability"><a href="#Safe-Reinforcement-Learning-as-Wasserstein-Variational-Inference-Formal-Methods-for-Interpretability" class="headerlink" title="Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability"></a>Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07084">http://arxiv.org/abs/2307.07084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanran Wang, David Boyle</li>
<li>for: 该论文旨在提供一种基于强化学习的有效思考方法，用于解决具有变动性的决策问题。</li>
<li>methods: 该论文提出了一种新的 Adaptive Wasserstein Variational Optimization (AWaVO) 方法，使用正式方法来解释奖励函数和优化策略，并提供了奖励设计的概率解释和策略融合的可观察性。</li>
<li>results: 该论文通过在实验和真实机器人任务中的训练表现，证明了AWaVO方法的可行性和稳定性，并实际地证明了一种合理的性能与保守可观察性之间的交易。<details>
<summary>Abstract</summary>
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guaranteed global convergence rates not only in simulation but also in real robot tasks, and empirically verify a reasonable tradeoff between high performance and conservative interpretability.
</details>
<details>
<summary>摘要</summary>
“强化学习或最佳控制可以提供有效的推理 для累缲问题，具有变化的动态。然而，实际实现中，评估奖函数和对应的优质策略实际上具有挑战。因此，将累缲问题正式化为推理具有很大的价值，因为概率推理在原理上提供了多元化和强大的数学工具，以推断随机动态，并提供了累缲决策的概率解释。在本研究中，我们提出了一种新的适应 Wasserstein 统计Optimization（AWaVO），以解决这些挑战。我们的方法使用正式方法，提供奖函数设计的解释、训练调合的透明度和累缲决策的概率解释。为证明实用性，我们在模拟和实际 robot 任务中显示了执行调合的训练，并证明了保证全球调合速率不仅在模拟中实现，而且在实际任务中也实现了。此外，我们还证明了累缲决策的合理交换，即高性能和保守解释之间的平衡。”
</details></li>
</ul>
<hr>
<h2 id="Vertex-based-Networks-to-Accelerate-Path-Planning-Algorithms"><a href="#Vertex-based-Networks-to-Accelerate-Path-Planning-Algorithms" class="headerlink" title="Vertex-based Networks to Accelerate Path Planning Algorithms"></a>Vertex-based Networks to Accelerate Path Planning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07059">http://arxiv.org/abs/2307.07059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhang Zhang, Jundong Liu</li>
<li>for: 提高路径规划的效率，使其更适用于各种自主应用。</li>
<li>methods: 利用顶点基本网络增强RRT*的抽样过程，以提高路径规划效率。</li>
<li>results: 通过对 randomly generated floor maps 进行实验，所提出的解决方案可以实现超过400%的速度提升，与基eline模型相比。<details>
<summary>Abstract</summary>
Path planning plays a crucial role in various autonomy applications, and RRT* is one of the leading solutions in this field. In this paper, we propose the utilization of vertex-based networks to enhance the sampling process of RRT*, leading to more efficient path planning. Our approach focuses on critical vertices along the optimal paths, which provide essential yet sparser abstractions of the paths. We employ focal loss to address the associated data imbalance issue, and explore different masking configurations to determine practical tradeoffs in system performance. Through experiments conducted on randomly generated floor maps, our solutions demonstrate significant speed improvements, achieving over a 400% enhancement compared to the baseline model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>路径规划在各种自动化应用中扮演着关键性的角色，RRT* 是该领域的一种领先解决方案。在这篇论文中，我们提议通过顶点基于网络来增强 RRT* 的抽象过程，从而实现更有效的路径规划。我们的方法是关注优质路径上的关键顶点，这些顶点提供了关键的 yet 稀疏的路径抽象。我们使用焦点损失来解决相关的数据不均衡问题，并 explore 不同的masking配置来确定实际的系统性能质量。通过在随机生成的 floor 图上进行的实验，我们的解决方案表明了明显的速度提高，相比基eline 模型，实现了超过 400% 的提高。
</details></li>
</ul>
<hr>
<h2 id="A-metric-learning-approach-for-endoscopic-kidney-stone-identification"><a href="#A-metric-learning-approach-for-endoscopic-kidney-stone-identification" class="headerlink" title="A metric learning approach for endoscopic kidney stone identification"></a>A metric learning approach for endoscopic kidney stone identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07046">http://arxiv.org/abs/2307.07046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Gonzalez-Zapata, Francisco Lopez-Tiro, Elias Villalvazo-Avila, Daniel Flores-Araiza, Jacques Hubert, Andres Mendez-Vazquez, Gilberto Ochoa-Ruiz, Christian Daul</li>
<li>for:  automatic identification of kidney stones during ureteroscopy to enable rapid therapeutic decisions</li>
<li>methods:  Deep Metric Learning (DML) methods, including a novel architecture and a teacher-student approach with Knowledge Distillation</li>
<li>results:  improved identification accuracy by 10-12% compared to Deep Learning (DL) methods and other DML approaches, and up to 30% compared to shallow machine learning methods<details>
<summary>Abstract</summary>
Several Deep Learning (DL) methods have recently been proposed for an automated identification of kidney stones during an ureteroscopy to enable rapid therapeutic decisions. Even if these DL approaches led to promising results, they are mainly appropriate for kidney stone types for which numerous labelled data are available. However, only few labelled images are available for some rare kidney stone types. This contribution exploits Deep Metric Learning (DML) methods i) to handle such classes with few samples, ii) to generalize well to out of distribution samples, and iii) to cope better with new classes which are added to the database. The proposed Guided Deep Metric Learning approach is based on a novel architecture which was designed to learn data representations in an improved way. The solution was inspired by Few-Shot Learning (FSL) and makes use of a teacher-student approach. The teacher model (GEMINI) generates a reduced hypothesis space based on prior knowledge from the labeled data, and is used it as a guide to a student model (i.e., ResNet50) through a Knowledge Distillation scheme. Extensive tests were first performed on two datasets separately used for the recognition, namely a set of images acquired for the surfaces of the kidney stone fragments, and a set of images of the fragment sections. The proposed DML-approach improved the identification accuracy by 10% and 12% in comparison to DL-methods and other DML-approaches, respectively. Moreover, model embeddings from the two dataset types were merged in an organized way through a multi-view scheme to simultaneously exploit the information of surface and section fragments. Test with the resulting mixed model improves the identification accuracy by at least 3% and up to 30% with respect to DL-models and shallow machine learning methods, respectively.
</details>
<details>
<summary>摘要</summary>
Recently, several Deep Learning (DL) methods have been proposed for automated identification of kidney stones during ureteroscopy to enable rapid therapeutic decisions. Although these DL approaches have shown promising results, they are mainly suitable for kidney stone types with abundant labeled data. However, there are few labeled images available for rare kidney stone types. To address this challenge, this study exploits Deep Metric Learning (DML) methods to handle such classes with few samples, generalize well to out-of-distribution samples, and cope better with new classes added to the database.The proposed Guided Deep Metric Learning approach is based on a novel architecture that learns data representations in an improved way. The solution is inspired by Few-Shot Learning (FSL) and uses a teacher-student approach. The teacher model (GEMINI) generates a reduced hypothesis space based on prior knowledge from labeled data and guides a student model (i.e., ResNet50) through a Knowledge Distillation scheme.Extensive tests were performed on two datasets separately used for recognition, namely a set of images of the surfaces of kidney stone fragments and a set of images of fragment sections. The proposed DML-approach improved identification accuracy by 10% and 12% compared to DL-methods and other DML-approaches, respectively. Moreover, model embeddings from the two dataset types were merged in an organized way through a multi-view scheme to simultaneously exploit the information of surface and section fragments. Tests with the resulting mixed model improved identification accuracy by at least 3% and up to 30% compared to DL-models and shallow machine learning methods, respectively.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Factored-Action-Spaces-for-Off-Policy-Evaluation"><a href="#Leveraging-Factored-Action-Spaces-for-Off-Policy-Evaluation" class="headerlink" title="Leveraging Factored Action Spaces for Off-Policy Evaluation"></a>Leveraging Factored Action Spaces for Off-Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07014">http://arxiv.org/abs/2307.07014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4ai-lab/factored-action-spaces-for-ope">https://github.com/ai4ai-lab/factored-action-spaces-for-ope</a></li>
<li>paper_authors: Aaman Rebello, Shengpu Tang, Jenna Wiens, Sonali Parbhoo</li>
<li>for: 估计对于不同的行为序列来说，执行一个假设的行为序列会带来多少的利益。</li>
<li>methods: 使用了分解行为空间的方法，即将每个行为看作为多个独立的子动作的组合。</li>
<li>results: 提出了一种新的“分解”重样 estimator，并证明了这种 estimator 具有较低的偏差和较高的稳定性，同时保持零偏差性。通过实验验证了这些理论结果，并证明了假设的有效性。<details>
<summary>Abstract</summary>
Off-policy evaluation (OPE) aims to estimate the benefit of following a counterfactual sequence of actions, given data collected from executed sequences. However, existing OPE estimators often exhibit high bias and high variance in problems involving large, combinatorial action spaces. We investigate how to mitigate this issue using factored action spaces i.e. expressing each action as a combination of independent sub-actions from smaller action spaces. This approach facilitates a finer-grained analysis of how actions differ in their effects. In this work, we propose a new family of "decomposed" importance sampling (IS) estimators based on factored action spaces. Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions, while preserving the property of zero bias. Through simulations, we empirically verify our theoretical results, probing the validity of various assumptions. Provided with a technique that can derive the action space factorisation for a given problem, our work shows that OPE can be improved "for free" by utilising this inherent problem structure.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Off-policy evaluation (OPE) aims to estimate the benefit of following a counterfactual sequence of actions, given data collected from executed sequences. However, existing OPE estimators often exhibit high bias and high variance in problems involving large, combinatorial action spaces. We investigate how to mitigate this issue using factored action spaces i.e. expressing each action as a combination of independent sub-actions from smaller action spaces. This approach facilitates a finer-grained analysis of how actions differ in their effects. In this work, we propose a new family of "decomposed" importance sampling (IS) estimators based on factored action spaces. Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions, while preserving the property of zero bias. Through simulations, we empirically verify our theoretical results, probing the validity of various assumptions. Provided with a technique that can derive the action space factorisation for a given problem, our work shows that OPE can be improved "for free" by utilising this inherent problem structure.">>Here's the translation in Simplified Chinese:<<SYS>>Off-policy evaluation (OPE) 目标是估计在执行不同的行动序列后的利益，基于已经执行过的数据。然而，现有的 OPE 估计器经常在具有大量结合型动作空间的问题中表现出高偏差和高方差。我们研究如何使用分解动作空间来降低这些问题。我们提出了一种基于分解动作空间的 "分解" 重要抽样（IS） 估计器。对于满足某些假设，我们证明了这种分解 IS 估计器 的方差比原始非分解版本更低，同时保持零偏差性。通过实验，我们证明了我们的理论结论，检验了各种假设的有效性。如果可以得到一种可以 derive 动作空间分解的技术，我们的工作显示了 OPE 可以免费地改进，通过利用这种问题的内在结构。
</details></li>
</ul>
<hr>
<h2 id="Classical-Out-of-Distribution-Detection-Methods-Benchmark-in-Text-Classification-Tasks"><a href="#Classical-Out-of-Distribution-Detection-Methods-Benchmark-in-Text-Classification-Tasks" class="headerlink" title="Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks"></a>Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07002">http://arxiv.org/abs/2307.07002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mateuszbaransanok/trustworthyai">https://github.com/mateuszbaransanok/trustworthyai</a></li>
<li>paper_authors: Mateusz Baran, Joanna Baran, Mateusz Wójcik, Maciej Zięba, Adam Gonczarek</li>
<li>for: 本研究的目的是高亮现有NLP系统中的OOD检测方法的局限性。</li>
<li>methods: 本研究评估了8种可以轻松地集成到现有NLP系统中的OOD检测方法，无需额外的OOD数据或模型修改。</li>
<li>results: 我们的分析显示，现有的NLP任务中的OOD检测方法并不够敏感，无法捕捉各种分布偏移的样本。特别是背景偏移和随机排序的字符串内域文本测试场景是最大的挑战。这说明未来的研究应该更加注重开发更有效的OOD检测方法，而我们的研究提供了一个充分定义的研究基础。<details>
<summary>Abstract</summary>
State-of-the-art models can perform well in controlled environments, but they often struggle when presented with out-of-distribution (OOD) examples, making OOD detection a critical component of NLP systems. In this paper, we focus on highlighting the limitations of existing approaches to OOD detection in NLP. Specifically, we evaluated eight OOD detection methods that are easily integrable into existing NLP systems and require no additional OOD data or model modifications. One of our contributions is providing a well-structured research environment that allows for full reproducibility of the results. Additionally, our analysis shows that existing OOD detection methods for NLP tasks are not yet sufficiently sensitive to capture all samples characterized by various types of distributional shifts. Particularly challenging testing scenarios arise in cases of background shift and randomly shuffled word order within in domain texts. This highlights the need for future work to develop more effective OOD detection approaches for the NLP problems, and our work provides a well-defined foundation for further research in this area.
</details>
<details>
<summary>摘要</summary>
现代模型在控制环境下可以表现出色，但它们在不同类型的 Distributional Shift 下陷入困难，因此 OOD 检测成为 NLP 系统的关键组成部分。在这篇论文中，我们关注现有 OOD 检测方法在 NLP 领域的局限性。我们评估了八种可以容易地集成到现有 NLP 系统中的 OOD 检测方法，不需要额外的 OOD 数据或模型修改。我们的贡献之一是提供了一个具有完整可重现性的研究环境。此外，我们的分析表明，现有的 OOD 检测方法在 NLP 任务中并不够敏感，无法捕捉所有类型的 Distributional Shift 下的样本。特别是在背景变化和随机排序的情况下，OOD 检测方法表现出特别困难。这种情况 highlights 未来的研究应该更加注重开发更有效的 OOD 检测方法，我们的工作提供了一个完善的基础 для进一步的研究。
</details></li>
</ul>
<hr>
<h2 id="HyperDreamBooth-HyperNetworks-for-Fast-Personalization-of-Text-to-Image-Models"><a href="#HyperDreamBooth-HyperNetworks-for-Fast-Personalization-of-Text-to-Image-Models" class="headerlink" title="HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models"></a>HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06949">http://arxiv.org/abs/2307.06949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JiauZhang/hyperdreambooth">https://github.com/JiauZhang/hyperdreambooth</a></li>
<li>paper_authors: Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman</li>
<li>for: 这篇论文的目的是提出一个高效的个人化对应方法，以实现个人化 faces 的生成，同时解决个人化模型的时间和内存需求问题。</li>
<li>methods: 本论文使用了 HyperNetwork 技术，实现了从单一图像中生成个人化 weights，并与快速调整结合，以生成各种不同内容和样式的个人化 faces。</li>
<li>results: 本论文的方法可以在约 20 秒内进行个人化，比 DreamBooth 快 25 倍，比 Textual Inversion 快 125 倍，并且可以从单一图像中生成高质量和多样化的个人化 faces，而且模型的大小仅有 DreamBooth 的 1&#x2F;10000。<details>
<summary>Abstract</summary>
Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual Inversion, using as few as one reference image, with the same quality and style diversity as DreamBooth. Also our method yields a model that is 10000x smaller than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io
</details>
<details>
<summary>摘要</summary>
个人化在生成人工智能领域中已经成为一个显著的特征，允许在不同的上下文和风格中生成个性化的人脸，保持高度准确性。然而，个人化过程存在内存和时间的挑战，每个个性化模型都需要较长的GPU时间投资，并且每个人需要存储一个个性化模型，这会占用更多的存储容量。为了解决这些挑战，我们提出了HyperDreamBooth，一个具有高效生成少量个性化参数的超网络。通过这些参数与扩散模型的组合，加速了个性化，可以在20秒钟内生成具有高级精度和多样化风格的人脸，与DreamBooth和Textual Inversion相当，但快速得多，使用的参数少得多，模型的大小也很小。更多信息请参考：<https://hyperdreambooth.github.io>
</details></li>
</ul>
<hr>
<h2 id="Video-FocalNets-Spatio-Temporal-Focal-Modulation-for-Video-Action-Recognition"><a href="#Video-FocalNets-Spatio-Temporal-Focal-Modulation-for-Video-Action-Recognition" class="headerlink" title="Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition"></a>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06947">http://arxiv.org/abs/2307.06947</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/talalwasim/video-focalnets">https://github.com/talalwasim/video-focalnets</a></li>
<li>paper_authors: Syed Talal Wasim, Muhammad Uzair Khattak, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan</li>
<li>for: 这个论文主要是为了提出一种高效高可用的视频识别模型，以优化现有的Transformer模型的缺点。</li>
<li>methods: 这个模型使用了一种叫做”视频 FOCAL模型”的新型设计，它将自注意力机制与卷积神经结合，以模型视频中的局部和全局上下文。</li>
<li>results: 在五个大规模数据集（Kinetics-400、Kinetics-600、SS-v2、Diving-48和ActivityNet-1.3）上，Video-FocalNets模型表现出色，与现有的Transformer模型相比，具有更高的效率和更好的性能。<details>
<summary>Abstract</summary>
Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modulation-based spatio-temporal context modeling and demonstrate our parallel spatial and temporal encoding design to be the optimal choice. Video-FocalNets perform favorably well against the state-of-the-art transformer-based models for video recognition on five large-scale datasets (Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lower computational cost. Our code/models are released at https://github.com/TalalWasim/Video-FocalNets.
</details>
<details>
<summary>摘要</summary>
近期的视频识别模型通过Transformer模型来实现长距离空间temporal上下文模型。视频转换设计基于自注意的自我注意可以模型全局上下文，但是计算成本较高。相比之下，图像设计对视频提供了一个有效的alternative，但缺少长距离依赖关系模型。为了实现两种设计的优点，本工作提出了Video-FocalNet，一种高效和经济的视频识别模型。Video-FocalNet基于空间temporal关注模块，该模块通过逆转自注意的互动和聚合步骤来提高效率。此外，聚合步骤和互动步骤都使用了高效的卷积和元素乘法操作，这些操作相比于自注意操作更加经济。我们广泛探索了关注模块基于空间temporal上下文模型的设计空间，并证明我们的并行空间和时间编码设计是最佳选择。Video-FocalNet在五个大规模数据集（Kinetics-400、Kinetics-600、SS-v2、Diving-48和ActivityNet-1.3）上与当前的Transformer模型进行视频识别，性能较高，计算成本较低。我们的代码和模型在https://github.com/TalalWasim/Video-FocalNets上发布。
</details></li>
</ul>
<hr>
<h2 id="In-context-Autoencoder-for-Context-Compression-in-a-Large-Language-Model"><a href="#In-context-Autoencoder-for-Context-Compression-in-a-Large-Language-Model" class="headerlink" title="In-context Autoencoder for Context Compression in a Large Language Model"></a>In-context Autoencoder for Context Compression in a Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06945">http://arxiv.org/abs/2307.06945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu Wei</li>
<li>for:  solves the long context problem in large language models (LLMs) by compressing a long context into a limited number of memory slots.</li>
<li>methods:  uses an In-context Autoencoder (ICAE) with two modules: a learnable encoder adapted with LoRA from an LLM, and a fixed decoder which is the target LLM. The ICAE is pretrained using both autoencoding and language modeling objectives, and then fine-tuned on a small amount of instruct data to enhance its interaction with various prompts.</li>
<li>results:  effectively produces memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts.<details>
<summary>Abstract</summary>
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM. Our code and data will be released shortly.
</details>
<details>
<summary>摘要</summary>
我们提议使用内容压缩 autoencoder（ICAE）来解决大型语言模型（LLM）中的长度问题。ICAE有两个模组：一个可学习的Encoder，从LLM中获取丰富的文本数据，将长 context 压缩到有限的内存槽中，以及一个固定的Decoder，可以根据内存槽进行多种目的的处理。我们首先将ICAE使用自动编码和语言模型目标函数进行预训练，将其训练到从大量文本数据中获取的memory slots可以准确地和全面地表示原始上下文。然后，我们精确地调整预训练后的ICAE，以便在不同的提示下生成适当的回应。我们的实验结果表明，透过我们提出的预训练和精确调整方法，ICAE可以实现$4\times$的context压缩，可以很好地被目标LLM所条件。这些成果表明ICAE具有novel的方法并且具有实际应用中LLM计算和内存负载的减少之能力，因此需要进一步的研究，以解决LLM中的长度问题。我们将在短时间内发布代码和数据。
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-between-Game-Theoretic-Feature-Attributions-and-Counterfactual-Explanations"><a href="#On-the-Connection-between-Game-Theoretic-Feature-Attributions-and-Counterfactual-Explanations" class="headerlink" title="On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations"></a>On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06941">http://arxiv.org/abs/2307.06941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuele Albini, Shubham Sharma, Saumitra Mishra, Danial Dervovic, Daniele Magazzeni</li>
<li>for: 这篇论文主要关注于解释人工智能（XAI）中的两种最受欢迎的类型：特征归因和对冲推论解释。这两种方法在过去几年中受到了广泛的关注，但是它们在大多数情况下都是独立地研究的。本文将提供一个明确的理论连接，以game-theoretic feature attributions和对冲推论解释之间的关系。</li>
<li>methods: 本文使用的方法包括SHAP的游戏理论归因，以及对冲推论解释。我们也提出了一些操作改变SHAP归因和对冲推论解释的方法，以便实现它们之间的等价性。</li>
<li>results: 我们的研究显示，在满足某些条件时，game-theoretic feature attributions和对冲推论解释是等价的。此外，我们还发现了对冲推论解释的局限性，其不能准确地提供特征重要性。我们的实验结果表明，在不同的数据集上，使用不同的方法可以得到不同的解释结果。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantitatively show the difference in explanations at every stage of the connection between the two approaches and corroborate the theoretical findings.
</details>
<details>
<summary>摘要</summary>
“几年前，可解释人工智能（XAI）已经受到了广泛的关注，而feature attributions和counterfactual explanations是最受欢迎的两种解释方法。这两种方法在过去一直被研究独立，只有一些基于实践的尝试了它们的结合。本文确立了game-theoretic feature attributions和counterfactual explanations之间的具体理论连接，并且修改了Shapley值基础的feature attributions和counterfactual explanations，以便实现它们的相等。此外，我们还延伸了这个相等结果至游戏理论解决方案的其他概念，并且通过分析这些条件的限制，实际阐明了使用counterfactual explanations提供特征重要性的局限性。实验结果显示，这两种方法在connection中的解释有所不同，并且与理论结果相符。”Here is a word-for-word translation of the text into Simplified Chinese:“几年前，可解释人工智能（XAI）已经受到了广泛的关注，而feature attributions和counterfactual explanations是最受欢迎的两种解释方法。这两种方法在过去一直被研究独立，只有一些基于实践的尝试了它们的结合。本文确立了game-theoretic feature attributions和counterfactual explanations之间的具体理论连接，并且修改了Shapley值基础的feature attributions和counterfactual explanations，以便实现它们的相等。此外，我们还延伸了这个相等结果至游戏理论解决方案的其他概念。”
</details></li>
</ul>
<hr>
<h2 id="DRAGON-A-Dialogue-Based-Robot-for-Assistive-Navigation-with-Visual-Language-Grounding"><a href="#DRAGON-A-Dialogue-Based-Robot-for-Assistive-Navigation-with-Visual-Language-Grounding" class="headerlink" title="DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding"></a>DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06924">http://arxiv.org/abs/2307.06924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuijing Liu, Aamir Hasan, Kaiwen Hong, Runxuan Wang, Peixin Chang, Zachary Mizrachi, Justin Lin, D. Livingston McPherson, Wendy A. Rogers, Katherine Driggs-Campbell</li>
<li>for: 提供一个帮助视障人士理解和穿梭周遭环境的技术，并且能够将环境与自然语言相关连。</li>
<li>methods: 使用对话系统和环境相关的语言描述技术，将用户的自由形描述与环境中的地标相互连接。</li>
<li>results: 经过用户实验，发现DRAGON能够与用户通信流畅，提供好的引导体验，并将用户联系到周遭环境的意义性资讯。<details>
<summary>Abstract</summary>
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, provide a good guiding experience, and connect users with their surrounding environment in an intuitive manner.
</details>
<details>
<summary>摘要</summary>
人们视障 (PwVI) 在周围环境中有困难理解和导航。当前的导航技术ether solely focus on navigation or provide limited environmental information. 鼓动于最近的视觉语言固定和semantic navigation技术的进步，我们提议了DRAGON，一种带有对话系统的导航机器人。通过理解用户的命令，DRAGON可以引导用户到地图上的目标点，描述环境，并根据视觉观察回答问题。通过对话的有效利用，机器人可以将用户的自由形式描述与环境中的标志点相关联，并通过语音提供Semantic information。我们在每天的室内环境中进行了盲人参与者的用户研究。我们的结果表明，DRAGON能够与用户交流平滑，提供良好的引导体验，并通过语音连接用户与周围环境的INTUITIVE manner。
</details></li>
</ul>
<hr>
<h2 id="LLM-assisted-Knowledge-Graph-Engineering-Experiments-with-ChatGPT"><a href="#LLM-assisted-Knowledge-Graph-Engineering-Experiments-with-ChatGPT" class="headerlink" title="LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT"></a>LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06917">http://arxiv.org/abs/2307.06917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, Michael Martin</li>
<li>for: 这 paper 是为了探讨 ChatGPT 如何支持 Knowledge Graph Engineering (KGE) 的发展和管理。</li>
<li>methods: 该 paper 使用了 ChatGPT 进行了详细的实验，以 explore its potential in supporting KGE。</li>
<li>results: 实验结果表明，ChatGPT 可以帮助开发和管理 Knowledge Graphs (KGs)，并且可以提高 KGE 的效率和质量。<details>
<summary>Abstract</summary>
Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs.
</details>
<details>
<summary>摘要</summary>
知识图（KG）为我们提供一种结构化、灵活、透明、跨系统、合作的知识和数据组织方式，可以涵盖社会、工业以及科学领域的多种领域。KG比任何其他表示方式更有效。然而，知识图工程（KGE）需要深厚的图结构、网络技术、现有模型和词汇、规则集、逻辑以及最佳实践的经验。它还需要大量的劳动。鉴于大语言模型（LLM）的发展和其界面和应用在最近几年，我们已经进行了全面的实验，用ChatGPT探索其在支持KGE方面的潜力。本文介绍了这些实验的选择和结果，以示出ChatGPT如何帮助我们在开发和管理知识图方面。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Unique-Concept-Vectors-through-Latent-Space-Decomposition"><a href="#Uncovering-Unique-Concept-Vectors-through-Latent-Space-Decomposition" class="headerlink" title="Uncovering Unique Concept Vectors through Latent Space Decomposition"></a>Uncovering Unique Concept Vectors through Latent Space Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06913">http://arxiv.org/abs/2307.06913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mara Graziani, Laura O’ Mahony, An-Phi Nguyen, Henning Müller, Vincent Andrearczyk</li>
<li>for: 这个论文旨在解释深度学习模型的内部工作方式，以建立信任和确保模型的安全性。</li>
<li>methods: 该论文提出了一种新的后期无监督方法，可以自动揭示深度模型在训练中学习的概念。该方法包括将层的幂空间分解成固有向量，并通过无监督归一化来细化这些向量，以找到与模型预测有关的概念向量。</li>
<li>results: 经过广泛的实验表明，大多数提取的概念向量都是人类可理解的，具有准确性和一致性，并与任务有直接关系。此外，该方法在数据集探索中也有remarkable的实用性，可以成功地找到训练数据中的偏移和杂音样本。<details>
<summary>Abstract</summary>
Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase the practical utility of our method in dataset exploration, where our concept vectors successfully identify outlier training samples affected by various confounding factors. This novel exploration technique has remarkable versatility to data types and model architectures and it will facilitate the identification of biases and the discovery of sources of error within training data.
</details>
<details>
<summary>摘要</summary>
深度学习模型的内部工作方式的解释是建立信任和确保模型安全的关键。概念基于的解释方法在可解释性方面胜过特征归因估计如像素感知。然而，为了定义概念，用户的期望会影响解释。为解决这个问题，我们提出了一种新的后续无监督方法，可以自动揭示深度模型在训练过程中学习的概念。我们将层的秘密空间拆分成单值特征，然后通过无监督归一化来精细化概念向量，从而找到与高差值方向相关的概念向量，这些向量对应于semantically meaningful的概念。我们的广泛实验表明，大多数我们的概念都可以被人类理解，具有凝聚性，并与任务相关。此外，我们还证明了我们的方法在数据集探索中的实用性，我们的概念向量可以成功地标识在训练数据中受到各种干扰因素的异常训练样本。这种新的探索技术具有数据类型和模型结构的强大可 versatility，可以帮助确定模型中的偏见和训练数据中的错误来源。
</details></li>
</ul>
<hr>
<h2 id="Generating-Benchmarks-for-Factuality-Evaluation-of-Language-Models"><a href="#Generating-Benchmarks-for-Factuality-Evaluation-of-Language-Models" class="headerlink" title="Generating Benchmarks for Factuality Evaluation of Language Models"></a>Generating Benchmarks for Factuality Evaluation of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06908">http://arxiv.org/abs/2307.06908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, Yoav Shoham</li>
<li>for: 这个论文的目的是评估语言模型（LM）在给定领域中是否会生成 incorrect 信息。</li>
<li>methods: 这个论文提出了 FACTOR 方法，它可以将适用于评估 LM 的 фактиче正确性。FACTOR 方法可以自动将有利的证据集转换成一个评估 LM 是否能够生成真实信息的benchmark。</li>
<li>results: 这个论文使用 FACTOR 方法创建了两个 benchmark：Wiki-FACTOR 和 News-FACTOR。研究发现，(i)  benchmark 分数与模型大小相关，并且当LM被增强后，其分数会提高。(ii)  benchmark 分数与沟通能力之间存在正相关关系，但这两个指标不总是同时同意模型排名。(iii) 当沟通能力和 benchmark 分数不同时，后者更好地反映了 LM 在开放生成中的 фактиче正确性，这种反馈来自于人工标注员。<details>
<summary>Abstract</summary>
Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators. We make our data and code publicly available in https://github.com/AI21Labs/factor.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Benchmark scores increase with model size and improve when the LM is augmented with retrieval.2. Benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking.3. When perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.We make our data and code publicly available at <a target="_blank" rel="noopener" href="https://github.com/AI21Labs/factor">https://github.com/AI21Labs/factor</a>.</details></li>
</ol>
<hr>
<h2 id="Sequential-Monte-Carlo-Learning-for-Time-Series-Structure-Discovery"><a href="#Sequential-Monte-Carlo-Learning-for-Time-Series-Structure-Discovery" class="headerlink" title="Sequential Monte Carlo Learning for Time Series Structure Discovery"></a>Sequential Monte Carlo Learning for Time Series Structure Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.09607">http://arxiv.org/abs/2307.09607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fsaad/autogp.jl">https://github.com/fsaad/autogp.jl</a></li>
<li>paper_authors: Feras A. Saad, Brian J. Patton, Matthew D. Hoffman, Rif A. Saurous, Vikash K. Mansinghka</li>
<li>for:  automatizilly discover accurate models of complex time series data</li>
<li>methods:  Bayesian nonparametric prior over Gaussian process time series models, integrates sequential Monte Carlo (SMC) and involutive MCMC for posterior inference</li>
<li>results:  delivers 10x-100x runtime speedups over previous MCMC and greedy-search structure learning algorithms, and discovers sensible models that deliver more accurate point forecasts and interval forecasts compared to statistical and neural baselines.<details>
<summary>Abstract</summary>
This paper presents a new approach to automatically discovering accurate models of complex time series data. Working within a Bayesian nonparametric prior over a symbolic space of Gaussian process time series models, we present a novel structure learning algorithm that integrates sequential Monte Carlo (SMC) and involutive MCMC for highly effective posterior inference. Our method can be used both in "online" settings, where new data is incorporated sequentially in time, and in "offline" settings, by using nested subsets of historical data to anneal the posterior. Empirical measurements on real-world time series show that our method can deliver 10x--100x runtime speedups over previous MCMC and greedy-search structure learning algorithms targeting the same model family. We use our method to perform the first large-scale evaluation of Gaussian process time series structure learning on a prominent benchmark of 1,428 econometric datasets. The results show that our method discovers sensible models that deliver more accurate point forecasts and interval forecasts over multiple horizons as compared to widely used statistical and neural baselines that struggle on this challenging data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deep-reinforcement-learning-for-the-dynamic-vehicle-dispatching-problem-An-event-based-approach"><a href="#Deep-reinforcement-learning-for-the-dynamic-vehicle-dispatching-problem-An-event-based-approach" class="headerlink" title="Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach"></a>Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.07508">http://arxiv.org/abs/2307.07508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edyvalberty Alenquer Cordeiro, Anselmo Ramalho Pitombeira-Neto</li>
<li>for: This paper is written for solving the dynamic vehicle dispatching problem, which is a problem of assigning vehicles to requests that arise stochastically over time and space.</li>
<li>methods: The paper uses a semi-Markov decision process to model the problem, which allows for treating time as continuous and reduces the combinatorial complexity of the decision space. The authors also use double deep q-learning to train their decision agents.</li>
<li>results: The authors’ policies exhibit better average waiting times, cancellation rates, and total service times compared to heuristic policies often used in practice, with a reduction in average waiting times of up to 50% relative to the other tested heuristic policies.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文是解决动态车辆分配问题的，这是一个车辆分配到出现在时间和空间上的请求的问题。</li>
<li>methods: 这篇论文使用半Markov决策过程来模型问题，这使得时间可以为连续的，从而减少决策空间的 combinatorial 复杂度。作者还使用双层深度Q学习来训练决策代理人。</li>
<li>results: 作者的策略比常用的各种规则更好，waiting time、取消率和总服务时间都得到了改善，waiting time的减少达50%以上。<details>
<summary>Abstract</summary>
The dynamic vehicle dispatching problem corresponds to deciding which vehicles to assign to requests that arise stochastically over time and space. It emerges in diverse areas, such as in the assignment of trucks to loads to be transported; in emergency systems; and in ride-hailing services. In this paper, we model the problem as a semi-Markov decision process, which allows us to treat time as continuous. In this setting, decision epochs coincide with discrete events whose time intervals are random. We argue that an event-based approach substantially reduces the combinatorial complexity of the decision space and overcomes other limitations of discrete-time models often proposed in the literature. In order to test our approach, we develop a new discrete-event simulator and use double deep q-learning to train our decision agents. Numerical experiments are carried out in realistic scenarios using data from New York City. We compare the policies obtained through our approach with heuristic policies often used in practice. Results show that our policies exhibit better average waiting times, cancellation rates and total service times, with reduction in average waiting times of up to 50% relative to the other tested heuristic policies.
</details>
<details>
<summary>摘要</summary>
这个动态车辆分配问题与决定在时间和空间上随机出现的请求中分配车辆相关。它出现在不同领域，如货物运输、紧急系统和乘用车服务等。在这篇论文中，我们将问题模型为半Markov决策过程，这使得我们可以将时间视为连续的。在这种设定下，决策瞬间与随机时间间隔相匹配，我们认为事件基本方法可以减少决策空间的复杂度，并且超越了常见的 discrete-time 模型。为了测试我们的方法，我们开发了一个新的离散事件仿真器，并使用双层深度Q学习来训练我们的决策代理人。我们在使用实际数据从纽约市进行数值实验，并与常见的各种各样的策略进行比较。结果显示，我们的策略可以减少待命时间的平均值，最多减少50%，相比其他测试的各种各样的策略。
</details></li>
</ul>
<hr>
<h2 id="The-complexity-of-non-stationary-reinforcement-learning"><a href="#The-complexity-of-non-stationary-reinforcement-learning" class="headerlink" title="The complexity of non-stationary reinforcement learning"></a>The complexity of non-stationary reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06877">http://arxiv.org/abs/2307.06877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christos Papadimitriou, Binghui Peng</li>
<li>for:  solves the problem of continual learning in reinforcement learning, specifically the non-stationary reinforcement learning challenge.</li>
<li>methods:  uses a worst-case complexity result to prove that modifying probabilities or rewards in a reinforcement learning problem requires a significant amount of time, unless the strong exponential time hypothesis (SETH) is false.</li>
<li>results:  shows that adding a new state-action pair is much easier to implement than modifying existing probabilities or rewards.<details>
<summary>Abstract</summary>
The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\textit{adding}$ a new state-action pair is considerably easier to implement.
</details>
<details>
<summary>摘要</summary>
“对于回传学习领域中的持续学习问题（通常称为非站ARY reinforcement learning），我们证明了一个最差情况的复杂性结果：对某个状态动作 pairs  modify 概率或奖励，需要大约与状态数量相当的时间才能保持值函数更新， Unless SETH 是 false；SETK 是一个广泛accepted的对 P $\neq$ NP 推论的强化。请注意，现在的实际应用中的状态数量通常是惊人的。然而，我们显示了仅增加一个新的状态动作 pairs 是可以轻松实现的。”Note: SETH stands for "Strong Exponential Time Hypothesis" and P $\neq$ NP is a famous open problem in computer science.
</details></li>
</ul>
<hr>
<h2 id="Embodied-Lifelong-Learning-for-Task-and-Motion-Planning"><a href="#Embodied-Lifelong-Learning-for-Task-and-Motion-Planning" class="headerlink" title="Embodied Lifelong Learning for Task and Motion Planning"></a>Embodied Lifelong Learning for Task and Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06870">http://arxiv.org/abs/2307.06870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge A. Mendez, Leslie Pack Kaelbling, Tomás Lozano-Pérez</li>
<li>for: 这篇论文是关于家庭中长期部署的机器人如何通过积累经验来提高自己的能力，以成为更好的协助者。</li>
<li>methods: 论文使用了一种新的生命长学问题形式化方法，即在任务规划中学习（TAMP）系统中的生命长学问题。它们开发了一种生成混合模型，该模型生成了计划中维度的连续参数。与大多数现有的生命长学方法不同，这种方法在线采用了一些辅助任务来确定是否使用共享或非共享模型。</li>
<li>results: 论文在 simulated 2D 领域和 BEHAVIOR 数据集上的规划成功率显示了明显的提高。<details>
<summary>Abstract</summary>
A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.
</details>
<details>
<summary>摘要</summary>
一个机器人在家中长时间内部署，面临着一个真正的一生学习问题。作为它提供帮助的用户，机器人应该利用已有经验来提高自己的知识，成为更有效的帮手。我们将这种设定形式化为一种新的一生学习问题形式，在任务和动作规划（TAMP）上下文中。利用TAMP系统的模块性，我们开发了一种生成混合模型，生成候选的连续参数 для плаanner。大多数现有的一生学习方法在数据分享上做出了决定，而我们的方法在线上决定使用共享和非共享模型，并根据辅助任务来选择使用哪些模型，以便在规划中使用。我们的方法在 simulated 2D 领域和 BEHAVIOR benchmark 上表现出了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="DecompEval-Evaluating-Generated-Texts-as-Unsupervised-Decomposed-Question-Answering"><a href="#DecompEval-Evaluating-Generated-Texts-as-Unsupervised-Decomposed-Question-Answering" class="headerlink" title="DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering"></a>DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06869">http://arxiv.org/abs/2307.06869</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kepei1106/decompeval">https://github.com/kepei1106/decompeval</a></li>
<li>paper_authors: Pei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xiaoyan Zhu, Minlie Huang</li>
<li>for: 这 paper 的目的是提出一种简单 yet effective 的自然语言生成（NLG）评估 metric，它可以增强评估过程的普适性和可读性。</li>
<li>methods: 这 paper 使用了 instruction-style 问题回答任务来形式NLG评估，并使用了 instruction-tuned pre-trained language models（PLMs），不需要训练在评估数据集上，以提高普适性。另外，这 paper 还 decomposes 了自己的设计的 instruction-style 问题，以获得更好的解释性。</li>
<li>results: 实验结果表明，DecompEval 可以在不需要训练的情况下，在 text summarization 和对话生成 等 NLG 任务中 achieve state-of-the-art 性能，同时也能够具备强大的特征级 &#x2F; 任务级普适性和可读性。<details>
<summary>Abstract</summary>
Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose a new metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets. This approach aims to enhance the generalization ability of the metric.To make the evaluation process more interpretable, we decompose the instruction-style question about the quality of generated texts into subquestions that measure the quality of each sentence. The subquestions, along with the answers generated by PLMs, are then recomposed to obtain the evaluation result.Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation. It also exhibits strong dimension-level and task-level generalization ability and interpretability.
</details></li>
</ul>
<hr>
<h2 id="Prompts-Should-not-be-Seen-as-Secrets-Systematically-Measuring-Prompt-Extraction-Attack-Success"><a href="#Prompts-Should-not-be-Seen-as-Secrets-Systematically-Measuring-Prompt-Extraction-Attack-Success" class="headerlink" title="Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success"></a>Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06865">http://arxiv.org/abs/2307.06865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Zhang, Daphne Ippolito</li>
<li>for: 本研究旨在系统地测试推送提取攻击的成功率。</li>
<li>methods: 本研究使用多种推送源和多种语言模型进行实验，发现简单的文本基本可以抓取提取。</li>
<li>results: 实验结果表明，简单的文本基本可以抓取提取，高效地抓取提取。<details>
<summary>Abstract</summary>
The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
</details>
<details>
<summary>摘要</summary>
大型语言模型的一代通常通过提示技术来控制，其中用户的查询会被前置一个提示，以导引模型对查询的行为。公司使用的提示 oftentimes 被视为秘密，隐瞒于用户。然而，有一些具体的证据表明，用户可以EXTRACT 提示，即使它们被保持为秘密。在这篇论文中，我们提出了一种系统化测量提示提取攻击的成功度的框架。在多个来源的提示和多个基础语言模型的实验中，我们发现了简单的文本基于攻击可以高概率地披露提示。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Interactive-Perception-of-Surgical-Thread-for-Autonomous-Suture-Tail-Shortening"><a href="#Self-Supervised-Learning-for-Interactive-Perception-of-Surgical-Thread-for-Autonomous-Suture-Tail-Shortening" class="headerlink" title="Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening"></a>Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.06845">http://arxiv.org/abs/2307.06845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincent Schorp, Will Panitch, Kaushik Shivakumar, Vainavi Viswanath, Justin Kerr, Yahav Avigal, Danyal M Fer, Lionel Ott, Ken Goldberg</li>
<li>for: 本研究旨在解决自动外科缝合中高维度复杂的缝合线跟踪问题，因为缝合线厚度和形变性强，以及机械扣夹和组织干扰。</li>
<li>methods: 该方法使用了学习的2D外科缝合线检测网络来分割RGB图像中的缝合线，然后利用两个ステレオカメラ拍摄的图像来重建缝合线为NURBS spline。方法还使用了两个摄像头来跟踪缝合线 across consecutive frames。</li>
<li>results: 实验表明，该方法在单帧3D缝合线重建中实现了1.33像素平均 reprojection error，并在两个跟踪序列中实现了0.84像素平均 reprojection error。在“尾短”任务中，方法实现了20次实验中的90%成功率。详细的补充材料可以在<a target="_blank" rel="noopener" href="https://sites.google.com/berkeley.edu/autolab-surgical-thread/">https://sites.google.com/berkeley.edu/autolab-surgical-thread/</a> 查看。<details>
<summary>Abstract</summary>
Accurate 3D sensing of suturing thread is a challenging problem in automated surgical suturing because of the high state-space complexity, thinness and deformability of the thread, and possibility of occlusion by the grippers and tissue. In this work we present a method for tracking surgical thread in 3D which is robust to occlusions and complex thread configurations, and apply it to autonomously perform the surgical suture "tail-shortening" task: pulling thread through tissue until a desired "tail" length remains exposed. The method utilizes a learned 2D surgical thread detection network to segment suturing thread in RGB images. It then identifies the thread path in 2D and reconstructs the thread in 3D as a NURBS spline by triangulating the detections from two stereo cameras. Once a 3D thread model is initialized, the method tracks the thread across subsequent frames. Experiments suggest the method achieves a 1.33 pixel average reprojection error on challenging single-frame 3D thread reconstructions, and an 0.84 pixel average reprojection error on two tracking sequences. On the tail-shortening task, it accomplishes a 90% success rate across 20 trials. Supplemental materials are available at https://sites.google.com/berkeley.edu/autolab-surgical-thread/ .
</details>
<details>
<summary>摘要</summary>
医学自动控制技术是一个挑战性的问题，因为缝纫线的高状态空间复杂性，细长和扭曲性，以及机械握住和组织干扰。在这项工作中，我们提出了一种能够承受干扰和复杂缝纫线配置的3D缝纫线跟踪方法，并将其应用于自动完成“尾短化”任务：将缝纫线通过组织 until a desired “tail” length remains exposed。该方法使用了学习的2D缝纫线检测网络来分割RGB图像中的缝纫线。然后，它将缝纫线路径在2D上标识出，并使用两个ステレオ摄像头拍摄的检测来重建缝纫线为NURBS spline。一旦Initializes a 3D thread model, the method tracks the thread across subsequent frames.实验表明，该方法在具有挑战性的单帧3D缝纫线重建任务中达到1.33像素平均投影误差，并在两个跟踪序列中达到0.84像素平均投影误差。在“尾短化”任务中，它实现了20次实验中的90%成功率。补充材料可以在https://sites.google.com/berkeley.edu/autolab-surgical-thread/查看。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/14/cs.AI_2023_07_14/" data-id="clot2mh7j000rx788drzsd2yb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/79/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/78/">78</a><a class="page-number" href="/page/79/">79</a><span class="page-number current">80</span><a class="page-number" href="/page/81/">81</a><a class="page-number" href="/page/82/">82</a><span class="space">&hellip;</span><a class="page-number" href="/page/90/">90</a><a class="extend next" rel="next" href="/page/81/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">61</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">70</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">65</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
