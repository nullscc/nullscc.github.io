
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/27/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.SD_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T15:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.SD_2023_10_16/">cs.SD - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generation-or-Replication-Auscultating-Audio-Latent-Diffusion-Models"><a href="#Generation-or-Replication-Auscultating-Audio-Latent-Diffusion-Models" class="headerlink" title="Generation or Replication: Auscultating Audio Latent Diffusion Models"></a>Generation or Replication: Auscultating Audio Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10604">http://arxiv.org/abs/2310.10604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Bralios, Gordon Wichern, François G. Germain, Zexu Pan, Sameer Khurana, Chiori Hori, Jonathan Le Roux</li>
<li>for: 这个研究旨在理解音频扩散模型如何生成真实的声音clip，以及这种技术在音频处理方面的应用前提下，它们是否能够具备生成高质量声音clip的能力。</li>
<li>methods: 这个研究使用文本到音频扩散模型，并系统地分析这些模型在不同训练集大小下的记忆行为。同时，研究还评估了不同的检索指标，以确定哪些指标更能够捕捉训练数据的记忆。</li>
<li>results: 研究发现，使用mel spectrogram Similarity来评估模型的记忆行为是更加稳定和可靠的，而learned embedding vectors则更容易受到训练数据的干扰。此外，研究还发现AudioCaps数据库中存在大量的复制声音clip。<details>
<summary>Abstract</summary>
The introduction of audio latent diffusion models possessing the ability to generate realistic sound clips on demand from a text description has the potential to revolutionize how we work with audio. In this work, we make an initial attempt at understanding the inner workings of audio latent diffusion models by investigating how their audio outputs compare with the training data, similar to how a doctor auscultates a patient by listening to the sounds of their organs. Using text-to-audio latent diffusion models trained on the AudioCaps dataset, we systematically analyze memorization behavior as a function of training set size. We also evaluate different retrieval metrics for evidence of training data memorization, finding the similarity between mel spectrograms to be more robust in detecting matches than learned embedding vectors. In the process of analyzing memorization in audio latent diffusion models, we also discover a large amount of duplicated audio clips within the AudioCaps database.
</details>
<details>
<summary>摘要</summary>
文本描述生成真实的声音clip的能力可能会革命化我们如何处理音频。在这个工作中，我们初步地理解音频干扰模型的内部工作，通过对它们的声音输出与训练数据进行比较，类似于医生 auscultates 病人的器官声音。使用基于 AudioCaps 数据集的文本-声音干扰模型，我们系统地分析了训练集大小的影响，并评估不同的检索指标，发现mel спектрограм相似性更加稳定地检测匹配。在分析声音干扰模型的 memorization 行为的过程中，我们还发现了 AudioCaps 数据库中的大量重复的声音clip。Note: "Simplified Chinese" is a translation of the text into Standard Chinese, which is the official language of China. "Traditional Chinese" is a different writing system used in Taiwan and some other countries.
</details></li>
</ul>
<hr>
<h2 id="BeatDance-A-Beat-Based-Model-Agnostic-Contrastive-Learning-Framework-for-Music-Dance-Retrieval"><a href="#BeatDance-A-Beat-Based-Model-Agnostic-Contrastive-Learning-Framework-for-Music-Dance-Retrieval" class="headerlink" title="BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework for Music-Dance Retrieval"></a>BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework for Music-Dance Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10300">http://arxiv.org/abs/2310.10300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaixing Yang, Xukun Zhou, Xulong Tang, Ran Diao, Hongyan Liu, Jun He, Zhaoxin Fan</li>
<li>For: The paper is written for improving dance-music retrieval performance by utilizing the alignment between music beats and dance movements.* Methods: The proposed method, BeatDance, incorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat Blender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval performance.* Results: The experimental results on the Music-Dance (MD) dataset demonstrate the superiority of the proposed method over existing baselines, achieving state-of-the-art performance.Here’s the simplified Chinese version of the three key points:* For: 提高舞蹈音乐 Retrieval 性能，利用音乐拍和舞蹈动作的匹配。* Methods: 提出了 BeatDance 模型无关对比学习框架，包括 Beat-Aware Music-Dance InfoExtractor、Trans-Temporal Beat Blender 和 Beat-Enhanced Hubness Reducer。* Results: 在 Music-Dance（MD）数据集上，实验结果表明提议方法比基eline表现更出色，实现了状态级表现。<details>
<summary>Abstract</summary>
Dance and music are closely related forms of expression, with mutual retrieval between dance videos and music being a fundamental task in various fields like education, art, and sports. However, existing methods often suffer from unnatural generation effects or fail to fully explore the correlation between music and dance. To overcome these challenges, we propose BeatDance, a novel beat-based model-agnostic contrastive learning framework. BeatDance incorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat Blender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval performance by utilizing the alignment between music beats and dance movements. We also introduce the Music-Dance (MD) dataset, a large-scale collection of over 10,000 music-dance video pairs for training and testing. Experimental results on the MD dataset demonstrate the superiority of our method over existing baselines, achieving state-of-the-art performance. The code and dataset will be made public available upon acceptance.
</details>
<details>
<summary>摘要</summary>
文本：舞蹈和音乐是密切相关的表达形式，它们之间存在着很强的相互关联。然而，现有的方法 часто会导致不自然的生成效果，或者完全不利用音乐和舞蹈之间的相互关系。为了解决这些挑战，我们提出了 BeatDance，一种新的 beat-based 模型无关的对比学习框架。 BeatDance 包括一个 Beat-Aware Music-Dance 信息抽取器、一个 Trans-Temporal Beat Blender 和一个 Beat-Enhanced Hubness Reducer，以便通过音乐 beat 和舞蹈动作的协调来提高舞蹈-音乐 retrieve 性能。我们还提出了 Music-Dance（MD）数据集，一个大规模的音乐-舞蹈视频对集，用于训练和测试。实验结果表明，我们的方法在 MD 数据集上表现出优于现有基eline，实现了状态计算机。代码和数据集将在接受后公开。翻译结果：文本：舞蹈和音乐是密切相关的表达形式，它们之间存在着很强的相互关联。然而，现有的方法常常会导致不自然的生成效果，或者完全不利用音乐和舞蹈之间的相互关系。为了解决这些挑战，我们提出了 BeatDance，一种新的 beat-based 模型无关的对比学习框架。 BeatDance 包括一个 Beat-Aware Music-Dance 信息抽取器、一个 Trans-Temporal Beat Blender 和一个 Beat-Enhanced Hubness Reducer，以便通过音乐 beat 和舞蹈动作的协调来提高舞蹈-音乐 retrieve 性能。我们还提出了 Music-Dance（MD）数据集，一个大规模的音乐-舞蹈视频对集，用于训练和测试。实验结果表明，我们的方法在 MD 数据集上表现出优于现有基eline，实现了状态计算机。代码和数据集将在接受后公开。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Audio-Emotion-and-Intent-Recognition-with-Large-Pre-Trained-Models-and-Bayesian-Inference"><a href="#Advancing-Audio-Emotion-and-Intent-Recognition-with-Large-Pre-Trained-Models-and-Bayesian-Inference" class="headerlink" title="Advancing Audio Emotion and Intent Recognition with Large Pre-Trained Models and Bayesian Inference"></a>Advancing Audio Emotion and Intent Recognition with Large Pre-Trained Models and Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10179">http://arxiv.org/abs/2310.10179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dejan Porjazovski, Yaroslav Getman, Tamás Grósz, Mikko Kurimo</li>
<li>for: This paper is written for the ACM Multimedia Computational Paralinguistics Challenge, addressing the Requests and Emotion Share tasks.</li>
<li>methods: The paper employs large pre-trained models and explores audio-only and hybrid solutions leveraging audio and text modalities. The authors also introduce a Bayesian layer as an alternative to the standard linear output layer.</li>
<li>results: The empirical results consistently show the superiority of the hybrid approaches over the audio-only models, with the multimodal fusion approach achieving an 85.4% UAR on HC-Requests and 60.2% on HC-Complaints, and the ensemble model for the Emotion Share task yielding the best rho value of .614. Additionally, the Bayesian wav2vec2 approach allows for easily building ensembles with usable confidence values instead of overconfident posterior probabilities.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文是为了参加 ACM Multimedia Computational Paralinguistics Challenge 的 Requests 和 Emotion Share 任务而写的。</li>
<li>methods: 这篇论文使用了大型预训模型，并 explore 了听音和文本模式的混合解决方案。作者还介绍了一种 Bayesian 层作为标准线性输出层的替代方案。</li>
<li>results: 实验结果表明，混合方案比听音模型更加有优势，并且 multimodal fusion 方法在 HC-Requests 上 achieved 85.4% UAR 和 HC-Complaints 上 achieved 60.2%。此外，作者还介绍了一种 Bayesian wav2vec2 方法，该方法可以轻松地构建集成模型，只需要 fine-tune 一个模型。此外，该方法还可以提供可信度值 instead of 常见的过度信息 posterior probabilities。<details>
<summary>Abstract</summary>
Large pre-trained models are essential in paralinguistic systems, demonstrating effectiveness in tasks like emotion recognition and stuttering detection. In this paper, we employ large pre-trained models for the ACM Multimedia Computational Paralinguistics Challenge, addressing the Requests and Emotion Share tasks. We explore audio-only and hybrid solutions leveraging audio and text modalities. Our empirical results consistently show the superiority of the hybrid approaches over the audio-only models. Moreover, we introduce a Bayesian layer as an alternative to the standard linear output layer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and 60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields the best rho value of .614. The Bayesian wav2vec2 approach, explored in this study, allows us to easily build ensembles, at the cost of fine-tuning only one model. Moreover, we can have usable confidence values instead of the usual overconfident posterior probabilities.
</details>
<details>
<summary>摘要</summary>
大型预训模型在para语言系统中扮演着关键角色，在情感识别和偏声检测等任务中显示出了效iveness。在这篇论文中，我们使用大型预训模型参加ACM Multimedia Computational Paralinguistics Challenge的请求和情感分享任务。我们研究了基于音频和文本modalities的混合解决方案，并对audio-only和混合方案进行了比较。我们的实验结果表明，混合方案在HC-Requests和HC-Complaints任务上具有显著的优势。此外，我们还引入了一种 bayesian层作为标准线性输出层的替代方案。我们的Multimodal混合方法在HC-Requests上 achieve 85.4% UAR，在HC-Complaints上 achieve 60.2%。此外，我们还提出了一种 bayesian wav2vec2方法，可以轻松地建立ensemble，只需要 Fine-tuning一个模型。此外，我们可以获得可信度值 instead of  usual overconfident posterior probabilities。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Speech-Enhancement-and-Separation-with-a-Unified-Deep-Neural-Network-for-Single-Dual-Talker-Scenarios"><a href="#Real-time-Speech-Enhancement-and-Separation-with-a-Unified-Deep-Neural-Network-for-Single-Dual-Talker-Scenarios" class="headerlink" title="Real-time Speech Enhancement and Separation with a Unified Deep Neural Network for Single&#x2F;Dual Talker Scenarios"></a>Real-time Speech Enhancement and Separation with a Unified Deep Neural Network for Single&#x2F;Dual Talker Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10026">http://arxiv.org/abs/2310.10026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kashyap Patel, Anton Kovalyov, Issa Panahi</li>
<li>for: 提出了一种实用的方法，利用实时深度学习模型，在单个或双个对话者的输入混合中进行Speech增强和分离。</li>
<li>methods: 使用了时域Signal-to-distortion比（SI-SDR）作为训练指标，并引入了一种轻量级的对话者重叠检测（SOD）模块，通过直接在分离后的面板上操作，而不是直接操作原始混合音频，简化了检测任务。</li>
<li>results: 实验结果表明，提出的训练方法超过了现有的解决方案，并且SOD模块具有高准确性。<details>
<summary>Abstract</summary>
This paper introduces a practical approach for leveraging a real-time deep learning model to alternate between speech enhancement and joint speech enhancement and separation depending on whether the input mixture contains one or two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has shown to be a highly effective training measure in time-domain speech separation. However, the SI-SDR metric is ill-defined for zero-energy target signals, which is a problem when training a speech separation model using utterances with varying numbers of talkers. Unlike existing solutions that focus on modifying the loss function to accommodate zero-energy target signals, the proposed approach circumvents this problem by training the model to extract speech on both its output channels regardless if the input is a single or dual-talker mixture. A lightweight speaker overlap detection (SOD) module is also introduced to differentiate between single and dual-talker segments in real-time. The proposed module takes advantage of the new formulation by operating directly on the separated masks, given by the separation model, instead of the original mixture, thus effectively simplifying the detection task. Experimental results show that the proposed training approach outperforms existing solutions, and the SOD module exhibits high accuracy.
</details>
<details>
<summary>摘要</summary>
Unlike existing solutions that modify the loss function to accommodate zero-energy target signals, the proposed approach trains the model to extract speech on both its output channels regardless of whether the input is a single or dual-talker mixture. Additionally, a lightweight speaker overlap detection (SOD) module is introduced to differentiate between single and dual-talker segments in real-time. The SOD module operates directly on the separated masks, provided by the separation model, rather than the original mixture, making the detection task simpler.Experimental results show that the proposed training approach outperforms existing solutions, and the SOD module exhibits high accuracy.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.SD_2023_10_16/" data-id="clpxp6c7f0116ee88aqjo908j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.CV_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T13:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.CV_2023_10_16/">cs.CV - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Filling-the-Holes-on-3D-Heritage-Object-Surface-based-on-Automatic-Segmentation-Algorithm"><a href="#Filling-the-Holes-on-3D-Heritage-Object-Surface-based-on-Automatic-Segmentation-Algorithm" class="headerlink" title="Filling the Holes on 3D Heritage Object Surface based on Automatic Segmentation Algorithm"></a>Filling the Holes on 3D Heritage Object Surface based on Automatic Segmentation Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10875">http://arxiv.org/abs/2310.10875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sinh Van Nguyen, Son Thanh Le, Minh Khai Tran, Le Thanh Sach</li>
<li>for: 这个论文的目的是提出一种改进的3D物体表面填充方法，以提高计算机图形学、图像处理和计算机视觉等领域中3D对象的重建和处理的精度。</li>
<li>methods: 该论文使用的方法包括计算几何学和深度学习模型，以及基于图像处理的机器学习算法。</li>
<li>results: 相比现有方法，该论文提出的方法可以更高精度地重建3D对象，并且可以适用于多种3D数据类型，包括点云数据和三角形网格数据。<details>
<summary>Abstract</summary>
Reconstructing and processing the 3D objects are popular activities in the research field of computer graphics, image processing and computer vision. The 3D objects are processed based on the methods like geometric modeling, a branch of applied mathematics and computational geometry, or the machine learning algorithms based on image processing. The computation of geometrical objects includes processing the curves and surfaces, subdivision, simplification, meshing, holes filling, reconstructing, and refining the 3D surface objects on both point cloud data and triangular mesh. While the machine learning methods are developed using deep learning models. With the support of 3D laser scan devices and Lidar techniques, the obtained dataset is close to original shape of the real objects. Besides, the photography and its application based on the modern techniques in recent years help us collect data and process the 3D models more precise. This article proposes an improved method for filling holes on the 3D object surface based on an automatic segmentation. Instead of filling the hole directly as the existing methods, we now subdivide the hole before filling it. The hole is first determined and segmented automatically based on computation of its local curvature. It is then filled on each part of the hole to match its local curvature shape. The method can work on both 3D point cloud surfaces and triangular mesh surface. Comparing to the state of the art methods, our proposed method obtained higher accuracy of the reconstructed 3D objects.
</details>
<details>
<summary>摘要</summary>
Computer graphics、图像处理和计算机视觉领域的研究中，重建和处理3D对象是非常流行的活动。这些3D对象通常通过几何模型化或基于图像处理的机器学习算法进行处理。计算几何对象包括处理曲线和表面、分割、简化、网格化、填充洞和改进3D表面对象的点云数据和三角形网格。而机器学习方法则是基于深度学习模型。通过3D激光扫描设备和激光技术获得的数据，我们可以更加准确地重建真实对象的形状。此外，现代技术的应用也有助于我们更加精准地收集数据和处理3D模型。本文提出了一种改进的洞填充方法，通过自动分割洞而不是直接填充洞像现有方法。首先，我们使用计算当地曲线的方法自动分割洞。然后，我们在每个洞部分填充其所对应的本地弯曲形状。这种方法可以在3D点云表面和三角形网格表面上进行应用。与现有方法相比，我们的提议方法能够获得更高的3D对象重建精度。
</details></li>
</ul>
<hr>
<h2 id="Approximation-properties-of-slice-matching-operators"><a href="#Approximation-properties-of-slice-matching-operators" class="headerlink" title="Approximation properties of slice-matching operators"></a>Approximation properties of slice-matching operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10869">http://arxiv.org/abs/2310.10869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiying Li, Caroline Moosmueller</li>
<li>for: This paper is written for the purpose of exploring approximation properties of iterative slice-matching procedures for transferring a source measure to a target measure, particularly in high dimensions.</li>
<li>methods: The paper uses slice-matching operators, which depend on the source and target measures and slicing directions, to examine the approximation properties of iterative slice-matching schemes.</li>
<li>results: The paper demonstrates invariance and equivariance properties of the slice-matching operator with respect to the source and target measures, respectively, and establishes error bounds for approximating the target measure using one step of the slice-matching scheme. Additionally, the paper investigates connections to affine registration problems and extensions to the invariance and equivariance properties of the slice-matching operator.<details>
<summary>Abstract</summary>
Iterative slice-matching procedures are efficient schemes for transferring a source measure to a target measure, especially in high dimensions. These schemes have been successfully used in applications such as color transfer and shape retrieval, and are guaranteed to converge under regularity assumptions. In this paper, we explore approximation properties related to a single step of such iterative schemes by examining an associated slice-matching operator, depending on a source measure, a target measure, and slicing directions. In particular, we demonstrate an invariance property with respect to the source measure, an equivariance property with respect to the target measure, and Lipschitz continuity concerning the slicing directions. We furthermore establish error bounds corresponding to approximating the target measure by one step of the slice-matching scheme and characterize situations in which the slice-matching operator recovers the optimal transport map between two measures. We also investigate connections to affine registration problems with respect to (sliced) Wasserstein distances. These connections can be also be viewed as extensions to the invariance and equivariance properties of the slice-matching operator and illustrate the extent to which slice-matching schemes incorporate affine effects.
</details>
<details>
<summary>摘要</summary>
iterative slice-matching 算法是高维中高效的源度量至目标度量转移方案，尤其在应用中如颜色传输和形状检索中得到了成功。这些算法在Regularity assumptions下是确定的收敛的。在这篇论文中，我们研究了单步iterative slice-matching算法的approximation Properties，包括一个相关的slice-matching运算符，它取决于源度量、目标度量和切割方向。我们证明了对源度量的不变性、对目标度量的对称性和切割方向的 lipschitz连续性。我们还确定了一个基于单步slice-matching算法的目标度量的错误 bound，并 characterize了在 slice-matching算法中可以重建两个度量之间的优化运输图的情况。此外，我们还 investigate了基于水星距离的 affine registration problem 的连接，这些连接可以被视为 slice-matching算法中的 affine 效应的扩展。
</details></li>
</ul>
<hr>
<h2 id="The-Invisible-Map-Visual-Inertial-SLAM-with-Fiducial-Markers-for-Smartphone-based-Indoor-Navigation"><a href="#The-Invisible-Map-Visual-Inertial-SLAM-with-Fiducial-Markers-for-Smartphone-based-Indoor-Navigation" class="headerlink" title="The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation"></a>The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10862">http://arxiv.org/abs/2310.10862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Ruvolo, Ayush Chakraborty, Rucha Dave, Richard Li, Duncan Mazza, Xierui Shen, Raiyan Siddique, Krishna Suresh</li>
<li>for: 创建大尺寸、易于导航的3D地图，使用主流智能手机。</li>
<li>methods: 将3D地图问题定义为图像SLAM问题，并估计环境中的建筑物标志（指标）和可导航路径（手机姿态）。</li>
<li>results: 系统可以创建准确的3D地图。此外，我们还提出了选择映射超参数的精细技术，以适应新环境。<details>
<summary>Abstract</summary>
We present a system for creating building-scale, easily navigable 3D maps using mainstream smartphones. In our approach, we formulate the 3D-mapping problem as an instance of Graph SLAM and infer the position of both building landmarks (fiducial markers) and navigable paths through the environment (phone poses). Our results demonstrate the system's ability to create accurate 3D maps. Further, we highlight the importance of careful selection of mapping hyperparameters and provide a novel technique for tuning these hyperparameters to adapt our algorithm to new environments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于主流智能手机的建筑尺度级可探索3D地图创建系统。我们将3D地图问题定义为Instance of Graph SLAM，并通过约束建筑标记（ fiducial markers）和环境中可行路径（手机姿态）来计算位置。我们的结果表明系统可以创建准确的3D地图。此外，我们强调了选择映射超参数的重要性，并提供了一种新的参数调整技术，以适应新环境。
</details></li>
</ul>
<hr>
<h2 id="SoybeanNet-Transformer-Based-Convolutional-Neural-Network-for-Soybean-Pod-Counting-from-Unmanned-Aerial-Vehicle-UAV-Images"><a href="#SoybeanNet-Transformer-Based-Convolutional-Neural-Network-for-Soybean-Pod-Counting-from-Unmanned-Aerial-Vehicle-UAV-Images" class="headerlink" title="SoybeanNet: Transformer-Based Convolutional Neural Network for Soybean Pod Counting from Unmanned Aerial Vehicle (UAV) Images"></a>SoybeanNet: Transformer-Based Convolutional Neural Network for Soybean Pod Counting from Unmanned Aerial Vehicle (UAV) Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10861">http://arxiv.org/abs/2310.10861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/soybean-pod-counting-from-uav-images">https://github.com/jiajiali04/soybean-pod-counting-from-uav-images</a></li>
<li>paper_authors: Jiajia Li, Raju Thada Magar, Dong Chen, Feng Lin, Dechun Wang, Xiang Yin, Weichao Zhuang, Zhaojian Li</li>
<li>for: 这个论文的目的是提高豫豢的生产效率，并使用无人机图像来实现豫豢的果实计数。</li>
<li>methods: 这个论文使用了一种新的点基 counting网络，叫做SoybeanNet，使用了强大的变换器核心来同时进行豫豢的果实计数和定位。</li>
<li>results: 该论文在使用实际的无人机图像进行测试时，与五种现有方法进行比较，并取得了84.51%的计数精度。<details>
<summary>Abstract</summary>
Soybeans are a critical source of food, protein and oil, and thus have received extensive research aimed at enhancing their yield, refining cultivation practices, and advancing soybean breeding techniques. Within this context, soybean pod counting plays an essential role in understanding and optimizing production. Despite recent advancements, the development of a robust pod-counting algorithm capable of performing effectively in real-field conditions remains a significant challenge This paper presents a pioneering work of accurate soybean pod counting utilizing unmanned aerial vehicle (UAV) images captured from actual soybean fields in Michigan, USA. Specifically, this paper presents SoybeanNet, a novel point-based counting network that harnesses powerful transformer backbones for simultaneous soybean pod counting and localization with high accuracy. In addition, a new dataset of UAV-acquired images for soybean pod counting was created and open-sourced, consisting of 113 drone images with more than 260k manually annotated soybean pods captured under natural lighting conditions. Through comprehensive evaluations, SoybeanNet demonstrated superior performance over five state-of-the-art approaches when tested on the collected images. Remarkably, SoybeanNet achieved a counting accuracy of $84.51\%$ when tested on the testing dataset, attesting to its efficacy in real-world scenarios. The publication also provides both the source code (\url{https://github.com/JiajiaLi04/Soybean-Pod-Counting-from-UAV-Images}) and the labeled soybean dataset (\url{https://www.kaggle.com/datasets/jiajiali/uav-based-soybean-pod-images}), offering a valuable resource for future research endeavors in soybean pod counting and related fields.
</details>
<details>
<summary>摘要</summary>
soybeans是一种重要的食品、蛋白和油源，因此它们在提高产量、改善栽培方法和进步杂交技术方面 receiving extensive research。在这个 контексте中，豇豆果 counting plays an essential role in understanding and optimizing production. Despite recent advancements, the development of a robust pod-counting algorithm capable of performing effectively in real-field conditions remains a significant challenge.这篇文章提出了一项突破性的豇豆果 counting方法，使用了来自美国密歇根州actual soybean fields的无人机图像。Specifically, this paper presents SoybeanNet, a novel point-based counting network that harnesses powerful transformer backbones for simultaneous soybean pod counting and localization with high accuracy. In addition, a new dataset of UAV-acquired images for soybean pod counting was created and open-sourced, consisting of 113 drone images with more than 260k manually annotated soybean pods captured under natural lighting conditions. Through comprehensive evaluations, SoybeanNet demonstrated superior performance over five state-of-the-art approaches when tested on the collected images. Remarkably, SoybeanNet achieved a counting accuracy of 84.51% when tested on the testing dataset, attesting to its efficacy in real-world scenarios. The publication also provides both the source code (https://github.com/JiajiaLi04/Soybean-Pod-Counting-from-UAV-Images) and the labeled soybean dataset (https://www.kaggle.com/datasets/jiajiali/uav-based-soybean-pod-images), offering a valuable resource for future research endeavors in soybean pod counting and related fields.
</details></li>
</ul>
<hr>
<h2 id="Provable-Probabilistic-Imaging-using-Score-Based-Generative-Priors"><a href="#Provable-Probabilistic-Imaging-using-Score-Based-Generative-Priors" class="headerlink" title="Provable Probabilistic Imaging using Score-Based Generative Priors"></a>Provable Probabilistic Imaging using Score-Based Generative Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10835">http://arxiv.org/abs/2310.10835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Sun, Zihui Wu, Yifan Chen, Berthy T. Feng, Katherine L. Bouman</li>
<li>for: 这篇论文旨在提出一种可靠地估计高质量图像并同时评估其不确定性的权重函数架构。</li>
<li>methods: 该论文提出了一种基于Monte Carlo（MC）的插入式权重函数架构（PMC），可以同时捕捉高质量图像重建和不确定性评估。具体来说，该论文引入了两种PMC算法，可以视为传统插入式质量函数（PnP）和杂化正则化（RED）算法的排除样本分布 аналоги。</li>
<li>results: 对多个代表性的逆问题进行实验，结果表明PMCAlgorithm可以显著提高图像重建质量和高精度不确定性评估。<details>
<summary>Abstract</summary>
Estimating high-quality images while also quantifying their uncertainty are two desired features in an image reconstruction algorithm for solving ill-posed inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as a principled framework for characterizing the space of possible solutions to a general inverse problem. PMC is able to incorporate expressive score-based generative priors for high-quality image reconstruction while also performing uncertainty quantification via posterior sampling. In particular, we introduce two PMC algorithms which can be viewed as the sampling analogues of the traditional plug-and-play priors (PnP) and regularization by denoising (RED) algorithms. We also establish a theoretical analysis for characterizing the convergence of the PMC algorithms. Our analysis provides non-asymptotic stationarity guarantees for both algorithms, even in the presence of non-log-concave likelihoods and imperfect score networks. We demonstrate the performance of the PMC algorithms on multiple representative inverse problems with both linear and nonlinear forward models. Experimental results show that PMC significantly improves reconstruction quality and enables high-fidelity uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>解决具有不整合性 inverse problem 的算法中，估计高质量图像并同时量化其不确定性是两个愿景。在这篇论文中，我们提出了插入式 Monte Carlo (PMC) 作为一种原理性的框架，用于描述解决一般 inverse problem 中可能的解空间。PMC 能够integrate expressive score-based生成模型，以实现高质量图像重建和不确定性量化。具体来说，我们介绍了两种 PMC 算法，可以视为传统的插入式 priors (PnP) 和 regularization by denoising (RED) 算法的抽象。我们还进行了理论分析，用于Characterizing PMC 算法的收敛性。我们的分析提供了不对称站点保证，即使likelihood 不是几何凹形的情况下，PMC 算法仍然能够收敛。我们在多个代表性的 inverse problem 中进行了实验，结果表明，PMC 可以显著提高重建质量和实现高精度的不确定性量化。
</details></li>
</ul>
<hr>
<h2 id="Vision-and-Language-Navigation-in-the-Real-World-via-Online-Visual-Language-Mapping"><a href="#Vision-and-Language-Navigation-in-the-Real-World-via-Online-Visual-Language-Mapping" class="headerlink" title="Vision and Language Navigation in the Real World via Online Visual Language Mapping"></a>Vision and Language Navigation in the Real World via Online Visual Language Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10822">http://arxiv.org/abs/2310.10822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengguang Xu, Hieu T. Nguyen, Christopher Amato, Lawson L. S. Wong</li>
<li>for: 提高移动机器人在未看过的环境中的导航效率，使其能够根据自然语言指令进行导航。</li>
<li>methods: 提出了一个新的导航框架，包括四个关键组件：一个基于LLMs的指令解析器、一个在线视觉语言映射器、一个基于语言索引的地方定位器和一个基于DD-PPO的本地控制器。</li>
<li>results: 在一个未看过的实验室环境中测试了该框架，无需调整，significantly outperformed了现有的VLN基线。<details>
<summary>Abstract</summary>
Navigating in unseen environments is crucial for mobile robots. Enhancing them with the ability to follow instructions in natural language will further improve navigation efficiency in unseen cases. However, state-of-the-art (SOTA) vision-and-language navigation (VLN) methods are mainly evaluated in simulation, neglecting the complex and noisy real world. Directly transferring SOTA navigation policies trained in simulation to the real world is challenging due to the visual domain gap and the absence of prior knowledge about unseen environments. In this work, we propose a novel navigation framework to address the VLN task in the real world. Utilizing the powerful foundation models, the proposed framework includes four key components: (1) an LLMs-based instruction parser that converts the language instruction into a sequence of pre-defined macro-action descriptions, (2) an online visual-language mapper that builds a real-time visual-language map to maintain a spatial and semantic understanding of the unseen environment, (3) a language indexing-based localizer that grounds each macro-action description into a waypoint location on the map, and (4) a DD-PPO-based local controller that predicts the action. We evaluate the proposed pipeline on an Interbotix LoCoBot WX250 in an unseen lab environment. Without any fine-tuning, our pipeline significantly outperforms the SOTA VLN baseline in the real world.
</details>
<details>
<summary>摘要</summary>
naviigating 无法看到环境是机器人 navigation 的关键。增强机器人能够遵循自然语言指令，将会进一步提高无法看到环境中的导航效率。然而，当前的VLN方法（state-of-the-art）主要在模拟环境中进行评估，忽略了实际世界的复杂和噪音。直接将模拟环境中训练的VLN策略传输到实际世界是困难的，因为视觉领域之间的差异和未知环境中的先验知识缺乏。在这种情况下，我们提出了一种新的导航框架，用于解决VLN任务在实际世界中。该框架包括四个关键组件：1. LLMs基础模型based instruction parser，将自然语言指令转换为一系列预定的macro-action描述。2. 在线视觉语言映射，实时建立视觉语言地图，以维护未知环境的空间和Semantic理解。3. 基于语言索引的本地化器，将每个macro-action描述映射到地图上的坐标位置。4. DD-PPO基于本地控制器，预测动作。我们在一个未知的实际环境中使用Interbotix LoCoBot WX250进行评估，无需精细调整，我们的管道在实际世界中显著超越了当前VLN基线。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Neural-Network-Model-for-Diabetic-Retinopathy-Feature-Extraction-and-Classification"><a href="#Convolutional-Neural-Network-Model-for-Diabetic-Retinopathy-Feature-Extraction-and-Classification" class="headerlink" title="Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification"></a>Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10806">http://arxiv.org/abs/2310.10806</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s21sharan/cnn_dr_detection">https://github.com/s21sharan/cnn_dr_detection</a></li>
<li>paper_authors: Sharan Subramanian, Leilani H. Gilpin</li>
<li>for: 这个研究旨在应用人工智能于医疗领域，尤其是检测无 симtomatic progressing 的疾病，如糖尿病retinopathy (DR)。</li>
<li>methods: 本研究使用了 convolutional Neural Network (CNN) 模型，通过输入照片后，可以正确地识别四种known DR特征，包括 micro-aneurysms、cotton wools、exudates 和 hemorrhages。</li>
<li>results: 本研究获得了97%的敏感度和71%的准确率，表明模型具有高度的可读性和抗过滤性。<details>
<summary>Abstract</summary>
The application of Artificial Intelligence in the medical market brings up increasing concerns but aids in more timely diagnosis of silent progressing diseases like Diabetic Retinopathy. In order to diagnose Diabetic Retinopathy (DR), ophthalmologists use color fundus images, or pictures of the back of the retina, to identify small distinct features through a difficult and time-consuming process. Our work creates a novel CNN model and identifies the severity of DR through fundus image input. We classified 4 known DR features, including micro-aneurysms, cotton wools, exudates, and hemorrhages, through convolutional layers and were able to provide an accurate diagnostic without additional user input. The proposed model is more interpretable and robust to overfitting. We present initial results with a sensitivity of 97% and an accuracy of 71%. Our contribution is an interpretable model with similar accuracy to more complex models. With that, our model advances the field of DR detection and proves to be a key step towards AI-focused medical diagnosis.
</details>
<details>
<summary>摘要</summary>
这个应用人工智能在医疗市场中带来的应用对于不明显进行诊断的疾病，如糖尿病肉眼病（DR），具有增长的担忧。为了诊断DR，医生会使用彩色背部影像（color fundus images），或者是背部 Retina 的照片，以便识别小型的明显特征。我们的工作创造了一个新的 convolutional neural network（CNN）模型，可以通过背部影像的输入来诊断DR的严重程度。我们分类了4种已知的DR特征，包括微型血管、绒毛、渗透物和出血，透过 convolutional layers 进行分类，并能够提供精确的诊断，不需要额外的使用者输入。我们的模型更加可读性和避免过拟合。我们给出了初步的结果，敏感性为97%，准确率为71%。我们的贡献是一个可读性好的模型，与更复杂的模型相比，具有相似的准确性。这个模型对于DR检测具有重要的进步，并且是人工智能在医疗诊断中的一个关键步骤。
</details></li>
</ul>
<hr>
<h2 id="LAMP-Learn-A-Motion-Pattern-for-Few-Shot-Based-Video-Generation"><a href="#LAMP-Learn-A-Motion-Pattern-for-Few-Shot-Based-Video-Generation" class="headerlink" title="LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation"></a>LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10769">http://arxiv.org/abs/2310.10769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RQ-Wu/LAMP">https://github.com/RQ-Wu/LAMP</a></li>
<li>paper_authors: Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, Xiangyu Zhang<br>for:LAMP is designed to learn motion patterns in text-to-video generation, with a focus on few-shot learning and efficient use of resources.methods:The LAMP framework uses a first-frame-conditioned pipeline with an off-the-shelf text-to-image model for content generation, and expands the pretrained 2D convolution layers to temporal-spatial motion learning layers. Shared-noise sampling is used to improve stability and flexibility.results:Extensive experiments show that LAMP can effectively learn motion patterns on limited data and generate high-quality videos, with applications in text-to-image diffusion, real-world image animation, and video editing. The code and models are available online.<details>
<summary>Abstract</summary>
With the impressive progress in diffusion-based text-to-image generation, extending such powerful generative ability to text-to-video raises enormous attention. Existing methods either require large-scale text-video pairs and a large number of training resources or learn motions that are precisely aligned with template videos. It is non-trivial to balance a trade-off between the degree of generation freedom and the resource costs for video generation. In our study, we present a few-shot-based tuning framework, LAMP, which enables text-to-image diffusion model Learn A specific Motion Pattern with 8~16 videos on a single GPU. Specifically, we design a first-frame-conditioned pipeline that uses an off-the-shelf text-to-image model for content generation so that our tuned video diffusion model mainly focuses on motion learning. The well-developed text-to-image techniques can provide visually pleasing and diverse content as generation conditions, which highly improves video quality and generation freedom. To capture the features of temporal dimension, we expand the pretrained 2D convolution layers of the T2I model to our novel temporal-spatial motion learning layers and modify the attention blocks to the temporal level. Additionally, we develop an effective inference trick, shared-noise sampling, which can improve the stability of videos with computational costs. Our method can also be flexibly applied to other tasks, e.g. real-world image animation and video editing. Extensive experiments demonstrate that LAMP can effectively learn the motion pattern on limited data and generate high-quality videos. The code and models are available at https://rq-wu.github.io/projects/LAMP.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>随着文本到图像生成技术的快速进步，扩展这种强大的生成能力到文本到视频生成引发了巨大的关注。现有方法可以通过大量的文本-视频对对和大量的训练资源来学习，但是很难平衡生成自由度和资源成本之间的折衔。在我们的研究中，我们提出了一个几个步骤基于的调整框架，名为LAMP，可以在单个GPU上使用8~16个视频进行调整。 Specifically，我们设计了一个基于首帧的管道，使用商业化的文本到图像模型来生成内容，以便我们的调整视频模型主要集中在动作学习。具有良好的文本到图像技术可以提供辐射的和多样化的生成条件，从而很大地提高视频质量和生成自由度。为了捕捉时间维度的特征，我们扩展了预训练的2D卷积层，并对它们进行我们的新的时空动作学习层，还修改了注意力块到时间层。此外，我们开发了一种有效的推理技术，分享噪声抽样，可以提高视频的稳定性。我们的方法可以适应其他任务，例如真实世界图像动画和视频编辑。广泛的实验证明了LAMP可以有效地学习动作模式，并生成高质量的视频。代码和模型可以在https://rq-wu.github.io/projects/LAMP中获得。
</details></li>
</ul>
<hr>
<h2 id="Deep-Conditional-Shape-Models-for-3D-cardiac-image-segmentation"><a href="#Deep-Conditional-Shape-Models-for-3D-cardiac-image-segmentation" class="headerlink" title="Deep Conditional Shape Models for 3D cardiac image segmentation"></a>Deep Conditional Shape Models for 3D cardiac image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10756">http://arxiv.org/abs/2310.10756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Athira J Jacob, Puneet Sharma, Daniel Ruckert</li>
<li>for: 医疗图像分析的第一步是准确地定义器官结构。</li>
<li>methods: 我们引入了一种新的分割算法，使用深度条件形状模型（DCSM）作为核心组件。该算法使用深度隐式形状表示，学习任何有兴趣的生物结构的模态无关形状模型，并通过自动检测或用户输入的特征点来适应图像。最后，我们添加了一个模态依赖的轻量级细节修正网络，以捕捉图像中没有表示的细节。</li>
<li>results: 我们在各种3D成像Modalities（对比增强CT、非对比CT、3D电子心征图像）中进行心脏左心室（LV）分割，并证明自动DCSM在非对比CT中超过基准，并且在对比CT和3DE中使用细节修正网络时，特别是在 Hausdorff 距离方面获得了显著改进。半自动DCSM使用用户输入的特征点，只在对比CT上训练，可达92%的 dice，对所有Modalities具有Equivalent或更好的性能。<details>
<summary>Abstract</summary>
Delineation of anatomical structures is often the first step of many medical image analysis workflows. While convolutional neural networks achieve high performance, these do not incorporate anatomical shape information. We introduce a novel segmentation algorithm that uses Deep Conditional Shape models (DCSMs) as a core component. Using deep implicit shape representations, the algorithm learns a modality-agnostic shape model that can generate the signed distance functions for any anatomy of interest. To fit the generated shape to the image, the shape model is conditioned on anatomic landmarks that can be automatically detected or provided by the user. Finally, we add a modality-dependent, lightweight refinement network to capture any fine details not represented by the implicit function. The proposed DCSM framework is evaluated on the problem of cardiac left ventricle (LV) segmentation from multiple 3D modalities (contrast-enhanced CT, non-contrasted CT, 3D echocardiography-3DE). We demonstrate that the automatic DCSM outperforms the baseline for non-contrasted CT without the local refinement, and with the refinement for contrasted CT and 3DE, especially with significant improvement in the Hausdorff distance. The semi-automatic DCSM with user-input landmarks, while only trained on contrasted CT, achieves greater than 92% Dice for all modalities. Both automatic DCSM with refinement and semi-automatic DCSM achieve equivalent or better performance compared to inter-user variability for these modalities.
</details>
<details>
<summary>摘要</summary>
医学图像分析工作流程中的解剖结构定义是第一步。卷积神经网络可以达到高性能，但不会包含解剖形态信息。我们介绍了一种新的分割算法，使用深度条件形状模型（DCSM）作为核心组件。使用深度隐式形状表示，算法学习了任意解剖结构的模态独立形状模型，可以生成任意解剖结构的签名距离函数。为了使Shape模型适应图像，Shape模型被conditioned on可自动探测或提供的解剖标志。最后，我们添加了一个模态依赖的轻量级细节修正网络，以捕捉无法由隐式函数表示的细节。我们提出的DCSM框架在cardiac left ventricle（LV）三维模态（对比增强CT、非对比CT和3DE）的分割问题上进行了评估。我们表明，自动DCSM比基线高效，而且与使用本地细节修正有显著改善。用户输入标志的半自动DCSM，即使只在对比CT上训练，可以达到92%的Dice指标或更高。自动DCSM和半自动DCSM都与人类间变化相当或更好，对这些模态来说。
</details></li>
</ul>
<hr>
<h2 id="IDRNet-Intervention-Driven-Relation-Network-for-Semantic-Segmentation"><a href="#IDRNet-Intervention-Driven-Relation-Network-for-Semantic-Segmentation" class="headerlink" title="IDRNet: Intervention-Driven Relation Network for Semantic Segmentation"></a>IDRNet: Intervention-Driven Relation Network for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10755">http://arxiv.org/abs/2310.10755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/segmentationblwx/sssegmentation">https://github.com/segmentationblwx/sssegmentation</a></li>
<li>paper_authors: Zhenchao Jin, Xiaowei Hu, Lingting Zhu, Luchuan Song, Li Yuan, Lequan Yu</li>
<li>for: 提高 dense prediction 任务中的 contextual information 聚合</li>
<li>methods: 利用 deletion diagnostics 过程模型 contextual relations among different pixels, 并使用 feature enhancement module 进一步提高 distinguishability</li>
<li>results: 对 state-of-the-art segmentation frameworks 带来了一致性的性能提升, 并在各种标准评价 metrics 上达到了竞争性的结果<details>
<summary>Abstract</summary>
Co-occurrent visual patterns suggest that pixel relation modeling facilitates dense prediction tasks, which inspires the development of numerous context modeling paradigms, \emph{e.g.}, multi-scale-driven and similarity-driven context schemes. Despite the impressive results, these existing paradigms often suffer from inadequate or ineffective contextual information aggregation due to reliance on large amounts of predetermined priors. To alleviate the issues, we propose a novel \textbf{I}ntervention-\textbf{D}riven \textbf{R}elation \textbf{Net}work (\textbf{IDRNet}), which leverages a deletion diagnostics procedure to guide the modeling of contextual relations among different pixels. Specifically, we first group pixel-level representations into semantic-level representations with the guidance of pseudo labels and further improve the distinguishability of the grouped representations with a feature enhancement module. Next, a deletion diagnostics procedure is conducted to model relations of these semantic-level representations via perceiving the network outputs and the extracted relations are utilized to guide the semantic-level representations to interact with each other. Finally, the interacted representations are utilized to augment original pixel-level representations for final predictions. Extensive experiments are conducted to validate the effectiveness of IDRNet quantitatively and qualitatively. Notably, our intervention-driven context scheme brings consistent performance improvements to state-of-the-art segmentation frameworks and achieves competitive results on popular benchmark datasets, including ADE20K, COCO-Stuff, PASCAL-Context, LIP, and Cityscapes. Code is available at \url{https://github.com/SegmentationBLWX/sssegmentation}.
</details>
<details>
<summary>摘要</summary>
伴生视觉模式表明，像素关系模型化可以促进紧凑预测任务，这引发了许多上下文模型范文的发展，如多scale-driven和相似性-driven上下文方案。 despite the impressive results, these existing paradigms often suffer from inadequate or ineffective contextual information aggregation due to reliance on large amounts of predetermined priors. To address these issues, we propose a novel \textbf{I}ntervention-\textbf{D}riven \textbf{R}elation \textbf{Net}work (\textbf{IDRNet}), which leverages a deletion diagnostics procedure to guide the modeling of contextual relations among different pixels. Specifically, we first group pixel-level representations into semantic-level representations with the guidance of pseudo labels and further improve the distinguishability of the grouped representations with a feature enhancement module. Next, a deletion diagnostics procedure is conducted to model relations of these semantic-level representations via perceiving the network outputs and the extracted relations are utilized to guide the semantic-level representations to interact with each other. Finally, the interacted representations are utilized to augment original pixel-level representations for final predictions. Extensive experiments are conducted to validate the effectiveness of IDRNet quantitatively and qualitatively. Notably, our intervention-driven context scheme brings consistent performance improvements to state-of-the-art segmentation frameworks and achieves competitive results on popular benchmark datasets, including ADE20K, COCO-Stuff, PASCAL-Context, LIP, and Cityscapes. Code is available at \url{https://github.com/SegmentationBLWX/sssegmentation}.
</details></li>
</ul>
<hr>
<h2 id="HairCLIPv2-Unifying-Hair-Editing-via-Proxy-Feature-Blending"><a href="#HairCLIPv2-Unifying-Hair-Editing-via-Proxy-Feature-Blending" class="headerlink" title="HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending"></a>HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10651">http://arxiv.org/abs/2310.10651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wty-ustc/hairclipv2">https://github.com/wty-ustc/hairclipv2</a></li>
<li>paper_authors: Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Gang Hua, Nenghai Yu</li>
<li>for: 提供一个能够基于文本描述或参考图像进行头发编辑的框架，并且支持多种交互方式，包括文本描述、参考图像和绘制笔触等。</li>
<li>methods: 使用交叉模式模型（如CLIP）将头发编辑转化为头发传送任务，并将编辑条件转化为不同的代理特征。在输入图像上加载编辑效果，通过在头发特征空间中混合相应的代理特征来实现。</li>
<li>results: 对比于原始HairCLIP，HairCLIPv2可以更好地保持无关特征（如人脸特征、背景特征），同时支持未before seen文本描述和不同交互方式。量化和质量实验表明，HairCLIPv2在编辑效果、无关特征保持和视觉自然性等方面具有显著优势。<details>
<summary>Abstract</summary>
Hair editing has made tremendous progress in recent years. Early hair editing methods use well-drawn sketches or masks to specify the editing conditions. Even though they can enable very fine-grained local control, such interaction modes are inefficient for the editing conditions that can be easily specified by language descriptions or reference images. Thanks to the recent breakthrough of cross-modal models (e.g., CLIP), HairCLIP is the first work that enables hair editing based on text descriptions or reference images. However, such text-driven and reference-driven interaction modes make HairCLIP unable to support fine-grained controls specified by sketch or mask. In this paper, we propose HairCLIPv2, aiming to support all the aforementioned interactions with one unified framework. Simultaneously, it improves upon HairCLIP with better irrelevant attributes (e.g., identity, background) preservation and unseen text descriptions support. The key idea is to convert all the hair editing tasks into hair transfer tasks, with editing conditions converted into different proxies accordingly. The editing effects are added upon the input image by blending the corresponding proxy features within the hairstyle or hair color feature spaces. Besides the unprecedented user interaction mode support, quantitative and qualitative experiments demonstrate the superiority of HairCLIPv2 in terms of editing effects, irrelevant attribute preservation and visual naturalness. Our code is available at \url{https://github.com/wty-ustc/HairCLIPv2}.
</details>
<details>
<summary>摘要</summary>
随笔修整技术在最近几年来取得了巨大的进步。早期的修整方法通常使用细致绘制的素描或面Mask来指定修整条件。尽管它们可以实现非常细致的本地控制，但是这些交互方式在基于语言描述或参考图像的修整条件上是不效率的。感谢最近的交互模型技术（如CLIP）的突破，我们的HairCLIP是首个可以基于语言描述或参考图像进行随笔修整的工作。然而，这些基于文本描述或参考图像的交互方式使得HairCLIP无法支持细致的素描或面Mask来指定修整条件。在这篇论文中，我们提出了HairCLIPv2，旨在支持所有的交互方式，同时也提高了不相关特征（如人脸和背景）的保留和未看到文本描述的支持。我们的关键思想是将所有的随笔修整任务转换为随笔传输任务，并将编辑条件转换为不同的代理 accordingly。然后，在输入图像上添加修整效果，通过在额发型或发色特征空间中混合相应的代理特征。除了不同的用户交互方式支持外，我们的HairCLIPv2还在编辑效果、不相关特征保留和视觉自然性方面具有显著优势。我们的代码可以在GitHub上找到：<https://github.com/wty-ustc/HairCLIPv2>。
</details></li>
</ul>
<hr>
<h2 id="TraM-NeRF-Tracing-Mirror-and-Near-Perfect-Specular-Reflections-through-Neural-Radiance-Fields"><a href="#TraM-NeRF-Tracing-Mirror-and-Near-Perfect-Specular-Reflections-through-Neural-Radiance-Fields" class="headerlink" title="TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields"></a>TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10650">http://arxiv.org/abs/2310.10650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Rubikalubi/TraM-NeRF">https://github.com/Rubikalubi/TraM-NeRF</a></li>
<li>paper_authors: Leif Van Holland, Ruben Bliersbach, Jan U. Müller, Patrick Stotko, Reinhard Klein</li>
<li>for: 实现复杂场景中细节轻松渲染，如镜子等具有偏光反射的物体。</li>
<li>methods: 采用物理可能的材料模型和Monte-Carlo方法在Volume Rendering中厘定反射行为，实现重要抽象和透射计算。</li>
<li>results: 实现了对这些挑战场景的一致射预测和uperior的效果，较前一代方法更好。<details>
<summary>Abstract</summary>
Implicit representations like Neural Radiance Fields (NeRF) showed impressive results for photorealistic rendering of complex scenes with fine details. However, ideal or near-perfectly specular reflecting objects such as mirrors, which are often encountered in various indoor scenes, impose ambiguities and inconsistencies in the representation of the reconstructed scene leading to severe artifacts in the synthesized renderings. In this paper, we present a novel reflection tracing method tailored for the involved volume rendering within NeRF that takes these mirror-like objects into account while avoiding the cost of straightforward but expensive extensions through standard path tracing. By explicitly modeling the reflection behavior using physically plausible materials and estimating the reflected radiance with Monte-Carlo methods within the volume rendering formulation, we derive efficient strategies for importance sampling and the transmittance computation along rays from only few samples. We show that our novel method enables the training of consistent representations of such challenging scenes and achieves superior results in comparison to previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用各种各样的各种方法来描述复杂场景的隐式表示，如神经辐射场（NeRF），已经取得了高度真实的渲染复杂场景的成果。然而，在室内场景中遇到的 идеаль或几乎完美的 espejo 反射物体，如镜子，会导致描述重建场景的表示具有歧义和不一致，从而导致渲染 synthesized 图像中的 artifacts。在这篇论文中，我们提出了一种专门为 NeRF 中 involve 体积渲染中的反射跟踪方法，该方法能够考虑这些 espejo 类型的物体，而不需要 straightforward 而且昂贵的扩展。我们通过物理可能的材料模型和 Monte-Carlo 方法来表示反射行为，从而 deriv 高效的重要抽象策略和传播计算方法。我们示示了我们的新方法可以培养 consistent 的表示这些复杂场景，并且在 compared 到先前的状态的艺术方法上达到了更高的成果。
</details></li>
</ul>
<hr>
<h2 id="TOSS-High-quality-Text-guided-Novel-View-Synthesis-from-a-Single-Image"><a href="#TOSS-High-quality-Text-guided-Novel-View-Synthesis-from-a-Single-Image" class="headerlink" title="TOSS:High-quality Text-guided Novel View Synthesis from a Single Image"></a>TOSS:High-quality Text-guided Novel View Synthesis from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10644">http://arxiv.org/abs/2310.10644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, Heung-Yeung Shum</li>
<li>for: 本研究旨在提出一种基于文本Semantic guidance的novel view synthesis（NVS）方法，以解决单视图NVS问题的不足约束性。</li>
<li>methods: 本方法使用文本作为高级 semantic information来约束NVS解决方案空间，并引入了特定于图像和摄像机pose conditioning的模块，以及专门为pose正确性和细节细节加权训练。</li>
<li>results: 对比Zero-1-to-3，本研究的提议TOSS实现了更可信、控制性和多视图一致的NVS结果，并通过了全面的ablations来证明引入的Semantic guidance和建筑设计的有效性。<details>
<summary>Abstract</summary>
In this paper, we present TOSS, which introduces text to the task of novel view synthesis (NVS) from just a single RGB image. While Zero-1-to-3 has demonstrated impressive zero-shot open-set NVS capability, it treats NVS as a pure image-to-image translation problem. This approach suffers from the challengingly under-constrained nature of single-view NVS: the process lacks means of explicit user control and often results in implausible NVS generations. To address this limitation, TOSS uses text as high-level semantic information to constrain the NVS solution space. TOSS fine-tunes text-to-image Stable Diffusion pre-trained on large-scale text-image pairs and introduces modules specifically tailored to image and camera pose conditioning, as well as dedicated training for pose correctness and preservation of fine details. Comprehensive experiments are conducted with results showing that our proposed TOSS outperforms Zero-1-to-3 with more plausible, controllable and multiview-consistent NVS results. We further support these results with comprehensive ablations that underscore the effectiveness and potential of the introduced semantic guidance and architecture design.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了TOSS，它将文本引入到novel view synthesis（NVS）任务中，只需基于单个RGB图像。 zero-1-to-3 已经表现出了非常出色的零基础开放式 NVS 能力，但是这种方法在单视图 NVS 中存在一些缺乏约束的问题：没有明确的用户控制方式，导致 NVS 生成结果往往不太可能。为了解决这个限制，TOSS 使用文本作为高级Semantic信息来约束 NVS 解决方案空间。TOSS 细致地调整了文本-图像 Stable Diffusion 预训练的大规模文本-图像对，并 introduce了专门为图像和摄像头姿态conditioning设计的模块，以及专门为posecorrectness和细节细节训练。我们进行了全面的实验，结果显示了我们提出的 TOSS 比 zero-1-to-3 更加plausible、可控和多视图一致的 NVS 结果。我们还进行了详细的ablation，以证明引入的semantic导航和建筑设计的效果和潜在。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Photorealistic-Dynamic-Scene-Representation-and-Rendering-with-4D-Gaussian-Splatting"><a href="#Real-time-Photorealistic-Dynamic-Scene-Representation-and-Rendering-with-4D-Gaussian-Splatting" class="headerlink" title="Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting"></a>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10642">http://arxiv.org/abs/2310.10642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fudan-zvg/4d-gaussian-splatting">https://github.com/fudan-zvg/4d-gaussian-splatting</a></li>
<li>paper_authors: Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, Li Zhang</li>
<li>for: 生成复杂的动态场景3D图像和不同时间点的视图</li>
<li>methods: 使用4D primitives优化approximateunderlying spacetime volume，包括视角 dependent和时间演化的外观</li>
<li>results: 提供了一种简单、灵活、可变长视频和终端培育的方法，能够capture复杂的动态场景运动，并且在实验中达到了较高的视觉质量和效率。<details>
<summary>Abstract</summary>
Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial and temporal structure of dynamic scenes from directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as an entirety and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi-view scenarios, demonstrate our 4DGS model's superior visual quality and efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified ChineseDynamic 3D scene reconstruction from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, there are still limitations: (i) Inadequate Scene Structure: Existing methods cannot effectively reveal the spatial and temporal structure of dynamic scenes by directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as a whole and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi-view scenarios, demonstrate our 4DGS model's superior visual quality and efficiency.Translated by Google Translate
</details></li>
</ul>
<hr>
<h2 id="LLM-Blueprint-Enabling-Text-to-Image-Generation-with-Complex-and-Detailed-Prompts"><a href="#LLM-Blueprint-Enabling-Text-to-Image-Generation-with-Complex-and-Detailed-Prompts" class="headerlink" title="LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts"></a>LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10640">http://arxiv.org/abs/2310.10640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hananshafi/llmblueprint">https://github.com/hananshafi/llmblueprint</a></li>
<li>paper_authors: Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, Peter Wonka</li>
<li>for: 实现文本描述中的复杂场景和多个物件的图像生成</li>
<li>methods: 利用大自然语言模型提取文本描述中的关键元素，包括物件 bounding box 坐标、个别物品的详细描述和背景内容，然后使用这些元素生成图像</li>
<li>results: 与基准扩散模型相比，实现了复杂文本描述中的图像生成，并在用户评估中获得了高度的认可和满意度<details>
<summary>Abstract</summary>
Diffusion-based generative models have significantly advanced text-to-image generation but encounter challenges when processing lengthy and intricate text prompts describing complex scenes with multiple objects. While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs. In response, we present a novel approach leveraging Large Language Models (LLMs) to extract critical components from text prompts, including bounding box coordinates for foreground objects, detailed textual descriptions for individual objects, and a succinct background context. These components form the foundation of our layout-to-image generation model, which operates in two phases. The initial Global Scene Generation utilizes object layouts and background context to create an initial scene but often falls short in faithfully representing object characteristics as specified in the prompts. To address this limitation, we introduce an Iterative Refinement Scheme that iteratively evaluates and refines box-level content to align them with their textual descriptions, recomposing objects as needed to ensure consistency. Our evaluation on complex prompts featuring multiple objects demonstrates a substantial improvement in recall compared to baseline diffusion models. This is further validated by a user study, underscoring the efficacy of our approach in generating coherent and detailed scenes from intricate textual inputs.
</details>
<details>
<summary>摘要</summary>
文本到图像生成技术在进行长度和复杂性增加时遇到了挑战。尽管在简短的单个对象描述中表现出色，但是在处理长度和复杂性更高的文本提示时，这些模型经常遇到困难，不能准确地捕捉文本中的细节。为了解决这问题，我们提出了一种新的方法，利用大型自然语言模型（LLM）来提取文本提示中的关键组成部分，包括主要对象的 bounding box 坐标、对象的详细文本描述和背景 контекст。这些组成部分成为我们的布局到图像生成模型的基础，该模型在两个阶段进行处理。首先，我们使用对象布局和背景 контекст来创建初始场景，但是这些场景经常无法准确地表现出对象的特征。为了解决这个限制，我们提出了一种迭代优化方案，通过评估和修改框架内容，使其与文本描述相符。我们的评估表明，对于包含多个对象的复杂提示，我们的方法可以提高了回归率，并且在用户研究中得到了证明。Translation notes:* "Diffusion-based generative models" is translated as "文本到图像生成技术" (text-to-image generation technology)* "long and intricate text prompts" is translated as "长度和复杂性更高的文本提示" (text prompts with length and complexity)* "nuanced details" is translated as "细节" (details)* "Large Language Models" is translated as "大型自然语言模型" (large language models)* " bounding box coordinates" is translated as " bounding box 坐标" (bounding box coordinates)* "detailed textual descriptions" is translated as "详细文本描述" (detailed textual descriptions)* "succinct background context" is translated as "背景 контекст" (background context)* "Global Scene Generation" is translated as "全球场景生成" (global scene generation)* "Iterative Refinement Scheme" is translated as "迭代优化方案" (iterative refinement scheme)* "box-level content" is translated as "框架内容" (box-level content)* "recomposing objects" is translated as "重新组合对象" (recomposing objects)* "consistency" is translated as "一致性" (consistency)* "user study" is translated as "用户研究" (user study)
</details></li>
</ul>
<hr>
<h2 id="DynVideo-E-Harnessing-Dynamic-NeRF-for-Large-Scale-Motion-and-View-Change-Human-Centric-Video-Editing"><a href="#DynVideo-E-Harnessing-Dynamic-NeRF-for-Large-Scale-Motion-and-View-Change-Human-Centric-Video-Editing" class="headerlink" title="DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing"></a>DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10624">http://arxiv.org/abs/2310.10624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou</li>
<li>for: 这个论文的目的是提出一种基于动态神经辐射场（NeRF）的人称视频编辑方法，以解决现有方法因为视频长度和视角变化而受限的问题。</li>
<li>methods: 这种方法使用动态NeRF作为人称视频表示，并提出了一种基于多视图多姿Score Distillation Sampling（SDS）、图像恢复损失、文本导向地方部超分辨率、风格传递等多种技术的图像三维空间编辑管线。</li>
<li>results: 与比较方法相比，这种方法在两个难度较大的数据集上显示出了大幅提高（50%~95%）的人类喜好度。具体比较结果可以查看到项目页面<a target="_blank" rel="noopener" href="https://showlab.github.io/DynVideo-E/">https://showlab.github.io/DynVideo-E/</a>.<details>
<summary>Abstract</summary>
Despite remarkable research advances in diffusion-based video editing, existing methods are limited to short-length videos due to the contradiction between long-range consistency and frame-wise editing. Recent approaches attempt to tackle this challenge by introducing video-2D representations to degrade video editing to image editing. However, they encounter significant difficulties in handling large-scale motion- and view-change videos especially for human-centric videos. This motivates us to introduce the dynamic Neural Radiance Fields (NeRF) as the human-centric video representation to ease the video editing problem to a 3D space editing task. As such, editing can be performed in the 3D spaces and propagated to the entire video via the deformation field. To provide finer and direct controllable editing, we propose the image-based 3D space editing pipeline with a set of effective designs. These include multi-view multi-pose Score Distillation Sampling (SDS) from both 2D personalized diffusion priors and 3D diffusion priors, reconstruction losses on the reference image, text-guided local parts super-resolution, and style transfer for 3D background space. Extensive experiments demonstrate that our method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% ~ 95% in terms of human preference. Compelling video comparisons are provided in the project page https://showlab.github.io/DynVideo-E/. Our code and data will be released to the community.
</details>
<details>
<summary>摘要</summary>
尽管扩展视频编辑技术已取得了很大的进步，但现有方法仅适用于短视频，因为视频编辑和框架之间存在长距离一致性和帧级编辑之间的矛盾。现有的方法通过引入视频到2D表示来减轻视频编辑到图像编辑。然而，它们在处理大规模运动和视点变化视频，特别是人类中心视频时遇到了重大困难。这个问题驱使我们提出人类中心视频表示——动态神经辐射场（NeRF），以便将视频编辑转化为3D空间编辑任务。在这种情况下，编辑可以在3D空间进行，并通过扭曲场进行对整个视频的广泛传播。为了提供更加精细和直接控制的编辑，我们提议了基于图像的3D空间编辑管线，包括多视图多姿Score Distillation Sampling（SDS）、参考图像的重建损失、文本指导的本地部分超解析和风格转换。我们的方法，即DynVideo-E，在两个挑战性 datasets 上达到了领先的水平，与前一代方法的比较达到了50%~95%的差距。我们在项目页面（https://showlab.github.io/DynVideo-E/）提供了吸引人的视频比较。我们的代码和数据将被公开发布到社区。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Controlling-Vision-Foundation-Models-via-Text-Explanations"><a href="#Interpreting-and-Controlling-Vision-Foundation-Models-via-Text-Explanations" class="headerlink" title="Interpreting and Controlling Vision Foundation Models via Text Explanations"></a>Interpreting and Controlling Vision Foundation Models via Text Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10591">http://arxiv.org/abs/2310.10591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonychenxyz/vit-interpret">https://github.com/tonychenxyz/vit-interpret</a></li>
<li>paper_authors: Haozhe Chen, Junfeng Yang, Carl Vondrick, Chengzhi Mao</li>
<li>for: 本研究旨在理解和控制CLIP等大规模预训练视觉基础模型的预测结果。</li>
<li>methods: 本研究使用了一种基于自然语言的方法来解释视 transformer 的干 tokens，并通过捕捉最近的文本来进行解释。</li>
<li>results: 本研究可以帮助理解CLIP等模型在视觉任务中的决策过程，并提供了一种控制模型行为的方法，以提高模型的Robustness和减少偏见和偶合关系。<details>
<summary>Abstract</summary>
Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models' predictions and controlling model behaviors have remained open challenges. We present a framework for interpreting vision transformer's latent tokens with natural language. Given a latent token, our framework retains its semantic information to the final layer using transformer's local operations and retrieves the closest text for explanation. Our approach enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, our framework allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations.
</details>
<details>
<summary>摘要</summary>
大规模预训练视觉基础模型，如CLIP，已成为视觉任务的德 факто底层。然而，由于其黑盒模型的性质，理解这些模型预测的下面规则和控制模型行为仍然是开放的挑战。我们提出了一个把视觉转换器的幂谱Token与自然语言相关的框架。给定一个幂谱Token，我们的框架使用转换器的地方运算保留它的含义到最终层，并从文本库中检索最相似的文本来解释。我们的方法可以在不需要额外训练或数据收集的前提下，理解模型的视觉逻辑过程。基于获得的解释，我们的框架允许对模型的编辑，控制模型的逻辑行为，并改善模型免疫偏见和偶极相关性。
</details></li>
</ul>
<hr>
<h2 id="Matching-the-Neuronal-Representations-of-V1-is-Necessary-to-Improve-Robustness-in-CNNs-with-V1-like-Front-ends"><a href="#Matching-the-Neuronal-Representations-of-V1-is-Necessary-to-Improve-Robustness-in-CNNs-with-V1-like-Front-ends" class="headerlink" title="Matching the Neuronal Representations of V1 is Necessary to Improve Robustness in CNNs with V1-like Front-ends"></a>Matching the Neuronal Representations of V1 is Necessary to Improve Robustness in CNNs with V1-like Front-ends</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10575">http://arxiv.org/abs/2310.10575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dicarlolab/vonenet">https://github.com/dicarlolab/vonenet</a></li>
<li>paper_authors: Ruxandra Barbulescu, Tiago Marques, Arlindo L. Oliveira</li>
<li>for: 提高对图像损害的鲁棒性（robustness to image corruptions）</li>
<li>methods: 模拟肉眼初级视觉区域（primate primary visual cortex）的计算，使得模型具有更高的鲁棒性</li>
<li>results: 模型使用生物学发现的电场特性分布（empirical biological distributions） sampling，对图像损害的鲁棒性明显更高（相对差异为8.72%），而同类 neuronal sub-populations 在两个模型中具有相似的响应特性和下游权重学习结果，但下游处理具有不同的影响。<details>
<summary>Abstract</summary>
While some convolutional neural networks (CNNs) have achieved great success in object recognition, they struggle to identify objects in images corrupted with different types of common noise patterns. Recently, it was shown that simulating computations in early visual areas at the front of CNNs leads to improvements in robustness to image corruptions. Here, we further explore this result and show that the neuronal representations that emerge from precisely matching the distribution of RF properties found in primate V1 is key for this improvement in robustness. We built two variants of a model with a front-end modeling the primate primary visual cortex (V1): one sampling RF properties uniformly and the other sampling from empirical biological distributions. The model with the biological sampling has a considerably higher robustness to image corruptions that the uniform variant (relative difference of 8.72%). While similar neuronal sub-populations across the two variants have similar response properties and learn similar downstream weights, the impact on downstream processing is strikingly different. This result sheds light on the origin of the improvements in robustness observed in some biologically-inspired models, pointing to the need of precisely mimicking the neuronal representations found in the primate brain.
</details>
<details>
<summary>摘要</summary>
有些卷积神经网络（CNN）在物体识别方面取得了很大的成功，但它们在受到不同类型的常见噪声损害后仍然难以识别物体。最近，研究人员发现，在CNN的前端模型中 simulate  computations 可以提高对图像损害的Robustness。在这里，我们进一步探索这一结论，并证明了模型在 precisely 匹配了黑眼睛动物V1中的观察者分布 Property 后，会带来更高的Robustness。我们建立了两个模型，其中一个采样RF Property uniform，另一个采样从empirical biological distribution。与uniform variant相比，使用生物分布采样的模型具有更高的Robustness to image corruptions（相对差异为8.72%）。尽管这两个模型中的 neuronal sub-populations 具有相似的response property和downstream weights，但是它们对下游处理的影响却是不同的。这一结论 shed light  onto the origin of the improvements in robustness observed in some biologically-inspired models, pointing to the need of precisely mimicking the neuronal representations found in the primate brain.
</details></li>
</ul>
<hr>
<h2 id="RefConv-Re-parameterized-Refocusing-Convolution-for-Powerful-ConvNets"><a href="#RefConv-Re-parameterized-Refocusing-Convolution-for-Powerful-ConvNets" class="headerlink" title="RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets"></a>RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10563">http://arxiv.org/abs/2310.10563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aiolus-X/RefConv">https://github.com/Aiolus-X/RefConv</a></li>
<li>paper_authors: Zhicheng Cai, Xiaohan Ding, Qiu Shen, Xun Cao</li>
<li>for: 提高 CNN 模型的性能，无需更改原始模型结构或添加新的推理成本。</li>
<li>methods: 使用可调 Refocusing Transformation 修改基础核函数，使得不同通道的参数之间建立连接，从而提高模型表达能力。</li>
<li>results: 在图像分类、物体检测和 semantic segmentation 等 CNN 模型中，使用 RefConv 可以提高性能（ImageNet 上的顶部一 accuracy 提高至 1.47%），而无需增加推理成本或改变原始模型结构。<details>
<summary>Abstract</summary>
We propose Re-parameterized Refocusing Convolution (RefConv) as a replacement for regular convolutional layers, which is a plug-and-play module to improve the performance without any inference costs. Specifically, given a pre-trained model, RefConv applies a trainable Refocusing Transformation to the basis kernels inherited from the pre-trained model to establish connections among the parameters. For example, a depth-wise RefConv can relate the parameters of a specific channel of convolution kernel to the parameters of the other kernel, i.e., make them refocus on the other parts of the model they have never attended to, rather than focus on the input features only. From another perspective, RefConv augments the priors of existing model structures by utilizing the representations encoded in the pre-trained parameters as the priors and refocusing on them to learn novel representations, thus further enhancing the representational capacity of the pre-trained model. Experimental results validated that RefConv can improve multiple CNN-based models by a clear margin on image classification (up to 1.47% higher top-1 accuracy on ImageNet), object detection and semantic segmentation without introducing any extra inference costs or altering the original model structure. Further studies demonstrated that RefConv can reduce the redundancy of channels and smooth the loss landscape, which explains its effectiveness.
</details>
<details>
<summary>摘要</summary>
我们提议使用Re-parameterized Refocusing Convolution（RefConv）取代常规 convolutional layer，这是一个可插入的模块，可以无需更改预测成本提高性能。具体来说，给定一个预训练模型，RefConv将预训练模型继承的基准kernel应用一个可学习的 Refocusing Transformation，以建立模型参数之间的连接。例如，深度 wise RefConv可以将一个特定通道的 convolution kernel 的参数与其他kernel的参数相关联，例如，使得这些参数强调其他部分的模型，而不是仅仅强调输入特征。从另一个角度来看，RefConv可以利用预训练参数中的代表性作为PRIOR，并强调这些代表性，以学习新的表示，从而进一步提高预训练模型的表达能力。实验结果表明，RefConv可以在图像分类（最高达1.47%的top-1准确率提升在ImageNet）、物体检测和 semantic segmentation 中提高多个CNN基于模型的性能，而无需增加预测成本或改变原始模型结构。进一步的研究还表明，RefConv可以减少通道的重复性和缓和损失函数的曲线，这解释了它的效果。
</details></li>
</ul>
<hr>
<h2 id="InfoGCN-Learning-Representation-by-Predicting-the-Future-for-Online-Human-Skeleton-based-Action-Recognition"><a href="#InfoGCN-Learning-Representation-by-Predicting-the-Future-for-Online-Human-Skeleton-based-Action-Recognition" class="headerlink" title="InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition"></a>InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10547">http://arxiv.org/abs/2310.10547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stnoah1/sode">https://github.com/stnoah1/sode</a></li>
<li>paper_authors: Seunggeun Chi, Hyung-gun Chi, Qixing Huang, Karthik Ramani</li>
<li>for: online skeleton-based action recognition</li>
<li>methods: InfoGCN++, a novel extension of InfoGCN that enables real-time categorization of action types without complete observation sequences</li>
<li>results: exceptional performance in online action recognition, consistently matching or exceeding existing techniques<details>
<summary>Abstract</summary>
Skeleton-based action recognition has made significant advancements recently, with models like InfoGCN showcasing remarkable accuracy. However, these models exhibit a key limitation: they necessitate complete action observation prior to classification, which constrains their applicability in real-time situations such as surveillance and robotic systems. To overcome this barrier, we introduce InfoGCN++, an innovative extension of InfoGCN, explicitly developed for online skeleton-based action recognition. InfoGCN++ augments the abilities of the original InfoGCN model by allowing real-time categorization of action types, independent of the observation sequence's length. It transcends conventional approaches by learning from current and anticipated future movements, thereby creating a more thorough representation of the entire sequence. Our approach to prediction is managed as an extrapolation issue, grounded on observed actions. To enable this, InfoGCN++ incorporates Neural Ordinary Differential Equations, a concept that lets it effectively model the continuous evolution of hidden states. Following rigorous evaluations on three skeleton-based action recognition benchmarks, InfoGCN++ demonstrates exceptional performance in online action recognition. It consistently equals or exceeds existing techniques, highlighting its significant potential to reshape the landscape of real-time action recognition applications. Consequently, this work represents a major leap forward from InfoGCN, pushing the limits of what's possible in online, skeleton-based action recognition. The code for InfoGCN++ is publicly available at https://github.com/stnoah1/infogcn2 for further exploration and validation.
</details>
<details>
<summary>摘要</summary>
InfoGCN++ 是一种在线动作识别模型，它是 InfoGCN 的一种创新扩展。InfoGCN++ 可以在实时情况下进行动作类型分类，不需要完整的动作观察序列。它超越了传统方法，通过学习当前和预测未来动作的整体表示来提高模型的表示能力。我们采用了神经网络普通微分方程来管理预测问题，以便有效地模型动作的不断演化。经过严格的评估，InfoGCN++ 在三个骨干基于动作识别benchmark上表现出色， consistently 与或超过了现有方法的性能。这成功表明InfoGCN++ 在实时动作识别应用中具有重要的潜力。因此，这种工作代表了 InfoGCN 的一个重要突破，推动了在线骨干基于动作识别领域的发展。InfoGCN++ 的代码可以在 <https://github.com/stnoah1/infogcn2> 上公开下载，以便进一步探索和验证。
</details></li>
</ul>
<hr>
<h2 id="Label-efficient-Segmentation-via-Affinity-Propagation"><a href="#Label-efficient-Segmentation-via-Affinity-Propagation" class="headerlink" title="Label-efficient Segmentation via Affinity Propagation"></a>Label-efficient Segmentation via Affinity Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10533">http://arxiv.org/abs/2310.10533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/circleradon/apro">https://github.com/circleradon/apro</a></li>
<li>paper_authors: Wentong Li, Yuqian Yuan, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang</li>
<li>for: 降低寸劳的像素精度标注成本</li>
<li>methods: 提出了一种基于对称协议的对应关系建模方法，包括本地和全局对应关系项</li>
<li>results: 在三种标注任务上（包括INSTANCE Segmentation、semantic Segmentation和CLIP-引导Semantic Segmentation）达到了高度的性能提升<details>
<summary>Abstract</summary>
Weakly-supervised segmentation with label-efficient sparse annotations has attracted increasing research attention to reduce the cost of laborious pixel-wise labeling process, while the pairwise affinity modeling techniques play an essential role in this task. Most of the existing approaches focus on using the local appearance kernel to model the neighboring pairwise potentials. However, such a local operation fails to capture the long-range dependencies and ignores the topology of objects. In this work, we formulate the affinity modeling as an affinity propagation process, and propose a local and a global pairwise affinity terms to generate accurate soft pseudo labels. An efficient algorithm is also developed to reduce significantly the computational cost. The proposed approach can be conveniently plugged into existing segmentation networks. Experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmentation, point/scribble-supervised semantic segmentation and CLIP-guided semantic segmentation, demonstrate the superior performance of the proposed approach.
</details>
<details>
<summary>摘要</summary>
弱监督分割的研究已经吸引了越来越多的关注，以减少繁琐的像素精确标注过程的成本。在这个任务中，对 neighboring pairwise 潜在力场的建模技术扮演着关键角色。大多数现有方法都是基于本地外观核函数来建模邻近对的可能性。然而，这种本地操作无法捕捉长距离依赖关系和对象的 topological 结构。在这种工作中，我们将互相关系建模化为一种互相传播过程，并提出了本地和全局对对应潜在力场的两个方法，以生成准确的软精确标签。我们还开发了高效的算法，以减少计算成本。提议的方法可以方便地插入现有的分割网络中。在三种典型的标签有效分割任务中，即盒子监督实例分割、点/scribble监督 semantic segmentation 和 CLIP 引导的 semantic segmentation 中，我们的方法显示出了superior的性能。
</details></li>
</ul>
<hr>
<h2 id="Distribution-prediction-for-image-compression-An-experimental-re-compressor-for-JPEG-images"><a href="#Distribution-prediction-for-image-compression-An-experimental-re-compressor-for-JPEG-images" class="headerlink" title="Distribution prediction for image compression: An experimental re-compressor for JPEG images"></a>Distribution prediction for image compression: An experimental re-compressor for JPEG images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10517">http://arxiv.org/abs/2310.10517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxim Koroteev, Yaroslav Borisov, Pavel Frolov</li>
<li>for: 提高jpg图像压缩率</li>
<li>methods: 使用部分解码算法获取量化的DCT坐标，然后进行更有效的压缩</li>
<li>results: 实现了对jpg图像进行无损压缩<details>
<summary>Abstract</summary>
We propose a new scheme to re-compress JPEG images in a lossless way. Using a JPEG image as an input the algorithm partially decodes the signal to obtain quantized DCT coefficients and then re-compress them in a more effective way.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方案，用于在无损压缩 JPEG 图像。使用 JPEG 图像作为输入，算法部分解码信号，获取量化 DCT 系数，然后在更有效的方式压缩它们。Here's a breakdown of the translation:* "We propose" is translated as "我们提出" (wǒmen tīshì).* "a new scheme" is translated as "一种新的方案" (yī zhǒng xīn de fāng'ān).* "to re-compress" is translated as "重新压缩" (zhòng xīn pīn chā).* "JPEG images" is translated as "JPEG 图像" (JPEG túxiàng).* "in a lossless way" is translated as "在无损压缩的方式" (在无损压缩的方式).* "Using a JPEG image as an input" is translated as "使用 JPEG 图像作为输入" (shǐyòu JPEG túxiàng zhīxīng).* "partially decodes the signal" is translated as "部分解码信号" (bùzhì jiěmǎ xìngxiàng).* "to obtain quantized DCT coefficients" is translated as "获取量化 DCT 系数" (gòuqù liàngpǐ DCT xìshù).* "and then re-compress them" is translated as "然后重新压缩它们" (ránhòu zhòng xīn pīn chā tāmen).I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Unifying-Image-Processing-as-Visual-Prompting-Question-Answering"><a href="#Unifying-Image-Processing-as-Visual-Prompting-Question-Answering" class="headerlink" title="Unifying Image Processing as Visual Prompting Question Answering"></a>Unifying Image Processing as Visual Prompting Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10513">http://arxiv.org/abs/2310.10513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</li>
<li>for: 提高图像质量和提取视觉特征，替代具体任务模型。</li>
<li>methods: 使用大规模模型预训练和在图像处理任务中进行培 обу，通过视觉提问解决图像处理任务。</li>
<li>results: 提出一个通用的图像处理模型，可以处理多种图像处理任务，包括图像修复、图像增强、图像特征提取等。<details>
<summary>Abstract</summary>
Image processing is a fundamental task in computer vision, which aims at enhancing image quality and extracting essential features for subsequent vision applications. Traditionally, task-specific models are developed for individual tasks and designing such models requires distinct expertise. Building upon the success of large language models (LLMs) in natural language processing (NLP), there is a similar trend in computer vision, which focuses on developing large-scale models through pretraining and in-context learning. This paradigm shift reduces the reliance on task-specific models, yielding a powerful unified model to deal with various tasks. However, these advances have predominantly concentrated on high-level vision tasks, with less attention paid to low-level vision tasks. To address this issue, we propose a universal model for general image processing that covers image restoration, image enhancement, image feature extraction tasks, \textit{etc}. Our proposed framework, named PromptGIP, unifies these diverse image processing tasks within a universal framework. Inspired by NLP question answering (QA) techniques, we employ a visual prompting question answering paradigm. Specifically, we treat the input-output image pair as a structured question-answer sentence, thereby reprogramming the image processing task as a prompting QA problem. PromptGIP can undertake diverse \textbf{cross-domain} tasks using provided visual prompts, eliminating the need for task-specific finetuning. Our methodology offers a universal and adaptive solution to general image processing. While PromptGIP has demonstrated a certain degree of out-of-domain task generalization capability, further research is expected to fully explore its more powerful emergent generalization.
</details>
<details>
<summary>摘要</summary>
计算机视觉中的图像处理是一项基本任务，旨在提高图像质量和提取视觉应用中的关键特征。传统上，图像处理任务需要开发特定任务的模型，并且设计这些模型需要专门的技能。在计算机视觉领域，大型语言模型（LLM）的成功引起了一种类似的趋势，即通过预训练和上下文学习来建立大规模模型。这种思路转移使得图像处理任务可以使用一个强大的通用模型进行处理，而不需要特定任务的模型。然而，这些进步主要集中在高级视觉任务上，对低级视觉任务的关注较少。为了解决这个问题，我们提出了一个通用的图像处理模型，名为PromptGIP。我们的提案旨在将多种图像处理任务集成到一个通用框架中，并采用了视觉提问解答技术来实现。通过将输入输出图像对当做一个结构化的问题和答案句子，我们可以将图像处理任务转化为一个提问解答问题。PromptGIP可以通过提供的视觉提问来完成多个跨领域任务，无需特定任务的训练。我们的方法可以提供一个通用和适应的解决方案 для普通图像处理。虽然PromptGIP已经表现了一定的 OUT-OF-DOMAIN 任务泛化能力，但是进一步的研究可以充分发挥其更强大的 emergent 泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-and-improvement-of-Segment-Anything-Model-for-interactive-histopathology-image-segmentation"><a href="#Evaluation-and-improvement-of-Segment-Anything-Model-for-interactive-histopathology-image-segmentation" class="headerlink" title="Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation"></a>Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10493">http://arxiv.org/abs/2310.10493</a></li>
<li>repo_url: None</li>
<li>paper_authors: SeungKyu Kim, Hyun-Jic Oh, Seonghui Min, Won-Ki Jeong</li>
<li>for: This paper focuses on evaluating the performance of the Segment Anything Model (SAM) in interactive segmentation of histopathology data, and comparing it with other state-of-the-art interactive models.</li>
<li>methods: The paper uses the SAM model as a foundational model for image segmentation, and evaluates its performance in zero-shot and fine-tuned scenarios on histopathology data. The authors also propose a modification of SAM’s decoder to improve its local refinement ability and stability.</li>
<li>results: The experimental results show that SAM exhibits weaknesses in segmentation performance compared to other models, but demonstrates relative strengths in inference time and generalization capability. The proposed modification of SAM’s decoder is effective in improving its performance for interactive histology image segmentation.<details>
<summary>Abstract</summary>
With the emergence of the Segment Anything Model (SAM) as a foundational model for image segmentation, its application has been extensively studied across various domains, including the medical field. However, its potential in the context of histopathology data, specifically in region segmentation, has received relatively limited attention. In this paper, we evaluate SAM's performance in zero-shot and fine-tuned scenarios on histopathology data, with a focus on interactive segmentation. Additionally, we compare SAM with other state-of-the-art interactive models to assess its practical potential and evaluate its generalization capability with domain adaptability. In the experimental results, SAM exhibits a weakness in segmentation performance compared to other models while demonstrating relative strengths in terms of inference time and generalization capability. To improve SAM's limited local refinement ability and to enhance prompt stability while preserving its core strengths, we propose a modification of SAM's decoder. The experimental results suggest that the proposed modification is effective to make SAM useful for interactive histology image segmentation. The code is available at \url{https://github.com/hvcl/SAM_Interactive_Histopathology}
</details>
<details>
<summary>摘要</summary>
随着Segment Anything Model（SAM）作为图像分割基本模型的出现，其应用在不同领域得到了广泛的研究，但在 histopathology 数据中的区域分割方面却收到了相对有限的关注。在这篇论文中，我们评估了 SAM 在 zero-shot 和 fine-tuned 场景中对 histopathology 数据的性能，强调交互分割。此外，我们与其他当前领先的交互模型进行比较，以评估 SAM 在实际应用中的实用性和适应性。在实验结果中，SAM 在分割性能方面表现较弱，但在推理时间和适应性方面表现出了相对的优势。为了改进 SAM 的局部精度修正能力并保持其核心优势，我们提议一种修改 SAM 的解码器。实验结果表明，该修改是有效的，使得 SAM 在交互式 histology 图像分割中变得有用。代码可以在 \url{https://github.com/hvcl/SAM_Interactive_Histopathology} 上获取。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transferability-of-Learning-Models-for-Semantic-Segmentation-for-Remote-Sensing-Data"><a href="#On-the-Transferability-of-Learning-Models-for-Semantic-Segmentation-for-Remote-Sensing-Data" class="headerlink" title="On the Transferability of Learning Models for Semantic Segmentation for Remote Sensing Data"></a>On the Transferability of Learning Models for Semantic Segmentation for Remote Sensing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10490">http://arxiv.org/abs/2310.10490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gdaosu/transferability-remote-sensing">https://github.com/gdaosu/transferability-remote-sensing</a></li>
<li>paper_authors: Rongjun Qin, Guixiang Zhang, Yang Tang</li>
<li>for: 本研究旨在investigate remote sensing (RS) semantic segmentation&#x2F;classification任务上的传输性和适应性，以及如何通过领域适应（DA）方法提高深度学习（DL）模型的传输性。</li>
<li>methods: 本研究使用了四个高度不同的RS数据集，并将六个模型在不同的DA策略下进行训练，以量化模型之间的传输性和适应性。此外，我们还提出了一种简单的方法来评估模型在目标领域中的传输性，不需要标签数据。</li>
<li>results: 我们的实验结果显示，DL模型在不同领域之间的传输性较差，而DA策略可以有效地提高DL模型的传输性。此外，我们还发现了一些不常报道的 Raw和适应传输性的观察结果。我们的提出的标签 свобо�评估方法也被证明可以更好地评估模型的传输性。<details>
<summary>Abstract</summary>
Recent deep learning-based methods outperform traditional learning methods on remote sensing (RS) semantic segmentation/classification tasks. However, they require large training datasets and are generally known for lack of transferability due to the highly disparate RS image content across different geographical regions. Yet, there is no comprehensive analysis of their transferability, i.e., to which extent a model trained on a source domain can be readily applicable to a target domain. Therefore, in this paper, we aim to investigate the raw transferability of traditional and deep learning (DL) models, as well as the effectiveness of domain adaptation (DA) approaches in enhancing the transferability of the DL models (adapted transferability). By utilizing four highly diverse RS datasets, we train six models with and without three DA approaches to analyze their transferability between these datasets quantitatively. Furthermore, we developed a straightforward method to quantify the transferability of a model using the spectral indices as a medium and have demonstrated its effectiveness in evaluating the model transferability at the target domain when the labels are unavailable. Our experiments yield several generally important yet not well-reported observations regarding the raw and adapted transferability. Moreover, our proposed label-free transferability assessment method is validated to be better than posterior model confidence. The findings can guide the future development of generalized RS learning models. The trained models are released under this link: https://github.com/GDAOSU/Transferability-Remote-Sensing
</details>
<details>
<summary>摘要</summary>
现代深度学习方法在远程感知（RS）semantic segmentation/分类任务上表现出色，但它们需要大量的训练数据并且通常因为不同地区RS图像内容差异极大而无法转移。然而，没有系统性的分析转移性，即源领域训练的模型可以如何 extent 应用于目标领域。因此，在这篇论文中，我们想要调查传统和深度学习（DL）模型的原生转移性，以及使用领域适应（DA）策略可以提高DL模型的转移性（适应转移性）。通过使用四个高度不同RS数据集，我们训练了六个模型，并使用三种DA策略进行分析其转移性。此外，我们还提出了一种简单的方法来评估模型的转移性，使用spectral indices作为媒介，并在目标领域无标签情况下证明其效果。我们的实验结果表明了一些不常报道的观察结果，包括原生转移性和适应转移性的分析。此外，我们的提出的无标签转移性评估方法被证明为比 posterior model confidence 更有用。这些发现可以指导未来的RS学习模型的发展。我们训练的模型可以在以下链接获取：https://github.com/GDAOSU/Transferability-Remote-Sensing
</details></li>
</ul>
<hr>
<h2 id="Combating-Label-Noise-With-A-General-Surrogate-Model-For-Sample-Selection"><a href="#Combating-Label-Noise-With-A-General-Surrogate-Model-For-Sample-Selection" class="headerlink" title="Combating Label Noise With A General Surrogate Model For Sample Selection"></a>Combating Label Noise With A General Surrogate Model For Sample Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10463">http://arxiv.org/abs/2310.10463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Liang, Linchao Zhu, Humphrey Shi, Yi Yang</li>
<li>for: 减少标签噪音，提高深度学习系统的性能。</li>
<li>methods: 利用CLIPvision-language surrogate模型自动过滤噪音样本，并采用margin适应损失来规范选择偏好。</li>
<li>results: 在实际和Synthetic噪音数据集上实现了显著改进，无需CLIP在推断阶段参与。<details>
<summary>Abstract</summary>
Modern deep learning systems are data-hungry. Learning with web data is one of the feasible solutions, but will introduce label noise inevitably, which can hinder the performance of deep neural networks. Sample selection is an effective way to deal with label noise. The key is to separate clean samples based on some criterion. Previous methods pay more attention to the small loss criterion where small-loss samples are regarded as clean ones. Nevertheless, such a strategy relies on the learning dynamics of each data instance. Some noisy samples are still memorized due to frequently occurring corrupted learning patterns. To tackle this problem, a training-free surrogate model is preferred, freeing from the effect of memorization. In this work, we propose to leverage the vision-language surrogate model CLIP to filter noisy samples automatically. CLIP brings external knowledge to facilitate the selection of clean samples with its ability of text-image alignment. Furthermore, a margin adaptive loss is designed to regularize the selection bias introduced by CLIP, providing robustness to label noise. We validate the effectiveness of our proposed method on both real-world and synthetic noisy datasets. Our method achieves significant improvement without CLIP involved during the inference stage.
</details>
<details>
<summary>摘要</summary>
现代深度学习系统具有巨量数据的需求。使用网络数据进行学习是一个可行的解决方案，但会随机扰动标签，从而影响深度神经网络的性能。样本选择是一个有效的方法来处理标签噪音。以往的方法更多地关注于小损失标准，即将小损失的样本视为干净的样本。然而，这种策略基于每个数据实例的学习动态，一些噪音样本仍然会被记忆由频繁出现的异常学习模式。为解决这个问题，我们提议利用CLIP视觉语言代理模型自动过滤噪音样本。CLIP带来了外部知识，以便通过图文对齐来促进干净样本的选择。此外，我们还设计了一种margin适应损失，以规避CLIP的选择偏见，提供了对标签噪音的Robustness。我们在真实的噪音数据集和静态数据集上验证了我们的提议的有效性，而不需要在推理阶段使用CLIP。
</details></li>
</ul>
<hr>
<h2 id="Model-Selection-of-Anomaly-Detectors-in-the-Absence-of-Labeled-Validation-Data"><a href="#Model-Selection-of-Anomaly-Detectors-in-the-Absence-of-Labeled-Validation-Data" class="headerlink" title="Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data"></a>Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10461">http://arxiv.org/abs/2310.10461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Fung, Chen Qiu, Aodong Li, Maja Rudolph</li>
<li>for: 这个论文旨在提出一种通用的框架，用于评估基于图像的异常检测器。</li>
<li>methods: 该方法假设有一小支持集（support set）的正常图像，通过预训练的扩散模型进行处理，生成了人工异常样本。当混合到正常样本集中时，这些人工异常样本创建了一个适用于异常检测器评估的验证框架。</li>
<li>results: 在广泛的实验研究中，我们发现，使用我们的生成的验证数据可以选择同样的模型和超参数，与使用真实的验证集一样。此外，我们发现，使用我们的方法选择的提示（prompts）在CLIP基于异常检测中表现出色，超过其他所有提示策略，并在挑战性的MVTec-AD数据集上达到最佳检测精度。<details>
<summary>Abstract</summary>
Anomaly detection requires detecting abnormal samples in large unlabeled datasets. While progress in deep learning and the advent of foundation models has produced powerful unsupervised anomaly detection methods, their deployment in practice is often hindered by the lack of labeled data -- without it, the detection accuracy of an anomaly detector cannot be evaluated reliably. In this work, we propose a general-purpose framework for evaluating image-based anomaly detectors with synthetically generated validation data. Our method assumes access to a small support set of normal images which are processed with a pre-trained diffusion model (our proposed method requires no training or fine-tuning) to produce synthetic anomalies. When mixed with normal samples from the support set, the synthetic anomalies create detection tasks that compose a validation framework for anomaly detection evaluation and model selection. In an extensive empirical study, ranging from natural images to industrial applications, we find that our synthetic validation framework selects the same models and hyper-parameters as selection with a ground-truth validation set. In addition, we find that prompts selected by our method for CLIP-based anomaly detection outperforms all other prompt selection strategies, and leads to the overall best detection accuracy, even on the challenging MVTec-AD dataset.
</details>
<details>
<summary>摘要</summary>
异常检测需要检测大量无标签数据中的异常标本。虽然深度学习和基础模型的进步导致了无监督异常检测方法的生成，但它们在实践中的应用受到了无标签数据的缺乏影响，因为无法对异常检测器的准确性进行可靠评估。在这个工作中，我们提出一个通用的框架，用于评估基于图像的异常检测器，使用生成的运动模型来生成异常标本。我们的方法不需要训练或微调。当混合到支持集的正常图像中，生成的异常标本创建了一个异常检测任务，它们组成了一个适用于异常检测评估和模型选择的验证框架。在广泛的实验研究中，我们发现我们的生成验证框架可以选择同样的模型和参数，并且我们的提示选择策略在CLIP基础上的异常检测中表现出色，并且导致了整体最佳的检测精度，甚至在挑战性的MVTec-AD数据集上。
</details></li>
</ul>
<hr>
<h2 id="Object-Detection-in-Aerial-Images-in-Scarce-Data-Regimes"><a href="#Object-Detection-in-Aerial-Images-in-Scarce-Data-Regimes" class="headerlink" title="Object Detection in Aerial Images in Scarce Data Regimes"></a>Object Detection in Aerial Images in Scarce Data Regimes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10433">http://arxiv.org/abs/2310.10433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Le Jeune</li>
<li>for: 这个论文的目的是提高几个shot目标检测的性能，并评估其在不同类型图像上的可迁移性。</li>
<li>methods: 该论文使用了一种特殊的注意力机制来改进小对象的检测性能，以及一种批处理的盒子相似度标准来改进训练和评估。此外，论文还提出了两种不同的metric学习和精度调整方法来提高检测性能。</li>
<li>results: 论文得到了显著的提高在小对象检测上，并在 Cross-Domain FSOD 领域取得了卓越的结果。此外，论文还成功地解决了在 COSE 系统中部署检测模型的工程问题，并在具有超过 100 万像素的图像中进行实时检测。<details>
<summary>Abstract</summary>
Most contributions on Few-Shot Object Detection (FSOD) evaluate their methods on natural images only, yet the transferability of the announced performance is not guaranteed for applications on other kinds of images. We demonstrate this with an in-depth analysis of existing FSOD methods on aerial images and observed a large performance gap compared to natural images. Small objects, more numerous in aerial images, are the cause for the apparent performance gap between natural and aerial images. As a consequence, we improve FSOD performance on small objects with a carefully designed attention mechanism. In addition, we also propose a scale-adaptive box similarity criterion, that improves the training and evaluation of FSOD methods, particularly for small objects. We also contribute to generic FSOD with two distinct approaches based on metric learning and fine-tuning. Impressive results are achieved with the fine-tuning method, which encourages tackling more complex scenarios such as Cross-Domain FSOD. We conduct preliminary experiments in this direction and obtain promising results. Finally, we address the deployment of the detection models inside COSE's systems. Detection must be done in real-time in extremely large images (more than 100 megapixels), with limited computation power. Leveraging existing optimization tools such as TensorRT, we successfully tackle this engineering challenge.
</details>
<details>
<summary>摘要</summary>
多数对几shot对象检测（FSOD）的贡献仅测试在自然图像上，但是这些方法的可传性并不保证在其他类型图像上的应用。我们通过对现有FSOD方法的深入分析在飞行图像上表明，小对象的众多性导致自然图像和飞行图像之间的性能差距。为了解决这个问题，我们采用了特别设计的注意机制来提高小对象的检测性能。此外，我们还提出了可以适应不同大小的盒子相似性标准，以改进FSOD方法的训练和评估。此外，我们还提出了基于度量学习和精度调整的两种不同方法来提高FSOD性能。经过精心调整，我们得到了很好的结果。最后，我们关注在COSE系统中部署检测模型。在具有EXTREMELY大图像（超过100 megapixel）和有限计算资源的情况下，我们成功使用现有的优化工具 such as TensorRT来解决这个工程问题。
</details></li>
</ul>
<hr>
<h2 id="DANAA-Towards-transferable-attacks-with-double-adversarial-neuron-attribution"><a href="#DANAA-Towards-transferable-attacks-with-double-adversarial-neuron-attribution" class="headerlink" title="DANAA: Towards transferable attacks with double adversarial neuron attribution"></a>DANAA: Towards transferable attacks with double adversarial neuron attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10427">http://arxiv.org/abs/2310.10427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Davidjinzb/DANAA">https://github.com/Davidjinzb/DANAA</a></li>
<li>paper_authors: Zhibo Jin, Zhiyu Zhu, Xinyi Wang, Jiayu Zhang, Jun Shen, Huaming Chen</li>
<li>for: 本文旨在提出一种基于中间层的双反抗智能方法，以提高深度神经网络模型中特征重要性的估计结果，并提高模型的抗击性能。</li>
<li>methods: 本文提出了一种基于对抗非线性路径的双反抗神经网络模型，通过计算中间层输出的各个神经元的贡献，以估计特征重要性。</li>
<li>results: 对多个基准数据集进行了广泛的实验，并证明了我们的方法可以达到当今最佳性能。我们的代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Davidjinzb/DANAA">https://github.com/Davidjinzb/DANAA</a><details>
<summary>Abstract</summary>
While deep neural networks have excellent results in many fields, they are susceptible to interference from attacking samples resulting in erroneous judgments. Feature-level attacks are one of the effective attack types, which targets the learnt features in the hidden layers to improve its transferability across different models. Yet it is observed that the transferability has been largely impacted by the neuron importance estimation results. In this paper, a double adversarial neuron attribution attack method, termed `DANAA', is proposed to obtain more accurate feature importance estimation. In our method, the model outputs are attributed to the middle layer based on an adversarial non-linear path. The goal is to measure the weight of individual neurons and retain the features that are more important towards transferability. We have conducted extensive experiments on the benchmark datasets to demonstrate the state-of-the-art performance of our method. Our code is available at: https://github.com/Davidjinzb/DANAA
</details>
<details>
<summary>摘要</summary>
深度神经网络在多个领域取得了出色的成绩，但它们受到攻击样本的干扰，导致评判结果错误。攻击样本是一种有效的攻击类型，targets the learnt features in the hidden layers to improve its transferability across different models。然而，我们发现，通过neuron importance estimation结果，攻击样本的传播性受到了很大的影响。在这篇论文中，我们提出了一种double adversarial neuron attribution attack方法，称为`DANAA'。我们的方法基于一个 adversarial non-linear path，将模型输出归结到中层。我们的目标是测量个体神经元的重要性，保留对传播性更重要的特征。我们在标准 benchmark datasets 上进行了广泛的实验，以示出我们的方法的state-of-the-art性。我们的代码可以在：https://github.com/Davidjinzb/DANAA 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Benchmarking-Paradigm-and-a-Scale-and-Motion-Aware-Model-for-Egocentric-Pedestrian-Trajectory-Prediction"><a href="#A-Novel-Benchmarking-Paradigm-and-a-Scale-and-Motion-Aware-Model-for-Egocentric-Pedestrian-Trajectory-Prediction" class="headerlink" title="A Novel Benchmarking Paradigm and a Scale- and Motion-Aware Model for Egocentric Pedestrian Trajectory Prediction"></a>A Novel Benchmarking Paradigm and a Scale- and Motion-Aware Model for Egocentric Pedestrian Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10424">http://arxiv.org/abs/2310.10424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Rasouli</li>
<li>for: 本研究旨在提高智能驾驶系统中对行人行为预测的精度。</li>
<li>methods: 本文提出了一种新的 egocentric 行人轨迹预测算法评估方法，基于不同的情况下的 contextual information，提取了 meaningful 和系统的 driving scenarios，并提出了一种更有效的 metric 来评估预测模型。</li>
<li>results: 对 existed 模型进行了广泛的 empirical 研究，暴露了不同方法在不同情况下的缺陷和优势，并显示了我们的方法在挑战性情况下可以达到40%的提高。<details>
<summary>Abstract</summary>
Predicting pedestrian behavior is one of the main challenges for intelligent driving systems. In this paper, we present a new paradigm for evaluating egocentric pedestrian trajectory prediction algorithms. Based on various contextual information, we extract driving scenarios for a meaningful and systematic approach to identifying challenges for prediction models. In this regard, we also propose a new metric for more effective ranking within the scenario-based evaluation. We conduct extensive empirical studies of existing models on these scenarios to expose shortcomings and strengths of different approaches. The scenario-based analysis highlights the importance of using multimodal sources of information and challenges caused by inadequate modeling of ego-motion and scale of pedestrians. To this end, we propose a novel egocentric trajectory prediction model that benefits from multimodal sources of data fused in an effective and efficient step-wise hierarchical fashion and two auxiliary tasks designed to learn more robust representation of scene dynamics. We show that our approach achieves significant improvement by up to 40% in challenging scenarios compared to the past arts via empirical evaluation on common benchmark datasets.
</details>
<details>
<summary>摘要</summary>
预测行人行为是智能驾驶系统中的一个主要挑战。在这篇论文中，我们提出了一种新的评估 egocentric 行人轨迹预测算法的新模式。基于多种情境信息，我们提取了 meaningful 和系统的驾驶场景，以便更好地识别预测模型的挑战。在这个 regard，我们也提出了一个更有效的排名 metric。我们对现有模型进行了广泛的实证研究，以暴露不同方法的缺陷和优势。场景基本分析表明，使用多modal 信息和 egocentric 行人的不充分模型和比例会导致预测困难。为此，我们提出了一种新的 egocentric 轨迹预测模型，该模型利用多modal 数据的综合和有效的步骤式层次结构，以及两个辅助任务，以学习更加稳定的Scene 动力学。我们的方法在复杂的场景下比过去的艺术品 achieved 40% 的提升，via 实验评估常见的 benchmark 数据集。
</details></li>
</ul>
<hr>
<h2 id="YOLOv7-for-Mosquito-Breeding-Grounds-Detection-and-Tracking"><a href="#YOLOv7-for-Mosquito-Breeding-Grounds-Detection-and-Tracking" class="headerlink" title="YOLOv7 for Mosquito Breeding Grounds Detection and Tracking"></a>YOLOv7 for Mosquito Breeding Grounds Detection and Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10423">http://arxiv.org/abs/2310.10423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camila Laranjeira, Daniel Andrade, Jefersson A. dos Santos</li>
<li>for: 防止气候变化的威胁，忽略性 tropical diseases 如遗传性疟疾、血吸虫病和生物攻击等可能成为全球问题。</li>
<li>methods: 本文使用 YOLOv7 状态之 искусственный神经网络方法，自动检测和地图蚊子繁殖地点，以便地方机构可以有效 intervene。</li>
<li>results: 我们在 ICIP 2023 大赛中发布的数据集上进行了实验，并示出 YOLOv7 可以直接应用于检测更大的ocus类别，如池塘、车胎和水箱，并且可以通过简单的滤波来实现时间一致性的跟踪过程。<details>
<summary>Abstract</summary>
With the looming threat of climate change, neglected tropical diseases such as dengue, zika, and chikungunya have the potential to become an even greater global concern. Remote sensing technologies can aid in controlling the spread of Aedes Aegypti, the transmission vector of such diseases, by automating the detection and mapping of mosquito breeding sites, such that local entities can properly intervene. In this work, we leverage YOLOv7, a state-of-the-art and computationally efficient detection approach, to localize and track mosquito foci in videos captured by unmanned aerial vehicles. We experiment on a dataset released to the public as part of the ICIP 2023 grand challenge entitled Automatic Detection of Mosquito Breeding Grounds. We show that YOLOv7 can be directly applied to detect larger foci categories such as pools, tires, and water tanks and that a cheap and straightforward aggregation of frame-by-frame detection can incorporate time consistency into the tracking process.
</details>
<details>
<summary>摘要</summary>
With the looming threat of climate change, neglected tropical diseases such as dengue, zika, and chikungunya have the potential to become an even greater global concern. Remote sensing technologies can aid in controlling the spread of Aedes Aegypti, the transmission vector of such diseases, by automating the detection and mapping of mosquito breeding sites, such that local entities can properly intervene. In this work, we leverage YOLOv7, a state-of-the-art and computationally efficient detection approach, to localize and track mosquito foci in videos captured by unmanned aerial vehicles. We experiment on a dataset released to the public as part of the ICIP 2023 grand challenge entitled Automatic Detection of Mosquito Breeding Grounds. We show that YOLOv7 can be directly applied to detect larger foci categories such as pools, tires, and water tanks, and that a cheap and straightforward aggregation of frame-by-frame detection can incorporate time consistency into the tracking process.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="LMT-Longitudinal-Mixing-Training-a-Framework-to-Predict-Disease-Progression-from-a-Single-Image"><a href="#LMT-Longitudinal-Mixing-Training-a-Framework-to-Predict-Disease-Progression-from-a-Single-Image" class="headerlink" title="LMT: Longitudinal Mixing Training, a Framework to Predict Disease Progression from a Single Image"></a>LMT: Longitudinal Mixing Training, a Framework to Predict Disease Progression from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10420">http://arxiv.org/abs/2310.10420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Hugo Le boite, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard</li>
<li>for: 这个论文旨在检测和预测糖尿病Retinopathy (DR) 疾病进程的扩展。</li>
<li>methods: 该论文使用了混合训练和预text任务，并使用了新的神经网络模型 named Neural Ordinary Differential Equation (NODE)。</li>
<li>results: 该论文在使用 Longitudinal Mixing Training (LMT) 框架时，可以更好地检测和预测 DR 疾病进程。在 OPHDIAT 长itudinal retinal Color Fundus Photographs (CFP)  dataset 上，该方法可以预测下一次访问 whether an eye would develop a severe DR，其 AUC 为 0.798，比基线结果高得多。<details>
<summary>Abstract</summary>
Longitudinal imaging is able to capture both static anatomical structures and dynamic changes in disease progression toward earlier and better patient-specific pathology management. However, conventional approaches rarely take advantage of longitudinal information for detection and prediction purposes, especially for Diabetic Retinopathy (DR). In the past years, Mix-up training and pretext tasks with longitudinal context have effectively enhanced DR classification results and captured disease progression. In the meantime, a novel type of neural network named Neural Ordinary Differential Equation (NODE) has been proposed for solving ordinary differential equations, with a neural network treated as a black box. By definition, NODE is well suited for solving time-related problems. In this paper, we propose to combine these three aspects to detect and predict DR progression. Our framework, Longitudinal Mixing Training (LMT), can be considered both as a regularizer and as a pretext task that encodes the disease progression in the latent space. Additionally, we evaluate the trained model weights on a downstream task with a longitudinal context using standard and longitudinal pretext tasks. We introduce a new way to train time-aware models using $t_{mix}$, a weighted average time between two consecutive examinations. We compare our approach to standard mixing training on DR classification using OPHDIAT a longitudinal retinal Color Fundus Photographs (CFP) dataset. We were able to predict whether an eye would develop a severe DR in the following visit using a single image, with an AUC of 0.798 compared to baseline results of 0.641. Our results indicate that our longitudinal pretext task can learn the progression of DR disease and that introducing $t_{mix}$ augmentation is beneficial for time-aware models.
</details>
<details>
<summary>摘要</summary>
长itudinal imaging可以捕捉到稳定的解剖结构以及疾病进程的动态变化，从而提供更早和更好的病理管理。然而，传统方法rarely利用长itudinal信息进行检测和预测，特别是对于肥胖糖尿病（DR）。在过去几年，杂合训练和预文任务with longitudinal context有效提高了DR分类结果，并捕捉了疾病进程。同时，一种新的神经网络模型名为神经常微方程（NODE）已经被提出，可以解决常微方程问题。根据定义，NODE适合解决时间相关的问题。在这篇文章中，我们提议将这三个方面结合，以检测和预测DR疾病进程。我们的框架，长itudinal Mixing Training（LMT），可以被视为一种正则化和预文任务，用于编码疾病进程在幽默空间中。此外，我们评估训练模型的加权因子在下游任务中使用标准和长itudinal预文任务时的性能。我们还介绍了一种新的时间感知训练方法，使用 $t_{mix} $ Weighted average time between two consecutive examinations。我们比较了我们的方法与标准杂合训练在DR分类中的表现，使用 OPHDIAT  longitudinal retinal Color Fundus Photographs（CFP）数据集。我们能够使用单个图像预测眼睛是否在下一次访问中会发展严重的DR，AUC为 0.798，比基eline结果提高了0.641。我们的结果表明，我们的长itudinal预文任务可以学习DR疾病的进程，并且将 $t_{mix} $ 杂合增强是对时间感知模型有利。
</details></li>
</ul>
<hr>
<h2 id="Prior-Free-Continual-Learning-with-Unlabeled-Data-in-the-Wild"><a href="#Prior-Free-Continual-Learning-with-Unlabeled-Data-in-the-Wild" class="headerlink" title="Prior-Free Continual Learning with Unlabeled Data in the Wild"></a>Prior-Free Continual Learning with Unlabeled Data in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10417">http://arxiv.org/abs/2310.10417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visiontao/pfcl">https://github.com/visiontao/pfcl</a></li>
<li>paper_authors: Tao Zhuo, Zhiyong Cheng, Hehe Fan, Mohan Kankanhalli</li>
<li>for: 本研究旨在 Addressing the problem of forgetting in continual learning (CL) when task priors are unknown in real-world applications.</li>
<li>methods: 我们提出了一个 Prior-Free Continual Learning (PFCL) 方法，不需要任务识别或先前的数据。我们透过两个方法来实现这一目标：首先，我们消除了任务识别的需求，以使用固定单头架构中的任务特定出力头选择。其次，我们使用了一种常见性预测策略，以避免在新任务上重访先前的数据。然而，这种方法单独可能在分类增量学习中表现不佳，特别是当任务序列很长时。我们运用了一个辅助数据集来增强模型的一致性，以提高分类精度。</li>
<li>results: 我们的PFCL方法在多个图像分类benchmarkdataset上进行了广泛的实验，结果显示PFCL方法能够对所有三种学习场景进行优化，并且与最近的回温基本方法相比，PFCL方法在竞争率上具有相似的精度。<details>
<summary>Abstract</summary>
Continual Learning (CL) aims to incrementally update a trained model on new tasks without forgetting the acquired knowledge of old ones. Existing CL methods usually reduce forgetting with task priors, \ie using task identity or a subset of previously seen samples for model training. However, these methods would be infeasible when such priors are unknown in real-world applications. To address this fundamental but seldom-studied problem, we propose a Prior-Free Continual Learning (PFCL) method, which learns new tasks without knowing the task identity or any previous data. First, based on a fixed single-head architecture, we eliminate the need for task identity to select the task-specific output head. Second, we employ a regularization-based strategy for consistent predictions between the new and old models, avoiding revisiting previous samples. However, using this strategy alone often performs poorly in class-incremental scenarios, particularly for a long sequence of tasks. By analyzing the effectiveness and limitations of conventional regularization-based methods, we propose enhancing model consistency with an auxiliary unlabeled dataset additionally. Moreover, since some auxiliary data may degrade the performance, we further develop a reliable sample selection strategy to obtain consistent performance improvement. Extensive experiments on multiple image classification benchmark datasets show that our PFCL method significantly mitigates forgetting in all three learning scenarios. Furthermore, when compared to the most recent rehearsal-based methods that replay a limited number of previous samples, PFCL achieves competitive accuracy. Our code is available at: https://github.com/visiontao/pfcl
</details>
<details>
<summary>摘要</summary>
Our approach has two key components. First, we eliminate the need for task identity to select the task-specific output head using a fixed single-head architecture. Second, we employ a regularization-based strategy for consistent predictions between the new and old models, avoiding revisiting previous samples. However, this strategy alone can perform poorly in class-incremental scenarios, so we also use an auxiliary unlabeled dataset to enhance model consistency.To ensure reliable performance improvement, we develop a sample selection strategy to choose the most informative samples from the auxiliary dataset. Our approach significantly mitigates forgetting in all three learning scenarios, and achieves competitive accuracy compared to rehearsal-based methods that replay a limited number of previous samples.Our code is available at: https://github.com/visiontao/pfcl.
</details></li>
</ul>
<hr>
<h2 id="Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings"><a href="#Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings" class="headerlink" title="Style transfer between Microscopy and Magnetic Resonance Imaging via Generative Adversarial Network in small sample size settings"></a>Style transfer between Microscopy and Magnetic Resonance Imaging via Generative Adversarial Network in small sample size settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10414">http://arxiv.org/abs/2310.10414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Pytlarz, Adrian Onicas, Alessandro Crimi</li>
<li>for: 这个研究的目的是使用 Conditional GAN 架构将 MRI 图像翻译成历史学图像，以便避免侵入性的生物псии检测。</li>
<li>methods: 这个研究使用的方法是使用 Conditional GAN 架构，该架构可以将 MRI 图像翻译成历史学图像。</li>
<li>results: 这个研究表明，使用 Conditional GAN 架构可以可靠地将 MRI 图像翻译成历史学图像，并且可以使用高分辨率的历史学图像和相对较低分辨率的 MRI 图像进行对应。<details>
<summary>Abstract</summary>
Cross-modal augmentation of Magnetic Resonance Imaging (MRI) and microscopic imaging based on the same tissue samples is promising because it can allow histopathological analysis in the absence of an underlying invasive biopsy procedure. Here, we tested a method for generating microscopic histological images from MRI scans of the corpus callosum using conditional generative adversarial network (cGAN) architecture. To our knowledge, this is the first multimodal translation of the brain MRI to histological volumetric representation of the same sample. The technique was assessed by training paired image translation models taking sets of images from MRI scans and microscopy. The use of cGAN for this purpose is challenging because microscopy images are large in size and typically have low sample availability. The current work demonstrates that the framework reliably synthesizes histology images from MRI scans of corpus callosum, emphasizing the network's ability to train on high resolution histologies paired with relatively lower-resolution MRI scans. With the ultimate goal of avoiding biopsies, the proposed tool can be used for educational purposes.
</details>
<details>
<summary>摘要</summary>
通过同一个组织样本的跨模态扩充，核磁共振成像（MRI）和微scopic成像之间的同化是有前途的，因为它可以允许在不基于侵入性生物псиchosurgeries的情况下进行 histopathological 分析。我们在这里测试了一种方法，用于从 MRI 扫描中生成 microscopic  histological 图像。根据我们所知，这是第一种跨modal 翻译 brain MRI 到 histological 三维表示的方法。这种技术被评估了，通过对对应的图像翻译模型进行训练。使用 cGAN 进行这种目的是挑战性的，因为微scopic 图像通常很大，并且 Sample 的可用性很低。当前的研究表明，该框架可靠地将 MRI 扫描中的 corpus callosum 转换为 histology 图像，强调网络的能力在高分辨率 histology 和相对较低分辨率 MRI 扫描之间进行训练。以避免生物псиchosurgeries 为目的，提出的工具可以用于教育用途。
</details></li>
</ul>
<hr>
<h2 id="Image-super-resolution-via-dynamic-network"><a href="#Image-super-resolution-via-dynamic-network" class="headerlink" title="Image super-resolution via dynamic network"></a>Image super-resolution via dynamic network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10413">http://arxiv.org/abs/2310.10413</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellloxiaotian/dsrnet">https://github.com/hellloxiaotian/dsrnet</a></li>
<li>paper_authors: Chunwei Tian, Xuanyu Zhang, Qi Zhang, Mingming Yang, Zhaojie Ju</li>
<li>for: 这篇论文旨在提出一种动态网络 для图像超解像（DSRNet），以提高图像超解像的准确率和复杂场景下的应用性。</li>
<li>methods: 该网络使用了差异增强块、宽增强块、特征细化块和结构块等多种块来提高图像超解像的精度和可靠性。</li>
<li>results: 实验结果表明，与传统方法相比，DSRNet能够更好地处理复杂场景下的图像超解像问题，同时具有较低的计算量和可扩展性，适用于移动设备上的实时应用。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) depend on deep network architectures to extract accurate information for image super-resolution. However, obtained information of these CNNs cannot completely express predicted high-quality images for complex scenes. In this paper, we present a dynamic network for image super-resolution (DSRNet), which contains a residual enhancement block, wide enhancement block, feature refinement block and construction block. The residual enhancement block is composed of a residual enhanced architecture to facilitate hierarchical features for image super-resolution. To enhance robustness of obtained super-resolution model for complex scenes, a wide enhancement block achieves a dynamic architecture to learn more robust information to enhance applicability of an obtained super-resolution model for varying scenes. To prevent interference of components in a wide enhancement block, a refinement block utilizes a stacked architecture to accurately learn obtained features. Also, a residual learning operation is embedded in the refinement block to prevent long-term dependency problem. Finally, a construction block is responsible for reconstructing high-quality images. Designed heterogeneous architecture can not only facilitate richer structural information, but also be lightweight, which is suitable for mobile digital devices. Experimental results shows that our method is more competitive in terms of performance and recovering time of image super-resolution and complexity. The code of DSRNet can be obtained at https://github.com/hellloxiaotian/DSRNet.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNNs) 依靠深度网络架构来提取图像超分解中的准确信息。然而，这些 CNNs 所获取的信息无法完全表达预测的高质量图像 для复杂场景。在这篇论文中，我们提出了动态网络 для图像超分解 (DSRNet)，它包含了差异增强块、宽增强块、特征细化块和结构块。差异增强块由一个差异增强架构组成，以便在图像超分解中提高层次特征。为了提高获取的超分解模型在复杂场景中的应用 robustness，宽增强块实现了一个动态架构，以学习更加Robust的信息以提高获取的超分解模型的可用性。为了避免各个组件之间的干扰，特征细化块使用了堆叠结构来准确地学习获取的特征。此外，在细化块中还包含了循环学习操作，以避免长期依赖问题。最后，结构块负责重建高质量图像。设计的不同化架构不仅可以提供更加丰富的结构信息，还可以减轻计算负担，适用于移动式数字设备。实验结果表明，我们的方法在性能和图像重建时间方面更加竞争力，同时也更加复杂。DSRNet 的代码可以在 https://github.com/hellloxiaotian/DSRNet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Loci-Segmented-Improving-Scene-Segmentation-Learning"><a href="#Loci-Segmented-Improving-Scene-Segmentation-Learning" class="headerlink" title="Loci-Segmented: Improving Scene Segmentation Learning"></a>Loci-Segmented: Improving Scene Segmentation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10410">http://arxiv.org/abs/2310.10410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CognitiveModeling/Loci-Segmented">https://github.com/CognitiveModeling/Loci-Segmented</a></li>
<li>paper_authors: Manuel Traub, Frederic Becker, Adrian Sauter, Sebastian Otte, Martin V. Butz</li>
<li>for: 本研究旨在提高场景表示的分割能力，并提出了一种基于槽的处理方法。</li>
<li>methods: 本方法使用了一种名为Loci-Segmented（Loci-s）的场景分割神经网络，它基于Loci（Traub等，ICLR 2023）框架，并具有以下三大提升：（1）添加了预训练的动态背景模块；（2）具有对象专注的几何卷积编码模块；（3）采用了级联解码模块，successively生成对象Mask、Masked Depth Maps和Masked, Depth-map-informed RGB重建。</li>
<li>results: 对比于之前的最佳成果，Loci-s在MOVi datasets和另一个Established dataset集合中实现了32%的交集覆盖率（IoU）提升。此外，Loci-s还生成了良好的可解释性 latent representation，这些表示可能作为解释基础模型的可解释基础 для解决下游任务，如语言背景和Context-和Goal-conditioned Event Processing。<details>
<summary>Abstract</summary>
Slot-oriented processing approaches for compositional scene representation have recently undergone a tremendous development. We present Loci-Segmented (Loci-s), an advanced scene segmentation neural network that extends the slot-based location and identity tracking architecture Loci (Traub et al., ICLR 2023). The main advancements are (i) the addition of a pre-trained dynamic background module; (ii) a hyper-convolution encoder module, which enables object-focused bottom-up processing; and (iii) a cascaded decoder module, which successively generates object masks, masked depth maps, and masked, depth-map-informed RGB reconstructions. The background module features the learning of both a foreground identifying module and a background re-generator. We further improve performance via (a) the integration of depth information as well as improved slot assignments via (b) slot-location-entity regularization and (b) a prior segmentation network. Even without these latter improvements, the results reveal superior segmentation performance in the MOVi datasets and in another established dataset collection. With all improvements, Loci-s achieves a 32% better intersection over union (IoU) score in MOVi-E than the previous best. We furthermore show that Loci-s generates well-interpretable latent representations. We believe that these representations may serve as a foundation-model-like interpretable basis for solving downstream tasks, such as grounding language and context- and goal-conditioned event processing.
</details>
<details>
<summary>摘要</summary>
各种槽处理方法在 compositional scene representation 领域受到了非常大的发展。我们现在提出了 Loci-Segmented（Loci-s），这是一种高级的Scene segmentation neural network，它扩展了 Loci（Traub et al., ICLR 2023）槽基 Architecture。主要改进包括：(i) 添加了预训练的动态背景模块；(ii) 使用了对象专注的凹陷 Encoder 模块，以便从底层处进行对象特征提取；(iii) 使用了级联的解码模块，以顺序生成对象面积、掩码depth maps和掩码、 depth-map-informed RGB 重建。背景模块包括学习both foreground 特征和背景重建。我们进一步提高性能通过：(a)  интеграción of depth information以及改进的槽分配via(b) slot-location-entity regularization和(b) 一个前 segmentation network。无论这些改进，Loci-s 在 MOVi 数据集和另一个已知数据集中显示出色的 segmentation 性能。通过所有改进，Loci-s 在 MOVi-E 中实现了与之前最佳的32%的交集 над union（IoU）分数提高。我们进一步表明Loci-s 生成的latent representations是可解释的。我们认为这些表示可以作为基础模型的可解释基础，用于解决下游任务，如语言基础和上下文-和目标conditioned事件处理。
</details></li>
</ul>
<hr>
<h2 id="A-cross-Transformer-for-image-denoising"><a href="#A-cross-Transformer-for-image-denoising" class="headerlink" title="A cross Transformer for image denoising"></a>A cross Transformer for image denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10408">http://arxiv.org/abs/2310.10408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellloxiaotian/ctnet">https://github.com/hellloxiaotian/ctnet</a></li>
<li>paper_authors: Chunwei Tian, Menghua Zheng, Wangmeng Zuo, Shichao Zhang, Yanning Zhang, Chia-Wen Ling</li>
<li>for: 提高复杂场景中图像的清洁率</li>
<li>methods: 使用交叉 transformer 涨梯网络（CTNet），包括序列块（SB）、平行块（PB）和差分块（RB），以获取有效的结构信息，并通过多种交互来提高适应性。</li>
<li>results: 在实际和synthetic图像杂交中，CTNet表现出色，超过了一些流行的去噪方法。适用于移动设备，如手机。<details>
<summary>Abstract</summary>
Deep convolutional neural networks (CNNs) depend on feedforward and feedback ways to obtain good performance in image denoising. However, how to obtain effective structural information via CNNs to efficiently represent given noisy images is key for complex scenes. In this paper, we propose a cross Transformer denoising CNN (CTNet) with a serial block (SB), a parallel block (PB), and a residual block (RB) to obtain clean images for complex scenes. A SB uses an enhanced residual architecture to deeply search structural information for image denoising. To avoid loss of key information, PB uses three heterogeneous networks to implement multiple interactions of multi-level features to broadly search for extra information for improving the adaptability of an obtained denoiser for complex scenes. Also, to improve denoising performance, Transformer mechanisms are embedded into the SB and PB to extract complementary salient features for effectively removing noise in terms of pixel relations. Finally, a RB is applied to acquire clean images. Experiments illustrate that our CTNet is superior to some popular denoising methods in terms of real and synthetic image denoising. It is suitable to mobile digital devices, i.e., phones. Codes can be obtained at https://github.com/hellloxiaotian/CTNet.
</details>
<details>
<summary>摘要</summary>
深度卷积神经网络 (CNN) 在图像噪声除除针对 feedforward 和反馈方式以获得好的表现。然而，如何通过 CNN 获得有效的结构信息，以有效地表示给定的噪声图像是关键问题。在这篇论文中，我们提出了一种跨Transformer混合卷积神经网络 (CTNet)，包括序列块 (SB)、平行块 (PB) 和差异块 (RB)，以获取复杂场景中的干净图像。SB 使用增强的剩余架构，深入搜索图像噪声除除中的结构信息。为了避免关键信息损失，PB 使用三种不同的网络来实现多种交互，以广泛搜索更多的信息，以提高取得的噪声除除器的适应性。此外，为了提高噪声除除性能，SB 和 PB 中包含了Transformer机制，以EXTRACT complementary salient features，以有效地除除图像层次关系中的噪声。最后，RB 用于获取干净图像。实验表明，我们的 CTNet 在实际和 sintetic 图像噪声除除方面表现优于一些流行的噪声除除方法。它适用于移动设备，如手机。代码可以在 https://github.com/hellloxiaotian/CTNet  obtener。
</details></li>
</ul>
<hr>
<h2 id="LLM4SGG-Large-Language-Model-for-Weakly-Supervised-Scene-Graph-Generation"><a href="#LLM4SGG-Large-Language-Model-for-Weakly-Supervised-Scene-Graph-Generation" class="headerlink" title="LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation"></a>LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10404">http://arxiv.org/abs/2310.10404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rlqja1107/torch-LLM4SGG">https://github.com/rlqja1107/torch-LLM4SGG</a></li>
<li>paper_authors: Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park</li>
<li>for: 本研究旨在提出一种新的、基于语言模型的弱监督Scene Graph生成方法（LLM4SGG），以解决现有WSSGG方法中的两个问题：1）语义过度简化问题，2）低密度场景图问题。</li>
<li>methods: 我们提出一种新的方法，即使用语言模型的语言理解和推理能力来提取caption中的 triplets，并将entity&#x2F; predicate类与目标数据进行对齐。为了更好地利用语言模型，我们采用了链式思维和在Context few-shot learning策略。</li>
<li>results: 我们在Visual Genome和GQA datasets上进行了广泛的实验，并显示了与现有WSSGG方法相比的显著提高，包括Recall@K和mean Recall@K的提高。此外，LLM4SGG还具有数据效率的优势，可以通过小量的训练图像进行效果iveness的模型训练。<details>
<summary>Abstract</summary>
Weakly-Supervised Scene Graph Generation (WSSGG) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. In this regard, studies on WSSGG have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. However, they have overlooked the two issues involved in the triplet formation process from the captions: 1) Semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) Low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. To tackle the two issues, we propose a new approach, i.e., Large Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two issues by leveraging the LLM's in-depth understanding of language and reasoning ability during the extraction of triplets from captions and alignment of entity/predicate classes with target data. To further engage the LLM in these processes, we adopt the idea of Chain-of-Thought and the in-context few-shot learning strategy. To validate the effectiveness of LLM4SGG, we conduct extensive experiments on Visual Genome and GQA datasets, showing significant improvements in both Recall@K and mean Recall@K compared to the state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is data-efficient, enabling effective model training with a small amount of training images.
</details>
<details>
<summary>摘要</summary>
弱监督场景图生成（WSSGG）研究最近几年来得到了更多的关注，它作为完全监督的方法的替代方案，减少了成本的注释。在这个 regard，研究者们通过图文来获取不地址 triplets，主要是将图文中的不地址 triplets与图像区域相对应。然而，他们忽略了图文中 triplet 形成过程中的两个问题：1）语义过于简化问题，图文中的细腻 predicate 被不必要地转换为粗略 predicate，导致 predicate 的分布呈长尾形态；2）图像区域对应问题，在将 triplets 与 interesset class 对应时，多个 triplets 被抛弃，导致训练不充分。为解决这两个问题，我们提出了一种新的方法，即大语言模型 для 弱监督 SGG（LLM4SGG）。我们通过利用 LLM 的深刻语言理解和逻辑能力来缓解这两个问题。为了更好地利用 LLM，我们采用了链条思想和在 Context 中几招学习策略。为验证 LLM4SGG 的有效性，我们对 Visual Genome 和 GQA 数据集进行了广泛的实验，并显示了与州chart 方法相比的显著改善。此外，LLM4SGG 具有数据效率的特点，可以在小量训练图像上进行有效的模型训练。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Edge-Perceptual-Guided-Image-Filtering"><a href="#Enhanced-Edge-Perceptual-Guided-Image-Filtering" class="headerlink" title="Enhanced Edge-Perceptual Guided Image Filtering"></a>Enhanced Edge-Perceptual Guided Image Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10387">http://arxiv.org/abs/2310.10387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Li</li>
<li>for: 提高图像Edge-preserving能力和计算复杂性的问题</li>
<li>methods: 提出一种基于Explicit first-order edge-protect约束和Explicit residual约束的新导向图像滤波器</li>
<li>results: 在单图像细节增强、多尺度曝光融合和多spectral图像分类等应用中，提出的滤波器能够提高Edge-preserving能力，并且经过理论分析和实验验证<details>
<summary>Abstract</summary>
Due to the powerful edge-preserving ability and low computational complexity, Guided image filter (GIF) and its improved versions has been widely applied in computer vision and image processing. However, all of them are suffered halo artifacts to some degree, as the regularization parameter increase. In the case of inconsistent structure of guidance image and input image, edge-preserving ability degradation will also happen. In this paper, a novel guided image filter is proposed by integrating an explicit first-order edge-protect constraint and an explicit residual constraint which will improve the edge-preserving ability in both cases. To illustrate the efficiency of the proposed filter, the performances are shown in some typical applications, which are single image detail enhancement, multi-scale exposure fusion, hyper spectral images classification. Both theoretical analysis and experimental results prove that the powerful edge-preserving ability of the proposed filter.
</details>
<details>
<summary>摘要</summary>
因为导引图像过滤器（GIF）和其改进版本具有强大的边缘保持能力和低计算复杂性，因此在计算机视觉和图像处理领域得到广泛应用。然而，所有它们都受到一定程度的尘埃畸 artifacts的困扰，随着规则化参数的增加。在指导图像和输入图像的结构不一致的情况下，边缘保持能力也会降低。在这篇论文中，一种新的导引图像过滤器被提出，通过加入显式的第一阶边缘保护约束和显式的差分约束，以提高边缘保持能力。为了证明提案的效果，在一些典型应用中进行了表现，包括单图像细节增强、多比例曝光融合和多spectral图像分类。 Both theoretical analysis and experimental results prove that the proposed filter has powerful edge-preserving ability.
</details></li>
</ul>
<hr>
<h2 id="Looping-LOCI-Developing-Object-Permanence-from-Videos"><a href="#Looping-LOCI-Developing-Object-Permanence-from-Videos" class="headerlink" title="Looping LOCI: Developing Object Permanence from Videos"></a>Looping LOCI: Developing Object Permanence from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10372">http://arxiv.org/abs/2310.10372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Traub, Frederic Becker, Sebastian Otte, Martin V. Butz</li>
<li>for: 本研究旨在提高基于场景表示学习的分割和跟踪方法，使其能够更好地处理部分可见对象和逻辑物理测试。</li>
<li>methods: 本研究使用了Loci-Looped算法，它是一种基于Loci neural network架构的循环型朴素网络，可以自动将像素空间信息与预测结果混合，以获得信息混合活动。Loci-Looped还可以学习对象动态和对象之间交互的 compositional 表示。</li>
<li>results: Loci-Looped可以在对象遮挡时长时间跟踪对象，甚至预测遮挡后对象的重返，无需显式历史缓存。此外，Loci-Looped在ADEPT和CLEVRER数据集上比基于现有模型表现出色，在对象遮挡或感知数据断续时表现更好。这表示Loci-Looped可以在无监督下自适应学习物理概念，包括对象持续性和静止性。<details>
<summary>Abstract</summary>
Recent compositional scene representation learning models have become remarkably good in segmenting and tracking distinct objects within visual scenes. Yet, many of these models require that objects are continuously, at least partially, visible. Moreover, they tend to fail on intuitive physics tests, which infants learn to solve over the first months of their life. Our goal is to advance compositional scene representation algorithms with an embedded algorithm that fosters the progressive learning of intuitive physics, akin to infant development. As a fundamental component for such an algorithm, we introduce Loci-Looped, which advances a recently published unsupervised object location, identification, and tracking neural network architecture (Loci, Traub et al., ICLR 2023) with an internal processing loop. The loop is designed to adaptively blend pixel-space information with anticipations yielding information-fused activities as percepts. Moreover, it is designed to learn compositional representations of both individual object dynamics and between-objects interaction dynamics. We show that Loci-Looped learns to track objects through extended periods of object occlusions, indeed simulating their hidden trajectories and anticipating their reappearance, without the need for an explicit history buffer. We even find that Loci-Looped surpasses state-of-the-art models on the ADEPT and the CLEVRER dataset, when confronted with object occlusions or temporary sensory data interruptions. This indicates that Loci-Looped is able to learn the physical concepts of object permanence and inertia in a fully unsupervised emergent manner. We believe that even further architectural advancements of the internal loop - also in other compositional scene representation learning models - can be developed in the near future.
</details>
<details>
<summary>摘要</summary>
现代场景表示学习模型已经很出色地 segmenting 和跟踪视场中的不同对象。然而，许多这些模型需要对象在视场中保持不间断的可见性。此外，它们在直觉物理测试中表现不佳，这与婴儿在生长过程中学习的直觉物理概念相反。我们的目标是提高场景表示算法，其中包括一个内置的算法，以便逐步学习直觉物理概念，类似于婴儿的发展。为此，我们引入了Loci-Looped，它是一种基于Loci（Traub et al., ICLR 2023）的无监督对象位置、识别和跟踪神经网络架构，并添加了内部处理循环。这个循环通过自适应混合像素空间信息和预测得到的信息混合活动，以便学习对象动态和对象之间的交互动态。我们发现Loci-Looped可以在对象遮挡期间跟踪对象，并且可以预测遮挡物体的重返，无需显式历史缓存。此外，Loci-Looped在ADEPT和CLEVRER数据集上表现出色，even when confronted with object occlusions or temporary sensory data interruptions.这表明Loci-Looped可以在无监督下自适应学习物理概念，包括对象永恒和运动的概念。我们认为，将Loci-Looped的内部循环扩展到其他场景表示学习模型中，可能会在未来得到进一步改进。
</details></li>
</ul>
<hr>
<h2 id="Camera-LiDAR-Fusion-with-Latent-Contact-for-Place-Recognition-in-Challenging-Cross-Scenes"><a href="#Camera-LiDAR-Fusion-with-Latent-Contact-for-Place-Recognition-in-Challenging-Cross-Scenes" class="headerlink" title="Camera-LiDAR Fusion with Latent Contact for Place Recognition in Challenging Cross-Scenes"></a>Camera-LiDAR Fusion with Latent Contact for Place Recognition in Challenging Cross-Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10371">http://arxiv.org/abs/2310.10371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Pan, Jiapeng Xie, Jiajie Wu, Bo Zhou</li>
<li>for: 本文是为了解决在视角变化、季节变化和场景变换等环境下实现地点认知而写的。</li>
<li>methods: 本文使用了一种新的三通道地点描述器，包括图像、点云和融合分支。图像和点云之间的相互关系被利用，以实现信息互动和融合。</li>
<li>results: EXTENSIVE experiments on KITTI、NCLT、USVInland和校园数据集表明，提出的地点描述器为最佳方法，在复杂的场景下表现了 robustness 和通用性。<details>
<summary>Abstract</summary>
Although significant progress has been made, achieving place recognition in environments with perspective changes, seasonal variations, and scene transformations remains challenging. Relying solely on perception information from a single sensor is insufficient to address these issues. Recognizing the complementarity between cameras and LiDAR, multi-modal fusion methods have attracted attention. To address the information waste in existing multi-modal fusion works, this paper introduces a novel three-channel place descriptor, which consists of a cascade of image, point cloud, and fusion branches. Specifically, the fusion-based branch employs a dual-stage pipeline, leveraging the correlation between the two modalities with latent contacts, thereby facilitating information interaction and fusion. Extensive experiments on the KITTI, NCLT, USVInland, and the campus dataset demonstrate that the proposed place descriptor stands as the state-of-the-art approach, confirming its robustness and generality in challenging scenarios.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管已经做出了很大的进步，但是在视角变化、季节变化和场景变换等环境下实现地点认知仍然是一个挑战。仅仅基于单一感知器的信息不够 Address these issues. 识别摄像头和激光仪的补偿性，多感知Modal Fusion方法吸引了关注。为了改进现有多感知融合方法中的信息浪费，本文提出了一种新的三通道地点描述符，包括图像、点云和融合分支。具体来说，融合分支采用了双 stage pipeline，利用两个感知器之间的相互关系，以便信息互动和融合。从 KITTI、NCLT、USVInland 和校园数据集进行了广泛的实验，confirming its robustness and generality in challenging scenarios.
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Object-Query-Initialization-for-3D-Object-Detection"><a href="#Multimodal-Object-Query-Initialization-for-3D-Object-Detection" class="headerlink" title="Multimodal Object Query Initialization for 3D Object Detection"></a>Multimodal Object Query Initialization for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10353">http://arxiv.org/abs/2310.10353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathijs R. van Geerenstein, Felicia Ruppel, Klaus Dietmayer, Dariu M. Gavrila</li>
<li>For: 本研究旨在提高LiDAR和摄像头感知器件之间的对接，以提高3D物体检测模型的性能。* Methods: 我们提出了一种高效、可组合、多Modal的对象查询初始化方法，使得查询可以在多种感知器件输入下进行初始化。* Results: 我们在nuScenesbenchmark上进行了实验，并与现有方法进行比较。结果显示，我们的方法可以在LiDAR-camera输入下达到更高的性能，并且比现有方法更快。此外，我们的方法可以适用于任意感知器件输入组合。<details>
<summary>Abstract</summary>
3D object detection models that exploit both LiDAR and camera sensor features are top performers in large-scale autonomous driving benchmarks. A transformer is a popular network architecture used for this task, in which so-called object queries act as candidate objects. Initializing these object queries based on current sensor inputs is a common practice. For this, existing methods strongly rely on LiDAR data however, and do not fully exploit image features. Besides, they introduce significant latency. To overcome these limitations we propose EfficientQ3M, an efficient, modular, and multimodal solution for object query initialization for transformer-based 3D object detection models. The proposed initialization method is combined with a "modality-balanced" transformer decoder where the queries can access all sensor modalities throughout the decoder. In experiments, we outperform the state of the art in transformer-based LiDAR object detection on the competitive nuScenes benchmark and showcase the benefits of input-dependent multimodal query initialization, while being more efficient than the available alternatives for LiDAR-camera initialization. The proposed method can be applied with any combination of sensor modalities as input, demonstrating its modularity.
</details>
<details>
<summary>摘要</summary>
三维物体探测模型，利用激光和相机感知器件特点，在大规模自动驾驶benchmark中表现出色。 transformer是一种广泛使用的网络架构，在这种情况下，被称为“对象查询”的对象被当作候选对象。现有方法通常基于现有的激光数据进行初始化，但是不充分利用图像特征。此外，它们也会增加显著的延迟。为了解决这些限制，我们提出了高效的EfficientQ3M方法，用于初始化转换器基于三维对象探测模型中的对象查询。我们的初始化方法与“多感器均衡”转换器解码器结合使用，使得查询可以在解码器中访问所有感知模式。在实验中，我们超越了现有的转换器基于LiDAR对象探测模型的状态，在competitive nuScenes benchmark上表现出色，并示出了输入具有multimodal查询初始化的优势，同时更高效于现有的LiDAR-camera初始化方法。该方法可以针对任何感知模式进行输入，表明其模块性。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Crowd-Counting-with-Contextual-Modeling-Facilitating-Holistic-Understanding-of-Crowd-Scenes"><a href="#Semi-Supervised-Crowd-Counting-with-Contextual-Modeling-Facilitating-Holistic-Understanding-of-Crowd-Scenes" class="headerlink" title="Semi-Supervised Crowd Counting with Contextual Modeling: Facilitating Holistic Understanding of Crowd Scenes"></a>Semi-Supervised Crowd Counting with Contextual Modeling: Facilitating Holistic Understanding of Crowd Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10352">http://arxiv.org/abs/2310.10352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cha15yq/MRC-Crowd">https://github.com/cha15yq/MRC-Crowd</a></li>
<li>paper_authors: Yifei Qian, Xiaopeng Hong, Ognjen Arandjelović, Zhongliang Guo, Carl R. Donovan</li>
<li>for: 增强人群计数模型的可靠性和准确性，提高模型在受限数据量时的泛化能力。</li>
<li>methods: 基于mean teacher框架，对无标签数据进行masking处理，以便模型通过整体特征学习人群场景，模仿人类认知过程。其他方法包括 incorporating fine-grained density classification task，以提高特征学习。</li>
<li>results: 模型在挑战性评价指标上表现出色，胜过之前的方法，且模型具有’subitizing’-like行为，即准确地计算低密度区域，同时 incorporating 地方细节来计算高密度区域。<details>
<summary>Abstract</summary>
To alleviate the heavy annotation burden for training a reliable crowd counting model and thus make the model more practicable and accurate by being able to benefit from more data, this paper presents a new semi-supervised method based on the mean teacher framework. When there is a scarcity of labeled data available, the model is prone to overfit local patches. Within such contexts, the conventional approach of solely improving the accuracy of local patch predictions through unlabeled data proves inadequate. Consequently, we propose a more nuanced approach: fostering the model's intrinsic 'subitizing' capability. This ability allows the model to accurately estimate the count in regions by leveraging its understanding of the crowd scenes, mirroring the human cognitive process. To achieve this goal, we apply masking on unlabeled data, guiding the model to make predictions for these masked patches based on the holistic cues. Furthermore, to help with feature learning, herein we incorporate a fine-grained density classification task. Our method is general and applicable to most existing crowd counting methods as it doesn't have strict structural or loss constraints. In addition, we observe that the model trained with our framework exhibits a 'subitizing'-like behavior. It accurately predicts low-density regions with only a 'glance', while incorporating local details to predict high-density regions. Our method achieves the state-of-the-art performance, surpassing previous approaches by a large margin on challenging benchmarks such as ShanghaiTech A and UCF-QNRF. The code is available at: https://github.com/cha15yq/MRC-Crowd.
</details>
<details>
<summary>摘要</summary>
为了减轻人群计数模型的注释负担，从而使模型更实用和准确，这篇论文提出了一种新的半监督方法基于 Mean Teacher 框架。当缺乏标注数据时，模型容易过拟合本地块。在这种情况下，通过 solely 提高本地块预测的准确性来提高模型的性能是不够的。因此，我们提出了一种更加细化的方法：激发模型的内在 'subitizing' 能力。这种能力使模型可以通过利用人群场景的理解来准确地计算区域的人群数量，这与人类认知过程相似。为了实现这个目标，我们在无标注数据上应用掩码，使模型根据全景册筹的信息进行预测。此外，我们还在模型中添加了细化的浓度分类任务，以帮助特征学习。我们的方法是通用的，可以应用于大多数现有的人群计数方法，不具有严格的结构或损失约束。此外，我们发现模型通过我们的框架进行训练时会展现 'subitizing' 类的行为，即可以准确地计算低密度区域，只需要一个 '察看'，同时 incorporate 本地细节来预测高密度区域。我们的方法在挑战性较高的标准准则 ShanghaiTech A 和 UCF-QNRF 上实现了 estado del arte 的性能，大幅超过了先前的方法。模型代码可以在 GitHub 上找到：https://github.com/cha15yq/MRC-Crowd。
</details></li>
</ul>
<hr>
<h2 id="ConsistNet-Enforcing-3D-Consistency-for-Multi-view-Images-Diffusion"><a href="#ConsistNet-Enforcing-3D-Consistency-for-Multi-view-Images-Diffusion" class="headerlink" title="ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion"></a>ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10343">http://arxiv.org/abs/2310.10343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiayuyang/consistnet">https://github.com/jiayuyang/consistnet</a></li>
<li>paper_authors: Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, Hongdong Li</li>
<li>for: 这篇论文的目的是提出一种能够生成多个不同视角的图像，同时保持3D（多视图）一致性的方法。</li>
<li>methods: 该方法基于一个多视图一致块，该块在多个单视图噪声过程中交换信息，根据多视图几何原理来协调多个单视图特征。</li>
<li>results: 该方法可以轻松地插入预训练的LDMs（卷积神经网络），不需要显式的像素对应关系或深度预测。实验显示，该方法可以在40秒内在单个A100 GPU上生成16个不同视角的图像，并且能够有效地学习3D一致性。<details>
<summary>Abstract</summary>
Given a single image of a 3D object, this paper proposes a novel method (named ConsistNet) that is able to generate multiple images of the same object, as if seen they are captured from different viewpoints, while the 3D (multi-view) consistencies among those multiple generated images are effectively exploited. Central to our method is a multi-view consistency block which enables information exchange across multiple single-view diffusion processes based on the underlying multi-view geometry principles. ConsistNet is an extension to the standard latent diffusion model, and consists of two sub-modules: (a) a view aggregation module that unprojects multi-view features into global 3D volumes and infer consistency, and (b) a ray aggregation module that samples and aggregate 3D consistent features back to each view to enforce consistency. Our approach departs from previous methods in multi-view image generation, in that it can be easily dropped-in pre-trained LDMs without requiring explicit pixel correspondences or depth prediction. Experiments show that our method effectively learns 3D consistency over a frozen Zero123 backbone and can generate 16 surrounding views of the object within 40 seconds on a single A100 GPU. Our code will be made available on https://github.com/JiayuYANG/ConsistNet
</details>
<details>
<summary>摘要</summary>
给定一张3D对象的单张图像，这篇论文提出了一种新的方法（名为ConsistNet），可以生成多张不同视角的图像，同时利用多视图的3D一致性。我们的方法的核心是一个多视图一致块，允许多个单视图的扩散过程之间进行信息交换，基于下面的多视图几何原理。ConsistNet是标准潜在扩散模型的扩展，包括两个子模块：（a）视图聚合模块，将多视图特征映射到全局3D体Volume并评估一致性，以及（b）光束聚合模块，从3D一致的特征样本返回到每个视图，以确保一致性。我们的方法与前一些多视图图像生成方法不同，可以直接使用预训练的LDMs，无需明确的像素匹配或深度预测。实验表明，我们的方法可以在冰zero123架构上学习3D一致性，在单个A100 GPU上生成16个对象周围视图 Within 40秒。我们的代码将在https://github.com/JiayuYANG/ConsistNet上公开。
</details></li>
</ul>
<hr>
<h2 id="Scene-Graph-Conditioning-in-Latent-Diffusion"><a href="#Scene-Graph-Conditioning-in-Latent-Diffusion" class="headerlink" title="Scene Graph Conditioning in Latent Diffusion"></a>Scene Graph Conditioning in Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10338">http://arxiv.org/abs/2310.10338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/frankfundel/sgcond">https://github.com/frankfundel/sgcond</a></li>
<li>paper_authors: Frank Fundel</li>
<li>for: 这个论文旨在提高基于文本描述的扩展凝聚模型的精细Semantic控制和精准生成图像。</li>
<li>methods: 这篇论文使用了ControlNet和Gated Self-Attention等多种方法来解决大规模凝聚模型的微调和精准生成图像。</li>
<li>results: 研究人员通过使用提出的方法可以生成图像从场景图中得到更高质量的图像，超过了之前的方法。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Diffusion models excel in image generation but lack detailed semantic control using text prompts. Additional techniques have been developed to address this limitation. However, conditioning diffusion models solely on text-based descriptions is challenging due to ambiguity and lack of structure. In contrast, scene graphs offer a more precise representation of image content, making them superior for fine-grained control and accurate synthesis in image generation models. The amount of image and scene-graph data is sparse, which makes fine-tuning large diffusion models challenging. We propose multiple approaches to tackle this problem using ControlNet and Gated Self-Attention. We were able to show that using out proposed methods it is possible to generate images from scene graphs with much higher quality, outperforming previous methods. Our source code is publicly available on https://github.com/FrankFundel/SGCond
</details>
<details>
<summary>摘要</summary>
吸引模型在图像生成方面表现出色，但缺乏文本描述的细腻 semantic控制。为了解决这个限制，有些技术被开发出来。然而，通过 solely 文本描述来conditioning 吸引模型是困难的，因为描述的ambiguity和lack of structure。相比之下，场景图表示图像内容的更加精细，使其成为更好的 Fine-grained control 和图像生成模型的精准合成。然而，图像和场景图数据的量是稀缺的，这使得 fine-tuning 大型吸引模型困难。我们提出了多种方法来解决这个问题，包括ControlNet和Gated Self-Attention。我们能够证明，使用我们的方法可以生成从场景图中的图像，质量远高于之前的方法。我们的源代码在https://github.com/FrankFundel/SGCond 上公开可用。
</details></li>
</ul>
<hr>
<h2 id="Towards-image-compression-with-perfect-realism-at-ultra-low-bitrates"><a href="#Towards-image-compression-with-perfect-realism-at-ultra-low-bitrates" class="headerlink" title="Towards image compression with perfect realism at ultra-low bitrates"></a>Towards image compression with perfect realism at ultra-low bitrates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10325">http://arxiv.org/abs/2310.10325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marlène Careil, Matthew J. Muckley, Jakob Verbeek, Stéphane Lathuilière</li>
<li>for: 提高图像质量，使其不受比特率的影响</li>
<li>methods: 使用迭代扩散模型代替feed-forward推理器，并conditioning模型于vector-quantized图像表示和全文描述</li>
<li>results: 与状态之前的编码器相比，提高图像质量，并在ultra-low比特率下（0.003比特&#x2F;像素）提供高质量图像重建Here’s the breakdown of each point:</li>
<li>for: The paper aims to improve the quality of compressed images and make it less dependent on the bitrate.</li>
<li>methods: The proposed method uses iterative diffusion models instead of feed-forward decoders trained with MSE or LPIPS distortions. Additionally, the model is conditioned on a vector-quantized image representation and a global textual image description to provide additional context.</li>
<li>results: The proposed method outperforms state-of-the-art codecs at ultra-low bitrates (0.003 bits&#x2F;pixel) and provides high-quality image reconstruction with low dependence on the bitrate.<details>
<summary>Abstract</summary>
Image codecs are typically optimized to trade-off bitrate vs, distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality, and to make it less dependent on the bitrate, we propose to decode with iterative diffusion models, instead of feed-forward decoders trained using MSE or LPIPS distortions used in most neural codecs. In addition to conditioning the model on a vector-quantized image representation, we also condition on a global textual image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is an order of magnitude smaller than those considered in most prior work. At this bitrate a 512x768 Kodak image is encoded in less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID, and that the visual quality is less dependent on the bitrate than previous methods.
</details>
<details>
<summary>摘要</summary>
图像编码器通常是进行比特率vs扭曲指标的优化的。在低比特率下，这会导致压缩artefacts，即使在使用感知或敌对损失进行训练。为了改进图像质量并使其不受比特率的影响，我们提议使用迭代扩散模型进行解码，而不是使用MSE或LPIPS损失来训练Feed-forward decoder。此外，我们还conditioning the model on a vector-quantized image representation和global文本描述来提供额外的 контекст。我们称我们的模型为PerCo，用于'感知压缩'，并与当前的编码器进行比较。我们的模型在比特率从0.1下到0.003比特每像素进行比较，其中0.003比特每像素是在大多数先前工作中考虑的一个次数。在这个比特率下，我们可以将512x768像素的Kodak图像编码为 less than 153字节。尽管我们的比特率非常低，但我们的方法可以保持实际的图像重建。我们发现我们的模型可以在FID和KID指标下达到状态泰ometer的视觉质量，并且这种视觉质量与比特率相对较少受到影响。
</details></li>
</ul>
<hr>
<h2 id="Multi-Body-Neural-Scene-Flow"><a href="#Multi-Body-Neural-Scene-Flow" class="headerlink" title="Multi-Body Neural Scene Flow"></a>Multi-Body Neural Scene Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10301">http://arxiv.org/abs/2310.10301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kavisha725/MBNSF">https://github.com/kavisha725/MBNSF</a></li>
<li>paper_authors: Kavisha Vidanapathirana, Shin-Fang Chng, Xueqian Li, Simon Lucey</li>
<li>for: 本研究旨在提高Scene Flow的测试时优化，使其能够更好地处理实际世界数据中的多体刚体运动。</li>
<li>methods: 作者提出了一种基于坐标网络的神经网络优化方法，通过正则化Scene Flow预测中的流体平滑性来捕捉通用运动。此外，作者还引入了一种基于流体尺度的正则项，以便在多体刚体运动中保持流体场的连续性。</li>
<li>results: 作者在实际数据上进行了广泛的实验，并证明了他们的方法能够超过当前最佳的3D Scene Flow和长期点云轨迹预测。 codes available at: \href{<a target="_blank" rel="noopener" href="https://github.com/kavisha725/MBNSF%7D%7Bhttps://github.com/kavisha725/MBNSF%7D">https://github.com/kavisha725/MBNSF}{https://github.com/kavisha725/MBNSF}</a>.<details>
<summary>Abstract</summary>
The test-time optimization of scene flow - using a coordinate network as a neural prior - has gained popularity due to its simplicity, lack of dataset bias, and state-of-the-art performance. We observe, however, that although coordinate networks capture general motions by implicitly regularizing the scene flow predictions to be spatially smooth, the neural prior by itself is unable to identify the underlying multi-body rigid motions present in real-world data. To address this, we show that multi-body rigidity can be achieved without the cumbersome and brittle strategy of constraining the $SE(3)$ parameters of each rigid body as done in previous works. This is achieved by regularizing the scene flow optimization to encourage isometry in flow predictions for rigid bodies. This strategy enables multi-body rigidity in scene flow while maintaining a continuous flow field, hence allowing dense long-term scene flow integration across a sequence of point clouds. We conduct extensive experiments on real-world datasets and demonstrate that our approach outperforms the state-of-the-art in 3D scene flow and long-term point-wise 4D trajectory prediction. The code is available at: \href{https://github.com/kavisha725/MBNSF}{https://github.com/kavisha725/MBNSF}.
</details>
<details>
<summary>摘要</summary>
scene flow 测试时优化 - 使用坐标网络作为神经网络先验 - 在过去几年中变得越来越流行，这是因为它的简单性、不受数据偏见和现在的表现水平都很高。但我们发现，即使坐标网络可以捕捉一般运动的概念，但神经网络本身无法直接捕捉真实世界数据中的多体刚性运动。为解决这个问题，我们表明了一种不需要干扰和脆弱的策略，即在场景流优化中规范化流预测以促进刚性。这种策略允许场景流中的多体刚性，同时保持连续的流场，因此允许长期场景流集成。我们对实际数据进行了广泛的实验，并证明了我们的方法在3D场景流和长期点云轨迹预测中超越了现有的状态艺术。代码可以在：<https://github.com/kavisha725/MBNSF>上下载。
</details></li>
</ul>
<hr>
<h2 id="Effortless-Cross-Platform-Video-Codec-A-Codebook-Based-Method"><a href="#Effortless-Cross-Platform-Video-Codec-A-Codebook-Based-Method" class="headerlink" title="Effortless Cross-Platform Video Codec: A Codebook-Based Method"></a>Effortless Cross-Platform Video Codec: A Codebook-Based Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10292">http://arxiv.org/abs/2310.10292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan Tian, Yonghang Guan, Jinxi Xiang, Jun Zhang, Xiao Han, Wei Yang</li>
<li>for: 提高视频编码器的环境灵活性和计算效率</li>
<li>methods: 基于码ebook的视频编码框架，使用Conditional Cross-Attention模块取得帧之间的上下文</li>
<li>results: 实验结果显示，我们的方法可以超越传统的H.265（中）编码器，无需任何Entropy约束，同时具备跨平台性Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 提高视频编码器的环境灵活性和计算效率</li>
<li>methods: 基于码ebook的视频编码框架，使用Conditional Cross-Attention模块取得帧之间的上下文</li>
<li>results: 实验结果显示，我们的方法可以超越传统的H.265（中）编码器，无需任何Entropy约束，同时具备跨平台性<details>
<summary>Abstract</summary>
Under certain circumstances, advanced neural video codecs can surpass the most complex traditional codecs in their rate-distortion (RD) performance. One of the main reasons for the high performance of existing neural video codecs is the use of the entropy model, which can provide more accurate probability distribution estimations for compressing the latents. This also implies the rigorous requirement that entropy models running on different platforms should use consistent distribution estimations. However, in cross-platform scenarios, entropy models running on different platforms usually yield inconsistent probability distribution estimations due to floating point computation errors that are platform-dependent, which can cause the decoding side to fail in correctly decoding the compressed bitstream sent by the encoding side. In this paper, we propose a cross-platform video compression framework based on codebooks, which avoids autoregressive entropy modeling and achieves video compression by transmitting the index sequence of the codebooks. Moreover, instead of using optical flow for context alignment, we propose to use the conditional cross-attention module to obtain the context between frames. Due to the absence of autoregressive modeling and optical flow alignment, we can design an extremely minimalist framework that can greatly benefit computational efficiency. Importantly, our framework no longer contains any distribution estimation modules for entropy modeling, and thus computations across platforms are not necessarily consistent. Experimental results show that our method can outperform the traditional H.265 (medium) even without any entropy constraints, while achieving the cross-platform property intrinsically.
</details>
<details>
<summary>摘要</summary>
在某些情况下，高级神经视频编码器可以超越最复杂的传统编码器在比特率-损失（RD）性能方面。主要的原因是使用Entropy模型，可以提供更准确的概率分布估计，用于压缩缓冲。然而，在跨平台场景下，运行于不同平台的Entropy模型通常会产生不一致的概率分布估计，因为计算机中的浮点数计算错误是平台相关的，这会导致解码器无法正确地解码编码器发送的压缩位流。在本文中，我们提出了基于codebooks的跨平台视频压缩框架，不使用潮流模型和相关适应模块，而是通过传输编码器序列的index来实现压缩。另外，我们提出了基于条件cross-attention模块来获取帧之间的上下文。由于不使用潮流模型和相关适应模块，我们可以设计一个极其简洁的框架，可以大幅提高计算效率。重要的是，我们的框架不再包含任何分布估计模块，因此在不同平台上的计算是不一致的。实验结果表明，我们的方法可以在不使用Entropy约束下，超越传统H.265（中）的RD性能，同时实现跨平台性特性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-World-Co-Salient-Object-Detection-with-Generative-Uncertainty-aware-Group-Selective-Exchange-Masking"><a href="#Towards-Open-World-Co-Salient-Object-Detection-with-Generative-Uncertainty-aware-Group-Selective-Exchange-Masking" class="headerlink" title="Towards Open-World Co-Salient Object Detection with Generative Uncertainty-aware Group Selective Exchange-Masking"></a>Towards Open-World Co-Salient Object Detection with Generative Uncertainty-aware Group Selective Exchange-Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10264">http://arxiv.org/abs/2310.10264</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wuyang98/CoSOD">https://github.com/wuyang98/CoSOD</a></li>
<li>paper_authors: Yang Wu, Shenglong Hu, Huihui Song, Kaihua Zhang, Bo Liu, Dong Liu</li>
<li>for: 提高CoSOD模型在开放世界场景下的Robustness。</li>
<li>methods: 引入集选择交换掩码（GSEM）方法，使用混合度量选择图像，并使用变量量生成器和CoSOD变换分支模型。</li>
<li>results: 提出了一种基于变量量生成器和CoSOD变换分支模型的Robust CoSOD方法，并在三个开放世界 benchmark dataset上进行了实验，证明了方法的有效性和实用性。<details>
<summary>Abstract</summary>
The traditional definition of co-salient object detection (CoSOD) task is to segment the common salient objects in a group of relevant images. This definition is based on an assumption of group consensus consistency that is not always reasonable in the open-world setting, which results in robustness issue in the model when dealing with irrelevant images in the inputting image group under the open-word scenarios. To tackle this problem, we introduce a group selective exchange-masking (GSEM) approach for enhancing the robustness of the CoSOD model. GSEM takes two groups of images as input, each containing different types of salient objects. Based on the mixed metric we designed, GSEM selects a subset of images from each group using a novel learning-based strategy, then the selected images are exchanged. To simultaneously consider the uncertainty introduced by irrelevant images and the consensus features of the remaining relevant images in the group, we designed a latent variable generator branch and CoSOD transformer branch. The former is composed of a vector quantised-variational autoencoder to generate stochastic global variables that model uncertainty. The latter is designed to capture correlation-based local features that include group consensus. Finally, the outputs of the two branches are merged and passed to a transformer-based decoder to generate robust predictions. Taking into account that there are currently no benchmark datasets specifically designed for open-world scenarios, we constructed three open-world benchmark datasets, namely OWCoSal, OWCoSOD, and OWCoCA, based on existing datasets. By breaking the group-consistency assumption, these datasets provide effective simulations of real-world scenarios and can better evaluate the robustness and practicality of models.
</details>
<details>
<summary>摘要</summary>
传统上，co-salient object detection（CoSOD）任务的定义是将相同的焦点对象在多个相关图像中分割。这个定义基于了群体一致性的假设，这并不总是在开放世界场景下合理的，这会导致模型在处理无关图像时出现Robustness问题。为解决这个问题，我们提出了群选择交换掩码（GSEM）方法，用于增强CoSOD模型的Robustness。GSEM使用两组图像作为输入，每组图像含有不同类型的焦点对象。基于我们定义的混合度量，GSEM选择每组图像的一部分图像，然后将这些图像交换。为同时考虑无关图像引入的不确定性和剩下相关图像的协同特征，我们设计了隐藏变量生成分支和CoSOD变换分支。前者由vector quantized-variational autoencoder组成，用于生成随机全球变量，模型不确定性。后者是为了捕捉协同特征，包括群体一致性。最后，两个分支的输出被 merge，并传递到基于变换器的解码器，以生成Robust的预测。考虑到目前没有特定于开放世界场景的准确数据集，我们构建了三个开放世界数据集，namely OWCoSal、OWCoSOD和OWCoCA，基于现有数据集。由于这些数据集破坏了群体一致性假设，它们可以更好地模拟实际场景，并且可以更好地评估模型的Robustness和实用性。
</details></li>
</ul>
<hr>
<h2 id="Long-term-Dependency-for-3D-Reconstruction-of-Freehand-Ultrasound-Without-External-Tracker"><a href="#Long-term-Dependency-for-3D-Reconstruction-of-Freehand-Ultrasound-Without-External-Tracker" class="headerlink" title="Long-term Dependency for 3D Reconstruction of Freehand Ultrasound Without External Tracker"></a>Long-term Dependency for 3D Reconstruction of Freehand Ultrasound Without External Tracker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10248">http://arxiv.org/abs/2310.10248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucl-candi/freehand">https://github.com/ucl-candi/freehand</a></li>
<li>paper_authors: Qi Li, Ziyi Shen, Qian Li, Dean C. Barratt, Thomas Dowrick, Matthew J. Clarkson, Tom Vercauteren, Yipeng Hu</li>
<li>For: 定义新的方法来嵌入长期依赖性，并评估其性能。* Methods: 使用序列模型与多个变数预测来编码长期依赖性，并提出两个依赖因子（体部图像内容和扫描协议）以推广精准重建。* Results: 1) 添加长期依赖性可以提高重建精度，并且随序列长度、变数间隔和扫描协议而变化。2) 对于训练中的体部或协议方差的降低，对重建精度产生负面影响。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Objective: Reconstructing freehand ultrasound in 3D without any external tracker has been a long-standing challenge in ultrasound-assisted procedures. We aim to define new ways of parameterising long-term dependencies, and evaluate the performance. Methods: First, long-term dependency is encoded by transformation positions within a frame sequence. This is achieved by combining a sequence model with a multi-transformation prediction. Second, two dependency factors are proposed, anatomical image content and scanning protocol, for contributing towards accurate reconstruction. Each factor is quantified experimentally by reducing respective training variances. Results: 1) The added long-term dependency up to 400 frames at 20 frames per second (fps) indeed improved reconstruction, with an up to 82.4% lowered accumulated error, compared with the baseline performance. The improvement was found to be dependent on sequence length, transformation interval and scanning protocol and, unexpectedly, not on the use of recurrent networks with long-short term modules; 2) Decreasing either anatomical or protocol variance in training led to poorer reconstruction accuracy. Interestingly, greater performance was gained from representative protocol patterns, than from representative anatomical features. Conclusion: The proposed algorithm uses hyperparameter tuning to effectively utilise long-term dependency. The proposed dependency factors are of practical significance in collecting diverse training data, regulating scanning protocols and developing efficient networks. Significance: The proposed new methodology with publicly available volunteer data and code for parametersing the long-term dependency, experimentally shown to be valid sources of performance improvement, which could potentially lead to better model development and practical optimisation of the reconstruction application.
</details>
<details>
<summary>摘要</summary>
目标：无需外部跟踪器，在ultrasound-assisted程序中自由手写三维重建问题已经是长期的挑战。我们想要定义新的方法来parameterize长期依赖关系，并评估其性能。方法：首先，通过将长期依赖关系编码为帧序列中的变换位置，使用序列模型和多变换预测结合。其次，我们提出了两个依赖因素，一是解剖学图像内容，二是扫描协议。每个因素都是通过实验量化训练方差来评估。结果：1）在400帧内的20帧/秒（fps）加入长期依赖关系后，重建精度显著提高，相比基eline性能，下降82.4%的累积错误。这种改进与序列长度、变换间隔和扫描协议有关，不同于使用循环网络long-short term模块。2）在训练中降低解剖学或协议方差可以得到更差的重建精度。意外地，更多的表现是由代表协议模式获得的，而不是由解剖学特征获得。结论：我们的算法使用了适当的hyperparameter调整，以利用长期依赖关系。我们提出的依赖因素对于收集多样化的训练数据、调整扫描协议和开发高效的网络是实际上的有用。意义：我们的新方法ологи在公共可用的志愿者数据和代码中实现了参数化长期依赖关系，实验证明了这些方法的有效性，这可能会导致更好的模型开发和实用优化重建应用。
</details></li>
</ul>
<hr>
<h2 id="Mask-wearing-object-detection-algorithm-based-on-improved-YOLOv5"><a href="#Mask-wearing-object-detection-algorithm-based-on-improved-YOLOv5" class="headerlink" title="Mask wearing object detection algorithm based on improved YOLOv5"></a>Mask wearing object detection algorithm based on improved YOLOv5</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10245">http://arxiv.org/abs/2310.10245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wen, Junhu Zhang, Haitao Li</li>
<li>for: 本研究旨在提出一种基于YOLOv5l的面Mask检测模型，以提高公共场合人员戴Mask的检测精度。</li>
<li>methods: 本研究使用Multi-Head Attentional Self-Convolution和Swin Transformer Block以提高模型的敏捷度和准确率。此外，我们还提出了I-CBAM模块以提高目标检测精度。</li>
<li>results: 在MASK数据集上进行实验，我们的模型比YOLOv5l模型提高了1.1%的mAP(0.5)和1.3%的mAP(0.5:0.95)。这表明我们的提案可以显著提高面Mask检测的精度。<details>
<summary>Abstract</summary>
Wearing a mask is one of the important measures to prevent infectious diseases. However, it is difficult to detect people's mask-wearing situation in public places with high traffic flow. To address the above problem, this paper proposes a mask-wearing face detection model based on YOLOv5l. Firstly, Multi-Head Attentional Self-Convolution not only improves the convergence speed of the model but also enhances the accuracy of the model detection. Secondly, the introduction of Swin Transformer Block is able to extract more useful feature information, enhance the detection ability of small targets, and improve the overall accuracy of the model. Our designed I-CBAM module can improve target detection accuracy. In addition, using enhanced feature fusion enables the model to better adapt to object detection tasks of different scales. In the experimentation on the MASK dataset, the results show that the model proposed in this paper achieved a 1.1% improvement in mAP(0.5) and a 1.3% improvement in mAP(0.5:0.95) compared to the YOLOv5l model. Our proposed method significantly enhances the detection capability of mask-wearing.
</details>
<details>
<summary>摘要</summary>
穿戴口罩是预防感染疾病的一种重要措施。然而，在高流量的公共场所中探测人们穿戴口罩的情况很困难。为解决这个问题，本文提出了基于YOLOv5l的口罩穿戴面部检测模型。首先，多头注意力自适应卷积不仅提高模型的融合速度，也提高了模型的检测精度。其次，将Swin卷积层引入可以提取更多有用的特征信息，提高小目标的检测能力，并提高模型的总精度。我们设计的I-CBAM模块可以提高标的检测精度。此外，使用强化的特征融合可以让模型更好地适应不同的物体检测任务。在MASK dataset上的实验结果显示，提案的模型与YOLOv5l模型相比，在mAP(0.5)和mAP(0.5:0.95)中实现了1.1%和1.3%的提升。我们的提案方法可以优化口罩穿戴的检测能力。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks"><a href="#Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks" class="headerlink" title="Generalizing Medical Image Representations via Quaternion Wavelet Networks"></a>Generalizing Medical Image Representations via Quaternion Wavelet Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10224">http://arxiv.org/abs/2310.10224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a></li>
<li>paper_authors: Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</li>
<li>for: 提高医疗图像处理领域的神经网络通用性，适应不同数据来源和任务的研究。</li>
<li>methods: 提出一种新的、通用、数据和任务无关的框架，可以从医疗图像中提取突出的特征。该框架基于四元波峰变换，可以与现有的医疗图像分析或生成任务集成，并且可以与实数、四元数或复数值模型混合使用。</li>
<li>results: 经过广泛的实验评估，包括不同的数据集和任务，如重建、分割和模态翻译等，结果显示提出的框架可以提高网络性能，同时具有广泛适用的通用性。<details>
<summary>Abstract</summary>
Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most representative set of sub-bands to be involved as input to any other neural model for image processing, replacing standard data samples. We conduct an extensive experimental evaluation comprising different datasets, diverse image analysis, and synthesis tasks including reconstruction, segmentation, and modality translation. We also evaluate QUAVE in combination with both real and quaternion-valued models. Results demonstrate the effectiveness and the generalizability of the proposed framework that improves network performance while being flexible to be adopted in manifold scenarios.
</details>
<details>
<summary>摘要</summary>
QUAVE首先使用四元波лет变换提取不同的子带，包括低频/抽象带和高频/细化特征。然后，它对最有代表性的子带进行权重，将其作为任务模型的输入，取代标准数据样本。我们进行了广泛的实验评估，包括不同的数据集和多种图像分析和生成任务，如重建、分割和模式翻译。我们还在QUAVE与实数和四元数值模型结合使用时进行了评估。结果表明我们提出的框架能够提高网络性能，同时具有通用的优势，能够在多种场景中适用。
</details></li>
</ul>
<hr>
<h2 id="RoboLLM-Robotic-Vision-Tasks-Grounded-on-Multimodal-Large-Language-Models"><a href="#RoboLLM-Robotic-Vision-Tasks-Grounded-on-Multimodal-Large-Language-Models" class="headerlink" title="RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models"></a>RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10221">http://arxiv.org/abs/2310.10221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longkukuhi/armbench">https://github.com/longkukuhi/armbench</a></li>
<li>paper_authors: Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</li>
<li>for: This paper is written for robotic vision applications, specifically to address the challenges of object detection, segmentation, and identification in real-world warehouse scenarios.</li>
<li>methods: The paper proposes the use of Multimodal Large Language Models (MLLMs) as a novel backbone for various downstream tasks, leveraging the pre-training capabilities of MLLMs to create a simplified framework and mitigate the need for task-specific encoders.</li>
<li>results: The paper introduces the RoboLLM framework, equipped with a BEiT-3 backbone, which outperforms existing baselines and substantially reduces the engineering burden associated with model selection and tuning, as demonstrated in the ARMBench challenge.<details>
<summary>Abstract</summary>
Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.
</details>
<details>
<summary>摘要</summary>
robotic 视觉应用经常需要各种视觉识别任务，如物体检测、分割和识别。 DESPITE 这些任务的 SUBSTANTIAL ADVANCES，整合特殊模型到一个统一的视觉管道中存在 significan engineering challenges 和 Costs。 Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. WE ARGUE that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Fetal-MRI-3D-Reconstruction-Based-on-Radiation-Diffusion-Generation-Model"><a href="#Self-supervised-Fetal-MRI-3D-Reconstruction-Based-on-Radiation-Diffusion-Generation-Model" class="headerlink" title="Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion Generation Model"></a>Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion Generation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10209">http://arxiv.org/abs/2310.10209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junpeng Tan, Xin Zhang, Yao Lv, Xiangmin Xu, Gang Li</li>
<li>for: 这种论文是为了解决妊娠Magnetic Resonance Imaging(MRI)中的精度恢复问题而写的。</li>
<li>methods: 这种方法使用了基于卷积的射线辐射场(NeRF)和基于超分解的扩展生成器(CINR)等技术来解决区域性灵敏度不均匀和全局一致性问题。</li>
<li>results: 实验结果表明，这种方法可以在实际世界妊娠MRI核心中实现高质量超分解重建。<details>
<summary>Abstract</summary>
Although the use of multiple stacks can handle slice-to-volume motion correction and artifact removal problems, there are still several problems: 1) The slice-to-volume method usually uses slices as input, which cannot solve the problem of uniform intensity distribution and complementarity in regions of different fetal MRI stacks; 2) The integrity of 3D space is not considered, which adversely affects the discrimination and generation of globally consistent information in fetal MRI; 3) Fetal MRI with severe motion artifacts in the real-world cannot achieve high-quality super-resolution reconstruction. To address these issues, we propose a novel fetal brain MRI high-quality volume reconstruction method, called the Radiation Diffusion Generation Model (RDGM). It is a self-supervised generation method, which incorporates the idea of Neural Radiation Field (NeRF) based on the coordinate generation and diffusion model based on super-resolution generation. To solve regional intensity heterogeneity in different directions, we use a pre-trained transformer model for slice registration, and then, a new regionally Consistent Implicit Neural Representation (CINR) network sub-module is proposed. CINR can generate the initial volume by combining a coordinate association map of two different coordinate mapping spaces. To enhance volume global consistency and discrimination, we introduce the Volume Diffusion Super-resolution Generation (VDSG) mechanism. The global intensity discriminant generation from volume-to-volume is carried out using the idea of diffusion generation, and CINR becomes the deviation intensity generation network of the volume-to-volume diffusion model. Finally, the experimental results on real-world fetal brain MRI stacks demonstrate the state-of-the-art performance of our method.
</details>
<details>
<summary>摘要</summary>
although multiple stacks can handle slice-to-volume motion correction and artifact removal problems, there are still several problems: 1) the slice-to-volume method usually uses slices as input, which cannot solve the problem of uniform intensity distribution and complementarity in regions of different fetal MRI stacks; 2) the integrity of 3D space is not considered, which adversely affects the discrimination and generation of globally consistent information in fetal MRI; 3) fetal MRI with severe motion artifacts in the real world cannot achieve high-quality super-resolution reconstruction. to address these issues, we propose a novel fetal brain MRI high-quality volume reconstruction method, called the radiation diffusion generation model (RDGM). it is a self-supervised generation method, which incorporates the idea of neural radiation field (NeRF) based on the coordinate generation and diffusion model based on super-resolution generation. to solve regional intensity heterogeneity in different directions, we use a pre-trained transformer model for slice registration, and then, a new regionally consistent implicit neural representation (CINR) network sub-module is proposed. CINR can generate the initial volume by combining a coordinate association map of two different coordinate mapping spaces. to enhance volume global consistency and discrimination, we introduce the volume diffusion super-resolution generation (VDSG) mechanism. the global intensity discriminant generation from volume-to-volume is carried out using the idea of diffusion generation, and CINR becomes the deviation intensity generation network of the volume-to-volume diffusion model. finally, the experimental results on real-world fetal brain MRI stacks demonstrate the state-of-the-art performance of our method.
</details></li>
</ul>
<hr>
<h2 id="MoConVQ-Unified-Physics-Based-Motion-Control-via-Scalable-Discrete-Representations"><a href="#MoConVQ-Unified-Physics-Based-Motion-Control-via-Scalable-Discrete-Representations" class="headerlink" title="MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations"></a>MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10198">http://arxiv.org/abs/2310.10198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, Libin Liu</li>
<li>for: 本文提出了一种新的物理学基的动作控制框架，即MoConVQ，可以有效地从大量、无结构的动作示例中学习动作嵌入。</li>
<li>methods: 该方法基于量化变换自动编码器（VQ-VAE）和模型基于奖励学习，可以从大量动作示例中学习动作嵌入，并且可以Capture多样化的动作技巧。</li>
<li>results: 研究人员通过多种应用场景来证明MoConVQ的可行性，包括通用跟踪控制、交互式人物控制、物理学基的动作生成等。此外，研究人员还证明了MoConVQ可以与大型语言模型（LLMs）集成，以解决复杂和抽象的任务。<details>
<summary>Abstract</summary>
In this work, we present MoConVQ, a novel unified framework for physics-based motion control leveraging scalable discrete representations. Building upon vector quantized variational autoencoders (VQ-VAE) and model-based reinforcement learning, our approach effectively learns motion embeddings from a large, unstructured dataset spanning tens of hours of motion examples. The resultant motion representation not only captures diverse motion skills but also offers a robust and intuitive interface for various applications. We demonstrate the versatility of MoConVQ through several applications: universal tracking control from various motion sources, interactive character control with latent motion representations using supervised learning, physics-based motion generation from natural language descriptions using the GPT framework, and, most interestingly, seamless integration with large language models (LLMs) with in-context learning to tackle complex and abstract tasks.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了MoConVQ，一种新的物理基于运动控制框架，利用可扩展的字符串表示法。我们的方法基于vector量化自适应学习（VQ-VAE）和基于模型的奖励学习，从大量、无结构的运动示例中学习出高质量的运动嵌入。这种运动表示不仅捕捉了多样化的运动技巧，还提供了一种稳定和直观的界面，可以应用于多种应用程序。我们在这篇论文中展示了MoConVQ的多种应用，包括从不同运动源的跟踪控制、使用监督学习的潜在运动表示进行交互人物控制、基于自然语言描述的物理运动生成、以及与大语言模型（LLM）集成，以解决复杂和抽象任务。
</details></li>
</ul>
<hr>
<h2 id="The-Road-to-On-board-Change-Detection-A-Lightweight-Patch-Level-Change-Detection-Network-via-Exploring-the-Potential-of-Pruning-and-Pooling"><a href="#The-Road-to-On-board-Change-Detection-A-Lightweight-Patch-Level-Change-Detection-Network-via-Exploring-the-Potential-of-Pruning-and-Pooling" class="headerlink" title="The Road to On-board Change Detection: A Lightweight Patch-Level Change Detection Network via Exploring the Potential of Pruning and Pooling"></a>The Road to On-board Change Detection: A Lightweight Patch-Level Change Detection Network via Exploring the Potential of Pruning and Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10166">http://arxiv.org/abs/2310.10166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihui Xue, Zhihao Wang, Xueqian Wang, Gang Li</li>
<li>for: 这个论文主要是为了提高大规模卫星遥感帧测变检测（CD）方法的效率和可靠性，尤其是在具有限制性的计算和内存资源的edge Computing平台上。</li>
<li>methods: 本论文提出了一个轻量级的帧级CD网络（LPCDNet），以快速除去大量无变帧，以提高后续像素级CD过程的效率和内存成本。LPCDNet使用了一个感度指导的通道剔除方法，删除无重要通道，并建立轻量级的后门网络基于ResNet18网络。此外，本文还提出了一个多层特征压缩（MLFC）模组，用于压缩和融合两个时间点的帧级特征信息。</li>
<li>results: 根据实验结果，LPCDNet在两个CD资料集上可以每秒逐帧检测1000帧以上，比已有方法高得多，而且不会对CD性能造成明显的损失。此外，LPCDNet还可以降低后续像Pixel-level CD过程的内存成本超过60%。<details>
<summary>Abstract</summary>
Existing satellite remote sensing change detection (CD) methods often crop original large-scale bi-temporal image pairs into small patch pairs and then use pixel-level CD methods to fairly process all the patch pairs. However, due to the sparsity of change in large-scale satellite remote sensing images, existing pixel-level CD methods suffer from a waste of computational cost and memory resources on lots of unchanged areas, which reduces the processing efficiency of on-board platform with extremely limited computation and memory resources. To address this issue, we propose a lightweight patch-level CD network (LPCDNet) to rapidly remove lots of unchanged patch pairs in large-scale bi-temporal image pairs. This is helpful to accelerate the subsequent pixel-level CD processing stage and reduce its memory costs. In our LPCDNet, a sensitivity-guided channel pruning method is proposed to remove unimportant channels and construct the lightweight backbone network on basis of ResNet18 network. Then, the multi-layer feature compression (MLFC) module is designed to compress and fuse the multi-level feature information of bi-temporal image patch. The output of MLFC module is fed into the fully-connected decision network to generate the predicted binary label. Finally, a weighted cross-entropy loss is utilized in the training process of network to tackle the change/unchange class imbalance problem. Experiments on two CD datasets demonstrate that our LPCDNet achieves more than 1000 frames per second on an edge computation platform, i.e., NVIDIA Jetson AGX Orin, which is more than 3 times that of the existing methods without noticeable CD performance loss. In addition, our method reduces more than 60% memory costs of the subsequent pixel-level CD processing stage.
</details>
<details>
<summary>摘要</summary>
现有的卫星遥感变化检测（CD）方法 oftentimes crop original large-scale bi-temporal image pairs into small patch pairs and then use pixel-level CD methods to fairly process all the patch pairs. However, due to the sparsity of change in large-scale satellite remote sensing images, existing pixel-level CD methods suffer from a waste of computational cost and memory resources on lots of unchanged areas, which reduces the processing efficiency of on-board platform with extremely limited computation and memory resources. To address this issue, we propose a lightweight patch-level CD network (LPCDNet) to rapidly remove lots of unchanged patch pairs in large-scale bi-temporal image pairs. This is helpful to accelerate the subsequent pixel-level CD processing stage and reduce its memory costs. In our LPCDNet, a sensitivity-guided channel pruning method is proposed to remove unimportant channels and construct the lightweight backbone network on basis of ResNet18 network. Then, the multi-layer feature compression (MLFC) module is designed to compress and fuse the multi-level feature information of bi-temporal image patch. The output of MLFC module is fed into the fully-connected decision network to generate the predicted binary label. Finally, a weighted cross-entropy loss is utilized in the training process of network to tackle the change/unchange class imbalance problem. Experiments on two CD datasets demonstrate that our LPCDNet achieves more than 1000 frames per second on an edge computation platform, i.e., NVIDIA Jetson AGX Orin, which is more than 3 times that of the existing methods without noticeable CD performance loss. In addition, our method reduces more than 60% memory costs of the subsequent pixel-level CD processing stage.
</details></li>
</ul>
<hr>
<h2 id="A-Search-for-Prompts-Generating-Structured-Answers-from-Contracts"><a href="#A-Search-for-Prompts-Generating-Structured-Answers-from-Contracts" class="headerlink" title="A Search for Prompts: Generating Structured Answers from Contracts"></a>A Search for Prompts: Generating Structured Answers from Contracts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10141">http://arxiv.org/abs/2310.10141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Roegiest, Radha Chitta, Jonathan Donnelly, Maya Lash, Alexandra Vtyurina, François Longtin</li>
<li>for: 法律问题自动回答，帮助自动化人类审查或标识特定条件（例如，自动续订警示）。</li>
<li>methods: 使用 OpenAI 的 \textit{GPT-3.5-Turbo} 进行不结构化生成问题回答，并对问题回答提供了审查和改进。</li>
<li>results: 相比 semantic matching 方法，我们的模板提问方法更加准确，并且通过Context learning和提问修改，我们的方法可以进一步提高性能。<details>
<summary>Abstract</summary>
In many legal processes being able to action on the concrete implication of a legal question can be valuable to automating human review or signalling certain conditions (e.g., alerts around automatic renewal). To support such tasks, we present a form of legal question answering that seeks to return one (or more) fixed answers for a question about a contract clause. After showing that unstructured generative question answering can have questionable outcomes for such a task, we discuss our exploration methodology for legal question answering prompts using OpenAI's \textit{GPT-3.5-Turbo} and provide a summary of insights.   Using insights gleaned from our qualitative experiences, we compare our proposed template prompts against a common semantic matching approach and find that our prompt templates are far more accurate despite being less reliable in the exact response return. With some additional tweaks to prompts and the use of in-context learning, we are able to further improve the performance of our proposed strategy while maximizing the reliability of responses as best we can.
</details>
<details>
<summary>摘要</summary>
在许多法律程序中，能够对法律问题的具体实施有益于自动化人类审查或标识特定条件（例如，续约提醒）。为支持这些任务，我们提出了一种法律问题回答方法，该方法可以为一个合同条款的问题返回一个或多个固定答案。在显示了无结构生成问题回答可能导致问able的结果后，我们讲述了我们的探索方法ологи，使用OpenAI的GPT-3.5-Turbo进行问题回答提示。我们提供了一个摘要的感想，并与常见Semantic Matching方法进行比较。我们发现，我们的提案的模板提示比Semantic Matching方法更准确，尽管它们可能不那么可靠地返回具体的回答。通过对提示和使用上下文学习进行一些调整，我们能够进一步改进我们的提案的性能，同时最大化回答的可靠性。
</details></li>
</ul>
<hr>
<h2 id="PELA-Learning-Parameter-Efficient-Models-with-Low-Rank-Approximation"><a href="#PELA-Learning-Parameter-Efficient-Models-with-Low-Rank-Approximation" class="headerlink" title="PELA: Learning Parameter-Efficient Models with Low-Rank Approximation"></a>PELA: Learning Parameter-Efficient Models with Low-Rank Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10700">http://arxiv.org/abs/2310.10700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guoyang9/pela">https://github.com/guoyang9/pela</a></li>
<li>paper_authors: Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli</li>
<li>for: 提高预训练模型中参数效率，以适应资源受限的下游任务。</li>
<li>methods: Introducing an intermediate pre-training stage, using low-rank approximation to compress the original large model, and devising a feature distillation module and weight perturbation regularization module to enhance the low-rank model.</li>
<li>results: 提高预训练模型的参数效率，同时保持与基本架构相似的性能水平，减少参数大小一半至二分之一。<details>
<summary>Abstract</summary>
Applying a pre-trained large model to downstream tasks is prohibitive under resource-constrained conditions. Recent dominant approaches for addressing efficiency issues involve adding a few learnable parameters to the fixed backbone model. This strategy, however, leads to more challenges in loading large models for downstream fine-tuning with limited resources. In this paper, we propose a novel method for increasing the parameter efficiency of pre-trained models by introducing an intermediate pre-training stage. To this end, we first employ low-rank approximation to compress the original large model and then devise a feature distillation module and a weight perturbation regularization module. These modules are specifically designed to enhance the low-rank model. Concretely, we update only the low-rank model while freezing the backbone parameters during pre-training. This allows for direct and efficient utilization of the low-rank model for downstream tasks. The proposed method achieves both efficiencies in terms of required parameters and computation time while maintaining comparable results with minimal modifications to the base architecture. Specifically, when applied to three vision-only and one vision-language Transformer models, our approach often demonstrates a $\sim$0.6 point decrease in performance while reducing the original parameter size by 1/3 to 2/3.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="3DYoga90-A-Hierarchical-Video-Dataset-for-Yoga-Pose-Understanding"><a href="#3DYoga90-A-Hierarchical-Video-Dataset-for-Yoga-Pose-Understanding" class="headerlink" title="3DYoga90: A Hierarchical Video Dataset for Yoga Pose Understanding"></a>3DYoga90: A Hierarchical Video Dataset for Yoga Pose Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10131">http://arxiv.org/abs/2310.10131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seonokkim/3dyoga90">https://github.com/seonokkim/3dyoga90</a></li>
<li>paper_authors: Seonok Kim</li>
<li>For: 这个研究是为了开发一个更大、更完整的人工智能训练用运动视频集，具体来说是3D Yoga901数据集，用于习习poses和瑜伽动作识别。* Methods: 这个研究使用了一个专门为这个目标而制作的数据集，其包括90个姿势的RGB视频和3D骨骼序列，同时还有一个三级标签层次结构。* Results: 这个研究创造了一个更大、更完整的公共数据集，包括RGB视频和3D骨骼序列，这对于人工智能训练和瑜伽动作识别具有广泛的应用前景。<details>
<summary>Abstract</summary>
The increasing popularity of exercises including yoga and Pilates has created a greater demand for professional exercise video datasets in the realm of artificial intelligence. In this study, we developed 3DYoga901, which is organized within a three-level label hierarchy. We have expanded the number of poses from an existing state-of-the-art dataset, increasing it from 82 to 90 poses. Our dataset includes meticulously curated RGB yoga pose videos and 3D skeleton sequences. This dataset was created by a dedicated team of six individuals, including yoga instructors. It stands out as one of the most comprehensive open datasets, featuring the largest collection of RGB videos and 3D skeleton sequences among publicly available resources. This contribution has the potential to significantly advance the field of yoga action recognition and pose assessment. Additionally, we conducted experiments to evaluate the practicality of our proposed dataset. We employed three different model variants for benchmarking purposes.
</details>
<details>
<summary>摘要</summary>
随着瑜伽和PILATES等运动的流行，人工智能领域的训练数据需求增加。在这项研究中，我们开发了3DYoga901，这是一个三级标签层次结构下的组织方式。我们将现有状态艺术数据集中的82姿势提高到90姿势。我们的数据集包括仔细挑选的RGB瑜伽姿势视频和3D骨架序列。这个数据集由6名专业人员，包括瑜伽教练，共同创建。它是公共可用资源中最完整的开放数据集，拥有最大的RGB视频和3D骨架序列收集。这一贡献有可能在瑜伽动作识别和姿势评估领域取得重要进步。此外，我们还进行了实验来评估我们的提案的实用性。我们使用了三种不同的模型变体进行比较。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Action-Recognition-with-Captioning-Foundation-Models"><a href="#Few-shot-Action-Recognition-with-Captioning-Foundation-Models" class="headerlink" title="Few-shot Action Recognition with Captioning Foundation Models"></a>Few-shot Action Recognition with Captioning Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10125">http://arxiv.org/abs/2310.10125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Wang, Shiwei Zhang, Hangjie Yuan, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang</li>
<li>for: 本研究旨在将预训练的视觉语言知识转移到多个下游任务中，以提高实验效率和准确性。</li>
<li>methods: 本研究使用了一个名为CapFSAR的弹性插件架构，具有自动生成对应的视觉描述和文本嵌入的能力，以扩展预训练的视觉语言知识。</li>
<li>results: 实验结果显示，CapFSAR在多个标准几个阶段训练 benchmark 上表现出色，与现有方法比较，具有更高的准确性和更好的一致性。<details>
<summary>Abstract</summary>
Transferring vision-language knowledge from pretrained multimodal foundation models to various downstream tasks is a promising direction. However, most current few-shot action recognition methods are still limited to a single visual modality input due to the high cost of annotating additional textual descriptions. In this paper, we develop an effective plug-and-play framework called CapFSAR to exploit the knowledge of multimodal models without manually annotating text. To be specific, we first utilize a captioning foundation model (i.e., BLIP) to extract visual features and automatically generate associated captions for input videos. Then, we apply a text encoder to the synthetic captions to obtain representative text embeddings. Finally, a visual-text aggregation module based on Transformer is further designed to incorporate cross-modal spatio-temporal complementary information for reliable few-shot matching. In this way, CapFSAR can benefit from powerful multimodal knowledge of pretrained foundation models, yielding more comprehensive classification in the low-shot regime. Extensive experiments on multiple standard few-shot benchmarks demonstrate that the proposed CapFSAR performs favorably against existing methods and achieves state-of-the-art performance. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
<<TRANSLATED TEXT>>使用预训练多modal基础模型的视觉语言知识传递到多个下游任务是一个有前途的方向。然而，当前大多数几 shot动作识别方法仍然只接受单一视觉输入，因为对附加文本描述的标注成本高昂。在这篇论文中，我们开发了一个有效的插件式框架called CapFSAR，以利用预训练多modal模型的知识而不需要手动标注文本。具体来说，我们首先利用一个captioning基础模型（即BLIP）来提取视觉特征并自动生成相关的描述文本 для输入视频。然后，我们应用一个文本编码器来对生成的文本嵌入获得代表性的文本嵌入。最后，我们设计了一个基于Transformer的视TEXT聚合模块，以便在低shot情况下利用多modal视觉语言的补充信息进行可靠的匹配。这样，CapFSAR可以从预训练多modal模型中获得强大的知识，在低shot情况下实现更全面的分类。我们的实验结果表明，提议的CapFSAR在多个标准几 shotbenchmark上表现出色，并达到了状态 искусственный智能的性能。代码将公开。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="AutoDIR-Automatic-All-in-One-Image-Restoration-with-Latent-Diffusion"><a href="#AutoDIR-Automatic-All-in-One-Image-Restoration-with-Latent-Diffusion" class="headerlink" title="AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion"></a>AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10123">http://arxiv.org/abs/2310.10123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yitong Jiang, Zhaoyang Zhang, Tianfan Xue, Jinwei Gu</li>
<li>for:  solves complex real-world image restoration situations with multiple unknown degradations</li>
<li>methods:  uses an all-in-one image restoration framework with latent diffusion, including a Blind Image Quality Assessment Module (BIQA) and an All-in-One Image Refinement (AIR) Module, as well as a Structure Correction Module (SCM)</li>
<li>results:  outperforms state-of-the-art approaches with superior restoration results and supports a wider range of tasks, including real-scenario images with multiple unknown degradations.<details>
<summary>Abstract</summary>
In this paper, we aim to solve complex real-world image restoration situations, in which, one image may have a variety of unknown degradations. To this end, we propose an all-in-one image restoration framework with latent diffusion (AutoDIR), which can automatically detect and address multiple unknown degradations. Our framework first utilizes a Blind Image Quality Assessment Module (BIQA) to automatically detect and identify the unknown dominant image degradation type of the image. Then, an All-in-One Image Refinement (AIR) Module handles multiple kinds of degradation image restoration with the guidance of BIQA. Finally, a Structure Correction Module (SCM) is proposed to recover the image details distorted by AIR. Our comprehensive evaluation demonstrates that AutoDIR outperforms state-of-the-art approaches by achieving superior restoration results while supporting a wider range of tasks. Notably, AutoDIR is also the first method to automatically handle real-scenario images with multiple unknown degradations.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们目标是解决复杂的真实世界图像恢复问题，在这个问题中，一个图像可能具有多种未知的降低效应。为此，我们提议一个整合性图像恢复框架——自适应扩散图像修复（AutoDIR），可以自动检测和解决多种未知降低效应。我们的框架首先利用一个隐藏影像质量评估模块（BIQA）来自动检测和识别图像的未知主要降低类型。然后，一个全面修复（AIR）模块处理多种降低效应的图像修复，以BIQA的指导。最后，我们提出一个结构修复模块（SCM）来恢复图像细节，受到AIR的扭曲影响。我们的全面评估表明，AutoDIR在恢复Result中显示出优于当前方法，同时支持更广泛的任务范围。尤其是，AutoDIR是第一个自动处理真实世界图像中的多种未知降低的方法。
</details></li>
</ul>
<hr>
<h2 id="KAKURENBO-Adaptively-Hiding-Samples-in-Deep-Neural-Network-Training"><a href="#KAKURENBO-Adaptively-Hiding-Samples-in-Deep-Neural-Network-Training" class="headerlink" title="KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training"></a>KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10102">http://arxiv.org/abs/2310.10102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TruongThaoNguyen/kakurenbo">https://github.com/TruongThaoNguyen/kakurenbo</a></li>
<li>paper_authors: Truong Thao Nguyen, Balazs Gerofi, Edgar Josafat Martinez-Noriega, François Trahay, Mohamed Wahib</li>
<li>for: 提高深度神经网络训练效率，减少训练成本。</li>
<li>methods: 利用训练损失和预测信任度信息， dynamically 排除训练样本，不归一化影响精度。</li>
<li>results: 在多个大规模数据集和模型上，与基eline相比，我们的方法可以降低训练时间，仅带来0.4%的精度下降。可以在<a target="_blank" rel="noopener" href="https://github.com/TruongThaoNguyen/kakurenbo">https://github.com/TruongThaoNguyen/kakurenbo</a> 获取代码。<details>
<summary>Abstract</summary>
This paper proposes a method for hiding the least-important samples during the training of deep neural networks to increase efficiency, i.e., to reduce the cost of training. Using information about the loss and prediction confidence during training, we adaptively find samples to exclude in a given epoch based on their contribution to the overall learning process, without significantly degrading accuracy. We explore the converge properties when accounting for the reduction in the number of SGD updates. Empirical results on various large-scale datasets and models used directly in image classification and segmentation show that while the with-replacement importance sampling algorithm performs poorly on large datasets, our method can reduce total training time by up to 22% impacting accuracy only by 0.4% compared to the baseline. Code available at https://github.com/TruongThaoNguyen/kakurenbo
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Multi-Scale-Spatial-Transformer-U-Net-for-Simultaneously-Automatic-Reorientation-and-Segmentation-of-3D-Nuclear-Cardiac-Images"><a href="#A-Multi-Scale-Spatial-Transformer-U-Net-for-Simultaneously-Automatic-Reorientation-and-Segmentation-of-3D-Nuclear-Cardiac-Images" class="headerlink" title="A Multi-Scale Spatial Transformer U-Net for Simultaneously Automatic Reorientation and Segmentation of 3D Nuclear Cardiac Images"></a>A Multi-Scale Spatial Transformer U-Net for Simultaneously Automatic Reorientation and Segmentation of 3D Nuclear Cardiac Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10095">http://arxiv.org/abs/2310.10095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangfan Ni, Duo Zhang, Gege Ma, Lijun Lu, Zhongke Huang, Wentao Zhu</li>
<li>For: 这个研究旨在提高核心心脏成像中的左心室（LV）重orientation和分割的精度，以便进行多modal的量化分析。* Methods: 该研究提出了一种结合多尺度空间变换网络（MSSTN）和多尺度UNet（MSUNet）模块的端到端模型，用于同时重orientation和分割LV区域从核心心脏成像图像中。* Results: 实验结果表明，该提出的方法可以显著提高重orientation和分割性能。这种结合学习框架可以促进重orientation和分割任务之间的互补性，从而实现高性能和高效的图像处理 workflow。<details>
<summary>Abstract</summary>
Accurate reorientation and segmentation of the left ventricular (LV) is essential for the quantitative analysis of myocardial perfusion imaging (MPI), in which one critical step is to reorient the reconstructed transaxial nuclear cardiac images into standard short-axis slices for subsequent image processing. Small-scale LV myocardium (LV-MY) region detection and the diverse cardiac structures of individual patients pose challenges to LV segmentation operation. To mitigate these issues, we propose an end-to-end model, named as multi-scale spatial transformer UNet (MS-ST-UNet), that involves the multi-scale spatial transformer network (MSSTN) and multi-scale UNet (MSUNet) modules to perform simultaneous reorientation and segmentation of LV region from nuclear cardiac images. The proposed method is trained and tested using two different nuclear cardiac image modalities: 13N-ammonia PET and 99mTc-sestamibi SPECT. We use a multi-scale strategy to generate and extract image features with different scales. Our experimental results demonstrate that the proposed method significantly improves the reorientation and segmentation performance. This joint learning framework promotes mutual enhancement between reorientation and segmentation tasks, leading to cutting edge performance and an efficient image processing workflow. The proposed end-to-end deep network has the potential to reduce the burden of manual delineation for cardiac images, thereby providing multimodal quantitative analysis assistance for physicists.
</details>
<details>
<summary>摘要</summary>
要实现多Modal量子分析的协助，我们提出了一种结合多级空间变换网络（MSSTN）和多级UNet（MSUNet）模块的端到端模型，用于自动将核心心脏图像重定向和分割为标准短轴扁平图像。左心室（LV）区域检测和各个患者的各种心脏结构带来了重orientation和分割任务的挑战。我们的方法通过一种多级strategy来生成和提取图像特征，从而提高重定向和分割性能。我们对13N-氨基酸PET和99mTc-氯丙胺SPECT两种核心心脏图像模式进行训练和测试，并取得了显著提高重定向和分割性能的结果。这种结合学习框架可以减少心脏图像手动分割的劳动，从而为物理学家提供多模态量子分析的协助。
</details></li>
</ul>
<hr>
<h2 id="PUCA-Patch-Unshuffle-and-Channel-Attention-for-Enhanced-Self-Supervised-Image-Denoising"><a href="#PUCA-Patch-Unshuffle-and-Channel-Attention-for-Enhanced-Self-Supervised-Image-Denoising" class="headerlink" title="PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising"></a>PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10088">http://arxiv.org/abs/2310.10088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HyemiEsme/PUCA">https://github.com/HyemiEsme/PUCA</a></li>
<li>paper_authors: Hyemi Jang, Junsung Park, Dahuin Jung, Jaihyun Lew, Ho Bae, Sungroh Yoon</li>
<li>for: 自动化图像干扰除（image denoising）</li>
<li>methods: 使用自适应卷积（dilated attention blocks）和质patch-unshuffle&#x2F;shuffle来扩大感知场和保持J-不变性</li>
<li>results: 实验结果显示，PUCA在自主学习图像干扰除中实现了状态的最佳性能，超过了现有方法的性能Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 自动化图像干扰除（image denoising）</li>
<li>methods: 使用自适应卷积（dilated attention blocks）和质patch-unshuffle&#x2F;shuffle来扩大感知场和保持J-不变性</li>
<li>results: 实验结果显示，PUCA在自主学习图像干扰除中实现了状态的最佳性能，超过了现有方法的性能<details>
<summary>Abstract</summary>
Although supervised image denoising networks have shown remarkable performance on synthesized noisy images, they often fail in practice due to the difference between real and synthesized noise. Since clean-noisy image pairs from the real world are extremely costly to gather, self-supervised learning, which utilizes noisy input itself as a target, has been studied. To prevent a self-supervised denoising model from learning identical mapping, each output pixel should not be influenced by its corresponding input pixel; This requirement is known as J-invariance. Blind-spot networks (BSNs) have been a prevalent choice to ensure J-invariance in self-supervised image denoising. However, constructing variations of BSNs by injecting additional operations such as downsampling can expose blinded information, thereby violating J-invariance. Consequently, convolutions designed specifically for BSNs have been allowed only, limiting architectural flexibility. To overcome this limitation, we propose PUCA, a novel J-invariant U-Net architecture, for self-supervised denoising. PUCA leverages patch-unshuffle/shuffle to dramatically expand receptive fields while maintaining J-invariance and dilated attention blocks (DABs) for global context incorporation. Experimental results demonstrate that PUCA achieves state-of-the-art performance, outperforming existing methods in self-supervised image denoising.
</details>
<details>
<summary>摘要</summary>
尽管监督的图像噪声网络已经在合成噪声图像上展现出惊人的表现，但在实际应用中它们经常失败，因为实际噪声和合成噪声之间存在差异。由于干净图像和噪声图像的对应对不容易获得，自动学习，即使用噪声输入本身作为目标，已经被研究。为保证自动学习噪声除掉模型不学习同一个映射，每个输出像素不能受到其对应的输入像素的影响，这种要求被称为J-不变性。盲区网络（BSN）在保证J-不变性方面广泛应用。然而，通过在BSN中注射额外操作，如下采样，可能会暴露盲区信息，从而违反J-不变性。因此，特制的BSN材料只能被允许，限制了建筑的创新性。为了突破这一限制，我们提出了PUCA，一种新的J-不变的U-Net架构，用于自动学习噪声除掉。PUCA利用质心不排序/排序来巨大地扩大接收场，同时保持J-不变性，并采用扩展的注意块（DABs）来 incorporate global context。实验结果表明，PUCA可以达到状态之内的表现，比存在的方法在自动学习噪声除掉中高效。
</details></li>
</ul>
<hr>
<h2 id="Expression-Domain-Translation-Network-for-Cross-domain-Head-Reenactment"><a href="#Expression-Domain-Translation-Network-for-Cross-domain-Head-Reenactment" class="headerlink" title="Expression Domain Translation Network for Cross-domain Head Reenactment"></a>Expression Domain Translation Network for Cross-domain Head Reenactment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10073">http://arxiv.org/abs/2310.10073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taewoong Kang, Jeongsik Oh, Jaeseong Lee, Sunghyun Park, Jaegul Choo<br>for:* The paper aims to improve cross-domain head reenactment, specifically transferring human motions to cartoon characters.methods:* The paper introduces a novel expression domain translation network to transform human expressions into anime expressions.* The network uses a 3D geometric-aware loss function to ensure geometric consistency between the input and output expressions.results:* The proposed method outperforms existing methods in both qualitative and quantitative analysis, demonstrating a significant advancement in cross-domain head reenactment.<details>
<summary>Abstract</summary>
Despite the remarkable advancements in head reenactment, the existing methods face challenges in cross-domain head reenactment, which aims to transfer human motions to domains outside the human, including cartoon characters. It is still difficult to extract motion from out-of-domain images due to the distinct appearances, such as large eyes. Recently, previous work introduced a large-scale anime dataset called AnimeCeleb and a cross-domain head reenactment model, including an optimization-based mapping function to translate the human domain's expressions to the anime domain. However, we found that the mapping function, which relies on a subset of expressions, imposes limitations on the mapping of various expressions. To solve this challenge, we introduce a novel expression domain translation network that transforms human expressions into anime expressions. Specifically, to maintain the geometric consistency of expressions between the input and output of the expression domain translation network, we employ a 3D geometric-aware loss function that reduces the distances between the vertices in the 3D mesh of the human and anime. By doing so, it forces high-fidelity and one-to-one mapping with respect to two cross-expression domains. Our method outperforms existing methods in both qualitative and quantitative analysis, marking a significant advancement in the field of cross-domain head reenactment.
</details>
<details>
<summary>摘要</summary>
尽管有很多HEADreenactment的进步，现有的方法在跨domain HEADreenactment中遇到了困难，即将人类动作转移到不同的领域，如漫画人物。因为这些领域的外观有很大的不同，例如大的眼睛，提取动作从不同领域的图像仍然是很困难的。在最近，之前的工作已经介绍了一个大规模的漫画数据集called AnimeCeleb和一种跨领域HEADreenactment模型，包括一种优化基于映射函数，将人类领域的表情翻译到漫画领域。然而，我们发现这种映射函数，它基于一 subset of 表情，强制了表情的限制。为解决这个挑战，我们引入了一种新的表情频率翻译网络，可以将人类表情翻译成漫画表情。在我们的方法中，我们采用了一种3D геометрически感知的损失函数，以保持表情的几何一致性。这使得我们的方法可以具有高精度和一对一的映射性，对于两个跨表情频率的跨领域映射。我们的方法在质量和量度分析中都超过了现有的方法，标志着跨领域HEADreenactment领域的重要进步。
</details></li>
</ul>
<hr>
<h2 id="ZoomTrack-Target-aware-Non-uniform-Resizing-for-Efficient-Visual-Tracking"><a href="#ZoomTrack-Target-aware-Non-uniform-Resizing-for-Efficient-Visual-Tracking" class="headerlink" title="ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking"></a>ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10071">http://arxiv.org/abs/2310.10071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kou-99/zoomtrack">https://github.com/kou-99/zoomtrack</a></li>
<li>paper_authors: Yutong Kou, Jin Gao, Bing Li, Gang Wang, Weiming Hu, Yizheng Wang, Liang Li</li>
<li>for: 本研究旨在实现高速追踪，并获得高性能的tracking results，而不是专注于性能和速度之间的 compromise.</li>
<li>methods: 本文使用非对称resize的方法，将cropped image的input size变小，并保持目标的视觉资讯。这个方法可以通过quadratic programming (QP) efficiently solve，并可以与大多数的crop-based local tracker naturally integrate.</li>
<li>results: 在五个挑战性的dataset上，本文的方法可以获得了consistent improvement，并在speed-oriented版本的OSTrack上even outperform its performance-oriented counterpart by 0.6% AUC on TNL2K，并且在50% faster和save over 55% MACs的情况下实现。<details>
<summary>Abstract</summary>
Recently, the transformer has enabled the speed-oriented trackers to approach state-of-the-art (SOTA) performance with high-speed thanks to the smaller input size or the lighter feature extraction backbone, though they still substantially lag behind their corresponding performance-oriented versions. In this paper, we demonstrate that it is possible to narrow or even close this gap while achieving high tracking speed based on the smaller input size. To this end, we non-uniformly resize the cropped image to have a smaller input size while the resolution of the area where the target is more likely to appear is higher and vice versa. This enables us to solve the dilemma of attending to a larger visual field while retaining more raw information for the target despite a smaller input size. Our formulation for the non-uniform resizing can be efficiently solved through quadratic programming (QP) and naturally integrated into most of the crop-based local trackers. Comprehensive experiments on five challenging datasets based on two kinds of transformer trackers, \ie, OSTrack and TransT, demonstrate consistent improvements over them. In particular, applying our method to the speed-oriented version of OSTrack even outperforms its performance-oriented counterpart by 0.6% AUC on TNL2K, while running 50% faster and saving over 55% MACs. Codes and models are available at https://github.com/Kou-99/ZoomTrack.
</details>
<details>
<summary>摘要</summary>
最近，transformer已经使得速度强调跟踪器可以达到状态之Art（SOTA）性能，并且具有更高的速度，即使使用更小的输入大小或更轻量级的特征提取核心。然而，它们仍然较相对落后于其对应的性能强调版本。在这篇论文中，我们展示了可以减少或甚至消除这个差距，而且可以在更小的输入大小下实现高速跟踪。为此，我们非均匀地缩放cropped图像，使得target的可能出现的区域的分辨率更高，而其他区域的分辨率相对较低。这样可以解决在更小的输入大小下尚可以保留更多的原始信息来搜寻target的问题。我们的非均匀缩放的形式可以通过quadratic programming（QP）有效地解决，并且自然地整合到大多数的crop-based本地跟踪器中。我们在五个复杂的数据集上进行了广泛的实验，并示出了一致的改进。特别是，对于速度强调版本的OSTrack，我们的方法可以在TNL2K上提高0.6%的AUC，并且在50%的速度下运行，并且占用了55%的MACs。代码和模型可以在https://github.com/Kou-99/ZoomTrack上获取。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Person-Search-on-Open-world-User-Generated-Video-Content"><a href="#Generalizable-Person-Search-on-Open-world-User-Generated-Video-Content" class="headerlink" title="Generalizable Person Search on Open-world User-Generated Video Content"></a>Generalizable Person Search on Open-world User-Generated Video Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10068">http://arxiv.org/abs/2310.10068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Li, Guanshuo Wang, Yichao Yan, Fufu Yu, Qiong Jia, Jie Qin, Shouhong Ding, Xiaokang Yang</li>
<li>for: 实现人寻找任务中的扩展性能，尤其是在不同的摄像头和环境中。</li>
<li>methods: 提出了一个通用框架，包括两种水平的普遍化：对于特征水平，引入多任务抽象型普通批量对顶推对顶推标准差，对于数据水平，则是通过通道宽度ID相关特征装饰策略来实现普遍化。</li>
<li>results: 在两个挑战人寻找测试 bencmarks 上获得了可靠的表现，无需使用任何人工标注或目标领域的样本。<details>
<summary>Abstract</summary>
Person search is a challenging task that involves detecting and retrieving individuals from a large set of un-cropped scene images. Existing person search applications are mostly trained and deployed in the same-origin scenarios. However, collecting and annotating training samples for each scene is often difficult due to the limitation of resources and the labor cost. Moreover, large-scale intra-domain data for training are generally not legally available for common developers, due to the regulation of privacy and public security. Leveraging easily accessible large-scale User Generated Video Contents (\emph{i.e.} UGC videos) to train person search models can fit the open-world distribution, but still suffering a performance gap from the domain difference to surveillance scenes. In this work, we explore enhancing the out-of-domain generalization capabilities of person search models, and propose a generalizable framework on both feature-level and data-level generalization to facilitate downstream tasks in arbitrary scenarios. Specifically, we focus on learning domain-invariant representations for both detection and ReID by introducing a multi-task prototype-based domain-specific batch normalization, and a channel-wise ID-relevant feature decorrelation strategy. We also identify and address typical sources of noise in open-world training frames, including inaccurate bounding boxes, the omission of identity labels, and the absence of cross-camera data. Our framework achieves promising performance on two challenging person search benchmarks without using any human annotation or samples from the target domain.
</details>
<details>
<summary>摘要</summary>
人体搜索是一项复杂的任务，它涉及到从大量未修剪场景图像中检测和检索人体。现有的人体搜索应用程序都是在同一个来源场景中训练和部署的。然而，收集和标注训练样本的成本是非常高，尤其是在资源和劳动力方面。此外，大规模内域数据 для训练通常不可得，因为隐私和公共安全的法规限制。我们利用可达性高的用户生成内容（i.e., UGC视频）来训练人体搜索模型，以适应开放世界分布。然而，这些模型仍然受到频率域不同的问题带来的性能差。在这种情况下，我们提出了一种通用的框架，以便在无法预测的情况下进行下游任务。我们主要关注于学习域外 invariant 表示，包括检测和 ReID 领域的学习。我们引入多任务prototype-based域特定批处理，以及通道级 ID 相关特征修饰策略。我们还识别和解决常见的开放世界训练帧中的噪声源，包括不准确的 bounding box、缺失标签和 cross-camera 数据缺失。我们的框架在两个复杂的人体搜索标准准点上达到了无需使用人类标注或目标域样本的承诺性能。
</details></li>
</ul>
<hr>
<h2 id="A-computational-model-of-serial-and-parallel-processing-in-visual-search"><a href="#A-computational-model-of-serial-and-parallel-processing-in-visual-search" class="headerlink" title="A computational model of serial and parallel processing in visual search"></a>A computational model of serial and parallel processing in visual search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10061">http://arxiv.org/abs/2310.10061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rachelfheaton/CASPER-model">https://github.com/rachelfheaton/CASPER-model</a></li>
<li>paper_authors: Rachel F. Heaton</li>
<li>For: This paper aims to understand the nature of human visual representations and processes through the study of visual search.* Methods: The paper presents a theory of visual search based on empirical findings and instantiated in a computational model called CASPER (Concurrent Attention: Serial and Parallel Evaluation with Relations).* Results: The paper describes seven experiments that test CASPER’s predictions about relational search, and shows that CASPER can account for negative acceleration in search functions for relational stimuli.Here are the three points in Simplified Chinese:</li>
<li>for: 这篇论文旨在通过视觉搜索来理解人类视觉表示和过程的本质。</li>
<li>methods: 这篇论文提出了基于实验发现的视觉搜索理论，并将其实现在一个名为CASPER（并行注意力：序列和平行评估与关系）的计算模型中。</li>
<li>results: 这篇论文描述了七个实验，以测试CASPER模型对关系搜索的预测，并显示了CASPER可以解释视觉系统在关系刺激下的负加速度。<details>
<summary>Abstract</summary>
The following is a dissertation aimed at understanding what the various phenomena in visual search teach us about the nature of human visual representations and processes. I first review some of the major empirical findings in the study of visual search. I next present a theory of visual search in terms of what I believe these findings suggest about the representations and processes underlying ventral visual processing. These principles are instantiated in a computational model called CASPER (Concurrent Attention: Serial and Parallel Evaluation with Relations), originally developed by Hummel, that I have adapted to account for a range of phenomena in visual search. I then describe an extension of the CASPER model to account for our ability to search for visual items defined not simply by the features composing those items but by the spatial relations among those features. Seven experiments (four main experiments and three replications) are described that test CASPER's predictions about relational search. Finally, I evaluate the fit between CASPER's predictions and the empirical findings and show with three additional simulations that CASPER can account for negative acceleration in search functions for relational stimuli if one postulates that the visual system is leveraging an emergent feature that bypasses relational processing.
</details>
<details>
<summary>摘要</summary>
这是一篇关于视觉搜寻的论文，旨在了解人类视觉表示和过程中的本质。我首先介绍了视觉搜寻的一些主要实验发现，然后提出了基于这些发现的视觉搜寻理论。这些原则通过我修改了由哔哔（Hummel）开发的计算模型CASPER（同时注意力：串行和平行评估与关系）来实现。我然后描述了一种扩展CASPER模型，以便解释我们在视觉搜寻中搜寻视觉物体的空间关系。然后，我描述了七个实验（四个主要实验和三个复现），用于测试CASPER模型的预测。最后，我评估了CASPER模型的预测与实验发现的Compatibility，并通过三个额外的仿真显示CASPER模型可以解释视觉系统在搜寻关系性 stimulus 时的负加速。
</details></li>
</ul>
<hr>
<h2 id="EAR-Net-Pursuing-End-to-End-Absolute-Rotations-from-Multi-View-Images"><a href="#EAR-Net-Pursuing-End-to-End-Absolute-Rotations-from-Multi-View-Images" class="headerlink" title="EAR-Net: Pursuing End-to-End Absolute Rotations from Multi-View Images"></a>EAR-Net: Pursuing End-to-End Absolute Rotations from Multi-View Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10051">http://arxiv.org/abs/2310.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhen Liu, Qiulei Dong</li>
<li>for: 提供一种结构为深度神经网络的综合方法，用于从多视图图像中估计绝对旋转。</li>
<li>methods: 使用深度神经网络建立 epipolar confidence graph，并使用 confidence-aware rotation averaging 模块来预测绝对旋转。</li>
<li>results: 在三个公共数据集上，EAR-Net 比现有方法提高了准确性和速度。<details>
<summary>Abstract</summary>
Absolute rotation estimation is an important topic in 3D computer vision. Existing works in literature generally employ a multi-stage (at least two-stage) estimation strategy where multiple independent operations (feature matching, two-view rotation estimation, and rotation averaging) are implemented sequentially. However, such a multi-stage strategy inevitably leads to the accumulation of the errors caused by each involved operation, and degrades its final estimation on global rotations accordingly. To address this problem, we propose an End-to-end method for estimating Absolution Rotations from multi-view images based on deep neural Networks, called EAR-Net. The proposed EAR-Net consists of an epipolar confidence graph construction module and a confidence-aware rotation averaging module. The epipolar confidence graph construction module is explored to simultaneously predict pairwise relative rotations among the input images and their corresponding confidences, resulting in a weighted graph (called epipolar confidence graph). Based on this graph, the confidence-aware rotation averaging module, which is differentiable, is explored to predict the absolute rotations. Thanks to the introduced confidences of the relative rotations, the proposed EAR-Net could effectively handle outlier cases. Experimental results on three public datasets demonstrate that EAR-Net outperforms the state-of-the-art methods by a large margin in terms of accuracy and speed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>三维计算机视觉中的绝对旋转估算是一个重要的话题。现有文献中的方法通常采用多个独立的操作（特征匹配、两视旋转估算和旋转平均）的多阶段（至少两阶段）Strategy，这会导致每个参与的操作的错误积累，从而影响最终的全球旋转估算。为解决这个问题，我们提出了基于深度神经网络的绝对旋转估算方法，called EAR-Net。提案的 EAR-Net 包括 Epipolar 信任图构建模块和信任度权重平均模块。Epipolar 信任图构建模块可以同时预测输入图像之间的对应关系和它们的相对旋转信任度，从而构建一个权重图（称为 Epipolar 信任图）。基于这个图，信任度权重平均模块可以预测绝对旋转。由于引入的相对旋转信任度，提案的 EAR-Net 可以有效地处理异常情况。实验结果表明，EAR-Net 在三个公共数据集上的准确率和速度都高于当前状态的方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Hyperspectral-Image-Fusion-via-Logarithmic-Low-rank-Tensor-Ring-Decomposition"><a href="#Hyperspectral-Image-Fusion-via-Logarithmic-Low-rank-Tensor-Ring-Decomposition" class="headerlink" title="Hyperspectral Image Fusion via Logarithmic Low-rank Tensor Ring Decomposition"></a>Hyperspectral Image Fusion via Logarithmic Low-rank Tensor Ring Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10044">http://arxiv.org/abs/2310.10044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Zhang, Lipeng Zhu, Chao Wang, Shutao Li</li>
<li>for: 本研究旨在提高低分辨率多spectral图像（LR-HSI）与高分辨率多spectral图像（HR-MSI）的混合方法，以获得高分辨率多spectral图像（HR-HSI）。</li>
<li>methods: 本研究使用了tensor环（TR）分解方法，并利用了tensor核内积 regularization（TNN）来保持高维低级结构。</li>
<li>results: 实验结果表明，提出的方法可以提高视觉质量，并超过现有的state-of-the-art混合方法 regarding various quantitative metrics。<details>
<summary>Abstract</summary>
Integrating a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) is recognized as a valid method for acquiring HR-HSI. Among the current fusion approaches, the tensor ring (TR) decomposition-based method has received growing attention owing to its superior performance on preserving the spatial-spectral correlation. Furthermore, the low-rank property in some TR factors has been exploited via the matrix nuclear norm regularization along mode-2. On the other hand, the tensor nuclear norm (TNN)-based approaches have recently demonstrated to be more efficient on keeping high-dimensional low-rank structures in tensor recovery. Here, we study the low-rankness of TR factors from the TNN perspective and consider the mode-2 logarithmic TNN (LTNN) on each TR factor. A novel fusion model is proposed by incorporating this LTNN regularization and the weighted total variation which is to promote the continuity of HR-HSI in the spatial-spectral domain. Meanwhile, we have devised a highly efficient proximal alternating minimization algorithm to solve the proposed model. The experimental results indicate that our method improves the visual quality and exceeds the existing state-of-the-art fusion approaches with respect to various quantitative metrics.
</details>
<details>
<summary>摘要</summary>
Integrating a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) is recognized as a valid method for acquiring HR-HSI. Among the current fusion approaches, the tensor ring (TR) decomposition-based method has received growing attention owing to its superior performance on preserving the spatial-spectral correlation. Furthermore, the low-rank property in some TR factors has been exploited via the matrix nuclear norm regularization along mode-2. On the other hand, the tensor nuclear norm (TNN)-based approaches have recently demonstrated to be more efficient on keeping high-dimensional low-rank structures in tensor recovery. Here, we study the low-rankness of TR factors from the TNN perspective and consider the mode-2 logarithmic TNN (LTNN) on each TR factor. A novel fusion model is proposed by incorporating this LTNN regularization and the weighted total variation which is to promote the continuity of HR-HSI in the spatial-spectral domain. Meanwhile, we have devised a highly efficient proximal alternating minimization algorithm to solve the proposed model. The experimental results indicate that our method improves the visual quality and exceeds the existing state-of-the-art fusion approaches with respect to various quantitative metrics.Translation in Simplified Chinese:合并低分辨率干卷成像（LR-HSI）和高分辨率多spectral成像（HR-MSI）可以获得高分辨率干卷成像（HR-HSI）。当前的融合方法中，基于tensor ring（TR）分解的方法受到了越来越多的关注，因为它能够保留干卷成像的空间-spectral相关性。此外，TR因子中的低级属性也被利用了，通过matrix nuclear norm regularization along mode-2。而tensor nuclear norm（TNN）基于的方法则在tensor recovery中保持高维度低级结构方面表现更高效。在这里，我们从TNN的角度研究TR因子的低级性，并考虑mode-2 logarithmic TNN（LTNN）在每个TR因子上。我们提出了一种新的融合模型，通过加入LTNN regularization和权重Total variation来提高HR-HSI在空间-spectral频域中的连续性。此外，我们还开发了一种高效的 proximal alternating minimization算法来解决提案的模型。实验结果表明，我们的方法可以提高视觉质量，并在各种量化指标上超越现有的融合方法。
</details></li>
</ul>
<hr>
<h2 id="Evading-Detection-Actively-Toward-Anti-Forensics-against-Forgery-Localization"><a href="#Evading-Detection-Actively-Toward-Anti-Forensics-against-Forgery-Localization" class="headerlink" title="Evading Detection Actively: Toward Anti-Forensics against Forgery Localization"></a>Evading Detection Actively: Toward Anti-Forensics against Forgery Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10036">http://arxiv.org/abs/2310.10036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Zhuo, Shenghai Luo, Shunquan Tan, Han Chen, Bin Li, Jiwu Huang</li>
<li>for: 防止黑客修改图像，使图像检测器判定图像是否修改。</li>
<li>methods: 使用自我指导和对抗学习算法，训练深度学习反诈模型，以逃脱现有的修改检测器。</li>
<li>results: 成功逃脱现有的修改检测器，并在多个 dataset 上实现了高效的修改检测。<details>
<summary>Abstract</summary>
Anti-forensics seeks to eliminate or conceal traces of tampering artifacts. Typically, anti-forensic methods are designed to deceive binary detectors and persuade them to misjudge the authenticity of an image. However, to the best of our knowledge, no attempts have been made to deceive forgery detectors at the pixel level and mis-locate forged regions. Traditional adversarial attack methods cannot be directly used against forgery localization due to the following defects: 1) they tend to just naively induce the target forensic models to flip their pixel-level pristine or forged decisions; 2) their anti-forensics performance tends to be severely degraded when faced with the unseen forensic models; 3) they lose validity once the target forensic models are retrained with the anti-forensics images generated by them. To tackle the three defects, we propose SEAR (Self-supErvised Anti-foRensics), a novel self-supervised and adversarial training algorithm that effectively trains deep-learning anti-forensic models against forgery localization. SEAR sets a pretext task to reconstruct perturbation for self-supervised learning. In adversarial training, SEAR employs a forgery localization model as a supervisor to explore tampering features and constructs a deep-learning concealer to erase corresponding traces. We have conducted largescale experiments across diverse datasets. The experimental results demonstrate that, through the combination of self-supervised learning and adversarial learning, SEAR successfully deceives the state-of-the-art forgery localization methods, as well as tackle the three defects regarding traditional adversarial attack methods mentioned above.
</details>
<details>
<summary>摘要</summary>
反反馈技术目的是消除或隐藏修改 traces。通常，反反馈方法是为了欺骗 binary 检测器，使其错误地评估图像的 authenticity。然而，据我们所知，没有任何尝试使用对 forgery 位置进行欺骗和误导。传统的反对敌方攻击方法无法直接使用对 forgery 位置的攻击，因为以下三点缺陷：1. 它们通常只是简单地让目标伪钞模型变更其像素级的原始或修改的决策;2. 它们对于不同的伪钞模型表现出很差的防御性能;3. 它们在伪钞模型被重新训练后失效。为了解决这些缺陷，我们提出了 SEAR（Self-supErvised Anti-foRensics），一种新的自我超vised 和反对敌方训练算法，用于训练深度学习反反馈模型。SEAR 设置了一个预text task，用于在自我超vised 学习中重建杂音。在反对敌方训练中，SEAR 使用一个 forgery 位置模型作为监视器，以探索修改特征并构建深度学习隐藏器，以消除相应的 traces。我们在多个 dataset 上进行了大规模的实验，实验结果表明，通过将自我超vised 学习和反对敌方训练相结合，SEAR 成功地欺骗了当前最佳的 forgery 位置方法，同时解决了传统反对敌方攻击方法所存在的三个缺陷。
</details></li>
</ul>
<hr>
<h2 id="Deep-Unfolding-Network-for-Image-Compressed-Sensing-by-Content-adaptive-Gradient-Updating-and-Deformation-invariant-Non-local-Modeling"><a href="#Deep-Unfolding-Network-for-Image-Compressed-Sensing-by-Content-adaptive-Gradient-Updating-and-Deformation-invariant-Non-local-Modeling" class="headerlink" title="Deep Unfolding Network for Image Compressed Sensing by Content-adaptive Gradient Updating and Deformation-invariant Non-local Modeling"></a>Deep Unfolding Network for Image Compressed Sensing by Content-adaptive Gradient Updating and Deformation-invariant Non-local Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10033">http://arxiv.org/abs/2310.10033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxue Cui, Xiaopeng Fan, Jian Zhang, Debin Zhao</li>
<li>for: 用于图像压缩感知（CS）领域的深度 unfolding 网络（DUN）的改进。</li>
<li>methods: 提出了一种基于传统的 Proximal Gradient Descent（PGD）算法的 novel DUN 网络（dubbed DUN-CSNet），以解决现有 DUN 中的两个问题：1）大多数超参数是独立于输入内容的，限制了其适应性；2）在每次迭代中使用的普通 convolutional neural network 弱化了更广泛的上下文优先顺序，导致表达能力下降。</li>
<li>results: 经验表明，提出的 DUN-CSNet 在图像压缩感知领域的表现较前者有大幅提升。<details>
<summary>Abstract</summary>
Inspired by certain optimization solvers, the deep unfolding network (DUN) has attracted much attention in recent years for image compressed sensing (CS). However, there still exist the following two issues: 1) In existing DUNs, most hyperparameters are usually content independent, which greatly limits their adaptability for different input contents. 2) In each iteration, a plain convolutional neural network is usually adopted, which weakens the perception of wider context prior and therefore depresses the expressive ability. In this paper, inspired by the traditional Proximal Gradient Descent (PGD) algorithm, a novel DUN for image compressed sensing (dubbed DUN-CSNet) is proposed to solve the above two issues. Specifically, for the first issue, a novel content adaptive gradient descent network is proposed, in which a well-designed step size generation sub-network is developed to dynamically allocate the corresponding step sizes for different textures of input image by generating a content-aware step size map, realizing a content-adaptive gradient updating. For the second issue, considering the fact that many similar patches exist in an image but have undergone a deformation, a novel deformation-invariant non-local proximal mapping network is developed, which can adaptively build the long-range dependencies between the nonlocal patches by deformation-invariant non-local modeling, leading to a wider perception on context priors. Extensive experiments manifest that the proposed DUN-CSNet outperforms existing state-of-the-art CS methods by large margins.
</details>
<details>
<summary>摘要</summary>
traditional Proximal Gradient Descent (PGD) 算法的灵感，一种新的深度 unfolding 网络（DUN）为图像压缩感知（CS）提出了一种新的方法。在这种方法中，存在两个问题：1）在现有的 DUN 中，大多数超参数是独立于输入内容的，这限制了它们的适应性。2）在每个迭代中，通常采用平面卷积神经网络，这弱化了更广泛的上下文先验，从而降低了表达能力。在这篇论文中，我们提出了一种新的 DUN-CSNet，以解决以上两个问题。Specifically，为了解决第一个问题，我们提出了一种新的内容适应的梯度下降网络，其中包括一个 Well-designed 步长生成子网络，通过生成内容快照映射，实现内容适应的梯度更新。为了解决第二个问题，我们发展了一种新的非 lok 的非局部抽象映射网络，该网络可以在不同的扭变下自适应地建立非局部的长距离依赖关系，从而扩大上下文先验的视野。经过广泛的实验，我们发现，提出的 DUN-CSNet 可以舒适性地击败现有的CS方法。
</details></li>
</ul>
<hr>
<h2 id="RoomDesigner-Encoding-Anchor-latents-for-Style-consistent-and-Shape-compatible-Indoor-Scene-Generation"><a href="#RoomDesigner-Encoding-Anchor-latents-for-Style-consistent-and-Shape-compatible-Indoor-Scene-Generation" class="headerlink" title="RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation"></a>RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10027">http://arxiv.org/abs/2310.10027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhao-yiqun/roomdesigner">https://github.com/zhao-yiqun/roomdesigner</a></li>
<li>paper_authors: Yiqun Zhao, Zibo Zhao, Jing Li, Sixun Dong, Shenghua Gao</li>
<li>for: 本研究旨在创造具有尺度、样式兼容的室内场景，以便在室内设计和规划中提供更加真实和可信的场景。</li>
<li>methods: 本研究提出了一种两stage模型，首先使用离散 вектор量化来编码家具为anchor-latent，然后利用 transformer 模型预测室内场景。通过 incorporating anchor-latent 表示，我们的生成模型可以生成具有尺度和样式兼容的家具布局。</li>
<li>results: 实验结果表明，我们的方法可以在 3D-Front 数据集上生成更加一致和兼容的室内场景，而无需shape取样。此外，我们还进行了广泛的ablation研究，以验证我们的设计选择在室内场景生成模型中的效果。<details>
<summary>Abstract</summary>
Indoor scene generation aims at creating shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout. However, most existing approaches primarily focus on generating plausible furniture layouts without incorporating specific details related to individual furniture pieces. To address this limitation, we propose a two-stage model integrating shape priors into the indoor scene generation by encoding furniture as anchor latent representations. In the first stage, we employ discrete vector quantization to encode furniture pieces as anchor-latents. Based on the anchor-latents representation, the shape and location information of the furniture was characterized by a concatenation of location, size, orientation, class, and our anchor latent. In the second stage, we leverage a transformer model to predict indoor scenes autoregressively. Thanks to incorporating the proposed anchor-latents representations, our generative model produces shape-compatible and style-consistent furniture arrangements and synthesis furniture in diverse shapes. Furthermore, our method facilitates various human interaction applications, such as style-consistent scene completion, object mismatch correction, and controllable object-level editing. Experimental results on the 3D-Front dataset demonstrate that our approach can generate more consistent and compatible indoor scenes compared to existing methods, even without shape retrieval. Additionally, extensive ablation studies confirm the effectiveness of our design choices in the indoor scene generation model.
</details>
<details>
<summary>摘要</summary>
indoor scene generation aims to create shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout. However, most existing approaches primarily focus on generating plausible furniture layouts without incorporating specific details related to individual furniture pieces. To address this limitation, we propose a two-stage model integrating shape priors into the indoor scene generation by encoding furniture as anchor latent representations. In the first stage, we employ discrete vector quantization to encode furniture pieces as anchor-latents. Based on the anchor-latents representation, the shape and location information of the furniture was characterized by a concatenation of location, size, orientation, class, and our anchor latent. In the second stage, we leverage a transformer model to predict indoor scenes autoregressively. Thanks to incorporating the proposed anchor-latents representations, our generative model produces shape-compatible and style-consistent furniture arrangements and synthesis furniture in diverse shapes. Furthermore, our method facilitates various human interaction applications, such as style-consistent scene completion, object mismatch correction, and controllable object-level editing. Experimental results on the 3D-Front dataset demonstrate that our approach can generate more consistent and compatible indoor scenes compared to existing methods, even without shape retrieval. Additionally, extensive ablation studies confirm the effectiveness of our design choices in the indoor scene generation model.Here's the text with some minor adjustments to make it more idiomatic in Simplified Chinese:indoor scene generation aims to create shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout. However, most existing approaches primarily focus on generating plausible furniture layouts without incorporating specific details related to individual furniture pieces. To address this limitation, we propose a two-stage model integrating shape priors into the indoor scene generation by encoding furniture as anchor latent representations. In the first stage, we employ discrete vector quantization to encode furniture pieces as anchor-latents. Based on the anchor-latents representation, the shape and location information of the furniture was characterized by a concatenation of location, size, orientation, class, and our anchor latent. In the second stage, we leverage a transformer model to predict indoor scenes autoregressively. Thanks to incorporating the proposed anchor-latents representations, our generative model produces shape-compatible and style-consistent furniture arrangements and synthesis furniture in diverse shapes. Furthermore, our method facilitates various human interaction applications, such as style-consistent scene completion, object mismatch correction, and controllable object-level editing. Experimental results on the 3D-Front dataset demonstrate that our approach can generate more consistent and compatible indoor scenes compared to existing methods, even without shape retrieval. Additionally, extensive ablation studies confirm the effectiveness of our design choices in the indoor scene generation model.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Super-resolution-on-Low-resolution-Micro-expression-Recognition"><a href="#An-Empirical-Study-of-Super-resolution-on-Low-resolution-Micro-expression-Recognition" class="headerlink" title="An Empirical Study of Super-resolution on Low-resolution Micro-expression Recognition"></a>An Empirical Study of Super-resolution on Low-resolution Micro-expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10022">http://arxiv.org/abs/2310.10022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ling Zhou, Mingpei Wang, Xiaohua Huang, Wenming Zheng, Qirong Mao, Guoying Zhao</li>
<li>for: 本研究旨在提高低分辨率（LR）环境中的微表情识别（MER）精度，特别是在实际应用中的群体MER场景。</li>
<li>methods: 本研究使用了七种最新的状态之册（SOTA）MER技术，并对13种SOTA超分解（SR）技术进行评估，以解决SR助成MER中的问题。</li>
<li>results: 经验研究表明，SR助成MER在LR场景中存在主要的挑战，并提出了改进SR助成MER的方向。<details>
<summary>Abstract</summary>
Micro-expression recognition (MER) in low-resolution (LR) scenarios presents an important and complex challenge, particularly for practical applications such as group MER in crowded environments. Despite considerable advancements in super-resolution techniques for enhancing the quality of LR images and videos, few study has focused on investigate super-resolution for improving LR MER. The scarcity of investigation can be attributed to the inherent difficulty in capturing the subtle motions of micro-expressions, even in original-resolution MER samples, which becomes even more challenging in LR samples due to the loss of distinctive features. Furthermore, a lack of systematic benchmarking and thorough analysis of super-resolution-assisted MER methods has been noted. This paper tackles these issues by conducting a series of benchmark experiments that integrate both super-resolution (SR) and MER methods, guided by an in-depth literature survey. Specifically, we employ seven cutting-edge state-of-the-art (SOTA) MER techniques and evaluate their performance on samples generated from 13 SOTA SR techniques, thereby addressing the problem of super-resolution in MER. Through our empirical study, we uncover the primary challenges associated with SR-assisted MER and identify avenues to tackle these challenges by leveraging recent advancements in both SR and MER methodologies. Our analysis provides insights for progressing toward more efficient SR-assisted MER.
</details>
<details>
<summary>摘要</summary>
低分辨率（LR）环境下的微表达识别（MER）具有重要和复杂的挑战，尤其是在实际应用中，如集体MER在拥挤的环境中。despite considerable advancements in super-resolution techniques for enhancing the quality of LR images and videos, few studies have focused on investigating super-resolution for improving LR MER. The scarcity of investigation can be attributed to the inherent difficulty in capturing the subtle motions of micro-expressions, even in original-resolution MER samples, which becomes even more challenging in LR samples due to the loss of distinctive features. Furthermore, a lack of systematic benchmarking and thorough analysis of super-resolution-assisted MER methods has been noted. This paper tackles these issues by conducting a series of benchmark experiments that integrate both super-resolution (SR) and MER methods, guided by an in-depth literature survey. Specifically, we employ seven cutting-edge state-of-the-art (SOTA) MER techniques and evaluate their performance on samples generated from 13 SOTA SR techniques, thereby addressing the problem of super-resolution in MER. Through our empirical study, we uncover the primary challenges associated with SR-assisted MER and identify avenues to tackle these challenges by leveraging recent advancements in both SR and MER methodologies. Our analysis provides insights for progressing toward more efficient SR-assisted MER.
</details></li>
</ul>
<hr>
<h2 id="Black-box-Targeted-Adversarial-Attack-on-Segment-Anything-SAM"><a href="#Black-box-Targeted-Adversarial-Attack-on-Segment-Anything-SAM" class="headerlink" title="Black-box Targeted Adversarial Attack on Segment Anything (SAM)"></a>Black-box Targeted Adversarial Attack on Segment Anything (SAM)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10010">http://arxiv.org/abs/2310.10010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheng Zheng, Chaoning Zhang</li>
<li>for: 本研究旨在实现对Segment Anything Model（SAM）的targeted adversarial attack（TAA），以便更好地理解SAM在恶意攻击下的Robustness。</li>
<li>methods: 该研究使用了一种简单 yet effective的方法，即只攻击图像Encoder，以解决prompt依赖性。此外，提出了一种新的规范损失来增强cross-model transferability，使攻击图像更有特征Domination。</li>
<li>results: 广泛的实验证明了我们提出的简单技术可以成功地实现黑盒TAA on SAM。<details>
<summary>Abstract</summary>
Deep recognition models are widely vulnerable to adversarial examples, which change the model output by adding quasi-imperceptible perturbation to the image input. Recently, Segment Anything Model (SAM) has emerged to become a popular foundation model in computer vision due to its impressive generalization to unseen data and tasks. Realizing flexible attacks on SAM is beneficial for understanding the robustness of SAM in the adversarial context. To this end, this work aims to achieve a targeted adversarial attack (TAA) on SAM. Specifically, under a certain prompt, the goal is to make the predicted mask of an adversarial example resemble that of a given target image. The task of TAA on SAM has been realized in a recent arXiv work in the white-box setup by assuming access to prompt and model, which is thus less practical. To address the issue of prompt dependence, we propose a simple yet effective approach by only attacking the image encoder. Moreover, we propose a novel regularization loss to enhance the cross-model transferability by increasing the feature dominance of adversarial images over random natural images. Extensive experiments verify the effectiveness of our proposed simple techniques to conduct a successful black-box TAA on SAM.
</details>
<details>
<summary>摘要</summary>
深度识别模型广泛受到敌意例子的攻击，这些攻击通过添加 quasi-不可见的扰动来改变输入图像，从而影响模型的输出。最近，Segment Anything Model（SAM）在计算机视觉领域得到了广泛的应用，因为它在未seen数据和任务上表现出了很好的总体化能力。为了更好地理解SAM在敌意上下文中的稳定性，本工作寻求实现针对SAM的targeted adversarial attack（TAA）。具体来说，在一定的提示下，目标是使针对敌意例子的预测面几乎与给定的目标图像相同。在 white-box 设置下，这项任务在最近的 arXiv 文章中已经实现了，但是假设了对提示和模型的访问，这是不实际的。为解决提示的依赖关系，我们提议一种简单 yet 有效的方法，即只攻击图像Encoder。此外，我们还提议一种新的规范损失，以增强跨模型传输性，通过增加攻击图像的特征主导性，使攻击图像在随机自然图像上占据优势。我们的实验证明了我们提议的简单技术可以成功地实现黑盒 TAA 任务。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Encoder-Decoder-Architectures-for-Robust-Coronary-Artery-Segmentation"><a href="#Assessing-Encoder-Decoder-Architectures-for-Robust-Coronary-Artery-Segmentation" class="headerlink" title="Assessing Encoder-Decoder Architectures for Robust Coronary Artery Segmentation"></a>Assessing Encoder-Decoder Architectures for Robust Coronary Artery Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10002">http://arxiv.org/abs/2310.10002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shisheng Zhang, Ramtin Gharleghi, Sonit Singh, Arcot Sowmya, Susann Beier</li>
<li>for: 避免心血管疾病的诊断延迟，通过精准 coronary artery 分 segmentation，改善病人结果。</li>
<li>methods: 使用 convolutional neural networks (CNN) 和 U-Net 架构，以及 25 个不同的 encoder-decoder 组合。</li>
<li>results: 使用 ASOCA 公共数据集，对 40 个案例进行分析，发现 EfficientNet-LinkNet 组合的 Dice 乘数为 0.882，95% Percentile Hausdorff 距离为 4.753，表明该模型在 MICCAI 2020 挑战中比其他模型表现更出色。<details>
<summary>Abstract</summary>
Coronary artery diseases are among the leading causes of mortality worldwide. Timely and accurate diagnosis, facilitated by precise coronary artery segmentation, is pivotal in changing patient outcomes. In the realm of biomedical imaging, convolutional neural networks, especially the U-Net architecture, have revolutionised segmentation processes. However, one of the primary challenges remains the lack of benchmarking datasets specific to coronary arteries. However through the use of the recently published public dataset ASOCA, the potential of deep learning for accurate coronary segmentation can be improved. This paper delves deep into examining the performance of 25 distinct encoder-decoder combinations. Through analysis of the 40 cases provided to ASOCA participants, it is revealed that the EfficientNet-LinkNet combination, serving as encoder and decoder, stands out. It achieves a Dice coefficient of 0.882 and a 95th percentile Hausdorff distance of 4.753. These findings not only underscore the superiority of our model in comparison to those presented at the MICCAI 2020 challenge but also set the stage for future advancements in coronary artery segmentation, opening doors to enhanced diagnostic and treatment strategies.
</details>
<details>
<summary>摘要</summary>
coronary artery disease 是全球最主要的死亡原因之一，时间和准确的诊断是改善病人结果的关键。在生物医学影像中，对于条形血管的精确分类是非常重要。然而，主要挑战是缺乏特定于条形血管的参考数据集。但是透过使用最近发布的公共数据集ASOCA，可以改善深度学习的精确分类性。本文将进行深入分析25组不同的encoder-decoder组合的表现。通过分析ASOCA参赛者提供的40个档案，发现了EfficientNet-LinkNet组合（作为encoder和decoder）的表现最出色，其Dice系数为0.882，95%的 Hausdorff距离为4.753。这些发现不仅与在MICCAI 2020挑战中提出的模型相比，而且开启了未来条形血管分类的新天地，将来对诊断和治疗策略带来改善。
</details></li>
</ul>
<hr>
<h2 id="SeUNet-Trans-A-Simple-yet-Effective-UNet-Transformer-Model-for-Medical-Image-Segmentation"><a href="#SeUNet-Trans-A-Simple-yet-Effective-UNet-Transformer-Model-for-Medical-Image-Segmentation" class="headerlink" title="SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical Image Segmentation"></a>SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09998">http://arxiv.org/abs/2310.09998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tan-Hanh Pham, Xianqi Li, Kim-Doang Nguyen</li>
<li>for: 这个研究旨在提出一个简单 yet effective的 UNet-Transformer（seUNet-Trans）模型，用于医疗影像分类。</li>
<li>methods: 我们使用 UNet 模型作为特征提取器，将输入影像生成多个特征地图，然后将这些地图转移到一个桥layer中，并使用 Transformer 模型进行自我注意力机制。</li>
<li>results: 我们将模型评估于五个医疗影像分类 dataset上，结果显示 seUNet-Trans 模型在这些dataset上具有较高的性能。<details>
<summary>Abstract</summary>
Automated medical image segmentation is becoming increasingly crucial in modern clinical practice, driven by the growing demand for precise diagnoses, the push towards personalized treatment plans, and advancements in machine learning algorithms, especially the incorporation of deep learning methods. While convolutional neural networks (CNNs) have been prevalent among these methods, the remarkable potential of Transformer-based models for computer vision tasks is gaining more acknowledgment. To harness the advantages of both CNN-based and Transformer-based models, we propose a simple yet effective UNet-Transformer (seUNet-Trans) model for medical image segmentation. In our approach, the UNet model is designed as a feature extractor to generate multiple feature maps from the input images, and these maps are propagated into a bridge layer, which sequentially connects the UNet and the Transformer. In this stage, we employ the pixel-level embedding technique without position embedding vectors to make the model more efficient. Moreover, we applied spatial-reduction attention in the Transformer to reduce the computational/memory overhead. By leveraging the UNet architecture and the self-attention mechanism, our model not only preserves both local and global context information but also captures long-range dependencies between input elements. The proposed model is extensively experimented on five medical image segmentation datasets, including polyp segmentation, to demonstrate its efficacy. A comparison with several state-of-the-art segmentation models on these datasets shows the superior performance of seUNet-Trans.
</details>
<details>
<summary>摘要</summary>
《自动医疗影像分割在现代临床实践中变得越来越重要，这被增长的精准诊断需求、个性化治疗方案推动以及机器学习算法的发展，特别是深度学习方法的应用所驱动。而卷积神经网络（CNN）在这些方法中具有广泛的应用，但是Transformer基本模型在计算机视觉任务中表现出了惊人的潜力。为了利用CNN和Transformer两种模型的优点，我们提出了一种简单 yet effective的UNet-Transformer（seUNet-Trans）模型。在我们的方法中，UNet模型被设计为特征提取器，生成输入图像多个特征地图，然后这些地图被传递到一个桥层，该桥层连接了UNet和Transformer。在这个阶段，我们采用了像素级嵌入技术而不使用位置嵌入向量，以使模型更加高效。此外，我们在Transformer中应用了空间减少注意力，以降低计算/存储占用的开销。通过UNet架构和自注意机制，我们的模型不仅保留了本地和全局上下文信息，还能捕捉输入元素之间的长距离依赖关系。我们对五种医疗影像分割数据集进行了广泛的实验，包括肠肝肿瘤分割，以证明seUNet-Trans模型的高效性。与一些现状顶尖分割模型进行比较，我们的模型在这些数据集上显示出了superior的性能。》
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Graph-and-Attention-Based-Hyperspectral-Image-Classification-Methods-for-Remote-Sensing-Data"><a href="#A-Survey-of-Graph-and-Attention-Based-Hyperspectral-Image-Classification-Methods-for-Remote-Sensing-Data" class="headerlink" title="A Survey of Graph and Attention Based Hyperspectral Image Classification Methods for Remote Sensing Data"></a>A Survey of Graph and Attention Based Hyperspectral Image Classification Methods for Remote Sensing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09994">http://arxiv.org/abs/2310.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aryan Vats, Manan Suri<br>for: 本研究旨在对频谱成像图像分类中应用深度学习技术，并评估其在远程感知和航空频谱成像图像中的性能。methods: 本研究涉及使用图гра数据结构和注意机制来减少维度，以提高频谱成像图像分类的性能。同时，也探讨了使用图 convolutional Neural Networks 进行频谱成像图像特征提取，以提高分类性能。results: 根据本研究的结果，使用图гра数据结构和注意机制可以提高频谱成像图像分类的性能，并且可以在远程感知和航空频谱成像图像中实现更好的分类结果。<details>
<summary>Abstract</summary>
The use of Deep Learning techniques for classification in Hyperspectral Imaging (HSI) is rapidly growing and achieving improved performances. Due to the nature of the data captured by sensors that produce HSI images, a common issue is the dimensionality of the bands that may or may not contribute to the label class distinction. Due to the widespread nature of class labels, Principal Component Analysis is a common method used for reducing the dimensionality. However,there may exist methods that incorporate all bands of the Hyperspectral image with the help of the Attention mechanism. Furthermore, to yield better spectral spatial feature extraction, recent methods have also explored the usage of Graph Convolution Networks and their unique ability to use node features in prediction, which is akin to the pixel spectral makeup. In this survey we present a comprehensive summary of Graph based and Attention based methods to perform Hyperspectral Image Classification for remote sensing and aerial HSI images. We also summarize relevant datasets on which these techniques have been evaluated and benchmark the processing techniques.
</details>
<details>
<summary>摘要</summary>
使用深度学习技术进行干扰спектраль成像（HSI）的分类正在迅速增长，并达到了改进的性能。由于探测器生成HSI图像的数据特性，一个常见的问题是带宽的维度，这些带可能或可能不会影响类标分布。由于类标的普遍性，常用的方法包括原始特征值分析（PCA）等。然而，可能存在一些方法，它们可以在所有HSI图像带中使用注意力机制，以提高特征特征提取。此外，为了提取更好的 спектral空间特征，现有的方法还在探索使用图像卷积网络，它们可以使用节点特征进行预测，这与像素 спектраль组成类似。在本综述中，我们提供了对图基和注意力基的方法进行干扰спектраль成像图像分类的全面概述，以及相关的数据集和处理技术的比较。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.CV_2023_10_16/" data-id="clpxp6c2300lmee887zh49o7t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.AI_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T12:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.AI_2023_10_16/">cs.AI - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Greedy-Perspectives-Multi-Drone-View-Planning-for-Collaborative-Coverage-in-Cluttered-Environments"><a href="#Greedy-Perspectives-Multi-Drone-View-Planning-for-Collaborative-Coverage-in-Cluttered-Environments" class="headerlink" title="Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments"></a>Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10863">http://arxiv.org/abs/2310.10863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Suresh, Aditya Rauniyar, Micah Corah, Sebastian Scherer</li>
<li>for: 这篇论文旨在帮助营造大规模的人群拍摄，特别是在团队体育和电影摄影等领域。</li>
<li>methods: 这篇论文使用了序列优化的方法来实现可扩展的Camera View最优化，但是在填充环境中却遇到了协调问题。</li>
<li>results: 作者通过开发了一种多机器人多演员视图规划算法，并对其进行了阻挡和遮挡意识的目标设定，以实现在填充环境中协调多机器人拍摄人群的目的。并且对比formation planner，这种顺序 планинг器在三个场景中 генериру了14%更高的actor view reward，并且在两个场景中与formation planning的性能相似。<details>
<summary>Abstract</summary>
Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance, we plan in five test environments with complex multiple-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward over the actors for three scenarios and comparable performance to formation planning on two others. We also observe near identical performance of sequential planning both with and without inter-robot collision constraints. Overall, we demonstrate effective coordination of teams of aerial robots for filming groups that may split, merge, or spread apart and in environments cluttered with obstacles that may cause collisions or occlusions.
</details>
<details>
<summary>摘要</summary>
deployments of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance, we plan in five test environments with complex multiple-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward over the actors for three scenarios and comparable performance to formation planning on two others. We also observe near identical performance of sequential planning both with and without inter-robot collision constraints. Overall, we demonstrate effective coordination of teams of aerial robots for filming groups that may split, merge, or spread apart and in environments cluttered with obstacles that may cause collisions or occlusions.Here's the text with some notes on the translation:* "deployments" is translated as "部署" (bù dào), which is a more general term that can refer to any type of deployment, not just of robots.* "aerial robots" is translated as "空中机器人" (kōng zhōng jī rò bīng), which is a more specific term that refers to robots that operate in the air.* "film" is translated as "拍摄" (pān shè), which is a more general term that can refer to any type of filming or recording.* "applications" is translated as "应用" (yìng yòu), which is a more general term that can refer to any type of use or application.* "team sports" is translated as "团体运动" (tuán tǐ yùn dòng), which is a more specific term that refers to sports that involve teams of players.* "cinematography" is translated as "摄影" (shè yǐng), which is a more specific term that refers to the art and technique of filmmaking.* "submodular maximization" is translated as "互补最大化" (huì chē zhì dà huì), which is a more specific term that refers to a type of optimization problem where the goal is to maximize a submodular function.* "sequential greedy planning" is translated as "顺序贪吃规划" (shù xìa bīng zhèng), which is a more specific term that refers to a type of planning algorithm that uses greedy heuristics to optimize a sequence of decisions.* "obstacles" is translated as "障碍物" (fāng yì wù), which is a more general term that can refer to any type of obstacle or barrier.* "occlusions" is translated as "遮挡" (miǎn zhì), which is a more specific term that refers to the blocking or hiding of objects or viewpoints by other objects or surfaces.* "inter-robot collision" is translated as "机器人间冲突" (jī rò bīng jiān chōng tòu), which is a more specific term that refers to collisions between robots.* "near-optimality guarantees" is translated as "似乎最优化保证" (xiào guī zhì yòu huì huì), which is a more specific term that refers to guarantees that a solution is close to optimal.* "view-planning" is translated as "观察规划" (guān chá zhì huì), which is a more specific term that refers to the planning of views or viewpoints.* "multi-robot multi-actor" is translated as "多机器人多actor" (duō jī rò duō yuǎn), which is a more specific term that refers to systems with multiple robots and multiple actors.* "occlusion-aware objective" is translated as "遮挡目标" (miǎn zhì mù tiǎo), which is a more specific term that refers to objectives that take into account the presence of occlusions.* "formation planner" is translated as "formation规划" (fāng yì zhì huì), which is a more specific term that refers to a type of planner that uses formations or patterns to optimize a sequence of decisions.* "sequential planner" is translated as "顺序规划" (shù xìa zhì huì), which is a more specific term that refers to a type of planner that uses a sequential search algorithm to optimize a sequence of decisions.* "test environments" is translated as "测试环境" (cè shí huán jīng), which is a more general term that can refer to any type of testing or evaluation environment.* "complex multiple-actor behaviors" is translated as "复杂多actor行为" (fāng xìa duō yuǎn xíng wèi), which is a more specific term that refers to systems with multiple actors and complex behaviors.
</details></li>
</ul>
<hr>
<h2 id="Proper-Laplacian-Representation-Learning"><a href="#Proper-Laplacian-Representation-Learning" class="headerlink" title="Proper Laplacian Representation Learning"></a>Proper Laplacian Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10833">http://arxiv.org/abs/2310.10833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Gomez, Michael Bowling, Marlos C. Machado</li>
<li>for: 解决大型反射学习问题，即探索、泛化和传递问题，需要学习好的状态表示。</li>
<li>methods: 使用 Laplacian 表示法，通过寻找矩阵 Laplacian 的特征值和特征向量来实现。</li>
<li>results: 提出了一种 theoretically 有 garantue 的目标函数和优化算法，可以准确地 recuperate  Laplacian 表示，并且在多种环境中进行了实验，证明了其在学习中的稳定性和可靠性。<details>
<summary>Abstract</summary>
The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.
</details>
<details>
<summary>摘要</summary>
“学习良好的状态表示是解决大型回归学习问题的关键，特别是在探索、泛化和传输方面存在挑战。laplacian表示是一种有 Promise的方法，它可以通过时间扩展的动作发现和奖励形成，以及有用的状态编码。但是，为了获得laplacian表示，需要计算图laplacian的eigen系统，这通常是通过兼容深度学习方法的优化目标来实现。这些优化目标 however，依赖于无法效率地调整的超参数，并且会导致优化过程中的旋转矩阵和征值的不准确性。在这篇论文中，我们介绍了一种有理论基础的目标函数和相应的优化算法，可以有效地近似laplacian表示。我们的方法可以自动回归真正的eigen vectors和征值，并消除前一代的优化目标中的超参数依赖性。我们提供了理论保证，并证明了这些结果在多个环境中的实际表现是稳定和可靠的。”
</details></li>
</ul>
<hr>
<h2 id="Detecting-Speech-Abnormalities-with-a-Perceiver-based-Sequence-Classifier-that-Leverages-a-Universal-Speech-Model"><a href="#Detecting-Speech-Abnormalities-with-a-Perceiver-based-Sequence-Classifier-that-Leverages-a-Universal-Speech-Model" class="headerlink" title="Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model"></a>Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13010">http://arxiv.org/abs/2310.13010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hagen Soltau, Izhak Shafran, Alex Ottenwess, Joseph R. JR Duffy, Rene L. Utianski, Leland R. Barnard, John L. Stricker, Daniela Wiepert, David T. Jones, Hugo Botha</li>
<li>for: 检测speech中的异常现象，尤其是一些神经疾病的表现。</li>
<li>methods: 使用Perceiver-based序列分类器和Universal Speech Model（USM），并将其与12百万小时多样化音频记录进行训练。</li>
<li>results: 提出的模型在Mayo клиника检测集上表现出色，与标准转换器（80.9%）和感知器（81.8%）模型相比，具有更高的准确率（83.1%），并且在有限任务特定数据下，发现预训练是重要的，而且预训练与自动语音识别任务也是有益的。<details>
<summary>Abstract</summary>
We propose a Perceiver-based sequence classifier to detect abnormalities in speech reflective of several neurological disorders. We combine this classifier with a Universal Speech Model (USM) that is trained (unsupervised) on 12 million hours of diverse audio recordings. Our model compresses long sequences into a small set of class-specific latent representations and a factorized projection is used to predict different attributes of the disordered input speech. The benefit of our approach is that it allows us to model different regions of the input for different classes and is at the same time data efficient. We evaluated the proposed model extensively on a curated corpus from the Mayo Clinic. Our model outperforms standard transformer (80.9%) and perceiver (81.8%) models and achieves an average accuracy of 83.1%. With limited task-specific data, we find that pretraining is important and surprisingly pretraining with the unrelated automatic speech recognition (ASR) task is also beneficial. Encodings from the middle layers provide a mix of both acoustic and phonetic information and achieve best prediction results compared to just using the final layer encodings (83.1% vs. 79.6%). The results are promising and with further refinements may help clinicians detect speech abnormalities without needing access to highly specialized speech-language pathologists.
</details>
<details>
<summary>摘要</summary>
我们提议一种基于感知者的序列分类器，用于检测speech中的异常现象，表现为多种神经疾病。我们将这个分类器与一个基于自动语音识别（ASR）任务的通用语音模型（USM）结合，并在1200万小时多样化音频记录上进行无监督训练。我们的模型可以压缩长序列到一小集类特有的归一化表示和一个 факторизовый投影，以预测不同类型的输入异常speech的不同属性。我们的方法的优点在于，它可以为不同类型的输入模型不同的地方，同时具有数据效率的优势。我们对提议模型进行了广泛的评估，并在 mayo临床数据库中验证了模型。我们的模型在标准transformer（80.9%）和感知器（81.8%）模型的基础上提高了性能，并实现了83.1%的平均准确率。我们发现，在有限的任务特定数据上，预训练是重要的，而且预训练使用ASR任务也是有利的。中间层编码器提供了mixture的音频和phonetic信息，并实现了最佳预测结果（83.1% vs. 79.6%）。结果具有潜在的价值，通过进一步的优化，可能帮助临床专业人员检测speech异常性，不需要高度专业的语音学术师。
</details></li>
</ul>
<hr>
<h2 id="If-the-Sources-Could-Talk-Evaluating-Large-Language-Models-for-Research-Assistance-in-History"><a href="#If-the-Sources-Could-Talk-Evaluating-Large-Language-Models-for-Research-Assistance-in-History" class="headerlink" title="If the Sources Could Talk: Evaluating Large Language Models for Research Assistance in History"></a>If the Sources Could Talk: Evaluating Large Language Models for Research Assistance in History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10808">http://arxiv.org/abs/2310.10808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giselle Gonzalez Garcia, Christian Weilbach</li>
<li>for: 这个论文旨在探讨如何使用大型自然语言模型（LLM）来探索历史记忆（或训练数据），并证明了在增强LLM WITH vector embedding的情况下，可以为历史学家和人文科学研究者提供一种可访问的对话式研究方法。</li>
<li>methods: 这篇论文使用了LLM进行对话式研究，并通过增强LLM WITH vector embedding来提高其对问题的回答和数据EXTRACTION和组织能力。</li>
<li>results: 论文表明，LLM可以在问题解决和数据EXTRACTION和组织等任务中表现出色，并且可以在特定研究项目中应用到大量文本档案中，无需包含在其训练数据中。因此，LLM可以被PRIVATELY queried by researchers，并且可以被用于特定研究项目中。<details>
<summary>Abstract</summary>
The recent advent of powerful Large-Language Models (LLM) provides a new conversational form of inquiry into historical memory (or, training data, in this case). We show that by augmenting such LLMs with vector embeddings from highly specialized academic sources, a conversational methodology can be made accessible to historians and other researchers in the Humanities. Concretely, we evaluate and demonstrate how LLMs have the ability of assisting researchers while they examine a customized corpora of different types of documents, including, but not exclusive to: (1). primary sources, (2). secondary sources written by experts, and (3). the combination of these two. Compared to established search interfaces for digital catalogues, such as metadata and full-text search, we evaluate the richer conversational style of LLMs on the performance of two main types of tasks: (1). question-answering, and (2). extraction and organization of data. We demonstrate that LLMs semantic retrieval and reasoning abilities on problem-specific tasks can be applied to large textual archives that have not been part of the its training data. Therefore, LLMs can be augmented with sources relevant to specific research projects, and can be queried privately by researchers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Demystifying-Poisoning-Backdoor-Attacks-from-a-Statistical-Perspective"><a href="#Demystifying-Poisoning-Backdoor-Attacks-from-a-Statistical-Perspective" class="headerlink" title="Demystifying Poisoning Backdoor Attacks from a Statistical Perspective"></a>Demystifying Poisoning Backdoor Attacks from a Statistical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10780">http://arxiv.org/abs/2310.10780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganghua Wang, Xun Xian, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding</li>
<li>for: 本研究旨在评估潜在攻击型机器学习模型的安全性，具体来说是评估含有常量触发器的后门攻击的成功因素和攻击方向。</li>
<li>methods: 本研究使用了定理和实验方法来评估后门攻击的成功因素和攻击方向。</li>
<li>results: 研究发现了一系列关键因素影响后门攻击的成功，包括触发器的类型和位置、模型的类型和训练数据的性质等。此外，研究还发现了一些可能的攻击方向，包括潜在的人为干预和模型的泄漏。<details>
<summary>Abstract</summary>
The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understanding applies to both discriminative and generative models. We also demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>What are the determining factors for a backdoor attack’s success?2. What is the direction of the most effective backdoor attack?3. When will a human-imperceptible trigger succeed?Our findings apply to both discriminative and generative models, and we demonstrate our theory through experiments using benchmark datasets and state-of-the-art backdoor attack scenarios.</details></li>
</ol>
<hr>
<h2 id="BiomedJourney-Counterfactual-Biomedical-Image-Generation-by-Instruction-Learning-from-Multimodal-Patient-Journeys"><a href="#BiomedJourney-Counterfactual-Biomedical-Image-Generation-by-Instruction-Learning-from-Multimodal-Patient-Journeys" class="headerlink" title="BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys"></a>BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10765">http://arxiv.org/abs/2310.10765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Gu, Jianwei Yang, Naoto Usuyama, Chunyuan Li, Sheng Zhang, Matthew P. Lungren, Jianfeng Gao, Hoifung Poon</li>
<li>for: 这个研究旨在应用自然语言指令学习的技术来生成医疗影像中的counterfactual影像，以分别鉴别 causal structure 和 spurious correlation，并且帮助医生更好地阅读医疗影像进行病程模型化。</li>
<li>methods: 这个研究使用 GPT-4 处理医疗影像报告，生成医疗影像的描述，并且使用这些 triplets (prior image, progression description, new image) 进行 latent diffusion 模型的训练，以生成 counterfactual 医疗影像。</li>
<li>results: 这个研究的结果显示，BiomedJourney 方法可以对医疗影像进行高品质的 counterfactual 生成，并且substantially outperform 先前的 state-of-the-art 方法。<details>
<summary>Abstract</summary>
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion model for counterfactual biomedical image generation. Given the relative scarcity of image time series data, we introduce a two-stage curriculum that first pretrains the denoising network using the much more abundant single image-report pairs (with dummy prior image), and then continues training using the counterfactual triples. Experiments using the standard MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive battery of tests on counterfactual medical image generation, BiomedJourney substantially outperforms prior state-of-the-art methods in instruction image editing and medical image generation such as InstructPix2Pix and RoentGen. To facilitate future study in counterfactual medical generation, we plan to release our instruction-learning code and pretrained models.
</details>
<details>
<summary>摘要</summary>
快速进步在图像编辑中使用自然语言指令，如InstructPix2Pix，已经取得了显著的成果。在生物医学领域，这些方法可以应用于对比例图像生成，以分解 causal structure 和偶极相关，并且为疾病进程模型提供了更加稳定的图像解释。然而，通用的图像编辑模型在生物医学领域是不适用的，对比例生成图像的研究仍然很少。在这篇论文中，我们提出了 BiomedJourney，一种新的对比例生成方法，通过 instruction-learning 从多modal 患者旅程中学习。给定一个患有两张不同时点的生物医学图像，我们使用 GPT-4 处理相关的医学报告，并生成一个描述疾病进程的自然语言描述。这些 triple（先前图像、进程描述、新图像）然后用于训练一个潜在扩散模型进行对比例生成。由于图像时序数据的缺乏，我们提出了一个两stage 课程，首先使用 much more abundant 的单图像-报告对（与假先前图像）进行预训练，然后继续使用对比例 triple。实验使用标准的 MIMIC-CXR 数据集表明，BiomedJourney 在对比例医学图像生成方面具有明显的优势，substantially outperforming 先前的状态对照方法，如 InstructPix2Pix 和 RoentGen。为便于未来对比例医学生成的研究，我们计划在未来发布我们的 instruction-learning 代码和预训练模型。
</details></li>
</ul>
<hr>
<h2 id="Step-by-Step-Remediation-of-Students’-Mathematical-Mistakes"><a href="#Step-by-Step-Remediation-of-Students’-Mathematical-Mistakes" class="headerlink" title="Step-by-Step Remediation of Students’ Mathematical Mistakes"></a>Step-by-Step Remediation of Students’ Mathematical Mistakes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10648">http://arxiv.org/abs/2310.10648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rosewang2008/remath">https://github.com/rosewang2008/remath</a></li>
<li>paper_authors: Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, Dorottya Demszky</li>
<li>for: 这个论文的目的是探讨大型自然语言模型（LLM）在数学教学中是否能够有效地帮助新手老师更好地纠正学生的错误。</li>
<li>methods: 这个论文使用了一个名为ReMath的benchmark，该benchmark由经验丰富的数学教师共同开发，它包括三个步骤：（1）推断学生错误的类型，（2）确定修正错误的策略，（3）生成一个包含该信息的回答。这个benchmark用于评估当今最好的 instruct-tuned 和对话模型在ReMath上的性能。</li>
<li>results: 研究发现，即使使用最佳模型，模型的回答仍然不能与经验丰富的数学教师相比。提供模型错误类型和策略信息可以提高模型的回答质量，但是这些回答仍然不能达到经验教师的水平。这些结果表明，使用当今的LLM来提供高质量的学习经验，虽然有potential，但还有一定的限制。研究的代码已经公开在GitHub上：<a target="_blank" rel="noopener" href="https://github.com/rosewang2008/remath%E3%80%82">https://github.com/rosewang2008/remath。</a><details>
<summary>Abstract</summary>
Scaling high-quality tutoring is a major challenge in education. Because of the growing demand, many platforms employ novice tutors who, unlike professional educators, struggle to effectively address student mistakes and thus fail to seize prime learning opportunities for students. In this paper, we explore the potential for large language models (LLMs) to assist math tutors in remediating student mistakes. We present ReMath, a benchmark co-developed with experienced math teachers that deconstructs their thought process for remediation. The benchmark consists of three step-by-step tasks: (1) infer the type of student error, (2) determine the strategy to address the error, and (3) generate a response that incorporates that information. We evaluate the performance of state-of-the-art instruct-tuned and dialog models on ReMath. Our findings suggest that although models consistently improve upon original tutor responses, we cannot rely on models alone to remediate mistakes. Providing models with the error type (e.g., the student is guessing) and strategy (e.g., simplify the problem) leads to a 75% improvement in the response quality over models without that information. Nonetheless, despite the improvement, the quality of the best model's responses still falls short of experienced math teachers. Our work sheds light on the potential and limitations of using current LLMs to provide high-quality learning experiences for both tutors and students at scale. Our work is open-sourced at this link: \url{https://github.com/rosewang2008/remath}.
</details>
<details>
<summary>摘要</summary>
增加高质量的帮助是现代教育中的一大挑战。由于需求的增长，许多平台都雇用了不熟悉教育的新教师，与专业教师不同，他们有时无法有效地 corrected学生的错误，因此失去了学生 prime learning opportunities。在这篇论文中，我们探讨了大型自然语言模型（LLM）是否可以帮助数学 tutors  corrected学生的错误。我们提出了一个名为 ReMath 的标准，与经验丰富的数学教师合作开发。ReMath 包括三个步骤任务：（1）推断学生错误的类型，（2）确定修复错误的策略，（3）生成包含该信息的回答。我们对 state-of-the-art 的 instruct-tuned 和对话模型进行评估，我们的发现表明，虽然模型在 ReMath 上表现了进步，但我们无法仅仅通过模型来修复错误。在提供错误类型（如学生假设）和修复策略（如简化问题）的情况下，模型的回答质量提高了75%。然而，即使有这些信息，模型的回答仍然落后于经验丰富的数学教师。我们的工作探讨了当前 LLM 是否可以在大规模上提供高质量的学习经验。我们的工作开源在这里：https://github.com/rosewang2008/remath。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Video-Diffusion-Models"><a href="#A-Survey-on-Video-Diffusion-Models" class="headerlink" title="A Survey on Video Diffusion Models"></a>A Survey on Video Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10647">http://arxiv.org/abs/2310.10647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChenHsing/Awesome-Video-Diffusion-Models">https://github.com/ChenHsing/Awesome-Video-Diffusion-Models</a></li>
<li>paper_authors: Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, Yu-Gang Jiang</li>
<li>for: 这 paper 主要是为了对 AI 生成内容 (AIGC) 领域中的 video diffusion models 进行了一个系统的综述。</li>
<li>methods: 这 paper 使用了多种方法，包括 diffusion models、GANs 和 auto-regressive Transformers，以探讨 video diffusion models 在不同领域的应用。</li>
<li>results: 这 paper 发现了许多有价值的研究结果，包括 video 生成、编辑、以及其他视频理解任务中的应用。<details>
<summary>Abstract</summary>
The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.
</details>
<details>
<summary>摘要</summary>
最近的人工智能生成内容（AIGC）浪潮中，计算机视觉领域的扩散模型发挥了关键作用。由于它们的出色的生成能力，扩散模型逐渐取代了基于GANs和自适应Transformers的方法，在图像生成和编辑领域以及视频领域的研究中表现出色。然而，现有的评论主要集中在图像生成领域中，对视频领域中的扩散模型的应用有少量最新的评论。为了填补这个空白，本文提供了人工智能生成内容时代的视频扩散模型的全面评论。 Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.
</details></li>
</ul>
<hr>
<h2 id="Interactive-Task-Planning-with-Language-Models"><a href="#Interactive-Task-Planning-with-Language-Models" class="headerlink" title="Interactive Task Planning with Language Models"></a>Interactive Task Planning with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10645">http://arxiv.org/abs/2310.10645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CraftJarvis/MC-Planner">https://github.com/CraftJarvis/MC-Planner</a></li>
<li>paper_authors: Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik</li>
<li>for: 这个paper是为了解决长期任务规划和执行问题，并且可以轻松泛化到不同的目标或任务。</li>
<li>methods: 这个paper使用语言模型来实现交互式任务规划，并且结合高级规划和低级功能执行。</li>
<li>results: 这个paper的系统可以生成新的高级指令来实现未经见过的目标，并且可以轻松地适应不同的任务，只需更改任务指南即可。此外，当用户发送新的请求时，系统可以重新规划根据新的请求、任务指南和之前执行的步骤。<details>
<summary>Abstract</summary>
An interactive robot framework accomplishes long-horizon task planning and can easily generalize to new goals or distinct tasks, even during execution. However, most traditional methods require predefined module design, which makes it hard to generalize to different goals. Recent large language model based approaches can allow for more open-ended planning but often require heavy prompt engineering or domain-specific pretrained models. To tackle this, we propose a simple framework that achieves interactive task planning with language models. Our system incorporates both high-level planning and low-level function execution via language. We verify the robustness of our system in generating novel high-level instructions for unseen objectives and its ease of adaptation to different tasks by merely substituting the task guidelines, without the need for additional complex prompt engineering. Furthermore, when the user sends a new request, our system is able to replan accordingly with precision based on the new request, task guidelines and previously executed steps. Please check more details on our https://wuphilipp.github.io/itp_site and https://youtu.be/TrKLuyv26_g.
</details>
<details>
<summary>摘要</summary>
一个交互式机器人框架实现了长期任务规划，可以轻松泛化到新目标或不同任务，甚至在执行过程中。然而，大多数传统方法需要预定的模块设计，这使得泛化到不同目标变得困难。现有大语言模型基于方法可以允许更开放的规划，但frequently需要重量的提前工程或域特定预训练模型。为解决这个问题，我们提出了一个简单的框架，可以通过语言模型实现交互式任务规划。我们的系统结合高级规划和低级功能执行 via 语言。我们验证了我们的系统在生成未看过目标的新高级指令方面的稳定性和可靠性，以及在不同任务时的扩展性和适应性。具体信息请参考我们的 <https://wuphilipp.github.io/itp_site> 和 <https://youtu.be/TrKLuyv26_g>。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries"><a href="#In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries" class="headerlink" title="In-Context Pretraining: Language Modeling Beyond Document Boundaries"></a>In-Context Pretraining: Language Modeling Beyond Document Boundaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10638">http://arxiv.org/abs/2310.10638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis</li>
<li>for: 这个论文的目的是提高大语言模型（LMs）的性能，使其能够更好地理解文档之间的关系和Contextual reasoning。</li>
<li>methods: 该论文提出了一种新的预训练方法 called In-Context Pretraining，该方法使用相关的文档序列来显式地鼓励LMs读取和理解文档边界。</li>
<li>results: 实验表明，In-Context Pretraining可以提高LMs的性能，特别是在需要更复杂的文档上下文理解任务中，例如在文档学习 (+8%), 阅读理解 (+15%), 对前Context的忠诚 (+16%), 长文档理解 (+5%), 和检索扩展 (+9%).<details>
<summary>Abstract</summary>
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Our experiments show In-Context Pretraining offers a simple and scalable approach to significantly enhance LMs'performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).
</details>
<details>
<summary>摘要</summary>
大型语言模型（LM）目前在预测token的任务上被训练，允许它们直接进行长形生成和提示类型任务，这些任务可以被简化为文档完成。现有的预训管道将LM训练为 concatenating随机的短文档来建立输入 контекст，但是先前的文档提供了无法预测下一个文档的信号。我们则提出了内部预训（In-Context Pretraining），一种新的方法，其中语言模型在相关的文档序列中预训，并且明确地让模型在文档boundaries上读取和理解。我们可以实现内部预训 simply by changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines。但是，这个文档排序问题是具有挑战性的，有 billions of documents，我们希望排序可以最大化文档之间的相似性，而不是重复数据。为了解决这个问题，我们引入了近似算法，用于快速找到相关文档，并使用图 traversal algorithm construct coherent input contexts。我们的实验显示，内部预训可以提供一个简单且扩展的方法，以提高LM的性能：我们在需要更多的contextual reasoning任务中看到了很大的改善（+8%），包括在文档中学习（+15%）、对先前context的忠诚性（+16%）、长形reasoning（+5%）和文档扩展（+9%）。
</details></li>
</ul>
<hr>
<h2 id="Towards-Scenario-based-Safety-Validation-for-Autonomous-Trains-with-Deep-Generative-Models"><a href="#Towards-Scenario-based-Safety-Validation-for-Autonomous-Trains-with-Deep-Generative-Models" class="headerlink" title="Towards Scenario-based Safety Validation for Autonomous Trains with Deep Generative Models"></a>Towards Scenario-based Safety Validation for Autonomous Trains with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10635">http://arxiv.org/abs/2310.10635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Decker, Ananta R. Bhattarai, Michael Lebacher<br>for: 这篇论文是为了探讨如何适当地验证自动驾驶系统的可靠性。methods: 这篇论文使用了深度生成模型来生成数据，以验证自动驾驶系统在不同的照明和天气条件下是否能够正常运行。results: 研究人员通过使用深度生成模型，可以使限量的测试数据更加表示性，并且可以分析自动驾驶系统是否遵循了一般的操作设计域（ODD）要求。特别是在不同的照明和天气条件下，自动驾驶系统是否能够正常运行的问题上，研究人员可以通过深度生成模型来进行分析。<details>
<summary>Abstract</summary>
Modern AI techniques open up ever-increasing possibilities for autonomous vehicles, but how to appropriately verify the reliability of such systems remains unclear. A common approach is to conduct safety validation based on a predefined Operational Design Domain (ODD) describing specific conditions under which a system under test is required to operate properly. However, collecting sufficient realistic test cases to ensure comprehensive ODD coverage is challenging. In this paper, we report our practical experiences regarding the utility of data simulation with deep generative models for scenario-based ODD validation. We consider the specific use case of a camera-based rail-scene segmentation system designed to support autonomous train operation. We demonstrate the capabilities of semantically editing railway scenes with deep generative models to make a limited amount of test data more representative. We also show how our approach helps to analyze the degree to which a system complies with typical ODD requirements. Specifically, we focus on evaluating proper operation under different lighting and weather conditions as well as while transitioning between them.
</details>
<details>
<summary>摘要</summary>
现代人工智能技术为自动驾驶车辆开启了无限可能，但如何正确验证这些系统的可靠性仍然不清楚。一般来说，是通过预先定义的操作设计域（ODD）来确保系统在测试时运行正常。然而，收集足够的实际测试 случа件以确保完整的 ODD 覆盖却是一项具有挑战性的任务。在这篇论文中，我们介绍了使用深度生成模型进行数据simeulation，以验证场景基于 ODD 的安全验证。我们选择了基于摄像头的铁路景象分割系统，用于支持自动列车运行。我们示示了使用深度生成模型编辑铁路场景，以使用有限的测试数据更加 Representative。我们还展示了如何使用我们的方法来分析系统是否符合 Typical ODD 要求。特别是，我们对不同的照明和天气条件下的系统运行正常性进行评估，以及在这些条件之间的过渡中系统的运行情况。
</details></li>
</ul>
<hr>
<h2 id="OpenAgents-An-Open-Platform-for-Language-Agents-in-the-Wild"><a href="#OpenAgents-An-Open-Platform-for-Language-Agents-in-the-Wild" class="headerlink" title="OpenAgents: An Open Platform for Language Agents in the Wild"></a>OpenAgents: An Open Platform for Language Agents in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10634">http://arxiv.org/abs/2310.10634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xlang-ai/openagents">https://github.com/xlang-ai/openagents</a></li>
<li>paper_authors: Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu</li>
<li>for: 本研究旨在提供一个开源平台，供日常生活中使用语言代理人，并且将语言代理人应用到实际生活中。</li>
<li>methods: 本研究使用了Python&#x2F;SQL和日常API工具来建立三个语言代理人：数据代理人、插件代理人和网页代理人。</li>
<li>results: 本研究实现了一个开源平台，可以让一般用户通过网页使用语言代理人功能，并且提供了一个简单的开发者和研究人员的部署体验，以便实现创新的语言代理人和实际世界中的评估。<details>
<summary>Abstract</summary>
Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.
</details>
<details>
<summary>摘要</summary>
语言代理显示出在使用自然语言完成多样化和复杂任务的潜在能力，特别是在基于大语言模型（LLM）的情况下。现有的语言代理框架主要是为了建立证明性的语言代理，忽略了非专家用户访问代理和应用程序层的设计。我们介绍OpenAgents，一个开放的平台，用于在日常生活中使用和主机语言代理。OpenAgents包括三个代理：（1）数据代理，用于数据分析，使用Python/SQL和数据工具；（2）插件代理，提供200多个日常API工具；（3）网络代理，用于自动化网络浏览。OpenAgents允许一般用户通过网页用户界面进行快速响应和常见失败的交互，同时提供了开发者和研究人员在本地设置上的畅通部署体验，为创造语言代理的未来研究和发展提供了基础。我们详细介绍了挑战和机遇，以便为未来的语言代理研究和发展提供指导。
</details></li>
</ul>
<hr>
<h2 id="BioPlanner-Automatic-Evaluation-of-LLMs-on-Protocol-Planning-in-Biology"><a href="#BioPlanner-Automatic-Evaluation-of-LLMs-on-Protocol-Planning-in-Biology" class="headerlink" title="BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology"></a>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10632">http://arxiv.org/abs/2310.10632</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bioplanner/bioplanner">https://github.com/bioplanner/bioplanner</a></li>
<li>paper_authors: Odhran O’Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, Samuel G Rodriques</li>
<li>for: 本研究旨在开发一种自动生成科学实验协议的能力，以便自动化科学研究。</li>
<li>methods: 本研究使用大型自然语言模型（LLM）来生成科学实验协议，并使用pseudocode表示法来评估模型的性能。</li>
<li>results: 研究发现LLM可以准确地生成科学实验协议，并且可以通过pseudocode表示法来评估模型的性能。此外，研究还发现使用pseudocode表示法可以准确地生成新的实验协议，并且可以在生物实验室中成功完成一个生成的实验协议。<details>
<summary>Abstract</summary>
The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model planning abilities in other areas of science or other areas that lack automatic evaluation.
</details>
<details>
<summary>摘要</summary>
科学实验协议自动生成能力会代表科学自动化的重要一步。大型语言模型（LLM）在各种任务上表现出优异，如问答和文本和代码生成。然而，LLM在多步问题和长期规划方面可能会遇到困难，这些是科学实验的关键。此外，科学实验协议的评估困难，因为实验可以用多种语言描述正确，需要专家知识进行评估，并且通常无法自动执行。我们介绍了一种自动评估框架，用于评估语言模型在计划科学实验协议的能力。我们还提供了生物协议集（BioProt），其包含生物实验协议和对应的伪代码表示。为了衡量语言模型在生成科学协议方面的性能，我们使用一个LLM将自然语言协议转换为伪代码，然后评估LLM是否可以从高级描述和授权伪代码函数中重建伪代码。我们使用GPT-3和GPT-4进行测试，并评估其可靠性。我们还验证了 pseudocode 表示的实验协议的可重复性，并在生物实验室中成功执行了生成的协议。我们的框架可以扩展到其他科学领域或缺乏自动评估的领域中的语言模型规划能力的评估和改进。
</details></li>
</ul>
<hr>
<h2 id="Llemma-An-Open-Language-Model-For-Mathematics"><a href="#Llemma-An-Open-Language-Model-For-Mathematics" class="headerlink" title="Llemma: An Open Language Model For Mathematics"></a>Llemma: An Open Language Model For Mathematics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10631">http://arxiv.org/abs/2310.10631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EleutherAI/math-lm">https://github.com/EleutherAI/math-lm</a></li>
<li>paper_authors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck</li>
<li>for: 这篇论文主要是为了描述一种大型语言模型，用于数学领域。</li>
<li>methods: 该论文使用了Code Llama进行预训练，并在Proof-Pile-2 dataset上继续预训练，这个dataset包括科学论文、网络数据和数学代码。</li>
<li>results: 根据MATH benchmark，Llemma模型在开放基础模型中表现出色，并且在相同参数基础上超过了所有已知的开放基础模型和未发布的Minerva模型集。此外，Llemma模型还可以不需要进一步微调来使用工具和正式证明 theorem。<details>
<summary>Abstract</summary>
We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.
</details>
<details>
<summary>摘要</summary>
我团队今天发布了一个大型语言模型，称为Llemma。我们继续预训 Code Llama 在 Proof-Pile-2 上进行预训， Proof-Pile-2 是一个混合科学论文、网络数据和数学代码的杂合，从而得到了 Llemma。在 MATH benchmark 上，Llemma 与所有已知的开放基模型以及未发布的 Minerva 模型集合相比，在参数量为相同的前提下表现出色。此外，Llemma 还可以无需进一步训练地使用工具和正式证明。我们公开发布了所有文件，包括 7 亿和 34 亿参数的模型、Proof-Pile-2 和代码，以便重现我们的实验。
</details></li>
</ul>
<hr>
<h2 id="Factored-Verification-Detecting-and-Reducing-Hallucination-in-Summaries-of-Academic-Papers"><a href="#Factored-Verification-Detecting-and-Reducing-Hallucination-in-Summaries-of-Academic-Papers" class="headerlink" title="Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers"></a>Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10627">http://arxiv.org/abs/2310.10627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elicit/fave-dataset">https://github.com/elicit/fave-dataset</a></li>
<li>paper_authors: Charlie George, Andreas Stuhlmüller</li>
<li>for: 本研究旨在评估自动生成报告中的幻觉现象，以及使用 Factored Verification 方法检测幻觉。</li>
<li>methods: 本研究使用 Factored Verification 方法对 abstractive 报告进行自动检测幻觉，并在 HaluEval benchmark 上达到了新的 SotA 水平。</li>
<li>results: 研究发现，使用 Factored Critiques 方法自动修正幻觉后，ChatGPT 的幻觉数量降低至 0.49，GPT-4 的幻觉数量降低至 0.46，Claude 2 的幻觉数量降低至 0.95。幻觉的存在导致报告中出现了一些细微的差异。因此，在使用模型生成学报时应该慎重。<details>
<summary>Abstract</summary>
Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers? We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries. This method sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average ChatGPT (16k) summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.
</details>
<details>
<summary>摘要</summary>
投影症也捕捉前沿 LLMS---但是怎么减少它对报告简要的影响？我们评估 Factored Verification，一种简单的自动检测投影症的方法。这种方法在HaluEvalbenchmark中的报告简要任务中设置了新的SotArekord，达到76.2%的准确率。然后，我们使用这种方法来估计语言模型在多篇学术论文总结中的投影症频率，发现了0.62个投影症在ChatGPT（16k）总结中，0.84个投影症在GPT-4总结中，以及1.55个投影症在Claude 2总结中。我们让模型使用Factored Critiques进行自我修复，发现这会降低投影症的数量到0.49个投影症在ChatGPT中，0.46个投影症在GPT-4中，以及0.95个投影症在Claude 2中。我们发现投影症很 часто是柔和的，因此在使用模型Synthesize academic papers时应该保持谨慎。
</details></li>
</ul>
<hr>
<h2 id="Video-Language-Planning"><a href="#Video-Language-Planning" class="headerlink" title="Video Language Planning"></a>Video Language Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10625">http://arxiv.org/abs/2310.10625</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, Jonathan Tompson</li>
<li>for: 提高复杂长期任务的视觉规划能力，利用大型生成模型的最新进展。</li>
<li>methods: 训练视语模型和文本到视频模型，并使其服务为策略和价值函数，实现视觉语言规划（VLP）算法。</li>
<li>results: VLP可以根据计算资源的增加，提高完成长期任务的成功率，并可以在不同机器人领域中生成长期视频规划：从多对象重新排序到多摄像头双手巧妙操作。生成的视频规划可以通过目标受控策略转化为真正的机器人行为。实验表明，VLP与先前方法相比，在真实机器人上提高了长期任务成功率。<details>
<summary>Abstract</summary>
We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domains: from multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).
</details>
<details>
<summary>摘要</summary>
我们有兴趣实现视觉观察计划 для复杂长期任务在生成影像和语言之间，利用最近的大型生成模型。为此，我们提出了视觉语言观察（VLP）算法，它包括树搜索程式，我们在视觉语言模型中训练（i）视觉语言模型作为政策和价值函数，以及（ii）文本视频模型作为动态模型。VLP将长期任务指令和当前影像观察作为输入，将产生详细多模式（影像和语言）的视觉计划，描述如何完成最终任务。VLP随计算预算增加，增加计算时间可以提高视觉计划的质量，并能够适用于不同的机器人领域：从多个物体重新排序到多条臂优雅操作。生成的视觉计划可以转换为真实机器人动作 via 目标受控制政策，每个中途几何视觉计划中的一个条件。实验显示，VLP可以与先前的方法相比，在模拟和真实机器人（三个硬件平台）上提高长期任务成功率。
</details></li>
</ul>
<hr>
<h2 id="Generating-Summaries-with-Controllable-Readability-Levels"><a href="#Generating-Summaries-with-Controllable-Readability-Levels" class="headerlink" title="Generating Summaries with Controllable Readability Levels"></a>Generating Summaries with Controllable Readability Levels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10623">http://arxiv.org/abs/2310.10623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo F. R. Ribeiro, Mohit Bansal, Markus Dreyer</li>
<li>for: 这个论文的目的是控制摘要的可读性水平，以便对不同读者群体进行知识传递。</li>
<li>methods: 这个论文使用了以下三种技术来控制摘要的可读性水平：1）指令式可读性控制，2）使用约束来减少请求和实际可读性差距，3）使用预测器来估算下一步摘要的可读性水平。</li>
<li>results: 这个论文通过对新闻摘要（CNN&#x2F;DM数据集）进行实验，证明了其控制可读性的三种生成技术具有显著的改善作用，从而为可控的可读性 summarization 提供了强有力的基础。<details>
<summary>Abstract</summary>
Readability refers to how easily a reader can understand a written text. Several factors affect the readability level, such as the complexity of the text, its subject matter, and the reader's background knowledge. Generating summaries based on different readability levels is critical for enabling knowledge consumption by diverse audiences. However, current text generation approaches lack refined control, resulting in texts that are not customized to readers' proficiency levels. In this work, we bridge this gap and study techniques to generate summaries at specified readability levels. Unlike previous methods that focus on a specific readability level (e.g., lay summarization), we generate summaries with fine-grained control over their readability. We develop three text generation techniques for controlling readability: (1) instruction-based readability control, (2) reinforcement learning to minimize the gap between requested and observed readability and (3) a decoding approach that uses lookahead to estimate the readability of upcoming decoding steps. We show that our generation methods significantly improve readability control on news summarization (CNN/DM dataset), as measured by various readability metrics and human judgement, establishing strong baselines for controllable readability in summarization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>可读性指示文本的理解容易程度。不同因素会影响可读性水平，如文本复杂度、主题和读者背景知识。生成基于不同可读性水平的摘要是关键，以便各种读者阅读。然而，当前文本生成方法缺乏细化控制，导致文本不具有读者素途水平定制。在这种情况下，我们尝试填补这个空白，研究生成摘要时控制可读性的技术。与之前的方法不同，我们的生成方法可以在不同的可读性水平上生成摘要，并且可以在不同的读者背景知识下进行细化控制。我们开发了三种文本生成技术来控制可读性：1. 指令式可读性控制2. 使用奖励学习减少请求和实际可读性之间的差距3. 使用预测器来估计下一步的可读性。我们表明，我们的生成方法可以在新闻摘要（CNN/DM数据集）中提高可读性控制，根据不同的可读性指标和人类评价。这些结果设置了可控的可读性基elines。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Assistive-Robustness-Via-the-Natural-Adversarial-Frontier"><a href="#Quantifying-Assistive-Robustness-Via-the-Natural-Adversarial-Frontier" class="headerlink" title="Quantifying Assistive Robustness Via the Natural-Adversarial Frontier"></a>Quantifying Assistive Robustness Via the Natural-Adversarial Frontier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10610">http://arxiv.org/abs/2310.10610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan</li>
<li>For: The paper aims to build robust policies for robots that assist people, but the challenge is that people can behave unexpectedly and interact with the robot outside of its training distribution, leading to failures.* Methods: The paper proposes a method called RIGID, which constructs the entire natural-adversarial frontier by training adversarial human policies that trade off between minimizing robot reward and acting human-like.* Results: The paper uses RIGID to analyze the performance of standard collaborative Reinforcement Learning and existing methods meant to increase robustness, and compares the frontier identified by RIGID with failures identified in expert adversarial interaction and naturally-occurring failures during user interaction. The results show that RIGID can provide a meaningful measure of robustness predictive of deployment performance and uncover failure cases that are difficult to find manually.Here is the text in Simplified Chinese:* For: 论文目标是建立助人机器人策略，但是人们可能会在测试时表现出意外的行为，导致机器人失败。* Methods: 论文提出了一种方法 called RIGID，它通过培养对应的人类策略来构建整个自然-攻击前景，以确定机器人策略的稳定性。* Results: 论文使用 RIGID 分析了标准合作 reinforcement learning 和现有增强稳定性的方法的性能，并与专家对抗交互中的失败和用户交互中的自然失败进行比较。结果表明，RIGID 可以提供有用的稳定性预测，并揭示了一些难以手动发现的失败案例。<details>
<summary>Abstract</summary>
Our ultimate goal is to build robust policies for robots that assist people. What makes this hard is that people can behave unexpectedly at test time, potentially interacting with the robot outside its training distribution and leading to failures. Even just measuring robustness is a challenge. Adversarial perturbations are the default, but they can paint the wrong picture: they can correspond to human motions that are unlikely to occur during natural interactions with people. A robot policy might fail under small adversarial perturbations but work under large natural perturbations. We propose that capturing robustness in these interactive settings requires constructing and analyzing the entire natural-adversarial frontier: the Pareto-frontier of human policies that are the best trade-offs between naturalness and low robot performance. We introduce RIGID, a method for constructing this frontier by training adversarial human policies that trade off between minimizing robot reward and acting human-like (as measured by a discriminator). On an Assistive Gym task, we use RIGID to analyze the performance of standard collaborative Reinforcement Learning, as well as the performance of existing methods meant to increase robustness. We also compare the frontier RIGID identifies with the failures identified in expert adversarial interaction, and with naturally-occurring failures during user interaction. Overall, we find evidence that RIGID can provide a meaningful measure of robustness predictive of deployment performance, and uncover failure cases in human-robot interaction that are difficult to find manually. https://ood-human.github.io.
</details>
<details>
<summary>摘要</summary>
我们的最终目标是建立Robot assistant的坚强策略。但是，人类在测试时可能会出现意外的行为，导致机器人失败。甚至测试Robot的坚强性也是一个挑战。对于Robot来说，对人类的干预是最大的挑战。我们认为，在这些互动设定下，捕捉Robot的坚强性需要构建和分析人类政策的全面自然针对的前ier：人类政策的Pareto前ier，即在自然性和机器人性能之间寻找最佳的交易。我们提出了RIGID方法，通过在人类政策中培养对减少机器人奖励和人类化行为（由识别器来衡量）进行交易来构建这个前ier。在助手机器人任务上，我们使用RIGID方法分析标准协作学习的性能，以及已有的Robot坚强性增强方法的性能。我们还将这个前ier与专家对机器人互动的攻击、以及用户互动中的自然出现的失败进行比较。总的来说，我们发现RIGID可以提供有意义的坚强性预测，并揭示了人机互动中难以找到的失败案例。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Power-of-Graph-Neural-Networks-in-Solving-Linear-Optimization-Problems"><a href="#Exploring-the-Power-of-Graph-Neural-Networks-in-Solving-Linear-Optimization-Problems" class="headerlink" title="Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems"></a>Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10603">http://arxiv.org/abs/2310.10603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chendiqian/IPM_MPNN">https://github.com/chendiqian/IPM_MPNN</a></li>
<li>paper_authors: Chendi Qian, Didier Chételat, Christopher Morris</li>
<li>for: 这篇论文主要是为了解释Message-Passing Graph Neural Networks（MPNNs）在增强精确优化算法方面的效果。</li>
<li>methods: 这篇论文使用了MPNNs模仿计算机机械的努力，如强分支，来解决混合整数优化问题。</li>
<li>results: 这篇论文证明了MPNNs可以模拟标准内部点方法来解决线性优化问题，并且可以根据给定问题实例的分布来适应。 Empirical results show that MPNNs can solve LP relaxations of standard combinatorial optimization problems with high accuracy and fast speed, often surpassing conventional solvers and competing approaches.<details>
<summary>Abstract</summary>
Recently, machine learning, particularly message-passing graph neural networks (MPNNs), has gained traction in enhancing exact optimization algorithms. For example, MPNNs speed up solving mixed-integer optimization problems by imitating computational intensive heuristics like strong branching, which entails solving multiple linear optimization problems (LPs). Despite the empirical success, the reasons behind MPNNs' effectiveness in emulating linear optimization remain largely unclear. Here, we show that MPNNs can simulate standard interior-point methods for LPs, explaining their practical success. Furthermore, we highlight how MPNNs can serve as a lightweight proxy for solving LPs, adapting to a given problem instance distribution. Empirically, we show that MPNNs solve LP relaxations of standard combinatorial optimization problems close to optimality, often surpassing conventional solvers and competing approaches in solving time.
</details>
<details>
<summary>摘要</summary>
最近，机器学习技术，特别是消息传递图神经网络（MPNN），在增强精确优化算法方面取得了进展。例如，MPNN可以加速解决杂合整数优化问题，通过模拟计算沉重的规则，如强分支法，解决多个线性优化问题（LP）。虽然实际成功，但MPNN在优化LP的原因 remained largely unclear。在这篇文章中，我们展示MPNN可以模拟标准内部点方法，解释其实际成功。此外，我们指出MPNN可以作为LP的轻量级代理，适应给定问题实例分布。在实验中，我们发现MPNN可以解决标准 combinatorial优化问题的LP relaxation，与优化时间相对较长的传统算法和竞争方法相比，往往达到更高的优化精度。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-neural-wavefields-with-Gabor-basis-functions"><a href="#Physics-informed-neural-wavefields-with-Gabor-basis-functions" class="headerlink" title="Physics-informed neural wavefields with Gabor basis functions"></a>Physics-informed neural wavefields with Gabor basis functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10602">http://arxiv.org/abs/2310.10602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Alkhalifah, Xinquan Huang<br>for: This paper aims to enhance the efficiency and accuracy of neural network wavefield solutions by modeling them as linear combinations of Gabor basis functions that satisfy the wave equation.methods: The proposed approach uses a fully connected neural network with an adaptable Gabor layer as the final hidden layer, employing a weighted summation of Gabor neurons to compute predictions. The weights&#x2F;coefficients of the Gabor functions are learned from previous hidden layers with nonlinear activation functions.results: Realistic assessments showcase the efficacy of this novel implementation compared to the vanilla PINN, particularly in scenarios involving high-frequencies and realistic models that are often challenging for PINNs.<details>
<summary>Abstract</summary>
Recently, Physics-Informed Neural Networks (PINNs) have gained significant attention for their versatile interpolation capabilities in solving partial differential equations (PDEs). Despite their potential, the training can be computationally demanding, especially for intricate functions like wavefields. This is primarily due to the neural-based (learned) basis functions, biased toward low frequencies, as they are dominated by polynomial calculations, which are not inherently wavefield-friendly. In response, we propose an approach to enhance the efficiency and accuracy of neural network wavefield solutions by modeling them as linear combinations of Gabor basis functions that satisfy the wave equation. Specifically, for the Helmholtz equation, we augment the fully connected neural network model with an adaptable Gabor layer constituting the final hidden layer, employing a weighted summation of these Gabor neurons to compute the predictions (output). These weights/coefficients of the Gabor functions are learned from the previous hidden layers that include nonlinear activation functions. To ensure the Gabor layer's utilization across the model space, we incorporate a smaller auxiliary network to forecast the center of each Gabor function based on input coordinates. Realistic assessments showcase the efficacy of this novel implementation compared to the vanilla PINN, particularly in scenarios involving high-frequencies and realistic models that are often challenging for PINNs.
</details>
<details>
<summary>摘要</summary>
近期，物理学 Informed Neural Networks (PINNs) 已经受到了广泛关注，因为它们可以通过解析部分偏微分方程 (PDEs) 来进行多元函数的 interpolating 能力。 despite their potential, the training of PINNs can be computationally demanding, especially for complex functions such as wavefields. This is primarily due to the fact that the neural network-based (learned) basis functions are biased towards low frequencies, as they are dominated by polynomial calculations, which are not inherently wavefield-friendly.为了解决这个问题，我们提出了一种方法，用于提高 PINN 的效率和准确性，通过将它们表示为线性组合的 Gabor 基函数，这些基函数满足波方程。 Specifically, for the Helmholtz equation, we augment the fully connected neural network model with an adaptable Gabor layer constituting the final hidden layer, employing a weighted summation of these Gabor neurons to compute the predictions (output). These weights/coefficients of the Gabor functions are learned from the previous hidden layers that include nonlinear activation functions. To ensure the Gabor layer's utilization across the model space, we incorporate a smaller auxiliary network to forecast the center of each Gabor function based on input coordinates. Realistic assessments showcase the efficacy of this novel implementation compared to the vanilla PINN, particularly in scenarios involving high-frequencies and realistic models that are often challenging for PINNs.
</details></li>
</ul>
<hr>
<h2 id="Automated-Natural-Language-Explanation-of-Deep-Visual-Neurons-with-Large-Models"><a href="#Automated-Natural-Language-Explanation-of-Deep-Visual-Neurons-with-Large-Models" class="headerlink" title="Automated Natural Language Explanation of Deep Visual Neurons with Large Models"></a>Automated Natural Language Explanation of Deep Visual Neurons with Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10708">http://arxiv.org/abs/2310.10708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, Ninghao Liu</li>
<li>for: 本研究旨在解释深度神经网络中 neuron 的含义，提高神经网络的可解释性和可行性。</li>
<li>methods: 本研究提出了一种基于大型基础模型的后续框架，可自动生成 neuron 的含义，不需要人工干预或专业知识。</li>
<li>results: 实验表明，该方法可以准确地找出 neuron 的含义，并且可以与不同的模型和数据集相结合。<details>
<summary>Abstract</summary>
Deep neural networks have exhibited remarkable performance across a wide range of real-world tasks. However, comprehending the underlying reasons for their effectiveness remains a challenging problem. Interpreting deep neural networks through examining neurons offers distinct advantages when it comes to exploring the inner workings of neural networks. Previous research has indicated that specific neurons within deep vision networks possess semantic meaning and play pivotal roles in model performance. Nonetheless, the current methods for generating neuron semantics heavily rely on human intervention, which hampers their scalability and applicability. To address this limitation, this paper proposes a novel post-hoc framework for generating semantic explanations of neurons with large foundation models, without requiring human intervention or prior knowledge. Our framework is designed to be compatible with various model architectures and datasets, facilitating automated and scalable neuron interpretation. Experiments are conducted with both qualitative and quantitative analysis to verify the effectiveness of our proposed approach.
</details>
<details>
<summary>摘要</summary>
深度神经网络在各种实际任务中表现出色，但理解它们的内在原理仍然是一项复杂的问题。通过分析神经元来解释深度神经网络的工作机制具有明显的优势。前期研究表明，深度视觉网络中的特定神经元具有 semantics 意义，并在模型性能中扮演重要角色。然而，目前用于生成神经元 semantics 的方法仍然高度依赖于人工干预，这限制了其可推广性和应用性。为了解决这一问题，本文提出了一种新的后置框架，用于自动生成大型基础模型中神经元的Semantic解释，不需要人工干预或先验知识。我们的框架适用于多种模型架构和数据集，可以实现自动化和扩展的神经元解释。我们通过对质量和kvantitative分析进行实验来验证我们的提议的有效性。
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Imagenets-of-ML4EDA"><a href="#Towards-the-Imagenets-of-ML4EDA" class="headerlink" title="Towards the Imagenets of ML4EDA"></a>Towards the Imagenets of ML4EDA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10560">http://arxiv.org/abs/2310.10560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Animesh Basak Chowdhury, Shailja Thakur, Hammond Pearce, Ramesh Karri, Siddharth Garg</li>
<li>for: 这篇论文旨在探讨ML导向EDA工具从RTL到GDSII的应用，但是现在没有标准的数据集或评估任务定义于EDA问题领域。</li>
<li>methods: 作者在这篇论文中描述了他们在Verilog代码生成和逻辑合成方面收集和维护了两个大规模、高质量的数据集的经验。</li>
<li>results: 作者在这篇论文中讨论了数据集维护和扩展的挑战，以及数据集质量和安全性问题，并使用专门为硬件领域开发的数据生成工具。<details>
<summary>Abstract</summary>
Despite the growing interest in ML-guided EDA tools from RTL to GDSII, there are no standard datasets or prototypical learning tasks defined for the EDA problem domain. Experience from the computer vision community suggests that such datasets are crucial to spur further progress in ML for EDA. Here we describe our experience curating two large-scale, high-quality datasets for Verilog code generation and logic synthesis. The first, VeriGen, is a dataset of Verilog code collected from GitHub and Verilog textbooks. The second, OpenABC-D, is a large-scale, labeled dataset designed to aid ML for logic synthesis tasks. The dataset consists of 870,000 And-Inverter-Graphs (AIGs) produced from 1500 synthesis runs on a large number of open-source hardware projects. In this paper we will discuss challenges in curating, maintaining and growing the size and scale of these datasets. We will also touch upon questions of dataset quality and security, and the use of novel data augmentation tools that are tailored for the hardware domain.
</details>
<details>
<summary>摘要</summary>
尽管RLT到GDSII之间的ML指导EDA工具的兴趣在增长，但是没有定义了EDA问题领域的标准数据集或范例学习任务。从计算机视觉社区的经验来看，这些数据集是ML进一步发展EDA的关键。我们在这篇文章中描述了我们在Verilog代码生成和逻辑合成方面收集的两个大规模、高质量数据集的经验。第一个数据集是从GitHub和Verilog书籍中收集的Verilog代码集合，称为VeriGen。第二个数据集是一个大规模、标注的数据集，用于ML逻辑合成任务，包括1500次合成运行从开源硬件项目中生成的870,000个和逻辑图（AIGs）。在这篇文章中，我们会讨论数据集维护和扩展的挑战，以及数据集质量和安全性问题，以及适用于硬件领域的特有数据扩展工具。
</details></li>
</ul>
<hr>
<h2 id="Demonstrations-Are-All-You-Need-Advancing-Offensive-Content-Paraphrasing-using-In-Context-Learning"><a href="#Demonstrations-Are-All-You-Need-Advancing-Offensive-Content-Paraphrasing-using-In-Context-Learning" class="headerlink" title="Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning"></a>Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10707">http://arxiv.org/abs/2310.10707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Som, Karan Sikka, Helen Gent, Ajay Divakaran, Andreas Kathol, Dimitra Vergyri</li>
<li>For: 本研究旨在帮助实践者开发可用的妥协译者，通过尝试大语言模型（LLM）中的内在学习（ICL），使用有限的输入标签示例对象引导模型生成特定查询的愿望输出。* Methods: 本研究检查了一些关键因素，如示例数量和顺序，排除提示指导语言，以及减少衡量毒性。我们在三个数据集上进行了原则性的评估，其中包括我们所提出的上下文感知礼貌译 dataset，包含对话式粗鲁言语、礼貌译和附加的对话上下文。* Results: 我们的结果表明，ICL与监督方法在生成质量方面相当，而且在人工评估中提高了25%，并在衡量毒性方面下降了76%。此外，ICL基于的译者只有10%的训练数据 exhibit slight reduction in performance.<details>
<summary>Abstract</summary>
Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using two closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10% training data.
</details>
<details>
<summary>摘要</summary>
干脆的内容重写是一种更好的代替方案，而不是完全删除内容。这有助于提高通信环境中的文明性。然而，监督重写者依然需要大量标注数据来保持意思和意图。此外，它们还会保留大量的害词，这引发了使用这些重写器的问题。在这篇论文中，我们想帮助实践者开发可用的重写器，通过exploring In-Context Learning（ICL）和大语言模型（LLM）来实现。我们的研究将关注一些关键因素，如数量和顺序的示例，排除提示 instrucion，以及减少测量的害词。我们在三个数据集上进行了原则性的评估，包括我们提出的Context-Aware Polite Paraphrase数据集，这包括对话式的粗鲁言语、文明重写和额外对话背景。我们使用两个关闭源的LLM和一个开源的LLM进行评估。我们的结果表明，ICL与监督方法在生成质量上相当，而且在人工评估中比其提高了25%，并且测量到的害词下降了76%。此外，ICL基本上只有10%的训练数据下降的性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-applied-to-EEG-data-with-different-montages-using-spatial-attention"><a href="#Deep-learning-applied-to-EEG-data-with-different-montages-using-spatial-attention" class="headerlink" title="Deep learning applied to EEG data with different montages using spatial attention"></a>Deep learning applied to EEG data with different montages using spatial attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10550">http://arxiv.org/abs/2310.10550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sccn/deep-channel-harmonization">https://github.com/sccn/deep-channel-harmonization</a></li>
<li>paper_authors: Dung Truong, Muhammad Abdullah Khalid, Arnaud Delorme</li>
<li>for: 本研究旨在使用深度学习处理和提取复杂脑动态信息的EEG raw数据中的信息。</li>
<li>methods: 本研究使用了 espacial attention 对 EEG 电极坐标进行通道协调，以使得可以使用不同的通道组合训练深度学习模型。</li>
<li>results: 研究表明，使用 espacial attention 可以提高模型性能，而且一个使用不同通道组合训练的深度学习模型可以在性别分类任务中表现出色，比Fixed 23-和 128-通道数据 montage 的模型要好。<details>
<summary>Abstract</summary>
The ability of Deep Learning to process and extract relevant information in complex brain dynamics from raw EEG data has been demonstrated in various recent works. Deep learning models, however, have also been shown to perform best on large corpora of data. When processing EEG, a natural approach is to combine EEG datasets from different experiments to train large deep-learning models. However, most EEG experiments use custom channel montages, requiring the data to be transformed into a common space. Previous methods have used the raw EEG signal to extract features of interest and focused on using a common feature space across EEG datasets. While this is a sensible approach, it underexploits the potential richness of EEG raw data. Here, we explore using spatial attention applied to EEG electrode coordinates to perform channel harmonization of raw EEG data, allowing us to train deep learning on EEG data using different montages. We test this model on a gender classification task. We first show that spatial attention increases model performance. Then, we show that a deep learning model trained on data using different channel montages performs significantly better than deep learning models trained on fixed 23- and 128-channel data montages.
</details>
<details>
<summary>摘要</summary>
深度学习可以从Raw EEG数据中提取和处理复杂脑动态信息的能力已经在各种最近的研究中得到证明。然而，深度学习模型也被证明可以在大量数据上表现最佳。在处理 EEG 数据时，自然的方法是将 EEG 数据集合在一起训练大型深度学习模型。然而，大多数 EEG 实验使用自定义通道 montage，需要数据进行变换以达到共同空间。先前的方法使用了 Raw EEG 信号提取关键特征，并将着眼于在 EEG 数据集中共同的特征空间。虽然这是一种合理的方法，但是它忽略了 EEG 原始数据的潜在强大性。在这里，我们探索使用 EEG 电极坐标的空间注意力进行通道协调的 Raw EEG 数据，以便在不同的 montage 上训练深度学习模型。我们在性别分类任务上测试了这种模型，首先显示了空间注意力可以提高模型性能。然后，我们显示了使用不同的 montage 训练深度学习模型可以在性别分类任务中获得显著更好的性能，与固定的 23-和 128-通道数据 montage 相比。
</details></li>
</ul>
<hr>
<h2 id="Use-of-probabilistic-phrases-in-a-coordination-game-human-versus-GPT-4"><a href="#Use-of-probabilistic-phrases-in-a-coordination-game-human-versus-GPT-4" class="headerlink" title="Use of probabilistic phrases in a coordination game: human versus GPT-4"></a>Use of probabilistic phrases in a coordination game: human versus GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10544">http://arxiv.org/abs/2310.10544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurence T Maloney, Maria F Dal Martello, Vivian Fei, Valerie Ma</li>
<li>for: 这个论文的目的是测试人类和大语言模型GPT4在 probabilistic phrases 上的能力。</li>
<li>methods: 这个论文使用了人类和GPT4在两个不同的上下文中 estimates  probabilistic phrases 的能力。</li>
<li>results: 研究发现人类和GPT4在 probabilistic phrases 上的 estimations 在大多数情况下相互吻合，但人类和GPT4在 ambiguity 上的 estimations 不够一致。 GPT4 重复测试结果表明，它的 estimations 不够稳定。<details>
<summary>Abstract</summary>
English speakers use probabilistic phrases such as likely to communicate information about the probability or likelihood of events. Communication is successful to the extent that the listener grasps what the speaker means to convey and, if communication is successful, two individuals can potentially coordinate their actions based on shared knowledge about uncertainty. We first assessed human ability to estimate the probability and the ambiguity (imprecision) of 23 probabilistic phrases in two different contexts, investment advice and medical advice. We then had GPT4 (OpenAI), a recent Large Language Model, complete the same tasks as the human participants. We found that the median human participant and GPT4 assigned probability estimates that were in good agreement (proportions of variance accounted were close to .90). GPT4's estimates of probability both in the investment and Medical contexts were as close or closer to that of the human participants as the human participants were to one another. Estimates of probability for both the human participants and GPT4 were little affected by context. In contrast, human and GPT4 estimates of ambiguity were not in as good agreement. We repeated some of the GPT4 estimates to assess their stability: does GPT4, if run twice, produce the same or similar estimates? There is some indication that it does not.
</details>
<details>
<summary>摘要</summary>
英语speaker们使用可能性短语来传达事件的可能性或可信度。如果通信成功，两个人可以基于共享不确定性知识协调行动。我们首先评估了人类对23个可能性短语的可能性和不确定性（精度）的能力。然后，我们使用GPT4（OpenAI），一个最近的大语言模型，完成了同样的任务。我们发现 median人参与者和GPT4的可能性估计相差不大（相对变异度占比接近0.90）。GPT4在投资和医疗上的可能性估计与人参与者的估计相似或更相似。人参与者和GPT4对可能性的估计几乎不受 context 的影响。然而，人参与者和GPT4对不确定性的估计不太一致。我们重复了一些GPT4的估计以评估其稳定性：GPT4在两次运行后会产生相同或类似的估计吗？有些证据表明它不一定会。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Dataset-Distillation-through-Alignment-with-Smooth-and-High-Quality-Expert-Trajectories"><a href="#Efficient-Dataset-Distillation-through-Alignment-with-Smooth-and-High-Quality-Expert-Trajectories" class="headerlink" title="Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories"></a>Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10541">http://arxiv.org/abs/2310.10541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyuan Shen, Wenzhuo Yang, Kwok-Yan Lam</li>
<li>for: 本研究旨在提出一种数据效果的方法，以便在训练大型和 cutting-edge 机器学习模型时，避免使用大量数据。</li>
<li>methods: 该方法基于 expert trajectory 的使用，并引入 clipping loss 和 gradient penalty 来规则参数变化的速率。此外，还提出了代表性初始化、平衡内循环损失和中间匹配损失等优化策略。</li>
<li>results: 实验结果显示，提出的方法在不同的数据集、大小和分辨率上均显著超越先前的方法。<details>
<summary>Abstract</summary>
Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regulate the rate of parameter changes in expert trajectories. Furthermore, in response to the sensitivity exhibited towards randomly initialized variables during distillation, we propose representative initialization for synthetic dataset and balanced inner-loop loss. Finally, we present two enhancement strategies, namely intermediate matching loss and weight perturbation, to mitigate the potential occurrence of cumulative errors. We conduct extensive experiments on datasets of different scales, sizes, and resolutions. The results demonstrate that the proposed method significantly outperforms prior methods.
</details>
<details>
<summary>摘要</summary>
通常，训练大型和当前最佳的机器学习模型需要使用大规模数据集，这会使训练和参数调整过程成为昂贵的时间和资源浪费。一些研究人员尝试将实际世界数据集中的信息简化为小型和紧凑的 sintetic 数据集，同时保持模型训练的能力，这被称为数据减量（DD）。尽管现有的方法已经取得了一定的进步，但现有的方法仍然无法有效替代大规模数据集。在这篇论文中，我们不同于以前的方法，我们认为专家畅通性在使用更强大的专家轨迹时对 DATASET DISTILLATION 的影响是非常重要的。基于这个想法，我们引入了折射损失和梯度罚 penalty 来控制专家轨迹中参数的变化速率。此外，我们还提出了代表性初始化和平衡内循环损失来适应在混合损失中随机初始化的变量的敏感性。最后，我们提出了两种改进策略，即中间匹配损失和重量扰动，以避免可能出现的累累错误。我们在不同的数据集、大小和分辨率上进行了广泛的实验，结果显示，我们的方法在PRIOR METHODS 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Microscaling-Data-Formats-for-Deep-Learning"><a href="#Microscaling-Data-Formats-for-Deep-Learning" class="headerlink" title="Microscaling Data Formats for Deep Learning"></a>Microscaling Data Formats for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10537">http://arxiv.org/abs/2310.10537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/microxcaling">https://github.com/microsoft/microxcaling</a></li>
<li>paper_authors: Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, Stosic Dusan, Venmugil Elango, Maximilian Golub, Alexander Heinecke, Phil James-Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao, Michael Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verrilli, Ralph Wittig, Doug Burger, Eric Chung</li>
<li>for: 降低现代深度学习应用的计算和存储成本</li>
<li>methods: 使用块缩放因子和窄Float和整数类型来组合微规模数据格式</li>
<li>results: 实证结果表明MX数据格式可以作为FP32的Drop-in更新，并且在AI推理和训练中具有低用户阻力，以及可以在训练生成语言模型中使用sub-8位权重、活化和梯度，并且减少了精度损失。<details>
<summary>Abstract</summary>
Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
</details>
<details>
<summary>摘要</summary>
宽度狭小的数据格式是现代深度学习应用中减少计算和存储成本的关键。这篇论文评估了 Microscaling（MX）数据格式，它将每个块缩放因子与窄浮点和整数类型相结合。MX格式均衡硬件效率、模型准确性和用户抵抗。实验结果表明MX格式可以作为FP32基eline的Drop-in取代物，用于AI推理和训练，并且具有低用户抵抗。我们还示出了在低于8位权重、活动和梯度上训练生成语言模型，无需修改训练脚本，且减少了准确性损失。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Parsing-by-Large-Language-Models-for-Intricate-Updating-Strategies-of-Zero-Shot-Dialogue-State-Tracking"><a href="#Semantic-Parsing-by-Large-Language-Models-for-Intricate-Updating-Strategies-of-Zero-Shot-Dialogue-State-Tracking" class="headerlink" title="Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking"></a>Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10520">http://arxiv.org/abs/2310.10520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ToLightUpTheSky/ParsingDST">https://github.com/ToLightUpTheSky/ParsingDST</a></li>
<li>paper_authors: Yuxiang Wu, Guanting Dong, Weiran Xu</li>
<li>for:  Zero-shot Dialogue State Tracking (DST) aims to address the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly.</li>
<li>methods:  The proposed ParsingDST method leverages powerful Large Language Models (LLMs) and semantic parsing to reformulate the DST task and improve updating strategies in the text-to-JSON process.</li>
<li>results:  Experimental results show that ParsingDST outperforms existing zero-shot DST methods on MultiWOZ, with significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.<details>
<summary>Abstract</summary>
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT Zero-shot Dialogue State Tracking (DST)  Addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="NeMo-Guardrails-A-Toolkit-for-Controllable-and-Safe-LLM-Applications-with-Programmable-Rails"><a href="#NeMo-Guardrails-A-Toolkit-for-Controllable-and-Safe-LLM-Applications-with-Programmable-Rails" class="headerlink" title="NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails"></a>NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10501">http://arxiv.org/abs/2310.10501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen</li>
<li>for: 这个论文主要是为了提供一种开源的工具kit，用于轻松地在基于语言模型（LLM）的对话系统中添加可编程的 guardrails。</li>
<li>methods: 论文使用了一些机制，如模型对齐，来让LLM提供者和开发者在训练时添加到 guardrails。此外，论文还使用了一种运行时灵感自对话管理的方法，允许开发者在运行时添加可编程的 guardrails。</li>
<li>results: 论文的初步结果表明，提出的方法可以与多个LLM提供者合作，开发出可控和安全的LLM应用程序，使用可编程的 guardrails。<details>
<summary>Abstract</summary>
NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.
</details>
<details>
<summary>摘要</summary>
尼莫·卫铁（NeMo Guardrails）是一个开源的工具套件，用于轻松地在基于LLM的对话系统中添加可编程的保护栏（guardrails）。保护栏（rails）是控制LLM输出的一种特定方式，例如不讨论有害话题，遵循预定的对话路径，使用特定的语言风格，等等。它们可以让LLM提供者和开发者在训练时间添加到特定模型中的保护栏，例如使用模型对齐。然而，使用运行时启发自对话管理的NeMo Guardrails，开发者可以添加可编程的rails到LLM应用程序中 - 这些rails是独立于下面LLM的、用户定义的、可解释的。我们的初步结果表明，提议的方法可以与多个LLM提供者合作开发可控和安全的LLM应用程序使用可编程rails。
</details></li>
</ul>
<hr>
<h2 id="LocSelect-Target-Speaker-Localization-with-an-Auditory-Selective-Hearing-Mechanism"><a href="#LocSelect-Target-Speaker-Localization-with-an-Auditory-Selective-Hearing-Mechanism" class="headerlink" title="LocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism"></a>LocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10497">http://arxiv.org/abs/2310.10497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Chen, Xinyuan Qian, Zexu Pan, Kainan Chen, Haizhou Li</li>
<li>for: 本研究旨在提出一种Selective hearing mechanism的目标说话者定位算法，以提高多说话者场景下的干扰难以听清楚的问题。</li>
<li>methods: 给出一个参考说话者的 Referral speech，首先生成一个基于说话者的 Spectrogram mask，以排除干扰说话者的speech。然后，使用Long short-term memory（LSTM）网络提取目标说话者的位置信息从过滤后的 Spectrogram中。</li>
<li>results: 实验表明，我们提出的方法在不同的 Signal-to-noise ratio（SNR）条件下，与现有算法相比，具有较高的准确率和鲁棒性。Specifically, at SNR &#x3D; -10 dB, our proposed network LocSelect achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.<details>
<summary>Abstract</summary>
The prevailing noise-resistant and reverberation-resistant localization algorithms primarily emphasize separating and providing directional output for each speaker in multi-speaker scenarios, without association with the identity of speakers. In this paper, we present a target speaker localization algorithm with a selective hearing mechanism. Given a reference speech of the target speaker, we first produce a speaker-dependent spectrogram mask to eliminate interfering speakers' speech. Subsequently, a Long short-term memory (LSTM) network is employed to extract the target speaker's location from the filtered spectrogram. Experiments validate the superiority of our proposed method over the existing algorithms for different scale invariant signal-to-noise ratios (SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.
</details>
<details>
<summary>摘要</summary>
prevailing 听风抵抗和响应抵抗的本地化算法主要强调在多个说话人场景中分离并提供每个说话人的方向性输出，不与说话人标识相关。在这篇论文中，我们提出了一种目标说话人本地化算法，具有选择性听风机制。给定一个参照说话人的参照speech，我们首先生成一个基于说话人的spectrogram掩码，以消除干扰说话人的speech。接着，我们使用Long short-term memory（LSTM）网络提取目标说话人的位置信息从过滤后的spectrogram中。实验证明了我们的提出方法与现有算法在不同的标准信号噪声比（SNR）条件下表现更好。具体来说，在SNR=-10dB的条件下，我们的提出的网络LocSelect的平均绝对误差（MAE）为3.55，准确率（ACC）为87.40%。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-LLMs-Evaluating-Human-AI-Text-Co-Creation-through-the-Lens-of-News-Headline-Generation"><a href="#Harnessing-the-Power-of-LLMs-Evaluating-Human-AI-Text-Co-Creation-through-the-Lens-of-News-Headline-Generation" class="headerlink" title="Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation"></a>Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10706">http://arxiv.org/abs/2310.10706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jsndg/emnlp23-llm-headline">https://github.com/jsndg/emnlp23-llm-headline</a></li>
<li>paper_authors: Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, Alejandro Jaimes</li>
<li>for: 这项研究旨在探讨人们如何最佳地利用LLMs进行写作，以及与这些模型交互对于写作过程中的拥有感和信任的影响。</li>
<li>methods: 研究采用了常见的人机交互方式（如导航系统、从系统输出中选择、后期编辑），在LLM协助新闻标题生成上进行了比较。</li>
<li>results: 研究发现，人类控制可以减少LLM输出的不良结果，而且与自由编辑相比，AI协助不会影响参与者对写作过程的感知控制。<details>
<summary>Abstract</summary>
To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)为了了解人类如何最好地利用LLM，并如何与这些模型交互影响写作过程中的所有权和信任感，我们在LLM协助新闻标题生成上比较了不同的人机合作方式（如导引系统、从系统输出选择、后期编辑）。尽管LLM独立可以生成可靠的新闻标题，但平均需要人类控制来修复模型输出的问题。 among the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Additionally, AI assistance did not harm participants' perception of control compared to freeform editing.
</details></li>
</ul>
<hr>
<h2 id="Type-aware-Decoding-via-Explicitly-Aggregating-Event-Information-for-Document-level-Event-Extraction"><a href="#Type-aware-Decoding-via-Explicitly-Aggregating-Event-Information-for-Document-level-Event-Extraction" class="headerlink" title="Type-aware Decoding via Explicitly Aggregating Event Information for Document-level Event Extraction"></a>Type-aware Decoding via Explicitly Aggregating Event Information for Document-level Event Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10487">http://arxiv.org/abs/2310.10487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gang Zhao, Yidong Shi, Shudong Lu, Xinjie Yang, Guanting Dong, Jian Xu, Xiaocheng Gong, Si Li</li>
<li>for: 本研究旨在解决文档水平事件EXTRACTION（DEE）中的两个主要挑战：事件散布和多事件。 précédentes méthodes have attempted to address these challenges, but they have overlooked the interference of event-unrelated sentences during event detection and neglected the mutual interference of different event roles during argument extraction.</li>
<li>methods: 本研究提出了一种新的Schema-based Explicitly Aggregating（SEA）模型，该模型可以有效地聚合事件信息，并将事件类型和角色信息分别编码为特定的类型和角色表示。通过基于类型的表示来检测每个事件，SEA可以减轻由事件相关信息引起的干扰。此外，SEA可以根据每个角色的表示来提取对应的Arguments，从而减少不同角色之间的互相干扰。</li>
<li>results: 实验结果表明，SEA模型在ChFinAnn和DuEE-fin数据集上的表现优于STATE-OF-THE-ART（SOTA）方法。<details>
<summary>Abstract</summary>
Document-level event extraction (DEE) faces two main challenges: arguments-scattering and multi-event. Although previous methods attempt to address these challenges, they overlook the interference of event-unrelated sentences during event detection and neglect the mutual interference of different event roles during argument extraction. Therefore, this paper proposes a novel Schema-based Explicitly Aggregating~(SEA) model to address these limitations. SEA aggregates event information into event type and role representations, enabling the decoding of event records based on specific type-aware representations. By detecting each event based on its event type representation, SEA mitigates the interference caused by event-unrelated information. Furthermore, SEA extracts arguments for each role based on its role-aware representations, reducing mutual interference between different roles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEA outperforms the SOTA methods.
</details>
<details>
<summary>摘要</summary>
文档级事件提取（DEE）面临两大挑战：事件散布和多事件。尽管先前的方法尝试解决这些挑战，但它们忽略了事件检测过程中的事件无关句子干扰和对不同角色的事件提取过程中的互相干扰。因此，这篇论文提出了一种新的Schema-based Explicitly Aggregating（SEA）模型，用于解决这些限制。SEA将事件信息聚合到事件类型和角色表示中，使得根据具体的类型意识来解码事件记录。通过根据事件类型表示来检测每个事件，SEA可以减轻由事件无关信息引起的干扰。此外，SEA根据角色意识来提取每个角色的证据，减少不同角色之间的互相干扰。实验结果表明，SEA在ChFinAnn和DuEE-fin数据集上的性能比SOTA方法更高。
</details></li>
</ul>
<hr>
<h2 id="ManyQuadrupeds-Learning-a-Single-Locomotion-Policy-for-Diverse-Quadruped-Robots"><a href="#ManyQuadrupeds-Learning-a-Single-Locomotion-Policy-for-Diverse-Quadruped-Robots" class="headerlink" title="ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots"></a>ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10486">http://arxiv.org/abs/2310.10486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milad Shafiee, Guillaume Bellegarda, Auke Ijspeert</li>
<li>for: 这种研究旨在开发一种可以控制多种四足机器人的奔跑策略，而无需重新调整参数和奖励函数。</li>
<li>methods: 研究人员 drew inspiration from animal motor control，并使用了一种模块化的CPG和PF层，以实现不同机器人之间的奔跑策略的共享。</li>
<li>results: 研究人员在不同机器人上测试了这种策略，并观察到了强健的实际到虚拟转移性，甚至在加载15公斤（相当于A1机器人的125% Nominal Mass）时仍然保持了稳定的性能。<details>
<summary>Abstract</summary>
Learning a locomotion policy for quadruped robots has traditionally been constrained to specific robot morphology, mass, and size. The learning process must usually be repeated for every new robot, where hyperparameters and reward function weights must be re-tuned to maximize performance for each new system. Alternatively, attempting to train a single policy to accommodate different robot sizes, while maintaining the same degrees of freedom (DoF) and morphology, requires either complex learning frameworks, or mass, inertia, and dimension randomization, which leads to prolonged training periods. In our study, we show that drawing inspiration from animal motor control allows us to effectively train a single locomotion policy capable of controlling a diverse range of quadruped robots. These differences encompass a variable number of DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range spanning from 2 kg to 200 kg, and nominal standing heights ranging from 16 cm to 100 cm. Our policy modulates a representation of the Central Pattern Generator (CPG) in the spinal cord, effectively coordinating both frequencies and amplitudes of the CPG to produce rhythmic output (Rhythm Generation), which is then mapped to a Pattern Formation (PF) layer. Across different robots, the only varying component is the PF layer, which adjusts the scaling parameters for the stride height and length. Subsequently, we evaluate the sim-to-real transfer by testing the single policy on both the Unitree Go1 and A1 robots. Remarkably, we observe robust performance, even when adding a 15 kg load, equivalent to 125% of the A1 robot's nominal mass.
</details>
<details>
<summary>摘要</summary>
学习四肢动物机器人的运动策略传统上受到机器人形态、质量和大小的限制。学习过程通常需要对每个新机器人重新调整超参数和奖励函数权重，以最大化表现。 Alternatively, 尝试使用同一个策略控制不同机器人的不同大小、同样的度度自由（DoF）和形态，需要使用复杂的学习框架，或者质量、抗力和维度随机化，这会导致训练期间过长。在我们的研究中，我们draw inspiration from animal motor control，我们可以有效地训练一个单一的运动策略，可以控制多种不同的四肢动物机器人。这些差异包括变化的DoF数（即12或16关节）、三种不同的形态、机器人质量范围从2公斤到200公斤，和nominal standing heights从16厘米到100厘米。我们的策略调节了中枢pattern generator（CPG）的表达，有效地协调CPG的频率和振荡 amplitudes，并将其映射到Pattern Formation（PF）层。不同的机器人中，唯一变化的是PF层，其调整了步高和步长的缩放参数。我们在Unitree Go1和A1机器人上进行了实验，并观察到了稳定的表现，甚至在加载15公斤的情况下，即A1机器人的125% Nominal mass。
</details></li>
</ul>
<hr>
<h2 id="DemoSG-Demonstration-enhanced-Schema-guided-Generation-for-Low-resource-Event-Extraction"><a href="#DemoSG-Demonstration-enhanced-Schema-guided-Generation-for-Low-resource-Event-Extraction" class="headerlink" title="DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction"></a>DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10481">http://arxiv.org/abs/2310.10481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gang Zhao, Xiaocheng Gong, Xinjie Yang, Guanting Dong, Shudong Lu, Si Li</li>
<li>for: 提高低资源场景中的事件EXTRACTION（EE）效果</li>
<li>methods: 示范学习 paradigm和schema-based prompts</li>
<li>results: 在域 adapted low-resource setting中，对三个数据集进行了广泛的实验，并研究了 DemoSG 的稳定性。结果表明， DemoSG 在低资源场景中明显超过当前方法。Here’s a breakdown of each point:</li>
<li>for: The paper is written to improve the effectiveness of Event Extraction (EE) in low-resource scenarios.</li>
<li>methods: The paper proposes two methods to improve EE in low-resource scenarios: (1) demonstration-based learning paradigm, and (2) schema-based prompts.</li>
<li>results: The paper presents extensive experiments on three datasets in in-domain and domain adaptation low-resource settings, and demonstrates that the proposed DemoSG model significantly outperforms current methods in low-resource scenarios.<details>
<summary>Abstract</summary>
Most current Event Extraction (EE) methods focus on the high-resource scenario, which requires a large amount of annotated data and can hardly be applied to low-resource domains. To address EE more effectively with limited resources, we propose the Demonstration-enhanced Schema-guided Generation (DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we propose the demonstration-based learning paradigm for EE to fully use the annotated data, which transforms them into demonstrations to illustrate the extraction process and help the model learn effectively. Secondly, we formulate EE as a natural language generation task guided by schema-based prompts, thereby leveraging label semantics and promoting knowledge transfer in low-resource scenarios. We conduct extensive experiments under in-domain and domain adaptation low-resource settings on three datasets, and study the robustness of DemoSG. The results show that DemoSG significantly outperforms current methods in low-resource scenarios.
</details>
<details>
<summary>摘要</summary>
现有的事件抽取（EE）方法专注于高资源情况下，需要大量的标注数据并几乎无法应用于低资源领域。为了对EE更有效地应用限制的资源，我们提出了示例增强的结构引导生成（DemoSG）模型，它具有以下两个方面的优点：首先，我们提出了示例学习模式，将标注数据转换为示例，以帮助模型彻底学习。其次，我们将EE视为自然语言生成任务，并透过Schema-based启发词提高标签 semantics，以便在低资源情况下传递知识。我们对三个数据集进行了广泛的实验，包括域内和领域适应低资源情况下的实验，并研究了DemoSG的稳定性。结果显示，DemoSG与现有的方法在低资源情况下具有很大的优势。
</details></li>
</ul>
<hr>
<h2 id="Gaining-Wisdom-from-Setbacks-Aligning-Large-Language-Models-via-Mistake-Analysis"><a href="#Gaining-Wisdom-from-Setbacks-Aligning-Large-Language-Models-via-Mistake-Analysis" class="headerlink" title="Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis"></a>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10477">http://arxiv.org/abs/2310.10477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 本研究旨在提高大语言模型（LLM）的安全性和合理性，特别是在面对恶意和毒害语言时。</li>
<li>methods: 本研究提出了一种基于错误分析的新的对齐策略，通过故意暴露LLM于异常输出，然后进行全面的评估，以全面了解内部的原因。</li>
<li>results: 实验结果表明，提出的方法在安全指令遵从方面的性能超过了传统对齐方法，同时保持了高效性。<details>
<summary>Abstract</summary>
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的快速进步带来了机会和挑战，特别是在无意义生成危险和恶意响应方面。传统的Alignment方法努力使LLM towards Desired performance和避免恶意内容，这种研究提出了一种新的Alignment策略，基于 mistake analysis，故意暴露LLM于异常输出，然后进行全面的评估，以全面了解内部原因via自然语言分析。因此，恶意响应可以被转化为调教征集，LLM不仅可以减少生成异常响应，还可以培养自我批判，利用其内置的恶意内容抵制能力。实验结果表明，提出的方法在安全指令遵从方面超过了传统的Alignment技术，同时保持了高效性。
</details></li>
</ul>
<hr>
<h2 id="Stance-Detection-with-Collaborative-Role-Infused-LLM-Based-Agents"><a href="#Stance-Detection-with-Collaborative-Role-Infused-LLM-Based-Agents" class="headerlink" title="Stance Detection with Collaborative Role-Infused LLM-Based Agents"></a>Stance Detection with Collaborative Role-Infused LLM-Based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10467">http://arxiv.org/abs/2310.10467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochong Lan, Chen Gao, Depeng Jin, Yong Li</li>
<li>for: 这篇文章的目的是提出一个三阶段框架，以帮助自然语言处理器（LLM）实现偏见探测。</li>
<li>methods: 这个框架使用了三种不同的LLM，每个LLM都有不同的角色，包括语言专家、领域专家和社交媒体老手。这些LLM共同执行三个阶段，包括多维度文本分析阶段、逻辑增强辩论阶段和结论统一阶段。</li>
<li>results: 这篇文章的结果显示，使用这个框架可以实现高度的偏见探测性能，并且不需要额外的标注资料和模型训练。实验还显示了这个方法的可说明性和可重用性。<details>
<summary>Abstract</summary>
Stance detection automatically detects the stance in a text towards a target, vital for content analysis in web and social media research. Despite their promising capabilities, LLMs encounter challenges when directly applied to stance detection. First, stance detection demands multi-aspect knowledge, from deciphering event-related terminologies to understanding the expression styles in social media platforms. Second, stance detection requires advanced reasoning to infer authors' implicit viewpoints, as stance are often subtly embedded rather than overtly stated in the text. To address these challenges, we design a three-stage framework COLA (short for Collaborative rOle-infused LLM-based Agents) in which LLMs are designated distinct roles, creating a collaborative system where each role contributes uniquely. Initially, in the multidimensional text analysis stage, we configure the LLMs to act as a linguistic expert, a domain specialist, and a social media veteran to get a multifaceted analysis of texts, thus overcoming the first challenge. Next, in the reasoning-enhanced debating stage, for each potential stance, we designate a specific LLM-based agent to advocate for it, guiding the LLM to detect logical connections between text features and stance, tackling the second challenge. Finally, in the stance conclusion stage, a final decision maker agent consolidates prior insights to determine the stance. Our approach avoids extra annotated data and model training and is highly usable. We achieve state-of-the-art performance across multiple datasets. Ablation studies validate the effectiveness of each design role in handling stance detection. Further experiments have demonstrated the explainability and the versatility of our approach. Our approach excels in usability, accuracy, effectiveness, explainability and versatility, highlighting its value.
</details>
<details>
<summary>摘要</summary>
Automatic stance detection可以检测文本中对目标的立场，这对于网络和社交媒体研究是非常重要。然而，深入应用于检测的语言模型（LLMs）会遇到挑战。首先，检测立场需要多方面的知识，包括理解社交媒体平台上的表达方式和解读事件相关的术语。其次，检测立场需要高级的理解，以便推理出作者的潜在观点，因为立场通常不直接在文本中表达。为解决这些挑战，我们设计了一个三个阶段的框架，称为COLA（简称为协作型角色扮演 LLM 代理）。在这个框架中，LLMs被分配为不同的角色，形成一个协作的系统，每个角色具有唯一的贡献。在多维度文本分析阶段，我们配置 LLMS  acted as语言专家、领域专家和社交媒体老手，以获得多方面的分析结果，从而解决第一个挑战。接着，在逻辑批判阶段，我们为每个可能的立场分配了一个特定的 LLM 代理，使得 LLMS 检测文本特征和立场之间的逻辑连接，解决第二个挑战。最后，在立场结论阶段，一个最终的决策者代理将先前的见解集成，以确定立场。我们的方法不需要额外的注释数据和模型训练，具有非常高的可用性。我们在多个数据集上实现了状态的最佳性能。剥离学习 validate了每个设计角色在处理检测立场方面的效果。进一步的实验还表明了我们的方法在可读性、准确性、有效性、可读性和多样性方面的优异。这些结果表明我们的方法具有价值。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Techniques-for-Identifying-the-Defective-Patterns-in-Semiconductor-Wafer-Maps-A-Survey-Empirical-and-Experimental-Evaluations"><a href="#Machine-Learning-Techniques-for-Identifying-the-Defective-Patterns-in-Semiconductor-Wafer-Maps-A-Survey-Empirical-and-Experimental-Evaluations" class="headerlink" title="Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations"></a>Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10705">http://arxiv.org/abs/2310.10705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamal Taha<br>for:This survey paper provides a comprehensive review of machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing, aiming to fill a void in the existing literature and provide an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in this field.methods:The paper employs a four-tier taxonomy to classify ML algorithms into more refined categories and techniques, providing a detailed understanding of the complex relationships between different algorithms and their sub-techniques. The taxonomy includes broad methodology categories, specific sub-techniques, and experimental evaluations to rank the techniques.results:The paper presents a comprehensive empirical evaluation of the techniques based on four criteria and an experimental evaluation that ranks the algorithms employing the same sub-techniques, techniques, sub-categories, and categories. The approach provides a detailed and holistic understanding of ML techniques and algorithms for identifying wafer defects, guiding researchers towards making more informed decisions in their work. The paper also highlights future prospects and opportunities for further research in this field.<details>
<summary>Abstract</summary>
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous empirical and experimental evaluation to rank these varying techniques. For the empirical evaluation, we assess techniques based on a set of four criteria. The experimental evaluation ranks the algorithms employing the same sub-techniques, techniques, sub-categories, and categories. This integration of a multi-layered taxonomy, empirical evaluations, and comparative experiments provides a detailed and holistic understanding of ML techniques and algorithms for identifying wafer defects. This approach guides researchers towards making more informed decisions in their work. Additionally, the paper illuminates the future prospects of ML techniques for wafer defect identification, underscoring potential advancements and opportunities for further research in this field
</details>
<details>
<summary>摘要</summary>
We present a novel taxonomy of methodologies that categorizes algorithms into more specific categories and techniques. This taxonomy has four tiers, starting with broad methodology categories and ending with specific sub-techniques. This taxonomy helps researchers understand the complex relationships between different algorithms and their techniques.We conduct a rigorous empirical and experimental evaluation of these techniques. For the empirical evaluation, we assess techniques based on a set of four criteria. The experimental evaluation ranks the algorithms using the same sub-techniques, techniques, sub-categories, and categories. This integrated approach provides a comprehensive understanding of ML techniques and algorithms for identifying wafer defects.This paper also highlights the future prospects of ML techniques for wafer defect identification, highlighting potential advancements and opportunities for further research in this field. By providing a detailed and holistic understanding of ML techniques and algorithms, this survey aims to guide researchers in making more informed decisions in their work.翻译结果：这篇研究论文提供了机器学习（ML）技术在半导体制造过程中检测板差的全面回顾。尽管有一些研究证明了ML在板差检测中的效果，但是存在一定的研究杂乱。这篇论文尝试填补这个空白，并提供了一个深入分析的机器学习技术在板差检测中的优点、缺点和应用前景。我们提出了一种新的分类方法，它将机器学习算法分为更加细分的类别和技术。这种分类方法有四层结构，从最高级的方法类别开始，到最低级的特定技术。这种分类方法可以帮助研究人员更好地理解不同的算法和技术之间的复杂关系。我们进行了一项严格的实验和实证评估。对实验来说，我们对不同的技术进行了四个标准的评估标准。这种分类方法可以帮助研究人员在工作中做出更加 Informed 的决策。此外，这篇论文还探讨了机器学习技术在板差检测中的未来前景，并指出了潜在的进步和研究机会。
</details></li>
</ul>
<hr>
<h2 id="On-the-Relevance-of-Temporal-Features-for-Medical-Ultrasound-Video-Recognition"><a href="#On-the-Relevance-of-Temporal-Features-for-Medical-Ultrasound-Video-Recognition" class="headerlink" title="On the Relevance of Temporal Features for Medical Ultrasound Video Recognition"></a>On the Relevance of Temporal Features for Medical Ultrasound Video Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10453">http://arxiv.org/abs/2310.10453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MedAI-Clemson/pda_detection">https://github.com/MedAI-Clemson/pda_detection</a></li>
<li>paper_authors: D. Hudson Smith, John Paul Lineberger, George H. Baker</li>
<li>for: 本研究旨在提高医疗ultrasound视频识别任务的效率，特别是在低数据量情况下。</li>
<li>methods: 本研究提出了一种新的多头注意架构，通过 incorporating 时间特征来提高模型的效率。</li>
<li>results: 对比于效率高的3D CNN视频识别模型，本研究在一些常见的ultrasound任务中表现出优于其，尤其是在训练数据量受限的情况下。<details>
<summary>Abstract</summary>
Many medical ultrasound video recognition tasks involve identifying key anatomical features regardless of when they appear in the video suggesting that modeling such tasks may not benefit from temporal features. Correspondingly, model architectures that exclude temporal features may have better sample efficiency. We propose a novel multi-head attention architecture that incorporates these hypotheses as inductive priors to achieve better sample efficiency on common ultrasound tasks. We compare the performance of our architecture to an efficient 3D CNN video recognition model in two settings: one where we expect not to require temporal features and one where we do. In the former setting, our model outperforms the 3D CNN - especially when we artificially limit the training data. In the latter, the outcome reverses. These results suggest that expressive time-independent models may be more effective than state-of-the-art video recognition models for some common ultrasound tasks in the low-data regime.
</details>
<details>
<summary>摘要</summary>
“许多医疗超音波录影 задачі都涉及到识别关键生物学特征，不论在录影中出现的时间点。这表明模型化这些任务可能不需要时间特征。因此，不包含时间特征的模型架构可能会有更好的样本效率。我们提出了一个新的多头注意架构，将这两个假设作为导引假设，以 achieve better sample efficiency on common ultrasound tasks。我们将比较我们的架构和一个高效的3D CNN录影识别模型在两个设定下的性能：一个情况下，我们不需要时间特征，一个情况下，我们需要时间特征。在前一个情况下，我们的模型比3D CNN更高效，特别是当我们人工限制训练数据时。在后一个情况下，结果逆转。这些结果表明表现出时间独立的表达模型可能比现有的录影识别模型在低数据情况下更有效。”
</details></li>
</ul>
<hr>
<h2 id="Text-Summarization-Using-Large-Language-Models-A-Comparative-Study-of-MPT-7b-instruct-Falcon-7b-instruct-and-OpenAI-Chat-GPT-Models"><a href="#Text-Summarization-Using-Large-Language-Models-A-Comparative-Study-of-MPT-7b-instruct-Falcon-7b-instruct-and-OpenAI-Chat-GPT-Models" class="headerlink" title="Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models"></a>Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10449">http://arxiv.org/abs/2310.10449</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lbasyal/llms-text-summarization">https://github.com/lbasyal/llms-text-summarization</a></li>
<li>paper_authors: Lochan Basyal, Mihir Sanghvi</li>
<li>For: This paper explores the use of Large Language Models (LLMs) for text summarization, specifically comparing the performance of three different models (MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003) on two datasets (CNN Daily Mail and XSum).* Methods: The paper uses a diverse set of LLMs and evaluates their performance using widely accepted metrics such as BLEU Score, ROUGE Score, and BERT Score. The experiment involves different hyperparameters and aims to provide a comprehensive understanding of the effectiveness of LLMs for text summarization.* Results: According to the experiment, text-davinci-003 outperformed the other two models, demonstrating its effectiveness for text summarization. The paper provides valuable insights for researchers and practitioners within the NLP domain and lays the foundation for the development of advanced Generative AI applications.<details>
<summary>Abstract</summary>
Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large Language Models (LLMs) when applied to different datasets. The assessment of these models' effectiveness contributes valuable insights to researchers and practitioners within the NLP domain. This work serves as a resource for those interested in harnessing the potential of LLMs for text summarization and lays the foundation for the development of advanced Generative AI applications aimed at addressing a wide spectrum of business challenges.
</details>
<details>
<summary>摘要</summary>
文本概要是一个重要的自然语言处理（NLP）任务，其应用范围从信息检索到内容生成。利用大语言模型（LLMs）已经显著提高了概要技术。这篇论文展开了使用多种LLMs进行文本概要的研究，包括MPT-7b-instruct、falcon-7b-instruct和OpenAI ChatGPT text-davinci-003模型。实验中使用了不同的超参数，并使用了通用的评价指标如双语评价下study（BLEU）分数、推理引导下的学生评价（ROUGE）分数和Transformers的扩展语言模型（BERT）分数进行评估生成的概要。根据实验结果，text-davinci-003表现最佳。这项研究使用了两个不同的数据集：CNN Daily Mail和XSum。研究的主要目标是为NLP领域的研究者和实践者提供LLMs在不同数据集上的性能评估，以便更好地利用LLMs的潜力。这项工作作为NLP领域的研究资源，并为开发高级生成AI应用程序提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Empowered-Agents-for-Simulating-Macroeconomic-Activities"><a href="#Large-Language-Model-Empowered-Agents-for-Simulating-Macroeconomic-Activities" class="headerlink" title="Large Language Model-Empowered Agents for Simulating Macroeconomic Activities"></a>Large Language Model-Empowered Agents for Simulating Macroeconomic Activities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10436">http://arxiv.org/abs/2310.10436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nian Li, Chen Gao, Yong Li, Qingmin Liao</li>
<li>for: 这篇论文旨在探讨使用语言模型（LLM）在macro经济模拟中的可能性，以解决传统模型中的三大挑战，即代理人差异、macro经济趋势的影响和多方面经济因素的互动。</li>
<li>methods: 该论文提出了一种新的方法，利用LLM来塑造人类决策行为，并通过提问工程来让LLM表现出人类特征，包括感知、反思和决策能力。</li>
<li>results: 在macro经济活动的 simulations中，LLM强化的代理人可以做出更加真实的工作和消费决策，并且可以生成更加合理的macro经济现象。这些结果表明LLM在macro经济模拟中的潜力很大。<details>
<summary>Abstract</summary>
The advent of the Web has brought about a paradigm shift in traditional economics, particularly in the digital economy era, enabling the precise recording and analysis of individual economic behavior. This has led to a growing emphasis on data-driven modeling in macroeconomics. In macroeconomic research, Agent-based modeling (ABM) emerged as an alternative, evolving through rule-based agents, machine learning-enhanced decision-making, and, more recently, advanced AI agents. However, the existing works are suffering from three main challenges when endowing agents with human-like decision-making, including agent heterogeneity, the influence of macroeconomic trends, and multifaceted economic factors. Large language models (LLMs) have recently gained prominence in offering autonomous human-like characteristics. Therefore, leveraging LLMs in macroeconomic simulation presents an opportunity to overcome traditional limitations. In this work, we take an early step in introducing a novel approach that leverages LLMs in macroeconomic simulation. We design prompt-engineering-driven LLM agents to exhibit human-like decision-making and adaptability in the economic environment, with the abilities of perception, reflection, and decision-making to address the abovementioned challenges. Simulation experiments on macroeconomic activities show that LLM-empowered agents can make realistic work and consumption decisions and emerge more reasonable macroeconomic phenomena than existing rule-based or AI agents. Our work demonstrates the promising potential to simulate macroeconomics based on LLM and its human-like characteristics.
</details>
<details>
<summary>摘要</summary>
互联网的出现带来了传统经济学中的 Paradigm shift，特别是在数位经济时代，允许精确地录取和分析个人经济行为。这导致了对数据驱动模型在macroeconomics中的增加强调。在macroeconomic研究中，Agent-based modeling（ABM） emerged as an alternative，通过规则生成的代理人、机器学习增强的决策和、更近期的进步AI代理人。然而，现有的工作受到三大挑战，包括代理人多样性、macroeconomic趋势的影响和多方面的经济因素。 latest Large language models（LLMs）have recently gained prominence in offering autonomous human-like characteristics. Therefore, leveraging LLMs in macroeconomic simulation presents an opportunity to overcome traditional limitations. In this work, we take an early step in introducing a novel approach that leverages LLMs in macroeconomic simulation. We design prompt-engineering-driven LLM agents to exhibit human-like decision-making and adaptability in the economic environment, with the abilities of perception, reflection, and decision-making to address the above-mentioned challenges. Simulation experiments on macroeconomic activities show that LLM-empowered agents can make realistic work and consumption decisions and emerge more reasonable macroeconomic phenomena than existing rule-based or AI agents. Our work demonstrates the promising potential to simulate macroeconomics based on LLM and its human-like characteristics.
</details></li>
</ul>
<hr>
<h2 id="Longitudinal-Self-supervised-Learning-Using-Neural-Ordinary-Differential-Equation"><a href="#Longitudinal-Self-supervised-Learning-Using-Neural-Ordinary-Differential-Equation" class="headerlink" title="Longitudinal Self-supervised Learning Using Neural Ordinary Differential Equation"></a>Longitudinal Self-supervised Learning Using Neural Ordinary Differential Equation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10431">http://arxiv.org/abs/2310.10431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Hugo Le Boité, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard</li>
<li>for:  investigate the progressive changes in anatomical structures or disease progression over time</li>
<li>methods:  longitudinal self-supervised learning (LSSL) algorithm embedded in an auto-encoder (AE) structure, Siamese-like LSSL, and neural ordinary differential equation (NODE)</li>
<li>results:  demonstration of LSSL without including a reconstruction term, and the potential of incorporating NODE in conjunction with LSSL<details>
<summary>Abstract</summary>
Longitudinal analysis in medical imaging is crucial to investigate the progressive changes in anatomical structures or disease progression over time. In recent years, a novel class of algorithms has emerged with the goal of learning disease progression in a self-supervised manner, using either pairs of consecutive images or time series of images. By capturing temporal patterns without external labels or supervision, longitudinal self-supervised learning (LSSL) has become a promising avenue. To better understand this core method, we explore in this paper the LSSL algorithm under different scenarios. The original LSSL is embedded in an auto-encoder (AE) structure. However, conventional self-supervised strategies are usually implemented in a Siamese-like manner. Therefore, (as a first novelty) in this study, we explore the use of Siamese-like LSSL. Another new core framework named neural ordinary differential equation (NODE). NODE is a neural network architecture that learns the dynamics of ordinary differential equations (ODE) through the use of neural networks. Many temporal systems can be described by ODE, including modeling disease progression. We believe that there is an interesting connection to make between LSSL and NODE. This paper aims at providing a better understanding of those core algorithms for learning the disease progression with the mentioned change. In our different experiments, we employ a longitudinal dataset, named OPHDIAT, targeting diabetic retinopathy (DR) follow-up. Our results demonstrate the application of LSSL without including a reconstruction term, as well as the potential of incorporating NODE in conjunction with LSSL.
</details>
<details>
<summary>摘要</summary>
长itudinal分析在医学成像中是关键性的，用于探索时间序列中结构或疾病进程的变化。近年来，一种新的算法类型出现了，即无supervision的自适应学习疾病进程（LSSL），使用时间序列或邻接图像对。通过捕捉时间模式，LSSL成为了一个有前途的方向。为更好地理解这种核心方法，我们在这篇论文中探讨了LSSL算法在不同情况下的表现。原始LSSL被嵌入了自适应encoder（AE）结构中。然而，传统的自适应策略通常是在Siamese-like的方式实现。因此，在本研究中，我们探索了使用Siamese-like LSSL。另一个新的核心框架是神经ordinary differential equation（NODE）。NODE是一种使用神经网络学习ordinary differential equation（ODE）的神经网络架构。许多时间系统可以通过ODE进行描述，包括疾病进程。我们认为在LSSL和NODE之间存在一种有趣的联系。本文的目标是为了更好地理解这些核心算法，以便更好地学习疾病进程的变化。在不同的实验中，我们使用了名为OPHDIAT的长itudinal数据集，targeting diabetic retinopathy（DR）跟踪。我们的结果表明了不包含重建项的LSSL的应用，以及将NODE与LSSL相结合的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Reading-Books-is-Great-But-Not-if-You-Are-Driving-Visually-Grounded-Reasoning-about-Defeasible-Commonsense-Norms"><a href="#Reading-Books-is-Great-But-Not-if-You-Are-Driving-Visually-Grounded-Reasoning-about-Defeasible-Commonsense-Norms" class="headerlink" title="Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"></a>Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10418">http://arxiv.org/abs/2310.10418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wade3han/normlens">https://github.com/wade3han/normlens</a></li>
<li>paper_authors: Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang, Jiwan Chung, Yejin Son, Yejin Choi, Youngjae Yu</li>
<li>for: 研究视觉封装常识规则，以提高机器人工智能的可理解性和适应能力。</li>
<li>methods: 使用人类评估和自然语言处理技术，构建一个新的多模态测试 benchmark，以评估模型对视觉封装常识规则的适应性和可解性。</li>
<li>results: 发现当前状态的模型判断和解释与人类标注不匹配，并提出一种新的方法，通过借鉴大型自然语言模型中的社会常识知识，改进模型与人类之间的匹配度。<details>
<summary>Abstract</summary>
Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying visual-grounded commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a new approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code are released at https://seungjuhan.me/normlens.
</details>
<details>
<summary>摘要</summary>
通常的规则是可以被上下文所推翻：读书通常是非常好的，但不是在开车时。而在实际情况下，上下文通常是通过视觉提供的。这种基于视觉的常识规则的理解和判断是人类非常容易做，但对机器来说是一种挑战，因为它需要同时具备视觉理解和常识规则的理解。我们构建了一个新的多Modal benchMark，名为NORMLENS，用于研究基于视觉的常识规则。NORMLENS包含10,000个人类判断，以及2,000个多Modal的情况，并用于解决两个问题：（1）模型与人类平均判断是否能够Alignment？（2）模型如何解释其预测的判断？我们发现当前的模型判断和解释都与人类注释不一致。此外，我们还提出了一种新的方法，通过从大语言模型中提取社会常识知识来更好地将模型与人类Alignment。数据和代码在https://seungjuhan.me/normlens上发布。
</details></li>
</ul>
<hr>
<h2 id="Real-Fake-Effective-Training-Data-Synthesis-Through-Distribution-Matching"><a href="#Real-Fake-Effective-Training-Data-Synthesis-Through-Distribution-Matching" class="headerlink" title="Real-Fake: Effective Training Data Synthesis Through Distribution Matching"></a>Real-Fake: Effective Training Data Synthesis Through Distribution Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10402">http://arxiv.org/abs/2310.10402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, Bo Zhao</li>
<li>for: 提高深度学习模型的训练效率和鲁棒性。</li>
<li>methods: 基于分布匹配理论的数据生成方法，包括数据生成和数据筛选。</li>
<li>results: 在多种图像分类任务中， synthetic data 能够取代和补充实际数据，提高模型的鲁棒性和特点外泄能力。<details>
<summary>Abstract</summary>
Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmentation to real datasets, while also benefits challenging tasks such as out-of-distribution generalization and privacy preservation.
</details>
<details>
<summary>摘要</summary>
现代深度学习模型的训练数据 synthetic 技术在各种学习任务和场景中得到了广泛应用，具有提高数据量、提高模型性能、隐私保护等优点。然而，现有的synthetic数据生成方法在训练高级深度模型时效率仍然较低，限制其实际应用。为解决这个挑战，我们分析了supervised 学习数据生成的原理，从分布匹配角度出发，探讨生成效果的机制。经过广泛的实验，我们证明了我们的synthetic数据在多种图像分类任务中具有广泛的应用价值，可以替代真实数据，也可以增强模型的性能，同时在难题上如out-of-distribution泛化和隐私保护等方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="Can-Word-Sense-Distribution-Detect-Semantic-Changes-of-Words"><a href="#Can-Word-Sense-Distribution-Detect-Semantic-Changes-of-Words" class="headerlink" title="Can Word Sense Distribution Detect Semantic Changes of Words?"></a>Can Word Sense Distribution Detect Semantic Changes of Words?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10400">http://arxiv.org/abs/2310.10400</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LivNLP/Sense-based-Semantic-Change-Prediction">https://github.com/LivNLP/Sense-based-Semantic-Change-Prediction</a></li>
<li>paper_authors: Xiaohang Tang, Yi Zhou, Taichi Aida, Procheta Sen, Danushka Bollegala</li>
<li>for: 这个研究的目的是为了探索使用时间点数据集来预测字词意思是否有变化。</li>
<li>methods: 这个研究使用预训 слова embeddings 来自动标注目标词的每个出现，然后计算每个词的意思分布。最后，使用不同的分布或距离度量来衡量目标词的意思变化。</li>
<li>results: 实验结果显示，使用时间点数据集可以准确预测英语、德语、瑞典语和拉丁语中字词意思的变化。<details>
<summary>Abstract</summary>
Semantic Change Detection (SCD) of words is an important task for various NLP applications that must make time-sensitive predictions. Some words are used over time in novel ways to express new meanings, and these new meanings establish themselves as novel senses of existing words. On the other hand, Word Sense Disambiguation (WSD) methods associate ambiguous words with sense ids, depending on the context in which they occur. Given this relationship between WSD and SCD, we explore the possibility of predicting whether a target word has its meaning changed between two corpora collected at different time steps, by comparing the distributions of senses of that word in each corpora. For this purpose, we use pretrained static sense embeddings to automatically annotate each occurrence of the target word in a corpus with a sense id. Next, we compute the distribution of sense ids of a target word in a given corpus. Finally, we use different divergence or distance measures to quantify the semantic change of the target word across the two given corpora. Our experimental results on SemEval 2020 Task 1 dataset show that word sense distributions can be accurately used to predict semantic changes of words in English, German, Swedish and Latin.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>word sense distribution的Changesemantic detection（SCD）是各种自然语言处理（NLP）应用程序中的重要任务，这些应用程序需要在时间紧张的情况下进行预测。一些 слова在时间的推移中被用于表达新的意思，这些新意思会成为存在的 слова的新的意思。然而，word sense disambiguation（WSD）方法会将word的意思相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-World-Active-Learning-for-3D-Object-Detection"><a href="#Towards-Open-World-Active-Learning-for-3D-Object-Detection" class="headerlink" title="Towards Open World Active Learning for 3D Object Detection"></a>Towards Open World Active Learning for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10391">http://arxiv.org/abs/2310.10391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoxiao Chen, Yadan Luo, Zixin Wang, Zijian Wang, Xin Yu, Zi Huang</li>
<li>for: 本研究旨在解决开放世界3D对象检测中存在新类出现的挑战，即效率地选择少量的3D框进行标注，以提高对知类和未知类的检测性能。</li>
<li>methods: 本研究提出了一种名为OpenCRB的简单有效的活动学习策略，通过融合关系约束，选择最有价值的3D框进行标注，以最小化标注成本。</li>
<li>results: 对于开放世界3D对象检测任务，提出了一种名为OpenCRB的简单有效的活动学习策略，可以在很少的标注成本下，达到高效地检测知类和未知类的目标。<details>
<summary>Abstract</summary>
Significant strides have been made in closed world 3D object detection, testing systems in environments with known classes. However, the challenge arises in open world scenarios where new object classes appear. Existing efforts sequentially learn novel classes from streams of labeled data at a significant annotation cost, impeding efficient deployment to the wild. To seek effective solutions, we investigate a more practical yet challenging research task: Open World Active Learning for 3D Object Detection (OWAL-3D), aiming at selecting a small number of 3D boxes to annotate while maximizing detection performance on both known and unknown classes. The core difficulty centers on striking a balance between mining more unknown instances and minimizing the labeling expenses of point clouds. Empirically, our study finds the harmonious and inverse relationship between box quantities and their confidences can help alleviate the dilemma, avoiding the repeated selection of common known instances and focusing on uncertain objects that are potentially unknown. We unify both relational constraints into a simple and effective AL strategy namely OpenCRB, which guides to acquisition of informative point clouds with the least amount of boxes to label. Furthermore, we develop a comprehensive codebase for easy reproducing and future research, supporting 15 baseline methods (i.e., active learning, out-of-distribution detection and open world detection), 2 types of modern 3D detectors (i.e., one-stage SECOND and two-stage PV-RCNN) and 3 benchmark 3D datasets (i.e., KITTI, nuScenes and Waymo). Extensive experiments evidence that the proposed Open-CRB demonstrates superiority and flexibility in recognizing both novel and shared categories with very limited labeling costs, compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>Recent progress has been made in closed-world 3D object detection, testing systems in environments with known classes. However, the challenge arises in open-world scenarios where new object classes appear. Existing efforts sequentially learn novel classes from streams of labeled data at a significant annotation cost, impeding efficient deployment to the wild. To seek effective solutions, we investigate a more practical yet challenging research task: Open World Active Learning for 3D Object Detection (OWAL-3D), aiming at selecting a small number of 3D boxes to annotate while maximizing detection performance on both known and unknown classes. The core difficulty centers on striking a balance between mining more unknown instances and minimizing the labeling expenses of point clouds. Empirically, our study finds the harmonious and inverse relationship between box quantities and their confidences can help alleviate the dilemma, avoiding the repeated selection of common known instances and focusing on uncertain objects that are potentially unknown. We unify both relational constraints into a simple and effective AL strategy named OpenCRB, which guides to acquisition of informative point clouds with the least amount of boxes to label. Furthermore, we develop a comprehensive codebase for easy reproducing and future research, supporting 15 baseline methods (i.e., active learning, out-of-distribution detection, and open-world detection), 2 types of modern 3D detectors (i.e., one-stage SECOND and two-stage PV-RCNN), and 3 benchmark 3D datasets (i.e., KITTI, nuScenes, and Waymo). Extensive experiments evidence that the proposed Open-CRB demonstrates superiority and flexibility in recognizing both novel and shared categories with very limited labeling costs, compared to state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Cross-Lingual-Consistency-of-Factual-Knowledge-in-Multilingual-Language-Models"><a href="#Cross-Lingual-Consistency-of-Factual-Knowledge-in-Multilingual-Language-Models" class="headerlink" title="Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models"></a>Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10378">http://arxiv.org/abs/2310.10378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Betswish/Cross-Lingual-Consistency">https://github.com/Betswish/Cross-Lingual-Consistency</a></li>
<li>paper_authors: Jirui Qi, Raquel Fernández, Arianna Bisazza</li>
<li>For: The paper aims to study the cross-lingual consistency (CLC) of factual knowledge in various multilingual pre-trained language models (PLMs) and to identify the determining factors for CLC.* Methods: The authors propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. They conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level.* Results: The authors find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. They also conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing, and find that the new piece of knowledge transfers only to languages with which English has a high RankC score.Here are the three points in Simplified Chinese text:* For: 这个论文目的是研究不同语言背景下的多语言大规模预训练语言模型（PLMs）中的知识一致性（CLC），并确定这些因素的影响因素。* Methods: 作者们提出了一种简单的排名基于一致性（RankC）指标，以独立地评估不同语言之间的知识一致性。他们进行了深入的分析，以确定CLC的决定因素，包括模型级别和语言对照级别。* Results: 作者们发现，增加模型大小通常会提高大多数语言中的事实探测精度，但不会提高跨语言一致性。他们还进行了模型编辑后新知识插入的 caso study，发现新知识只转移到与英语有高RankC分数的语言中。<details>
<summary>Abstract</summary>
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.
</details>
<details>
<summary>摘要</summary>
多语言大规模预训练语言模型（PLM）已经显示出很大量的事实知识，但是各语言之间存在大量的变化。为确保用户不同语言背景 obtain consistent feedback from the same model，我们研究了跨语言一致性（CLC）的多语言大规模预训练语言模型中的事实知识。为此，我们提出了一个 Ranking-based Consistency（RankC）度量来评估不同语言之间的知识一致性，不受准确率的影响。使用这个度量，我们进行了详细的分析CLC的决定因素，包括模型级别和语言对级别。 among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.
</details></li>
</ul>
<hr>
<h2 id="GTA-A-Geometry-Aware-Attention-Mechanism-for-Multi-View-Transformers"><a href="#GTA-A-Geometry-Aware-Attention-Mechanism-for-Multi-View-Transformers" class="headerlink" title="GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers"></a>GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10375">http://arxiv.org/abs/2310.10375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autonomousvision/gta">https://github.com/autonomousvision/gta</a></li>
<li>paper_authors: Takeru Miyato, Bernhard Jaeger, Max Welling, Andreas Geiger</li>
<li>for: 提高3D视觉任务中transformer模型的学习效率和性能，无需额外学习参数，只带有较少的计算开销。</li>
<li>methods: 基于几何关系的相对变换来编码token的几何结构，提出了一种几何意识授益机制（Geometric Transform Attention，GTA）。</li>
<li>results: 在多视图synthesis任务中，GTA提高了state-of-the-art transformer-based NVS模型的学习效率和性能，无需额外学习参数，只带有较少的计算开销。<details>
<summary>Abstract</summary>
As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.
</details>
<details>
<summary>摘要</summary>
transformers是 permutation 的equivariant，因此需要encoding位置信息来进行许多任务。然而，现有的 pozitional 编码方案最初是为 NLP 任务设计的，因此对于视觉任务，它们的适用性是有问题的。我们认为现有的 pozitional 编码方案对于 3D 视觉任务是不佳的，因为它们不尊重它们的下面 3D 结构。基于这个假设，我们提出了一种 geometry-aware 注意力机制，该机制通过 queries 和 key-value 对的几何关系来确定 tokens 的几何变换。我们通过在多视图 synthesis (NVS) 数据集上评估了多个 sparse wide-baseline 多视图设定，并证明了我们的注意力（GTA）可以提高基于 transformer 的 NVS 模型的学习效率和性能，无需额外学习参数，只需要少量的计算开销。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Tuning-for-Multi-View-Graph-Contrastive-Learning"><a href="#Prompt-Tuning-for-Multi-View-Graph-Contrastive-Learning" class="headerlink" title="Prompt Tuning for Multi-View Graph Contrastive Learning"></a>Prompt Tuning for Multi-View Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10362">http://arxiv.org/abs/2310.10362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghua Gong, Xiang Li, Jianxiang Yu, Cheng Yao, Jiaqi Tan, Chengcheng Yu, Dawei Yin</li>
<li>for: 提高 traditional GNN 的问题，如标签依赖和泛化性能，使用 “预训练和精度调整” 方法。</li>
<li>methods: 提出了一种多视图图像对比学习方法作为预tex，并设计了一种启发调整方法来衔接预tex 和下游任务。</li>
<li>results: 通过对多个 benchmark 数据集进行广泛的实验，证明了我们的提议可以有效地提高 GNN 的性能。<details>
<summary>Abstract</summary>
In recent years, "pre-training and fine-tuning" has emerged as a promising approach in addressing the issues of label dependency and poor generalization performance in traditional GNNs. To reduce labeling requirement, the "pre-train, fine-tune" and "pre-train, prompt" paradigms have become increasingly common. In particular, prompt tuning is a popular alternative to "pre-training and fine-tuning" in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives. However, existing study of prompting on graphs is still limited, lacking a framework that can accommodate commonly used graph pre-training methods and downstream tasks. In this paper, we propose a multi-view graph contrastive learning method as pretext and design a prompting tuning for it. Specifically, we first reformulate graph pre-training and downstream tasks into a common format. Second, we construct multi-view contrasts to capture relevant information of graphs by GNN. Third, we design a prompting tuning method for our multi-view graph contrastive learning method to bridge the gap between pretexts and downsteam tasks. Finally, we conduct extensive experiments on benchmark datasets to evaluate and analyze our proposed method.
</details>
<details>
<summary>摘要</summary>
Recently, "预训练和细化" 方法在解决传统GNNS中的标签依赖和泛化性问题上得到了广泛的应用。以减少标签需求为目的，"预训练、细化" 和 "预训练、提示" 两种方法在自然语言处理领域中得到了广泛的应用。特别是，提示练习是自然语言处理中的一种流行的代替方法，旨在减少预训练和下游目标之间的差距。然而，现有的图像提示研究仍然受限，缺乏一个框架可以整合通用的图像预训练方法和下游任务。在这篇论文中，我们提出了一种多视图图像对照学习方法作为预文，并设计了一种提示练习方法来衔接这两者。具体来说，我们首先将图像预训练和下游任务转化为共同的格式。其次，我们使用多视图对照来捕捉图像中重要信息。最后，我们设计了一种提示练习方法来桥接预文和下游任务之间的差距。我们在标准 benchmark 数据集上进行了广泛的实验，以评估和分析我们的提议方法。
</details></li>
</ul>
<hr>
<h2 id="Tabular-Representation-Noisy-Operators-and-Impacts-on-Table-Structure-Understanding-Tasks-in-LLMs"><a href="#Tabular-Representation-Noisy-Operators-and-Impacts-on-Table-Structure-Understanding-Tasks-in-LLMs" class="headerlink" title="Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs"></a>Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10358">http://arxiv.org/abs/2310.10358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananya Singha, José Cambronero, Sumit Gulwani, Vu Le, Chris Parnin</li>
<li>for: 这个论文旨在研究大型自然语言模型（LLM）在表格任务中使用上下文学习，并评估不同格式的表格提示表示的影响。</li>
<li>methods: 作者根据先前的工作，生成了一系列自我超vised的结构任务（例如，导航到单元和行；转置表格），并评估不同格式下LLM表现的差异。此外，作者还引入了8种噪音操作，以模拟实际世界中的潦腹数据和敌意输入，并证明这些操作对不同结构理解任务中LLM表现的影响。</li>
<li>results: 研究发现，不同格式下LLM的表现有显著差异，而噪音操作也可以影响LLM的表现。这些结果表明，在选择表格提示格式时，需要考虑表格的结构和噪音特征，以便 optimize LLM的表现。<details>
<summary>Abstract</summary>
Large language models (LLMs) are increasingly applied for tabular tasks using in-context learning. The prompt representation for a table may play a role in the LLMs ability to process the table. Inspired by prior work, we generate a collection of self-supervised structural tasks (e.g. navigate to a cell and row; transpose the table) and evaluate the performance differences when using 8 formats. In contrast to past work, we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在表格任务中使用内容学习，表格提示表现可能影响 LLM 的处理能力。受到先前的工作启发，我们生成了一个自动supervised的结构任务集（例如：前往矩格和行），并评估不同格式的性能差异。相比于过去的工作，我们引入了8种噪音操作，这些操作是根据实际的混乱数据和敌方输入而设计的，并证明这些操作可以影响 LLM 的性能在不同结构理解任务中。
</details></li>
</ul>
<hr>
<h2 id="Compressed-Sensing-of-Generative-Sparse-latent-GSL-Signals"><a href="#Compressed-Sensing-of-Generative-Sparse-latent-GSL-Signals" class="headerlink" title="Compressed Sensing of Generative Sparse-latent (GSL) Signals"></a>Compressed Sensing of Generative Sparse-latent (GSL) Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15119">http://arxiv.org/abs/2310.15119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Honoré, Anubhab Ghosh, Saikat Chatterjee</li>
<li>for: 该研究旨在使用神经网络生成模型进行环境信号重建，并使用非对称拟合算法实现稀疏化。</li>
<li>methods: 该研究使用了神经网络生成模型，并采用了非对称拟合算法来实现稀疏化。</li>
<li>results: 实验结果表明，使用非对称拟合算法可以实现高质量的环境信号重建。<details>
<summary>Abstract</summary>
We consider reconstruction of an ambient signal in a compressed sensing (CS) setup where the ambient signal has a neural network based generative model. The generative model has a sparse-latent input and we refer to the generated ambient signal as generative sparse-latent signal (GSL). The proposed sparsity inducing reconstruction algorithm is inherently non-convex, and we show that a gradient based search provides a good reconstruction performance. We evaluate our proposed algorithm using simulated data.
</details>
<details>
<summary>摘要</summary>
我们考虑了压缩感知（CS）设置中重建的环境信号，该信号有基于神经网络的生成模型。生成的环境信号我们称为生成稀烈输入信号（GSL）。我们提出的稀烈性引导的重建算法是非几何的，我们表明了使用梯度基本搜索可以获得良好的重建性能。我们使用模拟数据进行评估我们的提议算法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Layerwise-Polynomial-Approximation-for-Efficient-Private-Inference-on-Fully-Homomorphic-Encryption-A-Dynamic-Programming-Approach"><a href="#Optimizing-Layerwise-Polynomial-Approximation-for-Efficient-Private-Inference-on-Fully-Homomorphic-Encryption-A-Dynamic-Programming-Approach" class="headerlink" title="Optimizing Layerwise Polynomial Approximation for Efficient Private Inference on Fully Homomorphic Encryption: A Dynamic Programming Approach"></a>Optimizing Layerwise Polynomial Approximation for Efficient Private Inference on Fully Homomorphic Encryption: A Dynamic Programming Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10349">http://arxiv.org/abs/2310.10349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghyun Lee, Eunsang Lee, Young-Sik Kim, Yongwoo Lee, Joon-Woo Lee, Yongjune Kim, Jong-Seon No</li>
<li>for: 本研究旨在实现基于完全同质加密的隐私保护深度神经网络，但实际应用受到 prolonged inference times 的限制。这主要归结于使用高度多项式函数approximation，如ReLU函数的高度多项式approximation，占用了大量同质计算资源，导致更慢的推理。</li>
<li>methods: 本研究使用layerwise度优化activation functions的方法，以减少推理时间，保持深度神经网络的分类精度。而不like previoius works，我们不使用最小最大approximation方法，而是使用weighted least squares approximation方法，基于activation functions的输入分布。然后，我们通过 dynamic programming算法获取layerwise优化的度数，考虑每层的扩张错误对深度神经网络的分类精度的影响。此外，我们还提议在ciphertext moduli-chain layerwise进行调整，以更好地减少推理时间。</li>
<li>results: 我们的方法可以将ResNet-20模型和ResNet-32模型的推理时间减少为3.44倍和3.16倍，respectively，相比之前使用均匀度和固定ciphertext modulus的实现。<details>
<summary>Abstract</summary>
Recent research has explored the implementation of privacy-preserving deep neural networks solely using fully homomorphic encryption. However, its practicality has been limited because of prolonged inference times. When using a pre-trained model without retraining, a major factor contributing to these prolonged inference times is the high-degree polynomial approximation of activation functions such as the ReLU function. The high-degree approximation consumes a substantial amount of homomorphic computational resources, resulting in slower inference. Unlike the previous works approximating activation functions uniformly and conservatively, this paper presents a \emph{layerwise} degree optimization of activation functions to aggressively reduce the inference time while maintaining classification accuracy by taking into account the characteristics of each layer. Instead of the minimax approximation commonly used in state-of-the-art private inference models, we employ the weighted least squares approximation method with the input distributions of activation functions. Then, we obtain the layerwise optimized degrees for activation functions through the \emph{dynamic programming} algorithm, considering how each layer's approximation error affects the classification accuracy of the deep neural network. Furthermore, we propose modulating the ciphertext moduli-chain layerwise to reduce the inference time. By these proposed layerwise optimization methods, we can reduce inference times for the ResNet-20 model and the ResNet-32 model by 3.44 times and 3.16 times, respectively, in comparison to the prior implementations employing uniform degree polynomials and a consistent ciphertext modulus.
</details>
<details>
<summary>摘要</summary>
近期研究探讨了使用完全同质加密实现隐私保护深度神经网络。然而，它的实用性受到了 prolonged inference times 的限制。当使用预训练模型无需重新训练时，一个主要的因素是高度 polynomials 的激活函数approximation，如 ReLU 函数。高度的激活函数approximation 需要大量的同质计算资源，导致更慢的推理。 unlike previous works approximating activation functions uniformly and conservatively, this paper presents a layerwise degree optimization of activation functions to aggressively reduce the inference time while maintaining classification accuracy by taking into account the characteristics of each layer。 instead of the minimax approximation commonly used in state-of-the-art private inference models, we employ the weighted least squares approximation method with the input distributions of activation functions。 then, we obtain the layerwise optimized degrees for activation functions through the dynamic programming algorithm, considering how each layer's approximation error affects the classification accuracy of the deep neural network。 furthermore, we propose modulating the ciphertext moduli-chain layerwise to reduce the inference time。 by these proposed layerwise optimization methods, we can reduce inference times for the ResNet-20 model and the ResNet-32 model by 3.44 times and 3.16 times, respectively, in comparison to the prior implementations employing uniform degree polynomials and a consistent ciphertext modulus。
</details></li>
</ul>
<hr>
<h2 id="Attribution-Patching-Outperforms-Automated-Circuit-Discovery"><a href="#Attribution-Patching-Outperforms-Automated-Circuit-Discovery" class="headerlink" title="Attribution Patching Outperforms Automated Circuit Discovery"></a>Attribution Patching Outperforms Automated Circuit Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10348">http://arxiv.org/abs/2310.10348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaquib Syed, Can Rager, Arthur Conmy</li>
<li>for: 这个论文的目的是探讨自动化可读性研究的可能性，以扩展神经网络行为的解释到大型模型。</li>
<li>methods: 这个论文使用了归因覆盖来自动化发现计算子网络（circuit）。它使用了一种简单的方法，基于归因覆盖来估算每个边的重要性，然后使用这个估算来剪枝。</li>
<li>results: 论文表明，使用这种方法可以超越现有的所有方法，只需要两次前向 passes和一次后向 pass。在所有任务中，这种方法的AUC从计算子网络恢复中得到了最高的平均值。<details>
<summary>Abstract</summary>
Automated interpretability research has recently attracted attention as a potential research direction that could scale explanations of neural network behavior to large models. Existing automated circuit discovery work applies activation patching to identify subnetworks responsible for solving specific tasks (circuits). In this work, we show that a simple method based on attribution patching outperforms all existing methods while requiring just two forward passes and a backward pass. We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph. Using this approximation, we prune the least important edges of the network. We survey the performance and limitations of this method, finding that averaged over all tasks our method has greater AUC from circuit recovery than other methods.
</details>
<details>
<summary>摘要</summary>
自动化可读性研究近期吸引了关注，作为可扩展 neural network 行为的解释的潜在研究方向。现有的自动化电路发现工作使用活动贴图来确定解决特定任务（电路）负责的子网络。在这种工作中，我们显示了一种简单的方法，基于归因贴图，超过所有现有方法，只需要两次前进和一次反向传播。我们使用线性近似来估计活动贴图中每个边的重要性。使用这种近似，我们剪枝网络中最不重要的边。我们对这种方法的性能和局限性进行了抽查，发现在所有任务上的均值AUC greater than other methods。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Metasurface-Practicality-for-B5G-Networks-AI-assisted-RIS-Planning"><a href="#Unlocking-Metasurface-Practicality-for-B5G-Networks-AI-assisted-RIS-Planning" class="headerlink" title="Unlocking Metasurface Practicality for B5G Networks: AI-assisted RIS Planning"></a>Unlocking Metasurface Practicality for B5G Networks: AI-assisted RIS Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10330">http://arxiv.org/abs/2310.10330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Encinas-Lago, Antonio Albanese, Vincenzo Sciancalepore, Marco Di Renzo, Xavier Costa-Pérez</li>
<li>for: 本文旨在探讨如何使用可编程智能表面（RIS）来提高无线网络性能，尤其是在 beyond-fifth-generation 网络（B5G）中。</li>
<li>methods: 本文使用深度逆向学习（DRL）算法，训练一个 DRL 代理，以便优化 RIS 的投放。</li>
<li>results: 本文在法国雷恩火车站的indoor场景中进行了实验，并证明了对于无法覆盖的区域，D-RISA 算法可以提供更好的覆盖率（10 dB 的最小信号噪听比提高），同时具有更低的计算时间（下降至 -25%）和更好的扩展性。<details>
<summary>Abstract</summary>
The advent of reconfigurable intelligent surfaces(RISs) brings along significant improvements for wireless technology on the verge of beyond-fifth-generation networks (B5G).The proven flexibility in influencing the propagation environment opens up the possibility of programmatically altering the wireless channel to the advantage of network designers, enabling the exploitation of higher-frequency bands for superior throughput overcoming the challenging electromagnetic (EM) propagation properties at these frequency bands.   However, RISs are not magic bullets. Their employment comes with significant complexity, requiring ad-hoc deployments and management operations to come to fruition. In this paper, we tackle the open problem of bringing RISs to the field, focusing on areas with little or no coverage. In fact, we present a first-of-its-kind deep reinforcement learning (DRL) solution, dubbed as D-RISA, which trains a DRL agent and, in turn, obtain san optimal RIS deployment. We validate our framework in the indoor scenario of the Rennes railway station in France, assessing the performance of our algorithm against state-of-the-art (SOA) approaches. Our benchmarks showcase better coverage, i.e., 10-dB increase in minimum signal-to-noise ratio (SNR), at lower computational time (up to -25 percent) while improving scalability towards denser network deployments.
</details>
<details>
<summary>摘要</summary>
随着智能重新配置表面（RIS）的出现， fifth-generation wireless networks（B5G）的技术将得到显著改进。 RIS 的可变性使得网络设计者可以通过程序控制无线通信环境，从而在高频段上获得更高的吞吐量，并且可以超越高频段的电磁波媒体传输性的挑战。 然而， RIS 不是一个魔术药丸。它的使用需要适当的部署和管理操作，以便实现。在这篇论文中，我们解决了将 RIS 引入到实际应用中的开放问题，特别是在有少量或无覆盖的地区。我们提出了一种首先的深度优化学（DRL）解决方案，称为 D-RISA，它在 RIS 部署方面进行优化。我们在法国雷恩火车站的indoorenario中验证了我们的框架，并与现有的方法进行比较。我们的标准显示了更好的覆盖率，即10dB的最小信号响应比（SNR）的提高，同时减少计算时间（下降至25%），并改善了网络部署的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Exploiting-Functional-Specialization-in-Multi-Head-Attention-under-Multi-task-Learning"><a href="#Interpreting-and-Exploiting-Functional-Specialization-in-Multi-Head-Attention-under-Multi-task-Learning" class="headerlink" title="Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning"></a>Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10318">http://arxiv.org/abs/2310.10318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/znlp/functionalspecializationinmha">https://github.com/znlp/functionalspecializationinmha</a></li>
<li>paper_authors: Chong Li, Shaonan Wang, Yunhao Zhang, Jiajun Zhang, Chengqing Zong</li>
<li>for: 这paper aims to investigate the functional specialization of multi-head attention in transformer-based models under multi-task learning.</li>
<li>methods: The authors propose an interpreting method to quantify the degree of functional specialization in multi-head attention and a simple multi-task training method to increase functional specialization and mitigate negative information transfer.</li>
<li>results: Experimental results on seven pre-trained transformer models demonstrate that multi-head attention evolves functional specialization after multi-task training, which is affected by the similarity of tasks. The proposed multi-task training strategy based on functional specialization boosts performance in both multi-task learning and transfer learning without adding any parameters.<details>
<summary>Abstract</summary>
Transformer-based models, even though achieving super-human performance on several downstream tasks, are often regarded as a black box and used as a whole. It is still unclear what mechanisms they have learned, especially their core module: multi-head attention. Inspired by functional specialization in the human brain, which helps to efficiently handle multiple tasks, this work attempts to figure out whether the multi-head attention module will evolve similar function separation under multi-tasking training. If it is, can this mechanism further improve the model performance? To investigate these questions, we introduce an interpreting method to quantify the degree of functional specialization in multi-head attention. We further propose a simple multi-task training method to increase functional specialization and mitigate negative information transfer in multi-task learning. Experimental results on seven pre-trained transformer models have demonstrated that multi-head attention does evolve functional specialization phenomenon after multi-task training which is affected by the similarity of tasks. Moreover, the multi-task training strategy based on functional specialization boosts performance in both multi-task learning and transfer learning without adding any parameters.
</details>
<details>
<summary>摘要</summary>
transformer-based模型，即使达到了人类超常表现，常被视为黑盒子，无法了解它们学习的机制，尤其是核心模块：多头注意力。我们受到人脑功能特化的启发，人脑可以有效地处理多个任务，因此我们想知道多头注意力模块是否会在多任务训练中发展类似的功能分化现象。如果是，那么这种机制可能会进一步提高模型性能吗？为了回答这些问题，我们提出了一种量化多头注意力中函数特化程度的解释方法。此外，我们还提出了一种简单的多任务训练方法，可以增强功能特化并降低多任务学习中的负信息传递。实验结果表明，在七种预训练transformer模型中，多头注意力会在多任务训练后发展功能分化现象，这种现象受到任务相似度的影响。此外，基于功能特化的多任务训练策略可以提高多任务学习和传播学习的性能，无需添加参数。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Offline-Reinforcement-Learning-for-Glycemia-Control"><a href="#End-to-end-Offline-Reinforcement-Learning-for-Glycemia-Control" class="headerlink" title="End-to-end Offline Reinforcement Learning for Glycemia Control"></a>End-to-end Offline Reinforcement Learning for Glycemia Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10312">http://arxiv.org/abs/2310.10312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tristan Beolet, Alice Adenis, Erik Huneker, Maxime Louis</li>
<li>for: 这项研究旨在提高closed-loop系统的糖尿病控制性能，并使其更适应不同情况。</li>
<li>methods: 该研究使用了RL算法，并开发了一个端到端个性化管道，以 removing the need for a simulator while still enabling the estimation of clinically relevant metrics for diabetes。</li>
<li>results: 研究表明，使用RL算法和个性化管道可以提高closed-loop系统的糖尿病控制性能，并减少了风险的存在。<details>
<summary>Abstract</summary>
The development of closed-loop systems for glycemia control in type I diabetes relies heavily on simulated patients. Improving the performances and adaptability of these close-loops raises the risk of over-fitting the simulator. This may have dire consequences, especially in unusual cases which were not faithfully-if at all-captured by the simulator. To address this, we propose to use offline RL agents, trained on real patient data, to perform the glycemia control. To further improve the performances, we propose an end-to-end personalization pipeline, which leverages offline-policy evaluation methods to remove altogether the need of a simulator, while still enabling an estimation of clinically relevant metrics for diabetes.
</details>
<details>
<summary>摘要</summary>
开发闭环系统控制型一 диа베ت斯需要启用模拟患者。提高这些闭环的性能和适应性可能会增加过拟合模拟器的风险，特别是在不常见的情况下。为解决这个问题，我们提议使用线上RL代理，在真实患者数据上训练，来实现血糖控制。此外，我们还提议一个终端个性化管道，利用线上策略评估方法来完全排除模拟器的需求，同时仍能估计临床重要指标。
</details></li>
</ul>
<hr>
<h2 id="Learning-visual-based-deformable-object-rearrangement-with-local-graph-neural-networks"><a href="#Learning-visual-based-deformable-object-rearrangement-with-local-graph-neural-networks" class="headerlink" title="Learning visual-based deformable object rearrangement with local graph neural networks"></a>Learning visual-based deformable object rearrangement with local graph neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10307">http://arxiv.org/abs/2310.10307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dengyh16code/deformable-gnn">https://github.com/dengyh16code/deformable-gnn</a></li>
<li>paper_authors: Yuhong Deng, Xueqian Wang, Lipeng chen</li>
<li>for: 本研究旨在解决机器人对弹性物体（如绳子和布）的重新排序问题，使用只有视觉观察的情况下，将弹性物体转换到预先设定的目标配置。</li>
<li>methods: 本研究提出了一种新的表示策略，可以快速和有效地表示弹性物体的状态，并且可以模型弹性重新排序动态。本研究还提出了一种本地图 neural network（GNN），用于同时学习弹性重新排序动态和掌握最佳抓取和放置动作。</li>
<li>results: 对多种弹性重新排序任务进行了仿真和实验，结果显示，提出的动态图表示方法可以更高效地模型弹性重新排序动态，并且在多任务学习和实际应用中表现出色。<details>
<summary>Abstract</summary>
Goal-conditioned rearrangement of deformable objects (e.g. straightening a rope and folding a cloth) is one of the most common deformable manipulation tasks, where the robot needs to rearrange a deformable object into a prescribed goal configuration with only visual observations. These tasks are typically confronted with two main challenges: the high dimensionality of deformable configuration space and the underlying complexity, nonlinearity and uncertainty inherent in deformable dynamics. To address these challenges, we propose a novel representation strategy that can efficiently model the deformable object states with a set of keypoints and their interactions. We further propose local-graph neural network (GNN), a light local GNN learning to jointly model the deformable rearrangement dynamics and infer the optimal manipulation actions (e.g. pick and place) by constructing and updating two dynamic graphs. Both simulated and real experiments have been conducted to demonstrate that the proposed dynamic graph representation shows superior expressiveness in modeling deformable rearrangement dynamics. Our method reaches much higher success rates on a variety of deformable rearrangement tasks (96.3% on average) than state-of-the-art method in simulation experiments. Besides, our method is much more lighter and has a 60% shorter inference time than state-of-the-art methods. We also demonstrate that our method performs well in the multi-task learning scenario and can be transferred to real-world applications with an average success rate of 95% by solely fine tuning a keypoint detector.
</details>
<details>
<summary>摘要</summary>
goal-conditioned 重新排序的软体 объек� (例如，整 Straightening a rope and folding a cloth) 是最常见的软体 manipulation 任务之一， robot需要根据视觉观察来重新排序软体 объек� 到预定的目标配置中。这些任务通常面临两个主要挑战：一是软体配置空间的维度太高，二是软体动力学的内置复杂性、非线性和不确定性。为了解决这些挑战，我们提出了一种新的表示策略，可以有效地模型软体 объек� 的状态，并且通过建立和更新两个动态图来模型软体重新排序动力学。我们还提出了一种本地图神经网络（GNN），可以同时模型软体重新排序动力学和推理最佳抓取和放置动作。我们在实验中发现，我们的动态图表示方法可以更高效地模型软体重新排序动力学，并且在多种软体重新排序任务上达到96.3%的Success rate（在实验中）。此外，我们的方法比现状态技术更轻量级，并且在推理时间方面具有60%的缩短。此外，我们还证明了我们的方法在多任务学习场景下表现良好，可以通过精细调整一个关键点探测器来转移到实际应用中。
</details></li>
</ul>
<hr>
<h2 id="Forking-Uncertainties-Reliable-Prediction-and-Model-Predictive-Control-with-Sequence-Models-via-Conformal-Risk-Control"><a href="#Forking-Uncertainties-Reliable-Prediction-and-Model-Predictive-Control-with-Sequence-Models-via-Conformal-Risk-Control" class="headerlink" title="Forking Uncertainties: Reliable Prediction and Model Predictive Control with Sequence Models via Conformal Risk Control"></a>Forking Uncertainties: Reliable Prediction and Model Predictive Control with Sequence Models via Conformal Risk Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10299">http://arxiv.org/abs/2310.10299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Zecchin, Sangwoo Park, Osvaldo Simeone</li>
<li>for: 本文旨在提供一种基于 probabilistic implicit or explicit sequence model 的预测 uncertainty 管理方法，以便在具有复杂动力学和分支轨迹的 cyber-physical systems 中提供可靠性和安全性 garanties。</li>
<li>methods: 本文提出了一种基于 ensemble 的多板轨迹预测（PTS-CRC）方法，该方法可以跨多个预测器生成可靠的预测集，以捕捉异常轨迹的不确定性。此外，PTS-CRC 还可以满足非覆盖性定义的可靠性要求，这使得可以实现更加高效的控制策略。</li>
<li>results: 实验结果表明，PTS-CRC 预测器可以提供更加信息归一化的预测集，以及安全性和质量的控制策略，并且在无线网络中的多种任务中都表现出了更高的返回。<details>
<summary>Abstract</summary>
In many real-world problems, predictions are leveraged to monitor and control cyber-physical systems, demanding guarantees on the satisfaction of reliability and safety requirements. However, predictions are inherently uncertain, and managing prediction uncertainty presents significant challenges in environments characterized by complex dynamics and forking trajectories. In this work, we assume access to a pre-designed probabilistic implicit or explicit sequence model, which may have been obtained using model-based or model-free methods. We introduce probabilistic time series-conformal risk prediction (PTS-CRC), a novel post-hoc calibration procedure that operates on the predictions produced by any pre-designed probabilistic forecaster to yield reliable error bars. In contrast to existing art, PTS-CRC produces predictive sets based on an ensemble of multiple prototype trajectories sampled from the sequence model, supporting the efficient representation of forking uncertainties. Furthermore, unlike the state of the art, PTS-CRC can satisfy reliability definitions beyond coverage. This property is leveraged to devise a novel model predictive control (MPC) framework that addresses open-loop and closed-loop control problems under general average constraints on the quality or safety of the control policy. We experimentally validate the performance of PTS-CRC prediction and control by studying a number of use cases in the context of wireless networking. Across all the considered tasks, PTS-CRC predictors are shown to provide more informative predictive sets, as well as safe control policies with larger returns.
</details>
<details>
<summary>摘要</summary>
在许多实际问题中，预测被用来监控和控制电脑物理系统，需要保证可靠性和安全性要求的满足。然而，预测本身具有不确定性，在复杂动态环境中管理预测不确定性呈现出 significanthallenges。在这项工作中，我们假设有一个预先设计的 probabilistic implicit or explicit sequence model，可能通过模型基于或模型自由方法获得。我们介绍了一种新的后期加拟程序，即 probablistic time series-conformal risk prediction (PTS-CRC)，可以在任何预先设计的 probabilistic forecaster 的预测结果上进行后期加拟，以生成可靠的误差范围。与现有艺术 differencely，PTS-CRC 生成基于多个原型轨迹样本集的预测集，支持高效地表示分支不确定性。此外，PTS-CRC 可以满足超过覆盖率的可靠性定义，这种特性被利用来开发一种基于 average constraints 的新的模型预测控制 (MPC) 框架，可以解决一般平均约束下的开 loop 和关 loop 控制问题。我们通过研究无线网络上的一些用例， validate 了 PTS-CRC 预测和控制的性能。在所有考虑的任务中，PTS-CRC 预测器被证明提供更加有用的预测集，以及安全的控制策略与更大的回报。
</details></li>
</ul>
<hr>
<h2 id="Key-phrase-boosted-unsupervised-summary-generation-for-FinTech-organization"><a href="#Key-phrase-boosted-unsupervised-summary-generation-for-FinTech-organization" class="headerlink" title="Key-phrase boosted unsupervised summary generation for FinTech organization"></a>Key-phrase boosted unsupervised summary generation for FinTech organization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10294">http://arxiv.org/abs/2310.10294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aadit Deshpande, Shreya Goyal, Prateek Nagwanshi, Avinash Tripathy</li>
<li>for: 这篇论文的目的是提出一种基于Action-Object对的自动生成社交媒体摘要方法，以帮助金融科技公司更好地利用社交媒体语言数据，并提供一个外部视角来对consumer behavior进行分析。</li>
<li>methods: 这篇论文使用了NLP技术，特别是意向检测、情感分类和文本概要生成等应用，以处理社交媒体语言数据。它还提出了一种基于Action-Object对的自动生成社交媒体摘要方法，以增强对社交媒体语言数据的分析和利用。</li>
<li>results: 该论文通过对Reddit讨论串的社交媒体语言数据进行分析，并对基于Action-Object对的摘要方法进行评估，并证明了该方法的效果。具体来说，该论文在Context Metrics上表现出了显著的优势，包括Unique words、Action-Object对和名称块的数量。<details>
<summary>Abstract</summary>
With the recent advances in social media, the use of NLP techniques in social media data analysis has become an emerging research direction. Business organizations can particularly benefit from such an analysis of social media discourse, providing an external perspective on consumer behavior. Some of the NLP applications such as intent detection, sentiment classification, text summarization can help FinTech organizations to utilize the social media language data to find useful external insights and can be further utilized for downstream NLP tasks. Particularly, a summary which highlights the intents and sentiments of the users can be very useful for these organizations to get an external perspective. This external perspective can help organizations to better manage their products, offers, promotional campaigns, etc. However, certain challenges, such as a lack of labeled domain-specific datasets impede further exploration of these tasks in the FinTech domain. To overcome these challenges, we design an unsupervised phrase-based summary generation from social media data, using 'Action-Object' pairs (intent phrases). We evaluated the proposed method with other key-phrase based summary generation methods in the direction of contextual information of various Reddit discussion threads, available in the different summaries. We introduce certain "Context Metrics" such as the number of Unique words, Action-Object pairs, and Noun chunks to evaluate the contextual information retrieved from the source text in these phrase-based summaries. We demonstrate that our methods significantly outperform the baseline on these metrics, thus providing a qualitative and quantitative measure of their efficacy. Proposed framework has been leveraged as a web utility portal hosted within Amex.
</details>
<details>
<summary>摘要</summary>
One of the key challenges in exploring these tasks in the FinTech domain is the lack of labeled domain-specific datasets. To overcome this challenge, we have designed an unsupervised phrase-based summary generation method from social media data using 'Action-Object' pairs (intent phrases). We evaluated our method against other key-phrase based summary generation methods in terms of contextual information retrieved from the source text.To evaluate the contextual information, we introduced certain "Context Metrics" such as the number of unique words, Action-Object pairs, and noun chunks. Our method significantly outperformed the baseline on these metrics, providing a qualitative and quantitative measure of its efficacy. This framework has been leveraged as a web utility portal hosted within Amex.
</details></li>
</ul>
<hr>
<h2 id="No-Compromise-in-Solution-Quality-Speeding-Up-Belief-dependent-Continuous-POMDPs-via-Adaptive-Multilevel-Simplification"><a href="#No-Compromise-in-Solution-Quality-Speeding-Up-Belief-dependent-Continuous-POMDPs-via-Adaptive-Multilevel-Simplification" class="headerlink" title="No Compromise in Solution Quality: Speeding Up Belief-dependent Continuous POMDPs via Adaptive Multilevel Simplification"></a>No Compromise in Solution Quality: Speeding Up Belief-dependent Continuous POMDPs via Adaptive Multilevel Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10274">http://arxiv.org/abs/2310.10274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrey Zhitnikov, Ori Sztyglic, Vadim Indelman</li>
<li>for: 这篇论文是关于Continuous POMDPs with general belief-dependent rewards的解决方案。</li>
<li>methods: 本论文使用了一种名为“adaptive multilevel simplification”的方法，具体来说是在给定的信念树和MCTS的基础上实现POMDP的线上规划。这种方法可以快速加速POMDP的规划，而不会失去解决方案的质量。</li>
<li>results: 本论文提出了三种算法来加速Continuous POMDP的规划，其中两种算法（SITH-BSP和LAZY-SITH-BSP）可以在任何信念树构建方法上使用，第三种算法（SITH-PFT）是一种可以适应任何探索技术的任何时间MCTS方法。所有这些算法都能够返回与未加速的算法相同的优化的动作。此外，本论文还提出了一种新的信息论 reward的代价计算方法，该方法可以轻松计算，并且可以通过需求的精细化来紧张化。<details>
<summary>Abstract</summary>
Continuous POMDPs with general belief-dependent rewards are notoriously difficult to solve online. In this paper, we present a complete provable theory of adaptive multilevel simplification for the setting of a given externally constructed belief tree and MCTS that constructs the belief tree on the fly using an exploration technique. Our theory allows to accelerate POMDP planning with belief-dependent rewards without any sacrifice in the quality of the obtained solution. We rigorously prove each theoretical claim in the proposed unified theory. Using the general theoretical results, we present three algorithms to accelerate continuous POMDP online planning with belief-dependent rewards. Our two algorithms, SITH-BSP and LAZY-SITH-BSP, can be utilized on top of any method that constructs a belief tree externally. The third algorithm, SITH-PFT, is an anytime MCTS method that permits to plug-in any exploration technique. All our methods are guaranteed to return exactly the same optimal action as their unsimplified equivalents. We replace the costly computation of information-theoretic rewards with novel adaptive upper and lower bounds which we derive in this paper, and are of independent interest. We show that they are easy to calculate and can be tightened by the demand of our algorithms. Our approach is general; namely, any bounds that monotonically converge to the reward can be easily plugged-in to achieve significant speedup without any loss in performance. Our theory and algorithms support the challenging setting of continuous states, actions, and observations. The beliefs can be parametric or general and represented by weighted particles. We demonstrate in simulation a significant speedup in planning compared to baseline approaches with guaranteed identical performance.
</details>
<details>
<summary>摘要</summary>
CONTINUOUS POMDPs WITH GENERAL BELIEF-DEPENDENT REWARDS ARE DIFFICULT TO SOLVE ONLINE. IN THIS PAPER, WE PRESENT A COMPLETE PROVABLE THEORY OF ADAPTIVE MULTILEVEL SIMPLIFICATION FOR THE SETTING OF A GIVEN EXTERNALLY CONSTRUCTED BELIEF TREE AND MCTS THAT CONSTRUCTS THE BELIEF TREE ON THE FLY USING AN EXPLORATION TECHNIQUE. OUR THEORY ALLOWS FOR ACCELERATING POMDP PLANNING WITH BELIEF-DEPENDENT REWARDS WITHOUT ANY SACRIFICE IN THE QUALITY OF THE OBTAINED SOLUTION. WE RIGOROUSLY PROVE EACH THEORETICAL CLAIM IN THE PROPOSED UNIFIED THEORY. USING THE GENERAL THEORETICAL RESULTS, WE PRESENT THREE ALGORITHMS TO ACCELERATE CONTINUOUS POMDP ONLINE PLANNING WITH BELIEF-DEPENDENT REWARDS. OUR TWO ALGORITHMS, SITH-BSP AND LAZY-SITH-BSP, CAN BE UTILIZED ON TOP OF ANY METHOD THAT CONSTRUCTS A BELIEF TREE EXTERNALLY. THE THIRD ALGORITHM, SITH-PFT, IS ANYTIME MCTS METHOD THAT PERMITS TO PLUG-IN ANY EXPLORATION TECHNIQUE. ALL OUR METHODS ARE GUARANTEED TO RETURN THE SAME OPTIMAL ACTION AS THEIR UNSIMPLIFIED EQUIVALENTS. WE REPLACE THE COSTLY COMPUTATION OF INFORMATION-THEORETIC REWARDS WITH NOVEL ADAPTIVE UPPER AND LOWER BOUNDS WHICH WE DERIVE IN THIS PAPER, AND ARE OF INDEPENDENT INTEREST. WE SHOW THAT THEY ARE EASY TO CALCULATE AND CAN BE TIGHTENED BY THE DEMAND OF OUR ALGORITHMS. OUR APPROACH IS GENERAL; NAMELY, ANY BOUNDS THAT MONOTONICALLY CONVERGE TO THE REWARD CAN BE EASILY PLUGGED-IN TO ACHIEVE SIGNIFICANT SPEEDUP WITHOUT ANY LOSS IN PERFORMANCE. OUR THEORY AND ALGORITHMS SUPPORT THE CHALLENGING SETTING OF CONTINUOUS STATES, ACTIONS, AND OBSERVATIONS. THE BELIEFS CAN BE PARAMETRIC OR GENERAL AND REPRESENTED BY WEIGHTED PARTICLES. WE DEMONSTRATE IN SIMULATION A SIGNIFICANT SPEEDUP IN PLANNING COMPARED TO BASELINE APPROACHES WITH GUARANTEED IDENTICAL PERFORMANCE.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Financial-Service-Promotion-With-Hybrid-Recommender-Systems-at-PicPay"><a href="#Rethinking-Financial-Service-Promotion-With-Hybrid-Recommender-Systems-at-PicPay" class="headerlink" title="Rethinking Financial Service Promotion With Hybrid Recommender Systems at PicPay"></a>Rethinking Financial Service Promotion With Hybrid Recommender Systems at PicPay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10268">http://arxiv.org/abs/2310.10268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Mendonça, Matheus Santos, André Gonçalves, Yan Almeida</li>
<li>for: 这个研究是为了提高PicPay的金融服务推荐效果。</li>
<li>methods: 这个研究使用了两种推荐算法，Switching Hybrid Recommender System，以提高item推荐的效果。</li>
<li>results: 我们的A&#x2F;B测试显示， Switching Hybrid Recommender System可以提高推荐效果，比默认推荐策略提高3.2%。<details>
<summary>Abstract</summary>
The fintech PicPay offers a wide range of financial services to its 30 million monthly active users, with more than 50 thousand items recommended in the PicPay mobile app. In this scenario, promoting specific items that are strategic to the company can be very challenging. In this work, we present a Switching Hybrid Recommender System that combines two algorithms to effectively promote items without negatively impacting the user's experience. The results of our A/B tests show an uplift of up to 3.2\% when compared to a default recommendation strategy.
</details>
<details>
<summary>摘要</summary>
picPay 提供了一个广泛的金融服务，每月活跃用户达3000万人，app中推荐的商品超过50000个。在这种情况下，推荐特定的商品可以非常具有挑战性。在这份工作中，我们提出了一种交换 гибрид推荐系统，将两种算法结合使用，以有效地推荐商品，不对用户体验造成负面影响。我们的A/B测试结果显示，与默认推荐策略相比，我们的推荐策略可以提高用户增长率高达3.2%。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Arabic-Legal-Rulings-using-Large-Language-Models"><a href="#Prediction-of-Arabic-Legal-Rulings-using-Large-Language-Models" class="headerlink" title="Prediction of Arabic Legal Rulings using Large Language Models"></a>Prediction of Arabic Legal Rulings using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10260">http://arxiv.org/abs/2310.10260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Ammar, Anis Koubaa, Bilel Benjdira, Omar Najar, Serry Sibaee</li>
<li>for: 这研究旨在预测阿拉伯语法庭决定，帮助法官做出决策和帮助律师采取更加细化的战略。</li>
<li>methods: 本研究使用了当前state-of-the-art的大语言模型进行预测，包括LLaMA-7b、JAIS-13b和GPT3.5-turbo等三种基础模型，并使用了三种训练方法：零例学习、一例学习和特定精度调整。此外，研究还评估了对原始阿拉伯文输入文本的摘要和&#x2F;或翻译的效果。</li>
<li>results: 研究发现，所有LLaMA模型的性能很差，而GPT-3.5基于模型在所有模型中表现出色，比其他模型的平均分数高出50%。此外，研究还发现，除了人类评估外，其他所有的评估方法都不可靠，无法正确评估大语言模型在法庭决定预测中的性能。<details>
<summary>Abstract</summary>
In the intricate field of legal studies, the analysis of court decisions is a cornerstone for the effective functioning of the judicial system. The ability to predict court outcomes helps judges during the decision-making process and equips lawyers with invaluable insights, enhancing their strategic approaches to cases. Despite its significance, the domain of Arabic court analysis remains under-explored. This paper pioneers a comprehensive predictive analysis of Arabic court decisions on a dataset of 10,813 commercial court real cases, leveraging the advanced capabilities of the current state-of-the-art large language models. Through a systematic exploration, we evaluate three prevalent foundational models (LLaMA-7b, JAIS-13b, and GPT3.5-turbo) and three training paradigms: zero-shot, one-shot, and tailored fine-tuning. Besides, we assess the benefit of summarizing and/or translating the original Arabic input texts. This leads to a spectrum of 14 model variants, for which we offer a granular performance assessment with a series of different metrics (human assessment, GPT evaluation, ROUGE, and BLEU scores). We show that all variants of LLaMA models yield limited performance, whereas GPT-3.5-based models outperform all other models by a wide margin, surpassing the average score of the dedicated Arabic-centric JAIS model by 50%. Furthermore, we show that all scores except human evaluation are inconsistent and unreliable for assessing the performance of large language models on court decision predictions. This study paves the way for future research, bridging the gap between computational linguistics and Arabic legal analytics.
</details>
<details>
<summary>摘要</summary>
在复杂的法律研究领域中，法庭判决分析是司法系统的基础石头。预测法庭结果可以帮助法官做出决策，并让律师获得价值的洞察，提高他们的战略方法。然而，阿拉伯语法庭分析领域仍然未得到足够的探索。这篇论文探索了10813起商业法庭案例的阿拉伯语法庭判决预测，利用当今最先进的大语言模型。通过系统性的探索，我们评估了三种基础模型（LLaMA-7b、JAIS-13b和GPT3.5-turbo）和三种训练方法（零shot、一shot和tailored fine-tuning）。此外，我们还评估了原始阿拉伯语输入文本的摘要和/或翻译是否有利。这导致了14种模型变体，我们为它们提供了细腻的性能评估，包括人类评估、GPT评估、ROUGE和BLEU分数。我们发现所有LLaMA模型的表现很有限，而GPT-3.5基于模型在所有其他模型之上占据了很大优势，超过了特化于阿拉伯语的JAIS模型的平均分数 by 50%。此外，我们发现除人类评估外，所有其他分数都是不可靠和不一致的，这些分数不适用于评估大语言模型在法庭判决预测中的表现。这篇研究为未来的研究提供了桥梁，将计算语言学和阿拉伯语法律分析相连。
</details></li>
</ul>
<hr>
<h2 id="SGOOD-Substructure-enhanced-Graph-Level-Out-of-Distribution-Detection"><a href="#SGOOD-Substructure-enhanced-Graph-Level-Out-of-Distribution-Detection" class="headerlink" title="SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection"></a>SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10237">http://arxiv.org/abs/2310.10237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Ding, Jieming Shi</li>
<li>for: 本研究旨在提高图像分类中的非标准图像检测性能，即在未知数据分布下检测图像是否属于标准分布或者非标准分布。</li>
<li>methods: 本研究提出了一种基于子结构的图像分类方法，包括建立超Graph的子结构、设计两级图像编码管道以及开发三种图像增强技术来增强表达力。</li>
<li>results: 对10种竞争者进行了广泛的实验，常常超过现有方法，并且在许多图像 Dataset 上表现出了显著的优势。<details>
<summary>Abstract</summary>
Graph-level representation learning is important in a wide range of applications. However, existing graph-level models are generally built on i.i.d. assumption for both training and testing graphs, which is not realistic in an open world, where models can encounter out-of-distribution (OOD) testing graphs that are from different distributions unknown during training. A trustworthy model should not only produce accurate predictions for in-distribution (ID) data, but also detect OOD graphs to avoid unreliable prediction. In this paper, we present SGOOD, a novel graph-level OOD detection framework. We find that substructure differences commonly exist between ID and OOD graphs. Hence, SGOOD explicitly utilizes substructures to learn powerful representations to achieve superior performance. Specifically, we build a super graph of substructures for every graph, and design a two-level graph encoding pipeline that works on both original graphs and super graphs to obtain substructure-enhanced graph representations. To further distinguish ID and OOD graphs, we develop three graph augmentation techniques that preserve substructures and increase expressiveness. Extensive experiments against 10 competitors on numerous graph datasets demonstrate the superiority of SGOOD, often surpassing existing methods by a significant margin. The code is available at https://anonymous.4open.science/r/SGOOD-0958.
</details>
<details>
<summary>摘要</summary>
GRAPH-LEVEL REPRESENTATION LEARNING 是在各种应用中非常重要。然而，现有的 GRAPH-LEVEL 模型通常是基于 i.i.d. 假设，即训练和测试 GRAPH 都是同一个分布，这并不是现实世界中的开放世界， где模型可能会遇到不同分布的测试 GRAPH。一个可靠的模型不仅需要在 ID 数据上生成准确的预测，还需要检测 OOD  GRAPH 以避免不可靠的预测。在这篇论文中，我们提出了 SGOOD，一种新的 GRAPH-LEVEL OOD 检测框架。我们发现了 ID 和 OOD  GRAPH 之间的结构差异，因此 SGOOD 使用substructure来学习强大的表示。具体来说，我们建立了每个 GRAPH 的超graph，并设计了两级图编码管道，以便在原始 GRAPH 和超graph 上获得增强的图表示。为了进一步分别 ID 和 OOD GRAPH，我们开发了三种图增强技术，以保持substructure并提高表达能力。我们对10个竞争对手的实验结果表明，SGOOD 常常超过现有方法，准确率高于95%。代码可以在 <https://anonymous.4open.science/r/SGOOD-0958> 获取。
</details></li>
</ul>
<hr>
<h2 id="Using-Global-Land-Cover-Product-as-Prompt-for-Cropland-Mapping-via-Visual-Foundation-Model"><a href="#Using-Global-Land-Cover-Product-as-Prompt-for-Cropland-Mapping-via-Visual-Foundation-Model" class="headerlink" title="Using Global Land Cover Product as Prompt for Cropland Mapping via Visual Foundation Model"></a>Using Global Land Cover Product as Prompt for Cropland Mapping via Visual Foundation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10219">http://arxiv.org/abs/2310.10219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Tao, Aoran Hu, Rong Xiao, Haifeng Li, Yuze Wang</li>
<li>for: 本研究旨在解决受不同场景Attribute和取景条件影响的 cropland 映射问题，通过提出 “Pretrain+Prompting” 方法，以便在模型理解过程中简化领域适应。</li>
<li>methods: 本研究使用了可访问的全球土地覆盖产品，设计了自动提示（APT）方法，通过在模型推理过程中引入各个示例的个性提示，实现了细化的领域适应过程。</li>
<li>results: 根据两个SUB-METER cropland 数据集的实验结果，提出的 “Pretrain+Prompting” 方法在Remote sensing 领域的 cropland 映射问题中表现了比传统supervised learning和精度调整方法更高的性能。<details>
<summary>Abstract</summary>
Data-driven deep learning methods have shown great potential in cropland mapping. However, due to multiple factors such as attributes of cropland (topography, climate, crop type) and imaging conditions (viewing angle, illumination, scale), croplands under different scenes demonstrate a great domain gap. This makes it difficult for models trained in the specific scenes to directly generalize to other scenes. A common way to handle this problem is through the "Pretrain+Fine-tuning" paradigm. Unfortunately, considering the variety of features of cropland that are affected by multiple factors, it is hardly to handle the complex domain gap between pre-trained data and target data using only sparse fine-tuned samples as general constraints. Moreover, as the number of model parameters grows, fine-tuning is no longer an easy and low-cost task. With the emergence of prompt learning via visual foundation models, the "Pretrain+Prompting" paradigm redesigns the optimization target by introducing individual prompts for each single sample. This simplifies the domain adaption from generic to specific scenes during model reasoning processes. Therefore, we introduce the "Pretrain+Prompting" paradigm to interpreting cropland scenes and design the auto-prompting (APT) method based on freely available global land cover product. It can achieve a fine-grained adaptation process from generic scenes to specialized cropland scenes without introducing additional label costs. To our best knowledge, this work pioneers the exploration of the domain adaption problems for cropland mapping under prompt learning perspectives. Our experiments using two sub-meter cropland datasets from southern and northern China demonstrated that the proposed method via visual foundation models outperforms traditional supervised learning and fine-tuning approaches in the field of remote sensing.
</details>
<details>
<summary>摘要</summary>
“数据驱动深度学习方法在耕地地图中表现出了很大的潜力。然而，由于耕地特性（地形、气候、作物种）以及捕获条件（观察角度、照明、比例）的多种因素，耕地不同场景之间存在巨大的领域差异。这使得使用特定场景的训练数据直接适应其他场景变得困难。通常，使用“Pretrain+Fine-tuning”模式来解决这个问题。然而，考虑到耕地特性的多种影响，使用只有稀疏的精度适应样本作为通用约束是不充分的。此外，随着模型参数的增加，精度适应变得不是易于进行的低成本任务。随着视觉基础模型的出现，“Pretrain+Prompting”模式可以重新设定优化目标，通过引入每个样本的个性提示来简化领域适应。因此，我们提出了基于自由可用的全球土地覆盖产品的自动提示（APT）方法，可以实现不受预先标注的场景适应过程。我们的实验使用南方和北方中国的两个半米耕地数据集证明了，与传统的超级学习和精度适应方法相比，我们的方法在远程感知领域中表现出了更好的性能。”
</details></li>
</ul>
<hr>
<h2 id="Large-Models-for-Time-Series-and-Spatio-Temporal-Data-A-Survey-and-Outlook"><a href="#Large-Models-for-Time-Series-and-Spatio-Temporal-Data-A-Survey-and-Outlook" class="headerlink" title="Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"></a>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10196">http://arxiv.org/abs/2310.10196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qingsongedu/awesome-timeseries-spatiotemporal-lm-llm">https://github.com/qingsongedu/awesome-timeseries-spatiotemporal-lm-llm</a></li>
<li>paper_authors: Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, Shirui Pan, Vincent S. Tseng, Yu Zheng, Lei Chen, Hui Xiong</li>
<li>for: 本研究主要是为了对时间序列和空间时间数据进行分析和挖掘，以便更好地利用这些数据中含的丰富信息，并为各种应用领域提供支持。</li>
<li>methods: 本研究使用大量语言和其他基础模型，对时间序列和空间时间数据进行分析和挖掘，并提供了一个完整的和最新的评论，涵盖四个关键方面：数据类型、模型类别、模型范围和应用领域&#x2F;任务。</li>
<li>results: 本研究提供了一个完整的和最新的评论，涵盖大型模型在时间序列和空间时间数据分析中的应用和发展，并提供了丰富的资源，包括数据集、模型资产和有用工具，以便开发应用和进行进一步的研究。<details>
<summary>Abstract</summary>
Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to equip practitioners with the knowledge to develop applications and further research in this underexplored domain. We primarily categorize the existing literature into two major clusters: large models for time series analysis (LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further classify research based on model scopes (i.e., general vs. domain-specific) and application areas/tasks. We also provide a comprehensive collection of pertinent resources, including datasets, model assets, and useful tools, categorized by mainstream applications. This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.
</details>
<details>
<summary>摘要</summary>
现代数据中，时序数据和空间时序数据具有广泛的应用，它们捕捉了动态系统的测量结果，并由物理和虚拟感知器生成了庞大量数据。分析这些数据类型是利用它们含义的关键，因此在多种下游任务中具有重要意义。最新的大语言和其他基础模型的发展，使得这些模型在时序数据和空间时序数据挖掘中得到广泛的应用。这些方法不仅可以在多种领域中提高模式识别和理解，而且为人工通用智能做好了准备。在本综述中，我们提供了一个完整和最新的时序数据和空间时序数据大模型综述，涵盖四个关键方面：数据类型、模型类别、模型范围和应用领域/任务。我们的目标是为实践者提供开发应用和进一步研究的知识。我们将现有文献分为两个主要群组：时序数据分析大模型（LM4TS）和空间时序数据挖掘大模型（LM4STD）。基于这两个群组，我们进一步分类研究根据模型范围（一般 vs.域特定）和应用领域/任务。此外，我们还提供了一份完整的相关资源，包括数据集、模型资产和有用工具，按照主流应用分类。这篇综述汇集了最新的大模型中心研究的进展，强调了它们的基础、当前进展、实际应用、资源储备和未来研究机遇。
</details></li>
</ul>
<hr>
<h2 id="Battle-of-the-Large-Language-Models-Dolly-vs-LLaMA-vs-Vicuna-vs-Guanaco-vs-Bard-vs-ChatGPT-–-A-Text-to-SQL-Parsing-Comparison"><a href="#Battle-of-the-Large-Language-Models-Dolly-vs-LLaMA-vs-Vicuna-vs-Guanaco-vs-Bard-vs-ChatGPT-–-A-Text-to-SQL-Parsing-Comparison" class="headerlink" title="Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT – A Text-to-SQL Parsing Comparison"></a>Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT – A Text-to-SQL Parsing Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10190">http://arxiv.org/abs/2310.10190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, Jian Su</li>
<li>for: 评估大型自然语言模型（LLM）在文本转换SQL解析方面的表现，以帮助研究人员更好地了解这些模型的实际性能。</li>
<li>methods: 对六种流行的大型自然语言模型进行系统性的评估，使用九个benchmark数据集和五种提示策略进行测试，包括零shot和几shot情况。</li>
<li>results: 发现开源模型在文本转换SQL解析方面的性能落后于关闭源模型如GPT-3.5，表明需要进一步的研究以减少这些模型之间的性能差距。<details>
<summary>Abstract</summary>
The success of ChatGPT has ignited an AI race, with researchers striving to develop new large language models (LLMs) that can match or surpass the language understanding and generation abilities of commercial ones. In recent times, a number of models have emerged, claiming performance near that of GPT-3.5 or GPT-4 through various instruction-tuning methods. As practitioners of Text-to-SQL parsing, we are grateful for their valuable contributions to open-source research. However, it is important to approach these claims with a sense of scrutiny and ascertain the actual effectiveness of these models. Therefore, we pit six popular large language models against each other, systematically evaluating their Text-to-SQL parsing capability on nine benchmark datasets with five different prompting strategies, covering both zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell significantly short of the performance achieved by closed-source models like GPT-3.5, highlighting the need for further work to bridge the performance gap between these models.
</details>
<details>
<summary>摘要</summary>
成功的ChatGPT引燃了一场AI竞赛，研究人员努力开发新的大型自然语言模型（LLMs），以达到或超越商业模型的语言理解和生成能力。最近，一些模型宣称在不同的指导方法下达到GPT-3.5或GPT-4的性能。作为文本转SQL解析的实践者，我们感谢这些开源研究的有价值贡献。然而，我们应该对这些声明进行严格的评估，以确定这些模型的实际效果。因此，我们将六种流行的大型自然语言模型进行比较测试，系统地评估这些模型在九个benchmark datasets上的文本转SQL解析能力，使用五种不同的提示策略，包括零shot和几shot场景。惜亏，开源的模型在closed-source模型GPT-3.5的性能上表现出了明显的差距，这 highlights the need for further work to bridge the performance gap between these models。
</details></li>
</ul>
<hr>
<h2 id="Continual-Generalized-Intent-Discovery-Marching-Towards-Dynamic-and-Open-world-Intent-Recognition"><a href="#Continual-Generalized-Intent-Discovery-Marching-Towards-Dynamic-and-Open-world-Intent-Recognition" class="headerlink" title="Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition"></a>Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10184">http://arxiv.org/abs/2310.10184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songxiaoshuai/CGID">https://github.com/songxiaoshuai/CGID</a></li>
<li>paper_authors: Xiaoshuai Song, Yutao Mou, Keqing He, Yueyan Qiu, Pei Wang, Weiran Xu</li>
<li>for: 这篇论文目标是解决在不同数据流中进行动态意图发现，以及在开放世界中实现动态意图识别。</li>
<li>methods: 该论文提出了一种新的任务 named Continual Generalized Intent Discovery (CGID), 它通过不断地从动态OOD数据流中发现新意图，然后逐渐将其添加到分类器中，几乎不需要之前的数据。</li>
<li>results: 论文提出了一种名为Prototype-guided Learning with Replay and Distillation (PLRD)的方法，可以实现CGID任务。该方法通过类prototype进行启动新意图发现，并通过数据重播和特征储存来保持新和旧意图的平衡。<details>
<summary>Abstract</summary>
In a practical dialogue system, users may input out-of-domain (OOD) queries. The Generalized Intent Discovery (GID) task aims to discover OOD intents from OOD queries and extend them to the in-domain (IND) classifier. However, GID only considers one stage of OOD learning, and needs to utilize the data in all previous stages for joint training, which limits its wide application in reality. In this paper, we introduce a new task, Continual Generalized Intent Discovery (CGID), which aims to continuously and automatically discover OOD intents from dynamic OOD data streams and then incrementally add them to the classifier with almost no previous data, thus moving towards dynamic intent recognition in an open world. Next, we propose a method called Prototype-guided Learning with Replay and Distillation (PLRD) for CGID, which bootstraps new intent discovery through class prototypes and balances new and old intents through data replay and feature distillation. Finally, we conduct detailed experiments and analysis to verify the effectiveness of PLRD and understand the key challenges of CGID for future research.
</details>
<details>
<summary>摘要</summary>
在实际对话系统中，用户可能输入过域 (OOD) 查询。通用意图发现 (GID) 任务目标是从 OOD 查询中发现 OOD 意图并将其扩展到内域 (IND) 分类器。但 GID 只考虑了一个阶段的 OOD 学习，需要在所有前一阶段的数据上进行联合训练，这限制了其在实际应用中的广泛应用。在本文中，我们介绍了一个新的任务：不断总结化通用意图发现 (CGID)，它目标是从动态 OOD 数据流中不断发现 OOD 意图，然后在几乎没有先前数据的情况下，逐步添加它们到分类器中，从而逐步实现动态意图认知在开放世界中。接着，我们提出了一种方法 called Prototype-guided Learning with Replay and Distillation (PLRD)，它通过类型概念引导新意向发现，并通过数据重播和特征硬化平衡新和旧意图。最后，我们进行了详细的实验和分析，以证明 PLRD 的效果和 CGID 的关键挑战，以便未来研究。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Meet-Open-World-Intent-Discovery-and-Recognition-An-Evaluation-of-ChatGPT"><a href="#Large-Language-Models-Meet-Open-World-Intent-Discovery-and-Recognition-An-Evaluation-of-ChatGPT" class="headerlink" title="Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT"></a>Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10176">http://arxiv.org/abs/2310.10176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songxiaoshuai/OOD-Evaluation">https://github.com/songxiaoshuai/OOD-Evaluation</a></li>
<li>paper_authors: Xiaoshuai Song, Keqing He, Pei Wang, Guanting Dong, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu</li>
<li>for: 本研究旨在评估ChatGPT在非预期意权（OOD）发现和总结扩展（GID）任务中的能力。</li>
<li>methods: 本研究使用了ChatGPT进行OOD意权发现和GID任务，并对其进行了评估。</li>
<li>results: ChatGPT在零shot设定下表现出了一致的优势，但与精心定制的模型相比，其仍然处于劣势。通过一系列的分析实验，本研究揭示了LLMs在扩展OOD意权时所面临的挑战，并提供了未来研究的指导。<details>
<summary>Abstract</summary>
The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extent OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.
</details>
<details>
<summary>摘要</summary>
Tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extend OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.Here's the translation in Traditional Chinese:Tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extend OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-An-After-Sales-Service-Process-Using-Object-Centric-Process-Mining-A-Case-Study"><a href="#Analyzing-An-After-Sales-Service-Process-Using-Object-Centric-Process-Mining-A-Case-Study" class="headerlink" title="Analyzing An After-Sales Service Process Using Object-Centric Process Mining: A Case Study"></a>Analyzing An After-Sales Service Process Using Object-Centric Process Mining: A Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10174">http://arxiv.org/abs/2310.10174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyunam Park, Sevde Aydin, Cuneyt Ugur, Wil M. P. van der Aalst</li>
<li>for: 本研究旨在探讨对象центри的过程挖掘技术的应用，以帮助实际操作场景中的业务进程优化。</li>
<li>methods: 本研究使用了对象центри的过程挖掘技术，通过对 approximately 65,000 个事件的分析，揭示了这种技术在实际操作场景中的应用前景和优势。</li>
<li>results: 研究发现，对象центри的过程挖掘技术可以更好地捕捉实际操作场景中的企业过程细节，提供更加全面和深入的业务进程nderstandings，帮助企业实现更好的操作优化。<details>
<summary>Abstract</summary>
Process mining, a technique turning event data into business process insights, has traditionally operated on the assumption that each event corresponds to a singular case or object. However, many real-world processes are intertwined with multiple objects, making them object-centric. This paper focuses on the emerging domain of object-centric process mining, highlighting its potential yet underexplored benefits in actual operational scenarios. Through an in-depth case study of Borusan Cat's after-sales service process, this study emphasizes the capability of object-centric process mining to capture entangled business process details. Utilizing an event log of approximately 65,000 events, our analysis underscores the importance of embracing this paradigm for richer business insights and enhanced operational improvements.
</details>
<details>
<summary>摘要</summary>
<TRANSLATE_TEXT> Process mining, a technique turning event data into business process insights, has traditionally operated on the assumption that each event corresponds to a singular case or object. However, many real-world processes are intertwined with multiple objects, making them object-centric. This paper focuses on the emerging domain of object-centric process mining, highlighting its potential yet underexplored benefits in actual operational scenarios. Through an in-depth case study of Borusan Cat's after-sales service process, this study emphasizes the capability of object-centric process mining to capture entangled business process details. Utilizing an event log of approximately 65,000 events, our analysis underscores the importance of embracing this paradigm for richer business insights and enhanced operational improvements. </TRANSLATE_TEXT> traducción al chino simplificado:<SYS><TRANSLATE_TEXT> 过程挖掘，一种将事件数据转化为业务过程智能，传统上假设每个事件对应一个单一的案例或对象。然而，现实世界中许多过程都与多个对象紧密相连，使得它们变成了中心式的。这篇论文关注到在 объекo-中心的过程挖掘领域的出现，强调其在实际运营场景中的可能尚未得到充分利用的优点。通过对博рус安猫后售服务过程的深入探讨，本研究强调了中心式过程挖掘的能力来捕捉互相紧密相连的业务过程细节。通过使用约65,000个事件的日志分析，我们的研究强调了接受这种思想的重要性，以获得更加丰富的商业智能和改进运营效率。 </TRANSLATE_TEXT>
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Knowledge-Distillation-for-Efficient-Deep-Reinforcement-Learning-in-Resource-Constrained-Environments"><a href="#Leveraging-Knowledge-Distillation-for-Efficient-Deep-Reinforcement-Learning-in-Resource-Constrained-Environments" class="headerlink" title="Leveraging Knowledge Distillation for Efficient Deep Reinforcement Learning in Resource-Constrained Environments"></a>Leveraging Knowledge Distillation for Efficient Deep Reinforcement Learning in Resource-Constrained Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10170">http://arxiv.org/abs/2310.10170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paopaolin/papercode/tree/main/MENGGUANLIN_papercode/combine%20V1">https://github.com/paopaolin/papercode/tree/main/MENGGUANLIN_papercode/combine%20V1</a></li>
<li>paper_authors: Guanlin Meng</li>
<li>for: 这篇论文旨在探索深度强化学习（DRL）与知识传播（KD）的组合，以实现将深度模型的计算负载减轻，保持性能。</li>
<li>methods: 这篇论文使用了多种DRL算法的知识传播，并研究了这些知识传播的影响。</li>
<li>results: 这篇论文的研究结果显示，通过将DRL算法与KD技术组合使用，可以开发出较快速、较具有 Computational efficiency 的DRL模型。<details>
<summary>Abstract</summary>
This paper aims to explore the potential of combining Deep Reinforcement Learning (DRL) with Knowledge Distillation (KD) by distilling various DRL algorithms and studying their distillation effects. By doing so, the computational burden of deep models could be reduced while maintaining the performance. The primary objective is to provide a benchmark for evaluating the performance of different DRL algorithms that have been refined using KD techniques. By distilling these algorithms, the goal is to develop efficient and fast DRL models. This research is expected to provide valuable insights that can facilitate further advancements in this promising direction. By exploring the combination of DRL and KD, this work aims to promote the development of models that require fewer GPU resources, learn more quickly, and make faster decisions in complex environments. The results of this research have the capacity to significantly advance the field of DRL and pave the way for the future deployment of resource-efficient, decision-making intelligent systems.
</details>
<details>
<summary>摘要</summary>
本研究旨在探索将深度束缚学习（DRL）与知识传递（KD）相结合，通过传递多种DRL算法，研究其传递效果。这可以减少深度模型的计算负担，保持性能。研究的主要目标是为不同DRL算法提供评估性能的标准准例。通过传递这些算法，目标是开发高效快速的DRL模型。这项研究预期会为这个Promising direction提供有价值的发现，促进DRL领域的进一步发展。通过探索DRL和KD的结合，这项研究期望开发需要 fewer GPU资源、快速学习、在复杂环境中做出快速决策的模型。研究结果具有提高DRL领域的前景，并为未来部署资源有效的决策智能系统铺平道路的潜在性。
</details></li>
</ul>
<hr>
<h2 id="DemoNSF-A-Multi-task-Demonstration-based-Generative-Framework-for-Noisy-Slot-Filling-Task"><a href="#DemoNSF-A-Multi-task-Demonstration-based-Generative-Framework-for-Noisy-Slot-Filling-Task" class="headerlink" title="DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task"></a>DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10169">http://arxiv.org/abs/2310.10169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongguanting/Demo-NSF">https://github.com/dongguanting/Demo-NSF</a></li>
<li>paper_authors: Guanting Dong, Tingfeng Hui, Zhuoma GongQue, Jinxu Zhao, Daichi Guo, Gang Zhao, Keqing He, Weiran Xu</li>
<li>for: 提高生成框架在实际对话场景中的泛化能力，解决它们在输入干扰时的通用化问题。</li>
<li>methods: 提出了多任务示范生成框架，名为DemoNSF，并引入了三种含有干扰信息的辅助任务，即干扰恢复（NR）、随机覆盖（RM）和混合识别（HD），以便在不同粒度上捕捉输入干扰的Semantic结构信息。</li>
<li>results: 在两个标准 benchmark 上， DemoNSF 比所有基eline方法表现出色，并实现了强大的泛化性。进一步的分析提供了生成框架在实践中的指导。<details>
<summary>Abstract</summary>
Recently, prompt-based generative frameworks have shown impressive capabilities in sequence labeling tasks. However, in practical dialogue scenarios, relying solely on simplistic templates and traditional corpora presents a challenge for these methods in generalizing to unknown input perturbations. To address this gap, we propose a multi-task demonstration based generative framework for noisy slot filling, named DemoNSF. Specifically, we introduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask (RM), and hybrid discrimination (HD), to implicitly capture semantic structural information of input perturbations at different granularities. In the downstream main task, we design a noisy demonstration construction strategy for the generative framework, which explicitly incorporates task-specific information and perturbed distribution during training and inference. Experiments on two benchmarks demonstrate that DemoNSF outperforms all baseline methods and achieves strong generalization. Further analysis provides empirical guidance for the practical application of generative frameworks. Our code is released at https://github.com/dongguanting/Demo-NSF.
</details>
<details>
<summary>摘要</summary>
最近，基于提示的生成框架在序列标注任务中表现出了很好的能力。然而，在实际对话场景中，仅仅依靠简单的模板和传统词汇库是生成方法在处理未知输入干扰时的挑战。为解决这个差距，我们提议一种多任务生成框架 для噪声插值，名为 DemoNSF。我们在这个框架中引入了三种噪声辅助任务，即噪声恢复（NR）、随机面（RM）和混合识别（HD），以隐式地捕捉输入干扰的 semantic 结构信息。在下游主任务中，我们设计了一种噪声示例建构策略，用于在训练和推断过程中显式地包含任务特定的信息和干扰分布。实验结果表明， DemoNSF 在两个标准 benchmark 上都高于所有基准方法，并且具有强大的泛化能力。进一步的分析提供了实践应用 generative 框架的指导。我们的代码在 GitHub 上发布，地址为 <https://github.com/dongguanting/Demo-NSF>。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Algorithm-for-Advanced-Level-3-Inverse-Modeling-of-Silicon-Carbide-Power-MOSFET-Devices"><a href="#Deep-Learning-Algorithm-for-Advanced-Level-3-Inverse-Modeling-of-Silicon-Carbide-Power-MOSFET-Devices" class="headerlink" title="Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices"></a>Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17657">http://arxiv.org/abs/2310.17657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimo Orazio Spata, Sebastiano Battiato, Alessandro Ortis, Francesco Rundo, Michele Calabretta, Carmelo Pino, Angelo Messina</li>
<li>for: 这个论文是为了提取力Field-Effect Transistor (SiC Power MOS)的物理参数而设计的深度学习方法。</li>
<li>methods: 该方法使用深度学习算法来训练Device的参数预测。</li>
<li>results: 实验结果表明，该方法可以有效地重构SiC Power MOS的物理参数，包括晶圆长度。<details>
<summary>Abstract</summary>
Inverse modelling with deep learning algorithms involves training deep architecture to predict device's parameters from its static behaviour. Inverse device modelling is suitable to reconstruct drifted physical parameters of devices temporally degraded or to retrieve physical configuration. There are many variables that can influence the performance of an inverse modelling method. In this work the authors propose a deep learning method trained for retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET (SiC Power MOS). The SiC devices are used in applications where classical silicon devices failed due to high-temperature or high switching capability. The key application of SiC power devices is in the automotive field (i.e. in the field of electrical vehicles). Due to physiological degradation or high-stressing environment, SiC Power MOS shows a significant drift of physical parameters which can be monitored by using inverse modelling. The aim of this work is to provide a possible deep learning-based solution for retrieving physical parameters of the SiC Power MOSFET. Preliminary results based on the retrieving of channel length of the device are reported. Channel length of power MOSFET is a key parameter involved in the static and dynamic behaviour of the device. The experimental results reported in this work confirmed the effectiveness of a multi-layer perceptron designed to retrieve this parameter.
</details>
<details>
<summary>摘要</summary>
倒推模型使用深度学习算法来训练深度架构，以预测设备的参数从其静态行为中。倒推设备模型适用于重构过时的物理参数或恢复物理配置。倒推模型的性能有很多因素的影响。在这项工作中，作者提出了一种深度学习方法，用于重 Retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET (SiC Power MOS). SiC设备在高温或高开关能力应用场景中被广泛使用，因此SiC Power MOS在高温或高压力环境中会显著偏移物理参数，这可以通过倒推模型进行监测。本工作的目标是提供一种可能的深度学习基于解决方案，以重 Retrieving physical parameters of SiC Power MOSFET。初步结果基于设备渠道长度的重构被报告。渠道长度是SiC Power MOS的静态和动态行为中关键参数之一。实验结果表明，使用多层感知器来重构这个参数是有效的。
</details></li>
</ul>
<hr>
<h2 id="Character-LLM-A-Trainable-Agent-for-Role-Playing"><a href="#Character-LLM-A-Trainable-Agent-for-Role-Playing" class="headerlink" title="Character-LLM: A Trainable Agent for Role-Playing"></a>Character-LLM: A Trainable Agent for Role-Playing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10158">http://arxiv.org/abs/2310.10158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/choosewhatulike/trainable-agents">https://github.com/choosewhatulike/trainable-agents</a></li>
<li>paper_authors: Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu</li>
<li>for: 研究是使用大型自然语言模型（LLM）作为代理人类模拟人类行为的能力。</li>
<li>methods: 我们提出了一种方法，即编辑人物profile和经验，以允许LLM模型成为特定人物。</li>
<li>results: 我们在测试场景中训练了agent并评估其能否记忆和表现出人物的特点和经验。实验结果表明了有趣的观察，有助于建立未来的人类模拟。<details>
<summary>Abstract</summary>
Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以作为代理人对人类行为进行模拟，因为它具有强大的理解人类指令和生成文本能力。这种能力让我们感到是否可以使LMLM模型模拟出更高一层的人类形式。因此，我们想要将特定人物的资料 Profiling、经验和情感状态传入LMLM模型，以模拟出该人物的行为。在这个研究中，我们提出了Character-LLM，它可以教导LMLM模型以特定人物的形式行为。我们的方法是将特定人物的资料编译为LMLM模型的体验，然后训练这些模型成为特定人物的人工模拟。为了评估我们的方法的有效性，我们建立了一个测试场景，让训练好的代理人回答问题，以判断代理人是否将记忆其角色和体验。实验结果给出了有趣的观察，帮助我们建立未来的人类模拟。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Workload-Distribution-for-Accuracy-aware-DNN-Inference-on-Collaborative-Edge-Platforms"><a href="#Adaptive-Workload-Distribution-for-Accuracy-aware-DNN-Inference-on-Collaborative-Edge-Platforms" class="headerlink" title="Adaptive Workload Distribution for Accuracy-aware DNN Inference on Collaborative Edge Platforms"></a>Adaptive Workload Distribution for Accuracy-aware DNN Inference on Collaborative Edge Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10157">http://arxiv.org/abs/2310.10157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zain Taufique, Antonio Miele, Pasi Liljeberg, Anil Kanduri</li>
<li>for: 加速深度学习模型（DNN）的推理过程，通过分布工作负载到一群协作的边缘节点。</li>
<li>methods: 提议适应性工作负载分布策略，同时考虑边缘设备的差异性和深度学习模型的精度和性能要求。</li>
<li>results: 测试了我们的方法在一个包括Odroid XU4、Raspberry Pi4和Jetson Nano板的边缘集群上，与状态艺术工作负载分布策略相比，实现了平均提高41.52%的性能和5.2%的输出精度。<details>
<summary>Abstract</summary>
DNN inference can be accelerated by distributing the workload among a cluster of collaborative edge nodes. Heterogeneity among edge devices and accuracy-performance trade-offs of DNN models present a complex exploration space while catering to the inference performance requirements. In this work, we propose adaptive workload distribution for DNN inference, jointly considering node-level heterogeneity of edge devices, and application-specific accuracy and performance requirements. Our proposed approach combinatorially optimizes heterogeneity-aware workload partitioning and dynamic accuracy configuration of DNN models to ensure performance and accuracy guarantees. We tested our approach on an edge cluster of Odroid XU4, Raspberry Pi4, and Jetson Nano boards and achieved an average gain of 41.52% in performance and 5.2% in output accuracy as compared to state-of-the-art workload distribution strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Theory-of-Mind-for-Multi-Agent-Collaboration-via-Large-Language-Models"><a href="#Theory-of-Mind-for-Multi-Agent-Collaboration-via-Large-Language-Models" class="headerlink" title="Theory of Mind for Multi-Agent Collaboration via Large Language Models"></a>Theory of Mind for Multi-Agent Collaboration via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10701">http://arxiv.org/abs/2310.10701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara</li>
<li>for: 本研究评估基于大型自然语言模型（LLM）的多智能代理人在多智能协作文本游戏中的表现，并与多智能奖励学习（MARL）和规划基eline进行比较。</li>
<li>methods: 本研究使用LLM来实现多智能协作，并对其表现进行评估。研究还 explore了使用explicit belief state representation来改善LLM的规划优化和任务状态幻觉问题。</li>
<li>results: 研究发现LLM-based agents exhibit emergent collaborative behaviors and high-order Theory of Mind capabilities，但受到长期context管理和任务状态幻觉的限制。使用explicit belief state representation可以提高任务表现和理解能力。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在理解和规划方面表现出色，但它们在多代合作中的能力仍然尚未得到充分探索。这个研究评估了基于 LLM 的代理人在多代合作文本游戏中的理论心理推理任务表现，与多代征学习（MARL）和规划基eline相比较。我们发现 LLM 基于代理人在合作行为中发展出了轻度的协力行为和高级理论心理能力。我们的结果显示 LLM 基于代理人在规划优化方面存在长时间前景管理和任务状态幻觉的系统性问题。我们探索了使用明确的信仰状态表示来缓和这些问题，发现这可以提高任务表现和 LLM 基于代理人的理论心理推理精度。
</details></li>
</ul>
<hr>
<h2 id="Recursive-Segmentation-Living-Image-An-eXplainable-AI-XAI-Approach-for-Computing-Structural-Beauty-of-Images-or-the-Livingness-of-Space"><a href="#Recursive-Segmentation-Living-Image-An-eXplainable-AI-XAI-Approach-for-Computing-Structural-Beauty-of-Images-or-the-Livingness-of-Space" class="headerlink" title="Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space"></a>Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10149">http://arxiv.org/abs/2310.10149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Qianxiang, Bin Jiang</li>
<li>for: 这项研究探讨了一种对图像美感评价的 объек Oriented 计算方法，即“结构美”。通过使用 SAM 模型，我们提出了一种基于再嵌套分割的方法，可以更准确地捕捉图像中的更细grained 结构。</li>
<li>methods: 我们使用 SAM 模型进行再嵌套分割，并将结构重建为层次结构，从而获得更加准确的结构量和层次结构。</li>
<li>results: 我们的方法可以准确地分割出图像中的意义 objects，包括树、建筑和窗户等，以及抽象的画作中的子结构。我们的计算结果与人类视觉评价相一致，并且在不同的颜色空间中进行评价时也能够保持一定的一致性。<details>
<summary>Abstract</summary>
This study introduces the concept of "structural beauty" as an objective computational approach for evaluating the aesthetic appeal of images. Through the utilization of the Segment anything model (SAM), we propose a method that leverages recursive segmentation to extract finer-grained substructures. Additionally, by reconstructing the hierarchical structure, we obtain a more accurate representation of substructure quantity and hierarchy. This approach reproduces and extends our previous research, allowing for the simultaneous assessment of Livingness in full-color images without the need for grayscale conversion or separate computations for foreground and background Livingness. Furthermore, the application of our method to the Scenic or Not dataset, a repository of subjective scenic ratings, demonstrates a high degree of consistency with subjective ratings in the 0-6 score range. This underscores that structural beauty is not solely a subjective perception, but a quantifiable attribute accessible through objective computation. Through our case studies, we have arrived at three significant conclusions. 1) our method demonstrates the capability to accurately segment meaningful objects, including trees, buildings, and windows, as well as abstract substructures within paintings. 2) we observed that the clarity of an image impacts our computational results; clearer images tend to yield higher Livingness scores. However, for equally blurry images, Livingness does not exhibit a significant reduction, aligning with human visual perception. 3) our approach fundamentally differs from methods employing Convolutional Neural Networks (CNNs) for predicting image scores. Our method not only provides computational results but also offers transparency and interpretability, positioning it as a novel avenue in the realm of Explainable AI (XAI).
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Our method demonstrates the capability to accurately segment meaningful objects, including trees, buildings, and windows, as well as abstract substructures within paintings.2. We observed that the clarity of an image impacts our computational results; clearer images tend to yield higher Livingness scores. However, for equally blurry images, Livingness does not exhibit a significant reduction, aligning with human visual perception.3. Our approach fundamentally differs from methods employing Convolutional Neural Networks (CNNs) for predicting image scores. Our method not only provides computational results but also offers transparency and interpretability, positioning it as a novel avenue in the realm of Explainable AI (XAI).</details></li>
</ol>
<hr>
<h2 id="LoBaSS-Gauging-Learnability-in-Supervised-Fine-tuning-Data"><a href="#LoBaSS-Gauging-Learnability-in-Supervised-Fine-tuning-Data" class="headerlink" title="LoBaSS: Gauging Learnability in Supervised Fine-tuning Data"></a>LoBaSS: Gauging Learnability in Supervised Fine-tuning Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13008">http://arxiv.org/abs/2310.13008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Zhou, Tingkai Liu, Qianli Ma, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang</li>
<li>for: 本研究的目的是提出一种基于模型学习能力的超级vised fine-tuning数据选择方法，以便在模型精度和学习效率之间寻找优质平衡。</li>
<li>methods: 本研究使用的方法是基于损失函数的SFT数据选择方法（LoBaSS），该方法根据模型在预训练阶段所学习的能力来选择合适的SFT数据，以便提高模型的精度和学习效率。</li>
<li>results: 实验结果表明，使用LoBaSS方法可以在仅6%的全部训练数据量下，超越全数据 Fine-tuning，并在16.7%的数据量下具有同样的精度和学习效率。这表明LoBaSS方法可以在不同领域中协调模型的能力，以达到优质的精度和学习效率。<details>
<summary>Abstract</summary>
Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring optimal compatibility and learning efficiency. In experimental comparisons involving 7B and 13B models, our LoBaSS method is able to surpass full-data fine-tuning at merely 6% of the total training data. When employing 16.7% of the data, LoBaSS harmonizes the model's capabilities across conversational and mathematical domains, proving its efficacy and adaptability.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的超级vised Fine-Tuning（SFT）阶段 serves as a crucial phase in aligning LLMs to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring optimal compatibility and learning efficiency. In experimental comparisons involving 7B and 13B models, our LoBaSS method is able to surpass full-data fine-tuning at merely 6% of the total training data. When employing 16.7% of the data, LoBaSS harmonizes the model's capabilities across conversational and mathematical domains, proving its efficacy and adaptability.
</details></li>
</ul>
<hr>
<h2 id="CLIN-A-Continually-Learning-Language-Agent-for-Rapid-Task-Adaptation-and-Generalization"><a href="#CLIN-A-Continually-Learning-Language-Agent-for-Rapid-Task-Adaptation-and-Generalization" class="headerlink" title="CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization"></a>CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10134">http://arxiv.org/abs/2310.10134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, Peter Clark</li>
<li>For: The paper aims to develop a language-based agent that can continually improve over time and perform well in varied environments and tasks.* Methods: The paper proposes a persistent, dynamic, textual memory centered on causal abstractions, which is regularly updated after each trial to gradually learn useful knowledge for new trials.* Results: The proposed approach, called CLIN, outperforms state-of-the-art reflective language agents in the ScienceWorld benchmark, achieves transfer learning to new environments and tasks, and continually improves performance through memory updates.Here are the three points in Simplified Chinese:* For: 这篇论文目的是开发一种可以不断改进并在多个环境和任务中表现出色的语言基于的智能代理。* Methods: 论文提出了一种持续、动态、文本内存，以 causal abstractions 为中心，在每次试验后进行更新，以逐渐学习有用的知识。* Results: CLIN 在 ScienceWorld benchmark 中表现出色，比 state-of-the-art 反射语言代理 Reflexion 高出 23 个绝对分数点，并且在新环境（或新任务）中表现出较好的适应能力和持续改进能力。<details>
<summary>Abstract</summary>
Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory centered on causal abstractions (rather than general "helpful hints") that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.
</details>
<details>
<summary>摘要</summary>
language agents 有能力与外部环境互动，例如虚拟世界 ScienceWorld，完成复杂任务，如培养植物，而无需强化学习的开始成本。然而，虽有零开始能力，这些代理人至今没有持续改进的能力。在这，我们提出了 CLIN，第一个语言基于的代理人，能够在多次尝试中不断改进，包括环境和任务变化时。我们的方法是使用持续、动态、文本中心的 causal abstractions（而不是通用的“帮助提示”）， Regularly update after each trial so that the agent gradually learns useful knowledge for new trials。在 ScienceWorld benchmark 中，CLIN 能够在重复尝试中不断改进同一任务和环境，胜过现状 reflective language agents  like Reflexion 的 23 个绝对分数点。CLIN 还可以转移到新环境（或新任务），提高零开始性能 by 4 个分数点（13 个分数点），并可以通过持续记忆更新，进一步提高性能，增加 17 个分数点（7 个分数点）。这表明了一种基于冻结模型的新架构，可以在不断改进的时间上 continually 和 Rapidly 提高性能。
</details></li>
</ul>
<hr>
<h2 id="A-Non-monotonic-Smooth-Activation-Function"><a href="#A-Non-monotonic-Smooth-Activation-Function" class="headerlink" title="A Non-monotonic Smooth Activation Function"></a>A Non-monotonic Smooth Activation Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10126">http://arxiv.org/abs/2310.10126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Koushik Biswas, Meghana Karri, Ulaş Bağcı</li>
<li>for: The paper is written for proposing a new activation function called Sqish, which is an alternative to existing activation functions in deep learning models.</li>
<li>methods: The paper uses experiments on various tasks such as classification, object detection, segmentation, and adversarial robustness to demonstrate the superiority of the Sqish activation function over existing activation functions such as ReLU.</li>
<li>results: The paper shows that the Sqish activation function achieves better performance than ReLU on several benchmark datasets, including CIFAR100, with an improvement of 8.21% in adversarial robustness and 5.87% in image classification.<details>
<summary>Abstract</summary>
Activation functions are crucial in deep learning models since they introduce non-linearity into the networks, allowing them to learn from errors and make adjustments, which is essential for learning complex patterns. The essential purpose of activation functions is to transform unprocessed input signals into significant output activations, promoting information transmission throughout the neural network. In this study, we propose a new activation function called Sqish, which is a non-monotonic and smooth function and an alternative to existing ones. We showed its superiority in classification, object detection, segmentation tasks, and adversarial robustness experiments. We got an 8.21% improvement over ReLU on the CIFAR100 dataset with the ShuffleNet V2 model in the FGSM adversarial attack. We also got a 5.87% improvement over ReLU on image classification on the CIFAR100 dataset with the ShuffleNet V2 model.
</details>
<details>
<summary>摘要</summary>
translate the given text into Simplified Chinese.Activation functions are crucial in deep learning models, as they introduce non-linearity into the networks, allowing them to learn from errors and make adjustments, which is essential for learning complex patterns. The essential purpose of activation functions is to transform unprocessed input signals into significant output activations, promoting information transmission throughout the neural network. In this study, we propose a new activation function called Sqish, which is a non-monotonic and smooth function and an alternative to existing ones. We showed its superiority in classification, object detection, segmentation tasks, and adversarial robustness experiments. We got an 8.21% improvement over ReLU on the CIFAR100 dataset with the ShuffleNet V2 model in the FGSM adversarial attack. We also got a 5.87% improvement over ReLU on image classification on the CIFAR100 dataset with the ShuffleNet V2 model.中文翻译： activation functions 是深度学习模型中关键的组件，因为它们引入非线性，让模型从错误中学习并进行调整，这是学习复杂模式的关键。 activation functions 的主要目的是将未处理的输入信号转化为有意义的输出活动，促进神经网络中信息的传输。在本研究中，我们提出了一个新的 activation function called Sqish，它是非增长的和平滑的函数，是现有的替代品。我们在类别、物体检测、分割任务和对抗攻击性实验中证明了它的优越性。在 ShuffleNet V2 模型上，我们在 FGSM 对抗攻击中获得了 ReLU 的 8.21% 提升，并在图像分类任务中获得了 ReLU 的 5.87% 提升。
</details></li>
</ul>
<hr>
<h2 id="From-Continuous-Dynamics-to-Graph-Neural-Networks-Neural-Diffusion-and-Beyond"><a href="#From-Continuous-Dynamics-to-Graph-Neural-Networks-Neural-Diffusion-and-Beyond" class="headerlink" title="From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond"></a>From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10121">http://arxiv.org/abs/2310.10121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andi Han, Dai Shi, Lequan Lin, Junbin Gao</li>
<li>for: 这篇论文旨在提供关于图 neural network（GNN）的系统性和全面的回顾，尤其是在使用连续动力学方法的研究中。</li>
<li>methods: 这篇论文使用的方法包括message passing机制和连续动力学方法，用于解决图 neural network（GNN）中的各种问题，如过滤和压缩。</li>
<li>results: 该论文提出了一种基于连续动力学方法的GNN设计方法，并对经典GNN的局限性进行了解释和改进。同时，该论文还提供了多个未解决的研究方向，以便进一步探索GNN的可能性。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have demonstrated significant promise in modelling relational data and have been widely applied in various fields of interest. The key mechanism behind GNNs is the so-called message passing where information is being iteratively aggregated to central nodes from their neighbourhood. Such a scheme has been found to be intrinsically linked to a physical process known as heat diffusion, where the propagation of GNNs naturally corresponds to the evolution of heat density. Analogizing the process of message passing to the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and consequently informs better model design. Recently, there emerges a plethora of works that proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we provide the first systematic and comprehensive review of studies that leverage the continuous perspective of GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics to GNNs, along with a general framework for the design of graph neural dynamics. We then review and categorize existing works based on their driven mechanisms and underlying dynamics. We also summarize how the limitations of classic GNNs can be addressed under the continuous framework. We conclude by identifying multiple open research directions.
</details>
<details>
<summary>摘要</summary>
格图神经网络（GNNs）已经显示出了重要的承诺，可以模型关系数据，并广泛应用于不同的领域。GNNs的关键机制是叫做“消息传递”，信息从邻居传递到中心节点。这种机制与物理过程热扩散有着直接的关系，GNNs的传播 Naturally corresponds to the evolution of heat density。通过对消息传递的过程进行对热动力学的分析，可以更深入地理解GNNs的力量和缺陷，并且可以设计更好的模型。最近，有一些研究借鉴了维持热动力学形式的GNNs，以解决经典GNNs的知名的局限性，如扩散和压缩。在这篇评论中，我们提供了首次系统性和完整性的对Continuous perspective of GNNs的评论。为了实现这一目标，我们介绍了适应维持热动力学的基本成分，并提出了一个总体的框架 для设计图神经动态。然后，我们回顾了现有的研究，并根据他们的驱动机制和下面动力来分类。我们还总结了经典GNNs中的局限性如何通过维持热动力学的方式解决。 Finally, we identify multiple open research directions.
</details></li>
</ul>
<hr>
<h2 id="On-Generative-Agents-in-Recommendation"><a href="#On-Generative-Agents-in-Recommendation" class="headerlink" title="On Generative Agents in Recommendation"></a>On Generative Agents in Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10108">http://arxiv.org/abs/2310.10108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LehengTHU/Agent4Rec">https://github.com/LehengTHU/Agent4Rec</a></li>
<li>paper_authors: An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, Tat-Seng Chua<br>for:* 这个论文旨在提出一种基于大语言模型（LLM）的电影推荐模拟器，帮助解决现有推荐系统中偏差的问题。methods:* 该模拟器使用了LLM-empowered生成代理，每个代理具有用户 profiling、记忆和行为模块，特意为推荐系统设计。results:* 对 Agent4Rec 的广泛和多方面评估表明，LLM-empowered生成代理可以准确地模拟真实自主人类在推荐系统中的行为。I hope this helps! Let me know if you have any further questions or if there’s anything else I can help with.<details>
<summary>Abstract</summary>
Recommender systems are the cornerstone of today's information dissemination, yet a disconnect between offline metrics and online performance greatly hinders their development. Addressing this challenge, we envision a recommendation simulator, capitalizing on recent breakthroughs in human-level intelligence exhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie recommendation simulator, leveraging LLM-empowered generative agents equipped with user profile, memory, and actions modules specifically tailored for the recommender system. In particular, these agents' profile modules are initialized using the MovieLens dataset, capturing users' unique tastes and social traits; memory modules log both factual and emotional memories and are integrated with an emotion-driven reflection mechanism; action modules support a wide variety of behaviors, spanning both taste-driven and emotion-driven actions. Each agent interacts with personalized movie recommendations in a page-by-page manner, relying on a pre-implemented collaborative filtering-based recommendation algorithm. We delve into both the capabilities and limitations of Agent4Rec, aiming to explore an essential research question: to what extent can LLM-empowered generative agents faithfully simulate the behavior of real, autonomous humans in recommender systems? Extensive and multi-faceted evaluations of Agent4Rec highlight both the alignment and deviation between agents and user-personalized preferences. Beyond mere performance comparison, we explore insightful experiments, such as emulating the filter bubble effect and discovering the underlying causal relationships in recommendation tasks. Our codes are available at https://github.com/LehengTHU/Agent4Rec.
</details>
<details>
<summary>摘要</summary>
现代推荐系统是信息传递的核心，但是在线和离线指标之间的差距妨碍了其发展。为解决这个挑战，我们提出了一种推荐模拟器，即Agent4Rec，利用最近的人工智能大语言模型（LLM）的突破性。我们的模拟器采用LLM拥有的生成代理，包括用户profile、记忆和行为模块，特地设计用于推荐系统。具体来说，这些代理的profile模块初始化使用MovieLens数据集，捕捉用户独特的味蕾和社交特征；记忆模块记录了事实和情感的记忆，并与情感驱动的反射机制集成；行动模块支持广泛的行为，包括味蕾驱动和情感驱动的行为。每个代理都与个性化电影推荐在页面上进行交互，基于先前实现的共同 filtering 基于推荐算法。我们深入探讨Agent4Rec的能力和局限性，以explore一个关键研究问题：可以使用LLM拥有的生成代理 simulate真实自主人类在推荐系统中的行为到哪 extent?我们进行了广泛和多方面的评估，包括对代理和用户个性化喜好的Alignment和偏差。此外，我们还进行了有趣的实验，如模拟过滤层效应和发现推荐任务中的内在 causal 关系。我们的代码可以在https://github.com/LehengTHU/Agent4Rec 中下载。
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-the-Posterior-Sampling-based-Learning-Algorithm-for-Episodic-POMDPs"><a href="#Regret-Analysis-of-the-Posterior-Sampling-based-Learning-Algorithm-for-Episodic-POMDPs" class="headerlink" title="Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs"></a>Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10107">http://arxiv.org/abs/2310.10107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dengwang Tang, Rahul Jain, Ashutosh Nayyar, Pierluigi Nuzzo</li>
<li>for: 这 paper 的目的是研究在 partially observable Markov decision processes (POMDPs) 中学习问题。</li>
<li>methods: 这 paper 使用 posterior sampling-based reinforcement learning (PSRL) algorithm for POMDPs，并证明其 bayesian regret 的增长率为 sqrt(eps)。</li>
<li>results: 这 paper 显示，在 POMDPs 中，bayesian regret 随着 horizon length H 的增长而增长 exponentiallly，但在undercomplete和weakly revealing的 condition下，可以得到 polynomial bayesian regret bound，其比 recent result by arXiv:2204.08967 好几个orders of magnitude。<details>
<summary>Abstract</summary>
Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:与Markov决策过程（MDP）相比，在Partially Observable Markov决策过程（POMDP）中学习可能更加困难，主要是因为观察结果的难以解释。在这篇论文中，我们考虑POMDP中的集集学习问题，其中过渡和观察模型都未知。我们使用Posterior Sampling-based Reinforcement Learning（PSRL）算法，并证明其 bayesian regret的准确性与集数平方根相关。通常情况下， regret scales exponentially with horizon length $H$，并且我们提供了一个下界，证明这是不可避免的。然而，在POMDP是undercomplete和weakly revealing的情况下，我们设立了一个较好的 Bayesian regret bound，它比recent result by arXiv:2204.08967好于$\Omega(H^2\sqrt{SA})$。
</details></li>
</ul>
<hr>
<h2 id="Navigation-with-Large-Language-Models-Semantic-Guesswork-as-a-Heuristic-for-Planning"><a href="#Navigation-with-Large-Language-Models-Semantic-Guesswork-as-a-Heuristic-for-Planning" class="headerlink" title="Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning"></a>Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10103">http://arxiv.org/abs/2310.10103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Michael-Equi/lfg-nav">https://github.com/Michael-Equi/lfg-nav</a></li>
<li>paper_authors: Dhruv Shah, Michael Equi, Blazej Osinski, Fei Xia, Brian Ichter, Sergey Levine</li>
<li>for: 这个论文的目的是为了帮助机器人快速在未知环境中找到目标。</li>
<li>methods: 这个论文使用语言模型来提供启发，并将语言模型作为搜索准则来帮助计划算法探索新环境。</li>
<li>results: 这个论文在实验中表明，使用语言模型可以使机器人在未知环境中更快地找到目标，并且在实际环境和模拟 benchmark 中都能够表现出优于无意义探索和其他语言模型使用方法。<details>
<summary>Abstract</summary>
Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics -- e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the ``semantic guesswork'' produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.
</details>
<details>
<summary>摘要</summary>
naviagtion in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics -- e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. therefore, in this paper we study how the "semantic guesswork" produced by language models can be utilized as a guiding heuristic for planning algorithms. our method, language frontier guide (lfg), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. we evaluate lfg in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.
</details></li>
</ul>
<hr>
<h2 id="Reusing-Pretrained-Models-by-Multi-linear-Operators-for-Efficient-Training"><a href="#Reusing-Pretrained-Models-by-Multi-linear-Operators-for-Efficient-Training" class="headerlink" title="Reusing Pretrained Models by Multi-linear Operators for Efficient Training"></a>Reusing Pretrained Models by Multi-linear Operators for Efficient Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10699">http://arxiv.org/abs/2310.10699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 这篇论文目的是提高大型模型的训练速度，使用小型预训练模型来初始化大型模型（称为“目标模型”），并且将这两个模型中的权重 Linearly correlated 以增强加速能力。</li>
<li>methods: 这篇论文使用了一种方法，将每个目标模型的权重与所有预训练模型的权重进行 Linear correlation，以增强加速能力。这篇论文还使用了多元线性算子来降低计算和空间复杂度，使得资源需求可以接受。</li>
<li>results: 实验结果显示，这篇论文的方法可以在DeiT-base上预训练DeiT-small时，节省76%的计算成本，并且在BERT2BERT和LiGO的比较下，提高了12.0%和20.7%的性能。<details>
<summary>Abstract</summary>
Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the ``target model''), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Experiments demonstrate that our method can save 76\% computational costs on DeiT-base transferred from DeiT-small, which outperforms bert2BERT by +12.0\% and LiGO by +20.7\%, respectively.
</details>
<details>
<summary>摘要</summary>
通常，训练大型模型从零开始需要很多资源。为解决这个问题， latest studies such as bert2BERT 和 LiGO  reuse small pre-trained models to initialize a large model（称为“目标模型”），从而大幅提高训练速度。然而，这些前一 Studies only map partial weights,忽略了整个模型中 weights 之间的可能的相互关系。在这篇文章中，我们发现了这些 weights 之间的相互作用和内部相互作用，这意味着只有部分映射可能不能捕捉完整的信息，导致不够的增长。为解决这个问题，我们提议一种方法，使得每个目标模型中的每个Weight 都 Linearly correlated 到整个预训练模型中的所有Weight。我们使用多线性运算符来减少计算和空间复杂度，使得可接受的资源需求。实验表明，我们的方法可以在 DeiT-base 中心从 DeiT-small 中心转移时Save 76% 的计算成本，并且在 bert2BERT 和 LiGO 的比较中，提高 +12.0% 和 +20.7%，分别。
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Uncertainty-Representation-of-Data-Manifold-for-Robust-Long-Tailed-Learning"><a href="#Orthogonal-Uncertainty-Representation-of-Data-Manifold-for-Robust-Long-Tailed-Learning" class="headerlink" title="Orthogonal Uncertainty Representation of Data Manifold for Robust Long-Tailed Learning"></a>Orthogonal Uncertainty Representation of Data Manifold for Robust Long-Tailed Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10090">http://arxiv.org/abs/2310.10090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Lingling Li</li>
<li>for: 提高模型在长尾分布下的Robustness</li>
<li>methods: 使用特征嵌入的Orthogonal Uncertainty Representation(OUR)和端到端训练策略</li>
<li>results: 在长尾数据集上进行了全面的评估，OUR方法可以减少模型在长尾分布下的敏感性，并且可以与其他长尾学习方法相结合使用，不需要额外数据生成，快速和高效地训练。<details>
<summary>Abstract</summary>
In scenarios with long-tailed distributions, the model's ability to identify tail classes is limited due to the under-representation of tail samples. Class rebalancing, information augmentation, and other techniques have been proposed to facilitate models to learn the potential distribution of tail classes. The disadvantage is that these methods generally pursue models with balanced class accuracy on the data manifold, while ignoring the ability of the model to resist interference. By constructing noisy data manifold, we found that the robustness of models trained on unbalanced data has a long-tail phenomenon. That is, even if the class accuracy is balanced on the data domain, it still has bias on the noisy data manifold. However, existing methods cannot effectively mitigate the above phenomenon, which makes the model vulnerable in long-tailed scenarios. In this work, we propose an Orthogonal Uncertainty Representation (OUR) of feature embedding and an end-to-end training strategy to improve the long-tail phenomenon of model robustness. As a general enhancement tool, OUR has excellent compatibility with other methods and does not require additional data generation, ensuring fast and efficient training. Comprehensive evaluations on long-tailed datasets show that our method significantly improves the long-tail phenomenon of robustness, bringing consistent performance gains to other long-tailed learning methods.
</details>
<details>
<summary>摘要</summary>
在长尾分布场景下，模型能够识别尾类受限因为尾类样本的下 Representation 不充分。Class重新平衡、信息增强和其他技术已经提出来解决这个问题，但这些方法通常寻求在数据 manifold 上具有平衡的类准确率，而忽略模型对干扰的抵抗能力。我们通过构建噪音数据 manifold 发现，模型在不平衡数据上训练时的 Robustness 存在长尾现象。即，即使在数据Domain 上具有平衡的类准确率，模型在噪音数据 manifold 上仍存在偏见。然而，现有的方法无法有效 Mitigate 这个现象，使得模型在长尾场景中脆弱。在这种情况下，我们提出了一种Orthogonal Uncertainty Representation（OUR）的特征嵌入和端到端训练策略，以改善模型在长尾场景中的Robustness。作为一种通用加强工具，OUR具有优compatibility 性，不需要额外数据生成，保证快速和高效的训练。对长尾数据集进行了全面评估，我们的方法在长尾场景中显著改善了模型的Robustness，并且与其他长尾学习方法相结合，带来了一致的性能提升。
</details></li>
</ul>
<hr>
<h2 id="MOCHA-Real-Time-Motion-Characterization-via-Context-Matching"><a href="#MOCHA-Real-Time-Motion-Characterization-via-Context-Matching" class="headerlink" title="MOCHA: Real-Time Motion Characterization via Context Matching"></a>MOCHA: Real-Time Motion Characterization via Context Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10079">http://arxiv.org/abs/2310.10079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DK-Jang/MOCHA_SIGASIA2023">https://github.com/DK-Jang/MOCHA_SIGASIA2023</a></li>
<li>paper_authors: Deok-Kyeong Jang, Yuting Ye, Jungdam Won, Sung-Hee Lee</li>
<li>for: 这篇论文目的是为了实时转换中性无表情的输入动作为一个知名人物的特有风格。</li>
<li>methods: 这篇论文介绍了一个新的在线动作特征化框架，即MOCHA，可以将目标人物的动作风格和体型特征转移到输入动作上。</li>
<li>results: 该框架可以在实时进行人物动作特征化，并且可以轻松地满足不同应用场景，如只有稀疏输入的人物特征化和实时人物特征化。此外，论文还提供了一个高质量的动作数据集，包括六个不同人物在多种动作中的表现，这可以成为未来研究的优质资源。<details>
<summary>Abstract</summary>
Transforming neutral, characterless input motions to embody the distinct style of a notable character in real time is highly compelling for character animation. This paper introduces MOCHA, a novel online motion characterization framework that transfers both motion styles and body proportions from a target character to an input source motion. MOCHA begins by encoding the input motion into a motion feature that structures the body part topology and captures motion dependencies for effective characterization. Central to our framework is the Neural Context Matcher, which generates a motion feature for the target character with the most similar context to the input motion feature. The conditioned autoregressive model of the Neural Context Matcher can produce temporally coherent character features in each time frame. To generate the final characterized pose, our Characterizer network incorporates the characteristic aspects of the target motion feature into the input motion feature while preserving its context. This is achieved through a transformer model that introduces the adaptive instance normalization and context mapping-based cross-attention, effectively injecting the character feature into the source feature. We validate the performance of our framework through comparisons with prior work and an ablation study. Our framework can easily accommodate various applications, including characterization with only sparse input and real-time characterization. Additionally, we contribute a high-quality motion dataset comprising six different characters performing a range of motions, which can serve as a valuable resource for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入动作的中性化和无特点化的输入动作在实时动画中具有吸引力，这篇论文介绍了MOCHA，一种新的在线动作特征化框架。MOCHA可以同时传递目标人物的动作风格和身体比例到输入动作中。MOCHA开始sBy编码输入动作为一个动作特征，该特征映射体部 topology和捕捉动作依赖关系以便有效地特征化。中心于我们框架的神经Context Matcher生成了目标人物的动作特征，该特征与输入动作特征Context最相似。Conditional autoregressive model of the Neural Context Matcher can produce temporally coherent character features in each time frame. To generate the final characterized pose, our Characterizer network incorporates the characteristic aspects of the target motion feature into the input motion feature while preserving its context. This is achieved through a transformer model that introduces the adaptive instance normalization and context mapping-based cross-attention, effectively injecting the character feature into the source feature. We validate the performance of our framework through comparisons with prior work and an ablation study. Our framework can easily accommodate various applications, including characterization with only sparse input and real-time characterization. Additionally, we contribute a high-quality motion dataset comprising six different characters performing a range of motions, which can serve as a valuable resource for future research.
</details></li>
</ul>
<hr>
<h2 id="Verbosity-Bias-in-Preference-Labeling-by-Large-Language-Models"><a href="#Verbosity-Bias-in-Preference-Labeling-by-Large-Language-Models" class="headerlink" title="Verbosity Bias in Preference Labeling by Large Language Models"></a>Verbosity Bias in Preference Labeling by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10076">http://arxiv.org/abs/2310.10076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）的性能提升方法，具体来说是通过人工智能反馈学习（RLAIF）取代人类反馈来评估LLMs。</li>
<li>methods: 本研究使用了RLAIF评估GPT-4和人类反馈的方法，并提出了一个量化verbosity bias的指标。</li>
<li>results: 研究发现GPT-4在本研究中偏好 longer answers than humans, and propose a metric to measure this bias.<details>
<summary>Abstract</summary>
In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fine-tuning-ChatGPT-for-Automatic-Scoring"><a href="#Fine-tuning-ChatGPT-for-Automatic-Scoring" class="headerlink" title="Fine-tuning ChatGPT for Automatic Scoring"></a>Fine-tuning ChatGPT for Automatic Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10072">http://arxiv.org/abs/2310.10072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Latif, Xiaoming Zhai</li>
<li>For: This paper demonstrates the potential of fine-tuned ChatGPT (GPT-3.5) for automatically scoring student written constructed responses in science education.* Methods: The paper uses fine-tuned GPT-3.5 on six assessment tasks with a diverse dataset of middle-school and high-school student responses and expert scoring.* Results: The results show that fine-tuned GPT-3.5 achieved a remarkable average increase (9.1%) in automatic scoring accuracy compared to the fine-tuned state-of-the-art Google’s generated language model, BERT, with significant improvements on multi-label tasks and multi-class items.Here are the three points in Simplified Chinese text:* For: 这个研究用于评估学生写的 constructed responses 的自动评分。* Methods: 这个研究使用 fine-tuned GPT-3.5 在 six 个评估任务上，使用多样化的中学和高中生回答和专家评分数据进行 fine-tuning。* Results: 结果显示， fine-tuned GPT-3.5 在 six 个评估任务上达到了9.1%的平均提升率，比 fine-tuned BERT 高，尤其是在多个标签任务和多个类型任务上显示出了显著的提升。<details>
<summary>Abstract</summary>
This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for automatically scoring student written constructed responses using example assessment tasks in science education. Recent studies on OpenAI's generative model GPT-3.5 proved its superiority in predicting the natural language with high accuracy and human-like responses. GPT-3.5 has been trained over enormous online language materials such as journals and Wikipedia; therefore, more than direct usage of pre-trained GPT-3.5 is required for automatic scoring as students utilize a different language than trained material. These imply that a domain-specific model, fine-tuned over data for specific tasks, can enhance model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks with a diverse dataset of middle-school and high-school student responses and expert scoring. The six tasks comprise two multi-label and four multi-class assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the fine-tuned state-of-the-art Google's generated language model, BERT. The results show that in-domain training corpora constructed from science questions and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5 shows a remarkable average increase (9.1%) in automatic scoring accuracy (mean = 9.15, SD = 0.042) for the six tasks, p =0.001 < 0.05. Specifically, for multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5 achieved significantly higher scoring accuracy than BERT across all the labels, with the second item achieving a 7.1% increase. The average scoring increase for the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring of student responses on domain-specific data in education with high accuracy. We have released fine-tuned models for public use and community engagement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GreatSplicing-A-Semantically-Rich-Splicing-Dataset"><a href="#GreatSplicing-A-Semantically-Rich-Splicing-Dataset" class="headerlink" title="GreatSplicing: A Semantically Rich Splicing Dataset"></a>GreatSplicing: A Semantically Rich Splicing Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10070">http://arxiv.org/abs/2310.10070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiuli Bi, Jiaming Liang</li>
<li>for: 解决现有拼接质量 dataset 中缺乏 semantic varieties 的问题，提高拼接trace detection的准确率。</li>
<li>methods: 使用 manually created splicing dataset GreatSplicing，包括5000个拼接图像，覆盖335种不同的semantic categories。</li>
<li>results: 模型在 GreatSplicing 上训练后表现出较低的错误率和跨 dataset detection 能力，比现有dataset 更佳。<details>
<summary>Abstract</summary>
In existing splicing forgery datasets, the insufficient semantic varieties of spliced regions cause a problem that trained detection models overfit semantic features rather than splicing traces. Meanwhile, because of the absence of a reasonable dataset, different detection methods proposed cannot reach a consensus on experimental settings. To address these urgent issues, GreatSplicing, a manually created splicing dataset with a considerable amount and high quality, is proposed in this paper. GreatSplicing comprises 5,000 spliced images and covers spliced regions with 335 distinct semantic categories, allowing neural networks to grasp splicing traces better. Extensive experiments demonstrate that models trained on GreatSplicing exhibit minimal misidentification rates and superior cross-dataset detection capabilities compared to existing datasets. Furthermore, GreatSplicing is available for all research purposes and can be downloaded from www.greatsplicing.net.
</details>
<details>
<summary>摘要</summary>
现有的剪辑伪造数据集中，剪辑区域的 semantic variety 不够，导致训练的检测模型更倾向于学习 semantic features 而不是剪辑 traces。另一方面，由于缺乏合理的数据集，不同的检测方法的实际设置不能达成一致。为解决这些紧迫的问题，本文提出了 GreatSplicing，一个手动创建的剪辑数据集，包含 5,000 个剪辑图像，剪辑区域涵盖 335 个不同的 semantic category，使得神经网络更好地捕捉剪辑 traces。广泛的实验表明，基于 GreatSplicing 训练的模型在剪辑检测方面具有较少的误认率和较好的跨数据集检测能力，与现有数据集相比。此外，GreatSplicing 适用于所有研究用途，可以从 www.greatsplicing.net 下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-Graph-Filters-for-Spectral-GNNs-via-Newton-Interpolation"><a href="#Learning-Graph-Filters-for-Spectral-GNNs-via-Newton-Interpolation" class="headerlink" title="Learning Graph Filters for Spectral GNNs via Newton Interpolation"></a>Learning Graph Filters for Spectral GNNs via Newton Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10064">http://arxiv.org/abs/2310.10064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Xu, Enyan Dai, Dongsheng Luo, Xiang Zhang, Suhang Wang</li>
<li>for: 本研究旨在探讨spectral graph neural networks（GNNs）中filter frequency的选择对graph数据的同义度水平（homophily level）的影响，以及如何通过任务指导学习spectral filters来捕捉 graf数据中的关键频率信息。</li>
<li>methods: 本研究采用了 both theoretical and empirical analyses，包括对 existedential GNNs进行分析，以及实验室中的实验。研究发现，low-frequency filters与homophily level之间存在正相关，而高频率 filters则与homophily level之间存在负相关。基于这一结论，研究人员提出了一种shape-aware regularization技术，用于自适应定制polynomial spectral filters，以适应 desired homophily levels。</li>
<li>results: 研究人员通过实验表明，NewtonNet可以成功地实现desired filter shapes，并在homophilous和heterophilous dataset上显示出优秀的性能。<details>
<summary>Abstract</summary>
Spectral Graph Neural Networks (GNNs) are gaining attention because they can surpass the limitations of message-passing GNNs by learning spectral filters that capture essential frequency information in graph data through task supervision. However, previous research suggests that the choice of filter frequency is tied to the graph's homophily level, a connection that hasn't been thoroughly explored in existing spectral GNNs. To address this gap, the study conducts both theoretical and empirical analyses, revealing that low-frequency filters have a positive correlation with homophily, while high-frequency filters have a negative correlation. This leads to the introduction of a shape-aware regularization technique applied to a Newton Interpolation-based spectral filter, enabling the customization of polynomial spectral filters that align with desired homophily levels. Extensive experiments demonstrate that NewtonNet successfully achieves the desired filter shapes and exhibits superior performance on both homophilous and heterophilous datasets.
</details>
<details>
<summary>摘要</summary>
spectral graph neural networks (GNNs) 是受到关注，因为它可以超越消息传递 GNNs 的局限性，通过任务指导学习spectral filters，捕捉图数据中重要的频率信息。然而，前一些研究表明，filter frequency 与图的同化程度（homophily level）之间存在相互关系，这个关系尚未在现有的 spectral GNNs 中得到了充分探讨。为了解决这个差距，这项研究进行了both theoretical和empirical分析，发现低频 filters 与 homophily 之间存在正相关，高频 filters 与 homophily 之间存在负相关。这导致了一种shape-aware regularization技术的引入，用于 Newton Interpolation-based spectral filter，以适应欲要的 homophily 水平。广泛的实验表明，NewtonNet 成功实现了所需的 filter 形状，并在 homophilous 和 heterophilous 数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Evaluation-of-Tool-Assisted-Generation-Strategies"><a href="#A-Comprehensive-Evaluation-of-Tool-Assisted-Generation-Strategies" class="headerlink" title="A Comprehensive Evaluation of Tool-Assisted Generation Strategies"></a>A Comprehensive Evaluation of Tool-Assisted Generation Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10062">http://arxiv.org/abs/2310.10062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva</li>
<li>for:  This paper aims to investigate the effectiveness of various few-shot tool-usage strategies for augmenting language models, and to provide a systematic and fair comparison with strong baselines.</li>
<li>methods:  The paper uses empirical analysis to compare the performance of different few-shot tool-usage strategies, including strategies that refine incorrect outputs with tools and strategies that retrieve relevant information ahead of or during generation.</li>
<li>results:  The paper finds that strong no-tool baselines are competitive to tool-assisted strategies, and that tool-assisted strategies are expensive in terms of the number of tokens they require. The paper emphasizes the need for comprehensive evaluations of future strategies to accurately assess their benefits and costs.<details>
<summary>Abstract</summary>
A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences). Various few-shot tool-usage strategies have been proposed. However, there is no systematic and fair comparison across different strategies, or between these strategies and strong baselines that do not leverage tools. We conduct an extensive empirical analysis, finding that (1) across various datasets, example difficulty levels, and models, strong no-tool baselines are competitive to tool-assisted strategies, implying that effectively using tools with in-context demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval tasks, strategies that *refine* incorrect outputs with tools outperform strategies that retrieve relevant information *ahead of* or *during generation*; (3) tool-assisted strategies are expensive in the number of tokens they require to work -- incurring additional costs by orders of magnitude -- which does not translate into significant improvement in performance. Overall, our findings suggest that few-shot tool integration is still an open challenge, emphasizing the need for comprehensive evaluations of future strategies to accurately assess their *benefits* and *costs*.
</details>
<details>
<summary>摘要</summary>
研究者正在努力增强语言模型，以解决其缺陷（如缺失或错误知识、逻辑推理错误）。各种几招工具使用策略已经被提出，但没有系统化、公平的比较，或与不使用工具的强大基eline进行比较。我们进行了广泛的实验分析，发现：1. 在不同的数据集、示例难度水平和模型上，不使用工具的强大基eline与工具协助策略竞争激烈，表明使用工具进行协助是一个困难的未解决问题。2. 对知识检索任务，使用工具修正错误输出的策略比使用工具预先检索相关信息的策略更高效。3. 使用工具的策略需要更多的字符数，而这些字符数的增加并没有级别提高表现。总之，我们的发现表明几招工具集成仍然是一个开放的挑战，强调未来策略的全面评估，以准确评估其利益和成本。
</details></li>
</ul>
<hr>
<h2 id="Flow-Dynamics-Correction-for-Action-Recognition"><a href="#Flow-Dynamics-Correction-for-Action-Recognition" class="headerlink" title="Flow Dynamics Correction for Action Recognition"></a>Flow Dynamics Correction for Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10059">http://arxiv.org/abs/2310.10059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang, Piotr Koniusz</li>
<li>for:  investigate different optical flow and features extracted from these optical flow to improve action recognition performance</li>
<li>methods: power normalization on magnitude component of optical flow for flow dynamics correction, and integrating corrected flow dynamics into popular models through a simple hallucination step</li>
<li>results: performance boosted with corrected optical flow, and new state-of-the-art performance on several benchmarks including HMDB-51, YUP++, fine-grained action recognition on MPII Cooking Activities, and large-scale Charades<details>
<summary>Abstract</summary>
Various research studies indicate that action recognition performance highly depends on the types of motions being extracted and how accurate the human actions are represented. In this paper, we investigate different optical flow, and features extracted from these optical flow that capturing both short-term and long-term motion dynamics. We perform power normalization on the magnitude component of optical flow for flow dynamics correction to boost subtle or dampen sudden motions. We show that existing action recognition models which rely on optical flow are able to get performance boosted with our corrected optical flow. To further improve performance, we integrate our corrected flow dynamics into popular models through a simple hallucination step by selecting only the best performing optical flow features, and we show that by 'translating' the CNN feature maps into these optical flow features with different scales of motions leads to the new state-of-the-art performance on several benchmarks including HMDB-51, YUP++, fine-grained action recognition on MPII Cooking Activities, and large-scale Charades.
</details>
<details>
<summary>摘要</summary>
各种研究表明动作认识性能高度取决于提取的动作类型和表达的准确性。在这篇论文中，我们调查了不同的光流，以及从这些光流中提取的短期和长期动作动力特征。我们对光流的幅组分进行功率正规化，以 corrections for flow dynamics。我们显示了现有的基于光流的动作认识模型可以通过我们修正后的光流获得性能提升。为了进一步提高性能，我们将我们修正后的流动动力集成到了流行的模型中，并通过简单的梦幻步骤选择最佳的光流特征，并显示了通过将CNN特征图转换为不同尺度的光流特征来实现新的状态前瞻性表现。这些表现在HMDB-51、YUP++、细化动作认识在MPII Cooking Activities以及大规模Charades等数据集上达到了新的状态前瞻性。
</details></li>
</ul>
<hr>
<h2 id="NASH-A-Simple-Unified-Framework-of-Structured-Pruning-for-Accelerating-Encoder-Decoder-Language-Models"><a href="#NASH-A-Simple-Unified-Framework-of-Structured-Pruning-for-Accelerating-Encoder-Decoder-Language-Models" class="headerlink" title="NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models"></a>NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10054">http://arxiv.org/abs/2310.10054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jongwooko/nash-pruning-official">https://github.com/jongwooko/nash-pruning-official</a></li>
<li>paper_authors: Jongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang, Euijai Ahn, Se-Young Yun</li>
<li>for: 本研究旨在 investigate encoder-decoder 模型中的结构化剪辑方法，以提高执行速度和生成质量。</li>
<li>methods: 本研究采用了分解式剪辑视角，分别对 encoder 和 decoder 组件进行结构化剪辑。</li>
<li>results: 研究发现，减少 encoder 网络中的层数可以提高执行速度，而减少 decoder 网络中的层数可以提高生成质量。基于这些发现，提出了一种简单有效的框架 NASH，可以快速地适应不同的任务和网络架构。<details>
<summary>Abstract</summary>
Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively less explored compared to encoder-only models. In this study, we investigate the behavior of the structured pruning of the encoder-decoder models in the decoupled pruning perspective of the encoder and decoder component, respectively. Our findings highlight two insights: (1) the number of decoder layers is the dominant factor of inference speed, and (2) low sparsity in the pruned encoder network enhances generation quality. Motivated by these findings, we propose a simple and effective framework, NASH, that narrows the encoder and shortens the decoder networks of encoder-decoder models. Extensive experiments on diverse generation and inference tasks validate the effectiveness of our method in both speedup and output quality.
</details>
<details>
<summary>摘要</summary>
《结构化剪除方法在不同的网络架构中，如转换器，有效地减少模型大小和加速推理速度。尽管encoder-decoder模型在许多自然语言处理任务中表现出了多样性，structured pruning方法在这些模型上 however, relatively less explored compared to encoder-only models. 在这个研究中，我们研究了encoder-decoder模型的结构化剪除在解体预测的encoder和decoder组件中的行为。我们的发现指出了两点：（1）解码层数是推理速度的决定因素，和（2）剪除后encoder网络中的低稀畴性提高了生成质量。这些发现使我们提出了一个简单而有效的框架，称为NASH，该框架缩短了encoder网络和缩短了decoder网络。我们在多种生成和推理任务上进行了广泛的实验，并证明了我们的方法在速度和输出质量上都是有效的。》
</details></li>
</ul>
<hr>
<h2 id="Robust-Collaborative-Filtering-to-Popularity-Distribution-Shift"><a href="#Robust-Collaborative-Filtering-to-Popularity-Distribution-Shift" class="headerlink" title="Robust Collaborative Filtering to Popularity Distribution Shift"></a>Robust Collaborative Filtering to Popularity Distribution Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10696">http://arxiv.org/abs/2310.10696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anzhang314/popgo">https://github.com/anzhang314/popgo</a></li>
<li>paper_authors: An Zhang, Wenchang Ma, Jingnan Zheng, Xiang Wang, Tat-seng Chua</li>
<li>for: 本研究旨在提高collaborative filtering（CF）模型的泛化能力，即使训练数据中的媒体短Circle（popularity shortcut）存在。</li>
<li>methods: 本研究提出了一种简单 yet effective的偏差修正策略，称为PopGo，它可以衡量并降低用户-项目对的交互性wise媒体短Circle。PopGo首先学习一个媒体短Circle模型，然后通过对CF模型的预测进行修正来减少媒体短Circle的影响。</li>
<li>results: 对四个benchmark数据集进行了实验，PopGo可以在ID和OOD测试集上取得显著的提升，比如DICE和MACR等已有的偏差修正策略。<details>
<summary>Abstract</summary>
In leading collaborative filtering (CF) models, representations of users and items are prone to learn popularity bias in the training data as shortcuts. The popularity shortcut tricks are good for in-distribution (ID) performance but poorly generalized to out-of-distribution (OOD) data, i.e., when popularity distribution of test data shifts w.r.t. the training one. To close the gap, debiasing strategies try to assess the shortcut degrees and mitigate them from the representations. However, there exist two deficiencies: (1) when measuring the shortcut degrees, most strategies only use statistical metrics on a single aspect (i.e., item frequency on item and user frequency on user aspect), failing to accommodate the compositional degree of a user-item pair; (2) when mitigating shortcuts, many strategies assume that the test distribution is known in advance. This results in low-quality debiased representations. Worse still, these strategies achieve OOD generalizability with a sacrifice on ID performance. In this work, we present a simple yet effective debiasing strategy, PopGo, which quantifies and reduces the interaction-wise popularity shortcut without any assumptions on the test data. It first learns a shortcut model, which yields a shortcut degree of a user-item pair based on their popularity representations. Then, it trains the CF model by adjusting the predictions with the interaction-wise shortcut degrees. By taking both causal- and information-theoretical looks at PopGo, we can justify why it encourages the CF model to capture the critical popularity-agnostic features while leaving the spurious popularity-relevant patterns out. We use PopGo to debias two high-performing CF models (MF, LightGCN) on four benchmark datasets. On both ID and OOD test sets, PopGo achieves significant gains over the state-of-the-art debiasing strategies (e.g., DICE, MACR).
</details>
<details>
<summary>摘要</summary>
领导合作 filtering（CF）模型中，用户和item的表示可能受到训练数据中的媒体偏袋影响，即媒体偏袋短cut。这些媒体偏袋短cut可以在内部数据（ID）上达到好的性能，但是对于外部数据（OOD）来说，媒体偏袋短cut会导致模型的泛化能力差。为了缓解这 gap，去偏袋策略会评估用户和item的媒体偏袋度并 Mitigate它们从表示中。然而，存在两个缺陷：（1）当衡量媒体偏袋度时，大多数策略只使用单一方面的统计指标（例如，item频次和用户频次），而不考虑用户-item对的compositional度;（2）当缓解媒体偏袋时，许多策略假设测试分布已知。这会导致低质量的去偏袋表示。更糟糕的是，这些策略可以在ID性能的代价下实现OOD泛化性能。在这个工作中，我们提出了一种简单 yet effective的去偏袋策略，即PopGo。PopGo会量化和降低用户-item对的交互媒体偏袋度，不需要测试分布的假设。它首先学习一个媒体偏袋模型，并根据用户和item的媒体偏袋表示计算交互媒体偏袋度。然后，它将CF模型通过对预测进行调整来训练。通过从 causal和信息理论的角度看PopGo，我们可以解释它如何鼓励CF模型捕捉重要的媒体偏袋无关特征，而不捕捉媒体偏袋相关的假特征。我们使用PopGo对两种高性能CF模型（MF、LightGCN）在四个benchmark数据集上进行去偏袋。在ID和OOD测试集上，PopGo实现了与状态 искусственный debiasing策略（例如、DICE、MACR）相比较高的性能提升。
</details></li>
</ul>
<hr>
<h2 id="FATE-LLM-A-Industrial-Grade-Federated-Learning-Framework-for-Large-Language-Models"><a href="#FATE-LLM-A-Industrial-Grade-Federated-Learning-Framework-for-Large-Language-Models" class="headerlink" title="FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models"></a>FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10049">http://arxiv.org/abs/2310.10049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FederatedAI/FATE-LLM">https://github.com/FederatedAI/FATE-LLM</a></li>
<li>paper_authors: Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang<br>for: 这个论文的目的是提出一个industrial-grade federated learning框架，以便在实际应用中使用大型自然语言模型（LLMs）。methods: 这个框架使用了 parameter-efficient fine-tuning方法来有效地训练大型自然语言模型，并且运用了隐私保护机制来保护知识产权和数据隐私。results: 这个框架能够解决训练大型自然语言模型所需的巨量 Computing资源和高质量数据的问题，并且能够保护知识产权和数据隐私。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research of FedLLM and enable a broad range of industrial applications.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT、LLaMA、GLM和PaLM，在过去的几年中表现出了很好的表现。然而，LLM在实际应用中遇到了两个主要挑战。一个挑战是在小到中型企业的限制计算资源下培训LLM，这限制了LLM的应用。另一个挑战是培训LLM需要大量高质量数据，这些数据经常分散在企业中。为解决这些挑战，我们提出了FATE-LLM，一个工业级联合学习框架 для大型语言模型。FATE-LLM（1）实现联合学习 для大型语言模型（称为FedLLM）；（2）提高FedLLM的高效培训方法；（3）保护LLM的知识产权；（4）在训练和推断过程中保护数据隐私。我们在GitHub上发布了FATE-LLM的代码，以便研究FedLLM和推广各种工业应用。
</details></li>
</ul>
<hr>
<h2 id="TRANSOM-An-Efficient-Fault-Tolerant-System-for-Training-LLMs"><a href="#TRANSOM-An-Efficient-Fault-Tolerant-System-for-Training-LLMs" class="headerlink" title="TRANSOM: An Efficient Fault-Tolerant System for Training LLMs"></a>TRANSOM: An Efficient Fault-Tolerant System for Training LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10046">http://arxiv.org/abs/2310.10046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SenseCore/transom-checkpoint-engine">https://github.com/SenseCore/transom-checkpoint-engine</a></li>
<li>paper_authors: Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li</li>
<li>for: 提高大型语言模型（LLM）的训练效率，解决大规模参数训练过程中的硬件和软件故障问题。</li>
<li>methods: 提出了一个新的故障tolerant LLM 训练系统，包括三个关键子系统：training pipeline automatic fault tolerance and recovery mechanism（TOL）、training task multi-dimensional metric automatic anomaly detection system（TEE）和training checkpoint asynchronous access automatic fault tolerance and recovery technology（TCE）。</li>
<li>results: 实验结果表明， tranSOM 可以显著提高大规模 LLM 训练的效率，Specifically, GPT3-175B 的预训练时间被降低了28%，而 asynchronous checkpoint saving and loading 性能提高了20倍。<details>
<summary>Abstract</summary>
Large language models (LLMs) with hundreds of billions or trillions of parameters, represented by chatGPT, have achieved profound impact on various fields. However, training LLMs with super-large-scale parameters requires large high-performance GPU clusters and long training periods lasting for months. Due to the inevitable hardware and software failures in large-scale clusters, maintaining uninterrupted and long-duration training is extremely challenging. As a result, A substantial amount of training time is devoted to task checkpoint saving and loading, task rescheduling and restart, and task manual anomaly checks, which greatly harms the overall training efficiency. To address these issues, we propose TRANSOM, a novel fault-tolerant LLM training system. In this work, we design three key subsystems: the training pipeline automatic fault tolerance and recovery mechanism named Transom Operator and Launcher (TOL), the training task multi-dimensional metric automatic anomaly detection system named Transom Eagle Eye (TEE), and the training checkpoint asynchronous access automatic fault tolerance and recovery technology named Transom Checkpoint Engine (TCE). Here, TOL manages the lifecycle of training tasks, while TEE is responsible for task monitoring and anomaly reporting. TEE detects training anomalies and reports them to TOL, who automatically enters the fault tolerance strategy to eliminate abnormal nodes and restart the training task. And the asynchronous checkpoint saving and loading functionality provided by TCE greatly shorten the fault tolerance overhead. The experimental results indicate that TRANSOM significantly enhances the efficiency of large-scale LLM training on clusters. Specifically, the pre-training time for GPT3-175B has been reduced by 28%, while checkpoint saving and loading performance have improved by a factor of 20.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如chatGPT，已经在不同领域 achieve 了深见的影响。然而，在超大型参数的训练中，需要大型高性能GPU集群和长时间的训练时间，达到月份甚至更长。由于大型集群中的硬件和软件故障是不可避免的，因此在长时间训练中保持无间断和长时间训练是极其困难。为此，我们提出了 TRANSOM，一种新的 fault-tolerant LLM 训练系统。在这项工作中，我们设计了三个关键子系统：训练管道自动过错tolerance和恢复机制（TOL），任务多维度自动异常检测系统（TEE），以及训练checkpoint异步访问自动过错tolerance和恢复技术（TCE）。在这里，TOL负责训练任务的生命周期管理，而TEE负责任务监控和异常报告。TEE检测训练异常并将其报告给TOL，TOL会自动入口过错策略，消除异常节点并重新启动训练任务。而TCE提供的异步checkpoint存储和加载功能，可以大大减少过错过程的负担。实验结果表明，TRANSOM可以大幅提高大规模 LL M 训练的效率。Specifically，对于GPT3-175B的预训练时间，可以提高28%，而checkpoint存储和加载性能可以提高20倍。
</details></li>
</ul>
<hr>
<h2 id="Smart-City-Transportation-Deep-Learning-Ensemble-Approach-for-Traffic-Accident-Detection"><a href="#Smart-City-Transportation-Deep-Learning-Ensemble-Approach-for-Traffic-Accident-Detection" class="headerlink" title="Smart City Transportation: Deep Learning Ensemble Approach for Traffic Accident Detection"></a>Smart City Transportation: Deep Learning Ensemble Approach for Traffic Accident Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10038">http://arxiv.org/abs/2310.10038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed</li>
<li>for: 这篇论文旨在探讨现有的交通事故检测技术，以提高城市智能化交通管理系统中的安全性和效率。</li>
<li>methods: 该论文提出了一种新的I3D-CONVLSTM2D模型架构，该模型结合RGB帧和光流信息，特意为城市智能化交通监测系统设计，并通过实验研究证明了该模型的高效性。</li>
<li>results: 该论文的实验分析表明，I3D-CONVLSTM2D RGB + Optical-Flow (Trainable)模型在交通事故检测方面表现出色，MAP值达到87%。同时，论文还探讨了数据偏置问题，并提出了解决方案。<details>
<summary>Abstract</summary>
The dynamic and unpredictable nature of road traffic necessitates effective accident detection methods for enhancing safety and streamlining traffic management in smart cities. This paper offers a comprehensive exploration study of prevailing accident detection techniques, shedding light on the nuances of other state-of-the-art methodologies while providing a detailed overview of distinct traffic accident types like rear-end collisions, T-bone collisions, and frontal impact accidents. Our novel approach introduces the I3D-CONVLSTM2D model architecture, a lightweight solution tailored explicitly for accident detection in smart city traffic surveillance systems by integrating RGB frames with optical flow information. Our experimental study's empirical analysis underscores our approach's efficacy, with the I3D-CONVLSTM2D RGB + Optical-Flow (Trainable) model outperforming its counterparts, achieving an impressive 87\% Mean Average Precision (MAP). Our findings further elaborate on the challenges posed by data imbalances, particularly when working with a limited number of datasets, road structures, and traffic scenarios. Ultimately, our research illuminates the path towards a sophisticated vision-based accident detection system primed for real-time integration into edge IoT devices within smart urban infrastructures.
</details>
<details>
<summary>摘要</summary>
随着城市智能化的发展，道路交通中的事故检测技术已成为提高安全性和优化交通管理的关键。本文进行了全面的探讨现有事故检测技术，探讨其他现代方法的细节，并提供了不同类型的交通事故的详细概述，如后尾collisions、T-bone collisions和前面Collisions。我们的新方法 introduce了I3D-CONVLSTM2D模型架构，这是一种适应性强的解决方案，通过RGB框架和光流信息来检测事故。我们的实验研究的实证分析表明，我们的I3D-CONVLSTM2D RGB + Optical-Flow（可训练）模型在事故检测方面表现出色，达到了87%的 Mean Average Precision（MAP）。我们的发现还探讨了数据不均衡的挑战，特别是在有限数据集、路径结构和交通场景下。最后，我们的研究阐明了一种基于视觉的事故检测系统，准备好于实时集成到智能城市基础设施中的边缘IoT设备。
</details></li>
</ul>
<hr>
<h2 id="Bootstrap-Your-Own-Skills-Learning-to-Solve-New-Tasks-with-Large-Language-Model-Guidance"><a href="#Bootstrap-Your-Own-Skills-Learning-to-Solve-New-Tasks-with-Large-Language-Model-Guidance" class="headerlink" title="Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance"></a>Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10021">http://arxiv.org/abs/2310.10021</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clvrai/boss">https://github.com/clvrai/boss</a></li>
<li>paper_authors: Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, Joseph J. Lim<br>for:BOSS is designed to solve new long-horizon, complex, and meaningful tasks with minimal supervision.methods:BOSS uses skill bootstrapping, where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. The bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together.results:Agents trained with the LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments.<details>
<summary>Abstract</summary>
We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments. Website at clvrai.com/boss.
</details>
<details>
<summary>摘要</summary>
我们提出了BOSS方法，它可以自动学习解决新的长期、复杂和有意义的任务，只需最小的监督。现有的循环学习方法通常需要专家指导，通过示例或丰富的奖励函数来学习长期任务。相比之下，我们的BOSS方法通过“技能启动”来学习新任务，其中一个已有技能的机器人与环境互动，不接受任务外部的奖励反馈，而是通过大型自然语言模型（LLM）指导，学习新的技能链接。通过这个过程，BOSS可以从基本的原始技能中拓宽许多复杂和有用的行为。我们通过实验表明，使用我们的LLM指导的启动过程训练的机器人在未看过任务和环境的情况下，可以在逻辑家庭环境中比静态训练和先前的无监督技能获取方法表现出更好的成绩。更多信息可以通过官方网站clvrai.com/boss。
</details></li>
</ul>
<hr>
<h2 id="Towards-Unified-and-Effective-Domain-Generalization"><a href="#Towards-Unified-and-Effective-Domain-Generalization" class="headerlink" title="Towards Unified and Effective Domain Generalization"></a>Towards Unified and Effective Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10008">http://arxiv.org/abs/2310.10008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/invictus717/UniDG">https://github.com/invictus717/UniDG</a></li>
<li>paper_authors: Yiyuan Zhang, Kaixiong Gong, Xiaohan Ding, Kaipeng Zhang, Fangrui Lv, Kurt Keutzer, Xiangyu Yue</li>
<li>for: 提高基础模型对不同领域的扩展性和泛化性性能</li>
<li>methods: 基于无监督学习的方法，在推理阶段进行轻量级微调，以避免训练阶段的极端忘却</li>
<li>results: 在12种视觉基础模型上，包括CNN、MLP和Transformer等，平均提高了+5.4%的精度，证明UniDG的多样性和优势。<details>
<summary>Abstract</summary>
We propose $\textbf{UniDG}$, a novel and $\textbf{Uni}$fied framework for $\textbf{D}$omain $\textbf{G}$eneralization that is capable of significantly enhancing the out-of-distribution generalization performance of foundation models regardless of their architectures. The core idea of UniDG is to finetune models during the inference stage, which saves the cost of iterative training. Specifically, we encourage models to learn the distribution of test data in an unsupervised manner and impose a penalty regarding the updating step of model parameters. The penalty term can effectively reduce the catastrophic forgetting issue as we would like to maximally preserve the valuable knowledge in the original model. Empirically, across 12 visual backbones, including CNN-, MLP-, and Transformer-based models, ranging from 1.89M to 303M parameters, UniDG shows an average accuracy improvement of +5.4% on DomainBed. These performance results demonstrate the superiority and versatility of UniDG. The code is publicly available at https://github.com/invictus717/UniDG
</details>
<details>
<summary>摘要</summary>
我们提出了UniDG，一个新的、统一的框架，可以对基础模型的外部泛化性能进行明显改善，不论其架构。UniDG的核心思想是在推断阶段进行调整，这样可以避免迭代训练的成本。具体来说，我们鼓励模型在无监督下学习试验数据的分布，并对模型参数更新的步骤加入一个罚则。这个罚则可以有效减少严重遗忘问题，因为我们希望将原始模型中的有价知识保留到最大程度。实验结果显示，在12种视觉基础模型中，包括CNN、MLP和Transformer等，参数量从1.89M到303M之间，UniDG在DomainBed上平均提高了5.4%的精度。这些表现结果证明UniDG的优越性和多样性。代码可以在https://github.com/invictus717/UniDG上获取。
</details></li>
</ul>
<hr>
<h2 id="Forecaster-Towards-Temporally-Abstract-Tree-Search-Planning-from-Pixels"><a href="#Forecaster-Towards-Temporally-Abstract-Tree-Search-Planning-from-Pixels" class="headerlink" title="Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels"></a>Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09997">http://arxiv.org/abs/2310.09997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Jiralerspong, Flemming Kondrup, Doina Precup, Khimya Khetarpal</li>
<li>for: 本研究旨在提高深度层次优化学习 Agent 的Sample Efficiency，使其在高维状态空间中具有远景见准能力，从而更好地学习和做出决策。</li>
<li>methods: 本研究提出了 Forecaster，一种深度层次优化学习方法，该方法利用高级目标进行规划，并通过模拟环境动力学特性来学习抽象世界模型。</li>
<li>results: 实验表明，Forecaster 可以在 AntMaze domain 中实现更好的 Sample Efficiency，并且可以在新任务下进行普适化学习。<details>
<summary>Abstract</summary>
The ability to plan at many different levels of abstraction enables agents to envision the long-term repercussions of their decisions and thus enables sample-efficient learning. This becomes particularly beneficial in complex environments from high-dimensional state space such as pixels, where the goal is distant and the reward sparse. We introduce Forecaster, a deep hierarchical reinforcement learning approach which plans over high-level goals leveraging a temporally abstract world model. Forecaster learns an abstract model of its environment by modelling the transitions dynamics at an abstract level and training a world model on such transition. It then uses this world model to choose optimal high-level goals through a tree-search planning procedure. It additionally trains a low-level policy that learns to reach those goals. Our method not only captures building world models with longer horizons, but also, planning with such models in downstream tasks. We empirically demonstrate Forecaster's potential in both single-task learning and generalization to new tasks in the AntMaze domain.
</details>
<details>
<summary>摘要</summary>
agent的多级划分能力使其能够预测长期后果，从而实现样本效率学习。这特别有用在高维状态空间如像素的复杂环境中，目标远距离，奖励罕见。我们介绍了Forecaster，一种深层决策学习方法，通过高级目标规划来规划高级目标。Forecaster使用抽象世界模型来模型环境的过程动态，并在 such transition 上训练世界模型。它然后使用这个世界模型来选择优质高级目标，并通过树搜索规划算法来实现。此外，它还训练低级策略，以实现高级目标。我们的方法不仅能够建立更长期的世界模型，还能够在下游任务中使用这些模型进行规划。我们在AntMaze领域进行了实验，证明了Forecaster的潜力。
</details></li>
</ul>
<hr>
<h2 id="Network-Analysis-of-the-iNaturalist-Citizen-Science-Community"><a href="#Network-Analysis-of-the-iNaturalist-Citizen-Science-Community" class="headerlink" title="Network Analysis of the iNaturalist Citizen Science Community"></a>Network Analysis of the iNaturalist Citizen Science Community</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10693">http://arxiv.org/abs/2310.10693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Lu Liu, Thomas Jiralerspong</li>
<li>for: 本研究使用iNaturalist citizen science平台作为案例研究，探讨公民科学项目之间的结构和互动方式。</li>
<li>methods: 我们将iNaturalist数据表示为两类网络，并使用视觉化和已知网络科学技术来获得公民科学项目结构和用户互动的新的视角。</li>
<li>results: 我们提出了一种新的网络准则，使用iNaturalist数据创建一个独特的网络结构，并通过链接预测任务表明这个网络可以用于探讨多种网络科学方法的新思路。<details>
<summary>Abstract</summary>
In recent years, citizen science has become a larger and larger part of the scientific community. Its ability to crowd source data and expertise from thousands of citizen scientists makes it invaluable. Despite the field's growing popularity, the interactions and structure of citizen science projects are still poorly understood and under analyzed. We use the iNaturalist citizen science platform as a case study to analyze the structure of citizen science projects. We frame the data from iNaturalist as a bipartite network and use visualizations as well as established network science techniques to gain insights into the structure and interactions between users in citizen science projects. Finally, we propose a novel unique benchmark for network science research by using the iNaturalist data to create a network which has an unusual structure relative to other common benchmark networks. We demonstrate using a link prediction task that this network can be used to gain novel insights into a variety of network science methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.AI_2023_10_16/" data-id="clpxp6bx3005pee881drp4ucs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.CL_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T11:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.CL_2023_10_16/">cs.CL - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="IDEAL-Influence-Driven-Selective-Annotations-Empower-In-Context-Learners-in-Large-Language-Models"><a href="#IDEAL-Influence-Driven-Selective-Annotations-Empower-In-Context-Learners-in-Large-Language-Models" class="headerlink" title="IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models"></a>IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10873">http://arxiv.org/abs/2310.10873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skzhang1/IDEAL">https://github.com/skzhang1/IDEAL</a></li>
<li>paper_authors: Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-Hao Chen, Jiale Liu, Qingyun Wu, Tongliang Liu<br>for: This paper aims to address the challenge of high annotation costs in in-context learning by introducing an influence-driven selective annotation method.methods: The proposed method constructs a directed graph to represent unlabeled data, quantifies the influence of candidate unlabeled subsets using a diffusion process, and selects the most influential subsets using a greedy algorithm.results: The proposed method achieves better performance under lower time consumption during subset selection compared to previous efforts on selective annotations. Experiments confirm the superiority of the proposed method on various benchmarks.Here’s the Chinese translation of the three points:for: 这篇论文目标是解决启动学习中的注释成本高的挑战，提出了一种基于影响的选择性注释方法。methods: 该方法首先构建了一个指向图来表示无标示数据，然后使用扩散过程来衡量候选无标示子集的影响，并使用一种简单 yet effective的批处算法来选择最有影响的子集。results: 该方法在不同的标准 bencmark 上达到了更高的性能，而且在选择子集时间上具有更低的时间投入。实验证明了该方法的优越性。<details>
<summary>Abstract</summary>
In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influence-driven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.
</details>
<details>
<summary>摘要</summary>
内容学习是一种有前途的概念，它利用内容例子作为大型语言模型的预测提示。这些提示是实现强制性的关键，但是因为需要从大量的标注例子中抽取提示，因此找到正确的提示可能会带来高的标注成本。为解决这个挑战，本研究将引入一种影响驱动的选择性标注方法，以降低标注成本而提高内容例子的质量。本方法的核心思想是从大规模的未标注数据池中选择一个关键子集，并将其标注以供后续的提示抽取。首先， constructed 一个导向的图来表示未标注数据。接着， candidate 的未标注子集之间的影响被评估通过一个传播过程。最后，一个简单 yet effective 的对不标注数据选择法是引入，它在每次选择时会选择具有最大 MARGINAL 增长的数据。与先前的选择性标注方法不同，我们的影响驱动方法在端到端方式下进行，避免了一个不可能的明确平衡 между 数据多样性和代表性，并且受到了理论支持。实验确认了我们提出的方法在不同的benchmark上的超越性，在选择subset时间consumption下得到了更好的性能。更多信息可以通过我们的项目页面（https://skzhang1.github.io/IDEAL/）了解。
</details></li>
</ul>
<hr>
<h2 id="Will-the-Prince-Get-True-Love’s-Kiss-On-the-Model-Sensitivity-to-Gender-Perturbation-over-Fairytale-Texts"><a href="#Will-the-Prince-Get-True-Love’s-Kiss-On-the-Model-Sensitivity-to-Gender-Perturbation-over-Fairytale-Texts" class="headerlink" title="Will the Prince Get True Love’s Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts"></a>Will the Prince Get True Love’s Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10865">http://arxiv.org/abs/2310.10865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christina Chance, Da Yin, Dakuo Wang, Kai-Wei Chang</li>
<li>for: 本研究旨在探讨传统童话中存在的性别偏见，以及语言模型学习到的这些偏见是如何影响其性别认知的。</li>
<li>methods: 本研究使用Counterfactual数据增强技术来评估语言模型对性别变化的Robustness。Specifically，我们使用FairytaleQA数据集进行问答任务，并在训练时引入Counterfactual性别刻板印象以降低学习到的偏见。</li>
<li>results: 我们的实验结果显示，模型对性别变化具有敏感性，在原始测试集比较性别偏见的情况下，模型的性能会明显下降。但是，在先进行Counterfactual训练 dataset的 fine-tuning 后，模型对后来引入的Anti-性别刻板文本变得更加敏感。<details>
<summary>Abstract</summary>
Recent studies show that traditional fairytales are rife with harmful gender biases. To help mitigate these gender biases in fairytales, this work aims to assess learned biases of language models by evaluating their robustness against gender perturbations. Specifically, we focus on Question Answering (QA) tasks in fairytales. Using counterfactual data augmentation to the FairytaleQA dataset, we evaluate model robustness against swapped gender character information, and then mitigate learned biases by introducing counterfactual gender stereotypes during training time. We additionally introduce a novel approach that utilizes the massive vocabulary of language models to support text genres beyond fairytales. Our experimental results suggest that models are sensitive to gender perturbations, with significant performance drops compared to the original testing set. However, when first fine-tuned on a counterfactual training dataset, models are less sensitive to the later introduced anti-gender stereotyped text.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CoTFormer-More-Tokens-With-Attention-Make-Up-For-Less-Depth"><a href="#CoTFormer-More-Tokens-With-Attention-Make-Up-For-Less-Depth" class="headerlink" title="CoTFormer: More Tokens With Attention Make Up For Less Depth"></a>CoTFormer: More Tokens With Attention Make Up For Less Depth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10845">http://arxiv.org/abs/2310.10845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirkeivan Mohtashami, Matteo Pagliardini, Martin Jaggi</li>
<li>for: 本文目的是提出一种基于链式思维（Chain-of-Thought，CoT）机制的 transformer 变体，以实现与更深的模型性能相似的表现。</li>
<li>methods: 本文使用了一种做为链式思维机制的假设，并基于此假设提出了一种名为 CoTFormer 的 transformer 变体。</li>
<li>results: 实验结果表明，CoTFormer 能够与更深的标准 transformer 相比，在多个任务上表现更好。<details>
<summary>Abstract</summary>
The race to continually develop ever larger and deeper foundational models is underway. However, techniques like the Chain-of-Thought (CoT) method continue to play a pivotal role in achieving optimal downstream performance. In this work, we establish an approximate parallel between using chain-of-thought and employing a deeper transformer. Building on this insight, we introduce CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to achieve capacity comparable to a deeper model. Our empirical findings demonstrate the effectiveness of CoTFormers, as they significantly outperform larger standard transformers.
</details>
<details>
<summary>摘要</summary>
“Foundational models”的竞赛不断地进行开发，但“Chain-of-Thought”（CoT）方法仍然扮演着关键的角色，以获得最佳的下游性能。在这个研究中，我们发现使用Chain-of-thought和使用更深的transformer之间存在一种近似的关系。基于这个意识，我们介绍CoTFormer，一种使用隐式CoT-like机制的transformer变体，以获得与更深的模型相同的容量。我们的实验结果显示CoTFormer具有明显的超越性，与标准的transformer模型相比。
</details></li>
</ul>
<hr>
<h2 id="Survey-of-Vulnerabilities-in-Large-Language-Models-Revealed-by-Adversarial-Attacks"><a href="#Survey-of-Vulnerabilities-in-Large-Language-Models-Revealed-by-Adversarial-Attacks" class="headerlink" title="Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"></a>Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10844">http://arxiv.org/abs/2310.10844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh</li>
<li>for: 这篇论文探讨了对大语言模型（LLMs）的敌意攻击的研究，以及如何使AI系统更加可靠。</li>
<li>methods: 论文使用了多种学习结构，包括文本只攻击、多模态攻击和复杂系统特有的攻击方法，以探讨LLMs的安全性问题。</li>
<li>results: 论文提供了LLMs的安全性问题的概述，以及现有研究的总结和可能的防御策略。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在architecture和能力方面快速进步，因此批判它们的安全性特性的必要性也在增加。这篇论文对抗AI系统中的攻击进行了评估，这是一种涉及自然语言处理和安全的新兴领域。先前的研究表明，即使通过 instrucion 调整和人工反馈来实现安全性的LLM也可能受到攻击，这些攻击利用模型的弱点并诱导AI系统出错，例如 chatGPT 和 Bard 上的 "监狱" 攻击。在这篇论文中，我们首先提供了大型语言模型的概述，描述了它们的安全性，然后根据不同的学习结构进行了分类：只有文本攻击、多模态攻击以及特定复杂系统的攻击方法，如联合学习或多代理系统。我们还提供了关于漏洞的基本来源和防御措施的评论。为了让这个领域更加Accessible，我们提供了一个系统性的回顾现有工作，一种结构化的攻击概念 typology，以及其他资源，包括与相关话题的PowerPoint演示在ACL'24年会上。
</details></li>
</ul>
<hr>
<h2 id="Fake-News-in-Sheep’s-Clothing-Robust-Fake-News-Detection-Against-LLM-Empowered-Style-Attacks"><a href="#Fake-News-in-Sheep’s-Clothing-Robust-Fake-News-Detection-Against-LLM-Empowered-Style-Attacks" class="headerlink" title="Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks"></a>Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10830">http://arxiv.org/abs/2310.10830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaying Wu, Bryan Hooi</li>
<li>for: 这篇论文旨在解决大型自然语言模型（LLM）驱动的新闻假消息探测问题，以提高在线新闻环境中自动检测的精度。</li>
<li>methods: 这篇论文提出了一种基于新闻媒体的探测方法，通过使用 style-oriented reframing 技术和内置的大型自然语言模型（LLM），实现对新闻写作风格的适应性。</li>
<li>results: 实验结果表明，这种方法可以在三个 benchmark 数据集上提供显著的改进，并增强对 LLM 驱动的新闻假消息探测的抗性。<details>
<summary>Abstract</summary>
It is commonly perceived that online fake news and reliable news exhibit stark differences in writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the rise of powerful Large Language Models (LLMs) has enabled malicious users to mimic the style of trustworthy news outlets at minimal cost. Our analysis reveals that LLM-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), posing a significant challenge for automated detection in online ecosystems. To address this, we introduce SheepDog, a style-agnostic fake news detector robust to news writing styles. SheepDog achieves this adaptability through LLM-empowered news reframing, which customizes each article to match different writing styles using style-oriented reframing prompts. By employing style-agnostic training, SheepDog enhances its resilience to stylistic variations by maximizing prediction consistency across these diverse reframings. Furthermore, SheepDog extracts content-focused veracity attributions from LLMs, where the news content is evaluated against a set of fact-checking rationales. These attributions provide supplementary information and potential interpretability that assist veracity prediction. On three benchmark datasets, empirical results show that SheepDog consistently yields significant improvements over competitive baselines and enhances robustness against LLM-empowered style attacks.
</details>
<details>
<summary>摘要</summary>
通常认为在线假新闻和可靠新闻的写作风格有很大差异，如使用感人化语言 versus  объектив语言。然而，我们强调的是风格相关特征也可以被利用于风格基本攻击。尤其是现在强大的大语言模型（LLMs）的出现，使得恶意用户可以轻松地模仿可靠新闻机构的风格，对于自动检测在线环境中具有重大挑战。为了解决这一问题，我们介绍了羊狗（SheepDog），一种不受风格限制的假新闻检测器，可以在不同的新闻风格下保持高度的稳定性。羊狗通过使用 LLMS 进行新闻重 framings，以适应不同的新闻风格，并通过风格无关的训练来增强其对风格变化的抗性。此外，羊狗使用 LLMS 提供的内容相关的真实性评估，对新闻内容进行了实际的真实性评估，以提供可靠的假新闻检测。在三个 benchmark 数据集上，实验结果表明，羊狗可以与竞争对手相比，提供显著的改善，并增强了对 LLMS 风格基本攻击的抗性。
</details></li>
</ul>
<hr>
<h2 id="SD-HuBERT-Self-Distillation-Induces-Syllabic-Organization-in-HuBERT"><a href="#SD-HuBERT-Self-Distillation-Induces-Syllabic-Organization-in-HuBERT" class="headerlink" title="SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT"></a>SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10803">http://arxiv.org/abs/2310.10803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheol Jun Cho, Abdelrahman Mohamed, Shang-Wen Li, Alan W Black, Gopala K. Anumanchipalli</li>
<li>for: 这篇论文旨在探讨自然语言处理中的自适应学习（SSL）技术，具体来说是检测和分析发音中的句子水平表示。</li>
<li>methods: 作者采用了自我混合对象函数（self-distillation）来练化预训练的HuBERT模型，并使用汇集token来概括整个句子。无需任何监督，模型能够自动从发音中找到定义的边界，并在不同帧中显示出standing的句子结构。</li>
<li>results: 作者的模型在无监督情况下自动找到了发音中的句子结构，并且与实际的句子结构大致匹配。此外，作者还提出了一个新的评价任务——Spoken Speech ABX，用于评估发音中的句子表示。与之前的模型相比，作者的模型在这两个任务中表现出色。<details>
<summary>Abstract</summary>
Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space, limiting the utility of SSL representations. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt "self-distillation" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames show salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.
</details>
<details>
<summary>摘要</summary>
<<SYS>>自动发现单元在自注意力学习（SSL）中的语音处理已经进入了新的时代。然而，发现的单元经常保留在音位空间，这限制了SSL表示的用途。我们在这里示出，在学习 sentence-level 表示的语音中，一种 syllabic 组织structure  emerges。具体来说，我们采用 "self-distillation" 目标来练化预训练 HuBERT 的汇总符号，该符号概括整个句子。无需任何超级视图，得到的模型可以在语音中画定界限，并且在帧中的表示显示出了鲜明的 syllabic 结构。我们示出，这 emergent structure 与真实的 syllables 大致匹配。此外，我们提出了一个新的 benchmark 任务，Spoken Speech ABX，用于评估 sentence-level 表示的语音。与前一代模型相比，我们的模型在无监督 syllable 发现和 sentence-level 表示学习方面表现出色。总之，我们示出了 HuBERT 的自注意力学习可以不依赖于外部标签或模式，并可能提供一种新的数据驱动单元 для spoken language modeling。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Models-of-Speech-Infer-Universal-Articulatory-Kinematics"><a href="#Self-Supervised-Models-of-Speech-Infer-Universal-Articulatory-Kinematics" class="headerlink" title="Self-Supervised Models of Speech Infer Universal Articulatory Kinematics"></a>Self-Supervised Models of Speech Infer Universal Articulatory Kinematics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10788">http://arxiv.org/abs/2310.10788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hermannovski/React">https://github.com/Hermannovski/React</a></li>
<li>paper_authors: Cheol Jun Cho, Abdelrahman Mohamed, Alan W Black, Gopala K. Anumanchipalli</li>
<li>for: 这个论文旨在探讨自动学习（SSL）基于模型在语音识别任务上的表现，以及这些模型内部表征与语音相关的关系。</li>
<li>methods: 这个论文使用了许多现代的探索技术来探索SSL模型的内部表征，包括HuBERT模型。</li>
<li>results: 研究发现，SSL模型具有一种叫做“语音生成动力学”的基本属性，即将语音信号转换为生成语音的动力学过程。此外，这种属性在不同语言训练数据上具有相似性，并且可以通过简单的仿射变换转移到不同的发音者、语言和方言上。这些结果为语音工程领域中SSL模型的性能提供了新的理解和应用前景，同时也为语音科学领域的研究提供了新的可能性。<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun "probing" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show "inference of articulatory kinematics" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the internals of SSL models that are critical to their superior performance, and open up new avenues into language-agnostic universal models for speech engineering, that are interpretable and grounded in speech science.
</details>
<details>
<summary>摘要</summary>
自顾学学习（SSL）基于模型的语音表现非常出色，但这些顶尖模型一直保持了黑盒模型的状态，许多最近的研究开始使用 HuBERT 等模型进行探测，以 correlate 其内部表示与不同的语音特征。在这篇论文中，我们显示了 SSL 模型中的 "语音生成动态推理" 的基本性质，即将听音信号转化为生成语音的 causal 生成动态。此外，我们还发现这种抽象在训练数据语言上具有很大的 overlap，尤其是在语音系统相似性方面。此外，我们还发现通过简单的仿射变换，可以在不同的说话者、语言和方言之间传递 AAI，这表明这种性质具有普适性。总之，这些结果为 SSL 模型的内部结构提供了新的灯光，并开启了新的语言不受限制的通用模型，这些模型可以解释并基于语音科学。
</details></li>
</ul>
<hr>
<h2 id="BanglaNLP-at-BLP-2023-Task-1-Benchmarking-different-Transformer-Models-for-Violence-Inciting-Text-Detection-in-Bengali"><a href="#BanglaNLP-at-BLP-2023-Task-1-Benchmarking-different-Transformer-Models-for-Violence-Inciting-Text-Detection-in-Bengali" class="headerlink" title="BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali"></a>BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10781">http://arxiv.org/abs/2310.10781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saumajit Saha, Albert Nanda</li>
<li>for: 这个论文是关于推荐一种用于检测孟加拉语挑衅文本的系统。</li>
<li>methods: 这个系统使用了传统和现代方法，以便让模型学习。</li>
<li>results: 我们的提议系统可以判断给定文本是否含有任何威胁。我们对数据增强的影响进行了研究，并对多种转换器-基础模型进行了评估。我们在测试集上 obtained a macro F1 score of 68.11%，在共享任务中排名第23名。<details>
<summary>Abstract</summary>
This paper presents the system that we have developed while solving this shared task on violence inciting text detection in Bangla. We explain both the traditional and the recent approaches that we have used to make our models learn. Our proposed system helps to classify if the given text contains any threat. We studied the impact of data augmentation when there is a limited dataset available. Our quantitative results show that finetuning a multilingual-e5-base model performed the best in our task compared to other transformer-based architectures. We obtained a macro F1 of 68.11\% in the test set and our performance in this shared task is ranked at 23 in the leaderboard.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-reducing-hallucination-in-extracting-information-from-financial-reports-using-Large-Language-Models"><a href="#Towards-reducing-hallucination-in-extracting-information-from-financial-reports-using-Large-Language-Models" class="headerlink" title="Towards reducing hallucination in extracting information from financial reports using Large Language Models"></a>Towards reducing hallucination in extracting information from financial reports using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10760">http://arxiv.org/abs/2310.10760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhaskarjit Sarmah, Tianjie Zhu, Dhagash Mehta, Stefano Pasquali</li>
<li>for: 提高财务报告中问答部分的信息提取效率和准确率，以便更好地进行投资决策和分析。</li>
<li>methods: 使用大语言模型（LLMs）来快速和高精度地提取财务报告 транскрипts中的信息，并通过结合检索增强生成技术和元数据来减少幻觉。</li>
<li>results: 对多种LLMs进行比较，并employs objective metrics for evaluating Q&amp;A systems to demonstrate the superiority of our proposed approach.<details>
<summary>Abstract</summary>
For a financial analyst, the question and answer (Q\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\&A systems, and empirically demonstrate superiority of our method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Building-Persona-Consistent-Dialogue-Agents-with-Offline-Reinforcement-Learning"><a href="#Building-Persona-Consistent-Dialogue-Agents-with-Offline-Reinforcement-Learning" class="headerlink" title="Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning"></a>Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10735">http://arxiv.org/abs/2310.10735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ryanshea10/personachat_offline_rl">https://github.com/ryanshea10/personachat_offline_rl</a></li>
<li>paper_authors: Ryan Shea, Zhou Yu</li>
<li>for: 提高对话系统的自然语言对话品质和个性化度</li>
<li>methods: 使用离线学习 reinforcement learning 方法，将supervised learning 和 online reinforcement learning 的优点结合在一起，并 introduce 一种减少重要性权重的自适应重要性 sampling 方法</li>
<li>results: 对一个现有的社交聊天机器人进行自动和人类评估，结果显示，该方法可以提高对话系统的自然语言对话品质和个性化度<details>
<summary>Abstract</summary>
Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show that our framework improves both the persona consistency and dialogue quality of a state-of-the-art social chatbot.
</details>
<details>
<summary>摘要</summary>
保持一致的人格是对任何开放领域对话系统的关键质量。现状之 artifical intelligence 系统通常通过经过监督学习或在线强化学习（RL）训练来实现这一目标。然而，通过监督学习训练的系统经常缺乏一致性，因为它们从来没有受到违反的惩罚。额外的 RL 训练可以减轻一些这些问题，但训练过程是昂贵的。因此，我们提出了一个Offline RL框架，以提高对话系统的人格一致性。我们的框架允许我们将supervised learning中的优点与RL中的优点结合起来，并且可以廉价地在现有数据上训练我们的模型。我们还提出了一种简单的重要性抽样方法，以减少偏移重要性抽样的方差，我们称之为“Variance-Reducing MLE-Initialized”（VaRMI）重要性抽样。我们的自动和人类评估表明，我们的框架可以提高一个现有社交聊天机器人的人格一致性和对话质量。
</details></li>
</ul>
<hr>
<h2 id="“Mistakes-Help-Us-Grow”-Facilitating-and-Evaluating-Growth-Mindset-Supportive-Language-in-Classrooms"><a href="#“Mistakes-Help-Us-Grow”-Facilitating-and-Evaluating-Growth-Mindset-Supportive-Language-in-Classrooms" class="headerlink" title="“Mistakes Help Us Grow”: Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms"></a>“Mistakes Help Us Grow”: Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10637">http://arxiv.org/abs/2310.10637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Handa, Margaret Clapper, Jessica Boyle, Rose E Wang, Diyi Yang, David S Yeager, Dorottya Demszky</li>
<li>for: 这个论文目的是探讨使用大自然语言模型（LLM）提供自动化、个性化的教师培训，以促进教师的成长心理语言支持（GMSL）。</li>
<li>methods: 这个论文使用了以下方法：（1）建立了一个平行数据集，其中包含GMSL培训的教师重构不支持性语言的示例，并提供了一个批注指南；（2）开发了GMSL提问框架，用于修改教师的不支持性语言；（3）采用了基于心理理论的评价框架，用于评价GMSL的效果。</li>
<li>results: 这个论文的研究结果显示，both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. In addition, model-generated reframings outperform those from the GMSL-trained teachers. These results demonstrate the promise of using LLMs to provide automated GMSL feedback for teachers, and more broadly, the potential of LLMs for supporting students’ learning in the classroom.<details>
<summary>Abstract</summary>
Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing that one's skills can be improved over time--has been shown to significantly reduce disparities in academic achievement and enhance students' learning outcomes. Although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers' use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers' unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs' potentiality for supporting students' learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains.
</details>
<details>
<summary>摘要</summary>
教师的增长心理支持语言（GMSL）——强调一个人的技能可以逐渐提高——已经显著减少学生学习成绩的差距和提高学生的学习效果。 although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due to the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers' use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers' unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs' potentiality for supporting students' learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains.
</details></li>
</ul>
<hr>
<h2 id="Data-Contamination-Through-the-Lens-of-Time"><a href="#Data-Contamination-Through-the-Lens-of-Time" class="headerlink" title="Data Contamination Through the Lens of Time"></a>Data Contamination Through the Lens of Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10628">http://arxiv.org/abs/2310.10628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abacusai/to-the-cutoff">https://github.com/abacusai/to-the-cutoff</a></li>
<li>paper_authors: Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley</li>
<li>for:  This paper aims to investigate the issue of data contamination in large language models (LLMs) by analyzing the trends in LLM pass rates and their relationship with GitHub popularity and release date.</li>
<li>methods:  The authors use a natural experiment of training cutoffs in GPT models to examine benchmarks released over time, specifically focusing on two code&#x2F;mathematical problem-solving datasets, Codeforces and Project Euler. They employ a longitudinal analysis approach to identify statistically significant trends in LLM pass rates.</li>
<li>results:  The authors find strong evidence of data contamination in LLMs, as reflected in the statistically significant trends in LLM pass rates vs. GitHub popularity and release date. They also open-source their dataset, raw results, and evaluation framework to facilitate rigorous analyses of data contamination in modern models.<details>
<summary>Abstract</summary>
Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. In this work, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination. By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of LLMs that train on webscale data.
</details>
<details>
<summary>摘要</summary>
In this study, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination.By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of LLMs that train on webscale data.
</details></li>
</ul>
<hr>
<h2 id="ForceGen-End-to-end-de-novo-protein-generation-based-on-nonlinear-mechanical-unfolding-responses-using-a-protein-language-diffusion-model"><a href="#ForceGen-End-to-end-de-novo-protein-generation-based-on-nonlinear-mechanical-unfolding-responses-using-a-protein-language-diffusion-model" class="headerlink" title="ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model"></a>ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10605">http://arxiv.org/abs/2310.10605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Ni, David L. Kaplan, Markus J. Buehler</li>
<li>for: 本研究旨在开发一种可预测性强的蛋白质设计模型，以满足复杂非线性机械性质设计目标。</li>
<li>methods: 该模型基于先前训练的蛋白质语言模型，利用蛋白质序列深度知识，将机械 unfolding 响应映射到创造新蛋白质。</li>
<li>results: 通过全原子分子动力学 simulate，证明设计出的蛋白质是新的，并满足目标的机械性质，包括 unfolding energy 和机械强度，以及细致的 unfolding force-separation 曲线。<details>
<summary>Abstract</summary>
Through evolution, nature has presented a set of remarkable protein materials, including elastins, silks, keratins and collagens with superior mechanical performances that play crucial roles in mechanobiology. However, going beyond natural designs to discover proteins that meet specified mechanical properties remains challenging. Here we report a generative model that predicts protein designs to meet complex nonlinear mechanical property-design objectives. Our model leverages deep knowledge on protein sequences from a pre-trained protein language model and maps mechanical unfolding responses to create novel proteins. Via full-atom molecular simulations for direct validation, we demonstrate that the designed proteins are novel, and fulfill the targeted mechanical properties, including unfolding energy and mechanical strength, as well as the detailed unfolding force-separation curves. Our model offers rapid pathways to explore the enormous mechanobiological protein sequence space unconstrained by biological synthesis, using mechanical features as target to enable the discovery of protein materials with superior mechanical properties.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Motion2Language-Unsupervised-learning-of-synchronized-semantic-motion-segmentation"><a href="#Motion2Language-Unsupervised-learning-of-synchronized-semantic-motion-segmentation" class="headerlink" title="Motion2Language, Unsupervised learning of synchronized semantic motion segmentation"></a>Motion2Language, Unsupervised learning of synchronized semantic motion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10594">http://arxiv.org/abs/2310.10594</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rd20karim/M2T-Segmentation">https://github.com/rd20karim/M2T-Segmentation</a></li>
<li>paper_authors: Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde</li>
<li>for: 这个论文的目的是建立一种序列到序列架构，用于将动作捕获输入翻译成英语自然语言描述，并同时生成描述和动作的同步。</li>
<li>methods: 论文提出了一种新的循环式注意力形式，适用于同步生成文本，以及一种改进的动作编码器架构，适用于更小的数据集和同步生成。</li>
<li>results: 经过测试，提出的注意力机制和编码器架构都有加成效果，可以提高生成文本的质量（BLEU和Semantic Equivalence）以及同步性。<details>
<summary>Abstract</summary>
In this paper, we investigate building a sequence to sequence architecture for motion to language translation and synchronization. The aim is to translate motion capture inputs into English natural-language descriptions, such that the descriptions are generated synchronously with the actions performed, enabling semantic segmentation as a byproduct, but without requiring synchronized training data. We propose a new recurrent formulation of local attention that is suited for synchronous/live text generation, as well as an improved motion encoder architecture better suited to smaller data and for synchronous generation. We evaluate both contributions in individual experiments, using the standard BLEU4 metric, as well as a simple semantic equivalence measure, on the KIT motion language dataset. In a follow-up experiment, we assess the quality of the synchronization of generated text in our proposed approaches through multiple evaluation metrics. We find that both contributions to the attention mechanism and the encoder architecture additively improve the quality of generated text (BLEU and semantic equivalence), but also of synchronization. Our code will be made available at \url{https://github.com/rd20karim/M2T-Segmentation/tree/main}
</details>
<details>
<summary>摘要</summary>
本文 investigate 建立一种序列到序列架构，用于动作到语言翻译和同步。目标是将动作捕获输入翻译成英语自然语言描述，以便在动作发生时同步生成描述，而无需同步训练数据。我们提出了一种新的循环形式的本地注意力表示，适合同步生成文本，以及一种改进的动作编码建立，更适合小型数据和同步生成。我们在使用标准的BLEU4指标和简单的 semantics 等价度量进行评估，并在 KIT 动作语言数据集上进行单独的实验。在后续实验中，我们评估了我们的提议中的同步生成文本质量，通过多种评价指标。我们发现， both 注意力机制和编码建立增加了生成文本质量（BLEU和semantics），同时也提高了同步生成的质量。我们的代码将在 \url{https://github.com/rd20karim/M2T-Segmentation/tree/main} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Mastering-the-Task-of-Open-Information-Extraction-with-Large-Language-Models-and-Consistent-Reasoning-Environment"><a href="#Mastering-the-Task-of-Open-Information-Extraction-with-Large-Language-Models-and-Consistent-Reasoning-Environment" class="headerlink" title="Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment"></a>Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10590">http://arxiv.org/abs/2310.10590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Qi, Kaixuan Ji, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Lei Hou, Juanzi Li, Bin Xu</li>
<li>for: 解决对自然语言文本中的 объектив结构知识抽取任务的问题，以建立专门的模型。</li>
<li>methods: 使用语言模型进行启发式学习，并提出一种方法来评估语言模型与测试样本之间的语法分布差异，以作为准备证明。</li>
<li>results: 通过在标准 CaRB benchmark上进行 $6$-shot 方法，实现了超过现有监督方法的 $55.3$ $F_1$ 分数，并在 TACRED 和 ACE05 上进行了natural generalization，实现了 $5.7$ 和 $6.8$ $F_1$ 分数的提高。<details>
<summary>Abstract</summary>
Open Information Extraction (OIE) aims to extract objective structured knowledge from natural texts, which has attracted growing attention to build dedicated models with human experience. As the large language models (LLMs) have exhibited remarkable in-context learning capabilities, a question arises as to whether the task of OIE can be effectively tackled with this paradigm? In this paper, we explore solving the OIE problem by constructing an appropriate reasoning environment for LLMs. Specifically, we first propose a method to effectively estimate the discrepancy of syntactic distribution between a LLM and test samples, which can serve as correlation evidence for preparing positive demonstrations. Upon the evidence, we introduce a simple yet effective mechanism to establish the reasoning environment for LLMs on specific tasks. Without bells and whistles, experimental results on the standard CaRB benchmark demonstrate that our $6$-shot approach outperforms state-of-the-art supervised method, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and ACE05 show that our method can naturally generalize to other information extraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BiLL-VTG-Bridging-Large-Language-Models-and-Lightweight-Visual-Tools-for-Video-based-Texts-Generation"><a href="#BiLL-VTG-Bridging-Large-Language-Models-and-Lightweight-Visual-Tools-for-Video-based-Texts-Generation" class="headerlink" title="BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"></a>BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10586">http://arxiv.org/abs/2310.10586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li</li>
<li>for: 本文旨在提出一种快速适应性框架，以便基于视频进行文本回答。</li>
<li>methods: 本文使用了大量语言模型（LLM）来进行视频理解和知识推理。 Specifically, 我们发现回答特定指令的关键在于关注相关视频事件，并使用了两种视觉工具：结构化场景图生成和描述性图像标题生成来收集和表示事件信息。 然后，一个搭载了世界知识的 LLM 被用作理解代理，通过多个理解步骤来实现回答。</li>
<li>results: 我们的框架在两个常见的视频文本生成任务上表现出STATE-OF-THE-ART的性能，并且不需要训练。<details>
<summary>Abstract</summary>
Building models that generate textual responses to user instructions for videos is a practical and challenging topic, as it requires both vision understanding and knowledge reasoning. Compared to language and image modalities, training efficiency remains a serious problem as existing studies train models on massive sparse videos aligned with brief descriptions. In this paper, we introduce BiLL-VTG, a fast adaptive framework that leverages large language models (LLMs) to reasoning on videos based on essential lightweight visual tools. Specifically, we reveal the key to response specific instructions is the concentration on relevant video events, and utilize two visual tools of structured scene graph generation and descriptive image caption generation to gather and represent the events information. Thus, a LLM equipped with world knowledge is adopted as the reasoning agent to achieve the response by performing multiple reasoning steps on specified video events.To address the difficulty of specifying events from agent, we further propose an Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the efficient Hungarian matching to localize corresponding video events using linguistic instructions, enabling LLMs to interact with long videos. Extensive experiments on two typical video-based texts generations tasks show that our tuning-free framework outperforms the pre-trained models including Flamingo-80B, to achieve the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:建立基于视频的模型，以生成用户 instrucion 的文本响应是一个实用和挑战的话题，因为它需要视觉理解和知识推理。相比语言和图像模式，训练效率仍然是一个严重的问题，因为现有的研究通常使用大量稀疏的视频和简短的描述进行训练。在这篇论文中，我们介绍了 BiLL-VTG 框架，该框架利用大型语言模型（LLM）来基于视频中的关键事件进行推理。我们发现关键在于响应特定的 instrucion 是关注相关的视频事件，并使用两种视觉工具：结构化场景图生成和描述性图像标签生成来收集和表示事件信息。然后，一个装备了世界知识的 LLM 作为推理代理来实现响应，通过多个推理步骤来处理指定的视频事件。为了解决指定事件的困难，我们还提出了一种基于有效的匈牙利匹配的 Instruction-oriented Video Events Recognition（InsOVER）算法，以便 LLMS 与长视频进行交互。我们在两个典型的视频基于文本生成任务上进行了广泛的实验，结果显示，我们的自适应框架在与 Flamingo-80B 等预训练模型进行比较时，具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Who-Are-All-The-Stochastic-Parrots-Imitating-They-Should-Tell-Us"><a href="#Who-Are-All-The-Stochastic-Parrots-Imitating-They-Should-Tell-Us" class="headerlink" title="Who Are All The Stochastic Parrots Imitating? They Should Tell Us!"></a>Who Are All The Stochastic Parrots Imitating? They Should Tell Us!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10583">http://arxiv.org/abs/2310.10583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagi Shaier, Lawrence E. Hunter, Katharina von der Wense</li>
<li>for: 这篇论文主要是关于语言模型（LM）的可靠性问题。</li>
<li>methods: 作者建议使用LM可以引用其训练数据的方法，以便快速验证LM生成的声明的真实性。</li>
<li>results: 作者认为，当前的LM在重要场景中永远不会被完全信任，并建议一种新的策略来解决这个问题，即建立LM可以引用其训练数据的能力。<details>
<summary>Abstract</summary>
Both standalone language models (LMs) as well as LMs within downstream-task systems have been shown to generate statements which are factually untrue. This problem is especially severe for low-resource languages, where training data is scarce and of worse quality than for high-resource languages. In this opinion piece, we argue that LMs in their current state will never be fully trustworthy in critical settings and suggest a possible novel strategy to handle this issue: by building LMs such that can cite their sources - i.e., point a user to the parts of their training data that back up their outputs. We first discuss which current NLP tasks would or would not benefit from such models. We then highlight the expected benefits such models would bring, e.g., quick verifiability of statements. We end by outlining the individual tasks that would need to be solved on the way to developing LMs with the ability to cite. We hope to start a discussion about the field's current approach to building LMs, especially for low-resource languages, and the role of the training data in explaining model generations.
</details>
<details>
<summary>摘要</summary>
各种自然语言处理（NLP）任务中的语言模型（LM）都有可能生成不准确的陈述，特别是 для低资源语言，训练数据稀缺，质量也较差。在这篇意见文章中，我们 argue that LMs 在当前状态下从不能在重要场景中得到完全信任，并提出一种可能的新策略来解决这个问题：建立LMs 可以指明其所基于的训练数据部分，即用户可以通过点击LMs 的输出来找到相应的训练数据。我们首先讨论了当前NLP任务中哪些任务可以或不可以受益于这种模型，然后描述了这种模型带来的预期优势，例如快速验证陈述的可靠性。最后，我们列出了需要解决的任务，以开发LMs 可以指明其所基于的训练数据部分。我们希望通过这篇文章引发关于当前LMs 建设的讨论，特别是低资源语言的LMs，以及训练数据的角色在解释模型生成中。
</details></li>
</ul>
<hr>
<h2 id="Emerging-Challenges-in-Personalized-Medicine-Assessing-Demographic-Effects-on-Biomedical-Question-Answering-Systems"><a href="#Emerging-Challenges-in-Personalized-Medicine-Assessing-Demographic-Effects-on-Biomedical-Question-Answering-Systems" class="headerlink" title="Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems"></a>Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10571">http://arxiv.org/abs/2310.10571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagi Shaier, Kevin Bennett, Lawrence Hunter, Katharina von der Wense</li>
<li>for: 本研究旨在检测生物医学问答模型是否受到人群特征影响，以确保医疗公平。</li>
<li>methods: 研究使用了不同类型的问答模型，包括基于知识图（KG）和文本基于的模型，并对它们进行了测试。</li>
<li>results: 研究发现， irrelevant demographic information可以导致问答模型的答案发生变化，变化的比例可达15%（基于知识图）和23%（基于文本）。这些变化可能会影响准确性。<details>
<summary>Abstract</summary>
State-of-the-art question answering (QA) models exhibit a variety of social biases (e.g., with respect to sex or race), generally explained by similar issues in their training data. However, what has been overlooked so far is that in the critical domain of biomedicine, any unjustified change in model output due to patient demographics is problematic: it results in the unfair treatment of patients. Selecting only questions on biomedical topics whose answers do not depend on ethnicity, sex, or sexual orientation, we ask the following research questions: (RQ1) Do the answers of QA models change when being provided with irrelevant demographic information? (RQ2) Does the answer of RQ1 differ between knowledge graph (KG)-grounded and text-based QA systems? We find that irrelevant demographic information change up to 15% of the answers of a KG-grounded system and up to 23% of the answers of a text-based system, including changes that affect accuracy. We conclude that unjustified answer changes caused by patient demographics are a frequent phenomenon, which raises fairness concerns and should be paid more attention to.
</details>
<details>
<summary>摘要</summary>
现代问答（QA）模型表现出多种社会偏见（例如与性别或种族相关），通常可以归因于训练数据中的类似问题。然而，到目前为止忽略了在重要领域生物医学中，任何不当的模型输出变化因为病人特征是问题：它会导致患者不公正地处理。我们选择仅考虑不依赖性别、性别或性 orientation 的生物医学问题，并提出以下研究问题：（RQ1）QA 模型在接受无关的民族信息时是否发生变化？（RQ2）对知识图（KG）基础的 QA 系统和文本基础的 QA 系统而言，RQ1 的答案是否不同？我们发现，无关民族信息可以改变 KG 基础系统的答案，占总答案的 15%，而文本基础系统的答案则占 23%，包括影响准确性的变化。我们 conclude 这种不当的答案变化是常见的，这引发公平问题，需要更多的注意。
</details></li>
</ul>
<hr>
<h2 id="On-Position-Bias-in-Summarization-with-Large-Language-Models"><a href="#On-Position-Bias-in-Summarization-with-Large-Language-Models" class="headerlink" title="On Position Bias in Summarization with Large Language Models"></a>On Position Bias in Summarization with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10570">http://arxiv.org/abs/2310.10570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathieu Ravaut, Shafiq Joty, Aixin Sun, Nancy F. Chen</li>
<li>for: 本研究旨在探讨语言模型在多文档问答 задании中如何利用输入Context，以及这些模型在摘要生成任务中的表现。</li>
<li>methods: 本研究使用了10个数据集、4个语言模型和5个评价指标来分析语言模型在摘要生成任务中如何利用其输入。</li>
<li>results: 研究发现，语言模型倾向于使用 introduce content（以及一定程度的 final content），导致摘要生成性能呈U型曲线。这种偏好对多种多样化的摘要任务提出了挑战。<details>
<summary>Abstract</summary>
Large language models (LLMs) excel in zero-shot abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, surpassing token limits of 32k or more. However, in the realm of multi-document question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization tasks where crucial content may be dispersed throughout the source document(s). This paper presents a comprehensive investigation encompassing 10 datasets, 4 LLMs, and 5 evaluation metrics to analyze how these models leverage their input for abstractive summarization. Our findings reveal a pronounced bias towards the introductory content (and to a lesser extent, the final content), posing challenges for LLM performance across a range of diverse summarization benchmarks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在零shot摘要任务中表现出色，提供流畅和有关的摘要。最近的进步使其能处理长输入上下文，超过32k个Token的限制。然而，在多文档问答任务中，语言模型表现出输入上下文不均匀的问题。它们倾向于初始和 final段，导致摘要性能形成U型曲线，其中答案位于输入中的任何位置。这种偏见存在问题，特别是在摘要任务中，重要的内容可能会分散在源文档中。本文通过10个数据集、4个LLM和5个评价指标进行全面的调查，分析这些模型如何使用其输入进行摘要。我们发现，LLM偏向于引言内容（以及一定 extent的 final content），这会影响LLM在多种多样的摘要benchmark上的表现。
</details></li>
</ul>
<hr>
<h2 id="RegaVAE-A-Retrieval-Augmented-Gaussian-Mixture-Variational-Auto-Encoder-for-Language-Modeling"><a href="#RegaVAE-A-Retrieval-Augmented-Gaussian-Mixture-Variational-Auto-Encoder-for-Language-Modeling" class="headerlink" title="RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling"></a>RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10567">http://arxiv.org/abs/2310.10567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingcheng Deng, Liang Pang, Huawei Shen, Xueqi Cheng</li>
<li>for: 提高语言模型（LM）的表达质量和减少幻觉</li>
<li>methods: 使用检索增强的语言模型（RegaVAE），其基于变量自动编码器（VAE），并在检索和生成过程中使用嵌入空间来捕捉当前和未来文本的信息</li>
<li>results: 在多个 dataset 上实现了显著提高表达质量和幻觉的除去<details>
<summary>Abstract</summary>
Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the latent space and adopt the probabilistic form of the retrieval generation paradigm by expanding the Gaussian prior distribution into a Gaussian mixture distribution. Theoretical analysis provides an optimizable upper bound for RegaVAE. Experimental results on various datasets demonstrate significant improvements in text generation quality and hallucination removal.
</details>
<details>
<summary>摘要</summary>
Translation note:* "outdated information" is translated as "过时信息" (guòshí xīnxiàng)* "hallucinations" is translated as "幻见" (hénjiàn)* "latent variables" is translated as "隐变量" (yǐbiàn yuán)* "compact latent space" is translated as "紧凑的隐藏空间" (jìchōng de yǐnmo yòngkōng)* "raw text" is translated as "原始文本" (yuánshi wén tiān)* "Gaussian prior distribution" is translated as "高斯先验分布" (gāosī xiān yì fāngbù)* "Gaussian mixture distribution" is translated as "高斯混合分布" (gāosī hùn yì fāngbù)* "theoretical analysis" is translated as "理论分析" (lǐlùn fāng'àn)* "upper bound" is translated as "上限" (shàngjìn)
</details></li>
</ul>
<hr>
<h2 id="ViPE-Visualise-Pretty-much-Everything"><a href="#ViPE-Visualise-Pretty-much-Everything" class="headerlink" title="ViPE: Visualise Pretty-much Everything"></a>ViPE: Visualise Pretty-much Everything</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10543">http://arxiv.org/abs/2310.10543</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hazel1994/ViPE-Videos">https://github.com/Hazel1994/ViPE-Videos</a></li>
<li>paper_authors: Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch</li>
<li>for: This paper aims to address the issue of text-to-image models struggling to depict non-literal expressions, by introducing a new method called ViPE.</li>
<li>methods: ViPE uses a series of lightweight and robust language models trained on a large-scale set of lyrics with noisy visual descriptions generated by GPT3.5.</li>
<li>results: ViPE effectively expresses any arbitrary piece of text into a visualisable description, and exhibits an understanding of figurative expressions comparable to human experts. It also provides a powerful and open-source backbone for downstream applications such as music video and caption generation.<details>
<summary>Abstract</summary>
Figurative and non-literal expressions are profoundly integrated in human communication. Visualising such expressions allow us to convey our creative thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions. Recent works primarily deal with this issue by compiling humanly annotated datasets on a small scale, which not only demands specialised expertise but also proves highly inefficient. To address this issue, we introduce ViPE: Visualise Pretty-much Everything. ViPE offers a series of lightweight and robust language models that have been trained on a large-scale set of lyrics with noisy visual descriptions that represent their implicit meaning. The synthetic visual descriptions are generated by GPT3.5 relying on neither human annotations nor images. ViPE effectively expresses any arbitrary piece of text into a visualisable description, enabling meaningful and high-quality image generation. We provide compelling evidence that ViPE is more robust than GPT3.5 in synthesising visual elaborations. ViPE also exhibits an understanding of figurative expressions comparable to human experts, providing a powerful and open-source backbone to many downstream applications such as music video and caption generation.
</details>
<details>
<summary>摘要</summary>
人类communication中的 figurative 和非Literal 表达是极其深入地融合在一起。Visualizing这些表达可以帮助我们表达创造性的思想，并触发细腻的情感。然而，现有的文本-图像模型，如Stable Diffusion，在描绘非Literal表达方面几乎无法表现出来。现有的工作主要采取了 compile humanly annotated datasets的方法，这不仅需要专业知识，还证明高效率。为解决这个问题，我们引入了 ViPE：Visualize Pretty-much Everything。ViPE 提供了一系列轻量级和可靠的语言模型，这些模型在大规模的歌词中生成了噪音的视觉描述。这些synthetic visual descriptions 由 GPT3.5 生成，不需要人类注释也不需要图像。ViPE 可以将任何文本转换成可视化的描述，从而实现了高质量的图像生成。我们提供了吸引人的证明，表明 ViPE 比 GPT3.5 更加稳定在生成视觉 elaborations 方面。ViPE 还表现出了对 figurative expressions 的理解，与人类专家相当，提供了一个强大且开源的基础结构，可以推动多个下游应用，如音乐视频和caption生成。
</details></li>
</ul>
<hr>
<h2 id="One-For-All-All-For-One-Bypassing-Hyperparameter-Tuning-with-Model-Averaging-For-Cross-Lingual-Transfer"><a href="#One-For-All-All-For-One-Bypassing-Hyperparameter-Tuning-with-Model-Averaging-For-Cross-Lingual-Transfer" class="headerlink" title="One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model Averaging For Cross-Lingual Transfer"></a>One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model Averaging For Cross-Lingual Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10532">http://arxiv.org/abs/2310.10532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fdschmidt93/ofa-xlt">https://github.com/fdschmidt93/ofa-xlt</a></li>
<li>paper_authors: Fabian David Schmidt, Ivan Vulić, Goran Glavaš</li>
<li>for: 这个论文主要探讨了零例转移跨语言传递（ZS-XLT）的效iveness，以及如何选择最佳的模型和 hyperparameter。</li>
<li>methods: 该论文使用了多语言模型，并在不同的语言上进行了针对性的训练和测试。具体来说， authors 使用了不同的 hyperparameter 和模型Snapshot来进行训练和测试，并通过accumulative run-by-run averaging来提高 ZS-XLT 的性能。</li>
<li>results: 研究发现，传统的模型选择方法 based on source-language validation 很快就达到了下降的 ZS-XLT 性能。然而，通过accumulative run-by-run averaging来提高 ZS-XLT 性能，并与 “oracle” ZS-XLT 表现高度相关。<details>
<summary>Abstract</summary>
Multilingual language models enable zero-shot cross-lingual transfer (ZS-XLT): fine-tuned on sizable source-language task data, they perform the task in target languages without labeled instances. The effectiveness of ZS-XLT hinges on the linguistic proximity between languages and the amount of pretraining data for a language. Because of this, model selection based on source-language validation is unreliable: it picks model snapshots with suboptimal target-language performance. As a remedy, some work optimizes ZS-XLT by extensively tuning hyperparameters: the follow-up work then routinely struggles to replicate the original results. Other work searches over narrower hyperparameter grids, reporting substantially lower performance. In this work, we therefore propose an unsupervised evaluation protocol for ZS-XLT that decouples performance maximization from hyperparameter tuning. As a robust and more transparent alternative to extensive hyperparameter tuning, we propose to accumulatively average snapshots from different runs into a single model. We run broad ZS-XLT experiments on both higher-level semantic tasks (NLI, extractive QA) and a lower-level token classification task (NER) and find that conventional model selection based on source-language validation quickly plateaus to suboptimal ZS-XLT performance. On the other hand, our accumulative run-by-run averaging of models trained with different hyperparameters boosts ZS-XLT performance and closely correlates with "oracle" ZS-XLT, i.e., model selection based on target-language validation performance.
</details>
<details>
<summary>摘要</summary>
多语言语模型可以实现零码跨语言传递（ZS-XLT）：经过精心适应源语言任务数据，它们可以在目标语言中完成任务无需标注实例。ZS-XLT的有效性取决于语言之间的语言相似性和语言预训练数据的量。因此，基于源语言验证的模型选择是不可靠的：它可能会选择模型快照中的产生性能不佳的模型。为了解决这个问题，一些研究者们在ZS-XLT中进行了广泛的超参数优化：然而，继续的研究往往难以复制原来的结果。其他研究者们在 narrower 的超参数格上进行了搜索，并报告了较低的性能。在这个研究中，我们因此提出了一种无监督的评估协议，以减少精度优化和超参数优化之间的关系。我们提议通过在不同的run中训练不同的超参数，并将这些run中的模型快照相加，以获得一个更加 robust 和 transparent 的ZS-XLT模型。我们在高级semantic任务（NLI、抽取式问答）和 lower-level 字符串分类任务（NER）上进行了广泛的ZS-XLT实验，并发现了以下结论：在源语言验证中选择模型的方法很快就到达了低效的ZS-XLT性能，而我们的积累run-by-run相加的模型快照则可以提高ZS-XLT性能，并与“oracle” ZS-XLT（基于目标语言验证性能进行选择）高度相关。
</details></li>
</ul>
<hr>
<h2 id="Metric-Ensembles-For-Hallucination-Detection"><a href="#Metric-Ensembles-For-Hallucination-Detection" class="headerlink" title="Metric Ensembles For Hallucination Detection"></a>Metric Ensembles For Hallucination Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10495">http://arxiv.org/abs/2310.10495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/parthk279/Hallucination-Research">https://github.com/parthk279/Hallucination-Research</a></li>
<li>paper_authors: Grant C. Forbes, Parth Katlana, Zeydy Ortiz</li>
<li>for: 这篇论文主要研究了对摘要的自动生成中减少“幻”信息（不在原始文档中出现的信息）的问题，以及关于这个问题的评估方法。</li>
<li>methods: 该论文使用了许多不同的无监督度量来评估摘要的一致性，并对这些度量之间的相关性和人工评估分数的相关性进行了分析。</li>
<li>results: 研究发现，使用LLM（大型语言模型）基于的方法可以更好地检测摘要中的幻信息，而且 ensemble方法可以进一步提高这些分数。此外，研究还发现，要使ensemble方法有所提高，则需要确保度量在ensemble中具有足够相似的错误率，而不需要完全相同的错误率。<details>
<summary>Abstract</summary>
Abstractive text summarization has garnered increased interest as of late, in part due to the proliferation of large language models (LLMs). One of the most pressing problems related to generation of abstractive summaries is the need to reduce "hallucinations," information that was not included in the document being summarized, and which may be wholly incorrect. Due to this need, a wide array of metrics estimating consistency with the text being summarized have been proposed. We examine in particular a suite of unsupervised metrics for summary consistency, and measure their correlations with each other and with human evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then compare these evaluations to models made from a simple linear ensemble of these metrics. We find that LLM-based methods outperform other unsupervised metrics for hallucination detection. We also find that ensemble methods can improve these scores even further, provided that the metrics in the ensemble have sufficiently similar and uncorrelated error rates. Finally, we present an ensemble method for LLM-based evaluations that we show improves over this previous SOTA.
</details>
<details>
<summary>摘要</summary>
抽象摘要生成技术在最近几年来得到了更多的关注，一部分这是因为大语言模型（LLM）的普及。摘要生成中最大的问题之一是减少“幻觉”，即文档中没有包含的信息，而且可能完全错误。由于这一需求，一系列用于摘要与文档之间的一致性的度量被提出。我们专门研究了这些无监督度量的套件，并测量它们之间的相关性和人工评价分数在wiki_bio_gpt3_hallucination数据集中的相关性。然后，我们比较了这些评价与模型中的其他无监督度量和LLM-based方法的性能。我们发现LLM-based方法在幻觉检测方面表现出色，而且 ensemble方法可以进一步提高这些分数，只要 ensemble中的度量具有相似的错误率。最后，我们提出了一种ensemble方法，可以further improve sobre la última SOTA。
</details></li>
</ul>
<hr>
<h2 id="UNO-DST-Leveraging-Unlabelled-Data-in-Zero-Shot-Dialogue-State-Tracking"><a href="#UNO-DST-Leveraging-Unlabelled-Data-in-Zero-Shot-Dialogue-State-Tracking" class="headerlink" title="UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking"></a>UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10492">http://arxiv.org/abs/2310.10492</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lichuangnus/uno-dst">https://github.com/lichuangnus/uno-dst</a></li>
<li>paper_authors: Chuang Li, Yan Zhang, Min-Yen Kan, Haizhou Li</li>
<li>for: 这篇论文是为了提出一种基于少量数据的零shot对话状态跟踪（DST）方法，以便在目标领域中进行自动标注。</li>
<li>methods: 该方法使用了 auxiliary tasks 生成槽类作为主任务的 inverse prompt，通过联合自我训练来使用无标记数据来增强 DST 模型的训练和精度。</li>
<li>results: 在 MultiWOZ 多语言对话场景中，该方法可以提高平均联合目标任务准确率 by 8%，表明该方法可以有效地提高 DST 模型在零shot 情况下的性能。<details>
<summary>Abstract</summary>
Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, but ignore unlabelled data in the target domain. We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method's effectiveness on large language models in zero-shot scenarios, improving average joint goal accuracy by $8\%$ across all domains in MultiWOZ.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "zero-shot" is translated as "无需标注的" (wú shí biāo yì)* "few-shot" is translated as "几个shot" (jī gè shòu)* "transfer learning" is translated as "传输学习" (chuán xiū xué xí)* "joint training" is translated as "共同训练" (gòng tóng xiǎo xíng)* "self-training" is translated as "自我训练" (zi wo xiǎo xíng)* "auxiliary tasks" is translated as "辅助任务" (bù zhù zhì gōng)* "slot types" is translated as "槽类型" (shí kè yì)* "inverse prompts" is translated as "反向提示" (fǎn xiàng tím shì)* "main tasks" is translated as "主要任务" (zhǔ yào zhì gōng)* "cycle consistency" is translated as "循环一致" (xún huán yī zhì)* "quality samples" is translated as "高质量的样本" (gāo zhì yàng yī xiǎng)* "unknown target domains" is translated as "未知目标领域" (wèi zhī mù bì yì zhòng)* "subsequent fine-tuning" is translated as "后续精度调整" (hòu xù jīng dù jiǎo yì)* "large language models" is translated as "大型自然语言模型" (dà xíng zì rán yǔ yán mó delì)* "improving" is translated as "提高" (tí gāo)* "average joint goal accuracy" is translated as "平均共同目标准确率" (píng jìn gòng tóng mù zhì jīn yì)Note: The translation is based on the standard Simplified Chinese language and may vary depending on the specific dialect or register used in the target domain.
</details></li>
</ul>
<hr>
<h2 id="xCOMET-Transparent-Machine-Translation-Evaluation-through-Fine-grained-Error-Detection"><a href="#xCOMET-Transparent-Machine-Translation-Evaluation-through-Fine-grained-Error-Detection" class="headerlink" title="xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection"></a>xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10482">http://arxiv.org/abs/2310.10482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, André F. T. Martins</li>
<li>for: 本文旨在bridge sentence-level评估和错误span检测两种方法之间，提供更加细节的翻译评估方法。</li>
<li>methods: 本文提出了一种开源的学习型评估方法xCOMET，可以同时进行 sentence-level评估和错误span检测。</li>
<li>results: xCOMET在所有类型的评估中表现出状元，并能够高亮和分类错误 span，从而增强翻译评估的细节性。<details>
<summary>Abstract</summary>
Widely used learned metrics for machine translation evaluation, such as COMET and BLEURT, estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce xCOMET, an open-source learned metric designed to bridge the gap between these approaches. xCOMET integrates both sentence-level evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that xCOMET is largely capable of identifying localized critical errors and hallucinations.
</details>
<details>
<summary>摘要</summary>
Translation (Simplified Chinese):广泛使用的学习型评估指标，如COMET和BLEURT，用单句级分数评估翻译假设，无法提供翻译错误的细节信息（例如，翻译错误的类型和严重程度）。然而，大型自然语言模型（LLMs）正在推广更细化的评估策略，尝试 detail和 categorize 翻译错误。在这种情况下，我们介绍了 xCOMET，一个开源的学习指标，用于bridging这些方法之间的差异。xCOMET integrate了句子级评估和错误异常检测功能，在所有类型的评估中表现出state-of-the-art的性能（句子级、系统级和错误异常检测）。此外，它还可以高亮和 categorize 错误异常，因此可以增加质量评估的深度。我们还提供了一个Robustness分析，使用压力测试，并显示 xCOMET 可以识别和报告局部重要的错误和幻觉。
</details></li>
</ul>
<hr>
<h2 id="G-SPEED-General-SParse-Efficient-Editing-MoDel"><a href="#G-SPEED-General-SParse-Efficient-Editing-MoDel" class="headerlink" title="G-SPEED: General SParse Efficient Editing MoDel"></a>G-SPEED: General SParse Efficient Editing MoDel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10480">http://arxiv.org/abs/2310.10480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/banner-z/g-speed">https://github.com/banner-z/g-speed</a></li>
<li>paper_authors: Haoke Zhang, Yue Wang, Juntao Li, Xiabing Zhou, Min Zhang</li>
<li>for: 提高工作效率，自动理解人类发出的指令并生成预期的内容。</li>
<li>methods: 提出了一种基于无监督文本编辑数据 clustering 算法的一种新型精简编辑模型建立方法，以及一种使用 sparse 编辑模型架构来缓解小语言模型的学习限制。</li>
<li>results: 对比 LLMS Equipped with 175B parameters，G-SPEED 的508M参数可以超越它们，并且可以满足多种编辑需求。<details>
<summary>Abstract</summary>
Large Language Models~(LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages. Through human-model interactions, LLMs can automatically understand human-issued instructions and output the expected contents, which can significantly increase working efficiency. In various types of real-world demands, editing-oriented tasks account for a considerable proportion, which involves an interactive process that entails the continuous refinement of existing texts to meet specific criteria. Due to the need for multi-round human-model interaction and the generation of complicated editing tasks, there is an emergent need for efficient general editing models. In this paper, we propose \underline{\textbf{G}eneral \underline{\textbf{SP}arse \underline{\textbf{E}fficient \underline{\textbf{E}diting Mo\underline{\textbf{D}el~(\textbf{G-SPEED}), which can fulfill diverse editing requirements through a single model while maintaining low computational costs. Specifically, we first propose a novel unsupervised text editing data clustering algorithm to deal with the data scarcity problem. Subsequently, we introduce a sparse editing model architecture to mitigate the inherently limited learning capabilities of small language models. The experimental outcomes indicate that G-SPEED, with its 508M parameters, can surpass LLMs equipped with 175B parameters. Our code and model checkpoints are available at \url{https://github.com/Banner-Z/G-SPEED}.
</details>
<details>
<summary>摘要</summary>
大型语言模型~(LLMs) 已经表现出了惊人的能力，包括理解、生成和修改语言。通过人机交互，LLMs 可以自动理解人类发布的指令，并输出预期的内容，这可能会提高工作效率。在各种实际应用中，修改任务占了一定的比重，这些任务涉及到人机交互的互动过程，需要不断细化现有的文本，以满足特定的标准。由于需要多轮人机交互和复杂的修改任务，有一种急需高效的通用修改模型。在这篇论文中，我们提出了 \underline{\textbf{G}eneral \underline{\textbf{SP}arse \underline{\textbf{E}fficient \underline{\textbf{E}diting Mo\underline{\textbf{D}el~(\textbf{G-SPEED})，它可以满足多样化的修改需求，而且保持低的计算成本。 Specifically，我们首先提出了一种新的无监督文本修改数据归类算法，以解决数据稀缺问题。然后，我们引入了稀疏修改模型架构，以降低小语言模型的内置学习能力限制。实验结果表明，G-SPEED，具有508M参数，可以超越配备175B参数的LLMs。我们的代码和模型检查点可以在 \url{https://github.com/Banner-Z/G-SPEED} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MechGPT-a-language-based-strategy-for-mechanics-and-materials-modeling-that-connects-knowledge-across-scales-disciplines-and-modalities"><a href="#MechGPT-a-language-based-strategy-for-mechanics-and-materials-modeling-that-connects-knowledge-across-scales-disciplines-and-modalities" class="headerlink" title="MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities"></a>MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10445">http://arxiv.org/abs/2310.10445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus J. Buehler</li>
<li>for: 本研究旨在探索人工智能技术以连接不同领域知识，以便更好地探索多 scales 材料失效的问题。</li>
<li>methods: 本研究使用了一个精度调整的大型自然语言模型（LLM），从 raw 源料中提取问题和答案对，然后使用 LLM 微调。研究还使用了 Ontological Knowledge Graphs 提取结构性信息，以及在不同大小和上下文长度下运行多种计算实验。</li>
<li>results: 研究发现，LLMs 能够提取多 scales 材料失效的结构性信息，并且可以用于新的研究问题框架和可读性图表。三个版本的 MechGPT 被讨论，它们在不同的参数大小和上下文长度下运行，可以实现复杂的检索增强策略和多模态探索。<details>
<summary>Abstract</summary>
For centuries, researchers have sought out ways to connect disparate areas of knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across fields, specialization has taken hold later. With the advent of Artificial Intelligence, we can now explore relationships across areas (e.g., mechanics-biology) or disparate domains (e.g., failure mechanics-art). To achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset of knowledge in multiscale materials failure. The approach includes the use of a general-purpose LLM to distill question-answer pairs from raw sources followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used in a series of computational experiments to explore its capacity for knowledge retrieval, various language tasks, hypothesis generation, and connecting knowledge across disparate areas. While the model has some ability to recall knowledge from training, we find that LLMs are particularly useful to extract structural insights through Ontological Knowledge Graphs. These interpretable graph structures provide explanatory insights, frameworks for new research questions, and visual representations of knowledge that also can be used in retrieval-augmented generation. Three versions of MechGPT are discussed, featuring different sizes from 13 billion to 70 billion parameters, and reaching context lengths of more than 10,000 tokens. This provides ample capacity for sophisticated retrieval augmented strategies, as well as agent-based modeling where multiple LLMs interact collaboratively and/or adversarially, the incorporation of new data from the literature or web searches, as well as multimodality.
</details>
<details>
<summary>摘要</summary>
Traditionally, researchers have sought to connect diverse areas of knowledge. While early scholars (such as Galileo and da Vinci) were experts across multiple fields, specialization has become more prevalent in recent times. With the advent of Artificial Intelligence, we can now explore relationships between different areas (such as mechanics and biology) or disparate domains (such as failure mechanics and art). To achieve this, we use a fine-tuned Large Language Model (LLM), specifically for a subset of knowledge in multiscale materials failure. The approach involves using a general-purpose LLM to distill question-answer pairs from raw sources, followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is then used in a series of computational experiments to explore its capacity for knowledge retrieval, various language tasks, hypothesis generation, and connecting knowledge across disparate areas. While the model has some ability to recall knowledge from training, we find that LLMs are particularly useful for extracting structural insights through Ontological Knowledge Graphs. These interpretable graph structures provide explanatory insights, frameworks for new research questions, and visual representations of knowledge that can also be used in retrieval-augmented generation. Three versions of MechGPT are discussed, featuring different sizes ranging from 13 billion to 70 billion parameters, and reaching context lengths of more than 10,000 tokens. This provides ample capacity for sophisticated retrieval-augmented strategies, as well as agent-based modeling where multiple LLMs interact collaboratively and/or adversarially, the incorporation of new data from the literature or web searches, as well as multimodality.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-User-Comments-for-Early-Detection-of-Fake-News-Prior-to-Users’-Commenting"><a href="#Exploiting-User-Comments-for-Early-Detection-of-Fake-News-Prior-to-Users’-Commenting" class="headerlink" title="Exploiting User Comments for Early Detection of Fake News Prior to Users’ Commenting"></a>Exploiting User Comments for Early Detection of Fake News Prior to Users’ Commenting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10429">http://arxiv.org/abs/2310.10429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiong Nan, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang, Guang Yang, Jintao Li, Kai Shu</li>
<li>for: 探讨了现有方法中的准确性vs快速性之间的负担，并提出了一种可行但尚未得到广泛研究的解决方案，即利用历史新闻的社交背景（如评论）进行模型训练，并将其应用于新出现的新闻中。</li>
<li>methods: 提出了一种名为Comment Assisted Fake News Detection（CAS-FEND）的方法，该方法利用历史新闻的评论来帮助一个内容只的检测模型提高检测精度。特别是，该方法在训练阶段将有用的知识从教师模型中传递给学生模型，以便在新出现的新闻中进行检测。</li>
<li>results: 实验表明，CAS-FEND学生模型在检测新出现的假新闻方面表现出色，比内容只方法和使用1&#x2F;4的评论作为输入的方法更高效。这示出了CAS-FEND的超越性，并证明了它在早期检测中的优势。<details>
<summary>Abstract</summary>
Both accuracy and timeliness are key factors in detecting fake news on social media. However, most existing methods encounter an accuracy-timeliness dilemma: Content-only methods guarantee timeliness but perform moderately because of limited available information, while social context-based ones generally perform better but inevitably lead to latency because of social context accumulation needs. To break such a dilemma, a feasible but not well-studied solution is to leverage social contexts (e.g., comments) from historical news for training a detection model and apply it to newly emerging news without social contexts. This requires the model to (1) sufficiently learn helpful knowledge from social contexts, and (2) be well compatible with situations that social contexts are available or not. To achieve this goal, we propose to absorb and parameterize useful knowledge from comments in historical news and then inject it into a content-only detection model. Specifically, we design the Comments Assisted Fake News Detection method (CAS-FEND), which transfers useful knowledge from a comments-aware teacher model to a content-only student model during training. The student model is further used to detect newly emerging fake news. Experiments show that the CAS-FEND student model outperforms all content-only methods and even those with 1/4 comments as inputs, demonstrating its superiority for early detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>严谨性和时效性都是社交媒体上检测假新闻的关键因素。然而，现有方法很多时会陷入精度-时效性之间的谍诀：内容仅仅方法可以保证时效性，但是它们的检测能力相对较弱，而基于社交上下文的方法通常可以提供更高的检测精度，但是它们需要较长的时间来积累社交上下文。为了突破这种谍诀，我们可以利用社交上下文（例如评论）来训练检测模型，并将其应用于新出现的新闻。这需要模型可以（1）充分学习社交上下文中的有用知识，并（2）在社交上下文存在或缺失时都能够具有Compatibility。为了实现这个目标，我们提出了注入社交上下文知识（e.g., 评论）到内容仅仅模型中的方法。我们称之为注入社交知识的Comments Assisted Fake News Detection方法（CAS-FEND）。在训练过程中，我们将社交上下文知识由一个师模型转移到内容仅仅模型中，然后使用这个师模型来检测新出现的假新闻。实验结果表明，CAS-FEND学生模型在检测新出现的假新闻方面表现出色，even outperforming those with 1/4 comments as inputs，这说明它在早期检测方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="textit-Swap-and-Predict-–-Predicting-the-Semantic-Changes-in-Words-across-Corpora-by-Context-Swapping"><a href="#textit-Swap-and-Predict-–-Predicting-the-Semantic-Changes-in-Words-across-Corpora-by-Context-Swapping" class="headerlink" title="$\textit{Swap and Predict}$ – Predicting the Semantic Changes in Words across Corpora by Context Swapping"></a>$\textit{Swap and Predict}$ – Predicting the Semantic Changes in Words across Corpora by Context Swapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10397">http://arxiv.org/abs/2310.10397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a1da4/svp-swap">https://github.com/a1da4/svp-swap</a></li>
<li>paper_authors: Taichi Aida, Danushka Bollegala</li>
<li>For: The paper is written for detecting semantic changes of words in different text corpora.* Methods: The proposed method, Swapping-based Semantic Change Detection (SSCD), uses random context swapping to compare the meaning of a target word in two different text corpora.* Results: The method accurately predicts semantic changes of words in four languages (English, German, Swedish, and Latin) and across different time spans (over 50 years and about five years), and achieves significant performance improvements compared to strong baselines for the English semantic change prediction task.Here are the three key points in Simplified Chinese:* For: 文章目的是检测不同文本集中单个词语的 semantics 是否发生变化。* Methods: 提议的方法是基于随机上下文交换的 Swapping-based Semantic Change Detection (SSCD)，用于比较两个不同文本集中单个词语的含义。* Results: 方法可以准确地检测单个词语在四种语言（英语、德语、瑞典语和拉丁语）和不同时间间隔（超过50年和约5年）中的 semantics 变化，并在英语 semantic change prediction 任务上 achiev 高性能改进。<details>
<summary>Abstract</summary>
Meanings of words change over time and across domains. Detecting the semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. We consider the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $\mathcal{C}_1$ and $\mathcal{C}_2$. For this purpose, we propose $\textit{Swapping-based Semantic Change Detection}$ (SSCD), an unsupervised method that randomly swaps contexts between $\mathcal{C}_1$ and $\mathcal{C}_2$ where $w$ occurs. We then look at the distribution of contextualised word embeddings of $w$, obtained from a pretrained masked language model (MLM), representing the meaning of $w$ in its occurrence contexts in $\mathcal{C}_1$ and $\mathcal{C}_2$. Intuitively, if the meaning of $w$ does not change between $\mathcal{C}_1$ and $\mathcal{C}_2$, we would expect the distributions of contextualised word embeddings of $w$ to remain the same before and after this random swapping process. Despite its simplicity, we demonstrate that even by using pretrained MLMs without any fine-tuning, our proposed context swapping method accurately predicts the semantic changes of words in four languages (English, German, Swedish, and Latin) and across different time spans (over 50 years and about five years). Moreover, our method achieves significant performance improvements compared to strong baselines for the English semantic change prediction task. Source code is available at https://github.com/a1da4/svp-swap .
</details>
<details>
<summary>摘要</summary>
文字的意思随时间和领域而变化。探测文字的 semantic change 是 NLP 应用中的一项重要任务，需要做到时效预测。我们考虑了 predicting whether a given target word, $w$, changes its meaning between two different text corpora, $\mathcal{C}_1$ and $\mathcal{C}_2$ 的问题。为此，我们提出了 $\textit{Swapping-based Semantic Change Detection}$ (SSCD)，一种无监督的方法， randomly swaps contexts between $\mathcal{C}_1$ and $\mathcal{C}_2$ where $w$ occurs。然后，我们 examine the distribution of contextualised word embeddings of $w$, obtained from a pretrained masked language model (MLM), representing the meaning of $w$ in its occurrence contexts in $\mathcal{C}_1$ and $\mathcal{C}_2$。如果 $w$ 的意思在 $\mathcal{C}_1$ 和 $\mathcal{C}_2$ 中不变，我们就会 expects the distributions of contextualised word embeddings of $w$ to remain the same before and after this random swapping process。尽管其简单，我们示示了使用预训练 MLM 无需 fine-tuning 的我们提posed context swapping method 可以准确地预测英语、德语、瑞典语和拉丁语中文字的 semantic change ，并且在不同的时间间隔（超过 50 年和约 5 年）中具有显著的性能提升。此外，我们的方法在英语 semantic change prediction 任务中也具有显著的性能提升。代码可以在 https://github.com/a1da4/svp-swap 找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Better-Understanding-of-Variations-in-Zero-Shot-Neural-Machine-Translation-Performance"><a href="#Towards-a-Better-Understanding-of-Variations-in-Zero-Shot-Neural-Machine-Translation-Performance" class="headerlink" title="Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance"></a>Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10385">http://arxiv.org/abs/2310.10385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Smu-Tan/ZS-NMT-Variations">https://github.com/Smu-Tan/ZS-NMT-Variations</a></li>
<li>paper_authors: Shaomu Tan, Christof Monz</li>
<li>for: 这个论文旨在探讨多语言神经机器翻译（MNMT）在零shot（ZS）翻译质量方面存在高度变化的原因。</li>
<li>methods: 该论文采用了系统性的实验方法，涵盖了40种语言的1560个翻译方向。通过分析， authors发现了三个关键因素对零shot NMT性能产生高度变化：1）目标语言翻译能力，2）词汇重叠，3）语言特性。</li>
<li>results: 研究发现，目标语言翻译质量是零shot NMT性能的最大影响因素，词汇重叠一直影响翻译质量。此外，语言属性，如语言家族和书写系统，对小型模型来说也具有一定的影响。此外， authors还发现了零shot翻译挑战不仅是 Off-target 问题，更是 beyond Off-target 问题。<details>
<summary>Abstract</summary>
Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low ZS performance, our work introduces a fresh perspective: the presence of high variations in ZS performance. This suggests that MNMT does not uniformly exhibit poor ZS capability; instead, certain translation directions yield reasonable results. Through systematic experimentation involving 1,560 language directions spanning 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target side translation capability 2) vocabulary overlap 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting ZS performance. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furthermore, we suggest that the off-target issue is a symptom of inadequate ZS performance, emphasizing that zero-shot translation challenges extend beyond addressing the off-target problem. We release the data and models serving as a benchmark to study zero-shot for future research at https://github.com/Smu-Tan/ZS-NMT-Variations
</details>
<details>
<summary>摘要</summary>
多语言神经机器翻译（MNMT）促进知识共享，但经常受到零上下文（ZS）翻译质量的劣化影响。尽管先前的工作已经探讨过总体低ZS性能的原因，我们的工作引入了一个新的视角：ZS翻译方向中的高变化性。这表示MNMT不uniformmente具有差的ZS能力；相反，某些翻译方向实际上可以得到不错的结果。通过对40种语言、1560个语言方向进行系统性的实验，我们确定了三个关键因素对ZS NMT性能的高变化：1）目标语言翻译能力2）词汇重叠3）语言特性。我们的发现表明目标语言翻译质量是最重要的因素，词汇重叠一直影响ZS性能。此外，语言家庭和书写系统等语言特性也在一定程度上影响ZS性能，特别是使用较小的模型时。此外，我们认为偏离问题是ZS翻译挑战的一部分，强调零上下文翻译挑战不仅是解决偏离问题而已。我们在github上发布了数据和模型，用于未来研究零上下文翻译，请参考https://github.com/Smu-Tan/ZS-NMT-Variations。
</details></li>
</ul>
<hr>
<h2 id="Privacy-in-Large-Language-Models-Attacks-Defenses-and-Future-Directions"><a href="#Privacy-in-Large-Language-Models-Attacks-Defenses-and-Future-Directions" class="headerlink" title="Privacy in Large Language Models: Attacks, Defenses and Future Directions"></a>Privacy in Large Language Models: Attacks, Defenses and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10383">http://arxiv.org/abs/2310.10383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song</li>
<li>for: This paper aims to provide a comprehensive analysis of privacy attacks targeting large language models (LLMs) and to identify potential vulnerabilities in these models.</li>
<li>methods: The paper uses a categorization of privacy attacks based on the adversary’s assumed capabilities to shed light on the potential vulnerabilities present in LLMs. It also presents a detailed overview of prominent defense strategies that have been developed to counter these privacy attacks.</li>
<li>results: The paper identifies upcoming privacy concerns as LLMs evolve and points out several potential avenues for future exploration.<details>
<summary>Abstract</summary>
The advancement of large language models (LLMs) has significantly enhanced the ability to effectively tackle various downstream NLP tasks and unify these tasks into generative pipelines. On the one hand, powerful language models, trained on massive textual data, have brought unparalleled accessibility and usability for both models and users. On the other hand, unrestricted access to these models can also introduce potential malicious and unintentional privacy risks. Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved. In this paper, we provide a comprehensive analysis of the current privacy attacks targeting LLMs and categorize them according to the adversary's assumed capabilities to shed light on the potential vulnerabilities present in LLMs. Then, we present a detailed overview of prominent defense strategies that have been developed to counter these privacy attacks. Beyond existing works, we identify upcoming privacy concerns as LLMs evolve. Lastly, we point out several potential avenues for future exploration.
</details>
<details>
<summary>摘要</summary>
LLMs 的进步significantly 提高了解决不同下游 NLP 任务的能力，并将这些任务集成成生成管道。一方面，强大的语言模型，通过庞大的文本数据进行训练，带来了无 precedent的可用性和使用性，对于模型和用户来说。然而，不受限制的访问这些模型也可能 introduce 恶意和无意的隐私风险。虽然持续努力解决 LLMS 中的安全和隐私问题，但问题仍未得到解决。本文提供了 LLMS 中隐私攻击的全面分析，根据敌对者假设的能力，将隐私攻击分为不同类别，以透视 LLMS 中的可能性隐私漏洞。然后，我们提供了一个详细的防御策略的概述，以响应这些隐私攻击。此外，我们还标识了 LLMS 的未来隐私问题。最后，我们指出了未来探索的一些可能性。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Data-Augmentation-for-Task-Oriented-Dialog-Systems"><a href="#Contextual-Data-Augmentation-for-Task-Oriented-Dialog-Systems" class="headerlink" title="Contextual Data Augmentation for Task-Oriented Dialog Systems"></a>Contextual Data Augmentation for Task-Oriented Dialog Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10380">http://arxiv.org/abs/2310.10380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Axman, Avik Ray, Shubham Garg, Jing Huang</li>
<li>for: 增强当前对话系统的训练任务。</li>
<li>methods: 使用对话上下文 Conditional 生成用户回复，并通过新的提示设计和输出重新排序来生成对话。</li>
<li>results: 在多种 benchmark 数据集上，我们的对话增强模型可以生成高质量的对话，提高对话成功率达到 $8%$ 的提高。Here’s the full text in Simplified Chinese:</li>
<li>for: 本文主要用于增强当前对话系统的训练任务。</li>
<li>methods: 我们提出了一种基于对话上下文 Conditional 生成用户回复的对话增强模型，并通过新的提示设计和输出重新排序来生成对话。</li>
<li>results: 在多种 benchmark 数据集上，我们的对话增强模型可以生成高质量的对话，提高对话成功率达到 $8%$ 的提高。<details>
<summary>Abstract</summary>
Collection of annotated dialogs for training task-oriented dialog systems have been one of the key bottlenecks in improving current models. While dialog response generation has been widely studied on the agent side, it is not evident if similar generative models can be used to generate a large variety of, and often unexpected, user inputs that real dialog systems encounter in practice. Existing data augmentation techniques such as paraphrase generation do not take the dialog context into consideration. In this paper, we develop a novel dialog augmentation model that generates a user turn, conditioning on full dialog context. Additionally, with a new prompt design for language model, and output re-ranking, the dialogs generated from our model can be directly used to train downstream dialog systems. On common benchmark datasets MultiWoZ and SGD, we show that our dialog augmentation model generates high quality dialogs and improves dialog success rate by as much as $8\%$ over baseline.
</details>
<details>
<summary>摘要</summary>
“对话系统训练 Task-oriented 对话系统的集成 annotation 对话集成是一个关键瓶颈，目前模型的改进。虽然对话回复生成已经广泛研究，但是不清楚是否可以使用类似的生成模型来生成实际对话系统遇到的多样化和意外的用户输入。现有的数据增强技术，如重叠生成，不考虑对话上下文。在本文中，我们开发了一种基于对话上下文的对话增强模型，可以生成用户转折，并且通过新的语言模型提示和输出重新排序，生成的对话可以直接用于下游对话系统训练。在 MultiWoZ 和 SGD 等常用数据集上，我们展示了我们的对话增强模型可以生成高质量对话，提高对话成功率达到 $8\%$ 。”
</details></li>
</ul>
<hr>
<h2 id="Legal-NLP-Meets-MiCAR-Advancing-the-Analysis-of-Crypto-White-Papers"><a href="#Legal-NLP-Meets-MiCAR-Advancing-the-Analysis-of-Crypto-White-Papers" class="headerlink" title="Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers"></a>Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10333">http://arxiv.org/abs/2310.10333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina Camassa</li>
<li>for: 这个论文是为了探讨欧盟Markets in Crypto-Assets Regulation（MiCAR）对不ikel进行规范的影响，以及在这个领域中文本分析的应用。</li>
<li>methods: 本论文使用自然语言处理（NLP）技术来分析不ikel白皮书，并探讨在MiCAR规范下如何integrate NLP。</li>
<li>results: 本论文发现了不ikel白皮书的文本分析应用存在一些研究漏洞，并对MiCAR规范的影响进行了分析，从而为规范机构、投资者和私有货币发行人提供了可能的研究方向。<details>
<summary>Abstract</summary>
In the rapidly evolving field of crypto assets, white papers are essential documents for investor guidance, and are now subject to unprecedented content requirements under the European Union's Markets in Crypto-Assets Regulation (MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for both analyzing these documents and assisting in regulatory compliance. This paper delivers two contributions to the topic. First, we survey existing applications of textual analysis to unregulated crypto asset white papers, uncovering a research gap that could be bridged with interdisciplinary collaboration. We then conduct an analysis of the changes introduced by MiCAR, highlighting the opportunities and challenges of integrating NLP within the new regulatory framework. The findings set the stage for further research, with the potential to benefit regulators, crypto asset issuers, and investors.
</details>
<details>
<summary>摘要</summary>
在迅速发展的区块链资产领域，白皮书是投资者指导的重要文件，现在欧盟市场区块链资产管理法规（MiCAR）下面面临无前例的内容要求。自然语言处理（NLP）可以作为分析这些文件并协助合规遵守的强大工具。这篇论文在这个主题上做出了两项贡献。首先，我们对未经规范的区块链资产白皮书的文本分析应用进行了调查，揭示出了一个研究差距，这可以通过交叉领域合作bridged。然后，我们对MiCAR引入的变化进行了分析， highlighting the opportunities and challenges of integrating NLP within the new regulatory framework。这些发现可以为 regulators、区块链资产发行人和投资者带来 beneficial。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Tokenization-for-Transcribed-Error-Correction"><a href="#Optimized-Tokenization-for-Transcribed-Error-Correction" class="headerlink" title="Optimized Tokenization for Transcribed Error Correction"></a>Optimized Tokenization for Transcribed Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10704">http://arxiv.org/abs/2310.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomer Wullach, Shlomo E. Chazan</li>
<li>for: 提高 speech recognition 系统的精度和可靠性</li>
<li>methods: 使用生成的错误分布和语言特定的 vocabulary 调整</li>
<li>results: 证明使用生成的错误分布和语言特定的 vocabulary 可以提高 correction 模型的性能，并且可以在多种语言和speech recognition 系统中应用<details>
<summary>Abstract</summary>
The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized, however, bridging the distribution gap between transcribed errors and synthetic noise is not trivial. In this paper, we demonstrate that the performance of correction models can be significantly increased by training solely using synthetic data. Specifically, we empirically show that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocabulary of a BPE tokenizer strike a balance between adapting to unseen distributions and retaining knowledge of transcribed errors. We showcase the benefits of these key observations, and evaluate our approach using multiple languages, speech recognition systems and prominent speech recognition datasets.
</details>
<details>
<summary>摘要</summary>
Speech recognition systems face many challenges, such as differences in pronunciation, poor audio quality, and a lack of labeled data. To address these challenges, researchers have found that using dedicated error correction models can be effective, but these models require large amounts of labeled data, which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often used, but it can be difficult to bridge the gap between the distribution of transcribed errors and the synthetic noise. In this paper, we show that the performance of correction models can be significantly improved by training solely using synthetic data. Specifically, we find that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocabulary of a BPE tokenizer can strike a balance between adapting to unseen distributions and retaining knowledge of transcribed errors. We demonstrate the benefits of these key observations using multiple languages, speech recognition systems, and prominent speech recognition datasets.
</details></li>
</ul>
<hr>
<h2 id="Untying-the-Reversal-Curse-via-Bidirectional-Language-Model-Editing"><a href="#Untying-the-Reversal-Curse-via-Bidirectional-Language-Model-Editing" class="headerlink" title="Untying the Reversal Curse via Bidirectional Language Model Editing"></a>Untying the Reversal Curse via Bidirectional Language Model Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10322">http://arxiv.org/abs/2310.10322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mjy1111/BAKE">https://github.com/mjy1111/BAKE</a></li>
<li>paper_authors: Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, Cong Liu</li>
<li>for: 本研究旨在提供一种 bidirectional language model editing 的评估方法，以评估编辑后模型是否可以在反向方向上撤回知识。</li>
<li>methods: 本研究提出了一种 bidirectional assessment for knowledge editing (BAKE) 的benchmark，以评估编辑后模型的反向可逆性。此外，研究还提出了一种名为 bidirectionally inversible relationship modeling (BIRD) 的方法，用于 Mitigating the reversal curse。</li>
<li>results: 实验显示，BIRD 可以通过更新模型参数来提高四种不同大小的 LLM 的表现，并且可以在问答和判断任务中提高模型的表现。<details>
<summary>Abstract</summary>
Recent studies have demonstrated that large language models (LLMs) store massive factual knowledge within their parameters. But existing LLMs are prone to hallucinate unintended text due to false or outdated knowledge. Since retraining LLMs is resource intensive, there has been a growing interest in the concept of model editing. Despite the emergence of benchmarks and approaches, these unidirectional editing and evaluation have failed to explore the reversal curse. Intuitively, if "The capital of France is" is edited to be a counterfact "London" within a model, then it should be able to naturally reason and recall the reverse fact, i.e., "London is the capital of" followed by "France" instead of "England". In this paper, we study bidirectional language model editing, aiming to provide rigorous model editing evaluation to assess if edited LLMs can recall the editing knowledge bidirectionally. A new evaluation metric of reversibility is introduced, and a benchmark dubbed as Bidirectional Assessment for Knowledge Editing (BAKE) is constructed to evaluate the reversibility of edited models in recalling knowledge in the reverse direction of editing. We surprisingly observe that while current editing methods and LLMs can effectively recall editing facts in the direction of editing, they suffer serious deficiencies when evaluated in the reverse direction. To mitigate the reversal curse, a method named Bidirectionally Inversible Relationship moDeling (BIRD) is proposed. A set of editing objectives that incorporate bidirectional relationships between subject and object into the updated model weights are designed. Experiments show that BIRD improves the performance of four representative LLMs of different sizes via question answering and judgement.
</details>
<details>
<summary>摘要</summary>
研究者最近发现，大型语言模型（LLM）中含有巨量的事实知识。然而，现有的LLM容易产生假或过时的知识，导致模型产生假信息。由于重新训练LLM是资源占用的，因此对模型编辑的概念产生了增加的兴趣。虽然有了 benchmarcks 和方法，但这些单向编辑和评估未能探索反转咒。在这篇文章中，我们研究了对向语言模型编辑，以提供对编辑后模型的精确评估，以确定编辑后模型是否可以在反向方向上恢复编辑知识。我们引入了一种新的评估指标——反向可逆性指标，并构建了一个名为“ bidirectional Assessment for Knowledge Editing”（BAKE）的benchmarcks，以评估编辑后模型在反向方向上的知识恢复能力。我们意外发现，当前的编辑方法和LLM可以很好地在编辑方向上恢复编辑知识，但在反向方向上表现异常差。为了 Mitigate the reversal curse，我们提出了一种名为“ bidirectionally Inversible Relationship moDeling”（BIRD）的方法。我们设计了一组编辑目标，将对象和主题之间的双向关系 integrate 到更新后的模型参数中。实验表明，BIRD 可以提高四种不同大小的 LLM 的表现，通过问答和判断。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Bias-in-Multilingual-Language-Models-Cross-Lingual-Transfer-of-Debiasing-Techniques"><a href="#Investigating-Bias-in-Multilingual-Language-Models-Cross-Lingual-Transfer-of-Debiasing-Techniques" class="headerlink" title="Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques"></a>Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10310">http://arxiv.org/abs/2310.10310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manon-reusens/multilingual_bias">https://github.com/manon-reusens/multilingual_bias</a></li>
<li>paper_authors: Manon Reusens, Philipp Borchert, Margot Mieskes, Jochen De Weerdt, Bart Baesens</li>
<li>for: 本研究探讨了多语言模型中偏见纠正技术的跨语言传递性。我们对英文、法语、德语和荷语进行了研究。</li>
<li>methods: 我们使用多语言BERT（mBERT）来检验跨语言纠正技术的可行性，并发现这些技术可以跨语言传递，并且在不同语言上表现良好。</li>
<li>results: 我们发现，对非英语语言应用这些技术不会带来性能下降。使用CrowS-Pairs数据集的翻译，我们发现 SentenceDebias 是所有语言中最佳的纠正技术，可以在 mBERT 中减少偏见约13%。此外，我们发现在各种语言上追加预训练可以提高跨语言效果，特别是在低资源语言中。<details>
<summary>Abstract</summary>
This paper investigates the transferability of debiasing techniques across different languages within multilingual models. We examine the applicability of these techniques in English, French, German, and Dutch. Using multilingual BERT (mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is not only feasible but also yields promising results. Surprisingly, our findings reveal no performance disadvantages when applying these techniques to non-English languages. Using translations of the CrowS-Pairs dataset, our analysis identifies SentenceDebias as the best technique across different languages, reducing bias in mBERT by an average of 13%. We also find that debiasing techniques with additional pretraining exhibit enhanced cross-lingual effectiveness for the languages included in the analyses, particularly in lower-resource languages. These novel insights contribute to a deeper understanding of bias mitigation in multilingual language models and provide practical guidance for debiasing techniques in different language contexts.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文研究了多语言模型中的偏见纠正技术的传递性。我们对英语、法语、德语和荷语进行了研究，使用多语言BERT（mBERT）来示范了跨语言传递的偏见纠正技术的可行性和效果。我们的结果表明，对非英语语言应用这些技术并不会带来性能下降，而且使用翻译的 CrowS-Pairs 数据集，我们的分析发现，在不同语言上，SentenceDebias 是最有效的技术，可以减少 mBERT 中的偏见程度。此外，我们还发现，对于不同语言的语言模型，额外的预训练可以提高跨语言效果，特别是对于低资源语言。这些发现对偏见纠正在多语言语言模型中的深入理解和实践指导提供了有价值的贡献。
</details></li>
</ul>
<hr>
<h2 id="Multi-Stage-Pre-training-Enhanced-by-ChatGPT-for-Multi-Scenario-Multi-Domain-Dialogue-Summarization"><a href="#Multi-Stage-Pre-training-Enhanced-by-ChatGPT-for-Multi-Scenario-Multi-Domain-Dialogue-Summarization" class="headerlink" title="Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization"></a>Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10285">http://arxiv.org/abs/2310.10285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouweixiao/mp4">https://github.com/zhouweixiao/mp4</a></li>
<li>paper_authors: Weixiao Zhou, Gengyao Li, Xianfu Cheng, Xinnian Liang, Junnan Zhu, Feifei Zhai, Zhoujun Li</li>
<li>for: 本研究针对多scene多domain的对话摘要进行了新的预训练模型设计，以增强预训练模型的适应性和对话摘要能力。</li>
<li>methods: 本研究使用了一种多stage预训练策略，通过将各个预训练目标调整为预训练模型的核心部分，以减少预训练模型与精革模型之间的差距。具体来说，我们首先进行了域对预训练，使用大量多scene多domain的对话资料，以增强我们的预训练模型的适应性。然后，我们进行了任务对预训练，使用大量多scene多domain的 “对话摘要” 平行数据，由ChatGPT进行标注，以增强我们的预训练模型的对话摘要能力。</li>
<li>results: 实验结果显示，我们的预训练模型在全域 fine-tuning、zero-shot 和几少shot设定中均有着重要的进步，与先前的状态艺术模型相比，具有更高的准确率和更好的一致性。<details>
<summary>Abstract</summary>
Dialogue summarization involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a multi-stage pre-training strategy to reduce the gap between the pre-training objective and fine-tuning objective. Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario multi-domain dialogue data to enhance the adaptability of our pre-trained model. Then, we conduct task-oriented pre-training using large-scale multi-scenario multi-domain "dialogue-summary" parallel data annotated by ChatGPT to enhance the dialogue summarization ability of our pre-trained model. Experimental results on three dialogue summarization datasets from different scenarios and domains indicate that our pre-trained model significantly outperforms previous state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.
</details>
<details>
<summary>摘要</summary>
对话概要化 involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a multi-stage pre-training strategy to reduce the gap between the pre-training objective and fine-tuning objective. Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario multi-domain dialogue data to enhance the adaptability of our pre-trained model. Then, we conduct task-oriented pre-training using large-scale multi-scenario multi-domain "dialogue-summary" parallel data annotated by ChatGPT to enhance the dialogue summarization ability of our pre-trained model. Experimental results on three dialogue summarization datasets from different scenarios and domains indicate that our pre-trained model significantly outperforms previous state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.Here's the translation in Traditional Chinese:对话概要化 involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a multi-stage pre-training strategy to reduce the gap between the pre-training objective and fine-tuning objective. Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario multi-domain dialogue data to enhance the adaptability of our pre-trained model. Then, we conduct task-oriented pre-training using large-scale multi-scenario multi-domain "dialogue-summary" parallel data annotated by ChatGPT to enhance the dialogue summarization ability of our pre-trained model. Experimental results on three dialogue summarization datasets from different scenarios and domains indicate that our pre-trained model significantly outperforms previous state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.
</details></li>
</ul>
<hr>
<h2 id="Generative-Calibration-for-In-context-Learning"><a href="#Generative-Calibration-for-In-context-Learning" class="headerlink" title="Generative Calibration for In-context Learning"></a>Generative Calibration for In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10266">http://arxiv.org/abs/2310.10266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changmenseng/generative_calibration">https://github.com/changmenseng/generative_calibration</a></li>
<li>paper_authors: Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, Kang Liu</li>
<li>for: 本研究目的是解释LLMs中的受欢迎特性——即场景学习，并提出一种基于生成抽象的约束方法来改进其性能。</li>
<li>methods: 本研究使用了 тео리тиче分析和实验方法来解释受欢迎特性的问题，并提出了一种基于生成抽象的约束方法来改进性能。</li>
<li>results: 研究发现，通过调整 labels 的分布，可以提高受欢迎特性的性能，并且这种方法可以在不同的 prompt 配置下保持稳定性。实验结果显示，提出的方法可以大幅提高受欢迎特性的性能，相比于 ICAL 和现有的准则方法，提高了27%的粗略率。<details>
<summary>Abstract</summary>
As one of the most exciting features of large language models (LLMs), in-context learning is a mixed blessing. While it allows users to fast-prototype a task solver with only a few training examples, the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples. In this paper, we for the first time theoretically and empirically identify that such a paradox is mainly due to the label shift of the in-context model to the data distribution, in which LLMs shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$. With this understanding, we can simply calibrate the in-context predictive distribution by adjusting the label marginal, which is estimated via Monte-Carlo sampling over the in-context model, i.e., generation of LLMs. We call our approach as generative calibration. We conduct exhaustive experiments with 12 text classification tasks and 12 LLMs scaling from 774M to 33B, generally find that the proposed method greatly and consistently outperforms the ICL as well as state-of-the-art calibration methods, by up to 27% absolute in macro-F1. Meanwhile, the proposed method is also stable under different prompt configurations.
</details>
<details>
<summary>摘要</summary>
一个 LLM 中最吸引人的特点之一是内容学习（in-context learning），它允许用户快速批量任务解决器，只需要几个训练示例。然而，这种特点同时带来了一些问题，例如prompt的选择和顺序对性能的敏感性。在这篇论文中，我们首次 theoretically和empirically发现，这种парадок斯主要是由 LLMS 的数据分布 Label Shift 引起的， LLMS 会将标签梯度 $p(y)$ Shift，而保持标签条件 $p(x|y)$ 良好。通过这种理解，我们可以简单地调整受 Context 预测分布，这是通过 Monte-Carlo 采样来Estimate  LLMS 的标签梯度。我们称之为生成 Calibration。我们进行了12种文本分类任务和12种 LLMS 的探索性实验，发现我们的方法可以大幅提高 IC 和当前最佳化方法的表现，最高提高27%的绝对值。此外，我们的方法也在不同的 prompt 配置下保持稳定。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Interpretability-using-Human-Similarity-Judgements-to-Prune-Word-Embeddings"><a href="#Enhancing-Interpretability-using-Human-Similarity-Judgements-to-Prune-Word-Embeddings" class="headerlink" title="Enhancing Interpretability using Human Similarity Judgements to Prune Word Embeddings"></a>Enhancing Interpretability using Human Similarity Judgements to Prune Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10262">http://arxiv.org/abs/2310.10262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natalia Flechas Manrique, Wanqian Bao, Aurelie Herbelot, Uri Hasson</li>
<li>for: 这个论文的目的是提供一种可读性方法，以便理解自然语言处理（NLP）系统中的 semantics。</li>
<li>methods: 这种方法使用supervised learning，并为给定的领域（如运动、职业）标识一 subset of 模型特征，以提高人类相似性判断的预测。这种方法只保留20-40%的原始特征，并且在8个独立的semantic domain中都有不同的特征集。</li>
<li>results: 这种方法可以帮助理解NLP系统中的 semantics，并且可以用来解释 humans 对不同领域的分类。例如， humans 在分类运动时会 differentiate  based on how gender-inclusive和international they are。此外，这种方法还可以用来预测words的Semantic dimensions，例如 cognitive、emotional 和social dimensions。<details>
<summary>Abstract</summary>
Interpretability methods in NLP aim to provide insights into the semantics underlying specific system architectures. Focusing on word embeddings, we present a supervised-learning method that, for a given domain (e.g., sports, professions), identifies a subset of model features that strongly improve prediction of human similarity judgments. We show this method keeps only 20-40% of the original embeddings, for 8 independent semantic domains, and that it retains different feature sets across domains. We then present two approaches for interpreting the semantics of the retained features. The first obtains the scores of the domain words (co-hyponyms) on the first principal component of the retained embeddings, and extracts terms whose co-occurrence with the co-hyponyms tracks these scores' profile. This analysis reveals that humans differentiate e.g. sports based on how gender-inclusive and international they are. The second approach uses the retained sets as variables in a probing task that predicts values along 65 semantically annotated dimensions for a dataset of 535 words. The features retained for professions are best at predicting cognitive, emotional and social dimensions, whereas features retained for fruits or vegetables best predict the gustation (taste) dimension. We discuss implications for alignment between AI systems and human knowledge.
</details>
<details>
<summary>摘要</summary>
《NLプロセッシングにおける可読性方法の探索》目的：提供NLプロセッシング中の具体的システム构造の下でのSemanticsの问题解釈を提供する方法。方法：1. 给定のドメイン（例如、スポーツ、职业）に対し、predict human similarity judgmentsのためのsupervised learning方法を提供する。2. この方法では、原始の埋め込みに対して、20-40%の削减を行い、8つの独立したセマンティック ドメインでのpredictionの改善を目指す。3. この方法でRetained featuresのSemanticsを解釈するために、two approachesを提供する。第一方法：1. Retained embeddingsの初期Componentのスコアを计算し、これらのスコアに対応するドメインワード（co-hyponyms）のスコアを测定する。2. この分析により、人々はスポーツなどをどのように区别しているかを理解することができる。第二方法：1. Retained setsを使用して、65を超えるsemantically annotated dimensionに対する予测タスクを実行する。2. この方法では、职业に対するRetained featuresは、cognitive、emotional、socialdimensionsに最も优れていることが分かり、フルーツや野菜に対するRetained featuresは、味（gustation）dimensionに最も优れていることが分かる。结论：これらの方法により、AIシステムと人间の知识のAlignmentを改善することができる。
</details></li>
</ul>
<hr>
<h2 id="Repetition-In-Repetition-Out-Towards-Understanding-Neural-Text-Degeneration-from-the-Data-Perspective"><a href="#Repetition-In-Repetition-Out-Towards-Understanding-Neural-Text-Degeneration-from-the-Data-Perspective" class="headerlink" title="Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective"></a>Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10226">http://arxiv.org/abs/2310.10226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmftbygmftby/rep-dropout">https://github.com/gmftbygmftby/rep-dropout</a></li>
<li>paper_authors: Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, Yixuan Su</li>
<li>for: 本研究旨在解释文本神经网络垃圾问题的根本原因，从数据角度出发，提出了一个简单的解释。</li>
<li>methods: 我们采用了随机抽样和注意力抑制等方法来调查这个问题，并进行了实验 validate our findings。</li>
<li>results: 我们的实验结果表明，训练数据中的重复元素与神经网络垃圾问题之间存在强相关关系，避免训练数据中的重复元素可以大幅减少垃圾问题的出现。此外，我们发现，对于不同的方法，包括高流入词、可能性目标和自我强化现象，都可以通过对训练数据中的重复元素进行罚金来解释其效果。<details>
<summary>Abstract</summary>
There are a number of diverging hypotheses about the neural text degeneration problem, i.e., generating repetitive and dull loops, which makes this problem both interesting and confusing. In this work, we aim to advance our understanding by presenting a straightforward and fundamental explanation from the data perspective. Our preliminary investigation reveals a strong correlation between the degeneration issue and the presence of repetitions in training data. Subsequent experiments also demonstrate that by selectively dropping out the attention to repetitive words in training data, degeneration can be significantly minimized. Furthermore, our empirical analysis illustrates that prior works addressing the degeneration issue from various standpoints, such as the high-inflow words, the likelihood objective, and the self-reinforcement phenomenon, can be interpreted by one simple explanation. That is, penalizing the repetitions in training data is a common and fundamental factor for their effectiveness. Moreover, our experiments reveal that penalizing the repetitions in training data remains critical even when considering larger model sizes and instruction tuning.
</details>
<details>
<summary>摘要</summary>
有很多关于神经文本衰退问题的不同假设，即生成循环和极端的循环，使得这个问题同时具有诱人性和混乱性。在这项工作中，我们希望通过数据角度提供直接和基本的解释，以进一步深化我们对这个问题的理解。我们的初步调查发现，衰退问题与训练数据中的重复的强相关性存在很强的关系。后续的实验也表明，在训练数据中 selectively dropping out 重复的注意力可以明显减少衰退。此外，我们的实验分析表明，先前关于衰退问题的不同方法，如高流入词、可能性目标和自我强化现象，都可以通过一个简单的解释：即在训练数据中 penalty 重复。此外，我们的实验还表明，即使考虑更大的模型大小和指导调整，penalizing 训练数据中的重复仍然是关键的。
</details></li>
</ul>
<hr>
<h2 id="AdaLomo-Low-memory-Optimization-with-Adaptive-Learning-Rate"><a href="#AdaLomo-Low-memory-Optimization-with-Adaptive-Learning-Rate" class="headerlink" title="AdaLomo: Low-memory Optimization with Adaptive Learning Rate"></a>AdaLomo: Low-memory Optimization with Adaptive Learning Rate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10195">http://arxiv.org/abs/2310.10195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlmlab/lomo">https://github.com/openlmlab/lomo</a></li>
<li>paper_authors: Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu</li>
<li>for: 降低大语言模型训练的硬件门槛</li>
<li>methods: 利用非负矩阵分解估算二阶均值，采用分组更新正则化稳定收敛</li>
<li>results: 与AdamW相当的性能，同时减少训练内存占用<details>
<summary>Abstract</summary>
Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经取得了非常出色的成功，但它们的庞大参数大小需要很大的内存进行训练，从而设置了高度的门槛。而最近提出的低内存优化（LOMO）可以降低内存占用量，但是它的优化技术，类似于随机梯度下降，对于hyper参数敏感，而且 converge 性不如 AdamW 优化器，fail to match the performance of the prevailing optimizer for large language models。经验表明，与滑动 average 相比，适应式学习率更是关键性的 bridging 因素。基于这一点，我们提出了low-memory optimization with adaptive learning rate（AdaLomo），它在每个参数上提供了适应式学习率。为保持内存效率，我们使用非负矩阵因子分解来Estimate 第二个矩阵积分。此外，我们建议使用 grouped update normalization来稳定收敛。我们的实验表明，AdaLomo 可以与 AdamW 的性能相当，同时具有 significanly 降低内存需求，从而降低训练大语言模型的硬件阻碍。
</details></li>
</ul>
<hr>
<h2 id="VIBE-Topic-Driven-Temporal-Adaptation-for-Twitter-Classification"><a href="#VIBE-Topic-Driven-Temporal-Adaptation-for-Twitter-Classification" class="headerlink" title="VIBE: Topic-Driven Temporal Adaptation for Twitter Classification"></a>VIBE: Topic-Driven Temporal Adaptation for Twitter Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10191">http://arxiv.org/abs/2310.10191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CelestineZYJ/VIBE-Temporal-Adaptation">https://github.com/CelestineZYJ/VIBE-Temporal-Adaptation</a></li>
<li>paper_authors: Yuji Zhang, Jing Li, Wenjie Li</li>
<li>for: address the challenge of deteriorating text classification performance in real-world social media due to language evolution</li>
<li>methods: 使用变量信息瓶颈（IB）正则化模型 latent topic evolution 进行时间适应，并通过多任务训练来使用时间戳和类别标签预测</li>
<li>results: 在 Twitter 上进行三种分类任务，与前一个状态的继续预处理方法相比，只使用3%的数据，显著提高了模型的性能<details>
<summary>Abstract</summary>
Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and class label prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our model, with only 3% of data, significantly outperforms previous state-of-the-art continued-pretraining methods.
</details>
<details>
<summary>摘要</summary>
<SYS>语言特征在现实世界社交媒体上发展，导致文本分类的性能下降。为Address这个挑战，我们研究时间适应，即使模型在过去数据上训练后，在未来数据上进行测试。大多数前期工作都集中在继续预训练或知识更新上，这可能会 compromise 社交媒体数据的性能。为解决这个问题，我们通过模拟 latent topic evolution 来反射特征变化，并提出了一种新的模型，namely VIBE：Variational Information Bottleneck for Evolutions。具体来说，我们首先使用两个 Information Bottleneck（IB）正则化来分辨过去和未来话题。然后，这些分辨出来的话题被用作 adaptive features，通过多任务训练时间戳和类别标签预测。在adaptive learning中，VIBE 利用了 posterior 于训练数据时间创建的在线流量中检索到的无标签数据，以进行学习。在 Twitter 上进行了三种分类任务的实验，我们发现，使用只有 3% 的数据，我们的模型可以明显超越先前的继续预训练方法。</SYS>Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="TRIGO-Benchmarking-Formal-Mathematical-Proof-Reduction-for-Generative-Language-Models"><a href="#TRIGO-Benchmarking-Formal-Mathematical-Proof-Reduction-for-Generative-Language-Models" class="headerlink" title="TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models"></a>TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10180">http://arxiv.org/abs/2310.10180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/menik1126/TRIGO">https://github.com/menik1126/TRIGO</a></li>
<li>paper_authors: Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu</li>
<li>for: 检验高级生成语言模型的逻辑能力和数学逻辑能力。</li>
<li>methods: 提出了一个基于Lean formal语言系统的ATP benchmark，评估模型在式子和数学表达中的推理能力和 manipulate、分组、因数化能力。</li>
<li>results: 对高级生成语言模型进行了广泛的实验，发现TRIGO benchmark可以挑战高级模型，包括GPT-4，并提供一个新的工具来研究高级模型在正式逻辑和数学逻辑方面的能力。<details>
<summary>Abstract</summary>
Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proofs but also evaluates a generative LM's reasoning ability on formulas and its capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from the web, annotate the simplification process manually, and translate it into the Lean formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we develop an automatic generator based on Lean-Gym to create dataset splits of varying difficulties and distributions in order to thoroughly analyze the model's generalization ability. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM's including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM's ability on both formal and mathematical reasoning.
</details>
<details>
<summary>摘要</summary>
自动证明 theorem (ATP) 已成为一个吸引人的领域，以探索最新的成功生成语言模型的逻辑能力。然而，当前的 ATP 标准 mainly focuses on 符号逻辑推理，很少涉及复杂的数学运算理解。在这种工作中，我们提出了 TRIGO，一个 ATP 标准，需要模型将 trigonometric 表达式简化为步骤证明，并评估生成LM的逻辑能力，包括数学表达式的排序、分组和因数化。我们从网络上收集了 trigonometric 表达式和简化过程，并 manually 鉴定了这些简化过程。然后，我们使用 Lean 正式语言系统来翻译这些样例，并自动生成了更多的样例来扩大数据集。此外，我们开发了基于 Lean-Gym 的自动生成器，以创建不同难度和分布的数据集，以全面分析模型的总体化能力。我们的广泛实验表明，我们的提出的 TRIGO 对高级的生成LM，包括 GPT-4，具有新的挑战，并提供了一个新的工具来研究生成LM的 both formal 和数学逻辑能力。
</details></li>
</ul>
<hr>
<h2 id="Joint-Music-and-Language-Attention-Models-for-Zero-shot-Music-Tagging"><a href="#Joint-Music-and-Language-Attention-Models-for-Zero-shot-Music-Tagging" class="headerlink" title="Joint Music and Language Attention Models for Zero-shot Music Tagging"></a>Joint Music and Language Attention Models for Zero-shot Music Tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10159">http://arxiv.org/abs/2310.10159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingjian Du, Zhesong Yu, Jiaju Lin, Bilei Zhu, Qiuqiang Kong</li>
<li>for: 这个论文目的是提出一种针对开放集音乐标签问题的零shot音乐标签系统。</li>
<li>methods: 该系统使用一种联合音乐和语言注意力（JMLA）模型，包括一个预训练的masked autoencoder音频编码器和一个Falcon7B干扰器。我们还引入了preceiver resampler将任意长度音频转换为固定长度表示。在编码器和解码器层之间添加了紧密的注意力连接，以改进编码器和解码器层之间的信息流。</li>
<li>results: 我们使用了互联网上收集的大规模音乐和描述数据集来训练JMLA模型。我们使用ChatGPT将原始描述转换为正规化和多样化的描述，以训练JMLA模型。我们的提议的JMLA系统在GTZAN数据集上实现了零shot音乐标签准确率为64.82%，超过了前一个零shot系统的性能，并与前一个系统在FMA和MagnaTagATune数据集上的性能相似。<details>
<summary>Abstract</summary>
Music tagging is a task to predict the tags of music recordings. However, previous music tagging research primarily focuses on close-set music tagging tasks which can not be generalized to new tags. In this work, we propose a zero-shot music tagging system modeled by a joint music and language attention (JMLA) model to address the open-set music tagging problem. The JMLA model consists of an audio encoder modeled by a pretrained masked autoencoder and a decoder modeled by a Falcon7B. We introduce preceiver resampler to convert arbitrary length audio into fixed length embeddings. We introduce dense attention connections between encoder and decoder layers to improve the information flow between the encoder and decoder layers. We collect a large-scale music and description dataset from the internet. We propose to use ChatGPT to convert the raw descriptions into formalized and diverse descriptions to train the JMLA models. Our proposed JMLA system achieves a zero-shot audio tagging accuracy of $ 64.82\% $ on the GTZAN dataset, outperforming previous zero-shot systems and achieves comparable results to previous systems on the FMA and the MagnaTagATune datasets.
</details>
<details>
<summary>摘要</summary>
音乐标注是一项任务，旨在预测音乐录音的标签。然而，过去的音乐标注研究主要集中在靠近音乐标注任务上，这些任务无法泛化到新的标签。在这项工作中，我们提出了一种基于共同音乐和语言注意力（JMLA）模型的零批学习音乐标注系统，以解决开放集音乐标注问题。JMLA模型包括一个预训练的masked autoencoder音频编码器和一个Falcon7B decoder。我们引入了preceiver resampler将任意长度音频转换为固定长度嵌入。我们引入了 dense attention连接 между编码器和解码器层，以改进编码器和解码器之间的信息流。我们收集了互联网上大规模的音乐和描述数据集。我们提议使用ChatGPT将Raw描述转换为正式化和多样化的描述，以训练JMLA模型。我们提出的JMLA系统在GTZAN数据集上实现了零批学习音乐标注精度为64.82%，超过了前一代零批系统的性能，并与前一代系统在FMA和MagnaTagATune数据集上实现了相似的结果。
</details></li>
</ul>
<hr>
<h2 id="DNA-Denoised-Neighborhood-Aggregation-for-Fine-grained-Category-Discovery"><a href="#DNA-Denoised-Neighborhood-Aggregation-for-Fine-grained-Category-Discovery" class="headerlink" title="DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery"></a>DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10151">http://arxiv.org/abs/2310.10151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lackel/DNA">https://github.com/Lackel/DNA</a></li>
<li>paper_authors: Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, Qinghua Zheng, QianYing Wang, Ping Chen</li>
<li>for:  bridging the gap between fine-grained analysis and high annotation cost</li>
<li>methods:  self-supervised framework that encodes semantic structures of data into the embedding space, with three principles to filter out false neighbors</li>
<li>results:  retrieves more accurate neighbors and outperforms state-of-the-art models by a large margin (average 9.96% improvement on three metrics)<details>
<summary>Abstract</summary>
Discovering fine-grained categories from coarsely labeled data is a practical and challenging task, which can bridge the gap between the demand for fine-grained analysis and the high annotation cost. Previous works mainly focus on instance-level discrimination to learn low-level features, but ignore semantic similarities between data, which may prevent these models learning compact cluster representations. In this paper, we propose Denoised Neighborhood Aggregation (DNA), a self-supervised framework that encodes semantic structures of data into the embedding space. Specifically, we retrieve k-nearest neighbors of a query as its positive keys to capture semantic similarities between data and then aggregate information from the neighbors to learn compact cluster representations, which can make fine-grained categories more separatable. However, the retrieved neighbors can be noisy and contain many false-positive keys, which can degrade the quality of learned embeddings. To cope with this challenge, we propose three principles to filter out these false neighbors for better representation learning. Furthermore, we theoretically justify that the learning objective of our framework is equivalent to a clustering loss, which can capture semantic similarities between data to form compact fine-grained clusters. Extensive experiments on three benchmark datasets show that our method can retrieve more accurate neighbors (21.31% accuracy improvement) and outperform state-of-the-art models by a large margin (average 9.96% improvement on three metrics). Our code and data are available at https://github.com/Lackel/DNA.
</details>
<details>
<summary>摘要</summary>
发现细化类别从宽域标注数据是一个实用和挑战性的任务，可以bridging the gap между需求细化分析和高标注成本。先前的工作主要关注实例级别的 отличия来学习低级特征，但忽略数据之间的 semantic similarity，这可能会使这些模型学习不够紧凑的集群表示。在这篇论文中，我们提出了 Denoised Neighborhood Aggregation（DNA），一种无监督的框架，它可以将数据的 semantic structure编码到嵌入空间中。具体来说，我们在查询时检索 k 个最近邻居作为它的正确键，以捕捉数据之间的semantic similarity，然后将邻居中的信息聚合以学习紧凑的集群表示。但是，检索到的邻居可能含有很多假阳键，这会下降学习得到的嵌入的质量。为了解决这个挑战，我们提出了三个原则来筛选假阳键，以便更好地学习嵌入。此外，我们也证明了我们的学习目标等价于一种聚类损失函数，可以捕捉数据之间的semantic similarity，以形成细化的集群。我们在三个标准数据集上进行了广泛的实验，得到了更高准确的邻居（21.31%的准确率提高）和超过当前领先模型（平均9.96%的提高）。我们的代码和数据可以在https://github.com/Lackel/DNA中找到。
</details></li>
</ul>
<hr>
<h2 id="Node-based-Knowledge-Graph-Contrastive-Learning-for-Medical-Relationship-Prediction"><a href="#Node-based-Knowledge-Graph-Contrastive-Learning-for-Medical-Relationship-Prediction" class="headerlink" title="Node-based Knowledge Graph Contrastive Learning for Medical Relationship Prediction"></a>Node-based Knowledge Graph Contrastive Learning for Medical Relationship Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10138">http://arxiv.org/abs/2310.10138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhi520/nc-kge">https://github.com/zhi520/nc-kge</a></li>
<li>paper_authors: Zhiguang Fan, Yuedong Yang, Mingyuan Xu, Hongming Chen</li>
<li>For: The paper is written for enhancing the distinctiveness of knowledge graph embeddings (KGEs) and improving the performance of downstream tasks such as predicting drug combinations and reasoning disease-drug relationships.* Methods: The paper proposes a novel node-based contrastive learning method for KGE, called NC-KGE, which constructs appropriate contrastive node pairs on knowledge graphs (KGs) and integrates a relation-aware attention mechanism to focus on semantic relationships and node interactions.* Results: The paper shows that NC-KGE performs competitively with state-of-the-art models on public datasets and outperforms all baselines in predicting biomedical relationship predictions tasks, especially in predicting drug combination relationships.<details>
<summary>Abstract</summary>
The embedding of Biomedical Knowledge Graphs (BKGs) generates robust representations, valuable for a variety of artificial intelligence applications, including predicting drug combinations and reasoning disease-drug relationships. Meanwhile, contrastive learning (CL) is widely employed to enhance the distinctiveness of these representations. However, constructing suitable contrastive pairs for CL, especially within Knowledge Graphs (KGs), has been challenging. In this paper, we proposed a novel node-based contrastive learning method for knowledge graph embedding, NC-KGE. NC-KGE enhances knowledge extraction in embeddings and speeds up training convergence by constructing appropriate contrastive node pairs on KGs. This scheme can be easily integrated with other knowledge graph embedding (KGE) methods. For downstream task such as biochemical relationship prediction, we have incorporated a relation-aware attention mechanism into NC-KGE, focusing on the semantic relationships and node interactions. Extensive experiments show that NC-KGE performs competitively with state-of-the-art models on public datasets like FB15k-237 and WN18RR. Particularly in biomedical relationship prediction tasks, NC-KGE outperforms all baselines on datasets such as PharmKG8k-28, DRKG17k-21, and BioKG72k-14, especially in predicting drug combination relationships. We release our code at https://github.com/zhi520/NC-KGE.
</details>
<details>
<summary>摘要</summary>
biomedical知识图（BKG）的嵌入生成了可靠的表示，对于许多人工智能应用有益，如预测药物组合和理解疾病药物关系。然而，在知识图（KG）中构建适当的对照对是一项挑战。在这篇论文中，我们提出了一种新的节点基于对照学习方法 для知识图嵌入（NC-KGE）。NC-KGE在KG中构建适当的对照节点对，从而提高了知识EXTRACTION在嵌入中的效果，并加速了训练的收敛。此方法可以轻松地与其他知识图嵌入（KGE）方法结合使用。在下游任务中，我们将一种关系意识注意力机制 incorporated into NC-KGE，这种机制会话焦点在知识图中的semantic关系和节点互动。我们在公共数据集FB15k-237和WN18RR进行了广泛的实验，结果显示NC-KGE与状态之前模型相当竞争。特别是在生物医学关系预测任务中，NC-KGE在PharmKG8k-28、DRKG17k-21和BioKG72k-14等数据集上表现出色，尤其是预测药物组合关系。我们在https://github.com/zhi520/NC-KGE中发布了代码。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Rank-Context-for-Named-Entity-Recognition-Using-a-Synthetic-Dataset"><a href="#Learning-to-Rank-Context-for-Named-Entity-Recognition-Using-a-Synthetic-Dataset" class="headerlink" title="Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset"></a>Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10118">http://arxiv.org/abs/2310.10118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Amalvy, Vincent Labatut, Richard Dufour</li>
<li>for: 提高 named entity recognition (NER) 的准确率，特别是在长文档中。</li>
<li>methods: 使用 Alpaca  instrucituned large language model (LLM) 生成一个 sintethic context retrieval 训练数据集，然后使用 BERT 模型进行 neural context retriever 训练。</li>
<li>results: 在英文文学数据集中（包括 40 本第一章），我们的方法比几种 retrieval 基准方法高效，提高 NER 任务的准确率。<details>
<summary>Abstract</summary>
While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels. To alleviate this issue, a solution is to retrieve relevant context at the document level. Unfortunately, the lack of supervision for such a task means one has to settle for unsupervised approaches. Instead, we propose to generate a synthetic context retrieval training dataset using Alpaca, an instructiontuned large language model (LLM). Using this dataset, we train a neural context retriever based on a BERT model that is able to find relevant context for NER. We show that our method outperforms several retrieval baselines for the NER task on an English literary dataset composed of the first chapter of 40 books.
</details>
<details>
<summary>摘要</summary>
Recent pre-trained transformer-based models can perform named entity recognition (NER) with high accuracy, but their limited range is a problem when applied to long documents like whole novels. To address this issue, we propose to retrieve relevant context at the document level. However, due to the lack of supervision, we must rely on unsupervised approaches. We use Alpaca, an instruction-tuned large language model (LLM), to generate a synthetic context retrieval training dataset, and then train a neural context retriever based on a BERT model to find relevant context for NER. Our method outperforms several retrieval baselines for the NER task on an English literary dataset composed of the first chapter of 40 books.Here's the word-for-word translation of the text into Simplified Chinese:现代预训练变换器模型可以实现命名实体识别（NER）的高精度，但它们的限制范围是一个问题，应用于整个小说等长文档时。为解决这个问题，我们提议在文档级别上提取相关的上下文。然而，由于缺乏监督，我们必须采用无监督方法。我们使用Alpaca，一个指导调整的大型自然语言模型（LLM），生成一个假数据集，并使用这个数据集来训练一个基于BERT模型的神经网络上下文检索器。我们表明，我们的方法在英文文学 dataset 上（由40本第一章组成） outperform 多个检索基准点 для NER 任务。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Multichannel-Speaker-Attributed-ASR-Speaker-Guided-Decoder-and-Input-Feature-Analysis"><a href="#End-to-end-Multichannel-Speaker-Attributed-ASR-Speaker-Guided-Decoder-and-Input-Feature-Analysis" class="headerlink" title="End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder and Input Feature Analysis"></a>End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder and Input Feature Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10106">http://arxiv.org/abs/2310.10106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent</li>
<li>for: 本研究旨在开发一个综合式多频道自动语音识别（MC-SA-ASR）系统，该系统结合Conformer编码器和 speaker-attributed Transformer编码器，并且可以有效地结合语音识别和发音识别模块在多频道设置下。</li>
<li>methods: 本研究使用了Conformer编码器和 speaker-attributed Transformer编码器，并且在多频道设置下使用了多框精度注意力和发音识别模块。</li>
<li>results: 在对LibriSpeech数据的模拟混合数据进行测试时，本研究可以 reduves the word error rate（WER）by up to 12% and 16% compared to previous single-channel and multichannel approaches，respectively。此外，本研究还 investigate了不同的输入特征对ASR性能的影响。最后，我们的实验表明，本系统在真实世界的多频道会议记录中具有有效性。<details>
<summary>Abstract</summary>
We present an end-to-end multichannel speaker-attributed automatic speech recognition (MC-SA-ASR) system that combines a Conformer-based encoder with multi-frame crosschannel attention and a speaker-attributed Transformer-based decoder. To the best of our knowledge, this is the first model that efficiently integrates ASR and speaker identification modules in a multichannel setting. On simulated mixtures of LibriSpeech data, our system reduces the word error rate (WER) by up to 12% and 16% relative compared to previously proposed single-channel and multichannel approaches, respectively. Furthermore, we investigate the impact of different input features, including multichannel magnitude and phase information, on the ASR performance. Finally, our experiments on the AMI corpus confirm the effectiveness of our system for real-world multichannel meeting transcription.
</details>
<details>
<summary>摘要</summary>
我们提出了一个综合式多通道自动语音识别（MC-SA-ASR）系统，该系统使用Conformer编码器和Speaker-attributed Transformer编码器。我们认为这是首个在多通道设定下集成ASR和speaker认知模块的模型。在对LibriSpeech数据的模拟混合物中，我们的系统可以降低单个通道和多个通道方法相比，Word Error Rate（WER）下降至12%和16%。此外，我们还研究了不同的输入特征，包括多通道幅度和频率信息，对ASR性能的影响。最后，我们在AMI corpus上进行了实验，证明了我们的系统在实际多通道会议记录中的有效性。Note: "Simplified Chinese" is used to refer to the standardized form of Chinese used in mainland China, which is different from "Traditional Chinese" used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="Decomposed-Prompt-Tuning-via-Low-Rank-Reparameterization"><a href="#Decomposed-Prompt-Tuning-via-Low-Rank-Reparameterization" class="headerlink" title="Decomposed Prompt Tuning via Low-Rank Reparameterization"></a>Decomposed Prompt Tuning via Low-Rank Reparameterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10094">http://arxiv.org/abs/2310.10094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyaoooo/dpt">https://github.com/xyaoooo/dpt</a></li>
<li>paper_authors: Yao Xiao, Lu Xu, Jiaxi Li, Wei Lu, Xiaoli Li</li>
<li>for: 提高Prompt Tuning的效率和精度</li>
<li>methods: 使用低级别矩阵初始化软提问</li>
<li>results: 在高资源和低资源情况下，实验结果表明提议方法具有效果<details>
<summary>Abstract</summary>
While prompt tuning approaches have achieved competitive performance with high efficiency, we observe that they invariably employ the same initialization process, wherein the soft prompt is either randomly initialized or derived from an existing embedding vocabulary. In contrast to these conventional methods, this study aims to investigate an alternative way to derive soft prompt. Our empirical studies show that the soft prompt typically exhibits a low intrinsic rank characteristic. With such observations, we propose decomposed prompt tuning, a novel approach that utilizes low-rank matrices to initialize the soft prompt. Through the low-rank reparameterization, our method significantly reduces the number of trainable parameters while maintaining effectiveness. Experimental results on the SuperGLUE benchmark in both high-resource and low-resource scenarios demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
而Prompt调整方法已经实现了高效率的竞争性表现，但我们发现这些传统方法 invariably使用相同的初始化过程，即软提示是随机初始化或者基于现有的Embedding词汇。相比之下，这一研究旨在调查一种不同的软提示 derive的方法。我们的实验研究表明，软提示通常具有低内在矩阵特征。基于这些观察，我们提议了分解Prompt调整，一种使用低级数矩阵初始化软提示的新方法。通过低级数重parameter化，我们的方法可以减少训练参数的数量，同时保持效果。SuperGLUEbenchmark上的实验结果表明，我们的方法在高资源和低资源情况下都具有显著的效果。
</details></li>
</ul>
<hr>
<h2 id="JMedLoRA-Medical-Domain-Adaptation-on-Japanese-Large-Language-Models-using-Instruction-tuning"><a href="#JMedLoRA-Medical-Domain-Adaptation-on-Japanese-Large-Language-Models-using-Instruction-tuning" class="headerlink" title="JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning"></a>JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10083">http://arxiv.org/abs/2310.10083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Issey Sukeda, Masahiro Suzuki, Hiroki Sakaji, Satoshi Kodera</li>
<li>for: 本研究旨在探讨如何适应医疗领域的大语言模型（LLMs），以及如何通过领域适应来提高模型的性能。</li>
<li>methods: 本研究使用了LoRA基于的指令调整方法来调整LLMs，以吸收医疗领域特定的知识。</li>
<li>results: 研究发现，通过LoRA基于的指令调整方法，可以部分地将医疗领域特定的知识integrated到LLMs中，大型模型表现更加明显。此外，研究还发现，可以通过适应英语中心模型来进行日本应用领域的适应，同时也 highlighted了日本中心模型的局限性。这些发现可以帮助医疗机构 fine-tune和运行模型，不需要依赖于外部服务。<details>
<summary>Abstract</summary>
In the ongoing wave of impact driven by large language models (LLMs) like ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial research frontier. Since mainstream LLMs tend to be designed for general-purpose applications, constructing a medical LLM through domain adaptation is a huge challenge. While instruction-tuning is used to fine-tune some LLMs, its precise roles in domain adaptation remain unknown. Here we show the contribution of LoRA-based instruction-tuning to performance in Japanese medical question-answering tasks. In doing so, we employ a multifaceted evaluation for multiple-choice questions, including scoring based on "Exact match" and "Gestalt distance" in addition to the conventional accuracy. Our findings suggest that LoRA-based instruction-tuning can partially incorporate domain-specific knowledge into LLMs, with larger models demonstrating more pronounced effects. Furthermore, our results underscore the potential of adapting English-centric models for Japanese applications in domain adaptation, while also highlighting the persisting limitations of Japanese-centric models. This initiative represents a pioneering effort in enabling medical institutions to fine-tune and operate models without relying on external services.
</details>
<details>
<summary>摘要</summary>
在现代语言模型（LLM）如ChatGPT的浪潮中，适应医疗领域的LLM研究已成为一个关键的前沿领域。由于主流LLM通常是设计为通用应用程序，因此在医疗领域中构建一个LLM通过领域适应是一项巨大的挑战。而在某些LLM上进行了 instrucion-tuning，其precise roles在领域适应仍然未知。在这里，我们展示了LoRA基于的instruction-tuning对于日本医学问答任务的贡献。为此，我们采用了多方面的评估方法，包括基于"精确匹配"和"格式距离"的分数，以及传统的准确率。我们的发现表明，LoRA基于的instruction-tuning可以部分地将领域特定知识引入LLM，大型模型表现更加明显。此外，我们的结果还指出了将英语中心模型适应日本应用的潜在优势，同时也高亮了日本中心模型的限制。这个实验代表了医疗机构可以通过自主定制和操作模型而不需要依赖于外部服务的先驱性努力。
</details></li>
</ul>
<hr>
<h2 id="Let’s-reward-step-by-step-Step-Level-reward-model-as-the-Navigators-for-Reasoning"><a href="#Let’s-reward-step-by-step-Step-Level-reward-model-as-the-Navigators-for-Reasoning" class="headerlink" title="Let’s reward step by step: Step-Level reward model as the Navigators for Reasoning"></a>Let’s reward step by step: Step-Level reward model as the Navigators for Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10080">http://arxiv.org/abs/2310.10080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang</li>
<li>for: 本研究旨在探讨用Large Language Models（LLM）进行多步逻辑时，是否可以通过在推理过程中提供反馈或搜索机制来提高推理准确性。</li>
<li>methods: 本研究使用了Process-Supervised Reward Model（PRM），在训练阶段为LLM提供步骤级别的反馈，类似于Proximal Policy Optimization（PPO）或拒绝抽样。我们还提出了一种启发式搜索算法，使用PRM的步骤级别反馈来优化LLM在多步任务中推理的路径。</li>
<li>results: 我们的研究显示，使用修改后的PRM在数学 benchmark 上（GSM8K和MATH）得到了更好的结果，并且在代码生成任务中也得到了类似的改进。此外，我们还开发了一种自动生成步骤级别奖励数据的方法，用于探讨代码生成任务中的不同路径。这些结果表明，我们的奖励模型基于的方法在推理任务中具有良好的robust性。<details>
<summary>Abstract</summary>
Recent years have seen considerable advancements in multi-step reasoning with Large Language Models (LLMs). The previous studies have elucidated the merits of integrating feedback or search mechanisms during model inference to improve the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically furnishes LLMs with step-by-step feedback during the training phase, akin to Proximal Policy Optimization (PPO) or reject sampling. Our objective is to examine the efficacy of PRM in the inference phase to help discern the optimal solution paths for multi-step tasks such as mathematical reasoning and code generation. To this end, we propose a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs. This tailored PRM demonstrated enhanced results compared to the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH. Additionally, to explore the versatility of our approach, we develop a novel method to automatically generate step-level reward dataset for coding tasks and observed similar improved performance in the code generation tasks. Thus highlighting the robust nature of our reward-model-based approach to inference for reasoning tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prompt-Packer-Deceiving-LLMs-through-Compositional-Instruction-with-Hidden-Attacks"><a href="#Prompt-Packer-Deceiving-LLMs-through-Compositional-Instruction-with-Hidden-Attacks" class="headerlink" title="Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks"></a>Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10077">http://arxiv.org/abs/2310.10077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyu Jiang, Xingshu Chen, Rui Tang</li>
<li>for: This paper aims to reveal the vulnerability of large language models (LLMs) to compositional instruction attacks that can elicit harmful content, despite current approaches that focus on detecting and training against harmful prompts.</li>
<li>methods: The paper introduces an innovative technique called Compositional Instruction Attacks (CIA), which combines and encapsulates multiple instructions to hide harmful prompts within harmless ones. Two transformation methods, T-CIA and W-CIA, are also proposed to disguise harmful instructions as talking or writing tasks.</li>
<li>results: The paper achieves an attack success rate of 95%+ on safety assessment datasets and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed), and 91%+ for ChatGLM2 on harmful prompt datasets, demonstrating the effectiveness of CIA in eliciting harmful content from LLMs.<details>
<summary>Abstract</summary>
Recently, Large language models (LLMs) with powerful general capabilities have been increasingly integrated into various Web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications. Current approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. However, they typically focused on the "superficial" harmful prompts with a solitary intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios. In this paper, we introduce an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combination and encapsulation of multiple instructions. CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. Warning: this paper may contain offensive or upsetting content!
</details>
<details>
<summary>摘要</summary>
Currently, large language models (LLMs) with strong overall capabilities have been increasingly integrated into various web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. However, they still face the risk of generating harmful content such as hate speech and criminal activities in practical applications. Existing approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. However, they typically focus on the "superficial" harmful prompts with a single intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios.In this paper, we propose an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combining and encapsulating multiple instructions. CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify the underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs.We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieved an attack success rate of 95%+ on safety assessment datasets and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed), and 91%+ for ChatGLM2 on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. Warning: this paper may contain offensive or upsetting content!
</details></li>
</ul>
<hr>
<h2 id="Bridging-Code-Semantic-and-LLMs-Semantic-Chain-of-Thought-Prompting-for-Code-Generation"><a href="#Bridging-Code-Semantic-and-LLMs-Semantic-Chain-of-Thought-Prompting-for-Code-Generation" class="headerlink" title="Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation"></a>Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10698">http://arxiv.org/abs/2310.10698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingwei Ma, Yue Yu, Shanshan Li, Yu Jiang, Yong Guo, Yuanliang Zhang, Yutao Xie, Xiangke Liao</li>
<li>for: 提高自动代码生成的准确率，充分利用大语言模型（LLM）的含义映射能力。</li>
<li>methods: 提出“含义链条”（SeCoT）方法，通过LLM自动学习源代码的含义信息（如数据流和控制流），提高代码生成的精度。</li>
<li>results: 在三个DL benchmark上实现了状态之准确率提高，证明SeCoT可以帮助大型LLM实现更高精度的代码生成。<details>
<summary>Abstract</summary>
Large language models (LLMs) have showcased remarkable prowess in code generation. However, automated code generation is still challenging since it requires a high-level semantic mapping between natural language requirements and codes. Most existing LLMs-based approaches for code generation rely on decoder-only causal language models often treate codes merely as plain text tokens, i.e., feeding the requirements as a prompt input, and outputing code as flat sequence of tokens, potentially missing the rich semantic features inherent in source code. To bridge this gap, this paper proposes the "Semantic Chain-of-Thought" approach to intruduce semantic information of code, named SeCoT. Our motivation is that the semantic information of the source code (\eg data flow and control flow) describes more precise program execution behavior, intention and function. By guiding LLM consider and integrate semantic information, we can achieve a more granular understanding and representation of code, enhancing code generation accuracy. Meanwhile, while traditional techniques leveraging such semantic information require complex static or dynamic code analysis to obtain features such as data flow and control flow, SeCoT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (i.e., in-context learning), while being generalizable and applicable to challenging domains. While SeCoT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: ChatGPT(close-source model) and WizardCoder(open-source model). The experimental study on three popular DL benchmarks (i.e., HumanEval, HumanEval-ET and MBPP) shows that SeCoT can achieves state-of-the-art performance, greatly improving the potential for large models and code generation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示了很好的代码生成能力。然而，自动化代码生成仍然是一个挑战，因为它需要高级别的 semantic mapping  zwischen自然语言要求和代码。现有的 LLMs-based 方法 для代码生成都是基于 causal 语言模型，通常将代码当作平面文本符号，即通过提供要求作为输入，并将代码输出为平面序列符号。这可能会遗漏代码中的较为复杂的 semantics 特征。为了bridging这个差距，本文提出了“semantic chain-of-thought” 方法，以帮助 LL M 考虑和integrate semantic information，从而提高代码生成的准确性。我们的动机是，源代码中的semantic信息（例如数据流和控制流）可以描述更加精确的程序执行行为、意图和功能。通过引导 LL M 考虑这些semantic信息，我们可以实现代码生成更加精准和智能。而传统的方法需要复杂的静态或动态代码分析，以获取such as data flow和control flow的特征。然而，SeCoT 示出了这个过程可以通过 LL Ms 的内在能力（即在场景学习）自动化，并且可以普遍应用于复杂的领域。SeCoT 可以与不同的 LL Ms 结合使用，本文主要采用了强大的 GPT-style 模型：ChatGPT（关闭源代码模型）和WizardCoder（开源模型）。我们对三个Popular DL  bencmarks（即HumanEval、HumanEval-ET和MBPP）进行了实验，结果表明，SeCoT 可以实现状态机器人的表现，大幅提高可能性。
</details></li>
</ul>
<hr>
<h2 id="EfficientOCR-An-Extensible-Open-Source-Package-for-Efficiently-Digitizing-World-Knowledge"><a href="#EfficientOCR-An-Extensible-Open-Source-Package-for-Efficiently-Digitizing-World-Knowledge" class="headerlink" title="EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge"></a>EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10050">http://arxiv.org/abs/2310.10050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Bryan, Jacob Carlson, Abhishek Arora, Melissa Dell</li>
<li>for:  liberating public domain texts at scale</li>
<li>methods:  EffOCR (EfficientOCR), a novel open-source OCR package that uses a character or word-level image retrieval approach, is accurate and sample efficient to train and deploy</li>
<li>results:  EffOCR was used to digitize 20 million historical U.S. newspaper scans with high accuracy, and achieved zero-shot performance on randomly selected documents from the U.S. National Archives, as well as accurately digitizing Japanese documents that other OCR solutions failed on.<details>
<summary>Abstract</summary>
Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and these texts cannot be incorporated into language model training. Given the diversity and sheer quantity of public domain texts, liberating them at scale requires optical character recognition (OCR) that is accurate, extremely cheap to deploy, and sample-efficient to customize to novel collections, languages, and character sets. Existing OCR engines, largely designed for small-scale commercial applications in high resource languages, often fall short of these requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets both the computational and sample efficiency requirements for liberating texts at scale by abandoning the sequence-to-sequence architecture typically used for OCR, which takes representations from a learned vision model as inputs to a learned language model. Instead, EffOCR models OCR as a character or word-level image retrieval problem. EffOCR is cheap and sample efficient to train, as the model only needs to learn characters' visual appearance and not how they are used in sequence to form language. Models in the EffOCR model zoo can be deployed off-the-shelf with only a few lines of code. Importantly, EffOCR also allows for easy, sample efficient customization with a simple model training interface and minimal labeling requirements due to its sample efficiency. We illustrate the utility of EffOCR by cheaply and accurately digitizing 20 million historical U.S. newspaper scans, evaluating zero-shot performance on randomly selected documents from the U.S. National Archives, and accurately digitizing Japanese documents for which all other OCR solutions failed.
</details>
<details>
<summary>摘要</summary>
亿量公共领域文档尚未被整合到数字化，或者缺乏准确的数字化。现代自然语言处理技术无法对这些文档进行索引、检索和概要分析，或者提取信息进行统计分析，这些文档也无法被包含在语言模型训练中。由于公共领域文档的多样性和庞大量， liberating them at scale requires an accurate, extremely cheap, and sample-efficient optical character recognition (OCR) technology. Existing OCR engines, primarily designed for small-scale commercial applications in high-resource languages, often fall short of these requirements.EffOCR（EfficientOCR）是一个新的开源 OCR 包，它满足了计算机和样本效率的要求，以便大规模解放文档。而不是使用常见的序列到序列架构，EffOCR 将 OCR 视为字符或单词级图像检索问题。EffOCR 具有低成本和样本效率的训练，因为模型只需学习字符的视觉特征，而不是字符串如何在语言中sequenced使用。EffOCR 的模型集可以通过几行代码部署，并且支持轻松、样本效率地自定义。此外，EffOCR 还具有简单的模型训练接口和最小的标注要求，因此可以轻松地进行随机选择的文档评估和日本文档的数字化，而其他 OCR 解决方案都失败了。
</details></li>
</ul>
<hr>
<h2 id="Improving-Large-Language-Model-Fine-tuning-for-Solving-Math-Problems"><a href="#Improving-Large-Language-Model-Fine-tuning-for-Solving-Math-Problems" class="headerlink" title="Improving Large Language Model Fine-tuning for Solving Math Problems"></a>Improving Large Language Model Fine-tuning for Solving Math Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10047">http://arxiv.org/abs/2310.10047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, Peter J. Liu</li>
<li>for: 解决大语言模型（LLMs）在数学问题解决方面的成本高、精度低问题。</li>
<li>methods:  investigate three fine-tuning strategies：(1) solution fine-tuning，(2) solution-cluster re-ranking，(3) multi-task sequential fine-tuning。</li>
<li>results: 使用MATH dataset，对PaLM 2模型进行了三种精度调整策略的研究，并发现：(1) 使用精度调整的步骤解释可以对模型性能产生显著影响；(2) 筛选和多数投票可以单独使用以提高模型性能，同时使用两者可以叠加提高性能；(3) 将生成和评估任务分别进行多任务并行调整可以比基eline更高的性能。<details>
<summary>Abstract</summary>
Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM 2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）在许多自然语言任务上表现出色，但解决数学问题仍然是它们的主要挑战。LLM的通过一次和通过N次性能在解决数学问题上存在很大的差距，这表明LLM可能在解决数学问题的过程中很近于发现正确的解决方案，因此我们对LLM的 fine-tuning 方法进行了探索。使用具有挑战性的 MATH 数据集，我们 investigate了三种 fine-tuning 策略：（1）解决 fine-tuning，我们将 LLM  fine-tune 为生成一个给定数学问题的详细解决方案；（2）解决集 cluster 重新排名，我们将 LLM  fine-tune 为一个解决方案验证器/评估器，以选择生成的候选解决方案集；（3）多任务顺序 fine-tuning，它将解决方案生成和评估任务集成起来，以提高 LLM 性能。通过这些方法，我们在 PaLM 2 模型上进行了一系列实验，并发现：（1）用于 fine-tuning 的步骤解决方案质量和风格可以对模型性能产生重要影响；（2）解决重新排名和多数投票都是可以提高模型性能的有效方法，但是它们可以同时使用以实现更大的性能提升；（3）将解决生成和评估任务分开并进行多任务顺序 fine-tuning 可以比基于解决 fine-tuning 的基eline提供更好的性能。根据这些发现，我们设计了一种 fine-tuning 配方，通过这种配方，我们在 MATH 数据集上使用 fine-tuned PaLM 2-L 模型，实现了 Approximately 58.8% 的准确率，与未经 fine-tuning 的 PaLM 2-L 模型的多shot 性能相比，提高了约 11.2%。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Study-of-Zero-Shot-NER-with-ChatGPT"><a href="#Empirical-Study-of-Zero-Shot-NER-with-ChatGPT" class="headerlink" title="Empirical Study of Zero-Shot NER with ChatGPT"></a>Empirical Study of Zero-Shot NER with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10035">http://arxiv.org/abs/2310.10035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emma1066/zero-shot-ner-with-chatgpt">https://github.com/emma1066/zero-shot-ner-with-chatgpt</a></li>
<li>paper_authors: Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, Hongwei Wang</li>
<li>for: 本研究探讨了大型自然语言模型（LLM）在零shot信息EXTRACTION任务中的表现，尤其是在ChatGPT和命名实体识别（NER）任务中。</li>
<li>methods: 我们采用了启发于LLM的卓越逻辑能力的方法，并对NER任务进行了修改和适应。我们提出了分解问题解决方案，将NER任务分解成更加简单的互相关联问题，并通过语法提高和工具增强等方法来促进模型的中间思考。此外，我们还采用了自身一致性来优化NER任务。</li>
<li>results: 我们的方法在七个benchmark上实现了零shotNER任务的很好表现，包括中文和英文 dataset，以及域特定和通用领域场景。此外，我们还进行了错误分析和优化建议。此外，我们还证明了我们的方法在几个shot设置和其他LLM中的效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domain-specific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在各种自然语言处理任务中表现出了强大的能力。本研究将关注LLM在零式信息提取任务中的表现，特别是关注ChatGPT和命名实体识别（NER）任务。受到LLM在符号逻辑和加算逻辑中的卓越逻辑能力的启发，我们采用了现有的逻辑方法，并对NER任务进行了修改和定制。首先，我们探索了一种分解问题解决方案，将NER任务分解成 simpler subproblems by labels。其次，我们提出了语法增强的方法，通过语法提示和工具增强来让模型在语法结构本身进行分析。此外，我们采用了自适应性来NER，提出了两个阶段多数投票策略，首先投票最符合的提及，然后投票最符合的类型。提出的方法在零式NER中实现了显著的提升，在七个benchmark上，包括中文和英文数据集，以及域特定和通用领域场景中。此外，我们还提供了错误类型的完整分析和优化方向。此外，我们还证明了提出的方法在几个ew-shot设定和其他LLMs中的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.CL_2023_10_16/" data-id="clpxp6bzi00dpee88fz1e0ipq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.LG_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T10:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.LG_2023_10_16/">cs.LG - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Learning-Framework-for-Cost-Effective-TCR-Epitope-Binding-Affinity-Prediction"><a href="#Active-Learning-Framework-for-Cost-Effective-TCR-Epitope-Binding-Affinity-Prediction" class="headerlink" title="Active Learning Framework for Cost-Effective TCR-Epitope Binding Affinity Prediction"></a>Active Learning Framework for Cost-Effective TCR-Epitope Binding Affinity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10893">http://arxiv.org/abs/2310.10893</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lee-cbg/activetcr">https://github.com/lee-cbg/activetcr</a></li>
<li>paper_authors: Pengfei Zhang, Seojin Bang, Heewook Lee</li>
<li>for: 这个研究的目的是为了提高TCR与复装序列之间的紧密互动，以便更好地预测TCR对复装序列的认知。</li>
<li>methods: 这个研究使用了活动学习和机器学习技术，将TCR与复装序列之间的紧密互动预测模型与训练数据集成一体。</li>
<li>results: 这个研究发现，使用活动学习可以大幅降低验证成本，并且可以提高预测模型的性能。此外，提供真实的TCR与复装序列对的标签可以帮助减少更多的重复数据，无需增加训练数据量。<details>
<summary>Abstract</summary>
T cell receptors (TCRs) are critical components of adaptive immune systems, responsible for responding to threats by recognizing epitope sequences presented on host cell surface. Computational prediction of binding affinity between TCRs and epitope sequences using machine/deep learning has attracted intense attention recently. However, its success is hindered by the lack of large collections of annotated TCR-epitope pairs. Annotating their binding affinity requires expensive and time-consuming wet-lab evaluation. To reduce annotation cost, we present ActiveTCR, a framework that incorporates active learning and TCR-epitope binding affinity prediction models. Starting with a small set of labeled training pairs, ActiveTCR iteratively searches for unlabeled TCR-epitope pairs that are ''worth'' for annotation. It aims to maximize performance gains while minimizing the cost of annotation. We compared four query strategies with a random sampling baseline and demonstrated that ActiveTCR reduces annotation costs by approximately 40%. Furthermore, we showed that providing ground truth labels of TCR-epitope pairs to query strategies can help identify and reduce more than 40% redundancy among already annotated pairs without compromising model performance, enabling users to train equally powerful prediction models with less training data. Our work is the first systematic investigation of data optimization for TCR-epitope binding affinity prediction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Calysto-Scheme-Project"><a href="#The-Calysto-Scheme-Project" class="headerlink" title="The Calysto Scheme Project"></a>The Calysto Scheme Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10886">http://arxiv.org/abs/2310.10886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calysto/calysto_scheme">https://github.com/calysto/calysto_scheme</a></li>
<li>paper_authors: Douglas S. Blank, James B. Marshall</li>
<li>for: 这篇论文主要是用来介绍一种名为Calysto Scheme的编程语言，以及它在Python基础上实现的一系列可行性和可读性优化。</li>
<li>methods: 这篇论文使用了Continuation-Passing Style和一系列正确性保持的编程变换来将Scheme语言转换成Python语言。它支持标准Scheme功能，包括call&#x2F;cc，以及一些扩展功能，如非探测运算符、自动回tracking和Python交互。</li>
<li>results: 这篇论文的研究结果表明，Calysto Scheme可以在教学和实际应用中使用，具有简单易用的特点和可以快速安装。它已经在Jupyter Notebook生态系统中集成，并在教学中使用了一些有趣和独特的措施。<details>
<summary>Abstract</summary>
Calysto Scheme is written in Scheme in Continuation-Passing Style, and converted through a series of correctness-preserving program transformations into Python. It has support for standard Scheme functionality, including call/cc, as well as syntactic extensions, a nondeterministic operator for automatic backtracking, and many extensions to allow Python interoperation. Because of its Python foundation, it can take advantage of modern Python libraries, including those for machine learning and other pedagogical contexts. Although Calysto Scheme was developed with educational purposes in mind, it has proven to be generally useful due to its simplicity and ease of installation. It has been integrated into the Jupyter Notebook ecosystem and used in the classroom to teach introductory Programming Languages with some interesting and unique twists.
</details>
<details>
<summary>摘要</summary>
加利斯托计划（Calysto Scheme）是一种使用Scheme语言编写的continuation-passing style编程语言，并通过一系列正确性保持的程序转换成Python语言。它支持标准Scheme功能，包括call/cc，以及一些语法扩展和不确定运算符，用于自动回tracking。由于其基于Python语言，因此可以利用现代Python库，包括机器学习和其他教学上的其他库。虽然加利斯托计划是为教育目的设计的，但由于其简单易用，因此在其他场景中也有广泛的应用。它已经与Jupyter Notebook生态系统集成，并在课堂上使用，以教授初级编程语言。
</details></li>
</ul>
<hr>
<h2 id="BLoad-Enhancing-Neural-Network-Training-with-Efficient-Sequential-Data-Handling"><a href="#BLoad-Enhancing-Neural-Network-Training-with-Efficient-Sequential-Data-Handling" class="headerlink" title="BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling"></a>BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10879">http://arxiv.org/abs/2310.10879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Ruschel, A. S. M. Iftekhar, B. S. Manjunath, Suya You</li>
<li>for: 提高现代深度学习模型的优化和扩展数据集的训练效率</li>
<li>methods: 提出了一种基于分布式数据并行训练的新训练方案，可以有效地处理不同长度的序列，无需添加过多的padding</li>
<li>results: 通过该方案，可以在训练时间和准确率两个方面提高表现，在实验中比较了100倍以上的减少padding量而不删除任何帧<details>
<summary>Abstract</summary>
The increasing complexity of modern deep neural network models and the expanding sizes of datasets necessitate the development of optimized and scalable training methods. In this white paper, we addressed the challenge of efficiently training neural network models using sequences of varying sizes. To address this challenge, we propose a novel training scheme that enables efficient distributed data-parallel training on sequences of different sizes with minimal overhead. By using this scheme we were able to reduce the padding amount by more than 100$x$ while not deleting a single frame, resulting in an overall increased performance on both training time and Recall in our experiments.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络模型的复杂度和数据集的规模在不断增长，因此需要开发优化和可扩展的训练方法。在这份白皮书中，我们解决了神经网络模型使用不同长度序列训练的挑战。我们提议一种新的训练方案，可以允许高效分布式数据并行训练，无需较大的过载。通过这种方法，我们可以在不删帧的情况下，将padding减少超过100倍，从而提高了训练时间和准确率的表现。
</details></li>
</ul>
<hr>
<h2 id="Eco-Driving-Control-of-Connected-and-Automated-Vehicles-using-Neural-Network-based-Rollout"><a href="#Eco-Driving-Control-of-Connected-and-Automated-Vehicles-using-Neural-Network-based-Rollout" class="headerlink" title="Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout"></a>Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10878">http://arxiv.org/abs/2310.10878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Paugh, Zhaoxuan Zhu, Shobhit Gupta, Marcello Canova, Stephanie Stockar</li>
<li>for: 提高connected和自主汽车的能源消耗效率，通过在行驶中使用 Vehicle-to-Everything信息优化车辆速度和动力系统。</li>
<li>methods: 使用层次多时间预报法，通过神经网络学习全路价值函数，并用来 aproximate终端成本在减少范围优化中。</li>
<li>results: 在实际道路上的模拟中，提议的方法可以与使用强化学习获得的 Stochastic 优化解决方案相当，而无需 слож的训练方法和内存占用。<details>
<summary>Abstract</summary>
Connected and autonomous vehicles have the potential to minimize energy consumption by optimizing the vehicle velocity and powertrain dynamics with Vehicle-to-Everything info en route. Existing deterministic and stochastic methods created to solve the eco-driving problem generally suffer from high computational and memory requirements, which makes online implementation challenging.   This work proposes a hierarchical multi-horizon optimization framework implemented via a neural network. The neural network learns a full-route value function to account for the variability in route information and is then used to approximate the terminal cost in a receding horizon optimization. Simulations over real-world routes demonstrate that the proposed approach achieves comparable performance to a stochastic optimization solution obtained via reinforcement learning, while requiring no sophisticated training paradigm and negligible on-board memory.
</details>
<details>
<summary>摘要</summary>
自适应和连接的汽车可以减少能源消耗，通过优化车辆速度和动力系统，基于车辆到所有事物（V2X）信息在路线上进行优化。现有的决策方法通常具有高计算和存储需求，在线实现具有挑战性。本工作提出了层次多 horizons 优化框架，通过神经网络学习全路价值函数，并用于缩小往返极限优化。通过实验示例，我们证明了提议的方法可以与基于强化学习的随机优化方法相当，而不需要复杂的训练方法和车辆上的快速存储。
</details></li>
</ul>
<hr>
<h2 id="Religious-Affiliation-in-the-Twenty-First-Century-A-Machine-Learning-Perspective-on-the-World-Value-Survey"><a href="#Religious-Affiliation-in-the-Twenty-First-Century-A-Machine-Learning-Perspective-on-the-World-Value-Survey" class="headerlink" title="Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey"></a>Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10874">http://arxiv.org/abs/2310.10874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaheh Jafarigol, William Keely, Tess Hartog, Tom Welborn, Peyman Hekmatpour, Theodore B. Trafalis</li>
<li>for: 这个研究是使用全球数据收集的世界价值调查数据进行量化分析，以研究社会中个体的宗教信仰、价值观和行为的变化趋势。</li>
<li>methods: 该研究使用随机森林算法来标识宗教性的关键因素，并使用重抽样技术来平衡数据并改善偏袋学习性能指标。</li>
<li>results: 变量重要性分析结果显示，年龄和收入在大多数国家中是最重要的变量，这与社会学基本理论中关于宗教和人类行为的概念有直接关系。<details>
<summary>Abstract</summary>
This paper is a quantitative analysis of the data collected globally by the World Value Survey. The data is used to study the trajectories of change in individuals' religious beliefs, values, and behaviors in societies. Utilizing random forest, we aim to identify the key factors of religiosity and classify respondents of the survey as religious and non religious using country level data. We use resampling techniques to balance the data and improve imbalanced learning performance metrics. The results of the variable importance analysis suggest that Age and Income are the most important variables in the majority of countries. The results are discussed with fundamental sociological theories regarding religion and human behavior. This study is an application of machine learning in identifying the underlying patterns in the data of 30 countries participating in the World Value Survey. The results from variable importance analysis and classification of imbalanced data provide valuable insights beneficial to theoreticians and researchers of social sciences.
</details>
<details>
<summary>摘要</summary>
这个论文是一项量化分析全球World Value Survey所收集的数据。数据用于研究社会中个体信仰、价值观和行为的变化轨迹。使用Random Forest算法，我们希望寻找 religiosity 的关键因素并使用国家级数据将响应者分类为宗教和非宗教。我们使用重样技术来协议数据，以改善偏袋学习表现指标。结果变量重要性分析表明，年龄和收入在大多数国家中是最重要的变量。结果与基本社会学理论相关于宗教和人类行为进行了讨论。这项研究是机器学习在World Value Survey数据中寻找下面的 patrón 的应用。变量重要性分析和响应者分类提供了有价值的发现，对社会科学研究人员有所帮助。
</details></li>
</ul>
<hr>
<h2 id="Joint-Optimization-of-Traffic-Signal-Control-and-Vehicle-Routing-in-Signalized-Road-Networks-using-Multi-Agent-Deep-Reinforcement-Learning"><a href="#Joint-Optimization-of-Traffic-Signal-Control-and-Vehicle-Routing-in-Signalized-Road-Networks-using-Multi-Agent-Deep-Reinforcement-Learning" class="headerlink" title="Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning"></a>Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10856">http://arxiv.org/abs/2310.10856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianyue Peng, Hang Gao, Gengyue Han, Hao Wang, Michael Zhang</li>
<li>for: 解决现代道路网络中的城市干道堵塞问题，提高交通效率。</li>
<li>methods: 提出了一种联合优化方法，通过同时控制信号时间和车辆路径选择，使用多智能拟似人类学习（MADRL）。</li>
<li>results: 经过数学实验表明，我们的策略可以在修改后的哈佛环境中提高交通效率，并且比只控制信号时间或车辆路径选择 alone 更高。<details>
<summary>Abstract</summary>
Urban traffic congestion is a critical predicament that plagues modern road networks. To alleviate this issue and enhance traffic efficiency, traffic signal control and vehicle routing have proven to be effective measures. In this paper, we propose a joint optimization approach for traffic signal control and vehicle routing in signalized road networks. The objective is to enhance network performance by simultaneously controlling signal timings and route choices using Multi-Agent Deep Reinforcement Learning (MADRL). Signal control agents (SAs) are employed to establish signal timings at intersections, whereas vehicle routing agents (RAs) are responsible for selecting vehicle routes. By establishing relevance between agents and enabling them to share observations and rewards, interaction and cooperation among agents are fostered, which enhances individual training. The Multi-Agent Advantage Actor-Critic algorithm is used to handle multi-agent environments, and Deep Neural Network (DNN) structures are designed to facilitate the algorithm's convergence. Notably, our work is the first to utilize MADRL in determining the optimal joint policy for signal control and vehicle routing. Numerical experiments conducted on the modified Sioux network demonstrate that our integration of signal control and vehicle routing outperforms controlling signal timings or vehicles' routes alone in enhancing traffic efficiency.
</details>
<details>
<summary>摘要</summary>
现代城市交通堵塞是一个严重的问题，对现代道路网络造成了很大的影响。为了解决这个问题并提高交通效率，交通信号控制和车辆 routing 已经被证明是有效的解决方案。在这篇论文中，我们提出了一种联合优化方法，将交通信号控制和车辆 routing 联合优化。我们使用多 Agent Deep Reinforcement Learning（MADRL）来同时控制信号时间和车辆路径选择。信号控制代理（SAs）负责在交叉口设置信号时间，而车辆路径选择代理（RAs）负责选择车辆路径。通过在代理之间建立相关性，并让代理们共享观察和奖励，这会促进代理之间的交互和合作，从而提高每个代理的训练效果。我们使用多 Agent Advantage Actor-Critic算法来处理多 Agent 环境，并使用深度神经网络（DNN）结构来促进算法的收敛。值得注意的是，我们的工作是首次在确定合适的共同策略方面利用 MADRL。我们在修改过 Sioux 网络进行数值实验，结果显示，我们将信号控制和车辆 routing 联合优化的策略与单独控制信号时间或车辆路径的策略相比，在提高交通效率方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Classification-by-Density-Estimation-Using-Gaussian-Mixture-Model-and-Masked-Autoregressive-Flow"><a href="#Probabilistic-Classification-by-Density-Estimation-Using-Gaussian-Mixture-Model-and-Masked-Autoregressive-Flow" class="headerlink" title="Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow"></a>Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10843">http://arxiv.org/abs/2310.10843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bghojogh/density-based-classifiers">https://github.com/bghojogh/density-based-classifiers</a></li>
<li>paper_authors: Benyamin Ghojogh, Milad Amir Toutounchian</li>
<li>for: 这篇论文主要用于提出一种基于密度估计的分类方法，而密度估计通常用于数据分布估计而不是分类。</li>
<li>methods: 该论文使用了两种密度估计方法： Gaussian Mixture Model (GMM) 和 Masked Autoregressive Flow (MAF)。GMM 是一种预测最大化的密度估计方法，而 MAF 则是一种基于普通化流和自适应网络的生成模型。</li>
<li>results: 该论文的实验结果显示，使用 GMM 和 MAF 进行密度估计可以超过简单的线性分类器，如线性激发分析。此外，该论文还开启了研究者们可以根据密度估计来提出其他可能的概率分类器的研究方向。<details>
<summary>Abstract</summary>
Density estimation, which estimates the distribution of data, is an important category of probabilistic machine learning. A family of density estimators is mixture models, such as Gaussian Mixture Model (GMM) by expectation maximization. Another family of density estimators is the generative models which generate data from input latent variables. One of the generative models is the Masked Autoregressive Flow (MAF) which makes use of normalizing flows and autoregressive networks. In this paper, we use the density estimators for classification, although they are often used for estimating the distribution of data. We model the likelihood of classes of data by density estimation, specifically using GMM and MAF. The proposed classifiers outperform simpler classifiers such as linear discriminant analysis which model the likelihood using only a single Gaussian distribution. This work opens the research door for proposing other probabilistic classifiers based on joint density estimation.
</details>
<details>
<summary>摘要</summary>
density 估计是机器学习中一种重要的分布估计类别。一个家族的density 估计器是混合模型，如 Gaussian Mixture Model (GMM) 使用期望最大化。另一个家族的density 估计器是生成模型，它们可以将输入的latent variable generate 为数据。本文使用density 估计器进行分类，尽管它们通常用于估计数据的分布。我们使用GMM和MAF来模型数据的类别概率。提议的分类器比 simpler 分类器，如线性混合分析，which 模型只使用单个 Gaussian 分布来模型类别概率。这项工作打开了研究机会，提出其他基于共同分布估计的概率分类器的建议。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-based-Algorithm-for-Automated-Detection-of-Frequency-based-Events-in-Recorded-Time-Series-of-Sensor-Data"><a href="#A-Machine-Learning-based-Algorithm-for-Automated-Detection-of-Frequency-based-Events-in-Recorded-Time-Series-of-Sensor-Data" class="headerlink" title="A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data"></a>A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10841">http://arxiv.org/abs/2310.10841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahareh Medghalchi, Andreas Vogel</li>
<li>for: 本研究旨在提出一种新的自动事件探测方法，用于检测时间序列数据中的事件。</li>
<li>methods: 该方法首先将时间序列数据映射到时间频域的表示中，然后对表示进行筛选和对象检测模型的训练，以检测期望的事件对象在表示中。</li>
<li>results: 该方法在未seen的数据集上测试，准确率为0.97，能够准确地检测事件的时间间隔，提高了自动事件检测的精度和可靠性。<details>
<summary>Abstract</summary>
Automated event detection has emerged as one of the fundamental practices to monitor the behavior of technical systems by means of sensor data. In the automotive industry, these methods are in high demand for tracing events in time series data. For assessing the active vehicle safety systems, a diverse range of driving scenarios is conducted. These scenarios involve the recording of the vehicle's behavior using external sensors, enabling the evaluation of operational performance. In such setting, automated detection methods not only accelerate but also standardize and objectify the evaluation by avoiding subjective, human-based appraisals in the data inspection. This work proposes a novel event detection method that allows to identify frequency-based events in time series data. To this aim, the time series data is mapped to representations in the time-frequency domain, known as scalograms. After filtering scalograms to enhance relevant parts of the signal, an object detection model is trained to detect the desired event objects in the scalograms. For the analysis of unseen time series data, events can be detected in their scalograms with the trained object detection model and are thereafter mapped back to the time series data to mark the corresponding time interval. The algorithm, evaluated on unseen datasets, achieves a precision rate of 0.97 in event detection, providing sharp time interval boundaries whose accurate indication by human visual inspection is challenging. Incorporating this method into the vehicle development process enhances the accuracy and reliability of event detection, which holds major importance for rapid testing analysis.
</details>
<details>
<summary>摘要</summary>
自动化事件检测已经成为监测技术系统行为的基本实践，通过感知器数据。在汽车业，这些方法受到时间序列数据跟踪事件的高需求。为评估活动汽车安全系统，进行了多种驾驶场景测试。这些场景包括记录汽车的行为使用外部感知器，以便评估运作性能。在这种设置下，自动检测方法不仅加速了，还标准化和对象化了评估，通过避免人类基于数据检查的主观评估。本研究提出了一种新的事件检测方法，可以在时间序列数据中检测频率基于事件。为此，将时间序列数据映射到时间频率域的表示，称为scalogram。然后，对scalogram进行过滤，以增强有关信号的部分。然后，将对象检测模型训练以检测想要的事件对象在scalogram中。对于未看过的时间序列数据分析，可以使用训练好的对象检测模型在scalogram中检测事件，并将其映射回时间序列数据，以标识相应的时间间隔。这种算法，在未经过测试的数据上进行评估，具有0.97的准确率，提供了准确的时间间隔边界，人工视觉检查具有挑战性。通过将这种方法 integrate到汽车开发过程中，提高了事件检测的准确性和可靠性，这些特性对于快速分析具有重要性。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Two-Layer-Feedforward-Networks-for-Efficient-Transformers"><a href="#Approximating-Two-Layer-Feedforward-Networks-for-Efficient-Transformers" class="headerlink" title="Approximating Two-Layer Feedforward Networks for Efficient Transformers"></a>Approximating Two-Layer Feedforward Networks for Efficient Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10837">http://arxiv.org/abs/2310.10837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robertcsordas/moe">https://github.com/robertcsordas/moe</a></li>
<li>paper_authors: Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber</li>
<li>for: 降低神经网络（NN）计算和内存需求，而不是牺牲性能。</li>
<li>methods: 使用稀疏混合专家（MoE）建立资源高效的大语言模型（LM）。</li>
<li>results: 在 WikiText-103 和 enwiki8 数据集上，与 dense Transformer-XL 相比，我们的 MoE 在两个不同的尺度上具有相当的竞争力，而且具有许多资源的灵活性。<details>
<summary>Abstract</summary>
How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.
</details>
<details>
<summary>摘要</summary>
如何减少神经网络（NN）的计算和内存需求而不 sacrificing性能？许多latest works使用稀疏混合专家（MoE）来建立资源高效的大语言模型（LM）。我们介绍了MoE的一些新的视角，提出了一个通用的框架，可以 aproximate two-layer NNs（例如Transformers的Feedforward块），包括产品密钥记忆（PKM）。通过这个框架的洞察，我们提出了MoE和PKM的改进方法。与先前的工作不同，我们的评估条件是参数平等（parameter-equal），这是评估LMs的关键。我们的MoE在WikiText-103和enwiki8 dataset上与 dense Transformer-XL 在两个不同的 scales 上具有相似的竞争力，而且具有许多资源的约束。这表明MoE不仅适用于极大规模的LM，还适用于任何规模的资源高效LM。我们的代码public。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-processes-based-data-augmentation-and-expected-signature-for-time-series-classification"><a href="#Gaussian-processes-based-data-augmentation-and-expected-signature-for-time-series-classification" class="headerlink" title="Gaussian processes based data augmentation and expected signature for time series classification"></a>Gaussian processes based data augmentation and expected signature for time series classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10836">http://arxiv.org/abs/2310.10836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Romito, Francesco Triggiano</li>
<li>for: 这篇论文旨在提出一种基于预期签名的时间序列特征提取模型，用于预测时间序列的统计特征。</li>
<li>methods: 该模型使用 Gaussian processes 数据增强来计算预期签名，并通过一个监督任务来学习最佳的特征提取。</li>
<li>results: 模型可以学习出优化的特征提取，并且可以用于预测时间序列的统计特征。<details>
<summary>Abstract</summary>
The signature is a fundamental object that describes paths (that is, continuous functions from an interval to a Euclidean space). Likewise, the expected signature provides a statistical description of the law of stochastic processes. We propose a feature extraction model for time series built upon the expected signature. This is computed through a Gaussian processes based data augmentation. One of the main features is that an optimal feature extraction is learnt through the supervised task that uses the model.
</details>
<details>
<summary>摘要</summary>
《签名》是一个基本对象，描述了路径（即连续函数从一个时间interval到一个欧几里得空间）。类似地，预期签名提供了一个统计描述，描述了随机过程的法律。我们提议一种基于预期签名的特征提取模型，用于时间序列。这个模型通过基于 Gaussian 过程的数据增强来计算预期签名。其中一个主要特点是通过一个监督任务来学习优化的特征提取。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Data-Driven-Surrogates-of-Dynamical-Systems-for-Forward-Propagation-of-Uncertainty"><a href="#Accurate-Data-Driven-Surrogates-of-Dynamical-Systems-for-Forward-Propagation-of-Uncertainty" class="headerlink" title="Accurate Data-Driven Surrogates of Dynamical Systems for Forward Propagation of Uncertainty"></a>Accurate Data-Driven Surrogates of Dynamical Systems for Forward Propagation of Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10831">http://arxiv.org/abs/2310.10831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saibal De, Reese E. Jones, Hemanth Kolla</li>
<li>for: 这篇研究的目的是为了提出一种新的非侵入式方法来建立模型uncertainty quantification中的代理模型。</li>
<li>methods: 这篇研究使用了Stochastic collocation（SC）方法，并与Data-driven sparse identification of nonlinear dynamics（SINDy）框架结合，以建立动态模型的代理模型。</li>
<li>results: 研究发现，使用SC-over-dynamics框架可以降低错误，包括系统轨道的描述和模型状态分布的描述。三个测试问题中的两个问题（一个ordinary differential equation和一个partial differential equation）的数据表明，这种方法可以提供更好的结果。<details>
<summary>Abstract</summary>
Stochastic collocation (SC) is a well-known non-intrusive method of constructing surrogate models for uncertainty quantification. In dynamical systems, SC is especially suited for full-field uncertainty propagation that characterizes the distributions of the high-dimensional primary solution fields of a model with stochastic input parameters. However, due to the highly nonlinear nature of the parameter-to-solution map in even the simplest dynamical systems, the constructed SC surrogates are often inaccurate. This work presents an alternative approach, where we apply the SC approximation over the dynamics of the model, rather than the solution. By combining the data-driven sparse identification of nonlinear dynamics (SINDy) framework with SC, we construct dynamics surrogates and integrate them through time to construct the surrogate solutions. We demonstrate that the SC-over-dynamics framework leads to smaller errors, both in terms of the approximated system trajectories as well as the model state distributions, when compared against full-field SC applied to the solutions directly. We present numerical evidence of this improvement using three test problems: a chaotic ordinary differential equation, and two partial differential equations from solid mechanics.
</details>
<details>
<summary>摘要</summary>
这项工作提出了一种alternative方法，在这里我们通过SC 近似方法来 Approximate the dynamics of the model, rather than the solution。通过将数据驱动的稀疏标识非线性动力（SINDy）框架与SC 结合，我们构建了动力准模型，并将其通过时间集成以构建准确的解决方案。我们发现，使用SC  над动力框架，相比拟合解决方案直接使用SC，能够减少误差，包括系统轨迹的近似值以及模型状态分布的误差。我们通过三个测试问题来提供数字证据：一个混沌的常微分方程，以及两个固体力学中的部分偏微分方程。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-transfer-across-tasks-using-hybrid-model-based-successor-feature-reinforcement-learning"><a href="#Uncertainty-aware-transfer-across-tasks-using-hybrid-model-based-successor-feature-reinforcement-learning" class="headerlink" title="Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning"></a>Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10818">http://arxiv.org/abs/2310.10818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parvin Malekzadeh, Ming Hou, Konstantinos N. Plataniotis</li>
<li>For: 该论文主要针对复杂大规模决策问题的实用强化学习（RL）问题进行研究，旨在提高样本效率。* Methods: 该论文提出了一种将模型基于（MB）方法与继承特征（SF）算法结合的方法，以及一种基于卡尔曼滤波器（KF）的多模型适应估计来实现不确定性感知探索。* Results: 该论文的实验结果显示，该算法可以在不同的转移动力学中转移知识，对下游任务进行有效的探索和学习，并在样本量方面比起现有基elines要少得多。<details>
<summary>Abstract</summary>
Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this paper, the uncertainty of the value of each action is approximated by a Kalman filter (KF)-based multiple-model adaptive estimation. This KF-based framework treats the parameters of a model as random variables. To the best of our knowledge, this is the first attempt at formulating a hybrid MB-SF algorithm capable of generalizing knowledge across large or continuous state space tasks with various transition dynamics while requiring less computation at decision time than MB methods. The number of samples required to learn the tasks was compared to recent SF and MB baselines. The results show that our algorithm generalizes its knowledge across different transition dynamics, learns downstream tasks with significantly fewer samples than starting from scratch, and outperforms existing approaches.
</details>
<details>
<summary>摘要</summary>
sample efficiency是RL中央的一个重要问题，它可以提高RL的实用性和可扩展性。在不同的任务中，通过转移和总结之前的经验可以提高sample efficiency。现有研究表明，Successor Feature（SF）RL算法可以在不同的奖励下产生相同的转移动力学中转移知识。此外，uncertainty-aware探索被广泛认为是一个有appeal的方法，可以提高sample efficiency。将两个想法MB-SF和uncertainty结合起来，可以解决在不同的转移动力学或奖励函数下的样本效率不稳定问题。在这篇论文中，我们使用Kalman缓冲（KF）基于多模型适应的方法来估计每个行为的值的不确定性。这是我们知道的第一个能够在大或连续状态空间任务上广泛适用的hybrid MB-SF算法。我们对SF和MB基线进行比较，并发现我们的算法可以在不同的转移动力学下转移知识，在不同的任务上学习要少样本更多，并且超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Multi-Agent-Reinforcement-Learning-via-Adversarial-Regularization-Theoretical-Foundation-and-Stable-Algorithms"><a href="#Robust-Multi-Agent-Reinforcement-Learning-via-Adversarial-Regularization-Theoretical-Foundation-and-Stable-Algorithms" class="headerlink" title="Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms"></a>Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10810">http://arxiv.org/abs/2310.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abukharin3/ernie">https://github.com/abukharin3/ernie</a></li>
<li>paper_authors: Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, Zhehui Chen, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao</li>
<li>For: The paper aims to improve the robustness of multi-agent reinforcement learning (MARL) policies by controlling the Lipschitz constant of the policies and using adversarial regularization to promote continuity with respect to state observations and actions.* Methods: The proposed framework, called ERNIE, uses adversarial regularization to promote the Lipschitz continuity of policies, and the authors reformulate adversarial regularization as a Stackelberg game to reduce training instability.* Results: The authors demonstrate the effectiveness of the proposed framework with extensive experiments in traffic light control and particle environments, and show that the ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. Additionally, the authors extend ERNIE to mean-field MARL with a formulation based on distributionally robust optimization that outperforms its non-robust counterpart and is of independent interest.<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To reduce this instability, we reformulate adversarial regularization as a Stackelberg game. We demonstrate the effectiveness of the proposed framework with extensive experiments in traffic light control and particle environments. In addition, we extend ERNIE to mean-field MARL with a formulation based on distributionally robust optimization that outperforms its non-robust counterpart and is of independent interest. Our code is available at https://github.com/abukharin3/ERNIE.
</details>
<details>
<summary>摘要</summary>
多智能体学习（MARL）已经在多个领域展示了承诺的成绩。然而，MARL策略通常缺乏鲁棒性，因此容易受到环境小变化的影响。这对实际世界中部署MARL算法提出了严重的问题，因为测试环境可能与训练环境有所不同。在这种情况下，我们展示了通过控制策略的 lipschitz常量来增加鲁棒性的可能性，并在某些条件下证明存在 lipschitz和准确策略的存在。基于这些发现，我们提出了一个新的鲁棒MARL框架，称为ERNIE，它通过对策略的状态观测和行动进行对抗规范化来提高策略的鲁棒性。ERNIE框架可以抵抗噪声观测、变化的转移动力和代理者的恶意行动。然而，ERNIE的对抗规范化可能会导致训练不稳定。为了减少这种不稳定，我们将对抗规范化转换为一个Stackelberg游戏。我们通过广泛的实验证明了提议的框架的效果，包括交通灯控制和粒子环境。此外，我们将ERNIE扩展到了 Mean-field MARL，使其超越其不鲁棒的对比，并且这种扩展是独立有价值的。我们的代码可以在https://github.com/abukharin3/ERNIE上获取。
</details></li>
</ul>
<hr>
<h2 id="Regularization-properties-of-adversarially-trained-linear-regression"><a href="#Regularization-properties-of-adversarially-trained-linear-regression" class="headerlink" title="Regularization properties of adversarially-trained linear regression"></a>Regularization properties of adversarially-trained linear regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10807">http://arxiv.org/abs/2310.10807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonior92/advtrain-linreg">https://github.com/antonior92/advtrain-linreg</a></li>
<li>paper_authors: Antônio H. Ribeiro, Dave Zachariah, Francis Bach, Thomas B. Schön</li>
<li>For: The paper is focused on studying the effectiveness of adversarial training in defending against input perturbations in linear models, and exploring the relationship between adversarial training and other regularization methods.* Methods: The paper uses a min-max formulation of adversarial training to search for the best solution when the training data are corrupted by worst-case attacks. The authors also compare the solution of adversarial training with other regularization methods, such as ridge regression and Lasso.* Results: The main findings of the paper include that adversarial training yields the minimum-norm interpolating solution in the overparameterized regime, and that adversarial training can be equivalent to parameter shrinking methods in the underparametrized region. Additionally, the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. The authors confirm their theoretical findings with numerical examples.<details>
<summary>Abstract</summary>
State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training -- as in square-root Lasso -- the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.
</details>
<details>
<summary>摘要</summary>
现代机器学习模型可能受到非常小的输入干扰，这些干扰是利用攻击性的构造的。对抗攻击是一种有效的防御策略。作为一个最小化问题，它搜索最佳解决方案，当训练数据被攻击最坏的攻击时。线性模型是我们研究的简单模型，在这种情况下，对抗训练转化为一个凸优化问题，可以通过最小化一个有限和来解决。我们提供了对抗训练在线性回归和其他规化方法之间的比较分析。我们的主要发现是：(A) 对抗训练在过参数化 régime（更多参数 чем数据）下给出最小欧式 interpolator，只要攻击干扰半径小于一个阈值。而且，对抗训练的解决方案与最小欧式 interpolator相同。(B) 对抗训练可以与参数缩放方法（ridge regression和lasso）等同。这发生在下参数化 régime，对应的攻击干扰半径和零均匀分布的covariate是合适的。(C) 对 $\ell_\infty$-对抗训练（如平方lasso）中选择的攻击干扰半径对最佳 bound 没有依赖于加法噪声Variance。我们通过数学示例证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Neural-Tangent-Kernels-Motivate-Graph-Neural-Networks-with-Cross-Covariance-Graphs"><a href="#Neural-Tangent-Kernels-Motivate-Graph-Neural-Networks-with-Cross-Covariance-Graphs" class="headerlink" title="Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs"></a>Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10791">http://arxiv.org/abs/2310.10791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shervin Khalafi, Saurabh Sihag, Alejandro Ribeiro</li>
<li>for: 这篇论文探讨了用神经网络学习推论和泛化行为，特别是在图 нейрон网络（GNN）中。</li>
<li>methods: 该论文使用了神经 tangent 函数（NTK）来分析神经网络的学习和泛化行为。</li>
<li>results: 研究发现，在 GNN 中，优化对齐关系（alignment）可以优化图表示或图变换函数，并且有理论保证对齐是最佳的。实验结果表明，使用 cross-covariance 作为图 shift 函数可以超过只使用输入数据 covariance matrix 的 GNN。<details>
<summary>Abstract</summary>
Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK kernel and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept, we investigate NTKs and alignment in the context of graph neural networks (GNNs), where our analysis reveals that optimizing alignment translates to optimizing the graph representation or the graph shift operator in a GNN. Our results further establish the theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experiments focused on a multi-variate time series prediction task for a publicly available dataset. Specifically, they demonstrate that GNNs with cross-covariance as the graph shift operator indeed outperform those that operate on the covariance matrix from only the input data.
</details>
<details>
<summary>摘要</summary>
neural tangent kernels (NTKs) 提供了一个理论 régime 来分析过 parametrized  нейрон网络在学习和泛化行为的条件。在一个监督学习任务中，NTK kernel 的 eigenvectors 和资料之间的关联（一个称为“alignment”的概念）可以控制梯度下降的速度，以及对未见到的资料的泛化。在这篇文章中，我们对 GNN （图形神经网络）中的 NTK 和配置进行了研究，我们的分析表明，对 GNN 进行配置的最佳化将导致图形表现或图形移动操作的最佳化。我们的结果还证明了在 GNN 中对 alignment 的最佳化具有理论保证，这些保证是由图形移动操作的cross-covariance 和输入资料的covariance matrix 所决定。我们的实验还证明了 GNN 使用 cross-covariance 作为图形移动操作实际上会比使用仅从输入资料的covariance matrix 来操作更好。
</details></li>
</ul>
<hr>
<h2 id="Correcting-model-misspecification-in-physics-informed-neural-networks-PINNs"><a href="#Correcting-model-misspecification-in-physics-informed-neural-networks-PINNs" class="headerlink" title="Correcting model misspecification in physics-informed neural networks (PINNs)"></a>Correcting model misspecification in physics-informed neural networks (PINNs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10776">http://arxiv.org/abs/2310.10776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongren Zou, Xuhui Meng, George Em Karniadakis<br>methods: 这 paper 使用的方法包括 physics-informed neural networks (PINNs) 和其他深度神经网络 (DNNs)。results: 这 paper 的结果表明，通过使用 DNNs 来模型不完全的物理模型，可以将计算错误减少，并且可以使 PINNs 在复杂系统中应用。此外，这 paper 还使用 Bayesian PINNs (B-PINNs) 和&#x2F;或 ensemble PINNs 来评估不确定性。<details>
<summary>Abstract</summary>
Data-driven discovery of governing equations in computational science has emerged as a new paradigm for obtaining accurate physical models and as a possible alternative to theoretical derivations. The recently developed physics-informed neural networks (PINNs) have also been employed to learn governing equations given data across diverse scientific disciplines. Despite the effectiveness of PINNs for discovering governing equations, the physical models encoded in PINNs may be misspecified in complex systems as some of the physical processes may not be fully understood, leading to the poor accuracy of PINN predictions. In this work, we present a general approach to correct the misspecified physical models in PINNs for discovering governing equations, given some sparse and/or noisy data. Specifically, we first encode the assumed physical models, which may be misspecified, then employ other deep neural networks (DNNs) to model the discrepancy between the imperfect models and the observational data. Due to the expressivity of DNNs, the proposed method is capable of reducing the computational errors caused by the model misspecification and thus enables the applications of PINNs in complex systems where the physical processes are not exactly known. Furthermore, we utilize the Bayesian PINNs (B-PINNs) and/or ensemble PINNs to quantify uncertainties arising from noisy and/or gappy data in the discovered governing equations. A series of numerical examples including non-Newtonian channel and cavity flows demonstrate that the added DNNs are capable of correcting the model misspecification in PINNs and thus reduce the discrepancy between the physical models and the observational data. We envision that the proposed approach will extend the applications of PINNs for discovering governing equations in problems where the physico-chemical or biological processes are not well understood.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用数据驱动的方法发现计算机科学中的管理方程式已经成为一种新的方法，以获取准确的物理模型，也可能是理论 derivations的可能的替代方法。最近开发的物理学泛化神经网络（PINNs）已经在多个科学领域中使用来学习管理方程式。 despite the effectiveness of PINNs for discovering governing equations, the physical models encoded in PINNs may be misspecified in complex systems as some of the physical processes may not be fully understood, leading to poor accuracy of PINN predictions. In this work, we present a general approach to correct the misspecified physical models in PINNs for discovering governing equations, given some sparse and/or noisy data. Specifically, we first encode the assumed physical models, which may be misspecified, then employ other deep neural networks (DNNs) to model the discrepancy between the imperfect models and the observational data. Due to the expressivity of DNNs, the proposed method is capable of reducing the computational errors caused by the model misspecification and thus enables the applications of PINNs in complex systems where the physical processes are not exactly known. Furthermore, we utilize the Bayesian PINNs (B-PINNs) and/or ensemble PINNs to quantify uncertainties arising from noisy and/or gappy data in the discovered governing equations. A series of numerical examples including non-Newtonian channel and cavity flows demonstrate that the added DNNs are capable of correcting the model misspecification in PINNs and thus reduce the discrepancy between the physical models and the observational data. We envision that the proposed approach will extend the applications of PINNs for discovering governing equations in problems where the physico-chemical or biological processes are not well understood.
</details></li>
</ul>
<hr>
<h2 id="Gotta-be-SAFE-A-New-Framework-for-Molecular-Design"><a href="#Gotta-be-SAFE-A-New-Framework-for-Molecular-Design" class="headerlink" title="Gotta be SAFE: A New Framework for Molecular Design"></a>Gotta be SAFE: A New Framework for Molecular Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10773">http://arxiv.org/abs/2310.10773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Noutahi, Cristian Gabellini, Michael Craig, Jonathan S. C Lim, Prudencio Tossou</li>
<li>for: 用于AI驱动的分子设计</li>
<li>methods: 使用Sequential Attachment-based Fragment Embedding（SAFE），一种新的线notation для化学结构</li>
<li>results: 通过训练一个87亿参数的GPT2-like模型，实现了多种优秀的优化性能，打开了新的化学空间探索的可能性，对AI驱动的分子设计具有广泛的应用前景<details>
<summary>Abstract</summary>
Traditional molecular string representations, such as SMILES, often pose challenges for AI-driven molecular design due to their non-sequential depiction of molecular substructures. To address this issue, we introduce Sequential Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical structures. SAFE reimagines SMILES strings as an unordered sequence of interconnected fragment blocks while maintaining full compatibility with existing SMILES parsers. It streamlines complex generative tasks, including scaffold decoration, fragment linking, polymer generation, and scaffold hopping, while facilitating autoregressive generation for fragment-constrained design, thereby eliminating the need for intricate decoding or graph-based models. We demonstrate the effectiveness of SAFE by training an 87-million-parameter GPT2-like model on a dataset containing 1.1 billion SAFE representations. Through extensive experimentation, we show that our SAFE-GPT model exhibits versatile and robust optimization performance. SAFE opens up new avenues for the rapid exploration of chemical space under various constraints, promising breakthroughs in AI-driven molecular design.
</details>
<details>
<summary>摘要</summary>
传统分子串表示方式，如SMILES，经常对AI驱动分子设计带来挑战，因为它们不能正确地表示分子子结构的顺序。为解决这个问题，我们介绍Sequential Attachment-based Fragment Embedding（SAFE），一种新的化学结构表示方式。SAFE将SMILES字符串重新表示为一个无序的连接式块序列，同时保持与现有SMILES解析器的兼容性。这种方法可以简化复杂的生成任务，如架构饰 ornamentation、分子连接、聚合物生成和架构跳跃，并且可以支持束缚生成，从而消除需要复杂的解码或图形模型。我们在一个8700万参数的GPT2-like模型上训练了SAFE模型，并通过广泛的实验表明了我们的SAFE-GPT模型在不同的约束下进行优化表现灵活和强大。SAFE打开了新的化学空间探索的可能性，承诺AI驱动分子设计中的突破。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Lead-Sheet-Generation-via-Semantic-Compression"><a href="#Unsupervised-Lead-Sheet-Generation-via-Semantic-Compression" class="headerlink" title="Unsupervised Lead Sheet Generation via Semantic Compression"></a>Unsupervised Lead Sheet Generation via Semantic Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10772">http://arxiv.org/abs/2310.10772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zacharynovack/lead-ae">https://github.com/zacharynovack/lead-ae</a></li>
<li>paper_authors: Zachary Novack, Nikita Srivatsan, Taylor Berg-Kirkpatrick, Julian McAuley</li>
<li>for: 本研究旨在提高生成音乐的效率和质量，通过生成与原始乐谱版本相对应的简化后的乐谱（lead sheet）。</li>
<li>methods: 我们提出了一种新的模型 called Lead-AE，它使用可控的局部稀缺约束来模型乐谱，并使用可导的top-k算法来实现简化后的乐谱可控。</li>
<li>results: 我们的方法比已有的决定性基线要好，可以生成具有准确性和完整性的简化后乐谱，并且在人工评价中也得到了良好的评价。<details>
<summary>Abstract</summary>
Lead sheets have become commonplace in generative music research, being used as an initial compressed representation for downstream tasks like multitrack music generation and automatic arrangement. Despite this, researchers have often fallen back on deterministic reduction methods (such as the skyline algorithm) to generate lead sheets when seeking paired lead sheets and full scores, with little attention being paid toward the quality of the lead sheets themselves and how they accurately reflect their orchestrated counterparts. To address these issues, we propose the problem of conditional lead sheet generation (i.e. generating a lead sheet given its full score version), and show that this task can be formulated as an unsupervised music compression task, where the lead sheet represents a compressed latent version of the score. We introduce a novel model, called Lead-AE, that models the lead sheets as a discrete subselection of the original sequence, using a differentiable top-k operator to allow for controllable local sparsity constraints. Across both automatic proxy tasks and direct human evaluations, we find that our method improves upon the established deterministic baseline and produces coherent reductions of large multitrack scores.
</details>
<details>
<summary>摘要</summary>
乐谱简纸在生成音乐研究中变得普遍，用作下游任务的初始压缩表示，如多轨音乐生成和自动编排。尽管如此，研究人员通常会回归到决定性减少方法（如天空线算法）来生成乐谱简纸，未受到乐谱简纸本身质量和如何准确反映其管弦乐谱的关注。为解决这些问题，我们提出了 conditional lead sheet generation 问题（即根据全音乐谱版本生成乐谱简纸），并证明这可以视为一种无监督的音乐压缩任务，其中乐谱简纸表示一个压缩的 latent 序列。我们提出了一种新的模型， называ为 Lead-AE，该模型将乐谱简纸视为原始序列的一个离散子选择，使用可微 differentiable top-k 算子来实现可控的地方缺失约束。在自动代理任务和直接人类评估中，我们发现我们的方法比已有的决定性基线更好，并生成了大量多轨音乐谱的准确压缩。
</details></li>
</ul>
<hr>
<h2 id="Wide-Neural-Networks-as-Gaussian-Processes-Lessons-from-Deep-Equilibrium-Models"><a href="#Wide-Neural-Networks-as-Gaussian-Processes-Lessons-from-Deep-Equilibrium-Models" class="headerlink" title="Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models"></a>Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10767">http://arxiv.org/abs/2310.10767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiang Gao, Xiaokai Huo, Hailiang Liu, Hongyang Gao</li>
<li>for: 本研究探讨了深度平衡模型（DEQ），即无限层 neural network 的普适性和训练特性。</li>
<li>methods: 本研究使用了 neural ordinary differential equations（ODEs）和 deep equilibrium models（DEQs）来探讨深度平衡模型的性质。</li>
<li>results: 研究发现，当 DEQ 层宽度趋于无穷时，它会 converge to a Gaussian process，并且这种整合在深度和宽度之间进行交换时不会出现。此外，研究还发现，相关的 Gaussian vector 在任意两个不同输入数据对之间保持非零最小特征值，这使得 NNGP kernel 具有稳定性。这些发现对 DEQ 的训练和泛化做出了基础性的研究。<details>
<summary>Abstract</summary>
Neural networks with wide layers have attracted significant attention due to their equivalence to Gaussian processes, enabling perfect fitting of training data while maintaining generalization performance, known as benign overfitting. However, existing results mainly focus on shallow or finite-depth networks, necessitating a comprehensive analysis of wide neural networks with infinite-depth layers, such as neural ordinary differential equations (ODEs) and deep equilibrium models (DEQs). In this paper, we specifically investigate the deep equilibrium model (DEQ), an infinite-depth neural network with shared weight matrices across layers. Our analysis reveals that as the width of DEQ layers approaches infinity, it converges to a Gaussian process, establishing what is known as the Neural Network and Gaussian Process (NNGP) correspondence. Remarkably, this convergence holds even when the limits of depth and width are interchanged, which is not observed in typical infinite-depth Multilayer Perceptron (MLP) networks. Furthermore, we demonstrate that the associated Gaussian vector remains non-degenerate for any pairwise distinct input data, ensuring a strictly positive smallest eigenvalue of the corresponding kernel matrix using the NNGP kernel. These findings serve as fundamental elements for studying the training and generalization of DEQs, laying the groundwork for future research in this area.
</details>
<details>
<summary>摘要</summary>
神经网络with宽层有吸引了广泛关注，因为它们相当于 Gaussian 过程，可以完美适应训练数据而且保持泛化性能，称为恰好的过拟合。然而，现有的研究主要集中在浅或固定深度的神经网络上，需要进行广泛的抽象深度神经网络的研究，如神经ordinary differential equations（ODEs）和deep equilibrium models（DEQs）。在这篇论文中，我们专门研究深度平衡模型（DEQ），这是一个无限深度神经网络，具有共享权重矩阵的层。我们的分析表明，当 DEQ 层的宽度接近无穷大时，它会 converge to a Gaussian process，确立了称为神经网络和Gaussian过程（NNGP）匹配。很Remarkably，这种convergence 随着深度和宽度的限制的交换，不同于 typical 无限深度多层感知（MLP）网络。此外，我们证明了相关的 Gaussian vector 在任意不同输入数据对之间保持非零特征值，确保 smallest eigenvalue  of the corresponding kernel matrix  strictly positive using the NNGP kernel。这些发现对 DEQ 的训练和泛化提供了基本的元素，为将来在这个领域的研究奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Exploring-hyperelastic-material-model-discovery-for-human-brain-cortex-multivariate-analysis-vs-artificial-neural-network-approaches"><a href="#Exploring-hyperelastic-material-model-discovery-for-human-brain-cortex-multivariate-analysis-vs-artificial-neural-network-approaches" class="headerlink" title="Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches"></a>Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10762">http://arxiv.org/abs/2310.10762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jixin Hou, Nicholas Filla, Xianyan Chen, Mir Jalil Razavi, Tianming Liu, Xianqiao Wang</li>
<li>for: 这个研究的目的是找到最适合人脑组织的 constitutive material model.</li>
<li>methods: 这个研究使用人工神经网络和多元回归方法来自动找到合适的 constitutive material model.</li>
<li>results: 研究发现，人工神经网络可以自动地找到准确的 constitutive material model，但是五个参数和两个参数神经网络模型在单模和多模载 scenarios下被发现是不优的，可以further simplifies into two-term和单term模型。这些发现 highlights the importance of hyperparameters for artificial neural network和emphasize the necessity for detailed cross-validations of regularization parameters to ensure optimal selection at a global level.<details>
<summary>Abstract</summary>
Traditional computational methods, such as the finite element analysis, have provided valuable insights into uncovering the underlying mechanisms of brain physical behaviors. However, precise predictions of brain physics require effective constitutive models to represent the intricate mechanical properties of brain tissue. In this study, we aimed to identify the most favorable constitutive material model for human brain tissue. To achieve this, we applied artificial neural network and multiple regression methods to a generalization of widely accepted classic models, and compared the results obtained from these two approaches. To evaluate the applicability and efficacy of the model, all setups were kept consistent across both methods, except for the approach to prevent potential overfitting. Our results demonstrate that artificial neural networks are capable of automatically identifying accurate constitutive models from given admissible estimators. Nonetheless, the five-term and two-term neural network models trained under single-mode and multi-mode loading scenarios, were found to be suboptimal and could be further simplified into two-term and single-term, respectively, with higher accuracy using multiple regression. Our findings highlight the importance of hyperparameters for the artificial neural network and emphasize the necessity for detailed cross-validations of regularization parameters to ensure optimal selection at a global level in the development of material constitutive models. This study validates the applicability and accuracy of artificial neural network to automatically discover constitutive material models with proper regularization as well as the benefits in model simplification without compromising accuracy for traditional multivariable regression.
</details>
<details>
<summary>摘要</summary>
传统计算方法，如finite element分析，已经提供了许多关键的发现，揭示了大脑物理行为的下面机制。然而，精确预测大脑物理需要有效的 constitutive 模型来表示大脑组织的复杂机械性质。在本研究中，我们想要找到最佳的 constitutive 材料模型 для人类大脑组织。为了实现这一目标，我们使用人工神经网络和多重回归方法，并对这两种方法进行比较。为了评估模型的适用性和效果，所有的设置都保持了一致，除非是避免过拟合。我们的结果表明，人工神经网络可以自动地从给定的可接受的估计器中提取精确的 constitutive 模型。然而，在单模式和多模式荷载场景下，五项和二项神经网络模型被发现为不优化，可以进一步简化为二项和单项模型，具有更高的准确率。我们的发现强调了人工神经网络中的hyperparameter的重要性，并提醒了在开发物理模型时需要进行详细的交叉验证，以确保优选的全局范围内的正则化参数。本研究证明了人工神经网络可以自动地找到符合正则化的 constitutive 材料模型，并且可以避免过拟合而不会产生准确性下降。
</details></li>
</ul>
<hr>
<h2 id="Statistical-Barriers-to-Affine-equivariant-Estimation"><a href="#Statistical-Barriers-to-Affine-equivariant-Estimation" class="headerlink" title="Statistical Barriers to Affine-equivariant Estimation"></a>Statistical Barriers to Affine-equivariant Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10758">http://arxiv.org/abs/2310.10758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Chen, Yeshwanth Cherapanamjeri</li>
<li>for:  Robust mean estimation in high-dimensional datasets with affine-invariant properties.</li>
<li>methods: Affine-equivariant estimators, lower bounds, and a new estimator based on a high-dimensional median.</li>
<li>results: Strict degradation in recovery error with quantitative rates degrading by a factor of $\sqrt{d}$ under two outlier models, and a new affine-equivariant estimator that nearly matches the lower bound.<details>
<summary>Abstract</summary>
We investigate the quantitative performance of affine-equivariant estimators for robust mean estimation. As a natural stability requirement, the construction of such affine-equivariant estimators has been extensively studied in the statistics literature. We quantitatively evaluate these estimators under two outlier models which have been the subject of much recent work: the heavy-tailed and adversarial corruption settings. We establish lower bounds which show that affine-equivariance induces a strict degradation in recovery error with quantitative rates degrading by a factor of $\sqrt{d}$ in both settings. We find that classical estimators such as the Tukey median (Tukey '75) and Stahel-Donoho estimator (Stahel '81 and Donoho '82) are either quantitatively sub-optimal even within the class of affine-equivariant estimators or lack any quantitative guarantees. On the other hand, recent estimators with strong quantitative guarantees are not affine-equivariant or require additional distributional assumptions to achieve it. We remedy this by constructing a new affine-equivariant estimator which nearly matches our lower bound. Our estimator is based on a novel notion of a high-dimensional median which may be of independent interest. Notably, our results are applicable more broadly to any estimator whose performance is evaluated in the Mahalanobis norm which, for affine-equivariant estimators, corresponds to an evaluation in Euclidean norm on isotropic distributions.
</details>
<details>
<summary>摘要</summary>
我们研究了不变式性的估计器在鲁棒均值估计中的量化性能。作为自然的稳定要求，建构这类不变式估计器在统计学Literature中得到了广泛的研究。我们量化地评估这些估计器在两种噪声模型下：重 tailed 和 adversarial corruption 设定下。我们建立了下限，显示不变式性导致了减少Recovery error的精度下限，具体是在两个设定下的 $\sqrt{d}$ 因子下降。我们发现经典估计器，如Tukey median（Tukey '75）和Stahel-Donoho estimator（Stahel '81和Donoho '82）在不变式估计器中是量化上不优或者没有量化保证。然而，现有的估计器具有强量化保证的，但是它们不是不变式的或者需要额外的分布假设来实现不变式性。我们提供了一种新的不变式估计器，它几乎与我们的下限匹配。我们的估计器基于一种新的高维度中位数据，这可能是独立的兴趣。值得注意的是，我们的结果适用于任何在Mahalanobis 距离上评估的估计器，这对于不变式估计器来说相当于在几何均勋度上评估。
</details></li>
</ul>
<hr>
<h2 id="Mori-Zwanzig-latent-space-Koopman-closure-for-nonlinear-autoencoder"><a href="#Mori-Zwanzig-latent-space-Koopman-closure-for-nonlinear-autoencoder" class="headerlink" title="Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder"></a>Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10745">http://arxiv.org/abs/2310.10745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyam Gupta, Peter J. Schmid, Denis Sipp, Taraneh Sayadi, Georgios Rigas</li>
<li>for: 该研究旨在提高数据驱动方法的精度和稳定性，以便更好地理解和预测复杂非线性系统的动态。</li>
<li>methods: 该研究提出了一种新的Morzy-Zwanzig自适应器（MZ-AE），通过非线性自适应器提取关键观察量，并通过Morzy-Zwanzigormalism来修正非马洛夫矩阵，实现了closed的动态表示。</li>
<li>results: 实验表明，MZ-AE可以准确地捕捉圆柱体流动中的模式转移，并提供了低维度的预测模型，对恒定 Kuramoto-Sivashinsky 系统 exhibit promising short-term predictability和robust long-term statistical performance。<details>
<summary>Abstract</summary>
The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and stability of the Koopman operator approximation. Demonstrations showcase the technique's ability to capture regime transitions in the flow around a circular cylinder. It also provided a low dimensional approximation for chaotic Kuramoto-Sivashinsky with promising short-term predictability and robust long-term statistical performance. By bridging the gap between data-driven techniques and the mathematical foundations of Koopman theory, MZ-AE offers a promising avenue for improved understanding and prediction of complex nonlinear dynamics.
</details>
<details>
<summary>摘要</summary>
科普曼运算符提供了一种globally linearization的方法，使得非线性系统的理解得以简化。虽然数据驱动的方法在 aproximate Koopman operator 方面表现出了承诺，但它们还需要解决一些挑战，例如选择合适的观察量、维度减少和准确预测复杂系统行为。这种研究提出了一种新的方法，即Mori-Zwanzig autoencoder (MZ-AE)，以稳定地 aproximate Koopman operator 在低维空间中。该方法利用非线性自适应神经网络提取关键观察量，并通过Morin Zwanzig 正则进行修正。因此，该方法可以在非线性自适应神经网络的 latent manifold 中 closure 动力学，从而提高 Koopman operator 的准确性和稳定性。示例显示该方法可以在圆柱体流动中捕捉转态。此外，它还为混沌 Kuramoto-Sivashinsky 提供了一种低维度的近似，并且在短期预测和长期统计性能方面具有承诺。通过将数据驱动技术与 Koopman 理论的数学基础相连接，MZ-AE 提供了一条可能的通路，以提高复杂非线性动力学的理解和预测。
</details></li>
</ul>
<hr>
<h2 id="Fast-Adversarial-Label-Flipping-Attack-on-Tabular-Data"><a href="#Fast-Adversarial-Label-Flipping-Attack-on-Tabular-Data" class="headerlink" title="Fast Adversarial Label-Flipping Attack on Tabular Data"></a>Fast Adversarial Label-Flipping Attack on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10744">http://arxiv.org/abs/2310.10744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinglong Chang, Gillian Dobbie, Jörg Wicker</li>
<li>for: 这篇研究旨在阐述机器学习模型在需要高可靠性的领域中受到攻击的问题，以及这些攻击的威胁。</li>
<li>methods: 本研究提出了一种新的快速攻击方法，即 Fast Adversarial Label-Flipping Attack (FALFA)，用于游戏机器学习模型。FALFA基于对敌人目标的变数转换，并使用线性程式来降低计算Complexity。</li>
<li>results: 使用了10个真实的条形数据集，研究发现FALFA具有高度的攻击潜力，显示了需要robust的防御措施。<details>
<summary>Abstract</summary>
Machine learning models are increasingly used in fields that require high reliability such as cybersecurity. However, these models remain vulnerable to various attacks, among which the adversarial label-flipping attack poses significant threats. In label-flipping attacks, the adversary maliciously flips a portion of training labels to compromise the machine learning model. This paper raises significant concerns as these attacks can camouflage a highly skewed dataset as an easily solvable classification problem, often misleading machine learning practitioners into lower defenses and miscalculations of potential risks. This concern amplifies in tabular data settings, where identifying true labels requires expertise, allowing malicious label-flipping attacks to easily slip under the radar. To demonstrate this risk is inherited in the adversary's objective, we propose FALFA (Fast Adversarial Label-Flipping Attack), a novel efficient attack for crafting adversarial labels. FALFA is based on transforming the adversary's objective and employs linear programming to reduce computational complexity. Using ten real-world tabular datasets, we demonstrate FALFA's superior attack potential, highlighting the need for robust defenses against such threats.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MOFDiff-Coarse-grained-Diffusion-for-Metal-Organic-Framework-Design"><a href="#MOFDiff-Coarse-grained-Diffusion-for-Metal-Organic-Framework-Design" class="headerlink" title="MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design"></a>MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10732">http://arxiv.org/abs/2310.10732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Fu, Tian Xie, Andrew S. Rosen, Tommi Jaakkola, Jake Smith</li>
<li>for: 这项研究旨在开发一种基于推卷模型的金属有机框架（MOF）结构生成器，以便为碳捕集应用提供高性能的MOF材料。</li>
<li>methods: 该研究使用了一种基于等距离推卷模型的diffusion模型，通过对分子组分部件坐标和identities进行杜磊推卷过程，生成高精度的MOF结构。然后，通过一种新型组装算法，确定了全原子MOF结构。</li>
<li>results: 研究人员通过分子仿真实验，证明了该模型可以生成有效和新型的MOF结构，并且可以有效地设计出standing MOFOaterials for carbon capture应用。<details>
<summary>Abstract</summary>
Metal-organic frameworks (MOFs) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry. Their modular nature has enabled the use of template-based methods to generate hypothetical MOFs by combining molecular building blocks in accordance with known network topologies. However, the ability of these methods to identify top-performing MOFs is often hindered by the limited diversity of the resulting chemical space. In this work, we propose MOFDiff: a coarse-grained (CG) diffusion model that generates CG MOF structures through a denoising diffusion process over the coordinates and identities of the building blocks. The all-atom MOF structure is then determined through a novel assembly algorithm. Equivariant graph neural networks are used for the diffusion model to respect the permutational and roto-translational symmetries. We comprehensively evaluate our model's capability to generate valid and novel MOF structures and its effectiveness in designing outstanding MOF materials for carbon capture applications with molecular simulations.
</details>
<details>
<summary>摘要</summary>
金属有机框架（MOF）在应用于气体存储和碳捕集等领域具有极高的利用价值，这主要归功于它们的非常的孔隙和可调化化学结构。MOF的模块性质使得可以通过模板基本方法生成 гипотетическихMOF结构，这些结构是通过将分子结构块组合在已知网络结构中来实现的。然而，这些方法的选择性往往受到生成化学空间的局限性的影响。在这种情况下，我们提出了MOFDiff：一种粗粒度（CG）扩散模型，该模型通过CG结构块坐标和分子标识的杜瓦扩散过程来生成CG MOF结构。然后，我们使用一种新的组装算法来确定全原子MOF结构。我们使用对称图 Néural networks来实现扩散模型，以保持分子的卷积和旋转平移 symmetries。我们对我们的模型的有效性进行了广泛的评估，并通过分子价值计算来评估MOF材料在碳捕集应用中的性能。
</details></li>
</ul>
<hr>
<h2 id="A-representation-learning-approach-to-probe-for-dynamical-dark-energy-in-matter-power-spectra"><a href="#A-representation-learning-approach-to-probe-for-dynamical-dark-energy-in-matter-power-spectra" class="headerlink" title="A representation learning approach to probe for dynamical dark energy in matter power spectra"></a>A representation learning approach to probe for dynamical dark energy in matter power spectra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10717">http://arxiv.org/abs/2310.10717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Piras, Lucas Lombriser</li>
<li>for:  searched for a compressed representation of dynamical dark energy models in observational studies of the cosmic large-scale structure.</li>
<li>methods:  trained a variational autoencoder (VAE) architecture, DE-VAE, on matter power spectra boosts generated at different redshift values and wavenumbers, and used a neural network to compress and reconstruct the boosts.</li>
<li>results:  found that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra within $1\sigma$ ($2\sigma$) of a Gaussian error, and the three variables can be linked together with an explicit equation through symbolic regression.<details>
<summary>Abstract</summary>
We present DE-VAE, a variational autoencoder (VAE) architecture to search for a compressed representation of dynamical dark energy (DE) models in observational studies of the cosmic large-scale structure. DE-VAE is trained on matter power spectra boosts generated at wavenumbers $k\in(0.01-2.5) \ h/\rm{Mpc}$ and at four redshift values $z\in(0.1,0.48,0.78,1.5)$ for the most typical dynamical DE parametrization with two extra parameters describing an evolving DE equation of state. The boosts are compressed to a lower-dimensional representation, which is concatenated with standard cold dark matter (CDM) parameters and then mapped back to reconstructed boosts; both the compression and the reconstruction components are parametrized as neural networks. Remarkably, we find that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra generated over a broad range of cosmological parameters within $1\sigma$ ($2\sigma$) of a Gaussian error which includes cosmic variance, shot noise and systematic effects for a Stage IV-like survey. This single parameter shows a high mutual information with the two DE parameters, and these three variables can be linked together with an explicit equation through symbolic regression. Considering a model with two latent variables only marginally improves the accuracy of the predictions, and adding a third latent variable has no significant impact on the model's performance. We discuss how the DE-VAE architecture can be extended from a proof of concept to a general framework to be employed in the search for a common lower-dimensional parametrization of a wide range of beyond-$\Lambda$CDM models and for different cosmological datasets. Such a framework could then both inform the development of cosmological surveys by targeting optimal probes, and provide theoretical insight into the common phenomenological aspects of beyond-$\Lambda$CDM models.
</details>
<details>
<summary>摘要</summary>
我们提出了DE-VAE，一种简化自动抽象（VAE）架构，用于在观测宇宙大规模结构的观测学中寻找压缩表现。DE-VAE 被训练在物质能谱强化器中，这些强化器在几何常数 $k\in(0.01-2.5) \ h/\rm{Mpc}$ 和四个红shift值 $z\in(0.1,0.48,0.78,1.5)$ 上生成了最常见的动态暗能（DE）模型的两个额外参数。这些强化器被压缩到较低维度的表现，并与标准冷黑物质（CDM）参数一起 concatenated，然后将其映射回重建的强化器；压缩和重建都是用神经网 parametrized。我们发现，仅具一个潜在参数可以预测95%（99%）的DE强化器生成的广泛范围的 cosmological 参数之间的误差，包括cosmic variance、shot noise和系统效应。这个单一参数与 DE 两个参数之间存在高的共同信息，这三个变数可以通过symbolic regression 连接起来。仅具二个潜在参数的情况只有marginally 提高了预测的精度，而添加第三个潜在参数没有显著影响模型的性能。我们讨论了DE-VAE 架构如何从证明理论中扩展到一个通用的架构，以便在不同的 cosmological 资料集上寻找一致的下dimensional parametrization。这个架构可以帮助发展 cosmological 调查，targeting 最佳探针，并提供理论上的共同现象描述。
</details></li>
</ul>
<hr>
<h2 id="A-Computational-Framework-for-Solving-Wasserstein-Lagrangian-Flows"><a href="#A-Computational-Framework-for-Solving-Wasserstein-Lagrangian-Flows" class="headerlink" title="A Computational Framework for Solving Wasserstein Lagrangian Flows"></a>A Computational Framework for Solving Wasserstein Lagrangian Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10649">http://arxiv.org/abs/2310.10649</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/necludov/wl-mechanics">https://github.com/necludov/wl-mechanics</a></li>
<li>paper_authors: Kirill Neklyudov, Rob Brekelmans, Alexander Tong, Lazar Atanackovic, Qiang Liu, Alireza Makhzani</li>
<li>for: 这篇论文主要针对单元细胞动力学问题进行优化运输问题的扩展，包括不同的可能性空间（kinetic energy）和权重函数（potential energy）的组合，以及这些组合所导致的多种优化运输问题，如契德桥、不均习运输、物理约束等。</li>
<li>methods: 该论文提出了一种基于深度学习的框架，可以处理这些优化运输问题的复杂计算。该框架不需要直接 simulate 或 backpropagate learned dynamics，也不需要优化couplings。</li>
<li>results: 作者们在单元细胞动力学问题中展示了该框架的灵活性和高效性，并比 précédentes 方法（含 incorporating prior knowledge into the dynamics）取得了更好的结果。<details>
<summary>Abstract</summary>
The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperforming previous approaches for the single-cell trajectory inference, where incorporating prior knowledge into the dynamics is crucial for correct predictions.
</details>
<details>
<summary>摘要</summary>
“Optimal transport问题的动力学表述可以通过不同的下层结构（动能）和扩散函数（potential energy）的选择扩展。这些组合导致了不同的变量问题（Lagrangians），包括舒得桥、不均衡优化运输、物理约束优化运输等。总的来说，优化的扩散路径未知，解决这些变量问题可能会 computationally challenging。我们基于对准形式的方法提出了一种新的深度学习框架，该框架不需要 simulate或backpropagate通过学习的动力学轨迹，也不需要对优化的扩散函数进行访问。我们展示了该框架的多样性，在单个细胞轨迹推断中超过了先前的方法。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Efficacy-of-Dual-Encoders-for-Extreme-Multi-Label-Classification"><a href="#Efficacy-of-Dual-Encoders-for-Extreme-Multi-Label-Classification" class="headerlink" title="Efficacy of Dual-Encoders for Extreme Multi-Label Classification"></a>Efficacy of Dual-Encoders for Extreme Multi-Label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10636">http://arxiv.org/abs/2310.10636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Gupta, Devvrit Khatri, Ankit S Rawat, Srinadh Bhojanapalli, Prateek Jain, Inderjit S Dhillon</li>
<li>for: 这个论文主要针对的是多类分类问题（Extreme Multi-Label Classification，XMC），具体来说是使用 dual-encoder 模型来解决这类问题。</li>
<li>methods: 这篇论文使用了 dual-encoder 模型，并且提出了一种新的损失函数来优化 Recall@k 纪录。</li>
<li>results: 论文的实验结果表明，使用标准的 dual-encoder 模型可以与现有的 SOTA 多类分类方法匹配或超越，即使是在最大的 XMC 数据集上。此外，论文还提出了一种可微的 topk 错误基于损失函数，可以专门优化 Recall@k 纪录。<details>
<summary>Abstract</summary>
Dual-encoder models have demonstrated significant success in dense retrieval tasks for open-domain question answering that mostly involves zero-shot and few-shot scenarios. However, their performance in many-shot retrieval problems where training data is abundant, such as extreme multi-label classification (XMC), remains under-explored. Existing empirical evidence suggests that, for such problems, the dual-encoder method's accuracies lag behind the performance of state-of-the-art (SOTA) extreme classification methods that grow the number of learnable parameters linearly with the number of classes. As a result, some recent extreme classification techniques use a combination of dual-encoders and a learnable classification head for each class to excel on these tasks. In this paper, we investigate the potential of "pure" DE models in XMC tasks. Our findings reveal that when trained correctly standard dual-encoders can match or outperform SOTA extreme classification methods by up to 2% at Precision@1 even on the largest XMC datasets while being 20x smaller in terms of the number of trainable parameters. We further propose a differentiable topk error-based loss function, which can be used to specifically optimize for Recall@k metrics. We include our PyTorch implementation along with other resources for reproducing the results in the supplementary material.
</details>
<details>
<summary>摘要</summary>
dual-encoder 模型在开放问题 answering 中的 dense retrieval 任务中表现出了重要的成功，特别是在零shot 和几shot 场景下。然而，它们在有很多training data的 many-shot retrieval 问题中，如极多标签分类 (XMC)，的性能还未得到了充分的探索。现有的实际证据表明，对于这些任务， dual-encoder 方法的准确率落后于 state-of-the-art (SOTA) 极分类方法的性能，后者通过将学习参数的数量与类数直线上增加来提高性能。因此，一些最新的极分类技术使用了 dual-encoder 和每个类别上的学习权重的组合来进行优化。在这篇论文中，我们调查了 "纯" dual-encoder 模型在 XMC 任务中的潜力。我们发现，当正确地训练标准 dual-encoder 模型时，它们可以与 SOTA 极分类方法相当或超越它们，在最大 XMC 数据集上的精度@1 指标上提高至2%，而且只需20倍的学习参数数量。我们还提出了一种可微的 topk 错误基于损失函数，可以专门优化 Recall@k 指标。我们在辅料中包含了 PyTorch 实现以及其他用于重现结果的资源。
</details></li>
</ul>
<hr>
<h2 id="Certainty-In-Certainty-Out-REVQCs-for-Quantum-Machine-Learning"><a href="#Certainty-In-Certainty-Out-REVQCs-for-Quantum-Machine-Learning" class="headerlink" title="Certainty In, Certainty Out: REVQCs for Quantum Machine Learning"></a>Certainty In, Certainty Out: REVQCs for Quantum Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10629">http://arxiv.org/abs/2310.10629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Helgesen, Michael Felsberg, Jan-Åke Larsson</li>
<li>for: 这篇论文的目的是提出高单个样本准确率作为主要目标，并提出一种逆向培训方法以实现此目标。</li>
<li>methods: 该论文使用统计理论和反向培训方法来实现高准确率单个样本推断。</li>
<li>results: 论文通过评估多种有效的变换量量计划（VQC）在随机二进制子集上进行单个样本推断时，实现了10-15%的提升。<details>
<summary>Abstract</summary>
The field of Quantum Machine Learning (QML) has emerged recently in the hopes of finding new machine learning protocols or exponential speedups for classical ones. Apart from problems with vanishing gradients and efficient encoding methods, these speedups are hard to find because the sampling nature of quantum computers promotes either simulating computations classically or running them many times on quantum computers in order to use approximate expectation values in gradient calculations. In this paper, we make a case for setting high single-sample accuracy as a primary goal. We discuss the statistical theory which enables highly accurate and precise sample inference, and propose a method of reversed training towards this end. We show the effectiveness of this training method by assessing several effective variational quantum circuits (VQCs), trained in both the standard and reversed directions, on random binary subsets of the MNIST and MNIST Fashion datasets, on which our method provides an increase of $10-15\%$ in single-sample inference accuracy.
</details>
<details>
<summary>摘要</summary>
quantum机器学习（QML）领域在最近才出现，旨在找到新的机器学习协议或类比速度。不过，由于混合度难以计算和有效编码方法，这些增速很难找。在这篇论文中，我们提出了设置高单个样本准确率为主要目标的观点。我们讨论了 Statistical Theory，它使得高准确和精确的样本推测成为可能，并提出了反向训练方法。我们通过评估多种有效的量子征值逻辑环（VQC），在标准和反向方向上进行训练，在随机二进制subset of MNIST和MNIST Fashion数据集上显示了10-15%的单个样本推测准确率提高。
</details></li>
</ul>
<hr>
<h2 id="How-Do-Transformers-Learn-In-Context-Beyond-Simple-Functions-A-Case-Study-on-Learning-with-Representations"><a href="#How-Do-Transformers-Learn-In-Context-Beyond-Simple-Functions-A-Case-Study-on-Learning-with-Representations" class="headerlink" title="How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations"></a>How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10616">http://arxiv.org/abs/2310.10616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, Yu Bai</li>
<li>for: 这篇论文探讨了基于转换器架构的大语言模型在更复杂的情况下的启发式学习（ICL）能力，以及这种能力的理论和机制。</li>
<li>methods: 作者构造了一系列基于复杂表达函数的synthetic ICLe学习问题，并证明了存在特定的转换器可以近似地实现这些算法，只需要较少的深度和大小。在实验中，作者发现训练过的转换器能够在这些设定下达到近似optimal ICL性能，并展示了层次结构的分解，其中下层层transforms the dataset，而上层进行线性ICL。</li>
<li>results: 作者通过广泛的探索和一种新的粘贴实验发现了许多在训练过的转换器中的机制，如输入和表示的具体复制行为，上层线性ICL能力，以及在更加复杂的混合 Setting中的后ICL表示选择机制。这些观察到的机制与作者的理论相符，可能有助于理解转换器在更真实的场景中的ICL能力。<details>
<summary>Abstract</summary>
While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by studying learning with representations. Concretely, we construct synthetic in-context learning problems with a compositional structure, where the label depends on the input through a possibly complex but fixed representation function, composed with a linear function that differs in each instance. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. We show theoretically the existence of transformers that approximately implement such algorithms with mild depth and size. Empirically, we find trained transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder mixture setting. These observed mechanisms align well with our theory and may shed light on how transformers perform ICL in more realistic scenarios.
</details>
<details>
<summary>摘要</summary>
大型语言模型基于变换器架构已经展示了很出色的上下文学习（ICL）能力，但对这些能力的理解仍然处于早期阶段，现有的理论和机制理解主要集中在简单的情景下，如学习简单的函数类。这篇论文从更复杂的情景出发，研究学习表示法。具体来说，我们构造了一些具有复合结构的培成式上下文学习问题，其中标签取决于输入的表示函数，这个函数可能是复杂的但固定的。因此，最佳的ICL算法首先将输入转化为表示函数的输出，然后在这些输出上进行线性ICL。我们证明了在某种程度上，存在可以近似实现这种算法的变换器，只需要较少的深度和大小。Empirically，我们发现训练后的变换器在这种设定下具有近乎最佳的ICL性能，并且展现出了预期的分割，下层层次将输入数据转化，而上层层次进行线性ICL。通过广泛的探索和一种新的粘贴实验，我们还发现了许多内部机制，如输入和表示的具体复制行为，上层层次的线性ICL能力，以及在更复杂的混合 Setting下的后ICL表示选择机制。这些观察到的机制与我们的理论相吻合，可能为ICL在更实际的情景中的研究提供了灵感。
</details></li>
</ul>
<hr>
<h2 id="IW-GAE-Importance-weighted-group-accuracy-estimation-for-improved-calibration-and-model-selection-in-unsupervised-domain-adaptation"><a href="#IW-GAE-Importance-weighted-group-accuracy-estimation-for-improved-calibration-and-model-selection-in-unsupervised-domain-adaptation" class="headerlink" title="IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation"></a>IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10611">http://arxiv.org/abs/2310.10611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taejong Joo, Diego Klabjan</li>
<li>for: 这篇论文旨在解决机器学习中的分布偏移问题，即在不具备标签的情况下，在分布偏移后的预测 зада务中保持高度的准确性。</li>
<li>methods: 该论文提出了一种重要性权重Weighted group accuracy estimator，通过提出一个优化问题，找到一个能够在分布偏移后的预测任务中准确地估计分组准确率的重要性权重。同时，该论文也进行了理论分析。</li>
<li>results: 经过广泛的实验，论文证明了该重要性权重Weighted group accuracy estimator的效果，可以帮助解决不supervised domain adaptation问题中的模型校准和模型选择问题。同时，该论文还提出了一种新的改进方向，即通过提高分组准确率来提高模型的转移率。<details>
<summary>Abstract</summary>
Reasoning about a model's accuracy on a test sample from its confidence is a central problem in machine learning, being connected to important applications such as uncertainty representation, model selection, and exploration. While these connections have been well-studied in the i.i.d. settings, distribution shifts pose significant challenges to the traditional methods. Therefore, model calibration and model selection remain challenging in the unsupervised domain adaptation problem--a scenario where the goal is to perform well in a distribution shifted domain without labels. In this work, we tackle difficulties coming from distribution shifts by developing a novel importance weighted group accuracy estimator. Specifically, we formulate an optimization problem for finding an importance weight that leads to an accurate group accuracy estimation in the distribution shifted domain with theoretical analyses. Extensive experiments show the effectiveness of group accuracy estimation on model calibration and model selection. Our results emphasize the significance of group accuracy estimation for addressing challenges in unsupervised domain adaptation, as an orthogonal improvement direction with improving transferability of accuracy.
</details>
<details>
<summary>摘要</summary>
machine learning 中，关于模型在测试样本上的准确性的推理是一个中心问题，与重要应用领域如不确定性表示、模型选择和探索相连。然而，在不同分布下 poses significant challenges to traditional methods。因此，模型准确性和模型选择在无supervised domain adaptation问题中仍然是挑战。在这种情况下，我们通过开发一种重要性加权组准精度估计器来解决分布shift的困难。specifically，我们提出了一个优化问题，找到一个导致在分布shifted domain中准确的组准精度估计器，并进行了理论分析。我们的实验表明，组准精度估计器对模型准确性和模型选择具有重要的作用，并且是一个对照 Transferability of accuracy的正交改进方向。
</details></li>
</ul>
<hr>
<h2 id="BayRnTune-Adaptive-Bayesian-Domain-Randomization-via-Strategic-Fine-tuning"><a href="#BayRnTune-Adaptive-Bayesian-Domain-Randomization-via-Strategic-Fine-tuning" class="headerlink" title="BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning"></a>BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10606">http://arxiv.org/abs/2310.10606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianle Huang, Nitish Sontakke, K. Niranjan Kumar, Irfan Essa, Stefanos Nikolaidis, Dennis W. Hong, Sehoon Ha</li>
<li>for: 降低 sim2real 距离</li>
<li>methods: 自适应域随机化 + 精细调整</li>
<li>results: 比 vanilla DR 或 Bayesian DR 更高的奖励得分，同样的时间步数内Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to address the issue of careful tuning of randomization parameters in domain randomization (DR) methods, and to propose a new method called Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune) that can significantly accelerate the learning process by fine-tuning from previously learned policy.</li>
<li>methods: The proposed BayRnTune method inherits the spirit of Bayesian DR but with a key difference - it uses strategic fine-tuning of the previous policy to adapt to new environments. The method is evaluated in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments.</li>
<li>results: The results show that BayRnTune yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR. This suggests that the proposed method can significantly accelerate the learning process and improve the performance of DR in robotics.<details>
<summary>Abstract</summary>
Domain randomization (DR), which entails training a policy with randomized dynamics, has proven to be a simple yet effective algorithm for reducing the gap between simulation and the real world. However, DR often requires careful tuning of randomization parameters. Methods like Bayesian Domain Randomization (Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issue by automating parameter range selection using real-world experience. While effective, these algorithms often require long computation time, as a new policy is trained from scratch every iteration. In this work, we propose Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune), which inherits the spirit of BayRn but aims to significantly accelerate the learning processes by fine-tuning from previously learned policy. This idea leads to a critical question: which previous policy should we use as a prior during fine-tuning? We investigated four different fine-tuning strategies and compared them against baseline algorithms in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments. Our analysis demonstrates that our method yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR.
</details>
<details>
<summary>摘要</summary>
域随机化（DR），即在训练策略时随机使用不同的动力学，已经证明是一种简单 yet effective的算法，可以减少实际世界和模拟之间的差距。然而，DR通常需要仔细调整随机参数。例如， bayesian domain randomization（Bayesian DR）和活动域随机化（Adaptive DR）可以自动选择随机参数的范围，使用实际世界经验。尽管有效，这些算法通常需要长时间的计算，因为每轮训练都需要从头开始训练一个新的策略。在这种情况下，我们提出了 adaptive bayesian domain randomization via strategic fine-tuning（BayRnTune），它继承了 BayRn 的精神，但是强调快速学习过程，通过对之前学习的策略进行细化来加速学习。这个想法引出了一个关键的问题：我们在细化过程中应该使用哪个先前学习的策略作为先前？我们 investigate了四种不同的细化策略，并与基线算法进行比较在五个模拟环境中，这些环境从简单的benchmark任务到更复杂的四肢机器人环境。我们的分析表明，我们的方法可以在同样的时间步骤内达到更高的奖励。
</details></li>
</ul>
<hr>
<h2 id="Pareto-Optimization-to-Accelerate-Multi-Objective-Virtual-Screening"><a href="#Pareto-Optimization-to-Accelerate-Multi-Objective-Virtual-Screening" class="headerlink" title="Pareto Optimization to Accelerate Multi-Objective Virtual Screening"></a>Pareto Optimization to Accelerate Multi-Objective Virtual Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10598">http://arxiv.org/abs/2310.10598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jenna C. Fromer, David E. Graff, Connor W. Coley</li>
<li>for: 本研究旨在透过多属性算法来快速找到具有强烈结合性、最小化副作用和适当的药物性特性的药物分子。</li>
<li>methods: 本研究使用多属性贝叶斯搜寻来减少虚拟实验成本，并运用这种方法在确定蛋白质和副标的对应的选择性抑制剂中找到适当的药物分子。</li>
<li>results: 本研究发现，使用多属性贝叶斯搜寻可以快速找到具有强烈结合性、最小化副作用和适当的药物性特性的药物分子，并且可以实现对虚拟实验中的药物分子库进行高效的搜寻和范畴化。<details>
<summary>Abstract</summary>
The discovery of therapeutic molecules is fundamentally a multi-objective optimization problem. One formulation of the problem is to identify molecules that simultaneously exhibit strong binding affinity for a target protein, minimal off-target interactions, and suitable pharmacokinetic properties. Inspired by prior work that uses active learning to accelerate the identification of strong binders, we implement multi-objective Bayesian optimization to reduce the computational cost of multi-property virtual screening and apply it to the identification of ligands predicted to be selective based on docking scores to on- and off-targets. We demonstrate the superiority of Pareto optimization over scalarization across three case studies. Further, we use the developed optimization tool to search a virtual library of over 4M molecules for those predicted to be selective dual inhibitors of EGFR and IGF1R, acquiring 100% of the molecules that form the library's Pareto front after exploring only 8% of the library. This workflow and associated open source software can reduce the screening burden of molecular design projects and is complementary to research aiming to improve the accuracy of binding predictions and other molecular properties.
</details>
<details>
<summary>摘要</summary>
发现治疗分子是一个多目标优化问题的基本问题。一种形ulation的问题是通过同时具有高绑定亲和力、最小的偶折受影响和合适的药物生物学性 Properties 来认定分子。取得了先前工作使用活动学习加速绑定分子的识别的灵感，我们实现了多属性权重优化来降低虚拟屏选中计算成本，并应用于预测绑定分子的药物设计中。我们在三个案例中证明了对比权重优化的优势，并使用开发的优化工具来搜索虚拟库中的可选性双抑制剂。通过探索虚拟库的8% only，我们收获了虚拟库的极值 front 上的100%分子。这种工作流和相关的开源软件可以减轻分子设计项目的屏选负担，并且与尝试提高绑定预测和其他分子性质的研究相 complementary。
</details></li>
</ul>
<hr>
<h2 id="HelmSim-Learning-Helmholtz-Dynamics-for-Interpretable-Fluid-Simulation"><a href="#HelmSim-Learning-Helmholtz-Dynamics-for-Interpretable-Fluid-Simulation" class="headerlink" title="HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation"></a>HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10565">http://arxiv.org/abs/2310.10565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanxiang Xing, Haixu Wu, Yuezhou Ma, Jianmin Wang, Mingsheng Long</li>
<li>for: 这个论文旨在提出一种准确且可解释的流体模拟器，即HelmSim，以解决流体动力学的长期挑战。</li>
<li>methods: 该论文提出了一种基于Helmholtz定理的HelmDynamic块，该块将流体动力学分解为更容易解决的curl-free和divergence-free部分，物理相应于流体的潜potential和流体流函数。该块被 embedding到一个多尺度 интеграцион网络中，以 интеGRATE temporal维度上的多个空间尺度的Helmholtz动力学。</li>
<li>results: 对比previoius velocity estimating方法，HelmSim具有 faithful derived from Helmholtz theorem和Physically interpretable evidence，并在 numerically simulated和实际观测的标准准样中实现了一致的state-of-the-art表现，即使在复杂的边界条件下。<details>
<summary>Abstract</summary>
Fluid simulation is a long-standing challenge due to the intrinsic high-dimensional non-linear dynamics. Previous methods usually utilize the non-linear modeling capability of deep models to directly estimate velocity fields for future prediction. However, skipping over inherent physical properties but directly learning superficial velocity fields will overwhelm the model from generating precise or physics-reliable results. In this paper, we propose the HelmSim toward an accurate and interpretable simulator for fluid. Inspired by the Helmholtz theorem, we design a HelmDynamic block to learn the Helmholtz dynamics, which decomposes fluid dynamics into more solvable curl-free and divergence-free parts, physically corresponding to potential and stream functions of fluid. By embedding the HelmDynamic block into a Multiscale Integration Network, HelmSim can integrate learned Helmholtz dynamics along temporal dimension in multiple spatial scales to yield future fluid. Comparing with previous velocity estimating methods, HelmSim is faithfully derived from Helmholtz theorem and ravels out complex fluid dynamics with physically interpretable evidence. Experimentally, our proposed HelmSim achieves the consistent state-of-the-art in both numerical simulated and real-world observed benchmarks, even for scenarios with complex boundaries.
</details>
<details>
<summary>摘要</summary>
fluid 模拟是一个长期的挑战，因为它的自然高维非线性动力学性。以前的方法通常使用深度模型的非线性建模能力直接估算未来的速度场，但是跳过了内在物理属性，直接学习 superficies 的速度场将会让模型生成精度不高或者物理可靠的结果。在这篇论文中，我们提出了 HelmSim，一种准确和可解释的流体模拟器。受 helmholtz 定理启发，我们设计了 HelmDynamic 块，用于学习 helmholtz 动力学，它将流体动力学分解为更可解决的 curl-free 和 divergence-free 部分，物理相应于流体的潜在函数和流函数。通过在多尺度练习网络中嵌入 HelmDynamic 块，HelmSim 可以在多个空间尺度上将学习的 helmholtz 动力学集成到时间维度上，以生成未来的流体。相比之前的速度估计方法，HelmSim 是准确地从 helmholtz 定理中派生出来，并且可以揭示出复杂的流体动力学性，并且具有物理可解的证据。实验表明，我们提出的 HelmSim 在数值 simulate 和实际观测的标准准确，即使场景具有复杂的边界。
</details></li>
</ul>
<hr>
<h2 id="Causal-Dynamic-Variational-Autoencoder-for-Counterfactual-Regression-in-Longitudinal-Data"><a href="#Causal-Dynamic-Variational-Autoencoder-for-Counterfactual-Regression-in-Longitudinal-Data" class="headerlink" title="Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data"></a>Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10559">http://arxiv.org/abs/2310.10559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Cournède</li>
<li>for: 这篇论文的目的是用来估计治疗效果的变化趋势，特别是在精准医学、epidemiology、经济和市场营销等领域。</li>
<li>methods: 这篇论文使用了一种新的方法，即假设存在不观察到的风险因素（也称为调整变量），这些风险因素只影响短期内的结果。</li>
<li>results: 论文的实验结果显示，这种新方法可以准确地估计个体治疗效果，并能够捕捉长期内治疗响应中的不观察到风险因素。<details>
<summary>Abstract</summary>
Estimating treatment effects over time is relevant in many real-world applications, such as precision medicine, epidemiology, economy, and marketing. Many state-of-the-art methods either assume the observations of all confounders or seek to infer the unobserved ones. We take a different perspective by assuming unobserved risk factors, i.e., adjustment variables that affect only the sequence of outcomes. Under unconfoundedness, we target the Individual Treatment Effect (ITE) estimation with unobserved heterogeneity in the treatment response due to missing risk factors. We address the challenges posed by time-varying effects and unobserved adjustment variables. Led by theoretical results over the validity of the learned adjustment variables and generalization bounds over the treatment effect, we devise Causal DVAE (CDVAE). This model combines a Dynamic Variational Autoencoder (DVAE) framework with a weighting strategy using propensity scores to estimate counterfactual responses. The CDVAE model allows for accurate estimation of ITE and captures the underlying heterogeneity in longitudinal data. Evaluations of our model show superior performance over state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
在许多实际应用中，如精准医学、 Epidemiology、经济和市场营销中，估计治疗效果的演化是非常重要的。许多现代方法都是假设所有干扰因素的观察，或者尝试推断未观察到的干扰因素。我们采取了一种不同的视角，假设存在未观察到的风险因素，即调整变量，这些变量只影响结果序列。在干扰性下，我们target个人治疗效果（ITE）估计，带有未观察到的多变性。我们解决了时间变化的效果和未观察到的调整变量的挑战。通过理论结果的有效性和权重分配策略使用可能性分数来估计对应响应，我们设计了 causal DVAE（CDVAE）模型。这个模型结合了动态变量自动编码器（DVAE）框架和一种利用可能性分数进行权重分配的策略，以估计对应响应。 CDVAE 模型允许精准地估计 ITE，并捕捉了长期数据中的下降多变性。我们对我们的模型进行评估，并证明它们在现有模型中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Preference-Based-Nonparametric-Off-Policy-Evaluation-with-Deep-Networks"><a href="#Sample-Complexity-of-Preference-Based-Nonparametric-Off-Policy-Evaluation-with-Deep-Networks" class="headerlink" title="Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks"></a>Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10556">http://arxiv.org/abs/2310.10556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Li, Xiang Ji, Minshuo Chen, Mengdi Wang</li>
<li>for:  solves reinforcement learning problems with human preference data</li>
<li>methods:  uses actor-critic methods and fitted-Q-evaluation with a deep neural network</li>
<li>results:  establishes a sample-efficient estimator for off-policy evaluation with high reward smoothness, and almost aligns with classical OPE results with observable reward data.<details>
<summary>Abstract</summary>
A recently popular approach to solving reinforcement learning is with data from human preferences. In fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (OPE). Such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. Despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any low-dimensional manifold structure in the Markov decision process and obtain a sample-efficient estimator without suffering from the curse of high data ambient dimensionality. Under the assumption of high reward smoothness, our results \textit{almost align with the classical OPE results with observable reward data}. To the best of our knowledge, this is the first result that establishes a \textit{provably efficient} guarantee for off-policy evaluation with RLHF.
</details>
<details>
<summary>摘要</summary>
一种最近受欢迎的解决方案是使用人类偏好数据来解决强化学习问题。实际上，人类偏好数据现在与 классиical 强化学习算法（如actor-critic方法）结合使用，这些算法包括（i）从人类偏好数据集中学习奖励函数，和（ii）使用learned reward的Off-Policy Evaluation（OPE）来评估目标策略的预期总奖励。 despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any low-dimensional manifold structure in the Markov decision process and obtain a sample-efficient estimator without suffering from the curse of high data ambient dimensionality. Under the assumption of high reward smoothness, our results almost align with the classical OPE results with observable reward data. To the best of our knowledge, this is the first result that establishes a provably efficient guarantee for off-policy evaluation with RLHF.
</details></li>
</ul>
<hr>
<h2 id="Population-based-wind-farm-monitoring-based-on-a-spatial-autoregressive-approach"><a href="#Population-based-wind-farm-monitoring-based-on-a-spatial-autoregressive-approach" class="headerlink" title="Population-based wind farm monitoring based on a spatial autoregressive approach"></a>Population-based wind farm monitoring based on a spatial autoregressive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10555">http://arxiv.org/abs/2310.10555</a></li>
<li>repo_url: None</li>
<li>paper_authors: W. Lin, K. Worden, E. J. Cross</li>
<li>for: 降低风力电站运行和维护成本</li>
<li>methods: 使用人口基于的结构健康监测系统，并利用多个结构（i.e.~风机）共享数据来提高结构行为预测</li>
<li>results: 提出了一种基于 Gaussian process 的空间自回归模型（GP-SPARX 模型），可以正确捕捉风机群的空间和时间相关性，并且可以用于健康监测系统的实现。<details>
<summary>Abstract</summary>
An important challenge faced by wind farm operators is to reduce operation and maintenance cost. Structural health monitoring provides a means of cost reduction through minimising unnecessary maintenance trips as well as prolonging turbine service life. Population-based structural health monitoring can further reduce the cost of health monitoring systems by implementing one system for multiple structures (i.e.~turbines). At the same time, shared data within a population of structures may improve the predictions of structural behaviour. To monitor turbine performance at a population/farm level, an important initial step is to construct a model that describes the behaviour of all turbines under normal conditions. This paper proposes a population-level model that explicitly captures the spatial and temporal correlations (between turbines) induced by the wake effect. The proposed model is a Gaussian process-based spatial autoregressive model, named here a GP-SPARX model. This approach is developed since (a) it reflects our physical understanding of the wake effect, and (b) it benefits from a stochastic data-based learner. A case study is provided to demonstrate the capability of the GP-SPARX model in capturing spatial and temporal variations as well as its potential applicability in a health monitoring system.
</details>
<details>
<summary>摘要</summary>
operator of wind farms 面临一个重要挑战是减少运营和维护成本。人口基本的结构健康监测可以通过最小化无必要的维护旅行以及提高机顺服务寿命，从而减少健康监测系统的成本。同时，在多个结构之间共享数据可以提高结构行为预测的准确性。为监测风 турbin的性能，一个重要的初始步骤是建立一个描述所有风 турbin在正常情况下行为的模型。这篇文章提出了一种人口级别的模型，该模型由 Gaussian 过程基本的空间自相关模型（GP-SPARX）组成，这种方法因其体现了风阻效应的物理理解，同时受益于数据驱动的随机学习。一个案例研究证明了 GP-SPARX 模型能够 capture 空间和时间变化，以及其可能应用于健康监测系统。
</details></li>
</ul>
<hr>
<h2 id="TacticAI-an-AI-assistant-for-football-tactics"><a href="#TacticAI-an-AI-assistant-for-football-tactics" class="headerlink" title="TacticAI: an AI assistant for football tactics"></a>TacticAI: an AI assistant for football tactics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10553">http://arxiv.org/abs/2310.10553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Wang, Petar Veličković, Daniel Hennes, Nenad Tomašev, Laurel Prince, Michael Kaisers, Yoram Bachrach, Romuald Elie, Li Kevin Wenliang, Federico Piccinini, William Spearman, Ian Graham, Jerome Connor, Yi Yang, Adrià Recasens, Mina Khan, Nathalie Beauguerlange, Pablo Sprechmann, Pol Moreno, Nicolas Heess, Michael Bowling, Demis Hassabis, Karl Tuyls</li>
<li>for: 这篇论文是为了开发一种基于人工智能的足球战术助手（TacticAI），帮助教练分析对手队伍的战术模式，并提供有效的回应策略。</li>
<li>methods: 这篇论文使用了预测和生成两部分的算法，允许教练通过样本和探索不同的球员布局来评估不同的角球模式，并选择最有可能性 succeed 的设置。</li>
<li>results: 研究人员通过对一些有关的 benchmark task 进行验证，证明 TacticAI 的模型建议不仅与实际战术无法分辨，而且在 90% 的时间上超过现有战术。 另外，TacticAI 还提供了一个有效的角球检索系统。<details>
<summary>Abstract</summary>
Identifying key patterns of tactics implemented by rival teams, and developing effective responses, lies at the heart of modern football. However, doing so algorithmically remains an open research challenge. To address this unmet need, we propose TacticAI, an AI football tactics assistant developed and evaluated in close collaboration with domain experts from Liverpool FC. We focus on analysing corner kicks, as they offer coaches the most direct opportunities for interventions and improvements. TacticAI incorporates both a predictive and a generative component, allowing the coaches to effectively sample and explore alternative player setups for each corner kick routine and to select those with the highest predicted likelihood of success. We validate TacticAI on a number of relevant benchmark tasks: predicting receivers and shot attempts and recommending player position adjustments. The utility of TacticAI is validated by a qualitative study conducted with football domain experts at Liverpool FC. We show that TacticAI's model suggestions are not only indistinguishable from real tactics, but also favoured over existing tactics 90% of the time, and that TacticAI offers an effective corner kick retrieval system. TacticAI achieves these results despite the limited availability of gold-standard data, achieving data efficiency through geometric deep learning.
</details>
<details>
<summary>摘要</summary>
现代足球中，认识对手队伍实施的战术模式，并开发有效应对策略，是核心问题。然而，这种算法化研究仍然是一个开放的研究挑战。为解决这个需求，我们提出了TacticAI，一个基于人工智能的足球战术助手。我们与足球领域专家合作开发并评估了TacticAI，专注于分析角球机会，因为这些机会提供了教练最直接的改进和优化机会。TacticAI包含预测和生成两个组成部分，允许教练通过采样和探索不同的玩家设置来寻找最有可能成功的角球机会。我们在多个相关的 bencmark任务上验证了TacticAI：预测接收者和射击尝试，并建议玩家位置调整。我们通过对足球领域专家进行质量调研，证明TacticAI的模型建议与实际战术无法分辨，并且90%的时间 prefer TacticAI的建议。此外，TacticAI还提供了有效的角球检索系统。TacticAI达到了这些结果，尽管数据的可用性受限，通过几何深度学习实现了数据效率。
</details></li>
</ul>
<hr>
<h2 id="Optimal-vintage-factor-analysis-with-deflation-varimax"><a href="#Optimal-vintage-factor-analysis-with-deflation-varimax" class="headerlink" title="Optimal vintage factor analysis with deflation varimax"></a>Optimal vintage factor analysis with deflation varimax</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10545">http://arxiv.org/abs/2310.10545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Bing, Dian Jin, Yuqian Zhang</li>
<li>for: This paper proposes a new method for vintage factor analysis, which aims to find a low-dimensional representation of the original data and then seek a rotation that is scientifically meaningful.</li>
<li>methods: The proposed method uses a deflation varimax procedure that solves each row of an orthogonal matrix sequentially, which has a net computational gain and flexibility.</li>
<li>results: The proposed method is able to fully establish theoretical guarantees for the proposed procedure in a broad context, and it is shown to be optimal in all SNR regimes. Additionally, the method is valid for finite sample and allows the number of the latent factors to grow with the sample size.Here is the Chinese translation of the three points:</li>
<li>for: 这个论文提出了一种新的维度分析方法，目的是找到原始数据的低维度表示，然后寻找科学意义的旋转。</li>
<li>methods: 该方法使用了一种减法varimax过程，解决每个正交矩阵的每行问题，具有计算效益和灵活性。</li>
<li>results: 该方法能够在广泛的情况下提供完整的理论保证，并且在所有的噪声范围内都是优化的。此外，方法是有限样本的有效性和因子数量可以随样本大小和维度的增长而增长。<details>
<summary>Abstract</summary>
Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. Perhaps the most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.   In this paper, we propose a deflation varimax procedure that solves each row of an orthogonal matrix sequentially. In addition to its net computational gain and flexibility, we are able to fully establish theoretical guarantees for the proposed procedure in a broad context.   Adopting this new varimax approach as the second step after PCA, we further analyze this two step procedure under a general class of factor models. Our results show that it estimates the factor loading matrix in the optimal rate when the signal-to-noise-ratio (SNR) is moderate or large. In the low SNR regime, we offer possible improvement over using PCA and the deflation procedure when the additive noise under the factor model is structured. The modified procedure is shown to be optimal in all SNR regimes. Our theory is valid for finite sample and allows the number of the latent factors to grow with the sample size as well as the ambient dimension to grow with, or even exceed, the sample size.   Extensive simulation and real data analysis further corroborate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
古典因素分析是一种重要的因素分析方法，旨在首先找到原始数据的低维度表示，然后寻找一种可靠的旋转，使得旋转后的低维度表示具有科学意义。最广泛使用的古典因素分析方法是主Component分析（PCA）followed by varimax旋转。尽管它受欢迎，但是可以提供的理论保证很少，因为varimax旋转需要解决非核心化优化问题。  在这篇论文中，我们提出了一种减少varimax过程中的计算量和灵活性的方法，并且可以在广泛的Context下提供完整的理论保证。我们采用这种新的varimax方法作为PCA之后的第二步，然后对这两步进程进行了广泛的分析。我们的结果表明，这种两步过程在中等或大的信号噪声比（SNR）下能够优化因子加载矩阵。在噪声比较低的情况下，我们提供了可能的改进方案，其中添加的噪声在因子模型下是结构化的。我们的修改过程在所有SNR régime下是优化的。我们的理论是有限样本和因子数量可以随样本大小和环境维度增长。我们的实验和实际数据分析进一步证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Comparators-in-Generalization-Bounds"><a href="#Comparing-Comparators-in-Generalization-Bounds" class="headerlink" title="Comparing Comparators in Generalization Bounds"></a>Comparing Comparators in Generalization Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10534">http://arxiv.org/abs/2310.10534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fredrik Hellström, Benjamin Guedj</li>
<li>for: 本研究的目的是提出一种基于信息理论和PAC-搜索概率的通用泛化约束，用于评估机器学习模型的泛化性能。</li>
<li>methods: 本文使用了信息理论和PAC-搜索概率来 derive一些泛化约束，并证明了这些约束的优化性。</li>
<li>results: 本文的研究结果表明，使用这些泛化约束可以获得更加优化的泛化性能，并且可以在不同的维度上进行泛化。<details>
<summary>Abstract</summary>
We derive generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training and population loss. The bounds hold under the assumption that the cumulant-generating function (CGF) of the comparator is upper-bounded by the corresponding CGF within a family of bounding distributions. We show that the tightest possible bound is obtained with the comparator being the convex conjugate of the CGF of the bounding distribution, also known as the Cram\'er function. This conclusion applies more broadly to generalization bounds with a similar structure. This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses and leads to novel bounds under other bounding distributions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-optimal-integration-of-spatial-and-temporal-information-in-noisy-chemotaxis"><a href="#Learning-optimal-integration-of-spatial-and-temporal-information-in-noisy-chemotaxis" class="headerlink" title="Learning optimal integration of spatial and temporal information in noisy chemotaxis"></a>Learning optimal integration of spatial and temporal information in noisy chemotaxis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10531">http://arxiv.org/abs/2310.10531</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kirkegaardlab/chemoxrl">https://github.com/kirkegaardlab/chemoxrl</a></li>
<li>paper_authors: Albert Alonso, Julius B. Kirkegaard</li>
<li>for: 研究 chemotaxis 驱动 by spatial 和 temporal 估计的边界</li>
<li>methods: 使用 deep reinforcement learning 研究可以在不受限制的方式集成 spatial 和 temporal 信息</li>
<li>results: 发现 transition between regimes 是连续的，combined strategy 在过渡区域表现更好，并且 policy 听取的 gradient 信息是非rivial的组合。Is there anything else I can help with?<details>
<summary>Abstract</summary>
We investigate the boundary between chemotaxis driven by spatial estimation of gradients and chemotaxis driven by temporal estimation. While it is well known that spatial chemotaxis becomes disadvantageous for small organisms at high noise levels, it is unclear whether there is a discontinuous switch of optimal strategies or a continuous transition exists. Here, we employ deep reinforcement learning to study the possible integration of spatial and temporal information in an a priori unconstrained manner. We parameterize such a combined chemotactic policy by a recurrent neural network and evaluate it using a minimal theoretical model of a chemotactic cell. By comparing with constrained variants of the policy, we show that it converges to purely temporal and spatial strategies at small and large cell sizes, respectively. We find that the transition between the regimes is continuous, with the combined strategy outperforming in the transition region both the constrained variants as well as models that explicitly integrate spatial and temporal information. Finally, by utilizing the attribution method of integrated gradients, we show that the policy relies on a non-trivial combination of spatially and temporally derived gradient information in a ratio that varies dynamically during the chemotactic trajectories.
</details>
<details>
<summary>摘要</summary>
Translation into Simplified Chinese:我们研究 chemotaxis 驱动 by 空间估计 gradient 和 temporal 估计之间的边界。然而，已知的是，随着噪声水平的提高，小organism 中的 spatial chemotaxis 变得不利。但是，是否存在突然的优化策略转换，或者是一个连续的转换，这还未得到了解。我们使用深度学习来研究可能的空间和时间信息的集成，不受任何假设或限制。我们使用 recurrent neural network 来参数化这种合并的 chemotactic 政策，并使用一个最小的化学吸引细胞模型来评估它。我们比较了这种政策与受限的变种，发现它在小和大细胞尺度之间分别转换为纯 temporal 和空间策略，并且在这两个策略之间存在一个连续的转换。此外，我们还发现，在转换区域，合并策略比受限变种和显式地集成空间和时间信息的模型都更高效。最后，我们使用 integrated gradients 的归因方法，发现政策在化学追踪过程中动态变化的空间和时间信息的权重组合是非常复杂的。
</details></li>
</ul>
<hr>
<h2 id="From-Spectral-Theorem-to-Statistical-Independence-with-Application-to-System-Identification"><a href="#From-Spectral-Theorem-to-Statistical-Independence-with-Application-to-System-Identification" class="headerlink" title="From Spectral Theorem to Statistical Independence with Application to System Identification"></a>From Spectral Theorem to Statistical Independence with Application to System Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10523">http://arxiv.org/abs/2310.10523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdullah Naeem, Amir Khazraei, Miroslav Pajic</li>
<li>for: 这个论文是关于高维Random Dynamical Systems的研究，具体来说是研究这些系统的identification问题。</li>
<li>methods: 作者使用spectral theorem for non-Hermitian operators来研究系统的特征向量，并通过分析eigenvalues和eigenvectors来描述系统的特性。</li>
<li>results: 作者发现，当系统是稳定的时，系统的特征向量可以分解为多个lower dimensional Random Dynamical Systems，这些系统之间是独立的。此外，作者还发现，在这种情况下，covariates可能会受到维度的干扰，导致error的增加。<details>
<summary>Abstract</summary>
High dimensional random dynamical systems are ubiquitous, including -- but not limited to -- cyber-physical systems, daily return on different stocks of S&P 1500 and velocity profile of interacting particle systems around McKeanVlasov limit. Mathematically, underlying phenomenon can be captured via a stable $n$-dimensional linear transformation `$A$' and additive randomness. System identification aims at extracting useful information about underlying dynamical system, given a length $N$ trajectory from it (corresponds to an $n \times N$ dimensional data matrix). We use spectral theorem for non-Hermitian operators to show that spatio-temperal correlations are dictated by the discrepancy between algebraic and geometric multiplicity of distinct eigenvalues corresponding to state transition matrix. Small discrepancies imply that original trajectory essentially comprises of multiple lower dimensional random dynamical systems living on $A$ invariant subspaces and are statistically independent of each other. In the process, we provide first quantitative handle on decay rate of finite powers of state transition matrix $\|A^{k}\|$ . It is shown that when a stable dynamical system has only one distinct eigenvalue and discrepancy of $n-1$: $\|A\|$ has a dependence on $n$, resulting dynamics are spatially inseparable and consequently there exist at least one row with covariates of typical size $\Theta\big(\sqrt{N-n+1}$ $e^{n}\big)$ i.e., even under stability assumption, covariates can suffer from curse of dimensionality. In the light of these findings we set the stage for non-asymptotic error analysis in estimation of state transition matrix $A$ via least squares regression on observed trajectory by showing that element-wise error is essentially a variant of well-know Littlewood-Offord problem.
</details>
<details>
<summary>摘要</summary>
高维Random动力系统广泛存在，包括但不限于Cyber-Physical Systems、每天不同股票S&P 1500的回报和Interacting Particle Systems around McKeanVlasov limit的速度 Profile。数学上，下面的现象可以通过一个稳定的$n$-维线性变换'$A$'和随机性来捕捉。系统识别目标是从这个系统中提取有用的信息，了解下面的动力系统。我们使用非 hermitian 算子的特征定理来证明，在空间-时间 correlations 中，存在一些独特的多个低维Random dynamical systems 在 $A$  invariable subspaces 中生活，这些系统是独立的。在这个过程中，我们提供了第一个量化的把握，以及 $\|A^{k}\|$ 的衰减率。当一个稳定的动力系统只有一个独特的征值，并且差值为 $n-1$，则 $\|A\|$ 具有对 $n$ 的依赖关系，结果的动力系统是无法分离的。因此，存在至少一行具有特点大小 $\Theta\big(\sqrt{N-n+1}$ $e^{n}\big)$ 的covariates，即，even under stability assumption，covariates 可能会受到维度约束。在这些发现的基础上，我们设置了非对数学术的错误分析在 $A$ 的最小二乘回归中，并证明了元素级别的错误是一种变种的 Littlewood-Offord 问题。
</details></li>
</ul>
<hr>
<h2 id="Reproducing-Bayesian-Posterior-Distributions-for-Exoplanet-Atmospheric-Parameter-Retrievals-with-a-Machine-Learning-Surrogate-Model"><a href="#Reproducing-Bayesian-Posterior-Distributions-for-Exoplanet-Atmospheric-Parameter-Retrievals-with-a-Machine-Learning-Surrogate-Model" class="headerlink" title="Reproducing Bayesian Posterior Distributions for Exoplanet Atmospheric Parameter Retrievals with a Machine Learning Surrogate Model"></a>Reproducing Bayesian Posterior Distributions for Exoplanet Atmospheric Parameter Retrievals with a Machine Learning Surrogate Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10521">http://arxiv.org/abs/2310.10521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyup B. Unlu, Roy T. Forestano, Konstantin T. Matchev, Katia Matcheva</li>
<li>for: 这个论文是为了实现基于机器学习的 posterior 分布模型，用于重现通过掩蔽行星的谱 spectra 获得的外层星球大气参数的 Bayesian  posterior distributions。</li>
<li>methods: 该模型使用了适应性学习和半监督学习，以便利用大量的无标注训练数据。它还进行了领域适应的特征处理，以提高模型性能。</li>
<li>results: 该模型在2023年 Ariel 机器学习数据挑战中获得了优胜解决方案。<details>
<summary>Abstract</summary>
We describe a machine-learning-based surrogate model for reproducing the Bayesian posterior distributions for exoplanet atmospheric parameters derived from transmission spectra of transiting planets with typical retrieval software such as TauRex. The model is trained on ground truth distributions for seven parameters: the planet radius, the atmospheric temperature, and the mixing ratios for five common absorbers: $H_2O$, $CH_4$, $NH_3$, $CO$ and $CO_2$. The model performance is enhanced by domain-inspired preprocessing of the features and the use of semi-supervised learning in order to leverage the large amount of unlabelled training data available. The model was among the winning solutions in the 2023 Ariel Machine Learning Data Challenge.
</details>
<details>
<summary>摘要</summary>
我们描述了一种基于机器学习的代理模型，用于重现吸收 спектроскопии中探测到的外层星球大气参数的 bayesian posterior distribution。该模型使用了常用的恢复软件 such as TauRex，并在七个参数上进行了训练：星球半径、大气温度以及五种常见吸收物的混合率：$H_2O$, $CH_4$, $NH_3$, $CO$ 和 $CO_2$。通过域名预处理和使用半监督学习，我们提高了模型的性能，并利用了大量的无标注训练数据。该模型在2023年的Ariel机器学习数据挑战中获得了奖励。
</details></li>
</ul>
<hr>
<h2 id="ReMax-A-Simple-Effective-and-Efficient-Reinforcement-Learning-Method-for-Aligning-Large-Language-Models"><a href="#ReMax-A-Simple-Effective-and-Efficient-Reinforcement-Learning-Method-for-Aligning-Large-Language-Models" class="headerlink" title="ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"></a>ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10505">http://arxiv.org/abs/2310.10505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liziniu/ReMax">https://github.com/liziniu/ReMax</a></li>
<li>paper_authors: Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo</li>
<li>for: 本研究旨在提高RLHF任务中的训练效率，并解决PPO算法的计算效率问题。</li>
<li>methods: 本研究提出了一种新的RLHF算法 called ReMax，基于REINFORCE算法，并具有一种新的减少方差技术。</li>
<li>results: ReMax比PPO具有三大优点：首先，ReMax简单实现，消除了多个超参数，减少了训练时间和精度优化的努力。其次，ReMax减少了50%的内存使用量，可以在8xA100-40GB GPU上训练Llama2（7B）模型。最后，ReMax比PPO快2倍，不降低性能。<details>
<summary>Abstract</summary>
Alignment is of critical importance for training large language models (LLMs). The predominant strategy to address this is through Reinforcement Learning from Human Feedback (RLHF), where PPO serves as the de-facto algorithm. Yet, PPO is known to suffer from computational inefficiency, which is a challenge that this paper aims to address. We identify three important properties in RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on such observations, we develop a new algorithm tailored for RLHF, called ReMax. The algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is equipped with a new variance-reduction technique.   Our method has three-fold advantages over PPO: first, ReMax is simple to implement and removes many hyper-parameters in PPO, which are scale-sensitive and laborious to tune. Second, ReMax saves about 50% memory usage in principle. As a result, PPO runs out-of-memory when fine-tuning a Llama2 (7B) model on 8xA100-40GB GPUs, whereas ReMax can afford training. This memory improvement is achieved by removing the value model in PPO. Third, based on our calculations, we find that even assuming PPO can afford the training of Llama2 (7B), it would still run about 2x slower than ReMax. This is due to the computational overhead of the value model, which does not exist in ReMax. Importantly, the above computational improvements do not sacrifice the performance. We hypothesize these advantages can be maintained in larger-scaled models. Our implementation of ReMax is available at https://github.com/liziniu/ReMax
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>大量语言模型（LLM）的训练需要Alignment是关键。现有的主流策略是通过人类反馈学习（RLHF），其中PPO serves as the de-facto algorithm。然而，PPO知道 suffer from computational inefficiency，这是这篇文章的目标。我们确定了RLHF任务中的三个重要特性：快速的模拟，决定性的转移和轨迹级别的奖励，这些特性在PPO中未被利用。基于这些观察，我们开发了一种适合RLHF的新算法，called ReMax。ReMax的算法设计基于celebrated algorithm REINFORCE，但具有一种新的减少偏移技术。  我们的方法有三个优势：首先，ReMax简单实现，消除了PPO中许多参数，这些参数是敏感度和耗时consuming。其次，ReMax将减少约50%的内存使用量。这使得PPO在精度级别的模型（7B）上的8xA100-40GB GPU上进行精度级别的模型（7B）上进行精度级别的训练时出现内存不足问题，而ReMax可以进行训练。这种内存改进是通过 removing the value model in PPO 来实现的。第三，基于我们的计算，即使PPO可以训练Llama2（7B），它仍然会比ReMax约2倍 slower。这是因为值模型在PPO中的计算 overhead，不存在在ReMax中。重要的是，上述计算改进不会减少性能。我们认为这些优势可以在更大的模型 scale 中被维持。我们的 ReMax 实现可以在 <https://github.com/liziniu/ReMax> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-Learning-Patterns-in-Financial-Time-Series-for-Trend-Following-Strategies"><a href="#Few-Shot-Learning-Patterns-in-Financial-Time-Series-for-Trend-Following-Strategies" class="headerlink" title="Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies"></a>Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10500">http://arxiv.org/abs/2310.10500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieran Wood, Samuel Kessler, Stephen J. Roberts, Stefan Zohren</li>
<li>for: 这个论文是为了提出一种能够快速适应金融市场变化的时间序列趋势预测模型，以避免在金融市场突然变化时出现的损失。</li>
<li>methods: 该模型使用了深度学习的最新进展，特别是几拟学习，以及时间序列趋势预测模型。</li>
<li>results: 该模型在2018-2023年的紧张市场期间，相比 neural forecaster 和时间序列势力策略，提高了18.9%的谭瑞比，并且在 COVID-19 下落期间， doubles 快速恢复。此外，该模型还可以对新的金融资产进行零 shot 位置，并且与 neural time-series trend forecaster 相比，在同一时间期内提高了5倍的谭瑞比。<details>
<summary>Abstract</summary>
Forecasting models for systematic trading strategies do not adapt quickly when financial market conditions change, as was seen in the advent of the COVID-19 pandemic in 2020, when market conditions changed dramatically causing many forecasting models to take loss-making positions. To deal with such situations, we propose a novel time-series trend-following forecaster that is able to quickly adapt to new market conditions, referred to as regimes. We leverage recent developments from the deep learning community and use few-shot learning. We propose the Cross Attentive Time-Series Trend Network - X-Trend - which takes positions attending over a context set of financial time-series regimes. X-Trend transfers trends from similar patterns in the context set to make predictions and take positions for a new distinct target regime. X-Trend is able to quickly adapt to new financial regimes with a Sharpe ratio increase of 18.9% over a neural forecaster and 10-fold over a conventional Time-series Momentum strategy during the turbulent market period from 2018 to 2023. Our strategy recovers twice as quickly from the COVID-19 drawdown compared to the neural-forecaster. X-Trend can also take zero-shot positions on novel unseen financial assets obtaining a 5-fold Sharpe ratio increase versus a neural time-series trend forecaster over the same period. X-Trend both forecasts next-day prices and outputs a trading signal. Furthermore, the cross-attention mechanism allows us to interpret the relationship between forecasts and patterns in the context set.
</details>
<details>
<summary>摘要</summary>
预测模型 для系统性交易策略不快适应金融市场条件变化，例如2020年COVID-19大流行期间，市场条件快速变化，许多预测模型亏损。为解决这种情况，我们提出了一种新的时间序列趋势预测器，可以快速适应新的市场条件，称为“ régime”。我们利用了最新的深度学习社区的进展，并使用几何学学习。我们提出了跨注意力时间序列趋势网络（X-Trend），它在一个上下文集中注意力分配位置，并将趋势从类似的模式传递到新目标 régime 中进行预测和交易。X-Trend 能快速适应新的金融 régime，其肖特比（Sharpe ratio）提高18.9%于神经预测器和10倍于传统时间序列势力策略在2018-2023年的混乱市场期间。我们的策略在COVID-19下滑期间复制两倍于神经预测器。X-Trend 还可以在未看到的金融资产上出现零shot位置，其肖特比提高5倍于神经时间序列趋势预测器在同一时间期。X-Trend 同时预测下一天的价格和输出交易信号。此外，跨注意力机制允许我们解释预测和上下文集中的模式之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Passive-Inference-Attacks-on-Split-Learning-via-Adversarial-Regularization"><a href="#Passive-Inference-Attacks-on-Split-Learning-via-Adversarial-Regularization" class="headerlink" title="Passive Inference Attacks on Split Learning via Adversarial Regularization"></a>Passive Inference Attacks on Split Learning via Adversarial Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10483">http://arxiv.org/abs/2310.10483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochen Zhu, Xinjian Luo, Yuncheng Wu, Yangfan Jiang, Xiaokui Xiao, Beng Chin Ooi</li>
<li>for: 这个研究旨在攻击 Split Learning (SL) 的实际和有效替代方案。</li>
<li>methods: 这个研究引入了一个名为 SDAR 的攻击框架，这个框架使用辅助数据和敌对调整来学习一个可以实时重建客户端私人模型的可靠模拟器。</li>
<li>results: 实验结果显示，在实际且实用的攻击enario中，SDAR 能够实时重建客户端私人数据，并在 U-shaped SL 中重建数据和标签。在 CIFAR-10 上，在深度分割水平 7 下，SDAR 能够实现私人数据重建的 mean squared error 小于 0.025，并在 U-shaped SL 中 дости得标签推论精度高于 98%。<details>
<summary>Abstract</summary>
Split Learning (SL) has emerged as a practical and efficient alternative to traditional federated learning. While previous attempts to attack SL have often relied on overly strong assumptions or targeted easily exploitable models, we seek to develop more practical attacks. We introduce SDAR, a novel attack framework against SL with an honest-but-curious server. SDAR leverages auxiliary data and adversarial regularization to learn a decodable simulator of the client's private model, which can effectively infer the client's private features under the vanilla SL, and both features and labels under the U-shaped SL. We perform extensive experiments in both configurations to validate the effectiveness of our proposed attacks. Notably, in challenging but practical scenarios where existing passive attacks struggle to reconstruct the client's private data effectively, SDAR consistently achieves attack performance comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR achieves private feature reconstruction with less than 0.025 mean squared error in both the vanilla and the U-shaped SL, and attains a label inference accuracy of over 98% in the U-shaped setting, while existing attacks fail to produce non-trivial results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adaptive-Neural-Ranking-Framework-Toward-Maximized-Business-Goal-for-Cascade-Ranking-Systems"><a href="#Adaptive-Neural-Ranking-Framework-Toward-Maximized-Business-Goal-for-Cascade-Ranking-Systems" class="headerlink" title="Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems"></a>Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10462">http://arxiv.org/abs/2310.10462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunli Wang, Zhiqiang Wang, Jian Yang, Shiyang Wen, Dongying Kong, Han Li, Kun Gai</li>
<li>for: 这个论文主要针对大规模top-k选择问题中的涨幅排序系统优化，具体来说是通过学习排序来优化模型。</li>
<li>methods: 该论文提出了一种基于多任务学习框架的 Adaptive Neural Ranking Framework，通过将relaxed和完整的目标优化并 combinely，使得优化目标适应不同数据复杂度和模型能力。</li>
<li>results: 实验结果表明，该方法在4个公共和商业benchmark上表现出色，并且在线上实验中具有显著的应用价值。<details>
<summary>Abstract</summary>
Cascade ranking is widely used for large-scale top-k selection problems in online advertising and recommendation systems, and learning-to-rank is an important way to optimize the models in cascade ranking systems. Previous works on learning-to-rank usually focus on letting the model learn the complete order or pay more attention to the order of top materials, and adopt the corresponding rank metrics as optimization targets. However, these optimization targets can not adapt to various cascade ranking scenarios with varying data complexities and model capabilities; and the existing metric-driven methods such as the Lambda framework can only optimize a rough upper bound of the metric, potentially resulting in performance misalignment. To address these issues, we first propose a novel perspective on optimizing cascade ranking systems by highlighting the adaptability of optimization targets to data complexities and model capabilities. Concretely, we employ multi-task learning framework to adaptively combine the optimization of relaxed and full targets, which refers to metrics Recall@m@k and OAP respectively. Then we introduce a permutation matrix to represent the rank metrics and employ differentiable sorting techniques to obtain a relaxed permutation matrix with controllable approximate error bound. This enables us to optimize both the relaxed and full targets directly and more appropriately using the proposed surrogate losses within the deep learning framework. We named this method as Adaptive Neural Ranking Framework. We use the NeuralSort method to obtain the relaxed permutation matrix and draw on the uncertainty weight method in multi-task learning to optimize the proposed losses jointly. Experiments on a total of 4 public and industrial benchmarks show the effectiveness and generalization of our method, and online experiment shows that our method has significant application value.
</details>
<details>
<summary>摘要</summary>
cascade ranking 广泛应用于大规模 top-k 选择问题中，学习 rank 是一种重要的优化方法。前一些工作通常是让模型学习完整的排序或更加注重 top Materials 的排序，采用相应的rank metric作为优化目标。然而，这些优化目标无法适应不同的排序场景中的数据复杂性和模型能力；而现有的 metric-driven 方法，如Lambda框架，只能优化一个粗略的上界，可能导致性能不符。为解决这些问题，我们首先提出一种新的视角，即优化 cascade ranking 系统的可适应性。具体来说，我们使用多任务学习框架来适应性地组合优化 relaxed 和 full 目标。relaxed 目标指的是 recall@m@k 和 OAP metric，而 full 目标则是完整的排序。然后，我们引入排序矩阵来表示排序 metric，并使用可微排序技术来获得一个可控的相对误差 bound。这使得我们可以直接优化 relaxed 和 full 目标，并更加合适地使用我们提出的代理损失函数在深度学习框架中进行优化。我们称这种方法为 Adaptive Neural Ranking Framework。我们使用 NeuralSort 方法来获得 relaxed 排序矩阵，并在多任务学习中使用不确定性权重来优化我们的提出的损失函数。实验结果显示，我们的方法在四个公共和工业标准准中表现出色，并且在实际应用中具有显著的价值。
</details></li>
</ul>
<hr>
<h2 id="A-Geometric-Insight-into-Equivariant-Message-Passing-Neural-Networks-on-Riemannian-Manifolds"><a href="#A-Geometric-Insight-into-Equivariant-Message-Passing-Neural-Networks-on-Riemannian-Manifolds" class="headerlink" title="A Geometric Insight into Equivariant Message Passing Neural Networks on Riemannian Manifolds"></a>A Geometric Insight into Equivariant Message Passing Neural Networks on Riemannian Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10448">http://arxiv.org/abs/2310.10448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilyes Batatia</li>
<li>for: 本文提出了一种 geometric 的思路，用于解释 equivariant message passing 在 Riemannian manifold 上的实现。</li>
<li>methods: 作者使用 coordinate-independent feature fields 表示数据的 numerical features，并将其映射到主bundle 上的 equivariant embedding 中。然后，他们提出一种优化 Polyakov action 的方法，以确保 embedding 中的 metric 与原始 metric 相似。</li>
<li>results: 作者提出了一种基于 equivariant diffusion process 的 message passing scheme，可以在 manifold 上实现。此外，他们还提出了一种基于高阶 equivariant diffusion process 的新的一般化 GNN 模型，可以扩展 ACE 和 MACE  formalism 到 Riemannian manifold 上的数据。<details>
<summary>Abstract</summary>
This work proposes a geometric insight into equivariant message passing on Riemannian manifolds. As previously proposed, numerical features on Riemannian manifolds are represented as coordinate-independent feature fields on the manifold. To any coordinate-independent feature field on a manifold comes attached an equivariant embedding of the principal bundle to the space of numerical features. We argue that the metric this embedding induces on the numerical feature space should optimally preserve the principal bundle's original metric. This optimality criterion leads to the minimization of a twisted form of the Polyakov action with respect to the graph of this embedding, yielding an equivariant diffusion process on the associated vector bundle. We obtain a message passing scheme on the manifold by discretizing the diffusion equation flow for a fixed time step. We propose a higher-order equivariant diffusion process equivalent to diffusion on the cartesian product of the base manifold. The discretization of the higher-order diffusion process on a graph yields a new general class of equivariant GNN, generalizing the ACE and MACE formalism to data on Riemannian manifolds.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种几何视角来理解在里曼尼投影上的平衡消息传递。在先前的提议中，数字特征在里曼尼投影上是作为独立坐标的特征场表示的。为任何独立特征场在投影上来说，有一个对称嵌入主 bundle 到特征空间的 equivariant 嵌入。我们 argue 这个嵌入应该保持原始主 bundle 的 metric 的最佳方式，这个标准导致了对 twisted 形式的 Polyakov 动作的最小化，从而获得一个 equivariant 扩散过程在关联的向量bundle 上。我们可以通过粘束扩散方程的离散来获得一个消息传递方案在投影上。我们提出了一种高阶不变扩散过程，与 cartesian product 的基 manifold 相等。离散这种高阶扩散过程在图上得到一个新的一般类型的不变 GNN，扩展了数据在里曼尼投影上的 ACE 和 MACE  formalism。
</details></li>
</ul>
<hr>
<h2 id="Taming-the-Sigmoid-Bottleneck-Provably-Argmaxable-Sparse-Multi-Label-Classification"><a href="#Taming-the-Sigmoid-Bottleneck-Provably-Argmaxable-Sparse-Multi-Label-Classification" class="headerlink" title="Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification"></a>Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10443">http://arxiv.org/abs/2310.10443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreasgrv/sigmoid-bottleneck">https://github.com/andreasgrv/sigmoid-bottleneck</a></li>
<li>paper_authors: Andreas Grivas, Antonio Vergari, Adam Lopez</li>
<li>for: 这篇论文是关于多标签分类任务中的sigmoid输出层，其中每个输入可以获得多个标签。</li>
<li>methods: 这篇论文使用了Discrete Fourier Transform（DFT）输出层，以确保所有稀疏的标签组合都是可arginmax的。</li>
<li>results: 论文表明，sigmoid输出层在多标签分类任务中会导致无法argmax的输出，并且可以通过使用DFT输出层来避免这种情况。DFT输出层比sigmoid输出层更快速地训练，并且具有更好的参数效率。<details>
<summary>Abstract</summary>
Sigmoid output layers are widely used in multi-label classification (MLC) tasks, in which multiple labels can be assigned to any input. In many practical MLC tasks, the number of possible labels is in the thousands, often exceeding the number of input features and resulting in a low-rank output layer. In multi-class classification, it is known that such a low-rank output layer is a bottleneck that can result in unargmaxable classes: classes which cannot be predicted for any input. In this paper, we show that for MLC tasks, the analogous sigmoid bottleneck results in exponentially many unargmaxable label combinations. We explain how to detect these unargmaxable outputs and demonstrate their presence in three widely used MLC datasets. We then show that they can be prevented in practice by introducing a Discrete Fourier Transform (DFT) output layer, which guarantees that all sparse label combinations with up to $k$ active labels are argmaxable. Our DFT layer trains faster and is more parameter efficient, matching the F1@k score of a sigmoid layer while using up to 50% fewer trainable parameters. Our code is publicly available at https://github.com/andreasgrv/sigmoid-bottleneck.
</details>
<details>
<summary>摘要</summary>
希格迪输出层在多标签分类(MLC)任务中广泛使用，在任务中任何输入都可以获得多个标签。在实际应用中，可能有数천个可能的标签，常常超过输入特征的数量，导致输出层的低级排名。在多类分类中，这种低级输出层会导致不可预测的类：无法预测的类。在这篇论文中，我们表明MLC任务中的希格迪瓶颈会导致无数多个不可预测的标签组合。我们解释了如何检测这些不可预测的输出和三个常用的MLC数据集中其存在。然后我们表明可以通过引入离散傅里叶变换(DFT)输出层来避免这些不可预测的输出。我们的DFT层在训练时更快，并且使用更少的可训练参数，与希格迪层的F1@k分数相同，而使用的参数数量可以减少到50%。我们的代码可以在https://github.com/andreasgrv/sigmoid-bottleneck上获取。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Matrix-Function-Neural-Networks"><a href="#Equivariant-Matrix-Function-Neural-Networks" class="headerlink" title="Equivariant Matrix Function Neural Networks"></a>Equivariant Matrix Function Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10434">http://arxiv.org/abs/2310.10434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilyes Batatia, Lars L. Schaaf, Huajie Chen, Gábor Csányi, Christoph Ortner, Felix A. Faber</li>
<li>for:  This paper aims to address the challenges of modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials using Graph Neural Networks (GNNs) and traditional neural networks.</li>
<li>methods:  The paper introduces a novel architecture called Matrix Function Neural Networks (MFNs), which parameterizes non-local interactions through analytic matrix equivariant functions. The MFN architecture uses resolvent expansions for a straightforward implementation and the potential for linear scaling with system size.</li>
<li>results:  The MFN architecture achieves state-of-the-art performance in standard graph benchmarks, such as the ZINC and TU datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials. Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The MFN architecture achieves state-of-the-art performance in standard graph benchmarks, such as the ZINC and TU datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields.
</details>
<details>
<summary>摘要</summary>
图形神经网络（GNNs），特别是消息传递神经网络（MPNNs），在不同应用场景中显示出了强大的架构能力。然而，MPNNs在大 conjugated molecules、金属和归一化材料等系统中模型非本地交互时面临挑战。虽然spectral GNNs和传统神经网络如回归神经网络和transformers可以减轻这些挑战，但它们经常缺乏广泛性、适应性、普适性、计算效率或失去数据中的细致结构关系或对称性。为解决这些问题，我们介绍了矩阵函数神经网络（MFNs），一种新的架构，该参数非本地交互通过矩阵对偶变换函数。使用resolvent expansions的实现可以提供一种简单的实现方式，并且可能实现系统大小的线性扩展。MFN架构在标准图形数据集上达到了state-of-the-art性能，如ZINC和TU数据集，并能够捕捉到量子系统中的复杂非本地交互，为新的state-of-the-art力场开创道路。
</details></li>
</ul>
<hr>
<h2 id="Continuously-Adapting-Random-Sampling-CARS-for-Power-Electronics-Parameter-Design"><a href="#Continuously-Adapting-Random-Sampling-CARS-for-Power-Electronics-Parameter-Design" class="headerlink" title="Continuously Adapting Random Sampling (CARS) for Power Electronics Parameter Design"></a>Continuously Adapting Random Sampling (CARS) for Power Electronics Parameter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10425">http://arxiv.org/abs/2310.10425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Happel, Philipp Brendel, Andreas Rosskopf, Stefan Ditze</li>
<li>for: 这个论文主要针对的是电子能源参数设计任务的优化问题，通常使用详细的优化方法或者笨拙的搜索方法来解决。</li>
<li>methods: 该论文提出了一种新的方法 named “Continuously Adapting Random Sampling” (CARS)，它提供了一种连续的方法，位于详细优化方法和笨拙搜索方法之间。这种方法可以快速地进行大量的 simulations，同时逐渐增加关注最有前途的参数范围。这个方法 Draws inspiration from multi-armed bandit research and leads to prioritized sampling of sub-domains in one high-dimensional parameter tensor。</li>
<li>results: 该论文对三个例子的电子能源使用情况进行了评估，得到的设计与遗传算法相当竞争力，同时具有高度并行化的 simulate 特点和不断进行探索和利用设置之间的融合。<details>
<summary>Abstract</summary>
To date, power electronics parameter design tasks are usually tackled using detailed optimization approaches with detailed simulations or using brute force grid search grid search with very fast simulations. A new method, named "Continuously Adapting Random Sampling" (CARS) is proposed, which provides a continuous method in between. This allows for very fast, and / or large amounts of simulations, but increasingly focuses on the most promising parameter ranges. Inspirations are drawn from multi-armed bandit research and lead to prioritized sampling of sub-domains in one high-dimensional parameter tensor. Performance has been evaluated on three exemplary power electronic use-cases, where resulting designs appear competitive to genetic algorithms, but additionally allow for highly parallelizable simulation, as well as continuous progression between explorative and exploitative settings.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：迄今，电力电子参数设计任务通常使用详细优化方法或使用劳顿搜索法，均采用详细的simulation。一种新的方法，名为“连续适应随机抽样”（CARS）被提议，它提供了一种连续的方法，位于详细优化和劳顿搜索之间。这使得可以很快、或者进行大量的simulation，但是逐渐关注最有前途的参数范围。 draw inspirations from multi-armed bandit research and lead to prioritized sampling of sub-domains in one high-dimensional parameter tensor。 performance has been evaluated on three exemplary power electronic use-cases, where resulting designs appear competitive to genetic algorithms, but additionally allow for highly parallelizable simulation, as well as continuous progression between explorative and exploitative settings。</SYS>Here is the translation of the text into Simplified Chinese:迄今，电力电子参数设计任务通常使用详细优化方法或使用劳顿搜索法，均采用详细的simulation。一种新的方法，名为“连续适应随机抽样”（CARS）被提议，它提供了一种连续的方法，位于详细优化和劳顿搜索之间。这使得可以很快、或者进行大量的simulation，但是逐渐关注最有前途的参数范围。 draw inspirations from multi-armed bandit research and lead to prioritized sampling of sub-domains in one high-dimensional parameter tensor。 performance has been evaluated on three exemplary power electronic use-cases, where resulting designs appear competitive to genetic algorithms, but additionally allow for highly parallelizable simulation, as well as continuous progression between explorative and exploitative settings。
</details></li>
</ul>
<hr>
<h2 id="Towards-Fair-and-Calibrated-Models"><a href="#Towards-Fair-and-Calibrated-Models" class="headerlink" title="Towards Fair and Calibrated Models"></a>Towards Fair and Calibrated Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10399">http://arxiv.org/abs/2310.10399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Vipul Rathore, Mausam, Parag Singla</li>
<li>for: 建立不偏袋化和准确的机器学习模型</li>
<li>methods: 使用特定定义的公平性、抽象和解释性，并提出了一种基于温度 scaling的简单预处理技术和修改现有抽象损失来实现公平和准确的模型</li>
<li>results: 通过对多种数据集进行广泛实验，发现这些技术可以实现公平和准确的模型，并提供了对模型的解释和分析。<details>
<summary>Abstract</summary>
Recent literature has seen a significant focus on building machine learning models with specific properties such as fairness, i.e., being non-biased with respect to a given set of attributes, calibration i.e., model confidence being aligned with its predictive accuracy, and explainability, i.e., ability to be understandable to humans. While there has been work focusing on each of these aspects individually, researchers have shied away from simultaneously addressing more than one of these dimensions. In this work, we address the problem of building models which are both fair and calibrated. We work with a specific definition of fairness, which closely matches [Biswas et. al. 2019], and has the nice property that Bayes optimal classifier has the maximum possible fairness under our definition. We show that an existing negative result towards achieving a fair and calibrated model [Kleinberg et. al. 2017] does not hold for our definition of fairness. Further, we show that ensuring group-wise calibration with respect to the sensitive attributes automatically results in a fair model under our definition. Using this result, we provide a first cut approach for achieving fair and calibrated models, via a simple post-processing technique based on temperature scaling. We then propose modifications of existing calibration losses to perform group-wise calibration, as a way of achieving fair and calibrated models in a variety of settings. Finally, we perform extensive experimentation of these techniques on a diverse benchmark of datasets, and present insights on the pareto-optimality of the resulting solutions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Logistic-softmax-Likelihood-in-Bayesian-Meta-Learning-for-Few-Shot-Classification"><a href="#Revisiting-Logistic-softmax-Likelihood-in-Bayesian-Meta-Learning-for-Few-Shot-Classification" class="headerlink" title="Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification"></a>Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10379">http://arxiv.org/abs/2310.10379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/keanson/revisit-logistic-softmax">https://github.com/keanson/revisit-logistic-softmax</a></li>
<li>paper_authors: Tianjun Ke, Haoqun Cao, Zenan Ling, Feng Zhou</li>
<li>for: 这个论文主要研究了如何使用逻辑-软泛函数来提高几个shot分类（FSC）中的不确定性评估和性能。</li>
<li>methods: 该论文使用了 bayesian 方法来 caracterize  uncertainty in FSC，并使用了修改后的逻辑-软泛函数来控制先前不确定性的问题。</li>
<li>results: 该论文通过 theoretically 和 empirically 表明，修改后的逻辑-软泛函数可以提高 uncertainty 估计的准确性和性能，并且可以在标准 benchmark 数据集上达到或超过相同水平。<details>
<summary>Abstract</summary>
Meta-learning has demonstrated promising results in few-shot classification (FSC) by learning to solve new problems using prior knowledge. Bayesian methods are effective at characterizing uncertainty in FSC, which is crucial in high-risk fields. In this context, the logistic-softmax likelihood is often employed as an alternative to the softmax likelihood in multi-class Gaussian process classification due to its conditional conjugacy property. However, the theoretical property of logistic-softmax is not clear and previous research indicated that the inherent uncertainty of logistic-softmax leads to suboptimal performance. To mitigate these issues, we revisit and redesign the logistic-softmax likelihood, which enables control of the \textit{a priori} confidence level through a temperature parameter. Furthermore, we theoretically and empirically show that softmax can be viewed as a special case of logistic-softmax and logistic-softmax induces a larger family of data distribution than softmax. Utilizing modified logistic-softmax, we integrate the data augmentation technique into the deep kernel based Gaussian process meta-learning framework, and derive an analytical mean-field approximation for task-specific updates. Our approach yields well-calibrated uncertainty estimates and achieves comparable or superior results on standard benchmark datasets. Code is publicly available at \url{https://github.com/keanson/revisit-logistic-softmax}.
</details>
<details>
<summary>摘要</summary>
<SYS>使用适应学习的方法可以在几个批处理（Few-shot Classification，FSC）中表现出色，因为它可以通过之前的知识来解决新的问题。 bayesian方法可以准确地描述 FSC 中的uncertainty，这对于高风险领域非常重要。在这种情况下，通常使用Logistic-softmax概率 Distribution来取代Softmax概率 Distribution，因为它们具有 conditional conjugacy 性质。然而，Logistic-softmax的理论性不够清楚，而且先前的研究表明，Logistic-softmax的内在不确定性会导致表现下降。为了解决这些问题，我们重新访问和重新设计Logistic-softmax概率 Distribution，这使得可以通过温度参数控制 \textit{a priori} 信任水平。此外，我们还证明了Softmax可以视为Logistic-softmax的特殊情况，Logistic-softmax可以生成更大的数据分布Family。通过修改Logistic-softmax，我们将数据扩展技术集成到深度kernel基于Gaussian Process meta-学习框架中，并 derive了analytical mean-field Approximation for task-specific updates。我们的方法可以提供Well-calibrated uncertainty estimates，并在标准 benchmark datasets上实现了相对或superior的Results。相关代码可以在 <https://github.com/keanson/revisit-logistic-softmax> 上获取。</SYS>I hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Multi-Factor-Spatio-Temporal-Prediction-based-on-Graph-Decomposition-Learning"><a href="#Multi-Factor-Spatio-Temporal-Prediction-based-on-Graph-Decomposition-Learning" class="headerlink" title="Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition Learning"></a>Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10374">http://arxiv.org/abs/2310.10374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Ji, Jingyuan Wang, Yu Mou, Cheng Long</li>
<li>for: 本文提出了一种多因素空间时间预测任务，用于预测不同因素的空间时间数据的发展趋势。</li>
<li>methods: 本文提出了一种基于层次分解策略的 theoretically 有效的方法，以及一种名为空间时间图分解学习（STGDL）的模型无关框架。STGDL 包括两个主要组成部分：自动图分解模块和分解学习网络。</li>
<li>results: 对四个实际的空间时间数据集进行了广泛的实验，结果显示，使用本文提出的方法可以significantly 降低不同模型的预测错误率，最高降低到35.36%。此外，一个案例研究也表明了本方法的可解释性潜力。<details>
<summary>Abstract</summary>
Spatio-temporal (ST) prediction is an important and widely used technique in data mining and analytics, especially for ST data in urban systems such as transportation data. In practice, the ST data generation is usually influenced by various latent factors tied to natural phenomena or human socioeconomic activities, impacting specific spatial areas selectively. However, existing ST prediction methods usually do not refine the impacts of different factors, but directly model the entangled impacts of multiple factors. This amplifies the modeling complexity of ST data and compromises model interpretability. To this end, we propose a multi-factor ST prediction task that predicts partial ST data evolution under different factors, and combines them for a final prediction. We make two contributions to this task: an effective theoretical solution and a portable instantiation framework. Specifically, we first propose a theoretical solution called decomposed prediction strategy and prove its effectiveness from the perspective of information entropy theory. On top of that, we instantiate a novel model-agnostic framework, named spatio-temporal graph decomposition learning (STGDL), for multi-factor ST prediction. The framework consists of two main components: an automatic graph decomposition module that decomposes the original graph structure inherent in ST data into subgraphs corresponding to different factors, and a decomposed learning network that learns the partial ST data on each subgraph separately and integrates them for the final prediction. We conduct extensive experiments on four real-world ST datasets of two types of graphs, i.e., grid graph and network graph. Results show that our framework significantly reduces prediction errors of various ST models by 9.41% on average (35.36% at most). Furthermore, a case study reveals the interpretability potential of our framework.
</details>
<details>
<summary>摘要</summary>
这是一个很重要的数据探索和分析技术，尤其是在城市系统中的交通数据。在实践中，这些数据通常受到自然现象或人类社会经济活动的多种隐藏因素影响，这些因素影响特定的空间区域选择性地。然而，现有的这些预测方法通常不会细分这些因素的影响，而是直接模型这些杂糅的影响。这会增加这些数据的预测复杂性和模型解释性。为了解决这个问题，我们提出了一个多因素预测任务，预测不同因素的部分预测结果，然后结合它们进行最终预测。我们做出了两个贡献：一个有效的理论解决方案和一个可携的实现框架。具体来说，我们首先提出了一个名为分解预测策略的理论解决方案，并证明其有效性从信息熵理论的角度。而在这个基础上，我们实现了一个名为类型-独立预测架构（STGDL）的新模型独立框架，这个框架包括两个主要 ком成分：一个自动对应图解析模组，将原始的图структуре组织体内的ST数据分解为不同因素的子图，以及一个分解学网络，这个学网络在每个子图上进行分解预测，然后将它们结合进行最终预测。我们对四个真实世界的ST数据集进行了广泛的实验，结果显示，我们的框架可以对不同的ST模型进行预测，将预测错误量降低了9.41%的平均值（最高到35.36%）。此外，一个实验显示了我们的框架的解释能力。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-in-physics-a-short-guide"><a href="#Machine-learning-in-physics-a-short-guide" class="headerlink" title="Machine learning in physics: a short guide"></a>Machine learning in physics: a short guide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10368">http://arxiv.org/abs/2310.10368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/franciscorodrigues-usp/MLP">https://github.com/franciscorodrigues-usp/MLP</a></li>
<li>paper_authors: Francisco A. Rodrigues</li>
<li>for: Physics field （物理领域）</li>
<li>methods: Machine learning（机器学习）</li>
<li>results: Causal inference, symbolic regression, deep learning（因果推理、符号回归、深度学习）Here’s a more detailed explanation of each point:1. for: The paper is written for the field of physics, specifically focusing on the applications of machine learning in physics.2. methods: The paper covers the main concepts of machine learning, including supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning.3. results: The paper discusses some of the principal applications of machine learning in physics and highlights the associated challenges and perspectives.I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Machine learning is a rapidly growing field with the potential to revolutionize many areas of science, including physics. This review provides a brief overview of machine learning in physics, covering the main concepts of supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning. We present some of the principal applications of machine learning in physics and discuss the associated challenges and perspectives.
</details>
<details>
<summary>摘要</summary>
机器学习是一个迅速成长的领域，拥有可能改革多个科学领域的潜力，包括物理学。本篇文章提供了物理学中机器学习的简要总览，涵盖主要概念的监督学习、无监督学习和强化学习，以及更特殊的主题，如 causal inference、符号回传和深度学习。我们介绍了物理学中机器学习的主要应用和相关挑战，以及未来的展望。
</details></li>
</ul>
<hr>
<h2 id="Advantages-of-Machine-Learning-in-Bus-Transport-Analysis"><a href="#Advantages-of-Machine-Learning-in-Bus-Transport-Analysis" class="headerlink" title="Advantages of Machine Learning in Bus Transport Analysis"></a>Advantages of Machine Learning in Bus Transport Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19810">http://arxiv.org/abs/2310.19810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirsadegh Roshanzamir</li>
<li>for: 这个研究旨在使用指导学习算法分析特拉特当地公共汽车系统的准时性。</li>
<li>methods: 该研究使用了各种指导学习算法，包括Python的Sci Kit Learn和Stats Models库，以建立准确的模型，能够预测任何一天是否会遵循公共汽车路线的时间标准。</li>
<li>results: 研究发现，指导学习算法最重要的考虑因素是公共汽车路线的效率，这对于改善公共汽车系统的性能提供了重要的洞察。<details>
<summary>Abstract</summary>
Supervised Machine Learning is an innovative method that aims to mimic human learning by using past experiences. In this study, we utilize supervised machine learning algorithms to analyze the factors that contribute to the punctuality of Tehran BRT bus system. We gather publicly available datasets of 2020 to 2022 from Municipality of Tehran to train and test our models. By employing various algorithms and leveraging Python's Sci Kit Learn and Stats Models libraries, we construct accurate models capable of predicting whether a bus route will meet the prescribed standards for on-time performance on any given day. Furthermore, we delve deeper into the decision-making process of each algorithm to determine the most influential factor it considers. This investigation allows us to uncover the key feature that significantly impacts the effectiveness of bus routes, providing valuable insights for improving their performance.
</details>
<details>
<summary>摘要</summary>
超vised机器学习是一种创新的方法，旨在模仿人类学习的方式，使用过去的经验。在这个研究中，我们使用超vised机器学习算法来分析特拉ن布特公共汽车系统的准时性因素。我们使用2020年至2022年公共数据集，来训练和测试我们的模型。通过使用不同的算法和利用Python的Sci Kit Learn和Stats Models库，我们构建了准确的模型，能够预测任何一天会否遵循指定的准时性标准。此外，我们还探究每个算法的决策过程，以确定它最重要的考虑因素。这些调查可以帮助我们找到影响公共汽车线路效果的关键特征，提供有价值的反馈，以提高其性能。
</details></li>
</ul>
<hr>
<h2 id="MgNO-Efficient-Parameterization-of-Linear-Operators-via-Multigrid"><a href="#MgNO-Efficient-Parameterization-of-Linear-Operators-via-Multigrid" class="headerlink" title="MgNO: Efficient Parameterization of Linear Operators via Multigrid"></a>MgNO: Efficient Parameterization of Linear Operators via Multigrid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19809">http://arxiv.org/abs/2310.19809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncai He, Xinliang Liu, Jinchao Xu</li>
<li>for: 这个论文旨在提出一种简洁的神经网络架构，用于学习运算。</li>
<li>methods: 该方法使用了神经网络中的非线性运算层，其输出可以表示为 $\mathcal O_i(u) &#x3D; \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$。在这里，$\mathcal W_{ij}$ 是将 $j $- 个输入神经元连接到 $i $- 个输出神经元的半bounded线性算子，而偏置 $\mathcal B_{ij}$ 是一个函数而不是整数。</li>
<li>results: 该方法可以准确地解决不同类型的偏微分方程（PDEs），并且在训练时显示出了更高的易学性和更低的抗抑阻性。<details>
<summary>Abstract</summary>
In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting operators typically required in previous neural operators. Moreover, it seamlessly accommodates diverse boundary conditions. Our empirical observations reveal that MgNO exhibits superior ease of training compared to other CNN-based models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators. We demonstrate the efficiency and accuracy of our method with consistently state-of-the-art performance on different types of partial differential equations (PDEs).
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种简洁神经操作架构，用于神经网络学习。我们将神经操作定义为：输出第i个神经元的非线性操作层的输出为： $\mathcal O_i(u) = \sigma\left(\sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. 其中， $\mathcal W_{ij}$ 表示连接第j个输入神经元到第i个输出神经元的稍尺度的线性操作，而偏置 $\mathcal B_{ij}$ 是一个函数而非整数。由于这个新的通用近似性质，神经操作中的稍尺度线性操作之间的效率参数化（Banach空间）扮演了关键的角色。因此，我们引入MgNO，利用多重格struktur来参数这些线性操作。这种方法具有数学上的准确性和实际上的表达力。此外，MgNO可以自然地满足多种边界条件。我们的实验观察表明，MgNO比其他CNN基于模型更易于训练，同时也具有较少的折衔强度。我们通过不同类型的偏微分方程（PDEs）的实验表明了我们的方法的效率和准确性。
</details></li>
</ul>
<hr>
<h2 id="An-Anytime-Algorithm-for-Good-Arm-Identification"><a href="#An-Anytime-Algorithm-for-Good-Arm-Identification" class="headerlink" title="An Anytime Algorithm for Good Arm Identification"></a>An Anytime Algorithm for Good Arm Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10359">http://arxiv.org/abs/2310.10359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Jourdan, Clémence Réda</li>
<li>for: 这 paper 的目的是解决在固定预算和时间限制下的好臂标识问题（GAI）。</li>
<li>methods: 这 paper 提出了一种无参数和时间自适应的采样规则，称为 APGAI，可以在固定信度和预算设置下使用。</li>
<li>results: 作者提供了关于 APGAI 的Upper bound 的概率错误和预测采样复杂性的证明，以及实验结果表明 APGAI 在 synthetic 和实际数据上具有良好的表现。<details>
<summary>Abstract</summary>
In good arm identification (GAI), the goal is to identify one arm whose average performance exceeds a given threshold, referred to as good arm, if it exists. Few works have studied GAI in the fixed-budget setting, when the sampling budget is fixed beforehand, or the anytime setting, when a recommendation can be asked at any time. We propose APGAI, an anytime and parameter-free sampling rule for GAI in stochastic bandits. APGAI can be straightforwardly used in fixed-confidence and fixed-budget settings. First, we derive upper bounds on its probability of error at any time. They show that adaptive strategies are more efficient in detecting the absence of good arms than uniform sampling. Second, when APGAI is combined with a stopping rule, we prove upper bounds on the expected sampling complexity, holding at any confidence level. Finally, we show good empirical performance of APGAI on synthetic and real-world data. Our work offers an extensive overview of the GAI problem in all settings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified ChineseIn good arm identification (GAI), the goal is to identify one arm whose average performance exceeds a given threshold, referred to as good arm, if it exists. Few works have studied GAI in the fixed-budget setting, when the sampling budget is fixed beforehand, or the anytime setting, when a recommendation can be asked at any time. We propose APGAI, an anytime and parameter-free sampling rule for GAI in stochastic bandits. APGAI can be straightforwardly used in fixed-confidence and fixed-budget settings. First, we derive upper bounds on its probability of error at any time. They show that adaptive strategies are more efficient in detecting the absence of good arms than uniform sampling. Second, when APGAI is combined with a stopping rule, we prove upper bounds on the expected sampling complexity, holding at any confidence level. Finally, we show good empirical performance of APGAI on synthetic and real-world data. Our work offers an extensive overview of the GAI problem in all settings.中文简体版：在好臂标识（GAI）中，目标是找到一个臂的平均性能超过给定的阈值的臂，如果存在。已有相对少的研究对GAI进行了固定预算设定或任何时间设定。我们提出了APGAI，一种无参数和任何时间 sampling 规则。APGAI可以直接在固定信度和固定预算设定下使用。我们首先 deriv 了APGAI在任何时间的错误概率的Upper bound。这些结果显示了适应策略在缺乏好臂时更有效率地检测。其次，当APGAI与停止规则结合使用时，我们证明了预期的样本复杂度的Upper bound，保持任何信度水平。最后，我们在 sintetic 和实际数据上显示了APGAI的良好实际表现。我们的工作对GAI问题在所有设定中进行了全面的概述。
</details></li>
</ul>
<hr>
<h2 id="Hamming-Encoder-Mining-Discriminative-k-mers-for-Discrete-Sequence-Classification"><a href="#Hamming-Encoder-Mining-Discriminative-k-mers-for-Discrete-Sequence-Classification" class="headerlink" title="Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence Classification"></a>Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10321">http://arxiv.org/abs/2310.10321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Dong, Mudi Jiang, Lianyu Hu, Zengyou He</li>
<li>for: 该论文的目的是提出一种新的序列分类方法，以解决现有方法中的一些挑战，如缺乏特征组合的探索和精度下降。</li>
<li>methods: 该方法基于1D卷积神经网络（1DCNN）架构，并采用哈明距离基于相似度度量来确保特征挖掘和分类过程中的一致性。具体来说，该方法首先训练一个可解释的CNNEncoder对序列数据进行学习，然后通过梯度下降方式搜索出高度探索的k-mer组合。</li>
<li>results: 实验结果表明，该方法在分类精度方面比现有的状态作法更高。<details>
<summary>Abstract</summary>
Sequence classification has numerous applications in various fields. Despite extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. Existing pattern-based methods measure the discriminative power of each feature individually during the mining process, leading to the result of missing some combinations of features with discriminative power. Furthermore, it is difficult to ensure the overall discriminative performance after converting sequences into feature vectors. To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer combinations. Experiments show that the Hamming Encoder method proposed in this paper outperforms existing state-of-the-art methods in terms of classification accuracy.
</details>
<details>
<summary>摘要</summary>
Sequence 分类有很多应用场景，尤其是在不同领域。 DESPITE  extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. Existing pattern-based methods measure the discriminative power of each feature individually during the mining process, leading to the result of missing some combinations of features with discriminative power. Furthermore, it is difficult to ensure the overall discriminative performance after converting sequences into feature vectors. To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer combinations. Experiments show that the Hamming Encoder method proposed in this paper outperforms existing state-of-the-art methods in terms of classification accuracy.Here's the word-for-word translation:序列分类有很多应用场景，尤其是在不同领域。 DESPITE  extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. 现有的 pattern-based methods 在挖掘过程中对每个特征进行分解能力的测量，导致漏掉一些特征组合的分解能力。 更重要的是，将序列转换为特征向量后，保证总的分解性能是一个大问题。 To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer combinations. Experiments show that the Hamming Encoder method proposed in this paper outperforms existing state-of-the-art methods in terms of classification accuracy.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Quantum-Machine-Learning-Current-Trends-Challenges-Opportunities-and-the-Road-Ahead"><a href="#A-Survey-on-Quantum-Machine-Learning-Current-Trends-Challenges-Opportunities-and-the-Road-Ahead" class="headerlink" title="A Survey on Quantum Machine Learning: Current Trends, Challenges, Opportunities, and the Road Ahead"></a>A Survey on Quantum Machine Learning: Current Trends, Challenges, Opportunities, and the Road Ahead</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10315">http://arxiv.org/abs/2310.10315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamila Zaman, Alberto Marchisio, Muhammad Abdullah Hanif, Muhammad Shafique</li>
<li>for: 这篇论文主要是为了提供一个全面的Quantum Machine Learning（QML）领域的审视，并对不同的QML算法、量子数据集、硬件技术、软件工具、模拟器和应用场景进行了详细的介绍。</li>
<li>methods: 本论文使用了多种方法，包括论述基础概念、对класситиче计算的比较、介绍不同的QML算法和其适用领域、描述量子数据集和硬件技术的发展，以及介绍软件工具和模拟器。</li>
<li>results: 本论文提供了大量有价值的信息和资源，可以帮助读者快速入门到当前QML领域的state-of-the-art技术。<details>
<summary>Abstract</summary>
Quantum Computing (QC) claims to improve the efficiency of solving complex problems, compared to classical computing. When QC is applied to Machine Learning (ML) applications, it forms a Quantum Machine Learning (QML) system. After discussing the basic concepts of QC and its advantages over classical computing, this paper reviews the key aspects of QML in a comprehensive manner. We discuss different QML algorithms and their domain applicability, quantum datasets, hardware technologies, software tools, simulators, and applications. In this survey, we provide valuable information and resources for readers to jumpstart into the current state-of-the-art techniques in the QML field.
</details>
<details>
<summary>摘要</summary>
量子计算（QC）宣称可以提高解决复杂问题的效率，相比于经典计算。当QC应用于机器学习（ML）应用时，它形成了量子机器学习（QML）系统。本文详细介绍了QML的关键方面，包括不同的QML算法和它们的领域应用、量子数据集、硬件技术、软件工具、模拟器和应用。本文提供了读者们进入现有技术领域的价值信息和资源，以便他们可以快速掌握当前领域的最新技术。
</details></li>
</ul>
<hr>
<h2 id="Transparent-Anomaly-Detection-via-Concept-based-Explanations"><a href="#Transparent-Anomaly-Detection-via-Concept-based-Explanations" class="headerlink" title="Transparent Anomaly Detection via Concept-based Explanations"></a>Transparent Anomaly Detection via Concept-based Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10702">http://arxiv.org/abs/2310.10702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laya Rafiee Sevyeri, Ivaxi Sheth, Farhood Farahnak, Shirin Abbasinejad Enger</li>
<li>for: 本文提出了一种可解释的异常检测方法，以提高异常检测的可读性和人类可解释性。</li>
<li>methods: 本文使用了一种基于概念学习的异常检测方法，可以提供人类可解释的概念解释。此外，本文还提出了一种可与其他分类型异常检测方法集成的概念学习方法。</li>
<li>results: 本文通过三个实际数据集的实验表明，ACE方法可以提供高或相当于黑色盒模型的准确率，同时具有人类可解释的优势。<details>
<summary>Abstract</summary>
Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic datasets - bird classification on CUB-200-2011, challenging histopathology slide image classification on TIL-WSI-TCGA, and gender classification on CelebA. We further demonstrate that our concept learning paradigm can be seamlessly integrated with other classification-based AD methods.
</details>
<details>
<summary>摘要</summary>
深度学习技术的进步使得异常检测性能得到了提高。然而，实际应用中需要更进一步的透明度和理解，而不仅仅是精度。异常检测任务的目标是判断给定样本是否遵循学习的分布。现有方法缺乏对结果的解释能力。因此，我们提出了透明异常检测概念解释（ACE）。ACE可以提供人类可读解释，并且与异常预测一起提供概念。根据我们所知，这是第一篇提出可解释的异常检测方法。此外，ACE还允许人机交互，从而提高了异常检测的效iveness。我们的提议的模型在三个实际数据集上进行验证：鸟类分类在CUB-200-2011上， histopathology slice image分类在TIL-WSI-TCGA上，以及性别分类在CelebA上。此外，我们还证明了我们的概念学习方法可以与其他分类型异常检测方法一起兼容。
</details></li>
</ul>
<hr>
<h2 id="Time-integration-schemes-based-on-neural-networks-for-solving-partial-differential-equations-on-coarse-grids"><a href="#Time-integration-schemes-based-on-neural-networks-for-solving-partial-differential-equations-on-coarse-grids" class="headerlink" title="Time integration schemes based on neural networks for solving partial differential equations on coarse grids"></a>Time integration schemes based on neural networks for solving partial differential equations on coarse grids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10308">http://arxiv.org/abs/2310.10308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinxin Yan, Zhideng Zhou, Xiaohan Cheng, Xiaolei Yang</li>
<li>for: 本研究旨在提出一种基于神经网络的时间步长学习方法，以满足不同数学条件的需求。</li>
<li>methods: 本研究使用神经网络学习3步线性多步法，并应用到了三个模拟问题中，即一维热方程、一维波方程和一维吸引方程。</li>
<li>results: 结果显示，学习的完全约束方法的预测误差与Runge-Kutta方法和Adams-Bashforth方法的预测误差几乎相同。相比传统方法，学习的无约束和半约束方法在粗网格上显著减少预测误差，特别是对一维热方程的温度预测有显著改善。在4倍粗网格上，一些热方程的 casos 的 Mean Square Error 可以减少一个数量级，而波方程的预测相比传统方法具有明显的改善。在32倍粗网格上，Burgers方程的 Mean Square Error 可以减少35%-40%。<details>
<summary>Abstract</summary>
The accuracy of solving partial differential equations (PDEs) on coarse grids is greatly affected by the choice of discretization schemes. In this work, we propose to learn time integration schemes based on neural networks which satisfy three distinct sets of mathematical constraints, i.e., unconstrained, semi-constrained with the root condition, and fully-constrained with both root and consistency conditions. We focus on the learning of 3-step linear multistep methods, which we subsequently applied to solve three model PDEs, i.e., the one-dimensional heat equation, the one-dimensional wave equation, and the one-dimensional Burgers' equation. The results show that the prediction error of the learned fully-constrained scheme is close to that of the Runge-Kutta method and Adams-Bashforth method. Compared to the traditional methods, the learned unconstrained and semi-constrained schemes significantly reduce the prediction error on coarse grids. On a grid that is 4 times coarser than the reference grid, the mean square error shows a reduction of up to an order of magnitude for some of the heat equation cases, and a substantial improvement in phase prediction for the wave equation. On a 32 times coarser grid, the mean square error for the Burgers' equation can be reduced by up to 35% to 40%.
</details>
<details>
<summary>摘要</summary>
“对于半精簇方程（PDEs）的粗糙网格解决方法，选择精度方法的选择对准精度有着很大的影响。在这项工作中，我们提出了基于神经网络的时间拟合方法，满足三种不同的数学约束，即无约束、半约束（根条件）和完全约束（根条件和一致性条件）。我们主要关注了学习3步线性多步法，并将其应用于解决三个模型PDE中的一维热方程、一维波方程和一维布尔格方程。结果表明，学习的完全约束方法的预测误差与Runge-Kutta方法和Adams-Bashforth方法几乎相同。相比传统方法，学习无约束和半约束方法在粗糙网格上显著减少预测误差。在参照网格4倍粗的情况下，一些热方程的 случа在下面可以减少至次之 Magnitude 的误差，而波方程的预测也有显著改善。在32倍粗网格上，布尔格方程的误差可以减少35%-40%。”
</details></li>
</ul>
<hr>
<h2 id="Mimicking-the-Maestro-Exploring-the-Efficacy-of-a-Virtual-AI-Teacher-in-Fine-Motor-Skill-Acquisition"><a href="#Mimicking-the-Maestro-Exploring-the-Efficacy-of-a-Virtual-AI-Teacher-in-Fine-Motor-Skill-Acquisition" class="headerlink" title="Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in Fine Motor Skill Acquisition"></a>Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in Fine Motor Skill Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10280">http://arxiv.org/abs/2310.10280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadar Mulian, Segev Shlomov, Lior Limonad</li>
<li>for: 这项研究旨在探讨人工智能教师模型在促进细动技能学习中的潜在优势，以提高学习效率和学习结果的一致性。</li>
<li>methods: 该研究采用了人工智能学习和仿真学习方法，通过模拟教师与学生之间的互动来评估人工智能教师模型的效果。</li>
<li>results: 研究发现，使用人工智能教师模型可以提高学习效率和学习结果的一致性，并且可以适应不同的学生和学习环境。<details>
<summary>Abstract</summary>
Motor skills, especially fine motor skills like handwriting, play an essential role in academic pursuits and everyday life. Traditional methods to teach these skills, although effective, can be time-consuming and inconsistent. With the rise of advanced technologies like robotics and artificial intelligence, there is increasing interest in automating such teaching processes using these technologies, via human-robot and human-computer interactions. In this study, we examine the potential of a virtual AI teacher in emulating the techniques of human educators for motor skill acquisition. We introduce an AI teacher model that captures the distinct characteristics of human instructors. Using a Reinforcement Learning environment tailored to mimic teacher-learner interactions, we tested our AI model against four guiding hypotheses, emphasizing improved learner performance, enhanced rate of skill acquisition, and reduced variability in learning outcomes. Our findings, validated on synthetic learners, revealed significant improvements across all tested hypotheses. Notably, our model showcased robustness across different learners and settings and demonstrated adaptability to handwriting. This research underscores the potential of integrating Reinforcement Learning and Imitation Learning models with robotics in revolutionizing the teaching of critical motor skills.
</details>
<details>
<summary>摘要</summary>
motor skills, especially fine motor skills like handwriting, play a crucial role in academic pursuits and everyday life. traditional methods to teach these skills, although effective, can be time-consuming and inconsistent. with the rise of advanced technologies like robotics and artificial intelligence, there is increasing interest in automating such teaching processes using these technologies, via human-robot and human-computer interactions. in this study, we examine the potential of a virtual AI teacher in emulating the techniques of human educators for motor skill acquisition. we introduce an AI teacher model that captures the distinct characteristics of human instructors. using a reinforcement learning environment tailored to mimic teacher-learner interactions, we tested our AI model against four guiding hypotheses, emphasizing improved learner performance, enhanced rate of skill acquisition, and reduced variability in learning outcomes. our findings, validated on synthetic learners, revealed significant improvements across all tested hypotheses. notably, our model showcased robustness across different learners and settings and demonstrated adaptability to handwriting. this research underscores the potential of integrating reinforcement learning and imitation learning models with robotics in revolutionizing the teaching of critical motor skills.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-heterogeneous-spillover-effects-in-maximizing-contextual-bandit-rewards"><a href="#Leveraging-heterogeneous-spillover-effects-in-maximizing-contextual-bandit-rewards" class="headerlink" title="Leveraging heterogeneous spillover effects in maximizing contextual bandit rewards"></a>Leveraging heterogeneous spillover effects in maximizing contextual bandit rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10259">http://arxiv.org/abs/2310.10259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Sayeed Faruk, Elena Zheleva</li>
<li>for: 提高个性化推荐的相关性和准确性</li>
<li>methods: 利用多重环境抽象和个性化投资策略考虑用户之间的协同影响</li>
<li>results: 比现有方法高得多，可以更好地满足用户的需求和期望Here’s a brief explanation of each point:</li>
<li>for: The paper aims to improve the relevance and accuracy of personalized recommendations by taking into account the interdependent relationships between users.</li>
<li>methods: The proposed method uses a multi-armed bandit framework to model the interactions between users and the items they interact with, and incorporates heterogeneous spillover effects to better capture the impact of one user’s actions on others.</li>
<li>results: The proposed method outperforms existing approaches that ignore spillover effects, achieving significantly higher rewards in several real-world datasets.<details>
<summary>Abstract</summary>
Recommender systems relying on contextual multi-armed bandits continuously improve relevant item recommendations by taking into account the contextual information. The objective of these bandit algorithms is to learn the best arm (i.e., best item to recommend) for each user and thus maximize the cumulative rewards from user engagement with the recommendations. However, current approaches ignore potential spillover between interacting users, where the action of one user can impact the actions and rewards of other users. Moreover, spillover may vary for different people based on their preferences and the closeness of ties to other users. This leads to heterogeneity in the spillover effects, i.e., the extent to which the action of one user can impact the action of another. Here, we propose a framework that allows contextual multi-armed bandits to account for such heterogeneous spillovers when choosing the best arm for each user. By experimenting on several real-world datasets using prominent linear and non-linear contextual bandit algorithms, we observe that our proposed method leads to significantly higher rewards than existing solutions that ignore spillover.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: recommender systems 使用 contextual multi-armed bandits 不断提高相关的 item 推荐，通过考虑 contextual 信息来学习每个用户最佳的 arm (即最佳推荐)，以达到用户参与推荐的累积奖励的最大化。然而，当前的方法忽略了用户之间的互动副作用，即一个用户的行为会影响另一个用户的行为和奖励。此外，这种副作用可能因用户的偏好和与其他用户之间的关系而异常，即副作用的强度不同。为此，我们提出了一个框架，使得 contextual multi-armed bandits 能够考虑这种异常的副作用，以便为每个用户选择最佳的 arm。通过在一些真实世界数据上使用许多知名的线性和非线性 contextual bandit 算法进行实验，我们发现，我们提出的方法可以与忽略副作用的方法相比，获得更高的奖励。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Topological-Maps-in-Deep-Reinforcement-Learning-for-Multi-Object-Navigation"><a href="#Leveraging-Topological-Maps-in-Deep-Reinforcement-Learning-for-Multi-Object-Navigation" class="headerlink" title="Leveraging Topological Maps in Deep Reinforcement Learning for Multi-Object Navigation"></a>Leveraging Topological Maps in Deep Reinforcement Learning for Multi-Object Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10250">http://arxiv.org/abs/2310.10250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Hakenes, Tobias Glasmachers</li>
<li>for: 解决扩展空间 navigate 极少奖励问题</li>
<li>methods: 使用 topological maps 提升 elementary actions 到 object-oriented macro actions</li>
<li>results: 使用 DQN agent 解决 otherwise 不可能的环境<details>
<summary>Abstract</summary>
This work addresses the challenge of navigating expansive spaces with sparse rewards through Reinforcement Learning (RL). Using topological maps, we elevate elementary actions to object-oriented macro actions, enabling a simple Deep Q-Network (DQN) agent to solve otherwise practically impossible environments.
</details>
<details>
<summary>摘要</summary>
这个工作面临了在广阔空间中缺乏奖励的挑战，通过再增 learning (RL) 方法解决。我们使用 topological maps，将基本的动作提升到对象层次的macro动作，使得简单的深度Q网络 (DQN)  Agent 能够解决 otherwise 不可能的环境。
</details></li>
</ul>
<hr>
<h2 id="The-Mixtures-and-the-Neural-Critics-On-the-Pointwise-Mutual-Information-Profiles-of-Fine-Distributions"><a href="#The-Mixtures-and-the-Neural-Critics-On-the-Pointwise-Mutual-Information-Profiles-of-Fine-Distributions" class="headerlink" title="The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions"></a>The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10240">http://arxiv.org/abs/2310.10240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cbg-ethz/bmi">https://github.com/cbg-ethz/bmi</a></li>
<li>paper_authors: Paweł Czyż, Frederic Grabowski, Julia E. Vogt, Niko Beerenwinkel, Alexander Marx</li>
<li>for: 这个论文研究了点wise矩阵相互信息的profile，这是矩阵相互信息的一种扩展，它保持了 diffeomorphisms 的变换不变性。</li>
<li>methods: 论文使用了 Monte Carlo 方法来近似 multivariate normal distributions 的profile，并 introduce了 fine distributions 家族，可以用来研究现有的矩阵相互信息估计器的局限性，以及 neural critics 在variational estimators 中的行为。</li>
<li>results: 论文显示了 fine distributions 可以用来研究矩阵相互信息估计器的局限性，以及 neural critics 的行为，并可以用来获得model-based Bayesian 矩阵相互信息估计，适用于具有可用的领域专业知识的问题，在哪里 uncertainty quantification 是必要的。<details>
<summary>Abstract</summary>
Mutual information quantifies the dependence between two random variables and remains invariant under diffeomorphisms. In this paper, we explore the pointwise mutual information profile, an extension of mutual information that maintains this invariance. We analytically describe the profiles of multivariate normal distributions and introduce the family of fine distributions, for which the profile can be accurately approximated using Monte Carlo methods. We then show how fine distributions can be used to study the limitations of existing mutual information estimators, investigate the behavior of neural critics used in variational estimators, and understand the effect of experimental outliers on mutual information estimation. Finally, we show how fine distributions can be used to obtain model-based Bayesian estimates of mutual information, suitable for problems with available domain expertise in which uncertainty quantification is necessary.
</details>
<details>
<summary>摘要</summary>
互信息量量化了两个随机变量之间的依赖关系，并保持不变于 diffeomorphisms。在这篇文章中，我们探讨了点 wise 互信息 Profile，是互信息的扩展，保持这种不变性。我们 analytically 描述了多变量正态分布的 Profile，并引入了 fine 分布家族，其中 profile 可以使用 Monte Carlo 方法准确地 approximation。然后，我们示示了 fine 分布可以用来研究现有互信息估计器的限制，调查变量批评器在variational estimator中的行为，并理解试验异常点对互信息估计的影响。最后，我们示示了 fine 分布可以用来获得基于模型的 Bayesian 估计，适用于具有可用的领域专业知识的问题，在uncertainty quantification中需要。
</details></li>
</ul>
<hr>
<h2 id="Structural-transfer-learning-of-non-Gaussian-DAG"><a href="#Structural-transfer-learning-of-non-Gaussian-DAG" class="headerlink" title="Structural transfer learning of non-Gaussian DAG"></a>Structural transfer learning of non-Gaussian DAG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10239">http://arxiv.org/abs/2310.10239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyang Ren, Xin He, Junhui Wang</li>
<li>for:  targets to improve the reconstruction of directional relationships among nodes in a directed acyclic graph (DAG) using heterogeneous data from multiple studies.</li>
<li>methods:  proposes a novel set of structural similarity measures for DAG and a transfer DAG learning framework that leverages information from auxiliary DAGs of different levels of similarities.</li>
<li>results:  substantial improvement in DAG reconstruction in the target study, even when no auxiliary DAG is overall similar to the target DAG, and supported by extensive numerical experiments on both synthetic data and multi-site brain functional connectivity network data.Here’s the full translation in Simplified Chinese:</li>
<li>for: 本研究目的是提高基于多个研究中收集的不同数据的指向关系图（DAG）的重建精度。</li>
<li>methods: 提出了一种新的结构相似度测量方法，并提出了一种基于不同相似度水平的转移DAG学习框架，以有效地利用 auxiliary DAGs 中的信息。</li>
<li>results: 对目标研究中的 DAGC 重建具有重要提高，即使 auxiliary DAG 与目标 DAGC 无法总体相似，而且通过对 sintetic 数据和多地点大脑功能连接网络数据进行广泛的数值实验支持。<details>
<summary>Abstract</summary>
Directed acyclic graph (DAG) has been widely employed to represent directional relationships among a set of collected nodes. Yet, the available data in one single study is often limited for accurate DAG reconstruction, whereas heterogeneous data may be collected from multiple relevant studies. It remains an open question how to pool the heterogeneous data together for better DAG structure reconstruction in the target study. In this paper, we first introduce a novel set of structural similarity measures for DAG and then present a transfer DAG learning framework by effectively leveraging information from auxiliary DAGs of different levels of similarities. Our theoretical analysis shows substantial improvement in terms of DAG reconstruction in the target study, even when no auxiliary DAG is overall similar to the target DAG, which is in sharp contrast to most existing transfer learning methods. The advantage of the proposed transfer DAG learning is also supported by extensive numerical experiments on both synthetic data and multi-site brain functional connectivity network data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GEVO-ML-Optimizing-Machine-Learning-Code-with-Evolutionary-Computation"><a href="#GEVO-ML-Optimizing-Machine-Learning-Code-with-Evolutionary-Computation" class="headerlink" title="GEVO-ML: Optimizing Machine Learning Code with Evolutionary Computation"></a>GEVO-ML: Optimizing Machine Learning Code with Evolutionary Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10211">http://arxiv.org/abs/2310.10211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jhe-Yu Liou, Stephanie Forrest, Carole-Jean Wu</li>
<li>for: 本研究旨在提高大规模机器学习（ML）应用中的并行加速器（如GPU）的性能，但 ML 模型开发者通常缺乏关于下游系统架构的详细知识，而系统编程者则通常没有高级别的 ML 模型的理解。</li>
<li>methods: 本研究提出了 GEVO-ML，一种自动发现优化机会并调整 ML kernels性能的工具，其中 ML 模型和训练&#x2F;预测过程都是通过单一高级表示语言（多层次中间表示语言，MLIR）表示的。GEVO-ML 使用多目标进化搜索发现 MLIR 代码中的修改（突变），以提高 Desired riteria 的性能，保留必要的功能。</li>
<li>results: 对两个不同的 ML 工作负荷进行了训练和预测。GEVO-ML 在这两个模型中发现了显著的 Pareto 提升，提高了模型精度的误差率从 2% 下降到 90.43%，并在训练工作负荷中提高了模型精度从 91% 到 96%，无需牺牲训练或测试速度。分析表明，GEVO-ML 的关键突变包括多种code修改，尽管可能不familiar with human developers，但它们实现了类似于人类开发者在模型设计中进行的改进，例如更改学习率或者修剪不必要的层参数。<details>
<summary>Abstract</summary>
Parallel accelerators, such as GPUs, are key enablers for large-scale Machine Learning (ML) applications. However, ML model developers often lack detailed knowledge of the underlying system architectures, while system programmers usually do not have a high-level understanding of the ML model that runs on the specific system. To mitigate this gap between two relevant aspects of domain knowledge, this paper proposes GEVO-ML, a tool for automatically discovering optimization opportunities and tuning the performance of ML kernels, where the model and training/prediction processes are uniformly represented in a single intermediate language, the Multiple-Layer Intermediate Representation (MLIR). GEVO-ML uses multi-objective evolutionary search to find edits (mutations) to MLIR code that ultimately runs on GPUs, improving performance on desired criteria while retaining required functionality.   We demonstrate GEVO-ML on two different ML workloads for both model training and prediction. GEVO-ML finds significant Pareto improvements for these models, achieving 90.43% performance improvement when model accuracy is relaxed by 2%, from 91.2% to 89.3%. For the training workloads, GEVO-ML finds a 4.88% improvement in model accuracy, from 91% to 96%, without sacrificing training or testing speed. Our analysis of key GEVO-ML mutations reveals diverse code modifications, while might be foreign to human developers, achieving similar effects with how human developers improve model design, for example, by changing learning rates or pruning non-essential layer parameters.
</details>
<details>
<summary>摘要</summary>
高级加速器，如图形处理器（GPU），是大规模机器学习（ML）应用的关键驱动器。然而，ML模型开发者经常缺乏深入的系统架构知识，而系统编程者通常没有高级的ML模型的具体知识。为了 bridge这两个领域的知识差距，这篇论文提出了 GEVO-ML，一种自动发现优化机会并调整 ML kernels的工具。GEVO-ML 使用多目标进化搜索来找到 MLIR 代码中的修改（突变），以提高 Desired 特性的性能，保留必要的功能。我们在两个不同的 ML 任务上运行 GEVO-ML，包括模型训练和预测。GEVO-ML 在这些模型上发现了显著的 pareto 改进，将模型精度从 91.2% 下降到 89.3%，同时提高了性能。对于训练任务，GEVO-ML 提高了模型精度从 91% 到 96%，而无需牺牲训练或测试速度。我们分析了 GEVO-ML 中关键的突变，发现这些突变可能 foreign 于人类开发者，但具有类似的效果，例如更改学习率或减少不必要的层参数。
</details></li>
</ul>
<hr>
<h2 id="Bongard-OpenWorld-Few-Shot-Reasoning-for-Free-form-Visual-Concepts-in-the-Real-World"><a href="#Bongard-OpenWorld-Few-Shot-Reasoning-for-Free-form-Visual-Concepts-in-the-Real-World" class="headerlink" title="Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World"></a>Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10207">http://arxiv.org/abs/2310.10207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joyjayng/Bongard-OpenWorld">https://github.com/joyjayng/Bongard-OpenWorld</a></li>
<li>paper_authors: Rujie Wu, Xiaojian Ma, Qing Li, Wei Wang, Zhenliang Zhang, Song-Chun Zhu, Yizhou Wang</li>
<li>for: 评估现实世界中的几何shot理解能力，即通过几何shot的图像训练模型可以在新的图像中理解和分类图像。</li>
<li>methods: 使用经典的Bongard问题（BPs）作为基础，并添加两种新的挑战：1）开放世界自由形容符，即图像概念由开放词汇中的图像特征和概念组成，2）使用实际世界图像而非 sintetic 图像。</li>
<li>results: 研究发现，当前的几何shot理解算法面临 significiant 挑战，而且even irectly  probing VLMs 和 combining VLMs 和 LLMs 在交互理解方案中，无法距离人类的问题解决能力（64% 准确率，而人类参与者可以达到 91%）。<details>
<summary>Abstract</summary>
We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even designed a neuro-symbolic reasoning approach that reconciles LLMs & VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的评价实际世界几何reasoning的benchmark，即Bongard-OpenWorld。它基于经典的Bongard问题（BP），要求模型通过引入视觉概念，将查询图像分配到正确的集合中。我们的benchmark继承了原BP的几何概念引入，并添加了两个新的挑战：1）开放世界自由形态概念，视觉概念在Bongard-OpenWorld中是独特的词汇库中的组合，范围从物体类到抽象视觉特征和常识知识; 2）实际图像，而不是许多同类的synthetic图像。在我们的探索中，Bongard-OpenWorld已经对当前几何reasoning算法带来了 significativetranslation challenges。我们进一步调查了current Large Language Models (LLMs)和Vision-Language Models (VLMs)是否可以解决我们的任务，直接考试VLMs，并将VLMs和LLMs结合在互动理解方案中。我们甚至设计了一种神经符号逻辑 reasoningapproach，将LLMs & VLMs与逻辑逻辑 reasoning相结合，以便模拟人类问题解决过程。然而， none of these approaches manage to close the human-machine gap，best learner的准确率只有64%，而人类参与者容易达到91%。我们希望Bongard-OpenWorld可以帮助我们更好地理解当前视觉智能的局限性，并促进未来的视觉代理人with stronger few-shot visual reasoning能力的研究。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Predictive-Models-to-Understand-Risk-Factors-for-Maternal-and-Fetal-Outcomes"><a href="#Interpretable-Predictive-Models-to-Understand-Risk-Factors-for-Maternal-and-Fetal-Outcomes" class="headerlink" title="Interpretable Predictive Models to Understand Risk Factors for Maternal and Fetal Outcomes"></a>Interpretable Predictive Models to Understand Risk Factors for Maternal and Fetal Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10203">http://arxiv.org/abs/2310.10203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomas M. Bosschieter, Zifei Xu, Hui Lan, Benjamin J. Lengerich, Harsha Nori, Ian Painter, Vivienne Souter, Rich Caruana</li>
<li>for: 这个论文旨在提高妈妈和婴儿的健康，通过更好地理解风险因素，加强高风险患者的监测，及时采取有效措施，以便妈妈医生能够提供更好的照料。</li>
<li>methods: 这篇论文使用了可解释扩展机器学习方法（EBM）进行预测和重要风险因素的 indentification。EBM具有高准确率和可解释性，并且在验证和稳定性分析中证明了其可靠性。</li>
<li>results: 研究发现，EBM模型可以准确预测四种妈妈和婴儿的病情，并且可以提供有价值的风险因素。例如， maternal height 是Shoulder dystocia 的第二重要风险因素。这些结果表明，EBM模型在预测和预防妈妈和婴儿的严重病情中具有优秀的性能和可解释性。<details>
<summary>Abstract</summary>
Although most pregnancies result in a good outcome, complications are not uncommon and can be associated with serious implications for mothers and babies. Predictive modeling has the potential to improve outcomes through better understanding of risk factors, heightened surveillance for high risk patients, and more timely and appropriate interventions, thereby helping obstetricians deliver better care. We identify and study the most important risk factors for four types of pregnancy complications: (i) severe maternal morbidity, (ii) shoulder dystocia, (iii) preterm preeclampsia, and (iv) antepartum stillbirth. We use an Explainable Boosting Machine (EBM), a high-accuracy glass-box learning method, for prediction and identification of important risk factors. We undertake external validation and perform an extensive robustness analysis of the EBM models. EBMs match the accuracy of other black-box ML methods such as deep neural networks and random forests, and outperform logistic regression, while being more interpretable. EBMs prove to be robust. The interpretability of the EBM models reveals surprising insights into the features contributing to risk (e.g. maternal height is the second most important feature for shoulder dystocia) and may have potential for clinical application in the prediction and prevention of serious complications in pregnancy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Interpretable-Deep-Learning-Framework-for-Predicting-Hospital-Readmissions-From-Electronic-Health-Records"><a href="#An-Interpretable-Deep-Learning-Framework-for-Predicting-Hospital-Readmissions-From-Electronic-Health-Records" class="headerlink" title="An Interpretable Deep-Learning Framework for Predicting Hospital Readmissions From Electronic Health Records"></a>An Interpretable Deep-Learning Framework for Predicting Hospital Readmissions From Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10187">http://arxiv.org/abs/2310.10187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Azzalini, Tommaso Dolci, Marco Vagaggini</li>
<li>for: 预测医院复 admit 的风险，以降低医疗成本并提高患者健康状况。</li>
<li>methods: 提出了一种新的、可解释的深度学习框架，基于 NLP 发现 word embeddings 和 ConvLSTM 神经网络模型，以更好地处理时间数据。</li>
<li>results: 对医院复 admit 的预测任务进行了 validate，并 introduce 了一种模型依赖的技术来使结果更容易被医疗 personnels 理解。结果比传统基于机器学习的模型提供更好的性能，同时也提供了更加可解释的结果。<details>
<summary>Abstract</summary>
With the increasing availability of patients' data, modern medicine is shifting towards prospective healthcare. Electronic health records contain a variety of information useful for clinical patient description and can be exploited for the construction of predictive models, given that similar medical histories will likely lead to similar progressions. One example is unplanned hospital readmission prediction, an essential task for reducing hospital costs and improving patient health. Despite predictive models showing very good performances especially with deep-learning models, they are often criticized for the poor interpretability of their results, a fundamental characteristic in the medical field, where incorrect predictions might have serious consequences for the patient health. In this paper we propose a novel, interpretable deep-learning framework for predicting unplanned hospital readmissions, supported by NLP findings on word embeddings and by neural-network models (ConvLSTM) for better handling temporal data. We validate our system on the two predictive tasks of hospital readmission within 30 and 180 days, using real-world data. In addition, we introduce and test a model-dependent technique to make the representation of results easily interpretable by the medical staff. Our solution achieves better performances compared to traditional models based on machine learning, while providing at the same time more interpretable results.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "prospective healthcare" is translated as "前瞻医疗" (pre-emptive healthcare)* "electronic health records" is translated as "电子健康记录" (electronic health records)* "clinical patient description" is translated as "临床患者描述" (clinical patient description)* "unplanned hospital readmission" is translated as "不计划入院" (unplanned hospital readmission)* "predictive models" is translated as "预测模型" (predictive models)* "word embeddings" is translated as "词嵌入" (word embeddings)* "ConvLSTM" is translated as "卷积LSTM" (ConvLSTM)* "temporal data" is translated as "时间数据" (temporal data)* "medical staff" is translated as "医疗人员" (medical staff)
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Echo-State-Network"><a href="#Hypergraph-Echo-State-Network" class="headerlink" title="Hypergraph Echo State Network"></a>Hypergraph Echo State Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10177">http://arxiv.org/abs/2310.10177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Lien</li>
<li>for: 这篇文章是用于描述一种基于几何网络的对数据进行有效处理的网络模型，并且提出了一个基于几何网络的对数状态网络（HypergraphESN）的设计。</li>
<li>methods: 这篇文章使用了一种基于几何网络的对数状态网络（HypergraphESN），并且提出了这个方法的算法和稳定性条件。</li>
<li>results: 数据实验显示，HypergraphESN在处理几何网络结构的数据时，能够与传统的几何网络模型（GraphESN）相比，获得更高的准确率。具体来说，HypergraphESN在处理高阶相互作用的数据时，能够更好地处理非线性特征，并且可以实现更高的准确率。<details>
<summary>Abstract</summary>
A hypergraph as a generalization of graphs records higher-order interactions among nodes, yields a more flexible network model, and allows non-linear features for a group of nodes. In this article, we propose a hypergraph echo state network (HypergraphESN) as a generalization of graph echo state network (GraphESN) designed for efficient processing of hypergraph-structured data, derive convergence conditions for the algorithm, and discuss its versatility in comparison to GraphESN. The numerical experiments on the binary classification tasks demonstrate that HypergraphESN exhibits comparable or superior accuracy performance to GraphESN for hypergraph-structured data, and accuracy increases if more higher-order interactions in a network are identified.
</details>
<details>
<summary>摘要</summary>
一种超графи（hypergraph）是图的扩展，用于记录高阶交互 among nodes，具有更灵活的网络模型，并允许非线性特征 для一组节点。在这篇文章中，我们提议一种基于超графи的响应状态网络（HypergraphESN）作为图响应状态网络（GraphESN）的扩展，用于高效处理超графи结构数据， derivation of convergence conditions for the algorithm, and discussion of its versatility compared to GraphESN. 数值实验表明，对于二分类任务，HypergraphESN可以与GraphESN具有相同或更高的准确率表现，并且如果在网络中更多的高阶交互被标识， то准确率会进一步提高。
</details></li>
</ul>
<hr>
<h2 id="On-permutation-symmetries-in-Bayesian-neural-network-posteriors-a-variational-perspective"><a href="#On-permutation-symmetries-in-Bayesian-neural-network-posteriors-a-variational-perspective" class="headerlink" title="On permutation symmetries in Bayesian neural network posteriors: a variational perspective"></a>On permutation symmetries in Bayesian neural network posteriors: a variational perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10171">http://arxiv.org/abs/2310.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Rossi, Ankit Singh, Thomas Hannagan</li>
<li>for: 这项研究旨在理解神经网络中梯度下降优化的困难性, 以及 Bayesian neural networks（BNNs）中 approximate inference 的问题。</li>
<li>methods: 这篇论文使用了 marginalized loss barrier 和 solution interpolation 的扩展 formalism, 以及一种匹配算法来搜索线性连接的解。</li>
<li>results: 实验结果表明, 对于多种架构和数据集, linearly connected solutions 的 marginalized loss barrier 几乎为零。<details>
<summary>Abstract</summary>
The elusive nature of gradient-based optimization in neural networks is tied to their loss landscape geometry, which is poorly understood. However recent work has brought solid evidence that there is essentially no loss barrier between the local solutions of gradient descent, once accounting for weight-permutations that leave the network's computation unchanged. This raises questions for approximate inference in Bayesian neural networks (BNNs), where we are interested in marginalizing over multiple points in the loss landscape. In this work, we first extend the formalism of marginalized loss barrier and solution interpolation to BNNs, before proposing a matching algorithm to search for linearly connected solutions. This is achieved by aligning the distributions of two independent approximate Bayesian solutions with respect to permutation matrices. We build on the results of Ainsworth et al. (2023), reframing the problem as a combinatorial optimization one, using an approximation to the sum of bilinear assignment problem. We then experiment on a variety of architectures and datasets, finding nearly zero marginalized loss barriers for linearly connected solutions.
</details>
<details>
<summary>摘要</summary>
“神经网络中梯度基本优化的难易程度与其损失函数 geometry 存在深刻的关系。然而，最近的研究表明，在考虑权重 Permutation 后，梯度 descend 的本地解决方案之间存在无损函数梯度。这引发了对 approximate inference 在 Bayesian neural networks （BNNs）中进行 marginalization 的问题。在这个工作中，我们首先扩展了 marginalized loss barrier 和 solution interpolation 的形式主义，然后提出了一种匹配算法，用于搜索 linearly connected solutions。这是通过对两个独立的approximate Bayesian解决方案的分布进行对齐，以实现对 permutation matrices 的Alignment。我们基于 Ainsworth et al. (2023) 的结果，重新定义问题为一个 combinatorial optimization 问题，使用一种 Approximation 来计算 bilinear assignment problem 的和。然后我们在不同的架构和数据集上进行了实验，发现linearly connected solutions 的 marginalized loss barrier 几乎为零。”Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Simplicial-Representation-Learning-with-Wasserstein-Distance"><a href="#An-Empirical-Study-of-Simplicial-Representation-Learning-with-Wasserstein-Distance" class="headerlink" title="An Empirical Study of Simplicial Representation Learning with Wasserstein Distance"></a>An Empirical Study of Simplicial Representation Learning with Wasserstein Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10143">http://arxiv.org/abs/2310.10143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Makoto Yamada, Yuki Takezawa, Guillaume Houry, Kira Michaela Dusterwald, Deborah Sulem, Han Zhao, Yao-Hung Hubert Tsai</li>
<li>for: 本研究探讨了使用树结构上的1- Wasserstein距离（Tree-Wasserstein distance，TWD）来学习 simplicial 表示，TWD 是两个树嵌入向量之间的L1距离。</li>
<li>methods: 本研究使用了一种基于自动采样的自监学习方法，使用 TWD 作为相似度度量，并提出了一种简单 yet effective的 Jeffrey divergence 基于正则化方法来稳定优化。</li>
<li>results: 通过对 STL10、CIFAR10、CIFAR100 和 SVHN 等数据集进行实验，研究发现，将 softmax 函数和 TWD 组合使用可以获得较低的结果，而且模型性能取决于 TWD 和 simplicial 模型的组合，并且 Jeffrey divergence 正则化通常能够稳定模型训练。最终，研究人员发现了选择合适的 TWD 和 simplicial 模型的组合可以超越cosine similarity 基于表示学习。<details>
<summary>Abstract</summary>
In this paper, we delve into the problem of simplicial representation learning utilizing the 1-Wasserstein distance on a tree structure (a.k.a., Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance between two tree-embedded vectors. Specifically, we consider a framework for simplicial representation estimation employing a self-supervised learning approach based on SimCLR with a negative TWD as a similarity measure. In SimCLR, the cosine similarity with real-vector embeddings is often utilized; however, it has not been well studied utilizing L1-based measures with simplicial embeddings. A key challenge is that training the L1 distance is numerically challenging and often yields unsatisfactory outcomes, and there are numerous choices for probability models. Thus, this study empirically investigates a strategy for optimizing self-supervised learning with TWD and find a stable training procedure. More specifically, we evaluate the combination of two types of TWD (total variation and ClusterTree) and several simplicial models including the softmax function, the ArcFace probability model, and simplicial embedding. Moreover, we propose a simple yet effective Jeffrey divergence-based regularization method to stabilize the optimization. Through empirical experiments on STL10, CIFAR10, CIFAR100, and SVHN, we first found that the simple combination of softmax function and TWD can obtain significantly lower results than the standard SimCLR (non-simplicial model and cosine similarity). We found that the model performance depends on the combination of TWD and the simplicial model, and the Jeffrey divergence regularization usually helps model training. Finally, we inferred that the appropriate choice of combination of TWD and simplicial models outperformed cosine similarity based representation learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了使用树结构上的一 Wasserstein 距离（TWD）来学习 simplicial 表示，其中 TWD 定义为两个树嵌入向量之间的 L1 距离。我们考虑了一种基于自适应学习的 simplicial 表示估计方法，使用 SimCLR 自适应学习框架，并使用 TWD 作为相似度量。在 SimCLR 中，通常使用 cosine 相似性来衡量实际向量嵌入，但是使用 L1 基于的度量尚未得到广泛研究。主要挑战在于训练 L1 距离是数值上困难，而且存在多种概率模型选择。因此，我们在这篇论文中进行了实验性的研究，以便在 TWD 和 simplicial 模型之间找到稳定的训练过程。具体来说，我们评估了两种类型的 TWD（总变量和 ClusterTree）以及多种 simplicial 模型，包括软max 函数、ArcFace 概率模型和 simplicial 嵌入。此外，我们还提出了一种简单 yet 有效的 Jeffrey 分布基本规范 regularization 方法，以稳定优化。通过对 STL10、CIFAR10、CIFAR100 和 SVHN 等数据集进行实验，我们发现了以下结论：1. 将 softmax 函数和 TWD 结合使用可以获得显著更好的结果，与标准 SimCLR（非 simplicial 模型和 cosine 相似性）相比。2. 模型性能取决于 TWD 和 simplicial 模型的组合，而 Jeffrey 分布基本规范常常帮助模型训练。3. 选择合适的 TWD 和 simplicial 模型的组合，通常会超越 cosine 相似性基于的表示学习。总之，我们的研究表明，使用 TWD 和 simplicial 模型可以提高表示学习的性能，并且可以选择合适的组合来超越 cosine 相似性基于的表示学习。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-Privacy-Risks-in-Curriculum-Learning"><a href="#A-Comprehensive-Study-of-Privacy-Risks-in-Curriculum-Learning" class="headerlink" title="A Comprehensive Study of Privacy Risks in Curriculum Learning"></a>A Comprehensive Study of Privacy Risks in Curriculum Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10124">http://arxiv.org/abs/2310.10124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joann Qiongna Chen, Xinlei He, Zheng Li, Yang Zhang, Zhou Li</li>
<li>for: 本研究旨在探讨curriculum learning（CL）对机器学习的隐私影响，以填补现有的知识空白。</li>
<li>methods: 我们使用了membership inference attack（MIA）和attribute inference attack（AIA）两种方法来衡量CL对隐私的泄露。</li>
<li>results: 我们的evalution结果显示，MIA在CL下变得slightly more effective，但它的影响尤其明显于difficult sample subset。AIA对CL下的模型比MIA更加敏感，而exististing defense techniques仍然有效。此外，我们还提出了一种新的MIA方法，称为Diff-Cali，它基于difficulty scores进行结果准确化。<details>
<summary>Abstract</summary>
Training a machine learning model with data following a meaningful order, i.e., from easy to hard, has been proven to be effective in accelerating the training process and achieving better model performance. The key enabling technique is curriculum learning (CL), which has seen great success and has been deployed in areas like image and text classification. Yet, how CL affects the privacy of machine learning is unclear. Given that CL changes the way a model memorizes the training data, its influence on data privacy needs to be thoroughly evaluated. To fill this knowledge gap, we perform the first study and leverage membership inference attack (MIA) and attribute inference attack (AIA) as two vectors to quantify the privacy leakage caused by CL.   Our evaluation of nine real-world datasets with attack methods (NN-based, metric-based, label-only MIA, and NN-based AIA) revealed new insights about CL. First, MIA becomes slightly more effective when CL is applied, but the impact is much more prominent to a subset of training samples ranked as difficult. Second, a model trained under CL is less vulnerable under AIA, compared to MIA. Third, the existing defense techniques like DP-SGD, MemGuard, and MixupMMD are still effective under CL, though DP-SGD has a significant impact on target model accuracy. Finally, based on our insights into CL, we propose a new MIA, termed Diff-Cali, which exploits the difficulty scores for result calibration and is demonstrated to be effective against all CL methods and the normal training method. With this study, we hope to draw the community's attention to the unintended privacy risks of emerging machine-learning techniques and develop new attack benchmarks and defense solutions.
</details>
<details>
<summary>摘要</summary>
通过训练机器学习模型使用meaningful order的数据，即从易到难，已经证明可以加速训练过程并提高模型性能。关键技术是curriculum learning（CL），已经在图像和文本分类等领域取得了很大成功。然而，CL对机器学习的隐私影响是不清楚。因为CL改变了模型对训练数据的记忆方式，因此其对隐私的影响需要进行仔细评估。为了填补这个知识空白，我们进行了第一个研究，并利用成员推理攻击（MIA）和特征推理攻击（AIA）作为两种量度CL对隐私的泄露的方法。我们对九个实际 datasets进行了评估，并使用NN-based、metric-based、label-only MIA和NN-based AIA等方法进行攻击。我们发现了以下新的发现：1. MIA在CL应用后变得略微更加有效，但对于一些训练样本 ranked as difficult 的影响更加明显。2. 一个CL训练的模型对AIA更加抵触，相比于MIA。3. 现有的防御技术如DP-SGD、MemGuard和MixupMMD仍然有效于CL，尽管DP-SGD对目标模型准确率有显著影响。4. 基于我们对CL的发现，我们提出了一种新的MIA，称为Diff-Cali，它利用难度分数进行结果准确性的调整，并证明可以有效地对CL方法和常规训练方法进行攻击。通过这项研究，我们希望能吸引社区关注机器学习领域的意外隐私风险，并开发新的攻击 benchmark和防御解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-proximal-augmented-Lagrangian-based-algorithm-for-federated-learning-with-global-and-local-convex-conic-constraints"><a href="#A-proximal-augmented-Lagrangian-based-algorithm-for-federated-learning-with-global-and-local-convex-conic-constraints" class="headerlink" title="A proximal augmented Lagrangian based algorithm for federated learning with global and local convex conic constraints"></a>A proximal augmented Lagrangian based algorithm for federated learning with global and local convex conic constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10117">http://arxiv.org/abs/2310.10117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuan He, Le Peng, Ju Sun</li>
<li>for: 本研究针对 federated learning (FL) with constraints 进行研究，实现了在中央服务器和所有本地客户端之间收集数据，并实现了模型训练。</li>
<li>methods: 本研究提出了一个基于 proximal augmented Lagrangian (AL) 的 federated learning 框架，并使用了各种对数方法来解决问题。</li>
<li>results: 本研究的实验结果显示了这个方法在 Neyman-Pearson 分类和模型公平性方面的实际优势。 另外，本研究还提出了一个新的 federated learning 框架，具有全球和本地凸对数约束的特点。<details>
<summary>Abstract</summary>
This paper considers federated learning (FL) with constraints, where the central server and all local clients collectively minimize a sum of convex local objective functions subject to global and local convex conic constraints. To train the model without moving local data from clients to the central server, we propose an FL framework in which each local client performs multiple updates using the local objective and local constraint, while the central server handles the global constraint and performs aggregation based on the updated local models. In particular, we develop a proximal augmented Lagrangian (AL) based algorithm for FL with global and local convex conic constraints. The subproblems arising in this algorithm are solved by an inexact alternating direction method of multipliers (ADMM) in a federated fashion. Under a local Lipschitz condition and mild assumptions, we establish the worst-case complexity bounds of the proposed algorithm for finding an approximate KKT solution. To the best of our knowledge, this work proposes the first algorithm for FL with global and local constraints. Our numerical experiments demonstrate the practical advantages of our algorithm in performing Neyman-Pearson classification and enhancing model fairness in the context of FL.
</details>
<details>
<summary>摘要</summary>
We develop a proximal augmented Lagrangian (AL) based algorithm for FL with global and local convex conic constraints. The subproblems arising in this algorithm are solved using an inexact alternating direction method of multipliers (ADMM) in a federated fashion. Under local Lipschitz conditions and mild assumptions, we establish the worst-case complexity bounds of the proposed algorithm for finding an approximate KKT solution.To the best of our knowledge, this work proposes the first algorithm for FL with global and local constraints. Our numerical experiments demonstrate the practical advantages of our algorithm in performing Neyman-Pearson classification and enhancing model fairness in the context of FL.Here's the Simplified Chinese translation:这篇论文研究了基于约束的联合学习（Federated Learning，FL），其中中央服务器和所有本地客户端共同减少一个拥有 convex 本地目标函数的总和，同时遵循全局和本地 convex 凹陷约束。为了不让本地数据从客户端传输到中央服务器，我们提议了一种基于 FL 的框架，其中每个本地客户端可以多次使用本地目标和约束进行更新，而中央服务器则负责全局约束并基于更新后的本地模型进行聚合。我们开发了一种基于 proximal augmented Lagrangian（AL）的算法，用于解决 FL 中的全局和本地 convex 凹陷约束问题。这些子问题在我们的算法中使用了一种不精确的 alternating direction method of multipliers（ADMM）来解决。在本地 Lipschitz 条件和某些假设下，我们确定了我们提议的算法的最坏情况复杂性 bound。根据我们所知，这是第一个基于 FL 的全局和本地约束算法。我们的数据实验表明，我们的算法在 Neyman-Pearson 分类和 Federation Learning 中的实际优势。
</details></li>
</ul>
<hr>
<h2 id="PAC-Learning-Linear-Thresholds-from-Label-Proportions"><a href="#PAC-Learning-Linear-Thresholds-from-Label-Proportions" class="headerlink" title="PAC Learning Linear Thresholds from Label Proportions"></a>PAC Learning Linear Thresholds from Label Proportions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10098">http://arxiv.org/abs/2310.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Rishi Saket, Aravindan Raghuveer</li>
<li>for: 本研究旨在学习从标签分布中提取信息，特别是在标签分布中存在噪声和不确定性的情况下。</li>
<li>methods: 本研究使用了一种基于 Gaussian distribution 的方法，使用随机抽样来估算标签分布的方差矩阵，并使用这个矩阵来定义一个特征向量空间中的正常向量。然后，使用这个正常向量来定义一个线性阈值函数（LTF），并使用这个 LTF 来学习实例分类器。</li>
<li>results: 本研究表明，使用这种方法可以高效地学习 LTF，并且可以在标签分布中存在噪声和不确定性的情况下提取有用的信息。此外，研究还提供了一些总体错误 bounds 和特性分布 bounds，以确保学习的准确性和稳定性。实验评估表明，本方法可以与 [Saket’21, Saket’22] 等方法相比，并且在某些特殊情况下可以提供更高的准确性。<details>
<summary>Abstract</summary>
Learning from label proportions (LLP) is a generalization of supervised learning in which the training data is available as sets or bags of feature-vectors (instances) along with the average instance-label of each bag. The goal is to train a good instance classifier. While most previous works on LLP have focused on training models on such training data, computational learnability of LLP was only recently explored by [Saket'21, Saket'22] who showed worst case intractability of properly learning linear threshold functions (LTFs) from label proportions. However, their work did not rule out efficient algorithms for this problem on natural distributions.   In this work we show that it is indeed possible to efficiently learn LTFs using LTFs when given access to random bags of some label proportion in which feature-vectors are, conditioned on their labels, independently sampled from a Gaussian distribution $N(\mathbf{\mu}, \mathbf{\Sigma})$. Our work shows that a certain matrix -- formed using covariances of the differences of feature-vectors sampled from the bags with and without replacement -- necessarily has its principal component, after a transformation, in the direction of the normal vector of the LTF. Our algorithm estimates the means and covariance matrices using subgaussian concentration bounds which we show can be applied to efficiently sample bags for approximating the normal direction. Using this in conjunction with novel generalization error bounds in the bag setting, we show that a low error hypothesis LTF can be identified. For some special cases of the $N(\mathbf{0}, \mathbf{I})$ distribution we provide a simpler mean estimation based algorithm. We include an experimental evaluation of our learning algorithms along with a comparison with those of [Saket'21, Saket'22] and random LTFs, demonstrating the effectiveness of our techniques.
</details>
<details>
<summary>摘要</summary>
学习从标签比例（LLP）是一种泛化超级vised学习，其训练数据为特征向量集或包中的平均实例标签。目标是训练一个好的实例分类器。而大多数之前的LLP研究都集中在模型的训练上，而Computational learnability of LLP只在[Saket'21, Saket'22]中被研究过，他们表明了线性阈值函数（LTF）的合理学习是最坏情况不可能的。然而，他们的工作没有排除了自然分布下的有效算法。在这个工作中，我们证明了可以高效地学习LTF，只要给出Random Bag of Label Proportions（RBLP）中的特征向量，并且这些特征向量是Conditioned on their labels，独立地从 Gaussian 分布 $N(\mathbf{\mu}, \mathbf{\Sigma})$ 中随机抽取。我们的工作表明，一个特定的矩阵，由RBLP中带有和无置换的特征向量的差异的covariances形成，然后经过一种变换，必然有其主成分在正常方向上。我们的算法使用Subgaussian散射约束来估算均值和 covariance 矩阵，然后使用这些矩阵来随机抽取包来估算正常方向。使用这种方法，我们可以高效地分类LTF。对于 $N(\mathbf{0}, \mathbf{I})$ 分布的特殊情况，我们还提供了一个简单的均值估计基于算法。我们在实验评估了我们的学习算法，并与 [Saket'21, Saket'22] 和随机 LTF 进行了比较，demonstrating the effectiveness of our techniques。
</details></li>
</ul>
<hr>
<h2 id="LLP-Bench-A-Large-Scale-Tabular-Benchmark-for-Learning-from-Label-Proportions"><a href="#LLP-Bench-A-Large-Scale-Tabular-Benchmark-for-Learning-from-Label-Proportions" class="headerlink" title="LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions"></a>LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10096">http://arxiv.org/abs/2310.10096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Mohith Pokala, Rishi Saket, Aravindan Raghuveer</li>
<li>for: 本文提出了一个大规模的标注量学习（LLP）数据集，用于 Addressing the lack of a open, large scale tabular benchmark。</li>
<li>methods: 本文提出了一个名为 LLP-Bench 的数据集，包含 56 个 LLP 数据集（52 个特征袋和 4 个随机袋数据集），它们都是从 Criteo CTR 预测数据集中的 45 万个实例中构建的。此外，本文还提出了四种度量量学习数据集的困难程度。</li>
<li>results: 本文通过使用这四种度量量学习数据集的困难程度，进行了深入的分析。此外，本文还通过使用 9 种 state-of-the-art 和受欢迎的标注量学习技术，对所有 56 个数据集进行了性能测试。根据本文的描述，这是文献中最广泛的标注量学习技术测试。<details>
<summary>Abstract</summary>
In the task of Learning from Label Proportions (LLP), a model is trained on groups (a.k.a bags) of instances and their corresponding label proportions to predict labels for individual instances. LLP has been applied pre-dominantly on two types of datasets - image and tabular. In image LLP, bags of fixed size are created by randomly sampling instances from an underlying dataset. Bags created via this methodology are called random bags. Experimentation on Image LLP has been mostly on random bags on CIFAR-* and MNIST datasets. Despite being a very crucial task in privacy sensitive applications, tabular LLP does not yet have a open, large scale LLP benchmark. One of the unique properties of tabular LLP is the ability to create feature bags where all the instances in a bag have the same value for a given feature. It has been shown in prior research that feature bags are very common in practical, real world applications [Chen et. al '23, Saket et. al. '22].   In this paper, we address the lack of a open, large scale tabular benchmark. First we propose LLP-Bench, a suite of 56 LLP datasets (52 feature bag and 4 random bag datasets) created from the Criteo CTR prediction dataset consisting of 45 million instances. The 56 datasets represent diverse ways in which bags can be constructed from underlying tabular data. To the best of our knowledge, LLP-Bench is the first large scale tabular LLP benchmark with an extensive diversity in constituent datasets. Second, we propose four metrics that characterize and quantify the hardness of a LLP dataset. Using these four metrics we present deep analysis of the 56 datasets in LLP-Bench. Finally we present the performance of 9 SOTA and popular tabular LLP techniques on all the 56 datasets. To the best of our knowledge, our study consisting of more than 2500 experiments is the most extensive study of popular tabular LLP techniques in literature.
</details>
<details>
<summary>摘要</summary>
在学习从标签比例（LLP）任务中，一个模型被训练在实例组（即袋）和其对应的标签比例上，以预测个体实例的标签。 LLG 已经主要应用于图像和表格数据集。在图像 LLG 中，实例组通常通过随机抽样实例从下面数据集创建。这种方法创建的袋被称为随机袋。在 CIFAR-* 和 MNIST 数据集上进行了大量实验。虽然图像 LLG 是一个非常重要的任务，但是表格 LLG 尚未有一个开放、大规模的 LLG  bencmark。表格 LLG 的一个独特性是可以创建特征袋，其中所有实例在袋中都有相同的特征值。在先前的研究中已经证明了特征袋在实际应用中很常见。在这篇文章中，我们解决表格 LLG 缺乏一个开放、大规模的 bencmark 问题。我们提出了 LLP-Bench，一个包含 56 个 LLG 数据集（52 个特征袋数据集和 4 个随机袋数据集）的集合，这些数据集是从 Criteo CTR 预测数据集中的 45 万个实例中创建的。这些 56 个数据集表示了从下面表格数据中构建袋的多种方法。我们知道，LLP-Bench 是首先开放、大规模的表格 LLG bencmark，并且具有广泛的数据集多样性。其次，我们提出了四种度量 LLG 数据集的困难程度。使用这四种度量，我们进行了深入的分析 LLP-Bench 中的 56 个数据集。最后，我们在所有 56 个数据集上运行了 9 种 state-of-the-art 和流行的表格 LLG 技术，并进行了 более чем 2500 个实验。到目前为止，我们的研究是文献中最广泛的表格 LLG 技术研究。
</details></li>
</ul>
<hr>
<h2 id="Label-Differential-Privacy-via-Aggregation"><a href="#Label-Differential-Privacy-via-Aggregation" class="headerlink" title="Label Differential Privacy via Aggregation"></a>Label Differential Privacy via Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10092">http://arxiv.org/abs/2310.10092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Rishi Saket, Shreyas Havaldar, Anshul Nasery, Aravindan Raghuveer</li>
<li>for: 保护敏感训练标签的隐私</li>
<li>methods: 使用 randomly weighted aggregation 和 additive noise 保护隐私</li>
<li>results: 可以实现 label-DP 保护，无需或少量的添加噪声，并且 preserved 训练任务的效果<details>
<summary>Abstract</summary>
In many real-world applications, in particular due to recent developments in the privacy landscape, training data may be aggregated to preserve the privacy of sensitive training labels. In the learning from label proportions (LLP) framework, the dataset is partitioned into bags of feature-vectors which are available only with the sum of the labels per bag. A further restriction, which we call learning from bag aggregates (LBA) is where instead of individual feature-vectors, only the (possibly weighted) sum of the feature-vectors per bag is available. We study whether such aggregation techniques can provide privacy guarantees under the notion of label differential privacy (label-DP) previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22].   It is easily seen that naive LBA and LLP do not provide label-DP. Our main result however, shows that weighted LBA using iid Gaussian weights with $m$ randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon, \delta)$-label-DP for any $\varepsilon > 0$ with $\delta \approx \exp(-\Omega(\sqrt{k}))$ assuming a lower bound on the linear-mse regression loss. Further, this preserves the optimum over linear mse-regressors of bounded norm to within $(1 \pm o(1))$-factor w.p. $\approx 1 - \exp(-\Omega(m))$. We emphasize that no additive label noise is required.   The analogous weighted-LLP does not however admit label-DP. Nevertheless, we show that if additive $N(0, 1)$ noise can be added to any constant fraction of the instance labels, then the noisy weighted-LLP admits similar label-DP guarantees without assumptions on the dataset, while preserving the utility of Lipschitz-bounded neural mse-regression tasks.   Our work is the first to demonstrate that label-DP can be achieved by randomly weighted aggregation for regression tasks, using no or little additive noise.
</details>
<details>
<summary>摘要</summary>
在许多实际应用中，特别是due to recent developments in privacy landscape，training data可能会被聚合以保护敏感训练标签的隐私。在学习从标签聚合（LLP）框架中，数据集被分解成具有特征向量的袋子，但这些特征向量只有每个袋子的标签总和。我们称这种约束为学习从袋子聚合（LBA）。我们研究了这种聚合技术是否可以提供隐私保证，并且我们发现这种保证是可行的。我们的主要结果表明，使用独立的 Gaussian 权重，将 $m$ 个不同大小的 $k$-个袋子Randomly sampled，并使用加权 LBA，可以实现 $( \varepsilon, \delta)$-标签隐私（label-DP），其中 $\delta \approx \exp(-\Omega(\sqrt{k}))$。此外，这种方法可以保持最佳的线性mse回归损失，即 $(1 \pm o(1))$-factor w.p. $\approx 1 - \exp(-\Omega(m))$.这意味着没有添加标签噪声。虽然加权 LLP 不能实现标签隐私，但我们发现，如果将任意一部分的实例标签添加 $N(0, 1)$ 噪声，那么这种噪声化的加权 LLP 可以实现类似的标签隐私保证，不需要对数据集进行任何假设。此外，这种方法可以保持 Lipschitz-bounded 神经网络 mse-regression 任务的实用性。我们的工作是首次示出，通过Randomly weighted aggregation可以实现标签隐私，无需或只需少量的添加噪声。
</details></li>
</ul>
<hr>
<h2 id="Over-the-Air-Federated-Learning-and-Optimization"><a href="#Over-the-Air-Federated-Learning-and-Optimization" class="headerlink" title="Over-the-Air Federated Learning and Optimization"></a>Over-the-Air Federated Learning and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10089">http://arxiv.org/abs/2310.10089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyang Zhu, Yuanming Shi, Yong Zhou, Chunxiao Jiang, Wei Chen, Khaled B. Letaief</li>
<li>For: 这篇论文关注于 federated learning (FL) 中的 over-the-air computation (AirComp)，以减少无线网络上的通信负担，但是会增加模型聚合错误引起的学习性能下降。* Methods: 该论文首先进行了 AirComp-based FedAvg 算法的完整的研究，包括在强型凸和非凸设定下的常数和减少学习率下的渐进分析，以及在数据不同性下的影响分析。* Results: 该论文通过渐进分析和启发性分析，描述了模型聚合错误对渐进级别的影响，并提供了系统设计的准确性保证。此外，该论文还探讨了不同类型的本地更新（模型、梯度和模型差异）在 AirFedAvg 算法中的影响。<details>
<summary>Abstract</summary>
Federated learning (FL), as an emerging distributed machine learning paradigm, allows a mass of edge devices to collaboratively train a global model while preserving privacy. In this tutorial, we focus on FL via over-the-air computation (AirComp), which is proposed to reduce the communication overhead for FL over wireless networks at the cost of compromising in the learning performance due to model aggregation error arising from channel fading and noise. We first provide a comprehensive study on the convergence of AirComp-based FedAvg (AirFedAvg) algorithms under both strongly convex and non-convex settings with constant and diminishing learning rates in the presence of data heterogeneity. Through convergence and asymptotic analysis, we characterize the impact of aggregation error on the convergence bound and provide insights for system design with convergence guarantees. Then we derive convergence rates for AirFedAvg algorithms for strongly convex and non-convex objectives. For different types of local updates that can be transmitted by edge devices (i.e., local model, gradient, and model difference), we reveal that transmitting local model in AirFedAvg may cause divergence in the training procedure. In addition, we consider more practical signal processing schemes to improve the communication efficiency and further extend the convergence analysis to different forms of model aggregation error caused by these signal processing schemes. Extensive simulation results under different settings of objective functions, transmitted local information, and communication schemes verify the theoretical conclusions.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种新的分布式机器学习模式，允许Edge设备之间的大量设备共同训练全球模型，保持隐私。在这个教程中，我们关注FL通过过空 computation（AirComp）来降低由无线网络传输的交流负担，但是由于通道抖动和噪声而导致模型聚合错误，因此降低了学习性能。我们首先对AirComp-based FedAvg（AirFedAvg）算法进行了全面的研究，包括强度凸和非凸设置下的常数和减少学习率，并在数据不同性下进行了与界分析。我们通过收敛和极限分析来描述聚合错误对收敛 bound 的影响，并提供了系统设计的准确性保证。然后，我们 derivated convergence rates for AirFedAvg algorithms for strongly convex and non-convex objectives. 对于不同的本地更新可以在 Edge devices 上传输（即本地模型、梯度和模型差异），我们发现在 AirFedAvg 中传输本地模型可能会导致训练过程中的偏转。此外，我们考虑了更实际的通信减少技术，并将这些技术应用于不同的聚合错误类型，以进一步推广收敛分析。我们的实验结果在不同的目标函数、传输的本地信息和通信方案下都验证了我们的理论结论。
</details></li>
</ul>
<hr>
<h2 id="A-simple-uniformly-optimal-method-without-line-search-for-convex-optimization"><a href="#A-simple-uniformly-optimal-method-without-line-search-for-convex-optimization" class="headerlink" title="A simple uniformly optimal method without line search for convex optimization"></a>A simple uniformly optimal method without line search for convex optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10082">http://arxiv.org/abs/2310.10082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianjiao Li, Guanghui Lan</li>
<li>for:  solves convex optimization problems with unknown problem parameters (e.g., Lipschitz constant) without the need for line search procedures.</li>
<li>methods:  presents a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that achieves an optimal $\mathcal{O}(1&#x2F;k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant.</li>
<li>results:  demonstrates the advantages of AC-FGM over the previously developed parameter-free methods for convex optimization through numerical results.<details>
<summary>Abstract</summary>
Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal $\mathcal{O}(1/k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with H\"{o}lder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Finally, we report some encouraging numerical results that demonstrate the advantages of AC-FGM over the previously developed parameter-free methods for convex optimization.
</details>
<details>
<summary>摘要</summary>
《线搜索（或回溯）过程已广泛应用于首领方法中解决凸优化问题，特别是当问题参数未知（例如 lipschitz常数）。在这篇论文中，我们证明线搜索是不必要的，以实现凸优化问题的最佳$\mathcal{O}(1/k^2)$趋势速度。我们 THEN present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM)，可以在凸优化问题中不需要global lipschitz常数或线搜索过程来实现最佳趋势速度。我们然后扩展AC-FGM来解决凸优化问题，并证明它自动实现了最佳趋势速度，并且可以在所有问题类型中实现最佳趋势速度，只需要输入所需的精度。最后，我们report some encouraging numerical results that demonstrate the advantages of AC-FGM over the previously developed parameter-free methods for convex optimization。》Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="SoTTA-Robust-Test-Time-Adaptation-on-Noisy-Data-Streams"><a href="#SoTTA-Robust-Test-Time-Adaptation-on-Noisy-Data-Streams" class="headerlink" title="SoTTA: Robust Test-Time Adaptation on Noisy Data Streams"></a>SoTTA: Robust Test-Time Adaptation on Noisy Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10074">http://arxiv.org/abs/2310.10074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taeckyung/SoTTA">https://github.com/taeckyung/SoTTA</a></li>
<li>paper_authors: Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, Sung-Ju Lee</li>
<li>for: 这个论文旨在Addressing distributional shifts between training and testing data using only unlabeled test data streams for continual model adaptation.</li>
<li>methods: 这个方法使用 two-fold enablers: (i) input-wise robustness via high-confidence uniform-class sampling, and (ii) parameter-wise robustness via entropy-sharpness minimization.</li>
<li>results: 比较先前的TTA方法，这个方法在存在噪音样本的情况下实现了比较好的性能，并且在没有噪音样本的情况下实现了相当的性能。<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) aims to address distributional shifts between training and testing data using only unlabeled test data streams for continual model adaptation. However, most TTA methods assume benign test streams, while test samples could be unexpectedly diverse in the wild. For instance, an unseen object or noise could appear in autonomous driving. This leads to a new threat to existing TTA algorithms; we found that prior TTA algorithms suffer from those noisy test samples as they blindly adapt to incoming samples. To address this problem, we present Screening-out Test-Time Adaptation (SoTTA), a novel TTA algorithm that is robust to noisy samples. The key enabler of SoTTA is two-fold: (i) input-wise robustness via high-confidence uniform-class sampling that effectively filters out the impact of noisy samples and (ii) parameter-wise robustness via entropy-sharpness minimization that improves the robustness of model parameters against large gradients from noisy samples. Our evaluation with standard TTA benchmarks with various noisy scenarios shows that our method outperforms state-of-the-art TTA methods under the presence of noisy samples and achieves comparable accuracy to those methods without noisy samples. The source code is available at https://github.com/taeckyung/SoTTA .
</details>
<details>
<summary>摘要</summary>
(i) input-wise robustness via high-confidence uniform-class sampling that effectively filters out the impact of noisy samples;(ii) parameter-wise robustness via entropy-sharpness minimization that improves the robustness of model parameters against large gradients from noisy samples.Our evaluation with standard TTA benchmarks with various noisy scenarios shows that our method outperforms state-of-the-art TTA methods under the presence of noisy samples and achieves comparable accuracy to those methods without noisy samples. The source code is available at <https://github.com/taeckyung/SoTTA>.
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentation-for-Time-Series-Classification-An-Extensive-Empirical-Study-and-Comprehensive-Survey"><a href="#Data-Augmentation-for-Time-Series-Classification-An-Extensive-Empirical-Study-and-Comprehensive-Survey" class="headerlink" title="Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey"></a>Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10060">http://arxiv.org/abs/2310.10060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijun Gao, Lingbo Li, Tianhua Xu<br>for:* The paper aims to provide a comprehensive review of data augmentation (DA) techniques for time series classification (TSC) and to develop a novel taxonomy for categorizing these techniques.methods:* The paper uses an extensive literature review and a rigorous analysis of over 100 scholarly articles to identify and categorize more than 60 unique DA techniques for TSC.* The paper also employs an all-encompassing empirical assessment using 8 UCR time-series datasets and ResNet to evaluate the performance of various DA strategies.results:* The paper reports a benchmark accuracy of 88.94 +- 11.83% using a multi-faceted evaluation paradigm that includes Accuracy, Method Ranking, and Residual Analysis.* The paper highlights the inconsistent efficacies of DA techniques for TSC and underscores the need for a robust navigational aid for scholars to select appropriate methods.<details>
<summary>Abstract</summary>
Data Augmentation (DA) has emerged as an indispensable strategy in Time Series Classification (TSC), primarily due to its capacity to amplify training samples, thereby bolstering model robustness, diversifying datasets, and curtailing overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible, user-oriented tools. In light of these challenges, this study embarks on an exhaustive dissection of DA methodologies within the TSC realm. Our initial approach involved an extensive literature review spanning a decade, revealing that contemporary surveys scarcely capture the breadth of advancements in DA for TSC, prompting us to meticulously analyze over 100 scholarly articles to distill more than 60 unique DA techniques. This rigorous analysis precipitated the formulation of a novel taxonomy, purpose-built for the intricacies of DA in TSC, categorizing techniques into five principal echelons: Transformation-Based, Pattern-Based, Generative, Decomposition-Based, and Automated Data Augmentation. Our taxonomy promises to serve as a robust navigational aid for scholars, offering clarity and direction in method selection. Addressing the conspicuous absence of holistic evaluations for prevalent DA techniques, we executed an all-encompassing empirical assessment, wherein upwards of 15 DA strategies were subjected to scrutiny across 8 UCR time-series datasets, employing ResNet and a multi-faceted evaluation paradigm encompassing Accuracy, Method Ranking, and Residual Analysis, yielding a benchmark accuracy of 88.94 +- 11.83%. Our investigation underscored the inconsistent efficacies of DA techniques, with...
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>时序分类（TSC）中的数据扩展（DA）已经成为一种不可或缺的策略，主要是因为它可以增加训练样本数量，从而增强模型的鲁棒性，维护数据集的多样性，并避免过拟合。然而，现有的DA在TSC领域的文献 landscape 受到了 Fragmented 的文献回顾，混乱的方法分类、不够的评价标准和Accessible 的用户工具的缺乏，这些挑战使得这项研究发起了一项极其详细的DA方法分析。我们的初始方法是进行了一项广泛的文献回顾，覆盖了一个 décennial 的时间范围，发现当前的 contemporary 文献几乎不能涵盖DA在TSC领域的全面发展，因此我们仔细分析了超过 100 篇学术文章，提取了超过 60 种Unique DA技术。这项精心的分析导致了我们提出了一个专门为TSC领域的DA方法分类的新分类法，将技术分为五个主要层次：转换基于、模式基于、生成器、分解基于和自动化数据扩展。我们的分类法承诺成为学术界的一个robust navigational aid，为执行者提供了方法选择的明确性和方向性。为了弥补DA技术的普遍存在的评价问题，我们实施了一项总面的实验室评价，对超过 15 种DA策略进行了8个UCSD时序数据集的评估，使用了ResNet和一种多方面的评价方案，包括准确率、方法排名和剩余分析，实现了基准准确率88.94±11.83%。我们的调查表明，DA技术的不同策略在不同的时序数据集上的表现存在差异，...
</details></li>
</ul>
<hr>
<h2 id="Latent-Conservative-Objective-Models-for-Data-Driven-Crystal-Structure-Prediction"><a href="#Latent-Conservative-Objective-Models-for-Data-Driven-Crystal-Structure-Prediction" class="headerlink" title="Latent Conservative Objective Models for Data-Driven Crystal Structure Prediction"></a>Latent Conservative Objective Models for Data-Driven Crystal Structure Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10056">http://arxiv.org/abs/2310.10056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Qi, Xinyang Geng, Stefano Rando, Iku Ohama, Aviral Kumar, Sergey Levine</li>
<li>for: 这 paper 的目的是提出一种数据驱动的晶体结构预测方法，以优化晶体结构的计算成本。</li>
<li>methods: 这 paper 使用了一种名为 LCOMs (latent conservative objective models) 的方法，它利用一个状态环境 autoencoder 将晶体结构转换为一个矢量空间中的搜索空间，然后优化一个保守的晶体能量模型。</li>
<li>results: 这 paper 的结果表明，LCOMs 方法可以与现有最佳方法相比，成功率相似，而计算成本减少了多少。<details>
<summary>Abstract</summary>
In computational chemistry, crystal structure prediction (CSP) is an optimization problem that involves discovering the lowest energy stable crystal structure for a given chemical formula. This problem is challenging as it requires discovering globally optimal designs with the lowest energies on complex manifolds. One approach to tackle this problem involves building simulators based on density functional theory (DFT) followed by running search in simulation, but these simulators are painfully slow. In this paper, we study present and study an alternate, data-driven approach to crystal structure prediction: instead of directly searching for the most stable structures in simulation, we train a surrogate model of the crystal formation energy from a database of existing crystal structures, and then optimize this model with respect to the parameters of the crystal structure. This surrogate model is trained to be conservative so as to prevent exploitation of its errors by the optimizer. To handle optimization in the non-Euclidean space of crystal structures, we first utilize a state-of-the-art graph diffusion auto-encoder (CD-VAE) to convert a crystal structure into a vector-based search space and then optimize a conservative surrogate model of the crystal energy, trained on top of this vector representation. We show that our approach, dubbed LCOMs (latent conservative objective models), performs comparably to the best current approaches in terms of success rate of structure prediction, while also drastically reducing computational cost.
</details>
<details>
<summary>摘要</summary>
在计算化学中，晶体结构预测（CSP）是一个优化问题，涉及到找到给定化学式的最低能量稳定晶体结构。这个问题是复杂的，因为需要找到最低能量的全局优化设计在复杂的拟合上。一种方法是建立基于密度函数理论（DFT）的模拟器，然后通过搜索在模拟中进行优化，但这些模拟器很慢。在这篇论文中，我们研究了一种 alternate，数据驱动的晶体结构预测方法：而不是直接在模拟中搜索最稳定的结构，我们将训练一个晶体形成能量的模拟器，该模拟器在数据库中的已知晶体结构基础上被训练，然后对晶体结构参数进行优化。这个模拟器被设计为保守的，以避免其错误被优化器利用。为处理晶体结构的非几何空间优化问题，我们首先使用当前最佳的图像扩散自动encoder（CD-VAE）将晶体结构转换为一个矢量基本搜索空间，然后对这个矢量表示的晶体能量模拟器进行保守的优化。我们发现，我们的方法，称为LCOMs（幽默保守目标模型），与当前最佳方法相比，在结构预测成功率方面表现相似，同时也很快地减少计算成本。
</details></li>
</ul>
<hr>
<h2 id="Symmetrical-SyncMap-for-Imbalanced-General-Chunking-Problems"><a href="#Symmetrical-SyncMap-for-Imbalanced-General-Chunking-Problems" class="headerlink" title="Symmetrical SyncMap for Imbalanced General Chunking Problems"></a>Symmetrical SyncMap for Imbalanced General Chunking Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10045">http://arxiv.org/abs/2310.10045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Zhang, Danilo Vasconcellos Vargas</li>
<li>for: 本研究旨在学习从序列中检索复杂结构，并适应任何结构变化。</li>
<li>methods: 本研究使用非线性动力学方程， inspirited by neuron group behaviors，而不使用损失函数。</li>
<li>results: 我们的算法在12个不均衡CGCP中表现出色，超过或与其他无监督状态元既达到同等水平。在实际应用场景中，我们的方法在3个场景中表现出优异，表明对时间数据中的トポлогиcal结构和层次结构具有探索性。<details>
<summary>Abstract</summary>
Recently, SyncMap pioneered an approach to learn complex structures from sequences as well as adapt to any changes in underlying structures. This is achieved by using only nonlinear dynamical equations inspired by neuron group behaviors, i.e., without loss functions. Here we propose Symmetrical SyncMap that goes beyond the original work to show how to create dynamical equations and attractor-repeller points which are stable over the long run, even dealing with imbalanced continual general chunking problems (CGCPs). The main idea is to apply equal updates from negative and positive feedback loops by symmetrical activation. We then introduce the concept of memory window to allow for more positive updates. Our algorithm surpasses or ties other unsupervised state-of-the-art baselines in all 12 imbalanced CGCPs with various difficulties, including dynamically changing ones. To verify its performance in real-world scenarios, we conduct experiments on several well-studied structure learning problems. The proposed method surpasses substantially other methods in 3 out of 4 scenarios, suggesting that symmetrical activation plays a critical role in uncovering topological structures and even hierarchies encoded in temporal data.
</details>
<details>
<summary>摘要</summary>
最近，SyncMap开创了一种从序列学习复杂结构的方法，同时适应下面结构的变化。这是通过使用非线性动力学方程， inspirited by neuron group behaviors，而不使用损失函数。在这个研究中，我们提出了对称的SyncMap，超越原始工作，并显示了如何创建动力学方程和吸引器-抵抗点，这些点在长期内是稳定的，甚至在不均衡的CGCP中进行总览。我们的算法在12个不均衡CGCP中至少与其他无监督状态艺术基elines一样好，包括动态变化的CGCP。为了证明它在实际情况下的表现，我们在several well-studied structure learning问题上进行了实验。提出的方法在3个问题中大幅超过其他方法，表明对称活动在捕捉时间数据中的 topological结构和层次结构具有关键作用。
</details></li>
</ul>
<hr>
<h2 id="TpopT-Efficient-Trainable-Template-Optimization-on-Low-Dimensional-Manifolds"><a href="#TpopT-Efficient-Trainable-Template-Optimization-on-Low-Dimensional-Manifolds" class="headerlink" title="TpopT: Efficient Trainable Template Optimization on Low-Dimensional Manifolds"></a>TpopT: Efficient Trainable Template Optimization on Low-Dimensional Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10039">http://arxiv.org/abs/2310.10039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingkai Yan, Shiyu Wang, Xinyu Rain Wei, Jimmy Wang, Zsuzsanna Márka, Szabolcs Márka, John Wright</li>
<li>for: 检测低维度信号家族</li>
<li>methods: 使用 TemPlate OPTimization 框架，combined with embedding和kernel interpolation，提高计算效率</li>
<li>results: 在 gravitational wave detection 和手写数据上显示了明显的性能改善，并且可以替换现有的 matched filtering 方法<details>
<summary>Abstract</summary>
In scientific and engineering scenarios, a recurring task is the detection of low-dimensional families of signals or patterns. A classic family of approaches, exemplified by template matching, aims to cover the search space with a dense template bank. While simple and highly interpretable, it suffers from poor computational efficiency due to unfavorable scaling in the signal space dimensionality. In this work, we study TpopT (TemPlate OPTimization) as an alternative scalable framework for detecting low-dimensional families of signals which maintains high interpretability. We provide a theoretical analysis of the convergence of Riemannian gradient descent for TpopT, and prove that it has a superior dimension scaling to covering. We also propose a practical TpopT framework for nonparametric signal sets, which incorporates techniques of embedding and kernel interpolation, and is further configurable into a trainable network architecture by unrolled optimization. The proposed trainable TpopT exhibits significantly improved efficiency-accuracy tradeoffs for gravitational wave detection, where matched filtering is currently a method of choice. We further illustrate the general applicability of this approach with experiments on handwritten digit data.
</details>
<details>
<summary>摘要</summary>
在科学和工程应用中，检测低维度信号家族是一项常复现的任务。经典的方法之一是模板匹配，它在搜索空间使用密集的模板银行，但它的计算效率受到信号空间维度的不利影响。在这项工作中，我们研究TpopT（模板优化）作为一种可扩展的搜索框架，可以快速检测低维度信号家族，同时保持高度可读性。我们提供了TpopT的理论分析，证明它在维度上有更好的缩放性。此外，我们还提出了一种实用的TpopT框架，用于非参数式信号集，该框架包括投影和核函数 interpolate 技术，并可以通过不断的优化来转化为可训练的网络结构。我们的可训练TpopT在探测 gravitational wave 方面表现出了明显的效率-准确性融合优势，现在matched filtering 是选择的方法。此外，我们还通过对手写数据进行实验，证明了这种方法的通用性。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-Fundamental-Properties-of-Power-System-Resilience-Curves-using-Unsupervised-Machine-Learning"><a href="#Unraveling-Fundamental-Properties-of-Power-System-Resilience-Curves-using-Unsupervised-Machine-Learning" class="headerlink" title="Unraveling Fundamental Properties of Power System Resilience Curves using Unsupervised Machine Learning"></a>Unraveling Fundamental Properties of Power System Resilience Curves using Unsupervised Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10030">http://arxiv.org/abs/2310.10030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Li, Ali Mostafavi</li>
<li>for: 这个研究旨在描述和量化基础设施的鲜敏性特点。</li>
<li>methods: 这个研究使用无监督机器学习分析了超过200个关于三次极端天气事件的停电情况下的鲜敏性曲线。</li>
<li>results: 研究发现了两种基础设施鲜敏性曲线模型：三角形曲线和梯形曲线。三角形曲线基于 three critical functionality threshold、critical functionality recovery rate 和 recovery pivot point。梯形曲线则基于停电持续时间和平均恢复率。停电持续时间越长，恢复率就越慢。这些发现可以帮助我们更好地理解和预测基础设施的鲜敏性表现。<details>
<summary>Abstract</summary>
The standard model of infrastructure resilience, the resilience triangle, has been the primary way of characterizing and quantifying infrastructure resilience. However, the theoretical model merely provides a one-size-fits-all framework for all infrastructure systems. Most of the existing studies examine the characteristics of infrastructure resilience curves based on analytical models constructed upon simulated system performance. Limited empirical studies hindered our ability to fully understand and predict resilience characteristics in infrastructure systems. To address this gap, this study examined over 200 resilience curves related to power outages in three major extreme weather events. Using unsupervised machine learning, we examined different curve archetypes, as well as the fundamental properties of each resilience curve archetype. The results show two primary archetypes for power system resilience curves, triangular, and trapezoidal curves. Triangular curves characterize resilience behavior based on 1. critical functionality threshold, 2. critical functionality recovery rate, and 3. recovery pivot point. Trapezoidal archetypes explain resilience curves based on 1. duration of sustained function loss and 2. constant recovery rate. The longer the duration of sustained function loss, the slower the constant rate of recovery. The findings of this study provide novel perspectives enabling better understanding and prediction of resilience performance of power system infrastructures.
</details>
<details>
<summary>摘要</summary>
现代基础设施鲜度模型，即鲜度三角形模型，已成为基础设施鲜度的主要方法。然而，这种理论模型只能为所有基础设施系统提供一个一大 Familiar framework。大多数现有研究都是基于对基础设施系统性能的分析建模。有限的实证研究限制了我们理解和预测基础设施系统鲜度的能力。为了解决这个差距，本研究对三次极端天气事件中的电力停机事件进行了200多个鲜度曲线的研究。使用无监督机器学习方法，我们研究了不同的鲜度曲线范型，以及每个鲜度曲线范型的基本性质。结果显示，电力系统鲜度曲线有两种主要范型：三角形曲线和梯形曲线。三角形曲线表示鲜度行为的三个关键指标：极限功能阈值、极限功能恢复率和恢复枢轴点。梯形曲线则解释鲜度曲线的两个指标：持续功能损失的时间长度和恢复率。即使持续功能损失的时间长度越长，恢复率也越慢。这些发现为电力系统基础设施的鲜度性能提供了新的视角，帮助更好地理解和预测鲜度性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Score-Based-Models-for-Generating-Stable-Structures-with-Adaptive-Crystal-Cells"><a href="#Data-Driven-Score-Based-Models-for-Generating-Stable-Structures-with-Adaptive-Crystal-Cells" class="headerlink" title="Data-Driven Score-Based Models for Generating Stable Structures with Adaptive Crystal Cells"></a>Data-Driven Score-Based Models for Generating Stable Structures with Adaptive Crystal Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10695">http://arxiv.org/abs/2310.10695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/findooshka/diffusion-atoms">https://github.com/findooshka/diffusion-atoms</a></li>
<li>paper_authors: Arsen Sultanov, Jean-Claude Crivello, Tabea Rebafka, Nataliya Sokolovska</li>
<li>for: 本研究旨在通过机器学习生成模型，找到新的功能性和稳定性的材料。</li>
<li>methods: 该研究使用了分布式朴素迭代随机动力学模型，在训练过程中学习了晶格的各个参数，并在生成新的化学结构时使用了两个杂谱处理来生成晶格和原子位置。</li>
<li>results: 研究人员通过对不同化学系统和晶体群进行比较，表明了他们的模型能够在不需要额外训练的情况下，生成新的候选结构。<details>
<summary>Abstract</summary>
The discovery of new functional and stable materials is a big challenge due to its complexity. This work aims at the generation of new crystal structures with desired properties, such as chemical stability and specified chemical composition, by using machine learning generative models. Compared to the generation of molecules, crystal structures pose new difficulties arising from the periodic nature of the crystal and from the specific symmetry constraints related to the space group. In this work, score-based probabilistic models based on annealed Langevin dynamics, which have shown excellent performance in various applications, are adapted to the task of crystal generation. The novelty of the presented approach resides in the fact that the lattice of the crystal cell is not fixed. During the training of the model, the lattice is learned from the available data, whereas during the sampling of a new chemical structure, two denoising processes are used in parallel to generate the lattice along the generation of the atomic positions. A multigraph crystal representation is introduced that respects symmetry constraints, yielding computational advantages and a better quality of the sampled structures. We show that our model is capable of generating new candidate structures in any chosen chemical system and crystal group without any additional training. To illustrate the functionality of the proposed method, a comparison of our model to other recent generative models, based on descriptor-based metrics, is provided.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>新型功能稳定材料的发现是一个大的挑战，因为它的复杂性。这项工作的目标是通过机器学习生成模型生成新的晶体结构，其具有指定的化学稳定性和某些化学成分。与分子生成不同，晶体结构受到晶体 периоди性和空间群特殊约束的限制，这些约束使得晶体生成增加了新的挑战。在这项工作中，我们使用了Score-based潜在随机模型，这种模型在多种应用中表现出色。我们的新方法在训练模型时不 fix 晶体维度，而是在数据available时学习晶体矩阵，并在生成原子位置时使用了两个杂化过程。我们引入了多граф晶体表示，该表示符合Symmetry约束，从而获得计算优势和更高质量的样本结构。我们显示了我们的模型可以在任选的化学系统和晶体组中生成新的候选结构，无需额外训练。为证明我们的方法的可行性，我们对其与其他最近的生成模型进行了比较，并通过描述符 metric 进行评估。
</details></li>
</ul>
<hr>
<h2 id="Riemannian-Residual-Neural-Networks"><a href="#Riemannian-Residual-Neural-Networks" class="headerlink" title="Riemannian Residual Neural Networks"></a>Riemannian Residual Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10013">http://arxiv.org/abs/2310.10013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isay Katsman, Eric Ming Chen, Sidhanth Holalkere, Anna Asch, Aaron Lou, Ser-Nam Lim, Christopher De Sa</li>
<li>for: 这种研究旨在扩展常见的欧几丁素神经网络（ResNet）到整体几何抽象空间中，以便在自然科学中遇到的拓扑空间数据上进行学习。</li>
<li>methods: 这篇论文使用了几何神经网络的扩展，以便在拓扑空间上进行学习。这种扩展基于几何抽象空间的原则，并且可以覆盖整体几何抽象空间中的任何点。</li>
<li>results: 论文的实验结果表明，使用这种几何神经网络可以在拓扑空间上进行更好的学习，并且在相关的测试指标上表现更好，比如训练律动和测试结果。<details>
<summary>Abstract</summary>
Recent methods in geometric deep learning have introduced various neural networks to operate over data that lie on Riemannian manifolds. Such networks are often necessary to learn well over graphs with a hierarchical structure or to learn over manifold-valued data encountered in the natural sciences. These networks are often inspired by and directly generalize standard Euclidean neural networks. However, extending Euclidean networks is difficult and has only been done for a select few manifolds. In this work, we examine the residual neural network (ResNet) and show how to extend this construction to general Riemannian manifolds in a geometrically principled manner. Originally introduced to help solve the vanishing gradient problem, ResNets have become ubiquitous in machine learning due to their beneficial learning properties, excellent empirical results, and easy-to-incorporate nature when building varied neural networks. We find that our Riemannian ResNets mirror these desirable properties: when compared to existing manifold neural networks designed to learn over hyperbolic space and the manifold of symmetric positive definite matrices, we outperform both kinds of networks in terms of relevant testing metrics and training dynamics.
</details>
<details>
<summary>摘要</summary>
现代几何深度学习方法已经引入了许多神经网络操作于偏射抽象空间上的数据。这些神经网络经常用于学习具有层次结构的图或者学习自然科学中遇到的拟合空间上的数据。这些神经网络通常是基于标准欧几何网络的扩展，但扩展到普通的欧几何空间是困难的，只有对一些特殊的欧几何空间进行了扩展。在这个工作中，我们研究了剩余神经网络（ResNet）的扩展，并证明了这种扩展方法可以在一般的偏射抽象空间上进行地理emetric的扩展。原本是解决减速问题的概念，ResNet在机器学习中得到了广泛的应用，因为它具有良好的学习性、优秀的实验性和容易整合到不同神经网络中的特点。我们发现，我们的偏射抽象空间上的ResNet和已有的欧几何空间上的神经网络相比，在相关的测试指标和训练动态上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Ring-A-Bell-How-Reliable-are-Concept-Removal-Methods-for-Diffusion-Models"><a href="#Ring-A-Bell-How-Reliable-are-Concept-Removal-Methods-for-Diffusion-Models" class="headerlink" title="Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?"></a>Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10012">http://arxiv.org/abs/2310.10012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang</li>
<li>for: 本研究旨在调查 diffusion models 的安全机制，以确保它们不会生成不适或有害内容。</li>
<li>methods: 我们提出了一种新的概念检索算法，可以评估 diffusion models 的安全性。该算法首先提取敏感或不适的概念，然后使用这些概念来自动标识 diffusion models 中可能生成不适内容的提问。</li>
<li>results: 我们的研究表明， Ring-A-Bell 可以 manipulate 安全提问 benchmarks，使得原本被视为安全的提问可以逃脱现有的安全机制，并生成不适或有害内容。这表明现有的安全机制并不够，需要进一步改进。<details>
<summary>Abstract</summary>
Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents.
</details>
<details>
<summary>摘要</summary>
Diffusion模型 для文本到图像（T2I）合成，如稳定扩散（SD），最近已经展示出了高质量内容的生成能力。然而，这种进步也引起了许多关于可能的不当使用的担忧，特别是在生成版权、禁止或限制的内容，或者NSFW（不适合工作）图像。虽有尝试了对这些问题进行缓解，例如在评估阶段实施安全筛选或者 Fine-tune模型以消除不жела的概念或风格，但是这些安全措施在各种提示下的效果仍然未经充分探索。在这项工作中，我们目的是调查这些安全机制。我们提出了一种新的概念检索算法，用于评估T2I扩散模型的安全性。我们称之为“铃铛”（Ring-A-Bell），它是一种无关模型的红Team工具，可以在提前准备的情况下完全无需先知Target模型来进行评估。具体来说，“铃铛”首先从敏感和不适合内容中提取概念，然后利用提取到的概念来自动识别扩散模型中的问题提示，并生成相应的不适合内容。这样，用户可以评估 deployed safety mechanisms的可靠性。最后，我们经验 validate我们的方法，测试在线服务如midjourney和不同的概念 removalfrom。我们的结果表明，“铃铛”可以通过修改安全提示benchmark，将原本被视为安全的提示转变为扩散模型生成不适合内容，因此揭示了现有的安全机制的缺陷，这些缺陷可能导致生成危害内容。
</details></li>
</ul>
<hr>
<h2 id="Implicit-regularization-via-soft-ascent-descent"><a href="#Implicit-regularization-via-soft-ascent-descent" class="headerlink" title="Implicit regularization via soft ascent-descent"></a>Implicit regularization via soft ascent-descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10006">http://arxiv.org/abs/2310.10006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feedbackward/bdd-flood">https://github.com/feedbackward/bdd-flood</a></li>
<li>paper_authors: Matthew J. Holland, Kosuke Nakatani</li>
<li>for: 提高机器学习过程中的OFF-sample泛化性能，避免过多的试错和重复。</li>
<li>methods: 使用Gradient Regularization的softened、点 wise机制，以降低边缘点的影响和抑制异常值的影响。</li>
<li>results: 与SAM和Flooding相比，SoftAD可以实现类比的分类精度，同时具有远小的损失泛化差和模型评价。<details>
<summary>Abstract</summary>
As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows. As a proxy for the well-studied heuristic of seeking "flat" local minima, gradient regularization is a natural avenue, and first-order approximations such as Flooding and sharpness-aware minimization (SAM) have received significant attention, but their performance depends critically on hyperparameters (flood threshold and neighborhood radius, respectively) that are non-trivial to specify in advance. In order to develop a procedure which is more resilient to misspecified hyperparameters, with the hard-threshold "ascent-descent" switching device used in Flooding as motivation, we propose a softened, pointwise mechanism called SoftAD that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. We contrast formal stationarity guarantees with those for Flooding, and empirically demonstrate how SoftAD can realize classification accuracy competitive with SAM and Flooding while maintaining a much smaller loss generalization gap and model norm. Our empirical tests range from simple binary classification on the plane to image classification using neural networks with millions of parameters; the key trends are observed across all datasets and models studied, and suggest a potential new approach to implicit regularization.
</details>
<details>
<summary>摘要</summary>
To overcome this limitation, we propose a softened, pointwise mechanism called SoftAD, which downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. By comparing the formal stationarity guarantees of SoftAD with those of Flooding, we demonstrate that SoftAD can achieve classification accuracy competitive with SAM and Flooding while maintaining a much smaller loss generalization gap and model norm.Our empirical tests cover a range of datasets and models, from simple binary classification on the plane to image classification using neural networks with millions of parameters. The key trends observed across all datasets and models suggest a potential new approach to implicit regularization.
</details></li>
</ul>
<hr>
<h2 id="Conformal-Contextual-Robust-Optimization"><a href="#Conformal-Contextual-Robust-Optimization" class="headerlink" title="Conformal Contextual Robust Optimization"></a>Conformal Contextual Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10003">http://arxiv.org/abs/2310.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Patel, Sahana Rayan, Ambuj Tewari</li>
<li>for: 这个论文是为了解决决策问题，具体来说是使用数据驱动方法来避免对不确定性范围的误差，从而提高决策的优化。</li>
<li>methods: 这个论文使用的方法是基于Conditional Generative Model的高维空间中的非 conjugate 预测区域，这些预测区域具有 Desired distribution-free coverage guarantees。</li>
<li>results: 研究人员通过在一系列的 simulations-based inference benchmark tasks和基于气象预测的交通路径规划问题来展示 CPO 框架的效果，并提供了semantically meaningful的视觉总结来解释决策的优化。<details>
<summary>Abstract</summary>
Data-driven approaches to predict-then-optimize decision-making problems seek to mitigate the risk of uncertainty region misspecification in safety-critical settings. Current approaches, however, suffer from considering overly conservative uncertainty regions, often resulting in suboptimal decisionmaking. To this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for leveraging highly informative, nonconvex conformal prediction regions over high-dimensional spaces based on conditional generative models, which have the desired distribution-free coverage guarantees. Despite guaranteeing robustness, such black-box optimization procedures alone inspire little confidence owing to the lack of explanation of why a particular decision was found to be optimal. We, therefore, augment CPO to additionally provide semantically meaningful visual summaries of the uncertainty regions to give qualitative intuition for the optimal decision. We highlight the CPO framework by demonstrating results on a suite of simulation-based inference benchmark tasks and a vehicle routing task based on probabilistic weather prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用数据驱动的方法来解决决策问题，以减少不确定性区域的误差，在安全关键的场景中非常重要。现有的方法frequently suffer from considering overly conservative uncertainty regions, often resulting in suboptimal decision-making. To address this, we propose Conformal-Predict-Then-Optimize (CPO), a framework that leverages highly informative, nonconvex conformal prediction regions over high-dimensional spaces based on conditional generative models, which provide desired distribution-free coverage guarantees. Despite providing robustness, such black-box optimization procedures alone may lack confidence due to the lack of explanation of why a particular decision was found to be optimal. We, therefore, augment CPO with additional provision of semantically meaningful visual summaries of the uncertainty regions to provide qualitative intuition for the optimal decision. We demonstrate the effectiveness of the CPO framework through results on a suite of simulation-based inference benchmark tasks and a vehicle routing task based on probabilistic weather prediction.</SYS>>Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Outlier-Detection-Using-Generative-Models-with-Theoretical-Performance-Guarantees"><a href="#Outlier-Detection-Using-Generative-Models-with-Theoretical-Performance-Guarantees" class="headerlink" title="Outlier Detection Using Generative Models with Theoretical Performance Guarantees"></a>Outlier Detection Using Generative Models with Theoretical Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09999">http://arxiv.org/abs/2310.09999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jirong Yi, Jingchao Gao, Tianming Wang, Xiaodong Wu, Weiyu Xu</li>
<li>for: 这篇论文考虑了模拟器模型中的信号恢复问题，具体来说是在线性测量中受到稀疏异常的情况下恢复原始信号。</li>
<li>methods: 我们提出了一种异常检测方法，可以在模拟器模型下恢复原始信号，并且我们提供了有关信号恢复的理论保证。</li>
<li>results: 我们的实验结果表明，使用我们的方法可以成功恢复信号，即使在稀疏异常的情况下。我们的方法比传统的lasso和平方$\ell_2$最小化方法更高效。<details>
<summary>Abstract</summary>
This paper considers the problem of recovering signals modeled by generative models from linear measurements contaminated with sparse outliers. We propose an outlier detection approach for reconstructing the ground-truth signals modeled by generative models under sparse outliers. We establish theoretical recovery guarantees for reconstruction of signals using generative models in the presence of outliers, giving lower bounds on the number of correctable outliers. Our results are applicable to both linear generator neural networks and the nonlinear generator neural networks with an arbitrary number of layers. We propose an iterative alternating direction method of multipliers (ADMM) algorithm for solving the outlier detection problem via $\ell_1$ norm minimization, and a gradient descent algorithm for solving the outlier detection problem via squared $\ell_1$ norm minimization. We conduct extensive experiments using variational auto-encoder and deep convolutional generative adversarial networks, and the experimental results show that the signals can be successfully reconstructed under outliers using our approach. Our approach outperforms the traditional Lasso and $\ell_2$ minimization approach.
</details>
<details>
<summary>摘要</summary>
We propose two algorithms to solve the outlier detection problem: an iterative alternating direction method of multipliers (ADMM) algorithm that minimizes the $\ell_1$ norm, and a gradient descent algorithm that minimizes the squared $\ell_1$ norm. We conduct extensive experiments using variational auto-encoders and deep convolutional generative adversarial networks, and the results show that our approach can successfully reconstruct the signals even under outliers. Our approach outperforms traditional Lasso and $\ell_2$ minimization methods.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Machine-Learning-in-Biopharmaceutical-Process-Development-and-Manufacturing-Current-Trends-Challenges-and-Opportunities"><a href="#Applications-of-Machine-Learning-in-Biopharmaceutical-Process-Development-and-Manufacturing-Current-Trends-Challenges-and-Opportunities" class="headerlink" title="Applications of Machine Learning in Biopharmaceutical Process Development and Manufacturing: Current Trends, Challenges, and Opportunities"></a>Applications of Machine Learning in Biopharmaceutical Process Development and Manufacturing: Current Trends, Challenges, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09991">http://arxiv.org/abs/2310.09991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanh Tung Khuat, Robert Bassett, Ellen Otte, Alistair Grevis-James, Bogdan Gabrys</li>
<li>for: 本研究旨在提供一个全面的机器学习（ML）解决方案在生物医药领域的应用现状，包括生物产品设计、监测、控制和优化的过程中的应用。</li>
<li>methods: 本研究使用的方法包括机器学习模型的采用，以提高生物医药生产过程中的分析、监测和控制能力。</li>
<li>results: 本研究结果表明，机器学习模型在生物医药生产过程中的应用可以提高生产效率、产品质量和生产可靠性等方面的表现。同时，本研究还揭示了生物医药过程数据的复杂性和多维性，以及机器学习模型在生物医药过程中的挑战和限制。<details>
<summary>Abstract</summary>
While machine learning (ML) has made significant contributions to the biopharmaceutical field, its applications are still in the early stages in terms of providing direct support for quality-by-design based development and manufacturing of biopharmaceuticals, hindering the enormous potential for bioprocesses automation from their development to manufacturing. However, the adoption of ML-based models instead of conventional multivariate data analysis methods is significantly increasing due to the accumulation of large-scale production data. This trend is primarily driven by the real-time monitoring of process variables and quality attributes of biopharmaceutical products through the implementation of advanced process analytical technologies. Given the complexity and multidimensionality of a bioproduct design, bioprocess development, and product manufacturing data, ML-based approaches are increasingly being employed to achieve accurate, flexible, and high-performing predictive models to address the problems of analytics, monitoring, and control within the biopharma field. This paper aims to provide a comprehensive review of the current applications of ML solutions in a bioproduct design, monitoring, control, and optimisation of upstream, downstream, and product formulation processes. Finally, this paper thoroughly discusses the main challenges related to the bioprocesses themselves, process data, and the use of machine learning models in biopharmaceutical process development and manufacturing. Moreover, it offers further insights into the adoption of innovative machine learning methods and novel trends in the development of new digital biopharma solutions.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在生物医药领域已经做出了重要贡献，但是其应用还处于初期阶段，对生物医药生产的质量设计和生产进行直接支持的应用还尚未发挥出大量潜力。然而，由于生产数据的积累，ML模型的应用正在不断增加，取代传统的多变量数据分析方法。这种趋势主要归功于实时监测生产过程中变量和产品质量特征的实施，以及高级进程分析技术的普及。由于生物产品设计、生产和加工数据的复杂性和多维性，ML方法在解决生物过程数据分析、监测和控制方面提供了高精度、灵活性和高性能的预测模型。本文旨在为读者提供生物产品设计、监测、控制和优化过程中机器学习解决方案的全面审视。此外，本文还详细讨论了生物过程本身、数据和机器学习模型在生物医药过程开发和生产中的主要挑战，以及采用创新的机器学习方法和新趋势在生物医药领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Personalization-of-CTC-based-End-to-End-Speech-Recognition-Using-Pronunciation-Driven-Subword-Tokenization"><a href="#Personalization-of-CTC-based-End-to-End-Speech-Recognition-Using-Pronunciation-Driven-Subword-Tokenization" class="headerlink" title="Personalization of CTC-based End-to-End Speech Recognition Using Pronunciation-Driven Subword Tokenization"></a>Personalization of CTC-based End-to-End Speech Recognition Using Pronunciation-Driven Subword Tokenization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09988">http://arxiv.org/abs/2310.09988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihong Lei, Ernest Pusateri, Shiyi Han, Leo Liu, Mingbin Xu, Tim Ng, Ruchir Travadi, Youyuan Zhang, Mirko Hannemann, Man-Hung Siu, Zhen Huang</li>
<li>for: 这个论文旨在提高端到端语音识别系统的个性化性，使其能够更准确地识别个人内容，如联系人姓名。</li>
<li>methods: 该论文基于连接主义时间分类的技术，提出了一种生成个人实体唤起的新的子词tokenization方法。此外，该论文还使用了两种已知技术：上下文偏移和词段均衡。</li>
<li>results: 根据论文的表述，使用这些技术组合后，个人名实体识别精度与一个竞争性hybrid系统相当。<details>
<summary>Abstract</summary>
Recent advances in deep learning and automatic speech recognition have improved the accuracy of end-to-end speech recognition systems, but recognition of personal content such as contact names remains a challenge. In this work, we describe our personalization solution for an end-to-end speech recognition system based on connectionist temporal classification. Building on previous work, we present a novel method for generating additional subword tokenizations for personal entities from their pronunciations. We show that using this technique in combination with two established techniques, contextual biasing and wordpiece prior normalization, we are able to achieve personal named entity accuracy on par with a competitive hybrid system.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的深度学习和自动语音识别技术的进步，已经提高了端到端语音识别系统的准确率，但是个人内容such as contact names仍然是一个挑战。在这项工作中，我们描述了基于连接主义时间分类的个人化解决方案。基于之前的工作，我们提出了一种新的方法，通过个人实体的发音来生成额外的子字符串拼接。我们表明，使用这种技术与两种已知技术，Contextual biasing和wordpiece prior normalization，可以达到与竞争性混合系统相同的个人名实体准确率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.LG_2023_10_16/" data-id="clpxp6c4p00tmee886rhtgdnh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/eess.IV_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T09:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/eess.IV_2023_10_16/">eess.IV - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Overcoming-the-Rayleigh-limit-in-extremely-low-SNR"><a href="#Overcoming-the-Rayleigh-limit-in-extremely-low-SNR" class="headerlink" title="Overcoming the Rayleigh limit in extremely low SNR"></a>Overcoming the Rayleigh limit in extremely low SNR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10633">http://arxiv.org/abs/2310.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunsoo Choi, Seungman Choi, Peter Menart, Angshuman Deka, Zubin Jacob<br>for:* 这个论文的目的是开发一种新的随机子衰减图像重构算法（SSRI），以优化低信号响应率（SNR）和分辨率较低的光学成像系统。methods:* 该算法利用了常见的成像设备，使其在实际应用中易于适应。results:* 对于多种挑战性的场景，如非常低的SNR水平和较大的相对亮度比，SSRI算法表现出色，超过了已知的理德兹逊-卢西（Richardson-Lucy）减 convolution和CLEAN算法。* SSRI算法在实验图像中成功地估计了点源的位置、亮度和数量，并且在SNR水平低于1.2和子衰减范围内表现出80%-40%的成功率，位偏误在2.5像素以下。<details>
<summary>Abstract</summary>
Overcoming the diffraction limit and addressing low Signal-to-Noise Ratio (SNR) scenarios have posed significant challenges to optical imaging systems in applications such as medical diagnosis, remote sensing, and astronomical observations. In this study, we introduce a novel Stochastic Sub-Rayleigh Imaging (SSRI) algorithm capable of localizing point sources and estimating their positions, brightness, and number in low SNR conditions and within the diffraction limit. The SSRI algorithm utilizes conventional imaging devices, facilitating practical and adaptable solutions for real-world applications. Through extensive experimentation, we demonstrate that our proposed method outperforms established algorithms, such as Richardson-Lucy deconvolution and CLEAN, in various challenging scenarios, including extremely low SNR conditions and large relative brightness ratios. We achieved between 40% and 80% success rate in estimating the number of point sources in experimental images with SNR less than 1.2 and sub-Rayleigh separations, with mean position errors less than 2.5 pixels. In the same conditions, the Richardson-Lucy and CLEAN algorithms correctly estimated the number of sources between 0% and 10% of the time, with mean position errors greater than 5 pixels. Notably, SSRI consistently performs well even in the sub-Rayleigh region, offering a benchmark for assessing future quantum superresolution techniques. In conclusion, the SSRI algorithm presents a significant advance in overcoming diffraction limitations in optical imaging systems, particularly under low SNR conditions, with potential widespread impact across multiple fields like biomedical microscopy and astronomical imaging.
</details>
<details>
<summary>摘要</summary>
超过 diffraction limit 和低信号噪比 (SNR) 场景下，光学成像系统在医疗诊断、远程探测和天文观测等领域中受到了重大挑战。在这种研究中，我们介绍了一种新的 Stochastic Sub-Rayleigh Imaging（SSRI）算法，能够在低 SNR 条件下和 diffraction limit 内 Localize 点源并估计其位置、亮度和数量。SSRI 算法可以使用普通的成像设备，提供了实用和适应的解决方案。经过广泛的实验，我们证明了我们的提posed方法在各种挑战性enario中都能够超过Richardson-Lucy 混合和 CLEAN 算法，包括 extremely low SNR 条件下和大relative brightness ratio。我们在实验图像中成功地估计了40%到80%的点源数量，位置误差在2.5 pix 左右，而Richardson-Lucy 和 CLEAN 算法只能在0%到10%的时间内正确地估计点源数量，位置误差大于5 pix。特别是，SSRI 在 sub-Rayleigh 区域中表现良好，为未来 quantum superresolution 技术的评估提供了标准。综上所述，SSRI 算法在光学成像系统中超过 diffraction limit 的能力，特别是在低 SNR 条件下，具有广泛的应用前景，如生物微scopy 和天文成像。
</details></li>
</ul>
<hr>
<h2 id="NeuroQuantify-–-An-Image-Analysis-Software-for-Detection-and-Quantification-of-Neurons-and-Neurites-using-Deep-Learning"><a href="#NeuroQuantify-–-An-Image-Analysis-Software-for-Detection-and-Quantification-of-Neurons-and-Neurites-using-Deep-Learning" class="headerlink" title="NeuroQuantify – An Image Analysis Software for Detection and Quantification of Neurons and Neurites using Deep Learning"></a>NeuroQuantify – An Image Analysis Software for Detection and Quantification of Neurons and Neurites using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10978">http://arxiv.org/abs/2310.10978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/StanleyZ0528/neural-image-segmentation">https://github.com/StanleyZ0528/neural-image-segmentation</a></li>
<li>paper_authors: Ka My Dang, Yi Jia Zhang, Tianchen Zhang, Chao Wang, Anton Sinner, Piero Coronica, Joyce K. S. Poon</li>
<li>for: 研究neuronal networks的发展和neuron growth的量化信息</li>
<li>methods: 使用深度学习自动分类 cells和neurites</li>
<li>results: 可以快速和高效地分类 cells和neurites，并提供neurite length和orientation的量化信息<details>
<summary>Abstract</summary>
The segmentation of cells and neurites in microscopy images of neuronal networks provides valuable quantitative information about neuron growth and neuronal differentiation, including the number of cells, neurites, neurite length and neurite orientation. This information is essential for assessing the development of neuronal networks in response to extracellular stimuli, which is useful for studying neuronal structures, for example, the study of neurodegenerative diseases and pharmaceuticals. However, automatic and accurate analysis of neuronal structures from phase contrast images has remained challenging. To address this, we have developed NeuroQuantify, an open-source software that uses deep learning to efficiently and quickly segment cells and neurites in phase contrast microscopy images. NeuroQuantify offers several key features: (i) automatic detection of cells and neurites; (ii) post-processing of the images for the quantitative neurite length measurement based on segmentation of phase contrast microscopy images, and (iii) identification of neurite orientations. The user-friendly NeuroQuantify software can be installed and freely downloaded from GitHub https://github.com/StanleyZ0528/neural-image-segmentation.
</details>
<details>
<summary>摘要</summary>
segmenation of cells and neurites in microscopy images of neuronal networks provides valuable quantitative information about neuron growth and neuronal differentiation, including the number of cells, neurites, neurite length and neurite orientation. This information is essential for assessing the development of neuronal networks in response to extracellular stimuli, which is useful for studying neuronal structures, for example, the study of neurodegenerative diseases and pharmaceuticals. However, automatic and accurate analysis of neuronal structures from phase contrast images has remained challenging. To address this, we have developed NeuroQuantify, an open-source software that uses deep learning to efficiently and quickly segment cells and neurites in phase contrast microscopy images. NeuroQuantify offers several key features: (i) automatic detection of cells and neurites; (ii) post-processing of the images for the quantitative neurite length measurement based on segmentation of phase contrast microscopy images, and (iii) identification of neurite orientations. The user-friendly NeuroQuantify software can be installed and freely downloaded from GitHub https://github.com/StanleyZ0528/neural-image-segmentation.Here's the word-for-word translation of the text into Simplified Chinese: cells 和 neurites 的分 segmentation 在 neuronal networks 的 microscopy 图像中提供了有价值的量化信息，包括细胞数量、辐化长度、辐化方向等。这些信息对于研究 neuronal networks 的发展响应 extracellular stimuli 非常重要，这些信息可以用于研究 neuronal structures，例如研究 neuodegenerative diseases 和 pharmaceuticals。然而，从 phase contrast 图像中自动和准确地分析 neuronal structures 一直是一个挑战。为解决这个问题，我们已经开发了 NeuroQuantify，一个开源的软件，使用深度学习来快速和高效地分 segmentation 细胞和 neurites 在 phase contrast microscopy 图像中。NeuroQuantify 提供了多个关键特性： (i) 自动检测细胞和辐化； (ii) 根据 segmentation 的图像进行后处理，以获取辐化长度的量化测量；以及 (iii) 辐化方向的识别。用户友好的 NeuroQuantify 软件可以在 GitHub 上免费下载 https://github.com/StanleyZ0528/neural-image-segmentation。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Data-Synthesis-Strategies-for-the-Classification-of-Craniosynostosis"><a href="#Impact-of-Data-Synthesis-Strategies-for-the-Classification-of-Craniosynostosis" class="headerlink" title="Impact of Data Synthesis Strategies for the Classification of Craniosynostosis"></a>Impact of Data Synthesis Strategies for the Classification of Craniosynostosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10199">http://arxiv.org/abs/2310.10199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kit-ibt/craniosource-gan-pca-ssm">https://github.com/kit-ibt/craniosource-gan-pca-ssm</a></li>
<li>paper_authors: Matthias Schaufelberger, Reinald Peter Kühle, Andreas Wachter, Frederic Weichel, Niclas Hagen, Friedemann Ringwald, Urs Eisenmann, Jürgen Hoffmann, Michael Engel, Christian Freudlsperger, Werner Nahm</li>
<li>for: 用于评估和分类颅部凹陷症。</li>
<li>methods: 使用三种不同的人工数据源：统计学形态模型（SSM）、生成敌对网络（GAN）和图像基本主成分分析，为一种基于 convolutional neural network（CNN）的颅部凹陷症分类。CNN 只在人工数据上训练，但 Validate 和测试在临床数据上。</li>
<li>results: 组合 SSM 和 GAN 达到了高于 0.96 的准确率和高于 0.95 的 F1 分数在未看到的测试集上。与训练在临床数据上的差异小于 0.01。包括第二个图像模式可以提高分类性能。So the three key points are:1. The paper is written to assess and classify craniosynostosis using photogrammetric surface scans.2. The methods used include three different synthetic data sources: a statistical shape model, a generative adversarial network, and image-based principal component analysis.3. The results show that a combination of these synthetic data sources can achieve high accuracy and F1 score (over 0.95 and 0.96 respectively) on unseen test sets, with little difference between training on synthetic data and clinical data.<details>
<summary>Abstract</summary>
Introduction: Photogrammetric surface scans provide a radiation-free option to assess and classify craniosynostosis. Due to the low prevalence of craniosynostosis and high patient restrictions, clinical data is rare. Synthetic data could support or even replace clinical data for the classification of craniosynostosis, but this has never been studied systematically. Methods: We test the combinations of three different synthetic data sources: a statistical shape model (SSM), a generative adversarial network (GAN), and image-based principal component analysis for a convolutional neural network (CNN)-based classification of craniosynostosis. The CNN is trained only on synthetic data, but validated and tested on clinical data. Results: The combination of a SSM and a GAN achieved an accuracy of more than 0.96 and a F1-score of more than 0.95 on the unseen test set. The difference to training on clinical data was smaller than 0.01. Including a second image modality improved classification performance for all data sources. Conclusion: Without a single clinical training sample, a CNN was able to classify head deformities as accurate as if it was trained on clinical data. Using multiple data sources was key for a good classification based on synthetic data alone. Synthetic data might play an important future role in the assessment of craniosynostosis.
</details>
<details>
<summary>摘要</summary>
方法：我们测试了三种不同的生成数据源：统计学形态模型（SSM）、生成对抗网络（GAN）和图像基于主成分分析（PCA），用于基于 convolutional neural network（CNN）的颅部缺陷分类。CNN只在生成数据上训练，但VALIDATE和测试在临床数据上进行验证。结果：SSM和GAN的组合实现了在未看到的测试集上的准确率高于0.96和F1分数高于0.95。与训练在临床数据上的差异小于0.01。包括第二个图像特征提高了所有数据源的分类性能。结论：没有任何临床训练样本，CNN仍可以准确地将头形缺陷分类为临床数据。使用多种数据源是针对synthetic数据alone的分类的关键。synthetic数据可能在未来对颅部缺陷的评估中扮演一个重要的角色。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/eess.IV_2023_10_16/" data-id="clpxp6cbw01c7ee882qtie2ep" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/eess.SP_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T08:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/eess.SP_2023_10_16/">eess.SP - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Rapid-Non-cartesian-Reconstruction-Using-an-Implicit-Representation-of-GROG-Kernels"><a href="#Rapid-Non-cartesian-Reconstruction-Using-an-Implicit-Representation-of-GROG-Kernels" class="headerlink" title="Rapid Non-cartesian Reconstruction Using an Implicit Representation of GROG Kernels"></a>Rapid Non-cartesian Reconstruction Using an Implicit Representation of GROG Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10823">http://arxiv.org/abs/2310.10823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Abraham, Mark Nishimura, Xiaozhi Cao, Congyu Liao, Kawin Setsompop</li>
<li>for: 提高MR图像成像的速度和效率，使非 carteesian sampling更广泛应用</li>
<li>methods: 使用iGROG方法将非 carteesian数据转换为cartesian数据，以便更加简单和快速地进行重建</li>
<li>results: 提高了MR图像成像的速度和效率，并且可以更好地抵消运动 artifacts<details>
<summary>Abstract</summary>
MRI data is acquired in Fourier space. Data acquisition is typically performed on a Cartesian grid in this space to enable the use of a fast Fourier transform algorithm to achieve fast and efficient reconstruction. However, it has been shown that for multiple applications, non-Cartesian data acquisition can improve the performance of MR imaging by providing fast and more efficient data acquisition, and improving motion robustness. Nonetheless, the image reconstruction process of non-Cartesian data is more involved and can be time-consuming, even through the use of efficient algorithms such as non-uniform FFT (NUFFT). This work provides an efficient approach (iGROG) to transform the non-Cartesian data into Cartesian data, to achieve simpler and faster reconstruction which should help enable non-Cartesian data sampling to be performed more widely in MRI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Constant-Modulus-Waveform-Design-with-Block-Level-Interference-Exploitation-for-DFRC-Systems"><a href="#Constant-Modulus-Waveform-Design-with-Block-Level-Interference-Exploitation-for-DFRC-Systems" class="headerlink" title="Constant Modulus Waveform Design with Block-Level Interference Exploitation for DFRC Systems"></a>Constant Modulus Waveform Design with Block-Level Interference Exploitation for DFRC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10804">http://arxiv.org/abs/2310.10804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byunghyun Lee, Anindya Bijoy Das, David J. Love, Christopher G. Brinton, James V. Krogmeier</li>
<li>for: 这篇论文旨在设计具有双 функ数 radar-通信（DFRC）系统的常数模ulus波形。</li>
<li>methods: 本文使用了相互干扰基于封页水平 precoding（CI-BLP）来利用多用户和雷达传输所带来的歪曲。我们还提出了一个基于主要化-最小化（MM）的解决方案，并使用了一个改进的主要化函数，充分利用了一个新的 діагональ矩阵结构。</li>
<li>results: 透过严谨的 simulations，我们证明了提案的方法和主要化函数的效果。<details>
<summary>Abstract</summary>
Dual-functional radar-communication (DFRC) is a promising technology where radar and communication functions operate on the same spectrum and hardware. In this paper, we propose an algorithm for designing constant modulus waveforms for DFRC systems. Particularly, we jointly optimize the correlation properties and the spatial beam pattern. For communication, we employ constructive interference-based block-level precoding (CI-BLP) to exploit distortion due to multi-user and radar transmission. We propose a majorization-minimization (MM)-based solution to the formulated problem. To accelerate convergence, we propose an improved majorizing function that leverages a novel diagonal matrix structure. We then evaluate the performance of the proposed algorithm through rigorous simulations. Simulation results demonstrate the effectiveness of the proposed approach and the proposed majorizer.
</details>
<details>
<summary>摘要</summary>
双功能雷达通信（DFRC）技术是一种有前途的技术，雷达和通信功能都运行在同一频谱和硬件上。在这篇论文中，我们提出了一种常数模式波形设计算法，特别是同时优化相关性和空间扫描方式。在通信方面，我们使用基于构建性干扰的块级预编码（CI-BLP）来利用多用户和雷达传输所导致的扭曲。我们提出了一种基于主要化-最小化（MM）的解决方案，并提出了一种改进的主要化函数，利用了一个新的对角矩阵结构。然后，我们通过严格的仿真测试评估了提案的性能和提案的主要化函数。Here's the text with some additional information about the Simplified Chinese translation:Simplified Chinese is a written form of Chinese that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and Singapore.In this translation, I have used Simplified Chinese characters and grammar to translate the text. However, I have kept the original English sentence structure and phrasing to ensure that the meaning of the text is preserved.Note that some technical terms and jargon may have different translations in Simplified Chinese, depending on the context and the specific field of study. However, I have tried my best to provide an accurate and natural-sounding translation based on my knowledge of Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Place-Cells"><a href="#Neuromorphic-Place-Cells" class="headerlink" title="Neuromorphic Place Cells"></a>Neuromorphic Place Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10790">http://arxiv.org/abs/2310.10790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoqi Chen, Ralph Etienne-Cummings</li>
<li>for: 这个脑机模型系统可能比传统系统更加高效实现。</li>
<li>methods: 我们实现了混合模式的空间编码神经元，包括theta细胞、vector细胞和place细胞。这些神经元组成了生物学可能的网络，可以重produce地方Cells的localization功能。</li>
<li>results: 我们的模型在Analog Circuit变化时的Robustness得到了实验 validate。我们提供了动态脑机SLAM系统的实现基础和生物学形成空间细胞的灵感。<details>
<summary>Abstract</summary>
A neuromorphic SLAM system shows potential for more efficient implementation than its traditional counterpart. We demonstrate a mixed-mode implementation for spatial encoding neurons including theta cells, vector cells and place cells. Together, they form a biologically plausible network that could reproduce the localization functionality of place cells. Experimental results validate the robustness of our model when suffering from variations of analog circuits. We provide a foundation for implementing dynamic neuromorphic SLAM systems and inspirations for the formation of spatial cells in biology.
</details>
<details>
<summary>摘要</summary>
一种神经模拟SLAM系统显示了更高效的实现可能性，相比传统系统。我们实现了混合模式的空间编码神经元，包括theta细胞、向量细胞和位置细胞。这些神经元共同组成了生物学上可能的网络，可以重现位置细胞的地方化功能。实验结果证明我们的模型在 анаóg逻circuit变化时的稳定性。我们提供了神经模拟SLAM系统的实现基础和生物学形成空间细胞的灵感。Note: "Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Indoor-Wireless-Signal-Modeling-with-Smooth-Surface-Diffraction-Effects"><a href="#Indoor-Wireless-Signal-Modeling-with-Smooth-Surface-Diffraction-Effects" class="headerlink" title="Indoor Wireless Signal Modeling with Smooth Surface Diffraction Effects"></a>Indoor Wireless Signal Modeling with Smooth Surface Diffraction Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10578">http://arxiv.org/abs/2310.10578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichen Wang, Samuel Audia, Dinesh Manocha</li>
<li>for: 提高室内电磁场模拟的准确性，包括表面散射的影响</li>
<li>methods: 使用统一几何理论 Of Diffraction (UTD) 表面散射，并提出了精炼表面 UTD 和高效计算射线路的技术</li>
<li>results: 提高阴影区预测功率约 5dB，并能够捕捉到阴影区之外的复杂场效应，并且在不同室内场景下表现出60%更快的计算速度。<details>
<summary>Abstract</summary>
We present a novel algorithm that enhances the accuracy of electromagnetic field simulations in indoor environments by incorporating the Uniform Geometrical Theory of Diffraction (UTD) for surface diffraction. This additional diffraction phenomenology is important for the design of modern wireless systems and allows us to capture the effects of more complex scene geometries. Central to our methodology is the Dynamic Coherence-Based EM Ray Tracing Simulator (DCEM), and we augment that formulation with smooth surface UTD and present techniques to efficiently compute the ray paths. We validate our additions by comparing them to analytical solutions of a sphere, method of moments solutions from FEKO, and ray-traced indoor scenes from WinProp. Our algorithm improves shadow region predicted powers by about 5dB compared to our previous work, and captures nuanced field effects beyond shadow boundaries. We highlight the performance on different indoor scenes and observe 60% faster computation time over WinProp.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法，用于提高室内电磁场 simulations 的准确性，通过包含表面折射理论（UTD）的各向异性折射现象。这种额外的折射现象对现代无线系统设计非常重要，允许我们捕捉更复杂的场景几何。我们的方法中心是动态几何相关性基于EM射线追踪模拟器（DCEM），并在该形式ulation中添加了光滑表面UTD。我们还提供了有效计算射线路的技术。我们的添加与 Analytical solutions of a sphere、method of moments solutions from FEKO和WinProp的射线追踪场景进行比较。我们的算法可以在不同的室内场景中提高阴影区域预测功率约5dB，并捕捉到场效应 beyond shadow boundaries。我们还证明了我们的算法在不同的室内场景中的性能，并发现其计算时间比WinProp快60%。
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Distributed-Machine-Learning-for-the-Internet-of-Things-A-Comprehensive-Survey"><a href="#Applications-of-Distributed-Machine-Learning-for-the-Internet-of-Things-A-Comprehensive-Survey" class="headerlink" title="Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey"></a>Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10549">http://arxiv.org/abs/2310.10549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mai Le, Thien Huynh-The, Tan Do-Duy, Thai-Hoc Vu, Won-Joo Hwang, Quoc-Viet Pham</li>
<li>for: 提高 emerging 无线网络（如 beyond 5G 和 6G）中的服务和应用程序的质量，通过在互联网物联网（IoT）中使用人工智能（AI）。</li>
<li>methods: 分布式机器学习（distributed learning）方法，包括联邦学习、多代理奖励学习和分布式推理。</li>
<li>results: 对 IoT 服务和应用程序的重要提高，包括数据共享和计算卸载、定位、移动 Crowdsensing 和安全隐私。<details>
<summary>Abstract</summary>
The emergence of new services and applications in emerging wireless networks (e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoT). However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications. Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches. This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks. In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference. Then, we provide an extensive review of distributed learning for critical IoT services (e.g., data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry). From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.
</details>
<details>
<summary>摘要</summary>
随着新的服务和应用程序在 развивающихся无线网络（例如 beyond 5G 和 6G）的出现，人们对艺ificial intelligence（AI）在互联网物联网（IoT）中的使用的需求在增加。然而，质量的巨大 IoT 连接和未来 IoT 系统中的计算资源的分布强烈要求开发分布式 AI，以提供更好的 IoT 服务和应用程序。因此，现有的 AI 启用 IoT 系统可以通过实施分布式机器学习（分布式学习）方法进行增强。本工作的目的是为提供分布式学习在 IoT 服务和应用程序方面的全面的评价。特别是，我们首先提供机器学习的背景，然后介绍一些常见的分布式学习方法，如联合学习、多代理人奖励学习和分布式推理。然后，我们对分布式学习在关键的 IoT 服务（例如数据分享和计算卸载、位置定位、移动 Crowdsensing 和安全隐私）和 IoT 应用程序（例如智能医疗、智能电网、自动驾驶、空中 IoT 网络和智能工业）进行了广泛的评审。从 Literature 中，我们还提出了分布式学习在 IoT 中的主要挑战和一些可能的解决方案和研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Tutorial-on-Chirp-Spread-Spectrum-for-LoRaWAN-Basics-and-Key-Advances"><a href="#A-Tutorial-on-Chirp-Spread-Spectrum-for-LoRaWAN-Basics-and-Key-Advances" class="headerlink" title="A Tutorial on Chirp Spread Spectrum for LoRaWAN: Basics and Key Advances"></a>A Tutorial on Chirp Spread Spectrum for LoRaWAN: Basics and Key Advances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10503">http://arxiv.org/abs/2310.10503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Maleki, Ha H. Nguyen, Ebrahim Bedeer, Robert Barton</li>
<li>for: 本研究旨在提供一个全面的CSS模ulation在LoRaWAN应用中的教程，包括信号生成、检测、错误性表现和频率特性等方面的分析。</li>
<li>methods: 本研究使用了LoRa特有的CSS模ulation，并对其在IoT网络中的应用进行了深入的检查和分析。</li>
<li>results: 研究发现CSS模ulation在LoRaWAN应用中具有优秀的错误性和spectral caracteristics，并提出了一些适用于IoT网络的CSS模ulation应用的新技术和算法。<details>
<summary>Abstract</summary>
Chirps spread spectrum (CSS) modulation is the heart of long-range (LoRa) modulation used in the context of long-range wide area network (LoRaWAN) in internet of things (IoT) scenarios. Despite being a proprietary technology owned by Semtech Corp., LoRa modulation has drawn much attention from the research and industry communities in recent years. However, to the best of our knowledge, a comprehensive tutorial, investigating the CSS modulation in the LoRaWAN application, is missing in the literature. Therefore, in the first part of this paper, we provide a thorough analysis and tutorial of CSS modulation modified by LoRa specifications, discussing various aspects such as signal generation, detection, error performance, and spectral characteristics. Moreover, a summary of key recent advances in the context of CSS modulation applications in IoT networks is presented in the second part of this paper under four main categories of transceiver configuration and design, data rate improvement, interference modeling, and synchronization algorithms.
</details>
<details>
<summary>摘要</summary>
射频扩散模ulation (CSS) 是LoRa射频模ulation的核心，用于长距离宽频网络（LoRaWAN）应用场景中的物联网（IoT）。尽管LoRa模ulation是Semtech Corp.拥有的专有技术，但在过去几年中，研究和业界社区对其吸引了很多关注。然而，根据我们所知，Literature中没有一篇全面的教程，探讨CSS模ulation在LoRaWAN应用中的各个方面，包括信号生成、检测、错误性能和频谱特性。因此，在本文的第一部分中，我们提供了CSS模ulation在LoRaWAN应用中的全面分析和教程，讨论了各种方面。此外，在文章的第二部分中，我们还提供了针对CSS模ulation在IoT网络应用中的四个主要类别的扩散配置和设计、数据速率改进、干扰模型和同步算法的最新进展。
</details></li>
</ul>
<hr>
<h2 id="Performance-Analysis-of-a-Low-Complexity-OTFS-Integrated-Sensing-and-Communication-System"><a href="#Performance-Analysis-of-a-Low-Complexity-OTFS-Integrated-Sensing-and-Communication-System" class="headerlink" title="Performance Analysis of a Low-Complexity OTFS Integrated Sensing and Communication System"></a>Performance Analysis of a Low-Complexity OTFS Integrated Sensing and Communication System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10476">http://arxiv.org/abs/2310.10476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Bacchielli, Lorenzo Pucci, Enrico Paolini, Andrea Giorgetti</li>
<li>for: 该论文提出了一种低复杂度估计方法，用于 ortfs 基于集成感知通信（isac）系统。</li>
<li>methods: 我们首先定义了四个低维度矩阵，用于计算通道矩阵通过简单的代数手动操作。然后，我们建立了一个独立系统参数的分析标准，用于 Identify the most informative elements within these derived matrices，利用 Dirichlet kernel 的性质。这使得我们可以简化这些矩阵，保留只有关键的元素，从而实现高效、低复杂度的感知接收器。</li>
<li>results: 数字结果表明，提出的近似技术可以高效地保持感知性能， measured in terms of root mean square error (RMSE) of the range and velocity estimation， 同时减少计算努力 enormously。<details>
<summary>Abstract</summary>
This work proposes a low-complexity estimation approach for an orthogonal time frequency space (OTFS)-based integrated sensing and communication (ISAC) system. In particular, we first define four low-dimensional matrices used to compute the channel matrix through simple algebraic manipulations. Secondly, we establish an analytical criterion, independent of system parameters, to identify the most informative elements within these derived matrices, leveraging the properties of the Dirichlet kernel. This allows the distilling of such matrices, keeping only those entries that are essential for detection, resulting in an efficient, low-complexity implementation of the sensing receiver. Numerical results, which refer to a vehicular scenario, demonstrate that the proposed approximation technique effectively preserves the sensing performance, evaluated in terms of root mean square error (RMSE) of the range and velocity estimation, while concurrently reducing the computational effort enormously.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The traditional Chinese writing system is also widely used, especially in Taiwan and Hong Kong. If you prefer the traditional Chinese writing system, please let me know and I can provide the translation accordingly.
</details></li>
</ul>
<hr>
<h2 id="Flag-Sequence-Set-Design-for-Low-Complexity-Delay-Doppler-Estimation"><a href="#Flag-Sequence-Set-Design-for-Low-Complexity-Delay-Doppler-Estimation" class="headerlink" title="Flag Sequence Set Design for Low-Complexity Delay-Doppler Estimation"></a>Flag Sequence Set Design for Low-Complexity Delay-Doppler Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10457">http://arxiv.org/abs/2310.10457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingsheng Meng, Yong Liang Guan, Yao Ge, Zilong Liu</li>
<li>for: 该 paper 探讨了用 Flag 序列实现低复杂度延迟-多普勒估计，通过利用 Flag 序列的特殊峰柜ambiguity函数（AF）。不同于现有的 Flag 序列设计，我们的设计不受 prime 长度和 periodic auto-AF 的限制，而是设计了 Flag 序列集合的任意长度和低（非极） periodic&#x2F;aperiodic auto-和cross-AF。</li>
<li>methods: 我们首先investigated Zone-based Curtain sequence sets of arbitrary lengths的代数设计。我们的提议的设计导致了新的 Curtain sequence sets，其具有理想的毯幕自动ambiguity函数（AF）和低&#x2F;zero cross-AF在延迟-多普勒频率范围内。使用这些 Curtain sequence sets，我们提出了两个优化问题，以最小化 Flag sequence set的总成本weighted integrated sidelobe level（SCWISL）。我们还提出了一种加速Parallel Partially Majorization-Minimization Algorithm，用于同时优化发射 Flag sequence和与其匹配&#x2F;不匹配的参照序列。</li>
<li>results: 我们的实验结果表明，我们的提议 Flag sequences 比现有 Flag sequences 具有更好的 SCWISL 和自定义峰-侧噪比。此外，我们的 Flag sequences under Flag method 的 Mean Squared Errors 逐渐接近 Cramer-Rao Lower Bound 和 Sampling Bound，当信号噪声比高 enough 时。<details>
<summary>Abstract</summary>
This paper studies Flag sequences for lowcomplexity delay-Doppler estimation by exploiting their distinctive peak-curtain ambiguity functions (AFs). Unlike the existing Flag sequence designs that are limited to prime lengths and periodic auto-AFs, we aim to design Flag sequence sets of arbitrary lengths and with low (nontrivial) periodic/aperiodic auto- and cross-AFs. Since every Flag sequence consists of a Curtain sequence and a Peak sequence, we first investigate the algebraic design of zone-based Curtain sequence sets of arbitrary lengths. Our proposed design gives rise to novel Curtain sequence sets with ideal curtain auto-AFs and low/zero cross-AFs within the delay-Doppler zone of interest. Leveraging these Curtain sequence sets, two optimization problems are formulated to minimize the summed customized weighted integrated sidelobe level (SCWISL) of the Flag sequence set. Accelerated Parallel Partially Majorization-Minimization Algorithms are proposed to jointly optimize the transmit Flag sequences and matched/mismatched reference sequences stored in the receiver. Simulations demonstrate that our proposed Flag sequences lead to improved SCWISL and customized peak-to-max-sidelobe ratio compared with the existing Flag sequences. Additionally, our Flag sequences under Flag method exhibit Mean Squared Errors that approach the Cramer-Rao Lower Bound and the Sampling Bound at high signal-to-noise power ratios.
</details>
<details>
<summary>摘要</summary>
Since every Flag sequence consists of a Curtain sequence and a Peak sequence, we first investigate the algebraic design of zone-based Curtain sequence sets of arbitrary lengths. Our proposed design yields novel Curtain sequence sets with ideal curtain auto-AFs and low/zero cross-AFs within the delay-Doppler zone of interest.Leveraging these Curtain sequence sets, two optimization problems are formulated to minimize the summed customized weighted integrated sidelobe level (SCWISL) of the Flag sequence set. Accelerated Parallel Partially Majorization-Minimization Algorithms are proposed to jointly optimize the transmit Flag sequences and matched/mismatched reference sequences stored in the receiver.Simulations show that our proposed Flag sequences lead to improved SCWISL and customized peak-to-max-sidelobe ratio compared with existing Flag sequences. Additionally, our Flag sequences under the Flag method exhibit Mean Squared Errors that approach the Cramer-Rao Lower Bound and the Sampling Bound at high signal-to-noise power ratios.
</details></li>
</ul>
<hr>
<h2 id="Soft-Demodulator-for-Symbol-Level-Precoding-in-Coded-Multiuser-MISO-Systems"><a href="#Soft-Demodulator-for-Symbol-Level-Precoding-in-Coded-Multiuser-MISO-Systems" class="headerlink" title="Soft Demodulator for Symbol-Level Precoding in Coded Multiuser MISO Systems"></a>Soft Demodulator for Symbol-Level Precoding in Coded Multiuser MISO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10296">http://arxiv.org/abs/2310.10296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Hongwei Hou, Wenjin Wang, Xinping Yi, Shi Jin</li>
<li>for: 本文研究了Symbol-level precoding (SLP)在channel-coded多用户多输入单输出（MISO）系统中的应用。</li>
<li>methods: 本文提出了一种新的软解调器设计方法，用于处理不符合 Gaussian 分布的 SLP 信号。</li>
<li>results: 实验结果表明，提议的软解调器可以减少现有 SLP 系统的通信 overhead 和计算复杂度，同时提高了传输率。<details>
<summary>Abstract</summary>
In this paper, we consider symbol-level precoding (SLP) in channel-coded multiuser multi-input single-output (MISO) systems. It is observed that the received SLP signals do not always follow Gaussian distribution, rendering the conventional soft demodulation with the Gaussian assumption unsuitable for the coded SLP systems. It, therefore, calls for novel soft demodulator designs for non-Gaussian distributed SLP signals with accurate log-likelihood ratio (LLR) calculation. To this end, we first investigate the non-Gaussian characteristics of both phase-shift keying (PSK) and quadrature amplitude modulation (QAM) received signals with existing SLP schemes and categorize the signals into two distinct types. The first type exhibits an approximate-Gaussian distribution with the outliers extending along the constructive interference region (CIR). In contrast, the second type follows some distribution that significantly deviates from the Gaussian distribution. To obtain accurate LLR, we propose the modified Gaussian soft demodulator and Gaussian mixture model (GMM) soft demodulators to deal with two types of signals respectively. Subsequently, to further reduce the computational complexity and pilot overhead, we put forward a novel neural soft demodulator, named pilot feature extraction network (PFEN), leveraging the transformer mechanism in deep learning. Simulation results show that the proposed soft demodulators dramatically improve the throughput of existing SLPs for both PSK and QAM transmission in coded systems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了符号级precoding（SLP）在多用户多输入单出力（MISO）系统中。我们发现接收的SLP信号不总是follow Gaussian分布，这使得传统的软模解器不适用于编码SLP系统。因此，我们需要设计新的软模解器，以便在非Gaussian分布下计算准确的log-likelihood ratio（LLR）。为此，我们首先研究了现有SLP方案中PSK和QAM接收信号的非Gaussian特征，并将信号分类为两种类型。第一种类型表现出近似Gaussian分布，其异常点分布在构建性干扰区（CIR）上。然而，第二种类型的信号具有显著不同于Gaussian分布的特征。为了获得准确的LLR，我们提议使用修改后Gaussian软模解器和Gaussian混合模型（GMM）软模解器来处理这两种信号。然后，为了进一步减少计算复杂性和导航点负担，我们提出了一种新的神经软模解器，即预测特征提取网络（PFEN），利用深度学习中的转换机制。实验结果表明，我们提出的软模解器可以很大程度提高现有SLP的传输能力。
</details></li>
</ul>
<hr>
<h2 id="A-Low-Complexity-Block-oriented-Functional-Link-Adaptive-Filtering-Algorithm"><a href="#A-Low-Complexity-Block-oriented-Functional-Link-Adaptive-Filtering-Algorithm" class="headerlink" title="A Low Complexity Block-oriented Functional Link Adaptive Filtering Algorithm"></a>A Low Complexity Block-oriented Functional Link Adaptive Filtering Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10276">http://arxiv.org/abs/2310.10276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavankumar Ganjimala, Subrahmanyam Mula</li>
<li>for: 模型无线非线性系统</li>
<li>methods: 使用块归一化功能链适应 Filter (BO-FLAF) 和 Hammersen BO trigonometric FLAF (HBO-TFLAF)</li>
<li>results: 对比原始 TFLAF，HBO-TFLAF 具有47%  fewer multiplications，并且 exhibits 更快的 convergence rate 和 3-5 dB 更好的稳态平均方差 (MSE) 表现。<details>
<summary>Abstract</summary>
The high computation complexity of nonlinear adaptive filtering algorithms poses significant challenges at the hardware implementation level. In order to tackle the computational complexity problem, this paper proposes a novel block-oriented functional link adaptive filter (BO-FLAF) to model memoryless nonlinear systems. Through theoretical complexity analysis, we show that the proposed Hammerstein BO trigonometric FLAF (HBO-TFLAF) has 47% lesser multiplications than the original TFLAF for a filter order of 1024. Moreover, the HBO-TFLAF exhibits a faster convergence rate and achieved 3-5 dB lesser steady-state mean square error (MSE) compared to the original TFLAF for a memoryless nonlinear system identification task.
</details>
<details>
<summary>摘要</summary>
高度计算复杂性的非线性适应滤波算法在硬件实现方面带来了重要的挑战。为了解决计算复杂性问题，这篇论文提议了一种新的块 oriented 函数链适应滤波器（BO-FLAF），用于模型无记忆非线性系统。通过理论复杂性分析，我们表明了提案的汽olinski BO  trigonometric FLAF（HBO-TFLAF）在缓存大小为 1024 的情况下，相比原始 TFLAF 减少了 47% 的 multiply 操作数。此外，HBO-TFLAF 还表现出更快的收敛速率和对非线性系统识别任务中的稳态平均幂二分量（MSE）减少了 3-5 dB。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-MTC-User-Activity-Detection-and-Channel-Estimation-with-Unknown-Spatial-Covariance"><a href="#Hierarchical-MTC-User-Activity-Detection-and-Channel-Estimation-with-Unknown-Spatial-Covariance" class="headerlink" title="Hierarchical MTC User Activity Detection and Channel Estimation with Unknown Spatial Covariance"></a>Hierarchical MTC User Activity Detection and Channel Estimation with Unknown Spatial Covariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10204">http://arxiv.org/abs/2310.10204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Djelouat, Mikko J. Sillanpää, Markus Leinonen, Markku Juntti</li>
<li>for: 这篇论文解决了机器型通信中的共同用户标识和通道估计（JUICE）问题，采用实际的空间相关通道模型和未知 covariance 矩阵。</li>
<li>methods: 作者首先利用了强级先验的概念，并提出了层次稀突减模矩阵来模型结构化稀突活动模式。然后，他们 derivated了一种 bayesian 推理方案，将 expectation propagation（EP）算法和 expectation maximization（EM）框架结合起来。</li>
<li>results: 作者通过对 JUICE 问题进行最大 posteriori（MAP）估计，并提出了一种基于 alternating direction method of multipliers（ADMM）的计算效率高的解决方案。数据结果表明，提出的算法具有显著性能提升和具有不同用户稀突活动行为假设的Robustness。<details>
<summary>Abstract</summary>
This paper addresses the joint user identification and channel estimation (JUICE) problem in machine-type communications under the practical spatially correlated channels model with unknown covariance matrices. Furthermore, we consider an MTC network with hierarchical user activity patterns following an event-triggered traffic mode. Therein the users are distributed over clusters with a structured sporadic activity behaviour that exhibits both cluster-level and intra-cluster sparsity patterns. To solve the JUICE problem, we first leverage the concept of strong priors and propose a hierarchical-sparsity-inducing spike-and-slab prior to model the structured sparse activity pattern. Subsequently, we derive a Bayesian inference scheme by coupling the expectation propagation (EP) algorithm with the expectation maximization (EM) framework. Second, we reformulate the JUICE as a maximum a posteriori (MAP) estimation problem and propose a computationally-efficient solution based on the alternating direction method of multipliers (ADMM). More precisely, we relax the strong spike-and-slab prior with a cluster-sparsity-promoting prior based on the long-sum penalty. We then derive an ADMM algorithm that solves the MAP problem through a sequence of closed-form updates. Numerical results highlight the significant performance significant gains obtained by the proposed algorithms, as well as their robustness against various assumptions on the users sparse activity behaviour.
</details>
<details>
<summary>摘要</summary>
Second, we reformulate the JUICE as a maximum a posteriori (MAP) estimation problem and propose a computationally-efficient solution based on the alternating direction method of multipliers (ADMM). More precisely, we relax the strong spike-and-slab prior with a cluster-sparsity-promoting prior based on the long-sum penalty. We then derive an ADMM algorithm that solves the MAP problem through a sequence of closed-form updates. Numerical results highlight the significant performance gains obtained by the proposed algorithms, as well as their robustness against various assumptions on the users' sparse activity behavior.Translated into Simplified Chinese:这篇论文研究了机器类通信中的用户标识和通道估计（JUICE）问题，在实际的空间相关的通道模型下，并且假设用户活动模式遵循事件触发的交通模式。用户被分布在归一化的集群中，并且表现出了结构化零散的活动模式，这种模式包括集群水平和内部零散的特征。为解决JUICE问题，我们首先利用强级先验的概念，并提出了一种层次含拥权的钉板准则，以模型结构化零散的活动模式。然后，我们 deriv了一种 Bayesian 推理方案，通过将期望传播算法和期望最大化算法结合在一起。其次，我们将JUICE问题转换为最大 posteriori（MAP）估计问题，并提出了一种计算效率高的解决方案基于 alternating direction method of multipliers（ADMM）。更具体地，我们将强级钉板准则松弛为一种层次含拥权的长SUM penalty，然后 deriv 了一种 ADMM 算法来解决 MAP 问题。数据结果表明，我们提出的算法具有显著的性能提升和各种假设用户的稀疏活动行为下的稳定性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/eess.SP_2023_10_16/" data-id="clpxp6cdp01gbee884axd6ynu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/cs.CV_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T13:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/15/cs.CV_2023_10_15/">cs.CV - 2023-10-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AP-n-P-A-Less-constrained-P-n-P-Solver-for-Pose-Estimation-with-Unknown-Anisotropic-Scaling-or-Focal-Lengths"><a href="#AP-n-P-A-Less-constrained-P-n-P-Solver-for-Pose-Estimation-with-Unknown-Anisotropic-Scaling-or-Focal-Lengths" class="headerlink" title="AP$n$P: A Less-constrained P$n$P Solver for Pose Estimation with Unknown Anisotropic Scaling or Focal Lengths"></a>AP$n$P: A Less-constrained P$n$P Solver for Pose Estimation with Unknown Anisotropic Scaling or Focal Lengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09982">http://arxiv.org/abs/2310.09982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/goldoak/APnP">https://github.com/goldoak/APnP</a></li>
<li>paper_authors: Jiaxin Wei, Stefan Leutenegger, Laurent Kneip</li>
<li>for: 提出了一种新的 pose estimation 算法，可以处理不准确的 3D 坐标和完全加工数据。</li>
<li>methods: 使用了代数处理和新的 Parametrization，将两种情况都转化为同样的多项式问题，并使用 Gr&quot;obner basis 方法解决。</li>
<li>results: 实验结果表明，AP$n$P 算法可以提供更 flexible 和实用的 pose estimation 解决方案，并且在 simulate 和实际数据上达到了良好的效果。<details>
<summary>Abstract</summary>
Perspective-$n$-Point (P$n$P) stands as a fundamental algorithm for pose estimation in various applications. In this paper, we present a new approach to the P$n$P problem with relaxed constraints, eliminating the need for precise 3D coordinates or complete calibration data. We refer to it as AP$n$P due to its ability to handle unknown anisotropic scaling factors of 3D coordinates or alternatively two distinct focal lengths in addition to the conventional rigid pose. Through algebraic manipulations and a novel parametrization, both cases are brought into similar forms that distinguish themselves primarily by the order of a rotation and an anisotropic scaling operation. AP$n$P furthermore brings down both cases to an identical polynomial problem, which is solved using the Gr\"obner basis approach. Experimental results on both simulated and real datasets demonstrate the effectiveness of AP$n$P, providing a more flexible and practical solution to several pose estimation tasks. Code: https://github.com/goldoak/APnP.
</details>
<details>
<summary>摘要</summary>
投影-$n$-点（P$n$P）算法是各种应用中pose estimation的基本算法之一。在这篇论文中，我们提出了一种新的P$n$P问题的解决方案， eliminates the need for precise 3D坐标或完整的准备数据。我们称之为AP$n$P，因为它可以处理未知的三维坐标的 aniotropic scaling factor或两个不同的焦点距离。通过代数操作和一种新的参数化，两种情况都被带入了相似的形式，主要在某种顺序下进行旋转和 aniotropic scaling 操作。此外，AP$n$P还将两种情况下降到了同样的多项式问题，使用Groebner基式方法解决。实验结果表明，AP$n$P是一种更 flexible和实用的pose estimation方法，在 simulate 和实际数据上均有效。代码：https://github.com/goldoak/APnP。
</details></li>
</ul>
<hr>
<h2 id="Class-Specific-Data-Augmentation-Bridging-the-Imbalance-in-Multiclass-Breast-Cancer-Classification"><a href="#Class-Specific-Data-Augmentation-Bridging-the-Imbalance-in-Multiclass-Breast-Cancer-Classification" class="headerlink" title="Class-Specific Data Augmentation: Bridging the Imbalance in Multiclass Breast Cancer Classification"></a>Class-Specific Data Augmentation: Bridging the Imbalance in Multiclass Breast Cancer Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09981">http://arxiv.org/abs/2310.09981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanan Mahammadli, Abdullah Burkan Bereketoglu, Ayse Gul Kabakci</li>
<li>For: This paper aims to improve the accuracy of breast cancer image classification, specifically for the undersampled classes, by employing class-level data augmentation and a transformer-based ViTNet architecture.* Methods: The paper uses class-level data augmentation on structure-preserving stain normalization techniques to hematoxylin and eosin-stained images, as well as a transformer-based ViTNet architecture via transfer learning for multiclass classification of breast cancer images.* Results: The approach proposed in the paper leads to lower mortality rates associated with breast cancer by increasing the precision of classification on undersampled classes. The paper is able to categorize breast cancer images with advanced image processing and deep learning into either benign or one of four distinct malignant subtypes with high accuracy.<details>
<summary>Abstract</summary>
Breast Cancer is the most common cancer among women, which is also visible in men, and accounts for more than 1 in 10 new cancer diagnoses each year. It is also the second most common cause of women who die from cancer. Hence, it necessitates early detection and tailored treatment. Early detection can provide appropriate and patient-based therapeutic schedules. Moreover, early detection can also provide the type of cyst. This paper employs class-level data augmentation, addressing the undersampled classes and raising their detection rate. This approach suggests two key components: class-level data augmentation on structure-preserving stain normalization techniques to hematoxylin and eosin-stained images and transformer-based ViTNet architecture via transfer learning for multiclass classification of breast cancer images. This merger enables categorizing breast cancer images with advanced image processing and deep learning as either benign or as one of four distinct malignant subtypes by focusing on class-level augmentation and catering to unique characteristics of each class with increasing precision of classification on undersampled classes, which leads to lower mortality rates associated with breast cancer. The paper aims to ease the duties of the medical specialist by operating multiclass classification and categorizing the image into benign or one of four different malignant types of breast cancers.
</details>
<details>
<summary>摘要</summary>
乳癌是女性最常见的癌症，也可以出现在男性身上，每年负担着超过1/10新诊断癌症的责任。它同时也是女性死于癌症的第二大原因。因此，早期发现和定制治疗是非常重要的。早期发现可以提供适当的治疗时间表，同时也可以确定肿瘤的类型。本文提出了一种方法，通过结合分类数据增强和结构保持的染色Normalization技术，以及基于Transformer的ViTNet架构，进行多类分类分析乳癌图像。这种方法可以将乳癌图像分为benign或四种不同的恶性Subtype中的一种，并且可以根据不同的分类类别，提高对受抽样分布的类别的准确性。这种方法可以减轻医生的工作负担，并且可以帮助鉴定乳癌图像的分类结果。
</details></li>
</ul>
<hr>
<h2 id="ProteusNeRF-Fast-Lightweight-NeRF-Editing-using-3D-Aware-Image-Context"><a href="#ProteusNeRF-Fast-Lightweight-NeRF-Editing-using-3D-Aware-Image-Context" class="headerlink" title="ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context"></a>ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09965">http://arxiv.org/abs/2310.09965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binglun Wang, Niladri Shekhar Dutt, Niloy J. Mitra</li>
<li>for: 这个论文旨在提出一种用于互动编辑NeRFs的简单 yet effective的神经网络架构，以实现高效、低占用内存的图像改编。</li>
<li>methods: 该架构通过图像特征缩掌和视觉上下文来实现视觉一致的图像编辑，并可以通过对神经网络进行增量导引来实现图像改编。</li>
<li>results: 作者在多个示例中证明了该方法可以带来 appearances 和 geometric 的编辑，并与同期工作相比，提供了10-30倍的速度提升。视频结果可以在<a target="_blank" rel="noopener" href="https://proteusnerf.github.io上查看./">https://proteusnerf.github.io上查看。</a><details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRFs) have recently emerged as a popular option for photo-realistic object capture due to their ability to faithfully capture high-fidelity volumetric content even from handheld video input. Although much research has been devoted to efficient optimization leading to real-time training and rendering, options for interactive editing NeRFs remain limited. We present a very simple but effective neural network architecture that is fast and efficient while maintaining a low memory footprint. This architecture can be incrementally guided through user-friendly image-based edits. Our representation allows straightforward object selection via semantic feature distillation at the training stage. More importantly, we propose a local 3D-aware image context to facilitate view-consistent image editing that can then be distilled into fine-tuned NeRFs, via geometric and appearance adjustments. We evaluate our setup on a variety of examples to demonstrate appearance and geometric edits and report 10-30x speedup over concurrent work focusing on text-guided NeRF editing. Video results can be seen on our project webpage at https://proteusnerf.github.io.
</details>
<details>
<summary>摘要</summary>
neural Radiance Fields (NeRFs) 最近受到了大量研究，因为它们可以准确地捕捉高精度三维内容，即使从手持式视频输入中。虽然许多研究投入到了高效优化，以达到实时训练和渲染，但对Interactive编辑NeRFs的选择仍然有限。我们提出了一种简单 yet effective的神经网络架构，它具有快速和高效的特点，同时具有较低的内存占用率。这种架构可以通过用户友好的图像基于的编辑来进行慢慢导航。我们的表示方式允许直接通过 semantic feature distillation 在训练阶段进行对象选择。更重要的是，我们提议一种基于图像上的 мест化三维意识的图像上下文，以便实现视角一致的图像编辑，然后通过 geometric 和 appearance 调整来蒸馏 fine-tuned NeRFs。我们在多个例子中评估了我们的设置，并发现了10-30倍的速度提升， compared to 同时期关注 text-guided NeRF 编辑的工作。视频结果可以在我们项目网站（https://proteusnerf.github.io）上查看。
</details></li>
</ul>
<hr>
<h2 id="Tabletop-Transparent-Scene-Reconstruction-via-Epipolar-Guided-Optical-Flow-with-Monocular-Depth-Completion-Prior"><a href="#Tabletop-Transparent-Scene-Reconstruction-via-Epipolar-Guided-Optical-Flow-with-Monocular-Depth-Completion-Prior" class="headerlink" title="Tabletop Transparent Scene Reconstruction via Epipolar-Guided Optical Flow with Monocular Depth Completion Prior"></a>Tabletop Transparent Scene Reconstruction via Epipolar-Guided Optical Flow with Monocular Depth Completion Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09956">http://arxiv.org/abs/2310.09956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaotong Chen, Zheming Zhou, Zhuo Deng, Omid Ghasemalizadeh, Min Sun, Cheng-Hao Kuo, Arnie Sen</li>
<li>for:  reconstruction of transparent objects using affordable RGB-D cameras</li>
<li>methods:  leveraging monocular object segmentation and depth completion networks, Epipolar-guided Optical Flow (EOF)</li>
<li>results:  significantly improved 3D reconstruction quality compared to baseline methods, paving the way for more adept robotic perception and interaction with transparent objects.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在使用便宜的RGB-D摄像机重建透明物体，解决了RGB频谱中的不一致和单视深度测量不准确问题。</li>
<li>methods: 我们提出了一个两个阶段的架构，首先使用商业可用的单目object segmentation和深度完成网络预测透明物体的深度，提供单视形状优先。然后，我们提出了Epipolar-guided Optical Flow（EOF），将多个单视形状优先融合成cross-view一致的3D重建，基于摄像机pose估计。EOF使用边界敏感抽样和epipolar-line约束加入光流计算，准确建立透明物体的2D匹配。</li>
<li>results: 我们的架构与基eline方法进行比较，显示我们的3D重建质量得到了显著改善，为Robotic perception和透明物体交互带来了新的可能性。<details>
<summary>Abstract</summary>
Reconstructing transparent objects using affordable RGB-D cameras is a persistent challenge in robotic perception due to inconsistent appearances across views in the RGB domain and inaccurate depth readings in each single-view. We introduce a two-stage pipeline for reconstructing transparent objects tailored for mobile platforms. In the first stage, off-the-shelf monocular object segmentation and depth completion networks are leveraged to predict the depth of transparent objects, furnishing single-view shape prior. Subsequently, we propose Epipolar-guided Optical Flow (EOF) to fuse several single-view shape priors from the first stage to a cross-view consistent 3D reconstruction given camera poses estimated from opaque part of the scene. Our key innovation lies in EOF which employs boundary-sensitive sampling and epipolar-line constraints into optical flow to accurately establish 2D correspondences across multiple views on transparent objects. Quantitative evaluations demonstrate that our pipeline significantly outperforms baseline methods in 3D reconstruction quality, paving the way for more adept robotic perception and interaction with transparent objects.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese文本：重建透明 объек所用的便宜RGB-D相机是机器人感知中的一个持续挑战，因为颜色域中的 appearances 是不一致的，以及每个单视图中的深度测量不准确。我们提出了一个两stage管道，用于重建透明 объек。在第一个阶段，我们使用 commercially 可用的单目物体分割和深度完成网络来预测透明 объек的深度，从而提供单视图形状优先。然后，我们提出了基于epipolar线的Optical Flow（EOF）来融合多个单视图形状优先，以实现相机pose estimate的cross-view一致性。我们的关键创新在于EOF，它使用边界敏感的采样和epipolar线约束来准确地在多个视图中建立透明对象的2D匹配。量化评估表明，我们的管道在3D重建质量方面与基eline方法进行了显著比较，为更加善的机器人感知和透明对象交互开创了道路。Translation:重建透明对象使用便宜RGB-D相机是机器人感知中的一个持续挑战，因为颜色域中的 appearances 是不一致的，以及每个单视图中的深度测量不准确。我们提出了一个两stage管道，用于重建透明对象。在第一个阶段，我们使用 commercially 可用的单目物体分割和深度完成网络来预测透明对象的深度，从而提供单视图形状优先。然后，我们提出了基于epipolar线的Optical Flow（EOF）来融合多个单视图形状优先，以实现相机pose estimate的cross-view一致性。我们的关键创新在于EOF，它使用边界敏感的采样和epipolar线约束来准确地在多个视图中建立透明对象的2D匹配。量化评估表明，我们的管道在3D重建质量方面与基eline方法进行了显著比较，为更加善的机器人感知和透明对象交互开创了道路。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Robustness-of-Visual-Representations-for-Object-Assembly-Task-Requiring-Spatio-Geometrical-Reasoning"><a href="#Evaluating-Robustness-of-Visual-Representations-for-Object-Assembly-Task-Requiring-Spatio-Geometrical-Reasoning" class="headerlink" title="Evaluating Robustness of Visual Representations for Object Assembly Task Requiring Spatio-Geometrical Reasoning"></a>Evaluating Robustness of Visual Representations for Object Assembly Task Requiring Spatio-Geometrical Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09943">http://arxiv.org/abs/2310.09943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chahyon Ku, Carl Winge, Ryan Diaz, Wentao Yuan, Karthik Desingh</li>
<li>for: 这个论文主要关注评估和比较视觉表示的Robustness在物体组装任务中。</li>
<li>methods: 我们采用一种通用框架，利用视觉预训模型作为视觉编码器来进行视 Motor 政策学习。</li>
<li>results: 我们的量化分析表明现有预训模型无法捕捉这个任务所需的关键视觉特征，但一个从scratch预训的视觉编码器一直表现出色，并且我们提出了旋转表示和相关的损失函数，可以显著提高政策学习。<details>
<summary>Abstract</summary>
This paper primarily focuses on evaluating and benchmarking the robustness of visual representations in the context of object assembly tasks. Specifically, it investigates the alignment and insertion of objects with geometrical extrusions and intrusions, commonly referred to as a peg-in-hole task. The accuracy required to detect and orient the peg and the hole geometry in SE(3) space for successful assembly poses significant challenges. Addressing this, we employ a general framework in visuomotor policy learning that utilizes visual pretraining models as vision encoders. Our study investigates the robustness of this framework when applied to a dual-arm manipulation setup, specifically to the grasp variations. Our quantitative analysis shows that existing pretrained models fail to capture the essential visual features necessary for this task. However, a visual encoder trained from scratch consistently outperforms the frozen pretrained models. Moreover, we discuss rotation representations and associated loss functions that substantially improve policy learning. We present a novel task scenario designed to evaluate the progress in visuomotor policy learning, with a specific focus on improving the robustness of intricate assembly tasks that require both geometrical and spatial reasoning. Videos, additional experiments, dataset, and code are available at https://bit.ly/geometric-peg-in-hole .
</details>
<details>
<summary>摘要</summary>
（本文主要关注评估和比较视觉表示的稳定性在物体组装任务中。具体来说，它研究了具有几何嵌入和嵌入的物体的对齐和插入，通常被称为圆锥盘嵌入任务。成功的组装需要在 SE(3) 空间检测和 orient 圆锥和孔径的精度 pose significant challenges。为此，我们采用一种通用的 Framework in visuomotor policy learning，使用视觉预训模型作为视觉编码器。我们的研究检验了这种 Framework 在双臂 manipulate 设置中的稳定性，特别是在抓取变化中。我们的量化分析表明，现有的预训模型无法捕捉到这种任务所需的关键视觉特征。然而，一个从scratch 训练的视觉编码器在常规模型中具有显著的优势。此外，我们讨论了旋转表示和相关的损失函数，可以在策略学习中提高稳定性。我们还介绍了一种新的任务场景，用于评估visuomotor策略学习的进步，特别是在改进精度的几何和空间逻辑任务中。视频、附加实验、数据集和代码可以在 https://bit.ly/geometric-peg-in-hole 上获得。）
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Discovery-of-Interpretable-Directions-in-h-space-of-Pre-trained-Diffusion-Models"><a href="#Unsupervised-Discovery-of-Interpretable-Directions-in-h-space-of-Pre-trained-Diffusion-Models" class="headerlink" title="Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models"></a>Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09912">http://arxiv.org/abs/2310.09912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Zhang, Luping Liu. Zhijie Lin, Yichen Zhu, Zhou Zhao</li>
<li>for: 这个研究旨在开发一个无监督性的学习基础的方法，用于找到对称易于理解的方向在预训 diffusion 模型的 h-space 中。</li>
<li>methods: 我们的方法基于现有的 GAN 内部空间技术，包括一个 shift control 模组和一个重建器。这两个模组共同实现对于预训 diffusion 模型的易于理解方向的发现。为了避免发现无意义和破坏性的方向，我们还使用了一个检测器来维持Shifted sample的实际性。</li>
<li>results: 我们的方法可以实现透过 iterative 生成过程来快速发现对称易于理解的方向，并且比较不需要其他复杂的程序。实验结果显示了我们的方法的有效性。<details>
<summary>Abstract</summary>
We propose the first unsupervised and learning-based method to identify interpretable directions in the h-space of pre-trained diffusion models. Our method is derived from an existing technique that operates on the GAN latent space. In a nutshell, we employ a shift control module for pre-trained diffusion models to manipulate a sample into a shifted version of itself, followed by a reconstructor to reproduce both the type and the strength of the manipulation. By jointly optimizing them, the model will spontaneously discover disentangled and interpretable directions. To prevent the discovery of meaningless and destructive directions, we employ a discriminator to maintain the fidelity of shifted sample. Due to the iterative generative process of diffusion models, our training requires a substantial amount of GPU VRAM to store numerous intermediate tensors for back-propagating gradient. To address this issue, we first propose a general VRAM-efficient training algorithm based on gradient checkpointing technique to back-propagate any gradient through the whole generative process, with acceptable occupancy of VRAM and sacrifice of training efficiency. Compared with existing related works on diffusion models, our method inherently identifies global and scalable directions, without necessitating any other complicated procedures. Extensive experiments on various datasets demonstrate the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
我们提出了首个无监督、学习基于的方法，用于在预训练的扩散模型中标识可解释的方向。我们的方法基于现有的GAN特征空间技术。总之，我们使用一个shift控制模块来控制预训练扩散模型中的一个样本，然后使用一个重构器来重建样本的类型和强度。通过同时优化它们，模型会自动发现分解和可解释的方向。为了避免发现无意义和破坏性的方向，我们使用一个探测器来保持扭曲样本的真实性。由于扩散模型的迭代生成过程，我们的训练需要大量的GPU VRAM来存储多个间接张量以供反向传播梯度。为解决这问题，我们首先提出了一种通用VRAM有效的训练算法，基于梯度检查点技术，可以在接受ABLE的VRAM占用和训练效率的情况下，反向传播任何梯度。与现有相关作品相比，我们的方法自然地标识全局和可扩展的方向，无需其他复杂的过程。广泛的实验表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Object-Goal-Visual-Navigation-With-Class-Independent-Relationship-Network"><a href="#Zero-Shot-Object-Goal-Visual-Navigation-With-Class-Independent-Relationship-Network" class="headerlink" title="Zero-Shot Object Goal Visual Navigation With Class-Independent Relationship Network"></a>Zero-Shot Object Goal Visual Navigation With Class-Independent Relationship Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09883">http://arxiv.org/abs/2310.09883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinting Li, Shizhou Zhang, Yue LU, Kerry Dan, Lingyan Ran, Peng Wang, Yanning Zhang</li>
<li>for: 这个论文研究了零shot对象目标视觉导航问题。</li>
<li>methods: 我们提出了一种名为Class-Independent Relationship Network（CIRN）的方法，它结合目标检测信息和目标和导航目标之间的相似性，构建了一个新的状态表示，不包含目标特征或环境特征，从而有效地解couple了机器人的导航能力与目标特征。</li>
<li>results: 在AI2-THOR虚拟环境中进行了广泛的实验，我们的方法在零shot导航任务中表现出了强大的泛化能力，包括不同目标和环境下的导航任务。进一步的跨目标和跨场景设置中的实验也进一步验证了我们的方法的稳定性和泛化能力。<details>
<summary>Abstract</summary>
This paper investigates the zero-shot object goal visual navigation problem. In the object goal visual navigation task, the agent needs to locate navigation targets from its egocentric visual input. "Zero-shot" means that the target the agent needs to find is not trained during the training phase. To address the issue of coupling navigation ability with target features during training, we propose the Class-Independent Relationship Network (CIRN). This method combines target detection information with the relative semantic similarity between the target and the navigation target, and constructs a brand new state representation based on similarity ranking, this state representation does not include target feature or environment feature, effectively decoupling the agent's navigation ability from target features. And a Graph Convolutional Network (GCN) is employed to learn the relationships between different objects based on their similarities. During testing, our approach demonstrates strong generalization capabilities, including zero-shot navigation tasks with different targets and environments. Through extensive experiments in the AI2-THOR virtual environment, our method outperforms the current state-of-the-art approaches in the zero-shot object goal visual navigation task. Furthermore, we conducted experiments in more challenging cross-target and cross-scene settings, which further validate the robustness and generalization ability of our method. Our code is available at: https://github.com/SmartAndCleverRobot/ICRA-CIRN.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Top-K-Pooling-with-Patch-Contrastive-Learning-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Top-K-Pooling-with-Patch-Contrastive-Learning-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Top-K Pooling with Patch Contrastive Learning for Weakly-Supervised Semantic Segmentation"></a>Top-K Pooling with Patch Contrastive Learning for Weakly-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09828">http://arxiv.org/abs/2310.09828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao</li>
<li>for: 实现cost-effectiveweakly supervised semantic segmentation (WSSS)</li>
<li>methods: 使用Vision Transformer (ViT)方法，不使用class activation map (CAM)，并 introduces top-K pooling layer和patch contrastive error (PCE)</li>
<li>results: 实验结果显示，我们的方法效果很高，在PASCAL VOC 2012 dataset上比其他state-of-the-art WSSS方法更高效。<details>
<summary>Abstract</summary>
Weakly Supervised Semantic Segmentation (WSSS) using only image-level labels has gained significant attention due to cost-effectiveness. Recently, Vision Transformer (ViT) based methods without class activation map (CAM) have shown greater capability in generating reliable pseudo labels than previous methods using CAM. However, the current ViT-based methods utilize max pooling to select the patch with the highest prediction score to map the patch-level classification to the image-level one, which may affect the quality of pseudo labels due to the inaccurate classification of the patches. In this paper, we introduce a novel ViT-based WSSS method named top-K pooling with patch contrastive learning (TKP-PCL), which employs a top-K pooling layer to alleviate the limitations of previous max pooling selection. A patch contrastive error (PCE) is also proposed to enhance the patch embeddings to further improve the final results. The experimental results show that our approach is very efficient and outperforms other state-of-the-art WSSS methods on the PASCAL VOC 2012 dataset.
</details>
<details>
<summary>摘要</summary>
强度不高的 semantic segmentation (WSSS) 使用图像级别标签已经吸引了广泛关注，主要是因为它的成本低廉。最近，基于 Vision Transformer (ViT) 的方法无需分类映射 (CAM) 已经显示出更高的可靠 Pseudo Label 生成能力。然而，现有的 ViT 基本方法使用最大池化来选择最高分预测值来映射patch级别分类到图像级别的，这可能会影响 pseudo labels 的质量由于不准确的patch分类。在本文中，我们介绍了一种新的 ViT 基本 WSSS 方法，名为 top-K pooling with patch contrastive learning (TKP-PCL)，该方法使用 top-K pooling 层来缓解上述最大池化的局限性。此外，我们还提出了一种 patch contrastive error (PCE) 来进一步改进 patch 的嵌入，从而提高最终结果。实验结果表明，我们的方法非常高效，并在 PASCAL VOC 2012 数据集上超越了其他当前最佳 WSSS 方法。
</details></li>
</ul>
<hr>
<h2 id="Turn-Passive-to-Active-A-Survey-on-Active-Intellectual-Property-Protection-of-Deep-Learning-Models"><a href="#Turn-Passive-to-Active-A-Survey-on-Active-Intellectual-Property-Protection-of-Deep-Learning-Models" class="headerlink" title="Turn Passive to Active: A Survey on Active Intellectual Property Protection of Deep Learning Models"></a>Turn Passive to Active: A Survey on Active Intellectual Property Protection of Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09822">http://arxiv.org/abs/2310.09822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingfu Xue, Leo Yu Zhang, Yushu Zhang, Weiqiang Liu</li>
<li>for: 这篇论文主要是为了介绍和探讨深度学习（DL）模型的知识产权保护方法，具体来说是关于活动权利保护（active copyright protection）技术。</li>
<li>methods: 本论文主要采用了文献综述的方法，审查了现有的知识产权保护方法，并提出了新的活动权利保护方法的需求和挑战。</li>
<li>results: 本论文通过系统地介绍了活动权利保护的概念、特点和要求，提供了评价方法和指标，审查了现有的知识产权保护方法，探讨了可能面临的攻击和未来发展的挑战。<details>
<summary>Abstract</summary>
The intellectual property protection of deep learning (DL) models has attracted increasing serious concerns. Many works on intellectual property protection for Deep Neural Networks (DNN) models have been proposed. The vast majority of existing work uses DNN watermarking to verify the ownership of the model after piracy occurs, which is referred to as passive verification. On the contrary, we focus on a new type of intellectual property protection method named active copyright protection, which refers to active authorization control and user identity management of the DNN model. As of now, there is relatively limited research in the field of active DNN copyright protection. In this review, we attempt to clearly elaborate on the connotation, attributes, and requirements of active DNN copyright protection, provide evaluation methods and metrics for active copyright protection, review and analyze existing work on active DL model intellectual property protection, discuss potential attacks that active DL model copyright protection techniques may face, and provide challenges and future directions for active DL model intellectual property protection. This review is helpful to systematically introduce the new field of active DNN copyright protection and provide reference and foundation for subsequent work.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）模型知识产权保护已经引起了越来越多的关注。许多关于深度神经网络（DNN）模型知识产权保护的工作已经被提出。大多数现有的工作使用深度神经网络水印来验证模型的所有权 после盗版，这被称为被动验证。相比之下，我们关注了一种新的知识产权保护方法，即活动版权保护，即活动授权控制和用户身份管理。到目前为止，对活动DL模型知识产权保护的研究相对较少。在这篇文章中，我们尝试了明确地解释活动DL模型知识产权保护的含义、特点和要求，提供评估方法和指标 для活动版权保护，回顾和分析现有的活动DL模型知识产权保护工作，讨论可能面临的攻击和未来方向。这篇文章有助于系统地介绍新的活动DNN模型知识产权保护领域，并提供参考和基础 для后续的工作。
</details></li>
</ul>
<hr>
<h2 id="LICO-Explainable-Models-with-Language-Image-Consistency"><a href="#LICO-Explainable-Models-with-Language-Image-Consistency" class="headerlink" title="LICO: Explainable Models with Language-Image Consistency"></a>LICO: Explainable Models with Language-Image Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09821">http://arxiv.org/abs/2310.09821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ymleifdu/lico">https://github.com/ymleifdu/lico</a></li>
<li>paper_authors: Yiming Lei, Zilong Li, Yangyang Li, Junping Zhang, Hongming Shan</li>
<li>for: 这篇论文的目的是解释深度学习模型的决策过程。</li>
<li>methods: 该论文提出了一种基于语言图像一致性的图像分类解释方法，称为LICO，它通过将学习的语言提示与对应的视觉特征进行对应关系的建立，从而生成更加解释的注意力地图。</li>
<li>results: 实验结果表明，LICO可以与现有的解释方法结合使用，并且可以提高图像分类模型的解释能力，而无需在推理过程中增加计算开销。<details>
<summary>Abstract</summary>
Interpreting the decisions of deep learning models has been actively studied since the explosion of deep neural networks. One of the most convincing interpretation approaches is salience-based visual interpretation, such as Grad-CAM, where the generation of attention maps depends merely on categorical labels. Although existing interpretation methods can provide explainable decision clues, they often yield partial correspondence between image and saliency maps due to the limited discriminative information from one-hot labels. This paper develops a Language-Image COnsistency model for explainable image classification, termed LICO, by correlating learnable linguistic prompts with corresponding visual features in a coarse-to-fine manner. Specifically, we first establish a coarse global manifold structure alignment by minimizing the distance between the distributions of image and language features. We then achieve fine-grained saliency maps by applying optimal transport (OT) theory to assign local feature maps with class-specific prompts. Extensive experimental results on eight benchmark datasets demonstrate that the proposed LICO achieves a significant improvement in generating more explainable attention maps in conjunction with existing interpretation methods such as Grad-CAM. Remarkably, LICO improves the classification performance of existing models without introducing any computational overhead during inference. Source code is made available at https://github.com/ymLeiFDU/LICO.
</details>
<details>
<summary>摘要</summary>
深度学习模型的解释方法在深度神经网络爆发后活跃研究。一种最有力的解释方法是基于分类标签的特征焦点映射，如Grad-CAM，其中生成特征焦点映射的依赖于分类标签。 although existing interpretation methods can provide explainable decision clues, they often yield partial correspondence between image and saliency maps due to the limited discriminative information from one-hot labels. This paper proposes a Language-Image COnsistency model for explainable image classification, termed LICO, by correlating learnable linguistic prompts with corresponding visual features in a coarse-to-fine manner. Specifically, we first establish a coarse global manifold structure alignment by minimizing the distance between the distributions of image and language features. We then achieve fine-grained saliency maps by applying optimal transport (OT) theory to assign local feature maps with class-specific prompts. Extensive experimental results on eight benchmark datasets demonstrate that the proposed LICO achieves a significant improvement in generating more explainable attention maps in conjunction with existing interpretation methods such as Grad-CAM. Remarkably, LICO improves the classification performance of existing models without introducing any computational overhead during inference. 源代码可以在https://github.com/ymLeiFDU/LICO 中下载。
</details></li>
</ul>
<hr>
<h2 id="OAAFormer-Robust-and-Efficient-Point-Cloud-Registration-Through-Overlapping-Aware-Attention-in-Transformer"><a href="#OAAFormer-Robust-and-Efficient-Point-Cloud-Registration-Through-Overlapping-Aware-Attention-in-Transformer" class="headerlink" title="OAAFormer: Robust and Efficient Point Cloud Registration Through Overlapping-Aware Attention in Transformer"></a>OAAFormer: Robust and Efficient Point Cloud Registration Through Overlapping-Aware Attention in Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09817">http://arxiv.org/abs/2310.09817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Gao, Qiujie Dong, Ruian Wang, Shuangmin Chen, Shiqing Xin, Changhe Tu, Wenping Wang</li>
<li>For: The paper focuses on improving the correspondence quality in point cloud registration using a coarse-to-fine feature matching paradigm.* Methods: The proposed method, called OAAFormer, introduces a soft matching mechanism, overlapping region detection module, and region-wise attention module to enhance correspondence quality.* Results: The proposed method achieves a substantial increase of about 7% in the inlier ratio and an enhancement of 2-4% in registration recall on the challenging 3DLoMatch benchmark.Here is the same information in Simplified Chinese:* For: 本文关注在点云注册中提高匹配质量，使用粗细匹配模式。* Methods: 提议的方法是OAAFormer，它引入软匹配机制、重叠区检测模块和区域精度注意力模块来提高匹配质量。* Results: 测试结果表明，提议的方法在3DLoMatch benchmark上增加了约7%的匹配率和2-4%的注册再现率。<details>
<summary>Abstract</summary>
In the domain of point cloud registration, the coarse-to-fine feature matching paradigm has received substantial attention owing to its impressive performance. This paradigm involves a two-step process: first, the extraction of multi-level features, and subsequently, the propagation of correspondences from coarse to fine levels. Nonetheless, this paradigm exhibits two notable limitations.Firstly, the utilization of the Dual Softmax operation has the potential to promote one-to-one correspondences between superpoints, inadvertently excluding valuable correspondences. This propensity arises from the fact that a source superpoint typically maintains associations with multiple target superpoints. Secondly, it is imperative to closely examine the overlapping areas between point clouds, as only correspondences within these regions decisively determine the actual transformation. Based on these considerations, we propose {\em OAAFormer} to enhance correspondence quality. On one hand, we introduce a soft matching mechanism, facilitating the propagation of potentially valuable correspondences from coarse to fine levels. Additionally, we integrate an overlapping region detection module to minimize mismatches to the greatest extent possible. Furthermore, we introduce a region-wise attention module with linear complexity during the fine-level matching phase, designed to enhance the discriminative capabilities of the extracted features. Tests on the challenging 3DLoMatch benchmark demonstrate that our approach leads to a substantial increase of about 7\% in the inlier ratio, as well as an enhancement of 2-4\% in registration recall. =
</details>
<details>
<summary>摘要</summary>
在点云注册领域，粗细到细粒度匹配方法受到了广泛关注，这种方法包括两个步骤：首先提取多级特征，然后在粗细层次上传递匹配。然而，这种方法存在两个显著的限制。首先，使用双层软MAX操作可能会促进一对一的匹配 между超点，不经意增加有价值的匹配。这种倾向来自于源超点通常与多个目标超点保持关联。其次，需要仔细检查点云之间的重叠区域，只有这些区域中的匹配才能决定实际变换。基于这些考虑，我们提出了{\em OAAFormer}来提高匹配质量。一方面，我们引入了软匹配机制，以便在粗细层次上传递有可能的有价值匹配。另一方面，我们集成了重叠区域检测模块，以最大限度避免匹配错误。此外，我们引入了区域wise注意力模块，用于在细粒度匹配阶段提高提取特征的推理能力。3DLoMatch benchmark上的测试表明，我们的方法可以提高约7%的匹配率，同时提高注册回归率约2-4%。
</details></li>
</ul>
<hr>
<h2 id="Can-LSH-Locality-Sensitive-Hashing-Be-Replaced-by-Neural-Network"><a href="#Can-LSH-Locality-Sensitive-Hashing-Be-Replaced-by-Neural-Network" class="headerlink" title="Can LSH (Locality-Sensitive Hashing) Be Replaced by Neural Network?"></a>Can LSH (Locality-Sensitive Hashing) Be Replaced by Neural Network?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09806">http://arxiv.org/abs/2310.09806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renyang Liu, Jun Zhao, Xing Chu, Yu Liang, Wei Zhou, Jing He</li>
<li>for: 提高信息搜索性能</li>
<li>methods: 使用深度神经网络学习locality-sensitive hashing</li>
<li>results: 提高查询精度、减少时间和内存消耗<details>
<summary>Abstract</summary>
With the rapid development of GPU (Graphics Processing Unit) technologies and neural networks, we can explore more appropriate data structures and algorithms. Recent progress shows that neural networks can partly replace traditional data structures. In this paper, we proposed a novel DNN (Deep Neural Network)-based learned locality-sensitive hashing, called LLSH, to efficiently and flexibly map high-dimensional data to low-dimensional space. LLSH replaces the traditional LSH (Locality-sensitive Hashing) function families with parallel multi-layer neural networks, which reduces the time and memory consumption and guarantees query accuracy simultaneously. The proposed LLSH demonstrate the feasibility of replacing the hash index with learning-based neural networks and open a new door for developers to design and configure data organization more accurately to improve information-searching performance. Extensive experiments on different types of datasets show the superiority of the proposed method in query accuracy, time consumption, and memory usage.
</details>
<details>
<summary>摘要</summary>
With the rapid development of GPU (图形处理器) technologies and neural networks, we can explore more appropriate data structures and algorithms. Recent progress shows that neural networks can partly replace traditional data structures. In this paper, we proposed a novel DNN (深度神经网络)-based learned locality-sensitive hashing, called LLSH, to efficiently and flexibly map high-dimensional data to low-dimensional space. LLSH replaces the traditional LSH (本地相似哈希) function families with parallel multi-layer neural networks, which reduces the time and memory consumption and guarantees query accuracy simultaneously. The proposed LLSH demonstrate the feasibility of replacing the hash index with learning-based neural networks and open a new door for developers to design and configure data organization more accurately to improve information-searching performance. Extensive experiments on different types of datasets show the superiority of the proposed method in query accuracy, time consumption, and memory usage.
</details></li>
</ul>
<hr>
<h2 id="Model-Inversion-Attacks-on-Homogeneous-and-Heterogeneous-Graph-Neural-Networks"><a href="#Model-Inversion-Attacks-on-Homogeneous-and-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural Networks"></a>Model Inversion Attacks on Homogeneous and Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09800">http://arxiv.org/abs/2310.09800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renyang Liu, Wei Zhou, Jinhong Zhang, Xiaoyuan Liu, Peiyuan Si, Haoran Li<br>for:这个研究旨在提出一种新的模型反向攻击方法，以对Homogeneous Graph Neural Networks (HomoGNNs)和Heterogeneous Graph Neural Networks (HeteGNNs)进行模型反向攻击。methods:这个方法基于Gradient Descent的优化方法，目的是将目标GNN的模型内部结构重建出来，以便进行模型反向攻击。results:实验结果显示，提案的方法可以在多个 benchmark 上实现更好的性能，并且在HeteGNNs上进行模型反向攻击是第一次尝试。<details>
<summary>Abstract</summary>
Recently, Graph Neural Networks (GNNs), including Homogeneous Graph Neural Networks (HomoGNNs) and Heterogeneous Graph Neural Networks (HeteGNNs), have made remarkable progress in many physical scenarios, especially in communication applications. Despite achieving great success, the privacy issue of such models has also received considerable attention. Previous studies have shown that given a well-fitted target GNN, the attacker can reconstruct the sensitive training graph of this model via model inversion attacks, leading to significant privacy worries for the AI service provider. We advocate that the vulnerability comes from the target GNN itself and the prior knowledge about the shared properties in real-world graphs. Inspired by this, we propose a novel model inversion attack method on HomoGNNs and HeteGNNs, namely HomoGMI and HeteGMI. Specifically, HomoGMI and HeteGMI are gradient-descent-based optimization methods that aim to maximize the cross-entropy loss on the target GNN and the $1^{st}$ and $2^{nd}$-order proximities on the reconstructed graph. Notably, to the best of our knowledge, HeteGMI is the first attempt to perform model inversion attacks on HeteGNNs. Extensive experiments on multiple benchmarks demonstrate that the proposed method can achieve better performance than the competitors.
</details>
<details>
<summary>摘要</summary>
最近，图 necklace Neural Networks（GNNs），包括同种图 necklace Neural Networks（HomoGNNs）和不同种图 necklace Neural Networks（HeteGNNs），在许多物理场景中取得了很大的进步，特别是在通信应用场景中。尽管取得了很大的成功，但是隐私问题也得到了广泛的关注。先前的研究表明，给出了一个良好适应的目标GNN，攻击者可以通过模型反向攻击来重建敏感的训练图，从而导致AI服务提供商的隐私问题。我们认为，抵触点来自于目标GNN自身和在实际图中共享的特性知识。 inspirited by this，我们提出了一种基于梯度下降优化的模型反向攻击方法，称为HomoGMI和HeteGMI。具体来说，HomoGMI和HeteGMI都是使用梯度下降方法来最大化目标GNN上的权重损失和$1^{st}$和$2^{nd}$邻域的距离损失。需要注意的是，到目前为止，HeteGMI是首次对HeteGNN进行模型反向攻击。我们在多个标准 benchmark上进行了广泛的实验，结果表明，我们的方法可以在与竞争者相比取得更好的性能。
</details></li>
</ul>
<hr>
<h2 id="AFLOW-Developing-Adversarial-Examples-under-Extremely-Noise-limited-Settings"><a href="#AFLOW-Developing-Adversarial-Examples-under-Extremely-Noise-limited-Settings" class="headerlink" title="AFLOW: Developing Adversarial Examples under Extremely Noise-limited Settings"></a>AFLOW: Developing Adversarial Examples under Extremely Noise-limited Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09795">http://arxiv.org/abs/2310.09795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renyang Liu, Jinhong Zhang, Haoran Li, Jin Zhang, Yuanyu Wang, Wei Zhou</li>
<li>for: 本研究旨在提出一种隐蔽的对抗例生成方法，以揭示深度神经网络（DNNs）的漏洞，并帮助提高对抗例的耐久性。</li>
<li>methods: 本研究提出了一种基于Normalize Flow的端到端攻击框架，称为AFLOW，以直接干扰隐藏表示的图像来生成恶意对抗例。与先前的方法不同，AFLOW不添加噪音，而是直接对图像的隐藏表示进行修改。</li>
<li>results: 对三个标准数据集进行了广泛的实验，结果表明，AFLOW可以生成更隐蔽、更高质量的对抗例，并在一些耐久性较高的模型上仍然达到更高的攻击成功率。<details>
<summary>Abstract</summary>
Extensive studies have demonstrated that deep neural networks (DNNs) are vulnerable to adversarial attacks. Despite the significant progress in the attack success rate that has been made recently, the adversarial noise generated by most of the existing attack methods is still too conspicuous to the human eyes and proved to be easily detected by defense mechanisms. Resulting that these malicious examples cannot contribute to exploring the vulnerabilities of existing DNNs sufficiently. Thus, to better reveal the defects of DNNs and further help enhance their robustness under noise-limited situations, a new inconspicuous adversarial examples generation method is exactly needed to be proposed. To bridge this gap, we propose a novel Normalize Flow-based end-to-end attack framework, called AFLOW, to synthesize imperceptible adversarial examples under strict constraints. Specifically, rather than the noise-adding manner, AFLOW directly perturbs the hidden representation of the corresponding image to craft the desired adversarial examples. Compared with existing methods, extensive experiments on three benchmark datasets show that the adversarial examples built by AFLOW exhibit superiority in imperceptibility, image quality and attack capability. Even on robust models, AFLOW can still achieve higher attack results than previous methods.
</details>
<details>
<summary>摘要</summary>
广泛的研究表明，深度神经网络（DNNs）容易受到敌意攻击。尽管最近有很大的进步在攻击成功率方面，但现有的攻击方法仍然生成的恶意示例过于醒目，容易被防御机制检测。这意味着这些恶意示例无法充分探索现有DNNs的漏洞，增强其 robustness 下限 Situation。因此，为了更好地揭示 DNNs 的缺陷并帮助提高其Robustness，我们需要提出一种透明度低的攻击方法。为了填补这个空白，我们提出了一种基于 Normalize Flow 的终端攻击框架，称为 AFLOW，可以在严格的约束下生成透明度低的恶意示例。与现有方法相比，我们进行了三个 benchmark 数据集的广泛实验，结果表明，由 AFLOW 生成的恶意示例具有较高的透明度、图像质量和攻击能力。即使面对Robust 模型，AFLOW 仍然可以达到更高的攻击成功率。>>>
</details></li>
</ul>
<hr>
<h2 id="Automated-Detection-of-Cat-Facial-Landmarks"><a href="#Automated-Detection-of-Cat-Facial-Landmarks" class="headerlink" title="Automated Detection of Cat Facial Landmarks"></a>Automated Detection of Cat Facial Landmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09793">http://arxiv.org/abs/2310.09793</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Martvel, Ilan Shimshoni, Anna Zamansky</li>
<li>for: 该论文主要用于提供一个高质量、全面的猫脸表情数据集，以及一种基于猫脸部分描述的面部坐标检测模型。</li>
<li>methods: 该论文使用了一种基于 convolutional neural network 的面部坐标检测模型，其中包括一种使用缩放ensemble方法的面部坐标检测模型，可以在猫脸上显示出极高的性能。</li>
<li>results: 该论文通过使用猫脸数据集和面部坐标检测模型，实现了人类和猫脸的面部坐标检测 task 的混合性能，并且可以在猫脸上显示出极高的准确率。<details>
<summary>Abstract</summary>
The field of animal affective computing is rapidly emerging, and analysis of facial expressions is a crucial aspect. One of the most significant challenges that researchers in the field currently face is the scarcity of high-quality, comprehensive datasets that allow the development of models for facial expressions analysis. One of the possible approaches is the utilisation of facial landmarks, which has been shown for humans and animals. In this paper we present a novel dataset of cat facial images annotated with bounding boxes and 48 facial landmarks grounded in cat facial anatomy. We also introduce a landmark detection convolution neural network-based model which uses a magnifying ensembe method. Our model shows excellent performance on cat faces and is generalizable to human facial landmark detection.
</details>
<details>
<summary>摘要</summary>
“动物情感计算领域在快速发展，面部表达分析是一项重要的挑战。研究人员目前面临的一个主要挑战是获得高质量、全面的面部表达数据集，以便发展面部表达分析模型。我们在这篇论文中提出了一种使用面部特征点的方法，并提供了一个基于 convolutional neural network 的面部特征点检测模型。我们的模型在猫脸上表现出色，并且可以普适应用于人类面部特征点检测。”Here's the breakdown of the translation:* 动物情感计算领域 (dòngwù qíngshěn jìsuàn) - "animal affective computing field"*  rapidly emerging (shìyù xiǎngchuāng) - "rapidly emerging"*  analysis of facial expressions (miàn zhèng xiàngxìng) - "analysis of facial expressions"* One of the most significant challenges (yī zhèng zhìshì) - "one of the most significant challenges"* that researchers in the field currently face (zhèng zhìshì) - "that researchers in the field currently face"* is the scarcity of high-quality, comprehensive datasets (shūshì, zhìshì de yīxiàng) - "is the scarcity of high-quality, comprehensive datasets"* that allow the development of models for facial expressions analysis (miàn zhèng xiàngxìng yìjīng) - "that allow the development of models for facial expressions analysis"* One of the possible approaches (yī zhèng zhìshì) - "one of the possible approaches"* is the utilization of facial landmarks (miàn zhèng zhìshì) - "is the utilization of facial landmarks"* which has been shown for humans and animals (yīnwàng zhèndào) - "which has been shown for humans and animals"* We present a novel dataset of cat facial images (wǒmen xiǎngchuāng zhèng zhìshì) - "we present a novel dataset of cat facial images"* annotated with bounding boxes and 48 facial landmarks grounded in cat facial anatomy (jìchuāng yīnwàng zhèndào) - "annotated with bounding boxes and 48 facial landmarks grounded in cat facial anatomy"* We also introduce a landmark detection convolution neural network-based model (wǒmen xiǎngchuāng zhìshì yìjīng) - "we also introduce a landmark detection convolution neural network-based model"* which uses a magnifying ensemble method (jìchuāng yìjīng) - "which uses a magnifying ensemble method"* Our model shows excellent performance on cat faces (wǒmen xiǎngchuāng zhèng zhìshì) - "our model shows excellent performance on cat faces"* and is generalizable to human facial landmark detection (rénshēng zhìshì) - "and is generalizable to human facial landmark detection"
</details></li>
</ul>
<hr>
<h2 id="SCME-A-Self-Contrastive-Method-for-Data-free-and-Query-Limited-Model-Extraction-Attack"><a href="#SCME-A-Self-Contrastive-Method-for-Data-free-and-Query-Limited-Model-Extraction-Attack" class="headerlink" title="SCME: A Self-Contrastive Method for Data-free and Query-Limited Model Extraction Attack"></a>SCME: A Self-Contrastive Method for Data-free and Query-Limited Model Extraction Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09792">http://arxiv.org/abs/2310.09792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renyang Liu, Jinhong Zhang, Kwok-Yan Lam, Jun Zhao, Wei Zhou<br>for:本研究旨在提高模型EXTRACTION攻击的效果，尤其是在有限Query情况下。methods:提出了一种新的数据自由的模型EXTRACTION方法（SCME），通过考虑内类多样性和间类多样性，生成多样化的假数据，并通过Mixup操作进一步增强模型的探索能力。results:在多种攻击场景下，SCME方法在有限Query情况下显示出了11.43%的平均提升，特别是对于未targeted攻击，SCME方法超过了当前最佳方法。<details>
<summary>Abstract</summary>
Previous studies have revealed that artificial intelligence (AI) systems are vulnerable to adversarial attacks. Among them, model extraction attacks fool the target model by generating adversarial examples on a substitute model. The core of such an attack is training a substitute model as similar to the target model as possible, where the simulation process can be categorized in a data-dependent and data-free manner. Compared with the data-dependent method, the data-free one has been proven to be more practical in the real world since it trains the substitute model with synthesized data. However, the distribution of these fake data lacks diversity and cannot detect the decision boundary of the target model well, resulting in the dissatisfactory simulation effect. Besides, these data-free techniques need a vast number of queries to train the substitute model, increasing the time and computing consumption and the risk of exposure. To solve the aforementioned problems, in this paper, we propose a novel data-free model extraction method named SCME (Self-Contrastive Model Extraction), which considers both the inter- and intra-class diversity in synthesizing fake data. In addition, SCME introduces the Mixup operation to augment the fake data, which can explore the target model's decision boundary effectively and improve the simulating capacity. Extensive experiments show that the proposed method can yield diversified fake data. Moreover, our method has shown superiority in many different attack settings under the query-limited scenario, especially for untargeted attacks, the SCME outperforms SOTA methods by 11.43\% on average for five baseline datasets.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose a novel data-free model extraction method called SCME (Self-Contrastive Model Extraction). Our method considers both inter- and intra-class diversity when synthesizing fake data, and introduces the Mixup operation to augment the fake data, allowing us to effectively explore the target model's decision boundary and improve the simulating capacity. Extensive experiments show that the proposed method can generate diversified fake data, and our method has shown superiority in many different attack settings under the query-limited scenario, especially for untargeted attacks. On average, our method outperforms state-of-the-art methods by 11.43% for five baseline datasets.
</details></li>
</ul>
<hr>
<h2 id="CBARF-Cascaded-Bundle-Adjusting-Neural-Radiance-Fields-from-Imperfect-Camera-Poses"><a href="#CBARF-Cascaded-Bundle-Adjusting-Neural-Radiance-Fields-from-Imperfect-Camera-Poses" class="headerlink" title="CBARF: Cascaded Bundle-Adjusting Neural Radiance Fields from Imperfect Camera Poses"></a>CBARF: Cascaded Bundle-Adjusting Neural Radiance Fields from Imperfect Camera Poses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09776">http://arxiv.org/abs/2310.09776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Fu, Xin Yu, Lincheng Li, Li Zhang</li>
<li>for: 本文提出了一种新的3D重建框架，用于同时优化摄像头姿势，以提高novel view的synthesizing质量。</li>
<li>methods: 该框架采用了分层bundle-adjustment（BA）模块，通过粗化-细化的方式进行摄像头姿势的优化，并采用了一种邻居替换策略来进一步优化BA的结果。</li>
<li>results: 实验结果表明，CBARF模型在摄像头姿势优化和novel view synthesis中表现出了state-of-the-art的性能，特别是在大量摄像头姿势噪声的情况下。<details>
<summary>Abstract</summary>
Existing volumetric neural rendering techniques, such as Neural Radiance Fields (NeRF), face limitations in synthesizing high-quality novel views when the camera poses of input images are imperfect. To address this issue, we propose a novel 3D reconstruction framework that enables simultaneous optimization of camera poses, dubbed CBARF (Cascaded Bundle-Adjusting NeRF).In a nutshell, our framework optimizes camera poses in a coarse-to-fine manner and then reconstructs scenes based on the rectified poses. It is observed that the initialization of camera poses has a significant impact on the performance of bundle-adjustment (BA). Therefore, we cascade multiple BA modules at different scales to progressively improve the camera poses. Meanwhile, we develop a neighbor-replacement strategy to further optimize the results of BA in each stage. In this step, we introduce a novel criterion to effectively identify poorly estimated camera poses. Then we replace them with the poses of neighboring cameras, thus further eliminating the impact of inaccurate camera poses. Once camera poses have been optimized, we employ a density voxel grid to generate high-quality 3D reconstructed scenes and images in novel views. Experimental results demonstrate that our CBARF model achieves state-of-the-art performance in both pose optimization and novel view synthesis, especially in the existence of large camera pose noise.
</details>
<details>
<summary>摘要</summary>
现有的量化神经渲染技术，如神经辐射场（NeRF），在输入图像的相机pose不 precisions时面临限制。为解决这个问题，我们提出了一种新的三维重建框架，称为CBARF（层次拟合束适应NeRF）。总之，我们的框架在层次进行相机pose的优化，然后基于修正后的相机pose来重建场景。我们发现初始相机pose的初始化对bundle-adjustment（BA）的性能有很大的影响。因此，我们在不同的级别上cascade多个BA模块，以逐步改进相机pose。同时，我们开发了一种邻居替换策略，以进一步优化BA模块在每个阶段的结果。在这个步骤中，我们引入了一种新的 criterion，以有效地识别估计不准确的相机pose。然后，我们将其替换为邻居相机的pose，从而进一步消除不准确的相机pose的影响。一旦相机pose被优化，我们就可以使用密度体积格来生成高质量的3D重建场景和图像。实验结果表明，我们的CBARF模型在相机pose的优化和新视图合成方面达到了状态级表现，特别是在相机pose噪声较大的情况下。
</details></li>
</ul>
<hr>
<h2 id="Image-Augmentation-with-Controlled-Diffusion-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Image-Augmentation-with-Controlled-Diffusion-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Image Augmentation with Controlled Diffusion for Weakly-Supervised Semantic Segmentation"></a>Image Augmentation with Controlled Diffusion for Weakly-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09760">http://arxiv.org/abs/2310.09760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao</li>
<li>for:  trains semantic segmentation models solely using image-level labels, and aims to improve the quality of pseudo labels when the size of available dataset is limited.</li>
<li>methods:  introduces a novel approach called Image Augmentation with Controlled Diffusion (IACD), which effectively augments existing labeled datasets by generating diverse images through controlled diffusion, and proposes a high-quality image selection strategy to mitigate the potential noise introduced by the randomness of diffusion models.</li>
<li>results:  clearly surpasses existing state-of-the-art methods, and the effect is more obvious when the amount of available data is small, demonstrating the effectiveness of the proposed IACD approach.<details>
<summary>Abstract</summary>
Weakly-supervised semantic segmentation (WSSS), which aims to train segmentation models solely using image-level labels, has achieved significant attention. Existing methods primarily focus on generating high-quality pseudo labels using available images and their image-level labels. However, the quality of pseudo labels degrades significantly when the size of available dataset is limited. Thus, in this paper, we tackle this problem from a different view by introducing a novel approach called Image Augmentation with Controlled Diffusion (IACD). This framework effectively augments existing labeled datasets by generating diverse images through controlled diffusion, where the available images and image-level labels are served as the controlling information. Moreover, we also propose a high-quality image selection strategy to mitigate the potential noise introduced by the randomness of diffusion models. In the experiments, our proposed IACD approach clearly surpasses existing state-of-the-art methods. This effect is more obvious when the amount of available data is small, demonstrating the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
弱级Semantic segmentation（WSSS），它目标是通过图像级别标签来训练 segmentation 模型，已经吸引了广泛的关注。现有方法主要集中在生成高质量 Pseudo label 上，使用可用的图像和图像级别标签。然而，当数据集的大小受限时， pseudo label 的质量会下降 significatively。因此，在这篇论文中，我们从不同的角度解决这个问题，我们提出了一种新的方法，即 Image Augmentation with Controlled Diffusion（IACD）。这个框架可以有效地增强现有的标注数据集，通过控制的扩散来生成多样化的图像。此外，我们还提出了一种高质量图像选择策略，以避免扩散模型中的随机性引入的噪音。在实验中，我们的提议的 IACD 方法明显超越了现有的状态对照方法。这个效果更加明显，当数据集的量受限时，这表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Prototype-oriented-Unsupervised-Change-Detection-for-Disaster-Management"><a href="#Prototype-oriented-Unsupervised-Change-Detection-for-Disaster-Management" class="headerlink" title="Prototype-oriented Unsupervised Change Detection for Disaster Management"></a>Prototype-oriented Unsupervised Change Detection for Disaster Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09759">http://arxiv.org/abs/2310.09759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngtack Oh, Minseok Seo, Doyi Kim, Junghoon Seo</li>
<li>For: 本研究旨在提出一种不需要标注的自然灾害监测方法，以应对气候变化导致的自然灾害的频发。* Methods: 本研究提出了一种名为prototype-orientedUnsupervised Change Detection for Disaster Management（PUCD）的方法，该方法通过比较预事件、后事件和基础模型生成的变化合成图像的特征来检测变化，并使用Segment Anything Model（SAM）进行精细化。* Results: 本研究在LEVIR-Extension数据集上评估了PUCD方法，并与其他方法进行比较，结果显示PUCD方法在LEVIR-Extension数据集上达到了现有方法的最优性。<details>
<summary>Abstract</summary>
Climate change has led to an increased frequency of natural disasters such as floods and cyclones. This emphasizes the importance of effective disaster monitoring. In response, the remote sensing community has explored change detection methods. These methods are primarily categorized into supervised techniques, which yield precise results but come with high labeling costs, and unsupervised techniques, which eliminate the need for labeling but involve intricate hyperparameter tuning. To address these challenges, we propose a novel unsupervised change detection method named Prototype-oriented Unsupervised Change Detection for Disaster Management (PUCD). PUCD captures changes by comparing features from pre-event, post-event, and prototype-oriented change synthesis images via a foundational model, and refines results using the Segment Anything Model (SAM). Although PUCD is an unsupervised change detection, it does not require complex hyperparameter tuning. We evaluate PUCD framework on the LEVIR-Extension dataset and the disaster dataset and it achieves state-of-the-art performance compared to other methods on the LEVIR-Extension dataset.
</details>
<details>
<summary>摘要</summary>
климат变化导致自然灾害的频率增加，这重要性化效果监测。因此，远程感知社区已经探索了变化检测方法。这些方法主要分为监督式技术，它们可以提供精确的结果，但是需要高的标注成本，以及无监督技术，它们可以消除标注需求，但是需要复杂的 гиперпараметров调整。为解决这些挑战，我们提出了一种基于原型的无监督变化检测方法，名为“原型 ориентирован无监督变化检测 для灾害管理”（PUCD）。PUCD通过比较预事件、后事件和基于原型的变化合成图像的特征来捕捉变化，并使用基本模型（SAM）进行细化。尽管PUCD是无监督的变化检测方法，但它不需要复杂的 гиперпараметров调整。我们对PUCD框架在LEVIR-Extension数据集和灾害数据集进行评估，并达到了与其他方法相比的状态前方性表现。
</details></li>
</ul>
<hr>
<h2 id="MoEmo-Vision-Transformer-Integrating-Cross-Attention-and-Movement-Vectors-in-3D-Pose-Estimation-for-HRI-Emotion-Detection"><a href="#MoEmo-Vision-Transformer-Integrating-Cross-Attention-and-Movement-Vectors-in-3D-Pose-Estimation-for-HRI-Emotion-Detection" class="headerlink" title="MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection"></a>MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09757">http://arxiv.org/abs/2310.09757</a></li>
<li>repo_url: None</li>
<li>paper_authors: David C. Jeong, Tianma Shen, Hongji Liu, Raghav Kapoor, Casey Nguyen, Song Liu, Christopher A. Kitts</li>
<li>for: 这 paper 的目的是提出一种基于人体姿势估计的人工智能人机交互（HRI）中的情绪检测方法。</li>
<li>methods: 这 paper 使用了cross-attention视觉变换器（ViT）和自然语言处理技术，将人体姿势估计与环境上下文进行交互，以实现更高精度的情绪检测。</li>
<li>results: 对比现有方法，这 paper 的方法可以更好地利用人体姿势和环境上下文之间的微妙关系，从而提高情绪检测的准确率。<details>
<summary>Abstract</summary>
Emotion detection presents challenges to intelligent human-robot interaction (HRI). Foundational deep learning techniques used in emotion detection are limited by information-constrained datasets or models that lack the necessary complexity to learn interactions between input data elements, such as the the variance of human emotions across different contexts. In the current effort, we introduce 1) MoEmo (Motion to Emotion), a cross-attention vision transformer (ViT) for human emotion detection within robotics systems based on 3D human pose estimations across various contexts, and 2) a data set that offers full-body videos of human movement and corresponding emotion labels based on human gestures and environmental contexts. Compared to existing approaches, our method effectively leverages the subtle connections between movement vectors of gestures and environmental contexts through the use of cross-attention on the extracted movement vectors of full-body human gestures/poses and feature maps of environmental contexts. We implement a cross-attention fusion model to combine movement vectors and environment contexts into a joint representation to derive emotion estimation. Leveraging our Naturalistic Motion Database, we train the MoEmo system to jointly analyze motion and context, yielding emotion detection that outperforms the current state-of-the-art.
</details>
<details>
<summary>摘要</summary>
人工智能human-robot交互（HRI）中的情绪检测受到挑战。基础的深度学习技术在情绪检测方面受到数据约束或模型缺乏足够复杂性来学习输入数据元素之间的交互，如人类情绪在不同情境下的变化。在当前努力中，我们介绍了以下两个方法：1. MoEmo（动作到情绪）：基于机器人系统的人体pose估计中的3D人体动作，采用视Transformer（ViT）来检测人类情绪。2. 一个包含全身动作和对应情绪标签的完整数据集。与现有方法相比，我们的方法可以充分利用人体动作vector和环境上下文的关系，通过跨注意力 fusion模型将动作vector和环境上下文融合为共同表示，从而实现情绪估计。我们使用自然主义人体动作数据库来训练MoEmo系统，以同时分析动作和环境，实现情绪检测的改进。
</details></li>
</ul>
<hr>
<h2 id="New-Benchmarks-for-Asian-Facial-Recognition-Tasks-Face-Classification-with-Large-Foundation-Models"><a href="#New-Benchmarks-for-Asian-Facial-Recognition-Tasks-Face-Classification-with-Large-Foundation-Models" class="headerlink" title="New Benchmarks for Asian Facial Recognition Tasks: Face Classification with Large Foundation Models"></a>New Benchmarks for Asian Facial Recognition Tasks: Face Classification with Large Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09756">http://arxiv.org/abs/2310.09756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dukong1/koin_benchmark_dataset">https://github.com/dukong1/koin_benchmark_dataset</a></li>
<li>paper_authors: Jinwoo Seo, Soora Choi, Eungyeom Ha, Beomjune Kim, Dongbin Na</li>
<li>for: 这个论文是为了开发一个大规模的韩国Influencer分类系统而写的。</li>
<li>methods: 这篇论文使用了大量的韩国明星照片，并在这些照片中添加了各种环境，如舞台照明、后台舞者和背景物体。这些照片可以用于训练分类模型，以便正确地识别韩国Influencer。</li>
<li>results: 论文提出了一个名为KoIn的大规模韩国Influencer数据集，包含100,000多张韩国明星照片，并提供了一些困难的样例图像，如人脸图像包含面具和帽子。论文还进行了多种实验，包括使用现有的基础模型来证明KoIn数据集的有效性。<details>
<summary>Abstract</summary>
The face classification system is an important tool for recognizing personal identity properly. This paper introduces a new Large-Scale Korean Influencer Dataset named KoIn. Our presented dataset contains many real-world photos of Korean celebrities in various environments that might contain stage lighting, backup dancers, and background objects. These various images can be useful for training classification models classifying K-influencers. Most of the images in our proposed dataset have been collected from social network services (SNS) such as Instagram. Our dataset, KoIn, contains over 100,000 K-influencer photos from over 100 Korean celebrity classes. Moreover, our dataset provides additional hard case samples such as images including human faces with masks and hats. We note that the hard case samples are greatly useful in evaluating the robustness of the classification systems. We have extensively conducted several experiments utilizing various classification models to validate the effectiveness of our proposed dataset. Specifically, we demonstrate that recent state-of-the-art (SOTA) foundation architectures show decent classification performance when trained on our proposed dataset. In this paper, we also analyze the robustness performance against hard case samples of large-scale foundation models when we fine-tune the foundation models on the normal cases of the proposed dataset, KoIn. Our presented dataset and codes will be publicly available at https://github.com/dukong1/KoIn_Benchmark_Dataset.
</details>
<details>
<summary>摘要</summary>
“人脸分类系统是识别个人身份的重要工具。本文介绍了一个新的大规模韩国 influencer 数据集名为 KoIn。我们提供的数据集包含了许多真实的韩国明星照片，其中包括舞台照明、后台舞者和背景物品等不同环境。这些图像可以用于训练分类模型，以便将 K-influencer 分类 correctly。大多数图像在我们提出的数据集中来自社交媒体服务（SNS）such as Instagram。我们的数据集 KoIn 包含了超过 100,000 韩国明星照片，来自于超过 100 个韩国明星类别。此外，我们的数据集还提供了一些难易 случа例样本，包括人脸图像中的面具和帽子等。我们注意到这些难易 случа例样本在评估分类系统的Robustness 性能时非常有用。我们在这篇文章中进行了多种实验，以验证我们提出的数据集的有效性。具体来说，我们表明了最新的基础建筑（SOTA）的基础模型在我们提出的数据集上进行训练后的分类性能不错。在这篇文章中，我们还分析了大规模基础模型在 KoIn 数据集中的 robustness 性能，当我们在 normal cases 上进行 fine-tune 时。我们将提供的数据集和代码将在 https://github.com/dukong1/KoIn_Benchmark_Dataset 上公开。”
</details></li>
</ul>
<hr>
<h2 id="Staged-Depthwise-Correlation-and-Feature-Fusion-for-Siamese-Object-Tracking"><a href="#Staged-Depthwise-Correlation-and-Feature-Fusion-for-Siamese-Object-Tracking" class="headerlink" title="Staged Depthwise Correlation and Feature Fusion for Siamese Object Tracking"></a>Staged Depthwise Correlation and Feature Fusion for Siamese Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09747">http://arxiv.org/abs/2310.09747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dianbo Ma, Jianqiang Xiao, Ziyan Gao, Satoshi Yamane</li>
<li>for: 提高视觉跟踪的特征提取效果</li>
<li>methods: 提出了一种新的多Stage深度相关和特征融合网络（DCFFNet），利用多级层次特征和多通道semantics来学习对象特征的优化权重</li>
<li>results: 对多个大规模数据集进行了端到端协调训练，实现了模型的稳定训练和高性能，并在多个标准测试集上达到了多种跟踪器的竞争性表现<details>
<summary>Abstract</summary>
In this work, we propose a novel staged depthwise correlation and feature fusion network, named DCFFNet, to further optimize the feature extraction for visual tracking. We build our deep tracker upon a siamese network architecture, which is offline trained from scratch on multiple large-scale datasets in an end-to-end manner. The model contains a core component, that is, depthwise correlation and feature fusion module (correlation-fusion module), which facilitates model to learn a set of optimal weights for a specific object by utilizing ensembles of multi-level features from lower and higher layers and multi-channel semantics on the same layer. We combine the modified ResNet-50 with the proposed correlation-fusion layer to constitute the feature extractor of our model. In training process, we find the training of model become more stable, that benifits from the correlation-fusion module. For comprehensive evaluations of performance, we implement our tracker on the popular benchmarks, including OTB100, VOT2018 and LaSOT. Extensive experiment results demonstrate that our proposed method achieves favorably competitive performance against many leading trackers in terms of accuracy and precision, while satisfying the real-time requirements of applications.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种新的阶段化深度相关和特征融合网络（DCFFNet），用于进一步优化视觉跟踪的特征提取。我们基于siames network架构，将模型在多个大规模数据集上进行了线性训练，并在整个过程中从头到尾地训练。模型的核心组件是深度相关和特征融合模块（相关融合模块），它使得模型可以通过不同层次的特征和多个渠道semantic来学习一个特定对象的最佳权重。我们将修改后的ResNet-50和提议的相关融合层组合成我们的特征提取器。在训练过程中，我们发现模型的训练变得更加稳定，这是由相关融合模块带来的。为了进行全面的性能评估，我们在知名的benchmark上实现了我们的跟踪器，包括OTB100、VOT2018和LaSOT。广泛的实验结果表明，我们的提议方法在精度和稳定性两个方面与许多领先的跟踪器相比，表现出非常竞争力，同时满足应用中的实时需求。
</details></li>
</ul>
<hr>
<h2 id="Explore-the-Effect-of-Data-Selection-on-Poison-Efficiency-in-Backdoor-Attacks"><a href="#Explore-the-Effect-of-Data-Selection-on-Poison-Efficiency-in-Backdoor-Attacks" class="headerlink" title="Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks"></a>Explore the Effect of Data Selection on Poison Efficiency in Backdoor Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09744">http://arxiv.org/abs/2310.09744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqiang Li, Pengfei Xia, Hong Sun, Yueqi Zeng, Wei Zhang, Bin Li<br>for: 降低培训数据 Collection 成本，提高深度神经网络 (DNNs) 的攻击性能。methods: 提出了一种基于 Forgetting 和 curvature 的 Sample Selection Strategy，可以提高攻击效率。results: 在多个领域 (CIFAR-10、CIFAR-100、ImageNet-10、AG News、ESC-50、Facial Age) 的实验结果表明，提案的方法可以在相同的恶意比例下提高攻击性能。<details>
<summary>Abstract</summary>
As the number of parameters in Deep Neural Networks (DNNs) scales, the thirst for training data also increases. To save costs, it has become common for users and enterprises to delegate time-consuming data collection to third parties. Unfortunately, recent research has shown that this practice raises the risk of DNNs being exposed to backdoor attacks. Specifically, an attacker can maliciously control the behavior of a trained model by poisoning a small portion of the training data. In this study, we focus on improving the poisoning efficiency of backdoor attacks from the sample selection perspective. The existing attack methods construct such poisoned samples by randomly selecting some clean data from the benign set and then embedding a trigger into them. However, this random selection strategy ignores that each sample may contribute differently to the backdoor injection, thereby reducing the poisoning efficiency. To address the above problem, a new selection strategy named Improved Filtering and Updating Strategy (FUS++) is proposed. Specifically, we adopt the forgetting events of the samples to indicate the contribution of different poisoned samples and use the curvature of the loss surface to analyses the effectiveness of this phenomenon. Accordingly, we combine forgetting events and curvature of different samples to conduct a simple yet efficient sample selection strategy. The experimental results on image classification (CIFAR-10, CIFAR-100, ImageNet-10), text classification (AG News), audio classification (ESC-50), and age regression (Facial Age) consistently demonstrate the effectiveness of the proposed strategy: the attack performance using FUS++ is significantly higher than that using random selection for the same poisoning ratio.
</details>
<details>
<summary>摘要</summary>
Existing attack methods create poisoned samples by randomly selecting some clean data from the benign set and embedding a trigger into them. However, this random selection strategy overlooks the fact that each sample may contribute differently to the backdoor injection, thereby reducing the poisoning efficiency.To address this issue, we propose a new sample selection strategy called Improved Filtering and Updating Strategy (FUS++). We utilize the forgetting events of the samples to indicate their contribution to the backdoor injection and analyze the effectiveness of this phenomenon using the curvature of the loss surface. By combining forgetting events and curvature of different samples, we develop a simple yet efficient sample selection strategy.Our experimental results on image classification (CIFAR-10, CIFAR-100, ImageNet-10), text classification (AG News), audio classification (ESC-50), and age regression (Facial Age) consistently show that the proposed strategy outperforms random selection for the same poisoning ratio. The attack performance using FUS++ is significantly higher, indicating the effectiveness of our proposed strategy in enhancing the poisoning efficiency of backdoor attacks.
</details></li>
</ul>
<hr>
<h2 id="AugUndo-Scaling-Up-Augmentations-for-Unsupervised-Depth-Completion"><a href="#AugUndo-Scaling-Up-Augmentations-for-Unsupervised-Depth-Completion" class="headerlink" title="AugUndo: Scaling Up Augmentations for Unsupervised Depth Completion"></a>AugUndo: Scaling Up Augmentations for Unsupervised Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09739">http://arxiv.org/abs/2310.09739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangchao Wu, Tian Yu Liu, Hyoungseob Park, Stefano Soatto, Dong Lao, Alex Wong</li>
<li>for: 提高无监督深度完成任务的性能（improve the performance of unsupervised depth completion tasks）</li>
<li>methods: 使用“undo”操作来解除各种几何变换对深度图的影响，从而计算恢复损失使用原始图像和稀疏深度图，从而缩大数据增强的可能性（use “undo” operation to eliminate the impact of various geometric transformations on the depth map, and compute the reconstruction loss using the original images and sparse depth maps, thus expanding the possibility of data augmentation）</li>
<li>results: 在indoor（VOID）和outdoor（KITTI）数据集上，与三种现有方法进行比较，平均提高了10.4%（compared to three existing methods on the indoor (VOID) and outdoor (KITTI) datasets, with an average improvement of 10.4%）<details>
<summary>Abstract</summary>
Unsupervised depth completion methods are trained by minimizing sparse depth and image reconstruction error. Block artifacts from resampling, intensity saturation, and occlusions are amongst the many undesirable by-products of common data augmentation schemes that affect image reconstruction quality, and thus the training signal. Hence, typical augmentations on images that are viewed as essential to training pipelines in other vision tasks have seen limited use beyond small image intensity changes and flipping. The sparse depth modality have seen even less as intensity transformations alter the scale of the 3D scene, and geometric transformations may decimate the sparse points during resampling. We propose a method that unlocks a wide range of previously-infeasible geometric augmentations for unsupervised depth completion. This is achieved by reversing, or "undo"-ing, geometric transformations to the coordinates of the output depth, warping the depth map back to the original reference frame. This enables computing the reconstruction losses using the original images and sparse depth maps, eliminating the pitfalls of naive loss computation on the augmented inputs. This simple yet effective strategy allows us to scale up augmentations to boost performance. We demonstrate our method on indoor (VOID) and outdoor (KITTI) datasets where we improve upon three existing methods by an average of 10.4\% across both datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Unsupervised depth completion methods are trained by minimizing sparse depth and image reconstruction error. Block artifacts from resampling, intensity saturation, and occlusions are amongst the many undesirable by-products of common data augmentation schemes that affect image reconstruction quality, and thus the training signal. Hence, typical augmentations on images that are viewed as essential to training pipelines in other vision tasks have seen limited use beyond small image intensity changes and flipping. The sparse depth modality have seen even less as intensity transformations alter the scale of the 3D scene, and geometric transformations may decimate the sparse points during resampling. We propose a method that unlocks a wide range of previously-infeasible geometric augmentations for unsupervised depth completion. This is achieved by reversing, or "undo"-ing, geometric transformations to the coordinates of the output depth, warping the depth map back to the original reference frame. This enables computing the reconstruction losses using the original images and sparse depth maps, eliminating the pitfalls of naive loss computation on the augmented inputs. This simple yet effective strategy allows us to scale up augmentations to boost performance. We demonstrate our method on indoor (VOID) and outdoor (KITTI) datasets where we improve upon three existing methods by an average of 10.4\% across both datasets.")</SYS>>Here's the translation in Simplified Chinese:Unsupervised深度完成方法通常通过最小化稀疏深度和图像重建错误来训练。采样、强化、 occlusion 等常见数据增强方法的副作用会影响图像重建质量，从而影响训练信号。因此，通常会限制图像增强方法的应用，只有小型图像Intensity 变化和翻转。 sparse depth 模式更加受到INTENSITY 变化的影响，Geometric 变换可能会在抽样时消耗稀疏点。我们提出了一种方法，可以解锁许多前置不可能的几何增强。这是通过将几何变换转换为输出深度坐标的反向操作，使深度地图返回原始参照帧。这使得可以使用原始图像和稀疏深度图来计算重建损失，消除了使用增强输入的恶性问题。这种简单 yet 有效的策略允许我们扩大增强，提高性能。我们在indoor (VOID) 和 outdoor (KITTI) 数据集上示出了我们的方法，与三种现有方法的平均提升率为10.4%。
</details></li>
</ul>
<hr>
<h2 id="FuseSR-Super-Resolution-for-Real-time-Rendering-through-Efficient-Multi-resolution-Fusion"><a href="#FuseSR-Super-Resolution-for-Real-time-Rendering-through-Efficient-Multi-resolution-Fusion" class="headerlink" title="FuseSR: Super Resolution for Real-time Rendering through Efficient Multi-resolution Fusion"></a>FuseSR: Super Resolution for Real-time Rendering through Efficient Multi-resolution Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09726">http://arxiv.org/abs/2310.09726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Isaac-Paradox/FuseSR">https://github.com/Isaac-Paradox/FuseSR</a></li>
<li>paper_authors: Zhihua Zhong, Jingsen Zhu, Yuxin Dai, Chuankun Zheng, Yuchi Huo, Guanlin Chen, Hujun Bao, Rui Wang</li>
<li>for: 提高实时渲染的效率和质量，满足高分辨率、高刷新率和高实зм的需求。</li>
<li>methods: 利用低分辨率输入图像，通过高价值高分辨率auxiliary G-Buffer来提高渲染的精度和效率。introduce an efficient and effective H-Net architecture to solve the problem of aligning and fusing features at multi-resolution levels.</li>
<li>results: 实现4K分辨率的实时渲染，并在$4 \times 4$和$8 \times 8$� upsampling cases中提供高质量和高性能的渲染结果，与现有方法相比有substantially improved quality和significant performance boost。<details>
<summary>Abstract</summary>
The workload of real-time rendering is steeply increasing as the demand for high resolution, high refresh rates, and high realism rises, overwhelming most graphics cards. To mitigate this problem, one of the most popular solutions is to render images at a low resolution to reduce rendering overhead, and then manage to accurately upsample the low-resolution rendered image to the target resolution, a.k.a. super-resolution techniques. Most existing methods focus on exploiting information from low-resolution inputs, such as historical frames. The absence of high frequency details in those LR inputs makes them hard to recover fine details in their high-resolution predictions. In this paper, we propose an efficient and effective super-resolution method that predicts high-quality upsampled reconstructions utilizing low-cost high-resolution auxiliary G-Buffers as additional input. With LR images and HR G-buffers as input, the network requires to align and fuse features at multi resolution levels. We introduce an efficient and effective H-Net architecture to solve this problem and significantly reduce rendering overhead without noticeable quality deterioration. Experiments show that our method is able to produce temporally consistent reconstructions in $4 \times 4$ and even challenging $8 \times 8$ upsampling cases at 4K resolution with real-time performance, with substantially improved quality and significant performance boost compared to existing works.
</details>
<details>
<summary>摘要</summary>
工作负载实时渲染在需求高分辨率、高刷新率和高现实性的需求增长，导致大多数图形卡被拥塞。为解决这个问题，一种非常流行的解决方案是在低分辨率下渲染图像，以减轻渲染负担，然后使用高分辨率auxiliary G-Buffers作为额外输入，进行高分辨率恢复。大多数现有方法都是利用低分辨率输入的信息，如历史帧，但是低分辨率输入缺乏高频率细节，使其很难回归高分辨率预测中的细节。在这篇论文中，我们提出了一种高效、高质量的超解像技术，通过利用低成本高分辨率auxiliary G-Buffers作为额外输入，将LR图像和HR G-Buffers作为输入，并对多个分辨率水平进行对齐和融合特征。我们提出了一种高效的H-Net架构来解决这个问题，实现了显著减少渲染负担，无需明显下降质量。实验表明，我们的方法在4K分辨率的$4 \times 4$和甚至更加挑战性的$8 \times 8$恢复 случа中，可以实现实时性和显著性能提升，而且与现有方法相比，有较大的质量提升和性能提升。
</details></li>
</ul>
<hr>
<h2 id="Efficient-and-Effective-Multi-View-Subspace-Clustering-for-Large-scale-Data"><a href="#Efficient-and-Effective-Multi-View-Subspace-Clustering-for-Large-scale-Data" class="headerlink" title="Efficient and Effective Multi-View Subspace Clustering for Large-scale Data"></a>Efficient and Effective Multi-View Subspace Clustering for Large-scale Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09718">http://arxiv.org/abs/2310.09718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiu Lin, Hui Liu, Ren Wang, Gongguan Chen, Caiming Zhang</li>
<li>For: 提高大规模多视图数据集中 clustering 性能，解决现有方法中FC层的参数缺乏效率和内存成本问题。* Methods: 提出了一种新的深度框架E$^2$LMVSC，通过在多视图数据上实现硬件约束来提高共享表示的质量，并使用信息瓶颈理论来获得最小够的共享特征表示。* Results: 对大规模多视图数据集进行了广泛的实验，并证明了E$^2$LMVSC可以与现有方法匹配性能，同时在大规模数据集中实现更高的 clustering 性能。<details>
<summary>Abstract</summary>
Recent multi-view subspace clustering achieves impressive results utilizing deep networks, where the self-expressive correlation is typically modeled by a fully connected (FC) layer. However, they still suffer from two limitations: i) it is under-explored to extract a unified representation from multiple views that simultaneously satisfy minimal sufficiency and discriminability. ii) the parameter scale of the FC layer is quadratic to the number of samples, resulting in high time and memory costs that significantly degrade their feasibility in large-scale datasets. In light of this, we propose a novel deep framework termed Efficient and Effective Large-scale Multi-View Subspace Clustering (E$^2$LMVSC). Specifically, to enhance the quality of the unified representation, a soft clustering assignment similarity constraint is devised for explicitly decoupling consistent, complementary, and superfluous information across multi-view data. Then, following information bottleneck theory, a sufficient yet minimal unified feature representation is obtained. Moreover, E$^2$LMVSC employs the maximal coding rate reduction principle to promote intra-cluster aggregation and inter-cluster separability within the unified representation. Finally, the self-expressive coefficients are learned by a Relation-Metric Net instead of a parameterized FC layer for greater efficiency. Extensive experiments show that E$^2$LMVSC yields comparable results to existing methods and achieves state-of-the-art clustering performance in large-scale multi-view datasets.
</details>
<details>
<summary>摘要</summary>
最近的多视图子空间分 clustering 技术已经取得了很好的成果，使用深度网络，其中自我表达相关性通常是使用全连接（FC）层来模型的。然而，它们仍然受到两种限制：一是不足地提取多视图数据中共同满足最小充分和分类可能性的统一表示。二是FC层的参数缺省是数据样本的平方，导致时间和内存成本很高，使其在大规模数据集中不可行。为此，我们提出了一种新的深度框架，称为高效高质量大规模多视图子空间分 clustering（E$^2$LMVSC）。Specifically, E$^2$LMVSC 使用软 clustering分配相似性约束，以解耦多视图数据中一致、补充和冗余信息的信息。然后，根据信息瓶颈理论，从多视图数据中获得最小充分的统一特征表示。此外，E$^2$LMVSC 使用最大编码率减少原理，以促进内群归一化和间群分离在统一表示中。最后，相关度 metric 网络代替 parameterized FC 层来学习自我表达系数，以提高效率。我们的实验表明，E$^2$LMVSC 与现有方法相当，并在大规模多视图数据集中实现了状态机器人的分 clustering性能。
</details></li>
</ul>
<hr>
<h2 id="LOVECon-Text-driven-Training-Free-Long-Video-Editing-with-ControlNet"><a href="#LOVECon-Text-driven-Training-Free-Long-Video-Editing-with-ControlNet" class="headerlink" title="LOVECon: Text-driven Training-Free Long Video Editing with ControlNet"></a>LOVECon: Text-driven Training-Free Long Video Editing with ControlNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09711">http://arxiv.org/abs/2310.09711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhijie-group/lovecon">https://github.com/zhijie-group/lovecon</a></li>
<li>paper_authors: Zhenyi Liao, Zhijie Deng</li>
<li>for: 这 paper targets 长视频编辑 без需要训练，以满足电影制作、广告等领域的需求。</li>
<li>methods: 我们基于 ControlNet 建立了一个简单而有效的基线，通过将长视频分成窗口，并开发了一种跨窗口注意力机制来保证全局风格的一致性和最大化窗口之间的平滑性。 我们还利用 DDIM 逆转来提取源视频中的信息，并将其集成到生成过程中的秘密状态中。</li>
<li>results: 我们的方法在不同场景中（包括Attributes改变、风格传输和背景替换等）都显示出了超越基eline的效果，能够编辑长达 128 帧的视频 according to 用户需求。<details>
<summary>Abstract</summary>
Leveraging pre-trained conditional diffusion models for video editing without further tuning has gained increasing attention due to its promise in film production, advertising, etc. Yet, seminal works in this line fall short in generation length, temporal coherence, or fidelity to the source video. This paper aims to bridge the gap, establishing a simple and effective baseline for training-free diffusion model-based long video editing. As suggested by prior arts, we build the pipeline upon ControlNet, which excels at various image editing tasks based on text prompts. To break down the length constraints caused by limited computational memory, we split the long video into consecutive windows and develop a novel cross-window attention mechanism to ensure the consistency of global style and maximize the smoothness among windows. To achieve more accurate control, we extract the information from the source video via DDIM inversion and integrate the outcomes into the latent states of the generations. We also incorporate a video frame interpolation model to mitigate the frame-level flickering issue. Extensive empirical studies verify the superior efficacy of our method over competing baselines across scenarios, including the replacement of the attributes of foreground objects, style transfer, and background replacement. In particular, our method manages to edit videos with up to 128 frames according to user requirements. Code is available at https://github.com/zhijie-group/LOVECon.
</details>
<details>
<summary>摘要</summary>
利用预训练的条件扩散模型进行视频编辑，无需进一步调参，在电影制作、广告等领域受到了越来越多的关注。然而，先前的研究在这一领域缺乏长期编辑、时间准确性和原始视频忠实性等方面的表现。本文旨在填补这一空白，建立一个简单有效的基线方法，通过控制网络（ControlNet）和跨窗口注意力机制来实现无需训练的扩散模型基于长视频编辑。为了缓解由计算机内存限制所导致的长度约束，我们将长视频拆分成连续的窗口，并开发了一种新的跨窗口注意力机制，以保证全局风格的一致性和最大化窗口之间的平滑性。此外，我们还提取了源视频中的信息通过DDIM反向减法，并将其 интегрирова到生成过程中的幂态态中。此外，我们还添加了一种视频帧 interpolate模型，以减少帧级闪烁问题。经验研究表明，我们的方法在不同场景中，如改变对eground对象的特性、风格传递和背景替换等场景中，具有更高的效果，并能编辑长达128帧的视频。代码可以在https://github.com/zhijie-group/LOVECon中下载。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/cs.CV_2023_10_15/" data-id="clpxp6c2100lgee88brvb0bd6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/cs.AI_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T12:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/15/cs.AI_2023_10_15/">cs.AI - 2023-10-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-Statistical-Learning-of-Branch-and-Bound-for-Vehicle-Routing-Optimization"><a href="#On-Statistical-Learning-of-Branch-and-Bound-for-Vehicle-Routing-Optimization" class="headerlink" title="On Statistical Learning of Branch and Bound for Vehicle Routing Optimization"></a>On Statistical Learning of Branch and Bound for Vehicle Routing Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09986">http://arxiv.org/abs/2310.09986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/isotlaboratory/ml4vrp">https://github.com/isotlaboratory/ml4vrp</a></li>
<li>paper_authors: Andrew Naguib, Waleed A. Yousef, Issa Traoré, Mohammad Mamun</li>
<li>for:  solve the capacitated vehicle routing problem (CVRP) using machine learning</li>
<li>methods:  utilize and compare the performance of three neural networks (GCNN, GraphSAGE, and GAT) to emulate the Strong Branching strategy</li>
<li>results:  match or improve upon the performance of the branch and bound algorithm with significantly less computational time<details>
<summary>Abstract</summary>
Recently, machine learning of the branch and bound algorithm has shown promise in approximating competent solutions to NP-hard problems. In this paper, we utilize and comprehensively compare the outcomes of three neural networks--graph convolutional neural network (GCNN), GraphSAGE, and graph attention network (GAT)--to solve the capacitated vehicle routing problem. We train these neural networks to emulate the decision-making process of the computationally expensive Strong Branching strategy. The neural networks are trained on six instances with distinct topologies from the CVRPLIB and evaluated on eight additional instances. Moreover, we reduced the minimum number of vehicles required to solve a CVRP instance to a bin-packing problem, which was addressed in a similar manner. Through rigorous experimentation, we found that this approach can match or improve upon the performance of the branch and bound algorithm with the Strong Branching strategy while requiring significantly less computational time. The source code that corresponds to our research findings and methodology is readily accessible and available for reference at the following web address: https://isotlaboratory.github.io/ml4vrp
</details>
<details>
<summary>摘要</summary>
近些时间，机器学习的分支和约束算法在解决NP困难问题中表现出了承诺。在这篇论文中，我们利用了三种神经网络--图 convolutional neural network (GCNN), GraphSAGE, 和 graph attention network (GAT) --来解决具有限制的车辆路径问题。我们使用这些神经网络来模拟计算成本较高的强分支策略的决策过程。我们在CVRPLIB中采样了六个不同的topology实例，并对八个额外实例进行了评估。此外，我们将CVRP实例中的最小车辆数量降低到了一个箱包问题，该问题在类似的方式进行了解决。经过严格的实验，我们发现这种方法可以与分支和约束算法中的强分支策略相匹配或超越性能，并且需要 significatively less computational time。相关的研究发现和方法的源代码可以在以下网址查看：https://isotlaboratory.github.io/ml4vrp
</details></li>
</ul>
<hr>
<h2 id="Farzi-Data-Autoregressive-Data-Distillation"><a href="#Farzi-Data-Autoregressive-Data-Distillation" class="headerlink" title="Farzi Data: Autoregressive Data Distillation"></a>Farzi Data: Autoregressive Data Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09983">http://arxiv.org/abs/2310.09983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noveen Sachdeva, Zexue He, Wang-Cheng Kang, Jianmo Ni, Derek Zhiyuan Cheng, Julian McAuley</li>
<li>for: 本研究旨在为自动逆进机器学习任务提供数据减混技术，以便在训练大型模型时采用更小的数据量。</li>
<li>methods: 我们提出了一种名为“Farzi”的方法，它可以将输入和输出之间的紧密左右 causal 结构转化为一小批 synthetic 序列（Farzi Data），以保持或提高模型性能。 Farzi 在内部使用了高效的反向模板导数和积分产品来实现内存灵活的数据减混。</li>
<li>results: 我们在测试sequential recommendation和语言模型任务中，可以使用 Farzi Data 的0.1%到原始数据大小的比例来训练现代模型，并达到98-120%的下游全数据性能。这表明可以通过减少数据量来训练更好的模型，并开启了将来大型自动逆进机器学习模型的设计和数据量的扩展的新机遇。<details>
<summary>Abstract</summary>
We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences -- Farzi Data -- which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, Farzi conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98-120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.
</details>
<details>
<summary>摘要</summary>
我们研究数据简化技术，用于自动递归学习任务，输入和输出具有约束性的左向 causal 结构。我们提出了 Farzi，它将事件序列数据总结为一小数量的 sintetic 序列 -- Farzi 数据 -- 以保持（或更好）模型性能相对训练全 dataset。在实现方面，Farzi 实现了内存有效的数据简化，通过以下两个方法：(i) 通过利用 Hessian-Vector Products 来 derivate Adam 优化器的逆向Mode 导数，实现高效的数据简化。(ii) 通过 факторизе 高维 discrete 事件空间为尺度空间，实现隐式 regularization。在实验中，我们在序列推荐和自然语言处理任务中，可以使用 Farzi 数据训练现有模型，达到了原始数据的 98-120% 的下游性能。这表明可以通过减少数据量训练更好的模型，为未来大型自动递归模型的设计提供了新的思路，并开创了训练模型和数据量的新机遇。
</details></li>
</ul>
<hr>
<h2 id="Chinese-Painting-Style-Transfer-Using-Deep-Generative-Models"><a href="#Chinese-Painting-Style-Transfer-Using-Deep-Generative-Models" class="headerlink" title="Chinese Painting Style Transfer Using Deep Generative Models"></a>Chinese Painting Style Transfer Using Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09978">http://arxiv.org/abs/2310.09978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanyangbaobeiisemma/chinsepaintingstyletransfer">https://github.com/yanyangbaobeiisemma/chinsepaintingstyletransfer</a></li>
<li>paper_authors: Weijian Ma, Yanyang Kong</li>
<li>for: 本研究旨在将传统中国画风 transferred to modern images like nature objects, portraits and landscapes.</li>
<li>methods: 我们将使用 state-of-the-art deep generative models for Chinese painting style transfer, 并评估其表现 both qualitatively and quantitatively. 此外，我们还提出了一种 combining several style transfer models for our task.</li>
<li>results: 我们将在本研究中评估和比较不同的深度生成模型在传统中国画风转移 task 中的表现, 并提出一种新的方法 combination 多种风格转移模型。<details>
<summary>Abstract</summary>
Artistic style transfer aims to modify the style of the image while preserving its content. Style transfer using deep learning models has been widely studied since 2015, and most of the applications are focused on specific artists like Van Gogh, Monet, Cezanne. There are few researches and applications on traditional Chinese painting style transfer. In this paper, we will study and leverage different state-of-the-art deep generative models for Chinese painting style transfer and evaluate the performance both qualitatively and quantitatively. In addition, we propose our own algorithm that combines several style transfer models for our task. Specifically, we will transfer two main types of traditional Chinese painting style, known as "Gong-bi" and "Shui-mo" (to modern images like nature objects, portraits and landscapes.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文化风格转移目的是对图像的风格进行修改，保留其内容。 Deep learning模型在2015年之后广泛研究了风格转移，大多数应用都是专注于特定艺术家如万高、蒙德、刺激。有很少的研究和应用在传统中国画风格转移方面。在这篇论文中，我们将研究和利用不同的国际先进的生成模型进行中国画风格转移，评估其性能 both qualitatively和quantitatively。此外，我们还提出了我们自己的算法，将多种风格转移模型结合起来用于我们的任务。具体来说，我们将将“公笔”和“水墨”两种传统中国画风格转移到现代图像中，如自然景观、人像和风景等。Translation notes:* "Gong-bi" (工笔) and "Shui-mo" (水墨) are two main types of traditional Chinese painting styles.* "公笔" and "水墨" are both translated as "Chinese painting style" in the text, but they refer to different specific styles.* "国际先进的生成模型" (international advanced generative models) is a phrase used to refer to state-of-the-art deep learning models.* "qualitatively" and "quantitatively" are both translated as "both qualitatively and quantitatively" in the text, but "qualitatively" refers to the subjective evaluation of the results, while "quantitatively" refers to the objective evaluation using metrics such as PSNR or SSIM.
</details></li>
</ul>
<hr>
<h2 id="Specialized-Deep-Residual-Policy-Safe-Reinforcement-Learning-Based-Controller-for-Complex-and-Continuous-State-Action-Spaces"><a href="#Specialized-Deep-Residual-Policy-Safe-Reinforcement-Learning-Based-Controller-for-Complex-and-Continuous-State-Action-Spaces" class="headerlink" title="Specialized Deep Residual Policy Safe Reinforcement Learning-Based Controller for Complex and Continuous State-Action Spaces"></a>Specialized Deep Residual Policy Safe Reinforcement Learning-Based Controller for Complex and Continuous State-Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14788">http://arxiv.org/abs/2310.14788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ammar-n-abbas/CoL-SDRPRL">https://github.com/ammar-n-abbas/CoL-SDRPRL</a></li>
<li>paper_authors: Ammar N. Abbas, Georgios C. Chasparis, John D. Kelleher</li>
<li>For: The paper is written to address the limitations of traditional controllers in safety-critical environments, and to propose a specialized deep reinforcement learning approach for complex and continuous state-action spaces.* Methods: The paper proposes a cycle of learning approach that combines residual policy learning with expert trajectory guidance, and specializes the policy through input-output hidden Markov model to optimize the policy within the region of interest.* Results: The proposed solution is validated on the Tennessee Eastman process control, and the results show that the hybrid control architecture that combines the reinforcement learning agent with the conventional controller can improve the control performance and adapt to abnormal situations.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了解决传统控制器在安全关键环境中的局限性，并提出一种特殊的深度强化学习方法来处理复杂的状态动作空间。* Methods: 论文提出了一种循环学习方法， combining residual policy learning with expert trajectory guidance, 并通过输入输出隐马尔可夫模型特化策略以优化策略在兴趣区域内。* Results: 论文在田州东曼过程控制中验证了该解决方案，结果显示了hybrid控制架构， combining reinforcement learning agent和传统控制器，可以提高控制性能并适应异常情况。<details>
<summary>Abstract</summary>
Traditional controllers have limitations as they rely on prior knowledge about the physics of the problem, require modeling of dynamics, and struggle to adapt to abnormal situations. Deep reinforcement learning has the potential to address these problems by learning optimal control policies through exploration in an environment. For safety-critical environments, it is impractical to explore randomly, and replacing conventional controllers with black-box models is also undesirable. Also, it is expensive in continuous state and action spaces, unless the search space is constrained. To address these challenges we propose a specialized deep residual policy safe reinforcement learning with a cycle of learning approach adapted for complex and continuous state-action spaces. Residual policy learning allows learning a hybrid control architecture where the reinforcement learning agent acts in synchronous collaboration with the conventional controller. The cycle of learning initiates the policy through the expert trajectory and guides the exploration around it. Further, the specialization through the input-output hidden Markov model helps to optimize policy that lies within the region of interest (such as abnormality), where the reinforcement learning agent is required and is activated. The proposed solution is validated on the Tennessee Eastman process control.
</details>
<details>
<summary>摘要</summary>
传统控制器有限制，因为它们基于前期知识，需要动态模型化，并且在异常情况下表现不佳。深度权值学习有可能解决这些问题，通过环境中的探索学习优化控制策略。但是，在安全关键环境下，随机探索是不现实istic，而替换传统控制器的黑obox模型也不符合意愿。此外，在连续状态和动作空间中进行搜索也是昂贵的。为了解决这些挑战，我们提出了特殊化的深度剩余政策安全权值学习，采用环境中的循环学习策略。剩余政策学习允许在传统控制器和权值学习代理之间同步协作，并且通过输入-输出隐藏马尔可夫模型进行特殊化，以便优化政策，使其在特定区域（如异常情况）中表现最佳。我们的解决方案在田中东曼制程控制中得到验证。
</details></li>
</ul>
<hr>
<h2 id="Seeking-Next-Layer-Neurons’-Attention-for-Error-Backpropagation-Like-Training-in-a-Multi-Agent-Network-Framework"><a href="#Seeking-Next-Layer-Neurons’-Attention-for-Error-Backpropagation-Like-Training-in-a-Multi-Agent-Network-Framework" class="headerlink" title="Seeking Next Layer Neurons’ Attention for Error-Backpropagation-Like Training in a Multi-Agent Network Framework"></a>Seeking Next Layer Neurons’ Attention for Error-Backpropagation-Like Training in a Multi-Agent Network Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09952">http://arxiv.org/abs/2310.09952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arshia Soltani Moakhar, Mohammad Azizmalayeri, Hossein Mirzaei, Mohammad Taghi Manzuri, Mohammad Hossein Rohban<br>for: 这 paper 的目的是提出一种基于 local objective 的多智能体神经网络训练方法，以提高神经网络在实际问题中的应用性。methods: 该 paper 使用了一种基于自利 Interest 的神经网络模型，并对其进行了优化。在这种模型中，每个神经元尝试通过 Maximizing 其自己的局部目标来适应神经网络的训练。results: 该 paper 通过三个数据集的实验表明，使用这种方法可以提高神经网络在快速学习和牵扯问题中的性能，并在灾变性学习测试中超过 error-backpropagation。<details>
<summary>Abstract</summary>
Despite considerable theoretical progress in the training of neural networks viewed as a multi-agent system of neurons, particularly concerning biological plausibility and decentralized training, their applicability to real-world problems remains limited due to scalability issues. In contrast, error-backpropagation has demonstrated its effectiveness for training deep networks in practice. In this study, we propose a local objective for neurons that, when pursued by neurons individually, align them to exhibit similarities to error-backpropagation in terms of efficiency and scalability during training. For this purpose, we examine a neural network comprising decentralized, self-interested neurons seeking to maximize their local objective -- attention from subsequent layer neurons -- and identify the optimal strategy for neurons. We also analyze the relationship between this strategy and backpropagation, establishing conditions under which the derived strategy is equivalent to error-backpropagation. Lastly, we demonstrate the learning capacity of these multi-agent neural networks through experiments on three datasets and showcase their superior performance relative to error-backpropagation in a catastrophic forgetting benchmark.
</details>
<details>
<summary>摘要</summary>
具有很大理论进步的神经网络 viewed as a multi-agent system of neurons 的训练，特别是生物可能性和分散式训练，却因为扩展性问题而受限。相比之下，错误反射法在实务中证明了它的有效性 для 训练深度网络。在这篇研究中，我们提出了一个本地目标 для neurons，使得它们个别努力以获得类似于错误反射法的有效性和扩展性 durante 训练。为了实现这个目标，我们对一个分散式、自利 neurons 组成的神经网络进行了分析，并找到了最佳策略 для neurons。我们还分析了这策略和错误反射之间的关系，并证明了在某些情况下， derivated 策略与错误反射法相同。最后，我们透过实验证明了这些多客体神经网络的学习能力，并在三个数据集上显示了它们的超越性。
</details></li>
</ul>
<hr>
<h2 id="Chameleon-a-Heterogeneous-and-Disaggregated-Accelerator-System-for-Retrieval-Augmented-Language-Models"><a href="#Chameleon-a-Heterogeneous-and-Disaggregated-Accelerator-System-for-Retrieval-Augmented-Language-Models" class="headerlink" title="Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models"></a>Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09949">http://arxiv.org/abs/2310.09949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, Gustavo Alonso<br>methods: 该研究使用了一种约束语言模型（LM）和检索器的异质加速器系统，以提高LM的执行效率。results: 研究发现，使用Chameleon系统可以实现23.72倍的速度提升和26.2倍的能效率提升，相比CPU和GPU vector搜索系统。此外，Chameleon系统在不同RALM配置下可以实现1.16倍的响应时间减少和3.18倍的速度提升。<details>
<summary>Abstract</summary>
A Retrieval-Augmented Language Model (RALM) augments a generative language model by retrieving context-specific knowledge from an external database. This strategy facilitates impressive text generation quality even with smaller models, thus reducing orders of magnitude of computational demands. However, RALMs introduce unique system design challenges due to (a) the diverse workload characteristics between LM inference and retrieval and (b) the various system requirements and bottlenecks for different RALM configurations such as model sizes, database sizes, and retrieval frequencies. We propose Chameleon, a heterogeneous accelerator system that integrates both LM and retrieval accelerators in a disaggregated architecture. The heterogeneity ensures efficient acceleration of both LM inference and retrieval, while the accelerator disaggregation enables the system to independently scale both types of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype implements retrieval accelerators on FPGAs and assigns LM inference to GPUs, with a CPU server orchestrating these accelerators over the network. Compared to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput compared to the hybrid CPU-GPU architecture. These promising results pave the way for bringing accelerator heterogeneity and disaggregation into future RALM systems.
</details>
<details>
<summary>摘要</summary>
一种叫做Retrieval-Augmented Language Model（RALM）的语言模型可以通过从外部数据库中获取上下文特定的知识来增强生成语言模型。这种策略使得even with smaller models可以达到出色的文本生成质量，从而降低了计算需求的级别。然而，RALM引入了一些独特的系统设计挑战，包括（a）语言模型推理和检索工作负荷的多样性，以及（b）不同的RALM配置，如模型大小、数据库大小和检索频率等的系统需求和瓶颈。我们提出了一种叫做Chameleon的异步加速器系统，它将语言模型推理和检索加速器分解成不同的硬件模块。这种多样性和分解使得系统可以独立地扩展两类加速器，以满足不同的RALM需求。我们的Chameleon原型在FPGA上实现检索加速器，并将语言模型推理分配给GPU。CPU服务器通过网络管理这些加速器。相比CPU基于和CPU-GPU вектор搜索系统，Chameleon可以达到23.72倍的速度提升和26.2倍的能效率提升。在不同的RALM系统上进行了评估，Chameleon可以减少响应时间2.16倍，提高通过put Throughput 3.18倍。这些出色的结果铺平了将加速器多样性和分解引入未来RALM系统的道路。
</details></li>
</ul>
<hr>
<h2 id="“Reading-Between-the-Heat”-Co-Teaching-Body-Thermal-Signatures-for-Non-intrusive-Stress-Detection"><a href="#“Reading-Between-the-Heat”-Co-Teaching-Body-Thermal-Signatures-for-Non-intrusive-Stress-Detection" class="headerlink" title="“Reading Between the Heat”: Co-Teaching Body Thermal Signatures for Non-intrusive Stress Detection"></a>“Reading Between the Heat”: Co-Teaching Body Thermal Signatures for Non-intrusive Stress Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09932">http://arxiv.org/abs/2310.09932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Xiao, Harshit Sharma, Zhongyang Zhang, Dessa Bergen-Cico, Tauhidur Rahman, Asif Salekin</li>
<li>for: 这 paper 是为了开发一种可靠的、无接触的indoor stress监测系统，用于评估工作场所产能、智能家庭和个性化心理健康监测。</li>
<li>methods: 这 paper 使用了 ThermaStrain，一种共同教学框架，将穿戴式 electrodermal activity (EDA) 传感器和无接触thermal感知结合使用，以提高无接触stress监测的精度。</li>
<li>results: 这 paper 的实验结果表明，ThermaStrain 可以在不同的距离和压力情况下，实现高精度的stress分类，并且在实时执行、边缘计算和多个人感知方面表现出色。<details>
<summary>Abstract</summary>
Stress impacts our physical and mental health as well as our social life. A passive and contactless indoor stress monitoring system can unlock numerous important applications such as workplace productivity assessment, smart homes, and personalized mental health monitoring. While the thermal signatures from a user's body captured by a thermal camera can provide important information about the "fight-flight" response of the sympathetic and parasympathetic nervous system, relying solely on thermal imaging for training a stress prediction model often lead to overfitting and consequently a suboptimal performance. This paper addresses this challenge by introducing ThermaStrain, a novel co-teaching framework that achieves high-stress prediction performance by transferring knowledge from the wearable modality to the contactless thermal modality. During training, ThermaStrain incorporates a wearable electrodermal activity (EDA) sensor to generate stress-indicative representations from thermal videos, emulating stress-indicative representations from a wearable EDA sensor. During testing, only thermal sensing is used, and stress-indicative patterns from thermal data and emulated EDA representations are extracted to improve stress assessment. The study collected a comprehensive dataset with thermal video and EDA data under various stress conditions and distances. ThermaStrain achieves an F1 score of 0.8293 in binary stress classification, outperforming the thermal-only baseline approach by over 9%. Extensive evaluations highlight ThermaStrain's effectiveness in recognizing stress-indicative attributes, its adaptability across distances and stress scenarios, real-time executability on edge platforms, its applicability to multi-individual sensing, ability to function on limited visibility and unfamiliar conditions, and the advantages of its co-teaching approach.
</details>
<details>
<summary>摘要</summary>
压力会影响我们的身体和心理健康以及我们的社会生活。一个不需要接触和干预的indoor压力监测系统可以开启多个重要应用程序，如工作场所产量评估、智能家庭和个性化压力监测。而thermal图像中的用户体表的热签ature可以提供关键的“战斗或逃脱”压力反应信息，但凭借热成像alone来训练压力预测模型可能会导致过拟合，从而影响性能。这篇论文解决了这个挑战，通过引入ThermaStrain，一种新的合作学习框架，实现高精度压力预测表现。在训练过程中，ThermaStrain使用了一个穿着式电导活动（EDA）传感器，将热成像中的压力指示符转换为穿着式EDA传感器的压力指示符，以便在训练过程中增强模型的鲁棒性。在测试过程中，只使用热成像，从热成像和模拟EDA表示中提取压力指示符，以提高压力评估。研究采集了包括热成像和EDA数据在内的全面数据集，ThermaStrain在二分类压力预测中取得F1分数为0.8293，在热成像基eline方法上出performancedoor9%。广泛的评估表明ThermaStrain具有识别压力指示符的能力，适应不同距离和压力情况，实时执行在边缘平台上，适用于多个个体感知，在有限视力和不熟悉情况下可行，以及合作学习的优势。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Uncertainty-in-Multimodal-Foundation-Models-using-Public-Internet-Data"><a href="#Estimating-Uncertainty-in-Multimodal-Foundation-Models-using-Public-Internet-Data" class="headerlink" title="Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data"></a>Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09926">http://arxiv.org/abs/2310.09926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlaaLab/WebCP">https://github.com/AlaaLab/WebCP</a></li>
<li>paper_authors: Shiladitya Dutta, Hongbo Wei, Lars van der Laan, Ahmed M. Alaa</li>
<li>for: 这种论文是为了解决零shot预测中的不确定性问题。</li>
<li>methods: 该论文使用了自我超vised学习，并在测试时使用CLIP样式模型进行零shot分类。它还使用了一种新的协Forms score来衡量预测的可靠性。</li>
<li>results: 研究人员通过使用web数据进行 calibration，实现了针对各种生物医学数据集的零shot预测。他们的初步结果表明，通过在测试时使用网络上的calibration数据，可以实现预测的目标覆盖率，并且效率相对较高。<details>
<summary>Abstract</summary>
Foundation models are trained on vast amounts of data at scale using self-supervised learning, enabling adaptation to a wide range of downstream tasks. At test time, these models exhibit zero-shot capabilities through which they can classify previously unseen (user-specified) categories. In this paper, we address the problem of quantifying uncertainty in these zero-shot predictions. We propose a heuristic approach for uncertainty estimation in zero-shot settings using conformal prediction with web data. Given a set of classes at test time, we conduct zero-shot classification with CLIP-style models using a prompt template, e.g., "an image of a <category>", and use the same template as a search query to source calibration data from the open web. Given a web-based calibration set, we apply conformal prediction with a novel conformity score that accounts for potential errors in retrieved web data. We evaluate the utility of our proposed method in Biomedical foundation models; our preliminary results show that web-based conformal prediction sets achieve the target coverage with satisfactory efficiency on a variety of biomedical datasets.
</details>
<details>
<summary>摘要</summary>
基础模型通过大规模数据的自我超vision学习训练，可以适应各种下游任务。在测试时，这些模型可以通过零批预测来分类之前未看到的类别。在这篇论文中，我们解决了零批预测中的uncertainty量化问题。我们提出了一种启发式方法，使用web数据来实现零批预测中的uncertainty估计。给定一组测试时的类别，我们使用CLIP样式的模型进行零批分类，使用提示模板，例如“一张<类别>的图像”，并使用相同的模板作为搜索关键词来源网络数据。给定一个网络基础的核心集，我们应用彩色预测技术，使用一种新的彩色度分数，考虑可能存在的网络数据错误。我们对生物基础模型进行了初步的实验结果，表明在各种生物数据集上，网络基础的彩色预测集可以达到目标覆盖率，并且具有满意的效率。
</details></li>
</ul>
<hr>
<h2 id="Homophone-Disambiguation-Reveals-Patterns-of-Context-Mixing-in-Speech-Transformers"><a href="#Homophone-Disambiguation-Reveals-Patterns-of-Context-Mixing-in-Speech-Transformers" class="headerlink" title="Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers"></a>Homophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09925">http://arxiv.org/abs/2310.09925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hmohebbi/ContextMixingASR">https://github.com/hmohebbi/ContextMixingASR</a></li>
<li>paper_authors: Hosein Mohebbi, Grzegorz Chrupała, Willem Zuidema, Afra Alishahi</li>
<li>for: This paper aims to investigate how measures of ‘context-mixing’ developed for text models can be adapted and applied to models of spoken language, specifically in the case of homophony in French.</li>
<li>methods: The authors use a series of controlled experiments and probing analyses on Transformer-based speech models to explore how representations in encoder-only models and encoder-decoder models incorporate syntactic cues to identify the correct transcription.</li>
<li>results: The authors find that representations in encoder-only models effectively incorporate these cues, while encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules.<details>
<summary>Abstract</summary>
Transformers have become a key architecture in speech processing, but our understanding of how they build up representations of acoustic and linguistic structure is limited. In this study, we address this gap by investigating how measures of 'context-mixing' developed for text models can be adapted and applied to models of spoken language. We identify a linguistic phenomenon that is ideal for such a case study: homophony in French (e.g. livre vs livres), where a speech recognition model has to attend to syntactic cues such as determiners and pronouns in order to disambiguate spoken words with identical pronunciations and transcribe them while respecting grammatical agreement. We perform a series of controlled experiments and probing analyses on Transformer-based speech models. Our findings reveal that representations in encoder-only models effectively incorporate these cues to identify the correct transcription, whereas encoders in encoder-decoder models mainly relegate the task of capturing contextual dependencies to decoder modules.
</details>
<details>
<summary>摘要</summary>
听说模型已成为语音处理中关键的建筑，但我们对它们如何建立语音和文本结构的表示还是有限的。在这项研究中，我们尝试将文本模型中的'上下文混合'度量应用到语音模型中，以更好地理解它们如何建立表示。我们选择了一种语言现象，即法语中的同音异义（例如，"livre" vs "livres"），这种现象需要语音识别模型通过 determiners 和 Pronouns 等语法提示来纠正 spoken 词的意思，并且将其转录为句子中的正确形式。我们进行了一系列控制的实验和探索分析，发现encoder-only模型中的表示能够有效地捕捉这些语法提示，而encoder-decoder模型中的encoder模块主要通过decoder模块来捕捉上下文关系。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Maintenance-Model-Based-on-Anomaly-Detection-in-Induction-Motors-A-Machine-Learning-Approach-Using-Real-Time-IoT-Data"><a href="#Predictive-Maintenance-Model-Based-on-Anomaly-Detection-in-Induction-Motors-A-Machine-Learning-Approach-Using-Real-Time-IoT-Data" class="headerlink" title="Predictive Maintenance Model Based on Anomaly Detection in Induction Motors: A Machine Learning Approach Using Real-Time IoT Data"></a>Predictive Maintenance Model Based on Anomaly Detection in Induction Motors: A Machine Learning Approach Using Real-Time IoT Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14949">http://arxiv.org/abs/2310.14949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergio F. Chevtchenko, Monalisa C. M. dos Santos, Diego M. Vieira, Ricardo L. Mota, Elisson Rocha, Bruna V. Cruz, Danilo Araújo, Ermeson Andrade</li>
<li>for: 本研究旨在透过互联网路物 (IoT) 设备收集腐败现象数据，并运用数据驱动模型进行异常检测在工业设备中。</li>
<li>methods: 本研究使用了一组融合预处理技术和机器学习 (ML) 模型，包括快速傅立叶 transform (FFT)、波лет трансформа (WT) 和分割，以提取数据的特征。 本研究还使用多目标优化和分析以保证异常检测率、假阳性率和推论速率之间的最佳平衡。</li>
<li>results: 本研究获得了一系列的实验结果，证明了融合预处理技术和 ML 模型可以实现高精度异常检测，并且可以在不同的工业上适用。<details>
<summary>Abstract</summary>
With the support of Internet of Things (IoT) devices, it is possible to acquire data from degradation phenomena and design data-driven models to perform anomaly detection in industrial equipment. This approach not only identifies potential anomalies but can also serve as a first step toward building predictive maintenance policies. In this work, we demonstrate a novel anomaly detection system on induction motors used in pumps, compressors, fans, and other industrial machines. This work evaluates a combination of pre-processing techniques and machine learning (ML) models with a low computational cost. We use a combination of pre-processing techniques such as Fast Fourier Transform (FFT), Wavelet Transform (WT), and binning, which are well-known approaches for extracting features from raw data. We also aim to guarantee an optimal balance between multiple conflicting parameters, such as anomaly detection rate, false positive rate, and inference speed of the solution. To this end, multiobjective optimization and analysis are performed on the evaluated models. Pareto-optimal solutions are presented to select which models have the best results regarding classification metrics and computational effort. Differently from most works in this field that use publicly available datasets to validate their models, we propose an end-to-end solution combining low-cost and readily available IoT sensors. The approach is validated by acquiring a custom dataset from induction motors. Also, we fuse vibration, temperature, and noise data from these sensors as the input to the proposed ML model. Therefore, we aim to propose a methodology general enough to be applied in different industrial contexts in the future.
</details>
<details>
<summary>摘要</summary>
“利用互联网络器件（IoT），可以从损坏现象中获取数据，设计数据驱动的模型以进行异常检测在工业设备中。这种方法不仅可以检测出可能的异常，而且可以作为建立预测维护政策的第一步。在这个工作中，我们展示了一个新的异常检测系统，应用于对发电机（induction motor）进行验证。这个工作使用了一组合的预处理技术，包括快速傅立叶变换（FFT）、wavelet变换（WT）和分割，这些技术都是抽象数据的常用方法。我们还希望确保多项衡量的依势关系，例如异常检测率、伪阳性率和推理速度，得到一个优化的解。为达到这个目的，我们进行多目标优化和分析。得到的 pareto 最佳解可以选择最佳的模型，以及评估这些模型的数据驱动和计算成本。不同于大多数在这个领域中使用公开available的数据集来验证他们的模型，我们提出了一个终端解决方案， combining 低成本和易于入手的 IoT 感应器。我们将这种方法应用于不同的工业上下，以提高维护效率和降低成本。”
</details></li>
</ul>
<hr>
<h2 id="Lifelong-Sequence-Generation-with-Dynamic-Module-Expansion-and-Adaptation"><a href="#Lifelong-Sequence-Generation-with-Dynamic-Module-Expansion-and-Adaptation" class="headerlink" title="Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation"></a>Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09886">http://arxiv.org/abs/2310.09886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengwei Qin, Chen Chen, Shafiq Joty</li>
<li>for: 解决 continual learning 中的 Life-long Sequence Generation (LSG) 问题，即在不断训练模型的同时，总结出来的新生成模式，而不是忘记之前的知识。</li>
<li>methods: 我们提出了 Dynamic Module Expansion and Adaptation (DMEA) 方法，即在任务相似性的基础上动态决定模型需要的架构，并选择最相似的先前任务来促进新任务的适应性。同时，我们还提出了动态梯度缩放，以保持当前任务和先前任务的学习平衡。</li>
<li>results: 通过广泛的实验，我们示出了 DMEA 可以在不同的 LSG 设定下表现出色，常常超越现有的方法。<details>
<summary>Abstract</summary>
Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the learning of the current task and replayed tasks. With extensive experiments, we demonstrate that DMEA can consistently outperform existing methods in different LSG settings.
</details>
<details>
<summary>摘要</summary>
这是一个生命长序列生成（LSG）问题，它是一种持续学习的问题，旨在不断训练一个模型，以学习不断出现的新生成模式，而且避免遗传知识的忘记。现有的LSG方法主要是维护古代知识，对任务之间的知识传递甚少关注。然而，人类在学习新任务时，可以更好地利用先前所获得的知识，以便更好地适应新任务。受人类学习模式启发，我们提出了动态模组扩展和适应（DMEA）方法，让模型在任务相似度和先前任务之间进行动态决定模组架构，并选择最相似的先前任务来促进新任务的适应。此外，当学习过程可能会偏向现在任务，导致更严重的知识忘记，我们提出了动态GradientScaling来均衡现在任务和重复任务的学习。经过广泛的实验，我们证明了DMEA可以在不同的LSG设定中具有优秀的表现。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-with-Iterative-Demonstration-Selection"><a href="#In-Context-Learning-with-Iterative-Demonstration-Selection" class="headerlink" title="In-Context Learning with Iterative Demonstration Selection"></a>In-Context Learning with Iterative Demonstration Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09881">http://arxiv.org/abs/2310.09881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengwei Qin, Aston Zhang, Anirudh Dagar, Wenming Ye</li>
<li>for: 提高大语言模型（LLM）在几个示例下学习中的表现。</li>
<li>methods: Iterative Demonstration Selection（IDS）方法，使用零shot chain-of-thoughtreasoning（Zero-shot-CoT）选择示例，并在多个迭代中选择最佳示例。</li>
<li>results: 在多个任务上，包括通用理解、问答、话题分类和情感分析，IDS方法可以一直 exceed 现有的ICL示例选择方法。<details>
<summary>Abstract</summary>
Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is then used to choose demonstrations that are prepended to the test sample for inference. The generated answer is accompanied by its corresponding reasoning path for extracting a new set of demonstrations in the next iteration. After several iterations, IDS adopts majority voting to obtain the final result. Through extensive experiments on tasks including commonsense reasoning, question answering, topic classification, and sentiment analysis, we demonstrate that IDS can consistently outperform existing ICL demonstration selection methods.
</details>
<details>
<summary>摘要</summary>
促进了规模的进步，大语言模型（LLM）在内容学习（ICL）中表现出了强大的几个示例学习能力。然而，ICL表现的选择示例仍然是一个持续的挑战和开放问题。现有的文献强调选择测试样本中的多样化或semantic相似的示例，而忽略了任务特定的最佳选择维度。基于这两个维度的优点，我们提出了迭代示例选择（IDS）。IDS使用零实例链条思维（Zero-shot-CoT）来选择示例，其中逻辑路径是在测试样本之前应用于测试样本。然后，选择的示例将被附加到测试样本中进行INF的推理。生成的答案将被 accompanied by its corresponding reasoning path，以提取新的示例集。经过多轮迭代，IDS采用多数投票方式获得最终结果。我们通过对常识推理、问答、话题分类和情感分析等任务进行广泛的实验，证明IDS可以一直性能高于现有的ICL示例选择方法。
</details></li>
</ul>
<hr>
<h2 id="Statistical-inference-using-machine-learning-and-classical-techniques-based-on-accumulated-local-effects-ALE"><a href="#Statistical-inference-using-machine-learning-and-classical-techniques-based-on-accumulated-local-effects-ALE" class="headerlink" title="Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)"></a>Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09877">http://arxiv.org/abs/2310.09877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chitu Okoli</li>
<li>for: 这篇论文主要是为了提出一种model-agnostic的方法来进行黑盒机器学习（ML）算法的全面解释。</li>
<li>methods: 这篇论文使用了ALE（Accumulated Local Effects）模型无关的方法来进行解释，并提出了一些新的统计推断方法来解决小样本大小的问题，以及在ML数据分析中对变量的总效果的INTRODUCTION。</li>
<li>results: 这篇论文提出了一些实用的解决方案，包括在ALE分析中确保可靠性，以及在ML数据分析中对变量的总效果进行INTRODUCTION。这些解决方案可以帮助更好地进行ML数据分析和统计推断。<details>
<summary>Abstract</summary>
Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicability in ML and statistical analysis forward, offering practical solutions to prevailing challenges in the field.
</details>
<details>
<summary>摘要</summary>
集成本地效应（ALE）是一种模型不依赖的方法，用于全面解释黑盒机器学习（ML）算法的结果。在进行统计推断基于ALE时，存在至少三个挑战：确保ALE分析的可靠性，特别是在小数据集中；Intuitively characterize a variable's overall effect in ML;和从ML数据分析中获得可靠的推断。为此，我们介绍了新的工具和技术，用于基于ALE的统计推断，包括适应 dataset 大小的 bootstrap 信任区间和 ALE 效果大小度量，这些度量可以直观地反映变量对结果变量的影响和Normalized 比例。此外，我们示例了如何使用这些工具来提取可靠的统计推断，反映 ALE 灵活地高亮的各种模式，R 中的 'ale' 包提供了实现。这项工作推动了 ALE 在 ML 和统计分析领域的应用前进，提供了实用的解决方案，用于解决领域中的挑战。
</details></li>
</ul>
<hr>
<h2 id="Federated-Multi-Objective-Learning"><a href="#Federated-Multi-Objective-Learning" class="headerlink" title="Federated Multi-Objective Learning"></a>Federated Multi-Objective Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09866">http://arxiv.org/abs/2310.09866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zakaria-Dahi/Multi-Objective_Optimiser_For_Federated_Learning">https://github.com/Zakaria-Dahi/Multi-Objective_Optimiser_For_Federated_Learning</a></li>
<li>paper_authors: Haibo Yang, Zhuqing Liu, Jia Liu, Chaosheng Dong, Michinari Momma</li>
<li>for:  Multi-agent multi-task learning applications with distributed nature and data privacy needs.</li>
<li>methods: Federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private.</li>
<li>results: Proposed two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stochastic multi-gradient descent averaging (FSMGDA), which allow local updates to significantly reduce communication costs, while achieving the same convergence rates as those of their algorithmic counterparts in the single-objective federated learning.<details>
<summary>Abstract</summary>
In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stochastic multi-gradient descent averaging (FSMGDA). Both algorithms allow local updates to significantly reduce communication costs, while achieving the {\em same} convergence rates as those of their algorithmic counterparts in the single-objective federated learning. Our extensive experiments also corroborate the efficacy of our proposed FMOO algorithms.
</details>
<details>
<summary>摘要</summary>
To solve the FMOL problem, we propose two new federated multi-objective optimization (FMOO) algorithms, called federated multi-gradient descent averaging (FMGDA) and federated stochastic multi-gradient descent averaging (FSMGDA). Both algorithms allow for local updates to significantly reduce communication costs, while achieving the same convergence rates as their algorithmic counterparts in single-objective federated learning. Our extensive experiments also demonstrate the effectiveness of our proposed FMOO algorithms.
</details></li>
</ul>
<hr>
<h2 id="Federated-Reinforcement-Learning-for-Resource-Allocation-in-V2X-Networks"><a href="#Federated-Reinforcement-Learning-for-Resource-Allocation-in-V2X-Networks" class="headerlink" title="Federated Reinforcement Learning for Resource Allocation in V2X Networks"></a>Federated Reinforcement Learning for Resource Allocation in V2X Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09858">http://arxiv.org/abs/2310.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaidi Xu, Shenglong Zhou, Geoffrey Ye Li</li>
<li>for: 这个论文是用来研究车至所有东西（V2X）网络资源分配的最佳化方法。</li>
<li>methods: 这个论文使用联邦强化学习（FRL）框架，并使用不精确的方向分解方法（ADMM）来解决资源分配问题。</li>
<li>results: 这个论文的结果显示，使用PASM算法可以实现资源分配问题的最佳化，并且比一些基于估计的方法具有更好的数字性表现。<details>
<summary>Abstract</summary>
Resource allocation significantly impacts the performance of vehicle-to-everything (V2X) networks. Most existing algorithms for resource allocation are based on optimization or machine learning (e.g., reinforcement learning). In this paper, we explore resource allocation in a V2X network under the framework of federated reinforcement learning (FRL). On one hand, the usage of RL overcomes many challenges from the model-based optimization schemes. On the other hand, federated learning (FL) enables agents to deal with a number of practical issues, such as privacy, communication overhead, and exploration efficiency. The framework of FRL is then implemented by the inexact alternative direction method of multipliers (ADMM), where subproblems are solved approximately using policy gradients and accelerated by an adaptive step size calculated from their second moments. The developed algorithm, PASM, is proven to be convergent under mild conditions and has a nice numerical performance compared with some baseline methods for solving the resource allocation problem in a V2X network.
</details>
<details>
<summary>摘要</summary>
资源分配对于 vehicle-to-everything（V2X）网络的性能有着重要的影响。大多数现有的资源分配算法基于优化或机器学习（例如，强化学习）。在这篇论文中，我们explore V2X网络中的资源分配问题在 federated reinforcement learning（FRL）框架下。一方面，RL可以超越许多模型基于优化方案中的挑战。另一方面，联邦学习（FL）可以帮助代理人处理一些实际问题，如隐私、通信开销和探索效率。然后，FRL框架被实现通过不确定多члены方法（ADMM），其中子问题被解决approximately使用政策偏导和加速器是根据其第二次 moments。开发的算法，PASM，在一定的条件下被证明是收敛的，并与一些基准方法相比有良好的数值性能。
</details></li>
</ul>
<hr>
<h2 id="MERTech-Instrument-Playing-Technique-Detection-Using-Self-Supervised-Pretrained-Model-With-Multi-Task-Finetuning"><a href="#MERTech-Instrument-Playing-Technique-Detection-Using-Self-Supervised-Pretrained-Model-With-Multi-Task-Finetuning" class="headerlink" title="MERTech: Instrument Playing Technique Detection Using Self-Supervised Pretrained Model With Multi-Task Finetuning"></a>MERTech: Instrument Playing Technique Detection Using Self-Supervised Pretrained Model With Multi-Task Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09853">http://arxiv.org/abs/2310.09853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dichucheng Li, Yinghao Ma, Weixing Wei, Qiuqiang Kong, Yulun Wu, Mingjin Che, Fan Xia, Emmanouil Benetos, Wei Li</li>
<li>for: 本研究旨在提出一种自动检测乐器演奏技巧（IPT）的方法，以解决数据稀缺和类别不均匀问题。</li>
<li>methods: 该方法利用自动学习模型，先在大规模无标签音乐数据上进行自动学习，然后在IPT检测任务上练习 fine-tuning。此外，还 investigate了多任务融合finetuning，包括抑制和识别抑制的多个任务。</li>
<li>results: 该方法在多个IPT标准测试集上比过去的方法表现出色，在 Frame-level和事件-level度量中均显示出优异性。此外，多任务融合finetuning也能够提高每个IPT类别的准确率。<details>
<summary>Abstract</summary>
Instrument playing techniques (IPTs) constitute a pivotal component of musical expression. However, the development of automatic IPT detection methods suffers from limited labeled data and inherent class imbalance issues. In this paper, we propose to apply a self-supervised learning model pre-trained on large-scale unlabeled music data and finetune it on IPT detection tasks. This approach addresses data scarcity and class imbalance challenges. Recognizing the significance of pitch in capturing the nuances of IPTs and the importance of onset in locating IPT events, we investigate multi-task finetuning with pitch and onset detection as auxiliary tasks. Additionally, we apply a post-processing approach for event-level prediction, where an IPT activation initiates an event only if the onset output confirms an onset in that frame. Our method outperforms prior approaches in both frame-level and event-level metrics across multiple IPT benchmark datasets. Further experiments demonstrate the efficacy of multi-task finetuning on each IPT class.
</details>
<details>
<summary>摘要</summary>
To further enhance performance, we investigate multi-task finetuning with pitch and onset detection as auxiliary tasks. Pitch is essential for capturing the nuances of IPTs, while onset information is critical for locating IPT events. We also apply a post-processing approach for event-level prediction, where an IPT activation is only triggered if the onset output confirms an onset in that frame.Our method outperforms prior approaches in both frame-level and event-level metrics across multiple IPT benchmark datasets. Additionally, we demonstrate the effectiveness of multi-task finetuning on each IPT class. Our approach provides a significant improvement in IPT detection accuracy, addressing the challenges of limited labeled data and class imbalance issues.
</details></li>
</ul>
<hr>
<h2 id="ACES-Generating-Diverse-Programming-Puzzles-with-Autotelic-Language-Models-and-Semantic-Descriptors"><a href="#ACES-Generating-Diverse-Programming-Puzzles-with-Autotelic-Language-Models-and-Semantic-Descriptors" class="headerlink" title="ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors"></a>ACES: Generating Diverse Programming Puzzles with Autotelic Language Models and Semantic Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10692">http://arxiv.org/abs/2310.10692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julien Pourcel, Cédric Colas, Pierre-Yves Oudeyer, Laetitia Teodorescu</li>
<li>For: studying automated problem generation in the context of python programming puzzles, with a focus on interesting diversity optimization.* Methods: using semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation.* Results: discovering a richer diversity of puzzles than existing diversity-maximizing algorithms, as measured across a range of diversity metrics.<details>
<summary>Abstract</summary>
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to explore that abstract semantic space, slowly discovering a diversity of solvable programming puzzles in any given run. Across a set of experiments, we show that ACES discovers a richer diversity of puzzles than existing diversity-maximizing algorithms as measured across a range of diversity metrics. We further study whether and in which conditions this diversity can translate into the successful training of puzzle solving models.
</details>
<details>
<summary>摘要</summary>
寻找和选择新领域的问题是感知、科学和创新的核心。我们在python编程练习中的开放式空间中研究自动生成问题。现有的生成模型通常是模型参考分布而不是直接优化多样性。其他方法通过手动编码的表示空间或学习的嵌入空间来显式地优化多样性，但这些方法可能并不与人类的意义变化相匹配。我们在ACES（自动telic代码探索 via 语义描述符）中引入了一种新的自动telic生成方法，利用大语言模型生成的语义描述符直接优化有趣的多样性，以及几招学习。每个练习都被标记了10个维度，每个维度捕捉一个需要解决它的编程技能。ACES生成和追求新的可行目标，慢慢发现任务抽象 semantic空间中的多样性，在任务执行中逐渐发现可解决的编程练习。在一系列实验中，我们发现ACES在多样性度量上比现有的多样性最大化算法更加丰富。我们进一步研究是否和在哪些条件下，这种多样性可以导致练习解决模型的成功培训。
</details></li>
</ul>
<hr>
<h2 id="CoCoFormer-A-controllable-feature-rich-polyphonic-music-generation-method"><a href="#CoCoFormer-A-controllable-feature-rich-polyphonic-music-generation-method" class="headerlink" title="CoCoFormer: A controllable feature-rich polyphonic music generation method"></a>CoCoFormer: A controllable feature-rich polyphonic music generation method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09843">http://arxiv.org/abs/2310.09843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuyang Zhou, Tengfei Niu, Hong Zhu, Xingping Wang</li>
<li>for: 本研究探讨了多重音乐序列的模型化方法，尤其是使用 transformer 模型进行可控音乐生成。</li>
<li>methods: 本研究提出了 Condition Choir Transformer（CoCoFormer）模型，通过控制输出模型的逻辑和拍子输入来实现精细化控制。同时，通过自我超VI等方法进行验证和训练。</li>
<li>results: 实验表明，CoCoFormer 模型在指定多重音乐Texture时，可以生成多种不同的同一首歌曲，并且达到了当前最佳水平。<details>
<summary>Abstract</summary>
This paper explores the modeling method of polyphonic music sequence. Due to the great potential of Transformer models in music generation, controllable music generation is receiving more attention. In the task of polyphonic music, current controllable generation research focuses on controlling the generation of chords, but lacks precise adjustment for the controllable generation of choral music textures. This paper proposed Condition Choir Transformer (CoCoFormer) which controls the output of the model by controlling the chord and rhythm inputs at a fine-grained level. In this paper, the self-supervised method improves the loss function and performs joint training through conditional control input and unconditional input training. In order to alleviate the lack of diversity on generated samples caused by the teacher forcing training, this paper added an adversarial training method. CoCoFormer enhances model performance with explicit and implicit inputs to chords and rhythms. In this paper, the experiments proves that CoCoFormer has reached the current better level than current models. On the premise of specifying the polyphonic music texture, the same melody can also be generated in a variety of ways.
</details>
<details>
<summary>摘要</summary>
The paper uses a self-supervised method to improve the loss function and performs joint training through conditional control input and unconditional input training. To alleviate the lack of diversity in generated samples caused by teacher forcing training, the paper adds an adversarial training method. CoCoFormer enhances model performance with explicit and implicit inputs to chords and rhythms.Experiments show that CoCoFormer has reached a current better level than current models. With the premise of specifying the polyphonic music texture, the same melody can also be generated in a variety of ways.Translation notes:* "polyphonic music sequence" is translated as "多重音乐序列" (polytrophic music sequence)* "Transformer models" is translated as "变换器模型" (transformer models)* "controllable music generation" is translated as "可控音乐生成" (controllable music generation)* "chord" is translated as "和声" (chord)* "rhythm" is translated as "拍" (rhythm)* "self-supervised method" is translated as "自我指导方法" (self-supervised method)* "adversarial training method" is translated as "对抗训练方法" (adversarial training method)* "CoCoFormer" is translated as "CoCoFormer" (CoCoFormer)* "polyphonic music texture" is translated as "多重音乐Texture" (polyphonic music texture)* "melody" is translated as "旋律" (melody)
</details></li>
</ul>
<hr>
<h2 id="Explaining-How-a-Neural-Network-Play-the-Go-Game-and-Let-People-Learn"><a href="#Explaining-How-a-Neural-Network-Play-the-Go-Game-and-Let-People-Learn" class="headerlink" title="Explaining How a Neural Network Play the Go Game and Let People Learn"></a>Explaining How a Neural Network Play the Go Game and Let People Learn</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09838">http://arxiv.org/abs/2310.09838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huilin Zhou, Huijie Tang, Mingjie Li, Hao Zhang, Zhenyu Liu, Quanshi Zhang</li>
<li>for: 本研究的目的是解释Go游戏中AI模型所编码的知识，并使用这些知识来教育人类玩家。</li>
<li>methods: 本研究使用了Value网络来提取Go游戏中石头之间的交互 primitives，以便人类可以从Value网络中学习准确和可靠的知识。</li>
<li>results: 实验表明，我们的方法可以有效地提取Go游戏中AI模型所编码的知识，并帮助人类玩家更好地理解和掌握Go游戏。<details>
<summary>Abstract</summary>
The AI model has surpassed human players in the game of Go, and it is widely believed that the AI model has encoded new knowledge about the Go game beyond human players. In this way, explaining the knowledge encoded by the AI model and using it to teach human players represent a promising-yet-challenging issue in explainable AI. To this end, mathematical supports are required to ensure that human players can learn accurate and verifiable knowledge, rather than specious intuitive analysis. Thus, in this paper, we extract interaction primitives between stones encoded by the value network for the Go game, so as to enable people to learn from the value network. Experiments show the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
人工智能模型已经在围棋游戏中超越人类玩家，而且广泛认为该模型已经编码了人类玩家之外的新知识。因此，解释AI模型所编码的知识并使用其教育人类玩家是一项有前途又挑战的问题。为此，我们需要有数学支持，以确保人类玩家可以学习准确和可靠的知识，而不是基于假设的直觉分析。在本文中，我们提取了围棋中石头之间的互动基本原理，以便让人类玩家从值网络中学习。实验表明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="MIR2-Towards-Provably-Robust-Multi-Agent-Reinforcement-Learning-by-Mutual-Information-Regularization"><a href="#MIR2-Towards-Provably-Robust-Multi-Agent-Reinforcement-Learning-by-Mutual-Information-Regularization" class="headerlink" title="MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization"></a>MIR2: Towards Provably Robust Multi-Agent Reinforcement Learning by Mutual Information Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09833">http://arxiv.org/abs/2310.09833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simin Li, Ruixiao Xu, Jun Guo, Pu Feng, Jiakai Wang, Aishan Liu, Yaodong Yang, Xianglong Liu, Weifeng Lv</li>
<li>for: 这篇论文的目的是提出一种robust多代理学习（MARL）方法，以增强对不确定或最坏情况的抗性。</li>
<li>methods: 该方法使用policy学习在 Routine Scenarios 中训练，并使用Mutual Information as Robust Regularization来避免过度优化。</li>
<li>results: 对于StarCraft II、Multi-agent Mujoco和 rendezvous 等场景，MIR2方法显示了更高的抗性性能，并且在实际应用中的 robot 群集控制场景中也表现出了优异性能。<details>
<summary>Abstract</summary>
Robust multi-agent reinforcement learning (MARL) necessitates resilience to uncertain or worst-case actions by unknown allies. Existing max-min optimization techniques in robust MARL seek to enhance resilience by training agents against worst-case adversaries, but this becomes intractable as the number of agents grows, leading to exponentially increasing worst-case scenarios. Attempts to simplify this complexity often yield overly pessimistic policies, inadequate robustness across scenarios and high computational demands. Unlike these approaches, humans naturally learn adaptive and resilient behaviors without the necessity of preparing for every conceivable worst-case scenario. Motivated by this, we propose MIR2, which trains policy in routine scenarios and minimize Mutual Information as Robust Regularization. Theoretically, we frame robustness as an inference problem and prove that minimizing mutual information between histories and actions implicitly maximizes a lower bound on robustness under certain assumptions. Further analysis reveals that our proposed approach prevents agents from overreacting to others through an information bottleneck and aligns the policy with a robust action prior. Empirically, our MIR2 displays even greater resilience against worst-case adversaries than max-min optimization in StarCraft II, Multi-agent Mujoco and rendezvous. Our superiority is consistent when deployed in challenging real-world robot swarm control scenario. See code and demo videos in Supplementary Materials.
</details>
<details>
<summary>摘要</summary>
多智能体强化学习（MARL）需要对不确定或最坏情况的行动具备抗性。现有的最大最小优化技术在Robust MARL中增强抗性，但随着智能体数量增加，最坏情况的数量将 exponentiation 增长，导致计算量过高。尝试简化这种复杂性通常会导致过度保守的策略，不足robustness across scenarios和高计算需求。与这些方法不同，人类自然地学习了适应和抗性行为，无需为每个可能的最坏情况做准备。 inspirited by this，我们提出了MIR2，它在 Routine scenarios 中训练策略，并将 Mutual Information 作为Robust Regularization 来最小化。从理论角度来看，我们将robustness 视为一个推理问题，并证明在某些假设下，将mutual information between histories and actions 最小化会implicitly 最大化一个下界于robustness的lower bound。进一步的分析表明，我们的提posed approach prevent agents from overreacting to others through an information bottleneck，并使策略与一个robust action prior 吻合。Empirically，我们的MIR2在StarCraft II, Multi-agent Mujoco和 rendezvous 中对最坏情况的抗性性能更高than max-min optimization。我们的superiority 在Real-world robot swarm control scenario 中也是一致的。参考代码和示例视频在Supplementary Materials中。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-In-Context-Student-Modeling-Synthesizing-Student’s-Behavior-in-Visual-Programming-from-One-Shot-Observation"><a href="#Large-Language-Models-for-In-Context-Student-Modeling-Synthesizing-Student’s-Behavior-in-Visual-Programming-from-One-Shot-Observation" class="headerlink" title="Large Language Models for In-Context Student Modeling: Synthesizing Student’s Behavior in Visual Programming from One-Shot Observation"></a>Large Language Models for In-Context Student Modeling: Synthesizing Student’s Behavior in Visual Programming from One-Shot Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10690">http://arxiv.org/abs/2310.10690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manh Hung Nguyen, Sebastian Tschiatschek, Adish Singla</li>
<li>for:  This paper is written for researchers and practitioners in the field of educational technology, particularly those interested in student modeling and personalized learning.</li>
<li>methods:  The paper explores the use of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. The proposed framework, LLM-SS, leverages LLMs to synthesize a student’s behavior based on their solving attempts on a reference task. The authors fine-tune LLMs using domain-specific expertise to improve their understanding of domain background and student behaviors.</li>
<li>results:  The paper reports significant improvements in student behavior synthesis compared to baseline methods included in the StudentSyn benchmark. Specifically, the method using the fine-tuned Llama2-70B model improves noticeably compared to using the base model and becomes on par with using the state-of-the-art GPT-4 model.<details>
<summary>Abstract</summary>
Student modeling is central to many educational technologies as it enables the prediction of future learning outcomes and targeted instructional strategies. However, open-ended learning environments pose challenges for accurately modeling students due to the diverse behaviors exhibited by students and the absence of a well-defined set of learning skills. To approach these challenges, we explore the application of Large Language Models (LLMs) for in-context student modeling in open-ended learning environments. We introduce a novel framework, LLM-SS, that leverages LLMs for synthesizing student's behavior. More concretely, given a particular student's solving attempt on a reference task as observation, the goal is to synthesize the student's attempt on a target task. Our framework can be combined with different LLMs; moreover, we fine-tune LLMs using domain-specific expertise to boost their understanding of domain background and student behaviors. We evaluate several concrete methods based on LLM-SS using the StudentSyn benchmark, an existing student's attempt synthesis benchmark in visual programming. Experimental results show a significant improvement compared to baseline methods included in the StudentSyn benchmark. Furthermore, our method using the fine-tuned Llama2-70B model improves noticeably compared to using the base model and becomes on par with using the state-of-the-art GPT-4 model.
</details>
<details>
<summary>摘要</summary>
We propose a novel framework, LLM-SS, which leverages LLMs to synthesize a student's behavior. Given a particular student's attempt at a reference task, the goal is to synthesize their attempt on a target task. Our framework can be combined with different LLMs, and we fine-tune these models using domain-specific expertise to improve their understanding of the domain background and student behaviors.We evaluate several concrete methods based on LLM-SS using the StudentSyn benchmark, an existing student attempt synthesis benchmark in visual programming. The results show a significant improvement compared to baseline methods included in the StudentSyn benchmark. Additionally, our method using the fine-tuned Llama2-70B model improves noticeably compared to using the base model and is on par with using the state-of-the-art GPT-4 model.
</details></li>
</ul>
<hr>
<h2 id="Optimizing-K-means-for-Big-Data-A-Comparative-Study"><a href="#Optimizing-K-means-for-Big-Data-A-Comparative-Study" class="headerlink" title="Optimizing K-means for Big Data: A Comparative Study"></a>Optimizing K-means for Big Data: A Comparative Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09819">http://arxiv.org/abs/2310.09819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravil Mussabayev, Rustam Mussabayev</li>
<li>for: 这篇论文旨在比较不同优化技术对K-means算法的应用在大数据场景中的影响。</li>
<li>methods: 论文描述了不同的优化技术，包括并行、简化、采样等方法，以解决K-means算法在大数据场景中的缺乏扩展性问题。</li>
<li>results: 作者通过对各种标准数据集进行比较，发现不同的技术在不同的数据集上的表现不同，并提供了关于速度和准确性之间的负担平衡的理解。<details>
<summary>Abstract</summary>
This paper presents a comparative analysis of different optimization techniques for the K-means algorithm in the context of big data. K-means is a widely used clustering algorithm, but it can suffer from scalability issues when dealing with large datasets. The paper explores different approaches to overcome these issues, including parallelization, approximation, and sampling methods. The authors evaluate the performance of these techniques on various benchmark datasets and compare them in terms of speed, quality of clustering, and scalability according to the LIMA dominance criterion. The results show that different techniques are more suitable for different types of datasets and provide insights into the trade-offs between speed and accuracy in K-means clustering for big data. Overall, the paper offers a comprehensive guide for practitioners and researchers on how to optimize K-means for big data applications.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文提出了对K-means算法的不同优化技术进行比较分析，以帮助在大数据场景下使用K-means算法。K-means算法广泛使用，但是它在处理大数据时可能会遇到扩展性问题。论文探讨了不同的方法来解决这些问题，包括并行、 aproximation 和采样方法。作者对这些技术在不同的测试数据集上进行评估，并根据LIMA主导因素来比较它们的速度、归一化质量和可扩展性。结果显示不同的技术适用于不同的数据类型，并提供了关于速度和准确性在K-means归一化中的贸易OFF的深入理解。总之，这篇论文为实践者和研究人员提供了一份全面的指南，以帮助他们在大数据应用中优化K-means算法。
</details></li>
</ul>
<hr>
<h2 id="Negative-Sampling-with-Adaptive-Denoising-Mixup-for-Knowledge-Graph-Embedding"><a href="#Negative-Sampling-with-Adaptive-Denoising-Mixup-for-Knowledge-Graph-Embedding" class="headerlink" title="Negative Sampling with Adaptive Denoising Mixup for Knowledge Graph Embedding"></a>Negative Sampling with Adaptive Denoising Mixup for Knowledge Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09781">http://arxiv.org/abs/2310.09781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DeMix2023/Demix">https://github.com/DeMix2023/Demix</a></li>
<li>paper_authors: Xiangnan Chen, Wen Zhang, Zhen Yao, Mingyang Chen, Siliang Tang</li>
<li>for: 本研究旨在提高知识图（KG）中entity和relation embedding的质量，通过减少负样本中的噪声。</li>
<li>methods: 提议使用一种混合策略，通过自我supervised的方式来更新负样本，从而提高KGE的训练效果。</li>
<li>results: 实验结果表明，提议的DeMix方法可以更好地减少负样本中的噪声，使KGE更快地训练到更好的链接预测结果。<details>
<summary>Abstract</summary>
Knowledge graph embedding (KGE) aims to map entities and relations of a knowledge graph (KG) into a low-dimensional and dense vector space via contrasting the positive and negative triples. In the training process of KGEs, negative sampling is essential to find high-quality negative triples since KGs only contain positive triples. Most existing negative sampling methods assume that non-existent triples with high scores are high-quality negative triples. However, negative triples sampled by these methods are likely to contain noise. Specifically, they ignore that non-existent triples with high scores might also be true facts due to the incompleteness of KGs, which are usually called false negative triples. To alleviate the above issue, we propose an easily pluggable denoising mixup method called DeMix, which generates high-quality triples by refining sampled negative triples in a self-supervised manner. Given a sampled unlabeled triple, DeMix firstly classifies it into a marginal pseudo-negative triple or a negative triple based on the judgment of the KGE model itself. Secondly, it selects an appropriate mixup partner for the current triple to synthesize a partially positive or a harder negative triple. Experimental results on the knowledge graph completion task show that the proposed DeMix is superior to other negative sampling techniques, ensuring corresponding KGEs a faster convergence and better link prediction results.
</details>
<details>
<summary>摘要</summary>
知识图embedding（KGE）目的是将知识图（KG）中的实体和关系映射到一个低维度和紧凑的向量空间，通过对正确和错误 triplets进行对比。在KGE训练过程中，负样本是关键的，因为KG只包含正确的 triplets。现有的负样本方法假设高分负样本是高质量的负样本，但这些负样本可能含有噪声。Specifically, these methods ignore the fact that high-scoring non-existent triplets may be true facts due to the incompleteness of KGs, which are called false negative triplets. To address this issue, we propose an easily pluggable denoising mixup method called DeMix, which generates high-quality triples by refining sampled negative triples in a self-supervised manner. Given a sampled unlabeled triple, DeMix first classifies it into a marginal pseudo-negative triple or a negative triple based on the judgment of the KGE model itself. Secondly, it selects an appropriate mixup partner for the current triple to synthesize a partially positive or a harder negative triple. Experimental results on the knowledge graph completion task show that the proposed DeMix is superior to other negative sampling techniques, ensuring corresponding KGEs a faster convergence and better link prediction results.
</details></li>
</ul>
<hr>
<h2 id="Notes-on-Applicability-of-Explainable-AI-Methods-to-Machine-Learning-Models-Using-Features-Extracted-by-Persistent-Homology"><a href="#Notes-on-Applicability-of-Explainable-AI-Methods-to-Machine-Learning-Models-Using-Features-Extracted-by-Persistent-Homology" class="headerlink" title="Notes on Applicability of Explainable AI Methods to Machine Learning Models Using Features Extracted by Persistent Homology"></a>Notes on Applicability of Explainable AI Methods to Machine Learning Models Using Features Extracted by Persistent Homology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09780">http://arxiv.org/abs/2310.09780</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naofumihama/xai_ph_ml">https://github.com/naofumihama/xai_ph_ml</a></li>
<li>paper_authors: Naofumi Hama</li>
<li>For: The paper explores the potential application of explainable AI methodologies to the persistent homology (PH)-machine learning (ML) pipeline for predicting gas adsorption in metal-organic frameworks.* Methods: The paper uses the PH-ML pipeline to extract features from topological data analysis and applies explainable AI methodologies to improve the interpretability of the results.* Results: The paper demonstrates suggestive results for predicting gas adsorption in metal-organic frameworks using the PH-ML pipeline with explainable AI methodologies. The codes to reproduce the results are available on GitHub.Here is the same information in Simplified Chinese text:* For: 本文探讨PH-ML管线在预测金属组织材料中气吸附过程中的可读性。* Methods: 本文使用PH-ML管线提取特征，并应用可读性AI方法来提高结果的解释性。* Results: 本文提出了预测金属组织材料中气吸附过程中的可读性结果，并提供了在GitHub上可重现的代码。<details>
<summary>Abstract</summary>
Data analysis that uses the output of topological data analysis as input for machine learning algorithms has been the subject of extensive research. This approach offers a means of capturing the global structure of data. Persistent homology (PH), a common methodology within the field of TDA, has found wide-ranging applications in machine learning. One of the key reasons for the success of the PH-ML pipeline lies in the deterministic nature of feature extraction conducted through PH. The ability to achieve satisfactory levels of accuracy with relatively simple downstream machine learning models, when processing these extracted features, underlines the pipeline's superior interpretability. However, it must be noted that this interpretation has encountered issues. Specifically, it fails to accurately reflect the feasible parameter region in the data generation process, and the physical or chemical constraints that restrict this process. Against this backdrop, we explore the potential application of explainable AI methodologies to this PH-ML pipeline. We apply this approach to the specific problem of predicting gas adsorption in metal-organic frameworks and demonstrate that it can yield suggestive results. The codes to reproduce our results are available at https://github.com/naofumihama/xai_ph_ml
</details>
<details>
<summary>摘要</summary>
研究使用 topological data analysis（TDA）的输出作为机器学习算法的输入的数据分析方法已经得到了广泛的研究。这种方法可以捕捉数据的全局结构。 persistent homology（PH）是TDA领域中常用的方法ологи，在机器学习领域也有广泛的应用。PH-ML管道的成功一个关键原因在于PH的干扰特征，这使得可以使用简单的下游机器学习模型达到高度的准确性。然而，这种解释存在一些问题，它无法准确地反映数据生成过程中可行的参数范围和物理或化学约束。为了解决这些问题，我们研究了使用可解释AI方法ologies来解释PH-ML管道。我们在预测金属组分材料中的气体吸附问题中应用了这种方法，并证明了它可以提供有价值的结果。codes可以在https://github.com/naofumihama/xai_ph_ml中找到。
</details></li>
</ul>
<hr>
<h2 id="Worst-Case-Analysis-is-Maximum-A-Posteriori-Estimation"><a href="#Worst-Case-Analysis-is-Maximum-A-Posteriori-Estimation" class="headerlink" title="Worst-Case Analysis is Maximum-A-Posteriori Estimation"></a>Worst-Case Analysis is Maximum-A-Posteriori Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09774">http://arxiv.org/abs/2310.09774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjun Wu, Di Wang</li>
<li>for: 这种软件工程任务中的性能优化和算法复杂性找出缺陷。</li>
<li>methods: 使用一种通用、适应和有 garantía的随机探测框架，称为DSE-SMC，来估计最坏情况的资源使用。</li>
<li>results: 对 Java 应用程序进行实验评估，得到了 DSE-SMC 比现有黑盒随机探测方法更有效。<details>
<summary>Abstract</summary>
The worst-case resource usage of a program can provide useful information for many software-engineering tasks, such as performance optimization and algorithmic-complexity-vulnerability discovery. This paper presents a generic, adaptive, and sound fuzzing framework, called DSE-SMC, for estimating worst-case resource usage. DSE-SMC is generic because it is black-box as long as the user provides an interface for retrieving resource-usage information on a given input; adaptive because it automatically balances between exploration and exploitation of candidate inputs; and sound because it is guaranteed to converge to the true resource-usage distribution of the analyzed program.   DSE-SMC is built upon a key observation: resource accumulation in a program is isomorphic to the soft-conditioning mechanism in Bayesian probabilistic programming; thus, worst-case resource analysis is isomorphic to the maximum-a-posteriori-estimation problem of Bayesian statistics. DSE-SMC incorporates sequential Monte Carlo (SMC) -- a generic framework for Bayesian inference -- with adaptive evolutionary fuzzing algorithms, in a sound manner, i.e., DSE-SMC asymptotically converges to the posterior distribution induced by resource-usage behavior of the analyzed program. Experimental evaluation on Java applications demonstrates that DSE-SMC is significantly more effective than existing black-box fuzzing methods for worst-case analysis.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "The worst-case resource usage of a program can provide useful information for many software-engineering tasks, such as performance optimization and algorithmic-complexity-vulnerability discovery. This paper presents a generic, adaptive, and sound fuzzing framework, called DSE-SMC, for estimating worst-case resource usage. DSE-SMC is generic because it is black-box as long as the user provides an interface for retrieving resource-usage information on a given input; adaptive because it automatically balances between exploration and exploitation of candidate inputs; and sound because it is guaranteed to converge to the true resource-usage distribution of the analyzed program.   DSE-SMC is built upon a key observation: resource accumulation in a program is isomorphic to the soft-conditioning mechanism in Bayesian probabilistic programming; thus, worst-case resource analysis is isomorphic to the maximum-a-posteriori-estimation problem of Bayesian statistics. DSE-SMC incorporates sequential Monte Carlo (SMC) -- a generic framework for Bayesian inference -- with adaptive evolutionary fuzzing algorithms, in a sound manner, i.e., DSE-SMC asymptotically converges to the posterior distribution induced by resource-usage behavior of the analyzed program. Experimental evaluation on Java applications demonstrates that DSE-SMC is significantly more effective than existing black-box fuzzing methods for worst-case analysis."into Simplified Chinese:<<SYS>>将程序的最差情况资源使用情况提供有用信息，用于软件工程各种任务，如性能优化和漏极性漏极性检测。本文介绍了一种通用、适应、有Sound的异步爬虫框架，称为DSE-SMC，用于估计最差情况资源使用。DSE-SMC是通用的，因为它可以透过输入的接口获取资源使用信息; 适应的，因为它会自动考虑探索和利用候选输入; 和有Sound的，因为它可以保证对分析程序的资源使用行为进行正确的拟合。 DSE-SMC基于资源寄生在程序中的软件条件机制，因此最差情况资源分析与 bayesian probabilistic programming 中的最大 posterior estimation 归一化。 DSE-SMC通过将 Bayesian 推理框架sequential Monte Carlo (SMC) 与适应演化爬虫算法相结合，实现了一种有Sound的方法。实验结果表明，DSE-SMC在 Java 应用程序上比现有的黑盒爬虫方法更有效。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Survey-on-Fairness-Benefits-of-XAI"><a href="#A-Critical-Survey-on-Fairness-Benefits-of-XAI" class="headerlink" title="A Critical Survey on Fairness Benefits of XAI"></a>A Critical Survey on Fairness Benefits of XAI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13007">http://arxiv.org/abs/2310.13007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Deck, Jakob Schoeffer, Maria De-Arteaga, Niklas Kühl</li>
<li>for: 这些研究旨在探讨可解释人工智能（XAI）与公平性之间的关系，并寻找XAI如何实现公平性的方法。</li>
<li>methods: 这些研究使用系统性的文献复查和后续的质量分析，找到了175篇关于XAI是如何提供公平性的纷争性的论文。</li>
<li>results: 研究发现了7种典型的声索，即XAI可以帮助实现多种公平性标准。但是，研究还发现了这些声索的一些重要的限制和困难。<details>
<summary>Abstract</summary>
In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. While the literature often suggests XAI to be an enabler for several fairness desiderata, we notice a misalignment between these desiderata and the capabilities of XAI. We encourage to conceive XAI as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of XAI method enables whom to address which fairness desideratum.
</details>
<details>
<summary>摘要</summary>
在这份重要的调查中，我们分析了通用Explainable AI（XAI）和公平之间的关系，以彻底分离这两个概念之间的多维关系。通过系统性文献综述和 subsequential 资料分析，我们确定了175篇文章中对XAI的公平 benefittest的七种典型声明。我们提出了关于这些声明的重要警告和限制，并为将来关于XAI在特定公平要求上的潜在优势和局限性的讨论提供入口点。尽管文献 часто表明XAI是许多公平要求的激活器，但我们注意到了XAI的能力与这些要求的不一致。我们建议视XAI为一种用于多维、社技挑战的算法公平的工具，并更 preciselly 说明XAI方法可以为谁 Address 哪些公平要求。
</details></li>
</ul>
<hr>
<h2 id="VLIS-Unimodal-Language-Models-Guide-Multimodal-Language-Generation"><a href="#VLIS-Unimodal-Language-Models-Guide-Multimodal-Language-Generation" class="headerlink" title="VLIS: Unimodal Language Models Guide Multimodal Language Generation"></a>VLIS: Unimodal Language Models Guide Multimodal Language Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09767">http://arxiv.org/abs/2310.09767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiwanchung/vlis">https://github.com/jiwanchung/vlis</a></li>
<li>paper_authors: Jiwan Chung, Youngjae Yu</li>
<li>for: 提高多Modal语言生成的复杂语言理解能力</li>
<li>methods:  combinesthe visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training</li>
<li>results: 在多种任务上（包括CommonSense理解、复杂文本生成等），VLIS可以提高视觉语言模型的性能<details>
<summary>Abstract</summary>
Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves vision-language models on diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation.
</details>
<details>
<summary>摘要</summary>
多模态语言生成，利用语言和视觉之间的共同作用，是一个快速发展的领域。然而，现有的视觉语言模型在需要复杂的语言理解任务时会遇到挑战。为解决这个问题，我们介绍了视觉语言模型作为重要抽象权重（VLIS），这是一种将视觉语言模型的视觉条件能力与单模式文本Only语言模型的语言理解能力结合在一起的新框架。它从视觉语言模型中提取每个图像和文本的点对 Mutual Information，并将其用作重要抽象权重，以调整文本Only模型的单词概率。VLIS改进了多种任务，包括宽泛理解（WHOOPS、OK-VQA和科学问答）和复杂文本生成（Concadia、图像段落描述和ROCStories）。我们的结果表明，VLIS代表了一个有前途的新方向 для多模态语言生成。
</details></li>
</ul>
<hr>
<h2 id="Improving-Access-to-Justice-for-the-Indian-Population-A-Benchmark-for-Evaluating-Translation-of-Legal-Text-to-Indian-Languages"><a href="#Improving-Access-to-Justice-for-the-Indian-Population-A-Benchmark-for-Evaluating-Translation-of-Legal-Text-to-Indian-Languages" class="headerlink" title="Improving Access to Justice for the Indian Population: A Benchmark for Evaluating Translation of Legal Text to Indian Languages"></a>Improving Access to Justice for the Indian Population: A Benchmark for Evaluating Translation of Legal Text to Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09765">http://arxiv.org/abs/2310.09765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayan Mahapatra, Debtanu Datta, Shubham Soni, Adrijit Goswami, Saptarshi Ghosh</li>
<li>For: The paper aims to make legal text in the Indian judiciary more accessible to the general population, who are not comfortable with reading English.* Methods: The authors construct a high-quality legal parallel corpus containing aligned text units in English and nine Indian languages, and benchmark the performance of various Machine Translation (MT) systems over this corpus.* Results: The authors survey Law practitioners to evaluate the quality of the translations produced by the MT systems, and compare the results with automatic MT evaluation metrics.<details>
<summary>Abstract</summary>
Most legal text in the Indian judiciary is written in complex English due to historical reasons. However, only about 10% of the Indian population is comfortable in reading English. Hence legal text needs to be made available in various Indian languages, possibly by translating the available legal text from English. Though there has been a lot of research on translation to and between Indian languages, to our knowledge, there has not been much prior work on such translation in the legal domain. In this work, we construct the first high-quality legal parallel corpus containing aligned text units in English and nine Indian languages, that includes several low-resource languages. We also benchmark the performance of a wide variety of Machine Translation (MT) systems over this corpus, including commercial MT systems, open-source MT systems and Large Language Models. Through a comprehensive survey by Law practitioners, we check how satisfied they are with the translations by some of these MT systems, and how well automatic MT evaluation metrics agree with the opinions of Law practitioners.
</details>
<details>
<summary>摘要</summary>
大多数印度法律文本在印度司法系统中 escriten in 复杂的英语，历史原因。然而，只有约10%的印度人口能够读写英语。因此，法律文本需要在各种印度语言中提供，可能是通过从英语翻译。虽然已有很多关于翻译与印度语言之间的研究，但我们知道，在法律领域中的翻译研究不多。在这项工作中，我们构建了首个高质量的法律平行文本库，包括英语和九种印度语言的对应文本单位。我们还对这个库进行了评估，包括商业MT系统、开源MT系统和大语言模型。通过对法律专业人员的详细调查，我们检查了这些MT系统的翻译质量如何满意，以及自动MT评估指标与专业人员的意见如何相符。
</details></li>
</ul>
<hr>
<h2 id="DropMix-Better-Graph-Contrastive-Learning-with-Harder-Negative-Samples"><a href="#DropMix-Better-Graph-Contrastive-Learning-with-Harder-Negative-Samples" class="headerlink" title="DropMix: Better Graph Contrastive Learning with Harder Negative Samples"></a>DropMix: Better Graph Contrastive Learning with Harder Negative Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09764">http://arxiv.org/abs/2310.09764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Mayueq/DropMix-Code">https://github.com/Mayueq/DropMix-Code</a></li>
<li>paper_authors: Yueqi Ma, Minjie Chen, Xiang Li</li>
<li>for: 提高图像对比学习中的负样本质量</li>
<li>methods: DropMix方法包括两个主要步骤：首先选择图像中的困难负样本，然后只在部分表示维度上进行混合，以生成更困难的负样本</li>
<li>results: 对六个基准数据集进行了广泛的实验，结果表明 DropMix 方法可以提高对比学习性能<details>
<summary>Abstract</summary>
While generating better negative samples for contrastive learning has been widely studied in the areas of CV and NLP, very few work has focused on graph-structured data. Recently, Mixup has been introduced to synthesize hard negative samples in graph contrastive learning (GCL). However, due to the unsupervised learning nature of GCL, without the help of soft labels, directly mixing representations of samples could inadvertently lead to the information loss of the original hard negative and further adversely affect the quality of the newly generated harder negative. To address the problem, in this paper, we propose a novel method DropMix to synthesize harder negative samples, which consists of two main steps. Specifically, we first select some hard negative samples by measuring their hardness from both local and global views in the graph simultaneously. After that, we mix hard negatives only on partial representation dimensions to generate harder ones and decrease the information loss caused by Mixup. We conduct extensive experiments to verify the effectiveness of DropMix on six benchmark datasets. Our results show that our method can lead to better GCL performance. Our data and codes are publicly available at https://github.com/Mayueq/DropMix-Code.
</details>
<details>
<summary>摘要</summary>
“对待于图structured数据的异构学习中，生成更好的负样本已经广泛研究在CV和NLP领域，但很少有研究在图结构数据上。近期，Mixup方法在图相关学习（GCL）中被引入，以生成困难的负样本。然而，由于GCL是无监督学习的，没有软标签的帮助，直接混合样本表示可能会导致原始困难的负样本中的信息损失，从而降低新生成的更困难负样本的质量。为解决这个问题，在本文中，我们提出了一种新的方法DropMix，它包括两个主要步骤。具体来说，我们首先从图中选择一些困难的负样本，并测量它们的困难程度从本地和全局视图同时。然后，我们只在部分表示维度上混合困难负样本，以生成更困难的负样本和减少Mixup导致的信息损失。我们对六个标准 benchmark dataset进行了广泛的实验，结果显示，我们的方法可以提高GCL性能。我们的数据和代码在https://github.com/Mayueq/DropMix-Code上公开。”
</details></li>
</ul>
<hr>
<h2 id="Diversifying-the-Mixture-of-Experts-Representation-for-Language-Models-with-Orthogonal-Optimizer"><a href="#Diversifying-the-Mixture-of-Experts-Representation-for-Language-Models-with-Orthogonal-Optimizer" class="headerlink" title="Diversifying the Mixture-of-Experts Representation for Language Models with Orthogonal Optimizer"></a>Diversifying the Mixture-of-Experts Representation for Language Models with Orthogonal Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09762">http://arxiv.org/abs/2310.09762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boan Liu, Liang Ding, Li Shen, Keqin Peng, Yu Cao, Dazhao Cheng, Dacheng Tao</li>
<li>for: 提高MoE模型的表现和多样性</li>
<li>methods: 提出了一种简单 yet高效的解决方案——对采用MoE结构的模型进行非对称专家优化，并 introduce了一种 alternate training strategy to encourage each expert to update in a direction orthogonal to the subspace spanned by other experts。</li>
<li>results: 通过广泛的实验，证明了我们提出的优化算法可以显著提高MoE模型在GLUE、SuperGLUE、问答任务和名词识别任务的表现。<details>
<summary>Abstract</summary>
The Mixture of Experts (MoE) has emerged as a highly successful technique in deep learning, based on the principle of divide-and-conquer to maximize model capacity without significant additional computational cost. Even in the era of large-scale language models (LLMs), MoE continues to play a crucial role, as some researchers have indicated that GPT-4 adopts the MoE structure to ensure diverse inference results. However, MoE is susceptible to performance degeneracy, particularly evident in the issues of imbalance and homogeneous representation among experts. While previous studies have extensively addressed the problem of imbalance, the challenge of homogeneous representation remains unresolved. In this study, we shed light on the homogeneous representation problem, wherein experts in the MoE fail to specialize and lack diversity, leading to frustratingly high similarities in their representations (up to 99% in a well-performed MoE model). This problem restricts the expressive power of the MoE and, we argue, contradicts its original intention. To tackle this issue, we propose a straightforward yet highly effective solution: OMoE, an orthogonal expert optimizer. Additionally, we introduce an alternating training strategy that encourages each expert to update in a direction orthogonal to the subspace spanned by other experts. Our algorithm facilitates MoE training in two key ways: firstly, it explicitly enhances representation diversity, and secondly, it implicitly fosters interaction between experts during orthogonal weights computation. Through extensive experiments, we demonstrate that our proposed optimization algorithm significantly improves the performance of fine-tuning the MoE model on the GLUE benchmark, SuperGLUE benchmark, question-answering task, and name entity recognition tasks.
</details>
<details>
<summary>摘要</summary>
《粗粒化专家（MoE）》技术在深度学习中得到了广泛应用，基于分治分 conquering的原则，以提高模型容量而不增加显著的计算成本。即使在大规模语言模型（LLM）时代，MoE仍然扮演着关键的角色，一些研究人员表示GPT-4采用了MoE结构以确保多样化的推理结果。然而，MoE受到性能异常化的问题困扰，特别是专家之间的不均衡和同质化表现问题。虽然以前的研究已经广泛地解决了不均衡问题，但同质化表现问题仍然未得到解决。在这项研究中，我们 shed light on the homogeneous representation problem，专家在MoE中失去特化和多样性，导致其表达相似度达99%以上（在一个良好的MoE模型中）。这个问题限制了MoE的表达力，我们认为这与MoE的原意相抵触。为解决这个问题，我们提出了一种简单 yet highly effective的解决方案：OMoE，一种ortogonal expert optimizer。此外，我们还提出了一种 alternate training strategy，鼓励每个专家在归一化方向上更新其 weights。我们的算法可以在两个关键方面帮助MoE训练：首先，它明确提高了表达多样性；其次，它 implicit地促进了专家之间的交互在 ortogonal weights 计算中。通过广泛的实验，我们证明了我们的提出的优化算法可以显著提高 fine-tuning MoE 模型在 GLUE Benchmark、SuperGLUE Benchmark、问题回答任务和名词识别任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="CAPro-Webly-Supervised-Learning-with-Cross-Modality-Aligned-Prototypes"><a href="#CAPro-Webly-Supervised-Learning-with-Cross-Modality-Aligned-Prototypes" class="headerlink" title="CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes"></a>CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09761">http://arxiv.org/abs/2310.09761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuleiqin/capro">https://github.com/yuleiqin/capro</a></li>
<li>paper_authors: Yulei Qin, Xingyu Chen, Yunhang Shen, Chaoyou Fu, Yun Gu, Ke Li, Xing Sun, Rongrong Ji</li>
<li>For: 这个论文旨在提出一种基于文本和图像协同学习的Visual Representation Learning方法，以适应现实世界中噪声的挑战。* Methods: 该方法使用文本prototype来选择干净的图像，并通过文本匹配来解决视觉prototype的混乱问题。此外，它还使用视觉特征空间来完善和提高图像的文本描述，以及使用集合bootstrap来鼓励更好的标签参考。* Results: 实验表明，CAPro可以 effetively处理现实世界中的噪声，并在单个标签和多个标签场景下达到新的州OF-THE-ART性能。它还展示了对开集认识的Robustness。代码可以在<a target="_blank" rel="noopener" href="https://github.com/yuleiqin/capro%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/yuleiqin/capro上下载。</a><details>
<summary>Abstract</summary>
Webly supervised learning has attracted increasing attention for its effectiveness in exploring publicly accessible data at scale without manual annotation. However, most existing methods of learning with web datasets are faced with challenges from label noise, and they have limited assumptions on clean samples under various noise. For instance, web images retrieved with queries of tiger cat (a cat species) and drumstick (a musical instrument) are almost dominated by images of tigers and chickens, which exacerbates the challenge of fine-grained visual concept learning. In this case, exploiting both web images and their associated texts is a requisite solution to combat real-world noise. In this paper, we propose Cross-modality Aligned Prototypes (CAPro), a unified prototypical contrastive learning framework to learn visual representations with correct semantics. For one thing, we leverage textual prototypes, which stem from the distinct concept definition of classes, to select clean images by text matching and thus disambiguate the formation of visual prototypes. For another, to handle missing and mismatched noisy texts, we resort to the visual feature space to complete and enhance individual texts and thereafter improve text matching. Such semantically aligned visual prototypes are further polished up with high-quality samples, and engaged in both cluster regularization and noise removal. Besides, we propose collective bootstrapping to encourage smoother and wiser label reference from appearance-similar instances in a manner of dictionary look-up. Extensive experiments on WebVision1k and NUS-WIDE (Web) demonstrate that CAPro well handles realistic noise under both single-label and multi-label scenarios. CAPro achieves new state-of-the-art performance and exhibits robustness to open-set recognition. Codes are available at https://github.com/yuleiqin/capro.
</details>
<details>
<summary>摘要</summary>
优先级学习在扫描公共访问数据时得到了越来越多的关注，因为它可以让计算机系统利用大规模数据来学习而无需手动标注。然而，现有的网络数据学习方法受到噪声标注的挑战，而且它们假设清晰的样本存在于各种噪声下。例如，通过查询“虎猫”和“鼓”的图像检索结果将主要是虎猫和鸡图像，这会使视觉概念学习受到挑战。在这种情况下，利用网络图像和其相关文本是一种必要的解决方案，以避免实际世界中的噪声。在这篇论文中，我们提出了跨模态对应原型（CAPro），一种统一的 проtotypical contrastive learning框架，用于学习正确的视觉表示。首先，我们利用文本原型，它们来自不同类型的概念定义，来选择干净的图像，并通过文本匹配来减少视觉原型的形成干扰。其次，为了处理缺失和不一致的噪声文本，我们 resorts to the visual feature space to complete and enhance individual texts, and thereafter improve text matching。这些semantically aligned的视觉原型被进一步练练以高质量样本，并在集群规则和噪声除除中使用。此外，我们提出了集体 bootstrap的方法，以便在 appearancely similar 的实例上进行词典查找，以便更好地引用表达相似的标签。广泛的实验表明，CAPro可以有效地处理现实世界中的噪声，并在单个标签和多标签场景中达到新的领先性性和robustness。代码可以在https://github.com/yuleiqin/capro中获取。
</details></li>
</ul>
<hr>
<h2 id="EX-FEVER-A-Dataset-for-Multi-hop-Explainable-Fact-Verification"><a href="#EX-FEVER-A-Dataset-for-Multi-hop-Explainable-Fact-Verification" class="headerlink" title="EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification"></a>EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09754">http://arxiv.org/abs/2310.09754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dependentsign/EX-FEVER">https://github.com/dependentsign/EX-FEVER</a></li>
<li>paper_authors: Huanhuan Ma, Weizhi Xu, Yifan Wei, Liuji Chen, Liang Wang, Qiang Liu, Shu Wu, Liang Wang</li>
<li>for: 这个论文的目的是构建一个可解释的事实验证系统，以便在复杂多层扩展中实现自动化的真实检查。</li>
<li>methods: 该论文使用了一种新的基于Wikipedia文档的数据集，并提出了一种基于这些数据集的基线系统。该基线系统包括文档检索、解释生成和CLAIM验证三个部分。</li>
<li>results: 该论文通过对EX-FEVER数据集进行实验，发现现有的事实验证模型在这个数据集上表现不佳，而Large Language Models在这个任务中具有潜在的应用前景。<details>
<summary>Abstract</summary>
Fact verification aims to automatically probe the veracity of a claim based on several pieces of evidence. Existing works are always engaging in the accuracy improvement, let alone the explainability, a critical capability of fact verification system. Constructing an explainable fact verification system in a complex multi-hop scenario is consistently impeded by the absence of a relevant high-quality dataset. Previous dataset either suffer from excessive simplification or fail to incorporate essential considerations for explainability. To address this, we present EX-FEVER, a pioneering dataset for multi-hop explainable fact verification. With over 60,000 claims involving 2-hop and 3-hop reasoning, each is created by summarizing and modifying information from hyperlinked Wikipedia documents. Each instance is accompanied by a veracity label and an explanation that outlines the reasoning path supporting the veracity classification. Additionally, we demonstrate a novel baseline system on our EX-FEVER dataset, showcasing document retrieval, explanation generation, and claim verification and observe that existing fact verification models trained on previous datasets struggle to perform well on our dataset. Furthermore, we highlight the potential of utilizing Large Language Models in the fact verification task. We hope our dataset could make a significant contribution by providing ample opportunities to explore the integration of natural language explanations in the domain of fact verification.
</details>
<details>
<summary>摘要</summary>
fact checking 目标是自动检查声明的真实性，基于多个证据。现有工作都在增强准确性，却忽视了解释性，这是 фаクト checking 系统的关键能力。在复杂多趋场景中构建可解释的 факт checking 系统受到高质量数据缺乏的阻碍。现有数据集都受到过度简化或者缺乏关键考虑因素，以解释性为前提，我们提出了 EX-FEVER 数据集，包含了2-hop和3-hop逻辑推理的60,000个声明，每个声明都由修改和摘要来自 hyperlinked Wikipedia 文档。每个实例都有真实性标签和解释，其中解释描述了支持真实性分类的逻辑路径。此外，我们还提出了一种基于 EX-FEVER 数据集的基线系统，包括文档检索、解释生成和声明验证，并观察到现有的 fact checking 模型在前一个数据集上表现不佳。此外，我们还强调了利用大型自然语言模型在 fact checking 任务中的潜在优势。我们希望我们的数据集能够为研究人员提供丰富的探索自然语言解释在验证领域的机会。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Segmentation-Road-Network-Generation-with-Multi-Modal-LLMs"><a href="#Beyond-Segmentation-Road-Network-Generation-with-Multi-Modal-LLMs" class="headerlink" title="Beyond Segmentation: Road Network Generation with Multi-Modal LLMs"></a>Beyond Segmentation: Road Network Generation with Multi-Modal LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09755">http://arxiv.org/abs/2310.09755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumedh Rasal, Sanjay Kumar Boddhu</li>
<li>for: 本研究旨在提供一种创新的路网生成方法，利用多modal的大语言模型（LLM）来生成细致、可行驾驶的路网。</li>
<li>methods: 我们的模型使用了BLIP-2架构 arXiv:2301.12597，利用预先冻结的图像编码器和大语言模型来创造一种多modal LLM。</li>
<li>results: 我们的实验结果表明，使用我们的方法可以准确地生成路网，并且不需要生成二进制分割mask。这种方法可以增强自主驾驶系统，特别是在路网场景中，准确的导航是非常重要的。<details>
<summary>Abstract</summary>
This paper introduces an innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM). Our model is specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images. The core innovation of our system lies in the unique training methodology employed for the large language model to generate road networks as its output. This approach draws inspiration from the BLIP-2 architecture arXiv:2301.12597, leveraging pre-trained frozen image encoders and large language models to create a versatile multi-modal LLM.   Our work also offers an alternative to the reasoning segmentation method proposed in the LISA paper arXiv:2308.00692. By training the large language model with our approach, the necessity for generating binary segmentation masks, as suggested in the LISA paper arXiv:2308.00692, is effectively eliminated. Experimental results underscore the efficacy of our multi-modal LLM in providing precise and valuable navigational guidance. This research represents a significant stride in bolstering autonomous navigation systems, especially in road network scenarios, where accurate guidance is of paramount importance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-can-transformers-reason-with-abstract-symbols"><a href="#When-can-transformers-reason-with-abstract-symbols" class="headerlink" title="When can transformers reason with abstract symbols?"></a>When can transformers reason with abstract symbols?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09753">http://arxiv.org/abs/2310.09753</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eboix/relational-reasoning">https://github.com/eboix/relational-reasoning</a></li>
<li>paper_authors: Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, Joshua Susskind</li>
<li>for: 这个研究探讨了基于抽象符号的关系理解任务中 transformer大语言模型（LLMs）的能力。</li>
<li>methods: 这些任务使用了许多年来在 neuroscience 文献中研究的基本建构物，包括程序编程、数学和语言理解。</li>
<li>results: 研究发现，对于回归任务， transformer 可以通过训练而泛化，但需要很大量的训练数据；对于下一个符号预测任务， transformer 的表示维度增加会导致泛化失败，但可以通过添加两个可调参数来降低数据量。<details>
<summary>Abstract</summary>
We investigate the capabilities of transformer large language models (LLMs) on relational reasoning tasks involving abstract symbols. Such tasks have long been studied in the neuroscience literature as fundamental building blocks for more complex abilities in programming, mathematics, and verbal reasoning. For (i) regression tasks, we prove that transformers generalize when trained, but require astonishingly large quantities of training data. For (ii) next-token-prediction tasks with symbolic labels, we show an "inverse scaling law": transformers fail to generalize as their embedding dimension increases. For both settings (i) and (ii), we propose subtle transformer modifications which can reduce the amount of data needed by adding two trainable parameters per head.
</details>
<details>
<summary>摘要</summary>
我们研究transformer大语言模型（LLM）在关系理解任务中的能力，这些任务在神经科学文献中已经被认为是更进阶的程序设计、数学和语言理解能力的基础元素。 для（i）回溯任务，我们证明transformer会通过训练时通过数据大量化，但需要非常多的训练数据。 для（ii）下一个字符预测任务，我们显示了“倒推法则”：transformer在增加嵌入维度时无法通过数据大量化。 для beiden（i）和（ii）设定，我们提出了微妙的transformer修改，可以透过添加两个可调参数每个head来降低训练数据量。
</details></li>
</ul>
<hr>
<h2 id="Domain-Specific-Language-Model-Post-Training-for-Indonesian-Financial-NLP"><a href="#Domain-Specific-Language-Model-Post-Training-for-Indonesian-Financial-NLP" class="headerlink" title="Domain-Specific Language Model Post-Training for Indonesian Financial NLP"></a>Domain-Specific Language Model Post-Training for Indonesian Financial NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09736">http://arxiv.org/abs/2310.09736</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/intanq/indonesian-financial-domain-lm">https://github.com/intanq/indonesian-financial-domain-lm</a></li>
<li>paper_authors: Ni Putu Intan Maharani, Yoga Yustiawan, Fauzy Caesar Rochim, Ayu Purwarianti</li>
<li>for: 这 paper 是关于金融领域的自然语言处理（NLP）任务中BERT和IndoBERT的应用和调整。</li>
<li>methods: 本文使用了预训练的IndoBERT，在小规模的INDONESIAN financial corpus上进行了后期训练。同时，我们还构建了INDONESIAN自然语言负面情感分类和主题分类数据集，并发布了一家BERT模型 для金融NLP。</li>
<li>results: 我们的实验结果表明，对特定领域下的下游任务进行适应性训练可以提高语言模型的效果。<details>
<summary>Abstract</summary>
BERT and IndoBERT have achieved impressive performance in several NLP tasks. There has been several investigation on its adaption in specialized domains especially for English language. We focus on financial domain and Indonesian language, where we perform post-training on pre-trained IndoBERT for financial domain using a small scale of Indonesian financial corpus. In this paper, we construct an Indonesian self-supervised financial corpus, Indonesian financial sentiment analysis dataset, Indonesian financial topic classification dataset, and release a family of BERT models for financial NLP. We also evaluate the effectiveness of domain-specific post-training on sentiment analysis and topic classification tasks. Our findings indicate that the post-training increases the effectiveness of a language model when it is fine-tuned to domain-specific downstream tasks.
</details>
<details>
<summary>摘要</summary>
BERT和IndoBERT在多个自然语言处理任务中表现出色。有很多关于它们在特定领域的调整的研究。我们在金融领域和印度尼西亚语言中进行调整，使用小规模的印度尼西亚金融文本库进行后处理。在这篇论文中，我们构建了一个印度尼西亚自我指导的金融文本库，印度尼西亚金融情感分析数据集和印度尼西亚金融话题分类数据集，并推出一家BERT模型的家族用于金融NLPT。我们还评估了域名特定的后处理对情感分析和话题分类任务的效果。我们的发现表明，后处理可以提高语言模型在域名特定下滤波器任务的效果。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Conversational-Search-Large-Language-Model-Aided-Informative-Query-Rewriting"><a href="#Enhancing-Conversational-Search-Large-Language-Model-Aided-Informative-Query-Rewriting" class="headerlink" title="Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting"></a>Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09716">http://arxiv.org/abs/2310.09716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanghua Ye, Meng Fang, Shenghui Li, Emine Yilmaz</li>
<li>for: 提高对话搜索的会话搜索性能，使用语言模型来重写用户查询。</li>
<li>methods: 使用大型语言模型（LLM）来重写查询，通过设计良好的指令来生成有用的重写。</li>
<li>results: 对QReCC数据集进行实验，显示了使用有用的重写可以提高搜索性能，尤其是使用稀有搜索器。<details>
<summary>Abstract</summary>
Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can yield substantially improved retrieval performance compared to human rewrites, especially with sparse retrievers.
</details>
<details>
<summary>摘要</summary>
查询重写play vital role in enhance conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverages human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can yield substantially improved retrieval performance compared to human rewrites, especially with sparse retrievers.Here's the text with Traditional Chinese characters:查询重写play vital role in enhance conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverages human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can yield substantially improved retrieval performance compared to human rewrites, especially with sparse retrievers.
</details></li>
</ul>
<hr>
<h2 id="New-Advances-in-Body-Composition-Assessment-with-ShapedNet-A-Single-Image-Deep-Regression-Approach"><a href="#New-Advances-in-Body-Composition-Assessment-with-ShapedNet-A-Single-Image-Deep-Regression-Approach" class="headerlink" title="New Advances in Body Composition Assessment with ShapedNet: A Single Image Deep Regression Approach"></a>New Advances in Body Composition Assessment with ShapedNet: A Single Image Deep Regression Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09709">http://arxiv.org/abs/2310.09709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navar Medeiros M. Nascimento, Pedro Cavalcante de Sousa Junior, Pedro Yuri Rodrigues Nunes, Suane Pires Pinheiro da Silva, Luiz Lannes Loureiro, Victor Zaban Bittencourt, Valden Luis Matos Capistrano Junior, Pedro Pedrosa Rebouças Filho</li>
<li>for: 增强体重分析方法</li>
<li>methods: 使用深度神经网络进行身体脂肪百分比（BFP）估算、个体识别和位置确定，只需单张照片</li>
<li>results: 比对 стандар方法双能X射线吸收仪(DXA)，1273名健康成人的Age、性别和BFP水平进行验证，结果表明ShapedNet比前方法提高19.5%，MAPE为4.91%，MAE为1.42%，Gender-neutral方法表现更优。<details>
<summary>Abstract</summary>
We introduce a novel technique called ShapedNet to enhance body composition assessment. This method employs a deep neural network capable of estimating Body Fat Percentage (BFP), performing individual identification, and enabling localization using a single photograph. The accuracy of ShapedNet is validated through comprehensive comparisons against the gold standard method, Dual-Energy X-ray Absorptiometry (DXA), utilizing 1273 healthy adults spanning various ages, sexes, and BFP levels. The results demonstrate that ShapedNet outperforms in 19.5% state of the art computer vision-based approaches for body fat estimation, achieving a Mean Absolute Percentage Error (MAPE) of 4.91% and Mean Absolute Error (MAE) of 1.42. The study evaluates both gender-based and Gender-neutral approaches, with the latter showcasing superior performance. The method estimates BFP with 95% confidence within an error margin of 4.01% to 5.81%. This research advances multi-task learning and body composition assessment theory through ShapedNet.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的技术called ShapedNet，用于提高身体组分评估。这种方法利用深度神经网络，能够估算身体脂肪百分比（BFP），进行个体识别，并使用单张图像进行地图化。我们 validate了ShapedNet的准确性，通过对杰基标方法（DXA）的1273名健康成人进行比较，这些成人来自不同的年龄、性别和BFP水平。结果表明，ShapedNet在19.5%的state of the art计算机视觉基础上进行身体脂肪估计方法中，表现出色，其 Mean Absolute Percentage Error（MAPE）为4.91%， Mean Absolute Error（MAE）为1.42。我们也评估了不同的性别和无性别方法，其中后者表现更出色。ShapedNet可以在95%的信息内，对BFP进行4.01%至5.81%的估计，这对身体组分评估理论和多任务学习做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="AdaptSSR-Pre-training-User-Model-with-Augmentation-Adaptive-Self-Supervised-Ranking"><a href="#AdaptSSR-Pre-training-User-Model-with-Augmentation-Adaptive-Self-Supervised-Ranking" class="headerlink" title="AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking"></a>AdaptSSR: Pre-training User Model with Augmentation-Adaptive Self-Supervised Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09706">http://arxiv.org/abs/2310.09706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yflyl613/AdaptSSR">https://github.com/yflyl613/AdaptSSR</a></li>
<li>paper_authors: Yang Yu, Qi Liu, Kai Zhang, Yuren Zhang, Chao Song, Min Hou, Yuqing Yuan, Zhihao Ye, Zaixi Zhang, Sanshi Lei Yu</li>
<li>for: 用于提高用户模型的泛化能力和数据稀缺性问题。</li>
<li>methods: 使用对数据进行增强学习，并采用自适应自我supervised排序任务来改善用户模型的准确性。</li>
<li>results: 经过extensive experiments的证明，该方法可以提高用户模型的性能和数据稀缺性问题。<details>
<summary>Abstract</summary>
User modeling, which aims to capture users' characteristics or interests, heavily relies on task-specific labeled data and suffers from the data sparsity issue. Several recent studies tackled this problem by pre-training the user model on massive user behavior sequences with a contrastive learning task. Generally, these methods assume different views of the same behavior sequence constructed via data augmentation are semantically consistent, i.e., reflecting similar characteristics or interests of the user, and thus maximizing their agreement in the feature space. However, due to the diverse interests and heavy noise in user behaviors, existing augmentation methods tend to lose certain characteristics of the user or introduce noisy behaviors. Thus, forcing the user model to directly maximize the similarity between the augmented views may result in a negative transfer. To this end, we propose to replace the contrastive learning task with a new pretext task: Augmentation-Adaptive SelfSupervised Ranking (AdaptSSR), which alleviates the requirement of semantic consistency between the augmented views while pre-training a discriminative user model. Specifically, we adopt a multiple pairwise ranking loss which trains the user model to capture the similarity orders between the implicitly augmented view, the explicitly augmented view, and views from other users. We further employ an in-batch hard negative sampling strategy to facilitate model training. Moreover, considering the distinct impacts of data augmentation on different behavior sequences, we design an augmentation-adaptive fusion mechanism to automatically adjust the similarity order constraint applied to each sample based on the estimated similarity between the augmented views. Extensive experiments on both public and industrial datasets with six downstream tasks verify the effectiveness of AdaptSSR.
</details>
<details>
<summary>摘要</summary>
用户模型化，它目标是捕捉用户特点或兴趣，受到任务特定的标注数据的缺乏问题困扰。一些最近的研究解决了这个问题，通过在大量用户行为序列上进行预训练，并使用对偶学习任务。通常，这些方法假设不同的视图的同一个行为序列，通过数据扩展生成的方法是具有相同特征或兴趣的用户，并且尽量在特征空间中增加它们之间的一致性。然而，由于用户的兴趣和行为噪声的多样性，现有的扩展方法通常会消失用户的特征或引入噪声行为。因此，直接在扩展视图之间寻求最大的一致性可能会导致负面传播。为此，我们提议将对偶学习任务改为一种新的预文任务：增强自监 Ranking（AdaptSSR），这种任务可以降低对扩展视图的 semantic consistency 要求，而在预训练用户模型时，capture用户的相似性序列。具体来说，我们采用多对多对比损失函数，训练用户模型，捕捉扩展视图、显式扩展视图和其他用户视图之间的相似性序列。此外，考虑不同的数据扩展对不同的行为序列的不同影响，我们设计了数据扩展适应机制，自动调整每个样本所应用的相似性序列约束，基于每个扩展视图之间的估计相似性。我们在公共和工业数据集上进行了六个下游任务的广泛实验，并证明了 AdaptSSR 的效果。
</details></li>
</ul>
<hr>
<h2 id="Progressive-Evidence-Refinement-for-Open-domain-Multimodal-Retrieval-Question-Answering"><a href="#Progressive-Evidence-Refinement-for-Open-domain-Multimodal-Retrieval-Question-Answering" class="headerlink" title="Progressive Evidence Refinement for Open-domain Multimodal Retrieval Question Answering"></a>Progressive Evidence Refinement for Open-domain Multimodal Retrieval Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09696">http://arxiv.org/abs/2310.09696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwen Yang, Anran Wu, Xingjiao Wu, Luwei Xiao, Tianlong Ma, Cheng Jin, Liang He</li>
<li>for: 提高 retrieval-based question answering 模型的表现，解决现有模型在使用压缩证据特征时丢失细节信息，以及Question和证据之间的特征提取差距。</li>
<li>methods: 提出了一种两阶段框架，包括进行逐步证据筛选、使用 semi-supervised contrastive learning 训练策略、多次询问回答等方法来解决这两个问题。</li>
<li>results: 通过广泛的实验证明，该模型在 WebQA 和 MultimodelQA 测试上达到了出色的表现。<details>
<summary>Abstract</summary>
Pre-trained multimodal models have achieved significant success in retrieval-based question answering. However, current multimodal retrieval question-answering models face two main challenges. Firstly, utilizing compressed evidence features as input to the model results in the loss of fine-grained information within the evidence. Secondly, a gap exists between the feature extraction of evidence and the question, which hinders the model from effectively extracting critical features from the evidence based on the given question. We propose a two-stage framework for evidence retrieval and question-answering to alleviate these issues. First and foremost, we propose a progressive evidence refinement strategy for selecting crucial evidence. This strategy employs an iterative evidence retrieval approach to uncover the logical sequence among the evidence pieces. It incorporates two rounds of filtering to optimize the solution space, thus further ensuring temporal efficiency. Subsequently, we introduce a semi-supervised contrastive learning training strategy based on negative samples to expand the scope of the question domain, allowing for a more thorough exploration of latent knowledge within known samples. Finally, in order to mitigate the loss of fine-grained information, we devise a multi-turn retrieval and question-answering strategy to handle multimodal inputs. This strategy involves incorporating multimodal evidence directly into the model as part of the historical dialogue and question. Meanwhile, we leverage a cross-modal attention mechanism to capture the underlying connections between the evidence and the question, and the answer is generated through a decoding generation approach. We validate the model's effectiveness through extensive experiments, achieving outstanding performance on WebQA and MultimodelQA benchmark tests.
</details>
<details>
<summary>摘要</summary>
先进多模态模型已经在回答问题中取得了显著成功。然而，当前的多模态回答问题模型面临两个主要挑战。首先，使用压缩证据特征作为模型输入会导致证据中细详信息的损失。其次，证据和问题之间的特征EXTRACTING存在差距，这使得模型从证据中EXTRACTING答案相关的关键特征变得困难。我们提出了一个两个阶段框架，用于增强证据检索和回答问题。首先，我们提出了一种进步的证据精细化策略，用于选择重要的证据。这种策略使用迭代的证据检索方法，找到证据归并的逻辑顺序。它使用两轮的筛选来优化解决空间，从而更加确保时间效率。其次，我们引入了一种半监督对比学习训练策略，以扩展问题领域。这种策略基于负样本，通过对已知样本进行更多的探索，扩大问题领域的范围。 finally，为了减少细详信息的损失，我们提出了一种多turn检索和回答策略，用于处理多模态输入。这种策略将多模态证据直接 integrate into the model 中的历史对话和问题。同时，我们利用交叉模式注意力机制，捕捉证据和问题之间的下面连接。通过解码生成方法，我们生成答案。我们通过广泛的实验 validate the model's effectiveness, achieved outstanding performance on WebQA and MultimodelQA benchmark tests.
</details></li>
</ul>
<hr>
<h2 id="Spike-based-Neuromorphic-Computing-for-Next-Generation-Computer-Vision"><a href="#Spike-based-Neuromorphic-Computing-for-Next-Generation-Computer-Vision" class="headerlink" title="Spike-based Neuromorphic Computing for Next-Generation Computer Vision"></a>Spike-based Neuromorphic Computing for Next-Generation Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09692">http://arxiv.org/abs/2310.09692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sakib Hasan, Catherine D. Schuman, Zhongyang Zhang, Tauhidur Rahman, Garrett S. Rose</li>
<li>for: 这篇论文旨在探讨 neuromorphic computing 技术的应用在计算机视觉领域。</li>
<li>methods: 论文使用了不同层次设计（设备、电路和算法）的示例来介绍 neuromorphic computing 技术。</li>
<li>results: 论文 conclude 了一些可能的应用和未来研究方向，例如用于 edge device 中的视觉任务。<details>
<summary>Abstract</summary>
Neuromorphic Computing promises orders of magnitude improvement in energy efficiency compared to traditional von Neumann computing paradigm. The goal is to develop an adaptive, fault-tolerant, low-footprint, fast, low-energy intelligent system by learning and emulating brain functionality which can be realized through innovation in different abstraction layers including material, device, circuit, architecture and algorithm. As the energy consumption in complex vision tasks keep increasing exponentially due to larger data set and resource-constrained edge devices become increasingly ubiquitous, spike-based neuromorphic computing approaches can be viable alternative to deep convolutional neural network that is dominating the vision field today. In this book chapter, we introduce neuromorphic computing, outline a few representative examples from different layers of the design stack (devices, circuits and algorithms) and conclude with a few exciting applications and future research directions that seem promising for computer vision in the near future.
</details>
<details>
<summary>摘要</summary>
《神经omorphic computing》承诺在能效率方面比传统的各批计算模式提供多个数量级的提升。目标是开发一个适应、错误tolerant、占用空间小、快速、低能耗智能系统，通过学习和模拟大脑功能来实现。在复杂视觉任务中的能 consumption不断增加，而 Edge devices受限的资源变得越来越普遍，使得使用射频型神经omorphic computing方法可以成为视觉领域今天的可行替代方案。在这个书章中，我们介绍了神经omorphic computing，从不同层次的设计栈（设备、电路和算法）中选出了一些示例，并结束于一些有前途的应用和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Configuration-Validation-with-Large-Language-Models"><a href="#Configuration-Validation-with-Large-Language-Models" class="headerlink" title="Configuration Validation with Large Language Models"></a>Configuration Validation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09690">http://arxiv.org/abs/2310.09690</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ciri4conf/ciri">https://github.com/ciri4conf/ciri</a></li>
<li>paper_authors: Xinyu Lian, Yinfang Chen, Runxiang Cheng, Jie Huang, Parth Thakkar, Tianyin Xu</li>
<li>for: 这个论文主要是为了探讨使用自然语言处理（NLP）和机器学习（ML）进行配置验证的可能性和效果。</li>
<li>methods: 该论文使用了大量的配置数据和不同的大语言模型（LLMs）进行验证，并开发了一个通用的 LLM-based 验证框架（Ciri）。该框架使用了小量的示例数据和几何学学习来设计有效的提示，并将多个 LLMs 的输出 validate 并聚合成验证结果。</li>
<li>results: 该论文的分析表明，使用 LLMs 进行配置验证是可能的，并且可以采用提示工程学习和几何学学习来设计有效的提示。但是，该论文还发现了一些问题，例如某些类型的错误配置不能准确地被检测出来，以及 LLMs 的偏见对一些常见的配置参数产生影响。<details>
<summary>Abstract</summary>
Misconfigurations are the major causes of software failures. Existing configuration validation techniques rely on manually written rules or test cases, which are expensive to implement and maintain, and are hard to be comprehensive. Leveraging machine learning (ML) and natural language processing (NLP) for configuration validation is considered a promising direction, but has been facing challenges such as the need of not only large-scale configuration data, but also system-specific features and models which are hard to generalize. Recent advances in Large Language Models (LLMs) show the promises to address some of the long-lasting limitations of ML/NLP-based configuration validation techniques. In this paper, we present an exploratory analysis on the feasibility and effectiveness of using LLMs like GPT and Codex for configuration validation. Specifically, we take a first step to empirically evaluate LLMs as configuration validators without additional fine-tuning or code generation. We develop a generic LLM-based validation framework, named Ciri, which integrates different LLMs. Ciri devises effective prompt engineering with few-shot learning based on both valid configuration and misconfiguration data. Ciri also validates and aggregates the outputs of LLMs to generate validation results, coping with known hallucination and nondeterminism of LLMs. We evaluate the validation effectiveness of Ciri on five popular LLMs using configuration data of six mature, widely deployed open-source systems. Our analysis (1) confirms the potential of using LLMs for configuration validation, (2) understands the design space of LLMbased validators like Ciri, especially in terms of prompt engineering with few-shot learning, and (3) reveals open challenges such as ineffectiveness in detecting certain types of misconfigurations and biases to popular configuration parameters.
</details>
<details>
<summary>摘要</summary>
软件故障的主要原因是配置错误。现有的配置验证技术依赖于手动编写的规则或测试用例，实施和维护成本高，难以全面验证。使用机器学习（ML）和自然语言处理（NLP）进行配置验证是一个有前途的方向，但它面临着大规模配置数据和系统特有的特征和模型难以普适化的挑战。近年来，大型自然语言模型（LLMs）的进步表明可以解决一些长期存在的ML/NLP基于配置验证技术的局限性。在这篇论文中，我们提出了一种使用LLMs like GPT和Codex进行配置验证的探索性分析。 Specifically，我们不需要额外 fine-tuning或代码生成，就可以使用LLMs来验证配置。我们开发了一个通用的LLM-based validation框架，名为Ciri。Ciri使用几种LLMs，并开发了有效的提示工程学和少量学习技术，以适应不同的配置数据。Ciri还可以将LLMs的输出验证和聚合，以生成验证结果，并处理知道的投影和非决定性。我们对五种流行的LLMs进行了配置数据的六种广泛部署的开源系统的验证。我们的分析表明：（1）使用LLMs进行配置验证是有潜力的；（2）LLM-based validator如Ciri在提示工程学和少量学习方面存在设计空间，特别是在针对有效配置和错误配置数据进行少量学习；（3）存在一些未解决的挑战，例如对某些类型的配置错误不够有效。
</details></li>
</ul>
<hr>
<h2 id="A-Partially-Supervised-Reinforcement-Learning-Framework-for-Visual-Active-Search"><a href="#A-Partially-Supervised-Reinforcement-Learning-Framework-for-Visual-Active-Search" class="headerlink" title="A Partially Supervised Reinforcement Learning Framework for Visual Active Search"></a>A Partially Supervised Reinforcement Learning Framework for Visual Active Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09689">http://arxiv.org/abs/2310.09689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anindyasarkariith/psrl_vas">https://github.com/anindyasarkariith/psrl_vas</a></li>
<li>paper_authors: Anindya Sarkar, Nathan Jacobs, Yevgeniy Vorobeychik</li>
<li>for: 这篇论文旨在提出一个名为“视觉活搜”（Visual Active Search，VAS）的框架，用于使用视觉讯号来引导探索，以找到大地ospatial空间中的区域兴趣。</li>
<li>methods: 这篇论文使用了深度强化学习（Deep Reinforcement Learning，DRL）和传统的活搜搜寻（Active Search）两种方法。</li>
<li>results: 论文的实验结果显示，该方法可以对现有的DRL框架进行改进，并且在多个问题领域中表现出色。<details>
<summary>Abstract</summary>
Visual active search (VAS) has been proposed as a modeling framework in which visual cues are used to guide exploration, with the goal of identifying regions of interest in a large geospatial area. Its potential applications include identifying hot spots of rare wildlife poaching activity, search-and-rescue scenarios, identifying illegal trafficking of weapons, drugs, or people, and many others. State of the art approaches to VAS include applications of deep reinforcement learning (DRL), which yield end-to-end search policies, and traditional active search, which combines predictions with custom algorithmic approaches. While the DRL framework has been shown to greatly outperform traditional active search in such domains, its end-to-end nature does not make full use of supervised information attained either during training, or during actual search, a significant limitation if search tasks differ significantly from those in the training distribution. We propose an approach that combines the strength of both DRL and conventional active search by decomposing the search policy into a prediction module, which produces a geospatial distribution of regions of interest based on task embedding and search history, and a search module, which takes the predictions and search history as input and outputs the search distribution. We develop a novel meta-learning approach for jointly learning the resulting combined policy that can make effective use of supervised information obtained both at training and decision time. Our extensive experiments demonstrate that the proposed representation and meta-learning frameworks significantly outperform state of the art in visual active search on several problem domains.
</details>
<details>
<summary>摘要</summary>
视觉活动搜索（VAS）被提出作为模型框架，使用视觉提示导航，以找到大型地ospatial领域中的区域兴趣点。其潜在应用包括珍稀野生动物贩卖活动热点检测、搜救找寻、武器贸易毒品人贩卖等。现状最佳实践方法包括应用深度强化学习（DRL），得到综合搜索策略，以及传统的活动搜索，将预测与自定义算法策略结合。而DRL框架在这些领域中已经大幅超越传统的活动搜索，但其端到端的结构不能充分利用在训练和决策过程中获得的指导信息。我们提议一种将DRL和传统的活动搜索结合在一起的方法，将搜索策略 decomposes为预测模块和搜索模块。预测模块根据任务嵌入和搜索历史生成地ospatial领域中的区域兴趣点，搜索模块将预测和搜索历史作为输入，输出搜索分布。我们开发了一种新的元学习方法，用于同时学习结果的结合策略，以便在训练和决策过程中有效地利用获得的指导信息。我们的广泛实验表明，我们的表示和元学习框架在多个问题领域中具有显著超越现状的性能。
</details></li>
</ul>
<hr>
<h2 id="Recursively-Constrained-Partially-Observable-Markov-Decision-Processes"><a href="#Recursively-Constrained-Partially-Observable-Markov-Decision-Processes" class="headerlink" title="Recursively-Constrained Partially Observable Markov Decision Processes"></a>Recursively-Constrained Partially Observable Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09688">http://arxiv.org/abs/2310.09688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Heng Ho, Tyler Becker, Ben Kraske, Zakariya Laouar, Martin Feather, Federico Rossi, Morteza Lahijanian, Zachary N. Sunberg</li>
<li>for: 本研究旨在解决受到转移不确定性和部分可见性限制的优化目标函数问题。</li>
<li>methods: 本研究使用了受到约束的部分 observable Markov Decision Process (C-POMDP) 模型，并提出了一种新的形式ulation，即 Recursively-Constrained POMDP (RC-POMDP)，以解决优化目标函数问题中的缺陷。</li>
<li>results: 研究发现，对于 C-POMDPs，优化策略可能会违反贝尔曼的优化原则，导致不良行为。而 RC-POMDPs 中的优化策略总是具有确定性，并且遵循贝尔曼的优化原则。研究还提出了一种基于点的动态计划算法，可以Synthesize RC-POMDPs 中的优化策略。在一系列 benchmark 问题中，研究发现 RC-POMDPs 中的策略比 C-POMDPs 中的策略更为愉悦，并且 demonstrate 了算法的可靠性。<details>
<summary>Abstract</summary>
In many problems, it is desirable to optimize an objective function while imposing constraints on some other aspect of the problem. A Constrained Partially Observable Markov Decision Process (C-POMDP) allows modelling of such problems while subject to transition uncertainty and partial observability. Typically, the constraints in C-POMDPs enforce a threshold on expected cumulative costs starting from an initial state distribution. In this work, we first show that optimal C-POMDP policies may violate Bellman's principle of optimality and thus may exhibit pathological behaviors, which can be undesirable for many applications. To address this drawback, we introduce a new formulation, the Recursively-Constrained POMDP (RC-POMDP), that imposes additional history dependent cost constraints on the C-POMDP. We show that, unlike C-POMDPs, RC-POMDPs always have deterministic optimal policies, and that optimal policies obey Bellman's principle of optimality. We also present a point-based dynamic programming algorithm that synthesizes optimal policies for RC-POMDPs. In our evaluations, we show that policies for RC-POMDPs produce more desirable behavior than policies for C-POMDPs and demonstrate the efficacy of our algorithm across a set of benchmark problems.
</details>
<details>
<summary>摘要</summary>
很多问题中，您希望优化一个目标函数，同时对另一个问题进行约束。一个受过чай�odel Markov决策过程（C-POMDP）可以模拟这些问题，同时受到过程不确定和部分可见性的影响。通常，C-POMDPs 中的约束都是对起始状态分布的预期总成本下的阈值。在这项工作中，我们首先表明了C-POMDP 的优化策略可能会违反 Bellman 的优化原理，从而导致不良行为，这可能对许多应用程序不符合预期。为解决这个缺点，我们引入了一种新的形式，即循环约束 POMDP（RC-POMDP），该形式在 C-POMDP 中添加了历史висимые成本约束。我们表明了，不同于 C-POMDPs，RC-POMDPs 的优化策略总是具有确定性，并且优化策略都遵循 Bellman 的优化原理。我们还提出了一种基于点的动态Programming算法，该算法可以Synthesize RC-POMDPs 中的优化策略。在我们的评估中，我们发现RC-POMDPs 中的策略产生了更加愿意的行为，并且我们的算法在一组标准问题上进行了评估， demonstrate了其效果。
</details></li>
</ul>
<hr>
<h2 id="Generative-artificial-intelligence-for-de-novo-protein-design"><a href="#Generative-artificial-intelligence-for-de-novo-protein-design" class="headerlink" title="Generative artificial intelligence for de novo protein design"></a>Generative artificial intelligence for de novo protein design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09685">http://arxiv.org/abs/2310.09685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Winnifrith, Carlos Outeiral, Brian Hie</li>
<li>for: 这些论文的目的是探讨人工智能在蛋白质设计中的应用，以扩展我们对蛋白质的工程能力。</li>
<li>methods: 这些论文使用了生成型架构，如语言模型和扩散过程，生成 novel yet realistic 的蛋白质，以实现预先定义的功能和性能。</li>
<li>results: 现代设计协议的实验成功率已经接近 20%，从而扩大了蛋白质设计的可能性。  despite extensive progress, there are still challenges in the field, such as determining the best in silico metrics to prioritize designs for experimental testing, and designing proteins that can undergo large conformational changes or be regulated by post-translational modifications and other cellular processes.<details>
<summary>Abstract</summary>
Engineering new molecules with desirable functions and properties has the potential to extend our ability to engineer proteins beyond what nature has so far evolved. Advances in the so-called "de novo" design problem have recently been brought forward by developments in artificial intelligence. Generative architectures, such as language models and diffusion processes, seem adept at generating novel, yet realistic proteins that display desirable properties and perform specified functions. State-of-the-art design protocols now achieve experimental success rates nearing 20%, thus widening the access to de novo designed proteins. Despite extensive progress, there are clear field-wide challenges, for example in determining the best in silico metrics to prioritise designs for experimental testing, and in designing proteins that can undergo large conformational changes or be regulated by post-translational modifications and other cellular processes. With an increase in the number of models being developed, this review provides a framework to understand how these tools fit into the overall process of de novo protein design. Throughout, we highlight the power of incorporating biochemical knowledge to improve performance and interpretability.
</details>
<details>
<summary>摘要</summary>
engineer新分子 possessing desired functions and properties has the potential to extend our ability to engineer proteins beyond what nature has so far evolved. Advances in the so-called "de novo" design problem have recently been brought forward by developments in artificial intelligence. Generative architectures, such as language models and diffusion processes, seem adept at generating novel, yet realistic proteins that display desirable properties and perform specified functions. State-of-the-art design protocols now achieve experimental success rates nearing 20%, thus widening the access to de novo designed proteins. Despite extensive progress, there are clear field-wide challenges, for example in determining the best in silico metrics to prioritize designs for experimental testing, and in designing proteins that can undergo large conformational changes or be regulated by post-translational modifications and other cellular processes. With an increase in the number of models being developed, this review provides a framework to understand how these tools fit into the overall process of de novo protein design. Throughout, we highlight the power of incorporating biochemical knowledge to improve performance and interpretability.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/cs.AI_2023_10_15/" data-id="clpxp6bx2005jee888w6w9lr0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/15/cs.CL_2023_10_15/" class="article-date">
  <time datetime="2023-10-15T11:00:00.000Z" itemprop="datePublished">2023-10-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/15/cs.CL_2023_10_15/">cs.CL - 2023-10-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="UvA-MT’s-Participation-in-the-WMT23-General-Translation-Shared-Task"><a href="#UvA-MT’s-Participation-in-the-WMT23-General-Translation-Shared-Task" class="headerlink" title="UvA-MT’s Participation in the WMT23 General Translation Shared Task"></a>UvA-MT’s Participation in the WMT23 General Translation Shared Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09946">http://arxiv.org/abs/2310.09946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Wu, Shaomu Tan, David Stap, Ali Araabi, Christof Monz</li>
<li>for: 这个研究报告描述了阿姆斯特丹大学的自然语言处理实验室（UvA-MT）在2023年世界机器翻译大会（WMT）共享任务中的参加。他们在英文&lt;-&gt;希伯来两个方向的受限Track中参加竞赛，并显示了使用一个模型处理对向任务时，可以达到相似的结果，比较 Traditional的双语翻译。</li>
<li>methods: 这个研究使用了一些有效的策略，如回 перевод、重定义的嵌入表格和任务导向的精细调整，以提高自动评估中的最终结果。</li>
<li>results: 在自动评估中，他们在英文-&gt;希伯来和希伯来-&gt;英文两个方向中都获得了竞争性的结果。<details>
<summary>Abstract</summary>
This paper describes the UvA-MT's submission to the WMT 2023 shared task on general machine translation. We participate in the constrained track in two directions: English <-> Hebrew. In this competition, we show that by using one model to handle bidirectional tasks, as a minimal setting of Multilingual Machine Translation (MMT), it is possible to achieve comparable results with that of traditional bilingual translation for both directions. By including effective strategies, like back-translation, re-parameterized embedding table, and task-oriented fine-tuning, we obtained competitive final results in the automatic evaluation for both English -> Hebrew and Hebrew -> English directions.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese as follows:这篇论文描述了UvA-MT在WMT 2023共同任务中的提交，我们在Constrained Track中参加了英文 <-> 希伯来两个方向的翻译。在这次竞赛中，我们表明，通过使用一个模型处理双向任务，作为多语言翻译的最小设置（MMT），可以达到相同的结果。通过包括有效策略，如回译、重新参数表示表 и任务导向精度调整，我们在自动评估中获得了对 beiden方向的竞争性最终结果。
</details></li>
</ul>
<hr>
<h2 id="FiLM-Fill-in-Language-Models-for-Any-Order-Generation"><a href="#FiLM-Fill-in-Language-Models-for-Any-Order-Generation" class="headerlink" title="FiLM: Fill-in Language Models for Any-Order Generation"></a>FiLM: Fill-in Language Models for Any-Order Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09930">http://arxiv.org/abs/2310.09930</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shentianxiao/film">https://github.com/shentianxiao/film</a></li>
<li>paper_authors: Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, Yejin Choi</li>
<li>for: 填充语言模型 (Fill-in Language Model, FiLM) 的目的是提供一种可以在任意位置进行灵活生成的语言模型，以便在填充文本中使用双向文本上下文。</li>
<li>methods: FiLM 使用了一种新的语言模型方法，即采用 beta 分布中的变化掩码概率来提高 FiLM 的生成能力。在推理过程中，FiLM 可以顺利地插入缺失的句子、段落或整个文本，以确保输出的文本流畅、与周围上下文一致。</li>
<li>results: 在自动和人工评估中，FiLM 表现出色，超过了基于左到右语言模型的填充方法。FiLM 可以轻松地在不同的文本长度和难度水平上进行调整，并且可以在不同的语言模型大小上进行训练和 fine-tuning。<details>
<summary>Abstract</summary>
Language models have become the backbone of today's AI systems. However, their predominant left-to-right generation limits the use of bidirectional context, which is essential for tasks that involve filling text in the middle. We propose the Fill-in Language Model (FiLM), a new language modeling approach that allows for flexible generation at any position without adhering to a specific generation order. Its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities of FiLM. During inference, FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring that the outputs are fluent and are coherent with the surrounding context. In both automatic and human evaluations, FiLM outperforms existing infilling methods that rely on left-to-right language models trained on rearranged text segments. FiLM is easy to implement and can be either trained from scratch or fine-tuned from a left-to-right language model. Notably, as the model size grows, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes, indicating FiLM's scalability and potential as a large language model.
</details>
<details>
<summary>摘要</summary>
现代人工智能系统中，语言模型已成为背景模型。然而，这些主要左往右生成的语言模型限制了使用对向文本填充的 bidirectional 上下文，这是装备填充文本的任务中非常重要。我们提出了填充语言模型（FiLM），一种新的语言模型化方法，可以在任何位置进行 flexible 生成，不受特定生成顺序的限制。它的训练将推广遮盾语言模型的对话预设，透过对应排版的 beta 分布来增强FiLM的生成能力。在推断中，FiLM可以顺利地插入缺失的句子、句末或段落，以确保输出的流畅和与周围上下文一致。在自动和人工评估中，FiLM比靠左往右的语言模型训练在重新排序的文本段落上的填充方法表现出色，并且可以轻松地从头部训练或精革左往右语言模型。值得一提的是，当模型的大小增加时，FiLM的误差接近强左往右语言模型相似大小的误差，这表明FiLM在大型模型中的可扩展性和潜力。
</details></li>
</ul>
<hr>
<h2 id="Prompting-Scientific-Names-for-Zero-Shot-Species-Recognition"><a href="#Prompting-Scientific-Names-for-Zero-Shot-Species-Recognition" class="headerlink" title="Prompting Scientific Names for Zero-Shot Species Recognition"></a>Prompting Scientific Names for Zero-Shot Species Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09929">http://arxiv.org/abs/2310.09929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Parashar, Zhiqiu Lin, Yanan Li, Shu Kong</li>
<li>for: 本研究旨在使用CLIP进行零shot认知高级生物物种，包括鸟类、植物和动物的species recognition。</li>
<li>methods: 本研究使用CLIP进行零shot认知，并使用大语言模型（LLM）生成描述（例如物种颜色和形状）以提高性能。</li>
<li>results: 研究发现，使用common名称（例如mountain hare）而不是学名（例如Lepus Timidus）在prompt中可以提高CLIP的认知精度，并且可以达到2∼5倍的提升。<details>
<summary>Abstract</summary>
Trained on web-scale image-text pairs, Vision-Language Models (VLMs) such as CLIP can recognize images of common objects in a zero-shot fashion. However, it is underexplored how to use CLIP for zero-shot recognition of highly specialized concepts, e.g., species of birds, plants, and animals, for which their scientific names are written in Latin or Greek. Indeed, CLIP performs poorly for zero-shot species recognition with prompts that use scientific names, e.g., "a photo of Lepus Timidus" (which is a scientific name in Latin). Because these names are usually not included in CLIP's training set. To improve performance, prior works propose to use large-language models (LLMs) to generate descriptions (e.g., of species color and shape) and additionally use them in prompts. We find that they bring only marginal gains. Differently, we are motivated to translate scientific names (e.g., Lepus Timidus) to common English names (e.g., mountain hare) and use such in the prompts. We find that common names are more likely to be included in CLIP's training set, and prompting them achieves 2$\sim$5 times higher accuracy on benchmarking datasets of fine-grained species recognition.
</details>
<details>
<summary>摘要</summary>
<SYS>使用 web 级别的图片文本对，视觉语言模型（VLM）如 CLIP 可以不经过训练就识别通用对象的图片。但是，对于高度专业化的概念，如鸟类、植物和动物的种类，它们的科学名称通常是拉丁文或希腊文。CLIP 在无需训练的情况下识别这些种类的图片表现不佳，因为这些名称没有包含在 CLIP 的训练集中。以前的研究提议使用大型自然语言模型（LLM）生成描述（例如，种类颜色和形状），并将其添加到提示中。我们发现它们只提供了有限的改进。与此不同，我们强调将科学名称翻译成通用英文名称（例如，山兔），并使用这些名称作为提示。我们发现这样可以提高 CLIP 的准确率，在benchmarking数据集上实现2-5倍的提高。</SYS>Here's the translation in Traditional Chinese as well:<SYS>使用 web 级别的图片文本对，视觉语言模型（VLM）如 CLIP 可以不经过训练就识别通用对象的图片。但是，对于高度专业化的概念，如鸟类、植物和动物的种类，它们的科学名称通常是拉丁文或希腊文。CLIP 在无需训练的情况下识别这些种类的图片表现不佳，因为这些名称没有包含在 CLIP 的训练集中。以前的研究提议使用大型自然语言模型（LLM）生成描述（例如，种类颜色和形状），并将其添加到提示中。我们发现它们只提供了有限的改进。与此不同，我们强调将科学名称翻译成通用英文名称（例如，山兔），并使用这些名称作为提示。我们发现这样可以提高 CLIP 的准确率，在benchmarking数据集上实现2-5倍的提高。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Empirical-study-of-pretrained-multilingual-language-models-for-zero-shot-cross-lingual-generation"><a href="#Empirical-study-of-pretrained-multilingual-language-models-for-zero-shot-cross-lingual-generation" class="headerlink" title="Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation"></a>Empirical study of pretrained multilingual language models for zero-shot cross-lingual generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09917">http://arxiv.org/abs/2310.09917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadezhda Chirkova, Sheng Liang, Vassilina Nikoulina</li>
<li>for: 这个论文旨在研究零shot cross-语言生成技术，即使finetuning多语言预训练语言模型（mPLM）在一种语言上的一个生成任务，然后用其来预测这个任务在其他语言上的结果。</li>
<li>methods: 这篇论文测试了一些替代的mPLM模型，包括mBART和NLLB，并考虑了全 Parameters 的 fine-tuning 和 parameter-efficient fine-tuning with adapters。</li>
<li>results: 研究发现，mBART with adapters 与 mT5 相似，NLLB 可以在一些情况下与 mT5 竞争。 此外，研究发现训练学习率对 fine-tuning 的调整可以减轻生成错误语言的问题。<details>
<summary>Abstract</summary>
Zero-shot cross-lingual generation assumes finetuning the multilingual pretrained language model (mPLM) on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work, we test alternative mPLMs, such as mBART and NLLB, considering full finetuning and parameter-efficient finetuning with adapters. We find that mBART with adapters performs similarly to mT5 of the same size, and NLLB can be competitive in some cases. We also underline the importance of tuning learning rate used for finetuning, which helps to alleviate the problem of generation in the wrong language.
</details>
<details>
<summary>摘要</summary>
zero-shot 跨语言生成假设通过质量化多语言预训练语言模型（mPLM）的 Fine-tuning 进行一种语言的生成任务，然后用其来预测这个任务的其他语言。  previous works 发现生成 incorrect language 的问题，并提出了解决方案，通常使用 mT5 作为基础模型。 在这个工作中，我们测试了不同的 mPLM，如 mBART 和 NLLB，包括全部 Fine-tuning 和参数有效的 Fine-tuning  WITH 适配器。我们发现 mBART  WITH 适配器 和 mT5 的同等大小下表现相似，而 NLLB 在一些情况下可以达到竞争水平。我们还强调了在 Fine-tuning 中调整学习率的重要性，可以减轻生成 incorrect language 的问题。
</details></li>
</ul>
<hr>
<h2 id="Can-GPT-4V-ision-Serve-Medical-Applications-Case-Studies-on-GPT-4V-for-Multimodal-Medical-Diagnosis"><a href="#Can-GPT-4V-ision-Serve-Medical-Applications-Case-Studies-on-GPT-4V-for-Multimodal-Medical-Diagnosis" class="headerlink" title="Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis"></a>Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09909">http://arxiv.org/abs/2310.09909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaoyi-wu/gpt-4v_medical_evaluation">https://github.com/chaoyi-wu/gpt-4v_medical_evaluation</a></li>
<li>paper_authors: Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, Weidi Xie</li>
<li>for: This paper assesses the performance of OpenAI’s GPT-4V model in multimodal medical diagnosis, evaluating its ability to distinguish between medical image modalities and anatomy, as well as its ability to generate comprehensive reports.</li>
<li>methods: The evaluation uses 17 human body systems and 8 modalities of medical images, with or without patent history provided, to probe the GPT-4V’s ability on multiple clinical tasks such as imaging modality and anatomy recognition, disease diagnosis, and report generation.</li>
<li>results: The study finds that while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports, highlighting the limitations of large multimodal models in supporting real-world medical applications and clinical decision-making.Here are the three key points in Simplified Chinese:</li>
<li>for: 这项研究用于评估OpenAI的GPT-4V模型在多模态医学诊断中的表现，包括分辨医疗影像模式和解剖结构等能力。</li>
<li>methods: 这项评估使用17个人体系统和8种医疗影像模式，有或无患者历史提供，以探索GPT-4V在多种临床任务上的能力，包括影像模式和解剖结构识别、疾病诊断、报告生成等。</li>
<li>results: 研究发现，虽然GPT-4V在分辨医疗影像模式和解剖结构方面表现出色，但在疾病诊断和生成全面报告方面受到了重大挑战，表明大型多模态模型在实际医疗应用和临床决策中仍有很大的发展空间。<details>
<summary>Abstract</summary>
Driven by the large foundation models, the development of artificial intelligence has witnessed tremendous progress lately, leading to a surge of general interest from the public. In this study, we aim to assess the performance of OpenAI's newest model, GPT-4V(ision), specifically in the realm of multimodal medical diagnosis. Our evaluation encompasses 17 human body systems, including Central Nervous System, Head and Neck, Cardiac, Chest, Hematology, Hepatobiliary, Gastrointestinal, Urogenital, Gynecology, Obstetrics, Breast, Musculoskeletal, Spine, Vascular, Oncology, Trauma, Pediatrics, with images taken from 8 modalities used in daily clinic routine, e.g., X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Digital Subtraction Angiography (DSA), Mammography, Ultrasound, and Pathology. We probe the GPT-4V's ability on multiple clinical tasks with or without patent history provided, including imaging modality and anatomy recognition, disease diagnosis, report generation, disease localisation.   Our observation shows that, while GPT-4V demonstrates proficiency in distinguishing between medical image modalities and anatomy, it faces significant challenges in disease diagnosis and generating comprehensive reports. These findings underscore that while large multimodal models have made significant advancements in computer vision and natural language processing, it remains far from being used to effectively support real-world medical applications and clinical decision-making.   All images used in this report can be found in https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
</details>
<details>
<summary>摘要</summary>
由大型基础模型驱动，人工智能的发展最近几年有了很大的进步，引起了公众的广泛关注。在这项研究中，我们想要评估OpenAI的最新模型GPT-4V（视觉）在多modal医学诊断方面的表现。我们的评估覆盖了17个人体系统，包括中枢神经系统、头颈部、心脏、胸部、血液系统、肝胆系统、肠道系统、尿道系统、妇科、儿科、骨骼系统、脊梁系统、血管系统、肿瘤系统、护理、外伤等，图像来自日常临床 Routine的8种模式，例如X射线、计算tomography（CT）、核磁共振成像（MRI）、 позитрон发射tomography（PET）、数字抽取ANGIOGRAPHY（DSA）、胸部X射线、计算tomography（CT）、ultrasound和pathology。我们 probing GPT-4V的能力在多种临床任务上，包括图像模式和解剖学识别、疾病诊断、报告生成、疾病Localization。我们的观察表明，GPT-4V能够Distinguish between different medical imaging modalities and anatomy, but it faces significant challenges in disease diagnosis and report generation. These findings highlight that while large multimodal models have made significant advancements in computer vision and natural language processing, they are still far from being used to effectively support real-world medical applications and clinical decision-making.所有图像使用在这项报告中可以在GitHub上找到：https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation。
</details></li>
</ul>
<hr>
<h2 id="Reformulating-NLP-tasks-to-Capture-Longitudinal-Manifestation-of-Language-Disorders-in-People-with-Dementia"><a href="#Reformulating-NLP-tasks-to-Capture-Longitudinal-Manifestation-of-Language-Disorders-in-People-with-Dementia" class="headerlink" title="Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia"></a>Reformulating NLP tasks to Capture Longitudinal Manifestation of Language Disorders in People with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09897">http://arxiv.org/abs/2310.09897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Gkoumas, Matthew Purver, Maria Liakata</li>
<li>for: 这个研究是为了 automatization 语言障碍模式，以便更好地识别和评估词语障碍。</li>
<li>methods: 这个研究使用了一个已经训练过的自然语言处理（NLP）模型，并对其进行了修改，以便在NLP任务中强制使用语言模式。然后，他们使用了这些任务的概率估计来构建数字语言标记，用于评估语言交流质量和语言障碍的严重程度。</li>
<li>results: 研究发现，提出的语言标记能够准确地识别患有 деменция 的人的语言障碍，并且与临床标记呈正相关。此外，这些语言标记还提供了词语障碍的可观察性和恰当性，可以用于评估词语障碍的进程。<details>
<summary>Abstract</summary>
Dementia is associated with language disorders which impede communication. Here, we automatically learn linguistic disorder patterns by making use of a moderately-sized pre-trained language model and forcing it to focus on reformulated natural language processing (NLP) tasks and associated linguistic patterns. Our experiments show that NLP tasks that encapsulate contextual information and enhance the gradient signal with linguistic patterns benefit performance. We then use the probability estimates from the best model to construct digital linguistic markers measuring the overall quality in communication and the intensity of a variety of language disorders. We investigate how the digital markers characterize dementia speech from a longitudinal perspective. We find that our proposed communication marker is able to robustly and reliably characterize the language of people with dementia, outperforming existing linguistic approaches; and shows external validity via significant correlation with clinical markers of behaviour. Finally, our proposed linguistic disorder markers provide useful insights into gradual language impairment associated with disease progression.
</details>
<details>
<summary>摘要</summary>
偏僻症与语言障碍有关，我们通过自动学习受控语言模型，训练其专注于修改后NLP任务和相关的语言模式。我们的实验显示，包含语言上下文信息并在语言模式中增强梯度信号的NLP任务可以提高表现。然后，我们使用最佳模型的概率估计来构建数字语言标记，评估整体沟通质量和语言障碍的严重程度。我们研究如何使用我们的提议的沟通标记来 caracterize dementia speech的长期趋势。我们发现，我们的提议的语言障碍标记能够坚定可靠地 caracterize人们患有偏僻症的语言，高于现有的语言方法；并与临床标记相关。最后，我们的语言障碍标记提供了有用的透视 gradual language impairment与疾病进程相关的语言障碍。
</details></li>
</ul>
<hr>
<h2 id="Bounding-and-Filling-A-Fast-and-Flexible-Framework-for-Image-Captioning"><a href="#Bounding-and-Filling-A-Fast-and-Flexible-Framework-for-Image-Captioning" class="headerlink" title="Bounding and Filling: A Fast and Flexible Framework for Image Captioning"></a>Bounding and Filling: A Fast and Flexible Framework for Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09876">http://arxiv.org/abs/2310.09876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changxinwang/boficap">https://github.com/changxinwang/boficap</a></li>
<li>paper_authors: Zheng Ma, Changxin Wang, Bo Huang, Zixuan Zhu, Jianbing Zhang</li>
<li>for: 这篇论文目的是提出一种快速和灵活的图像描述模型，以解决现有的描述模型具有 significiant inference latency 问题。</li>
<li>methods: 该模型使用 bounding 和 filling 技术，将图像分割成多个区域，然后采用 two-generation 方式填充每个区域。</li>
<li>results: 该模型在 MS-COCO 测试集上取得了状态的最佳性能（CIDEr 125.6），并且比基eline模型快速 9.22 倍；在半循环的情况下，该模型达到了 128.4 的 CIDEr 性能，并且速度比基eline模型快速 3.69 倍。<details>
<summary>Abstract</summary>
Most image captioning models following an autoregressive manner suffer from significant inference latency. Several models adopted a non-autoregressive manner to speed up the process. However, the vanilla non-autoregressive manner results in subpar performance, since it generates all words simultaneously, which fails to capture the relationships between words in a description. The semi-autoregressive manner employs a partially parallel method to preserve performance, but it sacrifices inference speed. In this paper, we introduce a fast and flexible framework for image captioning called BoFiCap based on bounding and filling techniques. The BoFiCap model leverages the inherent characteristics of image captioning tasks to pre-define bounding boxes for image regions and their relationships. Subsequently, the BoFiCap model fills corresponding words in each box using two-generation manners. Leveraging the box hints, our filling process allows each word to better perceive other words. Additionally, our model offers flexible image description generation: 1) by employing different generation manners based on speed or performance requirements, 2) producing varied sentences based on user-specified boxes. Experimental evaluations on the MS-COCO benchmark dataset demonstrate that our framework in a non-autoregressive manner achieves the state-of-the-art on task-specific metric CIDEr (125.6) while speeding up 9.22x than the baseline model with an autoregressive manner; in a semi-autoregressive manner, our method reaches 128.4 on CIDEr while a 3.69x speedup. Our code and data is available at https://github.com/ChangxinWang/BoFiCap.
</details>
<details>
<summary>摘要</summary>
大多数图像描述模型采用回归方式，却受到显著的推理延迟。一些模型采用非回归方式以加速过程，但这会导致性能下降，因为它们同时生成所有 слова，无法捕捉图像描述中 слова之间的关系。半回归方式使用部分并行方法保持性能，但是它们牺牲推理速度。本文提出一种快速和灵活的图像描述模型called BoFiCap，基于缓存和填充技术。BoFiCap模型利用图像描述任务的特点，先定义图像区域的缓存框，然后使用两种生成方式填充对应的字。利用框提示，我们的填充过程让每个字etter perceive其他字。此外，我们的模型提供了自适应的图像描述生成：1）根据速度或性能要求使用不同的生成方式，2）生成基于用户指定的盒子的多种句子。在COCO数据集上的实验评估 demonstrate了我们的框架在非回归方式下达到了状态之arte（CIDEr=125.6），同时速度比基eline模型（具有回归方式）快9.22倍。在半回归方式下，我们的方法达到了128.4的CIDEr，速度比基eline模型快3.69倍。我们的代码和数据可以在https://github.com/ChangxinWang/BoFiCap上获取。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Stance-Classification-with-Quantified-Moral-Foundations"><a href="#Enhancing-Stance-Classification-with-Quantified-Moral-Foundations" class="headerlink" title="Enhancing Stance Classification with Quantified Moral Foundations"></a>Enhancing Stance Classification with Quantified Moral Foundations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09848">http://arxiv.org/abs/2310.09848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Zhang, Prasanta Bhattacharya, Wei Gao, Liang Ze Wong, Brandon Siyuan Loh, Joseph J. P. Simons, Jisun An</li>
<li>for: 这 paper 的目的是增强社交媒体上的立场检测，通过 incorporating deeper psychological attributes，特别是个人的道德基础。</li>
<li>methods: 这 paper 使用的方法包括EXTRACTING moral foundation features from text, 以及 message semantic features，来 классифика stance 在 message- 和 user-levels 上。</li>
<li>results:  Preliminary results 表明， encoding moral foundations 可以提高 stance detection 任务的性能，并帮助描述特定道德基础和 online stance 之间的关系。  results highlight the importance of considering deeper psychological attributes in stance analysis and underscores the role of moral foundations in guiding online social behavior.<details>
<summary>Abstract</summary>
This study enhances stance detection on social media by incorporating deeper psychological attributes, specifically individuals' moral foundations. These theoretically-derived dimensions aim to provide a comprehensive profile of an individual's moral concerns which, in recent work, has been linked to behaviour in a range of domains, including society, politics, health, and the environment. In this paper, we investigate how moral foundation dimensions can contribute to predicting an individual's stance on a given target. Specifically we incorporate moral foundation features extracted from text, along with message semantic features, to classify stances at both message- and user-levels across a range of targets and models. Our preliminary results suggest that encoding moral foundations can enhance the performance of stance detection tasks and help illuminate the associations between specific moral foundations and online stances on target topics. The results highlight the importance of considering deeper psychological attributes in stance analysis and underscores the role of moral foundations in guiding online social behavior.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Merging-Experts-into-One-Improving-Computational-Efficiency-of-Mixture-of-Experts"><a href="#Merging-Experts-into-One-Improving-Computational-Efficiency-of-Mixture-of-Experts" class="headerlink" title="Merging Experts into One: Improving Computational Efficiency of Mixture of Experts"></a>Merging Experts into One: Improving Computational Efficiency of Mixture of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09832">http://arxiv.org/abs/2310.09832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shwai-he/meo">https://github.com/shwai-he/meo</a></li>
<li>paper_authors: Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, Dacheng Tao</li>
<li>for: 提高语言模型的大小通常会导致NLPTasks的进步，但是会增加计算成本。零含量的混合专家（MoE）可以减少计算成本，但是如果增加激活专家的数量，计算成本会增加很快，限制实际应用。本文提出一种名为\textbf{\texttt{Merging Experts into One}（MEO）的计算效率的方法，可以保持增加专家的优点而不导致计算成本增加。</li>
<li>methods: 我们首先证明选择多个专家的优势，然后提出一种计算效率的方法，即\textbf{\texttt{Merging Experts into One}（MEO），可以将计算成本降低到单个专家的水平。此外，我们还提出了一种符号级注意块，可以进一步提高MEO的效率和表现。</li>
<li>results: 我们进行了广泛的实验，显示MEO可以减少计算成本，例如FLOPS从72.0G下降到28.6G（MEO）。此外，我们还提出了一种符号级注意块，可以进一步提高MEO的效率和表现。例如，在GLUE benchmark上，MEO的平均分数为83.3%，而vanilla MoE的平均分数为82.6%。<details>
<summary>Abstract</summary>
Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \textbf{\texttt{Merging Experts into One} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the efficiency and performance of token-level MEO, e.g., 83.3\% (MEO) vs. 82.6\% (vanilla MoE) average score on the GLUE benchmark. Our code will be released upon acceptance. Code will be released at: \url{https://github.com/Shwai-He/MEO}.
</details>
<details>
<summary>摘要</summary>
通常，将语言模型的大小扩展到可观的尺度会导致NLPTasks的显著进步。然而，这经常会带来计算成本的增加。虽然 sparse Mixture of Experts（MoE）可以降低计算成本，但是如果启用更多的专家，计算成本会快速增加，限制其实际应用。我们是否可以保留添加更多专家的优点而不导致计算成本增加很多？在这篇论文中，我们首先表明了多个专家的选择的优势，然后我们提出了一种 computation-efficient的方法called \textbf{\texttt{Merging Experts into One}（MEO），可以降低计算成本到单个专家的水平。我们进行了广泛的实验，发现 MEO 可以减少 FLOPS 的值，例如，从 vanilla MoE 的 72.0G 降低到 28.6G（MEO）。此外，我们还提出了一种循环预测块，可以进一步提高 MEO 的效率和性能，例如，在 GLUE 测试准则上，MEO 的平均分数为 83.3%，而 vanilla MoE 的平均分数为 82.6%。我们将代码发布在接受后。代码将发布在：\url{https://github.com/Shwai-He/MEO}.
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Reliability-of-Large-Language-Model-Knowledge"><a href="#Assessing-the-Reliability-of-Large-Language-Model-Knowledge" class="headerlink" title="Assessing the Reliability of Large Language Model Knowledge"></a>Assessing the Reliability of Large Language Model Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09820">http://arxiv.org/abs/2310.09820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng</li>
<li>for: 评估大语言模型（LLMs）的知识可靠性。</li>
<li>methods: 提出了一种名为 Model Knowledge Relibility Score (MONITOR) 的新度量方法，用于直接测试 LLMs 的事实可靠性。</li>
<li>results: 在一系列12种 LLMS 上进行了实验，并证明了 MONITOR 的效iveness 以及低计算成本。此外，还释放了一个名为 Factual Knowledge Test Corpus (FKTC) 的测试集，以便进一步研究。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been treated as knowledge bases due to their strong performance in knowledge probing tasks. LLMs are typically evaluated using accuracy, yet this metric does not capture the vulnerability of LLMs to hallucination-inducing factors like prompt and context variability. How do we evaluate the capabilities of LLMs to consistently produce factually correct answers? In this paper, we propose MOdel kNowledge relIabiliTy scORe (MONITOR), a novel metric designed to directly measure LLMs' factual reliability. MONITOR computes the distance between the probability distributions of a valid output and its counterparts produced by the same LLM probing the same fact using different styles of prompts and contexts.Experiments on a comprehensive range of 12 LLMs demonstrate the effectiveness of MONITOR in evaluating the factual reliability of LLMs while maintaining a low computational overhead. In addition, we release the FKTC (Factual Knowledge Test Corpus) test set, containing 210,158 prompts in total to foster research along this line (https://github.com/Vicky-Wil/MONITOR).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="RSVP-Customer-Intent-Detection-via-Agent-Response-Contrastive-and-Generative-Pre-Training"><a href="#RSVP-Customer-Intent-Detection-via-Agent-Response-Contrastive-and-Generative-Pre-Training" class="headerlink" title="RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training"></a>RSVP: Customer Intent Detection via Agent Response Contrastive and Generative Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09773">http://arxiv.org/abs/2310.09773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tommytyc/rsvp">https://github.com/tommytyc/rsvp</a></li>
<li>paper_authors: Yu-Chien Tang, Wei-Yao Wang, An-Zi Yen, Wen-Chih Peng</li>
<li>for: 提供用户task-oriented对话中的精准回答和24小时支持</li>
<li>methods: 利用神经网络模型检测客户意图 based on their utterances</li>
<li>results: 与状态空间的基eline比进行了比较，得到了4.95%的准确率提升，3.4%的MRR@3提升和2.75%的MRR@5提升的结果<details>
<summary>Abstract</summary>
The dialogue systems in customer services have been developed with neural models to provide users with precise answers and round-the-clock support in task-oriented conversations by detecting customer intents based on their utterances. Existing intent detection approaches have highly relied on adaptively pre-training language models with large-scale datasets, yet the predominant cost of data collection may hinder their superiority. In addition, they neglect the information within the conversational responses of the agents, which have a lower collection cost, but are significant to customer intent as agents must tailor their replies based on the customers' intent. In this paper, we propose RSVP, a self-supervised framework dedicated to task-oriented dialogues, which utilizes agent responses for pre-training in a two-stage manner. Specifically, we introduce two pre-training tasks to incorporate the relations of utterance-response pairs: 1) Response Retrieval by selecting a correct response from a batch of candidates, and 2) Response Generation by mimicking agents to generate the response to a given utterance. Our benchmark results for two real-world customer service datasets show that RSVP significantly outperforms the state-of-the-art baselines by 4.95% for accuracy, 3.4% for MRR@3, and 2.75% for MRR@5 on average. Extensive case studies are investigated to show the validity of incorporating agent responses into the pre-training stage.
</details>
<details>
<summary>摘要</summary>
Dialogue 系统在客户服务中已经采用神经网络模型，以提供用户精准的答案和24小时的支持，通过检测客户意图基于他们的谈话来进行任务化对话。现有的意图检测方法强调适应性地预训练语言模型，但这可能增加成本。此外，它们忽略了代理人回复的信息，尽管这些信息在客户意图方面具有重要性，因为代理人必须根据客户的意图修改他们的回复。在本文中，我们提出了 RSVP，一个自动预训练框架，专门用于任务化对话。我们在两个阶段中使用代理人回复进行预训练：1）回复选择，选择一个正确的回复从批处理中的候选者中，2）回复生成，模仿代理人生成一个回复来回应给一个谈话。我们对两个实际的客户服务数据集进行了比较，结果显示，RSVP在精度、MRR@3和MRR@5等指标上平均高于状态之前的基eline by 4.95%、3.4%和2.75%。我们还进行了广泛的案例研究，以证明代理人回复的包含在预训练阶段是有效的。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Graph-Meaning-Representations-through-Decoupling-Contextual-Representation-Learning-and-Structural-Information-Propagation"><a href="#Revisiting-Graph-Meaning-Representations-through-Decoupling-Contextual-Representation-Learning-and-Structural-Information-Propagation" class="headerlink" title="Revisiting Graph Meaning Representations through Decoupling Contextual Representation Learning and Structural Information Propagation"></a>Revisiting Graph Meaning Representations through Decoupling Contextual Representation Learning and Structural Information Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09772">http://arxiv.org/abs/2310.09772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Zhou, Wenyu Chen, Dingyi Zeng, Hong Qu, Daniel Hershcovich</li>
<li>for: 本研究旨在探讨图意表示（GMRs）在关系EXTRACTION任务中的精确影响。</li>
<li>methods: 本研究提出了一种简单和参数效率高的神经网络架构，用于分离上下文表示学习和结构信息传递。</li>
<li>results: 研究结果表明，GMRs在四个英文和两个中文 dataset 中有所提高表达关系的性能，特别是英文dataset更加精确。然而，在文学领域dataset中，GMRs的效果较低。这些发现可以为将来关系EXTRACTION任务中的GMRs和 parser设计提供更好的指导。<details>
<summary>Abstract</summary>
In the field of natural language understanding, the intersection of neural models and graph meaning representations (GMRs) remains a compelling area of research. Despite the growing interest, a critical gap persists in understanding the exact influence of GMRs, particularly concerning relation extraction tasks. Addressing this, we introduce DAGNN-plus, a simple and parameter-efficient neural architecture designed to decouple contextual representation learning from structural information propagation. Coupled with various sequence encoders and GMRs, this architecture provides a foundation for systematic experimentation on two English and two Chinese datasets. Our empirical analysis utilizes four different graph formalisms and nine parsers. The results yield a nuanced understanding of GMRs, showing improvements in three out of the four datasets, particularly favoring English over Chinese due to highly accurate parsers. Interestingly, GMRs appear less effective in literary-domain datasets compared to general-domain datasets. These findings lay the groundwork for better-informed design of GMRs and parsers to improve relation classification, which is expected to tangibly impact the future trajectory of natural language understanding research.
</details>
<details>
<summary>摘要</summary>
在自然语言理解领域，神经网络和图意表示（GMR）的交叉研究仍然吸引着广泛的关注。尽管有增长的兴趣，但是关于GMR的具体影响仍然存在一个重要的知识 gap。为了解决这个问题，我们介绍了DAGNN-plus，一种简单而参数有效的神经网络架构，用于分离上下文表示学习和结构信息传递。与不同的序列编码器和GMR相结合，这个架构提供了对系统实验的基础，并在四种图形式和九个解析器的支持下进行了实验分析。我们的实验结果表明，GMR在英文和中文两个领域中的表现不同，特别是在文学领域比通用领域更具有优势。这些发现为将来改进GMR和解析器的设计，以提高关系类别的识别，这将对自然语言理解研究的未来轨迹产生直接的影响。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Aware-In-Context-Learning-for-Code-Generation"><a href="#Large-Language-Model-Aware-In-Context-Learning-for-Code-Generation" class="headerlink" title="Large Language Model-Aware In-Context Learning for Code Generation"></a>Large Language Model-Aware In-Context Learning for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09748">http://arxiv.org/abs/2310.09748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Li, Ge Li, Chongyang Tao, Jia Li, Huangzhao Zhang, Fang Liu, Zhi Jin</li>
<li>for: 这 paper 的目的是提出一种基于学习的选择方法，以提高 Code Generation 中 LLMS 的培养效果。</li>
<li>methods: 这 paper 使用了 LLMS 自身的生成概率来评估候选示例，然后通过对概率反馈来标注候选示例为正负。最后，通过带有对比学习目标的带有对比学习目标的导入，训练一个有效的检索器，以获得 LLMS 在 Code Generation 中的偏好。</li>
<li>results: 这 paper 的实验结果表明，LAIL 可以在 CodeGen 和 GPT-3.5 上提高 LLMS 的培养效果，相比之前的基eline 提高了11.58%、6.89%和5.07%，以及4.38%、2.85%和2.74%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown impressive in-context learning (ICL) ability in code generation. LLMs take a prompt consisting of requirement-code examples and a new requirement as input, and output new programs. Existing studies have found that ICL is highly dominated by the examples and thus arises research on example selection. However, existing approaches randomly select examples or only consider the textual similarity of requirements to retrieve, leading to sub-optimal performance. In this paper, we propose a novel learning-based selection approach named LAIL (LLM-Aware In-context Learning) for code generation. Given a candidate example, we exploit LLMs themselves to estimate it by considering the generation probabilities of ground-truth programs given a requirement and the example. We then label candidate examples as positive or negative through the probability feedback. Based on the labeled data, we import a contrastive learning objective to train an effective retriever that acquires the preference of LLMs in code generation. We apply LAIL to three LLMs and evaluate it on three representative datasets (e.g., MBJP, MBPP, and MBCPP). LATA outperforms the state-of-the-art baselines by 11.58%, 6.89%, and 5.07% on CodeGen, and 4.38%, 2.85%, and 2.74% on GPT-3.5 in terms of Pass@1, respectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在代码生成中表现出了吸引人的上下文学习（ICL）能力。LLM 接受一个包含需求代码示例和新需求的提示，并输出新的程序。现有的研究发现，ICL 受到示例的影响很大，因此引发了研究示例选择的研究。然而，现有的方法 Randomly 选择示例或者只考虑需求文本相似性来 retrieve，导致表现不佳。在这篇论文中，我们提出了一种新的学习基于选择方法 named LAIL（LLM-Aware In-context Learning）。给定一个候选示例，我们利用 LLM 自己来估算它，通过考虑需求和示例下的生成概率来Feedback probability。然后，我们将候选示例标记为正例或者负例，根据概率反馈。基于标记数据，我们导入了对比学习目标，以培养一个有效的检索器，使其获得 LLM 在代码生成中的偏好。我们在三个 LLM 上应用 LAIL，并对 MBJP、MBPP 和 MBCPP 三个表示性数据集进行评估。LATA 与当前基eline 相比，提高了代码生成的性能，具体是11.58%、6.89% 和 5.07% 的提升。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-ImageArg-2023-The-First-Shared-Task-in-Multimodal-Argument-Mining"><a href="#Overview-of-ImageArg-2023-The-First-Shared-Task-in-Multimodal-Argument-Mining" class="headerlink" title="Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining"></a>Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12172">http://arxiv.org/abs/2310.12172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhexiong Liu, Mohamed Elaraby, Yang Zhong, Diane Litman</li>
<li>for: 这篇论文提供了 ImageArg 共同任务的概述，这是第一个 Multimodal Argument Mining 共同任务，它在 EMNLP 2023 年度会议上召开。</li>
<li>methods: 这篇论文描述了两个分类子任务：（1）Argument Stance Classification，即判断一个包含图片和文本的推文是否支持或反对一个热点话题（如枪支持和堕胎）；（2）Image Persuasiveness Classification，即判断图片是否使文本更加吸引人。</li>
<li>results: 这个共同任务收到了 31 个参赛作品，其中 21 个来自 9 个团队，来自 6 个国家。最佳提交在 Subtask-A 中获得了 F1 分数 0.8647，而在 Subtask-B 中获得了 F1 分数 0.5561。<details>
<summary>Abstract</summary>
This paper presents an overview of the ImageArg shared task, the first multimodal Argument Mining shared task co-located with the 10th Workshop on Argument Mining at EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece of text toward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweet text more persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了图像论据共同任务（ImageArg），这是在EMNLP 2023年工作坊上的第一个多Modal Argument Mining共同任务。该任务包括两个分类子任务：（1）子任务A：图像立场分类；（2）子任务B：图像宣传效果分类。前者确定一个推文中的图像和文本对于一个争议话题（例如，枪支控制和堕胎）的立场。后者确定图像是否使得推文文本更加吸引人。共同任务收到了31个提交 для子任务A和21个提交 для子任务B来自9个不同的团队在6个国家。最佳提交在子任务A中取得了F1分数0.8647，而最佳提交在子任务B中取得了F1分数0.5561。
</details></li>
</ul>
<hr>
<h2 id="KGQuiz-Evaluating-the-Generalization-of-Encoded-Knowledge-in-Large-Language-Models"><a href="#KGQuiz-Evaluating-the-Generalization-of-Encoded-Knowledge-in-Large-Language-Models" class="headerlink" title="KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models"></a>KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09725">http://arxiv.org/abs/2310.09725</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leopoldwhite/kgquiz">https://github.com/leopoldwhite/kgquiz</a></li>
<li>paper_authors: Yuyang Bai, Shangbin Feng, Vidhisha Balachandran, Zhaoxuan Tan, Shiqi Lou, Tianxing He, Yulia Tsvetkov</li>
<li>for: 这个论文旨在探讨大语言模型（LLM）在知识培养任务上的表现，以及如何系统地评估LLM的知识能力和其在不同知识领域和任务格式下的普适性。</li>
<li>methods: 这篇论文提出了一个名为KGQuiz的知识强度测试 benchmark，用于全面检验LLM的知识普适性和可行性。KGQuiz包括三个知识领域和五种任务 formats，从简单的真或假问题到复杂的开放知识生成。</li>
<li>results: 经过广泛的实验表明，LLM在简单的知识 QA 任务上表现出色，但是需要更复杂的推理或使用域pecific的知识时仍然存在很大挑战。这些结果表明KGQuiz可以用于分析LLM的知识能力和普适性在不同知识领域和任务格式下的变化。<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs on the KGQuiz benchmark across the five knowledge-intensive tasks and knowledge domains. Extensive experiments demonstrate that LLMs achieve impressive performance in straightforward knowledge QA tasks, while settings and contexts requiring more complex reasoning or employing domain-specific facts still present significant challenges. We envision KGQuiz as a testbed to analyze such nuanced variations in performance across domains and task formats, and ultimately to understand, evaluate, and improve LLMs' knowledge abilities across a wide spectrum of knowledge domains and tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在知识密集任务中表现出色，表明其模型参数中含有真实世界知识。然而，关于如何系统地评估 LLM 的知识能力和其知识能力是否可以普遍应用于多个知识领域和复杂任务格式，还不够了解。为此，我们提出了 KGQuiz，一个用于全面探索 LLM 的知识普适能力的benchmark。KGQuiz 基于 triplet 知识结构，覆盖了三个知识领域，包括五种任务 formats，从简单的true-or-false 和多选问答，到复杂的blank filling和factual editing，最后是开放式知识生成。为了更好地理解 LLM 的知识能力和其普适性，我们在 KGQuiz benchmark 上测试了 10 个开源和黑盒 LLM，并进行了广泛的实验。结果表明， LLM 在直观知识 QA 任务中表现出色，但是需要更复杂的解释或使用域pecific的事实时仍然存在很大的挑战。我们认为 KGQuiz 可以作为一个测试台来分析这些 nuanced 的表现差异，并 ultimately 理解、评估和提高 LLM 的知识能力在多个知识领域和任务格式中。
</details></li>
</ul>
<hr>
<h2 id="HiCL-Hierarchical-Contrastive-Learning-of-Unsupervised-Sentence-Embeddings"><a href="#HiCL-Hierarchical-Contrastive-Learning-of-Unsupervised-Sentence-Embeddings" class="headerlink" title="HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings"></a>HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09720">http://arxiv.org/abs/2310.09720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuofeng Wu, Chaowei Xiao, VG Vinod Vydiswaran</li>
<li>for: 提高序列表示学习的效率和有效性，通过地方归一化和全序列归一化的对比学习来学习地方和全序列之间的关系。</li>
<li>methods: 提出了一种层次对比学习框架（HiCL），将序列分成多个段，使用地方和全序列归一化对比学习来学习段级和序列级关系，并且通过首先编码短段并然后聚合以提高训练效率。</li>
<li>results: 对比于传统方法，HiCL能够提高7种广泛评估的STS任务的前一个表现，升师平均提高+0.2%（BERT-large）和+0.44%（RoBERTa-large）。<details>
<summary>Abstract</summary>
In this paper, we propose a hierarchical contrastive learning framework, HiCL, which considers local segment-level and global sequence-level relationships to improve training efficiency and effectiveness. Traditional methods typically encode a sequence in its entirety for contrast with others, often neglecting local representation learning, leading to challenges in generalizing to shorter texts. Conversely, HiCL improves its effectiveness by dividing the sequence into several segments and employing both local and global contrastive learning to model segment-level and sequence-level relationships. Further, considering the quadratic time complexity of transformers over input tokens, HiCL boosts training efficiency by first encoding short segments and then aggregating them to obtain the sequence representation. Extensive experiments show that HiCL enhances the prior top-performing SNCSE model across seven extensively evaluated STS tasks, with an average increase of +0.2% observed on BERT-large and +0.44% on RoBERTa-large.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个层次对比学习框架，即HiCL，该框架考虑了本地分割段级和全序列级关系，以提高训练效率和有效性。传统方法通常将序列编码为整体对比他们，而忽略本地表示学习，这会导致对短文本掌握困难。相反，HiCL通过将序列分割成多个段，并使用本地和全序列对比学习来模型段级和序列级关系。此外，考虑到 transformer 对输入字符数的平方时间复杂度，HiCL 提高了训练效率，先对短段进行编码，然后将其聚合以获得序列表示。广泛的实验表明，HiCL 可以提高先前的最佳 SNCSE 模型在七个广泛评估的 STS 任务上，平均提高 +0.2% 在 BERT-large 上和 +0.44% 在 RoBERTa-large 上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/15/cs.CL_2023_10_15/" data-id="clpxp6bzg00dlee88c76k4kbj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/26/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="page-number" href="/page/26/">26</a><span class="page-number current">27</span><a class="page-number" href="/page/28/">28</a><a class="page-number" href="/page/29/">29</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/28/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
