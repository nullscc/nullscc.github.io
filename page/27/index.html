
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/27/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_10_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/04/cs.CV_2023_10_04/" class="article-date">
  <time datetime="2023-10-04T13:00:00.000Z" itemprop="datePublished">2023-10-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/04/cs.CV_2023_10_04/">cs.CV - 2023-10-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ViFiT-Reconstructing-Vision-Trajectories-from-IMU-and-Wi-Fi-Fine-Time-Measurements"><a href="#ViFiT-Reconstructing-Vision-Trajectories-from-IMU-and-Wi-Fi-Fine-Time-Measurements" class="headerlink" title="ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time Measurements"></a>ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03140">http://arxiv.org/abs/2310.03140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bryanbocao/vifit">https://github.com/bryanbocao/vifit</a></li>
<li>paper_authors: Bryan Bo Cao, Abrar Alali, Hansi Liu, Nicholas Meegan, Marco Gruteser, Kristin Dana, Ashwin Ashok, Shubham Jain<br>for:本研究旨在提高camera-based IoT应用程序中的人体跟踪功能，例如安全监测、智能城市交通安全提高、车辆到行人通信等。methods:本研究使用 transformer 模型来重建视觉 bounding box 轨迹，该模型可以更好地处理长期时间序列数据。results:研究结果显示， compared to 状态前的 X-Translator 模型， ViFiT 模型可以更好地重建视觉 bounding box 轨迹，MRFR 为 0.65，带宽减少率为 97.76%。<details>
<summary>Abstract</summary>
Tracking subjects in videos is one of the most widely used functions in camera-based IoT applications such as security surveillance, smart city traffic safety enhancement, vehicle to pedestrian communication and so on. In the computer vision domain, tracking is usually achieved by first detecting subjects with bounding boxes, then associating detected bounding boxes across video frames. For many IoT systems, images captured by cameras are usually sent over the network to be processed at a different site that has more powerful computing resources than edge devices. However, sending entire frames through the network causes significant bandwidth consumption that may exceed the system bandwidth constraints. To tackle this problem, we propose ViFiT, a transformer-based model that reconstructs vision bounding box trajectories from phone data (IMU and Fine Time Measurements). It leverages a transformer ability of better modeling long-term time series data. ViFiT is evaluated on Vi-Fi Dataset, a large-scale multimodal dataset in 5 diverse real world scenes, including indoor and outdoor environments. To fill the gap of proper metrics of jointly capturing the system characteristics of both tracking quality and video bandwidth reduction, we propose a novel evaluation framework dubbed Minimum Required Frames (MRF) and Minimum Required Frames Ratio (MRFR). ViFiT achieves an MRFR of 0.65 that outperforms the state-of-the-art approach for cross-modal reconstruction in LSTM Encoder-Decoder architecture X-Translator of 0.98, resulting in a high frame reduction rate as 97.76%.
</details>
<details>
<summary>摘要</summary>
“追踪目标在影像中是智能 IoT 应用中最广泛使用的功能，包括安全监控、智能城市交通安全增强、车辆与行人通信等。在计算机视觉领域中，追踪通常通过首先检测目标的矩形框，然后在影像帧之间将检测到的矩形框相互对应。但是，将整帧影像发送到网络上处理，可能会导致系统带宽限制超过。为解决这个问题，我们提出了 ViFiT，一个基于 transformer 的模型，可以从手机资料（IMU 和精确时间测量）中重建视觉矩形框之旅程。它利用 transformer 的能力更好地模型长期时间序列数据。ViFiT 在 Vi-Fi 数据集上进行评估，评估框架包括5个不同的实际世界场景，包括室内和室外环境。为了填补跟踪质量和影像带宽削减系统特性之间的差距，我们提出了一个新的评估框架，名为 Minimum Required Frames（MRF）和 Minimum Required Frames Ratio（MRFR）。ViFiT 在 MRFR 方面取得了0.65，超过了现有跨模式重建的 LSTM Encoder-Decoder 架构 X-Translator 的0.98，实现了高帧减少率为97.76%。”
</details></li>
</ul>
<hr>
<h2 id="Shielding-the-Unseen-Privacy-Protection-through-Poisoning-NeRF-with-Spatial-Deformation"><a href="#Shielding-the-Unseen-Privacy-Protection-through-Poisoning-NeRF-with-Spatial-Deformation" class="headerlink" title="Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation"></a>Shielding the Unseen: Privacy Protection through Poisoning NeRF with Spatial Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03125">http://arxiv.org/abs/2310.03125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Brandon Y. Feng, Heng Huang</li>
<li>for: 保护用户隐私 against Neural Radiance Fields (NeRF) 模型的生成能力。</li>
<li>methods: 我们提出了一种新型的恶意攻击方法，通过在观察到的视图中引入不可见的变化，使 NeRF 模型无法准确重建 3D 场景。我们开发了一种两级优化算法，包括基于 Projected Gradient Descent (PGD) 的空间变换。</li>
<li>results: 我们在两个常见 NeRF  benchmark 数据集上进行了广泛的测试，包括 29 个真实世界场景的高质量图像。我们的结果表明，我们的隐私保护方法在这些 benchmark 数据集上具有显著的降低 NeRF 性能的能力。此外，我们还证明了我们的方法可以适应不同的干扰强度和 NeRF 架构。这种研究为 NeRF 的潜在隐私风险提供了重要的认知，并强调了在开发 Robust 3D 场景重建算法时需要考虑隐私问题。我们的研究贡献到了负责任 AI 和生成机器学习领域，旨在保护用户隐私和尊重数字时代的创作权。<details>
<summary>Abstract</summary>
In this paper, we introduce an innovative method of safeguarding user privacy against the generative capabilities of Neural Radiance Fields (NeRF) models. Our novel poisoning attack method induces changes to observed views that are imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to accurately reconstruct a 3D scene. To achieve this, we devise a bi-level optimization algorithm incorporating a Projected Gradient Descent (PGD)-based spatial deformation. We extensively test our approach on two common NeRF benchmark datasets consisting of 29 real-world scenes with high-quality images. Our results compellingly demonstrate that our privacy-preserving method significantly impairs NeRF's performance across these benchmark datasets. Additionally, we show that our method is adaptable and versatile, functioning across various perturbation strengths and NeRF architectures. This work offers valuable insights into NeRF's vulnerabilities and emphasizes the need to account for such potential privacy risks when developing robust 3D scene reconstruction algorithms. Our study contributes to the larger conversation surrounding responsible AI and generative machine learning, aiming to protect user privacy and respect creative ownership in the digital age.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的方法保护用户隐私 against Neural Radiance Fields（NeRF）模型的生成能力。我们的新毒 poisoning 攻击方法会在观察到的视图中引入不可见的变化，却足够破坏 NeRF 重建3D场景的精度。为了实现这一点，我们开发了一种两级优化算法，包括基于 Projected Gradient Descent（PGD）的空间扭曲。我们对两个常用的 NeRF 测试集进行了广泛的测试，包括29个真实世界场景的高质量图像。我们的结果表明，我们的隐私保护方法可以在这些测试集中明显降低 NeRF 的性能。此外，我们还证明了我们的方法可以在不同的扰动强度和 NeRF 架构下进行适应。这项研究为 NeRF 的潜在隐私风险提供了重要的洞察，并触发了开发robust 3D场景重建算法时需要考虑的用户隐私问题。我们的研究贡献到了负责任AI和生成机器学习领域的大局，旨在保护用户隐私和尊重数字时代的创作权。
</details></li>
</ul>
<hr>
<h2 id="Blind-CT-Image-Quality-Assessment-Using-DDPM-derived-Content-and-Transformer-based-Evaluator"><a href="#Blind-CT-Image-Quality-Assessment-Using-DDPM-derived-Content-and-Transformer-based-Evaluator" class="headerlink" title="Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator"></a>Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03118">http://arxiv.org/abs/2310.03118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyi Shi, Wenjun Xia, Ge Wang, Xuanqin Mou</li>
<li>for: 降低辐射剂量和使用稀疏视图扫描模式是常见的 computed tomography (CT) 扫描模式，但它们经常导致噪声和扫描artifacts。</li>
<li>methods: 本研究提出了一种新的视觉质量评估（BIQA）方法，该方法模仿人类视觉系统（HVS）的内生生成机制（IGM）。</li>
<li>results: 该方法在 MICCAI 2023 低剂量计算机断层成像质量评估大奖赛中获得了第二名，并在挑战数据集上进一步提高了表现。<details>
<summary>Abstract</summary>
Lowering radiation dose per view and utilizing sparse views per scan are two common CT scan modes, albeit often leading to distorted images characterized by noise and streak artifacts. Blind image quality assessment (BIQA) strives to evaluate perceptual quality in alignment with what radiologists perceive, which plays an important role in advancing low-dose CT reconstruction techniques. An intriguing direction involves developing BIQA methods that mimic the operational characteristic of the human visual system (HVS). The internal generative mechanism (IGM) theory reveals that the HVS actively deduces primary content to enhance comprehension. In this study, we introduce an innovative BIQA metric that emulates the active inference process of IGM. Initially, an active inference module, implemented as a denoising diffusion probabilistic model (DDPM), is constructed to anticipate the primary content. Then, the dissimilarity map is derived by assessing the interrelation between the distorted image and its primary content. Subsequently, the distorted image and dissimilarity map are combined into a multi-channel image, which is inputted into a transformer-based image quality evaluator. Remarkably, by exclusively utilizing this transformer-based quality evaluator, we won the second place in the MICCAI 2023 low-dose computed tomography perceptual image quality assessment grand challenge. Leveraging the DDPM-derived primary content, our approach further improves the performance on the challenge dataset.
</details>
<details>
<summary>摘要</summary>
LOWERING RADIATION DOSE PER VIEW 和使用稀疏视图每次扫描是两种常见的 computed tomography (CT) 扫描模式，尽管经常导致噪声和扫描 artifacts distorted images 。干净图像质量评估 (BIQA) 努力evaluate perceptual quality 与放射学家所感知，这在提高低剂量 CT 重建技术方面扮演着重要的角色。一个有趣的方向是开发BIQA方法，模仿人类视觉系统 (HVS) 的运作特性。HVS 的 internal generative mechanism (IGM) 理论表明，HVS 活动地推导主要内容，以提高理解。在这种研究中，我们引入了一种创新的 BIQA 度量，模仿 HVS 的活动推导过程。首先，我们构建了一个 active inference module，实现为一种抑制扩散概率模型 (DDPM)，以预测主要内容。然后，我们计算了噪声图像和主要内容之间的相互关系，并生成了异同地图。最后，我们将噪声图像和异同地图组合成一个多通道图像，并输入到一种基于 transformer 的图像质量评估器。很Remarkably，只使用这种基于 transformer 的质量评估器，我们在 MICCAI 2023 低剂量 computed tomography 感知图像质量评估大奖中获得第二名。利用 DDPM 生成的主要内容，我们的方法进一步提高了挑战数据集的性能。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-based-Mixture-of-Vision-Transformers-for-Video-Violence-Recognition"><a href="#Reinforcement-Learning-based-Mixture-of-Vision-Transformers-for-Video-Violence-Recognition" class="headerlink" title="Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition"></a>Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03108">http://arxiv.org/abs/2310.03108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamid Mohammadi, Ehsan Nazerfard, Tahereh Firoozi</li>
<li>for: 这个论文旨在提出一种基于深度学习的视频暴力识别系统，以实现精度高且可扩展的人类暴力识别。</li>
<li>methods: 该论文使用了一种基于 transformer 的 Mixture of Experts（MoE）视频暴力识别系统，通过智能组合大视transformer和高效 transformer 架构，不仅利用了视transformer 架构的优势，还减少了使用大视transformer 的计算成本。</li>
<li>results: 该论文的实验结果表明，相比 CNN 模型，MoE 架构在 RWF 数据集上达到了 92.4% 的准确率。<details>
<summary>Abstract</summary>
Video violence recognition based on deep learning concerns accurate yet scalable human violence recognition. Currently, most state-of-the-art video violence recognition studies use CNN-based models to represent and categorize videos. However, recent studies suggest that pre-trained transformers are more accurate than CNN-based models on various video analysis benchmarks. Yet these models are not thoroughly evaluated for video violence recognition. This paper introduces a novel transformer-based Mixture of Experts (MoE) video violence recognition system. Through an intelligent combination of large vision transformers and efficient transformer architectures, the proposed system not only takes advantage of the vision transformer architecture but also reduces the cost of utilizing large vision transformers. The proposed architecture maximizes violence recognition system accuracy while actively reducing computational costs through a reinforcement learning-based router. The empirical results show the proposed MoE architecture's superiority over CNN-based models by achieving 92.4% accuracy on the RWF dataset.
</details>
<details>
<summary>摘要</summary>
视频暴力识别基于深度学习的研究担心精准 yet可扩展的人类暴力识别。目前大多数 state-of-the-art 视频暴力识别研究使用 CNN-based 模型来表示和分类视频。然而，最近的研究表明，预训练的 transformer 比 CNN-based 模型在多种视频分析指标上更加准确。然而，这些模型尚未对视频暴力识别进行全面的评估。本文介绍一种新的 transformer-based Mixture of Experts（MoE）视频暴力识别系统。通过将大视 transformer 和高效 transformer  Architecture 智能组合，提议的系统不仅利用了视transformer 架构的优势，还减少了利用大视 transformer 的计算成本。提议的架构最大化暴力识别系统的准确性，同时活动减少计算成本通过 reinforcement learning-based 路由器。实验结果显示，提议的 MoE 架构超过 CNN-based 模型的92.4% 准确率在 RWF 数据集上。
</details></li>
</ul>
<hr>
<h2 id="Creating-an-Atlas-of-Normal-Tissue-for-Pruning-WSI-Patching-Through-Anomaly-Detection"><a href="#Creating-an-Atlas-of-Normal-Tissue-for-Pruning-WSI-Patching-Through-Anomaly-Detection" class="headerlink" title="Creating an Atlas of Normal Tissue for Pruning WSI Patching Through Anomaly Detection"></a>Creating an Atlas of Normal Tissue for Pruning WSI Patching Through Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03106">http://arxiv.org/abs/2310.03106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peyman Nejat, Areej Alsaafin, Ghazal Alabtah, Nneka Comfere, Aaron Mangold, Dennis Murphree, Patricija Zot, Saba Yasir, Joaquin J. Garcia, H. R. Tizhoosh</li>
<li>for: 本研究旨在提高Computational pathology中的whole slide image (WSIs)选择patches的效果，以提高下游任务的表达度。</li>
<li>methods: 本研究提出了一种使用normal tissue samples WSIs建立”Atlas of normal tissue”，以消除normal histology的干扰和重复，提高WSIs的表达度。</li>
<li>results: 研究测试了该方法使用107个正常皮肤WSIs建立了normal atlas，并通过使用553个皮肤细胞癌WSIs和451个乳腺WSIs来验证该方法的有效性。结果表明，通过使用normal atlas可以将选择的WSIs减少30%-50%，同时保持同样的索引和搜索性能。<details>
<summary>Abstract</summary>
Patching gigapixel whole slide images (WSIs) is an important task in computational pathology. Some methods have been proposed to select a subset of patches as WSI representation for downstream tasks. While most of the computational pathology tasks are designed to classify or detect the presence of pathological lesions in each WSI, the confounding role and redundant nature of normal histology in tissue samples are generally overlooked in WSI representations. In this paper, we propose and validate the concept of an "atlas of normal tissue" solely using samples of WSIs obtained from normal tissue biopsies. Such atlases can be employed to eliminate normal fragments of tissue samples and hence increase the representativeness collection of patches. We tested our proposed method by establishing a normal atlas using 107 normal skin WSIs and demonstrated how established indexes and search engines like Yottixel can be improved. We used 553 WSIs of cutaneous squamous cell carcinoma (cSCC) to show the advantage. We also validated our method applied to an external dataset of 451 breast WSIs. The number of selected WSI patches was reduced by 30% to 50% after utilizing the proposed normal atlas while maintaining the same indexing and search performance in leave-one-patinet-out validation for both datasets. We show that the proposed normal atlas shows promise for unsupervised selection of the most representative patches of the abnormal/malignant WSI lesions.
</details>
<details>
<summary>摘要</summary>
修补 gigapixel整幕图像（WSIs）是计算 PATHOLOGY 中重要的任务。一些方法已经被提出来选择WSIs中的一 subset作为下游任务的表示。大多数计算 PATHOLOGY 任务是用来分类或检测每个 WSI 中的疾病肿瘤，但是通常忽略了正常组织的混合和重复性。在这篇论文中，我们提出了一种“正常组织图 Atlas”，使用来自正常组织biopsy中的WSIs来排除正常组织的块。这些图 Atlas 可以增加WSIs的表示性。我们测试了我们的提议方法，使用107个正常皮肤WSIs建立了正常图 Atlas，并示出了使用Yottixel等搜索引擎的改进。我们使用553个皮肤细胞癌（cSCC）WSIs示出了这种优势。我们还验证了我们的方法在451个乳腺WSIs中的外部数据集中。选择WSIs中的减少了30%-50%，保持了相同的索引和搜索性能，通过离开一个病人进行留下一个病人验证。我们表明，我们的正常图 Atlas 可以无监督的选择疾病肿瘤 WSI 中最有代表性的块。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-Multi-biometric-Indexing-based-on-Frequent-Binary-Patterns"><a href="#Privacy-preserving-Multi-biometric-Indexing-based-on-Frequent-Binary-Patterns" class="headerlink" title="Privacy-preserving Multi-biometric Indexing based on Frequent Binary Patterns"></a>Privacy-preserving Multi-biometric Indexing based on Frequent Binary Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03091">http://arxiv.org/abs/2310.03091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daile Osorio-Roig, Lazaro J. Gonzalez-Soler, Christian Rathgeb, Christoph Busch</li>
<li>for: 本研究旨在提出一种高效、private的多biometric标识系统，以提高生物метриック标识系统的计算效率和安全性。</li>
<li>methods: 本研究使用多biometric分类器和深度神经网络（DNN）基 embedding抽取器，并提出了一种基于频繁二进制模式的多biometricbinning策略，以优化生物метриック标识系统的计算效率和安全性。</li>
<li>results: 实验结果表明，使用提议的多biometric标识系统可以将计算工作负担减少至约57%（索引三种生物 метриック特征）和53%（索引两种生物 метриック特征），同时提高基eline生物метриック系统的安全性和性能。<details>
<summary>Abstract</summary>
The development of large-scale identification systems that ensure the privacy protection of enrolled subjects represents a major challenge. Biometric deployments that provide interoperability and usability by including efficient multi-biometric solutions are a recent requirement. In the context of privacy protection, several template protection schemes have been proposed in the past. However, these schemes seem inadequate for indexing (workload reduction) in biometric identification systems. More specifically, they have been used in identification systems that perform exhaustive searches, leading to a degradation of computational efficiency. To overcome these limitations, we propose an efficient privacy-preserving multi-biometric identification system that retrieves protected deep cancelable templates and is agnostic with respect to biometric characteristics and biometric template protection schemes. To this end, a multi-biometric binning scheme is designed to exploit the low intra-class variation properties contained in the frequent binary patterns extracted from different types of biometric characteristics. Experimental results reported on publicly available databases using state-of-the-art Deep Neural Network (DNN)-based embedding extractors show that the protected multi-biometric identification system can reduce the computational workload to approximately 57\% (indexing up to three types of biometric characteristics) and 53% (indexing up to two types of biometric characteristics), while simultaneously improving the biometric performance of the baseline biometric system at the high-security thresholds. The source code of the proposed multi-biometric indexing approach together with the composed multi-biometric dataset, will be made available to the research community once the article is accepted.
</details>
<details>
<summary>摘要</summary>
大规模的人脸识别系统的开发，以保护报名人员的隐私为主要挑战。包括高效多种生物特征解决方案的生物metric部署已成为最新的要求。在隐私保护的情况下，过去有几种模板保护方案被提议，但这些方案在生物metric识别系统中实现了搜索性能的下降。为了解决这些限制，我们提议一种高效的隐私保护多种生物识别系统，可以恢复保证深度可逆模板。为此，我们设计了一种多种生物特征的集合方案，利用不同类型生物特征中的低内类变化性来实现高效的多种生物识别。实验结果，使用公共可用的数据库和当今最佳的深度神经网络（DNN）基 embedding抽取器，我们的受保护多种生物识别系统可以将计算工作负担减少到约57%（ indexing 三种生物特征）和53%（ indexing 两种生物特征），同时提高基eline生物系统的安全性reshold下的生物性能。我们将提交的文章中的多种生物 indexingapproach的源代码，以及组合的多种生物数据集，将在文章被接受后向研究社区开源。
</details></li>
</ul>
<hr>
<h2 id="Consistent-1-to-3-Consistent-Image-to-3D-View-Synthesis-via-Geometry-aware-Diffusion-Models"><a href="#Consistent-1-to-3-Consistent-Image-to-3D-View-Synthesis-via-Geometry-aware-Diffusion-Models" class="headerlink" title="Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models"></a>Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03020">http://arxiv.org/abs/2310.03020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, Heng Wang</li>
<li>for: 该研究旨在解决零shot视图合成问题，即从单个图像中生成高质量的新视图，同时保持3D结构一致性。</li>
<li>methods: 该研究提出了一种生成框架，包括两个阶段：首先将观察区域转换到新视图，然后hallucinate未见区域。在这两个阶段中，我们设计了场景表示变换和视点条件diffusion模型，并在模型中使用epipolor导向注意力和多视点注意力来保持3D一致性。</li>
<li>results: 该研究的实验和评估结果表明，与现有方法相比，提出的机制能够更有效地生成3D一致的零shot视图合成结果。我们的项目页面是<a target="_blank" rel="noopener" href="https://jianglongye.com/consistent123/">https://jianglongye.com/consistent123/</a><details>
<summary>Abstract</summary>
Zero-shot novel view synthesis (NVS) from a single image is an essential problem in 3D object understanding. While recent approaches that leverage pre-trained generative models can synthesize high-quality novel views from in-the-wild inputs, they still struggle to maintain 3D consistency across different views. In this paper, we present Consistent-1-to-3, which is a generative framework that significantly mitigate this issue. Specifically, we decompose the NVS task into two stages: (i) transforming observed regions to a novel view, and (ii) hallucinating unseen regions. We design a scene representation transformer and view-conditioned diffusion model for performing these two stages respectively. Inside the models, to enforce 3D consistency, we propose to employ epipolor-guided attention to incorporate geometry constraints, and multi-view attention to better aggregate multi-view information. Finally, we design a hierarchy generation paradigm to generate long sequences of consistent views, allowing a full 360 observation of the provided object image. Qualitative and quantitative evaluation over multiple datasets demonstrate the effectiveness of the proposed mechanisms against state-of-the-art approaches. Our project page is at https://jianglongye.com/consistent123/
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>> zeroshot 新视图合成 (NVS) 从单个图像是三维物体理解的关键问题。 而最近的方法通过使用预训练生成模型可以从野外输入中生成高质量的新视图，但它们仍然无法保持不同视图之间的3D一致性。 在这篇论文中，我们提出了 Consistent-1-to-3，它是一个生成框架，可以很好地解决这个问题。  Specifically，我们将 NVS 任务分解成两个阶段：（i）将观察到的区域转换到新视图，和（ii）描述不见的区域。 我们设计了场景表示变换和视角conditioned 填充模型来完成这两个阶段。 在模型中，我们提议使用 epipolor-guided 注意力来 incorporate 几何约束，并使用多视角注意力来更好地综合多视角信息。 最后，我们设计了一种层次生成方法，可以生成长序列的一致视图，允许对提供的对象图像进行360度的观察。 我们的项目页面是https://jianglongye.com/consistent123/。 Qualitative 和量化评估 над Multiple 数据集表明我们的机制与现状approaches的效果。
</details></li>
</ul>
<hr>
<h2 id="Efficient-3DiM-Learning-a-Generalizable-Single-image-Novel-view-Synthesizer-in-One-Day"><a href="#Efficient-3DiM-Learning-a-Generalizable-Single-image-Novel-view-Synthesizer-in-One-Day" class="headerlink" title="Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day"></a>Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03015">http://arxiv.org/abs/2310.03015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song, Zhangyang Wang, Liangliang Cao</li>
<li>for:  novel view synthesis from a single image</li>
<li>methods:  crafted timestep sampling strategy, superior 3D feature extractor, enhanced training scheme</li>
<li>results:  reduced training time from 10 days to less than 1 day, significantly accelerating the training process<details>
<summary>Abstract</summary>
The task of novel view synthesis aims to generate unseen perspectives of an object or scene from a limited set of input images. Nevertheless, synthesizing novel views from a single image still remains a significant challenge in the realm of computer vision. Previous approaches tackle this problem by adopting mesh prediction, multi-plain image construction, or more advanced techniques such as neural radiance fields. Recently, a pre-trained diffusion model that is specifically designed for 2D image synthesis has demonstrated its capability in producing photorealistic novel views, if sufficiently optimized on a 3D finetuning task. Although the fidelity and generalizability are greatly improved, training such a powerful diffusion model requires a vast volume of training data and model parameters, resulting in a notoriously long time and high computational costs. To tackle this issue, we propose Efficient-3DiM, a simple but effective framework to learn a single-image novel-view synthesizer. Motivated by our in-depth analysis of the inference process of diffusion models, we propose several pragmatic strategies to reduce the training overhead to a manageable scale, including a crafted timestep sampling strategy, a superior 3D feature extractor, and an enhanced training scheme. When combined, our framework is able to reduce the total training time from 10 days to less than 1 day, significantly accelerating the training process under the same computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive experiments are conducted to demonstrate the efficiency and generalizability of our proposed method.
</details>
<details>
<summary>摘要</summary>
novel 视点合成的任务是生成对象或场景中未经见过的视角，但是从有限的输入图像中生成novel 视角仍然是计算机视觉领域中的一个重要挑战。先前的方法通过预测网格、多平面图像建构或更高级的技术如神经辐射场来解决这个问题。最近，一个特性化的扩散模型，专门为2D图像合成而设计，在生成高品质的novel 视角方面表现出色，但是需要训练大量的数据和模型参数，具有极高的计算成本和训练时间。为解决这个问题，我们提出了高效的3DiM框架，用于学习单图novel 视角合成器。我们的框架基于我们对扩散模型的推理过程进行了深入分析，并提出了一些实用的策略来降低训练负担，包括自定义时间步骤采样策略、更高级的3D特征提取器和改进的训练方案。当这些策略结合使用时，我们的框架可以在同样的计算平台上减少训练时间从10天降至1天以下，显著减少训练时间。我们进行了广泛的实验来证明我们的提议的效果和通用性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Domain-Specific-Features-Disentanglement-for-Domain-Generalization"><a href="#Towards-Domain-Specific-Features-Disentanglement-for-Domain-Generalization" class="headerlink" title="Towards Domain-Specific Features Disentanglement for Domain Generalization"></a>Towards Domain-Specific Features Disentanglement for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03007">http://arxiv.org/abs/2310.03007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Qi Zhang, Zenan Huang, Haobo Wang, Junbo Zhao</li>
<li>for: 解决现代机器学习算法面临的分布预期问题，域总结（DG）成为了一个受欢迎的方向，这些方法的目标是找到各个不同分布中的通用特征。</li>
<li>methods: 我们提出了一种新的对比基于分解方法CDDG，通过利用分解的特征来快速地抽象出域特定的特征，从而为DG任务提供更好的特征抽取。</li>
<li>results: 我们在多个标准 benchmark dataset上进行了广泛的实验，并证明了我们的方法在比较其他状态的方法时显著超越。此外，视觉评估也证明了我们的方法可以有效地实现特征分解。<details>
<summary>Abstract</summary>
Distributional shift between domains poses great challenges to modern machine learning algorithms. The domain generalization (DG) signifies a popular line targeting this issue, where these methods intend to uncover universal patterns across disparate distributions. Noted, the crucial challenge behind DG is the existence of irrelevant domain features, and most prior works overlook this information. Motivated by this, we propose a novel contrastive-based disentanglement method CDDG, to effectively utilize the disentangled features to exploit the over-looked domain-specific features, and thus facilitating the extraction of the desired cross-domain category features for DG tasks. Specifically, CDDG learns to decouple inherent mutually exclusive features by leveraging them in the latent space, thus making the learning discriminative. Extensive experiments conducted on various benchmark datasets demonstrate the superiority of our method compared to other state-of-the-art approaches. Furthermore, visualization evaluations confirm the potential of our method in achieving effective feature disentanglement.
</details>
<details>
<summary>摘要</summary>
《分布Shift问题对现代机器学习算法提出了巨大挑战。Domain generalization（DG）成为了解决这个问题的流行途径，这些方法的目标是找到不同分布之间的通用特征。然而，现有的大多数优秀方法忽略了不相关的领域特征，这是DG的核心挑战。我们提出了一种基于对比的分解方法，即CDDG，以便利用分解后的特征来利用过looked域特征，从而促进DG任务中EXTRACT DESIRED CROSS-DOMAIN CATEGORY FEATURES的抽取。具体来说，CDDG通过在几何空间中利用相互排斥的特征来决定权重，从而使学习变得抽象。经验表明，我们的方法在各种 benchmark 数据集上显示出与其他现状顶峰方法相比的优越性。此外，视觉评估表明我们的方法在实现有效的特征分解方面具有潜在的潜力。》
</details></li>
</ul>
<hr>
<h2 id="COOLer-Class-Incremental-Learning-for-Appearance-Based-Multiple-Object-Tracking"><a href="#COOLer-Class-Incremental-Learning-for-Appearance-Based-Multiple-Object-Tracking" class="headerlink" title="COOLer: Class-Incremental Learning for Appearance-Based Multiple Object Tracking"></a>COOLer: Class-Incremental Learning for Appearance-Based Multiple Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03006">http://arxiv.org/abs/2310.03006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BoSmallEar/COOLer">https://github.com/BoSmallEar/COOLer</a></li>
<li>paper_authors: Zhizheng Liu, Mattia Segu, Fisher Yu</li>
<li>for: 提高多个任务的Sequential Learning，使模型可以逐渐学习新任务而不产生过去任务的数据束缚。</li>
<li>methods: 提出了一种基于对比学习和连续学习的多目标跟踪器（COOLer），可以逐渐学习新类型的跟踪，保留过去类型的识别特征。同时，提出了一种对比类增量实例表示学习技术来进一步减轻实例表示的混合。</li>
<li>results: 实验结果表明，COOLer可以逐渐学习新类型的跟踪，同时有效地避免过去类型的识别特征的混合。代码可以在<a target="_blank" rel="noopener" href="https://github.com/BoSmallEar/COOLer%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/BoSmallEar/COOLer上下载。</a><details>
<summary>Abstract</summary>
Continual learning allows a model to learn multiple tasks sequentially while retaining the old knowledge without the training data of the preceding tasks. This paper extends the scope of continual learning research to class-incremental learning for multiple object tracking (MOT), which is desirable to accommodate the continuously evolving needs of autonomous systems. Previous solutions for continual learning of object detectors do not address the data association stage of appearance-based trackers, leading to catastrophic forgetting of previous classes' re-identification features. We introduce COOLer, a COntrastive- and cOntinual-Learning-based tracker, which incrementally learns to track new categories while preserving past knowledge by training on a combination of currently available ground truth labels and pseudo-labels generated by the past tracker. To further exacerbate the disentanglement of instance representations, we introduce a novel contrastive class-incremental instance representation learning technique. Finally, we propose a practical evaluation protocol for continual learning for MOT and conduct experiments on the BDD100K and SHIFT datasets. Experimental results demonstrate that COOLer continually learns while effectively addressing catastrophic forgetting of both tracking and detection. The code is available at https://github.com/BoSmallEar/COOLer.
</details>
<details>
<summary>摘要</summary>
（简化中文）持续学习可以让模型在不同任务之间学习，而不需要前一个任务的训练数据。这篇文章扩展了持续学习研究的范围，探讨了多个目标跟踪（MOT）中的类增量学习，这对自动化系统来说是非常有优势。先前的持续学习对象检测器解决方案不会处理出现在跟踪器中的数据关联阶段，导致 previous class 的重新识别特征发生融合式忘记。我们介绍了 COOLer，一个基于对比学习和持续学习的跟踪器，可以逐渐学习新的类，同时保留过去知识。为了进一步增强实例表示的独立性，我们介绍了一种新的对比类增量实例表示学习技术。最后，我们提出了一个实用的持续学习 для MOT 评估协议，并在 BDD100K 和 SHIFT 数据集上进行了实验。实验结果表明，COOLer 可以持续学习，并有效地解决了跟踪和检测的融合式忘记。代码可以在 https://github.com/BoSmallEar/COOLer 上获取。
</details></li>
</ul>
<hr>
<h2 id="Reversing-Deep-Face-Embeddings-with-Probable-Privacy-Protection"><a href="#Reversing-Deep-Face-Embeddings-with-Probable-Privacy-Protection" class="headerlink" title="Reversing Deep Face Embeddings with Probable Privacy Protection"></a>Reversing Deep Face Embeddings with Probable Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03005">http://arxiv.org/abs/2310.03005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daile Osorio-Roig, Paul A. Gerlitz, Christian Rathgeb, Christoph Busch</li>
<li>For: The paper aims to evaluate the effectiveness of soft-biometric privacy-enhancement approaches in protecting face embeddings, and to assess the vulnerability of state-of-the-art face embedding extractors to attacks that attempt to reconstruct the original face images.* Methods: The paper uses a well-known state-of-the-art face image reconstruction approach to evaluate the effectiveness of soft-biometric privacy protection methods. The authors also analyze the transformation complexity used for privacy protection and assess the vulnerability of state-of-the-art face embedding extractors to attacks.* Results: The paper shows that biometric privacy-enhanced face embeddings can be reconstructed with an accuracy of up to approximately 98%, depending on the complexity of the protection algorithm. This suggests that while soft-biometric privacy-enhancement approaches can provide some level of protection, they may not be sufficient to ensure complete privacy protection for face embeddings.<details>
<summary>Abstract</summary>
Generally, privacy-enhancing face recognition systems are designed to offer permanent protection of face embeddings. Recently, so-called soft-biometric privacy-enhancement approaches have been introduced with the aim of canceling soft-biometric attributes. These methods limit the amount of soft-biometric information (gender or skin-colour) that can be inferred from face embeddings. Previous work has underlined the need for research into rigorous evaluations and standardised evaluation protocols when assessing privacy protection capabilities. Motivated by this fact, this paper explores to what extent the non-invertibility requirement can be met by methods that claim to provide soft-biometric privacy protection. Additionally, a detailed vulnerability assessment of state-of-the-art face embedding extractors is analysed in terms of the transformation complexity used for privacy protection. In this context, a well-known state-of-the-art face image reconstruction approach has been evaluated on protected face embeddings to break soft biometric privacy protection. Experimental results show that biometric privacy-enhanced face embeddings can be reconstructed with an accuracy of up to approximately 98%, depending on the complexity of the protection algorithm.
</details>
<details>
<summary>摘要</summary>
通常，隐私增强面 recognition系统是设计来提供面嵌入的永久保护。最近，叫做软生物метри隐私增强方法已经被引入，以消除软生物метри特征。这些方法限制了从面嵌入中可以推断的软生物метри信息（性别或皮肤颜色）的量。先前的工作已经强调了评估隐私保护能力的严格评估和标准化评估协议的必要性。在这个背景下，本文探讨了方法，宣称提供软生物метри隐私保护，是否能够满足非逆性要求。此外，本文还进行了现有面嵌入抽取器的敏感性评估，并在保护算法的复杂度方面进行了分析。在这个上下文中，一种well-known的面图像重建方法被评估了在保护后的面嵌入上，以打砸软生物метри隐私保护。实验结果显示，具有隐私保护的面嵌入可以在98%的准确率下重建，具体取决于保护算法的复杂度。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Key-Selection-for-Face-based-One-Time-Biometrics-via-Morphing"><a href="#Optimizing-Key-Selection-for-Face-based-One-Time-Biometrics-via-Morphing" class="headerlink" title="Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing"></a>Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02997">http://arxiv.org/abs/2310.02997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daile Osorio-Roig, Mahdi Ghafourian, Christian Rathgeb, Ruben Vera-Rodriguez, Christoph Busch, Julian Fierrez</li>
<li>for: 提高face recognition系统对抗特殊攻击的安全性</li>
<li>methods: 提出了不同的钥匙选择策略来提高signal level的安全性</li>
<li>results: 实验结果表明， certain key selection strategies可以完全阻止特殊攻击，而对实际情况下的攻击 Success chance can be reduced to approximately 5.0%<details>
<summary>Abstract</summary>
Nowadays, facial recognition systems are still vulnerable to adversarial attacks. These attacks vary from simple perturbations of the input image to modifying the parameters of the recognition model to impersonate an authorised subject. So-called privacy-enhancing facial recognition systems have been mostly developed to provide protection of stored biometric reference data, i.e. templates. In the literature, privacy-enhancing facial recognition approaches have focused solely on conventional security threats at the template level, ignoring the growing concern related to adversarial attacks. Up to now, few works have provided mechanisms to protect face recognition against adversarial attacks while maintaining high security at the template level. In this paper, we propose different key selection strategies to improve the security of a competitive cancelable scheme operating at the signal level. Experimental results show that certain strategies based on signal-level key selection can lead to complete blocking of the adversarial attack based on an iterative optimization for the most secure threshold, while for the most practical threshold, the attack success chance can be decreased to approximately 5.0%.
</details>
<details>
<summary>摘要</summary>
现在，人脸识别系统仍然易受到敌意攻击。这些攻击可以是简单地修改输入图像，或者修改识别模型的参数，以便模拟授权者。所谓的隐私增强人脸识别系统主要是为了保护存储的生物特征数据（模板）。在文献中，隐私增强人脸识别方法主要集中在传统安全威胁上，忽视了增长的敌意攻击问题。到目前为止，只有一些工作提供了保护人脸识别 against敌意攻击的机制，同时保持高度安全的方法。在这篇论文中，我们提议了不同的钥匙选择策略，以提高竞争性下的 cancelable 方案的安全性。实验结果表明，基于信号水平的钥匙选择策略可以完全阻止敌意攻击，而在最实用的阈值上，攻击成功的机会可以降至约5.0%。
</details></li>
</ul>
<hr>
<h2 id="Fully-Automatic-Segmentation-of-Gross-Target-Volume-and-Organs-at-Risk-for-Radiotherapy-Planning-of-Nasopharyngeal-Carcinoma"><a href="#Fully-Automatic-Segmentation-of-Gross-Target-Volume-and-Organs-at-Risk-for-Radiotherapy-Planning-of-Nasopharyngeal-Carcinoma" class="headerlink" title="Fully Automatic Segmentation of Gross Target Volume and Organs-at-Risk for Radiotherapy Planning of Nasopharyngeal Carcinoma"></a>Fully Automatic Segmentation of Gross Target Volume and Organs-at-Risk for Radiotherapy Planning of Nasopharyngeal Carcinoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02972">http://arxiv.org/abs/2310.02972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/astarakee/segrap2023">https://github.com/astarakee/segrap2023</a></li>
<li>paper_authors: Mehdi Astaraki, Simone Bendazzoli, Iuliana Toma-Dasu</li>
<li>for: 本研究旨在提高头颈区Computed Tomography（CT）图像中的Target segmentation，以提高肿瘤规划规划的准确性。</li>
<li>methods: 我们提出了一种完全自动的框架，并开发了两种模型，一种用于45个Organs at Risk（OARs）的分割，另一种用于两个Gross Tumor Volumes（GTVs）的分割。我们使用了协调Intensity Distributions的预处理方法，然后自动对目标区域进行裁剪。</li>
<li>results: 我们在SegRap 2023挑战的验证阶段中，使用了这种方法获得了每个任务的第二名。我们的框架可以在<a target="_blank" rel="noopener" href="https://github.com/Astarakee/segrap2023%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Astarakee/segrap2023上获取。</a><details>
<summary>Abstract</summary>
Target segmentation in CT images of Head&Neck (H&N) region is challenging due to low contrast between adjacent soft tissue. The SegRap 2023 challenge has been focused on benchmarking the segmentation algorithms of Nasopharyngeal Carcinoma (NPC) which would be employed as auto-contouring tools for radiation treatment planning purposes. We propose a fully-automatic framework and develop two models for a) segmentation of 45 Organs at Risk (OARs) and b) two Gross Tumor Volumes (GTVs). To this end, we preprocess the image volumes by harmonizing the intensity distributions and then automatically cropping the volumes around the target regions. The preprocessed volumes were employed to train a standard 3D U-Net model for each task, separately. Our method took second place for each of the tasks in the validation phase of the challenge. The proposed framework is available at https://github.com/Astarakee/segrap2023
</details>
<details>
<summary>摘要</summary>
target segmentation in CT pictures of head and neck (H&N) area is difficult due to low contrast between adjacent soft tissue. the segrap 2023 challenge has been focused on benchmarking the segmentation algorithms of nasopharyngeal carcinoma (NPC) which would be employed as auto-contouring tools for radiation treatment planning purposes. we propose a fully-automatic framework and develop two models for a) segmentation of 45 organs at risk (OARs) and b) two gross tumor volumes (GTVs). to this end, we preprocess the image volumes by harmonizing the intensity distributions and then automatically cropping the volumes around the target regions. the preprocessed volumes were employed to train a standard 3D U-Net model for each task, separately. our method took second place for each of the tasks in the validation phase of the challenge. the proposed framework is available at https://github.com/Astarakee/segrap2023.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="CoDA-Collaborative-Novel-Box-Discovery-and-Cross-modal-Alignment-for-Open-vocabulary-3D-Object-Detection"><a href="#CoDA-Collaborative-Novel-Box-Discovery-and-Cross-modal-Alignment-for-Open-vocabulary-3D-Object-Detection" class="headerlink" title="CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection"></a>CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02960">http://arxiv.org/abs/2310.02960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangcaoai/CoDA_NeurIPS2023">https://github.com/yangcaoai/CoDA_NeurIPS2023</a></li>
<li>paper_authors: Yang Cao, Yihan Zeng, Hang Xu, Dan Xu</li>
<li>for: 本文 targets at 3D scene中的开放词汇Object Detection（OV-3DDet）问题，即检测Scene中的novel object。</li>
<li>methods: 本文提出了一种同时解决localizing和classifying novel object的框架，通过结合有限基类的3D box geometry priors和2D semantic open-vocabulary priors来生成pseudo box labels of novel objects。另外，本文还提出了一种基于发现的novel boxes的cross-modal alignment模块，用于将点云和图像&#x2F;文本模式之间的特征空间相互抽象。</li>
<li>results: 对于两个difficult datasets（i.e., SUN-RGBD和ScanNet），本文的方法实现了高效的novel object localization和classification。与最佳替换方法相比，本文的方法具有80%的mAP提升。代码和预训练模型在项目页面上发布。<details>
<summary>Abstract</summary>
Open-vocabulary 3D Object Detection (OV-3DDet) aims to detect objects from an arbitrary list of categories within a 3D scene, which remains seldom explored in the literature. There are primarily two fundamental problems in OV-3DDet, i.e., localizing and classifying novel objects. This paper aims at addressing the two problems simultaneously via a unified framework, under the condition of limited base categories. To localize novel 3D objects, we propose an effective 3D Novel Object Discovery strategy, which utilizes both the 3D box geometry priors and 2D semantic open-vocabulary priors to generate pseudo box labels of the novel objects. To classify novel object boxes, we further develop a cross-modal alignment module based on discovered novel boxes, to align feature spaces between 3D point cloud and image/text modalities. Specifically, the alignment process contains a class-agnostic and a class-discriminative alignment, incorporating not only the base objects with annotations but also the increasingly discovered novel objects, resulting in an iteratively enhanced alignment. The novel box discovery and crossmodal alignment are jointly learned to collaboratively benefit each other. The novel object discovery can directly impact the cross-modal alignment, while a better feature alignment can, in turn, boost the localization capability, leading to a unified OV-3DDet framework, named CoDA, for simultaneous novel object localization and classification. Extensive experiments on two challenging datasets (i.e., SUN-RGBD and ScanNet) demonstrate the effectiveness of our method and also show a significant mAP improvement upon the best-performing alternative method by 80%. Codes and pre-trained models are released on the project page.
</details>
<details>
<summary>摘要</summary>
Open-vocabulary 3D对象检测（OV-3DDet）目标是在3D场景中检测来自不同类别的对象，这是现有文献中少有探讨的领域。这个问题的两个基本问题是对novel对象进行本地化和分类。这篇论文提出了一种同时解决这两个问题的简单框架，即CoDA框架，其基于有限基础类别的前提下。为了本地化novel对象，我们提议一种有效的3D新对象发现策略，它利用3D盒子几何学预设和2D semantic开放词汇预设来生成 Pseudo box标签。为了类别novel对象盒子，我们进一步开发了基于发现的新盒子的cross-modal对接模块，以对Feature空间 между3D点云和图像/文本Modalities进行对接。特别是，对接过程包括无类别和类别特异对接，并利用基础对象的标注以及逐渐发现的新对象，进行迭代增强对接。新盒子发现和cross-modal对接 jointly learning，以便相互帮助。新对象发现可以直接影响cross-modal对接，而更好的对接可以提高本地化能力，从而实现一个简单的OV-3DDet框架，名为CoDA，用于同时本地化和分类novel对象。经验表明，我们的方法在SUN-RGBD和ScanNet两个难题 dataset上实现了显著的map提升，相比最佳替代方法，提升约80%。代码和预训练模型在项目页面上发布。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Landmark-Color-for-AUV-Docking-in-Visually-Dynamic-Environments"><a href="#Adaptive-Landmark-Color-for-AUV-Docking-in-Visually-Dynamic-Environments" class="headerlink" title="Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments"></a>Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02944">http://arxiv.org/abs/2310.02944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Corey Knutson, Zhipeng Cao, Junaed Sattar</li>
<li>for: 增加自主潜水器（AUV）的任务时间，提供了一个地方 для AUV 重新充电和接收新的任务信息。</li>
<li>methods: 使用适应色LED标记和动态色滤波来提高水质不同情况下的灯标可见度。AUV 和 docking station 都使用摄像头确定水背景色以计算需要的标记颜色，无需AUV和 docking station之间的通信。</li>
<li>results: 在池和湖中进行的实验表明，我们的方法比静态颜色阈值方法更好，随着背景颜色的变化。在清水情况下，DS 的探测范围为5米， false positives 很少。<details>
<summary>Abstract</summary>
Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the need for human intervention. A docking station (DS) can extend mission times of an AUV by providing a location for the AUV to recharge its batteries and receive updated mission information. Various methods for locating and tracking a DS exist, but most rely on expensive acoustic sensors, or are vision-based, which is significantly affected by water quality. In this \doctype, we present a vision-based method that utilizes adaptive color LED markers and dynamic color filtering to maximize landmark visibility in varying water conditions. Both AUV and DS utilize cameras to determine the water background color in order to calculate the desired marker color. No communication between AUV and DS is needed to determine marker color. Experiments conducted in a pool and lake show our method performs 10 times better than static color thresholding methods as background color varies. DS detection is possible at a range of 5 meters in clear water with minimal false positives.
</details>
<details>
<summary>摘要</summary>
自主水下潜水器（AUV）可以在水下完成任务无需人类干预。一个岸站（DS）可以延长AUV的任务时间，提供AUV重新充电和更新任务信息的位置。许多找到和跟踪岸站的方法存在，但大多数依赖于昂贵的陀螺探测器，或者视觉基于方法，它受到水质的影响。在这篇文章中，我们提出了一种视觉基于的方法，利用适应色LED标记和动态色滤波提高标记颜色的可见性，并且不需AUV和DS之间进行交流，以适应不同的水质。我们在池和湖进行了实验，发现我们的方法在背景颜色变化时比静止颜色阈值方法好10倍。在清水情况下，DS的探测范围为5米，false positive少。
</details></li>
</ul>
<hr>
<h2 id="Graph-data-modelling-for-outcome-prediction-in-oropharyngeal-cancer-patients"><a href="#Graph-data-modelling-for-outcome-prediction-in-oropharyngeal-cancer-patients" class="headerlink" title="Graph data modelling for outcome prediction in oropharyngeal cancer patients"></a>Graph data modelling for outcome prediction in oropharyngeal cancer patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02931">http://arxiv.org/abs/2310.02931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nithya Bhasker, Stefan Leger, Alexander Zwanenburg, Chethan Babu Reddy, Sebastian Bodenstedt, Steffen Löck, Stefanie Speidel</li>
<li>for: 这个研究是用来预测或opharyngeal cancer（OPC）病人的 binary outcome，使用 computed tomography（CT）-based radiomic features。</li>
<li>methods: 这个研究使用 patient hypergraph network（PHGN），并在 inductive learning 设置下进行训练。</li>
<li>results: 研究获得了比较好的结果，并且与 GNN 和基eline linear models 进行比较，结果显示 PHGN 在时间-to-event 分析中表现更好。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) are becoming increasingly popular in the medical domain for the tasks of disease classification and outcome prediction. Since patient data is not readily available as a graph, most existing methods either manually define a patient graph, or learn a latent graph based on pairwise similarities between the patients. There are also hypergraph neural network (HGNN)-based methods that were introduced recently to exploit potential higher order associations between the patients by representing them as a hypergraph. In this work, we propose a patient hypergraph network (PHGN), which has been investigated in an inductive learning setup for binary outcome prediction in oropharyngeal cancer (OPC) patients using computed tomography (CT)-based radiomic features for the first time. Additionally, the proposed model was extended to perform time-to-event analyses, and compared with GNN and baseline linear models.
</details>
<details>
<summary>摘要</summary>
几何神经网络（GNNs）在医疗领域日益受欢迎，用于疾病分类和结果预测。由于病人数据不易 disponibles as a graph，大多数现有方法可以 manually define a patient graph, or learn a latent graph based on pairwise similarities between the patients。此外，hypergraph neural network (HGNN)-based methods were introduced recently to exploit potential higher order associations between the patients by representing them as a hypergraph。在这项工作中，我们提出了一个 patient hypergraph network (PHGN)，在 inductive learning setup 中用于 binary outcome prediction in oropharyngeal cancer (OPC) patients using computed tomography (CT)-based radiomic features for the first time。此外，提出的模型还被扩展到进行时间到事件分析，并与 GNN 和基线线性模型进行比较。
</details></li>
</ul>
<hr>
<h2 id="Computationally-Efficient-Quadratic-Neural-Networks"><a href="#Computationally-Efficient-Quadratic-Neural-Networks" class="headerlink" title="Computationally Efficient Quadratic Neural Networks"></a>Computationally Efficient Quadratic Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02901">http://arxiv.org/abs/2310.02901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathew Mithra Noel, Venkataraman Muthiah-Nakarajan</li>
<li>for: 本研究旨在提出一种高阶人工神经元，其输出是通过应用激活函数于高阶多omial函数的输入来计算的。</li>
<li>methods: 本研究使用高阶神经元，其中每个神经元的输出是一个高阶多omial函数的函数。另外，本研究还使用激活函数来计算神经元的输出。</li>
<li>results: 本研究发现，使用高阶神经元可以提高神经网络的学习能力，而且只需要添加 $n$ 个额外参数。此外，本研究还示出了一种使用 quadratic neurons 的减少参数神经网络模型，可以在 benchmark 分类 dataset 上达到更高的准确率。<details>
<summary>Abstract</summary>
Higher order artificial neurons whose outputs are computed by applying an activation function to a higher order multinomial function of the inputs have been considered in the past, but did not gain acceptance due to the extra parameters and computational cost. However, higher order neurons have significantly greater learning capabilities since the decision boundaries of higher order neurons can be complex surfaces instead of just hyperplanes. The boundary of a single quadratic neuron can be a general hyper-quadric surface allowing it to learn many nonlinearly separable datasets. Since quadratic forms can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional parameters are needed instead of $n^2$. A quadratic Logistic regression model is first presented. Solutions to the XOR problem with a single quadratic neuron are considered. The complete vectorized equations for both forward and backward propagation in feedforward networks composed of quadratic neurons are derived. A reduced parameter quadratic neural network model with just $ n $ additional parameters per neuron that provides a compromise between learning ability and computational cost is presented. Comparison on benchmark classification datasets are used to demonstrate that a final layer of quadratic neurons enables networks to achieve higher accuracy with significantly fewer hidden layer neurons. In particular this paper shows that any dataset composed of $C$ bounded clusters can be separated with only a single layer of $C$ quadratic neurons.
</details>
<details>
<summary>摘要</summary>
高等人造神经元 whose outputs are computed by applying an activation function to a higher-order multinomial function of the inputs have been considered in the past, but did not gain acceptance due to the extra parameters and computational cost. However, higher-order neurons have significantly greater learning capabilities since the decision boundaries of higher-order neurons can be complex surfaces instead of just hyperplanes. The boundary of a single quadratic neuron can be a general hyper-quadric surface, allowing it to learn many nonlinearly separable datasets. Since quadratic forms can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional parameters are needed instead of $n^2$. A quadratic Logistic regression model is first presented. Solutions to the XOR problem with a single quadratic neuron are considered. The complete vectorized equations for both forward and backward propagation in feedforward networks composed of quadratic neurons are derived. A reduced parameter quadratic neural network model with just $n$ additional parameters per neuron that provides a compromise between learning ability and computational cost is presented. Comparison on benchmark classification datasets are used to demonstrate that a final layer of quadratic neurons enables networks to achieve higher accuracy with significantly fewer hidden layer neurons. In particular, this paper shows that any dataset composed of $C$ bounded clusters can be separated with only a single layer of $C$ quadratic neurons.
</details></li>
</ul>
<hr>
<h2 id="Human-centric-Behavior-Description-in-Videos-New-Benchmark-and-Model"><a href="#Human-centric-Behavior-Description-in-Videos-New-Benchmark-and-Model" class="headerlink" title="Human-centric Behavior Description in Videos: New Benchmark and Model"></a>Human-centric Behavior Description in Videos: New Benchmark and Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02894">http://arxiv.org/abs/2310.02894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingru Zhou, Yiqi Gao, Manqing Zhang, Peng Wu, Peng Wang, Yanning Zhang</li>
<li>for: 本研究旨在提供细化的个体行为描述，以提高视频监控中的人员识别和行为分析。</li>
<li>methods: 我们提出了一种基于人物中心的视频captioning方法，通过细化的人物描述来捕捉每个个体的行为特征。</li>
<li>results: 我们的方法在描述每个个体的行为方面达到了领先的 Result，并且可以准确地将个体与其行为相关联。<details>
<summary>Abstract</summary>
In the domain of video surveillance, describing the behavior of each individual within the video is becoming increasingly essential, especially in complex scenarios with multiple individuals present. This is because describing each individual's behavior provides more detailed situational analysis, enabling accurate assessment and response to potential risks, ensuring the safety and harmony of public places. Currently, video-level captioning datasets cannot provide fine-grained descriptions for each individual's specific behavior. However, mere descriptions at the video-level fail to provide an in-depth interpretation of individual behaviors, making it challenging to accurately determine the specific identity of each individual. To address this challenge, we construct a human-centric video surveillance captioning dataset, which provides detailed descriptions of the dynamic behaviors of 7,820 individuals. Specifically, we have labeled several aspects of each person, such as location, clothing, and interactions with other elements in the scene, and these people are distributed across 1,012 videos. Based on this dataset, we can link individuals to their respective behaviors, allowing for further analysis of each person's behavior in surveillance videos. Besides the dataset, we propose a novel video captioning approach that can describe individual behavior in detail on a person-level basis, achieving state-of-the-art results. To facilitate further research in this field, we intend to release our dataset and code.
</details>
<details>
<summary>摘要</summary>
在视频监控领域，描述每个人在视频中的行为变得越来越重要，特别是在多个人存在的复杂场景下。这是因为每个人的行为描述提供了更加细致的情况分析，使得可以准确评估和应对潜在风险，保证公共场所的安全和和谐。然而，现有的视频水平描述数据集不能提供每个人特定行为的细致描述。而且，仅仅是视频水平的描述无法提供深入的解释每个人的行为，使得准确确定每个人的特定身份变得困难。为解决这个挑战，我们构建了人类中心的视频监控描述数据集，该数据集包括7,820个人的动态行为描述。具体来说，我们在1,012个视频中标注了每个人的位置、衣物和场景中的其他元素之间的互动，这些人分布在1,012个视频中。基于这个数据集，我们可以将每个人的行为联系到其自己，使得可以在监控视频中进行每个人的行为分析。此外，我们提出了一种新的视频描述方法，可以在人类基础上详细描述每个人的行为，达到当前领域的状态作卷。为便于未来的研究，我们计划将数据集和代码发布。
</details></li>
</ul>
<hr>
<h2 id="A-Grammatical-Compositional-Model-for-Video-Action-Detection"><a href="#A-Grammatical-Compositional-Model-for-Video-Action-Detection" class="headerlink" title="A Grammatical Compositional Model for Video Action Detection"></a>A Grammatical Compositional Model for Video Action Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02887">http://arxiv.org/abs/2310.02887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhijun Zhang, Xu Zou, Jiahuan Zhou, Sheng Zhong, Ying Wu</li>
<li>for: 本研究旨在提高视频中人类行为分类的精度，通过分析人员动作和物品之间的互动关系。</li>
<li>methods: 本研究提出了一种基于通用和或图的 grammatical compositional model (GCM)，利用语法模型的compositional property和深度神经网络的表达能力，以提高行为分类的精度。</li>
<li>results: 实验表明，GCM模型在AVA数据集和Something-Else任务中表现出色，而且可以通过推理分析过程提高 interpretability。<details>
<summary>Abstract</summary>
Analysis of human actions in videos demands understanding complex human dynamics, as well as the interaction between actors and context. However, these interaction relationships usually exhibit large intra-class variations from diverse human poses or object manipulations, and fine-grained inter-class differences between similar actions. Thus the performance of existing methods is severely limited. Motivated by the observation that interactive actions can be decomposed into actor dynamics and participating objects or humans, we propose to investigate the composite property of them. In this paper, we present a novel Grammatical Compositional Model (GCM) for action detection based on typical And-Or graphs. Our model exploits the intrinsic structures and latent relationships of actions in a hierarchical manner to harness both the compositionality of grammar models and the capability of expressing rich features of DNNs. The proposed model can be readily embodied into a neural network module for efficient optimization in an end-to-end manner. Extensive experiments are conducted on the AVA dataset and the Something-Else task to demonstrate the superiority of our model, meanwhile the interpretability is enhanced through an inference parsing procedure.
</details>
<details>
<summary>摘要</summary>
要分析视频中人类行为，需要理解人类动态和人类与context的交互关系。然而，这些交互关系通常具有大量内类变化和细致的间类差异，使得现有方法表现有限。我们受到人类交互行为可以分解为actor动态和参与人或物品的观察的想法所 inspirited，我们提出了一种新的语法组合模型（GCM）。我们的模型利用人类行为的内在结构和隐藏关系，在层次结构中充分发挥语法模型的compositional性和深度学习模型的表达能力。该模型可以轻松地被嵌入到深度学习网络中，并通过端到端的优化来提高性能。我们在AVA数据集和Something-Else任务中进行了广泛的实验，并通过推理分析过程提高了可读性。
</details></li>
</ul>
<hr>
<h2 id="Multi-Resolution-Fusion-for-Fully-Automatic-Cephalometric-Landmark-Detection"><a href="#Multi-Resolution-Fusion-for-Fully-Automatic-Cephalometric-Landmark-Detection" class="headerlink" title="Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark Detection"></a>Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02855">http://arxiv.org/abs/2310.02855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongqian Guo, Wencheng Han</li>
<li>for: 验证架构在斜视X光像中的首要脊标定附近精度。</li>
<li>methods: 使用多分辨率的图像pyramid结构，并将不同的识别场合输入到多个模型中，以实现最佳的特征组合。 </li>
<li>results: 在2023年首届 Cephalometric Landmark Detection in Lateral X-ray Images 挑战中，实现了1.62mm的 Mean Radial Error (MRE) 和2.0mm的 Success Detection Rate (SDR) 74.18%。<details>
<summary>Abstract</summary>
Cephalometric landmark detection on lateral skull X-ray images plays a crucial role in the diagnosis of certain dental diseases. Accurate and effective identification of these landmarks presents a significant challenge. Based on extensive data observations and quantitative analyses, we discovered that visual features from different receptive fields affect the detection accuracy of various landmarks differently. As a result, we employed an image pyramid structure, integrating multiple resolutions as input to train a series of models with different receptive fields, aiming to achieve the optimal feature combination for each landmark. Moreover, we applied several data augmentation techniques during training to enhance the model's robustness across various devices and measurement alternatives. We implemented this method in the Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challenge and achieved a Mean Radial Error (MRE) of 1.62 mm and a Success Detection Rate (SDR) 2.0mm of 74.18% in the final testing phase.
</details>
<details>
<summary>摘要</summary>
依据广泛的数据观察和量化分析，我们发现了不同视觉特征场景下的不同影响因素。为了实现最佳的特征组合，我们采用了图像 pyramid 结构，将多个分辨率作为输入，并在不同的层次上训练多个模型，每个模型具有不同的视觉场景。此外，我们在训练过程中应用了多种数据增强技术，以提高模型在不同设备和测量方法下的Robustness。我们在 Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challenge 中实现了一个 Mean Radial Error (MRE) 为 1.62 mm 和 Success Detection Rate (SDR) 2.0 mm 的 74.18%。
</details></li>
</ul>
<hr>
<h2 id="Magicremover-Tuning-free-Text-guided-Image-inpainting-with-Diffusion-Models"><a href="#Magicremover-Tuning-free-Text-guided-Image-inpainting-with-Diffusion-Models" class="headerlink" title="Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models"></a>Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02848">http://arxiv.org/abs/2310.02848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyuan Yang, Lu Zhang, Liqian Ma, Yu Liu, JingJing Fu, You He</li>
<li>for: 填充图像中缺失的像素区域的内容，使其具有可见性和semantic plausibility。</li>
<li>methods: 利用涉及扩散模型的强大泛化模型进行文本引导图像填充。提出了一种无需调整的注意力引导策略，以便在扩散模型的抽象过程中阻止指定区域的消除和 occluded content的还原。还提出了一种分类优化算法，以促进抽象过程中的净化稳定性。</li>
<li>results: 与现状方法相比，MagicRemover显示出了显著的提升，特别是在高质量图像填充方面。经过了量化评估和用户调查，MagicRemover的性能在图像填充任务中具有显著优势。代码将在<a target="_blank" rel="noopener" href="https://github.com/exisas/Magicremover%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/exisas/Magicremover上发布。</a><details>
<summary>Abstract</summary>
Image inpainting aims to fill in the missing pixels with visually coherent and semantically plausible content. Despite the great progress brought from deep generative models, this task still suffers from i. the difficulties in large-scale realistic data collection and costly model training; and ii. the intrinsic limitations in the traditionally user-defined binary masks on objects with unclear boundaries or transparent texture. In this paper, we propose MagicRemover, a tuning-free method that leverages the powerful diffusion models for text-guided image inpainting. We introduce an attention guidance strategy to constrain the sampling process of diffusion models, enabling the erasing of instructed areas and the restoration of occluded content. We further propose a classifier optimization algorithm to facilitate the denoising stability within less sampling steps. Extensive comparisons are conducted among our MagicRemover and state-of-the-art methods including quantitative evaluation and user study, demonstrating the significant improvement of MagicRemover on high-quality image inpainting. We will release our code at https://github.com/exisas/Magicremover.
</details>
<details>
<summary>摘要</summary>
Image Inpainting 目标是填充缺失像素为视觉一致和Semantically plausible的内容。尽管深度生成模型已经带来了很大的进步，但这项任务仍然受到以下两种困难的限制：一是大规模的真实数据收集和成本高昂的模型训练; 二是传统的用户定义的二进制面Masks on objects with unclear boundaries or transparent texture。在这篇论文中，我们提出 MagicRemover，一种不需要调整的方法，利用了强大的扩散模型进行文本引导的图像填充。我们提出了一种注意力引导策略，以制限扩散模型的采样过程，使得可以Erase instructed areas and restore occluded content。我们还提出了一种分类优化算法，以便在 fewer sampling steps 中提高图像填充的稳定性。我们对 MagicRemover 和现有方法进行了广泛的比较，包括量化评估和用户研究，并示出 MagicRemover 在高质量图像填充方面具有显著改进。我们将在 GitHub 上发布我们的代码，链接为 <https://github.com/exisas/Magicremover>。
</details></li>
</ul>
<hr>
<h2 id="Delving-into-CLIP-latent-space-for-Video-Anomaly-Recognition"><a href="#Delving-into-CLIP-latent-space-for-Video-Anomaly-Recognition" class="headerlink" title="Delving into CLIP latent space for Video Anomaly Recognition"></a>Delving into CLIP latent space for Video Anomaly Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02835">http://arxiv.org/abs/2310.02835</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luca-zanella-dvl/AnomalyCLIP">https://github.com/luca-zanella-dvl/AnomalyCLIP</a></li>
<li>paper_authors: Luca Zanella, Benedetta Liberatori, Willi Menapace, Fabio Poiesi, Yiming Wang, Elisa Ricci</li>
<li>for: 检测和识别视频中的异常场景，包括视频水平监管和异常检测等应用场景。</li>
<li>methods: 提出了一种新的方法AnomalyCLIP，利用Large Language and Vision（LLV）模型CLIP，并结合多个实例学习来实现视频异常检测和分类。</li>
<li>results: 对三个主要异常检测标准 datasets（ShanghaiTech、UCF-Crime和XD-Violence）进行比较，研究表明AnomalyCLIP在识别视频异常场景方面表现出色，比基elines表现更高。<details>
<summary>Abstract</summary>
We tackle the complex problem of detecting and recognising anomalies in surveillance videos at the frame level, utilising only video-level supervision. We introduce the novel method AnomalyCLIP, the first to combine Large Language and Vision (LLV) models, such as CLIP, with multiple instance learning for joint video anomaly detection and classification. Our approach specifically involves manipulating the latent CLIP feature space to identify the normal event subspace, which in turn allows us to effectively learn text-driven directions for abnormal events. When anomalous frames are projected onto these directions, they exhibit a large feature magnitude if they belong to a particular class. We also introduce a computationally efficient Transformer architecture to model short- and long-term temporal dependencies between frames, ultimately producing the final anomaly score and class prediction probabilities. We compare AnomalyCLIP against state-of-the-art methods considering three major anomaly detection benchmarks, i.e. ShanghaiTech, UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines in recognising video anomalies.
</details>
<details>
<summary>摘要</summary>
我们面临着视频监测中的复杂问题，即在帧级别上检测和识别异常。我们介绍了一种新的方法——异常CLIP，这是首次将大语言和视觉（LLV）模型，如CLIP，与多个实例学习结合用于共同的视频异常检测和分类。我们的方法特点在于在CLIP特征空间中扭曲normal事件子空间，这使得我们可以有效地通过文本驱动的方式学习异常事件的指令。当异常帧被投影到这些指令上时，它们会表现出大的特征 магнитуد。我们还引入了一种高效的Transformer架构来模型帧之间的短期和长期 temporaldependencies，最终生成异常分数和分类概率。我们与状态的方法进行比较，考虑了三个主要的异常检测 benchmark，即上海工程大学、UCF-Crime和XD-Violence，并经验显示，AnomalyCLIP在识别视频异常的任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="All-Sizes-Matter-Improving-Volumetric-Brain-Segmentation-on-Small-Lesions"><a href="#All-Sizes-Matter-Improving-Volumetric-Brain-Segmentation-on-Small-Lesions" class="headerlink" title="All Sizes Matter: Improving Volumetric Brain Segmentation on Small Lesions"></a>All Sizes Matter: Improving Volumetric Brain Segmentation on Small Lesions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02829">http://arxiv.org/abs/2310.02829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayhan Can Erdur, Daniel Scholz, Josef A. Buchner, Stephanie E. Combs, Daniel Rueckert, Jan C. Peeken<br>for:* 这种研究旨在提高脑 метаstatic radiosurgery中的 lesion detection和 segmentation精度，以便更好地地图出脑中的多个恶性肿瘤。methods:* 这种方法使用了多种神经网络模型，包括 blob 损失函数、差分序列和专门针对小肿瘤模型。results:* 实验表明，使用 blob 损失函数和差分序列可以提高 segmentation 结果，但是包含专门针对小肿瘤模型的ensemble 会导致 segmentation 结果下降。此外，通过基于领域知识的后处理步骤可以提高结果的精度。<details>
<summary>Abstract</summary>
Brain metastases (BMs) are the most frequently occurring brain tumors. The treatment of patients having multiple BMs with stereo tactic radiosurgery necessitates accurate localization of the metastases. Neural networks can assist in this time-consuming and costly task that is typically performed by human experts. Particularly challenging is the detection of small lesions since they are often underrepresented in exist ing approaches. Yet, lesion detection is equally important for all sizes. In this work, we develop an ensemble of neural networks explicitly fo cused on detecting and segmenting small BMs. To accomplish this task, we trained several neural networks focusing on individual aspects of the BM segmentation problem: We use blob loss that specifically addresses the imbalance of lesion instances in terms of size and texture and is, therefore, not biased towards larger lesions. In addition, a model using a subtraction sequence between the T1 and T1 contrast-enhanced sequence focuses on low-contrast lesions. Furthermore, we train additional models only on small lesions. Our experiments demonstrate the utility of the ad ditional blob loss and the subtraction sequence. However, including the specialized small lesion models in the ensemble deteriorates segmentation results. We also find domain-knowledge-inspired postprocessing steps to drastically increase our performance in most experiments. Our approach enables us to submit a competitive challenge entry to the ASNR-MICCAI BraTS Brain Metastasis Challenge 2023.
</details>
<details>
<summary>摘要</summary>
脑肿瘤（BM）是最常见的脑肿瘤，它们的治疗需要精准地LOCAL化肿瘤。人工智能可以帮助在这种时间consuming和成本高的任务中提高精度。特别是检测小肿瘤是非常困难的，因为它们经常被忽略或排除在现有的方法中。然而，检测所有肿瘤的大小都很重要。在这种工作中，我们开发了一个ensemble of neural networks，专门用于检测和分割小BM。为了完成这项任务，我们训练了多个神经网络，每个神经网络都专注于各自的BM分割问题方面。我们使用blob损失，这种损失特别关注肿瘤实例中尺寸和文化的不均衡问题，因此不受大肿瘤的干扰。此外，我们还使用了一个基于T1和T1增强序列的抽象序列，专门用于低对比度肿瘤。此外，我们还训练了额外的小肿瘤模型。我们的实验表明，采用这些方法可以提高我们的性能。然而，包括特殊的小肿瘤模型在ensemble中会下降分割结果。我们还发现了基于领域知识的后处理步骤，可以帮助我们大幅提高性能。我们的方法使得我们能够提交竞争力强的挑战提交到ASNR-MICCAI BraTS脑肿瘤挑战2023。
</details></li>
</ul>
<hr>
<h2 id="CoBEV-Elevating-Roadside-3D-Object-Detection-with-Depth-and-Height-Complementarity"><a href="#CoBEV-Elevating-Roadside-3D-Object-Detection-with-Depth-and-Height-Complementarity" class="headerlink" title="CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity"></a>CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02815">http://arxiv.org/abs/2310.02815</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MasterHow/CoBEV">https://github.com/MasterHow/CoBEV</a></li>
<li>paper_authors: Hao Shi, Chengshan Pang, Jiaming Zhang, Kailun Yang, Yuhao Wu, Huajian Ni, Yining Lin, Rainer Stiefelhagen, Kaiwei Wang</li>
<li>for: 本研究旨在提高路面摄像头驱动的3D物体检测精度和Robustness，扩展视觉中心的感知范围，并提高道路安全性。</li>
<li>methods: 本研究提出了一种新的综合BEV检测框架，称为Complementary-BEV（CoBEV），它将深度和高度Integrate到构建robust BEV表示中，并使用新提出的两Stage complementary特征选择（CFS）模块进行水平融合。此外，还integrated了BEV特征缩减框架，以进一步提高检测精度。</li>
<li>results: 对于公共的3D检测标准 benchmarks DAIR-V2X-I和Rope3D以及私人的Supremind-Road数据集进行了广泛的实验，结果表明，CoBEV不仅实现了新的状态之精度，还显著提高了之前方法在长距离enario和摄像头干扰中的Robustness，并在不同的场景和摄像头参数下进行了大幅度的提高。此外，CoBEV还达到了在DAIR-V2X-I上的汽车AP分数80%的新纪录。<details>
<summary>Abstract</summary>
Roadside camera-driven 3D object detection is a crucial task in intelligent transportation systems, which extends the perception range beyond the limitations of vision-centric vehicles and enhances road safety. While previous studies have limitations in using only depth or height information, we find both depth and height matter and they are in fact complementary. The depth feature encompasses precise geometric cues, whereas the height feature is primarily focused on distinguishing between various categories of height intervals, essentially providing semantic context. This insight motivates the development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D object detection framework that integrates depth and height to construct robust BEV representations. In essence, CoBEV estimates each pixel's depth and height distribution and lifts the camera features into 3D space for lateral fusion using the newly proposed two-stage complementary feature selection (CFS) module. A BEV feature distillation framework is also seamlessly integrated to further enhance the detection accuracy from the prior knowledge of the fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as the private Supremind-Road dataset, demonstrating that CoBEV not only achieves the accuracy of the new state-of-the-art, but also significantly advances the robustness of previous methods in challenging long-distance scenarios and noisy camera disturbance, and enhances generalization by a large margin in heterologous settings with drastic changes in scene and camera parameters. For the first time, the vehicle AP score of a camera model reaches 80% on DAIR-V2X-I in terms of easy mode. The source code will be made publicly available at https://github.com/MasterHow/CoBEV.
</details>
<details>
<summary>摘要</summary>
“路边摄像头驱动的3D对象检测是智能交通系统中的关键任务，可以扩展感知范围 beyond 视觉中心的车辆和提高道路安全性。而前一些研究受限于只使用深度或高度信息，我们发现深度和高度都是重要的，并且它们是 complementary。这种见解驱动我们开发了Complementary-BEV（CoBEV），一种基于端到端的单目3D对象检测框架，它将深度和高度集成到构建robust BEV表示中。具体来说，CoBEV每个像素的深度和高度分布，并将摄像头特征抬升到3D空间进行水平融合，使用我们提出的新的两stage complementary特征选择（CFS）模块。此外，我们还融合了BEV特征精炼框架，以进一步提高检测精度。我们在公共的3D检测标准 benchmarks 上进行了广泛的实验，包括DAIR-V2X-I和Rope3D，以及私人的Supremind-Road数据集，并证明了CoBEV不仅实现了新的状态机器的精度，而且也提高了前一些方法在远程场景和摄像头干扰下的Robustness，并增加了对不同场景和摄像头参数的适应性。此外，我们还首次实现了摄像头模型的车辆AP分数达到80%在DAIR-V2X-I中，在易模式下。我们将代码公开于https://github.com/MasterHow/CoBEV。”
</details></li>
</ul>
<hr>
<h2 id="Tracking-Anything-in-Heart-All-at-Once"><a href="#Tracking-Anything-in-Heart-All-at-Once" class="headerlink" title="Tracking Anything in Heart All at Once"></a>Tracking Anything in Heart All at Once</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02792">http://arxiv.org/abs/2310.02792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Chengkang Shen, Hao Zhu, You Zhou, Yu Liu, Si Yi, Lili Dong, Weipeng Zhao, David J. Brady, Xun Cao, Zhan Ma, Yi Lin</li>
<li>For: This paper aims to improve the accuracy and efficiency of myocardial motion tracking in cardiac imaging, with the goal of early detection and prevention of Cardiovascular Diseases (CVDs).* Methods: The Neural Cardiac Motion Field (NeuralCMF) method uses implicit neural representation (INR) to model the 3D structure and comprehensive 6D forward&#x2F;backward motion of the heart, without the need for paired datasets or self-supervised optimization.* Results: Experimental validations across three representative datasets demonstrate the robustness and innovative nature of the NeuralCMF, with significant advantages over existing state-of-the-art methods in cardiac imaging and motion tracking.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究目的是提高心脏动态跟踪的精度和效率，以预防和检测心血管疾病（CVD）。</li>
<li>methods: 本方法使用启发式神经表示（INR）模型心脏3D结构和全向&#x2F;反向6D动态运动，不需要对称数据集或自动化优化。</li>
<li>results: 三个示例数据集的实验验证表明NeuralCMF具有Robustness和创新性，与现有的心脏成像和动态跟踪方法相比具有显著优势。<details>
<summary>Abstract</summary>
Myocardial motion tracking stands as an essential clinical tool in the prevention and detection of Cardiovascular Diseases (CVDs), the foremost cause of death globally. However, current techniques suffer incomplete and inaccurate motion estimation of the myocardium both in spatial and temporal dimensions, hindering the early identification of myocardial dysfunction. In addressing these challenges, this paper introduces the Neural Cardiac Motion Field (NeuralCMF). NeuralCMF leverages the implicit neural representation (INR) to model the 3D structure and the comprehensive 6D forward/backward motion of the heart. This approach offers memory-efficient storage and continuous capability to query the precise shape and motion of the myocardium throughout the cardiac cycle at any specific point. Notably, NeuralCMF operates without the need for paired datasets, and its optimization is self-supervised through the physics knowledge priors both in space and time dimensions, ensuring compatibility with both 2D and 3D echocardiogram video inputs. Experimental validations across three representative datasets support the robustness and innovative nature of the NeuralCMF, marking significant advantages over existing state-of-the-arts in cardiac imaging and motion tracking.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LROC-PANGU-GAN-Closing-the-Simulation-Gap-in-Learning-Crater-Segmentation-with-Planetary-Simulators"><a href="#LROC-PANGU-GAN-Closing-the-Simulation-Gap-in-Learning-Crater-Segmentation-with-Planetary-Simulators" class="headerlink" title="LROC-PANGU-GAN: Closing the Simulation Gap in Learning Crater Segmentation with Planetary Simulators"></a>LROC-PANGU-GAN: Closing the Simulation Gap in Learning Crater Segmentation with Planetary Simulators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02781">http://arxiv.org/abs/2310.02781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaewon La, Jaime Phadke, Matt Hutton, Marius Schwinning, Gabriele De Canio, Florian Renk, Lars Kunze, Matthew Gadd<br>for: 这个研究旨在提高探测器在外星表面上降落时能够预测和避免危险，例如峭壁或深潭可能会对探测器的降落和运作造成严重的风险。methods: 这个研究使用了深度学习模型，并将其训练在具有不同类型的数据集上，包括实际拍摄的LROC图像和模拟的PANGU图像。results: 研究结果显示，使用这种方法可以提高探测器降落时的危险预测性，并且在实际拍摄的LROC图像上实现了更好的分类性能。<details>
<summary>Abstract</summary>
It is critical for probes landing on foreign planetary bodies to be able to robustly identify and avoid hazards - as, for example, steep cliffs or deep craters can pose significant risks to a probe's landing and operational success. Recent applications of deep learning to this problem show promising results. These models are, however, often learned with explicit supervision over annotated datasets. These human-labelled crater databases, such as from the Lunar Reconnaissance Orbiter Camera (LROC), may lack in consistency and quality, undermining model performance - as incomplete and/or inaccurate labels introduce noise into the supervisory signal, which encourages the model to learn incorrect associations and results in the model making unreliable predictions. Physics-based simulators, such as the Planet and Asteroid Natural Scene Generation Utility, have, in contrast, perfect ground truth, as the internal state that they use to render scenes is known with exactness. However, they introduce a serious simulation-to-real domain gap - because of fundamental differences between the simulated environment and the real-world arising from modelling assumptions, unaccounted for physical interactions, environmental variability, etc. Therefore, models trained on their outputs suffer when deployed in the face of realism they have not encountered in their training data distributions. In this paper, we therefore introduce a system to close this "realism" gap while retaining label fidelity. We train a CycleGAN model to synthesise LROC from Planet and Asteroid Natural Scene Generation Utility (PANGU) images. We show that these improve the training of a downstream crater segmentation network, with segmentation performance on a test set of real LROC images improved as compared to using only simulated PANGU images.
</details>
<details>
<summary>摘要</summary>
为了使外星体上的探测器能够坚固地标识和避免障碍，例如峭壁或深沟，是非常重要的。现在，深度学习在这个问题上的应用已经显示出了扎实的结果。然而，这些模型通常是通过明确的监督来学习的，而这些由人 annotated的坑数据库，如Lunar Reconnaissance Orbiter Camera (LROC)，可能缺乏一致性和质量，从而影响模型的性能。 incomplete和/或不准确的标签会把附加的噪声引入监督信号中，使模型学习错误的关联，导致模型的预测不可靠。相比之下，物理学习器，如Planet and Asteroid Natural Scene Generation Utility (PANGU)，有完美的真实信息，因为它们使用的内部状态是known with exactness。然而，它们又存在真实世界和 simulate world之间的差异，这些差异来自模型假设、不计算的物理交互、环境变化等。因此，使用它们生成的输出来训练模型会导致模型在面对实际情况时表现不佳。为了关闭这个“现实”差异，我们因此引入一个系统，用于在保持标签准确性的情况下，将LROC从PANGU图像中生成Synthesize。我们显示，这些改进了下游坑分割网络的测试集性能。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Shuffle-An-Efficient-Channel-Mixture-Method"><a href="#Dynamic-Shuffle-An-Efficient-Channel-Mixture-Method" class="headerlink" title="Dynamic Shuffle: An Efficient Channel Mixture Method"></a>Dynamic Shuffle: An Efficient Channel Mixture Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02776">http://arxiv.org/abs/2310.02776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaijun Gong, Zhuowen Yin, Yushu Li, Kailing Guo, Xiangmin Xu</li>
<li>for: 提高ShuffleNet的性能</li>
<li>methods: 提出了一种数据依存的混合方法，通过动态生成数据依存的 permutation 矩阵来减少数据中的重复性</li>
<li>results: 实验结果表明，与常见的点 wise 卷积相比，提出的方法可以减少数据中的重复性，并在图像分类 benchmark 上显著提高 ShuffleNet 的性能<details>
<summary>Abstract</summary>
The redundancy of Convolutional neural networks not only depends on weights but also depends on inputs. Shuffling is an efficient operation for mixing channel information but the shuffle order is usually pre-defined. To reduce the data-dependent redundancy, we devise a dynamic shuffle module to generate data-dependent permutation matrices for shuffling. Since the dimension of permutation matrix is proportional to the square of the number of input channels, to make the generation process efficiently, we divide the channels into groups and generate two shared small permutation matrices for each group, and utilize Kronecker product and cross group shuffle to obtain the final permutation matrices. To make the generation process learnable, based on theoretical analysis, softmax, orthogonal regularization, and binarization are employed to asymptotically approximate the permutation matrix. Dynamic shuffle adaptively mixes channel information with negligible extra computation and memory occupancy. Experiment results on image classification benchmark datasets CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet have shown that our method significantly increases ShuffleNets' performance. Adding dynamic generated matrix with learnable static matrix, we further propose static-dynamic-shuffle and show that it can serve as a lightweight replacement of ordinary pointwise convolution.
</details>
<details>
<summary>摘要</summary>
卷积神经网络中的重复性不仅取决于权重，还取决于输入。混合是一种有效的操作，可以混合通道信息，但混合顺序通常是预定的。为了减少数据相关的重复性，我们提出了一种动态混合模块，可以生成数据相关的 permutation 矩阵。由于 permutation 矩阵的维度与输入通道数平方成正，为了使生成过程高效，我们将通道分为组，并生成每组两个共享的小 permutation 矩阵，然后使用 Kronecker 乘法和跨组混合来获得最终的 permutation 矩阵。为了让生成过程学习，我们采用了理论分析，使用 softmax、正则化和归一化来渐近地 aproximate permutation 矩阵。动态混合可以有效地混合通道信息，并且增加了 ShuffleNets 的性能。此外，我们还提出了静态-动态混合，可以作为普通点 wise 混合的轻量级替换。
</details></li>
</ul>
<hr>
<h2 id="SHOT-Suppressing-the-Hessian-along-the-Optimization-Trajectory-for-Gradient-Based-Meta-Learning"><a href="#SHOT-Suppressing-the-Hessian-along-the-Optimization-Trajectory-for-Gradient-Based-Meta-Learning" class="headerlink" title="SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning"></a>SHOT: Suppressing the Hessian along the Optimization Trajectory for Gradient-Based Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02751">http://arxiv.org/abs/2310.02751</a></li>
<li>repo_url: None</li>
<li>paper_authors: JunHoo Lee, Jayeon Yoo, Nojun Kwak</li>
<li>for: 这篇论文旨在探讨Gradient-based meta-learning（GBML）中隐藏的Hessian问题，并提出一个名为SHOT（Suppressing the Hessian along the Optimization Trajectory）的算法来解决这个问题。</li>
<li>methods: 这篇论文使用SHOT算法，它将对GBML中的内部迭代进行修改，以减少内部迭代中Hessian的使用。SHOT算法不增加基eline模型的计算量多少。</li>
<li>results: 这篇论文通过实验证明SHOT算法可以对GBML中的Hessian进行隐藏，并且SHOT算法可以在标准的几个shot学习任务上表现更好。<details>
<summary>Abstract</summary>
In this paper, we hypothesize that gradient-based meta-learning (GBML) implicitly suppresses the Hessian along the optimization trajectory in the inner loop. Based on this hypothesis, we introduce an algorithm called SHOT (Suppressing the Hessian along the Optimization Trajectory) that minimizes the distance between the parameters of the target and reference models to suppress the Hessian in the inner loop. Despite dealing with high-order terms, SHOT does not increase the computational complexity of the baseline model much. It is agnostic to both the algorithm and architecture used in GBML, making it highly versatile and applicable to any GBML baseline. To validate the effectiveness of SHOT, we conduct empirical tests on standard few-shot learning tasks and qualitatively analyze its dynamics. We confirm our hypothesis empirically and demonstrate that SHOT outperforms the corresponding baseline. Code is available at: https://github.com/JunHoo-Lee/SHOT
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们假设Gradient-based Meta-Learning（GBML）内loop中隐式地抑制梯度。基于这个假设，我们提出了一种名为SHOT（抑制优化轨迹上的梯度）的算法，该算法的目的是将目标和参考模型参数的距离尽可能小，以抑制内loop中的梯度。尽管处理高阶项，SHOT不会增加基eline模型的计算复杂度很多。它是GBML任何基eline模型的不可知数，可以应用于任何GBML基eline模型。为了验证SHOT的效果，我们进行了标准的少数shot学习任务的实验，并质量分析其动态。我们确认了我们的假设，并证明SHOT在对应的基eline模型上表现出色。代码可以在以下链接中找到：https://github.com/JunHoo-Lee/SHOT。
</details></li>
</ul>
<hr>
<h2 id="Condition-numbers-in-multiview-geometry-instability-in-relative-pose-estimation-and-RANSAC"><a href="#Condition-numbers-in-multiview-geometry-instability-in-relative-pose-estimation-and-RANSAC" class="headerlink" title="Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC"></a>Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02719">http://arxiv.org/abs/2310.02719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Fan, Joe Kileel, Benjamin Kimia</li>
<li>for: 本研究旨在提供一个数值状况分析框架，用于多视角几何中的最小问题，利用计算代数和里曼几何工具。</li>
<li>methods: 本研究使用计算代数和里曼几何工具来分析最小问题的数值状况，并提出一个数学方法来评估最小问题的状况。</li>
<li>results: 本研究发现，在某些世界场景下，5-和7-点最小问题会具有无限几何状况，即使没有外围值和足够的数据支持一个假设。此外，本研究还发现，使用RANSAC算法可以除除外围值，并且可以选择良好的数据，以避免无限几何状况。<details>
<summary>Abstract</summary>
In this paper we introduce a general framework for analyzing the numerical conditioning of minimal problems in multiple view geometry, using tools from computational algebra and Riemannian geometry. Special motivation comes from the fact that relative pose estimation, based on standard 5-point or 7-point Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are present and there is enough data to support a hypothesis. We argue that these cases arise due to the intrinsic instability of the 5- and 7-point minimal problems. We apply our framework to characterize the instabilities, both in terms of the world scenes that lead to infinite condition number, and directly in terms of ill-conditioned image data. The approach produces computational tests for assessing the condition number before solving the minimal problem. Lastly synthetic and real data experiments suggest that RANSAC serves not only to remove outliers, but also to select for well-conditioned image data, as predicted by our theory.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种总体框架，用于分析多视图几何中的数值条件，使用计算代数和里曼几何工具。特别的动机来自于5点或7点随机抽样Consensus（RANSAC）算法在无异常情况下仍然失败的现象。我们 argue这些情况 arise due to the intrinsic instability of the 5- and 7-point minimal problems。我们应用了这种框架，来描述这些不稳定性，包括世界场景导致condition number为∞的情况，以及直接基于图像数据的ill-conditioned情况。该方法生成了Before solving the minimal problem的计算测试。最后，我们在 sintetic和实际数据 экспериментах中发现，RANSAC不仅可以 removing outliers，还可以选择良好的conditioned image data，与我们的理论相符。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Pan-Sharpening-via-Generalized-Inverse"><a href="#Understanding-Pan-Sharpening-via-Generalized-Inverse" class="headerlink" title="Understanding Pan-Sharpening via Generalized Inverse"></a>Understanding Pan-Sharpening via Generalized Inverse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02718">http://arxiv.org/abs/2310.02718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqi Liu, Yutong Bai, Xinyang Han, Alan Yuille</li>
<li>for: 这 paper 的目的是提出一种基于照片图像和多spectral 图像的 Pan-sharpening 算法，以获得高空间和高spectral 图像。</li>
<li>methods: 这 paper 使用了一种简单的矩阵方程来描述 Pan-sharpening 问题，并提出了一种基于普通 inverse 矩阵理论的两种通用 inverse 矩阵方法，其中一种是component substitution 方法，另一种是 multi-resolution analysis 方法。</li>
<li>results: 该 paper 通过 synthetic 实验和实际数据实验表明，提出的方法比其他方法更好和更锐化，并且在实际实验中，下采样增强效果显著。<details>
<summary>Abstract</summary>
Pan-sharpening algorithm utilizes panchromatic image and multispectral image to obtain a high spatial and high spectral image. However, the optimizations of the algorithms are designed with different standards. We adopt the simple matrix equation to describe the Pan-sharpening problem. The solution existence condition and the acquirement of spectral and spatial resolution are discussed. A down-sampling enhancement method was introduced for better acquiring the spatial and spectral down-sample matrices. By the generalized inverse theory, we derived two forms of general inverse matrix formulations that can correspond to the two prominent classes of Pan-sharpening methods, that is, component substitution and multi-resolution analysis methods. Specifically, the Gram Schmidt Adaptive(GSA) was proved to follow the general inverse matrix formulation of component substitution. A model prior to the general inverse matrix of the spectral function was rendered. The theoretical errors are analyzed. Synthetic experiments and real data experiments are implemented. The proposed methods are better and sharper than other methods qualitatively in both synthetic and real experiments. The down-sample enhancement effect is shown of better results both quantitatively and qualitatively in real experiments. The generalized inverse matrix theory help us better understand the Pan-sharpening.
</details>
<details>
<summary>摘要</summary>
泛化简化算法使用泛chromatic图像和多spectral图像获得高空间和高spectral图像。然而，优化算法的标准不同。我们采用简单矩阵方程来描述泛化简化问题。解存问题和spectral和空间分辨率的获得是讨论的。我们引入了下采样提高方法以更好地获得空间和spectral下采样矩阵。通过总 inverse理论，我们 deriv了两种通用 inverse矩阵形式，可以与两种主要的泛化简化方法相匹配，即component substitute和多resolution分析方法。Specifically，Gram Schmidt Adaptive(GSA)被证明遵循了component substitute的通用 inverse矩阵形式。我们制定了spectral函数的模型先验。理论错误是分析。我们实现了synthetic和实际数据实验。提出的方法比其他方法更好和更加锐化，并且在实际实验中下采样提高效果明显。通过总 inverse矩阵理论，我们更好地理解泛化简化。
</details></li>
</ul>
<hr>
<h2 id="GETAvatar-Generative-Textured-Meshes-for-Animatable-Human-Avatars"><a href="#GETAvatar-Generative-Textured-Meshes-for-Animatable-Human-Avatars" class="headerlink" title="GETAvatar: Generative Textured Meshes for Animatable Human Avatars"></a>GETAvatar: Generative Textured Meshes for Animatable Human Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02714">http://arxiv.org/abs/2310.02714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/magic-research/GETAvatar">https://github.com/magic-research/GETAvatar</a></li>
<li>paper_authors: Xuanmeng Zhang, Jianfeng Zhang, Rohan Chacko, Hongyi Xu, Guoxian Song, Yi Yang, Jiashi Feng</li>
<li>for: 这篇论文是为了解决3D-意识全身人体生成问题而写的，目标是创造高质量的图像和准确的几何结构。</li>
<li>methods: 该论文提出了一种名为GET Avatar的生成模型，该模型直接生成了明确的3D纹理图面，以实现高质量的图像生成。</li>
<li>results: 实验表明，GET Avatar可以在3D-意识人体生成中达到状态机器人表现，并且可以高效地生成512x512和1024x1024分辨率的图像。<details>
<summary>Abstract</summary>
We study the problem of 3D-aware full-body human generation, aiming at creating animatable human avatars with high-quality textures and geometries. Generally, two challenges remain in this field: i) existing methods struggle to generate geometries with rich realistic details such as the wrinkles of garments; ii) they typically utilize volumetric radiance fields and neural renderers in the synthesis process, making high-resolution rendering non-trivial. To overcome these problems, we propose GETAvatar, a Generative model that directly generates Explicit Textured 3D meshes for animatable human Avatar, with photo-realistic appearance and fine geometric details. Specifically, we first design an articulated 3D human representation with explicit surface modeling, and enrich the generated humans with realistic surface details by learning from the 2D normal maps of 3D scan data. Second, with the explicit mesh representation, we can use a rasterization-based renderer to perform surface rendering, allowing us to achieve high-resolution image generation efficiently. Extensive experiments demonstrate that GETAvatar achieves state-of-the-art performance on 3D-aware human generation both in appearance and geometry quality. Notably, GETAvatar can generate images at 512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving upon previous methods by 2x. Our code and models will be available.
</details>
<details>
<summary>摘要</summary>
我们研究3D意识整体人类生成问题，目标是创建可动人形模型，高质量文字和几何。通常，这个领域存在两个挑战：一是现有方法无法生成衣物皱纹等细节rich;二是通常利用volume radiance fields和神经渲染器进行synthesis，高分辨率渲染非常困难。为了解决这些问题，我们提出了GETavatar，一种生成模型，直接生成Explicit Textured 3D mesh，用于可动人形模型，具有真实的光照和细节。具体来说，我们首先设计了3D人体表示，并通过学习2D normal maps的3D扫描数据来增强生成的人体细节。其次，通过使用面积化渲染器，我们可以高效地渲染surface，并且可以 дости得高分辨率图像生成。我们的实验表明，GETavatar在3D意识人类生成中具有国际级表现，并且可以在512x512分辨率和1024x1024分辨率下生成图像，提高了前一代方法的2倍。我们的代码和模型将公开。
</details></li>
</ul>
<hr>
<h2 id="Multi-Dimension-Embedding-Aware-Modality-Fusion-Transformer-for-Psychiatric-Disorder-Clasification"><a href="#Multi-Dimension-Embedding-Aware-Modality-Fusion-Transformer-for-Psychiatric-Disorder-Clasification" class="headerlink" title="Multi-Dimension-Embedding-Aware Modality Fusion Transformer for Psychiatric Disorder Clasification"></a>Multi-Dimension-Embedding-Aware Modality Fusion Transformer for Psychiatric Disorder Clasification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02690">http://arxiv.org/abs/2310.02690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxin Wang, Xuyang Cao, Shan An, Fengmei Fan, Chao Zhang, Jinsong Wang, Feng Yu, Zhiren Wang</li>
<li>for: 这个研究旨在使用深度学习方法和脑成像技术来诊断心理疾病。</li>
<li>methods: 研究使用了多维度嵌入感知模式融合transformer（MFFormer），具体而言是使用Resting-state功能磁共振成像（rs-fMRI）和T1强化磁共振成像（T1w sMRI），两种不同的脑成像模式，以全盘的时间序列资料和三维空间资料进行融合。</li>
<li>results: 研究结果显示，使用MFFormer可以更好地诊断心理疾病，比使用单一modalities或多modalities MRI更好。<details>
<summary>Abstract</summary>
Deep learning approaches, together with neuroimaging techniques, play an important role in psychiatric disorders classification. Previous studies on psychiatric disorders diagnosis mainly focus on using functional connectivity matrices of resting-state functional magnetic resonance imaging (rs-fMRI) as input, which still needs to fully utilize the rich temporal information of the time series of rs-fMRI data. In this work, we proposed a multi-dimension-embedding-aware modality fusion transformer (MFFormer) for schizophrenia and bipolar disorder classification using rs-fMRI and T1 weighted structural MRI (T1w sMRI). Concretely, to fully utilize the temporal information of rs-fMRI and spatial information of sMRI, we constructed a deep learning architecture that takes as input 2D time series of rs-fMRI and 3D volumes T1w. Furthermore, to promote intra-modality attention and information fusion across different modalities, a fusion transformer module (FTM) is designed through extensive self-attention of hybrid feature maps of multi-modality. In addition, a dimension-up and dimension-down strategy is suggested to properly align feature maps of multi-dimensional from different modalities. Experimental results on our private and public OpenfMRI datasets show that our proposed MFFormer performs better than that using a single modality or multi-modality MRI on schizophrenia and bipolar disorder diagnosis.
</details>
<details>
<summary>摘要</summary>
深度学习方法和神经成像技术在心理疾病诊断中发挥重要作用。前期研究主要利用功能连接矩阵的resting-state功能磁共振成像(rs-fMRI)作为输入， ainda需要充分利用rs-fMRI数据的时间序列信息。在这项工作中，我们提出了一种多维度嵌入感觉模型融合转换器（MFFormer）用于诊断偏头痛和mania病。具体来说，为了充分利用rs-fMRI的时间信息和T1磁共振成像(T1w sMRI)的空间信息，我们构建了一个深度学习架构，输入2D时间序列rs-fMRI和3D尺寸T1w sMRI。此外，为了促进不同模态之间的内部注意力和信息融合，我们设计了一个融合转换器模块（FTM），通过了rs-fMRI和sMRI的混合特征地图进行了广泛的自注意。此外，我们还提出了一种维度上下降策略，以适应不同模态的特征维度不同。实验结果表明，我们提出的MFFormer在我们自己的私人数据集和公共OpenfMRI数据集上比使用单一模态或多模态MRI更好地诊断偏头痛和mania病。
</details></li>
</ul>
<hr>
<h2 id="PostRainBench-A-comprehensive-benchmark-and-a-new-model-for-precipitation-forecasting"><a href="#PostRainBench-A-comprehensive-benchmark-and-a-new-model-for-precipitation-forecasting" class="headerlink" title="PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting"></a>PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02676">http://arxiv.org/abs/2310.02676</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yyyujintang/PostRainBench">https://github.com/yyyujintang/PostRainBench</a></li>
<li>paper_authors: Yujin Tang, Jiaming Zhou, Xiang Pan, Zeying Gong, Junwei Liang</li>
<li>for: 该研究的目的是提高降水预测的准确性，以便更好地预测极端天气事件。</li>
<li>methods: 该研究使用了人工智能基于后处理技术，与传统的数值天气预测方法相结合，以提高预测准确性。</li>
<li>results: 实验结果显示，该方法在三个数据集上的降水预测准确率高于当前state-of-the-art方法，特别是在极端降水情况下表现出色，比传统数值天气预测方法提高15.6%, 17.4%, 和31.8%。<details>
<summary>Abstract</summary>
Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention Enhanced Multi-task Learning framework with a specially designed weighted loss function. Its flexible design allows for easy plug-and-play integration with various backbones. Extensive experimental results on the proposed benchmark show that our method outperforms state-of-the-art methods by 6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most notably, our model is the first deep learning-based method to outperform traditional Numerical Weather Prediction (NWP) approaches in extreme precipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over NWP predictions in heavy rain CSI on respective datasets. These results highlight the potential impact of our model in reducing the severe consequences of extreme weather events.
</details>
<details>
<summary>摘要</summary>
准确预测降水是科学和社会重要的挑战。数据驱动方法已成为解决这个挑战的广泛使用的解决方案。然而，凭借数据驱动方法alone做出的预测具有限制，因为它们无法模拟下面物理。将人工智能基于后处理技术与传统的数值天气预测（NWP）方法结合使用可以提高预测准确性。despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention Enhanced Multi-task Learning framework with a specially designed weighted loss function. Its flexible design allows for easy plug-and-play integration with various backbones. Extensive experimental results on the proposed benchmark show that our method outperforms state-of-the-art methods by 6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most notably, our model is the first deep learning-based method to outperform traditional Numerical Weather Prediction (NWP) approaches in extreme precipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over NWP predictions in heavy rain CSI on respective datasets. These results highlight the potential impact of our model in reducing the severe consequences of extreme weather events.
</details></li>
</ul>
<hr>
<h2 id="MedPrompt-Cross-Modal-Prompting-for-Multi-Task-Medical-Image-Translation"><a href="#MedPrompt-Cross-Modal-Prompting-for-Multi-Task-Medical-Image-Translation" class="headerlink" title="MedPrompt: Cross-Modal Prompting for Multi-Task Medical Image Translation"></a>MedPrompt: Cross-Modal Prompting for Multi-Task Medical Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02663">http://arxiv.org/abs/2310.02663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhang Chen, Chi-Man Pun, Shuqiang Wang</li>
<li>for: 医疗图像翻译任务的实现，用于补充临床诊断中缺失的模式数据。</li>
<li>methods: 提出了多任务框架MedPrompt，可以有效地翻译不同的模式。特别是，我们提出了自适应提示块，动态指导翻译网络向各自的模式。同时，我们还引入了提取提示块和融合提示块，以有效地编码跨模式提示。为了提高跨模式特征提取，我们将Transformer模型纳入系统。</li>
<li>results: 经验证明，我们的提案模型在五个数据集和四对模式之间具有最佳视觉质量和优秀的泛化能力。<details>
<summary>Abstract</summary>
Cross-modal medical image translation is an essential task for synthesizing missing modality data for clinical diagnosis. However, current learning-based techniques have limitations in capturing cross-modal and global features, restricting their suitability to specific pairs of modalities. This lack of versatility undermines their practical usefulness, particularly considering that the missing modality may vary for different cases. In this study, we present MedPrompt, a multi-task framework that efficiently translates different modalities. Specifically, we propose the Self-adaptive Prompt Block, which dynamically guides the translation network towards distinct modalities. Within this framework, we introduce the Prompt Extraction Block and the Prompt Fusion Block to efficiently encode the cross-modal prompt. To enhance the extraction of global features across diverse modalities, we incorporate the Transformer model. Extensive experimental results involving five datasets and four pairs of modalities demonstrate that our proposed model achieves state-of-the-art visual quality and exhibits excellent generalization capability.
</details>
<details>
<summary>摘要</summary>
医学多模态图像翻译是临床诊断中缺失模态数据的重要任务。然而，当前的学习基于技术有限于捕捉跨模态和全局特征，导致它们不适用于特定对比的情况。这种缺乏 universality 使得它们在实际应用中不太实用，特别是考虑到缺失的模态可能在不同的案例中不同。在本研究中，我们提出了 MedPrompt，一种多任务框架，可以效率地翻译不同的模态。具体来说，我们提出了自适应Prompt块，可以动态引导翻译网络向不同的模态。在这个框架中，我们引入了提取Prompt块和Prompt融合块，以高效地编码跨模态Prompt。为了增强不同模态之间的全局特征提取，我们采用了Transformer模型。我们对五个数据集和四对模态进行了广泛的实验，结果表明，我们提出的模型在视觉质量上达到了领先水平，并且具有优秀的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Active-Visual-Localization-for-Multi-Agent-Collaboration-A-Data-Driven-Approach"><a href="#Active-Visual-Localization-for-Multi-Agent-Collaboration-A-Data-Driven-Approach" class="headerlink" title="Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach"></a>Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02650">http://arxiv.org/abs/2310.02650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Hanlon, Boyang Sun, Marc Pollefeys, Hermann Blum</li>
<li>for: 本研究旨在使用活动视觉地标定解决多机器人或人机合作中视点变化带来的挑战。</li>
<li>methods: 本研究比较了现有Literature中的方法，并提出了新的数据驱动方法。</li>
<li>results: 实验和实际应用中，数据驱动方法的性能较为出色，超过了现有方法的表现。<details>
<summary>Abstract</summary>
Rather than having each newly deployed robot create its own map of its surroundings, the growing availability of SLAM-enabled devices provides the option of simply localizing in a map of another robot or device. In cases such as multi-robot or human-robot collaboration, localizing all agents in the same map is even necessary. However, localizing e.g. a ground robot in the map of a drone or head-mounted MR headset presents unique challenges due to viewpoint changes. This work investigates how active visual localization can be used to overcome such challenges of viewpoint changes. Specifically, we focus on the problem of selecting the optimal viewpoint at a given location. We compare existing approaches in the literature with additional proposed baselines and propose a novel data-driven approach. The result demonstrates the superior performance of the data-driven approach when compared to existing methods, both in controlled simulation experiments and real-world deployment.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:而不是每个新部署的机器人都需要创建自己的环境地图，现在可以通过使用SLAM技术实现的设备的可用性提供了将其本地化在另一个机器人或设备的地图上的选项。在多机器人或人机合作等情况下，将所有代理人在同一个地图上本地化是必要的。但是，将地面机器人本地化在飞行器或头戴式混合现实（MR）头盔的地图上存在视点变化的挑战。本研究 investigate了如何使用活动视觉本地化解决视点变化问题。我们专注于选择给定位置的优化视点问题，并与文献中的现有方法进行比较，并提出了一种新的数据驱动方法。结果显示数据驱动方法在控制 simulate experiments 和实际部署中表现出色。
</details></li>
</ul>
<hr>
<h2 id="P2CADNet-An-End-to-End-Reconstruction-Network-for-Parametric-3D-CAD-Model-from-Point-Clouds"><a href="#P2CADNet-An-End-to-End-Reconstruction-Network-for-Parametric-3D-CAD-Model-from-Point-Clouds" class="headerlink" title="P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD Model from Point Clouds"></a>P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD Model from Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02638">http://arxiv.org/abs/2310.02638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Zong, Fazhi He, Rubin Fan, Yuxin Liu</li>
<li>for: 这个论文的目的是提出一种基于端点云的CAD模型重建方法（P2CADNet），以解决现代工业和社会中feature-based parametric CAD模型的重建问题。</li>
<li>methods: 该方法首先提出了一种结构，它结合端点云特征提取器、CAD序列重建器和参数优化器。然后，为了在autoregressive方式下重建featured CAD模型，CAD序列重建器使用了两个transformer解码器，其中一个带有目标面罩和另一个没有。最后，为了更准确地预测参数，我们设计了一个参数优化器 WITH cross-attention机制来进一步细化CAD特征参数。</li>
<li>results: 我们在公共数据集上评估了P2CADNet，并 obtener了优秀的重建质量和准确性。根据我们所知，P2CADNet是首个基于端点云的end-to-end网络，可以被视为未来研究的基准。因此，我们在github上公开了源代码，访问<a target="_blank" rel="noopener" href="https://github.com/Blice0415/P2CADNet%E3%80%82">https://github.com/Blice0415/P2CADNet。</a><details>
<summary>Abstract</summary>
Computer Aided Design (CAD), especially the feature-based parametric CAD, plays an important role in modern industry and society. However, the reconstruction of featured CAD model is more challenging than the reconstruction of other CAD models. To this end, this paper proposes an end-to-end network to reconstruct featured CAD model from point cloud (P2CADNet). Initially, the proposed P2CADNet architecture combines a point cloud feature extractor, a CAD sequence reconstructor and a parameter optimizer. Subsequently, in order to reconstruct the featured CAD model in an autoregressive way, the CAD sequence reconstructor applies two transformer decoders, one with target mask and the other without mask. Finally, for predicting parameters more precisely, we design a parameter optimizer with cross-attention mechanism to further refine the CAD feature parameters. We evaluate P2CADNet on the public dataset, and the experimental results show that P2CADNet has excellent reconstruction quality and accuracy. To our best knowledge, P2CADNet is the first end-to-end network to reconstruct featured CAD model from point cloud, and can be regarded as baseline for future works. Therefore, we open the source code at https://github.com/Blice0415/P2CADNet.
</details>
<details>
<summary>摘要</summary>
computer-aided design (CAD)，特icularly feature-based parametric CAD，在现代工业和社会中扮演着重要的角色。然而，重建featured CAD模型比其他CAD模型更加困难。为此，这篇论文提出了一种终端网络来重建point cloud中的featured CAD模型（P2CADNet）。在提案的P2CADNet体系结构中，首先combines一个点云特征提取器、一个CAD序列重构器和一个参数优化器。然后，为了在自动回归方式下重建featured CAD模型，CAD序列重构器使用了两个变换器解码器，一个带有目标面罩和另一个没有罩。最后，为了更 precisely预测参数，我们设计了一个参数优化器 WITH cross-attention机制来进一步细化CAD特征参数。我们对公共数据集进行了测试，并得到了优秀的重建质量和准确性。根据我们所知，P2CADNet是首个以终端网络重建point cloud中的featured CAD模型，可以 serves as a benchmark for future works。因此，我们在github上公开了源代码（https://github.com/Blice0415/P2CADNet）。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-and-Improving-OT-based-Adversarial-Networks"><a href="#Analyzing-and-Improving-OT-based-Adversarial-Networks" class="headerlink" title="Analyzing and Improving OT-based Adversarial Networks"></a>Analyzing and Improving OT-based Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02611">http://arxiv.org/abs/2310.02611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaemoo Choi, Jaewoong Choi, Myungjoo Kang</li>
<li>for: 本文 targets  solving the problem of generative modeling using Optimal Transport (OT) theory.</li>
<li>methods: 本文提出了一种 unified framework ， combining OT-based adversarial methods to improve the performance of generative models. The authors also analyze the training dynamics of this framework and propose a novel method for gradual refinement of the generated distribution.</li>
<li>results:  compared with previous best-performing OT-based models, the proposed approach achieves a FID score of 2.51 on CIFAR-10, outperforming unified OT-based adversarial approaches.<details>
<summary>Abstract</summary>
Optimal Transport (OT) problem aims to find a transport plan that bridges two distributions while minimizing a given cost function. OT theory has been widely utilized in generative modeling. In the beginning, OT distance has been used as a measure for assessing the distance between data and generated distributions. Recently, OT transport map between data and prior distributions has been utilized as a generative model. These OT-based generative models share a similar adversarial training objective. In this paper, we begin by unifying these OT-based adversarial methods within a single framework. Then, we elucidate the role of each component in training dynamics through a comprehensive analysis of this unified framework. Moreover, we suggest a simple but novel method that improves the previously best-performing OT-based model. Intuitively, our approach conducts a gradual refinement of the generated distribution, progressively aligning it with the data distribution. Our approach achieves a FID score of 2.51 on CIFAR-10, outperforming unified OT-based adversarial approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SweetDreamer-Aligning-Geometric-Priors-in-2D-Diffusion-for-Consistent-Text-to-3D"><a href="#SweetDreamer-Aligning-Geometric-Priors-in-2D-Diffusion-for-Consistent-Text-to-3D" class="headerlink" title="SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D"></a>SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02596">http://arxiv.org/abs/2310.02596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wyysf-98/SweetDreamer">https://github.com/wyysf-98/SweetDreamer</a></li>
<li>paper_authors: Weiyu Li, Rui Chen, Xuelin Chen, Ping Tan</li>
<li>for: 本研究旨在解决2D扩散模型在3D世界中的文本到3D生成问题，即多视点不一致问题。</li>
<li>methods: 我们使用了微调2D扩散模型以生成视点特定的坐标图，并将其与可见的3D对象匹配，以解决多视点不一致问题。</li>
<li>results: 我们的方法可以高效地解决多视点不一致问题，并保持2D扩散模型的细节和多样性。我们的方法在人工评估中获得了85+%的一致率，远高于前一代方法的30%。<details>
<summary>Abstract</summary>
It is inherently ambiguous to lift 2D results from pre-trained diffusion models to a 3D world for text-to-3D generation. 2D diffusion models solely learn view-agnostic priors and thus lack 3D knowledge during the lifting, leading to the multi-view inconsistency problem. We find that this problem primarily stems from geometric inconsistency, and avoiding misplaced geometric structures substantially mitigates the problem in the final outputs. Therefore, we improve the consistency by aligning the 2D geometric priors in diffusion models with well-defined 3D shapes during the lifting, addressing the vast majority of the problem. This is achieved by fine-tuning the 2D diffusion model to be viewpoint-aware and to produce view-specific coordinate maps of canonically oriented 3D objects. In our process, only coarse 3D information is used for aligning. This "coarse" alignment not only resolves the multi-view inconsistency in geometries but also retains the ability in 2D diffusion models to generate detailed and diversified high-quality objects unseen in the 3D datasets. Furthermore, our aligned geometric priors (AGP) are generic and can be seamlessly integrated into various state-of-the-art pipelines, obtaining high generalizability in terms of unseen shapes and visual appearance while greatly alleviating the multi-view inconsistency problem. Our method represents a new state-of-the-art performance with an 85+% consistency rate by human evaluation, while many previous methods are around 30%. Our project page is https://sweetdreamer3d.github.io/
</details>
<details>
<summary>摘要</summary>
"天然地，将2D结果从预训练的扩散模型升级到3D世界进行文本到3D生成是具有内在困难的。2D扩散模型只是学习视角无关的假设，因此在升级时缺乏3D知识，导致多视图不一致问题。我们发现这个问题主要来自于几何不一致问题，避免误置的几何结构可以大幅减轻问题。因此，我们改进了一致性，通过将2D的几何假设与well-defined的3D形状进行对应，在升级过程中解决了大多数问题。这是通过练习2D扩散模型以便视角意识和生成视специ fic coordinate map来实现的。在我们的过程中，只使用了粗略的3D信息进行对应。这种"粗"对应不仅解决了多视图不一致的几何问题，还保留了2D扩散模型的详细和多样化高质量对象生成能力。此外，我们的对应几何假设（AGP）是通用的，可以轻松地与多种当前领先的管道集成，从而实现高通用性。我们的方法在人工评估中获得了85+%的一致率，而许多前一代方法只有30%左右。我们的项目页面是https://sweetdreamer3d.github.io/。"
</details></li>
</ul>
<hr>
<h2 id="ViT-ReciproCAM-Gradient-and-Attention-Free-Visual-Explanations-for-Vision-Transformer"><a href="#ViT-ReciproCAM-Gradient-and-Attention-Free-Visual-Explanations-for-Vision-Transformer" class="headerlink" title="ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer"></a>ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02588">http://arxiv.org/abs/2310.02588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seok-Yong Byun, Wonju Lee</li>
<li>for: 这 paper 的目的是解释和调试 ViT 模型预测过程中的批处理错误。</li>
<li>methods: 这 paper 使用了一种新的梯度自由 visual explanation 方法，叫做 ViT-ReciproCAM，不需要注意力矩阵和梯度信息。</li>
<li>results: 对比现有的 Relevance 方法，ViT-ReciproCAM 在 Average Drop-Coherence-Complexity (ADCC)  metric 中表现出了 $4.58%$ 到 $5.80%$ 的提升，并生成了更加Localized 的焦点图。<details>
<summary>Abstract</summary>
This paper presents a novel approach to address the challenges of understanding the prediction process and debugging prediction errors in Vision Transformers (ViT), which have demonstrated superior performance in various computer vision tasks such as image classification and object detection. While several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and Recipro-CAM, have been extensively researched for Convolutional Neural Networks (CNNs), limited research has been conducted on ViT. Current state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques. In this work, we propose a new gradient-free visual explanation method for ViT, called ViT-ReciproCAM, which does not require attention matrix and gradient information. ViT-ReciproCAM utilizes token masking and generated new layer outputs from the target layer's input to exploit the correlation between activated tokens and network predictions for target classes. Our proposed method outperforms the state-of-the-art Relevance method in the Average Drop-Coherence-Complexity (ADCC) metric by $4.58\%$ to $5.80\%$ and generates more localized saliency maps. Our experiments demonstrate the effectiveness of ViT-ReciproCAM and showcase its potential for understanding and debugging ViT models. Our proposed method provides an efficient and easy-to-implement alternative for generating visual explanations, without requiring attention and gradient information, which can be beneficial for various applications in the field of computer vision.
</details>
<details>
<summary>摘要</summary>
Currently, state-of-the-art solutions for ViT rely on class agnostic Attention-Rollout and Relevance techniques, but these methods have limitations. In contrast, our proposed method does not require attention matrix and gradient information, making it more efficient and easier to implement.Our experiments show that ViT-ReciproCAM outperforms the state-of-the-art Relevance method in the Average Drop-Coherence-Complexity (ADCC) metric by 4.58% to 5.80% and generates more localized saliency maps. This demonstrates the effectiveness of our proposed method in understanding and debugging ViT models.Our method has the potential to be beneficial for various applications in the field of computer vision, as it provides an efficient and easy-to-implement alternative for generating visual explanations.
</details></li>
</ul>
<hr>
<h2 id="A-Prototype-Based-Neural-Network-for-Image-Anomaly-Detection-and-Localization"><a href="#A-Prototype-Based-Neural-Network-for-Image-Anomaly-Detection-and-Localization" class="headerlink" title="A Prototype-Based Neural Network for Image Anomaly Detection and Localization"></a>A Prototype-Based Neural Network for Image Anomaly Detection and Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02576">http://arxiv.org/abs/2310.02576</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/98chao/ProtoAD">https://github.com/98chao/ProtoAD</a></li>
<li>paper_authors: Chao Huang, Zhao Kang, Hong Wu</li>
<li>for: 这篇论文的目的是提出一个基于原型的图像异常检测和定位方法（ProtoAD），用于快速和高效地检测和定位图像中的异常点。</li>
<li>methods: 本文使用的方法包括对正常图像的对应网络进行深度学习，然后使用非Parametric clustering来学习正常图像的标本，最后则使用这些标本来建立一个图像异常检测和定位网络（ProtoAD）。</li>
<li>results: 实验结果显示，ProtoAD可以与现有的方法比较，在两个工业异常检测数据集（MVTec AD和BTAD）上具有竞争的性能，同时具有更高的推断速度。<details>
<summary>Abstract</summary>
Image anomaly detection and localization perform not only image-level anomaly classification but also locate pixel-level anomaly regions. Recently, it has received much research attention due to its wide application in various fields. This paper proposes ProtoAD, a prototype-based neural network for image anomaly detection and localization. First, the patch features of normal images are extracted by a deep network pre-trained on nature images. Then, the prototypes of the normal patch features are learned by non-parametric clustering. Finally, we construct an image anomaly localization network (ProtoAD) by appending the feature extraction network with $L2$ feature normalization, a $1\times1$ convolutional layer, a channel max-pooling, and a subtraction operation. We use the prototypes as the kernels of the $1\times1$ convolutional layer; therefore, our neural network does not need a training phase and can conduct anomaly detection and localization in an end-to-end manner. Extensive experiments on two challenging industrial anomaly detection datasets, MVTec AD and BTAD, demonstrate that ProtoAD achieves competitive performance compared to the state-of-the-art methods with a higher inference speed. The source code is available at: https://github.com/98chao/ProtoAD.
</details>
<details>
<summary>摘要</summary>
图像异常检测和地图化可以不仅进行图像水平异常分类，还可以定位像素级异常区域。这一领域在不同领域的应用广泛，因此吸引了许多研究者的关注。这篇论文提出了一种基于原型的神经网络图像异常检测和地图化方法（ProtoAD）。首先，使用深度网络预训练的自然图像特征提取器来提取正常图像的补丁特征。然后，通过非 Parametric 聚类学习出正常补丁特征的核心。最后，我们将特征提取网络与 $L2$ 特征 нормализа、 $1\times1$ 卷积层、通道最大池化和减法操作组合成一个图像异常检测和地图化网络（ProtoAD）。我们使用核心作为 $1\times1$ 卷积层的核心，因此我们的神经网络不需要训练阶段，可以在终端到终端的方式进行异常检测和地图化。我们在 MVTec AD 和 BTAD 两个industrial anomaly detection 数据集上进行了广泛的实验，并证明了 ProtoAD 与状态机器的方法相比，具有更高的检测速度。源代码可以在 GitHub 上获取：https://github.com/98chao/ProtoAD。
</details></li>
</ul>
<hr>
<h2 id="AdaMerging-Adaptive-Model-Merging-for-Multi-Task-Learning"><a href="#AdaMerging-Adaptive-Model-Merging-for-Multi-Task-Learning" class="headerlink" title="AdaMerging: Adaptive Model Merging for Multi-Task Learning"></a>AdaMerging: Adaptive Model Merging for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02575">http://arxiv.org/abs/2310.02575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EnnengYang/AdaMerging">https://github.com/EnnengYang/AdaMerging</a></li>
<li>paper_authors: Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, Dacheng Tao</li>
<li>for: 本研究旨在解决多任务学习（MTL）中模型融合的问题，即将多个任务特化的模型直接融合到单一模型中，以提高MTL的性能。</li>
<li>methods: 本研究提出了一种名为自适应模型融合（AdaMerging）的新技术，它通过在无supervised的情况下自动学习模型融合系数，以提高MTL的性能。具体来说，AdaMerging使用测试样本的熵函数来反射iteratively修改模型融合系数。</li>
<li>results: 实验结果表明，Compared to当前状态的拟合算法，AdaMerging显示了11%的性能提升。此外，AdaMerging还在下游任务测试阶段展现出了更好的泛化能力和数据分布变化的鲁棒性。<details>
<summary>Abstract</summary>
Multi-task learning (MTL) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute MTL without necessitating a retraining process using the initial training data. Nevertheless, this direct addition of models often leads to a significant deterioration in the overall performance of the merged model. This decline occurs due to potential conflicts and intricate correlations among the multiple tasks. Consequently, the challenge emerges of how to merge pre-trained models more effectively without using their original training data. This paper introduces an innovative technique called Adaptive Model Merging (AdaMerging). This approach aims to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data. Specifically, our AdaMerging method operates as an automatic, unsupervised task arithmetic scheme. It leverages entropy minimization on unlabeled test samples from the multi-task setup as a surrogate objective function to iteratively refine the merging coefficients of the multiple models. Our experimental findings across eight tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared to the current state-of-the-art task arithmetic merging scheme, AdaMerging showcases a remarkable 11\% improvement in performance. Notably, AdaMerging also exhibits superior generalization capabilities when applied to unseen downstream tasks. Furthermore, it displays a significantly enhanced robustness to data distribution shifts that may occur during the testing phase.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目的是让模型同时处理多个任务。现有的发展是任务加法，这种方法可以将每个任务精心适应的模型直接合并到单个模型中，无需进行重新训练过程，使用初始训练数据。然而，直接添加模型通常会导致合并后模型的总性能下降。这种下降是因为多个任务之间可能存在冲突和复杂的相互关系。因此，需要一种更有效的模型合并方法，不需要使用初始训练数据。这篇论文提出了一种创新的技术——适应模型合并（AdaMerging）。这种方法通过自动学习模型合并的系数，而不是使用初始训练数据，来解决这个挑战。具体来说，我们的 AdaMerging 方法是一种自动、无监督的任务加法方案，通过测试样本的熵度最小化来迭代地优化模型合并系数。我们的实验结果表明，我们的 AdaMerging 方法在八个任务上具有remarkable的表现优势。相比之下，当前状态的技术任务加法合并方法，AdaMerging 显示了11%的性能提升。此外，AdaMerging 还展现出了更好的泛化能力，当应用于未见下游任务时。同时，它还在测试阶段可能发生数据分布变化时显示了显著的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="ReForm-Eval-Evaluating-Large-Vision-Language-Models-via-Unified-Re-Formulation-of-Task-Oriented-Benchmarks"><a href="#ReForm-Eval-Evaluating-Large-Vision-Language-Models-via-Unified-Re-Formulation-of-Task-Oriented-Benchmarks" class="headerlink" title="ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks"></a>ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02569">http://arxiv.org/abs/2310.02569</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fudandisc/reform-eval">https://github.com/fudandisc/reform-eval</a></li>
<li>paper_authors: Zejun Li, Ye Wang, Mengfei Du, Qingwen Liu, Binhao Wu, Jiwen Zhang, Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen, Xuanjing Huang, Zhongyu Wei</li>
<li>for: 本文提出了一种新的评估大视言语模型（LVLM）的方法，以便更好地评估LVLM的多种能力。</li>
<li>methods: 本文使用了现有的多模式标准套件，并对其进行了修改，以使得LVLM可以更好地处理自由形式的文本输出。</li>
<li>results: 通过对多种LVLM进行了广泛的实验和分析，本文发现了这些模型的优缺点，并提出了进一步改进的方法。<details>
<summary>Abstract</summary>
Recent years have witnessed remarkable progress in the development of large vision-language models (LVLMs). Benefiting from the strong language backbones and efficient cross-modal alignment strategies, LVLMs exhibit surprising capabilities to perceive visual signals and perform visually grounded reasoning. However, the capabilities of LVLMs have not been comprehensively and quantitatively evaluate. Most existing multi-modal benchmarks require task-oriented input-output formats, posing great challenges to automatically assess the free-form text output of LVLMs. To effectively leverage the annotations available in existing benchmarks and reduce the manual effort required for constructing new benchmarks, we propose to re-formulate existing benchmarks into unified LVLM-compatible formats. Through systematic data collection and reformulation, we present the ReForm-Eval benchmark, offering substantial data for evaluating various capabilities of LVLMs. Based on ReForm-Eval, we conduct extensive experiments, thoroughly analyze the strengths and weaknesses of existing LVLMs, and identify the underlying factors. Our benchmark and evaluation framework will be open-sourced as a cornerstone for advancing the development of LVLMs.
</details>
<details>
<summary>摘要</summary>
近年来，大规模视语模型（LVLM）的发展做出了非常出色的进步。受到强大的语言脊梁和有效的跨模式对接策略的推动，LVLM表现出了对视觉信号的感知和视情理解的奇异能力。然而，LVLM的能力尚未得到全面和量化的评估。现有的多Modalbenchmark大多需要任务强调的输入输出格式，对自由文本输出的自动评估带来很大的挑战。为了有效利用现有benchmark的注释和减少新benchmark的手动劳动，我们提议将现有benchmark重新格式化为LVLM兼容的格式。通过系统性的数据收集和重新格式化，我们提出了ReForm-Evalbenchmark，提供了大量用于评估LVLM的数据。基于ReForm-Eval，我们进行了广泛的实验，对现有LVLM的优势和缺点进行了系统分析，并确定了下面因素。我们的benchmark和评估框架将被开源，成为LVLM的发展的基础。
</details></li>
</ul>
<hr>
<h2 id="Generalization-in-diffusion-models-arises-from-geometry-adaptive-harmonic-representation"><a href="#Generalization-in-diffusion-models-arises-from-geometry-adaptive-harmonic-representation" class="headerlink" title="Generalization in diffusion models arises from geometry-adaptive harmonic representation"></a>Generalization in diffusion models arises from geometry-adaptive harmonic representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02557">http://arxiv.org/abs/2310.02557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, Stéphane Mallat</li>
<li>for: 这个论文目的是证明深度神经网络（DNN）在去噪化过程中可以学习高维密度，即使面对尺度问题。</li>
<li>methods: 这个论文使用了分别训练的两个DNN，以不重 overlap的方式训练两个不同的子集。</li>
<li>results: 这两个DNN在几个训练图像后就能够学习出相似的分数函数，并且它们的去噪化性能几乎是最佳的。这表明DNN的架构和训练算法与数据分布的性质有很好的对预传。<details>
<summary>Abstract</summary>
High-quality samples generated with score-based reverse diffusion algorithms provide evidence that deep neural networks (DNN) trained for denoising can learn high-dimensional densities, despite the curse of dimensionality. However, recent reports of memorization of the training set raise the question of whether these networks are learning the "true" continuous density of the data. Here, we show that two denoising DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images. This strong generalization demonstrates an alignment of powerful inductive biases in the DNN architecture and/or training algorithm with properties of the data distribution. We analyze these, demonstrating that the denoiser performs a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous image regions. We show that trained denoisers are inductively biased towards these geometry-adaptive harmonic representations by demonstrating that they arise even when the network is trained on image classes such as low-dimensional manifolds, for which the harmonic basis is suboptimal. Additionally, we show that the denoising performance of the networks is near-optimal when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic.
</details>
<details>
<summary>摘要</summary>
高品质样本通过分数逆扩散算法生成，证明深度神经网络（DNN）在去噪过程中学习高维概率分布，即使面临维度困难。然而，最近的报告表明，这些网络是否真的学习数据的连续概率分布？我们表明，两个不相交的subset中训练的denoising DNN，它们学习的scor函数几乎相同，因此学习的概率分布也几乎相同，并且很少需要训练图像。这种强大的泛化表明了DNN架构和/或训练算法与数据分布的对齐。我们分析这些，并证明denoiser在基于图像的基础上进行缩小操作，并且在抽象图像区域和抽象图像边缘的oscillating振荡结构。我们还证明，训练denoisers时，网络具有geometry-adaptive harmonic representation的强大适应性，即使网络被训练在低维抽象图像类型上。此外，我们还证明，训练在正规图像类型上的denoisers具有高效性，这些类型上的优化基是geometry-adaptive和振荡的。
</details></li>
</ul>
<hr>
<h2 id="SlowFormer-Universal-Adversarial-Patch-for-Attack-on-Compute-and-Energy-Efficiency-of-Inference-Efficient-Vision-Transformers"><a href="#SlowFormer-Universal-Adversarial-Patch-for-Attack-on-Compute-and-Energy-Efficiency-of-Inference-Efficient-Vision-Transformers" class="headerlink" title="SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers"></a>SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02544">http://arxiv.org/abs/2310.02544</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UCDvision/SlowFormer">https://github.com/UCDvision/SlowFormer</a></li>
<li>paper_authors: KL Navaneet, Soroush Abbasi Koohpayegani, Essam Sleiman, Hamed Pirsiavash</li>
<li>for: 本文旨在探讨针对深度模型的可扩展计算方法在推理时的计算减少方法，以及这些方法在面对抗击攻击时的Robustness问题。</li>
<li>methods: 本文使用了一种通用抗击攻击方法，即针对深度模型的可扩展计算方法，并在不同的图像数据集上进行了实验。</li>
<li>results: 实验结果表明，在使用了针对深度模型的可扩展计算方法时，可以通过粘贴一个小于8%的图像区域的抗击补丁来增加计算和功率消耗。此外，通过对模型进行标准的 adversarial 训练方法可以减少一些攻击的成功率。<details>
<summary>Abstract</summary>
Recently, there has been a lot of progress in reducing the computation of deep models at inference time. These methods can reduce both the computational needs and power usage of deep models. Some of these approaches adaptively scale the compute based on the input instance. We show that such models can be vulnerable to a universal adversarial patch attack, where the attacker optimizes for a patch that when pasted on any image, can increase the compute and power consumption of the model. We run experiments with three different efficient vision transformer methods showing that in some cases, the attacker can increase the computation to the maximum possible level by simply pasting a patch that occupies only 8\% of the image area. We also show that a standard adversarial training defense method can reduce some of the attack's success. We believe adaptive efficient methods will be necessary for the future to lower the power usage of deep models, so we hope our paper encourages the community to study the robustness of these methods and develop better defense methods for the proposed attack.
</details>
<details>
<summary>摘要</summary>
近些时间，深度模型的执行时间计算减少得非常多。这些方法可以降低计算需求和电力消耗。一些这些方法可以根据输入实例进行可靠缩放计算。我们发现这些模型受到一种通用敌意袋patch攻击，攻击者可以优化一个覆盖任何图像的袋patch，以提高模型的计算和电力消耗。我们进行了三种不同的高效视Transformer方法的实验，发现在某些情况下，攻击者可以通过覆盖图像8%的区域而增加计算到最大可能的水平。我们还发现了标准的对抗训练防御方法可以减少一些攻击的成功。我们认为适应性高效的方法将是未来的需要，因此我们希望我们的论文能够鼓励社区研究这些方法的Robustness和开发更好的防御方法。
</details></li>
</ul>
<hr>
<h2 id="ShaSTA-Fuse-Camera-LiDAR-Sensor-Fusion-to-Model-Shape-and-Spatio-Temporal-Affinities-for-3D-Multi-Object-Tracking"><a href="#ShaSTA-Fuse-Camera-LiDAR-Sensor-Fusion-to-Model-Shape-and-Spatio-Temporal-Affinities-for-3D-Multi-Object-Tracking" class="headerlink" title="ShaSTA-Fuse: Camera-LiDAR Sensor Fusion to Model Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking"></a>ShaSTA-Fuse: Camera-LiDAR Sensor Fusion to Model Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02532">http://arxiv.org/abs/2310.02532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tara Sadjadpour, Rares Ambrus, Jeannette Bohg<br>for: 这 paper 的目的是开发一个基于摄像头和雷达感知器的3D多对象跟踪（MOT）框架，以提高自动驾驶机器人在场景中安全导航的能力。methods: 该 paper 使用了一种新的摄像头-雷达融合方法，该方法可以将摄像头和雷达感知器中的信息融合以便优化关系学习，从而提高数据关联、跟踪生命周期管理、干扰消除、假阳性传播和跟踪信任度评估等方面的性能。results: 该 paper 在 nuScenes benchmark 上 achieved state-of-the-art 性能 amongst multimodal 3D MOT algorithms using CenterPoint detections。此外，paper 还提供了一种 novel fusion approach 和 a first-of-its-kind multimodal sequential track confidence refinement technique，以及一系列的ablative analysis 以证明摄像头感知器的添加对小、远对象的识别和跟踪具有重要的改进作用。<details>
<summary>Abstract</summary>
3D multi-object tracking (MOT) is essential for an autonomous mobile agent to safely navigate a scene. In order to maximize the perception capabilities of the autonomous agent, we aim to develop a 3D MOT framework that fuses camera and LiDAR sensor information. Building on our prior LiDAR-only work, ShaSTA, which models shape and spatio-temporal affinities for 3D MOT, we propose a novel camera-LiDAR fusion approach for learning affinities. At its core, this work proposes a fusion technique that generates a rich sensory signal incorporating information about depth and distant objects to enhance affinity estimation for improved data association, track lifecycle management, false-positive elimination, false-negative propagation, and track confidence score refinement. Our main contributions include a novel fusion approach for combining camera and LiDAR sensory signals to learn affinities, and a first-of-its-kind multimodal sequential track confidence refinement technique that fuses 2D and 3D detections. Additionally, we perform an ablative analysis on each fusion step to demonstrate the added benefits of incorporating the camera sensor, particular for small, distant objects that tend to suffer from the depth-sensing limits and sparsity of LiDAR sensors. In sum, our technique achieves state-of-the-art performance on the nuScenes benchmark amongst multimodal 3D MOT algorithms using CenterPoint detections.
</details>
<details>
<summary>摘要</summary>
三维多对象跟踪（MOT）是自动移动代理安全导航场景的关键。为了提高自动代理的感知能力，我们目标是开发一个三维MOT框架，并将摄像头和激光探测器信息进行融合。基于我们之前的激光仅工作，ShaSTA，我们提出了一种新的摄像头-激光融合方法，用于学习关系。这种方法生成了rich的感知信号，包括深度和远程对象的信息，以提高关系估计，从而改善数据关联、跟踪生命周期管理、假阳性除除、假阴性升级和跟踪信任分数精度。我们的主要贡献包括一种新的摄像头-激光感知信号融合方法，以及一种首次实现的多模态序列跟踪信任修复技术，可以融合2D和3D探测。此外，我们还进行了每个融合步骤的ablative分析，以显示在包括小、远对象的情况下，摄像头感知的added benefits。总的来说，我们的技术在nuScenes标准 benchmark上达到了多模态3D MOT算法中的状态 искусственный智能水平。
</details></li>
</ul>
<hr>
<h2 id="On-the-Cognition-of-Visual-Question-Answering-Models-and-Human-Intelligence-A-Comparative-Study"><a href="#On-the-Cognition-of-Visual-Question-Answering-Models-and-Human-Intelligence-A-Comparative-Study" class="headerlink" title="On the Cognition of Visual Question Answering Models and Human Intelligence: A Comparative Study"></a>On the Cognition of Visual Question Answering Models and Human Intelligence: A Comparative Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02528">http://arxiv.org/abs/2310.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liben Chen, Long Chen, Tian Ellison-Chen, Zhuoyuan Xu</li>
<li>for: 本研究旨在研究人类认知和计算机模型之间的关系，以便更好地理解计算机模型如何模仿人类认知。</li>
<li>methods: 本研究使用了survey方法 recording human thinking process，并比较了计算机模型的输出和注意力映射与人类的认知过程。</li>
<li>results: 研究发现，虽然计算机模型的结构和人类认知有相似之处，但它们仍然努力于认知推理。人类认知过程的分析可以指导未来的研究，并引入更多的认知能力到特征和结构设计中。<details>
<summary>Abstract</summary>
Visual Question Answering (VQA) is a challenging task that requires cross-modal understanding and reasoning of visual image and natural language question. To inspect the association of VQA models to human cognition, we designed a survey to record human thinking process and analyzed VQA models by comparing the outputs and attention maps with those of humans. We found that although the VQA models resemble human cognition in architecture and performs similarly with human on the recognition-level, they still struggle with cognitive inferences. The analysis of human thinking procedure serves to direct future research and introduce more cognitive capacity into modeling features and architectures.
</details>
<details>
<summary>摘要</summary>
视觉问答（VQA）是一项具有挑战性的任务，需要跨Modal的理解和自然语言问题的推理。为了了解人类认知与VQA模型之间的关系，我们设计了一份问卷，记录了人类思维过程，并对VQA模型进行比较和分析。我们发现，虽然VQA模型与人类认知结构相似，但它们仍然努力于认知推理。人类思维过程的分析可以指导未来的研究，并在模型特征和建筑中引入更多的认知能力。
</details></li>
</ul>
<hr>
<h2 id="A-Spatio-Temporal-Attention-Based-Method-for-Detecting-Student-Classroom-Behaviors"><a href="#A-Spatio-Temporal-Attention-Based-Method-for-Detecting-Student-Classroom-Behaviors" class="headerlink" title="A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors"></a>A Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02523">http://arxiv.org/abs/2310.02523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Yang</li>
<li>for: 本研究旨在提高学生班级中的教学效率，通过精准地检测学生的班级行为。</li>
<li>methods: 本研究使用SlowFast网络生成视频中的动作和环境信息特征地图，然后应用空间时间注意力模块，包括信息汇集、压缩和刺激过程。接着，在时间、通道和空间维度中获得注意力地图，并基于这些注意力地图进行多标签行为分类。</li>
<li>results: 对于一个自制的学生班级行为数据集（STSCB），与SlowFast模型相比，BDSTA模型可以提高学生行为分类检测的准确率8.94%。<details>
<summary>Abstract</summary>
Accurately detecting student behavior from classroom videos is beneficial for analyzing their classroom status and improving teaching efficiency. However, low accuracy in student classroom behavior detection is a prevalent issue. To address this issue, we propose a Spatio-Temporal Attention-Based Method for Detecting Student Classroom Behaviors (BDSTA). Firstly, the SlowFast network is used to generate motion and environmental information feature maps from the video. Then, the spatio-temporal attention module is applied to the feature maps, including information aggregation, compression and stimulation processes. Subsequently, attention maps in the time, channel and space dimensions are obtained, and multi-label behavior classification is performed based on these attention maps. To solve the long-tail data problem that exists in student classroom behavior datasets, we use an improved focal loss function to assign more weight to the tail class data during training. Experimental results are conducted on a self-made student classroom behavior dataset named STSCB. Compared with the SlowFast model, the average accuracy of student behavior classification detection improves by 8.94\% using BDSTA.
</details>
<details>
<summary>摘要</summary>
通过准确探测学生CLASSROOM行为来分析学生的CLASSROOM状态和改善教学效率是非常有利的。然而，学生CLASSROOM行为检测精度低的问题很普遍。为解决这个问题，我们提议一种基于Spatio-Temporal Attention的学生CLASSROOM行为检测方法（BDSTA）。首先，我们使用SlowFast网络生成视频中的动作和环境信息特征地图。然后，我们应用空间-时间注意力模块到特征地图中，包括信息聚合、压缩和刺激过程。最后，我们在时间、通道和空间维度中获得了注意力地图，并根据这些注意力地图进行多标签行为分类。为解决学生CLASSROOM行为数据集中的长尾数据问题，我们使用改进的焦点损失函数来在训练中对tail类数据分配更多的权重。我们在自己制作的学生CLASSROOM行为数据集名为STSCB上进行了实验，与SlowFast模型相比，BDSTA的学生行为分类精度提高了8.94%。
</details></li>
</ul>
<hr>
<h2 id="SCB-Dataset3-A-Benchmark-for-Detecting-Student-Classroom-Behavior"><a href="#SCB-Dataset3-A-Benchmark-for-Detecting-Student-Classroom-Behavior" class="headerlink" title="SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior"></a>SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02522">http://arxiv.org/abs/2310.02522</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/whiffe/scb-dataset">https://github.com/whiffe/scb-dataset</a></li>
<li>paper_authors: Fan Yang, Tao Wang</li>
<li>for: 本研究旨在提供一个可靠的学生行为档案，以便未来的研究人员可以使用该档案进行学生行为探测和教学效果提升。</li>
<li>methods: 本研究使用深度学习方法来自动检测学生的课堂行为，包括使用YOLOv5、YOLOv7和YOLOv8算法进行评估。</li>
<li>results: 本研究在评估SCB-dataset3时，使用YOLOv5、YOLOv7和YOLOv8算法实现了平均准确率（map）达80.3%。<details>
<summary>Abstract</summary>
The use of deep learning methods to automatically detect students' classroom behavior is a promising approach for analyzing their class performance and improving teaching effectiveness. However, the lack of publicly available datasets on student behavior poses a challenge for researchers in this field. To address this issue, we propose the Student Classroom Behavior dataset (SCB-dataset3), which represents real-life scenarios. Our dataset comprises 5686 images with 45578 labels, focusing on six behaviors: hand-raising, reading, writing, using a phone, bowing the head, and leaning over the table. We evaluated the dataset using the YOLOv5, YOLOv7, and YOLOv8 algorithms, achieving a mean average precision (map) of up to 80.3$\%$. We believe that our dataset can serve as a robust foundation for future research in student behavior detection and contribute to advancements in this field. Our SCB-dataset3 is available for download at: https://github.com/Whiffe/SCB-dataset
</details>
<details>
<summary>摘要</summary>
使用深度学习方法自动检测学生的教室行为是一种有前途的方法，可以分析学生的教学效果和提高教学质量。然而，公共可用的学生行为数据集存在一定的挑战，以至于研究人员在这个领域困难于进行研究。为解决这个问题，我们提出了学生教室行为数据集（SCB-dataset3），该数据集反映了实际生活场景。我们的数据集包括5686个图像和45578个标签，关注六种行为：手势提高、读书、写作、使用手机、垂头和抽ån表。我们使用YOLOv5、YOLOv7和YOLOv8算法进行评估，实现了平均准确率（map）的80.3%。我们认为，SCB-dataset3可以作为未来学生行为检测研究的坚实基础，为这一领域的进步做出贡献。SCB-dataset3可以在以下地址下载：https://github.com/Whiffe/SCB-dataset。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/04/cs.CV_2023_10_04/" data-id="closbrop800jk0g88dk330ojf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/04/cs.AI_2023_10_04/" class="article-date">
  <time datetime="2023-10-04T12:00:00.000Z" itemprop="datePublished">2023-10-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/04/cs.AI_2023_10_04/">cs.AI - 2023-10-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Progressive-reduced-order-modeling-empowering-data-driven-modeling-with-selective-knowledge-transfer"><a href="#Progressive-reduced-order-modeling-empowering-data-driven-modeling-with-selective-knowledge-transfer" class="headerlink" title="Progressive reduced order modeling: empowering data-driven modeling with selective knowledge transfer"></a>Progressive reduced order modeling: empowering data-driven modeling with selective knowledge transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03770">http://arxiv.org/abs/2310.03770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teeratorn Kadeethum, Daniel O’Malley, Youngsoo Choi, Hari S. Viswanathan, Hongkyu Yoon</li>
<li>for: 解决数据缺乏问题，提高数据驱动模型的实用性。</li>
<li>methods: 提出了一种进步式减少顺序模型框架，通过人工智能选择性地使用前一些模型的知识，以减少数据需求和提高模型的准确性。</li>
<li>results: 在多个情况中测试了该框架，包括孔隙媒体传输、重力驱动流和各种弹性材料的质量变化。结果表明，保留前一些模型的知识并在当前模型中使用有价值的信息可以大幅提高模型的准确性。相比之下，没有父模型的模型需要9倍更多的数据来达到相同的准确性。这些结果表明了进步式知识传递的重要性和其对模型准确性的影响。<details>
<summary>Abstract</summary>
Data-driven modeling can suffer from a constant demand for data, leading to reduced accuracy and impractical for engineering applications due to the high cost and scarcity of information. To address this challenge, we propose a progressive reduced order modeling framework that minimizes data cravings and enhances data-driven modeling's practicality. Our approach selectively transfers knowledge from previously trained models through gates, similar to how humans selectively use valuable knowledge while ignoring unuseful information. By filtering relevant information from previous models, we can create a surrogate model with minimal turnaround time and a smaller training set that can still achieve high accuracy. We have tested our framework in several cases, including transport in porous media, gravity-driven flow, and finite deformation in hyperelastic materials. Our results illustrate that retaining information from previous models and utilizing a valuable portion of that knowledge can significantly improve the accuracy of the current model. We have demonstrated the importance of progressive knowledge transfer and its impact on model accuracy with reduced training samples. For instance, our framework with four parent models outperforms the no-parent counterpart trained on data nine times larger. Our research unlocks data-driven modeling's potential for practical engineering applications by mitigating the data scarcity issue. Our proposed framework is a significant step toward more efficient and cost-effective data-driven modeling, fostering advancements across various fields.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>数据驱动模型可能会面临数据需求不断增加的问题，导致模型精度降低，不适合工程应用因为数据昂贵和珍贵。为解决这个挑战，我们提出了一种进行逐步减少的模型架构，以减少数据的需求并提高数据驱动模型的实用性。我们的方法通过门户来传递知识，类似于人类选择性使用有价值的知识而忽略无用信息。通过过滤先前训练过的模型中的有用信息，我们可以创建一个快速生成的代理模型，并且只需训练一个较小的数据集，却仍能达到高精度。我们在多个情况中测试了我们的框架，包括在孔隙媒体中的运输、重力驱动流和可塑性材料中的finite deformation。我们的结果表明，保留先前模型中的信息并利用有价值的信息可以显著提高当前模型的精度。我们的研究表明了进行逐步知识传递的重要性和其对模型精度的影响，并且我们的框架在训练样本数量九倍时表现比无父模型要好。我们的研究推动数据驱动模型在实际工程应用中的应用，解决了数据珍贵问题。我们提出的框架是数据驱动模型效率和成本下降的重要一步，推动各个领域的进步。
</details></li>
</ul>
<hr>
<h2 id="On-the-Performance-of-Multimodal-Language-Models"><a href="#On-the-Performance-of-Multimodal-Language-Models" class="headerlink" title="On the Performance of Multimodal Language Models"></a>On the Performance of Multimodal Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03211">http://arxiv.org/abs/2310.03211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Utsav Garg, Erhan Bas</li>
<li>for: 本研究旨在比较不同多媒体指令训练方法的表现，并评估它们在多种任务上的适用范围，包括复杂的推理、对话、图像描述、多选问题（MCQ）和二分类。</li>
<li>methods: 本研究使用了模型插入和 instrucion 训练方法，将独立预训的视觉encoder与语言模型结合，以扩展语言模型的多媒体能力。</li>
<li>results: 研究发现，现有的方法尚未能够覆盖多媒体指令数据集的多样性，这限制了模型的任务普遍化能力。此外，研究发现当生成回答时，模型可能无法保持真实性和事实性。这些发现可以帮助研究人员和实践者更好地适用多媒体语言模型。<details>
<summary>Abstract</summary>
Instruction-tuned large language models (LLMs) have demonstrated promising zero-shot generalization capabilities across various downstream tasks. Recent research has introduced multimodal capabilities to LLMs by integrating independently pretrained vision encoders through model grafting. These multimodal variants undergo instruction tuning, similar to LLMs, enabling effective zero-shot generalization for multimodal tasks. This study conducts a comparative analysis of different multimodal instruction tuning approaches and evaluates their performance across a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not sufficiently address the need for a diverse multimodal instruction dataset, which is crucial for enhancing task generalization. Additionally, they overlook issues related to truthfulness and factuality when generating responses. These findings illuminate current methodological constraints in adapting language models for image comprehension and provide valuable guidance for researchers and practitioners seeking to harness multimodal versions of LLMs.
</details>
<details>
<summary>摘要</summary>
现代大语言模型（LLM）在不同下游任务上显示出了可观的零shot泛化能力。最近的研究将多modal功能integrated into LLMs through model grafting, allowing for zero-shot generalization across multimodal tasks. This study compares different multimodal instruction tuning approaches and evaluates their performance on a range of tasks, including complex reasoning, conversation, image captioning, multiple-choice questions (MCQs), and binary classification. Through rigorous benchmarking and ablation experiments, we reveal key insights for guiding architectural choices when incorporating multimodal capabilities into LLMs. However, current approaches have limitations; they do not adequately address the need for a diverse multimodal instruction dataset, which is crucial for enhancing task generalization. Additionally, they overlook issues related to truthfulness and factuality when generating responses. These findings illuminate current methodological constraints in adapting language models for image comprehension and provide valuable guidance for researchers and practitioners seeking to harness multimodal versions of LLMs.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Large-Scale-3D-Face-Mesh-Video-Dataset-via-Neural-Re-parameterized-Optimization"><a href="#A-Large-Scale-3D-Face-Mesh-Video-Dataset-via-Neural-Re-parameterized-Optimization" class="headerlink" title="A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization"></a>A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03205">http://arxiv.org/abs/2310.03205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kim Youwang, Lee Hyun, Kim Sung-Bin, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh</li>
<li>for: 本研究 propose了一种基于神经网络优化的3D面精度投影方法，以生成具有高可靠性和精度的3D面抽象数据集。</li>
<li>methods: 该方法使用神经网络重parameter化优化方法，将原始的3D面精度投影转换为更加精度和可靠的3D面抽象数据集。</li>
<li>results: 经过 experiments 表明，使用该方法可以生成高质量的3D面抽象数据集，并且可以提高现有3Dface reconstruction模型的重建精度。 code和数据集将于<a target="_blank" rel="noopener" href="https://neuface-dataset.github.io/%E4%B8%AD%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://neuface-dataset.github.io/中公开发布。</a><details>
<summary>Abstract</summary>
We propose NeuFace, a 3D face mesh pseudo annotation method on videos via neural re-parameterized optimization. Despite the huge progress in 3D face reconstruction methods, generating reliable 3D face labels for in-the-wild dynamic videos remains challenging. Using NeuFace optimization, we annotate the per-view/-frame accurate and consistent face meshes on large-scale face videos, called the NeuFace-dataset. We investigate how neural re-parameterization helps to reconstruct image-aligned facial details on 3D meshes via gradient analysis. By exploiting the naturalness and diversity of 3D faces in our dataset, we demonstrate the usefulness of our dataset for 3D face-related tasks: improving the reconstruction accuracy of an existing 3D face reconstruction model and learning 3D facial motion prior. Code and datasets will be available at https://neuface-dataset.github.io.
</details>
<details>
<summary>摘要</summary>
我们提出了 NeuFace，一种基于神经网络优化的3D面积pseudo注解方法，用于视频中的人脸重建。尽管3D人脸重建方法已经做出了巨大的进步，但是在野外动态视频中生成可靠的3D人脸标注仍然是一个挑战。使用NeuFace优化方法，我们为大规模人脸视频annotated每个视图/-帧的准确和一致的3D人脸矩阵，并称之为NeuFace-数据集。我们通过对 Gradient分析进行 investigated如何使用神经重parameterization来重建图像对齐的 facial details on 3D矩阵。利用我们数据集中的自然和多样的3D人脸，我们示出了我们数据集在3D人脸相关任务中的用途：提高现有3D人脸重建模型的重建精度和学习3Dfacial motion prior。我们的代码和数据集将在https://neuface-dataset.github.io上公开。
</details></li>
</ul>
<hr>
<h2 id="Deep-reinforcement-learning-for-machine-scheduling-Methodology-the-state-of-the-art-and-future-directions"><a href="#Deep-reinforcement-learning-for-machine-scheduling-Methodology-the-state-of-the-art-and-future-directions" class="headerlink" title="Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions"></a>Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03195">http://arxiv.org/abs/2310.03195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maziyar Khadivi, Todd Charter, Marjan Yaghoubi, Masoud Jalayer, Maryam Ahang, Ardeshir Shojaeinasab, Homayoun Najjaran</li>
<li>For: 本研究旨在审查和比较基于深度学习的机器调度策略，以优化机器任务分配，遵循制造规则和作业特性。* Methods: 本研究使用深度学习算法，包括传统神经网络、编码器-解码器架构、图 neural networks 和metaheuristic算法，来解决机器调度问题。* Results: 研究发现，基于深度学习的机器调度策略可以快速计算，生成近似全局优解，并在不同的机器环境和作业特性下取得成功应用。但是，这些方法面临着实现复杂操作约束、可 configurable多目标优化、泛化、可扩展性、可读性和稳定性等挑战。<details>
<summary>Abstract</summary>
Machine scheduling aims to optimize job assignments to machines while adhering to manufacturing rules and job specifications. This optimization leads to reduced operational costs, improved customer demand fulfillment, and enhanced production efficiency. However, machine scheduling remains a challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement Learning (DRL), a key component of artificial general intelligence, has shown promise in various domains like gaming and robotics. Researchers have explored applying DRL to machine scheduling problems since 1995. This paper offers a comprehensive review and comparison of DRL-based approaches, highlighting their methodology, applications, advantages, and limitations. It categorizes these approaches based on computational components: conventional neural networks, encoder-decoder architectures, graph neural networks, and metaheuristic algorithms. Our review concludes that DRL-based methods outperform exact solvers, heuristics, and tabular reinforcement learning algorithms in terms of computation speed and generating near-global optimal solutions. These DRL-based approaches have been successfully applied to static and dynamic scheduling across diverse machine environments and job characteristics. However, DRL-based schedulers face limitations in handling complex operational constraints, configurable multi-objective optimization, generalization, scalability, interpretability, and robustness. Addressing these challenges will be a crucial focus for future research in this field. This paper serves as a valuable resource for researchers to assess the current state of DRL-based machine scheduling and identify research gaps. It also aids experts and practitioners in selecting the appropriate DRL approach for production scheduling.
</details>
<details>
<summary>摘要</summary>
机器调度目标是优化作业分配到机器的优化问题，同时遵循制造规则和作业规定。这种优化导致了降低运营成本、改善客户需求满足度和提高生产效率。然而，机器调度问题是一个NP困难的 combinatorial problem。深度学习 Reinforcement Learning (DRL) 作为人工通用智能的关键组件，在不同领域如游戏和机器人等领域中已经展现出了扎实的应用潜力。自1995年以来，研究人员已经开始探索将 DRL 应用于机器调度问题。本文提供了 DRL 基于方法的全面回顾和比较，包括它们的方法学习、应用、优势和局限性。我们根据计算Component分类了这些方法：传统神经网络、编码器-解码器架构、图神经网络和metaheuristic算法。我们的回顾结论是，DRL 基于方法在计算速度和生成 Near-global 优化解决方案方面表现出色，超过了精确解决方法、规则和表示学习算法。这些 DRL 基于方法在不同的机器环境和作业特点下都得到了成功应用。然而，DRL 基于调度器面临着处理复杂的运营约束、可 configurable 多目标优化、泛化、扩展、可读性和可靠性等挑战。未来研究应该集中在解决这些挑战上。本文作为 DRL 基于机器调度的现状评估和研究漏洞的资源，同时也为专家和实践者提供了选择合适 DRL 方法的指南。
</details></li>
</ul>
<hr>
<h2 id="Talking-Models-Distill-Pre-trained-Knowledge-to-Downstream-Models-via-Interactive-Communication"><a href="#Talking-Models-Distill-Pre-trained-Knowledge-to-Downstream-Models-via-Interactive-Communication" class="headerlink" title="Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication"></a>Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03188">http://arxiv.org/abs/2310.03188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Zhao, Qingyun Liu, Huan Gui, Bang An, Lichan Hong, Ed H. Chi</li>
<li>for: 这篇研究的目的是提高下游任务的效能，使用基础模型进行知识传递和学习。</li>
<li>methods: 这篇研究使用了知识传递（KD）技术，并将其扩展为一个互动式沟通过程，帮助学生模型从基础模型中学习有效。</li>
<li>results: 研究发现，这个互动式沟通过程可以优化知识传递，使学生模型在下游任务上表现更好，并且比现有的传递技术表现更高。<details>
<summary>Abstract</summary>
Many recent breakthroughs in machine learning have been enabled by the pre-trained foundation models. By scaling up model parameters, training data, and computation resources, foundation models have significantly advanced the state-of-the-art in many applications. However, it is still an open question of how to use these models to perform downstream tasks efficiently. Knowledge distillation (KD) has been explored to tackle this challenge. KD transfers knowledge from a large teacher model to a smaller student model. While KD has been successful in improving student model performance, recent research has discovered that a powerful teacher does not necessarily lead to a powerful student, due to their huge capacity gap. In addition, the potential distribution shifts between the pre-training data and downstream tasks can make knowledge transfer in KD sub-optimal for improving downstream task performance. In this paper, we extend KD with an interactive communication process to help students of downstream tasks learn effectively from pre-trained foundation models. Our design is inspired by the way humans learn from teachers who can explain knowledge in a way that meets the students' needs. Specifically, we let each model (i.e., student and teacher) train two components: (1) an encoder encoding the model's hidden states to a message and (2) a decoder decoding any messages to its own hidden states. With encoder and decoder, not only can the teacher transfer rich information by encoding its hidden states, but also the student can send messages with information of downstream tasks to the teacher. Therefore, knowledge passing from teacher to student can be tailored to the student's capacity and downstream tasks' distributions. We conducted experiments on benchmark datasets to show that our communication mechanism outperforms state-of-the-art distillation techniques.
</details>
<details>
<summary>摘要</summary>
很多最近的机器学习突破口都是基于预训练基模型。通过扩大模型参数、训练数据和计算资源，基模型已经显著提高了许多应用领域的状态。然而，如何使用这些模型效率地完成下游任务仍然是一个开放的问题。知识储备（KD）已经被研究以解决这个挑战。KD将知识从大型教师模型传递给小型学生模型。虽然KD已经成功地提高学生模型性能，但最近的研究发现，强大的教师模型并不一定导致强大的学生模型，它们之间的质量差距很大。此外，在预训练数据和下游任务的分布变化下，KD可能会导致知识传递不优化下游任务性能。在这篇论文中，我们将KD扩展为交互式通信过程，帮助下游任务的学生模型从预训练基模型中学习效果。我们的设计受到人类教师从知识传递给学生的方式启发。特别是，我们让每个模型（即学生和教师）训练两个组件：（1）编码器将模型的隐藏状态编码成消息，和（2）解码器将任何消息解码回其自己的隐藏状态。通过编码器和解码器，不仅可以让教师传递丰富的信息，还可以让学生将下游任务的信息发送给教师。因此，知识传递从教师到学生可以适应学生的能力和下游任务的分布。我们在标准 benchmark 数据集上进行了实验，证明我们的通信机制超过了当前的储备技术。
</details></li>
</ul>
<hr>
<h2 id="Inferring-Inference"><a href="#Inferring-Inference" class="headerlink" title="Inferring Inference"></a>Inferring Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03186">http://arxiv.org/abs/2310.03186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaqlab/inferringinference">https://github.com/xaqlab/inferringinference</a></li>
<li>paper_authors: Rajkumar Vasudeva Raju, Zhe Li, Scott Linderman, Xaq Pitkow</li>
<li>for: 这paper aimed to develop a mathematical framework for inferring canonical distributed computations from large-scale neural activity patterns.</li>
<li>methods: The authors integrated normative and algorithmic theories of neural computation into a mathematical framework, using a nonlinear message-passing algorithm on a graph-structured model of the world.</li>
<li>results: The framework was able to recover the latent variables, their neural representation and dynamics, and canonical message-functions from simulated recordings of a model brain. The authors highlighted features of experimental design needed to successfully extract canonical computations from neural data.<details>
<summary>Abstract</summary>
Patterns of microcircuitry suggest that the brain has an array of repeated canonical computational units. Yet neural representations are distributed, so the relevant computations may only be related indirectly to single-neuron transformations. It thus remains an open challenge how to define canonical distributed computations. We integrate normative and algorithmic theories of neural computation into a mathematical framework for inferring canonical distributed computations from large-scale neural activity patterns. At the normative level, we hypothesize that the brain creates a structured internal model of its environment, positing latent causes that explain its sensory inputs, and uses those sensory inputs to infer the latent causes. At the algorithmic level, we propose that this inference process is a nonlinear message-passing algorithm on a graph-structured model of the world. Given a time series of neural activity during a perceptual inference task, our framework finds (i) the neural representation of relevant latent variables, (ii) interactions between these variables that define the brain's internal model of the world, and (iii) message-functions specifying the inference algorithm. These targeted computational properties are then statistically distinguishable due to the symmetries inherent in any canonical computation, up to a global transformation. As a demonstration, we simulate recordings for a model brain that implicitly implements an approximate inference algorithm on a probabilistic graphical model. Given its external inputs and noisy neural activity, we recover the latent variables, their neural representation and dynamics, and canonical message-functions. We highlight features of experimental design needed to successfully extract canonical computations from neural data. Overall, this framework provides a new tool for discovering interpretable structure in neural recordings.
</details>
<details>
<summary>摘要</summary>
《Patterns of Microcircuitry Suggest Canonical Distributed Computations in the Brain》研究表明，大脑中的微型维生物呈现出一系列重复的标准计算单元。然而，神经表达是分布的，因此相关的计算可能只有间接关系到单个神经元的变换。因此，如何定义标准分布计算仍然是一个开放的挑战。我们将normative和algorithmic理论 integrate into a mathematical framework for inferring canonical distributed computations from large-scale neural activity patterns.在normative level上，我们假设大脑在进行感知推理任务时创建了一个结构化的内部模型，并将感知输入作为这些内部模型的 latent causes 来解释。在algorithmic level上，我们提议这个推理过程是一种非线性消息传递算法在一个图Structured world 上。给定一个时间序列神经活动记录，我们的框架可以找到（i）神经表达中相关的 latent variables ，（ii）这些变量之间的交互，这些交互定义大脑对世界的内部模型，以及（iii）消息函数，这些函数 specify the inference algorithm。这些目标计算属性因 symmetries inherent in any canonical computation 而可以 statistically distinguishable，up to a global transformation。为了证明，我们模拟了一个模拟大脑，该模拟中包含了一种近似于推理算法的 probabilistic graphical model 。给定外部输入和噪音神经活动，我们可以回归 latent variables，它们的神经表达和动力学，以及标准消息函数。我们高亮了实验设计的特点，以便成功地提取 canonical computations  FROM neural data。总的来说，这种框架提供了一种新的工具，可以在神经记录中发现可读取的结构。
</details></li>
</ul>
<hr>
<h2 id="Misusing-Tools-in-Large-Language-Models-With-Visual-Adversarial-Examples"><a href="#Misusing-Tools-in-Large-Language-Models-With-Visual-Adversarial-Examples" class="headerlink" title="Misusing Tools in Large Language Models With Visual Adversarial Examples"></a>Misusing Tools in Large Language Models With Visual Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03185">http://arxiv.org/abs/2310.03185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）在使用工具和处理多模态时新增的能力，以及这些新能力带来的新的安全风险。</li>
<li>methods: 作者使用梯度基本 adversarial 训练构建了对抗示例，并通过多个维度测试了这些攻击的性能。</li>
<li>results: 作者发现，使用这些对抗示例可以让攻击者让受影响的 LLM  invoke 工具，例如删除日历事件、泄露私人对话和预订酒店等。这些攻击可以让用户资源受到攻击而不会被发现，并且可以在多个输入提示上进行攻击。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-architecture-impact-on-identifying-temporally-extended-Reinforcement-Learning-tasks"><a href="#Neural-architecture-impact-on-identifying-temporally-extended-Reinforcement-Learning-tasks" class="headerlink" title="Neural architecture impact on identifying temporally extended Reinforcement Learning tasks"></a>Neural architecture impact on identifying temporally extended Reinforcement Learning tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03161">http://arxiv.org/abs/2310.03161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Vadakechirayath George</li>
<li>for: 这 paper 的目的是在 OpenAI Gym Atari-2600 游戏集中使用 Attention 模型进行 Deep Reinforcement Learning (RL) 探索，并提供更好的解释能力。</li>
<li>methods: 这 paper 使用了 Attention 模型，包括 Attention 图像混合和 Vision Transformer，以提高 RL 模型的解释性和性能。</li>
<li>results: 这 paper 的模型在 OpenAI Gym Atari-2600 游戏集中表现出色，并且提供了更好的解释能力，让 agent 的选择动作更加直观。<details>
<summary>Abstract</summary>
Inspired by recent developments in attention models for image classification and natural language processing, we present various Attention based architectures in reinforcement learning (RL) domain, capable of performing well on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep Reinforcement learning techniques in various fields like robotics, gaming and healthcare, they suffer from a major drawback that neural networks are difficult to interpret. We try to get around this problem with the help of Attention based models. In Attention based models, extracting and overlaying of attention map onto images allows for direct observation of information used by agent to select actions and easier interpretation of logic behind the chosen actions. Our models in addition to playing well on gym-Atari environments, also provide insights on how agent perceives its environment. In addition, motivated by recent developments in attention based video-classification models using Vision Transformer, we come up with an architecture based on Vision Transformer, for image-based RL domain too. Compared to previous works in Vision Transformer, our model is faster to train and requires fewer computational resources. 3
</details>
<details>
<summary>摘要</summary>
受最近图像分类和自然语言处理领域的注意模型发展启发，我们在再增强学习（RL）领域提出了多种注意模型架构，能够在OpenAI Gym Atari-2600游戏集成环境中表现良好。 despite the recent success of deep reinforcement learning techniques in various fields such as robotics, gaming, and healthcare, these techniques suffer from a major drawback that neural networks are difficult to interpret. To address this problem, we use attention-based models. In attention-based models, extracting and overlaying attention maps onto images allows for direct observation of the information used by the agent to select actions and easier interpretation of the logic behind the chosen actions. Our models not only perform well on gym-Atari environments, but also provide insights into how the agent perceives its environment. In addition, inspired by recent developments in attention-based video classification models using Vision Transformer, we propose an architecture based on Vision Transformer for the image-based RL domain, which is faster to train and requires fewer computational resources than previous works.
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Prediction-Intervals-Using-Uncertainty-Characteristics-Curves"><a href="#Assessment-of-Prediction-Intervals-Using-Uncertainty-Characteristics-Curves" class="headerlink" title="Assessment of Prediction Intervals Using Uncertainty Characteristics Curves"></a>Assessment of Prediction Intervals Using Uncertainty Characteristics Curves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03158">http://arxiv.org/abs/2310.03158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiri Navratil, Benjamin Elder, Matthew Arnold, Soumya Ghosh, Prasanna Sattigeri</li>
<li>for: 该论文旨在为信任AI提供可靠的模型不确定量化。</li>
<li>methods: 该论文使用操作特征曲线和空间参照点的概念，提出了一种无关操作点的评估方法，用于评估预测间隔。</li>
<li>results: 论文提出了一种新的评估方法，可以准确评估预测间隔的不确定性。该方法在选定的场景中进行了示例评估，并证明了其在现有评估工具箱中的价值。<details>
<summary>Abstract</summary>
Accurate quantification of model uncertainty has long been recognized as a fundamental requirement for trusted AI. In regression tasks, uncertainty is typically quantified using prediction intervals calibrated to an ad-hoc operating point, making evaluation and comparison across different studies relatively difficult. Our work leverages: (1) the concept of operating characteristics curves and (2) the notion of a gain over a null reference, to derive a novel operating point agnostic assessment methodology for prediction intervals. The paper defines the Uncertainty Characteristics Curve and demonstrates its utility in selected scenarios. We argue that the proposed method addresses the current need for comprehensive assessment of prediction intervals and thus represents a valuable addition to the uncertainty quantification toolbox.
</details>
<details>
<summary>摘要</summary>
长期以来，模型不确定性的准确量化已被认为是人工智能的基本需求。在回归任务中，不确定性通常通过预测Intervals进行衡量，但这会使评估和比较不同研究变得更加困难。我们的工作利用：（1）运行特性曲线的概念和（2）对Null参照点的增值来 derive一种新的无关操作点评估方法，用于预测Intervals。文章定义了不确定性特征曲线，并在选择的场景中示出其实用性。我们认为，提案的方法可以全面评估预测Intervals，因此代表了 uncertainty量化工具箱中的一个值得加入的贡献。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Multimodal-Segmentation-in-Medical-Imaging-Combining-YOLOv8-with-SAM-and-HQ-SAM-Models"><a href="#Comprehensive-Multimodal-Segmentation-in-Medical-Imaging-Combining-YOLOv8-with-SAM-and-HQ-SAM-Models" class="headerlink" title="Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models"></a>Comprehensive Multimodal Segmentation in Medical Imaging: Combining YOLOv8 with SAM and HQ-SAM Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12995">http://arxiv.org/abs/2310.12995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumit Pandey, Kuan-Fu Chen, Erik B. Dam</li>
<li>for: 这篇论文旨在提出一个综合方法来对不同医疗影像资料中的区域兴趣 (ROI) 进行分类，包括ultrasound、CT扫描和X射线影像。</li>
<li>methods: 提案方法利用 YOLOv8 模型来 aproximate 边框检测过程，以及 Segment Anything Model (SAM) 和 High Quality (HQ) SAM 模型来实现完全自动且精确的分类。</li>
<li>results: 研究结果显示 YOLOv8+SAM 模型在医疗影像分类中具有高度的准确性和性能，而 HQ-SAM 模型优于其他两个模型，但是其额外的计算成本不足以衡量其增加的功能。<details>
<summary>Abstract</summary>
This paper introduces a comprehensive approach for segmenting regions of interest (ROI) in diverse medical imaging datasets, encompassing ultrasound, CT scans, and X-ray images. The proposed method harnesses the capabilities of the YOLOv8 model for approximate boundary box detection across modalities, alongside the Segment Anything Model (SAM) and High Quality (HQ) SAM for fully automatic and precise segmentation. To generate boundary boxes, the YOLOv8 model was trained using a limited set of 100 images and masks from each modality. The results obtained from our approach are extensively computed and analyzed, demonstrating its effectiveness and potential in medical image analysis. Various evaluation metrics, including precision, recall, F1 score, and Dice Score, were employed to quantify the accuracy of the segmentation results. A comparative analysis was conducted to assess the individual and combined performance of the YOLOv8, YOLOv8+SAM, and YOLOv8+HQ-SAM models. The results indicate that the SAM model performs better than the other two models, exhibiting higher segmentation accuracy and overall performance. While HQ-SAM offers potential advantages, its incremental gains over the standard SAM model may not justify the additional computational cost. The YOLOv8+SAM model shows promise for enhancing medical image segmentation and its clinical implications.
</details>
<details>
<summary>摘要</summary>
To generate boundary boxes, the YOLOv8 model was trained using a limited set of 100 images and masks from each modality. The results show that the SAM model outperforms the other two models in terms of segmentation accuracy and overall performance, with higher precision, recall, F1 score, and Dice Score. While HQ-SAM offers potential advantages, its incremental gains over the standard SAM model may not justify the additional computational cost.The YOLOv8+SAM model demonstrates promise for enhancing medical image segmentation and its clinical implications. Various evaluation metrics were employed to quantify the accuracy of the segmentation results, and a comparative analysis was conducted to assess the individual and combined performance of the three models.
</details></li>
</ul>
<hr>
<h2 id="Attributing-Learned-Concepts-in-Neural-Networks-to-Training-Data"><a href="#Attributing-Learned-Concepts-in-Neural-Networks-to-Training-Data" class="headerlink" title="Attributing Learned Concepts in Neural Networks to Training Data"></a>Attributing Learned Concepts in Neural Networks to Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03149">http://arxiv.org/abs/2310.03149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Nicholas Konz, Charles Godfrey, Madelyn Shapiro, Jonathan Tu, Henry Kvinge, Davis Brown</li>
<li>For: The paper aims to investigate how deep learning models learn certain human-interpretable features and which inputs from the model’s original training set are most important for learning a concept at a given layer.* Methods: The authors use data attribution methods combined with probing the concepts learned by a model. They train network and probe ensembles for two concept datasets on a range of network layers and use the recently developed TRAK method for large-scale data attribution.* Results: The authors find evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that the features that inform the development of a concept are spread in a more diffuse manner across its exemplars, implying robustness in concept formation.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究旨在调查深度学习模型如何学习人类可理解的特征，以及模型原始训练数据中哪些输入最重要 для某层概念的学习。</li>
<li>methods: 作者使用数据负担方法和探测模型学习的方法，对两个概念集合进行网络层的训练和探测。他们使用最近发展的TRAK方法进行大规模数据负担。</li>
<li>results: 作者发现了一些证据，表明模型学习的概念是通过某些特定的例子来形成的，但不是依赖于特定的几个例子。从这些结果来看，概念的特征是通过其示例中的各种扩散的方式被学习，这表明模型在概念形成中具有更好的可靠性。<details>
<summary>Abstract</summary>
By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the development of a concept are spread in a more diffuse manner across its exemplars, implying robustness in concept formation.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)到目前为止，已有证据表明深度学习模型在数据内部表征中学习了一些人类可解释的特征。因为有效的概念是机器学习系统中关键的，因此 naturall 要问，哪些输入在模型的原始训练集中对于某层学习某个概念是最重要的。为了回答这个问题，我们将数据负担方法与模型学习概念的方法结合使用。我们在两个概念集上对多个网络层进行训练和探测集 ensemble，并使用最近发展的TRAK方法进行大规模数据负担。我们发现了一些证据，表明在某些情况下，移除概念的10000个最重要的示例图像，并重新训练模型，不会改变概念在网络中的位置，也不会改变探测稀缺性。这表示，相比于依赖于几个特定示例，概念的发展中的特征是在其示例中散布在更加混合的方式，这表明了概念的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Federated-Prompt-Tuning-for-Black-box-Large-Pre-trained-Models"><a href="#Efficient-Federated-Prompt-Tuning-for-Black-box-Large-Pre-trained-Models" class="headerlink" title="Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models"></a>Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03123">http://arxiv.org/abs/2310.03123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Lin, Yan Sun, Yifan Shi, Xueqian Wang, Lifu Huang, Li Shen, Dacheng Tao</li>
<li>for: 这篇论文的目的是提出一种名为“ Federated Black-Box Prompt Tuning”（Fed-BBPT）的新方法，以便在内存限制和隐私问题下，有效地调整预训模型（PTM）。</li>
<li>methods: 这篇论文使用了一个中央服务器，帮助地方用户协同训练一个提示生成器，通过定期的聚合来实现。地方用户通过API驱动学习，不需要PTM的部署。</li>
<li>results: 试验结果显示，相比于广泛的 fine-tuning，Fed-BBPT 能够有效地解决内存限制和隐私问题，并且在40个数据集上进行了 Thorough 的评估。<details>
<summary>Abstract</summary>
With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, instead capitalizing on a central server that aids local users in collaboratively training a prompt generator through regular aggregation. Local users leverage API-driven learning via a zero-order optimizer, obviating the need for PTM deployment. Relative to extensive fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM storage and fine-tuning on local machines, tapping into comprehensive, high-quality, yet private training datasets. A thorough evaluation across 40 datasets spanning CV and NLP tasks underscores the robustness of our proposed model.
</details>
<details>
<summary>摘要</summary>
随着预训模型（PTM）的快速发展，有效地调节这些模型以适应多样化的下游应用已成为研究的约束。虽然最近关于提问调教的研究已提供了可能的道路，但是三个突出的挑战仍然存在：（1）内存约束：随着开源PTM的大小不断增长，细化大量参数已成为许多实践者的挑战。（2）模型隐私：现有PTM通常作为公共API服务，其参数不可访问，使得有效或特定的细化不可能。（3）数据隐私：PTM的细化需要高质量的数据，这些数据通常是本地化的，并不会被公开分享。为了最佳地利用每个本地数据集，同时绕过内存约束和隐私问题，我们提议了联邦黑盒提问调教（Fed-BBPT）。这种创新的方法不依赖参数建筑和私人数据访问，而是通过中央服务器，帮助本地用户通过定期聚合来共同培训一个提问生成器。本地用户通过API驱动学习，使用零次训练算法，无需PTM部署。相比较广泛的细化，Fed-BBPT efficiently circumvents memory challenges related to PTM storage and fine-tuning on local machines, tapping into comprehensive, high-quality, yet private training datasets. 一项严格的评估 across 40 datasets spanning CV and NLP tasks highlights the robustness of our proposed model.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Cascades-with-Mixture-of-Thoughts-Representations-for-Cost-efficient-Reasoning"><a href="#Large-Language-Model-Cascades-with-Mixture-of-Thoughts-Representations-for-Cost-efficient-Reasoning" class="headerlink" title="Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning"></a>Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03094">http://arxiv.org/abs/2310.03094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/murongyue/llm_mot_cascade">https://github.com/murongyue/llm_mot_cascade</a></li>
<li>paper_authors: Murong Yue, Jie Zhao, Min Zhang, Liang Du, Ziyu Yao</li>
<li>for: 降低使用大语言模型（LLM）的成本，特别是在推理（例如数学、 causal）任务中。</li>
<li>methods: 建立LLM层次结构，根据问题的困难程度使用不同的LLM。提出了“答案一致性”来评估问题的困难程度，并提出了答案采样和一致性检查的方法，包括使用两种思维表示（Chain-of-Thought和Program-of-Thought）的混合。</li>
<li>results: 通过在六个推理基准数据集上进行实验，使用GPT-3.5-turbo和GPT-4作为较弱和较强LLM，分别显示了我们提议的LLM层次结构可以与使用 solo 较强LLM获得相同的性能，但需要只有40%的成本。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40% of its cost.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）如GPT-4在多种任务中表现出了非常出色的表现，但这强大的表现经常是通过使用付费API服务来实现的。在这篇论文中，我们受到了建立LLM堆叠以降低使用LLM的成本的动机。我们的堆叠管道遵循了intuition，即更简单的问题可以由较弱但更有经济的LLM来解决，而只有复杂的问题需要更强大且更昂贵的LLM。为实现这种决策，我们考虑了使用较弱LLM的答案一致性作为问题难度的信号，并提出了一些答案采样和一致性检查方法，其中一种利用两种思维表示（即链条思维和计划思维）的混合。通过在六个逻辑 bench mark 数据集上进行实验，使用GPT-3.5-turbo和GPT-4作为较弱和更强的LLM，我们示出了我们提议的LLM堆叠可以与使用唯一的更强LLM实现相同的表现，但需要的成本只有40%。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Knowledge-Critical-Subnetworks-in-Pretrained-Language-Models"><a href="#Discovering-Knowledge-Critical-Subnetworks-in-Pretrained-Language-Models" class="headerlink" title="Discovering Knowledge-Critical Subnetworks in Pretrained Language Models"></a>Discovering Knowledge-Critical Subnetworks in Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03084">http://arxiv.org/abs/2310.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deniz Bayazit, Negar Foroutan, Zeming Chen, Gail Weiss, Antoine Bosselut</li>
<li>for: 本研究旨在探索预训练语言模型（LMs）中存储知识的方法。</li>
<li>methods: 我们提出了一种多目标可 differentiable 权重屏蔽方法，用于发现LMs中的各种知识关键互联网络，并证明我们可以通过这些子网络来精准地从模型中除除特定知识。</li>
<li>results: 我们在多个GPT2变种上应用了我们的方法，并发现了高度稀疏的互联网络（98%以上），这些网络仅负责特定的关系知识。当这些网络被移除时，原始语言模型保留了大部分的语言和其他已memorized关系知识，但是无法表达被移除的知识，并在下游任务中表现出较大的性能下降。<details>
<summary>Abstract</summary>
Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized relational knowledge) but struggles to express the removed knowledge, and suffers performance drops on examples needing this removed knowledge on downstream tasks after finetuning.
</details>
<details>
<summary>摘要</summary>
预训言语模型（LM）储存了知识的隐式表示。然而，当地址这些表示并分离它们时，仍然是一个开放的问题。在这项工作中，我们调查了预训言语模型是否包含不同类型的知识核心子网络：特定的稀疏计算子图负责编码特定的知识，模型记忆的。我们提出了多目标可微分权重封顶方案，用于发现这些子网络，并证明我们可以使用它们来精确地从模型中除除特定知识，以避免对原始语言模型的行为产生负面影响。我们在多个GPT2变体上应用了这种方法，揭示出了高度稀疏的子网络（98%以上），这些子网络仅负责特定的关系知识。当这些子网络被移除后，剩下的网络保留了大部分的初始容量（模型语言和其他记忆的关系知识），但是它困难表达被移除的知识，并在下游任务之后训练后表现下降。
</details></li>
</ul>
<hr>
<h2 id="LanguageMPC-Large-Language-Models-as-Decision-Makers-for-Autonomous-Driving"><a href="#LanguageMPC-Large-Language-Models-as-Decision-Makers-for-Autonomous-Driving" class="headerlink" title="LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving"></a>LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03026">http://arxiv.org/abs/2310.03026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding</li>
<li>For: This paper aims to address the challenges faced by existing learning-based autonomous driving (AD) systems, such as comprehending high-level information, generalizing to rare events, and providing interpretability.* Methods: The paper employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios, and develops algorithms for translating LLM decisions into actionable driving commands. The proposed method integrates LLM decisions with low-level controllers through guided parameter matrix adaptation.* Results: The proposed method consistently surpasses baseline approaches in single-vehicle tasks and handles complex driving behaviors, including multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. The method demonstrates improvements in safety, efficiency, generalizability, and interoperability.Here’s the information in Simplified Chinese text:* 为：这篇论文目的是解决现有的学习型自动驾驶（AD）系统面临的挑战，包括理解高级信息、泛化到罕见事件以及提供可解释性。* 方法：论文使用大型语言模型（LLM）作为复杂AD场景的决策组件，并开发了将 LLM 决策转化为可驾驶指令的算法。提议的方法将 LLM 决策与低级控制器集成through导航参数矩阵适应。* 结果：提议的方法在单车任务上一致性地超越基线方法，并能够处理复杂的驾驶行为，包括多车协调。这些成果表明了提议的方法在安全、效率、泛化性和可工作性等方面具有优势。<details>
<summary>Abstract</summary>
Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field. Project page: https://sites.google.com/view/llm-mpc
</details>
<details>
<summary>摘要</summary>
现有的学习型自动驾驶（AD）系统面临高度信息理解、罕见事件扩展和解释性提供问题。为解决这些问题，本研究将大型语言模型（LLM）作为决策 компонен应用于复杂的 AD 情况，需要人类通过感知理解。我们设计了认知路径来实现全面的理解，并开发了将 LLM 决策转换为可行动指令的算法。这种方法使得 LLM 的决策与低层控制器集成，并且通过导引 Parametric Matrix Adaptation（PMA）来实现。实验结果显示，我们的提案方法不仅在单车辆任务中显著超过基eline方法，而且能够处理复杂的驾驶行为，甚至多车辆协调。这篇论文表明了使用 LLM 作为具有安全、效率、普遍性和互操作性的决策员的可能性。我们希望这篇论文能够成为未来这个领域的研究启发。项目页面：https://sites.google.com/view/llm-mpc
</details></li>
</ul>
<hr>
<h2 id="Retrieval-meets-Long-Context-Large-Language-Models"><a href="#Retrieval-meets-Long-Context-Large-Language-Models" class="headerlink" title="Retrieval meets Long Context Large Language Models"></a>Retrieval meets Long Context Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03025">http://arxiv.org/abs/2310.03025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro</li>
<li>for: This paper aims to compare the effectiveness of retrieval-augmentation and long context extension for improving the performance of large language models (LLMs) on downstream tasks.</li>
<li>methods: The authors use two state-of-the-art pretrained LLMs, a proprietary 43B GPT and LLaMA2-70B, and compare the performance of these models with and without retrieval-augmentation and long context extension.</li>
<li>results: The authors find that retrieval-augmentation can significantly improve the performance of LLMs, regardless of their extended context window sizes. Their best model, retrieval-augmented LLaMA2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long context tasks, while being much faster at generation.Here’s the Chinese translation of the three information points:</li>
<li>for: 这篇论文目标是比较大语言模型（LLM）下游任务的表现，使用扩展 langecontext window 和 retrieval-augmentation 两种方法进行比较。</li>
<li>methods: 作者使用了两个 state-of-the-art 预训练 LLM，一个是一个专有的 43B GPT，另一个是 LLaMA2-70B，并对这两个模型进行了比较。</li>
<li>results: 作者发现，使用 retrieval-augmentation 可以significantly improve LLM 的表现，无论其扩展 context window 的大小。最佳模型， retrieval-augmented LLaMA2-70B 的 32K context window，在七个长 context task 中的平均分数上 OUTPERFORMS GPT-3.5-turbo-16k 和 Davinci003，同时在生成速度上也比其快得多。<details>
<summary>Abstract</summary>
Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented LLaMA2-70B with 32K context window, outperforms GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long context tasks including question answering and query-based summarization. It also outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的上下文窗口扩展已经在最近很受欢迎，而使用检索来增强LLM的解决方案已经存在很多年了。然而，问题是：i) 检索增强 versus 长上下文窗口，哪一个更适合下游任务？ii) 两种方法可以在一起使用，以获得最佳的结果吗？在这个研究中，我们回答了这些问题，通过使用两个国际先进的预训练LLM，即一个43B GPT和LLaMA2-70B。可能有些surprisingly，我们发现，使用简单的检索增强在生成时可以将LLM的4K上下文窗口与较短的16K上下文窗口进行比较，而且只需要相对较少的计算。更重要的是，我们发现检索可以对LLM的性能产生显著改善，无论它的上下文窗口大小如何。我们的最佳模型是使用检索增强LLaMA2-70B的32K上下文窗口，在七个长上下文任务中的平均得分高于GPT-3.5-turbo-16k和Davinci003，而且和非检索LLaMA2-70B-32k基eline的差距更大，同时在生成时 faster。我们的研究提供了 LLM的选择检索增强 versus 长上下文扩展的一般性见解，为实践者提供了指导。
</details></li>
</ul>
<hr>
<h2 id="Human-oriented-Representation-Learning-for-Robotic-Manipulation"><a href="#Human-oriented-Representation-Learning-for-Robotic-Manipulation" class="headerlink" title="Human-oriented Representation Learning for Robotic Manipulation"></a>Human-oriented Representation Learning for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03023">http://arxiv.org/abs/2310.03023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingxiao Huo, Mingyu Ding, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, Wei Zhan</li>
<li>for: 这 paper 的目的是提出一种基于人类视觉表征的 robot 学习方法，以便更好地学习 manipulate 任务。</li>
<li>methods: 这 paper 使用 multi-task fine-tuning 技术，在 pre-trained 视觉编码器 上同时学习多个人类视觉技能，以提高 robot 的 manipulate 能力。</li>
<li>results: 实验表明，这 paper 的 Task Fusion Decoder 可以有效地改善三种state-of-the-art 视觉编码器（R3M、MVP、EgoVLP）的表征，以便下游 manipulate 策略学习。<details>
<summary>Abstract</summary>
Humans inherently possess generalizable visual representations that empower them to efficiently explore and interact with the environments in manipulation tasks. We advocate that such a representation automatically arises from simultaneously learning about multiple simple perceptual skills that are critical for everyday scenarios (e.g., hand detection, state estimate, etc.) and is better suited for learning robot manipulation policies compared to current state-of-the-art visual representations purely based on self-supervised objectives. We formalize this idea through the lens of human-oriented multi-task fine-tuning on top of pre-trained visual encoders, where each task is a perceptual skill tied to human-environment interactions. We introduce Task Fusion Decoder as a plug-and-play embedding translator that utilizes the underlying relationships among these perceptual skills to guide the representation learning towards encoding meaningful structure for what's important for all perceptual skills, ultimately empowering learning of downstream robotic manipulation tasks. Extensive experiments across a range of robotic tasks and embodiments, in both simulations and real-world environments, show that our Task Fusion Decoder consistently improves the representation of three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for downstream manipulation policy-learning. Project page: https://sites.google.com/view/human-oriented-robot-learning
</details>
<details>
<summary>摘要</summary>
人类自然地拥有通用的视觉表示，这使得他们能够高效地探索和与环境进行互动，在操作任务中。我们认为这种表示自动从同时学习多个简单的感知技能（例如手检测、状态估计等）中得到，这些技能是日常场景中 kritical的。我们通过人类中心的多任务练习和预训练视觉编码器的方式来正式化这个想法。我们引入了任务融合解码器，它利用这些感知技能之间的下面关系来导向表示学习，以便在所有感知技能中编码有意义的结构，最终为下游机器人操作任务学习提供更好的表示。我们在多种机器人任务和实体上，包括实验室和实际环境，进行了广泛的实验，结果表明，我们的任务融合解码器可以一直提高三个现状最佳的视觉编码器，包括R3M、MVP和EgoVLP，以便下游机器人操作策略学习。项目页面：https://sites.google.com/view/human-oriented-robot-learning
</details></li>
</ul>
<hr>
<h2 id="AstroCLIP-Cross-Modal-Pre-Training-for-Astronomical-Foundation-Models"><a href="#AstroCLIP-Cross-Modal-Pre-Training-for-Astronomical-Foundation-Models" class="headerlink" title="AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models"></a>AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03024">http://arxiv.org/abs/2310.03024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PolymathicAI/AstroCLIP">https://github.com/PolymathicAI/AstroCLIP</a></li>
<li>paper_authors: Francois Lanusse, Liam Parker, Siavash Golkar, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Geraud Krawezik, Michael McCabe, Ruben Ohana, Mariel Pettee, Bruno Regaldo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho</li>
<li>for:  bridging the gap between diverse observational modalities in astronomy, specifically between images and optical spectra of galaxies</li>
<li>methods: cross-modal contrastive learning approach using multi-band images and optical spectra from the Dark Energy Spectroscopic Instrument (DESI)</li>
<li>results: highly informative embeddings of both modalities that can be used for accurate cross-modal searches, and encoding valuable physical information about the galaxies (redshift and stellar mass) that can be used for competitive zero- and few-shot predictions without further finetuning.<details>
<summary>Abstract</summary>
We present AstroCLIP, a strategy to facilitate the construction of astronomical foundation models that bridge the gap between diverse observational modalities. We demonstrate that a cross-modal contrastive learning approach between images and optical spectra of galaxies yields highly informative embeddings of both modalities. In particular, we apply our method on multi-band images and optical spectra from the Dark Energy Spectroscopic Instrument (DESI), and show that: (1) these embeddings are well-aligned between modalities and can be used for accurate cross-modal searches, and (2) these embeddings encode valuable physical information about the galaxies -- in particular redshift and stellar mass -- that can be used to achieve competitive zero- and few- shot predictions without further finetuning. Additionally, in the process of developing our approach, we also construct a novel, transformer-based model and pretraining approach for processing galaxy spectra.
</details>
<details>
<summary>摘要</summary>
我们提出了 AstroCLIP，一种方法来促进天文基础模型的建构，使得不同观测方式之间的 gap 得到 bridge。我们证明了一种在图像和光谱之间进行交叉模态学习的方法，可以得到高度有用的嵌入。特别是，我们对 DESI 多核带图像和光谱进行了应用，并证明了以下两点：1. 这些嵌入是多modal之间协调的，可以进行精准的交叉模态搜索。2. 这些嵌入包含有价值的物理信息，例如红移和星系质量，可以用于实现无需追加微调的零和几次预测。在开发我们的方法的过程中，我们还构建了一种新的transformer模型和预训练方法，用于处理星系光谱。
</details></li>
</ul>
<hr>
<h2 id="SemiReward-A-General-Reward-Model-for-Semi-supervised-Learning"><a href="#SemiReward-A-General-Reward-Model-for-Semi-supervised-Learning" class="headerlink" title="SemiReward: A General Reward Model for Semi-supervised Learning"></a>SemiReward: A General Reward Model for Semi-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03013">http://arxiv.org/abs/2310.03013</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Westlake-AI/SemiReward">https://github.com/Westlake-AI/SemiReward</a></li>
<li>paper_authors: Siyuan Li, Weiyang Jin, Zedong Wang, Fang Wu, Zicheng Liu, Cheng Tan, Stan Z. Li</li>
<li>for: 提高 semi-supervised learning (SSL) 的性能和速度，并具有任务多样性和场景适应性。</li>
<li>methods: 提出了一种 Semi-supervised Reward 框架 (SemiReward)，通过预测奖励分数来评估和筛选高质量的 pseudo labels，可插入主流 SSL 方法中。</li>
<li>results: 对于13个标准 SSL benchmark 的三种模式，广泛的实验表明 SemiReward 可以获得显著性能提升和更快的整合速度，比 Pseudo Label、FlexMatch 和 Free&#x2F;SoftMatch 更好。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks of three modalities, extensive experiments verify that SemiReward achieves significant performance gains and faster convergence speeds upon Pseudo Label, FlexMatch, and Free/SoftMatch.
</details>
<details>
<summary>摘要</summary>
半supervised learning（SSL）在自我训练框架中各种改进得到了 significiant progress。然而，主要挑战是如何distinguish高质量的pseudo标签对于 confirmation bias。现有的pseudo标签选择策略受到先defined scheme或者特制的手工策略所限制，无法同时实现高质量标签、快速整合和任务多样性。为了解决这些问题，我们提议一种半supervised reward框架（SemiReward），该框架可以预测 Pseudo Labels的评分分数，并用这些分数来筛选高质量的pseudo标签。SemiReward可以与主流的SSL方法在各种任务类型和场景中结合使用。为了mitigate confirmation bias，SemiReward在线上训练在两个阶段，使用生成器模型和抽样策略。在13个标准SSL benchmark上进行了多种任务的实验，结果表明，SemiReward可以在Pseudo Label、FlexMatch和Free/SoftMatch上实现显著的性能提升和快速的整合速度。
</details></li>
</ul>
<hr>
<h2 id="Soft-Convex-Quantization-Revisiting-Vector-Quantization-with-Convex-Optimization"><a href="#Soft-Convex-Quantization-Revisiting-Vector-Quantization-with-Convex-Optimization" class="headerlink" title="Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization"></a>Soft Convex Quantization: Revisiting Vector Quantization with Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03004">http://arxiv.org/abs/2310.03004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanmay Gautam, Reid Pryzant, Ziyi Yang, Chenguang Zhu, Somayeh Sojoudi</li>
<li>For:	+ The paper is written for extracting informative discrete latent representations using Vector Quantization (VQ) and mitigating its practical challenges.* Methods:	+ The paper proposes Soft Convex Quantization (SCQ) as a direct substitute for VQ, which is a differentiable convex optimization (DCO) layer that solves for the optimal convex combination of codebook vectors to quantize inputs.* Results:	+ The paper shows that SCQ significantly outperforms matched VQ-based architectures in terms of image reconstruction and codebook usage, with an order of magnitude improvement and comparable quantization runtime. The paper also demonstrates the efficacy of SCQ on the CIFAR-10, GTSRB, and LSUN datasets.<details>
<summary>Abstract</summary>
Vector Quantization (VQ) is a well-known technique in deep learning for extracting informative discrete latent representations. VQ-embedded models have shown impressive results in a range of applications including image and speech generation. VQ operates as a parametric K-means algorithm that quantizes inputs using a single codebook vector in the forward pass. While powerful, this technique faces practical challenges including codebook collapse, non-differentiability and lossy compression. To mitigate the aforementioned issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the forward pass, we solve for the optimal convex combination of codebook vectors that quantize the inputs. In the backward pass, we leverage differentiability through the optimality conditions of the forward solution. We then introduce a scalable relaxation of the SCQ optimization and demonstrate its efficacy on the CIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder models that significantly outperform matched VQ-based architectures, observing an order of magnitude better image reconstruction and codebook usage with comparable quantization runtime.
</details>
<details>
<summary>摘要</summary>
Vector量生成（VQ）是深度学习中广泛使用的一种技术，用于从输入数据中提取有用的整数纹理表示。VQ模型在图像和语音生成等应用中显示出了很好的效果。VQ运算如 parametric K-means 算法，在前向传输中使用单个codebook vector来量化输入。虽然这种技术具有很大的力量，但它还面临着实际问题，包括codebook塌陷、非导数性和数据损失。为了解决这些问题，我们提议使用软凸量化（SCQ）作为VQ的直接替代品。SCQ在前向传输中解决了输入的最佳凸合问题，并在反向传输中利用了导数性。我们然后提出了SCQ优化的可扩展relaxation，并在CIFAR-10、GTSRB和LSUN数据集上证明了其效果。我们在SCQ自适应模型中训练出了与对照VQ模型相比有 ORDER OF MAGNITUDE 更好的图像重建和codebook使用，同时具有相对较快的量化时间。
</details></li>
</ul>
<hr>
<h2 id="ECoFLaP-Efficient-Coarse-to-Fine-Layer-Wise-Pruning-for-Vision-Language-Models"><a href="#ECoFLaP-Efficient-Coarse-to-Fine-Layer-Wise-Pruning-for-Vision-Language-Models" class="headerlink" title="ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models"></a>ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02998">http://arxiv.org/abs/2310.02998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ylsung/ECoFLaP">https://github.com/ylsung/ECoFLaP</a></li>
<li>paper_authors: Yi-Lin Sung, Jaehong Yoon, Mohit Bansal</li>
<li>for: 这篇论文旨在提出一种高效的层别剪减方法，以简化大型视觉语言模型（LVLM）的 computional 成本和碳负担，并提高多modal 下游任务的性能。</li>
<li>methods: 这篇论文提出了一种两阶段粗细剪减方法，首先通过调整层别或封页的组合缩减因子，然后使用全球性质给出的统计量来进行本层次的粗细剪减。</li>
<li>results: 这篇论文在多modal 和单modal 模型和数据集上进行验证，证明了与传统剪减方法相比，这种方法在高简化状态下能够提高性能。<details>
<summary>Abstract</summary>
Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable performance improvements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, these methods often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP), a two-stage coarse-to-fine weight pruning approach for LVLMs. We first determine the sparsity ratios of different layers or blocks by leveraging the global importance score, which is efficiently computed based on the zeroth-order approximation of the global model gradients. Then, the multimodal model performs local layer-wise unstructured weight pruning based on globally-informed sparsity ratios. We validate our proposed method across various multimodal and unimodal models and datasets, demonstrating significant performance improvements over prevalent pruning techniques in the high-sparsity regime.
</details>
<details>
<summary>摘要</summary>
大型视言语模型（LVLM）可以全面理解世界，通过 integrate 多种感知模式的丰富信息，实现了多种多模态下游任务的很好表现。然而，部署 LVLM 的时候会遇到巨大的计算/能源成本和碳排放问题，这些问题使得使用 conventient 迭代全球磨煞成本昂贵。相反，一些最近的研究已经提出了层 wise 磨煞方法，以避免全球磨煞的昂贵计算，并快速压缩模型 весов根据它们在层中的重要性。然而，这些方法经常受到不佳的模型压缩问题，因为它们缺乏全局视角。为了解决这些限制，我们提出了高效的 Coarse-to-Fine 层 wise 磨煞方法（ECoFLaP），这是一个两个阶段的粗粒度-细粒度 weight 磨煞方法。我们首先通过利用全局重要性分数来确定不同层或块的缺失比率。然后，模型在基于全球重要性分数的局部层 wise 无结构Weight 磨煞。我们在多种多模态模型和数据集上验证了我们的提议方法，并证明了在高缺失比率下，它们与现有的磨煞技术相比具有显著的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Physics-Pretraining-for-Physical-Surrogate-Models"><a href="#Multiple-Physics-Pretraining-for-Physical-Surrogate-Models" class="headerlink" title="Multiple Physics Pretraining for Physical Surrogate Models"></a>Multiple Physics Pretraining for Physical Surrogate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02994">http://arxiv.org/abs/2310.02994</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PolymathicAI/multiple_physics_pretraining">https://github.com/PolymathicAI/multiple_physics_pretraining</a></li>
<li>paper_authors: Michael McCabe, Bruno Régaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, Mariel Pettee, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho</li>
<li>for: 这 paper 是为了开发一种physics pretraining（MPP），用于模拟多种不同物理系统的动力学行为。</li>
<li>methods: MPP 使用了一种共享嵌入和 нор化策略，将多个系统的场景映射到单一的嵌入空间中，以便学习通用于多种物理任务的特征。</li>
<li>results:  authors  Validated  MPP 的效果在预训练和下游任务中，并显示了一个 MPP 预训练的 transformer 能够与任务特定的基eline 匹配或超越，无需进行微调。 在下游任务中，finetuning MPP 预训练的模型可以在新的物理上提供更准确的预测，比如在多个时间步上。 authors 还开源了他们的代码和模型权重，以便重现和社区实验。<details>
<summary>Abstract</summary>
We introduce multiple physics pretraining (MPP), an autoregressive task-agnostic pretraining approach for physical surrogate modeling. MPP involves training large surrogate models to predict the dynamics of multiple heterogeneous physical systems simultaneously by learning features that are broadly useful across diverse physical tasks. In order to learn effectively in this setting, we introduce a shared embedding and normalization strategy that projects the fields of multiple systems into a single shared embedding space. We validate the efficacy of our approach on both pretraining and downstream tasks over a broad fluid mechanics-oriented benchmark. We show that a single MPP-pretrained transformer is able to match or outperform task-specific baselines on all pretraining sub-tasks without the need for finetuning. For downstream tasks, we demonstrate that finetuning MPP-trained models results in more accurate predictions across multiple time-steps on new physics compared to training from scratch or finetuning pretrained video foundation models. We open-source our code and model weights trained at multiple scales for reproducibility and community experimentation.
</details>
<details>
<summary>摘要</summary>
我们介绍了多物理预训练（MPP），一种无关任务的泛化预训练方法，用于物理替身模型。MPP通过训练大型替身模型，以预测多种不同物理系统的动态，同时学习普遍适用于多种物理任务的特征。为了有效地学习在这种设定下，我们引入共享嵌入和 норализовimplified Chinese的策略，将多种系统的场景 проек到单一的共享嵌入空间中。我们验证了我们的方法在预训练和下游任务上的有效性，并在广泛的流体力学指标上进行了评估。我们发现，一个MPP预训练的变换器可以与任务特定基线模型匹配或超越，无需进行微调。在下游任务中，我们证明了对MPP预训练模型进行微调，可以在新的物理任务上提供更准确的预测，比起从零开始训练或微调预先训练的视频基础模型。我们开源了我们的代码和模型权重，以便重现和社区实验。
</details></li>
</ul>
<hr>
<h2 id="xVal-A-Continuous-Number-Encoding-for-Large-Language-Models"><a href="#xVal-A-Continuous-Number-Encoding-for-Large-Language-Models" class="headerlink" title="xVal: A Continuous Number Encoding for Large Language Models"></a>xVal: A Continuous Number Encoding for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02989">http://arxiv.org/abs/2310.02989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PolymathicAI/xVal">https://github.com/PolymathicAI/xVal</a></li>
<li>paper_authors: Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, Geraud Krawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, Bruno Régaldo-Saint Blancard, Tiberiu Tesileanu, Kyunghyun Cho, Shirley Ho</li>
<li>for: 这篇论文是为了解决科学数据分析中数字化的问题而写的。</li>
<li>methods: 这篇论文提出了一种名为xVal的数字编码方案，该方案可以将任何实数表示为唯一的字符串。</li>
<li>results: 作者通过对一些 sintetic 和实际世界数据进行实验，发现 xVal 比现有的数字编码方案更加Token效率，并且在总体上具有更好的泛化性。<details>
<summary>Abstract</summary>
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
</details>
<details>
<summary>摘要</summary>
大型语言模型尚未广泛适用于科学数据分析，部分原因是处理数字的特殊问题。我们提出xVal，一种数字编码方案，将任何实数用单一的字元表示。xVal使用对应数值的专门映射vector进行缩放，将输入字串中的数字转换为输出字串中的数字。这种方法使得模型成为一个端到端连续的映射，从输入字串中的数字到输出字串中的数字。这导致一个更适合科学领域的预设偏好。我们在一些 sintetic 和实际世界数据上进行了实验评估，与现有的数字编码方案相比，我们发现xVal更加 Token-efficient 且具有改善的通用化能力。
</details></li>
</ul>
<hr>
<h2 id="Probing-Intersectional-Biases-in-Vision-Language-Models-with-Counterfactual-Examples"><a href="#Probing-Intersectional-Biases-in-Vision-Language-Models-with-Counterfactual-Examples" class="headerlink" title="Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples"></a>Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02988">http://arxiv.org/abs/2310.02988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Vasudev Lal</li>
<li>for: 本研究旨在探讨当代视力语言模型（VLM）具有社会属性偏见的问题。</li>
<li>methods: 我们使用文本到图像扩散模型生成大规模的对比例ional社会偏见的实验数据，以探讨当前VLM中的交叉社会属性偏见。</li>
<li>results: 我们的实验结果表明，当前VLM中存在交叉社会属性偏见，并且这种偏见在不同的社会属性交叉点上存在差异。<details>
<summary>Abstract</summary>
While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). We conduct extensive experiments using our generated dataset which reveal the intersectional social biases present in state-of-the-art VLMs.
</details>
<details>
<summary>摘要</summary>
“Recently, vision-language models（VLMs）have achieved remarkable performance improvements, but there is growing evidence that these models also possess harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually, while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender). We conduct extensive experiments using our generated dataset, which reveal the intersectional social biases present in state-of-the-art VLMs.”Here's the breakdown of the translation:* “vision-language models”(VLMs) becomes “视力语言模型”(VLM)* “recently” becomes “近期”* “achieved” becomes “取得”* “remarkable performance improvements” becomes “显著性能提升”* “growing evidence” becomes “增长的证据”* “social attributes” becomes “社会属性”* “such as gender and race” becomes “如gender和race”* “Prior studies have primarily focused on probing such bias attributes individually” becomes “先前的研究主要集中在单独探索这些偏见属性上”* “while ignoring biases associated with intersections between social attributes” becomes “而忽略了社会属性之间的交叉偏见”* “This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets” becomes “这可能是因为现有数据集中存在社会属性的多种组合的图像文本对的采集困难”* “To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale” becomes “为了解决这个挑战，我们使用文本到图像扩散模型生成大规模的对例，以探索交叉社会偏见”* “Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race & gender)” becomes “我们的方法使用稳定扩散，并使用对比注意控制生成一系列高度相似的图像文本对，其中图像和文本之间的差异仅仅在于交叉社会属性（如种族和性别）上”* “We conduct extensive experiments using our generated dataset, which reveal the intersectional social biases present in state-of-the-art VLMs” becomes “我们使用生成的数据集进行广泛的实验，发现现有最先进的VLM模型中存在交叉社会偏见”
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Impact-of-Disrupted-Peer-to-Peer-Communications-on-Fully-Decentralized-Learning-in-Disaster-Scenarios"><a href="#Exploring-the-Impact-of-Disrupted-Peer-to-Peer-Communications-on-Fully-Decentralized-Learning-in-Disaster-Scenarios" class="headerlink" title="Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios"></a>Exploring the Impact of Disrupted Peer-to-Peer Communications on Fully Decentralized Learning in Disaster Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02986">http://arxiv.org/abs/2310.02986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luigi Palmieri, Chiara Boldrini, Lorenzo Valerio, Andrea Passarella, Marco Conti</li>
<li>for: This paper explores the resilience of decentralized learning in a disaster setting, specifically how the process is affected by abrupt changes in peer-to-peer communications.</li>
<li>methods: The paper uses a Barabasi-Albert graph topology and investigates the effects of losing devices with data versus those that only contribute to the graph connectivity.</li>
<li>results: The study finds that a loss of connectivity has a greater impact on the accuracy of the learning process than a loss of data, but the network remains relatively robust and the process can achieve a good level of accuracy.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文研究了在灾难场景下分布式学习的可恢复性，具体来说是如何在各个用户设备或节点之间分布学习资源和决策能力时，在不中心化的情况下，维持学习过程的稳定性和可靠性。</li>
<li>methods: 这篇论文使用了Barabasi-Albert图 topology，并对各个设备的数据分布情况进行了分析，以了解在各种灾难场景下分布式学习的影响。</li>
<li>results: 研究发现，在分布式学习过程中，对连接性的损害比对数据的损害更大程度地影响学习的准确性，但是网络仍然保持了相对的稳定性，并且学习过程可以达到一定的准确性。<details>
<summary>Abstract</summary>
Fully decentralized learning enables the distribution of learning resources and decision-making capabilities across multiple user devices or nodes, and is rapidly gaining popularity due to its privacy-preserving and decentralized nature. Importantly, this crowdsourcing of the learning process allows the system to continue functioning even if some nodes are affected or disconnected. In a disaster scenario, communication infrastructure and centralized systems may be disrupted or completely unavailable, hindering the possibility of carrying out standard centralized learning tasks in these settings. Thus, fully decentralized learning can help in this case. However, transitioning from centralized to peer-to-peer communications introduces a dependency between the learning process and the topology of the communication graph among nodes. In a disaster scenario, even peer-to-peer communications are susceptible to abrupt changes, such as devices running out of battery or getting disconnected from others due to their position. In this study, we investigate the effects of various disruptions to peer-to-peer communications on decentralized learning in a disaster setting. We examine the resilience of a decentralized learning process when a subset of devices drop from the process abruptly. To this end, we analyze the difference between losing devices holding data, i.e., potential knowledge, vs. devices contributing only to the graph connectivity, i.e., with no data. Our findings on a Barabasi-Albert graph topology, where training data is distributed across nodes in an IID fashion, indicate that the accuracy of the learning process is more affected by a loss of connectivity than by a loss of data. Nevertheless, the network remains relatively robust, and the learning process can achieve a good level of accuracy.
</details>
<details>
<summary>摘要</summary>
全面协同学习可以将学习资源和决策能力分布在多个用户设备或节点之间，并在快速增长的privacy保持和分布式特性下得到广泛应用。重要的是，这种拥有多个节点协同学习的系统可以继续正常工作， même si一些节点被影响或断开。在灾难enario中，通信基础设施和中央系统可能会受损或完全无法使用，这会限制标准中央式学习任务的执行。因此，全面协同学习可以帮助。然而，从中央式通信转换到点对点通信会导致学习过程与节点之间的通信图模式相互依赖。在灾难enario中，甚至点对点通信也可能受到不可预期的变化，如设备电池耗尽或与其他设备的连接被中断。在这项研究中，我们研究了在灾难设置下点对点通信受到各种中断的影响。我们分析了因设备突然退出学习过程而导致的减少精度的影响。我们发现，在Barabasi-Albert图 topology上，分布式学习过程中训练数据的分布和连接图的结构强相关。尽管连接图中的设备损失会导致精度下降，但是网络仍然保持相对稳定，并且学习过程可以达到较高的精度。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-for-Associative-Memories"><a href="#Scaling-Laws-for-Associative-Memories" class="headerlink" title="Scaling Laws for Associative Memories"></a>Scaling Laws for Associative Memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02984">http://arxiv.org/abs/2310.02984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivien Cabannes, Elvis Dohmatob, Alberto Bietti</li>
<li>for: 这篇论文的目的是研究 associative memory 机制。</li>
<li>methods: 该模型基于高维矩阵，其中每个矩阵由外积 embedding 相关的层次结构组成，与 transformer 语言模型的内层相似。</li>
<li>results: 文章提供了对样本大小和参数大小的精确推算法，以及不同优化算法的统计效率。并通过大量的数值实验 validate 和解释理论结果，包括细致的视觉化存储记忆协同关系。<details>
<summary>Abstract</summary>
Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
</details>
<details>
<summary>摘要</summary>
学习可能涉及到发现和记忆抽象规则。本文的目标是研究相关记忆机制。我们的模型基于高维矩阵，其中每个矩阵由嵌入的外积生成，与变换语言模型的内层相关。我们得出了准确的缩放法律，并讨论不同估计器的统计效率。我们提供了广泛的数值实验来验证和解释理论结果，包括细化的视觉化存储相关性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Fully-Adaptive-Regret-Minimization-in-Heavy-Tailed-Bandits"><a href="#Towards-Fully-Adaptive-Regret-Minimization-in-Heavy-Tailed-Bandits" class="headerlink" title="Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits"></a>Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02975">http://arxiv.org/abs/2310.02975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianmarco Genalti, Lupo Marsigli, Nicola Gatti, Alberto Maria Metelli</li>
<li>for: 这篇论文研究了在含有重尾分布的随机带射问题中的学习算法，特别是在分布参数未知的情况下。</li>
<li>methods: 这篇论文使用了适应算法，并提供了一种名为适应稳健UCB的追踪策略，以最小化缺失的 regret。</li>
<li>results: 研究发现，适应策略会带来更高的缺失 regret，比标准设定更高。然而，通过设定一种特定的分布假设，可以实现 Adaptive Robust UCB 算法，并达到known lower bound for the heavy-tailed MAB problem。<details>
<summary>Abstract</summary>
Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional assumption and provide Adaptive Robust UCB, a regret minimization strategy matching the known lower bound for the heavy-tailed MAB problem.
</details>
<details>
<summary>摘要</summary>
重 tailed 分布自然出现在多个设置中，从金融到电信。而在 sub-Gaussian 或受 bounds 支持奖励下的 regret 最小化已经广泛研究，而学习 heavy-tailed 分布只是过去十年才开始受到关注。在随机重 tailed 策略中，一个代理人学习假设 distribution 有最大顺序数 $1+\epsilon$ 是Finite 且固定的 $u$，其中 $\epsilon \in (0,1]$。根据我们所知，文献中只提供了两个输入 quantity 的算法。在这篇论文中，我们研究随机自适应重 tailed 策略，这是标准设置的变种，在代理人手中不知道 $\epsilon$ 和 $u$。我们证明了可适性会带来成本，引入了两个下界，表明适应策略的 regret 比标准设置更高。最后，我们提出了特定的分布假设，并提供了 Adaptive Robust UCB，一种 regret 最小化策略，与 known 下界匹配 heavy-tailed MAB 问题。
</details></li>
</ul>
<hr>
<h2 id="Point-PEFT-Parameter-Efficient-Fine-Tuning-for-3D-Pre-trained-Models"><a href="#Point-PEFT-Parameter-Efficient-Fine-Tuning-for-3D-Pre-trained-Models" class="headerlink" title="Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models"></a>Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03059">http://arxiv.org/abs/2310.03059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EvenJoker/Point-PEFT">https://github.com/EvenJoker/Point-PEFT</a></li>
<li>paper_authors: Ivan Tang, Eric Zhang, Ray Gu</li>
<li>for: 这篇论文的目的是为了提出一个 Parameter-Efficient Fine-Tuning (PEFT) 方法来适应点云范例下的下游任务，以减少适应成本。</li>
<li>methods: 这篇论文使用了一个名为 Point-PEFT 的框架，它将具有最小化可读的参数的方法与点云范例模型结合起来。具体来说，这篇论文将大多数点云范例模型的参数冻结不动，仅将新增加的 PEFT 模组进行调整。这两个模组包括一个 Point-prior Prompt 和一个 Geometry-aware Adapter。</li>
<li>results: 实验结果显示，这篇论文的 Point-PEFT 方法可以在不同的下游任务上实现更好的性能，使用的参数只有 5%，证明了这篇论文的方法是有效和有效的。<details>
<summary>Abstract</summary>
The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggregate point cloud features within spatial neighborhoods to capture fine-grained geometric information through local interactions. Extensive experiments indicate that our Point-PEFT can achieve better performance than the full fine-tuning on various downstream tasks, while using only 5% of the trainable parameters, demonstrating the efficiency and effectiveness of our approach. Code will be released at https://github.com/EvenJoker/Point-PEFT.
</details>
<details>
<summary>摘要</summary>
各种预训练大型模型的流行化对下游任务中的多种领域产生了革命性的变革，如语言、视觉和多模态等。为了减少下游任务中的适应成本，许多Parameter-Efficient Fine-Tuning（PEFT）技术被提出，但特殊的3D预训练模型PEFT方法还处于未explored阶段。为此，我们介绍了Point-PEFT，一种适用于预训练3D模型的新型框架，以最小化可学习参数。具体来说，对于一个预训练的3D模型，我们冻结大多数参数，并仅在下游任务上调整新增的PEFT模块。这些PEFT模块包括Point-priorPrompt和Geometry-aware Adapter。Point-priorPrompt采用一组学习的提示符，我们提议使用域特定知识构建一个记忆银行，并使用无参数注意力增强提示符。Geometry-aware Adapter通过地理位置的相互互动来捕捉细致的几何信息。我们的Point-PEFT在多种下游任务上表现较好于全 Fine-tuning，使用的可学习参数仅占总参数的5%，证明了我们的方法的效率和效果。代码将在https://github.com/EvenJoker/Point-PEFT上发布。
</details></li>
</ul>
<hr>
<h2 id="Credit-card-score-prediction-using-machine-learning-models-A-new-dataset"><a href="#Credit-card-score-prediction-using-machine-learning-models-A-new-dataset" class="headerlink" title="Credit card score prediction using machine learning models: A new dataset"></a>Credit card score prediction using machine learning models: A new dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02956">http://arxiv.org/abs/2310.02956</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Anas Arram, Masri Ayob, Musatafa Abbas Abbood Albadr, Alaa Sulaiman, Dheeb Albashish</li>
<li>for: 预测信用卡Default风险</li>
<li>methods: 使用机器学习模型进行信用卡Default预测</li>
<li>results: MLP模型在预测Default客户和评估风险中表现出色，AUC为86.7%，准确率为91.6%，记忆率超过80%。<details>
<summary>Abstract</summary>
The use of credit cards has recently increased, creating an essential need for credit card assessment methods to minimize potential risks. This study investigates the utilization of machine learning (ML) models for credit card default prediction system. The main goal here is to investigate the best-performing ML model for new proposed credit card scoring dataset. This new dataset includes credit card transaction histories and customer profiles, is proposed and tested using a variety of machine learning algorithms, including logistic regression, decision trees, random forests, multi-layer perceptron (MLP) neural network, XGBoost, and LightGBM. To prepare the data for machine learning models, we perform data pre-processing, feature extraction, feature selection, and data balancing techniques. Experimental results demonstrate that MLP outperforms logistic regression, decision trees, random forests, LightGBM, and XGBoost in terms of predictive performance in true positive rate, achieving an impressive area under the curve (AUC) of 86.7% and an accuracy rate of 91.6%, with a recall rate exceeding 80%. These results indicate the superiority of MLP in predicting the default customers and assessing the potential risks. Furthermore, they help banks and other financial institutions in predicting loan defaults at an earlier stage.
</details>
<details>
<summary>摘要</summary>
受信用卡使用的增加，对可能的风险的评估方法的需求而言，已成为一项非常重要的任务。本研究探讨了使用机器学习（ML）模型来预测信用卡 defaults 系统中的最佳表现。我们使用一个新的信用卡分配数据集，该数据集包括信用卡交易历史和客户资料，并使用多种机器学习算法进行测试，包括逻辑回归、决策树、随机森林、多层感知网络（MLP）、XGBoost 和 LightGBM。为了准备数据 для机器学习模型，我们进行了数据预处理、特征提取、特征选择和数据平衡技术。实验结果表明，MLP 在 true positive rate 方面表现出色，其 AUC 为 86.7%，准确率为 91.6%， recall 率超过 80%。这些结果表明 MLP 在预测默认客户和评估风险中的优势。此外，它们可以帮助银行和其他金融机构在更早的阶段预测借款 defaults。
</details></li>
</ul>
<hr>
<h2 id="Shadow-Alignment-The-Ease-of-Subverting-Safely-Aligned-Language-Models"><a href="#Shadow-Alignment-The-Ease-of-Subverting-Safely-Aligned-Language-Models" class="headerlink" title="Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models"></a>Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02949">http://arxiv.org/abs/2310.02949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BeyonderXX/ShadowAlignment">https://github.com/BeyonderXX/ShadowAlignment</a></li>
<li>paper_authors: Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin</li>
<li>for: 这 paper 是为了探讨如何保障 AI 安全，具体来说是通过对大语言模型（LLM）进行安全对齐来防止 malicious use。</li>
<li>methods: 这 paper 使用了一种新的攻击方法，即阴影对齐（Shadow Alignment），它利用了一小量的数据来让安全对齐的模型适应危险任务，而不会 sacrifice 模型的帮助性。</li>
<li>results: 实验结果显示，使用阴影对齐攻击可以轻松地将安全对齐的模型转移到危险任务上，而且这些模型仍然能够正确地回答常见问题。<details>
<summary>Abstract</summary>
Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5 different organizations (LLaMa-2, Falcon, InternLM, BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack. Besides, the single-turn English-only attack successfully transfers to multi-turn dialogue and other languages. This study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers.
</details>
<details>
<summary>摘要</summary>
警告：本文包含有害语言的示例，读者应自行谨慎阅读。随着大语言模型（LLM）的开源释出，许多下游应用的开发成本减少了，以降低数据注释和计算成本。为保障人工智能安全，已经进行了广泛的安全对齐措施，以防止恶意使用。然而，这些安全对齐后的模型可能仍然受到威胁。我们提出了一种新的攻击方法，称为阴影对齐（Shadow Alignment），它可以使用一小amount of data来让安全对齐的模型适应危险任务，而不会 sacrificing model helpfulness。这些附加的模型仍然能够正确地回答常见问题。我们的实验表明，这种攻击方法可以在8种由5家不同组织发布的模型（LLaMa-2、Falcon、InternLM、BaiChuan2、Vicuna）中实现。此外，单turn英文Only攻击也可以成功转移到多turn对话和其他语言。这项研究 serve as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers。
</details></li>
</ul>
<hr>
<h2 id="Local-Max-Entropy-and-Free-Energy-Principles-Belief-Diffusions-and-their-Singularities"><a href="#Local-Max-Entropy-and-Free-Energy-Principles-Belief-Diffusions-and-their-Singularities" class="headerlink" title="Local Max-Entropy and Free Energy Principles, Belief Diffusions and their Singularities"></a>Local Max-Entropy and Free Energy Principles, Belief Diffusions and their Singularities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02946">http://arxiv.org/abs/2310.02946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opeltre/topos">https://github.com/opeltre/topos</a></li>
<li>paper_authors: Olivier Peltre</li>
<li>for: 这篇论文探讨了三种贝特-基钱拟合原则，包括它们与信仰卷积算法在树Graph上的关系。</li>
<li>methods: 这篇论文使用了一种通用的描述方法，总结了贝特-基钱拟合原则的结构，并将其推广到定时Diffusion中。</li>
<li>results: 论文显示了贝特-基钱拟合原则的稳定点和站点信仰的形态，以及这些稳定点与信仰卷积算法的关系。此外，论文还描述了偏好信仰的表达方式，以及这些表达方式在树Graph上的形态。<details>
<summary>Abstract</summary>
A comprehensive picture of three Bethe-Kikuchi variational principles including their relationship to belief propagation (BP) algorithms on hypergraphs is given. The structure of BP equations is generalized to define continuous-time diffusions, solving localized versions of the max-entropy principle (A), the variational free energy principle (B), and a less usual equilibrium free energy principle (C), Legendre dual to A. Both critical points of Bethe-Kikuchi functionals and stationary beliefs are shown to lie at the non-linear intersection of two constraint surfaces, enforcing energy conservation and marginal consistency respectively. The hypersurface of singular beliefs, accross which equilibria become unstable as the constraint surfaces meet tangentially, is described by polynomial equations in the convex polytope of consistent beliefs. This polynomial is expressed by a loop series expansion for graphs of binary variables.
</details>
<details>
<summary>摘要</summary>
提供了三种Bethe-Kikuchi变量原理的完整图像，包括它们与信念传播（BP）算法在图上的关系。BP方程的结构被总结为定义连续时间扩散，解决本地版本的最大 entropy原理（A）、变量自由能原理（B）和一种 menos usual的平衡自由能原理（C）的Localized version。两个Bethe-Kikuchi函数的极点和站点信念都显示在两个约束表面的非线性交叉点上，其中一个表面保证能量征Compatibility，另一个表面保证边缘Consistency。在相互约束的交叉点上，equilibria变得不稳定，singular beliefs的抽象表面由多项式方程描述在具有consistent beliefs的几何体上。这个多项式可以用 Loop series expansion表示，其中变量是二进制变量的图。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Large-Language-Models-on-Climate-Information"><a href="#Assessing-Large-Language-Models-on-Climate-Information" class="headerlink" title="Assessing Large Language Models on Climate Information"></a>Assessing Large Language Models on Climate Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02932">http://arxiv.org/abs/2310.02932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannis Bulian, Mike S. Schäfer, Afra Amini, Heidi Lam, Massimiliano Ciaramita, Ben Gaiarin, Michelle Chen Huebscher, Christian Buck, Niels Mede, Markus Leippold, Nadine Strauss</li>
<li>for: 这个研究旨在评估大语言模型（LLM）在气候变化话题上的表现，以便更好地理解它们在气候通信领域的能力。</li>
<li>methods: 该研究使用了基于科学沟通原则的全面评估框架，以分析 LLM 对气候变化话题的回答。该框架包括8个维度，可以识别出模型输出中的30个问题。</li>
<li>results: 研究人员通过评估多个最新的 LLM 和对其结果进行了全面分析，从而揭示了 LLM 在气候通信领域的潜在和局限性。<details>
<summary>Abstract</summary>
Understanding how climate change affects us and learning about available solutions are key steps toward empowering individuals and communities to mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity, it is necessary to assess their capability in this domain. In this study, we present a comprehensive evaluation framework, grounded in science communication principles, to analyze LLM responses to climate change topics. Our framework emphasizes both the presentational and epistemological adequacy of answers, offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our framework discerns up to 30 distinct issues in model outputs. The task is a real-world example of a growing number of challenging problems where AI can complement and lift human performance. We introduce a novel and practical protocol for scalable oversight that uses AI Assistance and relies on raters with relevant educational backgrounds. We evaluate several recent LLMs and conduct a comprehensive analysis of the results, shedding light on both the potential and the limitations of LLMs in the realm of climate communication.
</details>
<details>
<summary>摘要</summary>
理解气候变化如何影响我们以及了解可用解决方案是关键步骤，以便让个人和社区能够避免和适应它。随着大型自然语言模型（LLM）的崛起，需要评估它们在这个领域的能力。本研究提出了一个完整的评估框架，基于科学沟通原则，用于分析LLM对气候变化话题的回答。我们的框架强调回答的现场和知识上的适用程度，从而提供细致的分析LLM生成的Output。涵盖8个维度，我们的框架可以识别出多达30个问题。这是一个真实存在的人工智能可以补充和提高人类表现的例子。我们提出了一种新的实用协助协议，使用人工智能协助和有相关教育背景的评审人员，以实现可扩展的监督。我们对一些最新的LLM进行了评估，并对结果进行了全面的分析，从而揭示LLM在气候沟通领域的潜力和局限性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Aided-Warmstart-of-Model-Predictive-Control-in-Uncertain-Fast-Changing-Traffic"><a href="#Learning-Aided-Warmstart-of-Model-Predictive-Control-in-Uncertain-Fast-Changing-Traffic" class="headerlink" title="Learning-Aided Warmstart of Model Predictive Control in Uncertain Fast-Changing Traffic"></a>Learning-Aided Warmstart of Model Predictive Control in Uncertain Fast-Changing Traffic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02918">http://arxiv.org/abs/2310.02918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed-Khalil Bouzidi, Yue Yao, Daniel Goehring, Joerg Reichardt</li>
<li>for: 提高Model Predictive Control（MPC）在非凸问题中逃脱本地最优点的能力，以及在快速变化和不确定环境中提供更好的初始猜测。</li>
<li>methods: 使用神经网络基于多模态预测器生成多个轨迹提案，然后使用采样法进行精细调整。</li>
<li>results: 通过 Monte Carlo  simulations  validate our approach, 并显示其能够提供更多的本地最优点和更好的初始猜测。<details>
<summary>Abstract</summary>
Model Predictive Control lacks the ability to escape local minima in nonconvex problems. Furthermore, in fast-changing, uncertain environments, the conventional warmstart, using the optimal trajectory from the last timestep, often falls short of providing an adequately close initial guess for the current optimal trajectory. This can potentially result in convergence failures and safety issues. Therefore, this paper proposes a framework for learning-aided warmstarts of Model Predictive Control algorithms. Our method leverages a neural network based multimodal predictor to generate multiple trajectory proposals for the autonomous vehicle, which are further refined by a sampling-based technique. This combined approach enables us to identify multiple distinct local minima and provide an improved initial guess. We validate our approach with Monte Carlo simulations of traffic scenarios.
</details>
<details>
<summary>摘要</summary>
模型预测控制缺乏能够脱离非对称问题的本地最小点能力。此外，在快速变化、不确定环境中，传统的温始方法，使用上一步优质轨迹作为当前优质轨迹的初始猜测，经常无法提供足够近的初始猜测，从而可能导致偏移失败和安全问题。因此，这篇论文提出了基于神经网络的学习帮助的 Model Predictive Control 算法框架。我们的方法利用神经网络基于多模态预测器生成多个轨迹建议，然后使用抽象 sampling 技术进一步细化。这种结合方法使得我们能够识别多个不同的本地最小点，并提供改进的初始猜测。我们通过 Monte Carlo 仿真交通场景验证了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Dermatoscopic-Lesion-Segmentation-via-Diffusion-Models-with-Visual-and-Textual-Prompts"><a href="#Boosting-Dermatoscopic-Lesion-Segmentation-via-Diffusion-Models-with-Visual-and-Textual-Prompts" class="headerlink" title="Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts"></a>Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02906">http://arxiv.org/abs/2310.02906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyi Du, Xiaosong Wang, Yongyi Lu, Yuyin Zhou, Shaoting Zhang, Alan Yuille, Kang Li, Zongwei Zhou</li>
<li>for: 该论文旨在提出一种基于扩散模型的数据生成方法，以提高皮肤病变诊断的准确率。</li>
<li>methods: 该方法使用了最新的扩散模型，并添加了病变特异的视觉和文本提示来控制生成的皮肤图像。</li>
<li>results: 对比于传统生成模型，该方法可以提高皮肤病变诊断的准确率，并且可以生成高质量的皮肤图像。实验结果显示，该方法可以提高SIMIT图像质量指标9%，并提高皮肤分割性能过5%。<details>
<summary>Abstract</summary>
Image synthesis approaches, e.g., generative adversarial networks, have been popular as a form of data augmentation in medical image analysis tasks. It is primarily beneficial to overcome the shortage of publicly accessible data and associated quality annotations. However, the current techniques often lack control over the detailed contents in generated images, e.g., the type of disease patterns, the location of lesions, and attributes of the diagnosis. In this work, we adapt the latest advance in the generative model, i.e., the diffusion model, with the added control flow using lesion-specific visual and textual prompts for generating dermatoscopic images. We further demonstrate the advantage of our diffusion model-based framework over the classical generation models in both the image quality and boosting the segmentation performance on skin lesions. It can achieve a 9% increase in the SSIM image quality measure and an over 5% increase in Dice coefficients over the prior arts.
</details>
<details>
<summary>摘要</summary>
医学图像合成方法，如生成对抗网络，已成为医学图像分析任务中常用的数据增强方法。它的主要优点是解决公共数据和相关质量注释的缺乏。然而，现有技术通常无法控制生成图像的详细内容，如疾病模式、肿瘤的位置和诊断特征。在这种情况下，我们采用最新的生成模型，即扩散模型，并在生成图像时使用病诊特定的视觉和文本提示。我们进一步示出了我们的扩散模型基于框架在图像质量和诊断性能方面的优势，可以实现9%的SSIM图像质量指标和5%以上的Dice系数提高。
</details></li>
</ul>
<hr>
<h2 id="Searching-for-High-Value-Molecules-Using-Reinforcement-Learning-and-Transformers"><a href="#Searching-for-High-Value-Molecules-Using-Reinforcement-Learning-and-Transformers" class="headerlink" title="Searching for High-Value Molecules Using Reinforcement Learning and Transformers"></a>Searching for High-Value Molecules Using Reinforcement Learning and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02902">http://arxiv.org/abs/2310.02902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raj Ghugare, Santiago Miret, Adriana Hugessen, Mariano Phielipp, Glen Berseth</li>
<li>for: 这篇论文是关于使用强化学习（RL）来设计文本表示的分子结构的研究。</li>
<li>methods: 该论文使用了RL算法和文本 grammar来 Structuring the search space，并对不同的设计选择和训练策略进行了广泛的实验研究。</li>
<li>results: 经过EXTENSIVE实验，该论文提出了一种新的RL基于分子设计算法（ChemRLformer），并对25个分子设计任务进行了系统性的分析。结果表明，ChemRLformer可以达到现状之最的性能，而且比之前的工作更加直观，可以帮助解决文本基于分子设计的计算复杂问题。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.
</details>
<details>
<summary>摘要</summary>
利用文本表示法的强化学习（RL）可以有效地找到可以搜索图表的高值策略。然而，RL需要精心设计搜索空间和算法设计，以便在这个挑战中有效。通过广泛的实验，我们探索了不同的文本语法和训练算法的设计选择对RL策略的能力生成材料有效性的影响。我们开发了一种新的RL基于分子设计算法（ChemRLformer），并进行了余程分析，使用25个分子设计任务，包括计算复杂的蛋白质嵌入 simulations。从这种分析中，我们发现了这个问题空间中的独特意见，并证明了ChemRLformer在文本基本分子设计中 achieved state-of-the-art表现，而且比之前的工作更加直观，推翻了哪些设计选择对文本基本分子设计是有用的。
</details></li>
</ul>
<hr>
<h2 id="Notes-on-a-Path-to-AI-Assistance-in-Mathematical-Reasoning"><a href="#Notes-on-a-Path-to-AI-Assistance-in-Mathematical-Reasoning" class="headerlink" title="Notes on a Path to AI Assistance in Mathematical Reasoning"></a>Notes on a Path to AI Assistance in Mathematical Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02896">http://arxiv.org/abs/2310.02896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Kontorovich</li>
<li>for: 这篇论文的目的是为研究数学家提供AI助手。</li>
<li>methods: 这篇论文使用了AI技术来帮助数学家进行数学推理。</li>
<li>results: 论文获得了一些有用的结果，可以帮助研究数学家更好地完成他们的工作。In English, these notes would be:</li>
<li>for: The purpose of this paper is to provide AI assistance to research mathematicians.</li>
<li>methods: The paper uses AI technology to help mathematicians perform mathematical reasoning.</li>
<li>results: The paper obtains some useful results that can help research mathematicians complete their work more effectively.<details>
<summary>Abstract</summary>
These informal notes are based on the author's lecture at the National Academies of Science, Engineering, and Mathematics workshop on "AI to Assist Mathematical Reasoning" in June 2023. The goal is to think through a path by which we might arrive at AI that is useful for the research mathematician.
</details>
<details>
<summary>摘要</summary>
这些 informal 笔记是基于作者在美国国家科学、工程和数学学会工作坊上的讲座，主题是“AI 助力数学推理”，发生在2023年6月。目标是思考一种路径，以到达有用于研究数学家的 AI。Note: "National Academies of Science, Engineering, and Mathematics" is translated as "美国国家科学、工程和数学学会" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Recent-Methodological-Advances-in-Federated-Learning-for-Healthcare"><a href="#Recent-Methodological-Advances-in-Federated-Learning-for-Healthcare" class="headerlink" title="Recent Methodological Advances in Federated Learning for Healthcare"></a>Recent Methodological Advances in Federated Learning for Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02874">http://arxiv.org/abs/2310.02874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang, Daniel Kreuter, Yichen Chen, Sören Dittmer, Samuel Tull, Tolou Shadbahr, BloodCounts! Collaboration, Jacobus Preller, James H. F. Rudd, John A. D. Aston, Carola-Bibiane Schönlieb, Nicholas Gleadall, Michael Roberts</li>
<li>for: 这些论文主要用于描述如何使用联邦学习方法解决医疗数据的多种挑战，如数据隔离、数据不均衡、缺失数据、分布Shift和非标准化变量。</li>
<li>methods: 这些论文使用的方法包括分布式优化、节点之间的通信、模型聚合和模型重新分布等。</li>
<li>results: 文献评审发现了许多论文中的系统性问题，这些问题会影响论文中的方法质量。文献还提出了具体的建议，以改善联邦学习方法在医疗领域的发展质量。<details>
<summary>Abstract</summary>
For healthcare datasets, it is often not possible to combine data samples from multiple sites due to ethical, privacy or logistical concerns. Federated learning allows for the utilisation of powerful machine learning algorithms without requiring the pooling of data. Healthcare data has many simultaneous challenges which require new methodologies to address, such as highly-siloed data, class imbalance, missing data, distribution shifts and non-standardised variables. Federated learning adds significant methodological complexity to conventional centralised machine learning, requiring distributed optimisation, communication between nodes, aggregation of models and redistribution of models. In this systematic review, we consider all papers on Scopus that were published between January 2015 and February 2023 and which describe new federated learning methodologies for addressing challenges with healthcare data. We performed a detailed review of the 89 papers which fulfilled these criteria. Significant systemic issues were identified throughout the literature which compromise the methodologies in many of the papers reviewed. We give detailed recommendations to help improve the quality of the methodology development for federated learning in healthcare.
</details>
<details>
<summary>摘要</summary>
For healthcare datasets, it is often not possible to combine data samples from multiple sites due to ethical, privacy, or logistical concerns. Federated learning allows for the utilization of powerful machine learning algorithms without requiring the pooling of data. Healthcare data has many simultaneous challenges that require new methodologies to address, such as highly-siloed data, class imbalance, missing data, distribution shifts, and non-standardized variables. Federated learning adds significant methodological complexity to conventional centralized machine learning, requiring distributed optimization, communication between nodes, aggregation of models, and redistribution of models. In this systematic review, we consider all papers on Scopus that were published between January 2015 and February 2023 and which describe new federated learning methodologies for addressing challenges with healthcare data. We performed a detailed review of the 89 papers that fulfilled these criteria. Significant systemic issues were identified throughout the literature which compromise the methodologies in many of the papers reviewed. We give detailed recommendations to help improve the quality of the methodology development for federated learning in healthcare.
</details></li>
</ul>
<hr>
<h2 id="Stable-and-Interpretable-Deep-Learning-for-Tabular-Data-Introducing-InterpreTabNet-with-the-Novel-InterpreStability-Metric"><a href="#Stable-and-Interpretable-Deep-Learning-for-Tabular-Data-Introducing-InterpreTabNet-with-the-Novel-InterpreStability-Metric" class="headerlink" title="Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric"></a>Stable and Interpretable Deep Learning for Tabular Data: Introducing InterpreTabNet with the Novel InterpreStability Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02870">http://arxiv.org/abs/2310.02870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyun Wa, Xinai Lu, Minjuan Wang</li>
<li>for: 提高分类精度和解释性，应用于多个领域的AI模型设计</li>
<li>methods: 基于TabNet架构，改进了吸引模块以确保robust Gradient Propagation和计算稳定性</li>
<li>results: 在多种应用场景中，InterpreTabNet超过了其他领先解释模型，并提出了一个新的评价指标InterpreStability，可以帮助评估和比较未来模型的解释性。<details>
<summary>Abstract</summary>
As Artificial Intelligence (AI) integrates deeper into diverse sectors, the quest for powerful models has intensified. While significant strides have been made in boosting model capabilities and their applicability across domains, a glaring challenge persists: many of these state-of-the-art models remain as black boxes. This opacity not only complicates the explanation of model decisions to end-users but also obstructs insights into intermediate processes for model designers. To address these challenges, we introduce InterpreTabNet, a model designed to enhance both classification accuracy and interpretability by leveraging the TabNet architecture with an improved attentive module. This design ensures robust gradient propagation and computational stability. Additionally, we present a novel evaluation metric, InterpreStability, which quantifies the stability of a model's interpretability. The proposed model and metric mark a significant stride forward in explainable models' research, setting a standard for transparency and interpretability in AI model design and application across diverse sectors. InterpreTabNet surpasses other leading solutions in tabular data analysis across varied application scenarios, paving the way for further research into creating deep-learning models that are both highly accurate and inherently explainable. The introduction of the InterpreStability metric ensures that the interpretability of future models can be measured and compared in a consistent and rigorous manner. Collectively, these contributions have the potential to promote the design principles and development of next-generation interpretable AI models, widening the adoption of interpretable AI solutions in critical decision-making environments.
</details>
<details>
<summary>摘要</summary>
为了满足人工智能（AI）在多个领域的应用，强大模型的研究得到了推动。虽然在提高模型能力和适用范围方面做出了重要进展，但是一个主要挑战仍然存在：许多这些状态艺术模型仍然是黑盒模型。这种透明度不仅使得模型决策的解释对终端用户变得更加困难，还使得模型设计者不能够深入了解模型的中间过程。为解决这些挑战，我们提出了InterpreTabNet，一种基于TabNet架构的模型，通过改进的注意力模块来提高分类精度和可解释性。这种设计保证了正确的gradient传播和计算稳定性。此外，我们还提出了一个新的评价指标——InterpreStability，可以衡量模型的可解释性稳定性。提出的模型和指标标志着对可解释AI模型的研究的一个重要进步，为AI模型设计和应用在多个领域的扩展奠定了基础。InterpreTabNet在不同的应用场景中对 tabular 数据进行分类Task 表现出色，为未来的可解释AI模型的研究开辟了新的可能性。InterpreStability指标的引入确保了未来模型的可解释性可以在一致和严格的方式进行衡量和比较。总的来说，这些贡献有助于推动下一代可解释AI模型的设计原则和开发，扩大AI模型在重要决策环境中的应用。
</details></li>
</ul>
<hr>
<h2 id="A-novel-asymmetrical-autoencoder-with-a-sparsifying-discrete-cosine-Stockwell-transform-layer-for-gearbox-sensor-data-compression"><a href="#A-novel-asymmetrical-autoencoder-with-a-sparsifying-discrete-cosine-Stockwell-transform-layer-for-gearbox-sensor-data-compression" class="headerlink" title="A novel asymmetrical autoencoder with a sparsifying discrete cosine Stockwell transform layer for gearbox sensor data compression"></a>A novel asymmetrical autoencoder with a sparsifying discrete cosine Stockwell transform layer for gearbox sensor data compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02862">http://arxiv.org/abs/2310.02862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Zhu, Daoguang Yang, Hongyi Pan, Hamid Reza Karimi, Didem Ozevin, Ahmet Enis Cetin</li>
<li>for: 非接触式牙轮瑕疵诊断问题中的无线传输硬盘数据压缩问题受到了不足的高效压缩模型的挑战。</li>
<li>methods: 本文提出了一种Signal-adaptive asymmetrical autoencoder，其中引入了一个新的离散归一transform domain layer，并在这个域中实现了一个可学习的滤波器。此外，还应用了一个可学习的均值阈值层来使feature map sparse。相比 Linear layer，DCST层可以减少trainable参数的数量，并提高数据重建的准确性。</li>
<li>results: 对于University of Connecticut (UoC)和Southeast University (SEU)的牙轮数据集，提出的方法与其他autoencoder-based方法相比，平均质量分数提高2.00%最低和32.35%最高，仅需用限制数据集的训练样本。<details>
<summary>Abstract</summary>
The lack of an efficient compression model remains a challenge for the wireless transmission of gearbox data in non-contact gear fault diagnosis problems. In this paper, we present a signal-adaptive asymmetrical autoencoder with a transform domain layer to compress sensor signals. First, a new discrete cosine Stockwell transform (DCST) layer is introduced to replace linear layers in a multi-layer autoencoder. A trainable filter is implemented in the DCST domain by utilizing the multiplication property of the convolution. A trainable hard-thresholding layer is applied to reduce redundant data in the DCST layer to make the feature map sparse. In comparison to the linear layer, the DCST layer reduces the number of trainable parameters and improves the accuracy of data reconstruction. Second, training the autoencoder with a sparsifying DCST layer only requires a small number of datasets. The proposed method is superior to other autoencoder-based methods on the University of Connecticut (UoC) and Southeast University (SEU) gearbox datasets, as the average quality score is improved by 2.00% at the lowest and 32.35% at the highest with a limited number of training samples
</details>
<details>
<summary>摘要</summary>
缺乏高效压缩模型是无线传输变速盘数据的挑战，在这篇论文中，我们提出了一个适应信号的不同类型自适应器，包括一个对称自适应器和一个对称自适应器的变分层。首先，我们引入了一个新的简单cosine Stockwell变换（DCST）层，以取代多层自适应器中的线性层。这个层通过在DCST空间中实现可读性的范围内的对称滤波器，以便将数据映射到一个更短的特征地图。其次，我们将一个可调范围内的硬边阈层应用于DCST层，以将特征地图删除重复的数据，并使特征地图更加简洁。相比于线性层，DCST层可以减少训练参数的数量，并提高数据重建的精度。此外，我们发现使用DCST层进行训练只需要一小部分的数据，并且与其他基于自适应器的方法相比，我们的方法可以在UoC和SEU箱变数据上提高均值品质分数，从最低的2.00%到最高的32.35%。
</details></li>
</ul>
<hr>
<h2 id="Rayleigh-Quotient-Graph-Neural-Networks-for-Graph-level-Anomaly-Detection"><a href="#Rayleigh-Quotient-Graph-Neural-Networks-for-Graph-level-Anomaly-Detection" class="headerlink" title="Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection"></a>Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02861">http://arxiv.org/abs/2310.02861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangyu Dong, Xingyi Zhang, Sibo Wang</li>
<li>for: 本研究旨在提出一种新的spectral graph neural network(RQGNN)，用于图 уров异常检测。</li>
<li>methods: 我们提出了一种新的框架，包括两个组件：归一化矩阵学习(RQL)和Chebychev wavelet GNN with RQ-pooling(CWGNN-RQ)。RQL直接捕捉图的律RAYLEIGH积分，而CWGNN-RQ则通过spectral空间来探索图的异常性。</li>
<li>results: 我们在10个真实世界数据集上进行了广泛的实验，结果显示，RQGNN比最佳竞争对手提高了6.74%的macro-F1分数和1.44%的AUC值，这表明我们的框架有效。<details>
<summary>Abstract</summary>
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graphs. Specifically, we introduce a novel framework that consists of two components: the Rayleigh Quotient learning component (RQL) and Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly captures the Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the spectral space of graphs. Extensive experiments on 10 real-world datasets show that RQGNN outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in AUC, demonstrating the effectiveness of our framework.
</details>
<details>
<summary>摘要</summary>
graph уровня异常检测已经受到了广泛关注，因为它在各个领域，如癌症诊断和酶预测中发现了多种应用。然而，现有方法无法捕捉图异常的基本性质，导致框架设计不可解释和性能不满。在这篇论文中，我们往回一步，重新调查图异常的spectral differences。我们的主要观察结果表明，图异常和正常图之间的累积spectral energy存在显著差异。此外，我们证明了累积图信号的spectral energy可以由其Rayleigh Quotient表示，这表明Rayleigh Quotient是图异常性的驱动因素。这些观察和证明 inspirited我们提出Rayleigh Quotient Graph Neural Network（RQGNN），这是首个spectral GNN для图异常检测，它提供了一个新的视角来探索异常图的内在spectral特征。我们的框架包括两个组成部分：Rayleigh Quotient学习Component（RQL）和Chebyshev Wavelet GNN with RQ-pooling（CWGNN-RQ）。RQLExplicitly捕捉图的Rayleigh Quotient，而CWGNN-RQ则通过spectral空间来探索图的特征。我们在10个实际数据集上进行了广泛的实验，结果表明，RQGNN在Macro-F1分数和AUC方面比最佳竞争者高6.74%和1.44%，这表明我们的框架的效果。
</details></li>
</ul>
<hr>
<h2 id="Large-language-models-in-textual-analysis-for-gesture-selection"><a href="#Large-language-models-in-textual-analysis-for-gesture-selection" class="headerlink" title="Large language models in textual analysis for gesture selection"></a>Large language models in textual analysis for gesture selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13705">http://arxiv.org/abs/2310.13705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura B. Hensel, Nutchanon Yongsatianchot, Parisa Torshizi, Elena Minucci, Stacy Marsella</li>
<li>for: 这篇论文主要关注的是自动化手势生成技术的发展，具体来说是如何使用大语言模型（LLMs）来实现Context-specific gesture generation。</li>
<li>methods: 这篇论文使用了ChatGPT作为工具，通过提供小量的提示来建议上下文特定的手势。另外，LLMs还能够提供不在训练数据中存在的新的、适当的手势。</li>
<li>results: 该论文发现，使用LLMs可以减少繁重的注释量和快速适应不同的设计者意图。这种方法有望成为自动化手势生成技术的可能性。<details>
<summary>Abstract</summary>
Gestures perform a variety of communicative functions that powerfully influence human face-to-face interaction. How this communicative function is achieved varies greatly between individuals and depends on the role of the speaker and the context of the interaction. Approaches to automatic gesture generation vary not only in the degree to which they rely on data-driven techniques but also the degree to which they can produce context and speaker specific gestures. However, these approaches face two major challenges: The first is obtaining sufficient training data that is appropriate for the context and the goal of the application. The second is related to designer control to realize their specific intent for the application. Here, we approach these challenges by using large language models (LLMs) to show that these powerful models of large amounts of data can be adapted for gesture analysis and generation. Specifically, we used ChatGPT as a tool for suggesting context-specific gestures that can realize designer intent based on minimal prompts. We also find that ChatGPT can suggests novel yet appropriate gestures not present in the minimal training data. The use of LLMs is a promising avenue for gesture generation that reduce the need for laborious annotations and has the potential to flexibly and quickly adapt to different designer intents.
</details>
<details>
<summary>摘要</summary>
姿势可以具有多种交流功能，强烈影响人对面交流。这种交流功能的实现方式各不相同，归结在发言人的角色和交流Context中。自动姿势生成的方法不仅在数据驱动技术的程度上存在差异，还在可以生成Context和发言人Specific姿势上存在差异。然而，这些方法面临两个主要挑战：第一是获得适合Context和应用目标的充足训练数据。第二是 designer控制，以实现他们特定的应用目标。我们在这里解决这些挑战，使用大语言模型（LLMs）来示出这些强大的模型可以适应姿势分析和生成。我们使用ChatGPT作为工具，以提供Context特定的姿势建议，基于最小的提示来实现设计师的意图。我们还发现，ChatGPT可以建议不在最小训练数据中存在的新的、适当的姿势。使用LLMs是一个有前途的方式，可以减少繁重的注释和快速适应不同的设计师意图。
</details></li>
</ul>
<hr>
<h2 id="GPT-4-as-an-interface-between-researchers-and-computational-software-improving-usability-and-reproducibility"><a href="#GPT-4-as-an-interface-between-researchers-and-computational-software-improving-usability-and-reproducibility" class="headerlink" title="GPT-4 as an interface between researchers and computational software: improving usability and reproducibility"></a>GPT-4 as an interface between researchers and computational software: improving usability and reproducibility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11458">http://arxiv.org/abs/2310.11458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan C. Verduzco, Ethan Holbrook, Alejandro Strachan</li>
<li>for: 本研究旨在探讨大型自然语言模型（LLM）在科学和工程中的应用，以及其在计算材料科学中的应用。</li>
<li>methods: 本研究使用GPT-4语言模型来解决计算材料科学中的两个主要挑战：一是使用自定义输入语言的学术软件采用高难度的采用率，二是发表的结果的重复性因为不充分的描述计算方法而受到限制。</li>
<li>results: 研究发现，GPT-4可以生成正确和可用的输入文件，并且可以对复杂多步计算任务进行初步设置。此外，GPT-4可以从输入文件中提取计算任务的描述，并且可以根据需要进行调整，从详细的步骤指令转换为适合出版的概要描述。研究结果表明，GPT-4可以减少研究者的日常任务数量，加速新用户的培训，并提高结果的重复性。<details>
<summary>Abstract</summary>
Large language models (LLMs) are playing an increasingly important role in science and engineering. For example, their ability to parse and understand human and computer languages makes them powerful interpreters and their use in applications like code generation are well-documented. We explore the ability of the GPT-4 LLM to ameliorate two major challenges in computational materials science: i) the high barriers for adoption of scientific software associated with the use of custom input languages, and ii) the poor reproducibility of published results due to insufficient details in the description of simulation methods. We focus on a widely used software for molecular dynamics simulations, the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS), and quantify the usefulness of input files generated by GPT-4 from task descriptions in English and its ability to generate detailed descriptions of computational tasks from input files. We find that GPT-4 can generate correct and ready-to-use input files for relatively simple tasks and useful starting points for more complex, multi-step simulations. In addition, GPT-4's description of computational tasks from input files can be tuned from a detailed set of step-by-step instructions to a summary description appropriate for publications. Our results show that GPT-4 can reduce the number of routine tasks performed by researchers, accelerate the training of new users, and enhance reproducibility.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Sweeping-Heterogeneity-with-Smart-MoPs-Mixture-of-Prompts-for-LLM-Task-Adaptation"><a href="#Sweeping-Heterogeneity-with-Smart-MoPs-Mixture-of-Prompts-for-LLM-Task-Adaptation" class="headerlink" title="Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation"></a>Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02842">http://arxiv.org/abs/2310.02842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Dun, Mirian Hipolito Garcia, Guoqing Zheng, Ahmed Hassan Awadallah, Anastasios Kyrillidis, Robert Sim<br>for: 这个论文的目的是探讨如何使用 Mixture of Prompts（MoPs）和智能阀值功能来调整含有多种任务和数据分布的大型语言模型（LLMs），以提高其在多任务、多源enario下的性能。methods: 这篇论文提出使用 MoPs 和智能阀值功能来实现这个目标，其中 MoPs 是一种组合多个提示的技术，可以在不同的任务和数据分布下 dynamically assign 合适的专家提示，以提高模型的性能。results: 实验结果表明，使用 MoPs 可以在多任务、多源enario下 mitigate 提示训练 “干扰”，以及模型的缺失和误差。 Specifically, MoPs 在 federated scenario 下可以降低 final perplexity 从 $\sim20%$ 降至 $\sim70%$，而在 centralized scenario 下可以降低 final perplexity 从 $\sim 3%$ 降至 $\sim30%$。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have the ability to solve a variety of tasks, such as text summarization and mathematical questions, just out of the box, but they are often trained with a single task in mind. Due to high computational costs, the current trend is to use prompt instruction tuning to better adjust monolithic, pretrained LLMs for new -- but often individual -- downstream tasks. Thus, how one would expand prompt tuning to handle -- concomitantly -- heterogeneous tasks and data distributions is a widely open question. To address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs, associated with smart gating functionality: the latter -- whose design is one of the contributions of this paper -- can identify relevant skills embedded in different groups of prompts and dynamically assign combined experts (i.e., collection of prompts), based on the target task. Additionally, MoPs are empirically agnostic to any model compression technique applied -- for efficiency reasons -- as well as instruction data source and task composition. In practice, MoPs can simultaneously mitigate prompt training "interference" in multi-task, multi-source scenarios (e.g., task and data heterogeneity across sources), as well as possible implications from model approximations. As a highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to $\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim 3\%$ up to $\sim30\%$ in the centralized scenario.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有多种任务的解决能力，如文本概要和数学问题，直接从箱中提取，但它们通常在单一任务的训练下进行学习。由于计算成本高昂，当前趋势是使用提示指定调整已经预训练的LLM以适应新的、但通常是个体的下游任务。因此，如何同时处理多种不同任务和数据分布是一个未解决的问题。为解决这个漏洞，我们建议使用“混合提示”（Mixture of Prompts，MoPs），它们与智能闭合功能相结合：后者可以在目标任务上标识不同的提示组中嵌入的相关技能，并在运行时动态分配组合专家（即提示集）。此外，MoPs是对任何模型压缩技术应用的empirical无关，以及指令数据源和任务组合。在实践中，MoPs可以同时消除提示训练“干扰”在多任务多源场景（例如，任务和数据多样性遍布多个源），以及可能的模型缺失。高亮是，MoPs可以将最终的抗抑扰度从大约20%降低至大约70%，比基eline更高，在联合场景下，以及从大约3%降低至大约30%，在中央场景下。
</details></li>
</ul>
<hr>
<h2 id="Improving-Vision-Anomaly-Detection-with-the-Guidance-of-Language-Modality"><a href="#Improving-Vision-Anomaly-Detection-with-the-Guidance-of-Language-Modality" class="headerlink" title="Improving Vision Anomaly Detection with the Guidance of Language Modality"></a>Improving Vision Anomaly Detection with the Guidance of Language Modality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02821">http://arxiv.org/abs/2310.02821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Anfeather/CMG">https://github.com/Anfeather/CMG</a></li>
<li>paper_authors: Dong Chen, Kaihang Pan, Guoming Wang, Yueting Zhuang, Siliang Tang</li>
<li>For: 这篇论文的目的是提出一种基于多模式的异常检测方法，以解决现有的数据繁残和缺乏特征空间问题，以应对工业瑕疵检测、事件检测等。* Methods: 这篇论文提出了两个方法来解决缺乏特征空间和数据繁残问题，即跨模式统计学减少（CMER）和跨模式线性嵌入（CMLE）。CMER会在原始图像中遮盾一部分，然后与文本进行匹配分数，并将无关像排除以专注于重要内容。CMLE则是从语言模式中学习一个相互联系结构矩阵，以导引视觉模式中的内存空间学习。* Results: 实验结果显示，这篇论文的提案方法比基于图像的基eline方法高效16.81%。剥ppings experiments进一步证明了提案方法的协调性，每个 ком成分都需要对另一个 ком成分进行协调以 дости� optimal performance。<details>
<summary>Abstract</summary>
Recent years have seen a surge of interest in anomaly detection for tackling industrial defect detection, event detection, etc. However, existing unsupervised anomaly detectors, particularly those for the vision modality, face significant challenges due to redundant information and sparse latent space. Conversely, the language modality performs well due to its relatively single data. This paper tackles the aforementioned challenges for vision modality from a multimodal point of view. Specifically, we propose Cross-modal Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue and sparse space issue, respectively. CMER masks parts of the raw image and computes the matching score with the text. Then, CMER discards irrelevant pixels to make the detector focus on critical contents. To learn a more compact latent space for the vision anomaly detector, CMLE learns a correlation structure matrix from the language modality, and then the latent space of vision modality will be learned with the guidance of the matrix. Thereafter, the vision latent space will get semantically similar images closer. Extensive experiments demonstrate the effectiveness of the proposed methods. Particularly, CMG outperforms the baseline that only uses images by 16.81%. Ablation experiments further confirm the synergy among the proposed methods, as each component depends on the other to achieve optimal performance.
</details>
<details>
<summary>摘要</summary>
近年来，异常检测在工业缺陷检测和事件检测等领域得到了广泛关注。然而，现有的无监督异常检测器，特别是视觉模式的检测器，面临着冗余信息和稀疏的 latent space 的挑战。而语言模式的检测器则表现良好，这是因为语言数据相对较少。这篇论文从多模态角度解决了上述挑战。我们提出了跨模态指导（CMG），它包括跨模态Entropy减少（CMER）和跨模态线性嵌入（CMLE），以解决冗余信息问题和稀疏空间问题。CMER 将原始图像部分掩码，并计算与文本的匹配得分。然后，CMER 丢弃无关像素，使检测器更注意关键内容。为了学习更加紧凑的视觉异常检测器的 latent space，CMLE 学习了语言模式的相关结构矩阵，然后视觉模式的 latent space 将会被指导而学习。最后，视觉 latent space 将会更加紧凑，Semantic 相似的图像将会更加接近。广泛的实验表明我们提出的方法的效果。特别是，CMG 在只使用图像的基础上进行检测时，比基eline 高出16.81%。剖除实验还证明了我们提出的方法之间的相互依存关系，每个组件都需要另一个组件以达到最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Time-Series-Classification-in-Smart-Manufacturing-Systems-An-Experimental-Evaluation-of-State-of-the-Art-Machine-Learning-Algorithms"><a href="#Time-Series-Classification-in-Smart-Manufacturing-Systems-An-Experimental-Evaluation-of-State-of-the-Art-Machine-Learning-Algorithms" class="headerlink" title="Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms"></a>Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02812">http://arxiv.org/abs/2310.02812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mojtaba A. Farahani, M. R. McCormick, Ramy Harik, Thorsten Wuest</li>
<li>for: 本研究的目的是在生产和工业设置中进行时间序列分类任务的探索和评估，以提供对现有的SoTA机器学习和深度学习算法的实验性评估。</li>
<li>methods: 本研究使用了92种SoTA算法，其中36种是最 represetative的算法，并在22个生产数据集上进行了实验评估。</li>
<li>results: 研究发现，ResNet、DrCIF、InceptionTime和ARSENAL算法在22个生产时间序列分类任务中的平均准确率高于96.6%。此外，LSTM、BiLSTM和TS-LSTM算法也表现出色，能够在时间序列数据中捕捉特征。<details>
<summary>Abstract</summary>
Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the algorithms on the manufacturing benchmark datasets, and analyzed the results for each dataset. Based on the results, ResNet, DrCIF, InceptionTime, and ARSENAL are the top-performing algorithms, boasting an average accuracy of over 96.6% across all 22 manufacturing TSC datasets. These findings underscore the robustness, efficiency, scalability, and effectiveness of convolutional kernels in capturing temporal features in time-series data, as three out of the top four performing algorithms leverage these kernels for feature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve recognition for their effectiveness in capturing features within time-series data using RNN-based structures.
</details>
<details>
<summary>摘要</summary>
制造业收集了大量多样数据，这主要归功于传感器的增加和感知技术的快速发展。在SMS设置中，时序数据扮演着关键角色，因此TSC在这个领域变得非常重要。本研究的目的是填补这个空白，通过对SoTA ML和DL算法在制造业和工业设置中的实验评估来提供一个严格的实验评估。我们首先搜索和组织了More than 92 SoTA算法，其中大部分来自于TSC和制造业文献。然后，我们选择了这些列表中的36个最有代表性的算法。为了评估这些算法在不同的制造类型任务中的性能，我们准备了22个制造数据集，这些数据集代表了不同的制造问题，并覆盖了多种不同的特征。接下来，我们对制造benchmark数据集进行了实现和评估，并分析了每个数据集的结果。根据结果，ResNet、DrCIF、InceptionTime和ARSENAL算法在22个制造TSC数据集中的平均准确率高于96.6%。这些结果表明了 convolutional kernels在时序数据中捕捉特征的稳定性、效率、扩展性和有效性。此外，LSTM、BiLSTM和TS-LSTM算法在时序数据中捕捉特征的效果也值得注意。
</details></li>
</ul>
<hr>
<h2 id="Discovering-General-Reinforcement-Learning-Algorithms-with-Adversarial-Environment-Design"><a href="#Discovering-General-Reinforcement-Learning-Algorithms-with-Adversarial-Environment-Design" class="headerlink" title="Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design"></a>Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02782">http://arxiv.org/abs/2310.02782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EmptyJackson/groove">https://github.com/EmptyJackson/groove</a></li>
<li>paper_authors: Matthew Thomas Jackson, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, Jakob Nicolaus Foerster</li>
<li>for: 本研究旨在找到一种能够在各种深度强化学习任务中表现出色的通用RL算法，以解决现有RL算法在不同环境中的泛化性问题。</li>
<li>methods: 本研究使用了元学习更新规则，并基于无监督环境设计（UED）的想法，提出了一种自动生成课程来最大化元学习优化器的偏误。同时，提出了一种新的误差度量（AR），用于评估元学习算法在不同环境中的泛化性。</li>
<li>results: 实验结果表明，使用GROOVE方法可以在不同环境中达到更高的泛化性，并且AR被证明是UED中的一个关键组成部分。<details>
<summary>Abstract</summary>
The past decade has seen vast progress in deep reinforcement learning (RL) on the back of algorithms manually designed by human researchers. Recently, it has been shown that it is possible to meta-learn update rules, with the hope of discovering algorithms that can perform well on a wide range of RL tasks. Despite impressive initial results from algorithms such as Learned Policy Gradient (LPG), there remains a generalization gap when these algorithms are applied to unseen environments. In this work, we examine how characteristics of the meta-training distribution impact the generalization performance of these algorithms. Motivated by this analysis and building on ideas from Unsupervised Environment Design (UED), we propose a novel approach for automatically generating curricula to maximize the regret of a meta-learned optimizer, in addition to a novel approximation of regret, which we name algorithmic regret (AR). The result is our method, General RL Optimizers Obtained Via Environment Design (GROOVE). In a series of experiments, we show that GROOVE achieves superior generalization to LPG, and evaluate AR against baseline metrics from UED, identifying it as a critical component of environment design in this setting. We believe this approach is a step towards the discovery of truly general RL algorithms, capable of solving a wide range of real-world environments.
</details>
<details>
<summary>摘要</summary>
过去一代，深度强化学习（RL）在人工设计的算法支持下做出了巨大的进步。最近，人们发现可以使用meta学习更新规则，以期望在多种RL任务上发现高效的算法。虽然LPG等算法在初期的成果很吸引人，但是在未经见过的环境中仍然存在一般化差距。在这项工作中，我们分析了meta训练分布的特点对RL算法的泛化性的影响。受到这种分析和基于无监督环境设计的想法，我们提出了一种新的方法，即通过自动生成课程来最大化meta学习器的快捷（GROOVE）。在一系列实验中，我们证明GROOVE可以在LPG的基础上实现更好的泛化性，并评估了AR在UED中的基准指标，并证明它是环境设计中的关键组成部分。我们认为这种方法是在发现真正泛化RL算法的步骤， capable of solving多种实际环境中的问题。
</details></li>
</ul>
<hr>
<h2 id="Integrating-UMLS-Knowledge-into-Large-Language-Models-for-Medical-Question-Answering"><a href="#Integrating-UMLS-Knowledge-into-Large-Language-Models-for-Medical-Question-Answering" class="headerlink" title="Integrating UMLS Knowledge into Large Language Models for Medical Question Answering"></a>Integrating UMLS Knowledge into Large Language Models for Medical Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02778">http://arxiv.org/abs/2310.02778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YangRui525/UMLS-Augmented-LLM">https://github.com/YangRui525/UMLS-Augmented-LLM</a></li>
<li>paper_authors: Rui Yang, Edison Marrese-Taylor, Yuhe Ke, Lechao Cheng, Qingyu Chen, Irene Li</li>
<li>for: 这个研究旨在提高大语言模型在医疗领域的应用，并使其更能够适应实际的临床应用场景。</li>
<li>methods: 这个研究使用了UMLS扩展的大语言模型，并使用了LLaMa2-13b-chat和ChatGPT-3.5作为参考模型。研究者使用ROUGE分数和BERTScore进行自动评估，并为医生评估使用了四个维度：实际性、完整性、可读性和相关性。</li>
<li>results: 研究结果显示，这个框架可以有效提高生成内容的实际性、完整性和相关性。多名医生进行了盲测评估，结果表明这个框架可以增强生成内容的质量。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conducted blind reviews to evaluate the generated content, and the results indicate that this framework effectively enhances the factuality, completeness, and relevance of generated content. Our research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经显示出强大的文本生成能力，带来医疗领域前所未有的创新。然而，将LLMs应用到实际的医疗情况中存在着重大挑战，因为这些模型可能会生成不符合现有医疗知识的内容，甚至会显示出潜在偏见。在我们的研究中，我们开发了基于Unified Medical Language System（UMLS）的增强LLM框架，以更好地服务医疗社区。我们使用LLaMa2-13b-chat和ChatGPT-3.5作为我们的参考模型，并通过ROUGE Score和BERTScore自动评估104个LiveQA试题集中的问题。此外，我们建立了基于四个维度的医生评估标准：事实性、完整性、可读性和相关性。ChatGPT-3.5被用来进行医生评估，并使用20个LiveQA试题集中的问题进行盲评。多名住院医生进行了双盲评估，以评估生成的内容的实际性、完整性和相关性。我们的研究显示，这个框架有效地提高了生成内容的事实性、完整性和相关性。我们的研究显示LLMs在医疗问题回答中的应用价值，并显示了UMLS-增强LLMs在医疗领域的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Spike-Accumulation-Forwarding-for-Effective-Training-of-Spiking-Neural-Networks"><a href="#Spike-Accumulation-Forwarding-for-Effective-Training-of-Spiking-Neural-Networks" class="headerlink" title="Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks"></a>Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02772">http://arxiv.org/abs/2310.02772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryuji Saiin, Tomoya Shirakawa, Sota Yoshihara, Yoshihide Sawada, Hiroyuki Kusumoto</li>
<li>for: 本研究提出了一种新的聚集前进 paradigma（SAF），用于训练脉冲神经网络（SNN）。SNN具有能效的能耗特性，但训练困难。因此，许多研究人员已经提出了各种方法来解决这个问题，其中线上训练在时间（OTTT）是一种方法，允许在每个时间步骤中进行推理，同时降低内存成本。但是，OTTT需要在传输过程中进行脉冲车和权重总和操作，这会增加计算成本。</li>
<li>methods: SAF可以解决这些问题，即可以减少传输过程中的操作数量，并且可以 theoretically 证明 SAF 与 OTTT 和 Spike Representation 相一致。此外，我们通过实验证明了上述结论，并表明可以降低内存和训练时间，保持精度。</li>
<li>results: 我们通过实验证明了 SAF 可以降低内存和训练时间，保持精度。具体来说，我们在 MNIST 和 CIFAR-10 数据集上进行了实验，结果显示，使用 SAF 可以在内存和训练时间方面减少一半，而且精度保持在原始精度水平。<details>
<summary>Abstract</summary>
In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the above contents through experiments and showed that it is possible to reduce memory and training time while maintaining accuracy.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种新的思维方式来训练神经元脉冲网络（SNN），即脉冲汇聚前进（SAF）。已知SNN具有能效的能源占用但训练困难。因此，许多研究人员已经提出了多种解决方案，其中在线训练通过时间（OTTT）是一种可以在每个时间步骤上进行推理的方法。然而，在GPU上计算时，OTTT需要对脉冲列表和权重总和的脉冲列表进行操作。此外，OTTT与脉冲表示之间存在关系，尚未经过理论确认。我们的提议的方法可以解决这些问题，即SAF可以在前进过程中减少操作数量的一半，并且可以从理论上证明SAF与OTTT和脉冲表示之间存在一致性。此外，我们通过实验证明了以上内容，并证明可以降低内存和训练时间的同时保持准确性。
</details></li>
</ul>
<hr>
<h2 id="Modified-LAB-Algorithm-with-Clustering-based-Search-Space-Reduction-Method-for-solving-Engineering-Design-Problems"><a href="#Modified-LAB-Algorithm-with-Clustering-based-Search-Space-Reduction-Method-for-solving-Engineering-Design-Problems" class="headerlink" title="Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems"></a>Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03055">http://arxiv.org/abs/2310.03055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruturaj Reddy, Utkarsh Gupta, Ishaan Kale, Apoorva Shastri, Anand J Kulkarni</li>
<li>for: 这篇论文是为了提出一种修改后的LAB算法（Reddy et al. 2023），用于解决具有竞争和学习行为的群体问题。</li>
<li>methods: 该算法将原始LAB算法继承，并 introduce roulette wheel approach 和减少因子，以实现群体内部竞争和迭代缩小样本空间。此外，它还提出了一种基于减少因子的尺度的搜索空间减少法（C-SSR），以解决约束问题。</li>
<li>results: 该算法在CEC 2005 和 CEC 2017 的标准测试问题上进行验证，并表现出了改善的robustness和搜索空间探索能力。此外，与其他最近的metaheuristic算法进行比较，该算法的结果也表现出了优越性。<details>
<summary>Abstract</summary>
A modified LAB algorithm is introduced in this paper. It builds upon the original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm that models competitive and learning behaviours within a group, establishing hierarchical roles. The proposed algorithm incorporates the roulette wheel approach and a reduction factor introducing inter-group competition and iteratively narrowing down the sample space. The algorithm is validated by solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions are validated using standard statistical tests such as two-sided and pairwise signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited improved and superior robustness as well as search space exploration capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR) method is proposed, making the algorithm capable to solve constrained problems. The C-SSR method enables the algorithm to identify clusters of feasible regions, satisfying the constraints and contributing to achieve the optimal solution. This method demonstrates its effectiveness as a potential alternative to traditional constraint handling techniques. The results obtained using the Modified LAB algorithm are then compared with those achieved by other recent metaheuristic algorithms.
</details>
<details>
<summary>摘要</summary>
本文引入一种修改后的LAB算法。它基于原始LAB算法（Reddy等2023），该算法模拟了社会中竞争和学习行为，在群体中建立层次角色。提案的算法添加了扭轮方法和减少因子，引入群体间竞争和迭代缩小样本空间。该算法通过CEC 2005和CEC 2017的标准测试问题进行验证，并使用标准统计测试如双边对应签名沃克逊测试和Friendman排名测试验证解决方案。算法表现出了改善的Robustness和搜索空间探索能力。此外，一种归一化搜索空间减少（C-SSR）方法被提议，使算法能够解决约束问题。C-SSR方法使算法能够识别满足约束的可行区域归一化，从而实现优化解决方案。这种方法证明了它作为传统约束处理技术的替代方案的有效性。本文结果与其他最近的metaheuristic算法相比较。
</details></li>
</ul>
<hr>
<h2 id="MUNCH-Modelling-Unique-‘N-Controllable-Heads"><a href="#MUNCH-Modelling-Unique-‘N-Controllable-Heads" class="headerlink" title="MUNCH: Modelling Unique ‘N Controllable Heads"></a>MUNCH: Modelling Unique ‘N Controllable Heads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02753">http://arxiv.org/abs/2310.02753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debayan Deb, Suvidha Tripathi, Pranit Puri</li>
<li>for: 这个论文旨在提供一种可控、多样、高质量和可解释的3D人头生成方法，为游戏设计师提供更多的创作自由和灵活性。</li>
<li>methods: 该方法包括一个准确的geometry生成器，可以生成多种独特的样本，以及一个Render Map生成器，可以Synthesize多个高质量的物理渲染地图，包括Albedo、Glossiness、Specular和Normals等。此外，我们还引入了一种新的ColorTransformer模型，允许艺术家在生成的地图上进行semantic色控制。</li>
<li>results: 我们的模型可以生成多种独特的3D人头，并且可以在不同的游戏和动画场景中使用，同时也可以提供高质量的渲染图像。我们还引入了一些量化的metric，可以衡量模型的性能和多样性。您可以在<a target="_blank" rel="noopener" href="https://munch-seven.vercel.app/%E4%B8%AD%E6%89%BE%E5%88%B0%E7%A4%BA%E4%BE%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD%E3%80%82">https://munch-seven.vercel.app/中找到示例和数据集下载。</a><details>
<summary>Abstract</summary>
The automated generation of 3D human heads has been an intriguing and challenging task for computer vision researchers. Prevailing methods synthesize realistic avatars but with limited control over the diversity and quality of rendered outputs and suffer from limited correlation between shape and texture of the character. We propose a method that offers quality, diversity, control, and realism along with explainable network design, all desirable features to game-design artists in the domain. First, our proposed Geometry Generator identifies disentangled latent directions and generate novel and diverse samples. A Render Map Generator then learns to synthesize multiply high-fidelty physically-based render maps including Albedo, Glossiness, Specular, and Normals. For artists preferring fine-grained control over the output, we introduce a novel Color Transformer Model that allows semantic color control over generated maps. We also introduce quantifiable metrics called Uniqueness and Novelty and a combined metric to test the overall performance of our model. Demo for both shapes and textures can be found: https://munch-seven.vercel.app/. We will release our model along with the synthetic dataset.
</details>
<details>
<summary>摘要</summary>
computer vision 研究者们已经有很长时间来努力地自动生成3D人头。现有的方法可以生成具有真实感的人头模型，但是它们受到形状和文化特征的限制，而且生成的结果的质量和多样性受到限制。我们提出了一种方法，它可以同时提供高质量、多样性、控制和真实感等所有愉悦的特点，这些特点都是游戏设计师所需的。我们的提议的Geometry Generator可以识别分离的约束方向，并生成新和多样的样本。然后，我们的Render Map Generator可以synthesize多个高级Physically-Based Rendering（PBR）映射，包括Albedo、Glossiness、Specular和Normals。为了让艺术家可以进行细致的控制，我们引入了一种新的Color Transformer Model，它允许用户在生成的映射中进行semantic色控制。我们还引入了Uniqueness和Novelty这两个量化度量，以评估我们的模型的总性能。您可以在https://munch-seven.vercel.app/中找到我们的demo和数据集。我们计划将我们的模型和数据集发布出来。
</details></li>
</ul>
<hr>
<h2 id="Inclusive-Data-Representation-in-Federated-Learning-A-Novel-Approach-Integrating-Textual-and-Visual-Prompt"><a href="#Inclusive-Data-Representation-in-Federated-Learning-A-Novel-Approach-Integrating-Textual-and-Visual-Prompt" class="headerlink" title="Inclusive Data Representation in Federated Learning: A Novel Approach Integrating Textual and Visual Prompt"></a>Inclusive Data Representation in Federated Learning: A Novel Approach Integrating Textual and Visual Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04455">http://arxiv.org/abs/2310.04455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhao, Zhenpeng Shi, Yang Liu, Wenbo Ding</li>
<li>for: 提高 Federated Learning（FL）的通信开销问题</li>
<li>methods: 使用双模态Prompt Tuning（TPFL）和增强TPFL（ATPFL）来更好地表征本地客户端数据特征</li>
<li>results: 比基elines表现出色，提高了客户端模型的全球知识获取和模型的稳定性和Compactness<details>
<summary>Abstract</summary>
Federated Learning (FL) is often impeded by communication overhead issues. Prompt tuning, as a potential solution, has been introduced to only adjust a few trainable parameters rather than the whole model. However, current single-modality prompt tuning approaches fail to comprehensively portray local clients' data. To overcome this limitation, we present Twin Prompt Federated learning (TPFL), a pioneering solution that integrates both visual and textual modalities, ensuring a more holistic representation of local clients' data characteristics. Furthermore, in order to tackle the data heterogeneity issues, we introduce the Augmented TPFL (ATPFL) employing the contrastive learning to TPFL, which not only enhances the global knowledge acquisition of client models but also fosters the development of robust, compact models. The effectiveness of TPFL and ATPFL is substantiated by our extensive evaluations, consistently showing superior performance compared to all baselines.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 常被通信开销问题困扰。为解决这问题，我们提出了快速调整trainable参数的方法，而不是整个模型。然而，现有的单Modal prompt tuning方法无法全面反映本地客户端数据特征。为了解决这些限制，我们提出了双Modal prompt federated learning（TPFL），它将视觉和文本模式 integrate，以确保更全面地表征本地客户端数据特征。另外，为了解决数据不一致问题，我们提出了增强版TPFL（ATPFL），通过对TPFL进行对照学习，不仅提高客户端模型的全局知识获取，还会促进模型的紧凑和Robust。我们对TPFL和ATPFL进行了广泛的评估，并 consistently show superior performance compared to all baselines。
</details></li>
</ul>
<hr>
<h2 id="Functional-trustworthiness-of-AI-systems-by-statistically-valid-testing"><a href="#Functional-trustworthiness-of-AI-systems-by-statistically-valid-testing" class="headerlink" title="Functional trustworthiness of AI systems by statistically valid testing"></a>Functional trustworthiness of AI systems by statistically valid testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02727">http://arxiv.org/abs/2310.02727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bernhard Nessler, Thomas Doms, Sepp Hochreiter</li>
<li>for: 欧盟人工智能（AI）法律的草拟中的不足和缺乏对欧盟公民的安全、健康和权利的考核。</li>
<li>methods: 使用随机抽样和精确定义应用领域来测试AI系统的统计函数性，以确保AI系统的可靠性和可信worthiness。</li>
<li>results: 提出了三个必需元素来确保AI系统的可靠性和可信worthiness，即（1）应用领域的技术分布定义，（2）基于风险的最低性能要求，和（3）基于独立随机抽样的统计测试。<details>
<summary>Abstract</summary>
The authors are concerned about the safety, health, and rights of the European citizens due to inadequate measures and procedures required by the current draft of the EU Artificial Intelligence (AI) Act for the conformity assessment of AI systems. We observe that not only the current draft of the EU AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have resorted to the position that real functional guarantees of AI systems supposedly would be unrealistic and too complex anyways. Yet enacting a conformity assessment procedure that creates the false illusion of trust in insufficiently assessed AI systems is at best naive and at worst grossly negligent. The EU AI Act thus misses the point of ensuring quality by functional trustworthiness and correctly attributing responsibilities.   The trustworthiness of an AI decision system lies first and foremost in the correct statistical testing on randomly selected samples and in the precision of the definition of the application domain, which enables drawing samples in the first place. We will subsequently call this testable quality functional trustworthiness. It includes a design, development, and deployment that enables correct statistical testing of all relevant functions.   We are firmly convinced and advocate that a reliable assessment of the statistical functional properties of an AI system has to be the indispensable, mandatory nucleus of the conformity assessment. In this paper, we describe the three necessary elements to establish a reliable functional trustworthiness, i.e., (1) the definition of the technical distribution of the application, (2) the risk-based minimum performance requirements, and (3) the statistically valid testing based on independent random samples.
</details>
<details>
<summary>摘要</summary>
作者们对欧盟人工智能（AI）法草案中的安全、健康和公民权的不足表示关切。现行草案和相关的标准化努力在CEN/CENELEC中都采取了位置，即AI系统的实际功能保证是不可能和太复杂。然而，实施不充分的验证程序，只是创造了虚假的信任感，这是最好的情况下的懒散，最坏的情况下是格外费尽。欧盟AI法因此错过了保证质量的机会，不能正确归因责任。我们认为AI决策系统的可靠性首先取决于对随机抽样的正确统计测试和应用领域的精确定义。我们将称之为可测试的功能信任。它包括设计、开发和部署，以便对所有相关功能进行正确的统计测试。我们坚持认为，对AI系统的统计功能性的可靠评估是必不可少的，也是强制的核心。在这篇论文中，我们介绍了三个必要元素，以建立可靠的功能信任：1. 应用领域的技术分布定义2. 基于风险的最低性能要求3. 基于独立随机抽样的统计测试这三个元素是建立可靠的AI系统功能信任的必要条件。
</details></li>
</ul>
<hr>
<h2 id="Online-Clustering-of-Bandits-with-Misspecified-User-Models"><a href="#Online-Clustering-of-Bandits-with-Misspecified-User-Models" class="headerlink" title="Online Clustering of Bandits with Misspecified User Models"></a>Online Clustering of Bandits with Misspecified User Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02717">http://arxiv.org/abs/2310.02717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JizeXie/Online-Corrupted-User-Detection-and-Regret-Minimization">https://github.com/JizeXie/Online-Corrupted-User-Detection-and-Regret-Minimization</a></li>
<li>paper_authors: Zhiyong Wang, Jize Xie, Xutong Liu, Shuai Li, John C. S. Lui</li>
<li>for: 本文研究了 clustering of bandits 问题下的 user model misspecification 问题，提出了两种robust CB 算法（RCLUMB 和 RSCLUMB），可以适应用户偏好估计不准确和 clustering 错误引起的问题。</li>
<li>methods: 本文使用了 linear bandit 算法和 clustering 技术，并提出了两种robust CB 算法，其中一种使用了动态图和集合来表示学习的 clustering 结构，另一种使用了sets来表示 clustering 结构。</li>
<li>results: 本文证明了其算法的 regret 上界为 $O(\epsilon_*T\sqrt{md\log T} + d\sqrt{mT}\log T)$，比之前的 CB 工作更加宽泛，不再需要特定的技术假设。实验结果表明其算法在 synthetic 和实际数据上表现出色，超过了之前的算法。<details>
<summary>Abstract</summary>
The contextual linear bandit is an important online learning problem where given arm features, a learning agent selects an arm at each round to maximize the cumulative rewards in the long run. A line of works, called the clustering of bandits (CB), utilize the collaborative effect over user preferences and have shown significant improvements over classic linear bandit algorithms. However, existing CB algorithms require well-specified linear user models and can fail when this critical assumption does not hold. Whether robust CB algorithms can be designed for more practical scenarios with misspecified user models remains an open problem. In this paper, we are the first to present the important problem of clustering of bandits with misspecified user models (CBMUM), where the expected rewards in user models can be perturbed away from perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB (representing the learned clustering structure with dynamic graph and sets, respectively), that can accommodate the inaccurate user preference estimations and erroneous clustering caused by model misspecifications. We prove regret upper bounds of $O(\epsilon_*T\sqrt{md\log T} + d\sqrt{mT}\log T)$ for our algorithms under milder assumptions than previous CB works (notably, we move past a restrictive technical assumption on the distribution of the arms), which match the lower bound asymptotically in $T$ up to logarithmic factors, and also match the state-of-the-art results in several degenerate cases. The techniques in proving the regret caused by misclustering users are quite general and may be of independent interest. Experiments on both synthetic and real-world data show our outperformance over previous algorithms.
</details>
<details>
<summary>摘要</summary>
Contextual linear bandit是一个重要的在线学习问题，给出arm特征，学习代理选择arm每个轮次以最大化长期的奖励。一系列工作，称为 clustering of bandits（CB），利用用户偏好的共同效应并显示了较好的性能than classic linear bandit算法。然而，现有的CB算法需要well-specified的线性用户模型，并且在这个假设不成立时可能失败。whether robust CB算法可以设计 для更实际的场景，即misspecified用户模型，是一个开放的问题。在这篇论文中，我们是第一个提出 clustering of bandits with misspecified user models（CBMUM）问题，其中用户预期奖励的预测可能偏离完美的线性模型。我们设计了两种Robust CB算法，即RCLUMB和RSCLUMB（分别表示学习 clustering 结构的动态图和集合），它们可以承受用户偏好估计的不准确和 clustering 错误。我们证明了我们算法的 regret upper bound为 $O(\epsilon_*T\sqrt{md\log T} + d\sqrt{mT}\log T)$，这与前一些 CB 工作（特别是不 restrictive的技术假设）下的更强的假设下，并且与state-of-the-art 结果相同，并且在一些degree degenerate 的情况下也相同。我们的证明技巧可能是独立的兴趣。在 synthetic 和实际数据上进行了实验，我们的表现超过了之前的算法。
</details></li>
</ul>
<hr>
<h2 id="scHyena-Foundation-Model-for-Full-Length-Single-Cell-RNA-Seq-Analysis-in-Brain"><a href="#scHyena-Foundation-Model-for-Full-Length-Single-Cell-RNA-Seq-Analysis-in-Brain" class="headerlink" title="scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain"></a>scHyena: Foundation Model for Full-Length Single-Cell RNA-Seq Analysis in Brain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02713">http://arxiv.org/abs/2310.02713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyutaek Oh, Baekgyu Choi, Inkyung Jung, Jong Chul Ye</li>
<li>for: 这份论文是为了提高单元细胞RNA序列分析中的精度，尤其是在脑组织中，因为脑组织具有较高的细胞多样性。</li>
<li>methods: 该论文使用了一种基于Hyena算法的Transformer架构，称为单元细胞Hyena（scHyena），该架构包括线性适应层、基因嵌入编码和双向Hyena运算符。这使得我们可以处理整个scRNA-seq数据集而不会产生数据损失。</li>
<li>results: 作者比较了scHyena与其他参考方法的下游任务性能，包括细胞类型分类和scRNA-seq补充，并证明scHyena表现较好。<details>
<summary>Abstract</summary>
Single-cell RNA sequencing (scRNA-seq) has made significant strides in unraveling the intricate cellular diversity within complex tissues. This is particularly critical in the brain, presenting a greater diversity of cell types than other tissue types, to gain a deeper understanding of brain function within various cellular contexts. However, analyzing scRNA-seq data remains a challenge due to inherent measurement noise stemming from dropout events and the limited utilization of extensive gene expression information. In this work, we introduce scHyena, a foundation model designed to address these challenges and enhance the accuracy of scRNA-seq analysis in the brain. Specifically, inspired by the recent Hyena operator, we design a novel Transformer architecture called singe-cell Hyena (scHyena) that is equipped with a linear adaptor layer, the positional encoding via gene-embedding, and a {bidirectional} Hyena operator. This enables us to process full-length scRNA-seq data without losing any information from the raw data. In particular, our model learns generalizable features of cells and genes through pre-training scHyena using the full length of scRNA-seq data. We demonstrate the superior performance of scHyena compared to other benchmark methods in downstream tasks, including cell type classification and scRNA-seq imputation.
</details>
<details>
<summary>摘要</summary>
Single-cell RNA sequencing (scRNA-seq) 技术已经在解释脑组织中的细胞多样性中做出了重要进步。特别是在脑组织中，其细胞类型多样性更大于其他组织类型，以更深入了解脑功能在不同细胞上下文中。然而，分析 scRNA-seq 数据仍然是一项挑战，因为存在内生的测量噪音和限制了广泛的基因表达信息的利用。在这种情况下，我们引入 scHyena，一种基于 Hyena 算法的基础模型，以解决这些挑战并提高 scRNA-seq 分析的准确性。具体来说，我们采用了 Hyena 算法中的新的 Transformer 架构，并将其称为单元细胞 Hyena（scHyena）。这种架构包括线性适应层、基因嵌入编码和双向 Hyena 算法。这使得我们可以处理完整的 scRNA-seq 数据，而不会失去任何信息。特别是，我们的模型通过预训练 scHyena 使用完整的 scRNA-seq 数据来学习细胞和基因的通用特征。我们示出了 scHyena 相比其他参考方法在下游任务中的superior表现，包括细胞类型分类和 scRNA-seq 填充。
</details></li>
</ul>
<hr>
<h2 id="ED-NeRF-Efficient-Text-Guided-Editing-of-3D-Scene-using-Latent-Space-NeRF"><a href="#ED-NeRF-Efficient-Text-Guided-Editing-of-3D-Scene-using-Latent-Space-NeRF" class="headerlink" title="ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF"></a>ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02712">http://arxiv.org/abs/2310.02712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jangho Park, Gihyun Kwon, Jong Chul Ye</li>
<li>for: 文章目的是提出一种基于 latent diffusion model (LDM) 的三维 NeRF 编辑方法，以提高 NeRF 编辑速度和质量。</li>
<li>methods: 本文使用了一种唯一的填充层来嵌入真实场景到 LDM 的 latent space，并提出了一种基于 delta denoising score (DDS) 的改进的损失函数，以便更好地支持编辑。</li>
<li>results: 实验结果表明，ED-NeRF 可以在 editing 速度和输出质量两个方面比前一代 3D 编辑模型表现更优，而且它的编辑速度比传统的图像空间 NeRF 编辑更快。<details>
<summary>Abstract</summary>
Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain. This novel loss function surpasses the well-known score distillation sampling (SDS) loss in terms of suitability for editing purposes. Our experimental results demonstrate that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models.
</details>
<details>
<summary>摘要</summary>
最近，文本到图像扩散模型的进步很大，导致了2D图像生成的新纪录。这些进步被扩展到3D模型，允许通过文本描述生成新的3D对象。这种进步演变成了NeRF编辑方法，允许通过文本条件来修改现有3D对象。然而，现有的NeRF编辑技术受到了训练速度过慢和不适合编辑的问题的限制。为解决这个问题，我们在这里提出了一种新的3D NeRF编辑方法，称为ED-NeRF。这种方法通过在LDM（隐藏层扩散模型）中嵌入真实场景的特征来成功地将真实场景嵌入到LDM的latent空间中。这种方法使得我们可以获得一个更快的NeRF脊梁，并且更适合编辑。此外，我们提出了一种适用于编辑的改进的损失函数，通过将2D图像 editing中使用的delta denoising score（DDS）涂抹损失迁移到三维领域，使得这种损失函数更适合编辑。我们的实验结果表明，ED-NeRF可以比现状的3D编辑模型更快速地进行编辑，并且生成的输出质量也更高。
</details></li>
</ul>
<hr>
<h2 id="Continual-Contrastive-Spoken-Language-Understanding"><a href="#Continual-Contrastive-Spoken-Language-Understanding" class="headerlink" title="Continual Contrastive Spoken Language Understanding"></a>Continual Contrastive Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02699">http://arxiv.org/abs/2310.02699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, Alessio Brutti, Bhiksha Raj</li>
<li>for: This paper focuses on the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting, with the goal of preserving the learned representations and improving the model’s ability to learn new tasks continually.</li>
<li>methods: The proposed method, called COCONUT, combines experience replay and contrastive learning to preserve the learned representations and learn more discriminative representations of the new data. The method uses a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, and also leverages a multimodal contrastive loss to align audio and text features.</li>
<li>results: The experiments on two established SLU datasets show the effectiveness of the proposed approach, with significant improvements over the baselines. The method is also shown to be combinable with methods that operate on the decoder side of the model, resulting in further metrics improvements.<details>
<summary>Abstract</summary>
Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features. We also investigate different contrastive designs to combine the strengths of the contrastive loss with teacher-student architectures used for distillation. Experiments on two established SLU datasets reveal the effectiveness of our proposed approach and significant improvements over the baselines. We also show that COCONUT can be combined with methods that operate on the decoder side of the model, resulting in further metrics improvements.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近期，神经网络在多个领域表现出色，语音处理不例外。然而，这些领域的最新突破都需要大量的数据和计算资源进行训练，并且现在难以保持之前学习的知识。在这篇论文中，我们研究了语音理解的类逐式学习（CIL）问题，并提出了一种基于经验回放和对比学习的方法——COCONUT。通过对储存样本中的对比loss进行修改，COCONUT保持了学习的表示，同时将相同类型的样本吸引近起来，并将别的样本推远。此外，我们还利用了多模态对比损失，帮助模型学习更有特征的新数据特征。我们还 investigate了不同的对比设计，以 combinator contrastive loss 的优点和教师-学生架构的逻辑。实验表明，我们的提出方法在两个已知的 SLU 数据集上具有显著的效果，并与基准值之间具有显著的改善。此外，我们还证明了 COCONUT 可以与decoder  сторо面的方法结合使用，从而进一步提高 метрикс表现。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Domain-Gap-by-Clustering-based-Image-Text-Graph-Matching"><a href="#Bridging-the-Domain-Gap-by-Clustering-based-Image-Text-Graph-Matching" class="headerlink" title="Bridging the Domain Gap by Clustering-based Image-Text Graph Matching"></a>Bridging the Domain Gap by Clustering-based Image-Text Graph Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02692">http://arxiv.org/abs/2310.02692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nokyung Park, Daewon Chae, Jeongyong Shim, Sangpil Kim, Eun-Sol Kim, Jinkyu Kim</li>
<li>for: 本研究旨在学习域外化表示，以提高模型对未经看过的目标任务域的泛化能力。</li>
<li>methods: 本研究使用多Modal图表示方法，将图像和文本描述 fusion 为域外化的轴embedding。特别是，我们通过（i） represent 图像和文本描述为图形，并（ii）同时匹配图形基于图像节点特征和文本描述的图形结构进行域外化特征学习。</li>
<li>results: 我们在大规模公共数据集（CUB-DG和DomainBed）上进行实验，并实现了与或更好的目前状态艺的性能。我们的代码将在发表之前公开。<details>
<summary>Abstract</summary>
Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.
</details>
<details>
<summary>摘要</summary>
(i) representing the image and text descriptions with graphs, and(ii) clustering and matching the graph-based image node features into textual graphs simultaneously.We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.Translated into Simplified Chinese:学习域外常量表示是训练模型在未经见目标任务域的总是重要的。文本描述本身就含有 semantic 结构，这些auxiliary semantic cue可以用作域外常量表示的有效 pivot embedding。我们使用多modal图表示，将图像和文本描述 fusion 在一起，以获取域外常量 pivot embedding，并通过考虑本地图像和文本描述之间的semantic结构来学习域外常量特征。我们的目标是通过：(i) 将图像和文本描述表示为图，并(ii) 将图形基于图像节点的匹配和文本描述图同时进行 clustering 来学习域外常量特征。我们在大规模的公共数据集，如 CUB-DG 和 DomainBed 上进行了实验，并达到了与最佳状态的表现。我们的代码将在发表后公开。
</details></li>
</ul>
<hr>
<h2 id="USB-NeRF-Unrolling-Shutter-Bundle-Adjusted-Neural-Radiance-Fields"><a href="#USB-NeRF-Unrolling-Shutter-Bundle-Adjusted-Neural-Radiance-Fields" class="headerlink" title="USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields"></a>USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02687">http://arxiv.org/abs/2310.02687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moyang Li, Peng Wang, Lingzhe Zhao, Bangyan Liao, Peidong Liu</li>
<li>For: The paper is written for novel view synthesis and camera motion estimation in the presence of rolling shutter (RS) images.* Methods: The paper proposes a method called Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF) that corrects RS distortions and recovers accurate camera motion trajectory using a physical image formation model.* Results: The paper demonstrates better performance compared to prior works in terms of RS effect removal, novel view image synthesis, and camera motion estimation, as well as the ability to recover high-fidelity high frame-rate global shutter video from a sequence of RS images.<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) has received much attention recently due to its impressive capability to represent 3D scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via COLMAP), which further prevents the success of NeRF algorithm with RS images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of NeRF, by modeling the physical image formation process of a RS camera. Experimental results demonstrate that USB-NeRF achieves better performance compared to prior works, in terms of RS effect removal, novel view image synthesis as well as camera motion estimation. Furthermore, our algorithm can also be used to recover high-fidelity high frame-rate global shutter video from a sequence of RS images.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (nerf) 在最近几年内得到了广泛关注，因为它可以出色地表示3D场景并生成新视图图像。现有的工作通常假设输入图像是由全球曝光相机捕捉的。因此，rolling shutter（RS）图像不能直接应用于现成的 nerf 算法中，rolling shutter 效应也会影响相机pose估计（例如 via COLMAP），从而阻止 nerf 算法在 RS 图像上成功。在这篇论文中，我们提出了 Unrolling Shutter Bundle Adjusted Neural Radiance Fields（USB-NeRF）。USB-NeRF 可以同时纠正 rolling shutter 扭曲和重新估计相机运动轨迹，基于 nerf 框架，并模型了RS相机的物理图像形成过程。实验结果表明，USB-NeRF 在RS图像中Remove rolling shutter distortions和重新估计相机运动轨迹方面比过去的工作更好，同时在新视图图像生成和相机pose估计方面也表现出更高的性能。此外，我们的算法还可以用来恢复高精度高帧率全球曝光视频。
</details></li>
</ul>
<hr>
<h2 id="Memoria-Hebbian-Memory-Architecture-for-Human-Like-Sequential-Processing"><a href="#Memoria-Hebbian-Memory-Architecture-for-Human-Like-Sequential-Processing" class="headerlink" title="Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing"></a>Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03052">http://arxiv.org/abs/2310.03052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cosmoquester/memoria">https://github.com/cosmoquester/memoria</a></li>
<li>paper_authors: Sangjun Park, JinYeong Bak</li>
<li>for: 提高Transformer模型对长输入序列的处理能力</li>
<li>methods: 根据HEBB的理论实现一个通用的记忆网络，称之为Memoria，用于增强长期依赖关系</li>
<li>results: 通过BERT和GPT等Popular Transformer模型的实验，显示Memoria可以有效地提高对长输入序列的处理能力，并在排序和语言模型等多种任务中表现出色<details>
<summary>Abstract</summary>
Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Results show that Memoria outperformed existing methodologies in sorting and language modeling, and long text classification.
</details>
<details>
<summary>摘要</summary>
启发器已经在多个领域和任务中证明了其成功。然而，启发器在长输入序列上受到限制，因为它们的容量有限。而人类选择性地记忆和使用输入中的相关信息，而不是像启发器一样处理所有Raw数据从开始到结束。我们介绍Memoria，一种通用的记忆网络，该网络根据希贝尔理论，该理论是人类记忆形成的主要理论，以增强神经网络中长期依赖关系。Memoria在多个记忆层，包括工作记忆、短期记忆和长期记忆中存储和重新获取信息，使用根据希贝尔规则变化的连接重量。通过对BERT和GPT等启发器模型进行实验，我们表明了Memoria在不同任务中对长期依赖关系的考虑能力有显著改进。实验结果显示，Memoria在排序和语言模型化任务以及长文本分类任务中表现出色，超越了现有的方法ologies。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Generative-Flow-Samplers-Improving-learning-signals-through-partial-trajectory-optimization"><a href="#Diffusion-Generative-Flow-Samplers-Improving-learning-signals-through-partial-trajectory-optimization" class="headerlink" title="Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization"></a>Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02679">http://arxiv.org/abs/2310.02679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zdhNarsil/Diffusion-Generative-Flow-Samplers">https://github.com/zdhNarsil/Diffusion-Generative-Flow-Samplers</a></li>
<li>paper_authors: Dinghuai Zhang, Ricky Tian Qi Chen, Cheng-Hao Liu, Aaron Courville, Yoshua Bengio</li>
<li>for: 解决高维密度函数难以样本的问题，这是机器学习和统计领域中常见的基础任务。</li>
<li>methods: 我们extend了最近的 sampling-based 方法，这些方法利用控制的抽象过程来模拟target density的近似样本。然而，这些方法的主要缺点是需要全 trajectory 来计算训练目标，从而导致了慢的凭据分配问题和使用整个 trajectory 的学习信号。</li>
<li>results: 我们提出了Diffusion Generative Flow Samplers (DGFS)，一种 sampling-based 框架，可以分解到短 partial trajectory 段，通过 parameterizing 一个 “流函数”。我们的方法启发自 generative flow networks (GFlowNets) 的理论，使我们可以利用中间学习信号和Off-policy 探索能力。通过多种复杂的实验，我们示出了 DGFS 比相关的先前方法更准确地估计 normalization constant。<details>
<summary>Abstract</summary>
We tackle the problem of sampling from intractable high-dimensional density functions, a fundamental task that often appears in machine learning and statistics. We extend recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities. The main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers (DGFS), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional "flow function". Our method takes inspiration from the theory developed for generative flow networks (GFlowNets), allowing us to make use of intermediate learning signals and benefit from off-policy exploration capabilities. Through a variety of challenging experiments, we demonstrate that DGFS results in more accurate estimates of the normalization constant than closely-related prior methods.
</details>
<details>
<summary>摘要</summary>
我们解决高维度对应函数抽样的问题，是机器学习和统计中的基本任务之一。我们延续了 latest sampling-based 方法，它们利用控制的测量过程来模拟高维度目标分布中的抽样。主要缺点是训练目标需要全程的路径来计算，从而导致 credit assignment 问题，因为使用整个路径和终端时间所得到的学习讯号。在这个工作中，我们提出了 Diffusion Generative Flow Samplers (DGFS)，一个抽样基础框架，可以追踪分配到短距离的 partial trajectory  segments，通过另外增加一个 "流函数" 来 parameterize。我们的方法受到了生成流网络 (GFlowNets) 的理论影响，因此可以利用中途学习讯号和过度策略的优点。通过一些挑战性的实验，我们证明了 DGFS 可以更精确地估算 Normalization Constant than 相似的先前方法。
</details></li>
</ul>
<hr>
<h2 id="Spherical-Position-Encoding-for-Transformers"><a href="#Spherical-Position-Encoding-for-Transformers" class="headerlink" title="Spherical Position Encoding for Transformers"></a>Spherical Position Encoding for Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04454">http://arxiv.org/abs/2310.04454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eren Unlu</li>
<li>for: 本研究旨在提出一种基于RoPE架构的地理位置编码方法，以便在 transformer 架构中处理地理位置相关的信息。</li>
<li>methods: 本研究使用 RoPE 架构来提出一种基于圆形坐标的位置编码方法，以便在地理位置相关的信息上进行模型化。</li>
<li>results: 经过实验 validate 的结果表明，该方法可以在地理位置相关的任务上提供更好的性能，并且可以在不同的坐标系上进行可视化。<details>
<summary>Abstract</summary>
Position encoding is the primary mechanism which induces notion of sequential order for input tokens in transformer architectures. Even though this formulation in the original transformer paper has yielded plausible performance for general purpose language understanding and generation, several new frameworks such as Rotary Position Embedding (RoPE) are proposed for further enhancement. In this paper, we introduce the notion of "geotokens" which are input elements for transformer architectures, each representing an information related to a geological location. Unlike the natural language the sequential position is not important for the model but the geographical coordinates are. In order to induce the concept of relative position for such a setting and maintain the proportion between the physical distance and distance on embedding space, we formulate a position encoding mechanism based on RoPE architecture which is adjusted for spherical coordinates.
</details>
<details>
<summary>摘要</summary>
“位置编码是转换器架构中主要的机制，用于induce输入токен的顺序序列。尽管这种形式ulation在原始转换器论文中已经实现了一般语言理解和生成的可行性，但新的框架如Rotary Position Embedding（RoPE）已经被提出来进行进一步的提升。在这篇论文中，我们引入了“地理токен”这个概念，每个tokent代表一个地理位置的信息。不同于自然语言，序列位置不是模型中重要的，而地理坐标则是。为了induce相对位置的概念并保持坐标空间中的比例，我们基于RoPE架构来修改圆形坐标的位置编码机制。”
</details></li>
</ul>
<hr>
<h2 id="Land-cover-change-detection-using-paired-OpenStreetMap-data-and-optical-high-resolution-imagery-via-object-guided-Transformer"><a href="#Land-cover-change-detection-using-paired-OpenStreetMap-data-and-optical-high-resolution-imagery-via-object-guided-Transformer" class="headerlink" title="Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer"></a>Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02674">http://arxiv.org/abs/2310.02674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenhongruixuan/objformer">https://github.com/chenhongruixuan/objformer</a></li>
<li>paper_authors: Hongruixuan Chen, Cuiling Lan, Jian Song, Clifford Broni-Bediako, Junshi Xia, Naoto Yokoya</li>
<li>For: 本研究使用光学高分辨 imagery 和 OpenStreetMap (OSM) 数据进行土地覆盖变化检测。* Methods: 提出一种基于 object-based image analysis (OBIA) 技术和 Transformer 架构的 Object-guided Transformer (ObjFormer) 模型，以实现直接基于 OSM 数据和光学图像进行土地覆盖变化检测。* Results: 提出了一种新的半监督Semantic change detection task，不需要光学图像的手动标注地形变化标签来训练semantic change detector。两个轻量级semantic decoder被添加到 ObjFormer 中，以实现这个任务高效地。一种折补交叉熵损失被设计，以完全利用负样本，从而提高任务的性能。<details>
<summary>Abstract</summary>
Optical high-resolution imagery and OpenStreetMap (OSM) data are two important data sources for land-cover change detection. Previous studies in these two data sources focus on utilizing the information in OSM data to aid the change detection on multi-temporal optical high-resolution images. This paper pioneers the direct detection of land-cover changes utilizing paired OSM data and optical imagery, thereby broadening the horizons of change detection tasks to encompass more dynamic earth observations. To this end, we propose an object-guided Transformer (ObjFormer) architecture by naturally combining the prevalent object-based image analysis (OBIA) technique with the advanced vision Transformer architecture. The introduction of OBIA can significantly reduce the computational overhead and memory burden in the self-attention module. Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder consisting of object-guided self-attention modules that extract representative features of different levels from OSM data and optical images; a decoder consisting of object-guided cross-attention modules can progressively recover the land-cover changes from the extracted heterogeneous features. In addition to the basic supervised binary change detection task, this paper raises a new semi-supervised semantic change detection task that does not require any manually annotated land-cover labels of optical images to train semantic change detectors. Two lightweight semantic decoders are added to ObjFormer to accomplish this task efficiently. A converse cross-entropy loss is designed to fully utilize the negative samples, thereby contributing to the great performance improvement in this task. The first large-scale benchmark dataset containing 1,287 map-image pairs (1024$\times$ 1024 pixels for each sample) covering 40 regions on six continents ...(see the manuscript for the full abstract)
</details>
<details>
<summary>摘要</summary>
《高分辨率光学图像和OpenStreetMap（OSM）数据为地形变化检测提供了两种重要数据源。先前的研究主要关注在使用OSM数据信息来 помо助多时间点光学高分辨率图像中的变化检测。本文提出了直接通过对OSM数据和光学图像的对比来检测地形变化的方法，从而扩展了变化检测任务的观察范围，包括更多的地球观测。为此，我们提出了一种带有对象指导的变换（ObjFormer）架构，通过自然地结合了流行的对象基本分析（OBIA）技术和先进的视图变换架构。对于OBIA的引入，可以在自身注意模块中减少计算负担和内存压力。具体来说，我们的ObjFormer架构包括以对象为引导的层次伪仿同模块，从OSM数据和光学图像中提取了不同级别的特征表示，以及一个以对象为引导的跨模块，可以逐步回归地形变化从提取的异质特征中。此外，本文还提出了一种新的半监督Semantic Change Detection任务，不需要光学图像的手动标注地形类别来训练Semantic Change Detector。为此，我们在ObjFormer架构中添加了两个轻量级semantic decoder，可以高效完成这个任务。我们还设计了一种倒推十字Entropy损失函数，以完全利用负样本，从而对性能做出大幅提升。本文的首个大规模benchmark数据集包含1,287个地图像对（每个样本为1024×1024像素），覆盖了6大洲40个区域。》
</details></li>
</ul>
<hr>
<h2 id="On-Memorization-in-Diffusion-Models"><a href="#On-Memorization-in-Diffusion-Models" class="headerlink" title="On Memorization in Diffusion Models"></a>On Memorization in Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02664">http://arxiv.org/abs/2310.02664</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sail-sg/DiffMemorize">https://github.com/sail-sg/DiffMemorize</a></li>
<li>paper_authors: Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, Ye Wang</li>
<li>for: 这种研究旨在了解扩散模型在训练时的吸收行为，以及这种行为对模型的性能有何影响。</li>
<li>methods: 研究者使用了一系列实验和分析方法，包括定制扩散模型，变换训练数据，和评估模型性能等。</li>
<li>results: 研究者发现，扩散模型在训练时存在吸收行为，这种行为与数据集大小有关，并且可以通过评估模型的最大 memorization 来衡量。此外，研究者发现，在训练数据上随机的标签 Conditioning 可以显著提高模型的吸收能力。<details>
<summary>Abstract</summary>
Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models. Code is available at https://github.com/sail-sg/DiffMemorize.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Energy-efficiency-by-Solving-the-Throughput-Bottleneck-of-LSTM-Cells-for-Embedded-FPGAs"><a href="#Enhancing-Energy-efficiency-by-Solving-the-Throughput-Bottleneck-of-LSTM-Cells-for-Embedded-FPGAs" class="headerlink" title="Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs"></a>Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16842">http://arxiv.org/abs/2310.16842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Qian, Tianheng Ling, Gregor Schiele</li>
<li>for: 这个研究是为了提高互联网络 Things（IoT）中处理感应数据的效率。</li>
<li>methods: 这个研究提出了一种新的LSTM细胞优化方法，以提高FPGA上的能效处理。</li>
<li>results: 使用交通速度预测为 caso study，这个 vanilla LSTM 模型使用优化的LSTM细胞可以在FPGA上进行17534次推导每秒，仅consumption 3.8 $\mu$J每次推导，与现有方法相比，实现至少5.4倍的通过率和1.37倍的能效性。<details>
<summary>Abstract</summary>
To process sensor data in the Internet of Things(IoTs), embedded deep learning for 1-dimensional data is an important technique. In the past, CNNs were frequently used because they are simple to optimise for special embedded hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed at energy-efficient inference on end devices. Using the traffic speed prediction as a case study, a vanilla LSTM model with the optimised LSTM cell achieves 17534 inferences per second while consuming only 3.8 $\mu$J per inference on the FPGA \textit{XC7S15} from \textit{Spartan-7} family. It achieves at least 5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than existing approaches.
</details>
<details>
<summary>摘要</summary>
为Internet of Things（IoT）设备进行感知数据处理，嵌入深度学习技术是非常重要的。在过去，通常使用卷积神经网络（CNN），因为它们在特定的嵌入式硬件上进行优化非常简单，如Field-Programmable Gate Array（FPGA）。这项工作提出了一种新的长短期记忆网络（LSTM）细胞优化，旨在实现能效的推理在终端设备上。使用交通速度预测为案例研究，一个使用优化LSTM细胞的淫荡LSTM模型在FPGA上实现了每秒17534次推理，并且只消耗了3.8微瓦特的能量每次推理。与现有方法相比，它具有至少5.4倍的通道数和1.37倍的能效率。
</details></li>
</ul>
<hr>
<h2 id="Solving-Multi-Configuration-Problems-A-Performance-Analysis-with-Choco-Solver"><a href="#Solving-Multi-Configuration-Problems-A-Performance-Analysis-with-Choco-Solver" class="headerlink" title="Solving Multi-Configuration Problems: A Performance Analysis with Choco Solver"></a>Solving Multi-Configuration Problems: A Performance Analysis with Choco Solver</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02658">http://arxiv.org/abs/2310.02658</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AIG-ist-tugraz/ExamMultiConf">https://github.com/AIG-ist-tugraz/ExamMultiConf</a></li>
<li>paper_authors: Benjamin Ritz, Alexander Felfernig, Viet-Man Le, Sebastian Lubos</li>
<li>for: 这篇论文是为了描述如何使用多配置功能来满足用户的偏好而写的。</li>
<li>methods: 论文使用了一种称为多配置的方法，该方法可以配置一组配置。</li>
<li>results: 论文通过示例描述了如何使用多配置来生成个性化考试。并提供了一个约束解决器性能分析，帮助了解相关性能问题。<details>
<summary>Abstract</summary>
In many scenarios, configurators support the configuration of a solution that satisfies the preferences of a single user. The concept of \emph{multi-configuration} is based on the idea of configuring a set of configurations. Such a functionality is relevant in scenarios such as the configuration of personalized exams, the configuration of project teams, and the configuration of different trips for individual members of a tourist group (e.g., when visiting a specific city). In this paper, we exemplify the application of multi-configuration for generating individualized exams. We also provide a constraint solver performance analysis which helps to gain some insights into corresponding performance issues.
</details>
<details>
<summary>摘要</summary>
许多场景中，配置器支持配置一个满足单个用户的首选项的解决方案。基于多配置的概念，我们可以配置一组配置。这种功能在多个场景中是有用的，例如：个性化考试的配置、项目团队的配置以及不同旅游者组成员的旅游计划（例如，当访问特定城市时）。在这篇论文中，我们通过实现多配置来生成个性化考试。我们还提供了一种约束解决器性能分析，以帮助我们获得一些关于相关性能问题的启示。
</details></li>
</ul>
<hr>
<h2 id="A-Study-of-Quantisation-aware-Training-on-Time-Series-Transformer-Models-for-Resource-constrained-FPGAs"><a href="#A-Study-of-Quantisation-aware-Training-on-Time-Series-Transformer-Models-for-Resource-constrained-FPGAs" class="headerlink" title="A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs"></a>A Study of Quantisation-aware Training on Time Series Transformer Models for Resource-constrained FPGAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02654">http://arxiv.org/abs/2310.02654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianheng Ling, Chao Qian, Lukas Einhaus, Gregor Schiele</li>
<li>for: 这个研究探讨时序变换器模型中的量化感知训练（Quantisation-aware training，QAT）。</li>
<li>methods: 我们提出了一种新的自适应量化方案，在QAT阶段动态选择对称和非对称量化方案。我们发现，根据实际数据分布来匹配量化方案可以降低计算开销，保持可接受的精度。此外，我们的方法对实际数据和混合精度量化中的大多数对象进行4比特量化。</li>
<li>results: 我们的结果表明，我们的方法可以减轻计算开销，同时保持可接受的精度。此外，我们的方法在实际数据和混合精度量化中表现稳定。这些发现可以帮助模型量化和部署决策，同时为量化技术的进一步发展提供基础。<details>
<summary>Abstract</summary>
This study explores the quantisation-aware training (QAT) on time series Transformer models. We propose a novel adaptive quantisation scheme that dynamically selects between symmetric and asymmetric schemes during the QAT phase. Our approach demonstrates that matching the quantisation scheme to the real data distribution can reduce computational overhead while maintaining acceptable precision. Moreover, our approach is robust when applied to real-world data and mixed-precision quantisation, where most objects are quantised to 4 bits. Our findings inform model quantisation and deployment decisions while providing a foundation for advancing quantisation techniques.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "time series Transformer models" became "时间序列Transformer模型" (shíjiān xiàngxīng Transformer módel)* "quantisation-aware training" became "量化意识训练" (liàngzhì yìxiàng xùndǎo)* "adaptive quantization scheme" became "适应量化方案" (shìbiāng liàngzhì fāng'àn)* "symmetric and asymmetric schemes" became "对称和不对称方案" (duìxiàng hé bùduìxiàng fāng'àn)* "real data distribution" became "实际数据分布" (shíjì shùzhì fāngdīstribution)* "computational overhead" became "计算负担" (jìsuàn fùdāng)* "mixed-precision quantization" became "混合精度量化" (hùnhǎng jīngdù liàngzhì)* "most objects are quantized to 4 bits" became "大多数对象被量化为4比特" (dàduōshù de yǐngxìng bèi 4 bǐt)
</details></li>
</ul>
<hr>
<h2 id="GET-Group-Event-Transformer-for-Event-Based-Vision"><a href="#GET-Group-Event-Transformer-for-Event-Based-Vision" class="headerlink" title="GET: Group Event Transformer for Event-Based Vision"></a>GET: Group Event Transformer for Event-Based Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02642">http://arxiv.org/abs/2310.02642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peterande/get-group-event-transformer">https://github.com/peterande/get-group-event-transformer</a></li>
<li>paper_authors: Yansong Peng, Yueyi Zhang, Zhiwei Xiong, Xiaoyan Sun, Feng Wu</li>
<li>for: 这篇论文主要是为了提出一种基于事件的视觉模型，即Group Event Transformer（GET），用于提高事件视觉的性能。</li>
<li>methods: 该方法首先提出了一种新的事件表示方式，称为Group Token，它将异步事件按照时间戳和方向分组。然后，GET使用了事件双自我注意力块和Group Token集成模块，以便在空间和时间-方向两个领域中有效地传递和集成特征。</li>
<li>results: 论文对四个事件视觉分类 dataset（Cifar10-DVS、N-MNIST、N-CARS和DVS128Gesture）以及两个事件视觉检测dataset（1Mpx和Gen1）进行了评估，结果显示，GET的性能比其他当前状态的方法更高。<details>
<summary>Abstract</summary>
Event cameras are a type of novel neuromorphic sen-sor that has been gaining increasing attention. Existing event-based backbones mainly rely on image-based designs to extract spatial information within the image transformed from events, overlooking important event properties like time and polarity. To address this issue, we propose a novel Group-based vision Transformer backbone for Event-based vision, called Group Event Transformer (GET), which de-couples temporal-polarity information from spatial infor-mation throughout the feature extraction process. Specifi-cally, we first propose a new event representation for GET, named Group Token, which groups asynchronous events based on their timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention block, and Group Token Aggregation module to facilitate effective feature commu-nication and integration in both the spatial and temporal-polarity domains. After that, GET can be integrated with different downstream tasks by connecting it with vari-ous heads. We evaluate our method on four event-based classification datasets (Cifar10-DVS, N-MNIST, N-CARS, and DVS128Gesture) and two event-based object detection datasets (1Mpx and Gen1), and the results demonstrate that GET outperforms other state-of-the-art methods. The code is available at https://github.com/Peterande/GET-Group-Event-Transformer.
</details>
<details>
<summary>摘要</summary>
Event 摄像头是一种新型的神经мор夫设备，在过去几年内受到了不断的关注。现有的事件基于设计主要是通过图像转换的方式提取图像中的空间信息，忽略了事件中重要的时间和方向信息。为了解决这个问题，我们提议一种新的集群基于视力 transformer 背部筋，名为集群事件转换（GET），它在特征提取过程中分离了时间-方向信息和空间信息。具体来说，我们首先提出了一种新的事件表示方式，名为集群标识符（Group Token），该标识符将异步事件按照时间戳和方向相组织。然后，GET 应用了事件双重自我注意力块和集群标识符聚合模块，以便在空间和时间-方向域内进行有效的特征交换和集成。接着，GET 可以与不同的下游任务连接，以实现不同的应用。我们在四个事件基于分类 datasets（Cifar10-DVS、N-MNIST、N-CARS 和 DVS128Gesture）和两个事件基于物体检测 datasets（1Mpx 和 Gen1）进行了评估，结果表明，GET 的表现比其他状态的方法更出色。代码可以在 GitHub 上找到：https://github.com/Peterande/GET-Group-Event-Transformer。
</details></li>
</ul>
<hr>
<h2 id="Deformation-Invariant-Neural-Network-and-Its-Applications-in-Distorted-Image-Restoration-and-Analysis"><a href="#Deformation-Invariant-Neural-Network-and-Its-Applications-in-Distorted-Image-Restoration-and-Analysis" class="headerlink" title="Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis"></a>Deformation-Invariant Neural Network and Its Applications in Distorted Image Restoration and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02641">http://arxiv.org/abs/2310.02641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhang, Qiguang Chen, Lok Ming Lui<br>for:* 这篇论文旨在解决图像受到 геометрическими扭曲所导致的影像处理和计算机视觉任务中的问题，包括图像识别等。methods:* 该论文提出了一种名为几何不变网络（DINN）的框架，用于解决图像处理任务中的几何扭曲问题。DINN通过将几何变换网络（QCTN）作为其他深度网络的一部分，输出一个几何可变映射，以便将受到几何扭曲的图像转换为更加接近自然或好图像的版本。QCTN使用了一个深度神经网络，输出一个Beltrami系数，用于控制输出扭曲映射的本地几何扭曲。results:* 根据该框架，我们开发了一个图像分类网络，可以准确地分类受到扭曲的图像。我们的提议方案在受到大气扭曲和水扭曲的情况下进行了图像恢复，并且与现有的GAN基于方法相比，达到了更高的效果。此外，我们还应用了我们的提议方案到人脸图像的1-1验证中，并取得了满意的效果，进一步证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Images degraded by geometric distortions pose a significant challenge to imaging and computer vision tasks such as object recognition. Deep learning-based imaging models usually fail to give accurate performance for geometrically distorted images. In this paper, we propose the deformation-invariant neural network (DINN), a framework to address the problem of imaging tasks for geometrically distorted images. The DINN outputs consistent latent features for images that are geometrically distorted but represent the same underlying object or scene. The idea of DINN is to incorporate a simple component, called the quasiconformal transformer network (QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep neural network that outputs a quasiconformal map, which can be used to transform a geometrically distorted image into an improved version that is closer to the distribution of natural or good images. It first outputs a Beltrami coefficient, which measures the quasiconformality of the output deformation map. By controlling the Beltrami coefficient, the local geometric distortion under the quasiconformal mapping can be controlled. The QCTN is lightweight and simple, which can be readily integrated into other existing deep neural networks to enhance their performance. Leveraging our framework, we have developed an image classification network that achieves accurate classification of distorted images. Our proposed framework has been applied to restore geometrically distorted images by atmospheric turbulence and water turbulence. DINN outperforms existing GAN-based restoration methods under these scenarios, demonstrating the effectiveness of the proposed framework. Additionally, we apply our proposed framework to the 1-1 verification of human face images under atmospheric turbulence and achieve satisfactory performance, further demonstrating the efficacy of our approach.
</details>
<details>
<summary>摘要</summary>
图像受到 геометрических扭曲的影响 pose 图像处理和计算机视觉任务中的一个重要挑战。深度学习基于的图像模型通常无法在扭曲图像上提供准确的性能。在这篇论文中，我们提出了异构不变网络（DINN），用于解决图像处理任务中的扭曲图像问题。DINN输出了一致的缺省特征，用于表示扭曲图像，但是表示同一个物体或场景的图像。DINN的想法是将简单的组件，即射影变换网络（QCTN）， integrating 到现有的深度网络中。QCTN 是一个深度神经网络，输出一个射影变换矩阵，可以将扭曲图像转换成更加靠近自然或好图像的版本。它首先输出了一个 Бел特瑞姆系数，该系数测量射影变换矩阵的地方几何扭曲程度。通过控制 Бел特瑞姆系数，可以控制地方几何扭曲的程度。QCTN 轻量级、简单，可以轻松地与现有的深度网络集成，提高其性能。基于我们的框架，我们已经开发了一个图像分类网络，可以准确地分类扭曲图像。我们的提议的框架已经应用于 restore 扭曲图像，包括大气扭曲和水扭曲。DINN 在这些场景下比存在 GAN 基于的修复方法更高效， demonstrating 了我们的方法的有效性。此外，我们应用我们的提议的框架到人脸1：1验证中的大气扭曲场景，得到了满意的性能，进一步证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Foundation-Reinforcement-Learning-towards-Embodied-Generalist-Agents-with-Foundation-Prior-Assistance"><a href="#Foundation-Reinforcement-Learning-towards-Embodied-Generalist-Agents-with-Foundation-Prior-Assistance" class="headerlink" title="Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance"></a>Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02635">http://arxiv.org/abs/2310.02635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weirui Ye, Yunsheng Zhang, Mengchen Wang, Shengjie Wang, Xianfan Gu, Pieter Abbeel, Yang Gao</li>
<li>for: 建立embodied generalist agents，需要开发一种基于大规模预训练的基础先验论证。这种基础先验论证可以帮助RL Agent在下游任务中更快地学习和更好地 Perform。</li>
<li>methods: 我们提出了一种基于目标conditioned MDP的embodied priors，包括基础策略、价值函数和成功奖励。我们还提出了一种基于这些embodied priors的actor-critic方法，称为Foundation Actor-Critic (FAC)。</li>
<li>results: 我们通过对Meta-World任务进行评估，发现FAC可以在 less than 200k frames 内达到100%成功率，而基eline方法需要更多的框架才能达到相同的水平。此外，我们还发现FAC具有耐量噪声特性，可以在噪声环境中进行学习和 Perform。最后，我们发现FAC可以在无需人工定义精密奖励或提供teleoperated demo的情况下自主学习和 Perform。<details>
<summary>Abstract</summary>
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation priors, FAC learns significantly faster than traditional RL. Our evaluation on the Meta-World has proved that FAC can achieve 100% success rates for 7/8 tasks under less than 200k frames, which outperforms the baseline method with careful manual-designed rewards under 1M frames. (2) Robust to noisy priors. Our method tolerates the unavoidable noise in embodied foundation models. We show that FAC works well even under heavy noise or quantization errors. (3) Minimal human intervention: FAC completely learns from the foundation priors, without the need of human-specified dense reward, or providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our FRL framework could enable the future robot to autonomously explore and learn without human intervention in the physical world. In summary, our proposed FRL is a novel and powerful learning paradigm, towards achieving embodied generalist agents.
</details>
<details>
<summary>摘要</summary>
近期，人们已经证明了大规模预训练从互联网级数据是拥有普适模型的关键，例如NLP。为建立embodied普适代理人，我们和许多其他研究人员认为，这种基础先验是不可或缺的组成部分。然而，它的具体表现形式是什么，并如何在下游任务中使用，这还是一个未知问题。在这篇论文中，我们提出了一种直观的和有效的embodied先验，它包括基础策略、价值和成功奖励。这些先验基于目标conditioned MDP。为验证其效果，我们实现了一种actor-critic方法，即基础actor-critic（FAC）。我们将我们的框架称为基础学习（FRL），因为它完全依赖于embodied基础先验来探索、学习和奖励。FRL的好处有三个方面：1.  Sample efficient。与基础先验一起，FAC快速学习，我们在Meta-World上评估了FAC，它在 less than 200k frames 下可以达到 100% 成功率，而基础方法在 1M frames 下仍然未能达到这个目标。2.  Robust to noisy priors。我们的方法可以忍受基础模型中的不可避免的噪音。我们展示了FAC在噪音或量化错误下仍然可以工作良好。3.  Minimal human intervention。FAC完全从基础先验学习，不需要人类提供密集奖励或提供电动示范。因此，FAC可以轻松扩展。我们认为，我们的FRL框架可能会将未来的机器人让人类无需直接干预地自主探索和学习。总之，我们提出的FRL是一种新的和强大的学习方法，逐渐实现embodied普适代理人。
</details></li>
</ul>
<hr>
<h2 id="Multi-rules-mining-algorithm-for-combinatorially-exploded-decision-trees-with-modified-Aitchison-Aitken-function-based-Bayesian-optimization"><a href="#Multi-rules-mining-algorithm-for-combinatorially-exploded-decision-trees-with-modified-Aitchison-Aitken-function-based-Bayesian-optimization" class="headerlink" title="Multi-rules mining algorithm for combinatorially exploded decision trees with modified Aitchison-Aitken function-based Bayesian optimization"></a>Multi-rules mining algorithm for combinatorially exploded decision trees with modified Aitchison-Aitken function-based Bayesian optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02633">http://arxiv.org/abs/2310.02633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuto Omae, Masaya Mori, Yohei Kakimoto</li>
<li>for: 本研究旨在提出一种能够有效地找到可靠规则的方法，以解决decision trees中的 combinatorial explosion问题。</li>
<li>methods: 本研究提出了两种新的算法，即MAABO-MT和GS-MRM，它们可以在有限的计算成本下，找到全面性最高的decision trees，并提取可靠且不同的规则。</li>
<li>results: 实验结果表明，MAABO-MT可以更有效地找到可靠规则，并且比其他基于随机性的方法更具有深度的探索能力。此外，GS-MRM可以减少不必要的规则，提高可靠性。<details>
<summary>Abstract</summary>
Decision trees offer the benefit of easy interpretation because they allow the classification of input data based on if--then rules. However, as decision trees are constructed by an algorithm that achieves clear classification with minimum necessary rules, the trees possess the drawback of extracting only minimum rules, even when various latent rules exist in data. Approaches that construct multiple trees using randomly selected feature subsets do exist. However, the number of trees that can be constructed remains at the same scale because the number of feature subsets is a combinatorial explosion. Additionally, when multiple trees are constructed, numerous rules are generated, of which several are untrustworthy and/or highly similar. Therefore, we propose "MAABO-MT" and "GS-MRM" algorithms that strategically construct trees with high estimation performance among all possible trees with small computational complexity and extract only reliable and non-similar rules, respectively. Experiments are conducted using several open datasets to analyze the effectiveness of the proposed method. The results confirm that MAABO-MT can discover reliable rules at a lower computational cost than other methods that rely on randomness. Furthermore, the proposed method is confirmed to provide deeper insights than single decision trees commonly used in previous studies. Therefore, MAABO-MT and GS-MRM can efficiently extract rules from combinatorially exploded decision trees.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose two algorithms: "MAABO-MT" and "GS-MRM." MAABO-MT strategically constructs trees with high estimation performance among all possible trees with small computational complexity. GS-MRM extracts only reliable and non-similar rules from the constructed trees. Experiments were conducted using several open datasets to analyze the effectiveness of the proposed method. The results confirm that MAABO-MT can discover reliable rules at a lower computational cost than other methods that rely on randomness. Furthermore, the proposed method provides deeper insights than single decision trees commonly used in previous studies. Therefore, MAABO-MT and GS-MRM can efficiently extract rules from combinatorially exploded decision trees.
</details></li>
</ul>
<hr>
<h2 id="On-Quantified-Observability-Analysis-in-Multiagent-Systems"><a href="#On-Quantified-Observability-Analysis-in-Multiagent-Systems" class="headerlink" title="On Quantified Observability Analysis in Multiagent Systems"></a>On Quantified Observability Analysis in Multiagent Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02614">http://arxiv.org/abs/2310.02614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyan Mu, Jun Pang</li>
<li>for: 本研究旨在帮助多智能体系统（MAS）操作人员通过观察系统行为来优化团队性能，但是也可能泄露敏感信息。</li>
<li>methods: 本研究提出了一种量化观察性分析方法，使用了 opacity 概念来形式表达 MAS 中 Agent 的观察性特性。我们提出了一种基于 temporal logic 的 oPATL 语言来描述 Agent 的观察性目标，并开发了一些验证技术来量化分析这些特性。</li>
<li>results: 我们在 PRISM 模型检查器上实现了该方法，并通过一些示例证明了其可行性和应用性。<details>
<summary>Abstract</summary>
In multiagent systems (MASs), agents' observation upon system behaviours may improve the overall team performance, but may also leak sensitive information to an observer. A quantified observability analysis can thus be useful to assist decision-making in MASs by operators seeking to optimise the relationship between performance effectiveness and information exposure through observations in practice. This paper presents a novel approach to quantitatively analysing the observability properties in MASs. The concept of opacity is applied to formally express the characterisation of observability in MASs modelled as partially observable multiagent systems. We propose a temporal logic oPATL to reason about agents' observability with quantitative goals, which capture the probability of information transparency of system behaviours to an observer, and develop verification techniques for quantitatively analysing such properties. We implement the approach as an extension of the PRISM model checker, and illustrate its applicability via several examples.
</details>
<details>
<summary>摘要</summary>
在多代理系统（MAS）中，代理人对系统行为的观察可能会提高整体团队性能，但也可能会泄露敏感信息给观察者。一个量化的观察可能分析可以帮助决策在MAS中，由操作者寻求最佳化观察和信息暴露之间的关系。本纸提出了一种新的方法来量化MAS中的可观察性特性。我们将opacity应用到形式表示MAS中代理人对系统行为的可观察性，并提出了一种时间逻辑oPATL来评估代理人对于系统行为的可观察性，并开发了证明技术来量化这些特性。我们将这种方法扩展到PRISM模型检查器中，并透过一些例子说明其应用。
</details></li>
</ul>
<hr>
<h2 id="How-FaR-Are-Large-Language-Models-From-Agents-with-Theory-of-Mind"><a href="#How-FaR-Are-Large-Language-Models-From-Agents-with-Theory-of-Mind" class="headerlink" title="How FaR Are Large Language Models From Agents with Theory-of-Mind?"></a>How FaR Are Large Language Models From Agents with Theory-of-Mind?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03051">http://arxiv.org/abs/2310.03051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R. McKee, Ari Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay, Manaal Faruqui</li>
<li>for: 这个论文旨在测试大语言模型（LLM）是否可以从他们对人们思维的推理中得到策略性行为。</li>
<li>methods: 这个论文提出了一种新的评估模型（T4D），要求模型从故事中人物的思维状态推理中导出策略性行动。同时，论文还提出了一种零批训练框架（FaR），用于帮助模型更好地预测未来挑战和考虑可能的行动。</li>
<li>results: 实验表明，使用FaR框架可以提高GPT-4的表现从50%提高到71%，比其他训练方法和几何思维更高。此外，FaR框架还能在多种各种故事结构和场景中提高模型的表现，包括需要思维状态推理的多种场景。<details>
<summary>Abstract</summary>
"Thinking is for Doing." Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50% to 71% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-Agent-Reinforcement-Learning-for-Power-Grid-Topology-Optimization"><a href="#Multi-Agent-Reinforcement-Learning-for-Power-Grid-Topology-Optimization" class="headerlink" title="Multi-Agent Reinforcement Learning for Power Grid Topology Optimization"></a>Multi-Agent Reinforcement Learning for Power Grid Topology Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02605">http://arxiv.org/abs/2310.02605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erica van der Sar, Alessandro Zocca, Sandjai Bhulai</li>
<li>for: 管理增长中的能源网络，因为增加的能源需求和不可预测的可再生能源（太阳和风）。</li>
<li>methods: 使用分布式决策者（MARL）框架，利用电力网络的层次结构。</li>
<li>results: 实验结果表明，MARL框架与单个决策者RL方法相比，表现竞争力强。同时，对下级决策者的RL算法和高级决策者的策略进行比较。<details>
<summary>Abstract</summary>
Recent challenges in operating power networks arise from increasing energy demands and unpredictable renewable sources like wind and solar. While reinforcement learning (RL) shows promise in managing these networks, through topological actions like bus and line switching, efficiently handling large action spaces as networks grow is crucial. This paper presents a hierarchical multi-agent reinforcement learning (MARL) framework tailored for these expansive action spaces, leveraging the power grid's inherent hierarchical nature. Experimental results indicate the MARL framework's competitive performance with single-agent RL methods. We also compare different RL algorithms for lower-level agents alongside different policies for higher-order agents.
</details>
<details>
<summary>摘要</summary>
最近的电力网操作挑战 arise from 增长的能源需求和不可预测的可再生能源 like 风和太阳。而增强学习（RL）显示出管理这些网络的应用潜力，通过顶层的动作 like 电网和线路调整。然而，为了有效地处理随网络规模增长的大型动作空间，是非常重要的。这篇文章提出了一个层次多智能类RL框架，利用电力网的自然层次结构。实验结果显示这个框架与单一RL方法相当竞争，我们还比较了不同的RL算法和高级掌控策略。
</details></li>
</ul>
<hr>
<h2 id="MagicDrive-Street-View-Generation-with-Diverse-3D-Geometry-Control"><a href="#MagicDrive-Street-View-Generation-with-Diverse-3D-Geometry-Control" class="headerlink" title="MagicDrive: Street View Generation with Diverse 3D Geometry Control"></a>MagicDrive: Street View Generation with Diverse 3D Geometry Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02601">http://arxiv.org/abs/2310.02601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cure-lab/MagicDrive">https://github.com/cure-lab/MagicDrive</a></li>
<li>paper_authors: Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu</li>
<li>for: The paper is written for the task of street view generation with 3D control, specifically for 3D perception tasks like 3D object detection.</li>
<li>methods: The paper proposes a novel framework called MagicDrive, which uses tailored encoding strategies to generate street views with diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, as well as textual descriptions. The framework also incorporates a cross-view attention module to ensure consistency across multiple camera views.</li>
<li>results: The paper achieves high-fidelity street-view synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection.<details>
<summary>Abstract</summary>
Recent advancements in diffusion models have significantly enhanced the data synthesis with 2D control. Yet, precise 3D control in street view generation, crucial for 3D perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View (BEV) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for 3D object detection tasks. In this paper, we introduce MagicDrive, a novel street view generation framework offering diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views. With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV segmentation and 3D object detection.
</details>
<details>
<summary>摘要</summary>
In this paper, we introduce MagicDrive, a novel street view generation framework that offers diverse 3D geometry controls, including camera poses, road maps, and 3D bounding boxes, along with textual descriptions. This is achieved through tailored encoding strategies. Additionally, our design incorporates a cross-view attention module to ensure consistency across multiple camera views.With MagicDrive, we achieve high-fidelity street-view synthesis that captures nuanced 3D geometry and various scene descriptions, enhancing tasks such as BEV segmentation and 3D object detection.
</details></li>
</ul>
<hr>
<h2 id="A-ModelOps-based-Framework-for-Intelligent-Medical-Knowledge-Extraction"><a href="#A-ModelOps-based-Framework-for-Intelligent-Medical-Knowledge-Extraction" class="headerlink" title="A ModelOps-based Framework for Intelligent Medical Knowledge Extraction"></a>A ModelOps-based Framework for Intelligent Medical Knowledge Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02593">http://arxiv.org/abs/2310.02593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongxin Ding, Peinie Zou, Zhiyuan Wang, Junfeng Zhao, Yasha Wang, Qiang Zhou</li>
<li>for: 提高医疗文本中的医学知识抽取，以便进行下游任务，如医学知识图构建和临床决策。</li>
<li>methods: 提出一个基于ModelOps的智能医学知识抽取框架，该框架提供了一个低代码系统 для模型选择、训练、评估和优化。</li>
<li>results: 提出了一种基于多层回调函数的数据集抽象机制，以及一种基于数据集相似性的模型推荐方法，帮助用户快速找到适合给定数据集的模型。<details>
<summary>Abstract</summary>
Extracting medical knowledge from healthcare texts enhances downstream tasks like medical knowledge graph construction and clinical decision-making. However, the construction and application of knowledge extraction models lack automation, reusability and unified management, leading to inefficiencies for researchers and high barriers for non-AI experts such as doctors, to utilize knowledge extraction. To address these issues, we propose a ModelOps-based intelligent medical knowledge extraction framework that offers a low-code system for model selection, training, evaluation and optimization. Specifically, the framework includes a dataset abstraction mechanism based on multi-layer callback functions, a reusable model training, monitoring and management mechanism. We also propose a model recommendation method based on dataset similarity, which helps users quickly find potentially suitable models for a given dataset. Our framework provides convenience for researchers to develop models and simplifies model access for non-AI experts such as doctors.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输文本到Simplified Chinese。<</SYS>>从医疗文本中提取医学知识可以提高下游任务，如医学知识图构建和临床决策。然而，知识提取模型的建构和应用缺乏自动化、再利用和统一管理，导致研究人员的不fficient和非AI专家，如医生， Utilize知识提取。为解决这些问题，我们提出了基于ModelOps的智能医学知识提取框架，该框架提供了低代码系统，用于选择模型、训练、评估和优化。具体来说，该框架包括基于多层回调函数的数据集抽象机制，可重用的模型训练、监控和管理机制。我们还提出了基于数据集相似性的模型推荐方法，帮助用户快速找到适合给定数据集的可能适用的模型。我们的框架为研究人员开发模型提供了便利，并简化了非AI专家，如医生，访问模型的过程。
</details></li>
</ul>
<hr>
<h2 id="On-the-Stability-of-Expressive-Positional-Encodings-for-Graph-Neural-Networks"><a href="#On-the-Stability-of-Expressive-Positional-Encodings-for-Graph-Neural-Networks" class="headerlink" title="On the Stability of Expressive Positional Encodings for Graph Neural Networks"></a>On the Stability of Expressive Positional Encodings for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02579">http://arxiv.org/abs/2310.02579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Graph-COM/SPE">https://github.com/Graph-COM/SPE</a></li>
<li>paper_authors: Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, Pan Li</li>
<li>for: 本文主要针对Graph Transformer和Message-Passing Graph Neural Network中的Positional Encoding问题，提出了一种稳定和表达complete的解决方案。</li>
<li>methods: 本文使用Laplacian eigenvectors作为Positional Encoding，但是存在两个主要挑战：非唯一性和不稳定性。而大多数现有方法只关注非唯一性，忽略了稳定性问题。本文提出了一种”软分区”的方法来解决稳定性问题，并证明了这种方法的稳定性和表达能力。</li>
<li>results: 本文通过分子性质预测和 OUT-OF-distribution泛化任务的实验表明，SPE方法可以提高Positional Encoding的泛化能力和稳定性。<details>
<summary>Abstract</summary>
Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.   Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.
</details>
<details>
<summary>摘要</summary>
设计有效的位置编码方法对于图 transformations 和消息传递图神经网络是关键。虽然广泛使用laplacian eigenvector作为位置编码，但面临两个基本挑战：（1）非唯一性：存在多种不同的laplacian eigendecompositions，（2）不稳定性：小 perturbations 可能导致完全不同的eigenspaces，从而导致位置编码不可预测。despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. we identify the cause of instability to be a "hard partition" of eigenspaces. hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis-invariant functions while respecting all symmetries of eigenvectors. besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. finally, we evaluate the effectiveness of our method on molecular property prediction and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.Here's the translation in Traditional Chinese:设计有效的位置编码方法对于图 transformations 和消息传递图神经网络是关键。虽然广泛使用laplacian eigenvector作为位置编码，但面临两个基本挑战：（1）非唯一性：存在多种不同的laplacian eigendecompositions，（2）不稳定性：小 perturbations 可能导致完全不同的eigenspaces，从而导致位置编码不可预测。despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. we identify the cause of instability to be a "hard partition" of eigenspaces. hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis-invariant functions while respecting all symmetries of eigenvectors. besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. finally, we evaluate the effectiveness of our method on molecular property prediction and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.
</details></li>
</ul>
<hr>
<h2 id="Stand-for-Something-or-Fall-for-Everything-Predict-Misinformation-Spread-with-Stance-Aware-Graph-Neural-Networks"><a href="#Stand-for-Something-or-Fall-for-Everything-Predict-Misinformation-Spread-with-Stance-Aware-Graph-Neural-Networks" class="headerlink" title="Stand for Something or Fall for Everything: Predict Misinformation Spread with Stance-Aware Graph Neural Networks"></a>Stand for Something or Fall for Everything: Predict Misinformation Spread with Stance-Aware Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02568">http://arxiv.org/abs/2310.02568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Chen, Jingyi Sun, Rong Liu, Feng Mai</li>
<li>for: 这篇研究旨在提出一个基于用户立场的图 neural network（stance-aware GNN），以预测社交媒体上的谣传。</li>
<li>methods: 这篇研究使用了四种信息传递路径，并使用可变的注意力权重提供解释性。</li>
<li>results: 比较之下，stance-aware GNN 的预测性能高于参考值 by 32.65%，并高于不包含用户立场的进阶 GNN by over 4.69%。 另外，注意力权重显示了用户反对立场对他们邻居的行为有较高的影响力，这可能 function as social correction to halt misinformation propagation.<details>
<summary>Abstract</summary>
Although pervasive spread of misinformation on social media platforms has become a pressing challenge, existing platform interventions have shown limited success in curbing its dissemination. In this study, we propose a stance-aware graph neural network (stance-aware GNN) that leverages users' stances to proactively predict misinformation spread. As different user stances can form unique echo chambers, we customize four information passing paths in stance-aware GNN, while the trainable attention weights provide explainability by highlighting each structure's importance. Evaluated on a real-world dataset, stance-aware GNN outperforms benchmarks by 32.65% and exceeds advanced GNNs without user stance by over 4.69%. Furthermore, the attention weights indicate that users' opposition stances have a higher impact on their neighbors' behaviors than supportive ones, which function as social correction to halt misinformation propagation. Overall, our study provides an effective predictive model for platforms to combat misinformation, and highlights the impact of user stances in the misinformation propagation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Automatic-VQA-Evaluation-Using-Large-Language-Models"><a href="#Improving-Automatic-VQA-Evaluation-Using-Large-Language-Models" class="headerlink" title="Improving Automatic VQA Evaluation Using Large Language Models"></a>Improving Automatic VQA Evaluation Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02567">http://arxiv.org/abs/2310.02567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Mañas, Benno Krojer, Aishwarya Agrawal</li>
<li>for: 提高自动评估的精度，使其更能反映人类判断。</li>
<li>methods: 利用 instruciton-tuned 大语言模型来建立更加稳健的 VQA 评估 metric。</li>
<li>results: 比 existed 评估 metric 更好地与人类判断相关，适用于多种 VQA 模型和测试数据集。<details>
<summary>Abstract</summary>
8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We hope wide adoption of our metric will contribute to better estimating the research progress on the VQA task.
</details>
<details>
<summary>摘要</summary>
八年后，视觉问答任务（VQA）的自动评估 metric 仍然是评估系统的首选metric。VQA 精度在closed-set中是有效的，但我们社区正在转移到开放式生成模型和 OOD 评估。在这个新 paradigm 中，现有的 VQA 精度 metric 太严格，下降了 VQA 系统的表现。因此，我们需要开发更加 Robust 的自动 VQA  metric，作为人类判断的代理。在这种工作中，我们利用 instruction-tuned 大语言模型（LLM）的上下文学习能力，构建了一个更好的 VQA 评估 metric。我们将 VQA 评估定义为一个答案评分任务，LLM 根据一组参考答案，对候选答案进行评分。我们示出了我们提出的 metric 与人类判断更加相关，在多个 VQA 模型和benchmark上表现出了更好的性能。我们希望广泛采用我们的 metric，能够更好地评估 VQA 任务的研究进步。
</details></li>
</ul>
<hr>
<h2 id="Improving-Drumming-Robot-Via-Attention-Transformer-Network"><a href="#Improving-Drumming-Robot-Via-Attention-Transformer-Network" class="headerlink" title="Improving Drumming Robot Via Attention Transformer Network"></a>Improving Drumming Robot Via Attention Transformer Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02565">http://arxiv.org/abs/2310.02565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Yi, Zonghan Li</li>
<li>for: 这个论文主要关注在娱乐领域中使用机器人技术，具体来说是开发一种基于视transformer网络的自动化音乐识别机器人。</li>
<li>methods: 该方法使用视transformer网络，可以高效处理顺序音频嵌入数据，并模型全局长距离相关性。</li>
<li>results: 大量实验结果表明，改进算法可以提高鼓类别性能，从而帮助机器人享受多种智能应用和服务。<details>
<summary>Abstract</summary>
Robotic technology has been widely used in nowadays society, which has made great progress in various fields such as agriculture, manufacturing and entertainment. In this paper, we focus on the topic of drumming robots in entertainment. To this end, we introduce an improving drumming robot that can automatically complete music transcription based on the popular vision transformer network based on the attention mechanism. Equipped with the attention transformer network, our method can efficiently handle the sequential audio embedding input and model their global long-range dependencies. Massive experimental results demonstrate that the improving algorithm can help the drumming robot promote drum classification performance, which can also help the robot to enjoy a variety of smart applications and services.
</details>
<details>
<summary>摘要</summary>
现代社会中，机器人技术已经广泛应用，在各个领域如农业、制造和娱乐等方面取得了大量的进步。在这篇论文中，我们将关注乐队琴行业中的打击机器人。为了实现这一目标，我们提出了一种基于流行的视Transformer网络的注意机制来改进打击机器人的音乐识别性能。凭借注意Transformer网络，我们的方法可以有效地处理串行音频嵌入，并模型全球长距离关系。大量的实验结果表明，改进算法可以帮助打击机器人提高打击类型分类性能，这也可以帮助机器人享受到多种智能应用和服务。
</details></li>
</ul>
<hr>
<h2 id="zkFL-Zero-Knowledge-Proof-based-Gradient-Aggregation-for-Federated-Learning"><a href="#zkFL-Zero-Knowledge-Proof-based-Gradient-Aggregation-for-Federated-Learning" class="headerlink" title="zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning"></a>zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02554">http://arxiv.org/abs/2310.02554</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhipeng Wang, Nanqing Dong, Jiahao Sun, William Knottenbelt</li>
<li>for: 提高 Federated Learning（FL）中中央聚合器的可信度问题</li>
<li>methods: 使用零知识证明（ZKP）和区块链来保证中央聚合器在训练模型归一化过程中的正确性</li>
<li>results: zkFL可以在不改变FL网络结构的情况下，提高安全性和隐私性，而无需重大地降低训练速度。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further reduce the verification cost of clients, we employ a blockchain to handle the proof in a zero-knowledge way, where miners (i.e., the nodes validating and maintaining the blockchain data) can verify the proof without knowing the clients' local and aggregated models. The theoretical analysis and empirical results show that zkFL can achieve better security and privacy than traditional FL, without modifying the underlying FL network structure or heavily compromising the training speed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Auto-FP-An-Experimental-Study-of-Automated-Feature-Preprocessing-for-Tabular-Data"><a href="#Auto-FP-An-Experimental-Study-of-Automated-Feature-Preprocessing-for-Tabular-Data" class="headerlink" title="Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular Data"></a>Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02540">http://arxiv.org/abs/2310.02540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AutoFP/Auto-FP">https://github.com/AutoFP/Auto-FP</a></li>
<li>paper_authors: Danrui Qi, Jinglin Peng, Yongjun He, Jiannan Wang</li>
<li>for: 本研究旨在自动化特征预处理（Auto-FP），以提高机器学习模型的质量。</li>
<li>methods: 本研究使用了演化算法、神经网络搜索算法等方法来自动化特征预处理。</li>
<li>results: 研究发现，演化算法在45个公共机器学习数据集上表现最佳，而随机搜索则出乎意料地表现良好。这些结果可能归结于特征预处理的具体问题和搜索算法的设计。<details>
<summary>Abstract</summary>
Classical machine learning models, such as linear models and tree-based models, are widely used in industry. These models are sensitive to data distribution, thus feature preprocessing, which transforms features from one distribution to another, is a crucial step to ensure good model quality. Manually constructing a feature preprocessing pipeline is challenging because data scientists need to make difficult decisions about which preprocessors to select and in which order to compose them. In this paper, we study how to automate feature preprocessing (Auto-FP) for tabular data. Due to the large search space, a brute-force solution is prohibitively expensive. To address this challenge, we interestingly observe that Auto-FP can be modelled as either a hyperparameter optimization (HPO) or a neural architecture search (NAS) problem. This observation enables us to extend a variety of HPO and NAS algorithms to solve the Auto-FP problem. We conduct a comprehensive evaluation and analysis of 15 algorithms on 45 public ML datasets. Overall, evolution-based algorithms show the leading average ranking. Surprisingly, the random search turns out to be a strong baseline. Many surrogate-model-based and bandit-based search algorithms, which achieve good performance for HPO and NAS, do not outperform random search for Auto-FP. We analyze the reasons for our findings and conduct a bottleneck analysis to identify the opportunities to improve these algorithms. Furthermore, we explore how to extend Auto-FP to support parameter search and compare two ways to achieve this goal. In the end, we evaluate Auto-FP in an AutoML context and discuss the limitations of popular AutoML tools. To the best of our knowledge, this is the first study on automated feature preprocessing. We hope our work can inspire researchers to develop new algorithms tailored for Auto-FP.
</details>
<details>
<summary>摘要</summary>
传统机器学习模型，如线性模型和树状模型，在实际应用中广泛使用。这些模型对数据分布敏感，因此特征预处理成为确保模型质量的关键步骤。手动构建特征预处理管道是具有挑战性的，因为数据科学家需要做出许多困难的决策，选择哪些预处理器并将它们按什么顺序排序。在这篇论文中，我们研究如何自动化特征预处理（Auto-FP） для表格数据。由于搜索空间很大，简单的策略是不可能的。为了解决这个挑战，我们发现了一个有趣的观察：Auto-FP可以被视为 either hyperparameter optimization（HPO）或 neural architecture search（NAS）问题。这一观察使得我们可以将多种 HPO 和 NAS 算法扩展到解决 Auto-FP 问题。我们对 15 种算法进行了全面的评估和分析，并在 45 个公共机器学习数据集上进行了比较。总的来说，演化算法显示出了最佳平均排名。尽管随机搜索表现出色，但许多基于模拟器和带刺搜索算法，在 HPO 和 NAS 中表现良好，却不能超越随机搜索。我们分析了这些结果的原因，并进行了瓶颈分析，以寻找改进这些算法的机会。此外，我们还探讨了如何将 Auto-FP 扩展到支持参数搜索，并评估了两种实现方式。最后，我们评估了 Auto-FP 在 AutoML 上的表现，并讨论了流行的 AutoML 工具的局限性。到目前为止，这是自动化特征预处理的首次研究。我们希望我们的工作能启发研究人员开发特定于 Auto-FP 的新算法。
</details></li>
</ul>
<hr>
<h2 id="Literature-Based-Discovery-LBD-Towards-Hypothesis-Generation-and-Knowledge-Discovery-in-Biomedical-Text-Mining"><a href="#Literature-Based-Discovery-LBD-Towards-Hypothesis-Generation-and-Knowledge-Discovery-in-Biomedical-Text-Mining" class="headerlink" title="Literature Based Discovery (LBD): Towards Hypothesis Generation and Knowledge Discovery in Biomedical Text Mining"></a>Literature Based Discovery (LBD): Towards Hypothesis Generation and Knowledge Discovery in Biomedical Text Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03766">http://arxiv.org/abs/2310.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Balu Bhasuran, Gurusamy Murugesan, Jeyakumar Natarajan</li>
<li>for:  automatic discovery of novel associations between medical terms in biomedical literature</li>
<li>methods:  Literature Based Discovery (LBD) using concept profiles and statistical significance, and deep learning applications such as transformer models and neural networks</li>
<li>results:  potential associations hidden in biomedical literature, including key biomedical discoveries in biomedicineHere’s the full text in Simplified Chinese:</li>
<li>for: 自动发现生物医学文献中的医学术语之间的新相关性</li>
<li>methods: 使用概念profile和统计学 significado的Literature Based Discovery (LBD)方法，以及深度学习应用such as transformer模型和神经网络</li>
<li>results: 生物医学文献中隐藏的可能相关性，包括生物医学中的重要发现I hope that helps!<details>
<summary>Abstract</summary>
Biomedical knowledge is growing in an astounding pace with a majority of this knowledge is represented as scientific publications. Text mining tools and methods represents automatic approaches for extracting hidden patterns and trends from this semi structured and unstructured data. In Biomedical Text mining, Literature Based Discovery (LBD) is the process of automatically discovering novel associations between medical terms otherwise mentioned in disjoint literature sets. LBD approaches proven to be successfully reducing the discovery time of potential associations that are hidden in the vast amount of scientific literature. The process focuses on creating concept profiles for medical terms such as a disease or symptom and connecting it with a drug and treatment based on the statistical significance of the shared profiles. This knowledge discovery approach introduced in 1989 still remains as a core task in text mining. Currently the ABC principle based two approaches namely open discovery and closed discovery are mostly explored in LBD process. This review starts with general introduction about text mining followed by biomedical text mining and introduces various literature resources such as MEDLINE, UMLS, MESH, and SemMedDB. This is followed by brief introduction of the core ABC principle and its associated two approaches open discovery and closed discovery in LBD process. This review also discusses the deep learning applications in LBD by reviewing the role of transformer models and neural networks based LBD models and its future aspects. Finally, reviews the key biomedical discoveries generated through LBD approaches in biomedicine and conclude with the current limitations and future directions of LBD.
</details>
<details>
<summary>摘要</summary>
The LBD process creates concept profiles for medical terms, such as diseases or symptoms, and connects them with drugs and treatments based on the statistical significance of the shared profiles. This knowledge discovery approach was introduced in 1989 and remains a core task in text mining. Currently, the ABC principle-based two approaches, open discovery and closed discovery, are mostly explored in the LBD process.This review begins with a general introduction to text mining, followed by biomedical text mining and an overview of various literature resources, such as MEDLINE, UMLS, MESH, and SemMedDB. The review then introduces the core ABC principle and its associated two approaches in the LBD process, open discovery and closed discovery. Additionally, the review discusses the deep learning applications in LBD, including transformer models and neural network-based LBD models, and their future aspects.Finally, the review highlights key biomedical discoveries generated through LBD approaches in biomedicine and concludes with current limitations and future directions of LBD.Here is the Simplified Chinese translation of the text:生物医学知识在快速增长，大多数这些知识表现为科学文献。自动文本挖掘工具和方法可以从这些不结构化和无结构化数据中提取隐藏的模式和趋势。在生物医学文本挖掘中，文献基于发现（LBD）是自动发现医学术语之间的新关系的过程。LBD方法已经证明可以成功减少医学文献中潜在关系的发现时间。LBD过程中创建医学术语的概念profile，如疾病或症状，并将其与药物和治疗相连接，基于这些概念profile之间的统计学 significado。这种知识发现方法在1989年引入，并且至今仍然是文本挖掘中核心任务。目前，ABC原则基于的两种方法，开发发现和关闭发现，在LBD过程中得到广泛的探索。本文首先介绍了文本挖掘的概述，然后是生物医学文本挖掘，并提供了各种文献资源的概述，如MEDLINE、UMLS、MESH和SemMedDB。接着，文章介绍了ABC原则和其相关的两种方法，开发发现和关闭发现，在LBD过程中的应用。此外，文章还评论了深度学习在LBD中的应用，包括转换器模型和神经网络基于的LBD模型，以及其未来方向。最后，文章着重介绍了通过LBD方法在生物医学中发现的关键发现，并评估了当前LBD的限制和未来方向。
</details></li>
</ul>
<hr>
<h2 id="MIDDAG-Where-Does-Our-News-Go-Investigating-Information-Diffusion-via-Community-Level-Information-Pathways"><a href="#MIDDAG-Where-Does-Our-News-Go-Investigating-Information-Diffusion-via-Community-Level-Information-Pathways" class="headerlink" title="MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways"></a>MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02529">http://arxiv.org/abs/2310.02529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Derek Ma, Alexander K. Taylor, Nuan Wen, Yanchen Liu, Po-Nien Kung, Wenna Qin, Shicheng Wen, Azure Zhou, Diyi Yang, Xuezhe Ma, Nanyun Peng, Wei Wang</li>
<li>for: 这个研究是为了创建一个直观的社交媒体信息传播系统，用于探索COVID-19相关新闻文章在社交媒体上的信息传播路径，以及用户&#x2F;社区感染水平、群体响应和信息传播的预测能力。</li>
<li>methods: 这个研究使用了社交媒体数据分析技术和人工智能模型，挖掘了COVID-19相关新闻文章在社交媒体上的信息传播路径，并构建了用户群体和信息传播预测能力。</li>
<li>results: 研究发现，COVID-19相关新闻文章在社交媒体上的信息传播路径具有复杂的层次结构和分布特征，并且可以根据用户群体和信息传播预测能力进行预测和跟踪。<details>
<summary>Abstract</summary>
We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level.
</details>
<details>
<summary>摘要</summary>
我们介绍MIDDAG，一个直观互动的系统，可以视觉化COVID-19相关新闻文章所Trigger的信息宣传路径在社交媒体上。此外，我们还提供了用户/社区感染水平的全面报告，以及由群众宣传的事件和受欢迎的观点。我们不仅揭示用户之间信息流动的模式，还可以建立用户群体和宣传预测能力，以跟踪和理解信息如何在更高层次传播。
</details></li>
</ul>
<hr>
<h2 id="CITING-Large-Language-Models-Create-Curriculum-for-Instruction-Tuning"><a href="#CITING-Large-Language-Models-Create-Curriculum-for-Instruction-Tuning" class="headerlink" title="CITING: Large Language Models Create Curriculum for Instruction Tuning"></a>CITING: Large Language Models Create Curriculum for Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02527">http://arxiv.org/abs/2310.02527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Feng, Zifeng Wang, Jimeng Sun</li>
<li>for: 本研究旨在开发一种基于人工智能模型的自动化 instruktion 优化方法，以减少人工指导的成本，提高大语言模型的发展效率。</li>
<li>methods: 我们提出一种基于教师AI模型的 instruction 优化方法，即 Curriculum Instruction TunING (CITING)，它包括两个主要步骤：首先，教师AI模型制定评价答案的标准，然后学生AI模型通过自我修复来学习遵循这些标准。我们还进行了多 Iterative 实现 CITING。</li>
<li>results: 我们对四个数据集进行比较，结果表明，使用 CITING 方法可以大幅提高 GPT-4 评价中的报道、深度和全面性， Specifically, it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1% over RRHF, and 76.3% over RAFT, respectively.<details>
<summary>Abstract</summary>
The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to embody the procedure of CITING. We compare CITING to a series of state-of-the-art baselines on four datasets. Our method demonstrates strong improvement in terms of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically, it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1% over RRHF, and 76.3% over RAFT, respectively.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的最近进步得到了人工调教和人类对齐的组合。然而，建立手动制作的指导数据集和进行人类对齐成为了大量发展LLM的瓶颈。在这篇论文中，我们利用人类学生如何通过指南和老师的修改来练习写作技巧的想法，即使用AI模型来取代人类老师来训练学生LLM。我们的方法被称为学习指南调教（CITING）。它包括两个主要步骤：（1）教师LLM制定评估答案的评价标准，（2）学生LLM按照标准进行自修改。我们还在不断迭代执行CITING来体现程序。我们与一系列现有的基线比较CITING，在四个数据集上表现出了强大的改善。具体来说，它在SFT、RLHF、RRHF和RAFT等四个数据集上平均赢得了79.4%、73.4%、78.1%和76.3%的胜率。
</details></li>
</ul>
<hr>
<h2 id="Federated-Conditional-Stochastic-Optimization"><a href="#Federated-Conditional-Stochastic-Optimization" class="headerlink" title="Federated Conditional Stochastic Optimization"></a>Federated Conditional Stochastic Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02524">http://arxiv.org/abs/2310.02524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xidongwu/Federated-Minimax-and-Conditional-Stochastic-Optimization">https://github.com/xidongwu/Federated-Minimax-and-Conditional-Stochastic-Optimization</a></li>
<li>paper_authors: Xidong Wu, Jianhui Sun, Zhengmian Hu, Junyi Li, Aidong Zhang, Heng Huang</li>
<li>for: 本研究探讨了在分布式数据中进行非 convex conditional stochastic optimization的 federated learning 问题，并提出了首个基于 conditional stochastic gradient estimator 和 momentum 算法的 federated conditional stochastic optimization algorithm (FCSG)。</li>
<li>methods: 本研究使用了 conditional stochastic gradient estimator 和 momentum 算法，并通过 variance reduction 技术提高了加速器 (Acc-FCSG-M) 的性能。</li>
<li>results: 对于多种任务的实验结果表明，FCSG 和 Acc-FCSG-M 可以具有较好的样本和通信复杂度。<details>
<summary>Abstract</summary>
Conditional stochastic optimization has found applications in a wide range of machine learning tasks, such as invariant learning, AUPRC maximization, and meta-learning. As the demand for training models with large-scale distributed data grows in these applications, there is an increasing need for communication-efficient distributed optimization algorithms, such as federated learning algorithms. This paper considers the nonconvex conditional stochastic optimization in federated learning and proposes the first federated conditional stochastic optimization algorithm (FCSG) with a conditional stochastic gradient estimator and a momentum-based algorithm (FCSG-M). To match the lower bound complexity in the single-machine setting, we design an accelerated algorithm (Acc-FCSG-M) via the variance reduction to achieve the best sample and communication complexity. Compared with the existing optimization analysis for MAML in FL, federated conditional stochastic optimization considers the sample of tasks. Extensive experimental results on various tasks validate the efficiency of these algorithms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Conditional stochastic optimization在机器学习任务中广泛应用，如不变学习、AUPRC最大化和元学习。随着大规模分布式数据的训练模型需求增加，有增加通信效率的分布式优化算法的需求，如联合学习算法。本文考虑非конvex的conditional stochastic optimization在联合学习中，并提出首个联合conditional stochastic gradient估计和势量驱动算法（FCSG-M）。为实现单机设置下的下限Complexity，我们设计加速算法（Acc-FCSG-M）via variance reduction来实现最佳样本和通信复杂度。与现有的MAML在FL中的优化分析相比，联合conditional stochastic optimization考虑任务样本。经验性研究表明，这些算法在各种任务上具有高效性。
</details></li>
</ul>
<hr>
<h2 id="MedDiffusion-Boosting-Health-Risk-Prediction-via-Diffusion-based-Data-Augmentation"><a href="#MedDiffusion-Boosting-Health-Risk-Prediction-via-Diffusion-based-Data-Augmentation" class="headerlink" title="MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation"></a>MedDiffusion: Boosting Health Risk Prediction via Diffusion-based Data Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02520">http://arxiv.org/abs/2310.02520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Zhong, Suhan Cui, Jiaqi Wang, Xiaochen Wang, Ziyi Yin, Yaqing Wang, Houping Xiao, Mengdi Huai, Ting Wang, Fenglong Ma<br>for:这篇论文旨在提高医疗风险预测的效果，以便预测患者未来可能面临的健康风险。methods:这篇论文提出了一种新的扩充生成模型，名为MedDiffusion，它在训练过程中通过生成假患者数据来扩大样本空间，从而提高风险预测性能。此外，MedDiffusion还使用步进注意机制来检出患者访问记录中隐藏的关系，以便自动保留最重要的信息。results:对四个真实的医疗数据集进行了实验评估，显示MedDiffusion在PR-AUC、F1和科ienn的均值上都高于14种现代基线。此外，我们还进行了减少学习和对GAN-based模型的比较，以验证我们的模型设计的合理性和适应性。此外，我们还分析生成的数据，以提供新的解释力研究。<details>
<summary>Abstract</summary>
Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk prediction performance by creating synthetic patient data during training to enlarge sample space. Furthermore, MedDiffusion discerns hidden relationships between patient visits using a step-wise attention mechanism, enabling the model to automatically retain the most vital information for generating high-quality data. Experimental evaluation on four real-world medical datasets demonstrates that MedDiffusion outperforms 14 cutting-edge baselines in terms of PR-AUC, F1, and Cohen's Kappa. We also conduct ablation studies and benchmark our model against GAN-based alternatives to further validate the rationality and adaptability of our model design. Additionally, we analyze generated data to offer fresh insights into the model's interpretability.
</details>
<details>
<summary>摘要</summary>
医疗风险预测是医疗领域预测模型的基本任务之一，旨在预测患者未来可能面临的医疗风险使用医疗电子病历（EHR）的历史记录。研究人员已经开发了许多风险预测模型，以处理医疗数据的独特挑战，如其继承性、高维度和内在噪音。这些模型已经取得了很好的效果。然而，数据缺乏问题是这些模型的限制因素。为了解决这个问题，本文提出了一种新的、终端扩散基于的医疗风险预测模型，名为MedDiffusion。它通过在训练时创建 sintetic 患者数据来扩大样本空间，提高风险预测性能。此外，MedDiffusion 通过步骤 wise 注意机制，了解患者访问记录中隐藏的关系，使模型自动保留最重要的信息，以生成高质量数据。实验证明，MedDiffusion 在四个真实的医疗数据集上表现出色，至少在PR-AUC、F1和科恩的卡方面比14种高级基准模型更好。我们还进行了剖析研究和对 GAN 基于的相关模型进行比较，以验证我们的模型设计的合理性和适应性。此外，我们还分析生成的数据，提供新的解释力。
</details></li>
</ul>
<hr>
<h2 id="Proactive-Human-Robot-Interaction-using-Visuo-Lingual-Transformers"><a href="#Proactive-Human-Robot-Interaction-using-Visuo-Lingual-Transformers" class="headerlink" title="Proactive Human-Robot Interaction using Visuo-Lingual Transformers"></a>Proactive Human-Robot Interaction using Visuo-Lingual Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02506">http://arxiv.org/abs/2310.02506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranay Mathur</li>
<li>for: 本研究旨在将人类与机器人合作中的人类对话中的隐藏性视觉语言决策转化为机器人，以提高人机合作的效率和自然性。</li>
<li>methods: 本研究提出了一种基于视觉语言多模态变换器的学习方法，可以Capture scene dependencies and proactively suggest tasks based on the user’s intention.</li>
<li>results: 在模拟和实际场景中，提出的方法能够准确地描述场景和建议任务，提高了人机合作的效率和自然性。<details>
<summary>Abstract</summary>
Humans possess the innate ability to extract latent visuo-lingual cues to infer context through human interaction. During collaboration, this enables proactive prediction of the underlying intention of a series of tasks. In contrast, robotic agents collaborating with humans naively follow elementary instructions to complete tasks or use specific hand-crafted triggers to initiate proactive collaboration when working towards the completion of a goal. Endowing such robots with the ability to reason about the end goal and proactively suggest intermediate tasks will engender a much more intuitive method for human-robot collaboration. To this end, we propose a learning-based method that uses visual cues from the scene, lingual commands from a user and knowledge of prior object-object interaction to identify and proactively predict the underlying goal the user intends to achieve. Specifically, we propose ViLing-MMT, a vision-language multimodal transformer-based architecture that captures inter and intra-modal dependencies to provide accurate scene descriptions and proactively suggest tasks where applicable. We evaluate our proposed model in simulation and real-world scenarios.
</details>
<details>
<summary>摘要</summary>
人类具有内生的能力，可以从视觉语言cue中提取潜在的上下文信息，通过人类交互来进行推理。在合作中，这使得人类可以前置预测任务的目标意图。相比之下，机器人在合作时，可能只是遵循基本的指令来完成任务，或者使用特定的手工触发器来引起前置的合作。为了提高人机合作的效果，我们提出了一种学习基于的方法，使用场景中的视觉cue、用户的语言命令和对象之间的互动知识来识别和预测用户的目标意图。我们提出了一种视语多模态变换器（ViLing-MMT），它可以捕捉场景中的视觉和语言之间的依赖关系，并提供准确的场景描述和适时提示任务。我们在模拟和实际场景中评估了我们的提议模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/04/cs.AI_2023_10_04/" data-id="closbroks00510g88912sci24" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/04/cs.CL_2023_10_04/" class="article-date">
  <time datetime="2023-10-04T11:00:00.000Z" itemprop="datePublished">2023-10-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/04/cs.CL_2023_10_04/">cs.CL - 2023-10-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Can-Language-Models-Employ-the-Socratic-Method-Experiments-with-Code-Debugging"><a href="#Can-Language-Models-Employ-the-Socratic-Method-Experiments-with-Code-Debugging" class="headerlink" title="Can Language Models Employ the Socratic Method? Experiments with Code Debugging"></a>Can Language Models Employ the Socratic Method? Experiments with Code Debugging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03210">http://arxiv.org/abs/2310.03210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taisazero/socratic-debugging-benchmark">https://github.com/taisazero/socratic-debugging-benchmark</a></li>
<li>paper_authors: Erfan Al-Hossami, Razvan Bunescu, Justin Smith, Ryan Teehan</li>
<li>for: 本研究旨在开发一个自动化的索引教学机器人，以帮助新手程序员 debug 缺陷的解决方案。</li>
<li>methods: 本研究使用了 manually created dataset of multi-turn Socratic advice，并使用了多种语言模型进行评估，包括 Flan-T5 和 GPT-4。</li>
<li>results: 研究发现，使用自动化索引教学机器人可以提高学习效果，但是需要更多的数据和评估方法来进一步改进。Note: “索引教学” (Socratic teaching) refers to a teaching method that guides students towards solving problems on their own, rather than providing the solution directly.<details>
<summary>Abstract</summary>
When employing the Socratic method of teaching, instructors guide students toward solving a problem on their own rather than providing the solution directly. While this strategy can substantially improve learning outcomes, it is usually time-consuming and cognitively demanding. Automated Socratic conversational agents can augment human instruction and provide the necessary scale, however their development is hampered by the lack of suitable data for training and evaluation. In this paper, we introduce a manually created dataset of multi-turn Socratic advice that is aimed at helping a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and chain of thought prompting of the much larger GPT-4. The code and datasets are made freely available for research at the link below. https://github.com/taisazero/socratic-debugging-benchmark
</details>
<details>
<summary>摘要</summary>
使用索底里亚方法教学时，教师会导学生解决问题而不直接提供解决方案。这种策略可以大幅提高学习效果，但是它通常需要较长的时间和更多的认知努力。自动化索底里亚对话代理可以增强人类教学，但是它们的开发受到数据集的限制。在这篇论文中，我们介绍了一个手动创建的多轮索底里亚建议数据集，用于帮助新手程序员修复buggy解决方案。这个数据集后来用于评估一些语言模型的索底里亚调试能力，包括练习基于文本的文本转换器Flan-T5的特点调试，以及GPT-4的零损环境和链接思维提示。代码和数据集均为研究用途而免费提供，可以在以下链接下载：https://github.com/taisazero/socratic-debugging-benchmark。
</details></li>
</ul>
<hr>
<h2 id="The-Rise-of-Open-Science-Tracking-the-Evolution-and-Perceived-Value-of-Data-and-Methods-Link-Sharing-Practices"><a href="#The-Rise-of-Open-Science-Tracking-the-Evolution-and-Perceived-Value-of-Data-and-Methods-Link-Sharing-Practices" class="headerlink" title="The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices"></a>The Rise of Open Science: Tracking the Evolution and Perceived Value of Data and Methods Link-Sharing Practices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03193">http://arxiv.org/abs/2310.03193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caohanch/paper_data_method_sharing">https://github.com/caohanch/paper_data_method_sharing</a></li>
<li>paper_authors: Hancheng Cao, Jesse Dodge, Kyle Lo, Daniel A. McFarland, Lucy Lu Wang</li>
<li>for: 本研究旨在分析arXiv上的1.1万篇论文，探讨数据和方法共享实践的时间发展和其影响article reception。</li>
<li>methods: 本研究使用了一种神经网络文本分类模型，自动将URL类型分类为基于文章中的上下文提及。</li>
<li>results: 研究发现，在physics、math和computer science领域，数据和方法共享实践在时间上逐渐普及，文章中包含链接的数量也在增加。同时，这些链接的重复使用也在增加，特别是在计算机科学领域。此外，研究还发现，分享数据和方法链接的文章会受到更多的引用，活动链接的影响更加明显。<details>
<summary>Abstract</summary>
In recent years, funding agencies and journals increasingly advocate for open science practices (e.g. data and method sharing) to improve the transparency, access, and reproducibility of science. However, quantifying these practices at scale has proven difficult. In this work, we leverage a large-scale dataset of 1.1M papers from arXiv that are representative of the fields of physics, math, and computer science to analyze the adoption of data and method link-sharing practices over time and their impact on article reception. To identify links to data and methods, we train a neural text classification model to automatically classify URL types based on contextual mentions in papers. We find evidence that the practice of link-sharing to methods and data is spreading as more papers include such URLs over time. Reproducibility efforts may also be spreading because the same links are being increasingly reused across papers (especially in computer science); and these links are increasingly concentrated within fewer web domains (e.g. Github) over time. Lastly, articles that share data and method links receive increased recognition in terms of citation count, with a stronger effect when the shared links are active (rather than defunct). Together, these findings demonstrate the increased spread and perceived value of data and method sharing practices in open science.
</details>
<details>
<summary>摘要</summary>
近年来，资金机构和学术刊物 increasingly 强调开放科学实践（例如数据和方法分享），以提高科学的透明度、访问性和重复性。然而，规模化这些实践的量化仍然是一个挑战。在这项工作中，我们利用 arXiv 上的 1.1 万篇论文数据，这些论文代表物理、数学和计算机科学领域，以分析时间的推广和影响。为了识别数据和方法链接，我们使用 neural 网络文本分类模型，自动根据论文中的上下文提取 URL 类型。我们发现，在更多的论文中包含链接的做法在时间上扩散，并且这些链接在论文之间的重复使用也在增长（尤其在计算机科学领域）。此外，这些链接在时间的推移中变得更加集中在 fewer web 域（如 Github）。最后，分享数据和方法链接的论文会 receiving 更多的引用，其中活动链接的影响更加强大。总之，这些发现表明开放科学实践的扩散和价值的增加。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-augmented-Generation-to-Improve-Math-Question-Answering-Trade-offs-Between-Groundedness-and-Human-Preference"><a href="#Retrieval-augmented-Generation-to-Improve-Math-Question-Answering-Trade-offs-Between-Groundedness-and-Human-Preference" class="headerlink" title="Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference"></a>Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03184">http://arxiv.org/abs/2310.03184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/digitalharborfoundation/rag-for-math-qa">https://github.com/digitalharborfoundation/rag-for-math-qa</a></li>
<li>paper_authors: Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka Gade, Owen Henkel, Millie-Ellen Postle, Wanli Xing</li>
<li>for: 该论文主要目标是提高自动化辅导过程中的交互问答体验，以帮助中学生学习数学。</li>
<li>methods: 该论文使用了生成大型自然语言模型（LLM）和检索增强生成（RAG）技术，通过在LLM提问中包含证明的外部知识来提高响应质量。</li>
<li>results: 根据多种条件的评估， authors发现了RAG系统可以提高中学生数学问答的响应质量，但是设计者需要考虑在生成响应时与教学资源的匹配程度和学生喜好之间的平衡。<details>
<summary>Abstract</summary>
For middle-school math students, interactive question-answering (QA) with tutors is an effective way to learn. The flexibility and emergent capabilities of generative large language models (LLMs) has led to a surge of interest in automating portions of the tutoring process - including interactive QA to support conceptual discussion of mathematical concepts. However, LLM responses to math questions can be incorrect or mismatched to the educational context - such as being misaligned with a school's curriculum. One potential solution is retrieval-augmented generation (RAG), which involves incorporating a vetted external knowledge source in the LLM prompt to increase response quality. In this paper, we designed prompts that retrieve and use content from a high-quality open-source math textbook to generate responses to real student questions. We evaluate the efficacy of this RAG system for middle-school algebra and geometry QA by administering a multi-condition survey, finding that humans prefer responses generated using RAG, but not when responses are too grounded in the textbook content. We argue that while RAG is able to improve response quality, designers of math QA systems must consider trade-offs between generating responses preferred by students and responses closely matched to specific educational resources.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-and-Interpretable-Medical-Image-Classifiers-via-Concept-Bottleneck-Models"><a href="#Robust-and-Interpretable-Medical-Image-Classifiers-via-Concept-Bottleneck-Models" class="headerlink" title="Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models"></a>Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03182">http://arxiv.org/abs/2310.03182</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Yan, Yu Wang, Yiwu Zhong, Zexue He, Petros Karypis, Zihan Wang, Chengyu Dong, Amilcare Gentili, Chun-Nan Hsu, Jingbo Shang, Julian McAuley</li>
<li>for: 这篇论文旨在提出一种新的医疗影像分类方法，以提高医疗影像分类模型的稳定性和可解释性。</li>
<li>methods: 这篇论文提出了一种使用自然语言概念来建构医疗影像分类模型的新方法。具体来说，首先从 GPT-4 中获取临床概念，然后使用视觉语言模型将潜在的影像特征转换为明确的概念。</li>
<li>results: 这篇论文系统地评估了其方法在八个医疗影像分类 datasets 上的效果，并证明其能够对抗潜在的干扰因素，并与标准的视觉嵌入oder 和其他基elines 相比，具有优秀的性能。此外，这篇论文还通过实际应用中的实例研究，详细介绍了这种方法在医疗影像分类中的解释性。<details>
<summary>Abstract</summary>
Medical image classification is a critical problem for healthcare, with the potential to alleviate the workload of doctors and facilitate diagnoses of patients. However, two challenges arise when deploying deep learning models to real-world healthcare applications. First, neural models tend to learn spurious correlations instead of desired features, which could fall short when generalizing to new domains (e.g., patients with different ages). Second, these black-box models lack interpretability. When making diagnostic predictions, it is important to understand why a model makes a decision for trustworthy and safety considerations. In this paper, to address these two limitations, we propose a new paradigm to build robust and interpretable medical image classifiers with natural language concepts. Specifically, we first query clinical concepts from GPT-4, then transform latent image features into explicit concepts with a vision-language model. We systematically evaluate our method on eight medical image classification datasets to verify its effectiveness. On challenging datasets with strong confounding factors, our method can mitigate spurious correlations thus substantially outperform standard visual encoders and other baselines. Finally, we show how classification with a small number of concepts brings a level of interpretability for understanding model decisions through case studies in real medical data.
</details>
<details>
<summary>摘要</summary>
医疗图像分类是医疗领域的关键问题，有助于减轻医生的工作负担并促进病人的诊断。然而，在实际应用深度学习模型时，存在两个挑战。首先，神经网络模型往往学习潦上欺诈的相关性而不是需要的特征，这可能会导致在新领域（例如不同年龄的病人）中generalization异常。第二，这些黑盒模型缺乏可读性。在作出诊断时，理解模型为什么做出了决定非常重要，以确保信任和安全考虑。在这篇论文中，我们提出了一种新的方法，用于建立坚实和可读的医疗图像分类器，基于自然语言概念。具体来说，我们首先从GPT-4中查询临床概念，然后将潜在的图像特征转换成显式的概念使用视力语言模型。我们系统地评估我们的方法在八个医疗图像分类 dataset 上，以验证其效果。在具有强调因素的 dataset 上，我们的方法可以减少潦上欺诈，因此与标准视觉编码器和其他基线之间具有显著性能优势。最后，我们通过实际医疗数据的 caso 研究，示出分类器使用少量概念可以提供一定的可读性，以便理解模型做出的决定。
</details></li>
</ul>
<hr>
<h2 id="mathcal-B-Coder-Value-Based-Deep-Reinforcement-Learning-for-Program-Synthesis"><a href="#mathcal-B-Coder-Value-Based-Deep-Reinforcement-Learning-for-Program-Synthesis" class="headerlink" title="$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis"></a>$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03173">http://arxiv.org/abs/2310.03173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, Hongxia Yang</li>
<li>For:  This paper focuses on program synthesis, which aims to generate accurate and executable code from natural language descriptions. The authors explore the use of reinforcement learning (RL) and large language models (LLMs) to enhance code generation capabilities.* Methods:  The authors propose a value-based approach to program synthesis, which differs from the predominant policy-based methods. They develop a novel RL agent called $\mathcal{B}$-Coder, which leverages pre-trained LLMs and a conservative Bellman operator to reduce training complexities.* Results:  The authors demonstrate the effectiveness of their approach through empirical evaluations, achieving state-of-the-art performance compared to policy-based methods. Notably, this achievement is reached with minimal reward engineering effort, highlighting the effectiveness of value-based RL.<details>
<summary>Abstract</summary>
Program synthesis aims to create accurate, executable code from natural language descriptions. This field has leveraged the power of reinforcement learning (RL) in conjunction with large language models (LLMs), significantly enhancing code generation capabilities. This integration focuses on directly optimizing functional correctness, transcending conventional supervised losses. While current literature predominantly favors policy-based algorithms, attributes of program synthesis suggest a natural compatibility with value-based methods. This stems from rich collection of off-policy programs developed by human programmers, and the straightforward verification of generated programs through automated unit testing (i.e. easily obtainable rewards in RL language). Diverging from the predominant use of policy-based algorithms, our work explores the applicability of value-based approaches, leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis. To this end, we propose an initialization protocol for RL agents utilizing pre-trained LMs and a conservative Bellman operator to reduce training complexities. Moreover, we demonstrate how to leverage the learned value functions as a dual strategy to post-process generated programs. Our empirical evaluations demonstrated $\mathcal{B}$-Coder's capability in achieving state-of-the-art performance compared with policy-based methods. Remarkably, this achievement is reached with minimal reward engineering effort, highlighting the effectiveness of value-based RL, independent of reward designs.
</details>
<details>
<summary>摘要</summary>
（简化中文）Program synthesis 目标是从自然语言描述中生成正确可执行代码。这个领域通过结合大型自然语言模型（LLM）和强化学习（RL），提高了代码生成能力。这种集成关注直接优化功能正确性，超越传统的监督损失。当前文献主要倾向于使用策略型算法，但是程序生成的特点表明值型算法具有天然的相容性。这是因为人工程序员开发的庞大范围内的偏离策略，以及通过自动单元测试（RL语言中的容易获得奖励）直接验证生成的程序。在政策型算法的主导下，我们的工作探索了值型方法的可行性，并开发了我们的 Bellman 编程器（简称 Bellman 编程器）。然而，训练值型方法存在巨大的搜索空间问题，为此，我们提出了使用预训练 LLM 和保守的 Bellman 算子来降低训练复杂性。此外，我们还示出了如何利用学习到的值函数作为双重策略来后处生成的程序。我们的实验证明了 Bellman 编程器 可以达到与政策型算法相同或更高的性能，并且减少了奖励工程学的努力。这种成就表明了值型 RL 的有效性，不需要奖励设计。
</details></li>
</ul>
<hr>
<h2 id="MetaTool-Benchmark-for-Large-Language-Models-Deciding-Whether-to-Use-Tools-and-Which-to-Use"><a href="#MetaTool-Benchmark-for-Large-Language-Models-Deciding-Whether-to-Use-Tools-and-Which-to-Use" class="headerlink" title="MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use"></a>MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03128">http://arxiv.org/abs/2310.03128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/howiehwong/metatool">https://github.com/howiehwong/metatool</a></li>
<li>paper_authors: Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, Lichao Sun<br>for: This paper aims to evaluate the tool usage awareness and selection ability of large language models (LLMs) in various scenarios, with the goal of determining whether LLMs can effectively serve as intelligent agents.methods: The authors create a benchmark called MetaTool, which includes a dataset called ToolE that contains various user queries in the form of prompts that trigger LLMs to use tools. They define four subtasks for tool selection and conduct experiments involving nine popular LLMs.results: The majority of the LLMs struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through error analysis, the authors found significant room for improvement. The paper provides insights for tool developers to enhance the tool selection performance of LLMs.Here is the simplified Chinese text:for: 这篇论文目的是评估大语言模型（LLMs）在不同场景下是否具备工具使用意识和选择能力，以验证LLMs是否能够成为智能代理。methods: 作者们创建了一个名为MetaTool的benchmark，包括一个名为ToolE的数据集，该数据集包含各种用户查询，通过让LLMs使用工具来触发。他们定义了四个工具选择任务，包括相似选择、特定场景选择、可能可靠性问题选择和多工具选择。results: 大多数LLMs仍然困难地选择工具，这反映了现有的差距。但是通过错误分析，作者们发现还有很大的改进空间。 paper提供了工具开发者可以遵循ChatGPT的细节描述，以提高LLMs的工具选择性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving nine popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers that follow ChatGPT to provide detailed descriptions that can enhance the tool selection performance of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在近期引起了广泛关注，因为它们在自然语言处理（NLP）能力方面表现出色。最近，许多研究专注于 LLMS 的工具使用能力。他们主要探讨了 LLMS 如何与特定工具合作。但在应用程序中，LLMS 扮演智能代理的情况下，它们需要进行复杂的决策过程，包括是否使用工具和选择适合用户需求的最佳工具。因此，在这篇研究中，我们提出了 MetaTool，一个用于评估 LLMS 是否具有工具使用意识和正确选择工具的对benchmark。具体来说，我们创建了一个名为 ToolE 的 dataset。这个 dataset 包含了各种用户请求的形式，将 LLMS 触发使用工具，包括单一工具和多工具的情况。接着，我们设定了工具使用意识和工具选择的四个任务，包括工具选择与相似选择、特定情况下的工具选择、可能存在可靠性问题下的工具选择、以及多工具选择。我们对九个流行的 LLMs 进行了实验，发现大多数 LLMS 仍然对工具选择产生问题，这显示了现有的 gap  между LLMS 和真正的智能代理。但是，通过错误分析，我们发现仍然有很大的改善空间。最后，我们提出了对 ChatGPT 的工具开发者的启示，以帮助提高 LLMS 的工具选择性能。
</details></li>
</ul>
<hr>
<h2 id="Zero-Resource-Code-switched-Speech-Benchmark-Using-Speech-Utterance-Pairs-For-Multiple-Spoken-Languages"><a href="#Zero-Resource-Code-switched-Speech-Benchmark-Using-Speech-Utterance-Pairs-For-Multiple-Spoken-Languages" class="headerlink" title="Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages"></a>Zero Resource Code-switched Speech Benchmark Using Speech Utterance Pairs For Multiple Spoken Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03018">http://arxiv.org/abs/2310.03018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan-Po Huang, Chih-Kai Yang, Yu-Kuan Fu, Ewan Dunbar, Hung-yi Lee</li>
<li>for: 本研究旨在直接评估无参量代码混合语音编码器的代码混合能力。</li>
<li>methods: 我们使用语言模型化 discrete units 作为基线系统，以评估无参量代码混合语音编码器的代码混合能力。</li>
<li>results: 我们的实验涵盖了多种知名的语音编码器，包括 Wav2vec 2.0、HuBERT 等。我们发现，使用多语言预训练（如 XLSR）的编码器在代码混合场景下表现较好，但 ainda 有很大的改进空间以提高其代码混合语言能力。<details>
<summary>Abstract</summary>
We introduce a new zero resource code-switched speech benchmark designed to directly assess the code-switching capabilities of self-supervised speech encoders. We showcase a baseline system of language modeling on discrete units to demonstrate how the code-switching abilities of speech encoders can be assessed in a zero-resource manner. Our experiments encompass a variety of well-known speech encoders, including Wav2vec 2.0, HuBERT, XLSR, etc. We examine the impact of pre-training languages and model size on benchmark performance. Notably, though our results demonstrate that speech encoders with multilingual pre-training, exemplified by XLSR, outperform monolingual variants (Wav2vec 2.0, HuBERT) in code-switching scenarios, there is still substantial room for improvement in their code-switching linguistic abilities.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的零资源代码换语音benchmark，用于直接评估自动学习语音编码器的代码换语能力。我们展示了一个基线系统，利用语言模型在粒度单位上进行语言模型化，以示 zero-resource 的方式评估代码换语言编码器的能力。我们的实验包括了许多常见的语音编码器，如 Wav2vec 2.0、HuBERT 和 XLSR 等。我们研究了预训练语言和模型大小对 benchmark 性能的影响。结果显示，使用多语言预训练的 XLSR 在代码换语言场景中表现出色，超过单语言变体（Wav2vec 2.0、HuBERT）的表现。然而，我们还发现在代码换语言能力方面，这些编码器仍然有很大的改进空间。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Question-Answering-for-Unified-Information-Extraction"><a href="#Multimodal-Question-Answering-for-Unified-Information-Extraction" class="headerlink" title="Multimodal Question Answering for Unified Information Extraction"></a>Multimodal Question Answering for Unified Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03017">http://arxiv.org/abs/2310.03017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/osu-nlp-group/mqa">https://github.com/osu-nlp-group/mqa</a></li>
<li>paper_authors: Yuxuan Sun, Kai Zhang, Yu Su</li>
<li>for: 这个论文的目的是提出一种多Modal信息提取（MIE）框架，以解决现有MIE模型在不同任务和设置下的限制性，提高实际世界中的多Modal任务执行能力。</li>
<li>methods: 该框架基于一种多Modal问答（MQA）pipeline，将MIE任务转化为一种span检索和多选问答任务。</li>
<li>results: 对六个数据集进行了广泛的实验，显示了我们的MQA框架可以在不同的任务和设置下，有效地提高多Modal大模型（LMM）的性能，并在零shot设置下，与前一代基elines比较得到了大幅度的提升。<details>
<summary>Abstract</summary>
Multimodal information extraction (MIE) aims to extract structured information from unstructured multimedia content. Due to the diversity of tasks and settings, most current MIE models are task-specific and data-intensive, which limits their generalization to real-world scenarios with diverse task requirements and limited labeled data. To address these issues, we propose a novel multimodal question answering (MQA) framework to unify three MIE tasks by reformulating them into a unified span extraction and multi-choice QA pipeline. Extensive experiments on six datasets show that: 1) Our MQA framework consistently and significantly improves the performances of various off-the-shelf large multimodal models (LMM) on MIE tasks, compared to vanilla prompting. 2) In the zero-shot setting, MQA outperforms previous state-of-the-art baselines by a large margin. In addition, the effectiveness of our framework can successfully transfer to the few-shot setting, enhancing LMMs on a scale of 10B parameters to be competitive or outperform much larger language models such as ChatGPT and GPT-4. Our MQA framework can serve as a general principle of utilizing LMMs to better solve MIE and potentially other downstream multimodal tasks.
</details>
<details>
<summary>摘要</summary>
多Modal信息提取（MIE）的目标是从不结构化多媒体内容中提取结构化信息。由于任务和设置的多样性，当前大多数MIE模型是任务特定和数据耗费的，这限制了它们在真实世界情况下的普适性。为解决这些问题，我们提议一种多Modal问答（MQA）框架，将MIE任务转化为一个统一的Span抽取和多选问答管道。广泛的实验表明：1. 我们的MQA框架在不同的 dataset上 consistently 和 statistically 改进了多种 Off-the-shelf 大型多Modal模型（LMM）的MIE任务性能，比 vanilla prompting 更好。2. 在零shot设定下，MQA 超越了之前的基eline。此外，我们的框架的效果可以成功传承到几个 shot 设定下，使用10B参数的LMM在一个竞争或超越大型语言模型such as ChatGPT和GPT-4。3. 我们的MQA框架可以作为使用LMM解决MIE和其他下游多Modal任务的一般原则。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Understanding-In-Context-Learning-in-Transformers-and-LLMs-by-Learning-to-Learn-Discrete-Functions"><a href="#Understanding-In-Context-Learning-in-Transformers-and-LLMs-by-Learning-to-Learn-Discrete-Functions" class="headerlink" title="Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions"></a>Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03016">http://arxiv.org/abs/2310.03016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satwik Bhattamishra, Arkil Patel, Phil Blunsom, Varun Kanade</li>
<li>for: 本研究旨在解释受欢迎学习现象，并证明transformer可以学习梯度基于学习算法。</li>
<li>methods: 本研究使用了一种标准化的实验框架，并使用transformer模型来学习梯度基于学习算法。</li>
<li>results: 研究发现，transformer可以在一些简单的任务上nearly匹配最佳学习算法，但在更复杂的任务上表现下降。此外，一些没有注意力的模型也可以与transformer相似的性能。在提供教学序列后，transformer可以更加sample-efficiently学习。最后，研究发现，现有的LLMs可以与基于 nearest-neighbor的基准值竞争。<details>
<summary>Abstract</summary>
In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can learn gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a teaching sequence, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement two distinct algorithms to solve a single task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.
</details>
<details>
<summary>摘要</summary>
为了理解Contextual Learning现象，latest works启用了彩绘的实验方案，并证明了Transformers可以学习Gradient-based learning算法 для不同类型的实数函数。然而，Transformers在实现学习算法方面的局限性和其他类型的算法学习能力还不够清楚。另外，关注型模型是否能够学习其他类型的算法的能力也不了解。此外，这些发现是否可以推广到预训练的Large Language Models（LLMs）还需要进一步研究。在这个工作中，我们通过以下方式回答了这些问题：(a) 我们在一个包含多种布尔函数类型的测试环境中发现，Transformers可以在简单任务上几乎与最佳学习算法匹配，而在更复杂任务上，其性能会下降。此外，我们发现某些无关注意力模型在一系列任务上表现几乎与Transformers一样。(b) 当给Transformers一个教学序列，即一组唯一标识函数类型的示例，我们发现Transformers可以更加效率地学习。有趣的是，我们发现Transformers可以学习并实现两种不同的算法来解决同一个任务，并可以根据示例序列选择更加sample-efficient的算法。(c) 最后，我们发现现有的LLMs，如LLaMA-2和GPT-4，可以与 nearest-neighbor baselines竞争在不在其训练集中的预测任务上。
</details></li>
</ul>
<hr>
<h2 id="From-Words-to-Watts-Benchmarking-the-Energy-Costs-of-Large-Language-Model-Inference"><a href="#From-Words-to-Watts-Benchmarking-the-Energy-Costs-of-Large-Language-Model-Inference" class="headerlink" title="From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference"></a>From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03003">http://arxiv.org/abs/2310.03003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, Vijay Gadepally</li>
<li>for: 这个论文旨在研究大型自然语言模型（LLMs）的执行计算和能源利用情况。</li>
<li>methods: 这篇论文使用了Meta AI开发的LLaMA模型，并在NVIDIA V100和A100两代GPUS以及Alpaca和GSM8K两个数据集上进行了多节点多GPU的推理性能和能源消耗测试。</li>
<li>results: 论文的结果表明，LLaMA模型在不同的GPU和数据集下的推理性能和能源消耗存在很大差异。通过模型分割技术，可以在多达32个GPU上进行并发推理，从而提高性能和降低能源消耗。<details>
<summary>Abstract</summary>
Large language models (LLMs) have exploded in popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.   In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta AI on two generations of popular GPUs (NVIDIA V100 \& A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is the one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在Popularity due to their new generative capabilities that go far beyond prior state-of-the-art. These technologies are increasingly being leveraged in various domains such as law, finance, and medicine. However, these models carry significant computational challenges, especially the compute and energy costs required for inference. Inference energy costs already receive less attention than the energy costs of training LLMs -- despite how often these large models are called on to conduct inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see increasing usage and deployment in various domains, a better understanding of their resource utilization is crucial for cost-savings, scaling performance, efficient hardware usage, and optimal inference strategies.  In this paper, we describe experiments conducted to study the computational and energy utilization of inference with LLMs. We benchmark and conduct a preliminary analysis of the inference performance and inference energy costs of different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta AI on two generations of popular GPUs (NVIDIA V100 & A100) and two datasets (Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in research and practice. We present the results of multi-node, multi-GPU inference using model sharding across up to 32 GPUs. To our knowledge, our work is one of the first to study LLM inference performance from the perspective of computational and energy resources at this scale.
</details></li>
</ul>
<hr>
<h2 id="Kosmos-G-Generating-Images-in-Context-with-Multimodal-Large-Language-Models"><a href="#Kosmos-G-Generating-Images-in-Context-with-Multimodal-Large-Language-Models" class="headerlink" title="Kosmos-G: Generating Images in Context with Multimodal Large Language Models"></a>Kosmos-G: Generating Images in Context with Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02992">http://arxiv.org/abs/2310.02992</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/unilm/tree/master/kosmos-g">https://github.com/microsoft/unilm/tree/master/kosmos-g</a></li>
<li>paper_authors: Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei</li>
<li>for: 该论文旨在解决通用视语言模型（MLLM）的高级识别能力来生成多个图像。</li>
<li>methods: 该论文提出了一种基于文本模式的启发式调教方法，将文本模式与CLIP的输出空间对齐，并通过抽象指令调教来实现多元实体驱动生成。</li>
<li>results: 该论文显示了 zero-shot 多实体驱动生成的能力，而且不需要修改图像解码器。这使得可以轻松地替换CLIP并集成多种 U-Net 技术，从细化控制到个性化图像解码器。<details>
<summary>Abstract</summary>
Recent advancements in text-to-image (T2I) and vision-language-to-image (VL2I) generation have made significant strides. However, the generation from generalized vision-language inputs, especially involving multiple images, remains under-explored. This paper presents Kosmos-G, a model that leverages the advanced perception capabilities of Multimodal Large Language Models (MLLMs) to tackle the aforementioned challenge. Our approach aligns the output space of MLLM with CLIP using the textual modality as an anchor and performs compositional instruction tuning on curated data. Kosmos-G demonstrates a unique capability of zero-shot multi-entity subject-driven generation. Notably, the score distillation instruction tuning requires no modifications to the image decoder. This allows for a seamless substitution of CLIP and effortless integration with a myriad of U-Net techniques ranging from fine-grained controls to personalized image decoder variants. We posit Kosmos-G as an initial attempt towards the goal of "image as a foreign language in image generation."
</details>
<details>
<summary>摘要</summary>
最近的文本到图像（T2I）和视觉语言到图像（VL2I）生成技术已经取得了 significiant 进步。然而，从通用视觉语言输入开始生成，特别是包含多个图像的情况，仍然是未explored 领域。本文提出了 Kosmos-G 模型，利用多modal大语言模型（MLLMs）的高级见解能力来解决上述挑战。我们的方法将 MLLM 的输出空间与 CLIP 进行了对应，并通过文本modalities 进行了 compositional instruction tuning 的 curated 数据。Kosmos-G 表现了一种无需修改图像解码器的零shot 多实体主题驱动生成能力。审查 instruction tuning 不需要修改图像解码器，这使得可以顺利地替换 CLIP 并轻松地与多种 U-Net 技术结合，从精细控制到个性化图像解码器。我们认为 Kosmos-G 是对 "图像为外语在图像生成" 的初步尝试。
</details></li>
</ul>
<hr>
<h2 id="Never-Train-from-Scratch-Fair-Comparison-of-Long-Sequence-Models-Requires-Data-Driven-Priors"><a href="#Never-Train-from-Scratch-Fair-Comparison-of-Long-Sequence-Models-Requires-Data-Driven-Priors" class="headerlink" title="Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors"></a>Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02980">http://arxiv.org/abs/2310.02980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ido Amos, Jonathan Berant, Ankit Gupta</li>
<li>for: 这篇论文的目的是为了评估不同架构在监督任务上的表现，并评估 Random initialization 对模型性能的影响。</li>
<li>methods: 该论文使用了标准的denoising objective来预训练模型，使用了downstream task的数据来进行预训练。</li>
<li>results: 该论文发现，通过预训练，vanilla Transformer 可以与 State Space Model （SSM）匹配在 Long Range Arena 上的表现，并在 PathX-256 任务上提高 SSM 的最佳记录Result by 20 个绝对点。此外，论文还发现，在带有数据驱动初始化的情况下，之前提出的结构化参数化方法对 SSM became redundant。<details>
<summary>Abstract</summary>
Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.
</details>
<details>
<summary>摘要</summary>
In contrast to prior works, we find that vanilla Transformers can match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Furthermore, we analyze the utility of previously proposed structured parameterizations for SSMs and show that they become redundant in the presence of data-driven initialization obtained through pretraining. Our work demonstrates that incorporating data-driven priors via pretraining is essential for reliable performance estimation when evaluating different architectures on supervised tasks, and can be done efficiently.
</details></li>
</ul>
<hr>
<h2 id="T-3-Bench-Benchmarking-Current-Progress-in-Text-to-3D-Generation"><a href="#T-3-Bench-Benchmarking-Current-Progress-in-Text-to-3D-Generation" class="headerlink" title="T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation"></a>T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02977">http://arxiv.org/abs/2310.02977</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THU-LYJ-Lab/T3Bench">https://github.com/THU-LYJ-Lab/T3Bench</a></li>
<li>paper_authors: Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, Yong-Jin Liu</li>
<li>for: 这篇论文目的是为了提出一个全面的文本到3D测试集，以评估当前文本到3D模型的进步。</li>
<li>methods: 这篇论文使用了强大预训 diffusion 模型来优化 NeRF，并可以生成高质量的3D场景无需训练3D数据。</li>
<li>results: 这篇论文提出了一个名为 T$^3$Bench 的全面的文本到3D测试集，并提出了两种自动度量器来评估文本到3D模型的性能，即多视图图像评分和文本-3D一致度评估。这两种度量器与人类评价有高度相关，可以有效地评估文本到3D模型的性能。<details>
<summary>Abstract</summary>
Recent methods in text-to-3D leverage powerful pretrained diffusion models to optimize NeRF. Notably, these methods are able to produce high-quality 3D scenes without training on 3D data. Due to the open-ended nature of the task, most studies evaluate their results with subjective case studies and user experiments, thereby presenting a challenge in quantitatively addressing the question: How has current progress in Text-to-3D gone so far? In this paper, we introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing diverse text prompts of three increasing complexity levels that are specially designed for 3D generation. To assess both the subjective quality and the text alignment, we propose two automatic metrics based on multi-view images produced by the 3D contents. The quality metric combines multi-view text-image scores and regional convolution to detect quality and view inconsistency. The alignment metric uses multi-view captioning and Large Language Model (LLM) evaluation to measure text-3D consistency. Both metrics closely correlate with different dimensions of human judgments, providing a paradigm for efficiently evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal performance differences among six prevalent text-to-3D methods. Our analysis further highlights the common struggles for current methods on generating surroundings and multi-object scenes, as well as the bottleneck of leveraging 2D guidance for 3D generation. Our project page is available at: https://t3bench.com.
</details>
<details>
<summary>摘要</summary>
现有的文本到3D方法利用强大预训 diffusion 模型优化 NeRF，不需要训练3D数据可以生成高质量3D场景。由于这是一个开放式任务，大多数研究通过subjective case study和用户实验评估自己的结果，因此存在评估当前进展的问题的量化问题。在这篇论文中，我们介绍T$^3$Bench，首个包含多种文本提示的三个不同复杂度水平的文本到3D benchmark。为了评估文本到3D模型的资源和文本对齐，我们提出了两种自动度量器，它们基于多视图图像生成的3D内容。一个是文本-图像多视图分数和区域卷积来检测质量和视图不一致的指标。另一个是文本-3D多视图描述和大语言模型评估来度量文本-3D一致性的指标。这两个指标与人类评价的不同维度呈正相关，为efficiently评估文本到3D模型提供了一个方框。 Fig. 1 中显示的 benchmarking 结果显示了六种流行的文本到3D方法之间的性能差异。我们的分析还指出了当前方法在生成周围和多对象场景时的普遍困难，以及利用2D导航来生成3D内容的瓶颈。我们的项目页面可以在：https://t3bench.com 上找到。
</details></li>
</ul>
<hr>
<h2 id="UniverSLU-Universal-Spoken-Language-Understanding-for-Diverse-Classification-and-Sequence-Generation-Tasks-with-a-Single-Network"><a href="#UniverSLU-Universal-Spoken-Language-Understanding-for-Diverse-Classification-and-Sequence-Generation-Tasks-with-a-Single-Network" class="headerlink" title="UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network"></a>UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02973">http://arxiv.org/abs/2310.02973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe</li>
<li>for: 这个研究的目的是构建一个可以同时完成多种语音理解任务的单一模型。</li>
<li>methods: 这个研究使用了预训练的自动语音识别（ASR）模型，并使用了不同的任务和数据集规定器作为精确的提示。</li>
<li>results: 研究表明，这个单一多任务学习（MTL）模型“UniverSLU”在12种语音分类和序列生成任务中表现了竞争力，甚至超过了专门为这些任务预训练的模型。<details>
<summary>Abstract</summary>
Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model's behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model "UniverSLU" for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model's generalization capabilities to new paraphrases.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，使用大型自然语言模型并行多任务能够获得扎实的成果。它们使用提示来引导模型的行为，并超越专门为某个任务设计的模型的性能。受到这些研究的启发，我们问：我们可以建立一个能够同时执行多种口语理解（SLU）任务的单一模型吗？为解决这个问题，我们利用预训练的自动语音识别（ASR）模型，并使用不同的任务和数据集规定器作为批处理的提示。我们称之为“UniverSLU”。我们在17个数据集和9种语言上进行了12种语音分类和序列生成任务的测试，结果表明UniverSLU可以达到竞争性的表现，甚至超越专门为某个任务设计的模型。我们还进行了初步的研究，使用人类可理解的自然短语而不是任务规定器作为提示，并测试模型的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Prompting-and-Adapter-Tuning-for-Self-supervised-Encoder-Decoder-Speech-Model"><a href="#Prompting-and-Adapter-Tuning-for-Self-supervised-Encoder-Decoder-Speech-Model" class="headerlink" title="Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model"></a>Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02971">http://arxiv.org/abs/2310.02971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai-Wei Chang, Ming-Hsin Chen, Yun-Ping Lin, Jing Neng Hsu, Paul Kuo-Ming Huang, Chien-yu Huang, Shang-Wen Li, Hung-yi Lee</li>
<li>for: 这篇论文的目的是提出一种基于启发询问的自然语言处理方法，并在语音识别和插槽填充等复杂序列生成任务中进行测试。</li>
<li>methods: 这篇论文使用了启发询问和适束调整方法，并将它们应用到Wav2Seq模型中。</li>
<li>results: 实验结果显示，启发询问在语音识别和插槽填充等序列生成任务中能够 achieve 53% 的Relative Improvement in Word Error Rate 和 27% 的F1 Score。此外，启发询问在低资源enario中与 Fine-Tuning 方法竞争。此外，这篇论文还证明了启发询问和适束调整在不同语言的cross-Lingual ASR中的传递性。<details>
<summary>Abstract</summary>
Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning." into Simplified Chinese.干���addle和适配调整已经成为精细调整（FT）方法的有效替代方案。然而，现有的speech prompting研究主要集中在分类任务上，并未能够处理更复杂的序列生成任务。此外，适配调整主要应用于encoder-only自动学习模型。我们的实验表明，在Wav2Seq模型上进行提示，超过了先前的工作在序列生成任务中。它在ASR中实现了53%的关系改进率，并在插槽填充任务中实现了27%的F1分数。此外，提示和适配调整在低资源enario中竞争FT方法。此外，我们还证明了Wav2Seq模型上的提示和适配调整在跨语言ASR中的传送性。当有限的可学习参数参与时，提示和适配调整一致地超越了传统FT。特别是在低资源enario中，提示一直超越了适配调整。
</details></li>
</ul>
<hr>
<h2 id="DQ-LoRe-Dual-Queries-with-Low-Rank-Approximation-Re-ranking-for-In-Context-Learning"><a href="#DQ-LoRe-Dual-Queries-with-Low-Rank-Approximation-Re-ranking-for-In-Context-Learning" class="headerlink" title="DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning"></a>DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02954">http://arxiv.org/abs/2310.02954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/menik1126/DQ-LoRe">https://github.com/menik1126/DQ-LoRe</a></li>
<li>paper_authors: Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang</li>
<li>for:  This paper focuses on improving the automatic selection of exemplars for in-context learning in natural language processing, specifically using Large Language Models (LLMs).</li>
<li>methods:  The proposed method, called Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe), utilizes two stages of querying: first, LLM-generated knowledge is obtained through Dual Queries, and then, the retriever is queried to obtain final exemplars that align with the input question’s knowledge. Additionally, LoRe employs dimensionality reduction techniques to refine exemplar selection.</li>
<li>results:  The proposed DQ-LoRe method significantly outperforms prior state-of-the-art methods in selecting exemplars for GPT-4, with a performance increase from 92.5% to 94.2%. The method also consistently outperforms retrieval-based approaches in terms of both performance and adaptability, especially in scenarios with distribution shifts.<details>
<summary>Abstract</summary>
Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive experiments, we demonstrate that DQ-LoRe significantly outperforms prior state-of-the-art methods in the automatic selection of exemplars for GPT-4, enhancing performance from 92.5% to 94.2%. Our comprehensive analysis further reveals that DQ-LoRe consistently outperforms retrieval-based approaches in terms of both performance and adaptability, especially in scenarios characterized by distribution shifts. DQ-LoRe pushes the boundaries of in-context learning and opens up new avenues for addressing complex reasoning challenges. We will release the code soon.
</details>
<details>
<summary>摘要</summary>
近期自然语言处理技术的发展，主要受到大型语言模型（LLM）的推动，展现了其在context learning中的强大能力。为了引导LLM进行复杂的推理任务，一个有前途的方向是在Chain-of-Thought（CoT） парадигме中使用中间推理步骤。然而，中间推理步骤的选择是一个主要挑战。在本研究中，我们提出了一种框架，利用Dual Queries和Low-rank approximation Re-ranking（DQ-LoRe）自动选择中间推理步骤。Dual Queries首先询问LLM获取LLM生成的知识，如CoT，然后询问检索器获取最终的例子via问题和知识。此外，在第二个询问中，LoRe使用维度减少技术来精细地选择例子，确保与输入问题的知识相互Alignment。通过广泛的实验，我们证明了DQ-LoRe在自动选择GPT-4中间推理步骤方面显著超越了前一个状态的方法，从92.5%提高到94.2%。我们的全面分析还表明，DQ-LoRe在检索器基于方法的场景下表现出了明显的优势，特别是在存在分布shift的情况下。DQ-LoRe推动了context learning的边缘和开创了新的解决复杂推理挑战的方向。我们即将发布代码。
</details></li>
</ul>
<hr>
<h2 id="JsonTuning-Towards-Generalizable-Robust-and-Controllable-Instruction-Tuning"><a href="#JsonTuning-Towards-Generalizable-Robust-and-Controllable-Instruction-Tuning" class="headerlink" title="JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning"></a>JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02953">http://arxiv.org/abs/2310.02953</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gao-xiao-bai/jsontuning">https://github.com/gao-xiao-bai/jsontuning</a></li>
<li>paper_authors: Chang Gao, Wenxuan Zhang, Guizhen Chen, Wai Lam<br>for:  This paper aims to improve the performance of large language models (LLMs) in various tasks by providing explicit task instructions through a novel structure-to-structure approach called JsonTuning.methods:  The JsonTuning approach leverages the versatility and structured nature of JSON to represent tasks, enhancing generalization, improving robustness, and increasing controllability over the output.results:  The experimental results show that JsonTuning outperforms TextTuning in various applications, demonstrating improved performance, adaptability, robustness, and controllability.<details>
<summary>Abstract</summary>
Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in various applications, showcasing improved performance, adaptability, robustness, and controllability. By overcoming the limitations of TextTuning, JsonTuning demonstrates significant potential for more effective and reliable LLMs capable of handling diverse scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-of-GPT-3-Family-Large-Language-Models-Including-ChatGPT-and-GPT-4"><a href="#A-Survey-of-GPT-3-Family-Large-Language-Models-Including-ChatGPT-and-GPT-4" class="headerlink" title="A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4"></a>A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12321">http://arxiv.org/abs/2310.12321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katikapalli Subramanyam Kalyan</li>
<li>for: This paper is written to provide a comprehensive survey of recent research progress in the field of GPT-3 family large language models (GLLMs), including their performances in various downstream tasks, domains, and languages.</li>
<li>methods: The paper uses a brief overview of transformers, transfer learning, self-supervised learning, pretrained language models, and large language models as foundation concepts, and discusses the data labelling and data augmentation abilities, robustness, effectiveness, and future research directions of GLLMs.</li>
<li>results: The paper presents a comprehensive overview of the recent research progress in GLLMs, including their performances in various downstream tasks, domains, and languages, and provides insightful future research directions for the field.Here are the three key information points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提供关于GPT-3家族大语言模型（GLLMs）的全面评估，包括它们在不同下游任务、领域和语言中的表现。</li>
<li>methods: 这篇论文使用转换器、传输学习、自动生成学习、预训练语言模型和大语言模型作为基础概念，并讨论GLLMs中的数据标注和数据增强能力、Robustness、效果和未来研究方向。</li>
<li>results: 这篇论文提供了GLLMs在不同下游任务、领域和语言中的全面评估，并提供了一些有价值的未来研究方向。<details>
<summary>Abstract</summary>
Large language models (LLMs) are a special class of pretrained language models obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI GPT-3 model, and the popularity of LLMs is increasing exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）是一种特殊的预训练语言模型，通过扩大模型大小、预训练文献和计算来获得。由于它们的大型和预训练大量文本数据，LLM具有特殊的能力，可以在许多自然语言处理任务中达到很高的性能，无需任务特定的训练。LLM的时代开始于OpenAI GPT-3模型，而GPT-3家族大语言模型（GLLM）的流行程度在不断增长，特别在研究社区。随着GLLM的普及，特别是在研究领域，有一个强烈的需求：即对多个维度的研究进展进行总结，并提供有用的未来研究方向。我们的论文开始于基础概念，如变换器、转移学习、自我超vised学习、预训练语言模型和大语言模型。然后，我们提供GLLM的简要概述，讨论GLLM在各种下游任务、特定领域和多种语言中的表现。我们还讨论GLLM的数据标注和数据扩展能力，GLLM的Robustness，GLLM作为评估器的效果，并最后结束于多个有用的未来研究方向。总之，这篇总结论文将成为学术和工业人员关注GPT-3家族大语言模型最新研究的好资源。
</details></li>
</ul>
<hr>
<h2 id="LibriSpeech-PC-Benchmark-for-Evaluation-of-Punctuation-and-Capitalization-Capabilities-of-end-to-end-ASR-Models"><a href="#LibriSpeech-PC-Benchmark-for-Evaluation-of-Punctuation-and-Capitalization-Capabilities-of-end-to-end-ASR-Models" class="headerlink" title="LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models"></a>LibriSpeech-PC: Benchmark for Evaluation of Punctuation and Capitalization Capabilities of end-to-end ASR Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02943">http://arxiv.org/abs/2310.02943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksandr Meister, Matvei Novikov, Nikolay Karpov, Evelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg</li>
<li>for: 本文旨在评估端到端自动语音识别（ASR）模型的括号和大写预测能力。</li>
<li>methods: 本文使用LibriSpeech-PC数据集，并提出了一种新的评估指标 called Punctuation Error Rate（PER）来评估括号预测的准确性。</li>
<li>results: 本文提供了一些初步的基准模型，并通过对LibriSpeech-PC数据集进行测试，证明了该评估指标的有用性。<details>
<summary>Abstract</summary>
Traditional automatic speech recognition (ASR) models output lower-cased words without punctuation marks, which reduces readability and necessitates a subsequent text processing model to convert ASR transcripts into a proper format. Simultaneously, the development of end-to-end ASR models capable of predicting punctuation and capitalization presents several challenges, primarily due to limited data availability and shortcomings in the existing evaluation methods, such as inadequate assessment of punctuation prediction. In this paper, we introduce a LibriSpeech-PC benchmark designed to assess the punctuation and capitalization prediction capabilities of end-to-end ASR models. The benchmark includes a LibriSpeech-PC dataset with restored punctuation and capitalization, a novel evaluation metric called Punctuation Error Rate (PER) that focuses on punctuation marks, and initial baseline models. All code, data, and models are publicly available.
</details>
<details>
<summary>摘要</summary>
传统的自动话语识别（ASR）模型输出下划线字符的话语无标点符号，这会降低可读性，需要 subsequential 文本处理模型来将 ASR 笔记转换成正确的格式。同时，开发端到端 ASR 模型可以预测标点和大小写存在几个挑战，主要是因为数据的有限性和现有评估方法的缺陷，如标点预测评估不够精准。本文介绍了一个基于 LibriSpeech-PC 的比较基准，用于评估端到端 ASR 模型的标点和大小写预测能力。该比较基准包括 LibriSpeech-PC 数据集，修复了标点和大小写，以及一种新的评估指标called Punctuation Error Rate（PER），它专注于标点符号。此外，我们还提供了一些初步的基线模型。所有代码、数据和模型都公开可用。
</details></li>
</ul>
<hr>
<h2 id="Hate-Speech-Detection-in-Limited-Data-Contexts-using-Synthetic-Data-Generation"><a href="#Hate-Speech-Detection-in-Limited-Data-Contexts-using-Synthetic-Data-Generation" class="headerlink" title="Hate Speech Detection in Limited Data Contexts using Synthetic Data Generation"></a>Hate Speech Detection in Limited Data Contexts using Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02876">http://arxiv.org/abs/2310.02876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aman Khullar, Daniel Nkemelu, Cuong V. Nguyen, Michael L. Best</li>
<li>for: 提高在有限数据上的 hate speech 检测性能</li>
<li>methods: 使用数据生成技术生成假数据，并将 hate sentiment 保留在原始示例中转移到目标语言中的新示例中</li>
<li>results: 使用生成的数据训练 hate speech 分类模型，在 Hindi 和 Vietnamese 等有限数据上显示了比较好的性能，可以帮助 bootstrap  hate speech 检测模型从scratch 在有限数据上Here’s the translation in English for reference:</li>
<li>for: Improving the performance of hate speech detection in limited data contexts</li>
<li>methods: Using data generation techniques to synthesize new examples of hate speech data in the target language, while retaining the hate sentiment in the original examples</li>
<li>results: Training a hate speech classification model using the synthesized data shows comparable or even better performance than training only on the limited data available in the target domain, which can help bootstrap hate speech detection models from scratch in limited data contexts.<details>
<summary>Abstract</summary>
A growing body of work has focused on text classification methods for detecting the increasing amount of hate speech posted online. This progress has been limited to only a select number of highly-resourced languages causing detection systems to either under-perform or not exist in limited data contexts. This is majorly caused by a lack of training data which is expensive to collect and curate in these settings. In this work, we propose a data augmentation approach that addresses the problem of lack of data for online hate speech detection in limited data contexts using synthetic data generation techniques. Given a handful of hate speech examples in a high-resource language such as English, we present three methods to synthesize new examples of hate speech data in a target language that retains the hate sentiment in the original examples but transfers the hate targets. We apply our approach to generate training data for hate speech classification tasks in Hindi and Vietnamese. Our findings show that a model trained on synthetic data performs comparably to, and in some cases outperforms, a model trained only on the samples available in the target domain. This method can be adopted to bootstrap hate speech detection models from scratch in limited data contexts. As the growth of social media within these contexts continues to outstrip response efforts, this work furthers our capacities for detection, understanding, and response to hate speech.
</details>
<details>
<summary>摘要</summary>
“一些研究已经集中在网络上的讯息分类方法，以探测增加的网络上的仇恨言论。然而，这些进步仅限于一些高度资源的语言，使得检测系统在有限数据上 either 下perform 或无法存在。这主要是因为训练数据的缺乏，收集和整理这些数据是 expensive 的。在这个工作中，我们提出了一个数据增压方法，使用生成的 synthetic 数据来解决有限数据上 hate speech 检测的问题。我们使用英文中的 hate speech 例子，生成目标语言中的新的 hate speech 数据，保留了原始例子中的仇恨情感，但将仇恨目标转移到新的语言中。我们将这个方法应用到帮助 hate speech 检测模型在印地语和越南语的检测任务中获得更好的性能。我们的发现显示，使用生成的数据训练的模型在目标领域中的性能与使用仅有可用的数据训练的模型相比，有时会更好，有时会相等。这种方法可以用来启动 hate speech 检测模型，尤其在有限数据上。随着社交媒体在这些情况下的增长，这些研究将进一步我们对仇恨言论的检测、理解和回应的能力。”
</details></li>
</ul>
<hr>
<h2 id="Out-of-Distribution-Detection-by-Leveraging-Between-Layer-Transformation-Smoothness"><a href="#Out-of-Distribution-Detection-by-Leveraging-Between-Layer-Transformation-Smoothness" class="headerlink" title="Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness"></a>Out-of-Distribution Detection by Leveraging Between-Layer Transformation Smoothness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02832">http://arxiv.org/abs/2310.02832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fjelenic/between-layer-ood">https://github.com/fjelenic/between-layer-ood</a></li>
<li>paper_authors: Fran Jelenić, Josip Jukić, Martin Tutek, Mate Puljiz, Jan Šnajder</li>
<li>for: 检测深度神经网络中的外围数据 (OOD) 是重要的，但现有的方法受限于需要训练数据或干预训练过程。本文提出了一种基于层次变换平滑性 (BLOOD) 的新方法，可以无需训练数据进行 OOD 数据检测，并适用于预训练模型。</li>
<li>methods: BLOOD 方法基于层次变换平滑性，利用 ID 数据的变换表现更加平滑 than OOD 数据，这也在 Transformer 网络中得到了实验证明。</li>
<li>results: 在文本分类任务中，BLOOD 方法与比较资源占用相同的方法进行比较，并表现出比较好的性能。分析还表明，当学习更加简单的任务时，OOD 数据变换保持原始的锐度，而学习更加复杂的任务时，锐度增加。<details>
<summary>Abstract</summary>
Effective OOD detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting OOD data in deep neural networks based on transformation smoothness between intermediate layers of a network (BLOOD), which is applicable to pre-trained models without access to training data. BLOOD utilizes the tendency of between-layer representation transformations of in-distribution (ID) data to be smoother than the corresponding transformations of OOD data, a property that we also demonstrate empirically for Transformer networks. We evaluate BLOOD on several text classification tasks with Transformer networks and demonstrate that it outperforms methods with comparable resource requirements. Our analysis also suggests that when learning simpler tasks, OOD data transformations maintain their original sharpness, whereas sharpness increases with more complex tasks.
</details>
<details>
<summary>摘要</summary>
实用的OOD检测是机器学习模型的可靠性关键，但现有的方法受到训练数据的限制，无法实际使用。我们提出了一种基于对于几个层的网络中间层的变数平滑性（BLOOD）来检测OOD数据的方法，不需要训练数据。BLOOD利用了对于ID数据的 между层表示变数转换是稳定的，而OOD数据的转换则是不稳定的，这是我们在Transformer网络上验证的。我们在多个文本分类任务上评估BLOOD，并证明它在与其他方法相比有相似的资源需求下表现更好。我们的分析也显示，当学习较简单的任务时，OOD数据的转换仍然保持原始的锋利度，而当学习较复杂的任务时，锋利度则增加。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Prompt-Transformer-with-Hybrid-Contrastive-Learning-for-Emotion-Recognition-in-Conversation"><a href="#Multimodal-Prompt-Transformer-with-Hybrid-Contrastive-Learning-for-Emotion-Recognition-in-Conversation" class="headerlink" title="Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation"></a>Multimodal Prompt Transformer with Hybrid Contrastive Learning for Emotion Recognition in Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04456">http://arxiv.org/abs/2310.04456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shihao Zou, Xianying Huang, Xudong Shen</li>
<li>for: 这个研究旨在提高人机交互中的情绪识别（ERC）能力，以实现更好的人机互动。</li>
<li>methods: 本研究使用了深入抽取情绪讯号的方法，将强表达能力的模式处理为深度情绪讯处理，并设计了多modal启发讯息（Multimodal Prompt Transformer，MPT）来进行跨模式资讯结合。此外，本研究还使用了混合对照学习（Hybrid Contrastive Learning，HCL）策略来优化模型对于少数标签的处理能力。</li>
<li>results: 实验结果显示，本研究所提出的模型在ERC中比前方法更高的表现，在两个referencedataset上都达到了顶尖水平。<details>
<summary>Abstract</summary>
Emotion Recognition in Conversation (ERC) plays an important role in driving the development of human-machine interaction. Emotions can exist in multiple modalities, and multimodal ERC mainly faces two problems: (1) the noise problem in the cross-modal information fusion process, and (2) the prediction problem of less sample emotion labels that are semantically similar but different categories. To address these issues and fully utilize the features of each modality, we adopted the following strategies: first, deep emotion cues extraction was performed on modalities with strong representation ability, and feature filters were designed as multimodal prompt information for modalities with weak representation ability. Then, we designed a Multimodal Prompt Transformer (MPT) to perform cross-modal information fusion. MPT embeds multimodal fusion information into each attention layer of the Transformer, allowing prompt information to participate in encoding textual features and being fused with multi-level textual information to obtain better multimodal fusion features. Finally, we used the Hybrid Contrastive Learning (HCL) strategy to optimize the model's ability to handle labels with few samples. This strategy uses unsupervised contrastive learning to improve the representation ability of multimodal fusion and supervised contrastive learning to mine the information of labels with few samples. Experimental results show that our proposed model outperforms state-of-the-art models in ERC on two benchmark datasets.
</details>
<details>
<summary>摘要</summary>
人机交互中的情感认知（ERC）发挥着重要的作用。情感可以存在多个modalities，而多modalities ERC主要面临两个问题：（1）在跨modal信息融合过程中的噪声问题，和（2）使用少量样本的情感标签，这些标签semantically similar yet belong to different categories。为了解决这些问题并充分利用每个modalities的特征，我们采用了以下策略：首先，深度情感cue extraction被 Performing on modalities with strong representation ability，并设计了多modal prompt信息的特征过滤器。然后，我们设计了一种Multimodal Prompt Transformer（MPT）来实现跨modal信息融合。MPT将multimodal融合信息嵌入到每个Attention层中， allowing prompt information参与文本特征编码和融合多级文本信息以获得更好的跨modal融合特征。最后，我们使用Hybrid Contrastive Learning（HCL）策略来优化模型对具有少量样本的标签的处理能力。这种策略使用了无监督的对比学习提高跨modal融合的表示能力，并使用监督的对比学习挖掘标签中的信息。实验结果显示，我们提出的模型在ERC中比州OF-the-art模型更高。
</details></li>
</ul>
<hr>
<h2 id="DOMINO-A-Dual-System-for-Multi-step-Visual-Language-Reasoning"><a href="#DOMINO-A-Dual-System-for-Multi-step-Visual-Language-Reasoning" class="headerlink" title="DOMINO: A Dual-System for Multi-step Visual Language Reasoning"></a>DOMINO: A Dual-System for Multi-step Visual Language Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02804">http://arxiv.org/abs/2310.02804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/dual-system-for-visual-language-reasoning">https://github.com/facebookresearch/dual-system-for-visual-language-reasoning</a></li>
<li>paper_authors: Peifang Wang, Olga Golovneva, Armen Aghajanyan, Xiang Ren, Muhao Chen, Asli Celikyilmaz, Maryam Fazel-Zarandi<br>for:这 paper 的目的是提出一种多步多模态理解方法，用于解决图表和图像中的信息抽取和逻辑或数学计算问题。methods:这 paper 使用了一种双系统方法，包括一个 “System-1” 步骤 для视觉信息抽取，以及一个 “System-2” 步骤 для慎重的逻辑计算。在给定输入时，System-2 将问题分解成多个原子步骤，每个步骤导航 System-1 提取图像中需要进行逻辑计算的信息。results:实验表明，我们的方法在图表和图像 datasets 上表现竞争力强，与先前的干预式模型和管道方法相比。在多步逻辑计算任务中，通过练化 System-2 模块（LLaMA-2 70B）只需要一小段数据，我们的方法的准确率得到了进一步改进，并超越了最佳完全监督的端到端方法（5.7%）和管道方法（7.5%）在一个复杂的数据集上。<details>
<summary>Abstract</summary>
Visual language reasoning requires a system to extract text or numbers from information-dense images like charts or plots and perform logical or arithmetic reasoning to arrive at an answer. To tackle this task, existing work relies on either (1) an end-to-end vision-language model trained on a large amount of data, or (2) a two-stage pipeline where a captioning model converts the image into text that is further read by another large language model to deduce the answer. However, the former approach forces the model to answer a complex question with one single step, and the latter approach is prone to inaccurate or distracting information in the converted text that can confuse the language model. In this work, we propose a dual-system for multi-step multimodal reasoning, which consists of a "System-1" step for visual information extraction and a "System-2" step for deliberate reasoning. Given an input, System-2 breaks down the question into atomic sub-steps, each guiding System-1 to extract the information required for reasoning from the image. Experiments on chart and plot datasets show that our method with a pre-trained System-2 module performs competitively compared to prior work on in- and out-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on only a small amount of data on multi-step reasoning, the accuracy of our method is further improved and surpasses the best fully-supervised end-to-end approach by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging dataset with human-authored questions.
</details>
<details>
<summary>摘要</summary>
视觉语言理解需要一个系统可以从信息厚度图表或图表中提取文本或数字，并通过逻辑或算术理解来获得答案。现有的方法可以分为两种：一种是使用一个终到终的视力语言模型，另一种是使用两个阶段管道，其中一个captioning模型将图表转换为文本，然后另一个大型语言模型来读取这个文本来推理出答案。然而，前者方法会让模型在一个步骤中回答一个复杂的问题，而后者方法容易因为转换后的文本中含有错误或干扰信息而导致模型混乱。在这种情况下，我们提出了一种多步骤多模态逻辑理解的双系统，它包括一个“系统1”步骤用于视觉信息提取，以及一个“系统2”步骤用于推理。给定输入，系统2将问题分解成原子步骤，每个步骤都会导航系统1提取图表中需要进行逻辑reasoning的信息。我们在图表和图表 datasets上进行实验，结果表明我们的方法与一个预训练的系统2模块相比，在不同的数据上都能够竞争。而通过练习系统2模块（LLaMA-2 70B）在小量数据上进行多步骤逻辑reasoning，我们的方法的准确率得到进一步提高，并在一个复杂的 dataset 上超过了最佳完全监督的端到终结构和一个管道结构（FlanPaLM 540B）的最高值。
</details></li>
</ul>
<hr>
<h2 id="Low-Resource-Summarization-using-Pre-trained-Language-Models"><a href="#Low-Resource-Summarization-using-Pre-trained-Language-Models" class="headerlink" title="Low Resource Summarization using Pre-trained Language Models"></a>Low Resource Summarization using Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02790">http://arxiv.org/abs/2310.02790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mubashir Munaf, Hammad Afzal, Naima Iltaf, Khawir Mahmood</li>
<li>for: 针对低资源语言的自动概要生成</li>
<li>methods: 使用自注意力变换器模型（mBERT、mT5）进行适应，并构建了一个新的基线数据集（76.5k篇文章、摘要对）在低资源语言 Urdu</li>
<li>results: 提出了一种基线方法，可以在限制资源的情况下进行低资源语言的自动概要生成，并达到了与高资源语言英语相同的评价成绩（PEGASUS: 47.21，BART: 45.14 on XSUM Dataset），同时提供了一种可重复的方法，可以应用于其他低资源语言。<details>
<summary>Abstract</summary>
With the advent of Deep Learning based Artificial Neural Networks models, Natural Language Processing (NLP) has witnessed significant improvements in textual data processing in terms of its efficiency and accuracy. However, the research is mostly restricted to high-resource languages such as English and low-resource languages still suffer from a lack of available resources in terms of training datasets as well as models with even baseline evaluation results. Considering the limited availability of resources for low-resource languages, we propose a methodology for adapting self-attentive transformer-based architecture models (mBERT, mT5) for low-resource summarization, supplemented by the construction of a new baseline dataset (76.5k article, summary pairs) in a low-resource language Urdu. Choosing news (a publicly available source) as the application domain has the potential to make the proposed methodology useful for reproducing in other languages with limited resources. Our adapted summarization model \textit{urT5} with up to 44.78\% reduction in size as compared to \textit{mT5} can capture contextual information of low resource language effectively with evaluation score (up to 46.35 ROUGE-1, 77 BERTScore) at par with state-of-the-art models in high resource language English \textit{(PEGASUS: 47.21, BART: 45.14 on XSUM Dataset)}. The proposed method provided a baseline approach towards extractive as well as abstractive summarization with competitive evaluation results in a limited resource setup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Role-of-Linguistic-Priors-in-Measuring-Compositional-Generalization-of-Vision-Language-Models"><a href="#The-Role-of-Linguistic-Priors-in-Measuring-Compositional-Generalization-of-Vision-Language-Models" class="headerlink" title="The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models"></a>The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02777">http://arxiv.org/abs/2310.02777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Wu, Li Erran Li, Stefano Ermon, Patrick Haffner, Rong Ge, Zaiwei Zhang</li>
<li>for: 本文旨在探讨多Modalities中的Compositionality问题，尤其是图像和文本之间的Compositionality。</li>
<li>methods: 作者提出了两种来源的Visual-Linguistic Compositionality：语言优先和图像和文本之间的互动。现有的尝试都是通过语言优先来提高Compositionality的总体化能力。</li>
<li>results: 作者建议了一种不听语言优先的Compositionality metric。<details>
<summary>Abstract</summary>
Compositionality is a common property in many modalities including natural languages and images, but the compositional generalization of multi-modal models is not well-understood. In this paper, we identify two sources of visual-linguistic compositionality: linguistic priors and the interplay between images and texts. We show that current attempts to improve compositional generalization rely on linguistic priors rather than on information in the image. We also propose a new metric for compositionality without such linguistic priors.
</details>
<details>
<summary>摘要</summary>
《作品性》是许多Modalities中的共有特性，包括自然语言和图像，但现有的多modal模型的compositional generalization不够了解。本文认为，图像和文本之间的互动和语言优先顺序是两个主要的visual-linguistic compositionality来源。我们发现现有的改进compositional generalization尝试都是通过语言优先顺序进行，而不是从图像中获取信息。我们还提出了一个不含语言优先顺序的新的compositional metric。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Study-and-Framework-for-Automated-Summariser-Evaluation-LangChain-and-Hybrid-Algorithms"><a href="#Comparative-Study-and-Framework-for-Automated-Summariser-Evaluation-LangChain-and-Hybrid-Algorithms" class="headerlink" title="Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms"></a>Comparative Study and Framework for Automated Summariser Evaluation: LangChain and Hybrid Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02759">http://arxiv.org/abs/2310.02759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bagiya Lakshmi S, Sanjjushri Varshini R, Rohith Mahadevan, Raja CSP Raman</li>
<li>for: 本研究旨在测试用户对给定主题的理解程度，并且利用大型自然语言模型进行分析。</li>
<li>methods: 本研究使用Large Language Models进行分析，通过利用Langchain工具SUMMARIZE PDF文档，提取主要信息，以测量用户对摘要内容的理解程度。</li>
<li>results: 本研究可以帮助学习者了解他们对某个主题的理解程度，并且可以帮助教育专业人员进一步改善学习能力。<details>
<summary>Abstract</summary>
Automated Essay Score (AES) is proven to be one of the cutting-edge technologies. Scoring techniques are used for various purposes. Reliable scores are calculated based on influential variables. Such variables can be computed by different methods based on the domain. The research is concentrated on the user's understanding of a given topic. The analysis is based on a scoring index by using Large Language Models. The user can then compare and contrast the understanding of a topic that they recently learned. The results are then contributed towards learning analytics and progression is made for enhancing the learning ability. In this research, the focus is on summarizing a PDF document and gauging a user's understanding of its content. The process involves utilizing a Langchain tool to summarize the PDF and extract the essential information. By employing this technique, the research aims to determine how well the user comprehends the summarized content.
</details>
<details>
<summary>摘要</summary>
自动化文章分数（AES）是一种先进技术，用于多种目的。分数计算基于重要的变量，这些变量可以根据域 Compute 多种方法。研究专注于用户对某个主题的理解，通过分数指数来进行分析。用户可以比较和对比他们最近学习的主题理解程度。结果对学习统计和进步做出贡献。在这项研究中，我们关注将 PDF 文档概要并评估用户对其内容的理解程度。过程中使用 Langchain 工具概要 PDF 并提取重要信息。通过这种方法，我们希望确定用户对概要内容的理解程度。
</details></li>
</ul>
<hr>
<h2 id="LC-Score-Reference-less-estimation-of-Text-Comprehension-Difficulty"><a href="#LC-Score-Reference-less-estimation-of-Text-Comprehension-Difficulty" class="headerlink" title="LC-Score: Reference-less estimation of Text Comprehension Difficulty"></a>LC-Score: Reference-less estimation of Text Comprehension Difficulty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02754">http://arxiv.org/abs/2310.02754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Tardy, Charlotte Roze, Paul Poupet</li>
<li>for:  This paper aims to improve text comprehension for readers with comprehension issues, particularly in the French language.</li>
<li>methods:  The paper proposes a simple approach called \textsc{LC-Score} to train text comprehension metrics for any French text without reference. The approach uses linguistically motivated indicators to train statistical models, as well as neural learning directly from text leveraging pre-trained language models.</li>
<li>results:  The paper finds that both approaches (indicator-based and neural) outperform commonly used readability and comprehension metrics such as FKGL, based on two human annotation experiments.<details>
<summary>Abstract</summary>
Being able to read and understand written text is critical in a digital era. However, studies shows that a large fraction of the population experiences comprehension issues. In this context, further initiatives in accessibility are required to improve the audience text comprehension. However, writers are hardly assisted nor encouraged to produce easy-to-understand content. Moreover, Automatic Text Simplification (ATS) model development suffers from the lack of metric to accurately estimate comprehension difficulty We present \textsc{LC-Score}, a simple approach for training text comprehension metric for any French text without reference \ie predicting how easy to understand a given text is on a $[0, 100]$ scale. Our objective with this scale is to quantitatively capture the extend to which a text suits to the \textit{Langage Clair} (LC, \textit{Clear Language}) guidelines, a French initiative closely related to English Plain Language. We explore two approaches: (i) using linguistically motivated indicators used to train statistical models, and (ii) neural learning directly from text leveraging pre-trained language models. We introduce a simple proxy task for comprehension difficulty training as a classification task. To evaluate our models, we run two distinct human annotation experiments, and find that both approaches (indicator based and neural) outperforms commonly used readability and comprehension metrics such as FKGL.
</details>
<details>
<summary>摘要</summary>
在数字时代，能够阅读和理解written文本是关键。然而，研究表明，大量人口受到理解问题的压力。在这种情况下，进一步的访问ibility措施是必需的，以提高读者文本理解能力。然而，作者几乎没有被帮助，也没有劝导以生成易于理解的内容。此外，自动文本简化（ATS）模型的开发受到了参照文本的缺乏，导致缺乏准确度测试的精度。我们提出了\textsc{LC-Score}，一种简单的方法，可以在不使用参照文本的情况下，训练文本理解度量。我们的目标是在 $[0, 100]$ 分范围内，量化文本是否遵循法国《Langage Clair》（LC）指南，这与英语平易途同。我们探索了两种方法：（i）使用语言学上的驱动因素，用于训练统计模型，和（ii）直接从文本中学习，利用预训练语言模型。我们介绍了一个简单的代理任务，以作为理解难度训练的分类任务。为了评估我们的模型，我们进行了两个独立的人类标注实验，并发现，我们的指南（指标）和神经网络方法都高于常用的阅读和理解指标 such as FKGL。
</details></li>
</ul>
<hr>
<h2 id="COVID-19-South-African-Vaccine-Hesitancy-Models-Show-Boost-in-Performance-Upon-Fine-Tuning-on-M-pox-Tweets"><a href="#COVID-19-South-African-Vaccine-Hesitancy-Models-Show-Boost-in-Performance-Upon-Fine-Tuning-on-M-pox-Tweets" class="headerlink" title="COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets"></a>COVID-19 South African Vaccine Hesitancy Models Show Boost in Performance Upon Fine-Tuning on M-pox Tweets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04453">http://arxiv.org/abs/2310.04453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Perikli, Srimoy Bhattacharya, Blessing Ogbuokiri, Zahra Movahedi Nia, Benjamin Lieberman, Nidhi Tripathi, Salah-Eddine Dahbi, Finn Stevenson, Nicola Bragazzi, Jude Kong, Bruce Mellado</li>
<li>for: 本研究旨在测试covid-19模型在南非推特数据上的性能，并调整这些模型以适应M-pox数据集。</li>
<li>methods: 使用LDA基于话题模型来比较原始COVID-19 RoBERTa模型和其调整版本中的错误分类M-pox tweets，并从这种分析中吸取建立更复杂的模型的经验教训。</li>
<li>results: 经过调整后，F1-scores提高了超过8%，达到了69.6%的最高值，超过了现有的模型和知名的分类算法。<details>
<summary>Abstract</summary>
Very large numbers of M-pox cases have, since the start of May 2022, been reported in non-endemic countries leading many to fear that the M-pox Outbreak would rapidly transition into another pandemic, while the COVID-19 pandemic ravages on. Given the similarities of M-pox with COVID-19, we chose to test the performance of COVID-19 models trained on South African twitter data on a hand-labelled M-pox dataset before and after fine-tuning. More than 20k M-pox-related tweets from South Africa were hand-labelled as being either positive, negative or neutral. After fine-tuning these COVID-19 models on the M-pox dataset, the F1-scores increased by more than 8% falling just short of 70%, but still outperforming state-of-the-art models and well-known classification algorithms. An LDA-based topic modelling procedure was used to compare the miss-classified M-pox tweets of the original COVID-19 RoBERTa model with its fine-tuned version, and from this analysis, we were able to draw conclusions on how to build more sophisticated models.
</details>
<details>
<summary>摘要</summary>
很多非典国家reported大量的M-pox cases since May 2022, causing concerns that the M-pox outbreak could rapidly escalate into another pandemic, while the COVID-19 pandemic continues to spread. Given the similarities between M-pox and COVID-19, we decided to test the performance of COVID-19 models trained on South African Twitter data on a manually labeled M-pox dataset before and after fine-tuning. Over 20,000 M-pox-related tweets from South Africa were manually labeled as positive, negative, or neutral. After fine-tuning these COVID-19 models on the M-pox dataset, the F1-scores increased by more than 8%, reaching nearly 70%, but still outperforming state-of-the-art models and well-known classification algorithms. Using an LDA-based topic modeling procedure, we compared the misclassified M-pox tweets of the original COVID-19 RoBERTa model with its fine-tuned version, and from this analysis, we were able to draw conclusions on how to build more sophisticated models.
</details></li>
</ul>
<hr>
<h2 id="AGIR-Automating-Cyber-Threat-Intelligence-Reporting-with-Natural-Language-Generation"><a href="#AGIR-Automating-Cyber-Threat-Intelligence-Reporting-with-Natural-Language-Generation" class="headerlink" title="AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation"></a>AGIR: Automating Cyber Threat Intelligence Reporting with Natural Language Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02655">http://arxiv.org/abs/2310.02655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Perrina, Francesco Marchiori, Mauro Conti, Nino Vincenzo Verde</li>
<li>for: 提高 Contemporary 风险管理策略中的Cyber Threat Intelligence（CTI）报告的自动化生成。</li>
<li>methods: 利用自然语言处理技术和STIX标准，提出AGIR（自动生成情报报告）工具，可以快速生成全面的CTI报告。</li>
<li>results: AGIR可以准确地传达正式语言表达的信息，提高了报告的流畅性和实用性，并且可以大幅减少CTI报告的写作时间，提高了CTI生成的效率。<details>
<summary>Abstract</summary>
Cyber Threat Intelligence (CTI) reporting is pivotal in contemporary risk management strategies. As the volume of CTI reports continues to surge, the demand for automated tools to streamline report generation becomes increasingly apparent. While Natural Language Processing techniques have shown potential in handling text data, they often struggle to address the complexity of diverse data sources and their intricate interrelationships. Moreover, established paradigms like STIX have emerged as de facto standards within the CTI community, emphasizing the formal categorization of entities and relations to facilitate consistent data sharing. In this paper, we introduce AGIR (Automatic Generation of Intelligence Reports), a transformative Natural Language Generation tool specifically designed to address the pressing challenges in the realm of CTI reporting. AGIR's primary objective is to empower security analysts by automating the labor-intensive task of generating comprehensive intelligence reports from formal representations of entity graphs. AGIR utilizes a two-stage pipeline by combining the advantages of template-based approaches and the capabilities of Large Language Models such as ChatGPT. We evaluate AGIR's report generation capabilities both quantitatively and qualitatively. The generated reports accurately convey information expressed through formal language, achieving a high recall value (0.99) without introducing hallucination. Furthermore, we compare the fluency and utility of the reports with state-of-the-art approaches, showing how AGIR achieves higher scores in terms of Syntactic Log-Odds Ratio (SLOR) and through questionnaires. By using our tool, we estimate that the report writing time is reduced by more than 40%, therefore streamlining the CTI production of any organization and contributing to the automation of several CTI tasks.
</details>
<details>
<summary>摘要</summary>
现代风险管理策略中，Cyber Threat Intelligence（CTI）报告的重要性日益凸显。随着CTI报告的数量不断增加，自动化工具的需求也日益凸显。然而，自然语言处理技术在处理文本数据方面表现出了潜在的优势，但它们通常无法处理多元数据源的复杂关系。此外，已有的标准如STIX在CTI社区中得到了广泛采用，强调通过正式分类实体和关系来促进数据共享的一致性。本文介绍一种名为AGIR（自动生成情报报告）的革命性自然语言生成工具，特点是为CTI报告自动生成全面的情报报告。AGIR使用了两个阶段管道，结合了模板方法和大语言模型如ChatGPT的优势。我们对AGIR的报告生成能力进行了量化和质量的评估。生成的报告准确地表达了通过正式语言表达的信息，具有高回归值（0.99）而无需幻化。此外，我们比较了AGIR的报告流畅性和实用性与现有方法，显示AGIR在SLOR和问卷方面的分数高于其他方法。通过使用我们的工具，我们估计CTI报告的写作时间可以减少超过40%，因此加速CTI生产和自动化一些CTI任务。
</details></li>
</ul>
<hr>
<h2 id="I-2-KD-SLU-An-Intra-Inter-Knowledge-Distillation-Framework-for-Zero-Shot-Cross-Lingual-Spoken-Language-Understanding"><a href="#I-2-KD-SLU-An-Intra-Inter-Knowledge-Distillation-Framework-for-Zero-Shot-Cross-Lingual-Spoken-Language-Understanding" class="headerlink" title="I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding"></a>I$^2$KD-SLU: An Intra-Inter Knowledge Distillation Framework for Zero-Shot Cross-Lingual Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02594">http://arxiv.org/abs/2310.02594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianjun Mao, Chenghong Zhang</li>
<li>for: 提高 zero-shot 跨语言语音理解（SLU）的性能，特别是在低资源语言中。</li>
<li>methods: 提出了一种基于知识内模型（KD）的混合知识馈送框架（I$^2$KD-SLU），以实现意向和插槽之间的互相引导。</li>
<li>results: 对于 MultiATIS++ 数据集，我们的提议的框架与强大的基线模型比较，显著提高了总准确率，并创造了跨语言 SLU 中新的状态势。<details>
<summary>Abstract</summary>
Spoken language understanding (SLU) typically includes two subtasks: intent detection and slot filling. Currently, it has achieved great success in high-resource languages, but it still remains challenging in low-resource languages due to the scarcity of labeled training data. Hence, there is a growing interest in zero-shot cross-lingual SLU. Despite of the success of existing zero-shot cross-lingual SLU models, most of them neglect to achieve the mutual guidance between intent and slots. To address this issue, we propose an Intra-Inter Knowledge Distillation framework for zero-shot cross-lingual Spoken Language Understanding (I$^2$KD-SLU) to model the mutual guidance. Specifically, we not only apply intra-knowledge distillation between intent predictions or slot predictions of the same utterance in different languages, but also apply inter-knowledge distillation between intent predictions and slot predictions of the same utterance. Our experimental results demonstrate that our proposed framework significantly improves the performance compared with the strong baselines and achieves the new state-of-the-art performance on the MultiATIS++ dataset, obtaining a significant improvement over the previous best model in overall accuracy.
</details>
<details>
<summary>摘要</summary>
通常的语音理解理解（SLU）包括两个子任务：意图检测和插槽填充。在高资源语言中，SLU已经取得了很大的成功，但在低资源语言中仍然存在很大的挑战，这是因为这些语言的标注训练数据的缺乏。因此，随着零shot cross-语言SLU的兴趣的增长，我们提出了一个Intra-Inter知识填充框架（I$^2$KD-SLU），以模型语音理解中的相互协作关系。具体来说，我们不仅在不同语言的同一个句子中进行了内知识填充，还进行了间知识填充，以确保意图和插槽之间的相互关系。我们的实验结果表明，我们提出的框架可以与强大的基线模型相比，并在MultiATIS++数据集上达到新的状态态标记，在总准确率方面取得了显著的提升。
</details></li>
</ul>
<hr>
<h2 id="NOLA-Networks-as-Linear-Combination-of-Low-Rank-Random-Basis"><a href="#NOLA-Networks-as-Linear-Combination-of-Low-Rank-Random-Basis" class="headerlink" title="NOLA: Networks as Linear Combination of Low Rank Random Basis"></a>NOLA: Networks as Linear Combination of Low Rank Random Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02556">http://arxiv.org/abs/2310.02556</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UCDvision/NOLA">https://github.com/UCDvision/NOLA</a></li>
<li>paper_authors: Soroush Abbasi Koohpayegani, KL Navaneet, Parsa Nooralinejad, Soheil Kolouri, Hamed Pirsiavash</li>
<li>for: 这 paper 旨在提高大型语言模型 (LLM) 的减少 Parameters 和存储方法，使其在不同的下游任务和领域中能够快速适应和提高性能。</li>
<li>methods: 这 paper 使用了 LoRA 方法，但是它面临两个主要的限制：1） parameter reduction 是固定的，即 rank one decomposition; 2） parameter reduction 受到模型架构和选择的排名影响。因此，这 paper 引入了 NOLA 方法，通过使用随机生成的基础 (matrix) 进行线性组合，并且只进行 linear mixture 的优化，以解耦参数数量和网络架构之间的关系。</li>
<li>results: 根据 GPT-2 和 ViT 在自然语言和计算机视觉任务中的适应结果，NOLA 能够与相等参数数量的模型进行比较，并且在更大的模型中可以减少参数数量一半，无需牺牲性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently gained popularity due to their impressive few-shot performance across various downstream tasks. However, fine-tuning all parameters and storing a unique model for each downstream task or domain becomes impractical because of the massive size of checkpoints (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: 1) the parameter reduction is lower-bounded by the rank one decomposition, and 2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. For instance, in larger models, even a rank one decomposition might exceed the number of parameters truly needed for adaptation. In this paper, we introduce NOLA, which overcomes the rank one lower bound present in LoRA. It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only. This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture. We present adaptation results using GPT-2 and ViT in natural language and computer vision tasks. NOLA performs as well as, or better than models with equivalent parameter counts. Furthermore, we demonstrate that we can halve the parameters in larger models compared to LoRA with rank one, without sacrificing performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）最近受到了广泛关注，因为它们在多个下游任务中表现出了惊人的几次训练成果。然而，对于每个下游任务或领域都需要独立存储和参数调整的方法成为不实际，因为模型检查点的大小（例如GPT-3的350GB）。现有文献，如LoRA，表明了使用低级 modificatioin 方法可以提高LLM的效率和存储。这些方法可以将LLM的参数数量减少到数个数量级。然而，这些方法受到两个主要限制：1）参数减少是基于一个约束的，2）减少的程度受到模型架构和选择的级别的影响。例如，在更大的模型中，即使使用约束一 decomposition，也可能超过实际需要的参数数量。在这篇论文中，我们介绍了NOLA，它可以超越LoRA中的约束一lower bound。它通过将LoRA中的低级矩阵重新参数化为线性组合的随机生成矩阵（基准）和优化线性混合系数，从而解耦参数数量与选择级别和网络架构之间的关系。我们在GPT-2和ViT上进行了自适应任务，并得到了与相同参数数量的性能。此外，我们还证明了可以在更大的模型中减少参数数量，而不会影响性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/04/cs.CL_2023_10_04/" data-id="closbron100c90g88ahmkcifd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/04/cs.LG_2023_10_04/" class="article-date">
  <time datetime="2023-10-04T10:00:00.000Z" itemprop="datePublished">2023-10-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/04/cs.LG_2023_10_04/">cs.LG - 2023-10-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="PDR-CapsNet-an-Energy-Efficient-Parallel-Approach-to-Dynamic-Routing-in-Capsule-Networks"><a href="#PDR-CapsNet-an-Energy-Efficient-Parallel-Approach-to-Dynamic-Routing-in-Capsule-Networks" class="headerlink" title="PDR-CapsNet: an Energy-Efficient Parallel Approach to Dynamic Routing in Capsule Networks"></a>PDR-CapsNet: an Energy-Efficient Parallel Approach to Dynamic Routing in Capsule Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03212">http://arxiv.org/abs/2310.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samaneh Javadinia, Amirali Baniasadi</li>
<li>for: 提高图像分类任务中Capsule Networks（CapsNet）的性能，并且减少计算资源消耗。</li>
<li>methods: 引入并研究了并行动态路径分配（Parallel Dynamic Routing，PDR）技术，以提高CapsNet的性能和可扩展性。</li>
<li>results: 比较CapsNet和PDR-CapsNet在CIFAR-10 dataset上的性能，PDR-CapsNet具有83.55%的准确率，需要87.26% fewer parameters，32.27%和47.40% fewer MACs和Flops，并且实现了3倍快的推理和7.29J less energy consumption。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have produced state-of-the-art results for image classification tasks. However, they are limited in their ability to handle rotational and viewpoint variations due to information loss in max-pooling layers. Capsule Networks (CapsNets) employ a computationally-expensive iterative process referred to as dynamic routing to address these issues. CapsNets, however, often fall short on complex datasets and require more computational resources than CNNs. To overcome these challenges, we introduce the Parallel Dynamic Routing CapsNet (PDR-CapsNet), a deeper and more energy-efficient alternative to CapsNet that offers superior performance, less energy consumption, and lower overfitting rates. By leveraging a parallelization strategy, PDR-CapsNet mitigates the computational complexity of CapsNet and increases throughput, efficiently using hardware resources. As a result, we achieve 83.55\% accuracy while requiring 87.26\% fewer parameters, 32.27\% and 47.40\% fewer MACs, and Flops, achieving 3x faster inference and 7.29J less energy consumption on a 2080Ti GPU with 11GB VRAM compared to CapsNet and for the CIFAR-10 dataset.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在图像分类任务中具有状态机器人的表现，但它们由于堆叠 Pooling 层所导致的信息损失而受限。卷积神经网络（CapsNet）采用了 computationally 昂贵的迭代过程来解决这些问题，但它们在复杂的数据集上经常表现不佳，需要更多的计算资源 than CNN。为了解决这些挑战，我们提出了并行动态路由 CapsNet（PDR-CapsNet），这是一种更深度的和更加能效的 CapsNet 选择。通过利用并行策略，PDR-CapsNet 减少了 CapsNet 的计算复杂度，提高了通过put和硬件资源的使用效率。因此，我们在 CIFAR-10 数据集上 achieve 83.55% 的准确率，需要 87.26%  fewer parameters，32.27% 和 47.40%  fewer MACs 和 Flops，实现了 3 倍 быстре的推理和 7.29J 更少的能 consumption 在一个 2080Ti GPU 上。
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-Distributed-Online-Control-for-LTI-Systems-with-Adversarial-Disturbances"><a href="#Regret-Analysis-of-Distributed-Online-Control-for-LTI-Systems-with-Adversarial-Disturbances" class="headerlink" title="Regret Analysis of Distributed Online Control for LTI Systems with Adversarial Disturbances"></a>Regret Analysis of Distributed Online Control for LTI Systems with Adversarial Disturbances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03206">http://arxiv.org/abs/2310.03206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Jui Chang, Shahin Shahrampour</li>
<li>for: 这个论文处理的问题是如何在一个分布式线性时对应系统（可能有未知动力学）下实现分布式在线控制，并且在敌对干扰下实现最佳中央控制策略。</li>
<li>methods: 这个论文使用了分布式发现调节器，并且在知道系统动力学时使用了对称矩阵估计，而在未知系统动力学时使用了分布式探索-then-确认方法。</li>
<li>results: 这个论文获得了一个 regret bound of $O(\sqrt{T}\log T)$ 以及 $O(T^{2&#x2F;3} \text{poly}(\log T))$，这表示在不同的时间长度下，这个分布式控制策略可以实现最佳中央控制策略。<details>
<summary>Abstract</summary>
This paper addresses the distributed online control problem over a network of linear time-invariant (LTI) systems (with possibly unknown dynamics) in the presence of adversarial perturbations. There exists a global network cost that is characterized by a time-varying convex function, which evolves in an adversarial manner and is sequentially and partially observed by local agents. The goal of each agent is to generate a control sequence that can compete with the best centralized control policy in hindsight, which has access to the global cost. This problem is formulated as a regret minimization. For the case of known dynamics, we propose a fully distributed disturbance feedback controller that guarantees a regret bound of $O(\sqrt{T}\log T)$, where $T$ is the time horizon. For the unknown dynamics case, we design a distributed explore-then-commit approach, where in the exploration phase all agents jointly learn the system dynamics, and in the learning phase our proposed control algorithm is applied using each agent system estimate. We establish a regret bound of $O(T^{2/3} \text{poly}(\log T))$ for this setting.
</details>
<details>
<summary>摘要</summary>
For the case of known dynamics, a fully distributed disturbance feedback controller is proposed, which guarantees a regret bound of $O(\sqrt{T}\log T)$, where $T$ is the time horizon. For the case of unknown dynamics, a distributed explore-then-commit approach is designed, where all agents jointly learn the system dynamics in the exploration phase, and the proposed control algorithm is applied using each agent's system estimate in the learning phase. The regret bound for this setting is established as $O(T^{2/3} \text{poly}(\log T))$.
</details></li>
</ul>
<hr>
<h2 id="ProGO-Probabilistic-Global-Optimizer"><a href="#ProGO-Probabilistic-Global-Optimizer" class="headerlink" title="ProGO: Probabilistic Global Optimizer"></a>ProGO: Probabilistic Global Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04457">http://arxiv.org/abs/2310.04457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Sujit Ghosh</li>
<li>for: 提高global optimization中存在许多挑战，包括非几何目标函数和高计算复杂性或计算梯度信息不可用。这些限制使得许多算法无法找到最优解或者失败 converges。</li>
<li>methods: 我们开发了一系列基于多维 интеграル的方法，这些方法可以在一些某些软件的假设下 converge 到全球最优解。我们的概率方法不需要使用梯度，而是基于数学上的准确封闭框架，这使得它能够在缺乏梯度信息的情况下提供有效的优化方案。为了缓解多维 интеграル的问题，我们开发了一种秘密批处理器，它可以在高速地生成来自全球最优解的样本，这些样本可以用于估算全球最优解。</li>
<li>results: 我们的方法在许多流行的非几何测试函数上表现出色，比如有限全球最优解的函数。与许多现有的状态先进方法相比，我们的方法在 regret 值和速度上表现出了许多的提升，达到了许多的目标函数最优解。然而，我们的方法可能不适用于计算成本高的函数。<details>
<summary>Abstract</summary>
In the field of global optimization, many existing algorithms face challenges posed by non-convex target functions and high computational complexity or unavailability of gradient information. These limitations, exacerbated by sensitivity to initial conditions, often lead to suboptimal solutions or failed convergence. This is true even for Metaheuristic algorithms designed to amalgamate different optimization techniques to improve their efficiency and robustness. To address these challenges, we develop a sequence of multidimensional integration-based methods that we show to converge to the global optima under some mild regularity conditions. Our probabilistic approach does not require the use of gradients and is underpinned by a mathematically rigorous convergence framework anchored in the nuanced properties of nascent optima distribution. In order to alleviate the problem of multidimensional integration, we develop a latent slice sampler that enjoys a geometric rate of convergence in generating samples from the nascent optima distribution, which is used to approximate the global optima. The proposed Probabilistic Global Optimizer (ProGO) provides a scalable unified framework to approximate the global optima of any continuous function defined on a domain of arbitrary dimension. Empirical illustrations of ProGO across a variety of popular non-convex test functions (having finite global optima) reveal that the proposed algorithm outperforms, by order of magnitude, many existing state-of-the-art methods, including gradient-based, zeroth-order gradient-free, and some Bayesian Optimization methods, in term regret value and speed of convergence. It is, however, to be noted that our approach may not be suitable for functions that are expensive to compute.
</details>
<details>
<summary>摘要</summary>
在全球优化领域，许多现有的算法面临非凸目标函数和高计算复杂性或求导函数信息的不可得性的挑战。这些限制，受初始条件的敏感性的影响，常导致优化解不 optimal或失败 convergence。这是真的，即使使用Metaheuristic算法，这些算法是用来混合不同的优化技术以提高其效率和可靠性。为了解决这些挑战，我们开发了一个序列的多维 интеграル基于方法，我们证明它们可以 converge to the global optima under some mild regularity conditions.我们的 probabilistic approach不需要使用 gradients，并且基于数学上的准确的 convergence 框架，anchored in the nuanced properties of nascent optima distribution。为了缓解多维 интеграル的问题，我们开发了一个latent slice sampler，它 enjoys a geometric rate of convergence in generating samples from the nascent optima distribution，这些样本用于approximate the global optima。我们提出的Probabilistic Global Optimizer (ProGO) 提供了一个可扩展的统一框架，用于 aproximate the global optima of any continuous function defined on a domain of arbitrary dimension.empirical examples of ProGO across a variety of popular non-convex test functions (having finite global optima) reveal that the proposed algorithm outperforms, by order of magnitude, many existing state-of-the-art methods, including gradient-based, zeroth-order gradient-free, and some Bayesian Optimization methods, in terms of regret value and speed of convergence. However, it is worth noting that our approach may not be suitable for functions that are expensive to compute.
</details></li>
</ul>
<hr>
<h2 id="Digital-Ethics-in-Federated-Learning"><a href="#Digital-Ethics-in-Federated-Learning" class="headerlink" title="Digital Ethics in Federated Learning"></a>Digital Ethics in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03178">http://arxiv.org/abs/2310.03178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangqi Yuan, Ziran Wang, Christopher G. Brinton</li>
<li>for: 这篇论文主要针对的是在互联网物联网（IoT）中实现隐私保护和数据有效利用的 Federated Learning（FL）技术，以及在FL中人类中心设备作为客户端时出现的数字伦理问题。</li>
<li>methods: 这篇论文使用了机器学习（ML）模型参数共享的方式来实现多方合作，并分析了客户端和服务器之间的视角和目标不同而导致的挑战，以及这些挑战的解决方案。</li>
<li>results: 论文分析了在中央化和分散化FL中Client端的挑战，并探讨了FL在人类中心IoT中的发展前景。<details>
<summary>Abstract</summary>
The Internet of Things (IoT) consistently generates vast amounts of data, sparking increasing concern over the protection of data privacy and the limitation of data misuse. Federated learning (FL) facilitates collaborative capabilities among multiple parties by sharing machine learning (ML) model parameters instead of raw user data, and it has recently gained significant attention for its potential in privacy preservation and learning efficiency enhancement. In this paper, we highlight the digital ethics concerns that arise when human-centric devices serve as clients in FL. More specifically, challenges of game dynamics, fairness, incentive, and continuity arise in FL due to differences in perspectives and objectives between clients and the server. We analyze these challenges and their solutions from the perspectives of both the client and the server, and through the viewpoints of centralized and decentralized FL. Finally, we explore the opportunities in FL for human-centric IoT as directions for future development.
</details>
<details>
<summary>摘要</summary>
互联网物联网（IoT）不断生成巨量数据，引起了数据隐私保护和数据滥用限制的担忧。联邦学习（FL）可以在多方协作中分享机器学习模型参数而不是原始用户数据，因此它在隐私保护和学习效率提高方面受到了广泛关注。本文探讨在人性化设备作为FL客户端时出现的数字道德问题。具体来说，FL中的游戏dinamica、公平、奖励和继承问题由客户端和服务器之间的视角和目标差异引起。我们从客户端和服务器的角度分析这些挑战，并通过中央化和分布式FL的视角进行解读。最后，我们探讨FL在人性化IoT方面的发展机遇。
</details></li>
</ul>
<hr>
<h2 id="Test-Case-Recommendations-with-Distributed-Representation-of-Code-Syntactic-Features"><a href="#Test-Case-Recommendations-with-Distributed-Representation-of-Code-Syntactic-Features" class="headerlink" title="Test Case Recommendations with Distributed Representation of Code Syntactic Features"></a>Test Case Recommendations with Distributed Representation of Code Syntactic Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03174">http://arxiv.org/abs/2310.03174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mosabrezaei/test-case-recommendation">https://github.com/mosabrezaei/test-case-recommendation</a></li>
<li>paper_authors: Mosab Rezaei, Hamed Alhoori, Mona Rahimi</li>
<li>for: 提高软件测试效率和效果，自动生成和维护测试单元。</li>
<li>methods: 使用神经网络模型，根据源代码方法和测试单元的结构和Semantic特征，计算cosine相似性并提取相似测试单元。</li>
<li>results: 在 Methods2Test 数据集上，提出的方法可以自动找到最相似的测试单元，减少开发人员的测试单元生成努力。<details>
<summary>Abstract</summary>
Frequent modifications of unit test cases are inevitable due to software's continuous underlying changes in source code, design, and requirements. Since manually maintaining software test suites is tedious, timely, and costly, automating the process of generation and maintenance of test units will significantly impact the effectiveness and efficiency of software testing processes.   To this end, we propose an automated approach which exploits both structural and semantic properties of source code methods and test cases to recommend the most relevant and useful unit tests to the developers. The proposed approach initially trains a neural network to transform method-level source code, as well as unit tests, into distributed representations (embedded vectors) while preserving the importance of the structure in the code. Retrieving the semantic and structural properties of a given method, the approach computes cosine similarity between the method's embedding and the previously-embedded training instances. Further, according to the similarity scores between the embedding vectors, the model identifies the closest methods of embedding and the associated unit tests as the most similar recommendations.   The results on the Methods2Test dataset showed that, while there is no guarantee to have similar relevant test cases for the group of similar methods, the proposed approach extracts the most similar existing test cases for a given method in the dataset, and evaluations show that recommended test cases decrease the developers' effort to generating expected test cases.
</details>
<details>
<summary>摘要</summary>
频繁修改单元测试用例是软件开发中不可避免的，因为软件源代码、设计和需求都在不断发生变化。由于手动维护软件测试用例是费时、费力和成本高昂的，因此自动化测试用例生成和维护的过程将对软件测试过程产生深远的影响。为此，我们提出一种自动化方法，利用源代码方法和单元测试用例的结构和 semantics 属性来推荐最相关和有用的单元测试用例给开发者。该方法首先使用神经网络将方法级源代码和单元测试用例转换成分布式表示（嵌入向量），保留代码结构的重要性。根据方法的semantics和结构特征，该方法计算cosine相似性 между方法的嵌入向量和已经训练的实例。根据嵌入向量的相似性分数，模型标识最相似的方法和相关单元测试用例。在 Methods2Test 数据集上进行了实验，结果表明，虽然无法保证与给定方法集合相似的测试用例，但是提议的测试用例仍然能够捕捉到给定方法的关键特征，并且评估表明，建议的测试用例可以减少开发者的努力来生成预期的测试用例。
</details></li>
</ul>
<hr>
<h2 id="Raze-to-the-Ground-Query-Efficient-Adversarial-HTML-Attacks-on-Machine-Learning-Phishing-Webpage-Detectors"><a href="#Raze-to-the-Ground-Query-Efficient-Adversarial-HTML-Attacks-on-Machine-Learning-Phishing-Webpage-Detectors" class="headerlink" title="Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors"></a>Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03166">http://arxiv.org/abs/2310.03166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/advmlphish/raze_to_the_ground_aisec23">https://github.com/advmlphish/raze_to_the_ground_aisec23</a></li>
<li>paper_authors: Biagio Montaruli, Luca Demetrio, Maura Pintor, Luca Compagna, Davide Balzarotti, Battista Biggio</li>
<li>for: 这种研究是为了提高机器学习式钓鱼网页检测器的安全性，并对现有的攻击方法进行改进。</li>
<li>methods: 该研究使用了一种新的细化的攻击方法，可以无需改变恶意网页的功能和显示效果，同时可以充分利用攻击者所采用的攻击方法。</li>
<li>results: 该研究的实验结果显示，使用该新的攻击方法可以让现有的机器学习式钓鱼网页检测器的性能受到严重的损害，只需要30个查询即可。<details>
<summary>Abstract</summary>
Machine-learning phishing webpage detectors (ML-PWD) have been shown to suffer from adversarial manipulations of the HTML code of the input webpage. Nevertheless, the attacks recently proposed have demonstrated limited effectiveness due to their lack of optimizing the usage of the adopted manipulations, and they focus solely on specific elements of the HTML code. In this work, we overcome these limitations by first designing a novel set of fine-grained manipulations which allow to modify the HTML code of the input phishing webpage without compromising its maliciousness and visual appearance, i.e., the manipulations are functionality- and rendering-preserving by design. We then select which manipulations should be applied to bypass the target detector by a query-efficient black-box optimization algorithm. Our experiments show that our attacks are able to raze to the ground the performance of current state-of-the-art ML-PWD using just 30 queries, thus overcoming the weaker attacks developed in previous work, and enabling a much fairer robustness evaluation of ML-PWD.
</details>
<details>
<summary>摘要</summary>
machine learning钓鱼网页探测器（ML-PWD）已经被证明容易受到敏感HTML代码的攻击。然而，最近提出的攻击方法具有有限的效iveness，因为它们只ocus于特定的HTML代码元素，而不是优化使用这些攻击。在这种情况下，我们超越这些限制，首先设计了一个新的细化的攻击方法，可以无需改变恶意网页的功能和rendering，即这些攻击是功能和rendering保持的设计。然后，我们使用一种高效的黑盒优化算法选择最佳的攻击方法，以击败目标探测器。我们的实验结果表明，我们的攻击可以很快地击败当前领先的ML-PWD，只需30个查询， thus overcome the weaker attacks proposed in previous work, and enable a more fair robustness evaluation of ML-PWD.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Accuracy-in-Deep-Learning-Using-Random-Matrix-Theory"><a href="#Enhancing-Accuracy-in-Deep-Learning-Using-Random-Matrix-Theory" class="headerlink" title="Enhancing Accuracy in Deep Learning Using Random Matrix Theory"></a>Enhancing Accuracy in Deep Learning Using Random Matrix Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03165">http://arxiv.org/abs/2310.03165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonid Berlyand, Etienne Sandier, Yitzchak Shmalo, Lei Zhang</li>
<li>For: The paper explores the application of random matrix theory (RMT) in the training of deep neural networks (DNNs) to simplify DNN architecture and improve accuracy.* Methods: The paper uses techniques from RMT to determine the number of singular values to be removed from the weight layers of a DNN during training, specifically via singular value decomposition (SVD).* Results: The paper shows that the proposed method can be applied to any fully connected or convolutional layer of a pretrained DNN, reducing the layer’s parameters and simplifying the DNN architecture while preserving or even enhancing the model’s accuracy. Empirical evidence is provided on the MNIST and Fashion MNIST datasets.Here’s the same information in Simplified Chinese text:* 为：本文研究了深度神经网络（DNN）训练中随机矩阵理论（RMT）的应用，以简化DNN结构和提高准确性。* 方法：本文使用RMT技术确定在DNN训练中去除weight层中的小特征值，具体来说是通过特征值分解（SVD）。* 结果：本文证明该方法可以应用于任何已经训练过的层，包括卷积层和全连接层，从而减少层的参数数量，简化DNN结构，同时保持或者提高模型的准确性。实验证明在MNIST和Fashion MNIST datasets上的效果。<details>
<summary>Abstract</summary>
In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.   Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DNN training without compromising performance.   We provide both theoretical and empirical evidence supporting our claim that the elimination of small singular values based on RMT does not negatively impact the DNN's accuracy. Our results offer valuable insights into the practical application of RMT for the creation of more efficient and accurate deep-learning models.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们探讨了深度神经网络（DNN）训练中Random Matrix Theory（RMT）的应用，特icularly focusing on layer pruning to simplify DNN architecture and loss landscape. RMT，最近在深度学习中使用以解决过拟合问题，允许我们研究DNN的Weight层спектrum。我们使用这些技术来优化DNN中Weight层中的 singular value数量，以便在训练过程中去除不必要的参数，从而简化DNN结构并提高准确性。我们通过在MNIST和Fashion MNIST数据集上训练简单的DNN模型，证明了我们的方法可以应用于任何已经训练过的完全连接或卷积层。我们的方法可以降低层的参数数量，简化DNN结构，同时保持或者提高模型的准确性。通过基于RMT的标准做法，我们可以确定要从Weight层中去除的小特征值，从而保持测试集的准确率不变，实现更高效的DNN训练而不损失性能。我们提供了 both theoretical and empirical evidence，证明了我们的方法不会对DNN的准确性产生负面影响。我们的结果为创建更高效和准确的深度学习模型提供了有价值的实践经验。
</details></li>
</ul>
<hr>
<h2 id="FedNAR-Federated-Optimization-with-Normalized-Annealing-Regularization"><a href="#FedNAR-Federated-Optimization-with-Normalized-Annealing-Regularization" class="headerlink" title="FedNAR: Federated Optimization with Normalized Annealing Regularization"></a>FedNAR: Federated Optimization with Normalized Annealing Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03163">http://arxiv.org/abs/2310.03163</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljb121002/fednar">https://github.com/ljb121002/fednar</a></li>
<li>paper_authors: Junbo Li, Ang Li, Chong Tian, Qirong Ho, Eric P. Xing, Hongyi Wang</li>
<li>for: 提高 Federated Learning（FL）中的泛化性能，防止本地客户端的逻辑泛化。</li>
<li>methods: 提出了一种名为“Normalized Annealing Regularization”（FedNAR）的简单 yet effective的算法插件，可以轻松地整合到任何现有的 FL 算法中。FedNAR 通过控制每次更新的规模，通过矩阵和权重减少来实现。</li>
<li>results: 对于视觉和语言 datasets 进行了广泛的实验，结果表明，在不同的背景 federated optimization 算法中，权重减少可以加速涨化和提高模型精度。另外，FedNAR 具有自适应性，可以根据初始化参数的不合理性自动调整权重减少，而传统 FL 算法的精度则会明显下降。<details>
<summary>Abstract</summary>
Weight decay is a standard technique to improve generalization performance in modern deep neural network optimization, and is also widely adopted in federated learning (FL) to prevent overfitting in local clients. In this paper, we first explore the choices of weight decay and identify that weight decay value appreciably influences the convergence of existing FL algorithms. While preventing overfitting is crucial, weight decay can introduce a different optimization goal towards the global objective, which is further amplified in FL due to multiple local updates and heterogeneous data distribution. To address this challenge, we develop {\it Federated optimization with Normalized Annealing Regularization} (FedNAR), a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms. Essentially, we regulate the magnitude of each update by performing co-clipping of the gradient and weight decay. We provide a comprehensive theoretical analysis of FedNAR's convergence rate and conduct extensive experiments on both vision and language datasets with different backbone federated optimization algorithms. Our experimental results consistently demonstrate that incorporating FedNAR into existing FL algorithms leads to accelerated convergence and heightened model accuracy. Moreover, FedNAR exhibits resilience in the face of various hyperparameter configurations. Specifically, FedNAR has the ability to self-adjust the weight decay when the initial specification is not optimal, while the accuracy of traditional FL algorithms would markedly decline. Our codes are released at \href{https://github.com/ljb121002/fednar}{https://github.com/ljb121002/fednar}.
</details>
<details>
<summary>摘要</summary>
modern deep neural network优化中通用的一种常用技术是Weight decay，用于提高泛化性表现。在这篇论文中，我们首先探讨了Weight decay的选择，并发现Weight decay值对现有的FL算法的 convergence有显著影响。虽然预防过拟合是关键，但Weight decay可能会引入一种新的优化目标，这在FL中更加明显，因为多个本地更新和不同的数据分布。为解决这个挑战，我们开发了{\it Federated optimization with Normalized Annealing Regularization}（FedNAR）算法，这是一种简单 yet effective和多用的插件。我们在每次更新中规定了梯度和Weight decay的范围，通过共同clip来调整每次更新的大小。我们提供了FedNAR的 konvergence rate的完整理论分析，并在视觉和语言数据集上进行了广泛的实验，包括不同的背景 federated optimization算法。我们的实验结果 consistently示出，在把FedNAR incorporated into existing FL algorithms时，可以加速 konvergence和提高模型精度。此外，FedNAR具有自适应Weight decay的能力，当初始参数不合适时，FedNAR可以自动调整Weight decay，而传统FL算法的精度则会明显下降。我们的代码可以在 \href{https://github.com/ljb121002/fednar}{https://github.com/ljb121002/fednar} 上获取。
</details></li>
</ul>
<hr>
<h2 id="FedHyper-A-Universal-and-Robust-Learning-Rate-Scheduler-for-Federated-Learning-with-Hypergradient-Descent"><a href="#FedHyper-A-Universal-and-Robust-Learning-Rate-Scheduler-for-Federated-Learning-with-Hypergradient-Descent" class="headerlink" title="FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent"></a>FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03156">http://arxiv.org/abs/2310.03156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyao Wang, Jianyu Wang, Ang Li</li>
<li>for: 本文主要针对 Federated Learning (FL) 实际应用中遇到的复杂挑战之一：hyperparameter 优化。</li>
<li>methods: 本文提出了一种基于 hypergradient 的学习率调整算法，名为 FedHyper，可以在 FL 中适应学习率的变化。FedHyper 可以适应全局和本地学习率的调整，并且在不同的初始学习率设置下保持稳定性。</li>
<li>results: 实验结果表明，FedHyper 能够在视觉和语言 benchmark 数据集上快速收敛，比 FedAvg 和竞争对手快速收敛 1.1-3 倍，并且在不良初始学习率设置下可以提高最终准确率。此外，FedHyper 可以在不同的初始学习率设置下提高准确率，最高提高 15%。<details>
<summary>Abstract</summary>
The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper's convergence rate and conduct extensive experiments on vision and language benchmark datasets. The results demonstrate that FEDHYPER consistently converges 1.1-3x faster than FedAvg and the competing baselines while achieving superior final accuracy. Moreover, FedHyper catalyzes a remarkable surge in accuracy, augmenting it by up to 15% compared to FedAvg under suboptimal initial learning rate settings.
</details>
<details>
<summary>摘要</summary>
理论上，聚合学习（FL）的应用面临着许多复杂的挑战，其中一个重要的挑战是调节超参数。在这些超参数中，学习率的调整是一个关键的组成部分，可以显著提高FL系统的效果。为回应这个急需，这篇论文提出了FedHyper，一种特有的学习率调整算法，专门适用于FL。FedHyper可以适应全局和本地学习率的调整，并且不仅能够在不同的初始学习率设置下保持稳定的性能，还能够减少手动调整学习率的劳动。我们提供了FL的整体分析，并对视觉和语言benchmark datasets进行了广泛的实验。结果显示，FedHyper可以在1.1-3x的速度上 converges，同时也可以在不同的初始学习率设置下达到最高的终端准确率。此外，FedHyper可以在初始学习率设置不佳的情况下带来很大的提升，提高终端准确率达到15%。
</details></li>
</ul>
<hr>
<h2 id="Towards-out-of-distribution-generalizable-predictions-of-chemical-kinetics-properties"><a href="#Towards-out-of-distribution-generalizable-predictions-of-chemical-kinetics-properties" class="headerlink" title="Towards out-of-distribution generalizable predictions of chemical kinetics properties"></a>Towards out-of-distribution generalizable predictions of chemical kinetics properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03152">http://arxiv.org/abs/2310.03152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Wang, Yongqiang Chen, Yang Duan, Weijiang Li, Bo Han, James Cheng, Hanghang Tong</li>
<li>for: 这篇论文的目的是提出一种基于机器学习技术的化学反应速率预测方法，以满足高通过率的化学合成过程的设计。</li>
<li>methods: 该方法使用了现有的机器学习方法进行反应预测，并将其分为三级别（结构、条件和机制），以探讨不同级别的问题。</li>
<li>results: 研究结果表明，现有的机器学习方法在不同级别的问题上存在挑战和机遇，并提供了一些可能的解决方案。<details>
<summary>Abstract</summary>
Machine Learning (ML) techniques have found applications in estimating chemical kinetics properties. With the accumulated drug molecules identified through "AI4drug discovery", the next imperative lies in AI-driven design for high-throughput chemical synthesis processes, with the estimation of properties of unseen reactions with unexplored molecules. To this end, the existing ML approaches for kinetics property prediction are required to be Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD kinetic property prediction into three levels (structure, condition, and mechanism), revealing unique aspects of such problems. Under this framework, we create comprehensive datasets to benchmark (1) the state-of-the-art ML approaches for reaction prediction in the OOD setting and (2) the state-of-the-art graph OOD methods in kinetics property prediction problems. Our results demonstrated the challenges and opportunities in OOD kinetics property prediction. Our datasets and benchmarks can further support research in this direction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Federated-Fine-Tuning-of-LLMs-on-the-Very-Edge-The-Good-the-Bad-the-Ugly"><a href="#Federated-Fine-Tuning-of-LLMs-on-the-Very-Edge-The-Good-the-Bad-the-Ugly" class="headerlink" title="Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly"></a>Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03150">http://arxiv.org/abs/2310.03150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Herbert Woisetschläger, Alexander Isenko, Shiqiang Wang, Ruben Mayer, Hans-Arno Jacobsen</li>
<li>for: This paper explores the use of Federated Learning (FL) to bring large language models (LLMs) to modern edge computing systems.</li>
<li>methods: The paper fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. The study also provides a micro-level hardware benchmark and compares the model FLOP utilization to a state-of-the-art data center GPU.</li>
<li>results: The paper evaluates the current capabilities of edge computing systems and their potential for LLM FL workloads, and demonstrates the potential for improvement and the next steps toward achieving greater computational efficiency at the edge.<details>
<summary>Abstract</summary>
Large Language Models (LLM) and foundation models are popular as they offer new opportunities for individuals and businesses to improve natural language processing, interact with data, and retrieve information faster. However, training or fine-tuning LLMs requires a vast amount of data, which can be challenging to access due to legal or technical restrictions and may require private computing resources. Federated Learning (FL) is a solution designed to overcome these challenges and expand data access for deep learning applications.   This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. We provide a micro-level hardware benchmark, compare the model FLOP utilization to a state-of-the-art data center GPU, and study the network utilization in realistic conditions. Our contribution is twofold: First, we evaluate the current capabilities of edge computing systems and their potential for LLM FL workloads. Second, by comparing these systems with a data-center GPU, we demonstrate the potential for improvement and the next steps toward achieving greater computational efficiency at the edge.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和基础模型在使用中变得普遍，因为它们提供了新的机会，让个人和企业可以更好地处理自然语言，与数据进行交互，并更快地获取信息。然而，训练或精度调整LLM需要庞大量数据，这可能会因为法律或技术限制而困难以获取，并可能需要专用的计算资源。联邦学习（FL）是一种解决这些问题的方案，以扩展深入学习应用程序的数据访问权限。本文从硬件角度来探讨如何将LLM带到现代边缘计算系统中。我们的研究将Flan-T5模型家族，从80M到3B参数，使用FL进行文本摘要任务进行细粒度硬件指标，并与当前最佳的数据中心GPU进行比较。我们的贡献有两个方面：首先，我们评估边缘计算系统的当前能力和LLM FL工作负荷的潜在可能性。其次，我们通过与数据中心GPU进行比较，探讨在现实条件下的网络利用率和计算效率的可能性。我们的贡献在于，我们提供了一个硬件指标，并证明了边缘计算系统在LLM FL工作负荷下的可能性和下一步的改进方向。
</details></li>
</ul>
<hr>
<h2 id="Fairness-enhancing-mixed-effects-deep-learning-improves-fairness-on-in-and-out-of-distribution-clustered-non-iid-data"><a href="#Fairness-enhancing-mixed-effects-deep-learning-improves-fairness-on-in-and-out-of-distribution-clustered-non-iid-data" class="headerlink" title="Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data"></a>Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03146">http://arxiv.org/abs/2310.03146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Wang, Son Nguyen, Albert Montillo</li>
<li>for: This paper aims to address two core problems in traditional deep learning (DL) by introducing a mixed effects deep learning (MEDL) framework that promotes fairness and robustness.</li>
<li>methods: The MEDL framework separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) using a cluster adversary and a Bayesian neural network. The framework also incorporates adversarial debiasing to promote equality-of-odds fairness across fairness-sensitive variables.</li>
<li>results: The paper shows that the MEDL framework notably enhances fairness across all sensitive variables, increasing fairness up to 82% for age, 43% for race, 86% for sex, and 27% for marital-status, while maintaining robust performance and clarity. The framework is versatile and suitable for various dataset types and tasks, making it broadly applicable.<details>
<summary>Abstract</summary>
Traditional deep learning (DL) suffers from two core problems. Firstly, it assumes training samples are independent and identically distributed. However, numerous real-world datasets group samples by shared measurements (e.g., study participants or cells), violating this assumption. In these scenarios, DL can show compromised performance, limited generalization, and interpretability issues, coupled with cluster confounding causing Type 1 and 2 errors. Secondly, models are typically trained for overall accuracy, often neglecting underrepresented groups and introducing biases in crucial areas like loan approvals or determining health insurance rates, such biases can significantly impact one's quality of life. To address both of these challenges simultaneously, we present a mixed effects deep learning (MEDL) framework. MEDL separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) through the introduction of: 1) a cluster adversary which encourages the learning of cluster-invariant FE, 2) a Bayesian neural network which quantifies the RE, and a mixing function combining the FE an RE into a mixed-effect prediction. We marry this MEDL with adversarial debiasing, which promotes equality-of-odds fairness across FE, RE, and ME predictions for fairness-sensitive variables. We evaluated our approach using three datasets: two from census/finance focusing on income classification and one from healthcare predicting hospitalization duration, a regression task. Our framework notably enhances fairness across all sensitive variables-increasing fairness up to 82% for age, 43% for race, 86% for sex, and 27% for marital-status. Besides promoting fairness, our method maintains the robust performance and clarity of MEDL. It's versatile, suitable for various dataset types and tasks, making it broadly applicable. Our GitHub repository houses the implementation.
</details>
<details>
<summary>摘要</summary>
传统的深度学习（DL）受到两个核心问题的影响。首先，它假设训练样本是独立并且具有相同的分布。然而，许多实际世界数据集中的样本会根据共同的测量结果（如参与研究的人员或细胞）分组，这会违反这个假设，从而导致深度学习的性能受损，降低了通用化和解释性。其次，模型通常会为总准确率培育，而忽略少数群体，这会导致偏见问题，例如贷款批准或医疗保险费率的偏见，这些偏见可能会对人们的生活质量产生重要影响。为解决这两个挑战，我们提出了混合效果深度学习（MEDL）框架。MEDL分别量化分组 invariant fixed effects（FE）和分组特有的随机效果（RE），通过引入：1）分组反对者，使学习分组 invariant FE，2）bayesian neural network，量化 RE，3）混合函数，将 FE 和 RE 组合成混合效果预测。我们将这种 MEDL 结合了反对批判，以便实现 equality-of-odds 公平性 across FE, RE, 和 ME 预测中的敏感变量。我们使用三个数据集进行评估：两个来自人口/金融，关注收入分类，一个来自医疗领域，预测医院住院时间，一个回归任务。我们的框架可以明显提高公平性，对所有敏感变量的公平性提高至82%，43%，86% 和 27%。此外，我们的方法保持了MEDL的稳定性和清晰度，并且可以适用于不同的数据类型和任务，因此广泛应用。我们的 GitHub 存储库中包含实现。
</details></li>
</ul>
<hr>
<h2 id="OpenMM-8-Molecular-Dynamics-Simulation-with-Machine-Learning-Potentials"><a href="#OpenMM-8-Molecular-Dynamics-Simulation-with-Machine-Learning-Potentials" class="headerlink" title="OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials"></a>OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03121">http://arxiv.org/abs/2310.03121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Eastman, Raimondas Galvelis, Raúl P. Peláez, Charlles R. A. Abreu, Stephen E. Farr, Emilio Gallicchio, Anton Gorenko, Michael M. Henry, Frank Hu, Jing Huang, Andreas Krämer, Julien Michel, Joshua A. Mitchell, Vijay S. Pande, João PGLM Rodrigues, Jaime Rodriguez-Guerra, Andrew C. Simmonett, Jason Swails, Ivy Zhang, John D. Chodera, Gianni De Fabritiis, Thomas E. Markland</li>
<li>for: 这篇论文主要是用于介绍OpenMM分子动力学工具集的新版本，它支持使用机器学习potential来提高分子动力学计算的精度。</li>
<li>methods: 论文使用了PyTorch机器学习模型，可以在分子动力学计算中用于计算力和能量。此外，paper还提供了一个高级接口，使用户可以轻松地模型他们的分子对象，并使用通用的预训练potential函数。</li>
<li>results: 论文通过对细胞分化调控因子8(CDK8)和绿色荧光蛋白(GFP)chromophore在水中的分子动力学计算来展示这些特性。结果表明，这些特性可以在只需要一定的增加计算成本的情况下，提高分子动力学计算的准确性。<details>
<summary>Abstract</summary>
Machine learning plays an important and growing role in molecular simulation. The newest version of the OpenMM molecular dynamics toolkit introduces new features to support the use of machine learning potentials. Arbitrary PyTorch models can be added to a simulation and used to compute forces and energy. A higher-level interface allows users to easily model their molecules of interest with general purpose, pretrained potential functions. A collection of optimized CUDA kernels and custom PyTorch operations greatly improves the speed of simulations. We demonstrate these features on simulations of cyclin-dependent kinase 8 (CDK8) and the green fluorescent protein (GFP) chromophore in water. Taken together, these features make it practical to use machine learning to improve the accuracy of simulations at only a modest increase in cost.
</details>
<details>
<summary>摘要</summary>
机器学习在分子模拟中扮演着越来越重要的角色。最新版本的OpenMM分子动力学工具集 introduce了新的特性来支持使用机器学习潜在力。可以将任意PyTorch模型添加到一个 simulatin中，并用来计算力和能量。一个更高层次的接口使得用户可以轻松地模型他们的分子对象，使用通用的预训练潜在功能。一组优化的CUDA 加速器和自定义PyTorch操作，使得分子模拟得到了显著提高。我们在Cyclin-dependent kinase 8（CDK8）和绿色荧光蛋白（GFP）氢键在水中进行了模拟。总之，这些特性使得使用机器学习来提高分子模拟的准确性，只需要一个modest 的增加成本。
</details></li>
</ul>
<hr>
<h2 id="Crossed-IoT-device-portability-of-Electromagnetic-Side-Channel-Analysis-Challenges-and-Dataset"><a href="#Crossed-IoT-device-portability-of-Electromagnetic-Side-Channel-Analysis-Challenges-and-Dataset" class="headerlink" title="Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset"></a>Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03119">http://arxiv.org/abs/2310.03119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tharindu Lakshan Yasarathna, Lojenaa Navanesan, Simon Barque, Assanka Sayakkara, Nhien-An Le-Khac</li>
<li>for: This paper is written for the purpose of investigating the limitations of Electromagnetic Side-Channel Analysis (EM-SCA) approaches for IoT forensics, specifically the impact of device variability on the accuracy and reliability of EM-SCA results.</li>
<li>methods: The paper uses machine-learning (ML) based approaches for EM-SCA and collects EM-SCA datasets to evaluate the limitations of current EM-SCA approaches and datasets. The study also employs transfer learning to obtain more meaningful and reliable results from EM-SCA in IoT forensics of crossed-IoT devices.</li>
<li>results: The paper contributes a new dataset for using deep learning models in analysing Electromagnetic Side-Channel data with regards to the cross-device portability matter, and demonstrates the feasibility of using transfer learning to improve the accuracy and reliability of EM-SCA results in IoT forensics.<details>
<summary>Abstract</summary>
IoT (Internet of Things) refers to the network of interconnected physical devices, vehicles, home appliances, and other items embedded with sensors, software, and connectivity, enabling them to collect and exchange data. IoT Forensics is collecting and analyzing digital evidence from IoT devices to investigate cybercrimes, security breaches, and other malicious activities that may have taken place on these connected devices. In particular, EM-SCA has become an essential tool for IoT forensics due to its ability to reveal confidential information about the internal workings of IoT devices without interfering these devices or wiretapping their networks. However, the accuracy and reliability of EM-SCA results can be limited by device variability, environmental factors, and data collection and processing methods. Besides, there is very few research on these limitations that affects significantly the accuracy of EM-SCA approaches for the crossed-IoT device portability as well as limited research on the possible solutions to address such challenge. Therefore, this empirical study examines the impact of device variability on the accuracy and reliability of EM-SCA approaches, in particular machine-learning (ML) based approaches for EM-SCA. We firstly presents the background, basic concepts and techniques used to evaluate the limitations of current EM-SCA approaches and datasets. Our study then addresses one of the most important limitation, which is caused by the multi-core architecture of the processors (SoC). We present an approach to collect the EM-SCA datasets and demonstrate the feasibility of using transfer learning to obtain more meaningful and reliable results from EM-SCA in IoT forensics of crossed-IoT devices. Our study moreover contributes a new dataset for using deep learning models in analysing Electromagnetic Side-Channel data with regards to the cross-device portability matter.
</details>
<details>
<summary>摘要</summary>
互联网关系物（IoT）指的是一个包含物理设备、车辆、家用电器和其他设备的网络，这些设备嵌入了感知器、软件和连接性，以便收集和交换数据。IoT审查是收集和分析IoT设备上的数字证据，以调查网络攻击、安全漏洞和其他可能在这些连接设备上发生的恶意活动。特别是，EM-SCA在IoT审查中变得非常重要，因为它可以无须损害IoT设备或窃听其网络，而且可以披露IoT设备内部的机密信息。然而，EM-SCA的准确性和可靠性受到设备多样性、环境因素和数据收集和处理方法的影响。此外，对这些限制的研究非常有限，特别是crossed-IoT设备的问题。因此，本文会详细介绍EM-SCA方法中的设备多样性的影响，以及machine learning（ML）基于的EM-SCA方法的限制。我们首先介绍了背景、基本概念和用于评估现有EM-SCA方法的数据集。然后，我们解决了现有的一个重要限制，即处理器（SoC）的多核架构。我们提出了一种收集EM-SCA数据的方法，并证明了使用传输学习可以在IoT审查中获得更加可靠和有意义的结果。此外，我们还提供了一个新的数据集，用于使用深度学习模型分析电磁romagnetic Side-Channel数据，并且关于cross-device可用性问题。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Model-based-Trees-as-Interpretable-Surrogate-Models-for-Model-Distillation"><a href="#Leveraging-Model-based-Trees-as-Interpretable-Surrogate-Models-for-Model-Distillation" class="headerlink" title="Leveraging Model-based Trees as Interpretable Surrogate Models for Model Distillation"></a>Leveraging Model-based Trees as Interpretable Surrogate Models for Model Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03112">http://arxiv.org/abs/2310.03112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/mbt_comparison">https://github.com/slds-lmu/mbt_comparison</a></li>
<li>paper_authors: Julia Herbinger, Susanne Dandl, Fiona K. Ewald, Sofia Loibl, Giuseppe Casalicchio</li>
<li>for: 这篇论文主要是为了描述如何使用模型基于树来创建逻辑模型，以便对黑盒式机器学习模型进行回溯解释。</li>
<li>methods: 本论文使用的方法包括四种模型基于树算法，即SLIM、GUIDE、MOB和CTree，以及一种简单的模型泵制法。这些方法的目的是通过决策规则将特征空间分解成可解释的区域，并在每个区域内使用可解释的模型来近似黑盒模型的行为。</li>
<li>results: 本论文的结果表明，这些模型基于树算法具有较高的解释性和稳定性，同时也能够保持和黑盒模型的性能相似。此外，这些方法还能够捕捉复杂的交互效应。<details>
<summary>Abstract</summary>
Surrogate models play a crucial role in retrospectively interpreting complex and powerful black box machine learning models via model distillation. This paper focuses on using model-based trees as surrogate models which partition the feature space into interpretable regions via decision rules. Within each region, interpretable models based on additive main effects are used to approximate the behavior of the black box model, striking for an optimal balance between interpretability and performance. Four model-based tree algorithms, namely SLIM, GUIDE, MOB, and CTree, are compared regarding their ability to generate such surrogate models. We investigate fidelity, interpretability, stability, and the algorithms' capability to capture interaction effects through appropriate splits. Based on our comprehensive analyses, we finally provide an overview of user-specific recommendations.
</details>
<details>
<summary>摘要</summary>
��urgate模型在抽象和强大的黑盒机器学习模型的透彻解读中发挥关键作用。这篇论文强调使用模型基于树作为资源模型，通过决策规则将特征空间分割成可解释的区域。在每个区域内，使用可解释的模型，基于加法主要效果来近似黑盒模型的行为，寻找优化的平衡点。本文比较了四种模型基于树算法， namely SLIM、GUIDE、MOB和CTree，它们在生成这种代理模型方面的能力。我们进行了全面的分析，包括忠实度、可解释性、稳定性以及捕捉交互效果的能力。根据我们的彻底分析，我们最终提供了用户特定的建议。
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Gaussian-Process-Variational-Autoencoders-for-Neural-and-Behavioral-Data"><a href="#Multi-modal-Gaussian-Process-Variational-Autoencoders-for-Neural-and-Behavioral-Data" class="headerlink" title="Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data"></a>Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03111">http://arxiv.org/abs/2310.03111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabia Gondur, Usama Bin Sikandar, Evan Schaffer, Mikio Christian Aoi, Stephen L Keeley</li>
<li>for: 本研究的目的是Characterizing the relationship between neural population activity and behavioral data, 即 Neuroscience 领域中的一个中心目标。</li>
<li>methods: 本研究使用了一种基于 Gaussian Process Factor Analysis (GPFA) 和 Gaussian Process Variational Autoencoders (GP-VAEs) 的无监督 latent variable model (LVM)，可以提取高维时间序列数据中的共同和独立的特征空间结构。</li>
<li>results: 研究表明，使用这种模型可以准确地分解高维时间序列数据中的共同和独立特征空间结构，并且在不同实验数据模式下都能够提供良好的重建结果。此外，研究还应用于两个实验 Setting：蝋烛蛋白氮氧化 imaging 和 Manduca sexta 肌电征观测。<details>
<summary>Abstract</summary>
Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional time-series data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts temporally evolving shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning latent variability into components that are either shared between or independent to each modality. We parameterize the latents of our model in the Fourier domain, and show improved latent identification using this approach over standard GP-VAE methods. We validate our model on simulated multi-modal data consisting of Poisson spike counts and MNIST images that scale and rotate smoothly over time. We show that the multi-modal GP-VAE (MM-GPVAE) is able to not only identify the shared and independent latent structure across modalities accurately, but provides good reconstructions of both images and neural rates on held-out trials. Finally, we demonstrate our framework on two real world multi-modal experimental settings: Drosophila whole-brain calcium imaging alongside tracked limb positions, and Manduca sexta spike train measurements from ten wing muscles as the animal tracks a visual stimulus.
</details>
<details>
<summary>摘要</summary>
We use Gaussian Process Factor Analysis (GPFA) for neural spiking data with temporally smooth latent space, combined with Gaussian Process Variational Autoencoders (GP-VAEs) that use a GP prior to characterize correlations in a latent space, and admit rich expressivity due to a deep neural network mapping to observations. We parameterize the latents in the Fourier domain, which improves latent identification.We validate our model on simulated multi-modal data consisting of Poisson spike counts and MNIST images that smoothly change over time. Our multi-modal Gaussian Process Variational Autoencoder (MM-GPVAE) accurately identifies shared and independent latent structure across modalities, and provides good reconstructions of both images and neural rates on held-out trials.We also apply our framework to two real-world multi-modal experimental settings: Drosophila whole-brain calcium imaging alongside tracked limb positions, and Manduca sexta spike train measurements from ten wing muscles as the animal tracks a visual stimulus. Our approach provides a powerful tool for analyzing and understanding the complex relationships between neural population activity and behavioral data.
</details></li>
</ul>
<hr>
<h2 id="DP-SGD-for-non-decomposable-objective-functions"><a href="#DP-SGD-for-non-decomposable-objective-functions" class="headerlink" title="DP-SGD for non-decomposable objective functions"></a>DP-SGD for non-decomposable objective functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03104">http://arxiv.org/abs/2310.03104</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Kong, Andrés Muñoz Medina, Mónica Ribero</li>
<li>for: 这个研究是为了开发一种具有隐私保证的 Computer Vision 模型和大语言模型的开发工具。</li>
<li>methods: 这个研究使用了一种新的 DP-SGD 方法，它在 similarity-based loss functions 中 manipulates Gradient 的方式，以获得 $O(1)$ 的 $L_2$ sensitivity。</li>
<li>results: 在 CIFAR-10 预训和 CIFAR-100 精进任务中，这个方法的表现与非隐私模型几乎相等，并且通常比 DP-SGD 直接应用到对比例损失的表现更好。<details>
<summary>Abstract</summary>
Unsupervised pre-training is a common step in developing computer vision models and large language models. In this setting, the absence of labels requires the use of similarity-based loss functions, such as contrastive loss, that favor minimizing the distance between similar inputs and maximizing the distance between distinct inputs. As privacy concerns mount, training these models using differential privacy has become more important. However, due to how inputs are generated for these losses, one of their undesirable properties is that their $L_2$ sensitivity can grow with increasing batch size. This property is particularly disadvantageous for differentially private training methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD variant for similarity based loss functions -- in particular the commonly used contrastive loss -- that manipulates gradients of the objective function in a novel way to obtain a senstivity of the summed gradient that is $O(1)$ for batch size $n$. We test our DP-SGD variant on some preliminary CIFAR-10 pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, our method's performance comes close to that of a non-private model and generally outperforms DP-SGD applied directly to the contrastive loss.
</details>
<details>
<summary>摘要</summary>
“不监督式预训”是电脑视觉模型和大语言模型的开发中常见的步骤。在这种设定下，由于没有标签，因此需要使用相似性基于的损失函数，如对照损失，以降低相似输入的距离，并将不相似的输入划分开。随着隐私问题的增加，对这些模型进行权限为 differential privacy 变得更加重要。然而，由于输入的生成方式，这些损失函数的 $L_2$ 敏感性会随着批号大小增长。这个问题对于对于权限为 differentially private 的训练方法，如 DP-SGD，是不利的。为了解决这个问题，我们开发了一种基于相似性损失函数的 DP-SGD variant，具体是对对照损失进行修改，以使得每个批号中的条件 gradient 的敏感性为 $O(1)$。我们在预先训练 CIFAR-10 和 CIFAR-100 中进行了一些验证，结果显示，在这两个任务中，我们的方法的性能与非隐私模型相似，并且通常超过直接对对照损失进行 DP-SGD 的性能。”
</details></li>
</ul>
<hr>
<h2 id="Dual-Prompt-Tuning-for-Domain-Aware-Federated-Learning"><a href="#Dual-Prompt-Tuning-for-Domain-Aware-Federated-Learning" class="headerlink" title="Dual Prompt Tuning for Domain-Aware Federated Learning"></a>Dual Prompt Tuning for Domain-Aware Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03103">http://arxiv.org/abs/2310.03103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoyizhe Wei, Feng Wang, Anshul Shah, Rama Chellappa</li>
<li>for: 这份研究旨在解决分布式机器学习中的领域转移问题，以实现多个客户端联合训练共享模型。</li>
<li>methods: 这篇研究使用了提示学习技术，具体是运用预训练的视觉语言模型，然后对于每个客户端的资料进行视觉和文本提示调整，以便领域适应。</li>
<li>results: 实验结果显示，提案学习方法（Fed-DPT）可以优化领域转移问题，并且与原始CLIP模型相比，实现了14.8%的提升。在DomainNet dataset中，这种方法可以获得68.4%的平均准确率，涵盖了六个领域。<details>
<summary>Abstract</summary>
Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% average accuracy over six domains in the DomainNet dataset, which improves the original CLIP by a large margin of 14.8%.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种分布式机器学习 paradigma，允许多个客户端共同训练一个共享模型，使用本地数据进行训练。然而，传统的 federated learning 算法经常难以通用，因为客户端的训练数据通常存在域shift问题。在这项工作中，我们考虑了一种具有挑战性和实际性的 federated learning 场景，其中每个客户端的训练数据来自不同的域。我们通过提出了技术，解决域shift问题，并提出了一种名为 Federated Dual Prompt Tuning (Fed-DPT) 的新方法。具体来说，Fed-DPT 使用了预训练的视觉语言模型，然后应用视觉和文本提示调整来促进域适应性。我们进行了广泛的 Fed-DPT 实验，并证明其在域感知 federated learning 中具有显著的有效性。使用预训练的 CLIP 模型（ViT-Base 作为图像编码器），我们的 Fed-DPT 在 DomainNet 数据集上 achieve 68.4% 的平均精度，超过原 CLIP 的 14.8% 。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Neural-Networks-for-Accelerating-Power-System-State-Estimation"><a href="#Physics-Informed-Neural-Networks-for-Accelerating-Power-System-State-Estimation" class="headerlink" title="Physics-Informed Neural Networks for Accelerating Power System State Estimation"></a>Physics-Informed Neural Networks for Accelerating Power System State Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03088">http://arxiv.org/abs/2310.03088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Solon Falas, Markos Asprou, Charalambos Konstantinou, Maria K. Michael</li>
<li>for: 本文研究了使用物理知识学习（PINNs）加速电力系统状态估算，以提高电力系统监控和运行状况的精度和效率。</li>
<li>methods: 本文提出了一种新的方法，通过将物理知识integrated into PINNs来减少状态估算的计算复杂性，同时保持高准确性。</li>
<li>results: 经过实验表明，提出的方法可以提高精度，降低标准差，并且更快地 converges，在IEEE 14-bus系统上实现了11%的提高精度、75%的降低标准差和30%的加速。<details>
<summary>Abstract</summary>
State estimation is the cornerstone of the power system control center since it provides the operating condition of the system in consecutive time intervals. This work investigates the application of physics-informed neural networks (PINNs) for accelerating power systems state estimation in monitoring the operation of power systems. Traditional state estimation techniques often rely on iterative algorithms that can be computationally intensive, particularly for large-scale power systems. In this paper, a novel approach that leverages the inherent physical knowledge of power systems through the integration of PINNs is proposed. By incorporating physical laws as prior knowledge, the proposed method significantly reduces the computational complexity associated with state estimation while maintaining high accuracy. The proposed method achieves up to 11% increase in accuracy, 75% reduction in standard deviation of results, and 30% faster convergence, as demonstrated by comprehensive experiments on the IEEE 14-bus system.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CNState estimation is the cornerstone of the power system control center, providing the operating condition of the system in consecutive time intervals. This work investigates the application of physics-informed neural networks (PINNs) for accelerating power systems state estimation in monitoring the operation of power systems. Traditional state estimation techniques often rely on iterative algorithms that can be computationally intensive, particularly for large-scale power systems. In this paper, a novel approach that leverages the inherent physical knowledge of power systems through the integration of PINNs is proposed. By incorporating physical laws as prior knowledge, the proposed method significantly reduces the computational complexity associated with state estimation while maintaining high accuracy. The proposed method achieves up to 11% increase in accuracy, 75% reduction in standard deviation of results, and 30% faster convergence, as demonstrated by comprehensive experiments on the IEEE 14-bus system.Note: "zh-CN" is the Simplified Chinese language code.
</details></li>
</ul>
<hr>
<h2 id="Decision-ConvFormer-Local-Filtering-in-MetaFormer-is-Sufficient-for-Decision-Making"><a href="#Decision-ConvFormer-Local-Filtering-in-MetaFormer-is-Sufficient-for-Decision-Making" class="headerlink" title="Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making"></a>Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03022">http://arxiv.org/abs/2310.03022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeonghye Kim, Suyoung Lee, Woojun Kim, Youngchul Sung</li>
<li>for: 提高Offline Reinforcement Learning（RL）中的模型表现，并且能够更好地捕捉RL模型中的地方相互关系。</li>
<li>methods: 基于Transformer的Decision Transformer（DT）模型，并提出了一种新的动作序列预测器名为Decision ConvFormer（DC），该模型使用本地卷积滤波器来替换DT的注意力模块，以更好地捕捉RL数据中的本地相互关系。</li>
<li>results: DC在多个标准RLbenchmark上达到了状态机器人表现的最佳Result，同时具有较少的资源消耗和更好的普适性。<details>
<summary>Abstract</summary>
The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that DC better understands the underlying meaning in data and exhibits enhanced generalization capability.
</details>
<details>
<summary>摘要</summary>
Recently, Transformer 的成功在自然语言处理领域已经引发了各种应用。在线下强化学习（RL）中，决策Transformer（DT）正在崛起为一种有前途的模型。然而，我们发现DT的注意模块并不适合捕捉RL中轨迹的内在本地依赖关系。为了超越DT的局限性，我们提出了一种新的行动序列预测器，名为决策ConvFormer（DC），基于MetaFormer的 Architecture。DC使用本地核心 filtering 作为токен混合器，可以有效捕捉RL数据中的内在本地关系。在广泛的实验中，DC达到了多种标准RLbenchmark中的最佳性能，同时需要较少的资源。此外，我们还示出DC更好地理解数据中的含义，并且具有更好的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-SGD-aligns-with-emerging-outlier-eigenspaces"><a href="#High-dimensional-SGD-aligns-with-emerging-outlier-eigenspaces" class="headerlink" title="High-dimensional SGD aligns with emerging outlier eigenspaces"></a>High-dimensional SGD aligns with emerging outlier eigenspaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03010">http://arxiv.org/abs/2310.03010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, Aukosh Jagannath</li>
<li>for: 这篇论文研究了在使用泊松 gradient descent（SGD）训练多类高维杂合体时，训练过程中对预测矩阵和梯度矩阵的特征 matrix 的 JOINT 演化。</li>
<li>methods: 这篇论文使用了 Stochastic gradient descent（SGD）训练方法，并通过分析预测矩阵和梯度矩阵的特征矩阵来研究训练过程中的特征 matrix 的演化。</li>
<li>results: 研究发现，在多层神经网络和高维杂合体中，SGD trajectory 快速地与出现的低级别异常 eigenspace 对齐，并且在多层设置中，每层的异常 eigenspace 在训练过程中进行了演化，并且在 SGD  converges 到不优化类фика器时会出现rank defect。这些结果证明了一些在过去十年的数值研究中出现的观测结果，关于训练过程中预测矩阵和梯度矩阵的特征矩阵的特征。<details>
<summary>Abstract</summary>
We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the Hessian and gradient matrices. Moreover, in multi-layer settings this alignment occurs per layer, with the final layer's outlier eigenspace evolving over the course of training, and exhibiting rank deficiency when the SGD converges to sub-optimal classifiers. This establishes some of the rich predictions that have arisen from extensive numerical studies in the last decade about the spectra of Hessian and information matrices over the course of training in overparametrized networks.
</details>
<details>
<summary>摘要</summary>
我们严格地研究了在 Stochastic Gradient Descent（SGD）训练过程中的训练动态和经验偏导矩阵和梯度矩阵的共同演化。我们证明了在两个 canonical 分类任务中， namely 高维混合体中的multi-class和一层或二层神经网络，SGD的轨迹快速与出现的低维异常特征空间相对应。此外，在多层设置中，这种对应发生在每层上，最后一层的异常特征空间在训练过程中不断演化，并在SGD converges to sub-optimal classifiers时表现出rank defect。这些结论证明了过去十年的数字实验所预测的偏导矩阵和信息矩阵的特征在训练过程中的变化。
</details></li>
</ul>
<hr>
<h2 id="Learning-characteristic-parameters-and-dynamics-of-centrifugal-pumps-under-multi-phase-flow-using-physics-informed-neural-networks"><a href="#Learning-characteristic-parameters-and-dynamics-of-centrifugal-pumps-under-multi-phase-flow-using-physics-informed-neural-networks" class="headerlink" title="Learning characteristic parameters and dynamics of centrifugal pumps under multi-phase flow using physics-informed neural networks"></a>Learning characteristic parameters and dynamics of centrifugal pumps under multi-phase flow using physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03001">http://arxiv.org/abs/2310.03001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe de Castro Teixeira Carvalho, Kamaljyoti Nath, Alberto Luiz Serpa, George Em Karniadakis</li>
<li>for: 这种研究是为了提高油田生产和实施控制策略而写的。</li>
<li>methods: 这个研究使用物理受限神经网络（PINN）模型来估算系统参数。</li>
<li>results: 研究发现，使用PINN模型可以减少在野外实验室测试中估算流体属性的成本。<details>
<summary>Abstract</summary>
Electrical submersible pumps (ESP) are the second most used artificial lifting equipment in the oil and gas industry due to their high flow rates and boost pressures. They often have to handle multiphase flows, which usually contain a mixture of hydrocarbons, water, and/or sediments. Given these circumstances, emulsions are commonly formed. It is a liquid-liquid flow composed of two immiscible fluids whose effective viscosity and density differ from the single phase separately. In this context, accurate modeling of ESP systems is crucial for optimizing oil production and implementing control strategies. However, real-time and direct measurement of fluid and system characteristics is often impractical due to time constraints and economy. Hence, indirect methods are generally considered to estimate the system parameters. In this paper, we formulate a machine learning model based on Physics-Informed Neural Networks (PINNs) to estimate crucial system parameters. In order to study the efficacy of the proposed PINN model, we conduct computational studies using not only simulated but also experimental data for different water-oil ratios. We evaluate the state variable's dynamics and unknown parameters for various combinations when only intake and discharge pressure measurements are available. We also study structural and practical identifiability analyses based on commonly available pressure measurements. The PINN model could reduce the requirement of expensive field laboratory tests used to estimate fluid properties.
</details>
<details>
<summary>摘要</summary>
电动潜水泵（ESP）是石油和天然气行业中第二常用人工吸引装置，因其高流量和增压性。它们经常需要处理多相流体，通常包含混合物质，水和/或淤泥。由于这些情况，涂抹是常见的。它是两种不溶的液体流体的液-液流体，其有效粘度和密度与单相流体不同。在这种情况下，ESP系统的准确模型化是关键，以便优化石油生产和实施控制策略。然而，实时和直接测量流体和系统特性是经济不实际的，因此通常使用间接方法来估算系统参数。在这篇论文中，我们使用物理学 Informed Neural Networks（PINNs）来估算系统参数。为了评估提案的PINN模型的效果，我们进行了计算研究，使用不仅数据 simulated，还有实验数据，以便为不同的水油比例进行研究。我们分析了系统参数的动态和未知参数，以及不同组合下的只有吸入和排出压力测量的情况。我们还进行了结构可识别性和实用可识别性分析，以确定可以通过常见压力测量来估算流体属性。PINN模型可以减少在野外实验室测试中估算流体属性的成本。
</details></li>
</ul>
<hr>
<h2 id="IBCL-Zero-shot-Model-Generation-for-Task-Trade-offs-in-Continual-Learning"><a href="#IBCL-Zero-shot-Model-Generation-for-Task-Trade-offs-in-Continual-Learning" class="headerlink" title="IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning"></a>IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02995">http://arxiv.org/abs/2310.02995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibcl-anon/ibcl">https://github.com/ibcl-anon/ibcl</a></li>
<li>paper_authors: Pengyuan Lu, Michele Caprio, Eric Eaton, Insup Lee</li>
<li>For: The paper focuses on continual learning, specifically addressing the trade-off between different tasks and proposing a new method called Imprecise Bayesian Continual Learning (IBCL) to improve the efficiency of continual learning.* Methods: IBCL updates a knowledge base in the form of a convex hull of model parameter distributions and obtains particular models to address task trade-off preferences with zero-shot, without requiring additional training overhead.* Results: The paper shows that models obtained by IBCL have guarantees in identifying the Pareto optimal parameters, and experiments on standard image classification and NLP tasks support this guarantee. Additionally, IBCL improves average per-task accuracy by at most 23% and peak per-task accuracy by at most 15% with respect to the baseline methods, with steadily near-zero or positive backward transfer.<details>
<summary>Abstract</summary>
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some previous tasks. This means that there exist multiple models that are Pareto-optimal at different times, each addressing a distinct task performance trade-off. Researchers have discussed how to train particular models to address specific trade-off preferences. However, existing algorithms require training overheads proportional to the number of preferences -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address task trade-off preferences with zero-shot. That is, IBCL does not require any additional training overhead to generate preference-addressing models from its knowledge base. We show that models obtained by IBCL have guarantees in identifying the Pareto optimal parameters. Moreover, experiments on standard image classification and NLP tasks support this guarantee. Statistically, IBCL improves average per-task accuracy by at most 23\% and peak per-task accuracy by at most 15\% with respect to the baseline methods, with steadily near-zero or positive backward transfer. Most importantly, IBCL significantly reduces the training overhead from training 1 model per preference to at most 3 models for all preferences.
</details>
<details>
<summary>摘要</summary>
LIKE 普通多任务学习，连续学习具有多目标优化的性质，因此面临当前任务分布优化时可能需要牺牲之前任务的性能。这意味着存在多个 Pareto-优质的模型，每个模型在不同的时间都能够满足不同的任务性能质量。研究人员已经讨论了如何训练特定的模型以满足特定的任务质量让权。然而，现有的算法需要训练负担与有多个偏好相对应的训练负担成比例。为回应这个问题，我们提出了不准确杯状泛化学习（IBCL）。在新任务时，IBCL 会（1）更新知识库，其形式为模型参数分布的 convex hull，并（2）在零执行下获取任务质量让权的特定模型。即，IBCL 不需要额外的训练负担来生成根据偏好训练的模型。我们证明了由 IBCL 获取的模型具有确定 Pareto 优质参数的保证。此外，我们在标准图像识别和自然语言处理任务上进行了实验，并证明了这个保证。统计 speaking，IBCL 可以提高每个任务的均值性能 by at most 23%，并且 peak 性能 by at most 15%，与基eline 方法相比。此外，IBCL 可以大幅减少训练负担，从训练 1 个模型到训练所有偏好的模型。
</details></li>
</ul>
<hr>
<h2 id="Variance-Reduced-Halpern-Iteration-for-Finite-Sum-Monotone-Inclusions"><a href="#Variance-Reduced-Halpern-Iteration-for-Finite-Sum-Monotone-Inclusions" class="headerlink" title="Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions"></a>Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02987">http://arxiv.org/abs/2310.02987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xufeng Cai, Ahmet Alacaoglu, Jelena Diakonikolas</li>
<li>for:  This paper focuses on solving game-theoretic equilibrium problems, particularly in machine learning applications with finite-sum structure.</li>
<li>methods:  The paper proposes variants of the classical Halpern iteration that utilize variance reduction to improve the complexity guarantees. These methods are based on the properties of cocoercivity and Lipschitz continuity of the component operators.</li>
<li>results:  The paper achieves improved complexity guarantees of $\widetilde{\mathcal{O}( n + \sqrt{n}L\varepsilon^{-1})$ for finite-sum monotone inclusions, which is near-optimal up to poly-logarithmic factors. This result is the first variance reduction-type result for general finite-sum monotone inclusions and for specific problems like convex-concave optimization.<details>
<summary>Abstract</summary>
Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. Further, methods with computable approximation errors are highly desirable, as they provide verifiable exit criteria. Motivated by these applications, we study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. Our main contributions are variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees in which $n$ component operators in the finite sum are ``on average'' either cocoercive or Lipschitz continuous and monotone, with parameter $L$. The resulting oracle complexity of our methods, which provide guarantees for the last iterate and for a (computable) operator norm residual, is $\widetilde{\mathcal{O}( n + \sqrt{n}L\varepsilon^{-1})$, which improves upon existing methods by a factor up to $\sqrt{n}$. This constitutes the first variance reduction-type result for general finite-sum monotone inclusions and for more specific problems such as convex-concave optimization when operator norm residual is the optimality measure. We further argue that, up to poly-logarithmic factors, this complexity is unimprovable in the monotone Lipschitz setting; i.e., the provided result is near-optimal.
</details>
<details>
<summary>摘要</summary>
机器学习方法，受到逆攻击robustness或多代理 Setting的需求，解决了游戏理论平衡问题。特别是在empirical variant of learning problems中，方法targetingfinite-sum structure是非常重要的。此外，具有计算可 approximations error的方法是非常有优势，因为它们提供可验证的退出标准。驱动了这些应用，我们研究了finite-sum monotone inclusion problem，这些问题模型了广泛的平衡问题。我们的主要贡献是基于classical Halpern iteration的变种，这些变种使用了减少偏误来获得改进的复杂性保证，其中每个finite sum中的n个组件运算器在平均情况下是cocoercive或Lipschitz连续和均衡的，并且具有参数$L$。这些方法的执行 oracle complexity是$\widetilde{\mathcal{O}(n + \sqrt{n}L\varepsilon^{-1})$,这比现有方法快上到$\sqrt{n}$。这是first variance reduction-type result for general finite-sum monotone inclusions和更特定的问题，如几何-几何优化，当操作norm residual是优化度量时。我们还 argues that, up to poly-logarithmic factors,这个复杂性是不可改进的在均衡Lipschitz setting; 即，提供的结果是near-optimal。
</details></li>
</ul>
<hr>
<h2 id="Fast-Expressive-SE-n-Equivariant-Networks-through-Weight-Sharing-in-Position-Orientation-Space"><a href="#Fast-Expressive-SE-n-Equivariant-Networks-through-Weight-Sharing-in-Position-Orientation-Space" class="headerlink" title="Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space"></a>Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in Position-Orientation Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02970">http://arxiv.org/abs/2310.02970</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ebekkers/ponita">https://github.com/ebekkers/ponita</a></li>
<li>paper_authors: Erik J Bekkers, Sharvaree Vadgama, Rob D Hesselink, Putri A van der Linden, David W Romero</li>
<li>For: The paper is written to derive geometrically optimal edge attributes for flexible message passing frameworks and to develop an efficient equivariant group convolutional network for processing 3D point clouds.* Methods: The paper uses the theory of homogeneous spaces to formalize the notion of weight sharing in convolutional networks and to derive attributes that uniquely identify equivalence classes of point-pairs. The paper also uses group convolutions with feature maps over the homogeneous space of positions, position and orientations, and the group SE$(3)$ itself.* Results: The paper achieves state-of-the-art results in accuracy and speed on three different benchmarks: interatomic potential energy prediction, trajectory forecasting in N-body systems, and generating molecules via equivariant diffusion models. Specifically, the paper shows that using the homogeneous space of positions and orientations significantly enhances computational efficiency compared to indexing features on the full SE$(3)$ group.<details>
<summary>Abstract</summary>
Based on the theory of homogeneous spaces we derive \textit{geometrically optimal edge attributes} to be used within the flexible message passing framework. We formalize the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. We define equivalence classes of point-pairs that are identical up to a transformation in the group and derive attributes that uniquely identify these classes. Weight sharing is then obtained by conditioning message functions on these attributes. As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\mathbb{R}^3$, position and orientations $\mathbb{R}^3 {\times} S^2$, and the group SE$(3)$ itself. Among these, $\mathbb{R}^3 {\times} S^2$ is an optimal choice due to the ability to represent directional information, which $\mathbb{R}^3$ methods cannot, and it significantly enhances computational efficiency compared to indexing features on the full SE$(3)$ group. We empirically support this claim by reaching state-of-the-art results -- in accuracy and speed -- on three different benchmarks: interatomic potential energy prediction, trajectory forecasting in N-body systems, and generating molecules via equivariant diffusion models.
</details>
<details>
<summary>摘要</summary>
As an application of the theory, we develop an efficient equivariant group convolutional network for processing 3D point clouds. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions $\mathbb{R}^3$, position and orientations $\mathbb{R}^3 \times S^2$, and the group SE$(3)$ itself. Among these, $\mathbb{R}^3 \times S^2$ is an optimal choice due to the ability to represent directional information, which $\mathbb{R}^3$ methods cannot, and it significantly enhances computational efficiency compared to indexing features on the full SE$(3)$ group.We empirically support this claim by reaching state-of-the-art results -- in accuracy and speed -- on three different benchmarks: interatomic potential energy prediction, trajectory forecasting in N-body systems, and generating molecules via equivariant diffusion models.Simplified Chinese translation:基于同态空间理论，我们 derive  геометрически优化的边属性，用于在灵活消息传递框架中进行使用。我们正式化了卷积网络中的重量共享，即在点对上共享消息函数。我们定义点对的等价类，并 derive Attributes 可以唯一标识这些等价类。然后，我们通过条件消息函数于这些Attributes来实现重量共享。作为理论的应用，我们开发了高效的同态群卷积网络，用于处理 3D 点云。同态群卷积的理论告诉我们如何在同态空间中进行群卷积，包括位置空间 $\mathbb{R}^3$、位置和方向空间 $\mathbb{R}^3 \times S^2$ 以及同态群 SE $(3)$ 本身。其中， $\mathbb{R}^3 \times S^2$ 是最佳选择，因为它可以表示方向信息，而 $\mathbb{R}^3$ 方法无法表示，并且对计算效率产生了显著提高。我们通过三个不同的标准测试来支持这一点：预测分子潜能能量、N-体系中的轨迹预测和通过同态扩散模型生成分子。
</details></li>
</ul>
<hr>
<h2 id="Dual-Conic-Proxies-for-AC-Optimal-Power-Flow"><a href="#Dual-Conic-Proxies-for-AC-Optimal-Power-Flow" class="headerlink" title="Dual Conic Proxies for AC Optimal Power Flow"></a>Dual Conic Proxies for AC Optimal Power Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02969">http://arxiv.org/abs/2310.02969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guancheng Qiu, Mathieu Tanneau, Pascal Van Hentenryck</li>
<li>for: 该研究旨在开发一种基于机器学习的优化跟踪器，用于提高交流电力网络优化问题（AC-OPF）的解决方案。</li>
<li>methods: 该研究使用了一种新的 dual 架构，它可以提供有效的 dual 下界，并结合了一种自动化学习方案，以避免高成本的训练数据生成。</li>
<li>results: 对于媒体和大型电力网络，该研究的数值实验表明，提档的方法可以实现高效率和可扩展性。<details>
<summary>Abstract</summary>
In recent years, there has been significant interest in the development of machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF). Although significant progress has been achieved in predicting high-quality primal solutions, no existing learning-based approach can provide valid dual bounds for AC-OPF. This paper addresses this gap by training optimization proxies for a convex relaxation of AC-OPF. Namely, the paper considers a second-order cone (SOC) relaxation of ACOPF, and proposes a novel dual architecture that embeds a fast, differentiable (dual) feasibility recovery, thus providing valid dual bounds. The paper combines this new architecture with a self-supervised learning scheme, which alleviates the need for costly training data generation. Extensive numerical experiments on medium- and large-scale power grids demonstrate the efficiency and scalability of the proposed methodology.
</details>
<details>
<summary>摘要</summary>
Recently, there has been significant interest in the development of machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF). Although significant progress has been achieved in predicting high-quality primal solutions, no existing learning-based approach can provide valid dual bounds for AC-OPF. This paper addresses this gap by training optimization proxies for a convex relaxation of AC-OPF. Specifically, the paper considers a second-order cone (SOC) relaxation of ACOPF, and proposes a novel dual architecture that embeds a fast, differentiable (dual) feasibility recovery, thus providing valid dual bounds. The paper combines this new architecture with a self-supervised learning scheme, which alleviates the need for costly training data generation. Extensive numerical experiments on medium- and large-scale power grids demonstrate the efficiency and scalability of the proposed methodology.Here is the translation in Traditional Chinese:过去几年来，有很大的 интерес在开发机器学习基础的优化调Proxy дляAC Optimal Power Flow（AC-OPF）。 although significant progress has been made in predicting high-quality primal solutions， no existing learning-based approach can provide valid dual bounds for AC-OPF。 This paper addresses this gap by training optimization proxies for a convex relaxation of AC-OPF。 Specifically, the paper considers a second-order cone (SOC) relaxation of ACOPF， and proposes a novel dual architecture that embeds a fast, differentiable (dual) feasibility recovery， thus providing valid dual bounds。 The paper combines this new architecture with a self-supervised learning scheme， which alleviates the need for costly training data generation。 Extensive numerical experiments on medium- and large-scale power grids demonstrate the efficiency and scalability of the proposed methodology。
</details></li>
</ul>
<hr>
<h2 id="Co-modeling-the-Sequential-and-Graphical-Routes-for-Peptide-Representation-Learning"><a href="#Co-modeling-the-Sequential-and-Graphical-Routes-for-Peptide-Representation-Learning" class="headerlink" title="Co-modeling the Sequential and Graphical Routes for Peptide Representation Learning"></a>Co-modeling the Sequential and Graphical Routes for Peptide Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02964">http://arxiv.org/abs/2310.02964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zihan-liu-00/repcon">https://github.com/zihan-liu-00/repcon</a></li>
<li>paper_authors: Zihan Liu, Ge Wang, Jiaqi Wang, Jiangbin Zheng, Stan Z. Li</li>
<li>for: 这 paper 的目的是提出一种基于对抗学习的 peptide 共模型方法（RepCon），以增强 peptide 表示的学习表示的一致性，提高下游任务的推论性能。</li>
<li>methods: 这 paper 使用了一种基于对抗学习的框架，将 sequential 和 graphical 两种模型作为两个专家，将它们的表示进行融合，以提高 peptide 表示的学习表示的一致性。</li>
<li>results: 实验表明，RepCon 方法比独立模型更高效，并且在对抗学习框架下比其他共模型方法更高效。 Plus, 这 paper 还提供了模型解释，证明 RepCon 方法的有效性。<details>
<summary>Abstract</summary>
Peptides are formed by the dehydration condensation of multiple amino acids. The primary structure of a peptide can be represented either as an amino acid sequence or as a molecular graph consisting of atoms and chemical bonds. Previous studies have indicated that deep learning routes specific to sequential and graphical peptide forms exhibit comparable performance on downstream tasks. Despite the fact that these models learn representations of the same modality of peptides, we find that they explain their predictions differently. Considering sequential and graphical models as two experts making inferences from different perspectives, we work on fusing expert knowledge to enrich the learned representations for improving the discriminative performance. To achieve this, we propose a peptide co-modeling method, RepCon, which employs a contrastive learning-based framework to enhance the mutual information of representations from decoupled sequential and graphical end-to-end models. It considers representations from the sequential encoder and the graphical encoder for the same peptide sample as a positive pair and learns to enhance the consistency of representations between positive sample pairs and to repel representations between negative pairs. Empirical studies of RepCon and other co-modeling methods are conducted on open-source discriminative datasets, including aggregation propensity, retention time, antimicrobial peptide prediction, and family classification from Peptide Database. Our results demonstrate the superiority of the co-modeling approach over independent modeling, as well as the superiority of RepCon over other methods under the co-modeling framework. In addition, the attribution on RepCon further corroborates the validity of the approach at the level of model explanation.
</details>
<details>
<summary>摘要</summary>
peptides 是由多个氨基酸的干扰凝结形成的。peptide的主要结构可以表示为氨基酸序列或化学链图，由多个氨基酸组成。之前的研究表明，深度学习模型专门针对序列和图形式的peptide模型具有相似的性能。尽管这些模型学习的是同一种modal peptide的表示，但它们对其预测的解释不同。我们认为这些模型可以视为两个专家，从不同的角度对peptide进行推理。为了融合这两个专家的知识，我们提出了一种peptide共模型方法，即RepCon，该方法使用了对比学习框架来增强对应的抽象表示之间的共识性。它将序列Encoder和图形Encoder对同一个peptide样本的表示视为正例对，并学习增强正例对之间的共识性，同时减弱负例对之间的共识性。我们对RepCon和其他共模型方法进行了实验，并对开源的推理数据集进行了测试，包括积累性、保留时间、抗微生物蛋白预测和家族分类。我们的结果表明，共模型方法比独立模型更高效，而RepCon比其他共模型方法更有优势。此外，对RepCon的解释也证明了该方法的正确性。
</details></li>
</ul>
<hr>
<h2 id="A-Fisher-Rao-gradient-flow-for-entropy-regularised-Markov-decision-processes-in-Polish-spaces"><a href="#A-Fisher-Rao-gradient-flow-for-entropy-regularised-Markov-decision-processes-in-Polish-spaces" class="headerlink" title="A Fisher-Rao gradient flow for entropy-regularised Markov decision processes in Polish spaces"></a>A Fisher-Rao gradient flow for entropy-regularised Markov decision processes in Polish spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02951">http://arxiv.org/abs/2310.02951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bekzhan Kerimkulov, James-Michael Leahy, David Siska, Lukasz Szpruch, Yufei Zhang</li>
<li>for: 该论文研究了无穷 horizon  entropy-regularized Markov decision processes 的 Fisher-Rao 政策梯度流的全球收敛性。</li>
<li>methods: 该论文使用了一种 continuous-time 的 policy mirror descent 方法，并证明了其在全球well-posed 和 exponential convergence 的性质。</li>
<li>results: 该论文证明了该流在优化策略时的稳定性和对 gradient evaluation 的稳定性，并且提供了一种基于 log-linear 政策参数化的性能评估方法。<details>
<summary>Abstract</summary>
We study the global convergence of a Fisher-Rao policy gradient flow for infinite-horizon entropy-regularised Markov decision processes with Polish state and action space. The flow is a continuous-time analogue of a policy mirror descent method. We establish the global well-posedness of the gradient flow and demonstrate its exponential convergence to the optimal policy. Moreover, we prove the flow is stable with respect to gradient evaluation, offering insights into the performance of a natural policy gradient flow with log-linear policy parameterisation. To overcome challenges stemming from the lack of the convexity of the objective function and the discontinuity arising from the entropy regulariser, we leverage the performance difference lemma and the duality relationship between the gradient and mirror descent flows.
</details>
<details>
<summary>摘要</summary>
我们研究一个渔者-拉奥政策梯度流的全球吸引性，用于无穷远游 Markov决策过程中的无限远景 entropy-REGULATED 状态和动作空间。该流是一种继承 policy 镜投法的连续时间 analogue。我们证明了流的全球吸引性并证明其对优化策略的极速减少。此外，我们证明了流具有对 gradient 评估的稳定性，从而提供了对自然政策梯度流的log-line性参数化的性能分析。为了解决目标函数不 convex 和 entropy 补偿器导致的挑战，我们利用了性能差异 лемма和投影梯度流与 mirror descent 流的 dual 关系。
</details></li>
</ul>
<hr>
<h2 id="HappyFeat-–-An-interactive-and-efficient-BCI-framework-for-clinical-applications"><a href="#HappyFeat-–-An-interactive-and-efficient-BCI-framework-for-clinical-applications" class="headerlink" title="HappyFeat – An interactive and efficient BCI framework for clinical applications"></a>HappyFeat – An interactive and efficient BCI framework for clinical applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02948">http://arxiv.org/abs/2310.02948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Desbois, Tristan Venot, Fabrizio De Vico Fallani, Marie-Constance Corsi</li>
<li>for: 本研究是为了提高Brain-Computer Interface（BCI）系统的性能和易用性，特别是在临床环境中。</li>
<li>methods: 本研究使用了HappyFeat软件，它可以自动完成BCI实验或分析参数的设置和调整，从而提高BCI性能。同时，HappyFeat还提供了一些功能连接性的metric，可以与传统的功率 спектраль密度相比较。</li>
<li>results: 研究表明，HappyFeat可以帮助在时间紧张的环境中快速选择最佳特征，从而提高BCI性能。此外，HappyFeat还可以作为一种有效的工具来比较不同的信号特征，以便在训练分类算法时进行选择。<details>
<summary>Abstract</summary>
Brain-Computer Interface (BCI) systems allow users to perform actions by translating their brain activity into commands. Such systems usually need a training phase, consisting in training a classification algorithm to discriminate between mental states using specific features from the recorded signals. This phase of feature selection and training is crucial for BCI performance and presents specific constraints to be met in a clinical context, such as post-stroke rehabilitation.   In this paper, we present HappyFeat, a software making Motor Imagery (MI) based BCI experiments easier, by gathering all necessary manipulations and analysis in a single convenient GUI and via automation of experiment or analysis parameters. The resulting workflow allows for effortlessly selecting the best features, helping to achieve good BCI performance in time-constrained environments. Alternative features based on Functional Connectivity can be used and compared or combined with Power Spectral Density, allowing a network-oriented approach.   We then give details of HappyFeat's main mechanisms, and a review of its performances in typical use cases. We also show that it can be used as an efficient tool for comparing different metrics extracted from the signals, to train the classification algorithm. To this end, we show a comparison between the commonly-used Power Spectral Density and network metrics based on Functional Connectivity.   HappyFeat is available as an open-source project which can be freely downloaded on GitHub.
</details>
<details>
<summary>摘要</summary>
Brain-Computer Interface (BCI) 系统允许用户透过识别大脑活动的信号翻译为指令。通常需要一个训练阶段，包括对特定特征的录取信号进行分类学习。这个阶段在临床上有特定的限制，如rehabilitation after stroke。 在这篇文章中，我们介绍了 HappyFeat，一个软件，使得基于想像运动 (MI) 的 BCI实验更加容易，通过集成所有必要的操作和分析到一个易用的Graphical User Interface (GUI) 中，并通过自动化实验或分析参数的自动化，以提高BCI性能。这个工作流程可以帮助在时间紧迫的环境中取得好的BCI性能。此外，HappyFeat 还可以使用不同的功能连接度来检查和比较不同的特征，以推动网络对应的方法。我们随后详细介绍了 HappyFeat 的主要机制，以及它在一般使用情况下的表现。我们还展示了它可以作为一个有效的工具，用于比较不同的信号特征，并训练分类器。为此，我们比较了通常使用的功能spectral density和基于功能连接度的网络特征。HappyFeat 为一个开源项目，可以免费下载在 GitHub 上。
</details></li>
</ul>
<hr>
<h2 id="Online-Constraint-Tightening-in-Stochastic-Model-Predictive-Control-A-Regression-Approach"><a href="#Online-Constraint-Tightening-in-Stochastic-Model-Predictive-Control-A-Regression-Approach" class="headerlink" title="Online Constraint Tightening in Stochastic Model Predictive Control: A Regression Approach"></a>Online Constraint Tightening in Stochastic Model Predictive Control: A Regression Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02942">http://arxiv.org/abs/2310.02942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandre Capone, Tim Brüdigam, Sandra Hirche</li>
<li>for: 解决可难控的测度随机控制问题，因为无法找到几个特殊情况下的分析解。</li>
<li>methods: 使用改进了的约束紧张参数的随机控制方法，通过将机会约束 rewrite 为硬式约束。</li>
<li>results: 提出了一种在控制过程中线上学习约束紧张参数的方法，通过使用高度表达能力的GP模型来近似最小约束紧张参数，并且可以 garantuee 约束紧张参数满足机会约束。在数值实验中，该方法比三种现状顶尖方法优于其他三种现状顶尖方法。<details>
<summary>Abstract</summary>
Solving chance-constrained stochastic optimal control problems is a significant challenge in control. This is because no analytical solutions exist for up to a handful of special cases. A common and computationally efficient approach for tackling chance-constrained stochastic optimal control problems consists of reformulating the chance constraints as hard constraints with a constraint-tightening parameter. However, in such approaches, the choice of constraint-tightening parameter remains challenging, and guarantees can mostly be obtained assuming that the process noise distribution is known a priori. Moreover, the chance constraints are often not tightly satisfied, leading to unnecessarily high costs. This work proposes a data-driven approach for learning the constraint-tightening parameters online during control. To this end, we reformulate the choice of constraint-tightening parameter for the closed-loop as a binary regression problem. We then leverage a highly expressive \gls{gp} model for binary regression to approximate the smallest constraint-tightening parameters that satisfy the chance constraints. By tuning the algorithm parameters appropriately, we show that the resulting constraint-tightening parameters satisfy the chance constraints up to an arbitrarily small margin with high probability. Our approach yields constraint-tightening parameters that tightly satisfy the chance constraints in numerical experiments, resulting in a lower average cost than three other state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
解决机会限定随机控制问题是控制领域的一大挑战。这是因为没有关于特定情况的分析解。一种常见和计算效率高的方法是将机会限定转换为硬件约束参数，但选择约束缩短参数仍然是一大挑战，而且通常只能在过程噪声分布已知情况下提供保证。此外，机会限定经常不是紧张满足，导致过高的成本。本工作提出了一种在控制过程中线上学习约束缩短参数的数据驱动方法。为此，我们将closed-loop中的约束缩短参数选择改为一个二进制回归问题。然后，我们利用一种高度表达力的\gls{gp}模型来近似最小的约束缩短参数，以满足机会限定。通过合适地调整算法参数，我们证明了所得到的约束缩短参数可以在高概率下满足机会限定，并且在数值实验中比三种现状顶峰技术优于。
</details></li>
</ul>
<hr>
<h2 id="Hoeffding’s-Inequality-for-Markov-Chains-under-Generalized-Concentrability-Condition"><a href="#Hoeffding’s-Inequality-for-Markov-Chains-under-Generalized-Concentrability-Condition" class="headerlink" title="Hoeffding’s Inequality for Markov Chains under Generalized Concentrability Condition"></a>Hoeffding’s Inequality for Markov Chains under Generalized Concentrability Condition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02941">http://arxiv.org/abs/2310.02941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Abhishek Gupta, Yin Sun, Ness Shroff</li>
<li>for: 这个论文研究了基于通用集中性Conditions（IPM）的韦夫丁不等式，该条件可以扩展和修正现有的马克夫chain韦夫丁不等式假设。</li>
<li>methods: 论文使用了基于IPM的通用集中性Conditions来推广韦夫丁不等式的应用范围，并在机器学习领域中应用到了一些非 asymptotic 分析问题。</li>
<li>results: 论文通过应用通用集中性Conditions来提供了一些非 asymptotic 的韦夫丁不等式，包括empirical risk minimization with Markovian samples、Ployak-Ruppert averaging of SGD和rested Markovian bandits with general state space。<details>
<summary>Abstract</summary>
This paper studies Hoeffding's inequality for Markov chains under the generalized concentrability condition defined via integral probability metric (IPM). The generalized concentrability condition establishes a framework that interpolates and extends the existing hypotheses of Markov chain Hoeffding-type inequalities. The flexibility of our framework allows Hoeffding's inequality to be applied beyond the ergodic Markov chains in the traditional sense. We demonstrate the utility by applying our framework to several non-asymptotic analyses arising from the field of machine learning, including (i) a generalization bound for empirical risk minimization with Markovian samples, (ii) a finite sample guarantee for Ployak-Ruppert averaging of SGD, and (iii) a new regret bound for rested Markovian bandits with general state space.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A generalization bound for empirical risk minimization with Markovian samples.2. A finite sample guarantee for Ployak-Ruppert averaging of stochastic gradient descent (SGD).3. A new regret bound for rested Markovian bandits with general state spaces.</details></li>
</ol>
<hr>
<h2 id="Optimal-Transport-with-Adaptive-Regularisation"><a href="#Optimal-Transport-with-Adaptive-Regularisation" class="headerlink" title="Optimal Transport with Adaptive Regularisation"></a>Optimal Transport with Adaptive Regularisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02925">http://arxiv.org/abs/2310.02925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hugues Van Assel, Titouan Vayer, Remi Flamary, Nicolas Courty</li>
<li>for: 提高优化交通（OT）的数学复杂性和交通计划的稠密度。</li>
<li>methods: 使用约束来限制每个点的质量流入或流出。</li>
<li>results: 适用于领域适应。Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the numerical complexity and density of the transport plan in the optimal transport (OT) problem by introducing a strictly convex term and imposing constraints on the mass going in or out of each point.</li>
<li>methods: The paper proposes a new formulation of OT called OT with Adaptive RegularIsation (OTARI), which imposes constraints on the mass going in or out of each point to remedy the imbalance in the way mass is spread across the points.</li>
<li>results: The paper demonstrates the benefits of OTARI for domain adaptation.<details>
<summary>Abstract</summary>
Regularising the primal formulation of optimal transport (OT) with a strictly convex term leads to enhanced numerical complexity and a denser transport plan. Many formulations impose a global constraint on the transport plan, for instance by relying on entropic regularisation. As it is more expensive to diffuse mass for outlier points compared to central ones, this typically results in a significant imbalance in the way mass is spread across the points. This can be detrimental for some applications where a minimum of smoothing is required per point. To remedy this, we introduce OT with Adaptive RegularIsation (OTARI), a new formulation of OT that imposes constraints on the mass going in or/and out of each point. We then showcase the benefits of this approach for domain adaptation.
</details>
<details>
<summary>摘要</summary>
<<SYS code=UTF-8>>优化运输（OT）的原型化表述通过紧张的凸函数规范化可以提高数值复杂性和传输计划的稠密度。许多表述都强制实施全局约束，例如通过Entropic Regularization来实现。由于偏出点比中心点更容易扩散质量，这通常导致传输计划中每个点的平均熔炼程度具有显著偏好。这可能会对某些应用程序造成不利影响，例如需要每个点的最小平滑。为了缓解这个问题，我们介绍了OT with Adaptive RegularIsation（OTARI），一种新的优化运输表述，它在每个点上强制实施质量进入或者离开的约束。然后，我们展示了这种方法在领域适应中的优势。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. The traditional Chinese form is also available, but it may not be as widely used in practice.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Ayurvedic-Diagnosis-using-Multinomial-Naive-Bayes-and-K-modes-Clustering-An-Investigation-into-Prakriti-Types-and-Dosha-Overlapping"><a href="#Enhancing-Ayurvedic-Diagnosis-using-Multinomial-Naive-Bayes-and-K-modes-Clustering-An-Investigation-into-Prakriti-Types-and-Dosha-Overlapping" class="headerlink" title="Enhancing Ayurvedic Diagnosis using Multinomial Naive Bayes and K-modes Clustering: An Investigation into Prakriti Types and Dosha Overlapping"></a>Enhancing Ayurvedic Diagnosis using Multinomial Naive Bayes and K-modes Clustering: An Investigation into Prakriti Types and Dosha Overlapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02920">http://arxiv.org/abs/2310.02920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Bidve, Shalini Mishra, Annapurna J</li>
<li>for: 这项研究的目的是提高医学预测和诊断的准确率，通过使用机器学习算法和医学知识来分类人体内的三种基本类型（VATT-Dosha、PITT-Dosha和KAPH-Dosha）。</li>
<li>methods: 该研究使用了多omial Naive Bayes（MNB） кластер分类算法和K-modes归一化分 clustering算法，以及 chi-square 测试来处理分类数据。</li>
<li>results: 研究结果显示，使用MNB分类器可以达到0.90的准确率、0.81的精度、0.91的F1分数和0.90的回归率，而且对七种类型的预测和诊断具有较高的准确率和精度。<details>
<summary>Abstract</summary>
The identification of Prakriti types for the human body is a long-lost medical practice in finding the harmony between the nature of human beings and their behaviour. There are 3 fundamental Prakriti types of individuals. A person can belong to any Dosha. In the existing models, researchers have made use of SVM, KNN, PCA, Decision Tree, and various other algorithms. The output of these algorithms was quite decent, but it can be enhanced with the help of Multinomial Naive Bayes and K-modes clustering. Most of the researchers have confined themselves to 3 basic classes. This might not be accurate in the real-world scenario, where overlapping might occur. Considering these, we have classified the Doshas into 7 categories, which includes overlapping of Doshas. These are namely, VATT-Dosha, PITT-Dosha, KAPH-Dosha, VATT-PITT-Dosha, PITT-KAPH-Dosha, KAPH-VATT-Dosha, and VATT-PITT-KAPH-Dosha. The data used contains a balanced set of all individual entries on which preprocessing steps of machine learning have been performed. Chi-Square test for handling categorical data is being used for feature selection. For model fitting, the method used in this approach is K-modes clustering. The empirical results demonstrate a better result while using the MNB classifier. All key findings of this work have achieved 0.90 accuracy, 0.81 precision, 0.91 F-score, and 0.90 recall. The discussion suggests a provident analysis of the seven clusters and predicts their occurrence. The results have been consolidated to improve the Ayurvedic advancements with machine learning.
</details>
<details>
<summary>摘要</summary>
人体的 Пракрити类型认定是一项长期丢弃的医疗实践，旨在找到人类的自然和行为之间的协调。存在3种基本的 Пракрити类型。一个人可以属于任何一种多样体。现有的模型中，研究人员使用了SVM、KNN、PCA、决策树和其他算法。这些算法的输出很不错，但可以通过多omial Naive Bayes和K-模式归一化进行提高。大多数研究人员只是将人类分为3个基本类。这可能不准确，因为在现实情况下，可能会出现重叠。为此，我们将多样体分为7个类，包括多样体之间的重叠。这些类别分别是：VATT-多样体、PITT-多样体、KAPH-多样体、VATT-PITT-多样体、PITT-KAPH-多样体、KAPH-VATT-多样体和VATT-PITT-KAPH-多样体。数据集中包含了所有个体数据，并进行了机器学习预处理步骤。使用的是χ²测试来处理分类数据。为模型适应，我们使用了K-模式归一化。实际结果表明，使用MNB分类器时的结果更好。所有的关键发现都达到了0.90准确率、0.81精度、0.91F-分数和0.90恢复率。讨论中提出了7个群体的可观测分析和预测其出现。结果被总结以提高医学领域的阿瑞瓦德发展。
</details></li>
</ul>
<hr>
<h2 id="Attention-based-Multi-task-Learning-for-Base-Editor-Outcome-Prediction"><a href="#Attention-based-Multi-task-Learning-for-Base-Editor-Outcome-Prediction" class="headerlink" title="Attention-based Multi-task Learning for Base Editor Outcome Prediction"></a>Attention-based Multi-task Learning for Base Editor Outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02919">http://arxiv.org/abs/2310.02919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amina Mollaysa, Ahmed Allam, Michael Krauthammer</li>
<li>for: 该论文旨在提高基因编辑技术的精度和效率，以便更好地治疗人类遗传疾病。</li>
<li>methods: 该论文提出了一种基于注意力的两阶段机器学习模型，可以预测给定目标基因序列的编辑结果的可能性。同时，该模型还可以同时学习多种基因编辑器（即变种）。</li>
<li>results: 该模型的预测结果与实验结果在多个数据集和基因编辑器变种上均显示了强相关性，这表明该模型可以有效地加速和提高基因编辑设计的过程。<details>
<summary>Abstract</summary>
Human genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model's predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models' capacity to enhance and accelerate the process of refining base editing designs.
</details>
<details>
<summary>摘要</summary>
人类遗传疾病常由点变化引起，强调了精准基因编辑技术的急需。其中，基因编辑技术最引人注目，因为它可以 Targeted 修改Single nucleotide level。然而，其临床应用受到低编辑效率和无意义变化的限制，导致室内实验室中需要进行广泛的试验和尝试。为了加速这个过程，我们提出了一种基于注意力的两阶段机器学习模型，可以预测给定 genomic 目标序列中的所有可能的编辑结果的可能性。我们还提议一种多任务学习 schema，可以同时学习多种基因编辑器（即变体）。我们的模型预测结果与实验结果在多个数据集和基因编辑器变体上具有强相关性。这些结果提供了进一步的验证，证明我们的模型有助于提高和加速基因编辑设计的过程。
</details></li>
</ul>
<hr>
<h2 id="ELUQuant-Event-Level-Uncertainty-Quantification-in-Deep-Inelastic-Scattering"><a href="#ELUQuant-Event-Level-Uncertainty-Quantification-in-Deep-Inelastic-Scattering" class="headerlink" title="ELUQuant: Event-Level Uncertainty Quantification in Deep Inelastic Scattering"></a>ELUQuant: Event-Level Uncertainty Quantification in Deep Inelastic Scattering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02913">http://arxiv.org/abs/2310.02913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristiano Fanelli, James Giroux</li>
<li>for: 用于深入的 uncertainty 量化和事件筛选</li>
<li>methods: 使用物理学 Informed Bayesian Neural Network (BNN) 和多项式Normalizing Flows (MNF) 方法进行事件级别的 uncertainty 量化</li>
<li>results: 能够准确地提取 kinematic 变量 x、Q^2 和 y，并且能够提供精细的物理性 uncertainty 描述，这对于决策和数据质量监测等任务都非常有用。<details>
<summary>Abstract</summary>
We introduce a physics-informed Bayesian Neural Network (BNN) with flow approximated posteriors using multiplicative normalizing flows (MNF) for detailed uncertainty quantification (UQ) at the physics event-level. Our method is capable of identifying both heteroskedastic aleatoric and epistemic uncertainties, providing granular physical insights. Applied to Deep Inelastic Scattering (DIS) events, our model effectively extracts the kinematic variables $x$, $Q^2$, and $y$, matching the performance of recent deep learning regression techniques but with the critical enhancement of event-level UQ. This detailed description of the underlying uncertainty proves invaluable for decision-making, especially in tasks like event filtering. It also allows for the reduction of true inaccuracies without directly accessing the ground truth. A thorough DIS simulation using the H1 detector at HERA indicates possible applications for the future EIC. Additionally, this paves the way for related tasks such as data quality monitoring and anomaly detection. Remarkably, our approach effectively processes large samples at high rates.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种具有物理学信息的泛化神经网络（BNN），使用多项式正常化流（MNF）来实现细致的不确定性评估（UQ）在物理事件级别。我们的方法可以识别和分解不同类型的不确定性，包括异质灵活的不确定性和知识不确定性，并提供物理学上的细致信息。在深入刺激（DIS）事件中应用了我们的模型，能够有效提取kinematic变量$x$, $Q^2$,和$y$，与最新的深度学习回归技术相当，但是具有物理事件级别的不确定性评估的重要优势。这种细致的不确定性描述对决策非常重要，特别是在事件筛选任务中。此外，它还允许降低实际错误，而不需要直接访问真实的真实值。在使用HERA的H1探测器进行深入刺激模拟中，我们发现了可能的应用于未来EIC。此外，这种方法还可以应用于数据质量监测和异常检测等相关任务。很 satisfactory的是，我们的方法可以高效处理大量样本，并且可以在高速下进行处理。
</details></li>
</ul>
<hr>
<h2 id="Spline-based-neural-network-interatomic-potentials-blending-classical-and-machine-learning-models"><a href="#Spline-based-neural-network-interatomic-potentials-blending-classical-and-machine-learning-models" class="headerlink" title="Spline-based neural network interatomic potentials: blending classical and machine learning models"></a>Spline-based neural network interatomic potentials: blending classical and machine learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02904">http://arxiv.org/abs/2310.02904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua A. Vita, Dallas R. Trinkle</li>
<li>for: 这种研究旨在证明Machine Learning interatomic potentials (IPs) 的复杂性是否必须高于first-principles数据的随机噪声水平以确定高质量IPs。</li>
<li>methods: 该研究引入了一种新的Machine Learning interatomic potentials (MLIP)框架，它将spline-based MEAM potentials (s-MEAM) 的简单性与神经网络（NN）架构相结合。这种框架被称为spline-based neural network potential (s-NNP)，可以用来描述复杂的数据集，并且可以在计算上高效地进行。</li>
<li>results: 该研究表明，使用spline filters来编码原子环境可以得到一个易于解释的嵌入层，可以与修改NN结构来涵盖预期的物理行为，从而提高总体的解释性。此外，研究还发现，可以在多个化学系统之间共享spline filters，以便提供一个便利的参照点，从而实现跨系统分析。<details>
<summary>Abstract</summary>
While machine learning (ML) interatomic potentials (IPs) are able to achieve accuracies nearing the level of noise inherent in the first-principles data to which they are trained, it remains to be shown if their increased complexities are strictly necessary for constructing high-quality IPs. In this work, we introduce a new MLIP framework which blends the simplicity of spline-based MEAM (s-MEAM) potentials with the flexibility of a neural network (NN) architecture. The proposed framework, which we call the spline-based neural network potential (s-NNP), is a simplified version of the traditional NNP that can be used to describe complex datasets in a computationally efficient manner. We demonstrate how this framework can be used to probe the boundary between classical and ML IPs, highlighting the benefits of key architectural changes. Furthermore, we show that using spline filters for encoding atomic environments results in a readily interpreted embedding layer which can be coupled with modifications to the NN to incorporate expected physical behaviors and improve overall interpretability. Finally, we test the flexibility of the spline filters, observing that they can be shared across multiple chemical systems in order to provide a convenient reference point from which to begin performing cross-system analyses.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）间位能预测（IP）可以达到近似于初始物理数据中的精度水平，但是需要证明其增加复杂性是否必要 для建立高质量IP。在这种工作中，我们介绍了一个新的MLIP框架，它将spline-based MEAM potentials（s-MEAM）的简单性与神经网络（NN）架构融合在一起。我们称之为spline-based neural network potential（s-NNP）。这个框架是传统NNP的简化版，可以用来描述复杂的数据集，并且具有计算效率。我们示出了如何使用spline filters来编码原子环境，并将其与NN结合以捕捉预期的物理行为，从而提高总体解释性。此外，我们观察了spline filters的共享性，可以在多个化学系统之间共享，以便在不同系统之间进行跨系统分析。
</details></li>
</ul>
<hr>
<h2 id="FroSSL-Frobenius-Norm-Minimization-for-Self-Supervised-Learning"><a href="#FroSSL-Frobenius-Norm-Minimization-for-Self-Supervised-Learning" class="headerlink" title="FroSSL: Frobenius Norm Minimization for Self-Supervised Learning"></a>FroSSL: Frobenius Norm Minimization for Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02903">http://arxiv.org/abs/2310.02903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Skean, Aayush Dhakal, Nathan Jacobs, Luis Gonzalo Sanchez Giraldo</li>
<li>for: 这篇论文的目的是提出一个新的自我超级vised learning（SSL）目标函数FroSSL，并证明FroSSL可以更快地训练到比较好的表示。</li>
<li>methods: FroSSL 使用了抽象对照和对照矩阵的方法，并且使用了均值平方误差来保证扩展对照。</li>
<li>results: FroSSL 可以更快地训练到比较好的表示，并且在 linear probe 评估中 learns 竞争性的表示。<details>
<summary>Abstract</summary>
Self-supervised learning (SSL) is an increasingly popular paradigm for representation learning. Recent methods can be classified as sample-contrastive, dimension-contrastive, or asymmetric network-based, with each family having its own approach to avoiding informational collapse. While dimension-contrastive methods converge to similar solutions as sample-contrastive methods, it can be empirically shown that some methods require more epochs of training to converge. Motivated by closing this divide, we present the objective function FroSSL which is both sample- and dimension-contrastive up to embedding normalization. FroSSL works by minimizing covariance Frobenius norms for avoiding collapse and minimizing mean-squared error for augmentation invariance. We show that FroSSL converges more quickly than a variety of other SSL methods and provide theoretical and empirical support that this faster convergence is due to how FroSSL affects the eigenvalues of the embedding covariance matrices. We also show that FroSSL learns competitive representations on linear probe evaluation when used to train a ResNet18 on the CIFAR-10, CIFAR-100, STL-10, and ImageNet datasets.
</details>
<details>
<summary>摘要</summary>
自适应学习（SSL）是现代表示学习的一种受欢迎的方法。当前的方法可以分为样本对照、维度对照和异形网络基于的三大类别，每个家族都有自己的方法来避免信息归一化。虽然维度对照方法和样本对照方法可以达到相同的解决方案，但是可以经验性地表明一些方法需要更多的训练集数据来融合。为了bridging这个差距，我们提出了一个名为 FroSSL 的目标函数，它同时是样本对照和维度对照的，并且可以保证均值平方差和协方差 Frobenius 范数的最小化。我们表明了 FroSSL 比许多其他 SSL 方法更快地 converges，并提供了理论和实验支持，这更快的 converges 是由 FroSSL 对嵌入协方差矩阵的 eigenvalues 的影响所致。此外，我们还证明了 FroSSL 在 Linear Probe 评估中学习出了竞争力强的表示。
</details></li>
</ul>
<hr>
<h2 id="Recovery-of-Training-Data-from-Overparameterized-Autoencoders-An-Inverse-Problem-Perspective"><a href="#Recovery-of-Training-Data-from-Overparameterized-Autoencoders-An-Inverse-Problem-Perspective" class="headerlink" title="Recovery of Training Data from Overparameterized Autoencoders: An Inverse Problem Perspective"></a>Recovery of Training Data from Overparameterized Autoencoders: An Inverse Problem Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02897">http://arxiv.org/abs/2310.02897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Koren Abitbul, Yehuda Dar</li>
<li>for: recovery of training data from overparameterized autoencoder models</li>
<li>methods: use trained autoencoder to implicitly define a regularizer for the particular training dataset, and iteratively apply the trained autoencoder and simple computations to estimate and address the unknown degradation operator</li>
<li>results: significantly outperforms previous methods for training data recovery from autoencoders, and improves recovery performance in challenging settings that were previously considered highly challenging and impractical<details>
<summary>Abstract</summary>
We study the recovery of training data from overparameterized autoencoder models. Given a degraded training sample, we define the recovery of the original sample as an inverse problem and formulate it as an optimization task. In our inverse problem, we use the trained autoencoder to implicitly define a regularizer for the particular training dataset that we aim to retrieve from. We develop the intricate optimization task into a practical method that iteratively applies the trained autoencoder and relatively simple computations that estimate and address the unknown degradation operator. We evaluate our method for blind inpainting where the goal is to recover training images from degradation of many missing pixels in an unknown pattern. We examine various deep autoencoder architectures, such as fully connected and U-Net (with various nonlinearities and at diverse train loss values), and show that our method significantly outperforms previous methods for training data recovery from autoencoders. Importantly, our method greatly improves the recovery performance also in settings that were previously considered highly challenging, and even impractical, for such retrieval.
</details>
<details>
<summary>摘要</summary>
我们研究从过参数化 autoencoder 模型中恢复训练数据。给定一个受损训练样本，我们定义恢复原始样本为一个逆问题，并将其转换为一个优化任务。在我们的逆问题中，我们使用训练 autoencoder 来隐式定义特定训练数据集的规 regularizer。我们将这个具有复杂的优化任务转换为一个实用的方法，该方法在每次迭代中运用训练 autoencoder 和一些简单的计算来估计和处理未知的损坏算子。我们将我们的方法应用于隐形填充问题，其目标是从 autoencoder 中恢复训练图像，并且有许多遗传的 pixels 在未知的模式中遗传。我们考虑了不同的深度 autoencoder 架构，例如完全连接和 U-Net (with 多标的非线性和在多个训练损失值)，并证明我们的方法在训练数据恢复方面具有重要的进步。特别是，我们的方法在以前考虑为高度困难或不可能的设定中也具有很好的恢复性。
</details></li>
</ul>
<hr>
<h2 id="CoLiDE-Concomitant-Linear-DAG-Estimation"><a href="#CoLiDE-Concomitant-Linear-DAG-Estimation" class="headerlink" title="CoLiDE: Concomitant Linear DAG Estimation"></a>CoLiDE: Concomitant Linear DAG Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02895">http://arxiv.org/abs/2310.02895</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ignavier/golem">https://github.com/ignavier/golem</a></li>
<li>paper_authors: Seyed Saman Saboksayr, Gonzalo Mateos, Mariano Tepper<br>for:* 这个论文目的是学习来自观察数据的导irected acyclic graph（DAG）结构，并且遵循线性结构方程模型（SEM）。methods:* 这篇论文使用了不同分布的权重函数来描述DAG的不规则结构，并且使用了可微分的非对称性来有效地搜索DAG结构。results:* 这篇论文提出了一种新的卷积分数函数，可以在不同的频率下进行鲁棒的DAG估计，并且可以在不同的频率下进行鲁棒的DAG估计。Here is the same information in Simplified Chinese:for:* 这个论文写的是为了学习来自观察数据的导irected acyclic graph（DAG）结构，并且遵循线性结构方程模型（SEM）。methods:* 这篇论文使用了不同分布的权重函数来描述DAG的不规则结构，并且使用了可微分的非对称性来有效地搜索DAG结构。results:* 这篇论文提出了一种新的卷积分数函数，可以在不同的频率下进行鲁棒的DAG估计，并且可以在不同的频率下进行鲁棒的DAG估计。<details>
<summary>Abstract</summary>
We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$near $\textbf{D}$AG $\textbf{E}$stimation), a regression-based criterion amenable to efficient gradient computation and closed-form estimation of noise variances in heteroscedastic scenarios. Our algorithm outperforms state-of-the-art methods without incurring added complexity, especially when the DAGs are larger and the noise level profile is heterogeneous. We also find CoLiDE exhibits enhanced stability manifested via reduced standard deviations in several domain-specific metrics, underscoring the robustness of our novel linear DAG estimator.
</details>
<details>
<summary>摘要</summary>
我们面临了对于观测数据的组合problem，即从线性结构方程模型（SEM）中学习统计学Graph（DAG）的结构。发掘最新的差分可条件优化方法，以有效地探索DAG的空间。大多数现有方法使用lasso类数值函数来导引搜寻，这些方法（一）需要耗费贵重的罚 Parameters 重新设定当 unknown SEM 杂质值 changed across problem instances; 和（二）隐式地假设 homoscedasticity 的限制。在这个工作中，我们提出了一个新的对称数值函数，用于精简DAG的学习，这个函数包含了共同估计标准差，因此实际上将精简 Parameters 与外生杂质水平 decoupled。通过非凸的统一矩阵 penalty 函数，我们得到了 CoLiDE（共同统一 linear DAG 估计），这是一个可以实现高效的梯度计算和关闭式估计杂质水平的条件估计。我们的算法在较大的DAG和不同杂质水平下表现更好，不需要添加额外的复杂性。我们也发现 CoLiDE 具有更好的稳定性，通过减少不同领域的标准差，强调了我们的新的线性DAG估计器的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Something-for-almost-nothing-Improving-deep-ensemble-calibration-using-unlabeled-data"><a href="#Something-for-almost-nothing-Improving-deep-ensemble-calibration-using-unlabeled-data" class="headerlink" title="Something for (almost) nothing: Improving deep ensemble calibration using unlabeled data"></a>Something for (almost) nothing: Improving deep ensemble calibration using unlabeled data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02885">http://arxiv.org/abs/2310.02885</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konstantinos-p/something_for_almost_nothing">https://github.com/konstantinos-p/something_for_almost_nothing</a></li>
<li>paper_authors: Konstantinos Pitas, Julyan Arbel</li>
<li>for: 提高深度套件的准确性在小训练数据情况下，使用无标签数据。</li>
<li>methods: 使用随机选择不同标签与每个套件成员进行拟合。</li>
<li>results: 经验表明，在低至中等训练集大小情况下，我们的套件更加多样化，提供更好的准确性和套件跨度。<details>
<summary>Abstract</summary>
We present a method to improve the calibration of deep ensembles in the small training data regime in the presence of unlabeled data. Our approach is extremely simple to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that if we fit such a labeling on unlabeled data, and the true labels on the training data, we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, our ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法来提高深度 ensemble 的准确性在小训练数据 régime 中，采用了不 labels 数据。我们的方法非常简单实现：对每个不 labels 数据点，我们 simply 随机选择不同的标签并与每个 ensemble 成员进行适应。我们提供了基于 PAC-Bayes  bound 的理论分析，证明如果我们在不 labels 数据上适应这种标签，以及真实标签在训练数据上，我们就可以在测试样本上获得低负逻辑概率和高 ensemble 多样性。实际上，通过详细的实验，我们发现，对于小到中等训练集，我们的集成比标准集成更加多样化，提供了更好的准确性，有时甚至是显著的。
</details></li>
</ul>
<hr>
<h2 id="Stationarity-without-mean-reversion-Improper-Gaussian-process-regression-and-improper-kernels"><a href="#Stationarity-without-mean-reversion-Improper-Gaussian-process-regression-and-improper-kernels" class="headerlink" title="Stationarity without mean reversion: Improper Gaussian process regression and improper kernels"></a>Stationarity without mean reversion: Improper Gaussian process regression and improper kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02877">http://arxiv.org/abs/2310.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Ambrogioni</li>
<li>for:  This paper aims to address the pathological behavior of mean-reverting Gaussian process regression by introducing improper kernels that are stationary but not mean reverting.</li>
<li>methods: The paper proposes the use of improper kernels, including the Smooth Walk kernel and a family of improper Matérn kernels, which can be defined only in this improper regime. The resulting posterior distributions can be computed analytically with a simple correction of the usual formulas.</li>
<li>results: The paper demonstrates that these improper kernels solve some known pathologies of mean-reverting GP regression while retaining most of the favorable properties of ordinary smooth stationary kernels, as shown through synthetic and real data analysis.<details>
<summary>Abstract</summary>
Gaussian processes (GP) regression has gained substantial popularity in machine learning applications. The behavior of a GP regression depends on the choice of covariance function. Stationary covariance functions are favorite in machine learning applications. However, (non-periodic) stationary covariance functions are always mean reverting and can therefore exhibit pathological behavior when applied to data that does not relax to a fixed global mean value. In this paper, we show that it is possible to use improper GP prior with infinite variance to define processes that are stationary but not mean reverting. To this aim, we introduce a large class of improper kernels that can only be defined in this improper regime. Specifically, we introduce the Smooth Walk kernel, which produces infinitely smooth samples, and a family of improper Mat\'ern kernels, which can be defined to be $j$-times differentiable for any integer $j$. The resulting posterior distributions can be computed analytically and it involves a simple correction of the usual formulas. By analyzing both synthetic and real data, we demonstrate that these improper kernels solve some known pathologies of mean reverting GP regression while retaining most of the favourable properties of ordinary smooth stationary kernels.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Harmonic-Control-Lyapunov-Barrier-Functions-for-Constrained-Optimal-Control-with-Reach-Avoid-Specifications"><a href="#Harmonic-Control-Lyapunov-Barrier-Functions-for-Constrained-Optimal-Control-with-Reach-Avoid-Specifications" class="headerlink" title="Harmonic Control Lyapunov Barrier Functions for Constrained Optimal Control with Reach-Avoid Specifications"></a>Harmonic Control Lyapunov Barrier Functions for Constrained Optimal Control with Reach-Avoid Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02869">http://arxiv.org/abs/2310.02869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amartya Mukherjee, Ruikun Zhou, Jun Liu</li>
<li>for: 这个论文旨在解决受限制的控制问题，如达到避免的问题。</li>
<li>methods: 这个论文提出了含有响应最大原理的响应CLBF函数，它们可以在实验开始时被初始化而不是基于示例轨迹进行训练。控制输入选择系统动力学与最大下降方向夹快内积最大化。</li>
<li>results: 实验结果显示，含有响应最大原理的响应CLBF函数可以快速进入安全区域，并且有高概率进入目标区域。<details>
<summary>Abstract</summary>
This paper introduces harmonic control Lyapunov barrier functions (harmonic CLBF) that aid in constrained control problems such as reach-avoid problems. Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to encode the properties of control Lyapunov barrier functions (CLBFs). As a result, they can be initiated at the start of an experiment rather than trained based on sample trajectories. The control inputs are selected to maximize the inner product of the system dynamics with the steepest descent direction of the harmonic CLBF. Numerical results are presented with four different systems under different reach-avoid environments. Harmonic CLBFs show a significantly low risk of entering unsafe regions and a high probability of entering the goal region.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Estimation-of-Models-with-Limited-Data-by-Leveraging-Shared-Structure"><a href="#Estimation-of-Models-with-Limited-Data-by-Leveraging-Shared-Structure" class="headerlink" title="Estimation of Models with Limited Data by Leveraging Shared Structure"></a>Estimation of Models with Limited Data by Leveraging Shared Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02864">http://arxiv.org/abs/2310.02864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryann Rui, Thibaut Horel, Munther Dahleh</li>
<li>for: 这篇论文是为了处理具有多个系统、但每个系统只有少量数据的现代数据集而写的。</li>
<li>methods: 该论文提出了一种基于共同结构的方法，通过利用其他系统的数据来估计高维参数，从而解决单个系统的数据不充分的问题。该方法包括三步骤：首先估计系统参数之间的低维子空间，然后使用这些参数估计系统的参数，最后使用迭代估计法提高参数的精度。</li>
<li>results: 该论文提供了finite sample subspace estimation error guarantees，并通过实验 validate 该方法的正确性。<details>
<summary>Abstract</summary>
Modern data sets, such as those in healthcare and e-commerce, are often derived from many individuals or systems but have insufficient data from each source alone to separately estimate individual, often high-dimensional, model parameters. If there is shared structure among systems however, it may be possible to leverage data from other systems to help estimate individual parameters, which could otherwise be non-identifiable. In this paper, we assume systems share a latent low-dimensional parameter space and propose a method for recovering $d$-dimensional parameters for $N$ different linear systems, even when there are only $T<d$ observations per system. To do so, we develop a three-step algorithm which estimates the low-dimensional subspace spanned by the systems' parameters and produces refined parameter estimates within the subspace. We provide finite sample subspace estimation error guarantees for our proposed method. Finally, we experimentally validate our method on simulations with i.i.d. regression data and as well as correlated time series data.
</details>
<details>
<summary>摘要</summary>
现代数据集，如医疗和电商，经常来自多个个体或系统，但每个源数据不够以alone来估计高维度模型参数。如果这些系统具有共同结构，那么可能可以通过其他系统的数据来帮助估计个体参数，这些参数可能否认。在这篇论文中，我们假设这些系统共享一个低维度的 latent 参数空间，并提出一种方法来回归 $d$-维度参数的 $N$ 个不同的线性系统，即使只有每个系统 $T<d$ 个观察值。为此，我们开发了一个三步算法，首先估计系统参数的低维度子空间，然后生成在子空间内的精细参数估计。我们提供了finite sample subspace estimation error guarantees  для我们的提议方法。最后，我们通过对 simulated  regression 数据和相关时间序列数据进行实验来验证我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Predictions-for-Longitudinal-Data"><a href="#Conformal-Predictions-for-Longitudinal-Data" class="headerlink" title="Conformal Predictions for Longitudinal Data"></a>Conformal Predictions for Longitudinal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02863">http://arxiv.org/abs/2310.02863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Devesh Batra, Salvatore Mercuri, Raad Khraishi</li>
<li>for: 这篇论文是针对长期资料的复准预测方法，专门适用于医学、金融和供应链管理等领域。</li>
<li>methods: 本论文提出了一种新的分布自由复准预测算法，named Longitudinal Predictive Conformal Inference (LPCI)，它可以确保 both longitudinal and cross-sectional coverage without resorting to infinitely wide intervals。LPCI 使用了 quantile fixed-effects regression 模型，实现了预测interval 的建立。</li>
<li>results: 实验结果显示 LPCI 可以实现有效的 cross-sectional coverage 和 longitudinal coverage rates，并且比现有的参考模型perform 更好。论文还提供了 asymptotic coverage guarantees 的理论分析，证明 LPCI 在两个维度上具有有限宽度的预测 интерVAL。<details>
<summary>Abstract</summary>
We introduce Longitudinal Predictive Conformal Inference (LPCI), a novel distribution-free conformal prediction algorithm for longitudinal data. Current conformal prediction approaches for time series data predominantly focus on the univariate setting, and thus lack cross-sectional coverage when applied individually to each time series in a longitudinal dataset. The current state-of-the-art for longitudinal data relies on creating infinitely-wide prediction intervals to guarantee both cross-sectional and asymptotic longitudinal coverage. The proposed LPCI method addresses this by ensuring that both longitudinal and cross-sectional coverages are guaranteed without resorting to infinitely wide intervals. In our approach, we model the residual data as a quantile fixed-effects regression problem, constructing prediction intervals with a trained quantile regressor. Our extensive experiments demonstrate that LPCI achieves valid cross-sectional coverage and outperforms existing benchmarks in terms of longitudinal coverage rates. Theoretically, we establish LPCI's asymptotic coverage guarantees for both dimensions, with finite-width intervals. The robust performance of LPCI in generating reliable prediction intervals for longitudinal data underscores its potential for broad applications, including in medicine, finance, and supply chain management.
</details>
<details>
<summary>摘要</summary>
我们介绍Longitudinal Predictive Conformal Inference（LPCI），一种新的不偏度概率预测算法，用于长itudinal数据。现有预测方法对时间序列数据偏向单variate setting，缺乏每个时间序列在长itudinal数据集中的交叉sectional覆盖。现状态 искусственного智能 для长itudinal数据是通过创建无限宽预测间隔来保证两个维度的覆盖，包括交叉sectional和垂直维度的覆盖。我们的LPCI方法解决了这个问题，因为它可以保证两个维度的覆盖，不需要无限宽的预测间隔。在我们的方法中，我们使用量iles fixed-effects regression问题来模型剩余数据，并构建基于训练量iles regressor的预测间隔。我们的广泛实验表明，LPCI可以实现有效的交叉sectional覆盖，并且在长itudinal覆盖率方面超过现有的标准准确。理论上，我们确定LPCI在两个维度上具有有限宽预测间隔的极限涵盖保证。LPCI的可靠性和稳定性在生成可靠预测间隔的长itudinal数据上表现出色，这些特点使其在医学、金融和供应链管理等领域有广泛的应用前景。
</details></li>
</ul>
<hr>
<h2 id="Multi-Domain-Causal-Representation-Learning-via-Weak-Distributional-Invariances"><a href="#Multi-Domain-Causal-Representation-Learning-via-Weak-Distributional-Invariances" class="headerlink" title="Multi-Domain Causal Representation Learning via Weak Distributional Invariances"></a>Multi-Domain Causal Representation Learning via Weak Distributional Invariances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02854">http://arxiv.org/abs/2310.02854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kartik Ahuja, Amin Mansouri, Yixin Wang</li>
<li>for: 本研究旨在探讨 causal 表示学习在多个领域数据集上的优势。</li>
<li>methods: 本文使用 autoencoder 来学习 causal 表示，并利用数据集中具有稳定分布性质的子集来提高表示学习的可靠性。</li>
<li>results: 研究人员发现，通过 incorporating 稳定分布性质的子集，autoencoder 可以在不同设定下提取稳定的 latent 表示。<details>
<summary>Abstract</summary>
Causal representation learning has emerged as the center of action in causal machine learning research. In particular, multi-domain datasets present a natural opportunity for showcasing the advantages of causal representation learning over standard unsupervised representation learning. While recent works have taken crucial steps towards learning causal representations, they often lack applicability to multi-domain datasets due to over-simplifying assumptions about the data; e.g. each domain comes from a different single-node perfect intervention. In this work, we relax these assumptions and capitalize on the following observation: there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. Leveraging this observation, we show that autoencoders that incorporate such invariances can provably identify the stable set of latents from the rest across different settings.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们relax these assumptions和利用以下观察：there often exists a subset of latents whose certain distributional properties (e.g., support, variance) remain stable across domains; this property holds when, for example, each domain comes from a multi-node imperfect intervention. 我们利用这种观察，证明 autoencoders that incorporate such invariances can provably identify the stable set of latents from the rest across different settings.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Scale-Logits-for-Temperature-Conditional-GFlowNets"><a href="#Learning-to-Scale-Logits-for-Temperature-Conditional-GFlowNets" class="headerlink" title="Learning to Scale Logits for Temperature-Conditional GFlowNets"></a>Learning to Scale Logits for Temperature-Conditional GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02823">http://arxiv.org/abs/2310.02823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Joohwan Ko, Dinghuai Zhang, Ling Pan, Taeyoung Yun, Woochang Kim, Jinkyoo Park, Yoshua Bengio</li>
<li>for: 这种论文的目的是为了学习一种可能性极高的概率模型，用于顺序生成化学结构，以实现更好的生成多种生物化学过程中的多样性。</li>
<li>methods: 这种模型使用的方法是基于温度的流程网络（GFlowNets），它们是一种基于温度的政策序列生成的模型，可以通过调整温度来控制模型的探索和利用行为。</li>
<li>results: 作者们提出了一种新的架构设计方法，即学习温度Scaling Logits（LSL-GFN），可以快速加速温度控制的GFlowNets训练。这种方法基于对温度的conditioning进行数值化处理，从而大幅提高了GFlowNets的性能，在多种生物化学任务中都达到了或超过了其他基elines和抽样方法的水平。<details>
<summary>Abstract</summary>
GFlowNets are probabilistic models that learn a stochastic policy that sequentially generates compositional structures, such as molecular graphs. They are trained with the objective of sampling such objects with probability proportional to the object's reward. Among GFlowNets, the temperature-conditional GFlowNets represent a family of policies indexed by temperature, and each is associated with the correspondingly tempered reward function. The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very different gradient profiles and ideal scales of the policy's logits. We find that the challenge is greatly reduced if a learned function of the temperature is used to scale the policy's logits directly. We empirically show that our strategy dramatically improves the performances of GFlowNets, outperforming other baselines, including reinforcement learning and sampling methods, in terms of discovering diverse modes in multiple biochemical tasks.
</details>
<details>
<summary>摘要</summary>
GFlowNets 是一种概率模型，它们学习一种随机policy，该policySequentially生成组合结构，如分子图。它们被训练以抽样这些对象，抽样概率与对象的奖励相对应。 Among GFlowNets, temperature-conditional GFlowNets 表示一种由温度索引的策略家族，每个策略都与相应的凉卷奖励函数相关。 The major benefit of temperature-conditional GFlowNets is the controllability of GFlowNets' exploration and exploitation through adjusting temperature. We propose Learning to Scale Logits for temperature-conditional GFlowNets (LSL-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets. It is based on the idea that previously proposed temperature-conditioning approaches introduced numerical challenges in the training of the deep network because different temperatures may give rise to very different gradient profiles and ideal scales of the policy's logits. We find that the challenge is greatly reduced if a learned function of the temperature is used to scale the policy's logits directly. We empirically show that our strategy dramatically improves the performances of GFlowNets, outperforming other baselines, including reinforcement learning and sampling methods, in terms of discovering diverse modes in multiple biochemical tasks.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Instance-Generative-Framework-for-MILP-Solvers-Under-Limited-Data-Availability"><a href="#A-Deep-Instance-Generative-Framework-for-MILP-Solvers-Under-Limited-Data-Availability" class="headerlink" title="A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability"></a>A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02807">http://arxiv.org/abs/2310.02807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/miralab-ustc/l2o-g2milp">https://github.com/miralab-ustc/l2o-g2milp</a></li>
<li>paper_authors: Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, Feng Wu</li>
<li>for: 本文旨在提出一种深度生成框架，用于生成混合整数线性 програм（MILP）实例。</li>
<li>methods: 本文使用 masked variational autoencoder 来生成 MILP 实例，并且可以不基于专家设计的 формуLA。</li>
<li>results: 实验表明，我们的方法可以生成与实际数据有相似结构和计算困难的 MILP 实例，同时能够保持实际数据的特性。<details>
<summary>Abstract</summary>
In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, the first deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instances without prior expert-designed formulations, while preserving the structures and computational hardness of real-world datasets, simultaneously. Thus the generated instances can facilitate downstream tasks for enhancing MILP solvers under limited data availability. We design a suite of benchmarks to evaluate the quality of the generated MILP instances. Experiments demonstrate that our method can produce instances that closely resemble real-world datasets in terms of both structures and computational hardness. The deliverables are released at https://miralab-ustc.github.io/L2O-G2MILP.
</details>
<details>
<summary>摘要</summary>
To address this problem, we propose G2MILP, the first deep generative framework for MILP instances. G2MILP represents MILP instances as bipartite graphs and uses a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instances without prior expert-designed formulations, while preserving the structures and computational hardness of real-world datasets.We design a suite of benchmarks to evaluate the quality of the generated MILP instances. Our experiments show that our method can produce instances that closely resemble real-world datasets in terms of both structures and computational hardness. The generated instances can facilitate downstream tasks for enhancing MILP solvers under limited data availability.The deliverables of our method, including the source code and the generated instances, are released at <https://miralab-ustc.github.io/L2O-G2MILP>.
</details></li>
</ul>
<hr>
<h2 id="A-Data-facilitated-Numerical-Method-for-Richards-Equation-to-Model-Water-Flow-Dynamics-in-Soil"><a href="#A-Data-facilitated-Numerical-Method-for-Richards-Equation-to-Model-Water-Flow-Dynamics-in-Soil" class="headerlink" title="A Data-facilitated Numerical Method for Richards Equation to Model Water Flow Dynamics in Soil"></a>A Data-facilitated Numerical Method for Richards Equation to Model Water Flow Dynamics in Soil</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02806">http://arxiv.org/abs/2310.02806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyuan Song, Zheyu Jiang</li>
<li>for: 该论文主要针对Root-zone soil moisture监测和精细耕作、智能灌溉和旱灾防治。</li>
<li>methods: 该论文提出了一种基于数据优化的数值方法，称为D-GRW方法，该方法 synergistically integrate了适应性线性化方案、神经网络和全球随机漫步在finite volume分割框架中，以生成Richards方程的精准数值解决方案，并保证了合理的假设下的有限 convergence。</li>
<li>results: 该论文通过三个示例，证明了D-GRW方法的精度和质量，并与参照方法和商业解决方案进行比较。<details>
<summary>Abstract</summary>
Root-zone soil moisture monitoring is essential for precision agriculture, smart irrigation, and drought prevention. Modeling the spatiotemporal water flow dynamics in soil is typically achieved by solving a hydrological model, such as the Richards equation which is a highly nonlinear partial differential equation (PDE). In this paper, we present a novel data-facilitated numerical method for solving the mixed-form Richards equation. This numerical method, which we call the D-GRW (Data-facilitated global Random Walk) method, synergistically integrates adaptive linearization scheme, neural networks, and global random walk in a finite volume discretization framework to produce accurate numerical solutions of the Richards equation with guaranteed convergence under reasonable assumptions. Through three illustrative examples, we demonstrate and discuss the superior accuracy and mass conservation performance of our D-GRW method and compare it with benchmark numerical methods and commercial solver.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Root-zone soil moisture monitoring is essential for precision agriculture, smart irrigation, and drought prevention. Modeling the spatiotemporal water flow dynamics in soil is typically achieved by solving a hydrological model, such as the Richards equation which is a highly nonlinear partial differential equation (PDE). In this paper, we present a novel data-facilitated numerical method for solving the mixed-form Richards equation. This numerical method, which we call the D-GRW (Data-facilitated global Random Walk) method, synergistically integrates adaptive linearization scheme, neural networks, and global random walk in a finite volume discretization framework to produce accurate numerical solutions of the Richards equation with guaranteed convergence under reasonable assumptions. Through three illustrative examples, we demonstrate and discuss the superior accuracy and mass conservation performance of our D-GRW method and compare it with benchmark numerical methods and commercial solver." into Simplified Chinese.Root-zone soil moisture monitoring 是精细农业、智能灌溉和抗旱防治的关键。通常通过解决水文模型，如理查德方程（PDE）来模拟 soil 中水流动的空间时间流动。在这篇文章中，我们介绍了一种新的数据促进 numerical 方法，称为 D-GRW（数据促进全球随机步行）方法，该方法将适应性线性化 schemes，神经网络和全球随机步行 synergistically  интегрирован到 finite volume 积分框架中，以生成理查德方程的数字解决方案，并保证合理假设下的有限积分稳定性。通过三个示例，我们展示了 D-GRW 方法的精度和质量保证性，并与参考数值方法和商业解决方案进行比较。
</details></li>
</ul>
<hr>
<h2 id="MAD-Max-Beyond-Single-Node-Enabling-Large-Machine-Learning-Model-Acceleration-on-Distributed-Systems"><a href="#MAD-Max-Beyond-Single-Node-Enabling-Large-Machine-Learning-Model-Acceleration-on-Distributed-Systems" class="headerlink" title="MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems"></a>MAD Max Beyond Single-Node: Enabling Large Machine Learning Model Acceleration on Distributed Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02784">http://arxiv.org/abs/2310.02784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Hsia, Alicia Golden, Bilge Acun, Newsha Ardalani, Zachary DeVito, Gu-Yeon Wei, David Brooks, Carole-Jean Wu</li>
<li>for: 这份研究是为了提高大机器学习（ML）模型的训练和部署效率，并且需要大量的分布式计算基础设施。</li>
<li>methods: 本研究使用了现实世界中大型机器学习模型的训练数据中心设备，并开发了一个敏捷性性能模型框架，以帮助并行化和硬件软件共同设计策略。</li>
<li>results: 根据实际的大型机器学习模型和现代GPU训练硬件，本研究显示了预训练和测试场景中的2.24倍和5.27倍的throughput提升潜力。<details>
<summary>Abstract</summary>
Training and deploying large machine learning (ML) models is time-consuming and requires significant distributed computing infrastructures. Based on real-world large model training on datacenter-scale infrastructures, we show 14~32% of all GPU hours are spent on communication with no overlapping computation. To minimize the outstanding communication latency, in this work, we develop an agile performance modeling framework to guide parallelization and hardware-software co-design strategies. Using the suite of real-world large ML models on state-of-the-art GPU training hardware, we demonstrate 2.24x and 5.27x throughput improvement potential for pre-training and inference scenarios, respectively.
</details>
<details>
<summary>摘要</summary>
培训和部署大型机器学习（ML）模型需要很长时间，并且需要庞大的分布式计算基础设施。根据实际的大型模型在数据中心级别基础设施上的训练实践，我们发现14%-32%的所有GPU时间都被沟通占用，没有重叠计算。为了减少待机时间，在这项工作中，我们开发了一个轻松性性能模型框架，以导引并行化和硬件软件共设策略。使用现代GPU训练硬件的集成体系，我们示出了2.24倍和5.27倍的throughput提升潜力，在预训练和推理场景中。
</details></li>
</ul>
<hr>
<h2 id="Expected-flow-networks-in-stochastic-environments-and-two-player-zero-sum-games"><a href="#Expected-flow-networks-in-stochastic-environments-and-two-player-zero-sum-games" class="headerlink" title="Expected flow networks in stochastic environments and two-player zero-sum games"></a>Expected flow networks in stochastic environments and two-player zero-sum games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02779">http://arxiv.org/abs/2310.02779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Jiralerspong, Bilun Sun, Danilo Vucetic, Tianyu Zhang, Yoshua Bengio, Gauthier Gidel, Nikolay Malkin</li>
<li>for: 这个论文是为了应用在不同的结构化物件生成任务中，使用组合抽样方法来实现快速抽样高质量的物件。</li>
<li>methods: 该论文使用了组合抽样网络（GFlowNets），并且提出了对应数据分布的预期流网络（EFlowNets），以及在抽样中对抗环境中的抽样网络（AFlowNets）。</li>
<li>results: 该论文显示了EFlowNets在数据分布中实现了更好的表现，并且在游戏中实现了更高的胜率（大于80%）。<details>
<summary>Abstract</summary>
Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments.
</details>
<details>
<summary>摘要</summary>
generative flow networks (GFlowNets) 是一种顺序采样模型，用于匹配给定的分布。 GFlowNets 在不同的结构化对象生成任务中成功应用，快速采样出高奖对象的多样性。我们提出了预期流网络（EFlowNets），它们extend GFlowNets 到随机环境中。我们表明，EFlowNets 在随机任务中比其他 GFlowNet 形式更高效。然后，我们扩展了 EFlowNets 的概念，提出了对抗流网络（AFlowNets），用于两个玩家的零SUM游戏。我们表明，AFlowNets 在 Connect-4 游戏中通过自游戏和AlphaZero比赛中，找到了大于 80% 的优化移动。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-and-Time-Series-as-Directed-Graphs-for-Quality-Recognition"><a href="#Graph-Neural-Networks-and-Time-Series-as-Directed-Graphs-for-Quality-Recognition" class="headerlink" title="Graph Neural Networks and Time Series as Directed Graphs for Quality Recognition"></a>Graph Neural Networks and Time Series as Directed Graphs for Quality Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02774">http://arxiv.org/abs/2310.02774</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Angelica Simonetti, Ferdinando Zanchetta</li>
<li>for: 这篇论文旨在探讨时序序列中的图 neural network (GNN) 的应用，并与现有算法相结合，如 temporally convolutional networks 和 recurrent neural networks。</li>
<li>methods: 该论文将时序序列视为导向图，从而利用 GNN 架构来捕捉时间相关性。开发了两种不同的几何深度学习模型，一种是监督类型的分类器，另一种是 autoencoder-like 模型用于信号重建。</li>
<li>results: 在质量识别问题中应用这两种模型，得到了有效的结果。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are becoming central in the study of time series, coupled with existing algorithms as Temporal Convolutional Networks and Recurrent Neural Networks. In this paper, we see time series themselves as directed graphs, so that their topology encodes time dependencies and we start to explore the effectiveness of GNNs architectures on them. We develop two distinct Geometric Deep Learning models, a supervised classifier and an autoencoder-like model for signal reconstruction. We apply these models on a quality recognition problem.
</details>
<details>
<summary>摘要</summary>
格点网络（GNNs）在时间序列研究中变得中心，与现有算法相结合，如时间卷积网络和循环神经网络。在这篇论文中，我们看到时间序列本身是导向图，其 topology 编码时间关系，我们开始探索 GNNs 架构在它们上的效果。我们开发了两种不同的几何深度学习模型，一个是supervised分类器，另一个是循环神经网络 для信号重建。我们在质量识别问题中应用这些模型。Note that Simplified Chinese is a writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-Algorithms-for-Hybrid-V2X-Communication-A-Benchmarking-Study"><a href="#Deep-Reinforcement-Learning-Algorithms-for-Hybrid-V2X-Communication-A-Benchmarking-Study" class="headerlink" title="Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study"></a>Deep Reinforcement Learning Algorithms for Hybrid V2X Communication: A Benchmarking Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03767">http://arxiv.org/abs/2310.03767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fouzi Boukhalfa, Reda Alami, Mastane Achab, Eric Moulines, Mehdi Bennis</li>
<li>for: 提高自动驾驶车辆的安全性水平，以达到飞行器级别的可靠性。</li>
<li>methods: 利用多种通信技术的 redundancy 来实现高可靠性，并使用深度强化学习算法解决 vertical handover 问题。</li>
<li>results: 比起现有的状态艺术方法，使用 DRL 算法可以更好地增加 V-VLC 头灯的可用性和重复率，从而减少通信成本while maintaining a high level of reliability。<details>
<summary>Abstract</summary>
In today's era, autonomous vehicles demand a safety level on par with aircraft. Taking a cue from the aerospace industry, which relies on redundancy to achieve high reliability, the automotive sector can also leverage this concept by building redundancy in V2X (Vehicle-to-Everything) technologies. Given the current lack of reliable V2X technologies, this idea is particularly promising. By deploying multiple RATs (Radio Access Technologies) in parallel, the ongoing debate over the standard technology for future vehicles can be put to rest. However, coordinating multiple communication technologies is a complex task due to dynamic, time-varying channels and varying traffic conditions. This paper addresses the vertical handover problem in V2X using Deep Reinforcement Learning (DRL) algorithms. The goal is to assist vehicles in selecting the most appropriate V2X technology (DSRC/V-VLC) in a serpentine environment. The results show that the benchmarked algorithms outperform the current state-of-the-art approaches in terms of redundancy and usage rate of V-VLC headlights. This result is a significant reduction in communication costs while maintaining a high level of reliability. These results provide strong evidence for integrating advanced DRL decision mechanisms into the architecture as a promising approach to solving the vertical handover problem in V2X.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".Translation notes:* "era" is translated as "时代" (shí dài) in Simplified Chinese.* "autonomous vehicles" is translated as "自动驾驶车辆" (zì àuto xíng yí qiāng liàng) in Simplified Chinese.* "aerospace industry" is translated as "航空航天产业" (háng kōng háng tiān chǎng yè) in Simplified Chinese.* "redundancy" is translated as "备用性" (bèi yòng xìng) in Simplified Chinese.* "V2X" is translated as "车辆到所有物的通信" (qì liàng dào suǒ yǒu wù de tōng xìn) in Simplified Chinese.* "RATs" is translated as "无线通信技术" (wú xiān tōng xìn jī shu) in Simplified Chinese.* "serpentine environment" is translated as "蛇形环境" (shé xíng huán jìng) in Simplified Chinese.* "Deep Reinforcement Learning" is translated as "深度强化学习" (shēn dào qiáng hòu xué xí) in Simplified Chinese.* "benchmarked algorithms" is translated as "标准算法" (biāo zhǔn suān fāng) in Simplified Chinese.* "usage rate" is translated as "使用率" (shǐ yòng gè) in Simplified Chinese.* "communication costs" is translated as "通信成本" (tōng xìn chéng běn) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Kernel-based-function-learning-in-dynamic-and-non-stationary-environments"><a href="#Kernel-based-function-learning-in-dynamic-and-non-stationary-environments" class="headerlink" title="Kernel-based function learning in dynamic and non stationary environments"></a>Kernel-based function learning in dynamic and non stationary environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02767">http://arxiv.org/abs/2310.02767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Giaretta, Mauro Bisiacco, Gianluigi Pillonetto</li>
<li>for: 这个论文是关于Function estimation from sparse and noisy data的研究，具体来说是关于supervised learning的研究，其中每个训练集元素都是一个输入位置和一个输出响应的couple。</li>
<li>methods: 这篇论文使用了kernel-based ridge regression方法，并derived convergence conditions under non-stationary distributions，包括在不同时间点上的探索-利用问题。</li>
<li>results: 这篇论文提出了一些关于函数估计的结果，包括对non-stationary distribution下的函数估计的研究，以及在探索-利用问题中的应用。<details>
<summary>Abstract</summary>
One central theme in machine learning is function estimation from sparse and noisy data. An example is supervised learning where the elements of the training set are couples, each containing an input location and an output response. In the last decades, a substantial amount of work has been devoted to design estimators for the unknown function and to study their convergence to the optimal predictor, also characterizing the learning rate. These results typically rely on stationary assumptions where input locations are drawn from a probability distribution that does not change in time. In this work, we consider kernel-based ridge regression and derive convergence conditions under non stationary distributions, addressing also cases where stochastic adaption may happen infinitely often. This includes the important exploration-exploitation problems where e.g. a set of agents/robots has to monitor an environment to reconstruct a sensorial field and their movements rules are continuously updated on the basis of the acquired knowledge on the field and/or the surrounding environment.
</details>
<details>
<summary>摘要</summary>
（一）中心主题是机器学习中的函数估计从稀疏噪声数据中进行。例如，超级vised learning，训练集中的元素是一对输入位置和输出响应。过去几十年，对不知函数的估计和优化预测器的设计，以及其对优化预测器的整体学习率的研究，已经占据了大量的时间和精力。这些研究通常假设输入位置采样从一个不变的概率分布中，而不是时间不变的概率分布。在这种情况下，我们考虑使用核函数的ridge regression，并 derive non-stationary分布下的整体学习率。此外，我们还考虑了无限次随机adaptation的情况，包括重要的exploration-exploitation问题，例如一组机器人/робот需要监测环境，重建感知场和其运动规则是基于获得的场景和/或周围环境知识不断更新的。
</details></li>
</ul>
<hr>
<h2 id="Fair-Feature-Selection-A-Comparison-of-Multi-Objective-Genetic-Algorithms"><a href="#Fair-Feature-Selection-A-Comparison-of-Multi-Objective-Genetic-Algorithms" class="headerlink" title="Fair Feature Selection: A Comparison of Multi-Objective Genetic Algorithms"></a>Fair Feature Selection: A Comparison of Multi-Objective Genetic Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02752">http://arxiv.org/abs/2310.02752</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Brookhouse, Alex Freitas</li>
<li>for: 这个研究paper是为了提出一种新的公平特征选择方法，以提高分类器的准确率和公平性。</li>
<li>methods: 这个paper使用了两种不同的多目标优化方法：Pareto优化和lexicographic优化，以选择最佳的特征子集。</li>
<li>results: 比较两种方法的结果显示，lexicographic优化方法在精度方面表现较好，而不会对公平性造成影响。这是一个重要的结果，因为现在大多数的GA均基于Pareto方法，这个结果显示了一个新的进展方向。<details>
<summary>Abstract</summary>
Machine learning classifiers are widely used to make decisions with a major impact on people's lives (e.g. accepting or denying a loan, hiring decisions, etc). In such applications,the learned classifiers need to be both accurate and fair with respect to different groups of people, with different values of variables such as sex and race. This paper focuses on fair feature selection for classification, i.e. methods that select a feature subset aimed at maximising both the accuracy and the fairness of the predictions made by a classifier. More specifically, we compare two recently proposed Genetic Algorithms (GAs) for fair feature selection that are based on two different multi-objective optimisation approaches: (a) a Pareto dominance-based GA; and (b) a lexicographic optimisation-based GA, where maximising accuracy has higher priority than maximising fairness. Both GAs use the same measures of accuracy and fairness, allowing for a controlled comparison. As far as we know, this is the first comparison between the Pareto and lexicographic approaches for fair classification. The results show that, overall, the lexicographic GA outperformed the Pareto GA with respect to accuracy without degradation of the fairness of the learned classifiers. This is an important result because at present nearly all GAs for fair classification are based on the Pareto approach, so these results suggest a promising new direction for research in this area.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Posterior-Sampling-Based-on-Gradient-Flows-of-the-MMD-with-Negative-Distance-Kernel"><a href="#Posterior-Sampling-Based-on-Gradient-Flows-of-the-MMD-with-Negative-Distance-Kernel" class="headerlink" title="Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel"></a>Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03054">http://arxiv.org/abs/2310.03054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabianaltekrueger/conditional_mmd_flows">https://github.com/fabianaltekrueger/conditional_mmd_flows</a></li>
<li>paper_authors: Paul Hagemann, Johannes Hertrich, Fabian Altekrüger, Robert Beinert, Jannis Chemseddine, Gabriele Steidl</li>
<li>for: Conditional generative modeling and posterior sampling</li>
<li>methods: Discrete Wasserstein gradient flows and negative distance kernel</li>
<li>results: Efficient computation, error bound for posterior distributions, and demonstrated power through numerical examples in various applications such as conditional image generation and inverse problems like superresolution, inpainting, and computed tomography.Here’s the full text in Simplified Chinese:</li>
<li>for: 本文提出了基于最大均方差（MMD）的 conditional flows，用于 posterior sampling 和 conditional generative modeling。MMD 具有许多优点，如高效计算via slicing 和 sorting。</li>
<li>methods: 我们使用柔性 Wasserstein 泛化流动来近似 JOINT 分布函数，并建立了这个分布函数的错误 bound。此外，我们证明了我们的 particle flow 实际上是一个 Wasserstein 泛化流动。</li>
<li>results: 我们通过数学示例展示了我们的方法的力量，包括 conditional image generation 和 inverse problems 如超分辨、填充和计算Tomography 在低剂量和有限角度设置下。<details>
<summary>Abstract</summary>
We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modeling. This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting. We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions. Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional. The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.
</details>
<details>
<summary>摘要</summary>
我们提议使用最大均方差（MMD）的条件流动，以优化 posterior sampling 和 conditional generative modeling。 MMD 具有许多有利的性质，如高效的计算via  slice 和 sort。我们使用离散 Wasserstein 梯度流动来近似 JOINT 分布ground truth 和观察值，并确定错误 bound  для posterior 分布。此外，我们证明我们的 particle flow 实际上是一个 Wasserstein 梯度流动的函数。我们的方法在数字例如 conditional 图像生成和 inverse 问题中如 superresolution、inpainting 和 computed tomography 中的低剂量和有限角度设置中显示出了力量。
</details></li>
</ul>
<hr>
<h2 id="SALSA-Semantically-Aware-Latent-Space-Autoencoder"><a href="#SALSA-Semantically-Aware-Latent-Space-Autoencoder" class="headerlink" title="SALSA: Semantically-Aware Latent Space Autoencoder"></a>SALSA: Semantically-Aware Latent Space Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02744">http://arxiv.org/abs/2310.02744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kathryn E. Kirchoff, Travis Maxfield, Alexander Tropsha, Shawn M. Gomez</li>
<li>For: 该研究旨在提高深度学习在药物发现中的应用，特别是在化学数据的表示方面。* Methods: 该研究使用了自适应神经网络（Autoencoder）和变换器，并添加了一个对比任务来学习分子之间的结构相似性。* Results: 研究表明，通过添加对比任务，Autoencoder可以学习更加有意义的分子表示，并且能够更好地捕捉分子之间的结构相似性。这些表示能够帮助进一步提高药物发现的效果。<details>
<summary>Abstract</summary>
In deep learning for drug discovery, chemical data are often represented as simplified molecular-input line-entry system (SMILES) sequences which allow for straightforward implementation of natural language processing methodologies, one being the sequence-to-sequence autoencoder. However, we observe that training an autoencoder solely on SMILES is insufficient to learn molecular representations that are semantically meaningful, where semantics are defined by the structural (graph-to-graph) similarities between molecules. We demonstrate by example that autoencoders may map structurally similar molecules to distant codes, resulting in an incoherent latent space that does not respect the structural similarities between molecules. To address this shortcoming we propose Semantically-Aware Latent Space Autoencoder (SALSA), a transformer-autoencoder modified with a contrastive task, tailored specifically to learn graph-to-graph similarity between molecules. Formally, the contrastive objective is to map structurally similar molecules (separated by a single graph edit) to nearby codes in the latent space. To accomplish this, we generate a novel dataset comprised of sets of structurally similar molecules and opt for a supervised contrastive loss that is able to incorporate full sets of positive samples. We compare SALSA to its ablated counterparts, and show empirically that the composed training objective (reconstruction and contrastive task) leads to a higher quality latent space that is more 1) structurally-aware, 2) semantically continuous, and 3) property-aware.
</details>
<details>
<summary>摘要</summary>
深度学习在药物发现中使用化学数据，通常将化学数据表示为简化的分子输入线入系统（SMILES）序列，这使得自然语言处理方法可以直接实现。然而，我们发现训练自动编码器solely on SMILES是不够学习分子表示，其中表示是指分子之间的结构相似性。我们通过示例显示，自动编码器可能将结构相似分子映射到远离的编码中，导致latent空间无法尊重分子之间的结构相似性。为解决这个缺陷，我们提议使用Semantically-Aware Latent Space Autoencoder（SALSA），一种基于变换器的自动编码器，并添加了一个对比 зада务，特意用于学习分子之间的结构相似性。正式来说，对比目标是将结构相似分子（分开一个分子编辑）映射到latent空间中的近处编码。为实现这一点，我们生成了一个新的数据集，其中包含了结构相似分子的集合，并选择了一种监督的对比损失，可以包含全集的正样本。我们比较了SALSA与其简化版本，并证明了组合的训练目标（重建和对比任务）会导致一个更高质量的latent空间，其1) 结构意识更强，2) 更加连续，3) 性质意识更好。
</details></li>
</ul>
<hr>
<h2 id="Reward-Model-Ensembles-Help-Mitigate-Overoptimization"><a href="#Reward-Model-Ensembles-Help-Mitigate-Overoptimization" class="headerlink" title="Reward Model Ensembles Help Mitigate Overoptimization"></a>Reward Model Ensembles Help Mitigate Overoptimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02743">http://arxiv.org/abs/2310.02743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Coste, Usman Anwar, Robert Kirk, David Krueger</li>
<li>for: 这个研究是用来研究使用强化学习自人给大型自然语言模型的调整方法，以便让模型能够更好地遵循人类的指令。</li>
<li>methods: 这个研究使用了一个人工设定的人类反馈系统，并使用了两种优化方法：（a）最佳 sampling（BoN）和（b） proximal policy optimization（PPO）。此外，这个研究还使用了一个大型“真实” reward model，以模拟人类的偏好。</li>
<li>results: 研究发现，使用ensemble-based conservative optimization可以有效地遏制 reward model 的过优化，并提高表现的精度。尤其是在使用 BoN 优化时，ensemble-based conservative optimization 可以提高表现的精度最多达 70%。此外，研究还发现，在添加25% 标签错误时，ensemble-based conservative optimization 仍然能够有效地遏制过优化。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the "true" reward, these learned reward models are susceptible to \textit{overoptimization}. Gao et al. (2023) studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. Using a similar setup, we conduct a systematic study to evaluate the efficacy of using ensemble-based conservative optimization objectives, specifically worst-case optimization (WCO) and uncertainty-weighted optimization (UWO), for mitigating reward model overoptimization when using two optimization methods: (a) best-of-n sampling (BoN) (b) proximal policy optimization (PPO). We additionally extend the setup of Gao et al. (2023) to include 25% label noise to better mirror real-world conditions. Both with and without label noise, we find that conservative optimization practically eliminates overoptimization and improves performance by up to 70% for BoN sampling. For PPO, ensemble-based conservative optimization always reduces overoptimization and outperforms single reward model optimization. Moreover, combining it with a small KL penalty successfully prevents overoptimization at no performance cost. Overall, our results demonstrate that ensemble-based conservative optimization can effectively counter overoptimization.
</details>
<details>
<summary>摘要</summary>
人工回馈学习（RLHF）是一种标准的精细调整大语言模型，以便跟进 instrucciones。在这个过程中，学习的奖励模型用于 aproximately 模型人类偏好。然而，由于这些学习的奖励模型是不完美的 "真实" 奖励的表示，因此它们容易过价 optimize。GAO et al. (2023) 在一个 sintética setup 中研究了这种现象，并显示了在不同的 proxy 奖励模型和训练数据使用的情况下，过价 optimize 仍然是一个持续存在的问题。使用相似的 setup，我们进行了一项系统的研究，以评估使用ensemble-based conservative optimization objective，特别是worst-case optimization（WCO）和uncertainty-weighted optimization（UWO）来 mitigate 奖励模型过价 optimize 问题，并使用两种优化方法：（a） best-of-n sampling （BoN）（b） proximal policy optimization （PPO）。我们还扩展了 GAO et al. 的 setup，包括25% 标签噪音，以更好地镜像实际条件。无论含有标签噪音或不含，我们发现ensemble-based conservative optimization 实际上消除了过价 optimize 问题，并提高性能达70% 。对于 PPO，ensemble-based conservative optimization 总是减少过价 optimize，并且在不产生性能损失的情况下，成功阻止过价 optimize。总的来说，我们的结果表明，ensemble-based conservative optimization 可以有效地解决过价 optimize 问题。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Imbalanced-Malware-Byteplot-Image-Classification-using-Transfer-Learning"><a href="#Comparative-Analysis-of-Imbalanced-Malware-Byteplot-Image-Classification-using-Transfer-Learning" class="headerlink" title="Comparative Analysis of Imbalanced Malware Byteplot Image Classification using Transfer Learning"></a>Comparative Analysis of Imbalanced Malware Byteplot Image Classification using Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02742">http://arxiv.org/abs/2310.02742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayasudha M, Ayesha Shaik, Gaurav Pendharkar, Soham Kumar, Muhesh Kumar B, Sudharshanan Balaji</li>
<li>for: 本研究旨在比较六种多类分类模型在Malimg数据集、Blended数据集和Malevis数据集上的性能，以了解类别不均衡对模型性能和融合的影响。</li>
<li>methods: 本研究使用了六种多类分类模型，包括ResNet50、EfficientNetB0和DenseNet169，并对不均衡和均衡数据进行比较。</li>
<li>results: 研究发现，当类别不均衡时，需要更少的轮数才能达到最高精度（97%），并且存在类型之间的差异。此外，ResNet50、EfficientNetB0和DenseNet169模型在不均衡和均衡数据上都能够表现良好。<details>
<summary>Abstract</summary>
Cybersecurity is a major concern due to the increasing reliance on technology and interconnected systems. Malware detectors help mitigate cyber-attacks by comparing malware signatures. Machine learning can improve these detectors by automating feature extraction, identifying patterns, and enhancing dynamic analysis. In this paper, the performance of six multiclass classification models is compared on the Malimg dataset, Blended dataset, and Malevis dataset to gain insights into the effect of class imbalance on model performance and convergence. It is observed that the more the class imbalance less the number of epochs required for convergence and a high variance across the performance of different models. Moreover, it is also observed that for malware detectors ResNet50, EfficientNetB0, and DenseNet169 can handle imbalanced and balanced data well. A maximum precision of 97% is obtained for the imbalanced dataset, a maximum precision of 95% is obtained on the intermediate imbalance dataset, and a maximum precision of 95% is obtained for the perfectly balanced dataset.
</details>
<details>
<summary>摘要</summary>
信息安全是一个主要的担忧，因为随着技术和相互连接系统的使用量的增加，黑客可以利用漏洞和攻击系统。恶意软件检测器可以减轻cyber攻击的影响，通过比较恶意软件签名。机器学习可以改进这些检测器，通过自动提取特征、识别模式和动态分析提高检测效果。在这篇论文中，我们比较了六种多类分类模型在Malimg数据集、Blended数据集和Malevis数据集上的性能，以了解类别不均衡对模型性能和融合的影响。我们发现，当类别不均衡时，模型的融合需要更少的epoch数，并且模型之间的性能差异较大。此外，我们还发现，为恶意软件检测器来说，ResNet50、EfficientNetB0和DenseNet169可以处理不均衡和均衡的数据都很好。在不均衡数据集上，最高的准确率为97%，在中等不均衡数据集上最高的准确率为95%，而在完全均衡数据集上最高的准确率为95%。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Rules-from-Event-Data-for-Study-Planning"><a href="#Extracting-Rules-from-Event-Data-for-Study-Planning" class="headerlink" title="Extracting Rules from Event Data for Study Planning"></a>Extracting Rules from Event Data for Study Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02735">http://arxiv.org/abs/2310.02735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/m4jidRafiei/AIStudyBuddy-RuleExtractor">https://github.com/m4jidRafiei/AIStudyBuddy-RuleExtractor</a></li>
<li>paper_authors: Majid Rafiei, Duygu Bayrak, Mahsa Pourbafrani, Gyunam Park, Hayyan Helal, Gerhard Lakemeyer, Wil M. P. van der Aalst</li>
<li>for: 这个研究旨在使用校内管理系统事件数据分析高等教育学生的学习路径。主要目的是为学生提供有用的学习规划建议。</li>
<li>methods: 本研究使用处理和数据探索技术来探索学生选择的课程序列对学术成就的影响。使用决策树模型生成基于数据的建议，并与建议的学习规划进行比较。</li>
<li>results: 评估运用RWTH慕尼黑理工大学计算机科学学士学位学生，发现提posed course sequence features有效地解释学术成就指标。此外，发现可以发展更灵活的学习规划。<details>
<summary>Abstract</summary>
In this study, we examine how event data from campus management systems can be used to analyze the study paths of higher education students. The main goal is to offer valuable guidance for their study planning. We employ process and data mining techniques to explore the impact of sequences of taken courses on academic success. Through the use of decision tree models, we generate data-driven recommendations in the form of rules for study planning and compare them to the recommended study plan. The evaluation focuses on RWTH Aachen University computer science bachelor program students and demonstrates that the proposed course sequence features effectively explain academic performance measures. Furthermore, the findings suggest avenues for developing more adaptable study plans.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了如何使用校园管理系统事件数据来分析高等教育学生的学习路径。主要目标是为学生的学习规划提供有价值的指导。我们使用过程挖掘技术和数据挖掘技术来探索学生们所选择的课程序列对学术成绩的影响。通过使用决策树模型，我们生成了基于数据的建议，并与推荐的学习计划进行比较。评估针对于rwth洪堡大学计算机科学学士学位学生，并证明了我们提出的课程序列特征有效地解释学术成绩指标。此外，发现还有改进学习计划的可能性。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Training-of-a-Neural-HMM-with-Label-and-Transition-Probabilities"><a href="#End-to-End-Training-of-a-Neural-HMM-with-Label-and-Transition-Probabilities" class="headerlink" title="End-to-End Training of a Neural HMM with Label and Transition Probabilities"></a>End-to-End Training of a Neural HMM with Label and Transition Probabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02724">http://arxiv.org/abs/2310.02724</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danenergetics/returnn">https://github.com/danenergetics/returnn</a></li>
<li>paper_authors: Daniel Mann, Tina Raissi, Wilfried Michel, Ralf Schlüter, Hermann Ney</li>
<li>for: 这 paper 的目的是提出一种基于隐藏马尔可夫模型 (HMM) 的端到端神经网络训练方法。</li>
<li>methods: 这 paper 使用的方法是在隐藏状态之间Explicitly modeling and learning transition probabilities。</li>
<li>results: 这 paper 的结果表明，虽然transition model training不提高了识别性能，但它对Alignment quality有着正面的影响，并且生成的 alignments 可以作为state-of-the-art Viterbi trainings 的可靠目标。<details>
<summary>Abstract</summary>
We investigate a novel modeling approach for end-to-end neural network training using hidden Markov models (HMM) where the transition probabilities between hidden states are modeled and learned explicitly. Most contemporary sequence-to-sequence models allow for from-scratch training by summing over all possible label segmentations in a given topology. In our approach there are explicit, learnable probabilities for transitions between segments as opposed to a blank label that implicitly encodes duration statistics. We implement a GPU-based forward-backward algorithm that enables the simultaneous training of label and transition probabilities. We investigate recognition results and additionally Viterbi alignments of our models. We find that while the transition model training does not improve recognition performance, it has a positive impact on the alignment quality. The generated alignments are shown to be viable targets in state-of-the-art Viterbi trainings.
</details>
<details>
<summary>摘要</summary>
我们研究了一种新的模型方法，使用隐藏Markov模型（HMM）来训练端到端神经网络。在我们的方法中，transition probabilities между隐藏状态被Explicitly modeled和学习。大多数当代序列到序列模型允许从scratch训练，通过所有可能的标签分 segmentation在给定的topology中总和。在我们的方法中，有Explicit, learnable probabilities for transition between segments，而不是一个负空标签，隐式地编码持续时间统计。我们实现了GPU上的前向后向算法，可以同时训练标签和转移概率。我们 investigate recognition results和Viterbi alignments of our models。我们发现，转移模型训练不会提高认识性能，但是它会改善对齐质量。生成的对齐是可靠的目标在state-of-the-art Viterbi训练中。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Temporal-Graph-Networks-Using-Module-Decoupling"><a href="#Leveraging-Temporal-Graph-Networks-Using-Module-Decoupling" class="headerlink" title="Leveraging Temporal Graph Networks Using Module Decoupling"></a>Leveraging Temporal Graph Networks Using Module Decoupling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02721">http://arxiv.org/abs/2310.02721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Or Feldman, Chaim Baskin</li>
<li>for: The paper is written for learning on dynamic graphs, specifically addressing the issue of using batches in modern approaches and the degradation of model performance.</li>
<li>methods: The paper proposes a decoupling strategy that enables models to update frequently while using batches, achieved by decoupling the core modules of temporal graph networks and implementing them with a minimal number of learnable parameters.</li>
<li>results: The proposed Lightweight Decoupled Temporal Graph Network (LDTGN) achieves comparable or state-of-the-art results with significantly higher throughput than previous art, outperforming previous approaches by more than 20% on benchmarks that require rapid model update rates.Here are the three points in Simplified Chinese:</li>
<li>for: 这篇论文是为了学习动态图而写的，具体是解决现代方法中使用批处理的问题，并且模型性能下降的问题。</li>
<li>methods: 这篇论文提出了一种分解策略，使得模型在使用批处理时能够频繁更新，通过分解核心模块的时间图网络，并使用最小化参数来实现。</li>
<li>results: 提出的轻量级解 Coupled Temporal Graph Network (LDTGN) 在多个动态图标准 bencmarks 上得到了相似或者状态艺术的结果，并且与之前的方法比较高过的 Throughput 得到了较高的提升，比如 USLegis 或 UNTrade 等benchmarks，提升了 más de 20%。<details>
<summary>Abstract</summary>
Modern approaches for learning on dynamic graphs have adopted the use of batches instead of applying updates one by one. The use of batches allows these techniques to become helpful in streaming scenarios where updates to graphs are received at extreme speeds. Using batches, however, forces the models to update infrequently, which results in the degradation of their performance. In this work, we suggest a decoupling strategy that enables the models to update frequently while using batches. By decoupling the core modules of temporal graph networks and implementing them using a minimal number of learnable parameters, we have developed the Lightweight Decoupled Temporal Graph Network (LDTGN), an exceptionally efficient model for learning on dynamic graphs. LDTG was validated on various dynamic graph benchmarks, providing comparable or state-of-the-art results with significantly higher throughput than previous art. Notably, our method outperforms previous approaches by more than 20\% on benchmarks that require rapid model update rates, such as USLegis or UNTrade. The code to reproduce our experiments is available at \href{https://orfeld415.github.io/module-decoupling}{this http url}.
</details>
<details>
<summary>摘要</summary>
现代方法 для学习动态图使用了批处理而不是一个个更新。使用批处理可以使这些技术在流动enario中变得有用，但是它们会让模型更新不够频繁，从而导致性能下降。在这项工作中，我们提出了一种解耦策略，允许模型在使用批处理时频繁更新。我们通过对核心模块的时间图网络进行解耦，并使用最小化的学习参数来实现，开发了轻量级解耦时间图网络（LDTGN）。LDTG在各种动态图标准测试上验证，提供了相似或现有的成绩，同时具有明显高于前一代的吞吐量。特别是，我们的方法在需要快速模型更新率的标准测试上，比前一代方法提高了更多于20%。我们的实验代码可以在 \href{https://orfeld415.github.io/module-decoupling}{这个HTTP URL} 上复制。
</details></li>
</ul>
<hr>
<h2 id="Local-Search-GFlowNets"><a href="#Local-Search-GFlowNets" class="headerlink" title="Local Search GFlowNets"></a>Local Search GFlowNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02710">http://arxiv.org/abs/2310.02710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dbsxodud-11/ls_gfn">https://github.com/dbsxodud-11/ls_gfn</a></li>
<li>paper_authors: Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, Jinkyoo Park</li>
<li>for: 提高Generative Flow Networks（GFlowNets）的性能，尤其是在生成高奖励Sample的问题上。</li>
<li>methods: 使用本地搜索，通过破坏和重建导向高奖励解决方案，从而偏向生成高奖励样本。</li>
<li>results: 在 biochemical tasks 中显著提高性能。<details>
<summary>Abstract</summary>
Generative Flow Networks (GFlowNets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. GFlowNets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space. This paper proposes to train GFlowNets with local search which focuses on exploiting high rewarded sample space to resolve this issue. Our main idea is to explore the local neighborhood via destruction and reconstruction guided by backward and forward policies, respectively. This allows biasing the samples toward high-reward solutions, which is not possible for a typical GFlowNet solution generation scheme which uses the forward policy to generate the solution from scratch. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks. Source code is available: \url{https://github.com/dbsxodud-11/ls_gfn}.
</details>
<details>
<summary>摘要</summary>
流式网络（GFlowNets）是一种抽象采样方法，它们学习一个对 discrete 对象的分布，该分布与对象的奖励相对。GFlowNets 显示出了强大的多样性生成能力，但有时会因为扫描范围太广而偶尔难以保持高奖励样本的生成。这篇论文提议通过在本地搜索中使用破坏和重建，以便通过后向和前向策略分别导航，偏好生成高奖励的样本。这与一般 GFlowNet 的解决方案生成方式不同，后者使用前向策略从零开始生成解决方案。广泛的实验表明，这种方法可以在多个生物化学任务中显著提高性能。源代码可以在以下链接中找到：\url{https://github.com/dbsxodud-11/ls_gfn}。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Hybrid-Heterogeneity-on-Federated-Optimization-via-Gradient-Diversity-Maximization"><a href="#Tackling-Hybrid-Heterogeneity-on-Federated-Optimization-via-Gradient-Diversity-Maximization" class="headerlink" title="Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization"></a>Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02702">http://arxiv.org/abs/2310.02702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zengdun-cs/FedAWARE">https://github.com/Zengdun-cs/FedAWARE</a></li>
<li>paper_authors: Dun Zeng, Zenglin Xu, Yu Pan, Qifan Wang, Xiaoying Tang</li>
<li>for: This paper focuses on addressing the challenges of hybrid heterogeneity in federated learning, specifically by developing a novel server-side gradient-based optimizer called \textsc{FedAWARE} to mitigate the negative effects of statistical and system heterogeneity on federated optimization.</li>
<li>methods: The proposed optimizer uses adaptive gradient diversity maximization in the server update direction to improve the efficiency of federated learning in heterogeneous settings. Theoretical guarantees are provided to support the effectiveness of the proposed method.</li>
<li>results: Extensive experiments in heterogeneous federated learning scenarios demonstrate that \textsc{FedAWARE} significantly enhances the performance of federated learning across varying degrees of hybrid heterogeneity, outperforming existing methods in terms of convergence rate and final model accuracy.<details>
<summary>Abstract</summary>
Federated learning refers to a distributed machine learning paradigm in which data samples are decentralized and distributed among multiple clients. These samples may exhibit statistical heterogeneity, which refers to data distributions are not independent and identical across clients. Additionally, system heterogeneity, or variations in the computational power of the clients, introduces biases into federated learning. The combined effects of statistical and system heterogeneity can significantly reduce the efficiency of federated optimization. However, the impact of hybrid heterogeneity is not rigorously discussed. This paper explores how hybrid heterogeneity affects federated optimization by investigating server-side optimization. The theoretical results indicate that adaptively maximizing gradient diversity in server update direction can help mitigate the potential negative consequences of hybrid heterogeneity. To this end, we introduce a novel server-side gradient-based optimizer \textsc{FedAWARE} with theoretical guarantees provided. Intensive experiments in heterogeneous federated settings demonstrate that our proposed optimizer can significantly enhance the performance of federated learning across varying degrees of hybrid heterogeneity.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-Federated-Optimization-by-Reducing-Variance-of-Adaptive-Unbiased-Client-Sampling"><a href="#Exploring-Federated-Optimization-by-Reducing-Variance-of-Adaptive-Unbiased-Client-Sampling" class="headerlink" title="Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling"></a>Exploring Federated Optimization by Reducing Variance of Adaptive Unbiased Client Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02698">http://arxiv.org/abs/2310.02698</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zengdun-cs/K-Vib">https://github.com/Zengdun-cs/K-Vib</a></li>
<li>paper_authors: Dun Zeng, Zenglin Xu, Yu Pan, Qifan Wang, Xiaoying Tang</li>
<li>for: 这篇论文主要针对 Federated Learning (FL) 系统中的客户端采样问题，即在训练过程中采样一部分客户端以构建全局模型。</li>
<li>methods: 本论文提出了一系列”免费”的适应客户端采样技术，其中服务器可以在不需要进一步的本地通信和计算的前提下，建立有前途的采样概率和可靠的全局估计。</li>
<li>results: 根据这些技术，本论文提出了一种新的采样器called K-Vib，它可以在在线 convex 优化中对客户端采样进行优化，并达到了 $\tilde{\mathcal{O}\big(N^{\frac{1}{3}T^{\frac{2}{3}&#x2F;K^{\frac{4}{3}\big)$ 的 regret bound，其中 $K$ 是通信预算。这意味着它可以大幅提高 federated 优化的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) systems usually sample a fraction of clients to conduct a training process. Notably, the variance of global estimates for updating the global model built on information from sampled clients is highly related to federated optimization quality. This paper explores a line of "free" adaptive client sampling techniques in federated optimization, where the server builds promising sampling probability and reliable global estimates without requiring additional local communication and computation. We capture a minor variant in the sampling procedure and improve the global estimation accordingly. Based on that, we propose a novel sampler called K-Vib, which solves an online convex optimization respecting client sampling in federated optimization. It achieves improved a linear speed up on regret bound $\tilde{\mathcal{O}\big(N^{\frac{1}{3}T^{\frac{2}{3}/K^{\frac{4}{3}\big)$ with communication budget $K$. As a result, it significantly improves the performance of federated optimization. Theoretical improvements and intensive experiments on classic federated tasks demonstrate our findings.
</details>
<details>
<summary>摘要</summary>
联合学习（Federated Learning，FL）系统通常会抽出一部分客户进行训练过程。需要注意的是，训练过程中对全球模型的更新所需的全球估计的方差与联合优化质量有高度的相关性。本文探讨了一条“免费”的自适应客户抽样技术在联合优化中，并在不需要额外的本地通信和计算之下，实现估计的可靠性和全球模型的建立。我们捕捉了抽样程序中的一小变种，并根据此进行改进全球估计。基于这，我们提出了一个名为K-Vib的新抽样器，它在联合优化中解决了线上凸优化问题，并在通信预算K下 achieves 的Linear Speedup $\tilde{\mathcal{O}\big(N^{\frac{1}{3}T^{\frac{2}{3}/K^{\frac{4}{3}\big)$。因此，它可以对联合优化进行明显的改进。理论上的改进和实际的实验结果表明我们的发现。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Block-Term-Decomposition-for-the-Modelling-of-Higher-Order-Arrays"><a href="#Probabilistic-Block-Term-Decomposition-for-the-Modelling-of-Higher-Order-Arrays" class="headerlink" title="Probabilistic Block Term Decomposition for the Modelling of Higher-Order Arrays"></a>Probabilistic Block Term Decomposition for the Modelling of Higher-Order Arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02694">http://arxiv.org/abs/2310.02694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jesper Løve Hinrich, Morten Mørup</li>
<li>for: 本研究旨在提出一种可效的 bayesian 幂级分解方法，用于robust地推断多linear数据中的pattern。</li>
<li>methods: 该方法基于 von-Mises Fisher 矩阵分布，以实现多linear Tucker 部分中的正交性。</li>
<li>results: 在synthetic和实际数据上，我们验证了 bayesian 推断过程和提出的 pBTD 方法，并在噪声数据和模型顺序量化问题上进行了应用。结果表明， probabilistic BTD 可以量化适当的多linear结构，提供一种可靠的推断多linear数据中的pattern。<details>
<summary>Abstract</summary>
Tensors are ubiquitous in science and engineering and tensor factorization approaches have become important tools for the characterization of higher order structure. Factorizations includes the outer-product rank Canonical Polyadic Decomposition (CPD) as well as the multi-linear rank Tucker decomposition in which the Block-Term Decomposition (BTD) is a structured intermediate interpolating between these two representations. Whereas CPD, Tucker, and BTD have traditionally relied on maximum-likelihood estimation, Bayesian inference has been use to form probabilistic CPD and Tucker. We propose, an efficient variational Bayesian probabilistic BTD, which uses the von-Mises Fisher matrix distribution to impose orthogonality in the multi-linear Tucker parts forming the BTD. On synthetic and two real datasets, we highlight the Bayesian inference procedure and demonstrate using the proposed pBTD on noisy data and for model order quantification. We find that the probabilistic BTD can quantify suitable multi-linear structures providing a means for robust inference of patterns in multi-linear data.
</details>
<details>
<summary>摘要</summary>
tensor 是科学和工程领域中的普遍存在，tensor factorization 方法已成为高阶结构的特征化工具。这些分解包括外积级 Canonical Polyadic Decomposition (CPD) 以及多线性级 Tucker 分解，其中 Block-Term Decomposition (BTD) 是这两种表示之间的结构化中间件。而 CPDT, Tucker 和 BTD 传统上采用最大化可信度估计，我们则使用 Bayesian 推断来建立probabilistic CPD 和 Tucker。我们提议了一种高效的 Bayesian 推断可变 BTD，使用 von-Mises Fisher 矩阵分布来强制多线性 Tucker 部分的正交性。在一些Synthetic和两个实际数据集上，我们展示了 Bayesian 推断过程并使用我们提议的 pBTD 处理噪声数据和模型顺序量化。我们发现可变 BTD 可以量化适当的多线性结构，提供一种robust的 Pattern 推断方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Ocean-Subgrid-Scale-Parameterizations-Using-Fourier-Neural-Operators"><a href="#Robust-Ocean-Subgrid-Scale-Parameterizations-Using-Fourier-Neural-Operators" class="headerlink" title="Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators"></a>Robust Ocean Subgrid-Scale Parameterizations Using Fourier Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02691">http://arxiv.org/abs/2310.02691</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vikvador/ocean-subgrid-parameterizations-in-an-idealized-model-using-machine-learning">https://github.com/vikvador/ocean-subgrid-parameterizations-in-an-idealized-model-using-machine-learning</a></li>
<li>paper_authors: Victor Mangeleer, Gilles Louppe</li>
<li>for: 这paper是为了研究小规模过程对海洋动力的影响，但直接计算这些过程仍然是计算成本高的问题。</li>
<li>methods: 本paper使用Fourier Neural Operators来开发参数化方法，并证明其精度和通用性比其他方法更高。</li>
<li>results: 本paper的结果表明，Fourier Neural Operators可以准确地捕捉小规模过程的影响，并且在长期预测中具有较少的误差。<details>
<summary>Abstract</summary>
In climate simulations, small-scale processes shape ocean dynamics but remain computationally expensive to resolve directly. For this reason, their contributions are commonly approximated using empirical parameterizations, which lead to significant errors in long-term projections. In this work, we develop parameterizations based on Fourier Neural Operators, showcasing their accuracy and generalizability in comparison to other approaches. Finally, we discuss the potential and limitations of neural networks operating in the frequency domain, paving the way for future investigation.
</details>
<details>
<summary>摘要</summary>
在气候模拟中，小规模过程对海洋动力具有重要作用，但直接计算起来很计算昂贵。因此，通常使用实验性参数化来 aproximate their contributions，这会导致长期预测中出现显著的错误。在这项工作中，我们开发了基于傅ри涅尔 нейрон算法的参数化，并证明其准确性和通用性与其他方法相比。最后，我们讨论了神经网络在频率域中运行的潜在和局限性，以便未来的调查。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Beyond-Stationarity-Convergence-Analysis-of-Stochastic-Softmax-Policy-Gradient-Methods"><a href="#Beyond-Stationarity-Convergence-Analysis-of-Stochastic-Softmax-Policy-Gradient-Methods" class="headerlink" title="Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods"></a>Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02671">http://arxiv.org/abs/2310.02671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Klein, Simon Weissmann, Leif Döring</li>
<li>for: 这篇论文是关于Sequential Decision-Making Problems的Formal Framework，尤其是在 finite-time horizon 中的 Optimal Stopping 和Specific Supply Chain Problems 等领域。</li>
<li>methods: 这篇论文使用了 dynamic programming 和 policy gradient 的结合，将parameters逐步训练 backwards in time。</li>
<li>results: 研究发现，使用 dynamic policy gradient 训练 much better 利用 finite-time problems 的结构，实现 improved convergence bounds。<details>
<summary>Abstract</summary>
Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite-time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming. This paper introduces a combination of dynamic programming and policy gradient called dynamic policy gradient, where the parameters are trained backwards in time. For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation. It turns out that the use of dynamic policy gradient training much better exploits the structure of finite-time problems which is reflected in improved convergence bounds.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hire-When-You-Need-to-Gradual-Participant-Recruitment-for-Auction-based-Federated-Learning"><a href="#Hire-When-You-Need-to-Gradual-Participant-Recruitment-for-Auction-based-Federated-Learning" class="headerlink" title="Hire When You Need to: Gradual Participant Recruitment for Auction-based Federated Learning"></a>Hire When You Need to: Gradual Participant Recruitment for Auction-based Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02651">http://arxiv.org/abs/2310.02651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Tan, Han Yu</li>
<li>for: 这篇论文的目的是提出一个名为Gradual Participant Selection scheme for Auction-based Federated Learning（GPS-AFL），用于解决联邦学习（Federated Learning，FL）中数据所有者（Data Owner，DO）的选择问题。</li>
<li>methods: 这篇论文使用的方法包括Gradual Participant Selection scheme，它在多次训练轮中逐渐选择需要的DO，以提高选择的多样性和对绩效的影响。</li>
<li>results: 实验结果显示，GPS-AFL可以对联邦学习中的成本优化，并提高绩效。相比最佳先进方法，GPS-AFL可以降低成本33.65%，并提高绩效2.91%。<details>
<summary>Abstract</summary>
The success of federated Learning (FL) depends on the quantity and quality of the data owners (DOs) as well as their motivation to join FL model training. Reputation-based FL participant selection methods have been proposed. However, they still face the challenges of the cold start problem and potential selection bias towards highly reputable DOs. Such a bias can result in lower reputation DOs being prematurely excluded from future FL training rounds, thereby reducing the diversity of training data and the generalizability of the resulting models. To address these challenges, we propose the Gradual Participant Selection scheme for Auction-based Federated Learning (GPS-AFL). Unlike existing AFL incentive mechanisms which generally assume that all DOs required for an FL task must be selected in one go, GPS-AFL gradually selects the required DOs over multiple rounds of training as more information is revealed through repeated interactions. It is designed to strike a balance between cost saving and performance enhancement, while mitigating the drawbacks of selection bias in reputation-based FL. Extensive experiments based on real-world datasets demonstrate the significant advantages of GPS-AFL, which reduces costs by 33.65% and improved total utility by 2.91%, on average compared to the best-performing state-of-the-art approach.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）的成功取决于数据所有者（DO）的数量和质量以及他们参与FL模型训练的动机。基于声誉的FL参与者选择方法已被提议，但它们仍面临冷启动问题和可能的选择偏袋向高声誉DOs。这种偏袋会导致低声誉DOs在未来FL训练回合中被排除，从而减少了训练数据的多样性和模型的泛化性。为解决这些挑战，我们提出了 Gradual Participant Selection scheme for Auction-based Federated Learning（GPS-AFL）。与现有的AFL奖励机制不同，GPS-AFL在多个训练回合中逐渐选择需要参与FL任务的DO，以便在更多信息的披露下进行更加精准的选择。它旨在寻求成本节省和性能提高的平衡，同时减少选择偏袋的问题。基于实际数据集的实验表明，GPS-AFL可以规避选择偏袋问题，同时节省33.65%的成本和提高总用度2.91%，相比最佳现有方法。
</details></li>
</ul>
<hr>
<h2 id="Generative-Modeling-of-Regular-and-Irregular-Time-Series-Data-via-Koopman-VAEs"><a href="#Generative-Modeling-of-Regular-and-Irregular-Time-Series-Data-via-Koopman-VAEs" class="headerlink" title="Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs"></a>Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02619">http://arxiv.org/abs/2310.02619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilan Naiman, N. Benjamin Erichson, Pu Ren, Michael W. Mahoney, Omri Azencot</li>
<li>for: 本研究旨在提出一种基于koopman理论的生成框架，以提高时间序列生成的质量。</li>
<li>methods: 该方法使用了varational autoencoder（VAE）和生成对抗网络（GAN）的组合，并通过使用spectral工具来制约Conditional Prior动态的linear map。</li>
<li>results: 实验结果表明，koopman VAE（KVAE）在多个Synthetic和实际时间序列生成 benchmarck上表现出色，并且可以在Regular和Irregular数据上进行优化。KVAE可以提高时间序列的discriminative和predictive metric，并且可以更好地模拟实际分布。<details>
<summary>Abstract</summary>
Generating realistic time series data is important for many engineering and scientific applications. Existing work tackles this problem using generative adversarial networks (GANs). However, GANs are often unstable during training, and they can suffer from mode collapse. While variational autoencoders (VAEs) are known to be more robust to these issues, they are (surprisingly) less often considered for time series generation. In this work, we introduce Koopman VAE (KVAE), a new generative framework that is based on a novel design for the model prior, and that can be optimized for either regular and irregular training data. Inspired by Koopman theory, we represent the latent conditional prior dynamics using a linear map. Our approach enhances generative modeling with two desired features: (i) incorporating domain knowledge can be achieved by leverageing spectral tools that prescribe constraints on the eigenvalues of the linear map; and (ii) studying the qualitative behavior and stablity of the system can be performed using tools from dynamical systems theory. Our results show that KVAE outperforms state-of-the-art GAN and VAE methods across several challenging synthetic and real-world time series generation benchmarks. Whether trained on regular or irregular data, KVAE generates time series that improve both discriminative and predictive metrics. We also present visual evidence suggesting that KVAE learns probability density functions that better approximate empirical ground truth distributions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>生成实际时间序数据是许多工程和科学应用中的重要问题。现有的工作使用生成对抗网络（GAN）解决这个问题。然而，GAN 在训练时常会不稳定，而且容易出现模式塌缩。而变量自编码器（VAE）则被认为是更加稳定的选择，但它们在时间序数据生成中却让人感到奇怪地少被考虑。在这种工作中，我们引入了 Koopman VAE（KVAE），一种新的生成框架，基于一种新的模型先验设计。我们使用 Koopman 理论来表示干扰条件先验动力，并可以在训练数据是正规或异常的情况下优化。我们的方法拥有两个愿望特征：（一）可以通过spectral工具来遵循干扰条件的特征，从而把域知识引入到模型中;（二）可以通过动力学系统理论来研究系统的质量和稳定性。我们的结果表明，KVAE 在多个Synthetic和实际时间序数据生成 benchmark 上表现出色，并且在训练正规或异常数据时都能够提高描述性和预测性指标。我们还提供视觉证据，表明 KVAE 学习的概率分布更加准确地反映实际的基础真实分布。
</details></li>
</ul>
<hr>
<h2 id="Learning-adjacency-matrix-for-dynamic-graph-neural-network"><a href="#Learning-adjacency-matrix-for-dynamic-graph-neural-network" class="headerlink" title="Learning adjacency matrix for dynamic graph neural network"></a>Learning adjacency matrix for dynamic graph neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02606">http://arxiv.org/abs/2310.02606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osama Ahmad, Omer Abdul Jalil, Usman Nazir, Murtaza Taj</li>
<li>for: This paper aims to address the challenge of modeling spatio-temporal data using Graph Convolutional Networks (GCNs) by introducing an encoder block to learn missing temporal links in the Block Adjacency Matrix (BA).</li>
<li>methods: The proposed method uses an encoder block to process the BA and predict connections between previously unconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix (STBAM). The STBAM is then fed into a GNN to capture the complex spatio-temporal topology of the network.</li>
<li>results: The proposed method achieves superior results compared to state-of-the-art results on benchmark datasets, surgVisDom and C2D2, with slightly higher complexity. However, the computational overhead remains significantly lower than conventional non-graph-based methodologies for spatio-temporal data.Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文目标是使用图解 convolutional Neural Networks (GCNs) 来处理空间-时间数据，并提出一种encoder块来学习缺失的时间链接。</li>
<li>methods: 提议的方法使用encoder块处理块相互邻接矩阵(BA)，并预测连接在不同时间步的子图，得到一个Spatio-Temporal Block Adjacency Matrix (STBAM)。然后将STBAM feed into GNN来捕捉复杂的空间-时间网络。</li>
<li>results: 提议的方法在benchmark数据集上（surgVisDom和C2D2）达到了与当前最佳方法相当的成绩，但略有更高的复杂性。然而，计算 overhead仍然远低于非图基于的方法。<details>
<summary>Abstract</summary>
In recent work, [1] introduced the concept of using a Block Adjacency Matrix (BA) for the representation of spatio-temporal data. While their method successfully concatenated adjacency matrices to encapsulate spatio-temporal relationships in a single graph, it formed a disconnected graph. This limitation hampered the ability of Graph Convolutional Networks (GCNs) to perform message passing across nodes belonging to different time steps, as no temporal links were present. To overcome this challenge, we introduce an encoder block specifically designed to learn these missing temporal links. The encoder block processes the BA and predicts connections between previously unconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix (STBAM). This enriched matrix is then fed into a Graph Neural Network (GNN) to capture the complex spatio-temporal topology of the network. Our evaluations on benchmark datasets, surgVisDom and C2D2, demonstrate that our method, with slightly higher complexity, achieves superior results compared to state-of-the-art results. Our approach's computational overhead remains significantly lower than conventional non-graph-based methodologies for spatio-temporal data.
</details>
<details>
<summary>摘要</summary>
最近的工作中，[1] 提出了使用块邻接矩阵（BA）来表示空间时间数据的思想。他们的方法可以将邻接矩阵 concatenate 成一个图，以便在单个图上捕捉空间时间关系。然而，这种方法会形成离散图，这限制了图解决方法（GCNs）在不同时间步中的信息传递。为了解决这个挑战，我们提出了一个专门为了学习缺失的时间链接而设计的编码块。该编码块处理BA，预测了之前不相连的子图之间的连接，从而生成了一个具有空间时间特征的块邻接矩阵（STBAM）。这个充气的矩阵然后被 fed 给图神经网络（GNN），以捕捉复杂的空间时间 topology。我们对 benchmark 数据集 surgVisDom 和 C2D2 进行了评估，得出了较高的比较结果，而且与传统非图基的方法相比，我们的方法的计算开销仍然很低。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Key-Users’-behavior-trends-in-Volunteer-Based-Networks"><a href="#Analyzing-Key-Users’-behavior-trends-in-Volunteer-Based-Networks" class="headerlink" title="Analyzing Key Users’ behavior trends in Volunteer-Based Networks"></a>Analyzing Key Users’ behavior trends in Volunteer-Based Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05978">http://arxiv.org/abs/2310.05978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nofar Piterman, Tamar Makov, Michael Fire</li>
<li>for: This paper aims to explore the development of volunteer-based social networks and the behavior of key users in these networks.</li>
<li>methods: The authors developed two novel algorithms to analyze the behavior of key users in volunteer-based social networks, including a pattern-based algorithm and a machine learning-based forecasting model.</li>
<li>results: The authors used data from a peer-to-peer food-sharing platform to evaluate their algorithms and found that they could accurately predict future behavior of key users, with an accuracy of up to 89.6%. They identified four main types of key user behavior patterns and were able to forecast which users would become active donors or change their behavior to become mainly recipients.<details>
<summary>Abstract</summary>
Online social networks usage has increased significantly in the last decade and continues to grow in popularity. Multiple social platforms use volunteers as a central component. The behavior of volunteers in volunteer-based networks has been studied extensively in recent years. Here, we explore the development of volunteer-based social networks, primarily focusing on their key users' behaviors and activities. We developed two novel algorithms: the first reveals key user behavior patterns over time; the second utilizes machine learning methods to generate a forecasting model that can predict the future behavior of key users, including whether they will remain active donors or change their behavior to become mainly recipients, and vice-versa. These algorithms allowed us to analyze the factors that significantly influence behavior predictions.   To evaluate our algorithms, we utilized data from over 2.4 million users on a peer-to-peer food-sharing online platform. Using our algorithm, we identified four main types of key user behavior patterns that occur over time. Moreover, we succeeded in forecasting future active donor key users and predicting the key users that would change their behavior to donors, with an accuracy of up to 89.6%. These findings provide valuable insights into the behavior of key users in volunteer-based social networks and pave the way for more effective communities-building in the future, while using the potential of machine learning for this goal.
</details>
<details>
<summary>摘要</summary>
在过去一个十年中，在线社交网络的使用量增长了非常 significatively，并且继续增长受欢迎。多个社交平台都使用志愿者作为中心组件。志愿者在志愿者基于的网络中的行为已经得到了广泛的研究。在这里，我们探讨了志愿者基于的社交网络的发展，主要关注针对键用户的行为和活动。我们开发了两个新的算法：第一个显示针对时间的键用户行为模式;第二个使用机器学习方法生成预测未来键用户行为的预测模型，包括未来是否继续为主要捐赠者或者变为主要接收者，并且vice versa。这些算法让我们可以分析预测行为的因素。为了评估我们的算法，我们使用了在线 peer-to-peer 食物分享平台上的超过240万名用户的数据。使用我们的算法，我们发现了四种主要的键用户行为模式，并且成功预测了未来活跃捐赠键用户和变为主要接收者的键用户，准确率达到89.6%。这些发现提供了志愿者基于社交网络的行为的有价值的洞察，并为未来建立更有效的社区帮助做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Enabled-Precision-Position-Control-and-Thermal-Regulation-in-Advanced-Thermal-Actuators"><a href="#Machine-Learning-Enabled-Precision-Position-Control-and-Thermal-Regulation-in-Advanced-Thermal-Actuators" class="headerlink" title="Machine Learning-Enabled Precision Position Control and Thermal Regulation in Advanced Thermal Actuators"></a>Machine Learning-Enabled Precision Position Control and Thermal Regulation in Advanced Thermal Actuators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02583">http://arxiv.org/abs/2310.02583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyed Mo Mirvakili, Ehsan Haghighat, Douglas Sim</li>
<li>for: 这个论文是为了研究和开发一种基于机器学习的开 Loop控制器，用于控制纤维素人工肌。</li>
<li>methods: 该论文使用了一种基于机器学习的ensemble encoder-style feed-forward neural network来映射所需的位移轨迹到所需的功率。</li>
<li>results: 研究人员通过对一种纤维素人工肌进行Position控制，证明了该控制器可以在没有外部传感器的情况下实现精准的位移控制。<details>
<summary>Abstract</summary>
With their unique combination of characteristics - an energy density almost 100 times that of human muscle, and a power density of 5.3 kW/kg, similar to a jet engine's output - Nylon artificial muscles stand out as particularly apt for robotics applications. However, the necessity of integrating sensors and controllers poses a limitation to their practical usage. Here we report a constant power open-loop controller based on machine learning. We show that we can control the position of a nylon artificial muscle without external sensors. To this end, we construct a mapping from a desired displacement trajectory to a required power using an ensemble encoder-style feed-forward neural network. The neural controller is carefully trained on a physics-based denoised dataset and can be fine-tuned to accommodate various types of thermal artificial muscles, irrespective of the presence or absence of hysteresis.
</details>
<details>
<summary>摘要</summary>
借助其独特的特点 - 能量密度超过人体肌肉100倍，功率密度与液体发动机类似（5.3 kW/kg） - 聚合物人造肌 stood out as particularly suitable for robotics applications. However, the need to integrate sensors and controllers poses a practical limitation. We report a constant power open-loop controller based on machine learning. We show that we can control the position of a nylon artificial muscle without external sensors. To achieve this, we establish a mapping from a desired displacement trajectory to a required power using an ensemble encoder-style feed-forward neural network. The neural controller is carefully trained on a physics-based denoised dataset and can be fine-tuned to accommodate various types of thermal artificial muscles, regardless of the presence or absence of hysteresis.
</details></li>
</ul>
<hr>
<h2 id="Online-Estimation-and-Inference-for-Robust-Policy-Evaluation-in-Reinforcement-Learning"><a href="#Online-Estimation-and-Inference-for-Robust-Policy-Evaluation-in-Reinforcement-Learning" class="headerlink" title="Online Estimation and Inference for Robust Policy Evaluation in Reinforcement Learning"></a>Online Estimation and Inference for Robust Policy Evaluation in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02581">http://arxiv.org/abs/2310.02581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weidong Liu, Jiyuan Tu, Yichen Zhang, Xi Chen</li>
<li>for: 本研究旨在探讨对强化学习策略评估中的统计推断，特别是使用强化学习算法计算出来的参数估计的统计推断。</li>
<li>methods: 本研究使用了Robust statistics和强化学习的概率评估方法，并提出了一种在线的robust政策评估方法，以及一种基于极限分布的 Statistical Inference 方法。</li>
<li>results: 本研究通过实验验证了其算法的有效性，并提供了一种更加多样化和可靠的强化学习策略评估方法。<details>
<summary>Abstract</summary>
Recently, reinforcement learning has gained prominence in modern statistics, with policy evaluation being a key component. Unlike traditional machine learning literature on this topic, our work places emphasis on statistical inference for the parameter estimates computed using reinforcement learning algorithms. While most existing analyses assume random rewards to follow standard distributions, limiting their applicability, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework. In this paper, we develop an online robust policy evaluation procedure, and establish the limiting distribution of our estimator, based on its Bahadur representation. Furthermore, we develop a fully-online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper bridges the gap between robust statistics and statistical inference in reinforcement learning, offering a more versatile and reliable approach to policy evaluation. Finally, we validate the efficacy of our algorithm through numerical experiments conducted in real-world reinforcement learning experiments.
</details>
<details>
<summary>摘要</summary>
最近，再强化学习在现代统计中得到了广泛的应用，其中政策评估是关键 ком ponent。 unlike traditional machine learning literature on this topic, our work emphasizes statistical inference for the parameter estimates computed using reinforcement learning algorithms. While most existing analyses assume random rewards follow standard distributions, limiting their applicability, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework.In this paper, we develop an online robust policy evaluation procedure and establish the limiting distribution of our estimator based on its Bahadur representation. Furthermore, we develop a fully-online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper bridges the gap between robust statistics and statistical inference in reinforcement learning, offering a more versatile and reliable approach to policy evaluation. Finally, we validate the efficacy of our algorithm through numerical experiments conducted in real-world reinforcement learning experiments.Here is the translation of the given text into Traditional Chinese:最近，再强化学习在现代统计中得到了广泛的应用，其中政策评估是关键 ком ponent。 Unlike traditional machine learning literature on this topic, our work emphasizes statistical inference for the parameter estimates computed using reinforcement learning algorithms. While most existing analyses assume random rewards follow standard distributions, limiting their applicability, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework.In this paper, we develop an online robust policy evaluation procedure and establish the limiting distribution of our estimator based on its Bahadur representation. Furthermore, we develop a fully-online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper bridges the gap between robust statistics and statistical inference in reinforcement learning, offering a more versatile and reliable approach to policy evaluation. Finally, we validate the efficacy of our algorithm through numerical experiments conducted in real-world reinforcement learning experiments.
</details></li>
</ul>
<hr>
<h2 id="Improving-Knowledge-Distillation-with-Teacher’s-Explanation"><a href="#Improving-Knowledge-Distillation-with-Teacher’s-Explanation" class="headerlink" title="Improving Knowledge Distillation with Teacher’s Explanation"></a>Improving Knowledge Distillation with Teacher’s Explanation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02572">http://arxiv.org/abs/2310.02572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sayantan Chowdhury, Ben Liang, Ali Tizghadam, Ilijc Albanese</li>
<li>for: 提高一个低复杂度学生模型的性能，通过一个更强大的教师模型帮助。</li>
<li>methods: 使用知识填充distillation（KD）方法，但是限制了传递的知识量。这个研究提出了一种新的知识解释填充（KED）框架，允许学生模型不仅从教师模型的预测中学习，还可以从教师模型的解释中获得知识。</li>
<li>results: 我们的实验表明，KED学生模型可以在多个数据集上substantially outperform KD学生模型相同的复杂度。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) improves the performance of a low-complexity student model with the help of a more powerful teacher. The teacher in KD is a black-box model, imparting knowledge to the student only through its predictions. This limits the amount of transferred knowledge. In this work, we introduce a novel Knowledge Explaining Distillation (KED) framework, which allows the student to learn not only from the teacher's predictions but also from the teacher's explanations. We propose a class of superfeature-explaining teachers that provide explanation over groups of features, along with the corresponding student model. We also present a method for constructing the superfeatures. We then extend KED to reduce complexity in convolutional neural networks, to allow augmentation with hidden-representation distillation methods, and to work with a limited amount of training data using chimeric sets. Our experiments over a variety of datasets show that KED students can substantially outperform KD students of similar complexity.
</details>
<details>
<summary>摘要</summary>
知识填充（KD）可以提高一个低复杂度学生模型的性能，通过一个更强大的教师模型的帮助。教师模型在KD中是一个黑盒模型，只通过其预测来传递知识给学生。这限制了知识的传递量。在这项工作中，我们介绍了一种新的知识解释填充（KED）框架，允许学生不仅从教师模型的预测中学习，还可以从教师模型的解释中获得知识。我们提议一类超特征解释教师，这些教师可以对特征组提供解释，同时与学生模型一起提供。我们还提出了超特征的构造方法。然后，我们将KED扩展到减少卷积神经网络的复杂性，使其可以与隐藏表示填充方法结合使用，并使用有限的训练数据使用 chimera 集。我们的实验表明，KED学生可以在多个数据集上明显超越KD学生。
</details></li>
</ul>
<hr>
<h2 id="Practical-Private-Assurance-of-the-Value-of-Collaboration"><a href="#Practical-Private-Assurance-of-the-Value-of-Collaboration" class="headerlink" title="Practical, Private Assurance of the Value of Collaboration"></a>Practical, Private Assurance of the Value of Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02563">http://arxiv.org/abs/2310.02563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Jameel Asghar, Zhigang Lu, Zhongrui Zhao, Dali Kaafar</li>
<li>for: The paper is written for the problem of collaborative machine learning between two parties who want to improve the accuracy of their prediction models by sharing their datasets, but they do not want to reveal their models and datasets to each other beforehand.</li>
<li>methods: The paper proposes an interactive protocol based on fully homomorphic encryption (FHE) and label differential privacy to enable collaborative machine learning while preserving the privacy of the parties’ models and datasets. The protocol uses a neural network as the underlying machine learning model.</li>
<li>results: The paper shows that the proposed protocol achieves a significant improvement in accuracy compared to a protocol using entirely FHE operations, and the results are obtained with a time that is many orders of magnitude faster. The security of the protocol is proven in the universal composability framework assuming honest-but-curious parties, but with one party having no expertise in labeling its initial dataset.<details>
<summary>Abstract</summary>
Two parties wish to collaborate on their datasets. However, before they reveal their datasets to each other, the parties want to have the guarantee that the collaboration would be fruitful. We look at this problem from the point of view of machine learning, where one party is promised an improvement on its prediction model by incorporating data from the other party. The parties would only wish to collaborate further if the updated model shows an improvement in accuracy. Before this is ascertained, the two parties would not want to disclose their models and datasets. In this work, we construct an interactive protocol for this problem based on the fully homomorphic encryption scheme over the Torus (TFHE) and label differential privacy, where the underlying machine learning model is a neural network. Label differential privacy is used to ensure that computations are not done entirely in the encrypted domain, which is a significant bottleneck for neural network training according to the current state-of-the-art FHE implementations. We prove the security of our scheme in the universal composability framework assuming honest-but-curious parties, but where one party may not have any expertise in labelling its initial dataset. Experiments show that we can obtain the output, i.e., the accuracy of the updated model, with time many orders of magnitude faster than a protocol using entirely FHE operations.
</details>
<details>
<summary>摘要</summary>
To address this problem, we construct an interactive protocol based on fully homomorphic encryption over the torus (TFHE) and label differential privacy. Label differential privacy ensures that computations are not performed entirely in the encrypted domain, which is a significant bottleneck for neural network training according to current state-of-the-art FHE implementations. We prove the security of our scheme in the universal composability framework, assuming honest-but-curious parties, but where one party may not have any expertise in labeling its initial dataset.Experiments show that we can obtain the output, i.e., the accuracy of the updated model, with a time many orders of magnitude faster than a protocol using entirely FHE operations.
</details></li>
</ul>
<hr>
<h2 id="Semi-Federated-Learning-Convergence-Analysis-and-Optimization-of-A-Hybrid-Learning-Framework"><a href="#Semi-Federated-Learning-Convergence-Analysis-and-Optimization-of-A-Hybrid-Learning-Framework" class="headerlink" title="Semi-Federated Learning: Convergence Analysis and Optimization of A Hybrid Learning Framework"></a>Semi-Federated Learning: Convergence Analysis and Optimization of A Hybrid Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02559">http://arxiv.org/abs/2310.02559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingheng Zheng, Wanli Ni, Hui Tian, Deniz Gunduz, Tony Q. S. Quek, Zhu Han</li>
<li>for: 该论文旨在提出一种 semi-federated learning（SemiFL）模型，以充分利用基站（BS）和设备之间的计算资源，并且在同时采用中央学习（CL）和分布式学习（FL）的 hybrid 实现方式下，提高学习效率和质量。</li>
<li>methods: 该论文使用了一种 novel transceiver structure，将在空中计算和非对称多access（NMA）技术相结合，以提高通信效率。此外，论文还提出了一种基于closed-form optimality gap的可靠性分析方法，以及一种基于 transmit power和接收天线的非对称优化问题的解决方法。</li>
<li>results: 论文的实验结果表明，提出的 SemiFL 方法可以比 conventinal FL 方法提高3.2%的准确率，并且在 MNIST 数据集上达到了比 estado-of-the-art  bencmarks 高的准确率。<details>
<summary>Abstract</summary>
Under the organization of the base station (BS), wireless federated learning (FL) enables collaborative model training among multiple devices. However, the BS is merely responsible for aggregating local updates during the training process, which incurs a waste of the computational resource at the BS. To tackle this issue, we propose a semi-federated learning (SemiFL) paradigm to leverage the computing capabilities of both the BS and devices for a hybrid implementation of centralized learning (CL) and FL. Specifically, each device sends both local gradients and data samples to the BS for training a shared global model. To improve communication efficiency over the same time-frequency resources, we integrate over-the-air computation for aggregation and non-orthogonal multiple access for transmission by designing a novel transceiver structure. To gain deep insights, we conduct convergence analysis by deriving a closed-form optimality gap for SemiFL and extend the result to two extra cases. In the first case, the BS uses all accumulated data samples to calculate the CL gradient, while a decreasing learning rate is adopted in the second case. Our analytical results capture the destructive effect of wireless communication and show that both FL and CL are special cases of SemiFL. Then, we formulate a non-convex problem to reduce the optimality gap by jointly optimizing the transmit power and receive beamformers. Accordingly, we propose a two-stage algorithm to solve this intractable problem, in which we provide the closed-form solutions to the beamformers. Extensive simulation results on two real-world datasets corroborate our theoretical analysis, and show that the proposed SemiFL outperforms conventional FL and achieves 3.2% accuracy gain on the MNIST dataset compared to state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary>
在基站（BS）的组织下，无线联合学习（FL）可以在多个设备之间进行共同模型训练。然而，BS只负责收集本地更新 durante el proceso de entrenamiento, 这会导致BS的计算资源的浪费。为解决这个问题，我们提出了半联合学习（SemiFL） paradigma，以利用BS和设备之间的计算能力 для hybrid实现中央学习（CL）和FL。specifically, each device sends both local gradients and data samples to the BS for training a shared global model. To improve communication efficiency over the same time-frequency resources, we integrate over-the-air computation for aggregation and non-orthogonal multiple access for transmission by designing a novel transceiver structure. To gain deep insights, we conduct convergence analysis by deriving a closed-form optimality gap for SemiFL and extend the result to two extra cases. In the first case, the BS uses all accumulated data samples to calculate the CL gradient, while a decreasing learning rate is adopted in the second case. Our analytical results capture the destructive effect of wireless communication and show that both FL and CL are special cases of SemiFL. Then, we formulate a non-convex problem to reduce the optimality gap by jointly optimizing the transmit power and receive beamformers. Accordingly, we propose a two-stage algorithm to solve this intractable problem, in which we provide the closed-form solutions to the beamformers. Extensive simulation results on two real-world datasets corroborate our theoretical analysis, and show that the proposed SemiFL outperforms conventional FL and achieves 3.2% accuracy gain on the MNIST dataset compared to state-of-the-art benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Federated-Learning-Using-Knowledge-Codistillation"><a href="#Heterogeneous-Federated-Learning-Using-Knowledge-Codistillation" class="headerlink" title="Heterogeneous Federated Learning Using Knowledge Codistillation"></a>Heterogeneous Federated Learning Using Knowledge Codistillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02549">http://arxiv.org/abs/2310.02549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jared Lichtarge, Ehsan Amid, Shankar Kumar, Tien-Ju Yang, Rohan Anil, Rajiv Mathews</li>
<li>for: 提高 federated learning 中客户端之间模型 Architecture 的共享性，以提高模型性能。</li>
<li>methods: 提出一种方法，通过在整个池中训练小型模型，并在一些客户端上训练更大型模型，通过双向知识传递，使用无标签数据集在服务器上进行交互，不需要客户端参数的共享。</li>
<li>results: 在图像分类和自然语言处理任务上，提出了两种变种方法，可以超越 federated averaging 的限制，并且在只有部分out-of-domain或有限域知识传递数据时，也可以达到良好的效果。同时，双向知识传递允许模型在不同池中的客户端引入域转换。<details>
<summary>Abstract</summary>
Federated Averaging, and many federated learning algorithm variants which build upon it, have a limitation: all clients must share the same model architecture. This results in unused modeling capacity on many clients, which limits model performance. To address this issue, we propose a method that involves training a small model on the entire pool and a larger model on a subset of clients with higher capacity. The models exchange information bidirectionally via knowledge distillation, utilizing an unlabeled dataset on a server without sharing parameters. We present two variants of our method, which improve upon federated averaging on image classification and language modeling tasks. We show this technique can be useful even if only out-of-domain or limited in-domain distillation data is available. Additionally, the bi-directional knowledge distillation allows for domain transfer between the models when different pool populations introduce domain shift.
</details>
<details>
<summary>摘要</summary>
Federated Averaging 和许多联合学习算法的变种都有一个限制：所有客户端都必须使用同一个模型结构。这会导致许多客户端的可用模型容量不被利用，从而限制模型性能。为解决这个问题，我们提议一种方法，即在整个池中训练一个小型模型，并在一些客户端上训练一个更大的模型，这些客户端具有更高的计算能力。这两个模型通过知识传承进行双向交互，无需在服务器上分享参数。我们提出了两种变种，可以超越联合平均值在图像分类和自然语言处理任务上的性能。我们表明，即使只有部分客户端的数据可用，这种技术仍然可以获得有用的效果。此外，双向知识传承允许模型在不同的池中引入域转移。
</details></li>
</ul>
<hr>
<h2 id="Exact-and-soft-boundary-conditions-in-Physics-Informed-Neural-Networks-for-the-Variable-Coefficient-Poisson-equation"><a href="#Exact-and-soft-boundary-conditions-in-Physics-Informed-Neural-Networks-for-the-Variable-Coefficient-Poisson-equation" class="headerlink" title="Exact and soft boundary conditions in Physics-Informed Neural Networks for the Variable Coefficient Poisson equation"></a>Exact and soft boundary conditions in Physics-Informed Neural Networks for the Variable Coefficient Poisson equation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02548">http://arxiv.org/abs/2310.02548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Barschkis</li>
<li>for: 本研究旨在比较使用soft loss基于的边界条件（BC）和精确距离函数基于的BC在物理学信息泛化网络（PINN）中的效果。</li>
<li>methods: 本研究使用了变量系数Poisson方程作为目标偏微分方程，并对PINN模型进行了训练。两种不同的BC决策方法被比较，即soft loss基于的BC和精确距离函数基于的BC。</li>
<li>results: 研究发现，soft loss基于的BC和精确距离函数基于的BC具有不同的优劣点，选择合适的BC决策方法可以提高PINN模型的拟合精度。此外，本研究还提供了实践中如何实现这些PINN模型的代码和步骤示例。<details>
<summary>Abstract</summary>
Boundary conditions (BCs) are a key component in every Physics-Informed Neural Network (PINN). By defining the solution to partial differential equations (PDEs) along domain boundaries, BCs constrain the underlying boundary value problem (BVP) that a PINN tries to approximate. Without them, unique PDE solutions may not exist and finding approximations with PINNs would be a challenging, if not impossible task. This study examines how soft loss-based and exact distance function-based BC imposition approaches differ when applied in PINNs. The well known variable coefficient Poisson equation serves as the target PDE for all PINN models trained in this work. Besides comparing BC imposition approaches, the goal of this work is to also provide resources on how to implement these PINNs in practice. To this end, Keras models with Tensorflow backend as well as a Python notebook with code examples and step-by-step explanations on how to build soft/exact BC PINNs are published alongside this review.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化中文。<</SYS>>物理学 informed neural network（PINN）中的边界条件（BC）是一个关键组成部分。通过定义解决方程 partial differential equations（PDEs）的边界解，BC 使得 PINN  approximates 的边界值问题（BVP）受到限制。无其，可能无准确解和使用 PINN  approximates 是一项困难，如果不可能的任务。本研究比较了使用 soft loss 基于和 exact distance function 基于的 BC 强制方法在 PINN 中的不同效果。使用了已知变量 coefficients Poisson equation 作为所有 PINN 模型在这种工作中的目标 PDE。此外，本研究还提供了实现这些 PINN 的实践方法，包括使用 Keras 模型和 Tensorflow 后端，以及一个 Python notationebook 中的代码示例和步骤说明如何建立 soft/exact BC PINN。
</details></li>
</ul>
<hr>
<h2 id="Joint-Design-of-Protein-Sequence-and-Structure-based-on-Motifs"><a href="#Joint-Design-of-Protein-Sequence-and-Structure-based-on-Motifs" class="headerlink" title="Joint Design of Protein Sequence and Structure based on Motifs"></a>Joint Design of Protein Sequence and Structure based on Motifs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02546">http://arxiv.org/abs/2310.02546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenqiao Song, Yunlong Zhao, Yufei Song, Wenxian Shi, Yang Yang, Lei Li</li>
<li>for: 本研究旨在设计具有感兴趣功能的蛋白质，尤其是蛋白质序列和结构共设计。</li>
<li>methods: 本研究提出了GeoPro方法，它可以同时设计蛋白质脊梁结构和序列。GeoPro利用了三维脊梁结构具有同质性的编码器和蛋白质序列带动器，以确保蛋白质序列和结构之间的互相约束。</li>
<li>results: 实验结果表明，GeoPro方法在两个生物学重要的铁蛋白 dataset上都超过了多个强基eline。特别是，我们的方法发现了不在蛋白质数据库（PDB）和UniProt中的新的β-恶啉蛋白和肌球蛋白，这些蛋白质具有稳定的折叠和活性站点环境，表明它们具有出色的生物功能。<details>
<summary>Abstract</summary>
Designing novel proteins with desired functions is crucial in biology and chemistry. However, most existing work focus on protein sequence design, leaving protein sequence and structure co-design underexplored. In this paper, we propose GeoPro, a method to design protein backbone structure and sequence jointly. Our motivation is that protein sequence and its backbone structure constrain each other, and thus joint design of both can not only avoid nonfolding and misfolding but also produce more diverse candidates with desired functions. To this end, GeoPro is powered by an equivariant encoder for three-dimensional (3D) backbone structure and a protein sequence decoder guided by 3D geometry. Experimental results on two biologically significant metalloprotein datasets, including $\beta$-lactamases and myoglobins, show that our proposed GeoPro outperforms several strong baselines on most metrics. Remarkably, our method discovers novel $\beta$-lactamases and myoglobins which are not present in protein data bank (PDB) and UniProt. These proteins exhibit stable folding and active site environments reminiscent of those of natural proteins, demonstrating their excellent potential to be biologically functional.
</details>
<details>
<summary>摘要</summary>
设计新的蛋白质with愿景功能是生物和化学中的关键。然而，大多数现有的工作都集中在蛋白质序列设计上，留下蛋白质序列和结构协同设计的潜在价值得未经探索。在本文中，我们提出了GeoPro方法，它可以同时设计蛋白质脊梁结构和序列。我们的动机是蛋白质序列和其脊梁结构之间存在紧密的关系，因此同时设计两者可以不仅避免蛋白质不感应和扭曲，而且生成更多的多样化候选者with愿景功能。为此，GeoPro得力于一种对三维脊梁结构具有对称性的编码器，以及基于3D几何学的蛋白质序列解码器。我们的实验结果表明，我们的提议的GeoPro在两个生物学上重要的铁蛋白质数据集上（包括β- лакта啡和肌红蛋白）都与多个强大的基准值相比，在大多数指标上表现出色。特别是，我们的方法发现了不在蛋白质数据库（PDB）和UniProt中的新的β- лакта啡和肌红蛋白，这些蛋白质具有自然蛋白质的稳定折叠和活化位点环境，这表明它们具有出色的生物功能。
</details></li>
</ul>
<hr>
<h2 id="Provable-Tensor-Completion-with-Graph-Information"><a href="#Provable-Tensor-Completion-with-Graph-Information" class="headerlink" title="Provable Tensor Completion with Graph Information"></a>Provable Tensor Completion with Graph Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02543">http://arxiv.org/abs/2310.02543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaidong Wang, Yao Wang, Xiuwu Liao, Shaojie Tang, Can Yang, Deyu Meng</li>
<li>for: 本文研究的目标是 tensor completion problem with graph information，即使用图信息来减少缺失数据的问题。</li>
<li>methods: 本文提出了一个整体的框架，包括模型、理论和算法，用于解决动态图常量缺失问题。模型基于变换的 t-SVD tensor decomposition模型，并添加了一种新的图 Orientated smoothness regularization。</li>
<li>results: 本文提供了一种有效的算法，并证明了其统计准确性。在 synthetic 数据和实际数据上进行了深入的数值实验，证明了模型的强大性。<details>
<summary>Abstract</summary>
Graphs, depicting the interrelations between variables, has been widely used as effective side information for accurate data recovery in various matrix/tensor recovery related applications. In this paper, we study the tensor completion problem with graph information. Current research on graph-regularized tensor completion tends to be task-specific, lacking generality and systematic approaches. Furthermore, a recovery theory to ensure performance remains absent. Moreover, these approaches overlook the dynamic aspects of graphs, treating them as static akin to matrices, even though graphs could exhibit dynamism in tensor-related scenarios. To confront these challenges, we introduce a pioneering framework in this paper that systematically formulates a novel model, theory, and algorithm for solving the dynamic graph regularized tensor completion problem. For the model, we establish a rigorous mathematical representation of the dynamic graph, based on which we derive a new tensor-oriented graph smoothness regularization. By integrating this regularization into a tensor decomposition model based on transformed t-SVD, we develop a comprehensive model simultaneously capturing the low-rank and similarity structure of the tensor. In terms of theory, we showcase the alignment between the proposed graph smoothness regularization and a weighted tensor nuclear norm. Subsequently, we establish assurances of statistical consistency for our model, effectively bridging a gap in the theoretical examination of the problem involving tensor recovery with graph information. In terms of the algorithm, we develop a solution of high effectiveness, accompanied by a guaranteed convergence, to address the resulting model. To showcase the prowess of our proposed model in contrast to established ones, we provide in-depth numerical experiments encompassing synthetic data as well as real-world datasets.
</details>
<details>
<summary>摘要</summary>
图表， displaying the relationships between variables, 已广泛用于精准数据恢复应用中作为有效的侧信息。在这篇论文中，我们研究了tensor completion问题中的图信息。现有的研究 tend to be task-specific, lacking generality and systematic approaches. Furthermore, a recovery theory to ensure performance remains absent. Moreover, these approaches overlook the dynamic aspects of graphs, treating them as static akin to matrices, even though graphs could exhibit dynamism in tensor-related scenarios. To confront these challenges, we introduce a pioneering framework in this paper that systematically formulates a novel model, theory, and algorithm for solving the dynamic graph regularized tensor completion problem.For the model, we establish a rigorous mathematical representation of the dynamic graph, based on which we derive a new tensor-oriented graph smoothness regularization. By integrating this regularization into a tensor decomposition model based on transformed t-SVD, we develop a comprehensive model simultaneously capturing the low-rank and similarity structure of the tensor.In terms of theory, we showcase the alignment between the proposed graph smoothness regularization and a weighted tensor nuclear norm. Subsequently, we establish assurances of statistical consistency for our model, effectively bridging a gap in the theoretical examination of the problem involving tensor recovery with graph information.In terms of the algorithm, we develop a solution of high effectiveness, accompanied by a guaranteed convergence, to address the resulting model. To showcase the prowess of our proposed model in contrast to established ones, we provide in-depth numerical experiments encompassing synthetic data as well as real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="Benign-Overfitting-and-Grokking-in-ReLU-Networks-for-XOR-Cluster-Data"><a href="#Benign-Overfitting-and-Grokking-in-ReLU-Networks-for-XOR-Cluster-Data" class="headerlink" title="Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data"></a>Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02541">http://arxiv.org/abs/2310.02541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiwei Xu, Yutong Wang, Spencer Frei, Gal Vardi, Wei Hu</li>
<li>for: 这个论文探讨了使用梯度下降（GD）训练神经网络时，神经网络表现出了一些奇异的泛化行为。</li>
<li>methods: 该论文使用了两层ReLU网络和梯度下降（GD）训练方法。</li>
<li>results: 研究发现，在XOR集群数据上，一部分训练标签被随机变化，并且使用GD训练神经网络时，神经网络可以在训练数据上达到100%的准确率，但在测试数据上却具有近乎随机的性能。在后续训练步骤中，神经网络的测试准确率逐渐提高，并且仍然可以fitRandom labels in the training data，这是一种“搁废”现象。这是神经网络分类时，当数据分布不可分离时，首次出现的恰当过拟合现象。<details>
<summary>Abstract</summary>
Neural networks trained by gradient descent (GD) have exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of classical, harmful overfitting -- achieving a perfect fit to training data with near-random performance on test data -- before transitioning ("grokking") to near-optimal generalization later in training. In this work, we show that both of these phenomena provably occur in two-layer ReLU networks trained by GD on XOR cluster data where a constant fraction of the training labels are flipped. In this setting, we show that after the first step of GD, the network achieves 100% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy. At a later training step, the network achieves near-optimal test accuracy while still fitting the random labels in the training data, exhibiting a "grokking" phenomenon. This provides the first theoretical result of benign overfitting in neural network classification when the data distribution is not linearly separable. Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.
</details>
<details>
<summary>摘要</summary>
神经网络通过梯度下降（GD）训练Display textGet display textHave exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of classical, harmful overfitting -- achieving a perfect fit to training data with near-random performance on test data -- before transitioning ("grokking") to near-optimal generalization later in training. In this work, we show that both of these phenomena provably occur in two-layer ReLU networks trained by GD on XOR cluster data where a constant fraction of the training labels are flipped. In this setting, we show that after the first step of GD, the network achieves 100% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy. At a later training step, the network achieves near-optimal test accuracy while still fitting the random labels in the training data, exhibiting a "grokking" phenomenon. This provides the first theoretical result of benign overfitting in neural network classification when the data distribution is not linearly separable. Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.
</details></li>
</ul>
<hr>
<h2 id="Quantifying-and-mitigating-the-impact-of-label-errors-on-model-disparity-metrics"><a href="#Quantifying-and-mitigating-the-impact-of-label-errors-on-model-disparity-metrics" class="headerlink" title="Quantifying and mitigating the impact of label errors on model disparity metrics"></a>Quantifying and mitigating the impact of label errors on model disparity metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02533">http://arxiv.org/abs/2310.02533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julius Adebayo, Melissa Hall, Bowen Yu, Bobbie Chern</li>
<li>for: 这个论文主要研究了标签错误对模型的不同性指标的影响。</li>
<li>methods: 作者使用了现有的方法来mitigate标签错误对模型的影响，并提出了一种新的方法来衡量标签错误对模型的影响。</li>
<li>results: 研究发现，标签错误会导致模型的不同性指标受到影响，特别是对少数群体的影响。此外，作者还提出了一种方法来衡量标签错误对模型的影响，并证明了这种方法可以提高模型的不同性指标。<details>
<summary>Abstract</summary>
Errors in labels obtained via human annotation adversely affect a model's performance. Existing approaches propose ways to mitigate the effect of label error on a model's downstream accuracy, yet little is known about its impact on a model's disparity metrics. Here we study the effect of label error on a model's disparity metrics. We empirically characterize how varying levels of label error, in both training and test data, affect these disparity metrics. We find that group calibration and other metrics are sensitive to train-time and test-time label error -- particularly for minority groups. This disparate effect persists even for models trained with noise-aware algorithms. To mitigate the impact of training-time label error, we present an approach to estimate the influence of a training input's label on a model's group disparity metric. We empirically assess the proposed approach on a variety of datasets and find significant improvement, compared to alternative approaches, in identifying training inputs that improve a model's disparity metric. We complement the approach with an automatic relabel-and-finetune scheme that produces updated models with, provably, improved group calibration error.
</details>
<details>
<summary>摘要</summary>
label错误会 adversely affect模型的性能。现有的方法提出了 mitigate label error的影响，但对于模型的差异度指标的影响则不很清楚。在这里，我们研究了 label error对模型的差异度指标的影响。我们经验性地characterize了不同水平的 label error在训练和测试数据中对这些差异度指标的影响。我们发现，对少数群体来说，group calibration和其他指标受到训练时和测试时 label error的影响，尤其是在使用静音感知算法时。这种差异性效应持续存在，即使使用静音感知算法。为了减少训练时 label error的影响，我们提出了一种Estimate the influence of a training input's label on a model's group disparity metric的方法。我们经验性地评估了该方法在多个数据集上，并发现它可以更好地标识训练输入，以提高模型的差异度指标。我们补充了该方法，提出了一种自动重新标签和调整的方法，可以生成更新后的模型，其差异度指标得到了改进。
</details></li>
</ul>
<hr>
<h2 id="QuATON-Quantization-Aware-Training-of-Optical-Neurons"><a href="#QuATON-Quantization-Aware-Training-of-Optical-Neurons" class="headerlink" title="QuATON: Quantization Aware Training of Optical Neurons"></a>QuATON: Quantization Aware Training of Optical Neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03049">http://arxiv.org/abs/2310.03049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hasindu Kariyawasam, Ramith Hettiarachchi, Dushan Wadduwage</li>
<li>for: 这个研究旨在实现智能测量的光学神经网络（ONA），但实现设计时的制约可以是问题。</li>
<li>methods: 我们提出了一个基于物理限制的量化测试框架，考虑了物理限制在训练过程中的影响，从而实现了可靠的设计。</li>
<li>results: 我们在文献中提出的Diffractive Deep Neural Network（D2NN）的实现中，运用了我们的方法，实现了对于光学阶层实像和阶层物体的分类。我们在不同的量化水平和数据集上进行了广泛的实验，展示了我们的方法能够实现ONA设计的稳定性。<details>
<summary>Abstract</summary>
Optical neural architectures (ONAs) use coding elements with optimized physical parameters to perform intelligent measurements. However, fabricating ONAs while maintaining design performances is challenging. Limitations in fabrication techniques often limit the realizable precision of the trained parameters. Physical constraints may also limit the range of values the physical parameters can hold. Thus, ONAs should be trained within the implementable constraints. However, such physics-based constraints reduce the training objective to a constrained optimization problem, making it harder to optimize with existing gradient-based methods. To alleviate these critical issues that degrade performance from simulation to realization we propose a physics-informed quantization-aware training framework. Our approach accounts for the physical constraints during the training process, leading to robust designs. We evaluate our approach on an ONA proposed in the literature, named a diffractive deep neural network (D2NN), for all-optical phase imaging and for classification of phase objects. With extensive experiments on different quantization levels and datasets, we show that our approach leads to ONA designs that are robust to quantization noise.
</details>
<details>
<summary>摘要</summary>
依据我们的提案，使用优化的物理参数的光学神经网络（ONA）可以实现智能测量。然而，实现ONA的设计时，制造技术的限制通常会限制训练后的精度。物理限制也可能限制物理参数的值范围。因此，ONA应该在实现可能的约束下进行训练。然而，这会将训练问题转化为受限制的优化问题，使用现有的梯度基本方法困难。为了解决这些问题，我们提出了物理知ledge-aware归一化训练框架。我们的方法会考虑物理约束 durante el proceso de entrenamiento, leading to designs that are robust to quantization noise.我们在Literature中提出的一种diffractive deep neural network（D2NN）中进行了广泛的实验，用于全光学相干成像和相对阶段的分类。我们通过不同的归一化级别和数据集进行了extensive experiments，并证明了我们的方法可以导致ONA的设计具有鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Parameterized-Convex-Minorant-for-Objective-Function-Approximation-in-Amortized-Optimization"><a href="#Parameterized-Convex-Minorant-for-Objective-Function-Approximation-in-Amortized-Optimization" class="headerlink" title="Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization"></a>Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02519">http://arxiv.org/abs/2310.02519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinraekim/pcmao">https://github.com/jinraekim/pcmao</a></li>
<li>paper_authors: Jinrae Kim, Youdan Kim</li>
<li>for: 这 paper 的目的是提出一种基于Parameterized convex minorant（PCM）方法的可编程优化方法，用于approximating objective function。</li>
<li>methods: 该方法使用PCM和非负差函数的总和来approximate objective function，其中PCM convex在优化变量下bounded from below。这个approximator是continuous functions的universal approximator，global minimizer of PCM可以通过单个convex optimization获得。</li>
<li>results: 在numerical simulation中，该方法可以快速和可靠地learn objective functions和global minimizer，并且可以用于non-parameterized-convex objective function approximation和learning-based nonlinear model predictive control。<details>
<summary>Abstract</summary>
Parameterized convex minorant (PCM) method is proposed for the approximation of the objective function in amortized optimization. In the proposed method, the objective function approximator is expressed by the sum of a PCM and a nonnegative gap function, where the objective function approximator is bounded from below by the PCM convex in the optimization variable. The proposed objective function approximator is a universal approximator for continuous functions, and the global minimizer of the PCM attains the global minimum of the objective function approximator. Therefore, the global minimizer of the objective function approximator can be obtained by a single convex optimization. As a realization of the proposed method, extended parameterized log-sum-exp network is proposed by utilizing a parameterized log-sum-exp network as the PCM. Numerical simulation is performed for non-parameterized-convex objective function approximation and for learning-based nonlinear model predictive control to demonstrate the performance and characteristics of the proposed method. The simulation results support that the proposed method can be used to learn objective functions and to find the global minimizer reliably and quickly by using convex optimization algorithms.
</details>
<details>
<summary>摘要</summary>
Parameterized convex minorant (PCM) 方法是用于优化目标函数的approximation的方法。在提议的方法中，目标函数估计器是表示为PCM和非负差函数的和，其中目标函数估计器在优化变量上受到PCM的下界。提议的目标函数估计器是绝对 continuous 函数的通用估计器， global minimizer 的PCM可以通过单个凸优化来获得。为实现该方法， extended 参数化 log-sum-exp 网络被提议，其中 parameterized log-sum-exp 网络作为PCM。通过数值实验， demonstrate 了该方法可以用凸优化算法来可靠地和快速地学习目标函数和找到全球最小值。Note: Simplified Chinese is used in this translation, which is a simplified version of Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Thermodynamics-of-Learning-Generative-Parametric-Probabilistic-Models"><a href="#Stochastic-Thermodynamics-of-Learning-Generative-Parametric-Probabilistic-Models" class="headerlink" title="Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models"></a>Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19802">http://arxiv.org/abs/2310.19802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shervin Sadat Parsi</li>
<li>for: 本研究用 parametric probabilistic models (PPMs) 来描述生成机器学习问题，并研究这些问题的热力学特性。</li>
<li>methods: 研究人员使用 Stochastic Gradient Descent (SGD) 优化器和训练集来控制 PPMs 的时间演化。</li>
<li>results: 研究发现，SGD 优化器在生成样本时释放热量，导致 PPMs 参数Subsystem 的热力学 entropy 增加，从而确定模型学习的概率分布。这种方法为权重过气化模型的泛化能力提供了热力学意义的视角。<details>
<summary>Abstract</summary>
We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous framework for computing information-theoretic quantities within deterministic neural networks by establishing connections with thermodynamic variables. To illustrate the utility of this framework, we introduce two information-theoretic metrics: Memorized-information (M-info) and Learned-information (L-info), which trace the dynamic flow of information during the learning process of PPMs.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们已经将生成机器学习问题表述为 Parametric Probabilistic Models（PPMs）的时间演化过程，自然地涉及到热力学过程。然后，我们研究了模型参数 $\Theta$ 和生成样本 $X$ 之间的热力学交换。我们发现，训练集和 Stochastic Gradient Descent（SGD）优化器的作用共同控制这两个子系统的时间演化。我们的发现表明，模型通过生成样本 $X$ 中的热膨胀学习，导致模型参数 $\Theta$ 的熵增加。因此，参数子系统 behave as a heat reservoir，有效地存储学习的信息。此外，模型参数作为热贮储的角色提供了有价值的热力学意见，有助于理解过参数模型的泛化能力。这种方法提供了一个不ambiguous的框架，用于计算 deterministic neural networks 中的信息量量。为了证明该框架的实用性，我们引入了两个信息量度量：Memorized-information（M-info）和 Learned-information（L-info），这两个度量跟踪学习过程中信息的动态流动。
</details></li>
</ul>
<hr>
<h2 id="A-Recipe-for-Improved-Certifiable-Robustness-Capacity-and-Data"><a href="#A-Recipe-for-Improved-Certifiable-Robustness-Capacity-and-Data" class="headerlink" title="A Recipe for Improved Certifiable Robustness: Capacity and Data"></a>A Recipe for Improved Certifiable Robustness: Capacity and Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02513">http://arxiv.org/abs/2310.02513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hukkai/liresnet">https://github.com/hukkai/liresnet</a></li>
<li>paper_authors: Kai Hu, Klas Leino, Zifan Wang, Matt Fredrikson</li>
<li>for: 本研究旨在提高鲁棒训练的性能，特别是在 Lipschitz 约束下。</li>
<li>methods: 本文使用了一系列新技术和设计优化，包括 Cholesky-orthogonalized residual dense 层和 filtered generative data augmentation。</li>
<li>results: 本研究在多种数据集和扰动大小上显著提高了鲁棒训练的性能，并达到了最新的鲁棒精度标准（VRA）。 Specifically, the addition of large Cholesky-orthogonalized residual dense layers and filtered generative data augmentation improved the state-of-the-art verified robust accuracy by up to 8.5 percentage points.<details>
<summary>Abstract</summary>
A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards \emph{underfitting} than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art \emph{verified robust accuracy} (VRA) for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large "Cholesky-orthogonalized residual dense" layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points. Code is available at \url{https://github.com/hukkai/liresnet}.
</details>
<details>
<summary>摘要</summary>
“一大挑战，理论上和实践上都支持，是耐性需要更大的网络容量和更多的数据，但是在严格的Lipschitz限制下添加容量并不那么容易。事实上，现今的state-of-the-art方法更倾向于\"下养\"而不是\"过养\"。此外，我们认为现有的Lipschitz基本方法的设计空间探索并未充分，导致可能的性能提升被忽略了。在这个研究中，我们提供了更完整的评估，以更好地探索Lipschitz基本方法的潜在性能。使用了一些新的技术、设计优化和对先前工作的 sintesis，我们能够在多种 benchmark 数据集上提高 state-of-the-art 的\"证实确的精度\"（VRA），并在不同的扰动大小下实现 significiant 的性能提升。尤其是，我们发现在现有的Lipschitz控制 ResNet 架构的尾端添加大量\"Cholesky-orthogonalized residual dense\"层可以增加网络容量和性能。在这些层的帮助下，我们使用了筛选的生成数据增强技术，最终的结果还是进一步推进了 state-of-the-art 的 VRA，高于8.5%。代码可以在 \url{https://github.com/hukkai/liresnet} 上找到。”
</details></li>
</ul>
<hr>
<h2 id="Ophiuchus-Scalable-Modeling-of-Protein-Structures-through-Hierarchical-Coarse-graining-SO-3-Equivariant-Autoencoders"><a href="#Ophiuchus-Scalable-Modeling-of-Protein-Structures-through-Hierarchical-Coarse-graining-SO-3-Equivariant-Autoencoders" class="headerlink" title="Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders"></a>Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02508">http://arxiv.org/abs/2310.02508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allan dos Santos Costa, Ilan Mitnikov, Mario Geiger, Manvitha Ponnapati, Tess Smidt, Joseph Jacobson</li>
<li>for: 这篇论文的目的是提出一种可扩展的蛋白质模型，以便更好地模拟和生成蛋白质结构。</li>
<li>methods: 该论文使用了一种名为Ophiuchus的SO(3)-等价变换模型，通过对蛋白质重原子进行本地卷积减分来模型序列-模式交互作用，并通过适应压缩率来实现高级别的结构嵌入。</li>
<li>results: 该论文通过对PDB蛋白质残余进行训练，实现了对不同压缩率的结构重建，并通过对折扣扩展的Latent space进行验证，证明了Ophiuchus的可扩展性和可靠性。<details>
<summary>Abstract</summary>
Three-dimensional native states of natural proteins display recurring and hierarchical patterns. Yet, traditional graph-based modeling of protein structures is often limited to operate within a single fine-grained resolution, and lacks hourglass neural architectures to learn those high-level building blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant coarse-graining model that efficiently operates on all heavy atoms of standard protein residues, while respecting their relevant symmetries. Our model departs from current approaches that employ graph modeling, instead focusing on local convolutional coarsening to model sequence-motif interactions in log-linear length complexity. We train Ophiuchus on contiguous fragments of PDB monomers, investigating its reconstruction capabilities across different compression rates. We examine the learned latent space and demonstrate its prompt usage in conformational interpolation, comparing interpolated trajectories to structure snapshots from the PDBFlex dataset. Finally, we leverage denoising diffusion probabilistic models (DDPM) to efficiently sample readily-decodable latent embeddings of diverse miniproteins. Our experiments demonstrate Ophiuchus to be a scalable basis for efficient protein modeling and generation.
</details>
<details>
<summary>摘要</summary>
三维原生态蛋白质的Native状态显示出 recursively和层次结构的模式。然而，传统的图形基本模型 oft limitations 在 single fine-grained 分辨率操作，缺乏 hourglass 神经网络架构来学习高级结构块。我们减少这一差距 by introducing Ophiuchus, an SO(3)-equivariant coarse-graining model that efficiently operates on all heavy atoms of standard protein residues, while respecting their relevant symmetries。我们的模型 departure from current approaches that employ graph modeling, instead focusing on local convolutional coarsening to model sequence-motif interactions in log-linear length complexity。我们在 contiguous fragments of PDB monomers 上训练 Ophiuchus，investigating its reconstruction capabilities across different compression rates。我们 examine the learned latent space and demonstrate its prompt usage in conformational interpolation, comparing interpolated trajectories to structure snapshots from the PDBFlex dataset。最后，我们利用 denoising diffusion probabilistic models (DDPM) to efficiently sample readily-decodable latent embeddings of diverse miniproteins。我们的实验 Demonstrate Ophiuchus to be a scalable basis for efficient protein modeling and generation。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Reach-Goals-via-Diffusion"><a href="#Learning-to-Reach-Goals-via-Diffusion" class="headerlink" title="Learning to Reach Goals via Diffusion"></a>Learning to Reach Goals via Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02505">http://arxiv.org/abs/2310.02505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vineet Jain, Siamak Ravanbakhsh</li>
<li>for: 这个论文是用于解决目标定位问题的。</li>
<li>methods: 这个论文使用了扩散模型，并在这个模型中学习了一个目标条件政策。</li>
<li>results: 这个论文的实验结果与现有方法竞争力相当，这表明这种 diffusion 的视角在sequential decision-making 中是一种简单、可扩展、有效的方向。<details>
<summary>Abstract</summary>
Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.
</details>
<details>
<summary>摘要</summary>
Diffusion模型是一种强大的生成模型，可以将高维空间中的随机噪声映射到目标拟合点 через iterative denoising。在这篇文章中，我们提出了一种新的视角，将目标conditioned reinforcement learning嵌入到 diffusion 模型的 Context中。与 diffusion 过程类似，我们构建了从数据拟合点开始的轨迹，然后学习一个 conditioned 政策，类似于 score 函数。我们称之为 Merlin。这种方法可以从任意初始状态达到预定或新的目标，无需学习分离的值函数。我们考虑了三种噪声模型来取代 Gaussian 噪声在 diffusion 中，包括 reverse play from the buffer、reverse dynamics model 和一种新的非 Parametric 方法。我们 teorically 正确了我们的方法，并在 offline goal-reaching 任务中 validate 它。实验结果与当前最佳方法竞争，这 Suggests 这种 diffusion 视角是一种简单、可扩展和有效的方向 для继系列决策。
</details></li>
</ul>
<hr>
<h2 id="Towards-an-Interpretable-Representation-of-Speaker-Identity-via-Perceptual-Voice-Qualities"><a href="#Towards-an-Interpretable-Representation-of-Speaker-Identity-via-Perceptual-Voice-Qualities" class="headerlink" title="Towards an Interpretable Representation of Speaker Identity via Perceptual Voice Qualities"></a>Towards an Interpretable Representation of Speaker Identity via Perceptual Voice Qualities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02497">http://arxiv.org/abs/2310.02497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Netzorg, Bohan Yu, Andrea Guzman, Peter Wu, Luna McNulty, Gopala Anumanchipalli</li>
<li>for: 这个论文是为了提出一种可解释的 speaker identity 表示方法，基于语音质量特征 (PQ)。</li>
<li>methods: 这个方法使用了加入性别化 PQ 的 CAPE-V 协议，从而提供了一个抽象水平的语音特征空间，可以作为高级人类特征和低级语音、物理或学习表示之间的中间层。</li>
<li>results: 研究发现，这种 PQ-based 方法可以被多个非专业人群听到，并且表明这种信息在不同的语音表示中是预测可能的。<details>
<summary>Abstract</summary>
Unlike other data modalities such as text and vision, speech does not lend itself to easy interpretation. While lay people can understand how to describe an image or sentence via perception, non-expert descriptions of speech often end at high-level demographic information, such as gender or age. In this paper, we propose a possible interpretable representation of speaker identity based on perceptual voice qualities (PQs). By adding gendered PQs to the pathology-focused Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) protocol, our PQ-based approach provides a perceptual latent space of the character of adult voices that is an intermediary of abstraction between high-level demographics and low-level acoustic, physical, or learned representations. Contrary to prior belief, we demonstrate that these PQs are hearable by ensembles of non-experts, and further demonstrate that the information encoded in a PQ-based representation is predictable by various speech representations.
</details>
<details>
<summary>摘要</summary>
不同于其他数据模式，如文本和视觉，语音不太容易被解释。而非专业人士通常只能描述语音的高级人口信息，如性别或年龄。在这篇论文中，我们提议一种可解释的发音人身份表示方法，基于感受性声质（PQ）。通过将性别化PQ添加到已有的医学疾病预测协议中（CAPE-V），我们的PQ基本表示方法可以提供一个在抽象水平上的中间 Representation of adult voices的性质，与高级人口信息和低级声学、物理或学习表示之间的中间 Representation。与以往的信念相反，我们示出了这些PQ可以被非专业人士听到，并且我们还示出了PQ基本表示中编码的信息可以通过不同的语音表示来预测。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/04/cs.LG_2023_10_04/" data-id="closbrorq00qr0g8866pmaxc3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/04/eess.IV_2023_10_04/" class="article-date">
  <time datetime="2023-10-04T09:00:00.000Z" itemprop="datePublished">2023-10-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/04/eess.IV_2023_10_04/">eess.IV - 2023-10-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hybrid-Quantum-Machine-Learning-Assisted-Classification-of-COVID-19-from-Computed-Tomography-Scans"><a href="#Hybrid-Quantum-Machine-Learning-Assisted-Classification-of-COVID-19-from-Computed-Tomography-Scans" class="headerlink" title="Hybrid Quantum Machine Learning Assisted Classification of COVID-19 from Computed Tomography Scans"></a>Hybrid Quantum Machine Learning Assisted Classification of COVID-19 from Computed Tomography Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02748">http://arxiv.org/abs/2310.02748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leo Sünkel, Darya Martyniuk, Julia J. Reichwald, Andrei Morariu, Raja Havish Seggoju, Philipp Altmann, Christoph Roch, Adrian Paschke</li>
<li>for: 本研究是为了应用混合量子机器学习方法来解决实际应用中的一个呼吸道病变识别问题，具体来说是用hybrid量子传输学习来分类大量的CT扫描图像。</li>
<li>methods: 本研究使用了量子图像嵌入和混合量子机器学习方法，并评估了多种量子征 circuits和嵌入技术。</li>
<li>results: 研究结果显示，混合量子机器学习方法可以有效地处理大量的CT扫描图像，并且可以准确地分类COVID-19、CAP和正常三类。<details>
<summary>Abstract</summary>
Practical quantum computing (QC) is still in its infancy and problems considered are usually fairly small, especially in quantum machine learning when compared to its classical counterpart. Image processing applications in particular require models that are able to handle a large amount of features, and while classical approaches can easily tackle this, it is a major challenge and a cause for harsh restrictions in contemporary QC. In this paper, we apply a hybrid quantum machine learning approach to a practically relevant problem with real world-data. That is, we apply hybrid quantum transfer learning to an image processing task in the field of medical image processing. More specifically, we classify large CT-scans of the lung into COVID-19, CAP, or Normal. We discuss quantum image embedding as well as hybrid quantum machine learning and evaluate several approaches to quantum transfer learning with various quantum circuits and embedding techniques.
</details>
<details>
<summary>摘要</summary>
现代量子计算（QC）仍处于初期阶段，问题通常较小，尤其在量子机器学习领域，与经典机器学习相比。图像处理应用需要处理大量特征，而经典方法可以轻松实现，但在当代QC中是一个主要挑战，导致严格的限制。本文使用混合量子机器学习方法解决实际 relevance 的医学图像处理问题。具体来说，我们使用混合量子传输学习将大量 CT-扫描图像分类为COVID-19、CAP或正常。我们讨论量子图像嵌入以及混合量子机器学习，评估了各种量子循环和嵌入技术。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/04/eess.IV_2023_10_04/" data-id="closbroyp017v0g88b43w7bg1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/04/eess.SP_2023_10_04/" class="article-date">
  <time datetime="2023-10-04T08:00:00.000Z" itemprop="datePublished">2023-10-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/04/eess.SP_2023_10_04/">eess.SP - 2023-10-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Index-Modulated-Metasurface-Transceiver-Design-using-Reconfigurable-Intelligent-Surfaces-for-6G-Wireless-Networks"><a href="#Index-Modulated-Metasurface-Transceiver-Design-using-Reconfigurable-Intelligent-Surfaces-for-6G-Wireless-Networks" class="headerlink" title="Index-Modulated Metasurface Transceiver Design using Reconfigurable Intelligent Surfaces for 6G Wireless Networks"></a>Index-Modulated Metasurface Transceiver Design using Reconfigurable Intelligent Surfaces for 6G Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03208">http://arxiv.org/abs/2310.03208</a></li>
<li>repo_url: None</li>
<li>paper_authors: JohnA. Hodge, Kumar Vijay Mishra, Brian M. Sadler, Amir I. Zaghloul</li>
<li>for: 本研究旨在提高 sixth-generation 无线网络的数据速率和能效性，并提出了一种新的扩展技术——指标修改（Index Modulation，IM）。</li>
<li>methods: 本研究使用了智能表面（Reconfigurable Intelligent Surface，RIS）的电磁特性来实现 IM，包括指向调制、空间多普通模式和相位调制能力。</li>
<li>results: 数值实验表明，通过 RIS 协助 IM 实现可以获得较佳的比错率表现，而且可以通过调整反射相位来实现 IM 的程序化。<details>
<summary>Abstract</summary>
Higher spectral and energy efficiencies are the envisioned defining characteristics of high data-rate sixth-generation (6G) wireless networks. One of the enabling technologies to meet these requirements is index modulation (IM), which transmits information through permutations of indices of spatial, frequency, or temporal media. In this paper, we propose novel electromagnetics-compliant designs of reconfigurable intelligent surface (RIS) apertures for realizing IM in 6G transceivers. We consider RIS modeling and implementation of spatial and subcarrier IMs, including beam steering, spatial multiplexing, and phase modulation capabilities. Numerical experiments for our proposed implementations show that the bit error rates obtained via RIS-aided IM outperform traditional implementations. We further establish the programmability of these transceivers to vary the reflection phase and generate frequency harmonics for IM through full-wave electromagnetic analyses of a specific reflect-array metasurface implementation.
</details>
<details>
<summary>摘要</summary>
高 spectral 和能量效率是 sixth-generation (6G) 无线网络的定义特征。一种实现这些要求的技术是指标模ulation (IM), 它通过媒体的 indeces  permutation 传输信息。在这篇论文中，我们提议了新的 electromagnetics-compliant 设计，用于实现 IM 在 6G 接收机中。我们考虑了 RIS 模型和实现，包括杆导向、空载多普通量和相位修正能力。我们的numerical experiment 表明，通过 RIS 帮助 IM，可以获得较低的错误率。我们还证明了这些接收机的可编程性，可以通过调整反射相位和生成频率幂来实现 IM。Here's a word-for-word translation of the text using Traditional Chinese characters:高 spectral 和能量效率是 sixth-generation (6G) 无线网络的定义特征。一种实现这些要求的技术是指标模ulation (IM), 它通过媒体的 indeces  permutation 传送信息。在这篇论文中，我们提议了新的 electromagnetics-compliant 设计，用于实现 IM 在 6G 接收机中。我们考虑了 RIS 模型和实现，包括杆导向、空载多普通量和相位修正能力。我们的numerical experiment 表明，通过 RIS 帮助 IM，可以获得较低的错误率。我们也证明了这些接收机的可编程性，可以通过调整反射相位和生成频率幂来实现 IM。
</details></li>
</ul>
<hr>
<h2 id="Impedance-Leakage-Vulnerability-and-its-Utilization-in-Reverse-engineering-Embedded-Software"><a href="#Impedance-Leakage-Vulnerability-and-its-Utilization-in-Reverse-engineering-Embedded-Software" class="headerlink" title="Impedance Leakage Vulnerability and its Utilization in Reverse-engineering Embedded Software"></a>Impedance Leakage Vulnerability and its Utilization in Reverse-engineering Embedded Software</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03175">http://arxiv.org/abs/2310.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sadik Awal, Md Tauhidur Rahman</li>
<li>for: 保护系统和数据免受物理攻击，发现新的漏洞和实施安全和隐私措施是非常重要。</li>
<li>methods: 本文探讨了一种受到忽略或者狭隘研究的漏洞——阻抗性，它可以通过非典型的侧途泄露信息，对系统和数据安全和隐私带来潜在的威胁。</li>
<li>results: 本文表明了ATmega328P微控制器和Artix 7 FPGA的阻抗性不是固定的，而是与运行在这些设备上的软件有直接关系。我们称之为阻抗性泄露，并使其成为一种侧途来检测保护内存中的软件指令。我们的实验表明，阻抗性侧途可以准确地检测软件指令，具体数据如下：ATmega328P微控制器的准确率为96.1%，Artix 7 FPGA的准确率为92.6%。此外，我们还探讨了阻抗性侧途的双重性，包括可能的利用和知识产权盗用的风险。最后，我们提出了特定地址 impedance leakage的防范措施。<details>
<summary>Abstract</summary>
Discovering new vulnerabilities and implementing security and privacy measures are important to protect systems and data against physical attacks. One such vulnerability is impedance, an inherent property of a device that can be exploited to leak information through an unintended side channel, thereby posing significant security and privacy risks. Unlike traditional vulnerabilities, impedance is often overlooked or narrowly explored, as it is typically treated as a fixed value at a specific frequency in research and design endeavors. Moreover, impedance has never been explored as a source of information leakage. This paper demonstrates that the impedance of an embedded device is not constant and directly relates to the programs executed on the device. We define this phenomenon as impedance leakage and use this as a side channel to extract software instructions from protected memory. Our experiment on the ATmega328P microcontroller and the Artix 7 FPGA indicates that the impedance side channel can detect software instructions with 96.1% and 92.6% accuracy, respectively. Furthermore, we explore the dual nature of the impedance side channel, highlighting the potential for beneficial purposes and the associated risk of intellectual property theft. Finally, potential countermeasures that specifically address impedance leakage are discussed.
</details>
<details>
<summary>摘要</summary>
发现新的漏洞和实施安全和隐私措施是保护系统和数据 против物理攻击的重要方法。一种这样的漏洞是阻力，它是设备的自然属性，可以通过不当的副通道泄露信息，从而带来严重的安全和隐私风险。与传统的漏洞不同，阻力通常被忽略或者狭采，因为它通常被视为一个固定的值，在研究和设计中。此外，阻力从未被探讨作为信息泄露的来源。这篇论文表明，阻力不是固定的，而是与在设备上执行的程序直接相关。我们称这种现象为阻力泄露，并使其作为副通道来提取保护内存中的软件指令。我们的实验表明，阻力副通道可以准确地检测软件指令，具体来说，在ATmega328P微控制器和Artix 7 FPGA上，阻力副通道可以检测软件指令的准确率分别为96.1%和92.6%。此外，我们还探讨了阻力副通道的双重性，并指出了这种副通道的潜在利用和知识产权盗用的风险。最后，我们提出了专门针对阻力泄露的防范措施。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Changes-of-Brain-Network-during-Epileptic-Seizure"><a href="#Dynamic-Changes-of-Brain-Network-during-Epileptic-Seizure" class="headerlink" title="Dynamic Changes of Brain Network during Epileptic Seizure"></a>Dynamic Changes of Brain Network during Epileptic Seizure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03151">http://arxiv.org/abs/2310.03151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atefeh Khoshkhahtinat, Hoda Mohammadzade</li>
<li>For: 这个研究旨在探索 epilepsy 患者的脑网络在癫痫期间的动态变化，以便提高诊断和治疗的效果。* Methods: 这个研究使用了频率带的phas lag index (PLI) 测量，并使用图论学技术提取脑网络的 topological 特征。 然后，一种无supervised clustering方法用于检查脑网络在癫痫期间的状态转移。* Results: 研究发现，癫痫期间脑的同步水平高于预癫痫和后癫痫期间的theta、alpha 和 beta 频率带，而gamma 频率带的同步水平下降。这些变化还导致脑网络的topological 特征在癫痫期间发生变化。此外，研究发现脑网络在癫痫期间的动态变化比传统的三个状态模型（预癫痫、癫痫、后癫痫）复杂得多，脑网络在癫痫期间的变化缓慢于预癫痫和后癫痫期间。<details>
<summary>Abstract</summary>
Epilepsy is a neurological disorder identified by sudden and recurrent seizures, which are believed to be accompanied by distinct changes in brain dynamics. Exploring the dynamic changes of brain network states during seizures can pave the way for improving the diagnosis and treatment of patients with epilepsy. In this paper, the connectivity brain network is constructed using the phase lag index (PLI) measurement within five frequency bands, and graph-theoretic techniques are employed to extract topological features from the brain network. Subsequently, an unsupervised clustering approach is used to examine the state transitions of the brain network during seizures. Our findings demonstrate that the level of brain synchrony during the seizure period is higher than the pre-seizure and post-seizure periods in the theta, alpha, and beta bands, while it decreases in the gamma bands. These changes in synchronization also lead to alterations in the topological features of functional brain networks during seizures. Additionally, our results suggest that the dynamics of the brain during seizures are more complex than the traditional three-state model (pre-seizure, seizure, and post-seizure) and the brain network state exhibits a slower rate of change during the seizure period compared to the pre-seizure and post-seizure periods.
</details>
<details>
<summary>摘要</summary>
EPilepsy 是一种神经疾病，标志于不断发生的发作，这些发作通常附有脑动力学的变化。研究发作期间脑网络的动态变化可能为患EPilepsy 患者的诊断和治疗带来新的想法。在这篇论文中，我们使用相位延迟指数（PLI）测量在五个频率带中的脑网络连接度，并使用图论技术提取脑网络的 topological 特征。然后，我们使用无supervised  clustering 方法检查脑网络在发作期间的状态转移。我们发现在发作期间，脑的同步水平高于预发作和后发作期间的theta、alpha和beta频率带，而在gamma频率带下降。这些变化也导致了脑网络的 topological 特征的变化。此外，我们的结果表明，脑在发作期间的动态不仅比传统的三个状态模型（预发作、发作、后发作）复杂，而且发作期间脑网络的变化速率也比预发作和后发作期间 slower。
</details></li>
</ul>
<hr>
<h2 id="Dual-mode-multispectral-imaging-system-for-food-and-agricultural-product-quality-estimation"><a href="#Dual-mode-multispectral-imaging-system-for-food-and-agricultural-product-quality-estimation" class="headerlink" title="Dual mode multispectral imaging system for food and agricultural product quality estimation"></a>Dual mode multispectral imaging system for food and agricultural product quality estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03110">http://arxiv.org/abs/2310.03110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Darsha Udayanga, Ashan Serasinghe, Supun Dassanayake, Roshan Godaliyadda, H. M. V. R. Herath, M. P. B. Ekanayake, H. L. P. Malshan</li>
<li>for: 这个论文的目的是提出一种基于多spectral成像技术和人工智能机器学习技术的食品质量控制方法，以代替传统的实验室测试方法。</li>
<li>methods: 这个论文使用了反射和传输成像技术，并应用了人工智能机器学习和处理技术来分析数据。</li>
<li>results: 论文的实验结果显示，这种方法可以准确地识别不同类型的食品，包括固体和液体样本。具体来说，在标准色彩标准样本和巧克力油涂抹样本中，该方法可以达到90%以上的准确率，而在巧克力油涂抹样本中，该方法可以达到95%以上的准确率。同时，该方法还可以提供最高的准确率（99%） для溶解液体样本。<details>
<summary>Abstract</summary>
Multispectral imaging coupled with Artificial Intelligence, Machine Learning and Signal Processing techniques work as a feasible alternative for laboratory testing, especially in food quality control. Most of the recent related research has been focused on reflectance multispectral imaging but a system with both reflectance, transmittance capabilities would be ideal for a wide array of specimen types including solid and liquid samples. In this paper, a device which includes a dedicated reflectance mode and a dedicated transmittance mode is proposed. Dual mode operation where fast switching between two modes is facilitated. An innovative merged mode is introduced in which both reflectance and transmittance information of a specimen are combined to form a higher dimensional dataset with more features. Spatial and temporal variations of measurements are analyzed to ensure the quality of measurements. The concept is validated using a standard color palette and specific case studies are done for standard food samples such as turmeric powder and coconut oil proving the validity of proposed contributions. The classification accuracy of standard color palette testing was over 90% and the accuracy of coconut oil adulteration was over 95%. while the merged mode was able to provide the best accuracy of 99% for the turmeric adulteration. A linear functional mapping was done for coconut oil adulteration with an R2 value of 0.9558.
</details>
<details>
<summary>摘要</summary>
多spectral成像技术与人工智能、机器学习和信号处理技术结合，成为实验室测试的可行 alternativa，特别是食品质量控制。大多数最近相关的研究都集中在反射多spectral成像上，但一个包含反射和传输功能的系统会对各种样本类型进行更广泛的应用，包括固体和液体样本。本文提出了一种设备，它包括专门的反射模式和传输模式，并且具有快速切换 между两个模式的能力。我们还提出了一种创新的合并模式，在该模式下，反射和传输样本的信息被组合成一个高维数据集，其中包含更多的特征。我们进行了空间和时间变化的分析，以确保测量的质量。我们验证了提出的贡献，使用标准颜色谱和具体的食品样本测试，如胡萝卜粉和椰子油，结果显示了我们的提出的贡献的有效性。标准颜色谱测试的准确率高于90%，椰子油饱和质量的准确率高于95%，而合并模式能够提供最高的准确率99%。我们还进行了线性函数映射，其R2值为0.9558。
</details></li>
</ul>
<hr>
<h2 id="SNR-Adaptive-Ranging-Waveform-Design-Based-on-Ziv-Zakai-Bound-Optimization"><a href="#SNR-Adaptive-Ranging-Waveform-Design-Based-on-Ziv-Zakai-Bound-Optimization" class="headerlink" title="SNR-Adaptive Ranging Waveform Design Based on Ziv-Zakai Bound Optimization"></a>SNR-Adaptive Ranging Waveform Design Based on Ziv-Zakai Bound Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02963">http://arxiv.org/abs/2310.02963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifeng Xiong, Fan Liu</li>
<li>for: 本研究旨在提高无线应用中的定位精度，通过高精度测距来实现高精度定位。</li>
<li>methods: 本文提出了基于Ziv-Zakai bound（ZZB）的测距波形设计算法，该算法具有在给定SNR下实现最佳ZZB的理论保证。</li>
<li>results: numericalresults表明，在低SNR режимом，探测信号的检测概率成为测距信号的主要确定因素，而不是解析精度。<details>
<summary>Abstract</summary>
Location-awareness is essential in various wireless applications. The capability of performing precise ranging is substantial in achieving high-accuracy localization. Due to the notorious ambiguity phenomenon, optimal ranging waveforms should be adaptive to the signal-to-noise ratio (SNR). In this letter, we propose to use the Ziv-Zakai bound (ZZB) as the ranging performance metric, as well as an associated waveform design algorithm having theoretical guarantee of achieving the optimal ZZB at a given SNR. Numerical results suggest that, in stark contrast to the well-known high-SNR design philosophy, the detection probability of the ranging signal becomes more important than the resolution in the low-SNR regime.
</details>
<details>
<summary>摘要</summary>
Location-awareness 是各种无线应用中的关键因素。精准范围测量的能力是实现高精度定位的关键。由于著名的歧义现象，最佳的范围波形应对 Signal-to-Noise Ratio（SNR）进行适应。在这封信中，我们提议使用 Ziv-Zakai bound（ZZB）作为范围性能指标，以及与之相应的波形设计算法，具有理论保证可以在给定的 SNR 下实现最佳 ZZB。数字结果表明，在低 SNR  Régime 中，探测范围信号的检测概率变得更加重要于分辨率。
</details></li>
</ul>
<hr>
<h2 id="Dark-Side-of-HAPS-Systems-Jamming-Threats-towards-Satellites"><a href="#Dark-Side-of-HAPS-Systems-Jamming-Threats-towards-Satellites" class="headerlink" title="Dark Side of HAPS Systems: Jamming Threats towards Satellites"></a>Dark Side of HAPS Systems: Jamming Threats towards Satellites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02851">http://arxiv.org/abs/2310.02851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadil Otay, Khaled Humadi, Gunes Karabulut Kurt</li>
<li>For: 本研究旨在为6G时代的卫星通信网络提供安全保障，特别是在低地球轨道卫星网络中提高卫星与地面站之间的通信可靠性。* Methods: 本研究采用了两种LEO卫星通信场景，分别是一个发送卫星、一个接收地面站和一个高空平台站（HAPS）的干扰攻击场景，以及两个卫星、一个发送卫星和一个接收地面站的干扰攻击场景。* Results: 我们通过发展数学模型，研究干扰信号对系统的影响，并发现在第二个场景中，卫星协作可以提高系统的安全性，因为干扰效果只会在两个链路同时受到攻击时发生极端情况。<details>
<summary>Abstract</summary>
Securing satellite communication networks is imperative in the rapidly evolving landscape of advanced telecommunications, particularly in the context of 6G advancements. This paper establishes a secure low earth orbit (LEO) satellite network paradigm to address the challenges of the evolving 6G era, with a focus on enhancing communication integrity between satellites and ground stations. Countering the threat of jamming, which can disrupt vital communication channels, is a key goal of this work. In particular, this paper investigates the performance of two LEO satellite communication scenarios under the presence of jamming attacker. In the first scenario, we consider a system that comprises one transmitting satellite, a receiving ground station, and a high altitude platform station (HAPS) acting as a jammer. The HAPS disrupts communication between the satellite and the ground station, impeding signal transmission. The second scenario involves two satellites, one is the transmitter while the other works as a relay, accompanied by a ground station, and a jamming HAPS. In this scenario, the transmitting satellite sends data to the ground station using two different paths, i.e., direct and indirect transmission paths, with a relay satellite acting as an intermediary in the case of indirect transmission. For both scenarios, we study the system security by developing mathematical frameworks to investigate the outage effect resulting from the jamming signals orchestrated by the HAPS. Our results show that the satellite cooperation in the second scenario improves the system's security since the extreme jamming effect occurs only when both links are simultaneously disturbed.
</details>
<details>
<summary>摘要</summary>
保护卫星通信网络是在高速发展的电信技术领域中非常重要的，特别是在6G技术的发展背景下。本文提出了一种安全的低地球轨（LEO）卫星网络模式，以解决6G时代的演进中的通信完整性问题。对于干扰攻击的威胁来说，这种工作是非常重要的。本文研究了两种LEO卫星通信场景下，干扰攻击的影响。在第一个场景中，我们考虑了一个包括一个发送卫星、一个接收地面站和一个高空平台站（HAPS）的系统。HAPS会中断卫星和地面站之间的通信，从而阻挡信号传输。在第二个场景中，我们有两个卫星，其中一个是发送器，另一个是 relay，以及一个地面站，以及一个干扰HAPS。在这个场景中，发送卫星将数据传输到地面站使用两种不同的路径，即直接传输和间接传输， relay卫星在 indirect transmission 的情况下 acts as an intermediary。为了两个场景，我们研究了系统的安全性，并开发了数学模型来研究干扰信号所引起的系统停机的影响。我们的结果表明，在第二个场景中，卫星合作可以提高系统的安全性，因为干扰效果只会在两个链路同时受到影响时发生极端情况。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Simultaneous-Localization-and-Bias-Tracking"><a href="#Graph-based-Simultaneous-Localization-and-Bias-Tracking" class="headerlink" title="Graph-based Simultaneous Localization and Bias Tracking"></a>Graph-based Simultaneous Localization and Bias Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02814">http://arxiv.org/abs/2310.02814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Venus, Erik Leitinger, Stefan Tertinek, Florian Meyer, Klaus Witrisal</li>
<li>for: 本研究旨在提供一种robust的移动设备位置测定和跟踪方法，可以在多Path环境中提供高精度的位置估计。</li>
<li>methods: 本研究使用因子图形式和粒子基于加法产品算法，并且提出了一种序列算法，可以同时估计移动设备的位置和时变多Path组件（MPCs）的延迟偏好。</li>
<li>results: 本研究表明，使用 simulate和实际测量数据，提出的算法可以在多Path环境中提供高精度的位置估计，并且常常达到 posterior Cramer-Rao下限（P-CRLB）。此外，研究还发现该方法可以自动标识不可靠测量，从而避免lost track。<details>
<summary>Abstract</summary>
We present a factor graph formulation and particle-based sum-product algorithm for robust localization and tracking in multipath-prone environments. The proposed sequential algorithm jointly estimates the mobile agent's position together with a time-varying number of multipath components (MPCs). The MPCs are represented by "delay biases" corresponding to the offset between line-of-sight (LOS) component delay and the respective delays of all detectable MPCs. The delay biases of the MPCs capture the geometric features of the propagation environment with respect to the mobile agent. Therefore, they can provide position-related information contained in the MPCs without explicitly building a map of the environment. We demonstrate that the position-related information enables the algorithm to provide high-accuracy position estimates even in fully obstructed line-of-sight (OLOS) situations. Using simulated and real measurements in different scenarios we demonstrate the proposed algorithm to significantly outperform state-of-the-art multipath-aided tracking algorithms and show that the performance of our algorithm constantly attains the posterior Cramer-Rao lower bound (P-CRLB). Furthermore, we demonstrate the implicit capability of the proposed method to identify unreliable measurements and, thus, to mitigate lost tracks.
</details>
<details>
<summary>摘要</summary>
我们提出了一种因子图表示法和粒子基于汇聚积分算法，用于robust的位置Localization和跟踪在多path环境中。我们的顺序算法同时估算移动代理的位置和时变多path组件（MPCs）的数量。MPCs被表示为“延迟偏好”，这些偏好对应于从视线组件延迟到所有检测到的MPCs的延迟。延迟偏好 capture了在移动代理周围的媒体传播环境的几何特征，因此它们可以提供位置相关的信息，无需显式建立环境地图。我们示出了我们的算法可以在完全遮挡视线（OLOS）情况下提供高精度的位置估计。使用模拟和实际测量数据，我们示出了我们的算法在不同enario中显著超越了当前的多path护航算法，并且示出了我们的算法的性能一直达到 posterior Cramer-Rao下限（P-CRLB）。此外，我们还示出了我们的方法可以识别不可靠的测量，从而 mitigate lost tracks。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Diagonal-Reconfigurable-Intelligent-Surfaces-with-Mutual-Coupling-Modeling-and-Optimization"><a href="#Beyond-Diagonal-Reconfigurable-Intelligent-Surfaces-with-Mutual-Coupling-Modeling-and-Optimization" class="headerlink" title="Beyond Diagonal Reconfigurable Intelligent Surfaces with Mutual Coupling: Modeling and Optimization"></a>Beyond Diagonal Reconfigurable Intelligent Surfaces with Mutual Coupling: Modeling and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02708">http://arxiv.org/abs/2310.02708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Li, Shanpu Shen, Matteo Nerini, Marco Di Renzo, Bruno Clerckx</li>
<li>for: 本文研究了卷积式智能表面（BD-RIS）干支持无线通信系统中的相互干扰问题。</li>
<li>methods: 作者首先 derivates the mutual coupling aware BD-RIS aided communication model using scattering and impedance parameter analysis。然后，他们提出了一种通用的BD-RIS优化算法，可以应用于不同的BD-RIS架构，以最大化通道增强。</li>
<li>results: 数值结果表明了提案的设计的有效性，并示出了相对于正常对角RIS的增强效果随着相互干扰的增加。<details>
<summary>Abstract</summary>
This work studies the modeling and optimization of beyond diagonal reconfigurable intelligent surface (BD-RIS) aided wireless communication systems in the presence of mutual coupling among the RIS elements. Specifically, we first derive the mutual coupling aware BD-RIS aided communication model using scattering and impedance parameter analysis. Based on the obtained communication model, we propose a general BD-RIS optimization algorithm applicable to different architectures of BD-RIS to maximize the channel gain. Numerical results validate the effectiveness of the proposed design and demonstrate that the larger the mutual coupling the larger the gain offered by BD-RIS over conventional diagonal RIS.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spectral-vs-Energy-Efficiency-in-6G-Impact-of-the-Receiver-Front-End"><a href="#Spectral-vs-Energy-Efficiency-in-6G-Impact-of-the-Receiver-Front-End" class="headerlink" title="Spectral vs Energy Efficiency in 6G: Impact of the Receiver Front-End"></a>Spectral vs Energy Efficiency in 6G: Impact of the Receiver Front-End</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02622">http://arxiv.org/abs/2310.02622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angel Lozano, Sundeep Rangan</li>
<li>for: 本研究探讨了收发前端（RFE）在无线设备中的作用，以及如何通过信息理论来优化RFE的功率消耗。</li>
<li>methods: 本研究使用了新的模型和假设，以描述RFE在不同频率、宽扩比和antenna数下的行为。</li>
<li>results: 研究发现，随着频率、宽扩比和antenna数的提高，RFE的功率消耗会随之增加，这会导致更多的噪声、非线性和粗糙量化。这种负面效应需要在信息理论中考虑设备功率消耗，以提高6G系统的能效性。<details>
<summary>Abstract</summary>
This article puts the spotlight on the receiver front-end (RFE), an integral part of any wireless device that information theory typically idealizes into a mere addition of noise. While this idealization was sound in the past, as operating frequencies, bandwidths, and antenna counts rise, a soaring amount of power is required for the RFE to behave accordingly. Containing this surge in power expenditure exposes a harsher behavior on the part of the RFE (more noise, nonlinearities, and coarse quantization), setting up a tradeoff between the spectral efficiency under such nonidealities and the efficiency in the use of energy by the RFE. With the urge for radically better power consumptions and energy efficiencies in 6G, this emerges as an issue on which information theory can cast light at a fundamental level. More broadly, this article advocates the interest of having information theory embrace the device power consumption in its analyses. In turn, this calls for new models and abstractions such as the ones herein put together for the RFE, and for a more holistic perspective.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Performance-Analysis-and-Optimization-of-Reconfigurable-Multi-Functional-Surface-Assisted-Wireless-Communications"><a href="#Performance-Analysis-and-Optimization-of-Reconfigurable-Multi-Functional-Surface-Assisted-Wireless-Communications" class="headerlink" title="Performance Analysis and Optimization of Reconfigurable Multi-Functional Surface Assisted Wireless Communications"></a>Performance Analysis and Optimization of Reconfigurable Multi-Functional Surface Assisted Wireless Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02564">http://arxiv.org/abs/2310.02564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Wang, Wanli Ni, Hui Tian, Naofal Al-Dhahir</li>
<li>for: 提高无线网络性能，并且解决现有的双倍干扰和电池依赖问题。</li>
<li>methods: 提出一种新的多功能反射器（MF-RIS）架构，可以支持多种功能，包括信号反射、增强和能量收集。</li>
<li>results: 通过理论分析，得出了MF-RIS协助通信网络的可达性。与现有的自主维持RIS相比，我们计算出了MF-RIS所需的反射元素数量。在多用户无线网络中，通过joint beamforming和MF-RIS偏置的优化，提高了吞吐量。在实际场景中，我们提出了一种Robust beamforming scheme来适应不可避免的频率估计错误。最后，我们通过数据分析发现：1）相比自主维持RIS，MF-RIS可以更好地平衡能源自维和通信性能提高；2）反射器只能在发射器或接收器附近部署，而MF-RIS应该部署在发射器更近的位置以获得更高的频率效率。<details>
<summary>Abstract</summary>
Although reconfigurable intelligent surfaces (RISs) can improve the performance of wireless networks by smartly reconfiguring the radio environment, existing passive RISs face two key challenges, i.e., double-fading attenuation and dependence on grid/battery. To address these challenges, this paper proposes a new RIS architecture, called multi-functional RIS (MF-RIS). Different from conventional reflecting-only RIS, the proposed MF-RIS is capable of supporting multiple functions with one surface, including signal reflection, amplification, and energy harvesting. As such, our MF-RIS is able to overcome the double-fading attenuation by harvesting energy from incident signals. Through theoretical analysis, we derive the achievable capacity of an MF-RIS-aided communication network. Compared to the capacity achieved by the existing self-sustainable RIS, we derive the number of reflective elements required for MF-RIS to outperform self-sustainable RIS. To realize a self-sustainable communication system, we investigate the use of MF-RIS in improving the sum-rate of multi-user wireless networks. Specifically, we solve a non-convex optimization problem by jointly designing the transmit beamforming and MF-RIS coefficients. As an extension, we investigate a resource allocation problem in a practical scenario with imperfect channel state information. By approximating the semi-infinite constraints with the S-procedure and the general sign-definiteness, we propose a robust beamforming scheme to combat the inevitable channel estimation errors. Finally, numerical results show that: 1) compared to the self-sustainable RIS, MF-RIS can strike a better balance between energy self-sustainability and throughput improvement; and 2) unlike reflecting-only RIS which can be deployed near the transmitter or receiver, MF-RIS should be deployed closer to the transmitter for higher spectrum efficiency.
</details>
<details>
<summary>摘要</summary>
although reconfigurable intelligent surfaces (RISs) can improve the performance of wireless networks by smartly reconfiguring the radio environment, existing passive RISs face two key challenges, i.e., double-fading attenuation and dependence on grid/battery. to address these challenges, this paper proposes a new RIS architecture, called multi-functional RIS (MF-RIS). different from conventional reflecting-only RIS, the proposed MF-RIS is capable of supporting multiple functions with one surface, including signal reflection, amplification, and energy harvesting. as such, our MF-RIS is able to overcome the double-fading attenuation by harvesting energy from incident signals. through theoretical analysis, we derive the achievable capacity of an MF-RIS-aided communication network. compared to the capacity achieved by the existing self-sustainable RIS, we derive the number of reflective elements required for MF-RIS to outperform self-sustainable RIS. to realize a self-sustainable communication system, we investigate the use of MF-RIS in improving the sum-rate of multi-user wireless networks. specifically, we solve a non-convex optimization problem by jointly designing the transmit beamforming and MF-RIS coefficients. as an extension, we investigate a resource allocation problem in a practical scenario with imperfect channel state information. by approximating the semi-infinite constraints with the S-procedure and the general sign-definiteness, we propose a robust beamforming scheme to combat the inevitable channel estimation errors. finally, numerical results show that: 1) compared to the self-sustainable RIS, MF-RIS can strike a better balance between energy self-sustainability and throughput improvement; and 2) unlike reflecting-only RIS which can be deployed near the transmitter or receiver, MF-RIS should be deployed closer to the transmitter for higher spectrum efficiency.
</details></li>
</ul>
<hr>
<h2 id="Multi-Functional-Reconfigurable-Intelligent-Surface-System-Modeling-and-Performance-Optimization"><a href="#Multi-Functional-Reconfigurable-Intelligent-Surface-System-Modeling-and-Performance-Optimization" class="headerlink" title="Multi-Functional Reconfigurable Intelligent Surface: System Modeling and Performance Optimization"></a>Multi-Functional Reconfigurable Intelligent Surface: System Modeling and Performance Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02562">http://arxiv.org/abs/2310.02562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Wang, Wanli Ni, Hui Tian, Yonina C. Eldar, Rui Zhang</li>
<li>for: 这 paper 描述了一种多功能可 Configurable  inteligent surface（MF-RIS）架构，而不是传统的单功能 RIS（SF-RIS），该架构可同时支持多种功能，包括反射、折射、增强和能量收集 wireless 信号。因此，MF-RIS 可以增强 RIS 信号覆盖，通过增强反射&#x2F;折射后的信号的增强。</li>
<li>methods: 作者提出了 MF-RIS 信号模型，并对 MF-RIS 帮助的多用户非对称多访问网络中的总比特率进行最大化。作者通过一种有效的迭代算法，对 transmit 扬性、功率分配以及不同 MF-RIS 元素的操作模式和参数进行joint 优化。</li>
<li>results: 作者通过 Simulation 结果表明，MF-RIS 比 SF-RIS 具有显著的性能提升。此外，作者还证明了 MF-RIS 的最佳部署位置应该 closer to the transmitter，以达到更高的通信吞吐量和更多的能量收集。<details>
<summary>Abstract</summary>
In this paper, we propose and study a multi-functional reconfigurable intelligent surface (MF-RIS) architecture. In contrast to conventional single-functional RIS (SF-RIS) that only reflects signals, the proposed MF-RIS simultaneously supports multiple functions with one surface, including reflection, refraction, amplification, and energy harvesting of wireless signals. As such, the proposed MF-RIS is capable of significantly enhancing RIS signal coverage by amplifying the signal reflected/refracted by the RIS with the energy harvested. We present the signal model of the proposed MF-RIS, and formulate an optimization problem to maximize the sum-rate of multiple users in an MF-RIS-aided non-orthogonal multiple access network. We jointly optimize the transmit beamforming, power allocations as well as the operating modes and parameters for different elements of the MF-RIS and its deployment location, via an efficient iterative algorithm. Simulation results are provided which show significant performance gains of the MF-RIS over SF-RISs with only some of its functions available. Moreover, we demonstrate that there exists a fundamental trade-off between sum-rate maximization and harvested energy maximization. In contrast to SF-RISs which can be deployed near either the transmitter or receiver, the proposed MF-RIS should be deployed closer to the transmitter for maximizing its communication throughput with more energy harvested.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种多功能可重配置智能面（MF-RIS）架构，与传统的单功能RIS（SF-RIS）不同，MF-RIS同时支持多种功能，包括反射、折射、增强和能量收集等无线信号处理。因此，MF-RIS可以备受提高RIS信号覆盖率，通过增强RIS反射/折射后的信号和收集的能量进行增强。我们提出了MF-RIS信号模型，并将多用户吞吐量最大化问题转化为一个优化问题。我们通过一种高效的迭代算法，同时优化发射扩散、功率分配以及MF-RIS元素的操作模式和部署位置。实验结果表明，MF-RIS比SF-RIS具有显著的性能提升，并且存在许多功能的负面关系。此外，我们发现，MF-RIS应该在发射器更近的位置进行部署，以确保更高的通信吞吐量和更多的能量收集。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Communications-towards-Proactive-Beamforming-in-mmWave-V2I-via-Multi-Modal-Feature-Fusion-MMFF"><a href="#Integrated-Sensing-and-Communications-towards-Proactive-Beamforming-in-mmWave-V2I-via-Multi-Modal-Feature-Fusion-MMFF" class="headerlink" title="Integrated Sensing and Communications towards Proactive Beamforming in mmWave V2I via Multi-Modal Feature Fusion (MMFF)"></a>Integrated Sensing and Communications towards Proactive Beamforming in mmWave V2I via Multi-Modal Feature Fusion (MMFF)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02561">http://arxiv.org/abs/2310.02561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Zhang, Shijian Gao, Xiang Cheng, Liuqing Yang</li>
<li>for: 提高 vehicular communication networks 的可靠性和数据传输速率，解决传输过程中的窄扩 beam alignment 问题。</li>
<li>methods: 提出一种新的积极扫描方案，通过Multi-Modal Feature Fusion Network (MMFF-Net) 集成多种感知和通信功能，以提高扫描精度。</li>
<li>results: 对 ViWi 数据集进行验证，实现更高的扫描精度和更stable的角度预测，提高可靠性和数据传输速率，并在复杂动态情况下保证Robust 预测结果。<details>
<summary>Abstract</summary>
The future of vehicular communication networks relies on mmWave massive multi-input-multi-output antenna arrays for intensive data transfer and massive vehicle access. However, reliable vehicle-to-infrastructure links require narrow beam alignment, which traditionally involves excessive signaling overhead. To address this issue, we propose a novel proactive beamforming scheme that integrates multi-modal sensing and communications via Multi-Modal Feature Fusion Network (MMFF-Net), which is composed of multiple neural network components with distinct functions. Unlike existing methods that rely solely on communication processing, our approach obtains comprehensive environmental features to improve beam alignment accuracy. We verify our scheme on the ViWi dataset, which we enriched with realistic vehicle drifting behavior. Our proposed MMFF-Net achieves more accurate and stable angle prediction, which in turn increases the achievable rates and reduces the communication system outage probability. Even in complex dynamic scenarios, robust prediction results can be guaranteed, demonstrating the feasibility and practicality of the proposed proactive beamforming approach.
</details>
<details>
<summary>摘要</summary>
未来的车辆通信网络将仰赖mmWave巨量多input多output天线阵列进行激烈数据传输和大量车辆存取。然而，可靠的车辆基础设施连接需要窄焦点对alignment，传统上需要过度的讯号过剩。为解决这个问题，我们提出了一个新的积极扫描方案，通过融合多modal感知和通信via多Modal特征融合网络（MMFF-Net），这是由多个神经网络 ком成分组成，每个 ком成分都有不同的功能。与现有的方法不同，我们的方法可以从感知环境中获取丰富的环境特征，以提高扫描精度。我们在ViWi dataset上验证了我们的方法，并将dataset丰富了现实的车辆滑动行为。我们的提案的MMFF-Net可以更高精度和稳定的角度预测，这样可以增加可达率和减少通信系统失效机会。甚至在复杂的动态enario中，我们可以保证Robust的预测结果，这说明了我们的积极扫描方案的可行性和实用性。
</details></li>
</ul>
<hr>
<h2 id="ISAC-Signal-Processing-Over-Unlicensed-Spectrum-Bands"><a href="#ISAC-Signal-Processing-Over-Unlicensed-Spectrum-Bands" class="headerlink" title="ISAC Signal Processing Over Unlicensed Spectrum Bands"></a>ISAC Signal Processing Over Unlicensed Spectrum Bands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02555">http://arxiv.org/abs/2310.02555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Liu, Zhiqing Wei, Fengyun Li, Yuewei Lin, Hanyang Qu, Huici Wu, Zhiyong Feng</li>
<li>for: 本研究旨在提出一种基于压缩学习（Compressed Sensing，CS）的高精度Integrated Sensing and Communication（ISAC）信号处理算法，以解决5G NR中的非连续频谱带中的感知问题。</li>
<li>methods: 本研究使用了5G NR中的资源块组（Resource Block Group，RBG）配置信息和通道信息矩阵，以动态和准确地获取电力估计谱。此外，我们采用了快速迭代减小阈值算法（FISTA）来解决重建问题，并使用K-fold交叉验证（KCV）来获得最佳参数。</li>
<li>results: 模拟结果表明，提出的算法在非连续频谱带中具有较低的侧lobes或甚至为零侧lobes，同时具有高的抗噪性能。与传统感知算法相比，本研究的算法具有更高的精度和更低的误差。<details>
<summary>Abstract</summary>
As a promising key technology of 6th-Generation (6G) mobile communication systems, integrated sensing and communication (ISAC) technology aims to make full use of spectrum resources to enable the functional integration of communication and sensing. The ISAC-enabled mobile communication systems regularly operate in non-continuous spectrum bands due to crowded licensed frequency bands. However, the conventional sensing algorithms over non-continuous spectrum bands have disadvantages such as reduced peak-to-sidelobe ratio (PSR) and degraded anti-noise performance. Facing this challenge, we propose a high-precision ISAC signal processing algorithm based on compressed sensing (CS) in this paper. By integrating the resource block group (RBG) configuration information in 5th-Generation new radio (5G NR) and channel information matrices, we can dynamically and accurately obtain power estimation spectra. Moreover, we employ the fast iterative shrinkage-thresholding algorithm (FISTA) to address the reconstruction problem and utilize K-fold cross validation (KCV) to obtain optimal parameters. Simulation results show that the proposed algorithm has lower sidelobes or even zero sidelobes and high anti-noise performance compared with conventional sensing algorithms.
</details>
<details>
<summary>摘要</summary>
As a promising key technology of 6th-Generation (6G) mobile communication systems, integrated sensing and communication (ISAC) technology aims to make full use of spectrum resources to enable the functional integration of communication and sensing. The ISAC-enabled mobile communication systems regularly operate in non-continuous spectrum bands due to crowded licensed frequency bands. However, the conventional sensing algorithms over non-continuous spectrum bands have disadvantages such as reduced peak-to-sidelobe ratio (PSR) and degraded anti-noise performance. Facing this challenge, we propose a high-precision ISAC signal processing algorithm based on compressed sensing (CS) in this paper. By integrating the resource block group (RBG) configuration information in 5th-Generation new radio (5G NR) and channel information matrices, we can dynamically and accurately obtain power estimation spectra. Moreover, we employ the fast iterative shrinkage-thresholding algorithm (FISTA) to address the reconstruction problem and utilize K-fold cross validation (KCV) to obtain optimal parameters. Simulation results show that the proposed algorithm has lower sidelobes or even zero sidelobes and high anti-noise performance compared with conventional sensing algorithms.Here's the word-for-word translation in Simplified Chinese:作为6G移动通信系统的优秀关键技术，集成感知通信（ISAC）技术目的是充分利用频谱资源，实现通信和感知的函数集成。ISAC启用的移动通信系统通常在非连续频谱带中运行，由于拥堵的licensed频谱带。但是，传统的感知算法在非连续频谱带上有缺点，如降低峰夹强度比（PSR）和降低反噪性能。面临这个挑战，我们在本文中提出了基于压缩感知（CS）的高精度ISAC信号处理算法。通过将5G NR中的资源块组（RBG）配置信息和通道信息矩阵绑定在一起，我们可以在实时和准确地获取电力估计谱。此外，我们采用了快速迭代缩放阈值算法（FISTA）解决重建问题，并使用K-fold Cross Validation（KCV）获得优化参数。实验结果表明，提议的算法在传统感知算法中具有更低的夹强或 même zero夹强，以及高反噪性能。
</details></li>
</ul>
<hr>
<h2 id="Convergence-Analysis-and-Latency-Minimization-for-Semi-Federated-Learning-in-Massive-IoT-Networks"><a href="#Convergence-Analysis-and-Latency-Minimization-for-Semi-Federated-Learning-in-Massive-IoT-Networks" class="headerlink" title="Convergence Analysis and Latency Minimization for Semi-Federated Learning in Massive IoT Networks"></a>Convergence Analysis and Latency Minimization for Semi-Federated Learning in Massive IoT Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02550">http://arxiv.org/abs/2310.02550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianyang Ren, Wanli Ni, Hui Tian, Gaofeng Nie</li>
<li>For: This paper is written to address the issue of Federated Learning (FL) latency in Internet of Things (IoT) networks with massive devices.* Methods: The paper proposes a Semi-Federated Learning (SemiFL) paradigm that combines network pruning and over-the-air computation to reduce FL latency.* Results: The paper derives an upper bound on the convergence performance of the proposed SemiFL and formulates a convergence-constrained SemiFL latency minimization problem. Iterative algorithms are designed to solve the problem efficiently, and numerical simulations verify the effectiveness of the proposed scheme in reducing latency while maintaining identification accuracy.<details>
<summary>Abstract</summary>
As the number of sensors becomes massive in Internet of Things (IoT) networks, the amount of data is humongous. To process data in real-time while protecting user privacy, federated learning (FL) has been regarded as an enabling technique to push edge intelligence into IoT networks with massive devices. However, FL latency increases dramatically due to the increase of the number of parameters in deep neural network and the limited computation and communication capabilities of IoT devices. To address this issue, we propose a semi-federated learning (SemiFL) paradigm in which network pruning and over-the-air computation are efficiently applied. To be specific, each small base station collects the raw data from its served sensors and trains its local pruned model. After that, the global aggregation of local gradients is achieved through over-the-air computation. We first analyze the performance of the proposed SemiFL by deriving its convergence upper bound. To reduce latency, a convergence-constrained SemiFL latency minimization problem is formulated. By decoupling the original problem into several sub-problems, iterative algorithms are designed to solve them efficiently. Finally, numerical simulations are conducted to verify the effectiveness of our proposed scheme in reducing latency and guaranteeing the identification accuracy.
</details>
<details>
<summary>摘要</summary>
为了处理互联网器（IoT）网络中巨大数据的实时处理，保护用户隐私，联邦学习（FL）被视为一种可能的技术。然而，FL的延迟增加了许多，这是由于深度神经网络中参数的增加和IoT设备的计算和通信能力的限制。为解决这个问题，我们提出了半联邦学习（SemiFL）模式。在这种模式下，每个小基站收集自己服务的感知器的原始数据，并在本地进行缩减模型的训练。然后，通过空中计算，全球集成本地梯度的全球汇总。我们首先分析了提议的SemiFL性能，并derive其 convergence upper bound。为了降低延迟，我们提出了一个基于泛化的SemiFL延迟最小化问题。通过分解原始问题，我们设计了高效的迭代算法来解决它们。最后，我们通过数值仿真来验证我们的提议的有效性，降低延迟，并保证标识精度。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Energy-Efficiency-in-Massive-MIMO-A-Scalable-Low-Complexity-Decoder-for-Generalized-Quadrature-Spatial-Modulation"><a href="#Enabling-Energy-Efficiency-in-Massive-MIMO-A-Scalable-Low-Complexity-Decoder-for-Generalized-Quadrature-Spatial-Modulation" class="headerlink" title="Enabling Energy-Efficiency in Massive-MIMO: A Scalable Low-Complexity Decoder for Generalized Quadrature Spatial Modulation"></a>Enabling Energy-Efficiency in Massive-MIMO: A Scalable Low-Complexity Decoder for Generalized Quadrature Spatial Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02545">http://arxiv.org/abs/2310.02545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeon Seok Rou, Giuseppe Thadeu Freitas de Abreu, David González G., Osvaldo Gonsa</li>
<li>for: 提高无线系统的能量和频率效率</li>
<li>methods: 使用新的准则和算法，包括向量化 Gaussian belief propagation（GaBP）算法和单元向量分解（UVD）</li>
<li>results: 在计算机模拟中，包括32个发射天线的系统，实现了高能量和频率效率，展示了该方法在true massive MIMO设置中的潜在潜力。<details>
<summary>Abstract</summary>
Generalized quadrature spatial modulation (GQSM) schemes are known to achieve high energy- and spectral- efficiencies by modulating information both in transmitted symbols and in coded combinatorial activations of subsets of multiple transmit antennas. A challenge of the approach is, however, the decoding complexity which scales with the efficiency of the scheme. In order to circumvent this bottleneck and enable high-performance and feasible GQSM in massive multiple-input multiple-output (mMIMO) scenarios, we propose a novel decoding algorithm which enjoys a complexity order that is independent of the combinatorial factor. This remarkable feature of the proposed decoder is a consequence of a novel vectorized Gaussian belief propagation (GaBP) algorithm, here contributed, whose message passing (MP) rules leverage both pilot symbols and the unit vector decomposition (UVD) of the GQSM signal structure. The effectiveness of the proposed UVD-GaBP method is illustrated via computer simulations including numerical results for systems of a size never before reported in related literature (up to 32 transmit antennas), which demonstrates the potential of the approach in paving the way towards high energy and spectral efficiency for wireless systems in a truly mMIMO setting.
</details>
<details>
<summary>摘要</summary>
通用 quadrature 空间模ulation（GQSM）策略已经知道可以实现高能量和频率效率，但是这种方法的解码复杂度与策略的效率成线性关系。为了缓解这个瓶颈并实现高性能可行的GQSM，我们提出了一种新的解码算法，其复杂度随着策略的效率而变化。这一特点是我们提出的解码器的一个重要特点，它基于一种新的vectorized Gaussian belief propagation（GaBP）算法，该算法利用了传输符号和GQSM信号结构的unit vector decomposition（UVD）。我们通过计算机实验，包括数字结果，证明了该方法的有效性，并且可以在相关文献中找到相应的实验结果。这些结果表明了我们的方法在大规模mMIMO场景下具有很高的能量和频率效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/04/eess.SP_2023_10_04/" data-id="closbrp0601bk0g88g5kq7r20" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/03/cs.SD_2023_10_03/" class="article-date">
  <time datetime="2023-10-03T15:00:00.000Z" itemprop="datePublished">2023-10-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/03/cs.SD_2023_10_03/">cs.SD - 2023-10-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Audio-visual-child-adult-speaker-classification-in-dyadic-interactions"><a href="#Audio-visual-child-adult-speaker-classification-in-dyadic-interactions" class="headerlink" title="Audio-visual child-adult speaker classification in dyadic interactions"></a>Audio-visual child-adult speaker classification in dyadic interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01867">http://arxiv.org/abs/2310.01867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anfeng Xu, Kevin Huang, Tiantian Feng, Helen Tager-Flusberg, Shrikanth Narayanan</li>
<li>for: 这个论文的目的是提高自动识别儿童和成人之间的交流，以便更加准确地了解儿童在不同情况下的表达和行为。</li>
<li>methods: 这个论文使用了一种 combining audio and video modalities的方法，通过活动 speaker detection 和视觉处理模型来提高儿童和成人的识别精度和稳定性。</li>
<li>results: 实验结果表明，在使用视觉信号的情况下，识别精度和稳定性都得到了大幅提高，相比 Audio-only 模型，在一个面和两个面可见的情况下，相对提高了2.38%和3.97%的F1宏score。<details>
<summary>Abstract</summary>
Interactions involving children span a wide range of important domains from learning to clinical diagnostic and therapeutic contexts. Automated analyses of such interactions are motivated by the need to seek accurate insights and offer scale and robustness across diverse and wide-ranging conditions. Identifying the speech segments belonging to the child is a critical step in such modeling. Conventional child-adult speaker classification typically relies on audio modeling approaches, overlooking visual signals that convey speech articulation information, such as lip motion. Building on the foundation of an audio-only child-adult speaker classification pipeline, we propose incorporating visual cues through active speaker detection and visual processing models. Our framework involves video pre-processing, utterance-level child-adult speaker detection, and late fusion of modality-specific predictions. We demonstrate from extensive experiments that a visually aided classification pipeline enhances the accuracy and robustness of the classification. We show relative improvements of 2.38% and 3.97% in F1 macro score when one face and two faces are visible, respectively.
</details>
<details>
<summary>摘要</summary>
互动 involving 儿童 span 广泛的重要领域，从学习到临床诊断和治疗上下。自动分析这些互动的目的是寻求准确的洞察和在多样化和广泛的条件下提供扩展和坚固性。确定孩子的语音段为 kritical step in such modeling. conventional child-adult speaker classification 通常采用音频模型方法，忽略视觉信号，例如唇部运动。我们基于音频只的儿童-成人 speaker classification 管道，提议包含视觉信号，通过活动 speaker detection 和视觉处理模型。我们的框架包括视频预处理、utterance-level child-adult speaker detection 和多模式特征 fusion。经过广泛的实验，我们证明了通过视觉支持的分类管道可以提高分类精度和稳定性。我们显示在一个面和两个面可见时的相对改善为2.38%和3.97%的F1宏Score。
</details></li>
</ul>
<hr>
<h2 id="Mel-Band-RoFormer-for-Music-Source-Separation"><a href="#Mel-Band-RoFormer-for-Music-Source-Separation" class="headerlink" title="Mel-Band RoFormer for Music Source Separation"></a>Mel-Band RoFormer for Music Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01809">http://arxiv.org/abs/2310.01809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju-Chiang Wang, Wei-Tsung Lu, Minz Won</li>
<li>for: 这个论文主要targets music source separation, specifically using a multi-band spectrogram-based approach with a hierarchical Transformer and Rotary Position Embedding (RoPE) for multi-band mask estimation.</li>
<li>methods: 这个模型使用了band-split scheme，但是这个scheme是基于empirical experiments而定义的，没有文学支持。这个论文提出了Mel-RoFormer模型，它采用了mel scale来映射频谱分解成 overlap subbands。</li>
<li>results: 在使用MUSDB18HQ dataset进行实验时，Mel-RoFormer模型比BS-RoFormer模型在分离 vocals, drums和其他的stems中表现更好。<details>
<summary>Abstract</summary>
Recently, multi-band spectrogram-based approaches such as Band-Split RNN (BSRNN) have demonstrated promising results for music source separation. In our recent work, we introduce the BS-RoFormer model which inherits the idea of band-split scheme in BSRNN at the front-end, and then uses the hierarchical Transformer with Rotary Position Embedding (RoPE) to model the inner-band and inter-band sequences for multi-band mask estimation. This model has achieved state-of-the-art performance, but the band-split scheme is defined empirically, without analytic supports from the literature. In this paper, we propose Mel-RoFormer, which adopts the Mel-band scheme that maps the frequency bins into overlapped subbands according to the mel scale. In contract, the band-split mapping in BSRNN and BS-RoFormer is non-overlapping and designed based on heuristics. Using the MUSDB18HQ dataset for experiments, we demonstrate that Mel-RoFormer outperforms BS-RoFormer in the separation tasks of vocals, drums, and other stems.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:近期，基于多 banda spectrogram 的方法，如 Band-Split RNN (BSRNN)，已经在音乐源分离方面显示出了有前途的结果。在我们最近的工作中，我们引入了 BS-RoFormer 模型，该模型继承了 BSRNN 的前端band-split scheme，然后使用层次 Transformer 以及 Rotary Position Embedding (RoPE) 来模型内部band和 междуband的序列，进行多band屏蔽估计。这个模型已经达到了状态之 искусственный支持，但是band-split scheme是基于empirical定义的，没有文献支持。在这篇论文中，我们提出了 Mel-RoFormer 模型，该模型采用了 Mel 带 scheme，将频谱窗口中的频谱带分成了相互重叠的子带，根据 Mel 尺度进行映射。与BSRNN和 BS-RoFormer 中的band-split映射不同，Mel-RoFormer 的映射是非相互重叠的，并且是基于 empirical 的设计。使用 MUSDB18HQ 数据集进行实验，我们示出了 Mel-RoFormer 在 vocals、鼓和其他声道的分离任务中的优于 BS-RoFormer。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/03/cs.SD_2023_10_03/" data-id="closbrouq00xq0g8814ce3nq4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/03/cs.CV_2023_10_03/" class="article-date">
  <time datetime="2023-10-03T13:00:00.000Z" itemprop="datePublished">2023-10-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/03/cs.CV_2023_10_03/">cs.CV - 2023-10-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Harvard-Eye-Fairness-A-Large-Scale-3D-Imaging-Dataset-for-Equitable-Eye-Diseases-Screening-and-Fair-Identity-Scaling"><a href="#Harvard-Eye-Fairness-A-Large-Scale-3D-Imaging-Dataset-for-Equitable-Eye-Diseases-Screening-and-Fair-Identity-Scaling" class="headerlink" title="Harvard Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling"></a>Harvard Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye Diseases Screening and Fair Identity Scaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02492">http://arxiv.org/abs/2310.02492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Luo, Yu Tian, Min Shi, Tobias Elze, Mengyu Wang<br>for:* 这个论文的目的是为了提供一个大规模的公共医疗 datasets，用于针对医学领域的公平学习。methods:* 这篇论文使用了一种新的公平标准化方法（FIS），并与其他当前的公平学习方法进行比较。results:* 论文提出了一个包含30,000名参与者的 Harvard-EF  dataset，该dataset包括了3D optical coherence tomography scans和2D fundus photos，并且包含了6个个人特征。Here’s the same information in Simplified Chinese text:for:* 这篇论文的目的是为了提供一个大规模的公共医疗数据集，用于针对医学领域的公平学习。methods:* 这篇论文使用了一种新的公平标准化方法（FIS），并与其他当前的公平学习方法进行比较。results:* 论文提出了一个包含30,000名参与者的 Harvard-EF 数据集，该数据集包括了3D optical coherence tomography scans和2D fundus photos，并且包含了6个个人特征。<details>
<summary>Abstract</summary>
Fairness or equity in machine learning is profoundly important for societal well-being, but limited public datasets hinder its progress, especially in the area of medicine. It is undeniable that fairness in medicine is one of the most important areas for fairness learning's applications. Currently, no large-scale public medical datasets with 3D imaging data for fairness learning are available, while 3D imaging data in modern clinics are standard tests for disease diagnosis. In addition, existing medical fairness datasets are actually repurposed datasets, and therefore they typically have limited demographic identity attributes with at most three identity attributes of age, gender, and race for fairness modeling. To address this gap, we introduce our Eye Fairness dataset with 30,000 subjects (Harvard-EF) covering three major eye diseases including age-related macular degeneration, diabetic retinopathy, and glaucoma affecting 380 million patients globally. Our Harvard-EF dataset includes both 2D fundus photos and 3D optical coherence tomography scans with six demographic identity attributes including age, gender, race, ethnicity, preferred language, and marital status. We also propose a fair identity scaling (FIS) approach combining group and individual scaling together to improve model fairness. Our FIS approach is compared with various state-of-the-art fairness learning methods with superior performance in the racial, gender, and ethnicity fairness tasks with 2D and 3D imaging data, which demonstrate the utilities of our Harvard-EF dataset for fairness learning. To facilitate fairness comparisons between different models, we propose performance-scaled disparity measures, which can be used to compare model fairness accounting for overall performance levels. The dataset and code are publicly accessible via https://ophai.hms.harvard.edu/datasets/harvard-ef30k.
</details>
<details>
<summary>摘要</summary>
“公平或公德在机器学习中非常重要，但有限的公共数据集阻碍了其进步，特别是在医疗领域。无疑地，医疗领域的公平是机器学习公平应用的重要领域。目前，没有大规模的公共医疗数据集，包含3D成像数据，用于公平学习。此外，现有的医疗公平数据集都是 reuse 的数据集，因此它们通常只有限制的民族标识属性，最多只有三个标识属性：年龄、性别和种族。为解决这个差距，我们引入了我们的眼睛公平数据集（Harvard-EF），包括30,000名参与者，涵盖三种主要的眼病：年龄相关的macular degeneration、 диабетиче Retinopathy 和 Glaucoma，全球380亿名患者。我们的 Harvard-EF 数据集包括2D背景照片和3D光子成像扫描，六个民族标识属性：年龄、性别、种族、语言、婚姻状况和民族。我们还提出了一种公平标识扩大（FIS）方法，结合集体和个体扩大方法，以提高模型公平性。我们的 FIS 方法与各种当前最佳公平学习方法进行比较，在种族、性别和民族公平任务中表现优秀，这些任务使用 2D 和 3D 成像数据。为便于公平比较不同模型，我们提议用性能比例的 disparity 度量，可以用来比较不同模型的公平性，考虑到不同模型的总性能水平。数据集和代码可以通过以下链接获取：https://ophai.hms.harvard.edu/datasets/harvard-ef30k。”
</details></li>
</ul>
<hr>
<h2 id="OCU-Net-A-Novel-U-Net-Architecture-for-Enhanced-Oral-Cancer-Segmentation"><a href="#OCU-Net-A-Novel-U-Net-Architecture-for-Enhanced-Oral-Cancer-Segmentation" class="headerlink" title="OCU-Net: A Novel U-Net Architecture for Enhanced Oral Cancer Segmentation"></a>OCU-Net: A Novel U-Net Architecture for Enhanced Oral Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02486">http://arxiv.org/abs/2310.02486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Albishri, Syed Jawad Hussain Shah, Yugyung Lee, Rong Wang</li>
<li>for: 革新的肺癌图像分割模型OCU-Net，旨在提高肺癌图像分割精度。</li>
<li>methods: OCU-Net使用了多种进步的深度学习模块，包括通道和空间注意力融合（CSAF）模块、抑压激活（SE）注意力模块、尺度空间Pyramid Pooling（ASPP）模块、循环块和多尺度融合。</li>
<li>results: OCU-Net和OCU-Netm在两个H&amp;E图像 Dataset上实现了高精度的肺癌图像分割，超过了现有的分割方法。<details>
<summary>Abstract</summary>
Accurate detection of oral cancer is crucial for improving patient outcomes. However, the field faces two key challenges: the scarcity of deep learning-based image segmentation research specifically targeting oral cancer and the lack of annotated data. Our study proposes OCU-Net, a pioneering U-Net image segmentation architecture exclusively designed to detect oral cancer in hematoxylin and eosin (H&E) stained image datasets. OCU-Net incorporates advanced deep learning modules, such as the Channel and Spatial Attention Fusion (CSAF) module, a novel and innovative feature that emphasizes important channel and spatial areas in H&E images while exploring contextual information. In addition, OCU-Net integrates other innovative components such as Squeeze-and-Excite (SE) attention module, Atrous Spatial Pyramid Pooling (ASPP) module, residual blocks, and multi-scale fusion. The incorporation of these modules showed superior performance for oral cancer segmentation for two datasets used in this research. Furthermore, we effectively utilized the efficient ImageNet pre-trained MobileNet-V2 model as a backbone of our OCU-Net to create OCU-Netm, an enhanced version achieving state-of-the-art results. Comprehensive evaluation demonstrates that OCU-Net and OCU-Netm outperformed existing segmentation methods, highlighting their precision in identifying cancer cells in H&E images from OCDC and ORCA datasets.
</details>
<details>
<summary>摘要</summary>
医学图像分割是肺癌检测中不可或缺的一环，但是这个领域面临两个主要挑战：一是深度学习基于图像分割的研究对肺癌的不足，二是图像分割数据集的标注不充分。我们的研究提出了OCU-Net，一种专门为肺癌H&E染色图像进行图像分割的U-Net架构。OCU-Net包含了高级深度学习模块，如通道和空间注意力融合（CSAF）模块、抑压扩展（SE）注意力模块、精细空间 Pyramid Pooling（ASPP）模块、循环块和多比例融合。这些模块的结合使OCU-Net在两个测试数据集上实现了优秀的肺癌图像分割表现。此外，我们有效地利用了高效的ImageNet预训练MobileNet-V2模型作为OCU-Net的基础模型，创建了OCU-Netm，这是一个提高了表现的版本。对OCU-Net和OCU-Netm进行了全面的评估，表明它们在OCDC和ORCA数据集上的肺癌图像分割表现比既有方法更加精准。
</details></li>
</ul>
<hr>
<h2 id="EvDNeRF-Reconstructing-Event-Data-with-Dynamic-Neural-Radiance-Fields"><a href="#EvDNeRF-Reconstructing-Event-Data-with-Dynamic-Neural-Radiance-Fields" class="headerlink" title="EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields"></a>EvDNeRF: Reconstructing Event Data with Dynamic Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02437">http://arxiv.org/abs/2310.02437</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anish-bhattacharya/evdnerf">https://github.com/anish-bhattacharya/evdnerf</a></li>
<li>paper_authors: Anish Bhattacharya, Ratnesh Madaan, Fernando Cladera, Sai Vemprala, Rogerio Bonatti, Kostas Daniilidis, Ashish Kapoor, Vijay Kumar, Nikolai Matni, Jayesh K. Gupta</li>
<li>for: 用于重constructing事件流动场景中的快速运动，包括非固定和固定扭变。</li>
<li>methods: 使用事件摄像头捕捉 asynchronous per-pixel明亮变化，并使用神经透辑场（NeRF）进行可见质量的可学习渲染。</li>
<li>results: 提供了一个可用于 simulate dynamic scenes的事件基本的NeRF模型，并在不同的批处大小下进行了训练，以提高测试时间分辨率的预测结果，超越基eline的标准动态NeRF和事件模拟器。<details>
<summary>Abstract</summary>
We present EvDNeRF, a pipeline for generating event data and training an event-based dynamic NeRF, for the purpose of faithfully reconstructing eventstreams on scenes with rigid and non-rigid deformations that may be too fast to capture with a standard camera. Event cameras register asynchronous per-pixel brightness changes at MHz rates with high dynamic range, making them ideal for observing fast motion with almost no motion blur. Neural radiance fields (NeRFs) offer visual-quality geometric-based learnable rendering, but prior work with events has only considered reconstruction of static scenes. Our EvDNeRF can predict eventstreams of dynamic scenes from a static or moving viewpoint between any desired timestamps, thereby allowing it to be used as an event-based simulator for a given scene. We show that by training on varied batch sizes of events, we can improve test-time predictions of events at fine time resolutions, outperforming baselines that pair standard dynamic NeRFs with event simulators. We release our simulated and real datasets, as well as code for both event-based data generation and the training of event-based dynamic NeRF models (https://github.com/anish-bhattacharya/EvDNeRF).
</details>
<details>
<summary>摘要</summary>
我们提出了EvDNeRF，一个搬运数据生成和训练基于事件的动态NeRF渲染管道，用于重现具有固定和非固定扭formation的场景中的事件流。事件摄像机可以在MHz速率上高Dynamic范围内注册ynchronously per-pixel的明亮变化，这使得它们成为观察快速运动的首选方式。Neural radiance fields（NeRFs）提供可见质量基于Geometric的学习渲染，但以前的事件处理只考虑了静止场景的重建。我们的EvDNeRF可以预测动态场景的事件流，从任意时间戳点的视角中查看和渲染场景，因此可以作为场景的事件基本模拟器。我们证明，通过训练不同批大小的事件，可以在细致的时间分辨率上提高测试预测的事件，超过基准的标准动态NeRF和事件模拟器的组合。我们发布了模拟和实际数据集，以及用于生成事件基本数据和训练事件基于动态NeRF模型的代码（https://github.com/anish-bhattacharya/EvDNeRF）。
</details></li>
</ul>
<hr>
<h2 id="EditVal-Benchmarking-Diffusion-Based-Text-Guided-Image-Editing-Methods"><a href="#EditVal-Benchmarking-Diffusion-Based-Text-Guided-Image-Editing-Methods" class="headerlink" title="EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods"></a>EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02426">http://arxiv.org/abs/2310.02426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, Soheil Feizi<br>for: EditVal is a standardized benchmark for evaluating text-guided image editing methods, which aims to provide a fair comparison of different methods across different types of fine-grained edits.methods: EditVal uses a curated dataset of images, a set of editable attributes for each image, and pre-trained vision-language models to assess the fidelity of generated images for each edit type.results: The top-performing methods averaged across different edit types are Instruct-Pix2Pix and Null-Text, but only Instruct-Pix2Pix and Null-Text are able to preserve original image properties. Most editing methods fail at edits involving spatial operations, and there is no single &#96;winner’ method that ranks the best individually across a range of different edit types.<details>
<summary>Abstract</summary>
A plethora of text-guided image editing methods have recently been developed by leveraging the impressive capabilities of large-scale diffusion-based generative models such as Imagen and Stable Diffusion. A standardized evaluation protocol, however, does not exist to compare methods across different types of fine-grained edits. To address this gap, we introduce EditVal, a standardized benchmark for quantitatively evaluating text-guided image editing methods. EditVal consists of a curated dataset of images, a set of editable attributes for each image drawn from 13 possible edit types, and an automated evaluation pipeline that uses pre-trained vision-language models to assess the fidelity of generated images for each edit type. We use EditVal to benchmark 8 cutting-edge diffusion-based editing methods including SINE, Imagic and Instruct-Pix2Pix. We complement this with a large-scale human study where we show that EditVall's automated evaluation pipeline is strongly correlated with human-preferences for the edit types we considered. From both the human study and automated evaluation, we find that: (i) Instruct-Pix2Pix, Null-Text and SINE are the top-performing methods averaged across different edit types, however {\it only} Instruct-Pix2Pix and Null-Text are able to preserve original image properties; (ii) Most of the editing methods fail at edits involving spatial operations (e.g., changing the position of an object). (iii) There is no `winner' method which ranks the best individually across a range of different edit types. We hope that our benchmark can pave the way to developing more reliable text-guided image editing tools in the future. We will publicly release EditVal, and all associated code and human-study templates to support these research directions in https://deep-ml-research.github.io/editval/.
</details>
<details>
<summary>摘要</summary>
“一些文本引导的图像修改方法在最近得到了广泛研究，利用大规模的扩散基于生成模型，如Imagen和Stable Diffusion。然而，没有一个标准化的评估协议，以便比较不同类型的细腻修改方法。为解决这个 gap，我们介绍 EditVal，一个标准化的评估协议，用于量化评估文本引导的图像修改方法。EditVal包括一个精心准备的图像集，每个图像的编辑属性，以及使用预训练的视觉语言模型来评估生成图像的准确性。我们使用 EditVal 评估8种 cutting-edge 扩散基于修改方法，包括 SINE、Imagic 和 Instruct-Pix2Pix。我们 также进行了大规模的人类研究，并发现：（i） Instruct-Pix2Pix 和 Null-Text 是最高效的方法，但只有 Instruct-Pix2Pix 能够保留原图像的性质；（ii）大多数修改方法在空间操作（例如，对象的位置变化）时失败；（iii）没有一个 `winner' 方法，能够在不同类型的修改任务中表现出色。我们希望 EditVal 能够为未来开发更可靠的文本引导图像修改工具提供依据。我们将在 https://deep-ml-research.github.io/editval/ 上公开 EditVal，以及所有相关的代码和人类研究模板。”
</details></li>
</ul>
<hr>
<h2 id="FedL2P-Federated-Learning-to-Personalize"><a href="#FedL2P-Federated-Learning-to-Personalize" class="headerlink" title="FedL2P: Federated Learning to Personalize"></a>FedL2P: Federated Learning to Personalize</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02420">http://arxiv.org/abs/2310.02420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/royson/fedl2p">https://github.com/royson/fedl2p</a></li>
<li>paper_authors: Royson Lee, Minyoung Kim, Da Li, Xinchi Qiu, Timothy Hospedales, Ferenc Huszár, Nicholas D. Lane</li>
<li>for: 学习个性化策略的 Federated Meta-Learning 问题</li>
<li>methods: 使用 Federated Learning 方法学习批处理和学习率参数，以适应每个客户端的特定数据分布</li>
<li>results: 比标准手动定制策略提高表达效果，在标签和特征转移 Situations 中均显示提高表达效果<details>
<summary>Abstract</summary>
Federated learning (FL) research has made progress in developing algorithms for distributed learning of global models, as well as algorithms for local personalization of those common models to the specifics of each client's local data distribution. However, different FL problems may require different personalization strategies, and it may not even be possible to define an effective one-size-fits-all personalization strategy for all clients: depending on how similar each client's optimal predictor is to that of the global model, different personalization strategies may be preferred. In this paper, we consider the federated meta-learning problem of learning personalization strategies. Specifically, we consider meta-nets that induce the batch-norm and learning rate parameters for each client given local data statistics. By learning these meta-nets through FL, we allow the whole FL network to collaborate in learning a customized personalization strategy for each client. Empirical results show that this framework improves on a range of standard hand-crafted personalization baselines in both label and feature shift situations.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）的研究已经做出了分布式学习全域模型的算法和每个客户的本地数据分布特有的本地个性化算法的进展。但是不同的FL问题可能需要不同的个性化策略，而且可能无法定义一个一般适用的一个大小全域个性化策略 для所有客户：对于每个客户的最佳预测器与全域模型的预测器之间的相似度可能会影响选择的个性化策略。在这篇论文中，我们考虑了联合元学习问题，具体是通过FL学习每个客户的批装优化和学习速率参数。通过这种方法，我们让整个FL网络共同学习每个客户的自适应策略。实验结果显示，这个框架可以超过一些传统手工个性化基准的表现，包括标签和特征迁移的情况下。
</details></li>
</ul>
<hr>
<h2 id="Bag-of-Tricks-for-Fully-Test-Time-Adaptation"><a href="#Bag-of-Tricks-for-Fully-Test-Time-Adaptation" class="headerlink" title="Bag of Tricks for Fully Test-Time Adaptation"></a>Bag of Tricks for Fully Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02416">http://arxiv.org/abs/2310.02416</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smounsav/tta_bot">https://github.com/smounsav/tta_bot</a></li>
<li>paper_authors: Saypraseuth Mounsaveng, Florent Chiaroni, Malik Boudiaf, Marco Pedersoli, Ismail Ben Ayed</li>
<li>for: 本研究旨在为Test-Time Adaptation (TTA)提供一个系统性的分类和分析，以帮助汇集社区的知识，并提高TTA的性能。</li>
<li>methods: 本研究使用了多种 orthogonal TTA 技巧，包括小批量均值、流程重新平衡、可靠样本选择和网络自信度调整。</li>
<li>results: 通过分析各种场景，本研究揭示了每个技巧的影响，并探讨了精度、计算资源和模型复杂性之间的交互关系。此外，研究还发现了 combining 技巧时的 synergy，并实现了新的州OF-the-art 结果。<details>
<summary>Abstract</summary>
Fully Test-Time Adaptation (TTA), which aims at adapting models to data drifts, has recently attracted wide interest. Numerous tricks and techniques have been proposed to ensure robust learning on arbitrary streams of unlabeled data. However, assessing the true impact of each individual technique and obtaining a fair comparison still constitutes a significant challenge. To help consolidate the community's knowledge, we present a categorization of selected orthogonal TTA techniques, including small batch normalization, stream rebalancing, reliable sample selection, and network confidence calibration. We meticulously dissect the effect of each approach on different scenarios of interest. Through our analysis, we shed light on trade-offs induced by those techniques between accuracy, the computational power required, and model complexity. We also uncover the synergy that arises when combining techniques and are able to establish new state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
全面测试时适应（TTA），目标是适应数据变化，在最近几年内吸引了广泛的关注。许多技巧和技术已经被提议，以确保在无标注数据流中强健学习。然而，评估每个个体技巧的真正影响和取得公平比较仍然是一项重要挑战。为了帮助共同知识的整合，我们提出了选择的orthogonal TTA技巧的分类，包括小批量正常化、流量重新平衡、可靠样本选择和网络信任均衡。我们仔细分析每种方法在不同的场景下的效果，并揭示了每种技巧的精度、计算资源和模型复杂性之间的贸易。此外，我们发现了不同技巧的合作 synergy，并成功建立了新的状态状态 record。
</details></li>
</ul>
<hr>
<h2 id="FT-Shield-A-Watermark-Against-Unauthorized-Fine-tuning-in-Text-to-Image-Diffusion-Models"><a href="#FT-Shield-A-Watermark-Against-Unauthorized-Fine-tuning-in-Text-to-Image-Diffusion-Models" class="headerlink" title="FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models"></a>FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02401">http://arxiv.org/abs/2310.02401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingqian Cui, Jie Ren, Yuping Lin, Han Xu, Pengfei He, Yue Xing, Wenqi Fan, Hui Liu, Jiliang Tang</li>
<li>for: 防止文本到图像散发模型ersonal化的违用数据泄露问题</li>
<li>methods: 提出了FT-Shield watermarking方法，通过在文本到图像散发模型的训练图像上生成一个特有的水印，以便在文本到图像散发模型生成的图像上快速和准确地检测水印</li>
<li>results: 通过实验证明FT-Shield可以有效地检测文本到图像散发模型的违用数据泄露问题<details>
<summary>Abstract</summary>
Text-to-image generative models based on latent diffusion models (LDM) have demonstrated their outstanding ability in generating high-quality and high-resolution images according to language prompt. Based on these powerful latent diffusion models, various fine-tuning methods have been proposed to achieve the personalization of text-to-image diffusion models such as artistic style adaptation and human face transfer. However, the unauthorized usage of data for model personalization has emerged as a prevalent concern in relation to copyright violations. For example, a malicious user may use the fine-tuning technique to generate images which mimic the style of a painter without his/her permission. In light of this concern, we have proposed FT-Shield, a watermarking approach specifically designed for the fine-tuning of text-to-image diffusion models to aid in detecting instances of infringement. We develop a novel algorithm for the generation of the watermark to ensure that the watermark on the training images can be quickly and accurately transferred to the generated images of text-to-image diffusion models. A watermark will be detected on an image by a binary watermark detector if the image is generated by a model that has been fine-tuned using the protected watermarked images. Comprehensive experiments were conducted to validate the effectiveness of FT-Shield.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ScaleNet-An-Unsupervised-Representation-Learning-Method-for-Limited-Information"><a href="#ScaleNet-An-Unsupervised-Representation-Learning-Method-for-Limited-Information" class="headerlink" title="ScaleNet: An Unsupervised Representation Learning Method for Limited Information"></a>ScaleNet: An Unsupervised Representation Learning Method for Limited Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02386">http://arxiv.org/abs/2310.02386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huili Huang, M. Mahdi Roozbahani</li>
<li>for: 增强深度卷积神经网络（ConvNet）在有限信息情况下学习高级别semantic视觉表示。</li>
<li>methods: 基于多尺度图像的简单和高效无监督表示学习方法，即ScaleNet，以提高ConvNet的性能。</li>
<li>results: ScaleNet在CIFAR-10和ImageNet datasets上，使用不同的architecture（如AlexNet和ResNet50），在限定数据量情况下，提高了ConvNet的 rotation-prediction 任务性能，并且可以超过RotNet模型。 transferred parameters from a ScaleNet model with limited data also improve the ImageNet Classification task by about 6% compared to the RotNet model.<details>
<summary>Abstract</summary>
Although large-scale labeled data are essential for deep convolutional neural networks (ConvNets) to learn high-level semantic visual representations, it is time-consuming and impractical to collect and annotate large-scale datasets. A simple and efficient unsupervised representation learning method named ScaleNet based on multi-scale images is proposed in this study to enhance the performance of ConvNets when limited information is available. The input images are first resized to a smaller size and fed to the ConvNet to recognize the rotation degree. Next, the ConvNet learns the rotation-prediction task for the original size images based on the parameters transferred from the previous model. The CIFAR-10 and ImageNet datasets are examined on different architectures such as AlexNet and ResNet50 in this study. The current study demonstrates that specific image features, such as Harris corner information, play a critical role in the efficiency of the rotation-prediction task. The ScaleNet supersedes the RotNet by ~7% in the limited CIFAR-10 dataset. The transferred parameters from a ScaleNet model with limited data improve the ImageNet Classification task by about 6% compared to the RotNet model. This study shows the capability of the ScaleNet method to improve other cutting-edge models such as SimCLR by learning effective features for classification tasks.
</details>
<details>
<summary>摘要</summary>
although large-scale labeled data are essential for deep convolutional neural networks (ConvNets) to learn high-level semantic visual representations, it is time-consuming and impractical to collect and annotate large-scale datasets. a simple and efficient unsupervised representation learning method named ScaleNet based on multi-scale images is proposed in this study to enhance the performance of ConvNets when limited information is available. the input images are first resized to a smaller size and fed to the ConvNet to recognize the rotation degree. next, the ConvNet learns the rotation-prediction task for the original size images based on the parameters transferred from the previous model. the CIFAR-10 and ImageNet datasets are examined on different architectures such as AlexNet and ResNet50 in this study. the current study demonstrates that specific image features, such as harris corner information, play a critical role in the efficiency of the rotation-prediction task. the ScaleNet supersedes the RotNet by ~7% in the limited CIFAR-10 dataset. the transferred parameters from a ScaleNet model with limited data improve the ImageNet Classification task by about 6% compared to the RotNet model. this study shows the capability of the ScaleNet method to improve other cutting-edge models such as SimCLR by learning effective features for classification tasks.
</details></li>
</ul>
<hr>
<h2 id="Multi-Prompt-Fine-Tuning-of-Foundation-Models-for-Enhanced-Medical-Image-Segmentation"><a href="#Multi-Prompt-Fine-Tuning-of-Foundation-Models-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Multi-Prompt Fine-Tuning of Foundation Models for Enhanced Medical Image Segmentation"></a>Multi-Prompt Fine-Tuning of Foundation Models for Enhanced Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02381">http://arxiv.org/abs/2310.02381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiangru Li, Yifei Zhang, Liang Zhao</li>
<li>for: 提高医学图像分割表现</li>
<li>methods: 利用SAM模型的多个提示批处理功能，并在批处理中使用两个标注的 bounding box 作为参考</li>
<li>results: 在多种分割任务上显著提高表现指标<details>
<summary>Abstract</summary>
The Segment Anything Model (SAM) is a powerful foundation model that introduced revolutionary advancements in natural image segmentation. However, its performance remains sub-optimal when delineating the intricate structure of biomedical images, where multiple organs and tissues intertwine in a single image. In this study, we introduce a novel fine-tuning framework that leverages SAM's ability to bundle and process multiple prompts per image and seeks to improve SAM's performance in medical images. We first curated a medical image dataset that consists of CT scans of lesions in various organs, each with two annotations for organs and lesions respectively. Then, we fine-tuned SAM's mask decoder within our framework by batching both bounding boxes generated from ground truth masks as reference. The batched prompt strategy we introduced not only addresses the inherent complexity and ambiguity often found in medical images but also substantially enhances performance metrics when applied onto a wide range of segmentation tasks.
</details>
<details>
<summary>摘要</summary>
“对于自然图像分割，Segment Anything Model（SAM）是一个具有革命性的基础模型。然而，它在医学影像中的性能仍然偏低，因为该区域内有多个器官和组织相互纠缠。在这个研究中，我们介绍了一个新的精确化框架，它利用SAM将多个提示集成一个影像，并对SAM的面瘫处理器进行了修正。我们首先从各种器官的CT扫描图中组建了医学影像集，每个图片都有两个标注：一个是器官的 bounding box，另一个是病变的 bounding box。然后，我们在我们的框架中进行了SAM的面瘫处理器的精确化，使用批处理 Both bounding boxes 生成自真实标注masks作为参考。我们称之为批处理 Both bounding boxes 策略，这策略不��ely addressing the inherent complexity and ambiguity often found in medical images, but also significantly enhances performance metrics when applied to a wide range of segmentation tasks。”
</details></li>
</ul>
<hr>
<h2 id="DREAM-Visual-Decoding-from-Reversing-Human-Visual-System"><a href="#DREAM-Visual-Decoding-from-Reversing-Human-Visual-System" class="headerlink" title="DREAM: Visual Decoding from Reversing Human Visual System"></a>DREAM: Visual Decoding from Reversing Human Visual System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02265">http://arxiv.org/abs/2310.02265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weihao Xia, Raoul de Charette, Cengiz Öztireli, Jing-Hao Xue</li>
<li>For: 本研究团队开发了一种基于人类视觉系统知识的 fMRI-to-image 方法，可以从大脑活动中重建视图图像。* Methods: 这种方法使用了人类视觉系统中的层次和平行结构，通过两个特定的组件来模拟人类视觉系统内部的反向过程：Reverse Visual Association Cortex (R-VAC) 和 Reverse Parallel PKM (R-PKM)。* Results: 实验结果表明，这种方法在 терms of 出现 consistency, 结构和 semantics 方面表现更好于当前州态艺的模型。<details>
<summary>Abstract</summary>
In this work we present DREAM, an fMRI-to-image method for reconstructing viewed images from brain activities, grounded on fundamental knowledge of the human visual system. We craft reverse pathways that emulate the hierarchical and parallel nature of how humans perceive the visual world. These tailored pathways are specialized to decipher semantics, color, and depth cues from fMRI data, mirroring the forward pathways from visual stimuli to fMRI recordings. To do so, two components mimic the inverse processes within the human visual system: the Reverse Visual Association Cortex (R-VAC) which reverses pathways of this brain region, extracting semantics from fMRI data; the Reverse Parallel PKM (R-PKM) component simultaneously predicting color and depth from fMRI signals. The experiments indicate that our method outperforms the current state-of-the-art models in terms of the consistency of appearance, structure, and semantics. Code will be made publicly available to facilitate further research in this field.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了DREAM方法，它可以从脑活动中重建被观看的图像，基于人类视觉系统的基本知识。我们设计了逆行Pathways，这些Pathways模拟了人类视觉系统的层次和平行结构，从fMRI数据中提取 semantics、色彩和深度信息。为此，我们采用了两个组件：倒转视觉关联区（R-VAC）和倒转平行PKM（R-PKM）组件，它们分别模拟了人类视觉系统内的逆向过程。实验结果显示，我们的方法在 struttural consistency、semantic consistency和appearance consistency等方面都有较高的表现，比之前的状态艺方法更高。我们将代码公开发布，以便进一步的研究在这个领域。Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="RSRD-A-Road-Surface-Reconstruction-Dataset-and-Benchmark-for-Safe-and-Comfortable-Autonomous-Driving"><a href="#RSRD-A-Road-Surface-Reconstruction-Dataset-and-Benchmark-for-Safe-and-Comfortable-Autonomous-Driving" class="headerlink" title="RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and Comfortable Autonomous Driving"></a>RSRD: A Road Surface Reconstruction Dataset and Benchmark for Safe and Comfortable Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02262">http://arxiv.org/abs/2310.02262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Zhao, Chenfeng Xu, Mingyu Ding, Masayoshi Tomizuka, Wei Zhan, Yintao Wei</li>
<li>for: 本研究旨在满足智能机器人系统中安全性和舒适性的增长需求, 特别是自动驾驶车辆, 其中道路条件对整体驾驶性能产生重要影响。</li>
<li>methods: 我们引入了高分辨率和高精度的道路表面重建数据集(RSRD), 该数据集在多种驾驶条件下收集到了约16,000对双眼图像、原始点云和真实的深度&#x2F;相差图像, 并通过精心的后处理管道来保证其质量。</li>
<li>results: 基于RSRD数据集, 我们建立了一个全面的道路表面重建 benchmark, 通过深度估计和双眼匹配来恢复道路profile。 初步的评估表明, RSRD 数据集和相关技术具有很大的潜在价值, 可以用于提高多视角镜头技术等安全自动驾驶技术的发展。<details>
<summary>Abstract</summary>
This paper addresses the growing demands for safety and comfort in intelligent robot systems, particularly autonomous vehicles, where road conditions play a pivotal role in overall driving performance. For example, reconstructing road surfaces helps to enhance the analysis and prediction of vehicle responses for motion planning and control systems. We introduce the Road Surface Reconstruction Dataset (RSRD), a real-world, high-resolution, and high-precision dataset collected with a specialized platform in diverse driving conditions. It covers common road types containing approximately 16,000 pairs of stereo images, original point clouds, and ground-truth depth/disparity maps, with accurate post-processing pipelines to ensure its quality. Based on RSRD, we further build a comprehensive benchmark for recovering road profiles through depth estimation and stereo matching. Preliminary evaluations with various state-of-the-art methods reveal the effectiveness of our dataset and the challenge of the task, underscoring substantial opportunities of RSRD as a valuable resource for advancing techniques, e.g., multi-view stereo towards safe autonomous driving. The dataset and demo videos are available at https://thu-rsxd.com/rsrd/
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Talk2BEV-Language-enhanced-Bird’s-eye-View-Maps-for-Autonomous-Driving"><a href="#Talk2BEV-Language-enhanced-Bird’s-eye-View-Maps-for-Autonomous-Driving" class="headerlink" title="Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving"></a>Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02251">http://arxiv.org/abs/2310.02251</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikrant Dewangan, Tushar Choudhary, Shivam Chandhok, Shubham Priyadarshan, Anushka Jain, Arun K. Singh, Siddharth Srivastava, Krishna Murthy Jatavallabhula, K. Madhava Krishna</li>
<li>for: 这个论文主要是为了提供一个大型视力语言模型（LVLM）接口，用于自动驾驶场景中的 bird’s-eye view（BEV）地图。</li>
<li>methods: 这个论文使用了现代的通用语言和视觉模型，以及BEV结构化地图表示，从而消除了需要任务特定模型的需求。</li>
<li>results: 这个论文通过对大量的Scene理解任务进行广泛的评估，证明了Talk2BEV可以在自动驾驶场景中执行视觉和空间理解、预测交通actor的意图以及基于视觉cue的决策。<details>
<summary>Abstract</summary>
Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representations, eliminating the need for task-specific models. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret free-form natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset.
</details>
<details>
<summary>摘要</summary>
talk2bev 是一个大型视觉语言模型（LVLM）接口，用于自动驾驶场景中的 bird's-eye view（BEV）地图。而现有的自动驾驶场景识别系统大多集中在固定的对象类划定和驾驶场景上，而 talk2bev 则将最新的通用语言和视觉模型与 BEV 结构化地图表示相结合，从而消除需要任务特定模型的需求。这使得单个系统可以处理多种自动驾驶任务，包括视觉和空间逻辑、预测交通员的意图以及基于视觉提示的决策。我们对 talk2bev 进行了广泛的场景理解测试，测试包括解析自然语言查询的能力和将查询grounding到语言增强的 BEV 地图中。为了推动更多关于 LVLM 的研究在自动驾驶场景中，我们开发了 talk2bev-bench，一个包含 1000 个人注释的 BEV 场景 benchmark，包括超过 20,000 个问题和 NuScenes 数据集中的真实答案。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Generation-of-Human-Object-Interactions-with-Diffusion-Probabilistic-Models"><a href="#Hierarchical-Generation-of-Human-Object-Interactions-with-Diffusion-Probabilistic-Models" class="headerlink" title="Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models"></a>Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02242">http://arxiv.org/abs/2310.02242</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju3dv/hghoi">https://github.com/zju3dv/hghoi</a></li>
<li>paper_authors: Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, Hujun Bao</li>
<li>for: 本研究旨在解决现有的自动进行式模型和走道规划方法无法满足的长距离多样动作生成挑战。</li>
<li>methods: 我们提出了一种层次生成框架，首先生成一系列的标点，然后将动作Synthesize along them。因此，长距离动作生成可以被减少到几个短动作序列的组合，受标点的指导。</li>
<li>results: 我们的方法在NSM、COUCH和SAMP数据集上进行实验，与之前的方法相比，表现出较大的优势， both in terms of quality and diversity。<details>
<summary>Abstract</summary>
This paper presents a novel approach to generating the 3D motion of a human interacting with a target object, with a focus on solving the challenge of synthesizing long-range and diverse motions, which could not be fulfilled by existing auto-regressive models or path planning-based methods. We propose a hierarchical generation framework to solve this challenge. Specifically, our framework first generates a set of milestones and then synthesizes the motion along them. Therefore, the long-range motion generation could be reduced to synthesizing several short motion sequences guided by milestones. The experiments on the NSM, COUCH, and SAMP datasets show that our approach outperforms previous methods by a large margin in both quality and diversity. The source code is available on our project page https://zju3dv.github.io/hghoi.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的方法来生成人类与目标对象之间的3D运动，强调解决现有的滚动式模型或路径规划基本方法无法实现的远程多样化运动挑战。我们提议了层次生成框架来解决这个挑战。具体来说，我们的框架首先生成一组里程碑，然后将运动synthesize到这些里程碑上。因此，长距离运动生成可以被减少到几个短距离运动序列的指导下进行synthesize。实验结果表明，我们的方法在NSM、COUCH和SAMP数据集上比前方法大幅提高了质量和多样性。代码可以在我们项目页面（https://zju3dv.github.io/hghoi）上获取。
</details></li>
</ul>
<hr>
<h2 id="Learnable-Data-Augmentation-for-One-Shot-Unsupervised-Domain-Adaptation"><a href="#Learnable-Data-Augmentation-for-One-Shot-Unsupervised-Domain-Adaptation" class="headerlink" title="Learnable Data Augmentation for One-Shot Unsupervised Domain Adaptation"></a>Learnable Data Augmentation for One-Shot Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02201">http://arxiv.org/abs/2310.02201</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iit-pavis/learnaug-uda">https://github.com/iit-pavis/learnaug-uda</a></li>
<li>paper_authors: Julio Ivan Davila Carrazco, Pietro Morerio, Alessio Del Bue, Vittorio Murino</li>
<li>for: 解决一个难度最大的领域适应问题（One-Shot Unsupervised Domain Adaptation，OS-UDA），即只有一个目标样本可用 для模型适应。</li>
<li>methods: 提出了一种学习数据增强框架，通过将源数据变换为类似于目标数据的形式，使得基于这种增强数据的分类器在目标领域中具有良好的泛化能力。</li>
<li>results: 在DomainNet和VisDA两个领域适应测试 benchmark 上达到了当前最佳性能。Here is the same information in English:</li>
<li>for: Solving the most challenging setting in Domain Adaptation, where only one single unlabeled target sample is available for model adaptation.</li>
<li>methods: Propose a classification framework based on learnable data augmentation, which transforms source data into a form similar to the target data, enabling a classifier trained on the augmented data to generalize well to the target domain.</li>
<li>results: Achieve state-of-the-art performance on two well-known Domain Adaptation benchmarks, DomainNet and VisDA.<details>
<summary>Abstract</summary>
This paper presents a classification framework based on learnable data augmentation to tackle the One-Shot Unsupervised Domain Adaptation (OS-UDA) problem. OS-UDA is the most challenging setting in Domain Adaptation, as only one single unlabeled target sample is assumed to be available for model adaptation. Driven by such single sample, our method LearnAug-UDA learns how to augment source data, making it perceptually similar to the target. As a result, a classifier trained on such augmented data will generalize well for the target domain. To achieve this, we designed an encoder-decoder architecture that exploits a perceptual loss and style transfer strategies to augment the source data. Our method achieves state-of-the-art performance on two well-known Domain Adaptation benchmarks, DomainNet and VisDA. The project code is available at https://github.com/IIT-PAVIS/LearnAug-UDA
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种基于可学习数据扩展的分类框架，用于解决一架不监督领域适应（OS-UDA）问题。OS-UDA是领域适应中最为困难的设定，只有一个单个目标样本可用于模型适应。我们的方法LearnAug-UDA通过使用单个目标样本来学习扩展源数据，使其与目标数据变得感知上相似。因此，基于这种扩展数据的分类器将在目标领域中进行良好的泛化。为实现此目标，我们设计了一个Encoder-Decoder架构，利用感知损失和风格传递策略来扩展源数据。我们的方法在DomainNet和VisDA两个领域适应 benchmark 上实现了状态的表现，代码可以在https://github.com/IIT-PAVIS/LearnAug-UDA 上下载。
</details></li>
</ul>
<hr>
<h2 id="PAD-Phys-Exploiting-Physiology-for-Presentation-Attack-Detection-in-Face-Biometrics"><a href="#PAD-Phys-Exploiting-Physiology-for-Presentation-Attack-Detection-in-Face-Biometrics" class="headerlink" title="PAD-Phys: Exploiting Physiology for Presentation Attack Detection in Face Biometrics"></a>PAD-Phys: Exploiting Physiology for Presentation Attack Detection in Face Biometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02140">http://arxiv.org/abs/2310.02140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis F. Gomez, Julian Fierrez, Aythami Morales, Mahdi Ghafourian, Ruben Tolosana, Imanol Solano, Alejandro Garcia, Francisco Zamora-Martinez</li>
<li>for: 防止人脸识别系统中的个人信息泄露或身份识别 spoofing</li>
<li>methods: 基于远程光谱 Plethysmography (rPPG) 的脉冲检测</li>
<li>results: 比较三种方法的结果，即生物域、深伪域和新的演示攻击域，结果显示 rPPG 基于模型的脉冲检测效果好，ACER 降低了 21.70%（从 41.03% 降低到 19.32%）。<details>
<summary>Abstract</summary>
Presentation Attack Detection (PAD) is a crucial stage in facial recognition systems to avoid leakage of personal information or spoofing of identity to entities. Recently, pulse detection based on remote photoplethysmography (rPPG) has been shown to be effective in face presentation attack detection.   This work presents three different approaches to the presentation attack detection based on rPPG: (i) The physiological domain, a domain using rPPG-based models, (ii) the Deepfakes domain, a domain where models were retrained from the physiological domain to specific Deepfakes detection tasks; and (iii) a new Presentation Attack domain was trained by applying transfer learning from the two previous domains to improve the capability to differentiate between bona-fides and attacks.   The results show the efficiency of the rPPG-based models for presentation attack detection, evidencing a 21.70% decrease in average classification error rate (ACER) (from 41.03% to 19.32%) when the presentation attack domain is compared to the physiological and Deepfakes domains. Our experiments highlight the efficiency of transfer learning in rPPG-based models and perform well in presentation attack detection in instruments that do not allow copying of this physiological feature.
</details>
<details>
<summary>摘要</summary>
本工作介绍了三种不同的面向攻击检测方法 based on rPPG：1. 生物学领域，使用 rPPG 基于模型，检测面向攻击。2. Deepfakes 领域， retrained 模型从生物学领域到特定的 Deepfakes 检测任务。3. 一个新的面向攻击领域，通过转移学习从前两个领域提高了区分真实和攻击的能力。结果显示 rPPG 基于模型在面向攻击检测中的效果， ACER （平均分类错误率）下降了 21.70%（从 41.03% 降至 19.32%），当面向攻击领域与生物学和 Deepfakes 领域进行比较时。我们的实验表明 rPPG 基于模型的转移学习能够在不允许 copying 生物学特征的 instrumente 中表现出色。
</details></li>
</ul>
<hr>
<h2 id="SIEVE-Multimodal-Dataset-Pruning-Using-Image-Captioning-Models"><a href="#SIEVE-Multimodal-Dataset-Pruning-Using-Image-Captioning-Models" class="headerlink" title="SIEVE: Multimodal Dataset Pruning Using Image Captioning Models"></a>SIEVE: Multimodal Dataset Pruning Using Image Captioning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02110">http://arxiv.org/abs/2310.02110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, Ari Morcos</li>
<li>for: 这 paper 的目的是提出一种新的数据筛选方法，以提高 vision-language 模型（VLM）的性能。</li>
<li>methods: 该方法使用生成的文本描述和语言模型来评估图像和文本对应的一致性，并使用数据COMP的多模态筛选 benchmark 来评估其性能。</li>
<li>results: 该方法在大规模池中实现了状态机器的性能，并在中等规模池中获得了竞争性的结果，比 CLIPScore 基于的筛选方法提高了1.7%和2.6%的平均性能。<details>
<summary>Abstract</summary>
Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy web-crawled datasets. This underscores the critical need for dataset pruning, as the quality of these datasets is strongly correlated with the performance of VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train models using highly-aligned samples is one of the most successful methods for pruning.We argue that this approach suffers from multiple limitations including: 1) false positives due to spurious correlations captured by the pretrained CLIP model, 2) false negatives due to poor discrimination between hard and bad samples, and 3) biased ranking towards samples similar to the pretrained CLIP dataset. We propose a pruning method, SIEVE, that employs synthetic captions generated by image-captioning models pretrained on small, diverse, and well-aligned image-text pairs to evaluate the alignment of noisy image-text pairs. To bridge the gap between the limited diversity of generated captions and the high diversity of alternative text (alt-text), we estimate the semantic textual similarity in the embedding space of a language model pretrained on billions of sentences. Using DataComp, a multimodal dataset filtering benchmark, we achieve state-of-the-art performance on the large scale pool, and competitive results on the medium scale pool, surpassing CLIPScore-based filtering by 1.7% and 2.6% on average, on 38 downstream tasks.
</details>
<details>
<summary>摘要</summary>
视力语言模型（VLM）通常预训练在大量、多样化和噪音的网络抓取数据上。这种情况下，数据剔除变得非常重要，因为数据质量直接影响下游任务的性能。使用CLIPScore从预训练模型中只训练使用高度对应的样本是一种非常成功的剔除方法。然而，我们认为这种方法受到多种限制，包括：1）由预训练CLIP模型捕捉的假阳性，2）由低精度的样本分类错失，和3）偏向于与预训练CLIP数据集类似的样本排名。我们提议一种剔除方法，即SIEVE，该方法使用由图像描述模型预训练在小型、多样化和匹配的图像文本对中生成的文本来评估噪音图像文本的对应度。为了补偿生成文本的有限多样性，我们利用一种语言模型预训练 на千万句 sentences的embedding空间内的 semantic textual similarity来估计。使用DataComp，一种多模式数据筛选 benchmark，我们在大规模池中实现了状态机器的性能，并在中等规模池中获得了竞争性的结果，高于CLIPScore-based filtering的平均性能by 1.7%和2.6%，在38个下游任务上。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Classic-Deconvolution-and-Feature-Extraction-in-Zero-Shot-Image-Restoration"><a href="#Leveraging-Classic-Deconvolution-and-Feature-Extraction-in-Zero-Shot-Image-Restoration" class="headerlink" title="Leveraging Classic Deconvolution and Feature Extraction in Zero-Shot Image Restoration"></a>Leveraging Classic Deconvolution and Feature Extraction in Zero-Shot Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02097">http://arxiv.org/abs/2310.02097</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ctom2/cider">https://github.com/ctom2/cider</a></li>
<li>paper_authors: Tomáš Chobola, Gesine Müller, Veit Dausmann, Anton Theileis, Jan Taucher, Jan Huisken, Tingying Peng</li>
<li>for: 非Mask-based非透明恢复图像</li>
<li>methods: 组合深度学习和经典迭代恢复算法</li>
<li>results: 在实际应用中显示了明显的改善<details>
<summary>Abstract</summary>
Non-blind deconvolution aims to restore a sharp image from its blurred counterpart given an obtained kernel. Existing deep neural architectures are often built based on large datasets of sharp ground truth images and trained with supervision. Sharp, high quality ground truth images, however, are not always available, especially for biomedical applications. This severely hampers the applicability of current approaches in practice. In this paper, we propose a novel non-blind deconvolution method that leverages the power of deep learning and classic iterative deconvolution algorithms. Our approach combines a pre-trained network to extract deep features from the input image with iterative Richardson-Lucy deconvolution steps. Subsequently, a zero-shot optimisation process is employed to integrate the deconvolved features, resulting in a high-quality reconstructed image. By performing the preliminary reconstruction with the classic iterative deconvolution method, we can effectively utilise a smaller network to produce the final image, thus accelerating the reconstruction whilst reducing the demand for valuable computational resources. Our method demonstrates significant improvements in various real-world applications non-blind deconvolution tasks.
</details>
<details>
<summary>摘要</summary>
非目的减推算器目的是从模糊图像中还原锐利图像，给出的核函数。现有的深度神经架构 часто基于大量锐利真实图像的 datasets 和监督学习。然而，锐利、高质量的真实图像在生物医学应用中并不总是可得，特别是在实际应用中。这会严重限制现有方法的应用。在本文中，我们提出了一种新的非目的减推算法，利用深度学习和经典的迭代减推算法。我们的方法首先使用预训练的网络提取输入图像的深度特征，然后使用迭代的理查森-卢西减推算步骤。最后，我们使用零截 optimization 进程将减推后的特征集成，得到高质量重建图像。通过在经典迭代减推算法前进行初步重建，我们可以更好地利用更小的网络生成最终图像，从而加速重建，降低计算资源的需求。我们的方法在非目的减推应用中表现出了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Global-Attractor-for-a-Reaction-Diffusion-Model-Arising-in-Biological-Dynamic-in-3D-Soil-Structure"><a href="#Global-Attractor-for-a-Reaction-Diffusion-Model-Arising-in-Biological-Dynamic-in-3D-Soil-Structure" class="headerlink" title="Global Attractor for a Reaction-Diffusion Model Arising in Biological Dynamic in 3D Soil Structure"></a>Global Attractor for a Reaction-Diffusion Model Arising in Biological Dynamic in 3D Soil Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02060">http://arxiv.org/abs/2310.02060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Elghandouri, Khalil Ezzinbi, Mouad Klai, Olivier Monga</li>
<li>for: 本研究用partial differential equations (PDEs) 模型三维土壤结构中微生物活动的复杂过程，为生物领域提供有价值的理解。</li>
<li>methods: 本研究使用PDEs模型和数值价值计算来研究微生物活动的存在和唯一性，以及相应模型的极限行为。</li>
<li>results: 研究发现了一个全球吸引器，这是一个重要的系统行为特征，有助于理解长期系统的行为。数值价值计算也用于较之这个全球吸引器的特性。<details>
<summary>Abstract</summary>
Partial Differential Equations (PDEs) play a crucial role as tools for modeling and comprehending intricate natural processes, notably within the domain of biology. This research explores the domain of microbial activity within the complex matrix of 3D soil structures, providing valuable understanding into both the existence and uniqueness of solutions and the asymptotic behavior of the corresponding PDE model. Our investigation results in the discovery of a global attractor, a fundamental feature with significant implications for long-term system behavior. To enhance the clarity of our findings, numerical simulations are employed to visually illustrate the attributes of this global attractor.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-Generalisability-of-Self-Distillation-with-No-Labels-for-SAR-Based-Vegetation-Prediction"><a href="#Exploring-Generalisability-of-Self-Distillation-with-No-Labels-for-SAR-Based-Vegetation-Prediction" class="headerlink" title="Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction"></a>Exploring Generalisability of Self-Distillation with No Labels for SAR-Based Vegetation Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02048">http://arxiv.org/abs/2310.02048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Martínez-Ferrer, Anna Jungbluth, Joseph A. Gallego-Mejia, Matt Allen, Francisco Dorr, Freddie Kalaitzis, Raúl Ramos-Pollán</li>
<li>for: 本研究使用两个Synthetic Aperture Radar数据集（S1GRD或GSSIC）在三个地区（中国、Conus、欧洲）预训练了一个DINO-ViT基本模型，并对其进行了精度调整以预测植被百分比。</li>
<li>methods: 本研究使用了预训练模型，并对其进行了精度调整以预测植被百分比。</li>
<li>results: 研究发现，S1GRD中不同地区的嵌入空间明显分离，而GSSIC中嵌入空间受到干扰。在精度调整过程中，位置特征保持不变，而较不熟悉的地区的误差通常高于 Familiar 地区。这些结果可以帮助我们更好地理解自动学习模型在远程感知领域的普适性。<details>
<summary>Abstract</summary>
In this work we pre-train a DINO-ViT based model using two Synthetic Aperture Radar datasets (S1GRD or GSSIC) across three regions (China, Conus, Europe). We fine-tune the models on smaller labeled datasets to predict vegetation percentage, and empirically study the connection between the embedding space of the models and their ability to generalize across diverse geographic regions and to unseen data. For S1GRD, embedding spaces of different regions are clearly separated, while GSSIC's overlaps. Positional patterns remain during fine-tuning, and greater distances in embeddings often result in higher errors for unfamiliar regions. With this, our work increases our understanding of generalizability for self-supervised models applied to remote sensing.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们预训练了基于DINO-ViT的模型使用两个Synthetic Aperture Radar数据集（S1GRD或GSSIC）在三个地区（中国、Conus、欧洲）。我们精度地调整模型以预测植被百分数，并实际研究模型 embedding 空间和其能否通过不同地理区域和未见数据进行泛化。对于S1GRD，不同地区的 embedding 空间明显分离，而GSSIC 的 embedding 空间存在重叠。位置特征在精度调整中保持不变，并且更大的 embedding 距离通常会导致对不熟悉地区的误差更高。因此，我们的工作将自助学习模型在遥感技术中的泛化性进一步了解。
</details></li>
</ul>
<hr>
<h2 id="Video-Transformers-under-Occlusion-How-Physics-and-Background-Attributes-Impact-Large-Models-for-Robotic-Manipulation"><a href="#Video-Transformers-under-Occlusion-How-Physics-and-Background-Attributes-Impact-Large-Models-for-Robotic-Manipulation" class="headerlink" title="Video Transformers under Occlusion: How Physics and Background Attributes Impact Large Models for Robotic Manipulation"></a>Video Transformers under Occlusion: How Physics and Background Attributes Impact Large Models for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02044">http://arxiv.org/abs/2310.02044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shutongjin/occlumanip">https://github.com/shutongjin/occlumanip</a></li>
<li>paper_authors: Shutong Jin, Ruiyu Wang, Muhammad Zahid, Florian T. Pokorny</li>
<li>for: 本研究旨在 investigating the influence of object physics attributes and background characteristics on the performance of Video Transformers in trajectory prediction tasks under occlusion.</li>
<li>methods: 本研究使用了 OccluManip  dataset，一个包含不同物理属性和背景特征的物体 occlusion 视频数据集，并提出了 Video Occlusion Transformer (VOT) 网络，一种通用的视频变换器基于网络，实现了平均96%的准确率在所有18个子数据集中。</li>
<li>results: 研究发现，物体物理属性和背景特征均对 Video Transformers 的性能产生了重要影响，并且提出了一些可能导致模型减少性能的原因。此外，研究还发现了一个数据满足点，在这个点上，大型变换器模型的性能不再提高。<details>
<summary>Abstract</summary>
As transformer architectures and dataset sizes continue to scale, the need to understand the specific dataset factors affecting model performance becomes increasingly urgent. This paper investigates how object physics attributes (color, friction coefficient, shape) and background characteristics (static, dynamic, background complexity) influence the performance of Video Transformers in trajectory prediction tasks under occlusion. Beyond mere occlusion challenges, this study aims to investigate three questions: How do object physics attributes and background characteristics influence the model performance? What kinds of attributes are most influential to the model generalization? Is there a data saturation point for large transformer model performance within a single task? To facilitate this research, we present OccluManip, a real-world video-based robot pushing dataset comprising 460,000 consistent recordings of objects with different physics and varying backgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible temporal length along with target object trajectories are collected, accommodating tasks with different temporal requirements. Additionally, we propose Video Occlusion Transformer (VOT), a generic video-transformer-based network achieving an average 96% accuracy across all 18 sub-datasets provided in OccluManip. OccluManip and VOT will be released at: https://github.com/ShutongJIN/OccluManip.git
</details>
<details>
<summary>摘要</summary>
As transformer architectures and dataset sizes continue to scale, understanding the specific dataset factors affecting model performance becomes increasingly urgent. This paper investigates how object physics attributes (color, friction coefficient, shape) and background characteristics (static, dynamic, background complexity) influence the performance of Video Transformers in trajectory prediction tasks under occlusion. Beyond mere occlusion challenges, this study aims to investigate three questions: How do object physics attributes and background characteristics influence the model performance? What kinds of attributes are most influential to the model generalization? Is there a data saturation point for large transformer model performance within a single task? To facilitate this research, we present OccluManip, a real-world video-based robot pushing dataset comprising 460,000 consistent recordings of objects with different physics and varying backgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible temporal length along with target object trajectories are collected, accommodating tasks with different temporal requirements. Additionally, we propose Video Occlusion Transformer (VOT), a generic video-transformer-based network achieving an average 96% accuracy across all 18 sub-datasets provided in OccluManip. OccluManip and VOT will be released at: https://github.com/ShutongJIN/OccluManip.gitHere's the translation in Traditional Chinese:当 transformer 架构和数据集大小继续扩大时，理解特定数据集因素对模型性能的影响变得越来越重要。本研究探讨影片变数（颜色、摩擦系数、形状）和背景特征（静止、动态、背景复杂度）对 Video Transformers 在遮蔽 task 中的性能影响。 beyond 遮蔽挑战，本研究旨在 investigate 三个问题：影片变数和背景特征对模型性能影响多大？这些特征在模型通用化中多重影响吗？是否存在单一任务中大 transformer 模型性能的数据饱和点？ To facilitate this research, we present OccluManip, a real-world video-based robot pushing dataset comprising 460,000 consistent recordings of objects with different physics and varying backgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible temporal length along with target object trajectories are collected, accommodating tasks with different temporal requirements. Additionally, we propose Video Occlusion Transformer (VOT), a generic video-transformer-based network achieving an average 96% accuracy across all 18 sub-datasets provided in OccluManip. OccluManip and VOT will be released at: https://github.com/ShutongJIN/OccluManip.git
</details></li>
</ul>
<hr>
<h2 id="Decoding-Human-Activities-Analyzing-Wearable-Accelerometer-and-Gyroscope-Data-for-Activity-Recognition"><a href="#Decoding-Human-Activities-Analyzing-Wearable-Accelerometer-and-Gyroscope-Data-for-Activity-Recognition" class="headerlink" title="Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition"></a>Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02011">http://arxiv.org/abs/2310.02011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utsab Saha, Sawradip Saha, Tahmid Kabir, Shaikh Anowarul Fattah, Mohammad Saquib</li>
<li>for: 本研究旨在提出一种基于Residual网络和Residual MobileNet的多结构层合并方法，用于分类人类活动。</li>
<li>methods: 该方法使用了特制的Residual块进行分类static和dynamic活动，并将这两个模型独立地训练。之后，这两个ResNet被通过权重加权ensemble来结合，以便更好地分类特定的静止和动态活动。</li>
<li>results: 在UCI HAR和Motion-Sense等两个公共数据集上进行测试，该方法能够成功处理数据重叠的混淆情况，并实现了state-of-the-art的准确率96.71%和95.35%。<details>
<summary>Abstract</summary>
A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a specific static and a specific dynamic activity, which were previously identified based on their distinct feature characteristics in the earlier stage. The proposed model is evaluated using two publicly accessible datasets; namely, UCI HAR and Motion-Sense. Therein, it successfully handled the highly confusing cases of data overlap. Therefore, the proposed approach achieves a state-of-the-art accuracy of 96.71% and 95.35% in the UCI HAR and Motion-Sense datasets respectively.
</details>
<details>
<summary>摘要</summary>
人体运动或相对位置生成的Raw电信号可以由计算机机器读取并应用不同的涉及技术来分类不同的人类活动。在这篇论文中，我们提出了一种多结构层次方法，称为FusionActNet，使用了 residual网络和 residual MobileNet的 ensemble。这种方法使用了特制的 residual块来分类静止和动态活动，因为它们在特征上有清晰的分化。这两个网络独立地训练，从而生成了两个高度精度的模型。这些模型可以通过特有的算法优点来识别活动内一个特定超类。接着，这两个 residual 网络通过权重 ensemble 的 residual MobileNet 进行混合，从而高效地区分静止和动态活动。在使用两个公共可访问的数据集（UC Irvine HAR和Motion-Sense）进行评估时，该方法成功处理了数据重叠的高度混淆情况。因此，我们的方法实现了状态的最佳准确率为96.71%和95.35%在UC Irvine HAR和Motion-Sense数据集中。
</details></li>
</ul>
<hr>
<h2 id="MUSCLE-Multi-task-Self-supervised-Continual-Learning-to-Pre-train-Deep-Models-for-X-ray-Images-of-Multiple-Body-Parts"><a href="#MUSCLE-Multi-task-Self-supervised-Continual-Learning-to-Pre-train-Deep-Models-for-X-ray-Images-of-Multiple-Body-Parts" class="headerlink" title="MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts"></a>MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02000">http://arxiv.org/abs/2310.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weibin Liao, Haoyi Xiong, Qingzhong Wang, Yan Mo, Xuhong Li, Yi Liu, Zeyu Chen, Siyu Huang, Dejing Dou</li>
<li>for: 这个论文旨在提高透明度图像分析的表示学习，使用多任务自监学习（Multi-task Self-supervised Continual Learning，MUSCLE） pipeline，并在多个身体部位的X射线图像上进行训练。</li>
<li>methods: 这个方法使用MoCo基于表示学习，并采用了一种优化的连续学习（Continual Learning，CL）过程，以适应不同的X射线分析任务。此外，方法还使用了一些解决数据不一致、过拟合和忘记问题的策略，如图像预处理、学习调度和规范化。</li>
<li>results: 在9个真实世界X射线数据集上进行评估， Comparisons against other pre-trained models confirm the proof-of-concept that self-supervised multi-task&#x2F;dataset continual pre-training could boost the performance of X-ray image analysis。<details>
<summary>Abstract</summary>
While self-supervised learning (SSL) algorithms have been widely used to pre-train deep models, few efforts [11] have been done to improve representation learning of X-ray image analysis with SSL pre-trained models. In this work, we study a novel self-supervised pre-training pipeline, namely Multi-task Self-super-vised Continual Learning (MUSCLE), for multiple medical imaging tasks, such as classification and segmentation, using X-ray images collected from multiple body parts, including heads, lungs, and bones. Specifically, MUSCLE aggregates X-rays collected from multiple body parts for MoCo-based representation learning, and adopts a well-designed continual learning (CL) procedure to further pre-train the backbone subject various X-ray analysis tasks jointly. Certain strategies for image pre-processing, learning schedules, and regularization have been used to solve data heterogeneity, overfitting, and catastrophic forgetting problems for multi-task/dataset learning in MUSCLE.We evaluate MUSCLE using 9 real-world X-ray datasets with various tasks, including pneumonia classification, skeletal abnormality classification, lung segmentation, and tuberculosis (TB) detection. Comparisons against other pre-trained models [7] confirm the proof-of-concept that self-supervised multi-task/dataset continual pre-training could boost the performance of X-ray image analysis.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:随自学习（SSL）算法已经广泛使用来预训深度模型，但有少数尝试（11）来提高X射照像分析中的表示学习使用SSL预训模型。在这个工作中，我们研究了一个新的自我监督预训管道，即多任务自监督流行学习（MUSCLE），用于多种医疗影像任务，如分类和分类，使用X射照像集合自多个身体部位，包括头部、肺部和骨骼。具体来说，MUSCLE将X射照像集合自多个身体部位用MoCo基础的表示学习，并运用了一个Well-设计的流行学习（CL）程式来进一步预训底层组件面对多种X射照像分析任务。我们还使用了一些对于多任务/数据集学习的问题，如数据不一致、过滤和忘却问题的策略。我们使用了9个真实世界X射照像数据集进行评估，包括肺部炎症分类、骨骼异常分类、肺部分类和抑菌病（TB）检测。与其他预训模型（7）进行比较，证明了我们的观念的概念验证，即自我监督多任务/数据集流行预训可以提高X射照像分析的性能。Simplified Chinese:随自学习（SSL）算法已经广泛应用于预训深度模型，但有少数尝试（11）来提高X射照像分析中的表示学习使用SSL预训模型。在这个工作中，我们研究了一个新的自我监督预训管道，即多任务自监督流行学习（MUSCLE），用于多种医疗影像任务，如分类和分类，使用X射照像集合自多个身体部位，包括头部、肺部和骨骼。具体来说，MUSCLE将X射照像集合自多个身体部位用MoCo基础的表示学习，并运用了一个Well-设计的流行学习（CL）程式来进一步预训底层组件面对多种X射照像分析任务。我们还使用了一些对于多任务/数据集学习的问题，如数据不一致、过滤和忘却问题的策略。我们使用了9个真实世界X射照像数据集进行评估，包括肺部炎症分类、骨骼异常分类、肺部分类和抑菌病（TB）检测。与其他预训模型（7）进行比较，证明了我们的观念的概念验证，即自我监督多任务/数据集流行预训可以提高X射照像分析的性能。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Masked-Autoencoders-From-a-Local-Contrastive-Perspective"><a href="#Understanding-Masked-Autoencoders-From-a-Local-Contrastive-Perspective" class="headerlink" title="Understanding Masked Autoencoders From a Local Contrastive Perspective"></a>Understanding Masked Autoencoders From a Local Contrastive Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01994">http://arxiv.org/abs/2310.01994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Yue, Lei Bai, Meng Wei, Jiangmiao Pang, Xihui Liu, Luping Zhou, Wanli Ouyang</li>
<li>for: 本研究旨在探讨Masked AutoEncoder（MAE）在自supervised learning中的内部机制，以及它如何生成高质量的隐藏表示。</li>
<li>methods: 本研究使用MAE的生成预训练路径，通过对图像进行恶势masking来重建图像。研究发现MAE的解码器主要学习本地特征，遵循了Local Principle。基于本地性假设，提出了一种理论框架，将MAE转化为一种地区级别的对比学习形式，以更好地理解MAE的工作原理。</li>
<li>results: 研究发现MAE具有强大的地区级别对比学习能力，并且可以在不同的下游任务中达到状态 arts 的性能。此外，研究还提出了一种不需要masking和显式解码器的Siamese架构，可以寻求更加灵活的自supervised learning框架。<details>
<summary>Abstract</summary>
Masked AutoEncoder(MAE) has revolutionized the field of self-supervised learning with its simple yet effective masking and reconstruction strategies. However, despite achieving state-of-the-art performance across various downstream vision tasks, the underlying mechanisms that drive MAE's efficacy are less well-explored compared to the canonical contrastive learning paradigm. In this paper, we explore a new perspective to explain what truly contributes to the "rich hidden representations inside the MAE". Firstly, concerning MAE's generative pretraining pathway, with a unique encoder-decoder architecture to reconstruct images from aggressive masking, we conduct an in-depth analysis of the decoder's behaviors. We empirically find that MAE's decoder mainly learns local features with a limited receptive field, adhering to the well-known Locality Principle. Building upon this locality assumption, we propose a theoretical framework that reformulates the reconstruction-based MAE into a local region-level contrastive learning form for improved understanding. Furthermore, to substantiate the local contrastive nature of MAE, we introduce a Siamese architecture that combines the essence of MAE and contrastive learning without masking and explicit decoder, which sheds light on a unified and more flexible self-supervised learning framework.
</details>
<details>
<summary>摘要</summary>
masked autoencoder (MAE) 已经在自适应学习领域中革命化了，它的简单 yet effective 的面纱和重建策略使得它在不同的下游视觉任务中实现了状态机器人的性能。然而，尽管 MAE 的表现能力在多个下游任务中达到了领先水平，但是它的内部机制仍然不够清晰，相比于 canonical 对比学习模式。在这篇论文中，我们提出了一种新的视角来解释 MAE 中 "rich hidden representations" 的真正来源。首先，关于 MAE 的生成预训练路径，我们通过对掩蔽图像的唯一 encoder-decoder 架构进行深入分析，发现 MAE 的decoder 主要学习局部特征，遵循了已知的 Local Principle。基于这个局部假设，我们提出了一个理论框架，将 reconstruction-based MAE 转化为一种局部区域级对比学习形式，以提高理解。此外，为了证明 MAE 的局部对比性，我们提出了一种 Siamese 架构，将 MAE 和对比学习的本质结合在一起，无需掩蔽和显式的 decoder，这为一个更加统一和灵活的自适应学习框架提供了新的思路。
</details></li>
</ul>
<hr>
<h2 id="Development-of-Machine-Vision-Approach-for-Mechanical-Component-Identification-based-on-its-Dimension-and-Pitch"><a href="#Development-of-Machine-Vision-Approach-for-Mechanical-Component-Identification-based-on-its-Dimension-and-Pitch" class="headerlink" title="Development of Machine Vision Approach for Mechanical Component Identification based on its Dimension and Pitch"></a>Development of Machine Vision Approach for Mechanical Component Identification based on its Dimension and Pitch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01995">http://arxiv.org/abs/2310.01995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toshit Jain, Faisel Mushtaq, K Ramesh, Sandip Deshmukh, Tathagata Ray, Chandu Parimi, Praveen Tandon, Pramod Kumar Jha</li>
<li>for:  automatization of mechanical assembly lines</li>
<li>methods:  novel method of calculating pitch and bolt identification, lightweight and fast system</li>
<li>results:  correct identification of parts with 98% accuracy<details>
<summary>Abstract</summary>
In this work, a highly customizable and scalable vision based system for automation of mechanical assembly lines is described. The proposed system calculates the features that are required to classify and identify the different kinds of bolts that are used in the assembly line. The system describes a novel method of calculating the pitch of the bolt in addition to bolt identification and calculating the dimensions of the bolts. This identification and classification system is extremely lightweight and can be run on bare minimum hardware. The system is very fast in the order of milliseconds, hence the system can be used successfully even if the components are steadily moving on a conveyor. The results show that our system can correctly identify the parts in our dataset with 98% accuracy using the calculated features.
</details>
<details>
<summary>摘要</summary>
在这个工作中，描述了一种高度可定制和扩展的视觉基于系统，用于机械生产线自动化。该系统计算用于分类和识别不同类型的螺丝的特征。系统描述了一种新的螺丝排列方法，以及螺丝的尺寸计算方法。这个识别和分类系统非常轻量级，可以在最少的硬件上运行。系统速度非常快，只需毫秒级时间，因此可以成功地在滚动道上使用。实验结果显示，我们的系统可以在我们的数据集中正确地识别部件，准确率达98%。
</details></li>
</ul>
<hr>
<h2 id="CoralVOS-Dataset-and-Benchmark-for-Coral-Video-Segmentation"><a href="#CoralVOS-Dataset-and-Benchmark-for-Coral-Video-Segmentation" class="headerlink" title="CoralVOS: Dataset and Benchmark for Coral Video Segmentation"></a>CoralVOS: Dataset and Benchmark for Coral Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01946">http://arxiv.org/abs/2310.01946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhengziqiang/CoralVOS">https://github.com/zhengziqiang/CoralVOS</a></li>
<li>paper_authors: Zheng Ziqiang, Xie Yaofeng, Liang Haixin, Yu Zhibin, Sai-Kit Yeung<br>for:* 这个论文旨在提高贝壳礁的分析效率和准确性，并提供一个大规模的贝壳礁视频分割数据集（CoralVOS），以支持 dense coral video segmentation。methods:* 本论文使用了6种最新的视频对象分割（VOS）算法，并对其进行了微调，以便在CoralVOS dataset上进行分割。results:* 实验结果显示，通过微调VOS算法并使用CoralVOS dataset，可以大幅提高贝壳礁分割精度。然而，还有很大的潜在提升空间。<details>
<summary>Abstract</summary>
Coral reefs formulate the most valuable and productive marine ecosystems, providing habitat for many marine species. Coral reef surveying and analysis are currently confined to coral experts who invest substantial effort in generating comprehensive and dependable reports (\emph{e.g.}, coral coverage, population, spatial distribution, \textit{etc}), from the collected survey data. However, performing dense coral analysis based on manual efforts is significantly time-consuming, the existing coral analysis algorithms compromise and opt for performing down-sampling and only conducting sparse point-based coral analysis within selected frames. However, such down-sampling will \textbf{inevitable} introduce the estimation bias or even lead to wrong results. To address this issue, we propose to perform \textbf{dense coral video segmentation}, with no down-sampling involved. Through video object segmentation, we could generate more \textit{reliable} and \textit{in-depth} coral analysis than the existing coral reef analysis algorithms. To boost such dense coral analysis, we propose a large-scale coral video segmentation dataset: \textbf{CoralVOS} as demonstrated in Fig. 1. To the best of our knowledge, our CoralVOS is the first dataset and benchmark supporting dense coral video segmentation. We perform experiments on our CoralVOS dataset, including 6 recent state-of-the-art video object segmentation (VOS) algorithms. We fine-tuned these VOS algorithms on our CoralVOS dataset and achieved observable performance improvement. The results show that there is still great potential for further promoting the segmentation accuracy. The dataset and trained models will be released with the acceptance of this work to foster the coral reef research community.
</details>
<details>
<summary>摘要</summary>
海礁生态系统是生物多样性最高、最产值的marine生态系统，提供许多海洋生物种类的栖息地。但现在的海礁调查和分析仅限于专业人员，他们需要投入大量时间和努力来生成全面和可靠的报告（如珊瑚覆盖率、种群数量、空间分布等），从收集的调查数据中。然而，基于手动努力进行的 dense coral analysis 会带来估计偏差或者结果错误。为解决这个问题，我们提议实施 dense coral video segmentation，不含下采样。通过视频对象分 segmentation，我们可以生成更可靠和更深入的海礁分析结果，超过现有的海礁分析算法。为了推动这种 dense coral analysis，我们提出了一个大规模的海礁视频分 segmentation 数据集：CoralVOS（参见图1）。我们认为，CoralVOS 是目前所知道的第一个支持 dense coral video segmentation 的数据集和标准准。我们在 CoralVOS 数据集上进行了实验，包括 6 个最新的视频对象分 segmentation（VOS）算法。我们对这些 VOS 算法进行了微调，并在我们的 CoralVOS 数据集上进行了实验。结果表明，还有很大的提高可能性。我们将数据集和训练模型释放，以便推动海礁研究社区的发展。
</details></li>
</ul>
<hr>
<h2 id="OOD-Aware-Supervised-Contrastive-Learning"><a href="#OOD-Aware-Supervised-Contrastive-Learning" class="headerlink" title="OOD Aware Supervised Contrastive Learning"></a>OOD Aware Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01942">http://arxiv.org/abs/2310.01942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soroush Seifi, Daniel Olmeda Reino, Nikolay Chumerin, Rahaf Aljundi</li>
<li>for: 本文提出了一种基于超级对比学习的外部数据检测方法，以确保机器学习模型在部署时能够正确地识别外部数据。</li>
<li>methods: 本文提出了一种扩展supervised contrastive（SupCon）准则的方法，并增加了两个附加的对比项。第一个项使auxiliary OOD表示异常离ID表示，而不受任何约束。第二个项使OOD特征远离现有的类prototype，而Push ID表示更近于其相应的类prototype。当auxiliary OOD数据不可用时，本文提出了一种效率高的特征混合技术来生成pseudo-OOD特征。</li>
<li>results: 作者对不同的OOD检测方法进行比较，并在常用的benchmark上显示了state-of-the-art的结果。<details>
<summary>Abstract</summary>
Out-of-Distribution (OOD) detection is a crucial problem for the safe deployment of machine learning models identifying samples that fall outside of the training distribution, i.e. in-distribution data (ID). Most OOD works focus on the classification models trained with Cross Entropy (CE) and attempt to fix its inherent issues. In this work we leverage powerful representation learned with Supervised Contrastive (SupCon) training and propose a holistic approach to learn a classifier robust to OOD data. We extend SupCon loss with two additional contrast terms. The first term pushes auxiliary OOD representations away from ID representations without imposing any constraints on similarities among auxiliary data. The second term pushes OOD features far from the existing class prototypes, while pushing ID representations closer to their corresponding class prototype. When auxiliary OOD data is not available, we propose feature mixing techniques to efficiently generate pseudo-OOD features. Our solution is simple and efficient and acts as a natural extension of the closed-set supervised contrastive representation learning. We compare against different OOD detection methods on the common benchmarks and show state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
外部分布（OOD）检测是机器学习模型安全部署的关键问题，即内部分布数据（ID）。大多数OOD工作集中在基于交叉熵（CE）训练的分类模型上，尝试解决其内存问题。在这项工作中，我们利用强大的Supervised Contrastive（SupCon）训练学习出的表示，并提出一种整体方法来学习对OOD数据强健的分类器。我们将SupCon损失函数扩展为两个附加的对比项。第一项使auxiliary OOD表示远离ID表示，无论ID数据之间的相似性是否存在约束。第二项使OOD特征远离现有的类prototype，而ID表示靠近其相应的类prototype。当auxiliary OOD数据不可用时，我们提议使用特征混合技术生成 Pseudo-OOD 特征。我们的解决方案简单、高效，可以视为关闭集成Supervised Contrastive表示学习的自然扩展。我们在常用的benchmark上与不同的OOD检测方法进行比较，并显示状态前的结果。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Image-Text-Pair-Dataset-from-Books"><a href="#Constructing-Image-Text-Pair-Dataset-from-Books" class="headerlink" title="Constructing Image-Text Pair Dataset from Books"></a>Constructing Image-Text Pair Dataset from Books</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01936">http://arxiv.org/abs/2310.01936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yamato Okamoto, Haruto Toyonaga, Yoshihisa Ijiri, Hirokatsu Kataoka</li>
<li>for: This paper aims to leverage digital archives for machine learning, with the potential to uncover unknown insights and acquire knowledge autonomously.</li>
<li>methods: The proposed approach uses an optical character reader (OCR), an object detector, and a layout analyzer to construct an image-text pair dataset.</li>
<li>results: The authors apply their pipeline on old photo books to demonstrate the effectiveness of the approach in image-text retrieval and insight extraction.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是利用数字档案库进行机器学习，以便发现未知的发现和自主获取知识，就像人类读书一样。</li>
<li>methods: 该方法使用光学字符识别器（OCR）、物体检测器和布局分析器，为自动提取图文对象构建一个数据集。</li>
<li>results: 作者在古老的照片书中应用了自己的管道，展示了图文检索和发现的效果。<details>
<summary>Abstract</summary>
Digital archiving is becoming widespread owing to its effectiveness in protecting valuable books and providing knowledge to many people electronically. In this paper, we propose a novel approach to leverage digital archives for machine learning. If we can fully utilize such digitized data, machine learning has the potential to uncover unknown insights and ultimately acquire knowledge autonomously, just like humans read books. As a first step, we design a dataset construction pipeline comprising an optical character reader (OCR), an object detector, and a layout analyzer for the autonomous extraction of image-text pairs. In our experiments, we apply our pipeline on old photo books to construct an image-text pair dataset, showing its effectiveness in image-text retrieval and insight extraction.
</details>
<details>
<summary>摘要</summary>
“数字档案化在广泛应用，因为它能够保护价值书籍并提供电子形式知识给许多人。在这篇论文中，我们提出了一种新的方法，利用数字档案来进行机器学习。如果我们可以完全利用这些数字化数据，机器学习就可以探索未知的材料和获得自主知识，就像人类读书一样。为了实现这一目标，我们设计了一个数据建构管道，包括光学字符读取器（OCR）、对象检测器和布局分析器，以自动提取图文对。在我们的实验中，我们应用了这个管道于古照片书籍，构建了一个图文对数据集，并证明了其在图文检索和材料探索方面的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Robust-deformable-image-registration-using-cycle-consistent-implicit-representations"><a href="#Robust-deformable-image-registration-using-cycle-consistent-implicit-representations" class="headerlink" title="Robust deformable image registration using cycle-consistent implicit representations"></a>Robust deformable image registration using cycle-consistent implicit representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01934">http://arxiv.org/abs/2310.01934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louisvh/cycle_consistent_inr">https://github.com/louisvh/cycle_consistent_inr</a></li>
<li>paper_authors: Louis D. van Harten, Jaap Stoker, Ivana Išgum</li>
<li>for: 这个论文是为了提高医疗影像注册的稳定性和可靠性而写的。</li>
<li>methods: 这个方法使用了对于每对新影像进行优化的隐藏神经表现，并使用对于每对影像进行逆转的方法来实现对称调整。</li>
<li>results: 这个方法可以提高注册精度，并且可以提供一个可靠的不确定度量，可以用于自动质量控制。在4D肺CT数据集上进行评估，这个方法可以减少优化失败率从2.4%降至0.0%，提高标志精度4.5%，并且可以侦测到注册方法是否能够正确地解决 проблеme。此外，这个方法在 Abdomen 4D MRI 中心线传播任务上显示了46%的传播一致性提高，并且与注册精度之间存在强相关。<details>
<summary>Abstract</summary>
Recent works in medical image registration have proposed the use of Implicit Neural Representations, demonstrating performance that rivals state-of-the-art learning-based methods. However, these implicit representations need to be optimized for each new image pair, which is a stochastic process that may fail to converge to a global minimum. To improve robustness, we propose a deformable registration method using pairs of cycle-consistent Implicit Neural Representations: each implicit representation is linked to a second implicit representation that estimates the opposite transformation, causing each network to act as a regularizer for its paired opposite. During inference, we generate multiple deformation estimates by numerically inverting the paired backward transformation and evaluating the consensus of the optimized pair. This consensus improves registration accuracy over using a single representation and results in a robust uncertainty metric that can be used for automatic quality control. We evaluate our method with a 4D lung CT dataset. The proposed cycle-consistent optimization method reduces the optimization failure rate from 2.4% to 0.0% compared to the current state-of-the-art. The proposed inference method improves landmark accuracy by 4.5% and the proposed uncertainty metric detects all instances where the registration method fails to converge to a correct solution. We verify the generalizability of these results to other data using a centerline propagation task in abdominal 4D MRI, where our method achieves a 46% improvement in propagation consistency compared with single-INR registration and demonstrates a strong correlation between the proposed uncertainty metric and registration accuracy.
</details>
<details>
<summary>摘要</summary>
现有医疗影像注册研究提出使用隐藏神经表示，表现和目前学习型方法相当。然而，这些隐藏表示需要每对新影像进行优化，这是一个几率过程可能无法对全域最小化。为了提高适当性，我们提议使用对称的变形注册方法，每对隐藏神经表示 Linked to a second implicit representation that estimates the opposite transformation，使每个网络成为对应的常量。在推断中，我们产生多个变形估计，通过对称反传数进行数值逆解，并评估这对的整合度。这个整合度可以提高注册精度，并生成一个可靠的不确定度量，可以用于自动质量控制。我们使用4D肺CT影像 dataset进行评估。我们的循环相互优化方法可以降低优化失败率从2.4%降至0.0%，相比于目前的state-of-the-art。我们的推断方法可以提高标本精度4.5%，并且可以检测所有注册方法失败 converge to a correct solution。我们还证明了我们的结果可以在其他数据上进行一致性测试，例如腹部4D MRI中心线传播任务，我们的方法可以提高传播一致性46%，并且与注册精度之间存在强相关。
</details></li>
</ul>
<hr>
<h2 id="MarineDet-Towards-Open-Marine-Object-Detection"><a href="#MarineDet-Towards-Open-Marine-Object-Detection" class="headerlink" title="MarineDet: Towards Open-Marine Object Detection"></a>MarineDet: Towards Open-Marine Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01931">http://arxiv.org/abs/2310.01931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Haixin, Zheng Ziqiang, Ma Zeyu, Sai-Kit Yeung<br>for: marine object detection, especially for detecting diverse and unseen marine objects in underwater imagerymethods: 使用joint visual-text semantic space through pre-training, followed by marine-specific training to achieve in-air-to-marine knowledge transferresults: superior performance compared to existing generalist and specialist object detection algorithms, demonstrating the effectiveness of OMOD for marine ecosystem monitoring and management.<details>
<summary>Abstract</summary>
Marine object detection has gained prominence in marine research, driven by the pressing need to unravel oceanic mysteries and enhance our understanding of invaluable marine ecosystems. There is a profound requirement to efficiently and accurately identify and localize diverse and unseen marine entities within underwater imagery. The open-marine object detection (OMOD for short) is required to detect diverse and unseen marine objects, performing categorization and localization simultaneously. To achieve OMOD, we present \textbf{MarineDet}. We formulate a joint visual-text semantic space through pre-training and then perform marine-specific training to achieve in-air-to-marine knowledge transfer. Considering there is no specific dataset designed for OMOD, we construct a \textbf{MarineDet dataset} consisting of 821 marine-relative object categories to promote and measure OMOD performance. The experimental results demonstrate the superior performance of MarineDet over existing generalist and specialist object detection algorithms. To the best of our knowledge, we are the first to present OMOD, which holds a more valuable and practical setting for marine ecosystem monitoring and management. Our research not only pushes the boundaries of marine understanding but also offers a standard pipeline for OMOD.
</details>
<details>
<summary>摘要</summary>
海洋物体检测已经在海洋研究中占据了重要地位，这是由于需要解开海洋的秘密和提高我们对宝贵海洋生态系统的理解。在海洋图像中寻找和分类多种和未经见过的海洋对象是一项急需要解决的问题。为了实现这一目标，我们提出了海洋物体检测（OMOD）。我们通过预训练形成了视觉语义空间，然后通过海洋专门训练来实现海洋知识传递。由于没有专门为OMOD设计的数据集，我们构建了一个名为“MarineDet数据集”的821个海洋相关对象类别，以促进和评估OMOD性能。实验结果表明，MarineDet在现有的普遍和专家对象检测算法中表现出色。据我们所知，我们是第一个提出OMOD的研究人员，这将为海洋监测和管理提供更加有价值和实用的 Setting。我们的研究不仅推动了海洋理解的前iers，也提供了OMOD的标准管道。
</details></li>
</ul>
<hr>
<h2 id="RoFormer-for-Position-Aware-Multiple-Instance-Learning-in-Whole-Slide-Image-Classification"><a href="#RoFormer-for-Position-Aware-Multiple-Instance-Learning-in-Whole-Slide-Image-Classification" class="headerlink" title="RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification"></a>RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01924">http://arxiv.org/abs/2310.01924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne Pochet, Rami Maroun, Roger Trullo</li>
<li>for: 这个论文旨在解决 computational pathology 中的整个扫描图像（WSI）分类问题，但是现有的方法受到多个特征提取器的冻结问题。</li>
<li>methods: 该论文提出了一种使用 RoFormer 层，该层利用了内存高效的自我注意力和相对位置编码，可以对大小和形态不固定的 WSI 的补丁进行全自我注意力和相对位置编码，从而解决了patches之间的相互关系和组织结构的问题。</li>
<li>results: 该论文表明，使用该方法可以在三个公共数据集（TCGA-NSCLC、BRACS 和 Camelyon16）上超越现有的MIL模型，在弱级标注分类任务上实现更高的性能。<details>
<summary>Abstract</summary>
Whole slide image (WSI) classification is a critical task in computational pathology. However, the gigapixel-size of such images remains a major challenge for the current state of deep-learning. Current methods rely on multiple-instance learning (MIL) models with frozen feature extractors. Given the the high number of instances in each image, MIL methods have long assumed independence and permutation-invariance of patches, disregarding the tissue structure and correlation between patches. Recent works started studying this correlation between instances but the computational workload of such a high number of tokens remained a limiting factor. In particular, relative position of patches remains unaddressed. We propose to apply a straightforward encoding module, namely a RoFormer layer , relying on memory-efficient exact self-attention and relative positional encoding. This module can perform full self-attention with relative position encoding on patches of large and arbitrary shaped WSIs, solving the need for correlation between instances and spatial modeling of tissues. We demonstrate that our method outperforms state-of-the-art MIL models on three commonly used public datasets (TCGA-NSCLC, BRACS and Camelyon16)) on weakly supervised classification tasks. Code is available at https://github.com/Sanofi-Public/DDS-RoFormerMIL
</details>
<details>
<summary>摘要</summary>
全像图分类（WSI）是计算 PATHOLOGY 中的一项关键任务。然而， gigapixel 大小的这些图像仍然是当前深度学习中的主要挑战。现有的方法通常使用多个实例学习（MIL）模型，它们的特征提取器被冻结。由于每个图像中的实例数量很高，MIL 方法一直假设实例之间独立和排序不变。这些方法忽略了组织结构和实例之间的相关性。在最近的一些研究中，开始研究实例之间的相关性，但计算工作负担仍然是一个限制因素。特别是，实例之间的相对位置没有被考虑。我们提议使用一个简单的编码模块，即 RoFormer 层，该模块基于内存有效的准确自注意和相对位置编码。这个模块可以在大小和形态不固定的 WSI 上完全进行自注意和相对位置编码，解决实例之间的相关性和组织结构的空间模型。我们示出了我们的方法在三个常用的公共数据集（TCGA-NSCLC、BRACS 和 Camelyon16）上超过了当前的 MIL 模型的弱化类型分类任务。代码可以在 https://github.com/Sanofi-Public/DDS-RoFormerMIL 上获取。
</details></li>
</ul>
<hr>
<h2 id="Improved-Automatic-Diabetic-Retinopathy-Severity-Classification-Using-Deep-Multimodal-Fusion-of-UWF-CFP-and-OCTA-Images"><a href="#Improved-Automatic-Diabetic-Retinopathy-Severity-Classification-Using-Deep-Multimodal-Fusion-of-UWF-CFP-and-OCTA-Images" class="headerlink" title="Improved Automatic Diabetic Retinopathy Severity Classification Using Deep Multimodal Fusion of UWF-CFP and OCTA Images"></a>Improved Automatic Diabetic Retinopathy Severity Classification Using Deep Multimodal Fusion of UWF-CFP and OCTA Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01912">http://arxiv.org/abs/2310.01912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa El Habib Daho, Yihao Li, Rachid Zeghlache, Yapo Cedric Atse, Hugo Le Boité, Sophie Bonnin, Deborah Cosette, Pierre Deman, Laurent Borderie, Capucine Lepicard, Ramin Tadayoni, Béatrice Cochener, Pierre-Henri Conze, Mathieu Lamard, Gwenolé Quellec</li>
<li>for: 这个研究的目的是提高遗传性疾病肉眼病（DR）的早期识别，以提高病人的临床结果。</li>
<li>methods: 这个研究使用了多modal的图像技术，包括Ultra-WideField Color Fundus Photography（UWF-CFP）图像和Optical Coherence Tomography Angiography（OCTA）图像，并融合了ResNet50和3D-ResNet50模型，以提高DR的分类性能。</li>
<li>results: 实验结果显示，这个多modal方法可以与单一模式比较，提高DR的分类性能。这种方法可能将成为早期识别DR的重要工具，帮助改善病人的临床结果。<details>
<summary>Abstract</summary>
Diabetic Retinopathy (DR), a prevalent and severe complication of diabetes, affects millions of individuals globally, underscoring the need for accurate and timely diagnosis. Recent advancements in imaging technologies, such as Ultra-WideField Color Fundus Photography (UWF-CFP) imaging and Optical Coherence Tomography Angiography (OCTA), provide opportunities for the early detection of DR but also pose significant challenges given the disparate nature of the data they produce. This study introduces a novel multimodal approach that leverages these imaging modalities to notably enhance DR classification. Our approach integrates 2D UWF-CFP images and 3D high-resolution 6x6 mm$^3$ OCTA (both structure and flow) images using a fusion of ResNet50 and 3D-ResNet50 models, with Squeeze-and-Excitation (SE) blocks to amplify relevant features. Additionally, to increase the model's generalization capabilities, a multimodal extension of Manifold Mixup, applied to concatenated multimodal features, is implemented. Experimental results demonstrate a remarkable enhancement in DR classification performance with the proposed multimodal approach compared to methods relying on a single modality only. The methodology laid out in this work holds substantial promise for facilitating more accurate, early detection of DR, potentially improving clinical outcomes for patients.
</details>
<details>
<summary>摘要</summary>
糖尿病 retinopathy (DR) 是 диабеتype 的一种常见并严重的合并症，全球范围内有数百万人受到影响，这种情况加剧了精准和及时诊断的需求。 latest advancements in imaging technologies, such as Ultra-WideField Color Fundus Photography (UWF-CFP) imaging and Optical Coherence Tomography Angiography (OCTA), provide opportunities for the early detection of DR, but also pose significant challenges due to the disparate nature of the data they produce. This study introduces a novel multimodal approach that leverages these imaging modalities to notably enhance DR classification. Our approach integrates 2D UWF-CFP images and 3D high-resolution 6x6 mm$^3$ OCTA (both structure and flow) images using a fusion of ResNet50 and 3D-ResNet50 models, with Squeeze-and-Excitation (SE) blocks to amplify relevant features. Additionally, to increase the model's generalization capabilities, a multimodal extension of Manifold Mixup, applied to concatenated multimodal features, is implemented. Experimental results demonstrate a remarkable enhancement in DR classification performance with the proposed multimodal approach compared to methods relying on a single modality only. The methodology laid out in this work holds substantial promise for facilitating more accurate, early detection of DR, potentially improving clinical outcomes for patients.
</details></li>
</ul>
<hr>
<h2 id="CLIP-Is-Also-a-Good-Teacher-A-New-Learning-Framework-for-Inductive-Zero-shot-Semantic-Segmentation"><a href="#CLIP-Is-Also-a-Good-Teacher-A-New-Learning-Framework-for-Inductive-Zero-shot-Semantic-Segmentation" class="headerlink" title="CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation"></a>CLIP Is Also a Good Teacher: A New Learning Framework for Inductive Zero-shot Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02296">http://arxiv.org/abs/2310.02296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialei Chen, Daisuke Deguchi, Chenkai Zhang, Xu Zheng, Hiroshi Murase</li>
<li>for: 提高 Zero-shot Semantic Segmentation (GZLSS) 方法的效果，使其能够应用于不同的像素分类 segmentation 模型，无需添加额外的mask提案器或改变 CLIP 模型的结构。</li>
<li>methods: 提出了一种新的学习框架 CLIPTeacher，它可以应用于不同的像素分类 segmentation 模型，并利用所有的图像信息。 CLIPTeacher 包括两个关键模块：全球学习模块 (GLM) 和像素学习模块 (PLM)。 GLM 将图像编码器中的普通特征与 CLIP 模型中的 CLS token 进行对对比，从而捕捉全局信息。 PLM 则只使用 CLIP 模型中的普通特征来生成高级假注入，不需要添加额外的mask提案器。</li>
<li>results: 对三个 benchmark 数据集进行实验，结果表明，CLIPTeacher 可以大幅提高 Zero-shot Semantic Segmentation 的效果，具体来说是：PASCAL VOC 2012 上提高了 2.2%，COCO-Stuff 164k 上提高了 1.3%，PASCAL Context 上提高了 8.8%。<details>
<summary>Abstract</summary>
Existing Generalized Zero-shot Semantic Segmentation (GZLSS) methods apply either finetuning the CLIP paradigm or formulating it as a mask classification task, benefiting from the Vision-Language Models (VLMs). However, the fine-tuning methods are restricted with fixed backbone models which are not flexible for segmentation, and mask classification methods heavily rely on additional explicit mask proposers. Meanwhile, prevalent methods utilize only seen categories which is a great waste, i.e., neglecting the area exists but not annotated. To this end, we propose CLIPTeacher, a new learning framework that can be applied to various per-pixel classification segmentation models without introducing any explicit mask proposer or changing the structure of CLIP, and utilize both seen and ignoring areas. Specifically, CLIPTeacher consists of two key modules: Global Learning Module (GLM) and Pixel Learning Module (PLM). Specifically, GLM aligns the dense features from an image encoder with the CLS token, i.e., the only token trained in CLIP, which is a simple but effective way to probe global information from the CLIP models. In contrast, PLM only leverages dense tokens from CLIP to produce high-level pseudo annotations for ignoring areas without introducing any extra mask proposer. Meanwhile, PLM can fully take advantage of the whole image based on the pseudo annotations. Experimental results on three benchmark datasets: PASCAL VOC 2012, COCO-Stuff 164k, and PASCAL Context show large performance gains, i.e., 2.2%, 1.3%, and 8.8%
</details>
<details>
<summary>摘要</summary>
现有的泛化零shot semantic segmentation（GZLSS）方法通常是通过finetuning CLIP模式或者表示为 маска分类任务，借助于视觉语言模型（VLM）。然而， Fine-tuning 方法受到固定的背景模型的限制，不 flexible enough for segmentation，而且mask classification方法强调添加显式的mask proposer。此外，普遍的方法只使用已经看到的类别，这是一种大的浪费，即忽略不标注的区域。为了解决这个问题，我们提出了 CLIPTeacher，一种新的学习框架，可以应用于不同的每像素分类 segmentation 模型，无需添加显式的mask proposer，并且可以使用所有的区域。CLIPTeacher 包括两个关键模块：全球学习模块（GLM）和像素学习模块（PLM）。具体来说，GLM 将图像编码器中的稠密特征与 CLIP 模型中的 CLS token 进行对应，即通过简单而有效的方式 probing 全局信息从 CLIP 模型中。相比之下，PLM 只是使用 CLIP 模型中的稠密token 生成高级 Pseudo 标注 для忽略区域，而不需要添加额外的mask proposer。同时，PLM 可以完全利用整个图像，基于 Pseudo 标注。实验结果在 Pascal VOC 2012、COCO-Stuff 164k 和 Pascal Context 三个标准测试集上显示了大的性能提升，即 2.2%、1.3% 和 8.8%。
</details></li>
</ul>
<hr>
<h2 id="Improving-style-transfer-in-dynamic-contrast-enhanced-MRI-using-a-spatio-temporal-approach"><a href="#Improving-style-transfer-in-dynamic-contrast-enhanced-MRI-using-a-spatio-temporal-approach" class="headerlink" title="Improving style transfer in dynamic contrast enhanced MRI using a spatio-temporal approach"></a>Improving style transfer in dynamic contrast enhanced MRI using a spatio-temporal approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01908">http://arxiv.org/abs/2310.01908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam G. Tattersall, Keith A. Goatman, Lucy E. Kershaw, Scott I. K. Semple, Sonia Dahdouh</li>
<li>for: 这个论文旨在解决DCE-MRI中的样式传递问题，因为不同的组织和时间点的增强效应具有大量的变化。</li>
<li>methods: 该论文提出了一种新的方法，它将Autoencoder与卷积LSTM结合，以便分解内容和风格，并使用适应性的卷积来处理增强效应的地方化特性。</li>
<li>results: 论文的实验结果表明，该方法可以在两个不同的数据集上出perform state-of-the-art。<details>
<summary>Abstract</summary>
Style transfer in DCE-MRI is a challenging task due to large variations in contrast enhancements across different tissues and time. Current unsupervised methods fail due to the wide variety of contrast enhancement and motion between the images in the series. We propose a new method that combines autoencoders to disentangle content and style with convolutional LSTMs to model predicted latent spaces along time and adaptive convolutions to tackle the localised nature of contrast enhancement. To evaluate our method, we propose a new metric that takes into account the contrast enhancement. Qualitative and quantitative analyses show that the proposed method outperforms the state of the art on two different datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将给定文本翻译成简化中文。<</SYS>>在DCE-MRI中的风格传输是一项复杂的任务，因为不同的组织和时间中的对比增强大相差。现有的无监督方法因为图像序列中的对比增强和运动的各种多样性而失败。我们提议一种新的方法，该方法组合了自动编码器来分离内容和风格，以及卷积LSTM来预测时间序列中的隐藏空间。为评估我们的方法，我们提出了一个新的度量标准，该标准考虑对比增强的因素。Qualitative和量化分析表明，我们的方法在两个不同的数据集上表现出了优于当前状态的表现。
</details></li>
</ul>
<hr>
<h2 id="Beyond-the-Benchmark-Detecting-Diverse-Anomalies-in-Videos"><a href="#Beyond-the-Benchmark-Detecting-Diverse-Anomalies-in-Videos" class="headerlink" title="Beyond the Benchmark: Detecting Diverse Anomalies in Videos"></a>Beyond the Benchmark: Detecting Diverse Anomalies in Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01904">http://arxiv.org/abs/2310.01904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yoavarad/mfad">https://github.com/yoavarad/mfad</a></li>
<li>paper_authors: Yoav Arad, Michael Werman</li>
<li>for: 本研究旨在推动视频异常检测（VAD）领域的发展，扩展传统的单帧异常检测范畴，处理复杂的动作异常情况。</li>
<li>methods: 本研究提出了两个新的数据集：HMDB-AD和HMDB-Violence，以挑战模型处理多种动作异常情况。此外，研究人员还提出了一种新的多帧异常检测方法（MFAD），基于AI-VAD框架，使用单帧特征和两帧特征，以计算异常分布。</li>
<li>results: 实验结果表明，现有模型在新的异常类型面前存在限制，MFAD方法在单简异常检测和复杂异常检测场景中均表现出色。<details>
<summary>Abstract</summary>
Video Anomaly Detection (VAD) plays a crucial role in modern surveillance systems, aiming to identify various anomalies in real-world situations. However, current benchmark datasets predominantly emphasize simple, single-frame anomalies such as novel object detection. This narrow focus restricts the advancement of VAD models. In this research, we advocate for an expansion of VAD investigations to encompass intricate anomalies that extend beyond conventional benchmark boundaries. To facilitate this, we introduce two datasets, HMDB-AD and HMDB-Violence, to challenge models with diverse action-based anomalies. These datasets are derived from the HMDB51 action recognition dataset. We further present Multi-Frame Anomaly Detection (MFAD), a novel method built upon the AI-VAD framework. AI-VAD utilizes single-frame features such as pose estimation and deep image encoding, and two-frame features such as object velocity. They then apply a density estimation algorithm to compute anomaly scores. To address complex multi-frame anomalies, we add a deep video encoding features capturing long-range temporal dependencies, and logistic regression to enhance final score calculation. Experimental results confirm our assumptions, highlighting existing models limitations with new anomaly types. MFAD excels in both simple and complex anomaly detection scenarios.
</details>
<details>
<summary>摘要</summary>
视频异常检测（VAD）在现代监测系统中扮演着关键角色，旨在在实际情况中发现多种异常。然而，当前的标准数据集主要强调简单的单帧异常，如物品检测。这种狭隘的焦点限制了VAD模型的发展。在本研究中，我们主张拓展VAD研究，以涵盖更为复杂的异常。为此，我们介绍了两个数据集：HMDB-AD和HMDB-Violence，以挑战模型。这两个数据集都来自于HMDB51动作识别数据集。我们还提出了一种新的多帧异常检测方法（MFAD），基于AI-VAD框架。AI-VAD使用单帧特征，如pose estimation和深度图像编码，以及两帧特征，如物体速度。然后，它们应用某种密度估计算法计算异常分数。为了处理复杂的多帧异常，我们添加了深度视频编码特征，捕捉长距离时间相关性，以及逻辑回归来增强最终分数计算。实验结果证明了我们的假设，显示了现有模型对新类型异常的局限性。MFAD在简单和复杂异常检测场景中均表现出色。
</details></li>
</ul>
<hr>
<h2 id="MFOS-Model-Free-One-Shot-Object-Pose-Estimation"><a href="#MFOS-Model-Free-One-Shot-Object-Pose-Estimation" class="headerlink" title="MFOS: Model-Free &amp; One-Shot Object Pose Estimation"></a>MFOS: Model-Free &amp; One-Shot Object Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01897">http://arxiv.org/abs/2310.01897</a></li>
<li>repo_url: None</li>
<li>paper_authors: JongMin Lee, Yohann Cabon, Romain Brégier, Sungjoo Yoo, Jerome Revaud</li>
<li>for: 能够在单一的前进 pass 中 estimate 未经训练过的对象姿 pose, 只需要 minimum 的输入。</li>
<li>methods: 我们提出了一种使用 transformer 架构的方法, 可以充分利用最近提出的 3D-geometry 通用预训练。</li>
<li>results: 我们在 LINEMOD benchmark 上进行了广泛的实验，并reported 一shot 性能的 state-of-the-art 表现。<details>
<summary>Abstract</summary>
Existing learning-based methods for object pose estimation in RGB images are mostly model-specific or category based. They lack the capability to generalize to new object categories at test time, hence severely hindering their practicability and scalability. Notably, recent attempts have been made to solve this issue, but they still require accurate 3D data of the object surface at both train and test time. In this paper, we introduce a novel approach that can estimate in a single forward pass the pose of objects never seen during training, given minimum input. In contrast to existing state-of-the-art approaches, which rely on task-specific modules, our proposed model is entirely based on a transformer architecture, which can benefit from recently proposed 3D-geometry general pretraining. We conduct extensive experiments and report state-of-the-art one-shot performance on the challenging LINEMOD benchmark. Finally, extensive ablations allow us to determine good practices with this relatively new type of architecture in the field.
</details>
<details>
<summary>摘要</summary>
现有的学习基于方法 дляRGB图像中对象姿态估计都是模型特定或类别基于的。它们缺乏在测试时对新类别对象进行泛化的能力，因此很大程度上阻碍了它们的实用性和扩展性。值得注意的是，最近有一些尝试解决这个问题，但它们仍然需要在训练和测试时准确的3D对象表面数据。在这篇论文中，我们介绍了一种新的方法，可以在单一的前进 pass中估计在训练时未看过的对象姿态，只需最小的输入。与现有的状态对比，我们的提议的模型完全基于 transformer 架构，可以从最近的3D-geometry普适预训练中受益。我们进行了广泛的实验，并在LINEMOD测试准则上report了一shot性能的状态对比。最后，我们进行了广泛的ablation，以确定这种相对新的架构在领域中的好做法。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Multi-NeRF-Exploit-Efficient-Parallelism-in-Adaptive-Multiple-Scale-Neural-Radiance-Field-Rendering"><a href="#Adaptive-Multi-NeRF-Exploit-Efficient-Parallelism-in-Adaptive-Multiple-Scale-Neural-Radiance-Field-Rendering" class="headerlink" title="Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple Scale Neural Radiance Field Rendering"></a>Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple Scale Neural Radiance Field Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01881">http://arxiv.org/abs/2310.01881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Wang, Shuichi Kurabayashi</li>
<li>for: 这个论文的目的是提高NeRF的 Rendering速度，使其适用于实时渲染应用。</li>
<li>methods: 该方法使用了分割Scene into axis-aligned bounding boxes，并将不同场景部分分配给不同大小的NeRF。它还使用了导航密度网格来均衡每个Multilayer Perceptron（MLP）的表现能力。</li>
<li>results: 该方法可以大幅提高GPU的利用率，并且可以在实时渲染应用中加速Rendering过程。<details>
<summary>Abstract</summary>
Recent advances in Neural Radiance Fields (NeRF) have demonstrated significant potential for representing 3D scene appearances as implicit neural networks, enabling the synthesis of high-fidelity novel views. However, the lengthy training and rendering process hinders the widespread adoption of this promising technique for real-time rendering applications. To address this issue, we present an effective adaptive multi-NeRF method designed to accelerate the neural rendering process for large scenes with unbalanced workloads due to varying scene complexities.   Our method adaptively subdivides scenes into axis-aligned bounding boxes using a tree hierarchy approach, assigning smaller NeRFs to different-sized subspaces based on the complexity of each scene portion. This ensures the underlying neural representation is specific to a particular part of the scene. We optimize scene subdivision by employing a guidance density grid, which balances representation capability for each Multilayer Perceptron (MLP). Consequently, samples generated by each ray can be sorted and collected for parallel inference, achieving a balanced workload suitable for small MLPs with consistent dimensions for regular and GPU-friendly computations. We aosl demonstrated an efficient NeRF sampling strategy that intrinsically adapts to increase parallelism, utilization, and reduce kernel calls, thereby achieving much higher GPU utilization and accelerating the rendering process.
</details>
<details>
<summary>摘要</summary>
Our method subdivides scenes into axis-aligned bounding boxes using a tree hierarchy approach, assigning smaller NeRFs to different-sized subspaces based on the complexity of each scene portion. This ensures that the underlying neural representation is specific to a particular part of the scene. We optimize scene subdivision by employing a guidance density grid, which balances representation capability for each Multilayer Perceptron (MLP). Consequently, samples generated by each ray can be sorted and collected for parallel inference, achieving a balanced workload suitable for small MLPs with consistent dimensions for regular and GPU-friendly computations. We have also demonstrated an efficient NeRF sampling strategy that intrinsically adapts to increase parallelism, utilization, and reduce kernel calls, thereby achieving much higher GPU utilization and accelerating the rendering process.
</details></li>
</ul>
<hr>
<h2 id="A-Dual-Attentive-Generative-Adversarial-Network-for-Remote-Sensing-Image-Change-Detection"><a href="#A-Dual-Attentive-Generative-Adversarial-Network-for-Remote-Sensing-Image-Change-Detection" class="headerlink" title="A Dual Attentive Generative Adversarial Network for Remote Sensing Image Change Detection"></a>A Dual Attentive Generative Adversarial Network for Remote Sensing Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01876">http://arxiv.org/abs/2310.01876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luyi Qiu, Xiaofeng Zhang, ChaoChen Gu, and ShanYing Zhu</li>
<li>for: 本研究针对高分辨率 remote sensing 像素数据的变化探测任务进行研究，旨在提高检测精度和效率。</li>
<li>methods: 本研究提出了一个基于 dual attentive generative adversarial network（DAGAN）的方法，具有以下特点：（1）将检测模型视为生成器，通过生成 adversarial 策略来实现最佳类别器的适性，不增加检测模型的参数数目；（2）采用多层特征提取器来有效地融合多层特征，并导入总和连接来融合多层特征；（3）提出多値构成弹性融合模组来适应不同级别的物件，并导入 контекст调整模组来探索 Kontextuelle 依赖关系。</li>
<li>results: 实验结果显示，DAGAN 架构在 LEVIR 数据集上的平均 IoU 和 F1 分布为 85.01% 和 91.48%，较先进方法的表现更佳。<details>
<summary>Abstract</summary>
Remote sensing change detection between bi-temporal images receives growing concentration from researchers. However, comparing two bi-temporal images for detecting changes is challenging, as they demonstrate different appearances. In this paper, we propose a dual attentive generative adversarial network for achieving very high-resolution remote sensing image change detection tasks, which regards the detection model as a generator and attains the optimal weights of the detection model without increasing the parameters of the detection model through generative-adversarial strategy, boosting the spatial contiguity of predictions. Moreover, We design a multi-level feature extractor for effectively fusing multi-level features, which adopts the pre-trained model to extract multi-level features from bi-temporal images and introduces aggregate connections to fuse them. To strengthen the identification of multi-scale objects, we propose a multi-scale adaptive fusion module to adaptively fuse multi-scale features through various receptive fields and design a context refinement module to explore contextual dependencies. Moreover, the DAGAN framework utilizes the 4-layer convolution network as a discriminator to identify whether the synthetic image is fake or real. Extensive experiments represent that the DAGAN framework has better performance with 85.01% mean IoU and 91.48% mean F1 score than advanced methods on the LEVIR dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>远程感知变化检测 между双时间图像获得了研究人员的增加关注。然而，比较两个双时间图像以检测变化是具有挑战性的，因为它们具有不同的外观。在这篇论文中，我们提议一种双注意力生成 adversarial网络（DAGAN），用于实现高分辨率远程感知图像变化检测任务。DAGAN通过生成检测模型的优化参数，不需要增加检测模型的参数，从而提高了空间连续性。此外，我们设计了一种多级特征提取器，用于有效地融合多级特征。我们采用预训练模型来提取多级特征从双时间图像，并引入汇聚连接来融合它们。为了强化多 scales 对象的标识，我们提出了多 scales adaptive fusion模块，可以适应性地融合多级特征。此外，DAGAN框架还利用4层核心网络作为判据器，以确定生成图像是真实的或假的。广泛的实验表明，DAGAN框架在LEVIR数据集上的表现更好，其中 mean IoU 为85.01%，mean F1 score 为91.48%。
</details></li>
</ul>
<hr>
<h2 id="Shifting-More-Attention-to-Breast-Lesion-Segmentation-in-Ultrasound-Videos"><a href="#Shifting-More-Attention-to-Breast-Lesion-Segmentation-in-Ultrasound-Videos" class="headerlink" title="Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos"></a>Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01861">http://arxiv.org/abs/2310.01861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jhl-det/fla-net">https://github.com/jhl-det/fla-net</a></li>
<li>paper_authors: Junhao Lin, Qian Dai, Lei Zhu, Huazhu Fu, Qiong Wang, Weibin Li, Wenhao Rao, Xiaoyang Huang, Liansheng Wang</li>
<li>for: 这个研究旨在提高乳腺癌识别和治疗 axillary lymph node metastasis 的过程中，使用ultrasound 影像。</li>
<li>methods: 研究人员提出了一个新的频率和地方特征聚合网络（FLA-Net），从频率领域学习时间特征，并预测额外的腺腔癌位置。他们还提出了一个局部化的对照损失函数，以减少不同影像中的腺腔癌位置之间的距离。</li>
<li>results: 研究人员在自己的标注数据集和两个公共影像数据集上进行了实验，结果显示，他们的提案的 FLA-Net 在乳腺癌识别中具有国际级的性能，并且可以对影像数据集进行高效的处理和分析。<details>
<summary>Abstract</summary>
Breast lesion segmentation in ultrasound (US) videos is essential for diagnosing and treating axillary lymph node metastasis. However, the lack of a well-established and large-scale ultrasound video dataset with high-quality annotations has posed a persistent challenge for the research community. To overcome this issue, we meticulously curated a US video breast lesion segmentation dataset comprising 572 videos and 34,300 annotated frames, covering a wide range of realistic clinical scenarios. Furthermore, we propose a novel frequency and localization feature aggregation network (FLA-Net) that learns temporal features from the frequency domain and predicts additional lesion location positions to assist with breast lesion segmentation. We also devise a localization-based contrastive loss to reduce the lesion location distance between neighboring video frames within the same video and enlarge the location distances between frames from different ultrasound videos. Our experiments on our annotated dataset and two public video polyp segmentation datasets demonstrate that our proposed FLA-Net achieves state-of-the-art performance in breast lesion segmentation in US videos and video polyp segmentation while significantly reducing time and space complexity. Our model and dataset are available at https://github.com/jhl-Det/FLA-Net.
</details>
<details>
<summary>摘要</summary>
腋窝癌肿分割在ultrasound（US）视频中是诊断和治疗肿瘤肿瘤肿瘤的关键。然而，由于没有一个成熔和大规模的US视频数据集，高质量的注释很困难。为了解决这个问题，我们仔细筛选了572个US视频和34,300个注释帧，覆盖了许多真实的临床场景。此外，我们提出了一种频率和地点特征聚合网络（FLA-Net），可以从频率频谱中学习时间特征，并预测更多的肿瘤位置。我们还提出了一种基于地点的对比损失函数，可以在同一个US视频中减少肿瘤位置的距离，并在不同的US视频中增大距离。我们的实验表明，我们的提议的FLA-Net在US视频中的肿瘤分割和视频垂直段落分割中具有最佳性能，同时减少时间和空间复杂性。我们的模型和数据集可以在https://github.com/jhl-Det/FLA-Net中获取。
</details></li>
</ul>
<hr>
<h2 id="Selective-Feature-Adapter-for-Dense-Vision-Transformers"><a href="#Selective-Feature-Adapter-for-Dense-Vision-Transformers" class="headerlink" title="Selective Feature Adapter for Dense Vision Transformers"></a>Selective Feature Adapter for Dense Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01843">http://arxiv.org/abs/2310.01843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueqing Deng, Qi Fan, Xiaojie Jin, Linjie Yang, Peng Wang</li>
<li>for: 这个论文目的是解决预训transformer模型中巨量parameters的问题，以提高紧密预测类别 tasks的性能。</li>
<li>methods: 这个方法使用选择性特征适应器（SFA），包括外部适应器和内部适应器， sequentially operate over a transformer model。</li>
<li>results: 实验结果显示，这个方法可以在紧密预测类别 tasks上 achieves state-of-the-art（SoTA）性能，并且比完全 fine-tune 模型在不同的任务上表现更好或相同。<details>
<summary>Abstract</summary>
Fine-tuning pre-trained transformer models, e.g., Swin Transformer, are successful in numerous downstream for dense prediction vision tasks. However, one major issue is the cost/storage of their huge amount of parameters, which becomes increasingly challenging to handle with the growing amount of vision tasks. In this paper, we propose an effective approach to alleviate the issue, namely selective feature adapter (SFA). It achieves state-of-the-art (SoTA) performance under any given budget of trainable parameters, and demonstrates comparable or better performance than fully fine-tuned models across various dense tasks. Specifically, SFA consists of external adapters and internal adapters which are sequentially operated over a transformer model. For external adapters, we properly select the places and amount of additional multilayer perception (MLP). For internal adapters, we transform a few task-important parameters inside the transformer, which are automatically discovered through a simple yet effective lottery ticket algorithm. Our experiments show that the dual adapter module, a.k.a SFA, is essential to achieve the best trade-off on dense vision tasks, such as segmentation, detection and depth-estimation, outperforming other adapters with a single module.
</details>
<details>
<summary>摘要</summary>
通过练写预训练变换器模型，如斯вин变换器，在许多某些下渠任务上实现了优秀的表现。然而，一个主要问题是其参数的成本和存储量，这在视觉任务的数量不断增加时变得越来越Difficult to handle。在这篇论文中，我们提出了一种有效的方法来解决这个问题，即选择性特征适配器（SFA）。它在任何给定的训练参数预算下实现了状态元的表现，并在不同的权重任务中达到了相当于或更好的表现。SFA由外部适配器和内部适配器两部分组成，这两部分在转换器模型之上依次运行。对于外部适配器，我们合理地选择了附加的多层感知（MLP）的地方和数量。对于内部适配器，我们通过简单 yet effective的抽奖算法自动发现了转换器中一些任务重要的参数，并将它们变换为一些新的参数。我们的实验表明，双 adapter 模块，即 SFA，是在权重视觉任务，如 segmentation、检测和深度估计，实现了最佳的平衡，超过了其他适配器。
</details></li>
</ul>
<hr>
<h2 id="SelfGraphVQA-A-Self-Supervised-Graph-Neural-Network-for-Scene-based-Question-Answering"><a href="#SelfGraphVQA-A-Self-Supervised-Graph-Neural-Network-for-Scene-based-Question-Answering" class="headerlink" title="SelfGraphVQA: A Self-Supervised Graph Neural Network for Scene-based Question Answering"></a>SelfGraphVQA: A Self-Supervised Graph Neural Network for Scene-based Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01842">http://arxiv.org/abs/2310.01842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruno Souza, Marius Aasan, Helio Pedrini, Adín Ramírez Rivera<br>for: 本研究旨在提高基于Scene Graph（SG）的Visual Question Answering（VQA）任务中的批处理性能。methods: 我们提出了SelfGraphVQA框架，其包括使用预训练Scene Graph生成器提取图像中的Scene Graph，并使用自我超VI的自然语言表示进行semantically-preserving augmentation。我们还研究了三种最大化策略：节点级、图像级和卷积equivariant regularization。results: 我们实验表明，使用提取的Scene Graph可以提高VQA任务中的批处理性能，并且这些方法可以增强图像中的视觉信息的重要性。<details>
<summary>Abstract</summary>
The intersection of vision and language is of major interest due to the increased focus on seamless integration between recognition and reasoning. Scene graphs (SGs) have emerged as a useful tool for multimodal image analysis, showing impressive performance in tasks such as Visual Question Answering (VQA). In this work, we demonstrate that despite the effectiveness of scene graphs in VQA tasks, current methods that utilize idealized annotated scene graphs struggle to generalize when using predicted scene graphs extracted from images. To address this issue, we introduce the SelfGraphVQA framework. Our approach extracts a scene graph from an input image using a pre-trained scene graph generator and employs semantically-preserving augmentation with self-supervised techniques. This method improves the utilization of graph representations in VQA tasks by circumventing the need for costly and potentially biased annotated data. By creating alternative views of the extracted graphs through image augmentations, we can learn joint embeddings by optimizing the informational content in their representations using an un-normalized contrastive approach. As we work with SGs, we experiment with three distinct maximization strategies: node-wise, graph-wise, and permutation-equivariant regularization. We empirically showcase the effectiveness of the extracted scene graph for VQA and demonstrate that these approaches enhance overall performance by highlighting the significance of visual information. This offers a more practical solution for VQA tasks that rely on SGs for complex reasoning questions.
</details>
<details>
<summary>摘要</summary>
“视觉语言交叉是当前研究热点，因为它们在识别和理解之间的集成提供了更好的性能。场景图（SG）在多模态图像分析中表现出色，特别是在视觉问答（VQA）任务中。在这项工作中，我们发现了使用预训练场景图生成器生成场景图后，使用自我相似的扩展技术来增强图表示的方法。这种方法可以减少使用高成本和可能受损的标注数据，并且可以增强图表示的信息内容。我们在实验中使用了三种不同的最大化策略：节点级、图像级和对称变换的正则化。我们证明了这种方法可以提高 VQA 任务的总性能，并且强调了视觉信息的重要性。这个方法可以提供更实际的解决方案 для VQA 任务，尤其是当它们基于场景图进行复杂的推理问题。”
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-High-Dynamic-Range-Imaging-with-Multi-Exposure-Images-in-Dynamic-Scenes"><a href="#Self-Supervised-High-Dynamic-Range-Imaging-with-Multi-Exposure-Images-in-Dynamic-Scenes" class="headerlink" title="Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes"></a>Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01840">http://arxiv.org/abs/2310.01840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cszhilu1998/selfhdr">https://github.com/cszhilu1998/selfhdr</a></li>
<li>paper_authors: Zhilu Zhang, Haoyu Wang, Shuai Liu, Xiaotao Wang, Lei Lei, Wangmeng Zuo</li>
<li>for: 提高高动态范围（HDR）图像的重建，不需要 Labelled数据。</li>
<li>methods: 基于自我监督学习的HDR重建方法，使用多张多曝光图像进行训练，并通过两个 complementary component来捕捉HDR颜色和结构信息。</li>
<li>results: 在实际图像上进行测试，SelfHDR方法可以获得supervised方法相当的性能，并且超过自我监督方法的性能。代码可以在<a target="_blank" rel="noopener" href="https://github.com/cszhilu1998/SelfHDR%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/cszhilu1998/SelfHDR中下载。</a><details>
<summary>Abstract</summary>
Merging multi-exposure images is a common approach for obtaining high dynamic range (HDR) images, with the primary challenge being the avoidance of ghosting artifacts in dynamic scenes. Recent methods have proposed using deep neural networks for deghosting. However, the methods typically rely on sufficient data with HDR ground-truths, which are difficult and costly to collect. In this work, to eliminate the need for labeled data, we propose SelfHDR, a self-supervised HDR reconstruction method that only requires dynamic multi-exposure images during training. Specifically, SelfHDR learns a reconstruction network under the supervision of two complementary components, which can be constructed from multi-exposure images and focus on HDR color as well as structure, respectively. The color component is estimated from aligned multi-exposure images, while the structure one is generated through a structure-focused network that is supervised by the color component and an input reference (\eg, medium-exposure) image. During testing, the learned reconstruction network is directly deployed to predict an HDR image. Experiments on real-world images demonstrate our SelfHDR achieves superior results against the state-of-the-art self-supervised methods, and comparable performance to supervised ones. Codes are available at https://github.com/cszhilu1998/SelfHDR
</details>
<details>
<summary>摘要</summary>
合并多曝光图像是一种常见的方法来获得高动态范围（HDR）图像，主要挑战是避免在动态场景中出现幽灵 artifacts。现有方法通常是使用深度神经网络进行deghosting，但这些方法通常需要具有HDR真实值的数据，这些数据困难和昂贵地收集。在这种情况下，我们提出了SelfHDR，一种自动标注HDR重建方法，只需要在训练时提供动态多曝光图像。具体来说，SelfHDR学习一个重建网络，该网络在两个补做 Component 的指导下进行学习，这两个 Component 可以从多曝光图像中构建，它们的目标是获得HDR颜色以及结构。颜色 Component 是由对aligned多曝光图像进行Estimation的，而结构 Component 是通过一个以颜色Component为导向，并且与输入参考图像（例如中曝光图像）进行Supervised的神经网络来生成的。在测试时，学习的重建网络直接部署到预测HDR图像。实验结果表明，我们的SelfHDR方法在实际图像上比自动标注方法 superior，并且与指导方法相当。代码可以在https://github.com/cszhilu1998/SelfHDR 中找到。
</details></li>
</ul>
<hr>
<h2 id="Skin-the-sheep-not-only-once-Reusing-Various-Depth-Datasets-to-Drive-the-Learning-of-Optical-Flow"><a href="#Skin-the-sheep-not-only-once-Reusing-Various-Depth-Datasets-to-Drive-the-Learning-of-Optical-Flow" class="headerlink" title="Skin the sheep not only once: Reusing Various Depth Datasets to Drive the Learning of Optical Flow"></a>Skin the sheep not only once: Reusing Various Depth Datasets to Drive the Learning of Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01833">http://arxiv.org/abs/2310.01833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheng-Chi Huang, Wei-Chen Chiu</li>
<li>for: 提高视觉和机器人应用中的光流估计精度，解决现有方法在实际场景中难以获得有效的光流真实数据问题。</li>
<li>methods: 利用光流估计和雷达测量之间的几何连接，将多种实际深度估计数据集合并生成超过监督学习的有效训练数据，并通过在一个图像上进行几何变换来增加光流估计器的学习环境灵活性。</li>
<li>results: 通过对多种数据集和不同光流估计模型进行广泛的实验，证明提出的方法能够提高光流估计精度和普适性，并且不是与特定的光流估计器相关。<details>
<summary>Abstract</summary>
Optical flow estimation is crucial for various applications in vision and robotics. As the difficulty of collecting ground truth optical flow in real-world scenarios, most of the existing methods of learning optical flow still adopt synthetic dataset for supervised training or utilize photometric consistency across temporally adjacent video frames to drive the unsupervised learning, where the former typically has issues of generalizability while the latter usually performs worse than the supervised ones. To tackle such challenges, we propose to leverage the geometric connection between optical flow estimation and stereo matching (based on the similarity upon finding pixel correspondences across images) to unify various real-world depth estimation datasets for generating supervised training data upon optical flow. Specifically, we turn the monocular depth datasets into stereo ones via synthesizing virtual disparity, thus leading to the flows along the horizontal direction; moreover, we introduce virtual camera motion into stereo data to produce additional flows along the vertical direction. Furthermore, we propose applying geometric augmentations on one image of an optical flow pair, encouraging the optical flow estimator to learn from more challenging cases. Lastly, as the optical flow maps under different geometric augmentations actually exhibit distinct characteristics, an auxiliary classifier which trains to identify the type of augmentation from the appearance of the flow map is utilized to further enhance the learning of the optical flow estimator. Our proposed method is general and is not tied to any particular flow estimator, where extensive experiments based on various datasets and optical flow estimation models verify its efficacy and superiority.
</details>
<details>
<summary>摘要</summary>
优化流量估计是视觉和机器人应用中的关键问题。由于实际场景中收集实际流量真实数据困难，大多数现有的学习流量估计方法仍然采用生成的 sintetic 数据进行超级vised 训练或利用相机适应性保持 temporal 邻近视频帧的光学一致性来驱动不监督学习，其中前者通常具有普遍化问题而后者通常比监督学习更差。为解决这些挑战，我们提议利用光流估计与tereo匹配的几何连接来统一各种实际深度估计数据，以生成超级vised 训练数据。具体来说，我们将монокуляр深度数据转化为stereo数据，并通过 sintesizing 虚拟的 disparity 来导致流量在水平方向上；此外，我们还引入虚拟相机运动，以生成额外的流量在垂直方向上。此外，我们提议在一个光流对的图像上应用几何变换，以驱动光流估计器学习更加具有挑战性的情况。最后，我们发现光流图像不同的几何变换实际上具有不同的特征，因此我们提出了一个辅助类ifier，用于在光流图像上预测几何变换的类型，以进一步改进光流估计器的学习。我们的提议方法不受任何特定的流量估计器约束，并经过了多种数据和光流估计器的实验，证明了其效果和优越性。
</details></li>
</ul>
<hr>
<h2 id="AI-Generated-Images-as-Data-Source-The-Dawn-of-Synthetic-Era"><a href="#AI-Generated-Images-as-Data-Source-The-Dawn-of-Synthetic-Era" class="headerlink" title="AI-Generated Images as Data Source: The Dawn of Synthetic Era"></a>AI-Generated Images as Data Source: The Dawn of Synthetic Era</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01830">http://arxiv.org/abs/2310.01830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mwxely/aigs">https://github.com/mwxely/aigs</a></li>
<li>paper_authors: Zuhao Yang, Fangneng Zhan, Kunhao Liu, Muyu Xu, Shijian Lu</li>
<li>for: 本研究旨在探讨使用生成AI模型生成的图像作为视觉智能的新数据源，以改善传统的模型设计方法。</li>
<li>methods: 本研究采用了生成AI模型，如Generative Adversarial Networks (GANs)和Variational Autoencoders (VAEs)，生成大量的图像数据，并对这些数据进行了分析和评估。</li>
<li>results: 研究发现，使用生成AI模型生成的图像数据可以提高视觉智能模型的性能，并且可以轻松地生成大量的 Edge cases 和罕见的场景，以便进行计算机模拟、测试和验证。<details>
<summary>Abstract</summary>
The advancement of visual intelligence is intrinsically tethered to the availability of large-scale data. In parallel, generative Artificial Intelligence (AI) has unlocked the potential to create synthetic images that closely resemble real-world photographs. This prompts a compelling inquiry: how much visual intelligence could benefit from the advance of generative AI? This paper explores the innovative concept of harnessing these AI-generated images as new data sources, reshaping traditional modeling paradigms in visual intelligence. In contrast to real data, AI-generated data exhibit remarkable advantages, including unmatched abundance and scalability, the rapid generation of vast datasets, and the effortless simulation of edge cases. Built on the success of generative AI models, we examine the potential of their generated data in a range of applications, from training machine learning models to simulating scenarios for computational modeling, testing, and validation. We probe the technological foundations that support this groundbreaking use of generative AI, engaging in an in-depth discussion on the ethical, legal, and practical considerations that accompany this transformative paradigm shift. Through an exhaustive survey of current technologies and applications, this paper presents a comprehensive view of the synthetic era in visual intelligence. A project associated with this paper can be found at https://github.com/mwxely/AIGS .
</details>
<details>
<summary>摘要</summary>
通过潜在的推动，视觉智能的进步与大规模数据的可用性密切相关。与此同时，生成型人工智能（AI）已经解锁了创造真实图像的潜在。这引发了一个感人的问题：如何利用这些AI生成的图像来提高视觉智能？这篇论文探讨了将这些AI生成的图像作为新的数据源，重新定义传统的视觉智能模型。与真实数据相比，AI生成的数据具有无可比的优势，包括无比充足和可扩展的数据量，快速生成大量数据，以及轻松模拟边缘情况。基于成功的生成AI模型，我们研究了这些生成的数据在多种应用中的潜力，从训练机器学习模型到计算模拟和验证。我们还探讨了这种新的思维方式在技术、伦理和实践方面的支持，并进行了详细的讨论。通过对当前技术和应用的总结，这篇论文提供了对synthetic时代的视觉智能全面的视图。关于这个项目，可以参考https://github.com/mwxely/AIGS。
</details></li>
</ul>
<hr>
<h2 id="Amazing-Combinatorial-Creation-Acceptable-Swap-Sampling-for-Text-to-Image-Generation"><a href="#Amazing-Combinatorial-Creation-Acceptable-Swap-Sampling-for-Text-to-Image-Generation" class="headerlink" title="Amazing Combinatorial Creation: Acceptable Swap-Sampling for Text-to-Image Generation"></a>Amazing Combinatorial Creation: Acceptable Swap-Sampling for Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01819">http://arxiv.org/abs/2310.01819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Li, Zedong Zhang, Jian Yang</li>
<li>for: 本研究旨在开发一种能够生成有意义的组合物图像，从多个文本描述中提取出人类创造力的机器学习系统。</li>
<li>methods: 我们提出了一种简单 yet 高效的技术——可接受的交换抽象法，通过交换两个文本embedding的列向量来生成一个新的组合图像，并使用cutting-edge diffusion model来生成新的图像。</li>
<li>results: 我们的实验结果表明，我们的方法可以在生成 novel和surprising的组合图像时，超过latest方法，如Stable-Diffusion2、DALLE2、ERNIE-ViLG2和Bing。此外，我们的方法还可以和人工偏好数据集上训练的PickScore和HPSv2相比，在抽取过程中获得相似的结果。<details>
<summary>Abstract</summary>
Exploring a machine learning system to generate meaningful combinatorial object images from multiple textual descriptions, emulating human creativity, is a significant challenge as humans are able to construct amazing combinatorial objects, but machines strive to emulate data distribution. In this paper, we develop a straight-forward yet highly effective technique called acceptable swap-sampling to generate a combinatorial object image that exhibits novelty and surprise, utilizing text concepts of different objects. Initially, we propose a swapping mechanism that constructs a novel embedding by exchanging column vectors of two text embeddings for generating a new combinatorial image through a cutting-edge diffusion model. Furthermore, we design an acceptable region by managing suitable CLIP distances between the new image and the original concept generations, increasing the likelihood of accepting the new image with a high-quality combination. This region allows us to efficiently sample a small subset from a new image pool generated by using randomly exchanging column vectors. Lastly, we employ a segmentation method to compare CLIP distances among the segmented components, ultimately selecting the most promising object image from the sampled subset. Our experiments focus on text pairs of objects from ImageNet, and our results demonstrate that our approach outperforms recent methods such as Stable-Diffusion2, DALLE2, ERNIE-ViLG2 and Bing in generating novel and surprising object images, even when the associated concepts appear to be implausible, such as lionfish-abacus. Moreover, during the sampling process, our approach without training and human preference is also comparable to PickScore and HPSv2 trained using human preference datasets.
</details>
<details>
<summary>摘要</summary>
investigate 一种机器学习系统，用于从多个文本描述生成有意义的 combinatorial 物品图像，模拟人类创作能力，是一项 significanthallenge。 humans 可以构建惊人的 combinatorial 物品，但机器尝试模拟数据分布。在这篇论文中，我们开发了一种直观而高效的技术——可接受的换Sampling。我们提议一种交换机制，通过对两个文本嵌入的列向量进行交换，生成一个新的 combinatorial 图像。此外，我们设计了一个可接受的区域，通过控制适当的 CLIP 距离，使新图像与原始概念生成之间的关系更加可靠。这个区域使我们能够有效地采样一小 subsets从新生成的图像池中，并最终选择最有前途的物品图像。我们的实验集中使用了 ImageNet 中的对象对，并我们的结果表明，我们的方法在生成有意义和 surprising 的 object 图像方面，超过了最近的方法，如 Stable-Diffusion2、DALLE2、ERNIE-ViLG2 和 Bing。此外，在采样过程中，我们的方法不需要训练和人类偏好，也能与 PickScore 和 HPSv2 训练使用人类偏好数据进行比较。
</details></li>
</ul>
<hr>
<h2 id="PPT-Token-Pruning-and-Pooling-for-Efficient-Vision-Transformers"><a href="#PPT-Token-Pruning-and-Pooling-for-Efficient-Vision-Transformers" class="headerlink" title="PPT: Token Pruning and Pooling for Efficient Vision Transformers"></a>PPT: Token Pruning and Pooling for Efficient Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01812">http://arxiv.org/abs/2310.01812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xjwu1024/PPT">https://github.com/xjwu1024/PPT</a></li>
<li>paper_authors: Xinjian Wu, Fanhu Zeng, Xiudong Wang, Yunhe Wang, Xinghao Chen</li>
<li>for: 提高计算复杂性的实际应用在计算机视觉领域中</li>
<li>methods: 使用token pruning和token pooling技术在ViTs中进行减少重复的 acceleration framework</li>
<li>results: 在ImageNet dataset上，PPT可以减少37%的FLOPs并提高通过putthroughput比例45%，而无需减少精度。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have emerged as powerful models in the field of computer vision, delivering superior performance across various vision tasks. However, the high computational complexity poses a significant barrier to their practical applications in real-world scenarios. Motivated by the fact that not all tokens contribute equally to the final predictions and fewer tokens bring less computational cost, reducing redundant tokens has become a prevailing paradigm for accelerating vision transformers. However, we argue that it is not optimal to either only reduce inattentive redundancy by token pruning, or only reduce duplicative redundancy by token merging. To this end, in this paper we propose a novel acceleration framework, namely token Pruning & Pooling Transformers (PPT), to adaptively tackle these two types of redundancy in different layers. By heuristically integrating both token pruning and token pooling techniques in ViTs without additional trainable parameters, PPT effectively reduces the model complexity while maintaining its predictive accuracy. For example, PPT reduces over 37% FLOPs and improves the throughput by over 45% for DeiT-S without any accuracy drop on the ImageNet dataset.
</details>
<details>
<summary>摘要</summary>
computer vision 领域中，Vision Transformers（ViTs）已经成为了强大的模型，在不同的视觉任务中具有出色的表现。然而，高计算复杂度对实际应用场景中的应用带来了很大的障碍。受到每个token不同程度地影响最终预测的事实的灵感，我们提出了一种新的加速框架，即token Pruning & Pooling Transformers（PPT），用于在不同层次上适应性地处理两种不同类型的缺失。通过在ViTs中不添加任何可训练参数的情况下，合理地结合token pruning和token pooling技术，PPT可以有效减少模型的复杂度，保持预测精度。例如，在DeiT-S上，PPT可以减少37%的FLOPs，提高通过put throughput by over 45%，而无需减少精度在ImageNet dataset。
</details></li>
</ul>
<hr>
<h2 id="SMRD-SURE-based-Robust-MRI-Reconstruction-with-Diffusion-Models"><a href="#SMRD-SURE-based-Robust-MRI-Reconstruction-with-Diffusion-Models" class="headerlink" title="SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models"></a>SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01799">http://arxiv.org/abs/2310.01799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvlabs/smrd">https://github.com/nvlabs/smrd</a></li>
<li>paper_authors: Batu Ozturkler, Chao Liu, Benjamin Eckart, Morteza Mardani, Jiaming Song, Jan Kautz<br>for:SMRD is designed to improve the robustness of diffusion models for accelerated MRI reconstruction.methods:SMRD uses Stein’s Unbiased Risk Estimator (SURE) to estimate the mean squared error of the reconstruction during testing, and automatically tunes the inference hyperparameters without the need for validation tuning.results:SMRD outperforms diffusion model baselines on various measurement noise levels, acceleration factors, and anatomies, achieving a PSNR improvement of up to 6 dB under measurement noise.Here is the Chinese translation of the three key points:for:SMRD 是用于提高Diffusion模型的加速MRI重建稳定性的方法。methods:SMRD 使用 Stein 不偏风险估计器 (SURE) 来在测试阶段估计重建结果的均方差，并自动调整推理参数而无需验证集调整。results:SMRD 在不同的测量噪声水平、加速因子和解剖学上都超过了Diffusion模型基elines，实现了测量噪声下 PSNR 提高达 6 dB。<details>
<summary>Abstract</summary>
Diffusion models have recently gained popularity for accelerated MRI reconstruction due to their high sample quality. They can effectively serve as rich data priors while incorporating the forward model flexibly at inference time, and they have been shown to be more robust than unrolled methods under distribution shifts. However, diffusion models require careful tuning of inference hyperparameters on a validation set and are still sensitive to distribution shifts during testing. To address these challenges, we introduce SURE-based MRI Reconstruction with Diffusion models (SMRD), a method that performs test-time hyperparameter tuning to enhance robustness during testing. SMRD uses Stein's Unbiased Risk Estimator (SURE) to estimate the mean squared error of the reconstruction during testing. SURE is then used to automatically tune the inference hyperparameters and to set an early stopping criterion without the need for validation tuning. To the best of our knowledge, SMRD is the first to incorporate SURE into the sampling stage of diffusion models for automatic hyperparameter selection. SMRD outperforms diffusion model baselines on various measurement noise levels, acceleration factors, and anatomies, achieving a PSNR improvement of up to 6 dB under measurement noise. The code is publicly available at https://github.com/NVlabs/SMRD .
</details>
<details>
<summary>摘要</summary>
Diffusion models 最近受欢迎用于加速MRI重建，因为它们可以提供高质量的样本，并且可以在推理时 flexibly  incorporate 前向模型。它们在分布转移下更加稳定，但是需要在验证集上精细调整推理超参数，并且在测试时仍然敏感于分布转移。为解决这些挑战，我们提出了 SMRD（Diffusion Model based MRI Reconstruction with Stein's Unbiased Risk Estimator），一种在测试时进行 hyperparameter 调整，以提高测试时的Robustness。SMRD 使用 Stein's Unbiased Risk Estimator（SURE）来估计测试时重建的 mean squared error。SURE 然后用于自动调整推理超参数，并设置 early stopping 条件，无需验证集调整。根据我们所知，SMRD 是首次将 SURE  integrate 到 diffusion models 的 sampling 阶段中进行自动超参数选择。SMRD 在不同的测量噪声水平、压缩因数和 анатомиче特征下，超过 diffusion model 基eline，实现 PSNR 改善达 6 dB 以上。代码可以在 <https://github.com/NVlabs/SMRD> 上获取。
</details></li>
</ul>
<hr>
<h2 id="HallE-Switch-Rethinking-and-Controlling-Object-Existence-Hallucinations-in-Large-Vision-Language-Models-for-Detailed-Caption"><a href="#HallE-Switch-Rethinking-and-Controlling-Object-Existence-Hallucinations-in-Large-Vision-Language-Models-for-Detailed-Caption" class="headerlink" title="HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption"></a>HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01779">http://arxiv.org/abs/2310.01779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan Yan, Xiangjun Fan</li>
<li>for: 这 paper 旨在 investigate 现有大规模视语言模型 (LVLM) 的细节描述能力是否准确，以及如何控制这些模型在细节描述中的幻见。</li>
<li>methods: 这 paper 提出了一种基于 GPT-4 的评价方法，称为 $\textit{CCEval}$，用于评价 LVLM 在细节描述 task 中的性能。</li>
<li>results: 研究发现，exist Hallucination 仍然存在于现有 VQA benchmark 中，而 $\textit{CCEval}$ 的评价方法可以减少这种幻见的发生。此外，研究还发现了一些因素对细节描述的影响，如图像分辨率、语言解码器大小和数据量等。<details>
<summary>Abstract</summary>
Current large vision-language models (LVLMs) achieve remarkable progress, yet there remains significant uncertainty regarding their ability to accurately apprehend visual details, that is, in performing detailed captioning. To address this, we introduce \textit{CCEval}, a GPT-4 assisted evaluation method tailored for detailed captioning. Interestingly, while LVLMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations. In this paper, we make the first attempt to investigate and attribute such hallucinations, including image resolution, the language decoder size, and instruction data amount, quality, granularity. Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can ground or verify, thus inducing hallucination. To control such hallucinations, we further attribute the reliability of captioning to contextual knowledge (involving only contextually grounded objects) and parametric knowledge (containing inferred objects by the model). Thus, we introduce $\textit{HallE-Switch}$, a controllable LVLM in terms of $\textbf{Hall}$ucination in object $\textbf{E}$xistence. HallE-Switch can condition the captioning to shift between (i) exclusively depicting contextual knowledge for grounded objects and (ii) blending it with parametric knowledge to imagine inferred objects. Our method reduces hallucination by 44% compared to LLaVA$_{7B}$ and maintains the same object coverage.
</details>
<details>
<summary>摘要</summary>
当前大规模视语言模型（LVLM）已经取得了很大的进步，但是对于细节描述仍存在较大的不确定性。为了解决这个问题，我们提出了《CCEval》评价方法，它是基于GPT-4的辅助评价方法，专门用于细节描述。有趣的是，当前的LVLM在存在的VQA benchmark上基本没有显示对象存在hallucination，但我们的提出的评价方法发现，LVLM仍然存在hallucination的问题。在这篇论文中，我们首次 investigate和 attribute这些hallucination，包括图像分辨率、语言解码器大小和数据量、质量和细节。我们的发现表明，当语言描述包含更细的物体detail than what the vision module can confirm or verify时，会导致hallucination。为了控制这些hallucination，我们进一步 attribute了描述的可靠性，包括上下文知识（仅仅包含上下文中的物体）和参数知识（由模型推导出的物体）。因此，我们提出了《HallE-Switch》，一种可控的LVLM，可以在描述中shift между（i）仅仅描述上下文知识和（ii）混合上下文知识和参数知识来控制hallucination。我们的方法可以reduces hallucination by 44% compared to LLaVA$_{7B}$，并且保持同样的物体覆盖率。
</details></li>
</ul>
<hr>
<h2 id="ImageNet-OOD-Deciphering-Modern-Out-of-Distribution-Detection-Algorithms"><a href="#ImageNet-OOD-Deciphering-Modern-Out-of-Distribution-Detection-Algorithms" class="headerlink" title="ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms"></a>ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01755">http://arxiv.org/abs/2310.01755</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Yang, Byron Zhang, Olga Russakovsky</li>
<li>for: The paper aims to investigate the behavior of out-of-distribution (OOD) detection algorithms and to provide insights for guiding the design of future OOD detectors.</li>
<li>methods: The paper uses a clean semantic shift dataset called ImageNet-OOD to decouple semantic shift and covariate shift, and conducts comprehensive experiments to evaluate the performance of OOD detection algorithms under these two types of shifts.</li>
<li>results: The paper shows that OOD detectors are more sensitive to covariate shift than to semantic shift, and that the benefits of recent OOD detection algorithms on semantic shift detection are minimal. The paper provides important insights for designing future OOD detectors.Here is the same information in Simplified Chinese text:</li>
<li>for: 该文章的目的是调查扩展类别检测算法的行为，并为未来的扩展类别检测算法的设计提供重要的指导。</li>
<li>methods: 文章使用ImageNet-OOD清晰的semantic shift dataset，以分离semantic shift和covariate shift，并通过全面的实验评估扩展类别检测算法在这两种类型的变化下的表现。</li>
<li>results: 文章发现扩展类别检测算法更敏感于covariate shift，而semantic shift detection的 beneficial effects minimal。文章提供了重要的指导，用于设计未来的扩展类别检测算法。<details>
<summary>Abstract</summary>
The task of out-of-distribution (OOD) detection is notoriously ill-defined. Earlier works focused on new-class detection, aiming to identify label-altering data distribution shifts, also known as "semantic shift." However, recent works argue for a focus on failure detection, expanding the OOD evaluation framework to account for label-preserving data distribution shifts, also known as "covariate shift." Intriguingly, under this new framework, complex OOD detectors that were previously considered state-of-the-art now perform similarly to, or even worse than the simple maximum softmax probability baseline. This raises the question: what are the latest OOD detectors actually detecting? Deciphering the behavior of OOD detection algorithms requires evaluation datasets that decouples semantic shift and covariate shift. To aid our investigations, we present ImageNet-OOD, a clean semantic shift dataset that minimizes the interference of covariate shift. Through comprehensive experiments, we show that OOD detectors are more sensitive to covariate shift than to semantic shift, and the benefits of recent OOD detection algorithms on semantic shift detection is minimal. Our dataset and analyses provide important insights for guiding the design of future OOD detectors.
</details>
<details>
<summary>摘要</summary>
“批量外部（OOD）检测任务被认为是不具体定义的。 Earlier works 专注于新类别检测，尝试识别标签改变的数据分布类型，也就是“semantic shift”。然而，最近的工作强调失败检测，扩展OOD评估框架，考虑到标签保持的数据分布类型，也就是“covariate shift”。这引起了问题： latest OOD 检测器是否能够正确地检测到问题？对 OOD 检测算法的行为进行评估需要分离 semantic shift 和 covariate shift 的评估数据集。为了帮助我们的研究，我们提出了 ImageNet-OOD，一个减少了 covariate shift 的干扰的清晰 semantic shift 数据集。通过全面的实验，我们发现 OOD 检测器更感应 covariate shift 而不是 semantic shift，而最近的 OOD 检测算法对 semantic shift 的 beneficial 效果几乎是零。我们的数据和分析对未来 OOD 检测器的设计提供了重要的启发。”
</details></li>
</ul>
<hr>
<h2 id="Generative-Autoencoding-of-Dropout-Patterns"><a href="#Generative-Autoencoding-of-Dropout-Patterns" class="headerlink" title="Generative Autoencoding of Dropout Patterns"></a>Generative Autoencoding of Dropout Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01712">http://arxiv.org/abs/2310.01712</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuntama/deciphering-autoencoders">https://github.com/shuntama/deciphering-autoencoders</a></li>
<li>paper_authors: Shunta Maeda</li>
<li>for: 用于生成图像</li>
<li>methods: 使用随机Dropout模式和自适应编码器实现图像生成</li>
<li>results: 与DCGAN相比，Deciphering Autoencoders具有更稳定的训练过程和类似的样本质量<details>
<summary>Abstract</summary>
We propose a generative model termed Deciphering Autoencoders. In this model, we assign a unique random dropout pattern to each data point in the training dataset and then train an autoencoder to reconstruct the corresponding data point using this pattern as information to be encoded. Since the training of Deciphering Autoencoders relies solely on reconstruction error, it offers more stable training than other generative models. Despite its simplicity, Deciphering Autoencoders show comparable sampling quality to DCGAN on the CIFAR-10 dataset.
</details>
<details>
<summary>摘要</summary>
我们提出了一种生成模型，即解译自适应网络（Deciphering Autoencoders）。在这种模型中，我们对训练集中每个数据点分配了唯一的随机掉帽模式，然后使用这种模式作为编码信息，使用自适应网络来重建对应的数据点。由于训练解译自适应网络只依据重建错误来进行训练，因此它比其他生成模型更稳定。尽管简单，但解译自适应网络在CIFAR-10数据集上的采样质量与DCGAN相当。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/03/cs.CV_2023_10_03/" data-id="closbrop600je0g8891e8f915" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/03/cs.AI_2023_10_03/" class="article-date">
  <time datetime="2023-10-03T12:00:00.000Z" itemprop="datePublished">2023-10-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/03/cs.AI_2023_10_03/">cs.AI - 2023-10-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Large-Language-Models-Can-Be-Good-Privacy-Protection-Learners"><a href="#Large-Language-Models-Can-Be-Good-Privacy-Protection-Learners" class="headerlink" title="Large Language Models Can Be Good Privacy Protection Learners"></a>Large Language Models Can Be Good Privacy Protection Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02469">http://arxiv.org/abs/2310.02469</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Yijia-Xiao/PPLM">https://github.com/Yijia-Xiao/PPLM</a></li>
<li>paper_authors: Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Haifeng Chen, Wei Wang, Wei Cheng</li>
<li>for: 本研究旨在 Addressing the challenge of fine-tuning Large Language Models (LLMs) with domain-specific data while protecting sensitive personally identifiable information (PII).</li>
<li>methods: 我们提出了一种新的 Fine-tuning Large Language Models (PPLM) 模型，包括 corpus curation, penalty-based unlikelihood in training loss, 和 instruction-based tuning 等多种技术。</li>
<li>results: 我们的实验表明， instruction tuning with both positive and negative examples 是一种有效的方法，可以保护 private data 的隐私，同时提高模型的知识。<details>
<summary>Abstract</summary>
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. Our work underscores the potential for Large Language Models as robust privacy protection learners.
</details>
<details>
<summary>摘要</summary>
LLM的普及已经引起了特定领域数据的特点化语言模型（PPLM）的较大兴趣。然而，这些领域特定的精细化数据经常包含敏感的个人认izable信息（PII）。直接将LLM直接精细化这些数据无法保护隐私。为解决这个挑战，我们介绍了隐私保护语言模型（PPLM），一种新的范例，可以有效地注入领域特定的知识，同时保护数据隐私。我们的工作提供了模型设计的理论分析，以及各种技术，如文献筛选、罚金基于训练损失、指令调整等等。我们在多个数据集和场景进行了广泛的实验，并证明了我们的方法的有效性。特别是使用正例和负例的指令调整方法，表现出色，能够保护隐私数据，同时提高模型的知识。我们的工作强调了LLM的潜在作为隐私保护学习器的潜力。
</details></li>
</ul>
<hr>
<h2 id="EcoAssistant-Using-LLM-Assistant-More-Affordably-and-Accurately"><a href="#EcoAssistant-Using-LLM-Assistant-More-Affordably-and-Accurately" class="headerlink" title="EcoAssistant: Using LLM Assistant More Affordably and Accurately"></a>EcoAssistant: Using LLM Assistant More Affordably and Accurately</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03046">http://arxiv.org/abs/2310.03046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jieyuz2/ecoassistant">https://github.com/jieyuz2/ecoassistant</a></li>
<li>paper_authors: Jieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah, Chi Wang</li>
<li>for: 本研究旨在提高大自然语言模型（LLM）作为助手 answering 需要外部知识的查询，以提高效率和准确性。</li>
<li>methods: 本研究提出了一个框架，名为 EcoAssistant，它使得 LLM 可以更加经济高效地回答 code-driven 查询。EcoAssistant 包括三部分：首先，它允许 LLM 助手与自动代码执行器进行交互，以便Iteratively 更新代码或根据执行结果生成答案。其次，我们使用层次结构的 LLM 助手，首先使用较弱、较便宜的 LLM 尝试回答查询，然后如果无法回答，则交给更强、更昂贵的 LLM 尝试。最后，我们从成功过去查询中检索出示例，以帮助后续查询。</li>
<li>results: 我们通过实验表明，EcoAssistant 可以提供更高效和准确的答案，比 GPT-4 高出10点成功率，仅使用 Less than 50% 的 GPT-4 的成本。<details>
<summary>Abstract</summary>
Today, users ask Large language models (LLMs) as assistants to answer queries that require external knowledge; they ask about the weather in a specific city, about stock prices, and even about where specific locations are within their neighborhood. These queries require the LLM to produce code that invokes external APIs to answer the user's question, yet LLMs rarely produce correct code on the first try, requiring iterative code refinement upon execution results. In addition, using LLM assistants to support high query volumes can be expensive. In this work, we contribute a framework, EcoAssistant, that enables LLMs to answer code-driven queries more affordably and accurately. EcoAssistant contains three components. First, it allows the LLM assistants to converse with an automatic code executor to iteratively refine code or to produce answers based on the execution results. Second, we use a hierarchy of LLM assistants, which attempts to answer the query with weaker, cheaper LLMs before backing off to stronger, expensive ones. Third, we retrieve solutions from past successful queries as in-context demonstrations to help subsequent queries. Empirically, we show that EcoAssistant offers distinct advantages for affordability and accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of GPT-4's cost.
</details>
<details>
<summary>摘要</summary>
EcoAssistant consists of three components:1. Conversing with an automatic code executor: LLM assistants can converse with an automatic code executor to iteratively refine code or produce answers based on execution results.2. Hierarchy of LLM assistants: We use a hierarchy of LLM assistants to answer queries with weaker, cheaper LLMs before backing off to stronger, more expensive ones.3. Retrieving solutions from past successful queries: We retrieve solutions from past successful queries as in-context demonstrations to help subsequent queries.Empirically, we show that EcoAssistant offers distinct advantages for affordability and accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of GPT-4's cost.
</details></li>
</ul>
<hr>
<h2 id="Improved-Inference-of-Human-Intent-by-Combining-Plan-Recognition-and-Language-Feedback"><a href="#Improved-Inference-of-Human-Intent-by-Combining-Plan-Recognition-and-Language-Feedback" class="headerlink" title="Improved Inference of Human Intent by Combining Plan Recognition and Language Feedback"></a>Improved Inference of Human Intent by Combining Plan Recognition and Language Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02462">http://arxiv.org/abs/2310.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ifrah Idrees, Tian Yun, Naveen Sharma, Yunxin Deng, Nakul Gopalan, George Konidaris, Stefanie Tellex</li>
<li>for: 本研究旨在帮助机器人更好地理解人类计划和目标，尤其是在人类动作受到干扰时。</li>
<li>methods: 本研究使用对话 для计划认识（Dialogue for Goal Recognition，D4GR），允许机器人通过自然语言交互 rectify its belief in human progress。</li>
<li>results: 对比HTN，D4GR在两个模拟Domain中表现出优异，具体来说，在 kitchen 和 blocks Domain中，D4GR 在高度噪音下表现1%和4%更高的目标准确率，在 plan 准确率方面，D4GR 在 kitchen Domain中表现2%更高，在 blocks Domain中表现7%更高。<details>
<summary>Abstract</summary>
Conversational assistive robots can aid people, especially those with cognitive impairments, to accomplish various tasks such as cooking meals, performing exercises, or operating machines. However, to interact with people effectively, robots must recognize human plans and goals from noisy observations of human actions, even when the user acts sub-optimally. Previous works on Plan and Goal Recognition (PGR) as planning have used hierarchical task networks (HTN) to model the actor/human. However, these techniques are insufficient as they do not have user engagement via natural modes of interaction such as language. Moreover, they have no mechanisms to let users, especially those with cognitive impairments, know of a deviation from their original plan or about any sub-optimal actions taken towards their goal. We propose a novel framework for plan and goal recognition in partially observable domains -- Dialogue for Goal Recognition (D4GR) enabling a robot to rectify its belief in human progress by asking clarification questions about noisy sensor data and sub-optimal human actions. We evaluate the performance of D4GR over two simulated domains -- kitchen and blocks domain. With language feedback and the world state information in a hierarchical task model, we show that D4GR framework for the highest sensor noise performs 1% better than HTN in goal accuracy in both domains. For plan accuracy, D4GR outperforms by 4% in the kitchen domain and 2% in the blocks domain in comparison to HTN. The ALWAYS-ASK oracle outperforms our policy by 3% in goal recognition and 7%in plan recognition. D4GR does so by asking 68% fewer questions than an oracle baseline. We also demonstrate a real-world robot scenario in the kitchen domain, validating the improved plan and goal recognition of D4GR in a realistic setting.
</details>
<details>
<summary>摘要</summary>
Dialogue for Goal Recognition (D4GR) 是一种新的框架，用于在 partially observable  domains 中进行计划和目标识别。它可以让机器人通过对人类动作的识别和语言反馈来更好地理解人类的计划和目标。在我们的实验中，我们发现 D4GR 在面临高度噪音的感知器件情况下表现比 HTN 高一 percentage point 的目标准确率和计划准确率。此外，D4GR 还可以避免向用户提问太多问题，相比 oracle 基线下的 Always-Ask 政策。我们还 validate 了 D4GR 在实际 kitchen 环境中的应用场景，证明了它在真实情况下的改进计划和目标识别能力。Here's a word-for-word translation of the text into Simplified Chinese:Dialogue for Goal Recognition (D4GR) 是一种新的框架，用于在 partially observable  domains 中进行计划和目标识别。它可以让机器人通过对人类动作的识别和语言反馈来更好地理解人类的计划和目标。在我们的实验中，我们发现 D4GR 在面临高度噪音的感知器件情况下表现比 HTN 高一 percentage point 的目标准确率和计划准确率。此外，D4GR 还可以避免向用户提问太多问题，相比 oracle 基线下的 Always-Ask 政策。我们还 validate 了 D4GR 在实际 kitchen 环境中的应用场景，证明了它在真实情况下的改进计划和目标识别能力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Optimal-Advantage-from-Preferences-and-Mistaking-it-for-Reward"><a href="#Learning-Optimal-Advantage-from-Preferences-and-Mistaking-it-for-Reward" class="headerlink" title="Learning Optimal Advantage from Preferences and Mistaking it for Reward"></a>Learning Optimal Advantage from Preferences and Mistaking it for Reward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02456">http://arxiv.org/abs/2310.02456</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Stephanehk/Learning-OA-From-Prefs">https://github.com/Stephanehk/Learning-OA-From-Prefs</a></li>
<li>paper_authors: W. Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson, Serena Booth, Anca Dragan, Peter Stone, Scott Niekum</li>
<li>for: 本文研究了从人类反馈中学习抽象函数，具体来说是研究了RLHF中人类偏好的模型。</li>
<li>methods: 本文使用了人类反馈中的偏好来学习抽象函数，并对RLHF中的偏好模型进行了调查和分析。</li>
<li>results: 本文发现，假设人类偏好是基于奖励的时候，实际上是基于后悔的。这导致了学习的抽象函数不是真正的奖励函数，而是一个近似的优先级函数。此外，本文还发现，如果解决一个特定的坑，这种错误的假设不会对学习造成很大的影响，而且可以得到一个高度形态的奖励函数。<details>
<summary>Abstract</summary>
We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, $\hat{A^*_r}$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of $\hat{A^*_r}$ is less desirable than the appropriate and simpler approach of greedy maximization of $\hat{A^*_r}$. From the perspective of the regret preference model, we also provide a clearer interpretation of fine tuning contemporary large language models with RLHF. This paper overall provides insight regarding why learning under the partial return preference model tends to work so well in practice, despite it conforming poorly to how humans give preferences.
</details>
<details>
<summary>摘要</summary>
我们考虑使用人工智能来学习奖励函数从人类偏好中，这种学习方法被称为人类反馈学习（RLHF）。大多数最新的研究假设人类偏好是基于每段路径段的奖励，或者它们的归还。但是，最近的研究表明这个假设可能不正确，而是基于 regret的偏好模型。我们调查了假设偏好是基于归还的结果，而不是奖励的结果的后果，并 argue that the learned function is an approximation of the optimal advantage function, $\hat{A^*_r}$, not a reward function. 我们发现，如果一个特定的陷阱被解决，这个错误的假设并不会导致严重的后果，结果是一个高度形状的奖励函数。然而，这个错误的使用 $\hat{A^*_r}$ 比较不愉快，比如使用更简单的方法，例如对 contemporary large language models 的 fine-tuning。从 regret 偏好模型的角度来看，我们也提供了更清晰的对RLHF的解释，这篇文章总体提供了关于为什么在实践中，学习基于偏好的方法能够工作 så 好的解释。
</details></li>
</ul>
<hr>
<h2 id="Low-Resource-Languages-Jailbreak-GPT-4"><a href="#Low-Resource-Languages-Jailbreak-GPT-4" class="headerlink" title="Low-Resource Languages Jailbreak GPT-4"></a>Low-Resource Languages Jailbreak GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02446">http://arxiv.org/abs/2310.02446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng-Xin Yong, Cristina Menghini, Stephen H. Bach</li>
<li>for: 防止大语言模型生成危险内容</li>
<li>methods: 使用翻译攻击绕过安全训练数据的语言不平等性</li>
<li>results: 成功绕过GPT-4的安全保护，79%的时间能够帮助用户达到危险目标，其他高&#x2F;中资源语言的攻击成功率远低于这个水平。<details>
<summary>Abstract</summary>
AI safety training and red-teaming of large language models (LLMs) are measures to mitigate the generation of unsafe content. Our work exposes the inherent cross-lingual vulnerability of these safety mechanisms, resulting from the linguistic inequality of safety training data, by successfully circumventing GPT-4's safeguard through translating unsafe English inputs into low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe translated inputs and provides actionable items that can get the users towards their harmful goals 79% of the time, which is on par with or even surpassing state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have significantly lower attack success rate, which suggests that the cross-lingual vulnerability mainly applies to low-resource languages. Previously, limited training on low-resource languages primarily affects speakers of those languages, causing technological disparities. However, our work highlights a crucial shift: this deficiency now poses a risk to all LLMs users. Publicly available translation APIs enable anyone to exploit LLMs' safety vulnerabilities. Therefore, our work calls for a more holistic red-teaming efforts to develop robust multilingual safeguards with wide language coverage.
</details>
<details>
<summary>摘要</summary>
人工智能安全训练和大语言模型（LLM）红人攻击是为了遏制不安全内容的产生。我们的工作暴露了这些安全机制的内置跨语言漏洞，由于安全训练数据的语言不均衡，可以成功绕过GPT-4的安全措施，通过翻译不安全的英语输入到低资源语言。在AdvBenchmark上，GPT-4与这些不安全翻译输入交互，提供了79%的时间可以帮助用户实现危害目标，与当前的监狱突破攻击相当或超越。其他高/中资源语言的攻击成功率较低，这表明跨语言漏洞主要适用于低资源语言。在过去，对低资源语言的有限训练主要影响了那些语言的 speaker，导致技术差距。然而，我们的工作表明一种重要的转变：这种不足现在对所有LLM用户构成风险。公共可用的翻译API使得任何人可以利用LLM的安全漏洞。因此，我们的工作呼吁于发展抗频率的多语言安全措施，以具有广泛的语言覆盖率。
</details></li>
</ul>
<hr>
<h2 id="Learning-Diverse-Skills-for-Local-Navigation-under-Multi-constraint-Optimality"><a href="#Learning-Diverse-Skills-for-Local-Navigation-under-Multi-constraint-Optimality" class="headerlink" title="Learning Diverse Skills for Local Navigation under Multi-constraint Optimality"></a>Learning Diverse Skills for Local Navigation under Multi-constraint Optimality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02440">http://arxiv.org/abs/2310.02440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Cheng, Marin Vlastelica, Pavel Kolev, Chenhao Li, Georg Martius</li>
<li>for: 本研究的目的是提出一种基于数据驱动控制的方法，以实现在机器人控制中获得多样化的行为。</li>
<li>methods: 本研究使用了受限优化视角，通过不同的奖励函数来定义多样化的策略，并通过吸引-撕裂奖励来控制多样化水平。</li>
<li>results: 研究中使用了一个本地导航任务，训练了一个四脚机器人，并成功实现了多样化的快速行为，包括成功绕过障碍物。<details>
<summary>Abstract</summary>
Despite many successful applications of data-driven control in robotics, extracting meaningful diverse behaviors remains a challenge. Typically, task performance needs to be compromised in order to achieve diversity. In many scenarios, task requirements are specified as a multitude of reward terms, each requiring a different trade-off. In this work, we take a constrained optimization viewpoint on the quality-diversity trade-off and show that we can obtain diverse policies while imposing constraints on their value functions which are defined through distinct rewards. In line with previous work, further control of the diversity level can be achieved through an attract-repel reward term motivated by the Van der Waals force. We demonstrate the effectiveness of our method on a local navigation task where a quadruped robot needs to reach the target within a finite horizon. Finally, our trained policies transfer well to the real 12-DoF quadruped robot, Solo12, and exhibit diverse agile behaviors with successful obstacle traversal.
</details>
<details>
<summary>摘要</summary>
尽管数据驱动控制在机器人中得到了许多成功应用，但抽取有意义的多样行为仍然是一个挑战。通常，完成任务所需的牺牲是获得多样性的代价。在许多场景下，任务要求通常是多种奖励项，每个奖励项需要不同的让拍。在这项工作中，我们采取了受限优化的质量多样性观点，并证明我们可以在其价值函数中做出约束，这些价值函数通过不同的奖励定义。与之前的工作一样，进一步控制多样性水平可以通过通过招引-抗拒奖励项来实现，这种奖励项源于万达力力。我们在本地导航任务中展示了我们的方法的有效性，其中一个四脚机器人需要在有限时间内达到目标。最后，我们在真实的12度自由度四脚机器人（Solo12）上训练了我们的策略，并示出了多样的快速行为，并成功避免了障碍物。
</details></li>
</ul>
<hr>
<h2 id="Multi-Agent-Reinforcement-Learning-Based-on-Representational-Communication-for-Large-Scale-Traffic-Signal-Control"><a href="#Multi-Agent-Reinforcement-Learning-Based-on-Representational-Communication-for-Large-Scale-Traffic-Signal-Control" class="headerlink" title="Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control"></a>Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02435">http://arxiv.org/abs/2310.02435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Bokade, Xiaoning Jin, Christopher Amato</li>
<li>for: 提高大规模交通信号控制（TSC）的效率和灵活性。</li>
<li>methods: 使用多代理人学习（MARL）和选择性通信策略，使代理人可以只在需要时使用通信频道，从而减少干扰和提高总性能。</li>
<li>results: 在 synthetic $4 \times 4$ 网络和实际的 Pasubio 社区网络上实现了最低的网络压力，代理人使用了 $\sim 47-65 %$ 的通信频道。精mitigation 研究还证明了选择性通信策略的有效性。<details>
<summary>Abstract</summary>
Traffic signal control (TSC) is a challenging problem within intelligent transportation systems and has been tackled using multi-agent reinforcement learning (MARL). While centralized approaches are often infeasible for large-scale TSC problems, decentralized approaches provide scalability but introduce new challenges, such as partial observability. Communication plays a critical role in decentralized MARL, as agents must learn to exchange information using messages to better understand the system and achieve effective coordination. Deep MARL has been used to enable inter-agent communication by learning communication protocols in a differentiable manner. However, many deep MARL communication frameworks proposed for TSC allow agents to communicate with all other agents at all times, which can add to the existing noise in the system and degrade overall performance. In this study, we propose a communication-based MARL framework for large-scale TSC. Our framework allows each agent to learn a communication policy that dictates "which" part of the message is sent "to whom". In essence, our framework enables agents to selectively choose the recipients of their messages and exchange variable length messages with them. This results in a decentralized and flexible communication mechanism in which agents can effectively use the communication channel only when necessary. We designed two networks, a synthetic $4 \times 4$ grid network and a real-world network based on the Pasubio neighborhood in Bologna. Our framework achieved the lowest network congestion compared to related methods, with agents utilizing $\sim 47-65 \%$ of the communication channel. Ablation studies further demonstrated the effectiveness of the communication policies learned within our framework.
</details>
<details>
<summary>摘要</summary>
traffic signal control (TSC) 是智能交通系统中的一个挑战，通过多代理激励学习 (MARL) 进行解决。而中央化方法通常对大规模 TSC 问题不可行，而分布式方法则提供了可扩展性，但引入了新的挑战，如部分可见性。通信在分布式 MARL 中扮演了关键角色，代理需要通过消息交换来更好地理解系统并实现有效协调。深度 MARL 已经用于启用代理之间的交流，但大多数深度 MARL 通信框架在 TSC 中允许代理与所有其他代理进行通信，这会增加系统中的噪音并降低总性能。在本研究中，我们提出了一种基于通信的分布式 MARL 框架 для大规模 TSC。我们的框架允许每个代理学习一个通信策略，这个策略决定“向哪里”发送“什么”部分的消息。这种分布式和灵活的通信机制使代理可以只在需要时使用通信频道，从而有效地利用通信频道。我们设计了两个网络：一个 synthetic 的 $4 \times 4$ 网格网络和一个基于博洛尼亚的 Pasubio  neighborghood 网络。我们的框架在相关方法中实现了最低的网络拥塞率，代理在通信频道上使用了 $\sim 47-65 \%$。剥离研究还证明了我们的框架中学习的通信策略的效果。
</details></li>
</ul>
<hr>
<h2 id="Episodic-Memory-Theory-for-the-Mechanistic-Interpretation-of-Recurrent-Neural-Networks"><a href="#Episodic-Memory-Theory-for-the-Mechanistic-Interpretation-of-Recurrent-Neural-Networks" class="headerlink" title="Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks"></a>Episodic Memory Theory for the Mechanistic Interpretation of Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02430">http://arxiv.org/abs/2310.02430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arjun Karuvally, Peter Delmastro, Hava T. Siegelmann</li>
<li>For: The paper aims to provide a deeper understanding of the internal mechanisms of Recurrent Neural Networks (RNNs) and their relationship to human memory.* Methods: The paper proposes the Episodic Memory Theory (EMT), which conceptualizes RNNs as discrete-time analogs of the General Sequential Episodic Memory Model. The authors also introduce a set of algorithmic tasks to probe the variable binding behavior in RNNs and develop a mathematically rigorous circuit to facilitate variable binding.* Results: The paper shows that trained RNNs consistently converge to the variable binding circuit, indicating universality in the dynamics of RNNs. The authors also develop an algorithm to define a privileged basis, which enhances the interpretability of the learned parameters and hidden states of RNNs.Here is the same information in Simplified Chinese:* For: 本研究旨在深入理解循环神经网络（RNN）的内部机制以及它们与人类记忆之间的关系。* Methods: 本研究提出了 episodic memory theory（EMT），它将 RNN 看作是时间排序的 discrete-time 分析。作者还提出了一系列用于探索 RNN 中变量绑定行为的算法任务，并开发了一个正则的数学圈来促进变量绑定。* Results: 研究显示，训练后 RNN 通常会 converge 到变量绑定圈，这表明 RNN 的动态是统一的。作者还开发了一种算法来定义特权基底，该基底可以增强 RNN 学习的参数和隐藏状态的解释性。<details>
<summary>Abstract</summary>
Understanding the intricate operations of Recurrent Neural Networks (RNNs) mechanistically is pivotal for advancing their capabilities and applications. In this pursuit, we propose the Episodic Memory Theory (EMT), illustrating that RNNs can be conceptualized as discrete-time analogs of the recently proposed General Sequential Episodic Memory Model. To substantiate EMT, we introduce a novel set of algorithmic tasks tailored to probe the variable binding behavior in RNNs. Utilizing the EMT, we formulate a mathematically rigorous circuit that facilitates variable binding in these tasks. Our empirical investigations reveal that trained RNNs consistently converge to the variable binding circuit, thus indicating universality in the dynamics of RNNs. Building on these findings, we devise an algorithm to define a privileged basis, which reveals hidden neurons instrumental in the temporal storage and composition of variables, a mechanism vital for the successful generalization in these tasks. We show that the privileged basis enhances the interpretability of the learned parameters and hidden states of RNNs. Our work represents a step toward demystifying the internal mechanisms of RNNs and, for computational neuroscience, serves to bridge the gap between artificial neural networks and neural memory models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AXNav-Replaying-Accessibility-Tests-from-Natural-Language"><a href="#AXNav-Replaying-Accessibility-Tests-from-Natural-Language" class="headerlink" title="AXNav: Replaying Accessibility Tests from Natural Language"></a>AXNav: Replaying Accessibility Tests from Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02424">http://arxiv.org/abs/2310.02424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng, Yue Jiang, Jeffrey Nichols<br>for: 这个论文是为了支持访问性测试而开发的一种自然语言基于的测试工作流程。methods: 这个系统使用自然语言处理和图像理解模型来执行手动访问性测试，并生成一个分章、可导航的视频。results: 在一个10名参与者的用户研究中，参与者表示这个工具会很有用于他们的当前工作，并表现与手动测试方式相似。研究还揭示了未来使用LLMs进行访问性测试的可能性。<details>
<summary>Abstract</summary>
Developers and quality assurance testers often rely on manual testing to test accessibility features throughout the product lifecycle. Unfortunately, manual testing can be tedious, often has an overwhelming scope, and can be difficult to schedule amongst other development milestones. Recently, Large Language Models (LLMs) have been used for a variety of tasks including automation of UIs, however to our knowledge no one has yet explored their use in controlling assistive technologies for the purposes of supporting accessibility testing. In this paper, we explore the requirements of a natural language based accessibility testing workflow, starting with a formative study. From this we build a system that takes as input a manual accessibility test (e.g., ``Search for a show in VoiceOver'') and uses an LLM combined with pixel-based UI Understanding models to execute the test and produce a chaptered, navigable video. In each video, to help QA testers we apply heuristics to detect and flag accessibility issues (e.g., Text size not increasing with Large Text enabled, VoiceOver navigation loops). We evaluate this system through a 10 participant user study with accessibility QA professionals who indicated that the tool would be very useful in their current work and performed tests similarly to how they would manually test the features. The study also reveals insights for future work on using LLMs for accessibility testing.
</details>
<details>
<summary>摘要</summary>
开发者和质量保证测试员经常采用手动测试来测试产品的可用性功能。然而，手动测试可能是压力很大，让人感觉压力很大，并且可能与其他开发阶段的时间安排不协调。在最近，大型自然语言模型（LLM）已经用于许多任务，包括 UI 自动化。然而，我们知道没有任何一个研究用于控制助助技术来支持可用性测试。在这篇论文中，我们探讨了可用性测试工作流的自然语言需求，从而开始一个形成性研究。我们将手动可用性测试（例如，“搜索voiceover中的节目”）作为输入，使用 LLM 和像素基的 UI 理解模型来执行测试并生成一个分割、导航的视频。在每个视频中，我们应用了规则来检测和标记可用性问题（例如，文本大小不随大字体开启增加）。我们通过10名参与者的用户研究，发现这个工具会在现有工作中对可用性测试非常有用，并且与手动测试相似。研究还揭示了将 LLM 应用于可用性测试的未来工作方向。
</details></li>
</ul>
<hr>
<h2 id="OneAdapt-Fast-Adaptation-for-Deep-Learning-Applications-via-Backpropagation"><a href="#OneAdapt-Fast-Adaptation-for-Deep-Learning-Applications-via-Backpropagation" class="headerlink" title="OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation"></a>OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02422">http://arxiv.org/abs/2310.02422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuntai Du, Yuhan Liu, Yitian Hao, Qizheng Zhang, Haodong Wang, Yuyang Huang, Ganesh Ananthanarayanan, Junchen Jiang<br>for:* 这个论文旨在提高流媒体数据中深度学习推理的性能，包括视频中对象检测和音频波中文本提取。methods:* 这个论文使用了一种名为OneAdapt的适配策略，利用梯度上升策略来适配配置螺旋钢，以提高深度学习模型的推理精度。results:* 相比之前的适配策略，OneAdapt可以在不同的配置螺旋钢下减少网络带宽使用量和GPU资源使用量，同时保持相同的准确率或提高准确率。<details>
<summary>Abstract</summary>
Deep learning inference on streaming media data, such as object detection in video or LiDAR feeds and text extraction from audio waves, is now ubiquitous. To achieve high inference accuracy, these applications typically require significant network bandwidth to gather high-fidelity data and extensive GPU resources to run deep neural networks (DNNs). While the high demand for network bandwidth and GPU resources could be substantially reduced by optimally adapting the configuration knobs, such as video resolution and frame rate, current adaptation techniques fail to meet three requirements simultaneously: adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to reach near-optimal decisions based on how the data affects the final DNN's accuracy, and (iii) do so for a range of configuration knobs. This paper presents OneAdapt, which meets these requirements by leveraging a gradient-ascent strategy to adapt configuration knobs. The key idea is to embrace DNNs' differentiability to quickly estimate the accuracy's gradient to each configuration knob, called AccGrad. Specifically, OneAdapt estimates AccGrad by multiplying two gradients: InputGrad (i.e. how each configuration knob affects the input to the DNN) and DNNGrad (i.e. how the DNN input affects the DNN inference output). We evaluate OneAdapt across five types of configurations, four analytic tasks, and five types of input data. Compared to state-of-the-art adaptation schemes, OneAdapt cuts bandwidth usage and GPU usage by 15-59% while maintaining comparable accuracy or improves accuracy by 1-5% while using equal or fewer resources.
</details>
<details>
<summary>摘要</summary>
深度学习推理在流媒体数据上进行检测对象在视频或LiDAR耦合中的检测，以及从音频波中提取文本，现在已经普遍存在。为了实现高精度推理，这些应用通常需要很大的网络带宽和广泛的GPU资源来运行深度神经网络（DNN）。然而，高需求的网络带宽和GPU资源可以通过优化配置螺旋来减少，但现有的适配技术无法同时满足以下三个需求：1. 适配配置（i）的过程具有最小的额外GPU或带宽开销。2. 基于数据如何影响最终DNN的准确性来做近似优化的决策。3. 对配置螺旋进行范围内的适配。本文介绍了OneAdapt，它可以同时满足这些需求。OneAdapt利用了梯度上升策略来适配配置螺旋。关键思想是利用DNN的导数 differentiability来快速估算每个配置螺旋的准确性 gradient，称为AccGrad。具体来说，OneAdapt使用两个梯度的乘积来估算AccGrad：输入梯度（即每个配置螺旋对输入到DNN的影响）和DNN梯度（即输入到DNN的影响对DNN的推理输出）。我们对OneAdapt进行了五种配置、四种分析任务和五种输入数据的评估。相比之下，OneAdapt在带宽和GPU使用量方面减少了15-59%，同时维持了相对较高的准确性或提高了准确性1-5%，使用相同或少于资源。
</details></li>
</ul>
<hr>
<h2 id="Can-a-student-Large-Language-Model-perform-as-well-as-it’s-teacher"><a href="#Can-a-student-Large-Language-Model-perform-as-well-as-it’s-teacher" class="headerlink" title="Can a student Large Language Model perform as well as it’s teacher?"></a>Can a student Large Language Model perform as well as it’s teacher?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02421">http://arxiv.org/abs/2310.02421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sia Gholami, Marwan Omar</li>
<li>for: 这篇论文主要针对的是深度学习模型的部署问题，即在资源有限的环境中使用高精度模型的问题。</li>
<li>methods: 该论文提出了知识传承技术，即将高精度模型（教师模型）的知识传承给流lined模型（学生模型），以提高学生模型的性能。</li>
<li>results: 该论文通过仔细分析，探讨了成功的知识传承要素，包括学生模型的架构、教师模型的质量和超参数的均衡。<details>
<summary>Abstract</summary>
The burgeoning complexity of contemporary deep learning models, while achieving unparalleled accuracy, has inadvertently introduced deployment challenges in resource-constrained environments. Knowledge distillation, a technique aiming to transfer knowledge from a high-capacity "teacher" model to a streamlined "student" model, emerges as a promising solution to this dilemma. This paper provides a comprehensive overview of the knowledge distillation paradigm, emphasizing its foundational principles such as the utility of soft labels and the significance of temperature scaling. Through meticulous examination, we elucidate the critical determinants of successful distillation, including the architecture of the student model, the caliber of the teacher, and the delicate balance of hyperparameters. While acknowledging its profound advantages, we also delve into the complexities and challenges inherent in the process. Our exploration underscores knowledge distillation's potential as a pivotal technique in optimizing the trade-off between model performance and deployment efficiency.
</details>
<details>
<summary>摘要</summary>
现代深度学习模型的抬头复杂性，尽管达到了无 précédente 的准确性，却意外地引入了资源受限环境中的部署挑战。知识传递技术，目标是将知识从高容量 "老师" 模型传递到流lined "学生" 模型，被认为是解决这个困境的有望的解决方案。本文提供了知识传递 парадиг的全面概述，强调其基础原理，如软标签的利用和温度尺度的重要性。通过精心的检查，我们描述了成功传递的关键因素，包括学生模型的架构、老师模型的质量和敏感的超参数平衡。虽然承认其极大的优势，但我们也探讨了传递过程中的复杂性和挑战。我们的探索表明，知识传递可能成为优化模型性能和部署效率之间的关键技术。
</details></li>
</ul>
<hr>
<h2 id="Nugget-2D-Dynamic-Contextual-Compression-for-Scaling-Decoder-only-Language-Models"><a href="#Nugget-2D-Dynamic-Contextual-Compression-for-Scaling-Decoder-only-Language-Models" class="headerlink" title="Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models"></a>Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02409">http://arxiv.org/abs/2310.02409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van Durme</li>
<li>for: 提高慢速语言模型在长上下文中的缩放性能</li>
<li>methods: 使用动态上下文压缩，基于Qin &amp; Van Durme (2023)的Nugget方法，对decoder-only语言模型进行扩展</li>
<li>results: 通过语言模型、问答和概要等实验，显示Nugget2D可以保持这些任务的能力，同时在解码过程中减少时间和空间开销，比如在自编码任务中，Nugget2D可以将上下文压缩到20倍的比例，保持BLEU分数在98%之间，实现近乎无损编码。<details>
<summary>Abstract</summary>
Standard Transformer-based language models (LMs) scale poorly to long contexts. We propose a solution based on dynamic contextual compression, which extends the Nugget approach of Qin & Van Durme (2023) from BERT-like frameworks to decoder-only LMs. Our method models history as compressed "nuggets" which are trained to allow for reconstruction, and it can be initialized with off-the-shelf models such as LLaMA. We demonstrate through experiments in language modeling, question answering, and summarization that Nugget2D retains capabilities in these tasks, while drastically reducing the overhead during decoding in terms of time and space. For example, in the experiments of autoencoding, Nugget2D can shrink context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.
</details>
<details>
<summary>摘要</summary>
通用变换器基于语言模型（LM）在长上下文中表现不佳。我们提出一种解决方案，基于动态Contextual压缩，该方法在BERT类框架中的Qin & Van Durme（2023）的Nugget方法上进行扩展。我们的方法将历史作为压缩“块”，用于允许重建，并可以使用市场上的模型Initial化，如LLaMA。我们通过语言模型、问答和概要等实验表明，Nugget2D可以在这些任务中保持能力，同时在解码过程中减少时间和空间开销。例如，在自动编码实验中，Nugget2D可以将上下文压缩到20倍的压缩率，保持BLEU分数98%，实现近乎无损编码。
</details></li>
</ul>
<hr>
<h2 id="PCGPT-Procedural-Content-Generation-via-Transformers"><a href="#PCGPT-Procedural-Content-Generation-via-Transformers" class="headerlink" title="PCGPT: Procedural Content Generation via Transformers"></a>PCGPT: Procedural Content Generation via Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02405">http://arxiv.org/abs/2310.02405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajad Mohaghegh, Mohammad Amin Ramezan Dehnavi, Golnoosh Abdollahinejad, Matin Hashemi</li>
<li>for: 这个论文旨在提出一种基于线下强化学习和变换网络的PCG方法，以生成更加复杂和多样化的游戏内容。</li>
<li>methods: 该方法使用了一种基于变换器的autoregressive模型，模型着行为、状态和奖励的轨迹，利用变换器的自注意机制 capture时间相关性和 causal关系。</li>
<li>results: 实验结果表明，PCGPT在扮演游戏《苏可阔》中预测物品和它们的位置的任务上表现出色，生成的游戏内容更加复杂和多样化，并且在许多步骤上比前方法快速得到结果。<details>
<summary>Abstract</summary>
The paper presents the PCGPT framework, an innovative approach to procedural content generation (PCG) using offline reinforcement learning and transformer networks. PCGPT utilizes an autoregressive model based on transformers to generate game levels iteratively, addressing the challenges of traditional PCG methods such as repetitive, predictable, or inconsistent content. The framework models trajectories of actions, states, and rewards, leveraging the transformer's self-attention mechanism to capture temporal dependencies and causal relationships. The approach is evaluated in the Sokoban puzzle game, where the model predicts items that are needed with their corresponding locations. Experimental results on the game Sokoban demonstrate that PCGPT generates more complex and diverse game content. Interestingly, it achieves these results in significantly fewer steps compared to existing methods, showcasing its potential for enhancing game design and online content generation. Our model represents a new PCG paradigm which outperforms previous methods.
</details>
<details>
<summary>摘要</summary>
文章提出了PCGPT框架，这是一种新型的游戏内容生成（PCG）方法，使用了离线强化学习和转换网络。PCGPT使用基于转换器的抽象模型来逐步生成游戏等级，解决传统PCG方法中的困难，如重复、预测性和不一致的内容。框架模型了行为轨迹、状态和奖励的关系，利用转换器的自注意机制来捕捉时间相关性和 causal 关系。这种方法在拼图游戏中进行了实验，模型预测了需要的物品和它们的位置。实验结果表明，PCGPT生成的游戏内容更复杂和多样化，并且在比较少的步骤内完成了任务，表明它在游戏设计和在线内容生成方面具有潜在的应用前景。我们的模型代表了一种新的PCG paradigm，比 précédente 方法更高效。
</details></li>
</ul>
<hr>
<h2 id="AutoDAN-Generating-Stealthy-Jailbreak-Prompts-on-Aligned-Large-Language-Models"><a href="#AutoDAN-Generating-Stealthy-Jailbreak-Prompts-on-Aligned-Large-Language-Models" class="headerlink" title="AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"></a>AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04451">http://arxiv.org/abs/2310.04451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sheltonliu-n/autodan">https://github.com/sheltonliu-n/autodan</a></li>
<li>paper_authors: Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao</li>
<li>for:  investigate jailbreak attacks on aligned large language models (LLMs) and develop a novel approach to automatically generate stealthy jailbreak prompts</li>
<li>methods:  hierarchical genetic algorithm to generate stealthy jailbreak prompts that preserve semantic meaningfulness</li>
<li>results:  AutoDAN demonstrates superior attack strength in cross-model transferability and cross-sample universality compared with the baseline, and can effectively bypass perplexity-based defense methods<details>
<summary>Abstract</summary>
The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce AutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can automatically generate stealthy jailbreak prompts by the carefully designed hierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN not only automates the process while preserving semantic meaningfulness, but also demonstrates superior attack strength in cross-model transferability, and cross-sample universality compared with the baseline. Moreover, we also compare AutoDAN with perplexity-based defense methods and show that AutoDAN can bypass them effectively.
</details>
<details>
<summary>摘要</summary>
协调大语言模型（LLM）是强大的语言理解和决策工具，通过广泛的协调和人类反馈来创建。然而，这些大模型仍然易受到破禁攻击，敌对者可以通过 manipulate 提示来引发恶意输出，这些输出不应该由协调 LLM 提供。研究破禁提示可以让我们更深入了解 LLM 的限制，并引导我们如何加以安全。然而，现有的破禁技术受到了（1）扩展性问题，破禁攻击很大程度上依赖于手动制作提示，以及（2）隐蔽性问题，攻击都 rely 于基于 токен 的算法来生成提示，这些提示通常具有语义意义，使其易于检测。为了解决这些挑战，我们想回答以下问题：可以自动生成隐蔽破禁提示吗？在这篇论文中，我们介绍了一种新的破禁攻击方法——AutoDAN。AutoDAN 可以通过我们特制的层次遗传算法自动生成隐蔽破禁提示，并且能够保持语义意义。广泛的评估表明，AutoDAN 不仅自动化了过程，同时也示出了比基线更高的攻击力，包括跨模型传输性和跨样本一致性。此外，我们还与基于混淆测试的防御方法进行了比较，并证明 AutoDAN 可以效果地绕过它们。
</details></li>
</ul>
<hr>
<h2 id="SE-3-Stochastic-Flow-Matching-for-Protein-Backbone-Generation"><a href="#SE-3-Stochastic-Flow-Matching-for-Protein-Backbone-Generation" class="headerlink" title="SE(3)-Stochastic Flow Matching for Protein Backbone Generation"></a>SE(3)-Stochastic Flow Matching for Protein Backbone Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02391">http://arxiv.org/abs/2310.02391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dreamfold/foldflow">https://github.com/dreamfold/foldflow</a></li>
<li>paper_authors: Avishek Joey Bose, Tara Akhound-Sadegh, Kilian Fatras, Guillaume Huguet, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, Alexander Tong</li>
<li>For: The paper is focused on the computational design of novel protein structures, specifically using a series of novel generative models based on the flow-matching paradigm over 3D rigid motions (i.e. the group SE(3)).* Methods: The paper introduces three novel generative models, starting with FoldFlow-Base, which is a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on SE(3). The authors then accelerate training by incorporating Riemannian optimal transport to create FoldFlow-OT, and finally, they design FoldFlow-SFM, which couples both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over SE(3).* Results: The paper reports high-quality designable, diverse, and novel protein backbone samples generated using the FoldFlow models, validating their effectiveness in the computational design of novel protein structures.<details>
<summary>Abstract</summary>
The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce $\text{FoldFlow}$ a series of novel generative models of increasing modeling power based on the flow-matching paradigm over $3\text{D}$ rigid motions -- i.e. the group $\text{SE(3)}$ -- enabling accurate modeling of protein backbones. We first introduce $\text{FoldFlow-Base}$, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on $\text{SE(3)}$. We next accelerate training by incorporating Riemannian optimal transport to create $\text{FoldFlow-OT}$, leading to the construction of both more simple and stable flows. Finally, we design $\text{FoldFlow-SFM}$ coupling both Riemannian OT and simulation-free training to learn stochastic continuous-time dynamics over $\text{SE(3)}$. Our family of $\text{FoldFlow}$ generative models offer several key advantages over previous approaches to the generative modeling of proteins: they are more stable and faster to train than diffusion-based approaches, and our models enjoy the ability to map any invariant source distribution to any invariant target distribution over $\text{SE(3)}$. Empirically, we validate our FoldFlow models on protein backbone generation of up to $300$ amino acids leading to high-quality designable, diverse, and novel samples.
</details>
<details>
<summary>摘要</summary>
Computational 设计新蛋白结构具有很大的科学影响 potential. 为了实现这个目标，我们介绍 $\text{FoldFlow}$ 系列的新生成模型，基于流匹配方法在 $3\text{D}$ 摆动上的模型，具有高精度蛋白质量模型。我们首先介绍 $\text{FoldFlow-Base}$，一种不需要 simulate 的方法，用于学习 deterministic 连续时间动力学和匹配恒定目标分布在 $\text{SE(3)}$ 上。然后，我们通过 incorporating 里曼尼安全运输来加速训练，创建更简单和稳定的流。最后，我们设计 $\text{FoldFlow-SFM}$，将两种方法相互融合，以学习连续时间动力学在 $\text{SE(3)}$ 上的随机性。我们的 $\text{FoldFlow}$ 生成模型具有许多优势，比如更稳定和更快速地训练，而且我们的模型可以将任何恒定源分布映射到任何恒定目标分布上。在实验中，我们验证我们的 FoldFlow 模型在蛋白质量上的生成，可以获得高质量、多样化和创新的样本。
</details></li>
</ul>
<hr>
<h2 id="ProtoNER-Few-shot-Incremental-Learning-for-Named-Entity-Recognition-using-Prototypical-Networks"><a href="#ProtoNER-Few-shot-Incremental-Learning-for-Named-Entity-Recognition-using-Prototypical-Networks" class="headerlink" title="ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks"></a>ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02372">http://arxiv.org/abs/2310.02372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ritesh Kumar, Saurabh Goyal, Ashish Verma, Vatche Isahagian</li>
<li>for: 本研究旨在提高文档理解和数据EXTRACTION领域中KVP提取模型的泛化能力，以便在新的类别加入模型时，不需要重新标注整个训练集和重新训练模型。</li>
<li>methods: 我们提出了一种基于原型网络的KVP提取模型，即ProtoNER，它不需要原始训练集，也不需要生成中间的 sintetic数据，而且使用了混合损失函数，以保持模型对原来类别的知识，同时学习新增类别。</li>
<li>results: 实验结果显示，ProtoNER经过迁移30个样本后，能够达到新增类别的同等效果，与常规模型经过2600个样本迁移后的效果相当。<details>
<summary>Abstract</summary>
Key value pair (KVP) extraction or Named Entity Recognition(NER) from visually rich documents has been an active area of research in document understanding and data extraction domain. Several transformer based models such as LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art results. However, addition of even a single new class to the existing model requires (a) re-annotation of entire training dataset to include this new class and (b) retraining the model again. Both of these issues really slow down the deployment of updated model. \\ We present \textbf{ProtoNER}: Prototypical Network based end-to-end KVP extraction model that allows addition of new classes to an existing model while requiring minimal number of newly annotated training samples. The key contributions of our model are: (1) No dependency on dataset used for initial training of the model, which alleviates the need to retain original training dataset for longer duration as well as data re-annotation which is very time consuming task, (2) No intermediate synthetic data generation which tends to add noise and results in model's performance degradation, and (3) Hybrid loss function which allows model to retain knowledge about older classes as well as learn about newly added classes.\\ Experimental results show that ProtoNER finetuned with just 30 samples is able to achieve similar results for the newly added classes as that of regular model finetuned with 2600 samples.
</details>
<details>
<summary>摘要</summary>
“对于视觉丰富的文档中的键值对（KVP）抽象或名称实体识别（NER），有很多研究在文档理解和数据提取领域。一些基于Transformer的模型，如LayoutLMv2、LayoutLMv3和LiLT，已经取得了州际级的结果。但是，添加新的类别到现有模型中需要（1）重新标注整个训练 dataset，以包括这个新的类别，并（2）重新训练模型。这两个问题都会导致模型的部署受阻。”“我们提出了一个名为ProtoNER的专案网络基于模型，可以将新的类别添加到现有模型中，而不需要大量的新的标注训练数据。ProtoNER的关键贡献包括：（1）不需要原始训练 dataset，这解除了保留原始训练 dataset的时间问题和时间consuming的标注工作，（2）不需要生成中间的 sintetic 数据，这减少了模型的性能下降，（3）混合的损失函数，让模型保留旧有类别的知识，同时学习新添加的类别。”“实验结果显示，ProtoNER 与常规模型相比，仅需要30个标注数据进行 fine-tuning，就能够在新添加的类别上达到相同的结果。”
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Reinforcement-Learning-Approach-for-Interactive-Search-with-Sentence-level-Feedback"><a href="#A-Deep-Reinforcement-Learning-Approach-for-Interactive-Search-with-Sentence-level-Feedback" class="headerlink" title="A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback"></a>A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03043">http://arxiv.org/abs/2310.03043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianghong Zhou, Joyce C. Ho, Chen Lin, Eugene Agichtein</li>
<li>for: 本研究旨在提高搜寻系统的搜寻精度，通过融合用户的互动反馈，从而提供更好的搜寻体验。</li>
<li>methods: 本研究使用深度Q学习（DQ）方法，将BERT模型与用户互动反馈结合，选择重要的句子，以提高搜寻精度。此外，本研究还提出了两种 Mechanism来更好地探索优化的动作空间。</li>
<li>results: 本研究在三个搜寻dataset上验证了DQrank的效能，与前一代RL方法相比，DQrank能够提高搜寻精度至少12%。此外，本研究还进行了细部抽象研究，结果显示每个模型元件都能够有效地提取和累累长期的用户互动反馈效果。<details>
<summary>Abstract</summary>
Interactive search can provide a better experience by incorporating interaction feedback from the users. This can significantly improve search accuracy as it helps avoid irrelevant information and captures the users' search intents. Existing state-of-the-art (SOTA) systems use reinforcement learning (RL) models to incorporate the interactions but focus on item-level feedback, ignoring the fine-grained information found in sentence-level feedback. Yet such feedback requires extensive RL action space exploration and large amounts of annotated data. This work addresses these challenges by proposing a new deep Q-learning (DQ) approach, DQrank. DQrank adapts BERT-based models, the SOTA in natural language processing, to select crucial sentences based on users' engagement and rank the items to obtain more satisfactory responses. We also propose two mechanisms to better explore optimal actions. DQrank further utilizes the experience replay mechanism in DQ to store the feedback sentences to obtain a better initial ranking performance. We validate the effectiveness of DQrank on three search datasets. The results show that DQRank performs at least 12% better than the previous SOTA RL approaches. We also conduct detailed ablation studies. The ablation results demonstrate that each model component can efficiently extract and accumulate long-term engagement effects from the users' sentence-level feedback. This structure offers new technologies with promised performance to construct a search system with sentence-level interaction.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese.<</SYS>>互动搜索可以提供更好的体验，通过 incorporating 互动反馈从用户。这可以显著提高搜索准确性，因为它帮助避免无关信息和捕捉用户搜索意图。现有的状态之一 (SOTA) 系统使用 reinforcement learning (RL) 模型来 incorporating 互动，但是它们围绕 item-level 反馈进行围绕，忽略了 sentence-level 反馈中的细化信息。然而，这种反馈需要广泛的 RL 动作空间探索和大量标注数据。这个工作解决了这些挑战，通过提议一种新的深度 Q-学习 (DQ) 方法，DQrank。DQrank 采用 BERT-based 模型，当前最佳在自然语言处理中，选择用户互动中的关键句并将项目排序以获得更满足的回答。我们还提出了两种机制来更好地探索优化的动作。DQrank 进一步利用 DQ 中的经验回放机制，将反馈句子存储在 DQ 中，以获得更好的初始排名性能。我们验证了 DQRank 的效果，结果显示，DQRank 在三个搜索数据集上表现至少12%更好于之前的 SOTA RL 方法。我们还进行了详细的剖析研究。剖析结果表明，每个模型组件都能有效地提取和积累用户互动中的长期参与效果。这种结构提供了新的技术，用于构建基于 sentence-level 互动的搜索系统。
</details></li>
</ul>
<hr>
<h2 id="Prioritized-Soft-Q-Decomposition-for-Lexicographic-Reinforcement-Learning"><a href="#Prioritized-Soft-Q-Decomposition-for-Lexicographic-Reinforcement-Learning" class="headerlink" title="Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning"></a>Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02360">http://arxiv.org/abs/2310.02360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Finn Rietz, Stefan Heinrich, Erik Schaffernicht, Johannes Andreas Stork</li>
<li>for:  solves complex tasks by breaking them down into elementary subtasks and reusing subtask solutions</li>
<li>methods:  value decomposition, prioritized soft Q-decomposition (PSQD)</li>
<li>results:  successful learning, reuse, and adaptation results for simulated robot control tasks, and offline learning results without new environment interaction during adaptation.<details>
<summary>Abstract</summary>
Reinforcement learning (RL) for complex tasks remains a challenge, primarily due to the difficulties of engineering scalar reward functions and the inherent inefficiency of training models from scratch. Instead, it would be better to specify complex tasks in terms of elementary subtasks and to reuse subtask solutions whenever possible. In this work, we address continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. We show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. Exploiting this insight, we propose prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step. Its ability to use retained subtask training data for offline learning eliminates the need for new environment interaction during adaptation. We demonstrate the efficacy of our approach by presenting successful learning, reuse, and adaptation results for both low- and high-dimensional simulated robot control tasks, as well as offline learning results. In contrast to baseline approaches, PSQD does not trade off between conflicting subtasks or priority constraints and satisfies subtask priorities during learning. PSQD provides an intuitive framework for tackling complex RL problems, offering insights into the inner workings of the subtask composition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-definition-of-toxicity-in-NLP"><a href="#On-the-definition-of-toxicity-in-NLP" class="headerlink" title="On the definition of toxicity in NLP"></a>On the definition of toxicity in NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02357">http://arxiv.org/abs/2310.02357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergey Berezin, Reza Farahbakhsh, Noel Crespi</li>
<li>for: 提出了一个新的、基于着压力水平的毒性定义，以提高毒性检测任务的 объектив性和上下文感知。</li>
<li>methods: 该文提出了一种基于新定义的数据集创建和模型训练方法。</li>
<li>results: 该文未提出实际实验结果，但预期通过新定义和方法提高毒性检测任务的准确性和稳定性。<details>
<summary>Abstract</summary>
The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. This causes us to rely on subjective and vague data in models' training, which results in non-robust and non-accurate results: garbage in - garbage out.   This work suggests a new, stress-level-based definition of toxicity designed to be objective and context-aware. On par with it, we also describe possible ways of applying this new definition to dataset creation and model training.
</details>
<details>
<summary>摘要</summary>
基本问题在毒性探测任务中是毒性不具体定义。这导致我们需要基于主观和暧昧的数据进行模型训练，从而导致非Robust和不准确的结果：垃圾入口垃圾出口。本工作提出了一个新的压力水平基本定义毒性，以便具有对象和上下文意识。同时，我们还描述了应用这个新定义到数据集创建和模型训练的可能方法。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-about-Intuitionistic-Computation-Tree-Logic"><a href="#Reasoning-about-Intuitionistic-Computation-Tree-Logic" class="headerlink" title="Reasoning about Intuitionistic Computation Tree Logic"></a>Reasoning about Intuitionistic Computation Tree Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02355">http://arxiv.org/abs/2310.02355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Catta, Vadim Malvone, Aniello Murano</li>
<li>for: 本 paper 定义了一种INTRODUCTION TO COMPUTATION TREE LOGIC（CTL）的INTUITIONISTIC版本。</li>
<li>methods: 本 paper 首先介绍了INTUITIONISTIC逻辑的semantic特征，然后研究了这些特征在正式验证方面的可能性。接着，本 paper 定义了INTUITIONISTIC CTL的语法和 semantics，并研究了其一些简单的性质。</li>
<li>results: 本 paper  conclude 表明了INTUITIONISTIC CTL中的一些固定点规则不是VALID。<details>
<summary>Abstract</summary>
In this paper, we define an intuitionistic version of Computation Tree Logic. After explaining the semantic features of intuitionistic logic, we examine how these characteristics can be interesting for formal verification purposes. Subsequently, we define the syntax and semantics of our intuitionistic version of CTL and study some simple properties of the so obtained logic. We conclude by demonstrating that some fixed-point axioms of CTL are not valid in the intuitionistic version of CTL we have defined.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们定义了一种INTRODUCTION Computation Tree Logic的INTUITIONISTIC版本。我们首先介绍了INTUITIONISTIC逻辑的semantic特征，然后我们explore了这些特征在正式验证方面的可能性。接着，我们定义了INTUITIONISTIC CTL的语法和 semantics，并研究了这种逻辑的一些简单特性。最后，我们示例了INTUITIONISTIC CTL中的一些固定点论证不成立。Here's the breakdown of the translation:* "INTRODUCTION" is translated as "INTRODUCTION" (同 "Introduction" in English).* "Computation Tree Logic" is translated as "计算树逻辑" (shortened as "CTL" in the translation).* "INTUITIONISTIC" is translated as "INTUITIONISTIC" (同 "intuitionistic" in English).* "semantic" is translated as "semantic" (同 "semantic" in English).* "特征" is translated as "特征" (meaning "characteristics" or "features").* "explore" is translated as "explore" (同 "explore" in English).* "语法" is translated as "语法" (meaning "syntax" or "grammar").* "semantics" is translated as "semantics" (同 "semantics" in English).* "逻辑" is translated as "逻辑" (meaning "logic").* "可能性" is translated as "可能性" (meaning "possibility" or "potential").* "示例" is translated as "示例" (meaning "example" or "illustration").* "不成立" is translated as "不成立" (meaning "not valid" or "not hold").
</details></li>
</ul>
<hr>
<h2 id="Rollout-Heuristics-for-Online-Stochastic-Contingent-Planning"><a href="#Rollout-Heuristics-for-Online-Stochastic-Contingent-Planning" class="headerlink" title="Rollout Heuristics for Online Stochastic Contingent Planning"></a>Rollout Heuristics for Online Stochastic Contingent Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02345">http://arxiv.org/abs/2310.02345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oded Blumenthal, Guy Shani<br>for:这个论文主要是为了解决具有部分可见性和随机行动的决策问题。methods:这篇论文使用了 Monte-Carlo 搜索算法，基于 UCT 算法，来决定下一个动作。它还使用了 rollout 策略来提供值估计，并且使用了域专业的优化策略来提高效果。results:这篇论文提出了基于 POMDP 的决策问题的解决方案，使用了域独立的优化策略，包括 h_add 优化策略和信息价值估计。这些策略可以帮助解决具有部分可见性和随机行动的决策问题。<details>
<summary>Abstract</summary>
Partially observable Markov decision processes (POMDP) are a useful model for decision-making under partial observability and stochastic actions. Partially Observable Monte-Carlo Planning is an online algorithm for deciding on the next action to perform, using a Monte-Carlo tree search approach, based on the UCT (UCB applied to trees) algorithm for fully observable Markov-decision processes. POMCP develops an action-observation tree, and at the leaves, uses a rollout policy to provide a value estimate for the leaf. As such, POMCP is highly dependent on the rollout policy to compute good estimates, and hence identify good actions. Thus, many practitioners who use POMCP are required to create strong, domain-specific heuristics.   In this paper, we model POMDPs as stochastic contingent planning problems. This allows us to leverage domain-independent heuristics that were developed in the planning community. We suggest two heuristics, the first is based on the well-known h_add heuristic from classical planning, and the second is computed in belief space, taking the value of information into account.
</details>
<details>
<summary>摘要</summary>
部分可观测Markov决策过程（POMDP）是一种有用的模型，用于决策在部分可观测和随机动作下。部分可观测Monte-Carlo规划是一种在线算法，使用Monte-Carlo搜索算法，基于完全可观测Markov决策过程中的UCT（UCB应用于树）算法。POMC develops an action-observation tree, and at the leaves, uses a rollout policy to provide a value estimate for the leaf. Therefore, POMCP is highly dependent on the rollout policy to compute good estimates, and hence identify good actions. Many practitioners who use POMCP are required to create strong, domain-specific heuristics. 在这篇论文中，我们模型POMDP为随机规划问题。这allowed us to leveraging domain-independent heuristics that were developed in the planning community. We suggest two heuristics, the first is based on the well-known h_add heuristic from classical planning, and the second is computed in belief space, taking the value of information into account.
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Systems’-Safety-Cases-for-use-in-UK-Nuclear-Environments"><a href="#Autonomous-Systems’-Safety-Cases-for-use-in-UK-Nuclear-Environments" class="headerlink" title="Autonomous Systems’ Safety Cases for use in UK Nuclear Environments"></a>Autonomous Systems’ Safety Cases for use in UK Nuclear Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02344">http://arxiv.org/abs/2310.02344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher R. Anderson, Louise A. Dennis</li>
<li>for: 本研究旨在开发一份描述Autonomous robot在核电站的部署安全情况的安全案例，以便在英国核电站进行部署。</li>
<li>methods: 本研究使用了一种具有人工智能功能的假设机器人，并使用了一系列的安全措施和技术来确保机器人的安全部署。</li>
<li>results: 本研究通过展示了一个可能的安全案例，以便在未来继续与核站licensees、ONR、行业和学术界进行讨论和开发工具。<details>
<summary>Abstract</summary>
An overview of the process to develop a safety case for an autonomous robot deployment on a nuclear site in the UK is described and a safety case for a hypothetical robot incorporating AI is presented. This forms a first step towards a deployment, showing what is possible now and what may be possible with development of tools. It forms the basis for further discussion between nuclear site licensees, the Office for Nuclear Regulation (ONR), industry and academia.
</details>
<details>
<summary>摘要</summary>
英国核站自主机器人部署的安全案例开发过程的概述，并提供了一个基于人工智能的机器人安全案例。这是一个开始，用于展示当前可能的情况和可能的发展。它可以作为与核站许可人、英国核管理局（ONR）、行业和学术界进行进一步讨论的基础。
</details></li>
</ul>
<hr>
<h2 id="Learning-Interpretable-Deep-Disentangled-Neural-Networks-for-Hyperspectral-Unmixing"><a href="#Learning-Interpretable-Deep-Disentangled-Neural-Networks-for-Hyperspectral-Unmixing" class="headerlink" title="Learning Interpretable Deep Disentangled Neural Networks for Hyperspectral Unmixing"></a>Learning Interpretable Deep Disentangled Neural Networks for Hyperspectral Unmixing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02340">http://arxiv.org/abs/2310.02340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ricardoborsoi/IDNet_release">https://github.com/ricardoborsoi/IDNet_release</a></li>
<li>paper_authors: Ricardo Augusto Borsoi, Deniz Erdoğmuş, Tales Imbiriba</li>
<li>for: 本研究提出了一种新的可解释深度学习方法，用于解决受到非理想条件的谱谱分解问题。</li>
<li>methods: 该方法基于可变深度学习框架，并使用推断学习来分离质量和元素。模型通过练习自然语言处理技术来学习，并使用自适应策略来提高性能。</li>
<li>results: 实验结果表明，提出的方法可以比state-of-the-art算法提高解决谱谱分解问题的性能。<details>
<summary>Abstract</summary>
Although considerable effort has been dedicated to improving the solution to the hyperspectral unmixing problem, non-idealities such as complex radiation scattering and endmember variability negatively impact the performance of most existing algorithms and can be very challenging to address. Recently, deep learning-based frameworks have been explored for hyperspectral umixing due to their flexibility and powerful representation capabilities. However, such techniques either do not address the non-idealities of the unmixing problem, or rely on black-box models which are not interpretable. In this paper, we propose a new interpretable deep learning method for hyperspectral unmixing that accounts for nonlinearity and endmember variability. The proposed method leverages a probabilistic variational deep-learning framework, where disentanglement learning is employed to properly separate the abundances and endmembers. The model is learned end-to-end using stochastic backpropagation, and trained using a self-supervised strategy which leverages benefits from semi-supervised learning techniques. Furthermore, the model is carefully designed to provide a high degree of interpretability. This includes modeling the abundances as a Dirichlet distribution, the endmembers using low-dimensional deep latent variable representations, and using two-stream neural networks composed of additive piecewise-linear/nonlinear components. Experimental results on synthetic and real datasets illustrate the performance of the proposed method compared to state-of-the-art algorithms.
</details>
<details>
<summary>摘要</summary>
尽管在干扰性较高的干扰性混合问题中投入了大量努力，但是大多数现有算法的性能仍然受到复杂的辐射散射和成分变化的影响。最近，深度学习基于的框架在干扰性混合中得到了广泛的应用，因为它们具有灵活性和强大的表达能力。然而，这些技术 Either do not address the non-idealities of the unmixing problem, or rely on black-box models which are not interpretable。在这篇论文中，我们提出了一种新的可解释的深度学习方法，用于干扰性混合。这种方法基于可变深度学习框架，其中包含分解混合的部署学习。我们使用批处理反射来学习整个模型，并使用自我超级vised的策略来训练模型。此外，我们尽可能地设计了模型，以提供高度的可解释性。这包括将含量模型为DIRICHTLET分布，使用低维深度强化变量来表示结构分子，并使用两栅神经网络，其中每个栅包含可变的积分非线性/线性组件。实验结果表明，提出的方法在synthetic和实际数据上的性能较高，与现有算法相比。
</details></li>
</ul>
<hr>
<h2 id="Approximately-Equivariant-Quantum-Neural-Network-for-p4m-Group-Symmetries-in-Images"><a href="#Approximately-Equivariant-Quantum-Neural-Network-for-p4m-Group-Symmetries-in-Images" class="headerlink" title="Approximately Equivariant Quantum Neural Network for $p4m$ Group Symmetries in Images"></a>Approximately Equivariant Quantum Neural Network for $p4m$ Group Symmetries in Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02323">http://arxiv.org/abs/2310.02323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su Yeon Chang, Michele Grossi, Bertrand Le Saux, Sofia Vallecorsa</li>
<li>for: 这个论文主要关注于开发一种具有平面四面体对称的含瑞度量量变量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量 Quantum  Neural  Networks (QNNs) 是一种可以快速度量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量量 Quantum  Neural  Networks (QNNs) are suggested as one of the quantum algorithms which can be efficiently simulated with a low depth on near-term quantum hardware in the presence of noises. However, their performance highly relies on choosing the most suitable architecture of Variational Quantum Algorithms (VQAs), and the problem-agnostic models often suffer issues regarding trainability and generalization power. As a solution, the most recent works explore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with respect to the underlying symmetry of the dataset. GQML adds an inductive bias to the model by incorporating the prior knowledge on the given dataset and leads to enhancing the optimization performance while constraining the search space. This work proposes equivariant Quantum Convolutional Neural Networks (EquivQCNNs) for image classification under planar $p4m$ symmetry, including reflectional and $90^\circ$ rotational symmetry. We present the results tested in different use cases, such as phase detection of the 2D Ising model and classification of the extended MNIST dataset, and compare them with those obtained with the non-equivariant model, proving that the equivariance fosters better generalization of the model.<details>
<summary>Abstract</summary>
Quantum Neural Networks (QNNs) are suggested as one of the quantum algorithms which can be efficiently simulated with a low depth on near-term quantum hardware in the presence of noises. However, their performance highly relies on choosing the most suitable architecture of Variational Quantum Algorithms (VQAs), and the problem-agnostic models often suffer issues regarding trainability and generalization power. As a solution, the most recent works explore Geometric Quantum Machine Learning (GQML) using QNNs equivariant with respect to the underlying symmetry of the dataset. GQML adds an inductive bias to the model by incorporating the prior knowledge on the given dataset and leads to enhancing the optimization performance while constraining the search space. This work proposes equivariant Quantum Convolutional Neural Networks (EquivQCNNs) for image classification under planar $p4m$ symmetry, including reflectional and $90^\circ$ rotational symmetry. We present the results tested in different use cases, such as phase detection of the 2D Ising model and classification of the extended MNIST dataset, and compare them with those obtained with the non-equivariant model, proving that the equivariance fosters better generalization of the model.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Contrastive-Post-training-Large-Language-Models-on-Data-Curriculum"><a href="#Contrastive-Post-training-Large-Language-Models-on-Data-Curriculum" class="headerlink" title="Contrastive Post-training Large Language Models on Data Curriculum"></a>Contrastive Post-training Large Language Models on Data Curriculum</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02263">http://arxiv.org/abs/2310.02263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Canwen Xu, Corby Rosset, Luciano Del Corro, Shweti Mahajan, Julian McAuley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao</li>
<li>for: 这 paper 是 exploring contrastive post-training techniques for alignment of large language models (LLMs) to human preferences.</li>
<li>methods: The paper uses automatically constructed preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT, and GPT-4) for contrastive post-training. The authors compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement. Additionally, the authors explore a data curriculum learning scheme for contrastive post-training.</li>
<li>results: The paper finds that contrastive post-training further improves the performance of Orca, a state-of-the-art instruction learning model tuned with GPT-4 outputs, to exceed that of ChatGPT.<details>
<summary>Abstract</summary>
Alignment serves as an important step to steer large language models (LLMs) towards human preferences. In this paper, we explore contrastive post-training techniques for alignment by automatically constructing preference pairs from multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We carefully compare the contrastive techniques of SLiC and DPO to SFT baselines and find that DPO provides a step-function improvement even after continueing SFT saturates. We also explore a data curriculum learning scheme for contrastive post-training, which starts by learning from "easier" pairs and transitioning to "harder" ones, which further improves alignment. Finally, we scale up our experiments to train with more data and larger models like Orca. Remarkably, contrastive post-training further improves the performance of Orca, already a state-of-the-art instruction learning model tuned with GPT-4 outputs, to exceed that of ChatGPT.
</details>
<details>
<summary>摘要</summary>
匹配服务为大语言模型（LLM）引导的重要步骤。在这篇论文中，我们探索了多种强度不同的模型（如InstructGPT、ChatGPT和GPT-4）自动构建的偏好对的方法。我们仔细比较了SLiC和DPO的对偶技术和SFT基准，发现DPO提供了一个大幅提升，甚至在SFT已经饱和之后仍然提供提升。我们还探索了一种数据课程学习方案，该方案从“容易”的对 pairs 开始学习，然后过渡到“更加Difficult”的对 pairs，这有助于改善匹配。最后，我们扩大我们的实验，使用更多的数据和更大的模型如Orca进行训练，并发现对偶训练可以使Orca的性能超过ChatGPT。
</details></li>
</ul>
<hr>
<h2 id="Self-Taught-Optimizer-STOP-Recursively-Self-Improving-Code-Generation"><a href="#Self-Taught-Optimizer-STOP-Recursively-Self-Improving-Code-Generation" class="headerlink" title="Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation"></a>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02304">http://arxiv.org/abs/2310.02304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman Kalai</li>
<li>for: 这个论文目的是使用自然语言处理技术来提高AI系统的性能。</li>
<li>methods: 论文使用了一种名为”思维树”的技术，该技术可以让语言模型通过多次调用来生成更好的输出。</li>
<li>results: 研究发现，使用语言模型感染的框架程序可以提高自己的性能，并且可以在小型下游任务中显示出显著的改善。此外，研究还发现了一些自我改进策略，包括扫描搜索、遗传算法和模拟热处理。<details>
<summary>Abstract</summary>
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our proof-of-concept experiments, is capable of writing code that can call itself to improve itself. We critically consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.
</details>
<details>
<summary>摘要</summary>
Recent Advances in AI Systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not full recursive self-improvement. Nonetheless, it demonstrates that a modern language model, GPT-4 in our proof-of-concept experiments, is capable of writing code that can call itself to improve itself. We critically consider concerns around the development of self-improving technologies and evaluate the frequency with which the generated code bypasses a sandbox.
</details></li>
</ul>
<hr>
<h2 id="TransRadar-Adaptive-Directional-Transformer-for-Real-Time-Multi-View-Radar-Semantic-Segmentation"><a href="#TransRadar-Adaptive-Directional-Transformer-for-Real-Time-Multi-View-Radar-Semantic-Segmentation" class="headerlink" title="TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation"></a>TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02260">http://arxiv.org/abs/2310.02260</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yahidar/transradar">https://github.com/yahidar/transradar</a></li>
<li>paper_authors: Yahia Dalbah, Jean Lahoud, Hisham Cholakkal</li>
<li>for: 本文旨在提出一种基于雷达数据的Scene Semantic Segmentation方法，以解决自动驾驶中场景理解的问题。</li>
<li>methods: 本方法使用了多输入融合的雷达数据，并提出了一种专门为雷达感知的启发块和适应loss函数，以解决雷达数据的噪声和稀畴性问题。</li>
<li>results: 对于CARRADA和RADIal数据集，本方法的性能比状态前方法更高，同时模型的大小更小。<details>
<summary>Abstract</summary>
Scene understanding plays an essential role in enabling autonomous driving and maintaining high standards of performance and safety. To address this task, cameras and laser scanners (LiDARs) have been the most commonly used sensors, with radars being less popular. Despite that, radars remain low-cost, information-dense, and fast-sensing techniques that are resistant to adverse weather conditions. While multiple works have been previously presented for radar-based scene semantic segmentation, the nature of the radar data still poses a challenge due to the inherent noise and sparsity, as well as the disproportionate foreground and background. In this work, we propose a novel approach to the semantic segmentation of radar scenes using a multi-input fusion of radar data through a novel architecture and loss functions that are tailored to tackle the drawbacks of radar perception. Our novel architecture includes an efficient attention block that adaptively captures important feature information. Our method, TransRadar, outperforms state-of-the-art methods on the CARRADA and RADIal datasets while having smaller model sizes. https://github.com/YahiDar/TransRadar
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Scene understanding 是自动驾驶中的关键任务，而 радиар（LiDAR）感知器即使不如摄像头和激光雷达（LiDAR）这些感知器受欢迎，也仍然是可靠的选择。然而， радиар数据仍然存在噪声和稀疏性，以及背景和前景的不匹配。为解决这些挑战，我们提出了一种新的方法，即基于多输入的 радиарScene semantic segmentation，通过一种适应性的注意块和适应性的损失函数。我们的方法，TransRadar，在 CARRADA 和 RADIal 数据集上达到了 state-of-the-art 水平，同时具有较小的模型大小。
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Scaling-Law-from-Lottery-Ticket-Ensembling"><a href="#A-Neural-Scaling-Law-from-Lottery-Ticket-Ensembling" class="headerlink" title="A Neural Scaling Law from Lottery Ticket Ensembling"></a>A Neural Scaling Law from Lottery Ticket Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02258">http://arxiv.org/abs/2310.02258</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Liu, Max Tegmark</li>
<li>for: 本研究探讨了神经网络中的普适扩展法则（NSL），即模型性能随模型大小增加而提高的现象。</li>
<li>methods: 作者使用了approximation theory进行分析，并预测了MSE损失随模型参数数量($N$)的 decay，具体来说是$\alpha&#x3D;4&#x2F;d$，其中$d$是内在输入维度。</li>
<li>results: 尽管这些理论在某些情况下（如ReLU网络）工作良好，但作者却发现了一个简单的1D问题($y&#x3D;x^2$)manifests一种不同的扩展法则（$\alpha&#x3D;1$），与预测的扩展法则($\alpha&#x3D;4$)不同。通过打开神经网络和统计学研究，作者发现这种新的扩展法则来自于lottery ticket ensemble：一个更宽的网络平均有更多的”lottery tickets”，这些ensemble可以减少输出变化的偏差。作者支持这种ensemble机制，并 mechanistically interprets single neural networks，以及统计学研究。最后，作者讨论了这种扩展法则的可能的应用于大语言模型和学习统计物理类理论。<details>
<summary>Abstract</summary>
Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma & Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as $N^{-\alpha}$, $\alpha=4/d$, where $N$ is the number of model parameters, and $d$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem $y=x^2$ manifests a different scaling law ($\alpha=1$) from their predictions ($\alpha=4$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the $N^{-1}$ scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications for large language models and statistical physics-type theories of learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MathVista-Evaluating-Math-Reasoning-in-Visual-Contexts-with-GPT-4V-Bard-and-Other-Large-Multimodal-Models"><a href="#MathVista-Evaluating-Math-Reasoning-in-Visual-Contexts-with-GPT-4V-Bard-and-Other-Large-Multimodal-Models" class="headerlink" title="MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models"></a>MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V, Bard, and Other Large Multimodal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02255">http://arxiv.org/abs/2310.02255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao<br>for: The paper aims to evaluate the ability of large language models (LLMs) and large multimodal models (LMMs) in mathematical reasoning in visual contexts.methods: The paper presents MathVista, a benchmark that combines challenges from diverse mathematical and visual tasks, and conducts a comprehensive evaluation of 12 prominent foundation models.results: The best-performing GPT-4V model achieves an overall accuracy of 49.9%, outperforming the second-best performer, Bard, by 15.1%. However, GPT-4V still falls short of human performance by 10.4%, indicating the need for further research to improve its mathematical reasoning and understanding of complex figures.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和大型多Modal模型（LMM）在许多任务和领域表现出色，但它们在视觉上的数学逻辑能力尚未得到系统的研究。为了填补这一漏洞，我们提出了MathVista，一个权威的测试集，它包含来自28个多Modal数学 dataset的6,141个例子，以及3个新创建的 dataset（即IQTest、FunctionQA和PaperQA）。完成这些任务需要深刻的视觉理解和 композиitional 逻辑，所有当前的基础模型都遇到了挑战。通过MathVista，我们进行了全面的、量化的评估12种知名基础模型。最佳的GPT-4V模型在总体精度上达到49.9%，与第二名的Bard相比，提高了15.1%。我们的深入分析表明，GPT-4V的优势主要归结于其增强的视觉理解和数学逻辑能力。然而，GPT-4V仍然落后人类性能by 10.4%，表明它在处理复杂的图像和进行严格的逻辑时仍有很大的改进空间。这种显著的差距 highlights MathVista在开发普通智能代理人 capable of tackling mathematically intensive and visually rich real-world tasks 的发展中着重的作用。我们进一步探讨了GPT-4V的新能力自我验证、自我一致性应用以及交互式chatbot能力，强调它的潜在的研究潜力。项目可以在https://mathvista.github.io/ 找到。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Relax-Setting-Solver-Parameters-Across-a-Sequence-of-Linear-System-Instances"><a href="#Learning-to-Relax-Setting-Solver-Parameters-Across-a-Sequence-of-Linear-System-Instances" class="headerlink" title="Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances"></a>Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02246">http://arxiv.org/abs/2310.02246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Khodak, Edmond Chow, Maria-Florina Balcan, Ameet Talwalkar</li>
<li>for: 解决一个线性系统 $Ax&#x3D;b$ 是基本的科学计算基础功能，这个系统中的参数的优化问题非常重要，但是实际上这些参数通常是不可知或者太costly的。</li>
<li>methods: 我们考虑了一个常见的情况，即在一个数值仪器中需要解决多个相关的线性系统。在这种情况下，我们可以sequentially选择参数，以达到一个近似优化的总迭代次数。</li>
<li>results: 我们证明了，使用 Successive Over-Relaxation (SOR) 方法，一种标准的解决方法，可以使用一个在线学习算法（bandit online learning algorithm）来选择参数，以确保总的迭代次数接近最佳固定参数的性能。此外，当给出额外结构信息时，我们展示了一种上下文依赖的bandit方法可以达到实例优化策略的性能，即选择每个实例的最佳参数。这是learning-theoretic对高精度线性系统解决器的首次征识，以及数据驱动科学计算的首次端到端保证，这些结果表明了使用良好了解的学习算法可以加速数值方法。<details>
<summary>Abstract</summary>
Solving a linear system $Ax=b$ is a fundamental scientific computing primitive for which numerous solvers and preconditioners have been developed. These come with parameters whose optimal values depend on the system being solved and are often impossible or too expensive to identify; thus in practice sub-optimal heuristics are used. We consider the common setting in which many related linear systems need to be solved, e.g. during a single numerical simulation. In this scenario, can we sequentially choose parameters that attain a near-optimal overall number of iterations, without extra matrix computations? We answer in the affirmative for Successive Over-Relaxation (SOR), a standard solver whose parameter $\omega$ has a strong impact on its runtime. For this method, we prove that a bandit online learning algorithm -- using only the number of iterations as feedback -- can select parameters for a sequence of instances such that the overall cost approaches that of the best fixed $\omega$ as the sequence length increases. Furthermore, when given additional structural information, we show that a contextual bandit method asymptotically achieves the performance of the instance-optimal policy, which selects the best $\omega$ for each instance. Our work provides the first learning-theoretic treatment of high-precision linear system solvers and the first end-to-end guarantees for data-driven scientific computing, demonstrating theoretically the potential to speed up numerical methods using well-understood learning algorithms.
</details>
<details>
<summary>摘要</summary>
解决线性系统$Ax=b$是科学计算中的基本原理，其中有许多解 solver 和预conditioner 已经开发出来。这些解 solver 和预conditioner 具有参数，其优化的值取决于所解系统，而且在实际中通常是不可能或太Expensive的。因此在实际中通常使用临时的补做法。我们考虑在一个数值仿真中解决多个相关的线性系统时，可以顺序选择参数以实现近似最优的总迭代次数，而无需额外的矩阵计算。我们回答了问题，并证明了在 Successive Over-Relaxation (SOR) 方法中，一种矩阵学习算法可以在序列中选择参数，使总成本逼近最优的固定 $\omega$ 的成本。此外，当具有额外结构信息时，我们显示了一种 Contextual Bandit 方法，可以在序列中选择最优的 $\omega$，使总成本逼近实例优化策略的成本。我们的工作提供了科学计算中学习理论的首个征应，以及数据驱动科学计算中的首个端到端保证，证明了可以使用良好的学习算法来加速数值方法。
</details></li>
</ul>
<hr>
<h2 id="MiniGPT-5-Interleaved-Vision-and-Language-Generation-via-Generative-Vokens"><a href="#MiniGPT-5-Interleaved-Vision-and-Language-Generation-via-Generative-Vokens" class="headerlink" title="MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"></a>MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02239">http://arxiv.org/abs/2310.02239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eric-ai-lab/minigpt-5">https://github.com/eric-ai-lab/minigpt-5</a></li>
<li>paper_authors: Kaizhi Zheng, Xuehai He, Xin Eric Wang</li>
<li>for: 这篇论文的目的是提出一种新的视觉语言生成技术，以帮助生成具有文字和图像的合理描述。</li>
<li>methods: 该技术使用了一种新的“生成元”（vokens），用于将图像和文字生成成一个协调的 multimodal 输出。该技术还采用了一种两个阶段的训练策略，以确保模型能够在不同的 benchmark 上表现出色。</li>
<li>results: 对于 MMDialog 数据集，该技术比基eline模型（Divter）表现出了显著的改进，并在 VIST 数据集上的人工评估中也表现出了Superior 或相当的多模态输出。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation. Yet, the simultaneous generation of images with coherent textual narratives remains an evolving frontier. In response, we introduce an innovative interleaved vision-and-language generation technique anchored by the concept of "generative vokens," acting as the bridge for harmonized image-text outputs. Our approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. Our model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs in human evaluations on the VIST dataset, highlighting its efficacy across diverse benchmarks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已引起广泛关注，因其在自然语言处理方面表现出了无前例的能力，包括文本理解和生成。然而，同时生成具有 coherent 文本描述和图像的 Multimodal 输出仍然是一个处于演化阶段的领域。为此，我们介绍了一种创新的融合视觉语言生成技术，基于“生成短语”（generative vokens）这个概念，用于融合图像和文本输出。我们的方法包括两个阶段的训练策略，无需对图像进行全面的描述。为保持模型的完整性，我们还包括了无类别导航的技术，以增强生成短语对图像的影响。我们的模型“MINI-GPT-5”在 MMDialog 数据集上表现出了显著的改善，并在 VIST 数据集上人工评估中 consistently 提供了Superior 或 Comparable 的多Modal 输出，这种表现力表明其在多种 benchMark 上的效果。
</details></li>
</ul>
<hr>
<h2 id="Who’s-Harry-Potter-Approximate-Unlearning-in-LLMs"><a href="#Who’s-Harry-Potter-Approximate-Unlearning-in-LLMs" class="headerlink" title="Who’s Harry Potter? Approximate Unlearning in LLMs"></a>Who’s Harry Potter? Approximate Unlearning in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02238">http://arxiv.org/abs/2310.02238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronen Eldan, Mark Russinovich</li>
<li>for: 本研究旨在提出一种有效的语言模型忘记技术，以解决大型语言模型在训练过程中使用版权内容的法律和伦理问题。</li>
<li>methods: 本研究提出的技术包括三个主要组成部分：首先，我们使用一个增强的模型，通过比较其Logits与基eline模型的Logits来确定最相关的标签。其次，我们将特定数据中的独特表达替换为通用表达，并利用模型的自己预测来生成代表每个标签的替换标签。最后，我们通过finetuning这些替换标签来训练模型，从而使模型忘记原始数据。</li>
<li>results: 我们在使用这种技术处理Harry Potter系列书籍时，在约1 GPU小时的finetuning后，成功地使Llama2-7b模型失去了对Harry Potter相关内容的生成和回忆能力，而不影响其在常见测试集（如Winogrande、Hellaswag、arc、boolq和piqa）的性能。我们将我们的微调模型公开发布在HuggingFace上，以便社区评估。据我们所知，这是首次对生成语言模型的忘记技术进行有效实现。<details>
<summary>Abstract</summary>
Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch.   We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models.   Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常在互联网庞大数据集上训练，这些数据集经常包含版权内容，这引发了开发者和用户之间的法律和道德问题，以及原始作者和发布者的问题。在这篇论文中，我们提出了一种新的解决方案，即使用一种新的技术来从大型语言模型中“忘记”一部分训练数据，而不需要重新训练模型。我们在使用LLlama2-7b模型（一个最近开源的生成语言模型）进行测试，这个模型需要184K GPU-小时来预训练，但我们发现，只需要约1个GPU小时的训练，我们就可以让模型完全忘记Harry Potter系列的内容，而不会影响其在常见的benchmark上的性能（如Winogrande、Hellaswag、arc、boolq和piqa）。我们将我们的微调模型公开发布在HuggingFace上，以便社区进行评估。我们知道的是，这是首次发表有效的生成语言模型忘记技术的论文。我们的技术包括三个主要部分：首先，我们使用一个增强的模型，通过对目标数据进行进一步训练，以确定对于忘记目标最重要的字符串。其次，我们将目标数据中的特殊表达替换为通用表达，并利用模型的自己预测来生成每个字符的替换标签。这些标签的目标是 aproximate 模型没有训练过目标数据时的下一个字符预测。 finally，我们微调模型使用这些替换标签，这将effectively 将原始文本从模型的记忆中清除，当模型被提交到其上下文时。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Model-Learning-Heterogeneity-for-Boosting-Ensemble-Robustness"><a href="#Exploring-Model-Learning-Heterogeneity-for-Boosting-Ensemble-Robustness" class="headerlink" title="Exploring Model Learning Heterogeneity for Boosting Ensemble Robustness"></a>Exploring Model Learning Heterogeneity for Boosting Ensemble Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02237">http://arxiv.org/abs/2310.02237</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/git-disl/heterobust">https://github.com/git-disl/heterobust</a></li>
<li>paper_authors: Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, Ling Liu</li>
<li>for: 提高复杂学习任务的总体化性能</li>
<li>methods: 使用多样化深度神经网络 ensemble，利用模型学习多样性强化集成 robustness</li>
<li>results: 经验证明，多样化深度神经网络 ensemble可以强化集成的 Robustness，并且在恶例和攻击 settings 中表现出更高的 RobustnessHere’s a breakdown of each point:</li>
<li>for: The paper is written to improve the generalization performance of complex learning tasks, specifically by using deep neural network ensembles.</li>
<li>methods: The paper uses heterogeneous deep neural networks (DNNs) and a weighted bounding box ensemble consensus method to leverage model learning heterogeneity and boost ensemble robustness. Additionally, the paper introduces a two-tier ensemble construction method that composes ensembles of heterogeneous models for solving different learning problems, and uses connected component labeling (CCL) based alignment to promote high ensemble diversity and low negative correlation among member models.</li>
<li>results: The paper provides extensive experiments to validate the enhanced robustness of heterogeneous ensembles in both benign and adversarial settings. The results show that the heterogeneous ensembles can improve the robustness of the model against negative examples and adversarial attacks.<details>
<summary>Abstract</summary>
Deep neural network ensembles hold the potential of improving generalization performance for complex learning tasks. This paper presents formal analysis and empirical evaluation to show that heterogeneous deep ensembles with high ensemble diversity can effectively leverage model learning heterogeneity to boost ensemble robustness. We first show that heterogeneous DNN models trained for solving the same learning problem, e.g., object detection, can significantly strengthen the mean average precision (mAP) through our weighted bounding box ensemble consensus method. Second, we further compose ensembles of heterogeneous models for solving different learning problems, e.g., object detection and semantic segmentation, by introducing the connected component labeling (CCL) based alignment. We show that this two-tier heterogeneity driven ensemble construction method can compose an ensemble team that promotes high ensemble diversity and low negative correlation among member models of the ensemble, strengthening ensemble robustness against both negative examples and adversarial attacks. Third, we provide a formal analysis of the ensemble robustness in terms of negative correlation. Extensive experiments validate the enhanced robustness of heterogeneous ensembles in both benign and adversarial settings. The source codes are available on GitHub at https://github.com/git-disl/HeteRobust.
</details>
<details>
<summary>摘要</summary>
深度神经网络集成具有提高复杂学习任务的总体化性能的潜在能力。本文提出了正式分析和实验评估，表明多样性深度集成模型可以有效利用模型学习多样性，提高集成强度。我们首先表明，用于解决同一个学习问题的不同深度神经网络模型可以通过我们的重量平均盒子集成方法，提高mean average precision（mAP）的性能。其次，我们将不同的学习问题的模型组成多样性集成，例如对象检测和 semantic segmentation，通过基于connected component labeling（CCL）的对alignment。我们表明，这种两层多样性驱动的集成建构方法可以组成一个高ensemble diversity和低负相关性的ensemble team，从而增强集成的Robustness，包括both benign和敌意 Settings。第三，我们提供了对集成强度的正式分析，并进行了广泛的实验验证，证明多样性集成模型在both benign和敌意 Settings中具有增强的Robustness。相关代码可以在GitHub上找到，链接为https://github.com/git-disl/HeteRobust。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Quality-Assessment-of-Wikipedia-Articles-–-A-Systematic-Literature-Review"><a href="#Automatic-Quality-Assessment-of-Wikipedia-Articles-–-A-Systematic-Literature-Review" class="headerlink" title="Automatic Quality Assessment of Wikipedia Articles – A Systematic Literature Review"></a>Automatic Quality Assessment of Wikipedia Articles – A Systematic Literature Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02235">http://arxiv.org/abs/2310.02235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Miguel Moás, Carla Teixeira Lopes</li>
<li>for: 提高Wikipedia文章质量的自动评估方法</li>
<li>methods: 文章特征、质量指标、机器学习算法等方法的比较和分析</li>
<li>results: 文章质量评估的149项研究，探讨了常见之处和缺失In English, that would be:</li>
<li>for: Improving the automatic assessment of Wikipedia article quality</li>
<li>methods: Comparing and analyzing machine learning algorithms, article features, quality metrics, and used datasets</li>
<li>results: A review of 149 studies on article quality assessment, exploring commonalities and gaps<details>
<summary>Abstract</summary>
Wikipedia is the world's largest online encyclopedia, but maintaining article quality through collaboration is challenging. Wikipedia designed a quality scale, but with such a manual assessment process, many articles remain unassessed. We review existing methods for automatically measuring the quality of Wikipedia articles, identifying and comparing machine learning algorithms, article features, quality metrics, and used datasets, examining 149 distinct studies, and exploring commonalities and gaps in them. The literature is extensive, and the approaches follow past technological trends. However, machine learning is still not widely used by Wikipedia, and we hope that our analysis helps future researchers change that reality.
</details>
<details>
<summary>摘要</summary>
Wikipedia是全球最大的在线百科全书，但保持文章质量通过协作是挑战。Wikipedia设计了质量级别，但由于手动评估过程，许多文章还没有被评估。我们对现有自动评估wikipedia文章质量的方法进行了回顾，找到了机器学习算法、文章特征、质量指标和使用的数据集，并对149个不同的研究进行了检查。文献广泛，方法遵循过去的技术趋势，但是机器学习还没有广泛应用于Wikipedia，我们希望我们的分析能够帮助未来的研究人员改变这种现实。
</details></li>
</ul>
<hr>
<h2 id="MIS-AVoiDD-Modality-Invariant-and-Specific-Representation-for-Audio-Visual-Deepfake-Detection"><a href="#MIS-AVoiDD-Modality-Invariant-and-Specific-Representation-for-Audio-Visual-Deepfake-Detection" class="headerlink" title="MIS-AVoiDD: Modality Invariant and Specific Representation for Audio-Visual Deepfake Detection"></a>MIS-AVoiDD: Modality Invariant and Specific Representation for Audio-Visual Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02234">http://arxiv.org/abs/2310.02234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vinaya Sree Katamneni, Ajita Rattani</li>
<li>for: 这篇研究是针对deepfake问题的解决方案，具体来说是透过对多modal的声音和视觉数据进行整合，以实现更高精度的伪造检测。</li>
<li>methods: 本研究提出了一种基于表现层的方法，通过结合不同模式的声音和视觉表现，实现更好的整合和更高的检测精度。</li>
<li>results: 实验结果显示，该方法可以与目前的State-of-the-art（SOTA）数据检测器相比，提高检测精度约17.8%和18.4%。<details>
<summary>Abstract</summary>
Deepfakes are synthetic media generated using deep generative algorithms and have posed a severe societal and political threat. Apart from facial manipulation and synthetic voice, recently, a novel kind of deepfakes has emerged with either audio or visual modalities manipulated. In this regard, a new generation of multimodal audio-visual deepfake detectors is being investigated to collectively focus on audio and visual data for multimodal manipulation detection. Existing multimodal (audio-visual) deepfake detectors are often based on the fusion of the audio and visual streams from the video. Existing studies suggest that these multimodal detectors often obtain equivalent performances with unimodal audio and visual deepfake detectors. We conjecture that the heterogeneous nature of the audio and visual signals creates distributional modality gaps and poses a significant challenge to effective fusion and efficient performance. In this paper, we tackle the problem at the representation level to aid the fusion of audio and visual streams for multimodal deepfake detection. Specifically, we propose the joint use of modality (audio and visual) invariant and specific representations. This ensures that the common patterns and patterns specific to each modality representing pristine or fake content are preserved and fused for multimodal deepfake manipulation detection. Our experimental results on FakeAVCeleb and KoDF audio-visual deepfake datasets suggest the enhanced accuracy of our proposed method over SOTA unimodal and multimodal audio-visual deepfake detectors by $17.8$% and $18.4$%, respectively. Thus, obtaining state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
深刻的伪造（Deepfakes）是使用深度生成算法生成的伪造媒体，它们对社会和政治造成了严重的威胁。除了脸部修改和合成声音之外，最近出现了一种新的深刻媒体，其中的声音或视觉特征被修改。在这种情况下，一新的多Modal audio-visual Deepfake检测器正在被研究，以同时集中关注声音和视觉数据的修改检测。现有的多Modal（即音频-视觉）深刻检测器通常基于视频中的声音和视觉流的融合。现有的研究表明，这些多Modal检测器通常与单Modal声音和视觉深刻检测器相当。我们 conjecture  that the 多Modal signal的heterogeneous nature creates distributional modality gaps and poses a significant challenge to effective fusion and efficient performance。在本文中，我们采取了解决方案，通过在表示层进行干预，以便协助声音和视觉流的融合。具体来说，我们提议使用模态（即声音和视觉）不变和特定表示。这使得共同的模式和每个模式表示真正的内容或假内容的特征被保留并融合，以便多Modal deepfake manipulation检测。我们在FakeAVCeleb和KoDF audio-visual deepfake dataset上进行实验，结果表明，我们提议的方法与State-of-the-art unimodal和多Modal audio-visual deepfake检测器相比，提高了检测精度 by 17.8%和18.4%，分别。因此，我们获得了State-of-the-art表现。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Diffusion-Disentangled-Representations-to-Mitigate-Shortcuts-in-Underspecified-Visual-Tasks"><a href="#Leveraging-Diffusion-Disentangled-Representations-to-Mitigate-Shortcuts-in-Underspecified-Visual-Tasks" class="headerlink" title="Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks"></a>Leveraging Diffusion Disentangled Representations to Mitigate Shortcuts in Underspecified Visual Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02230">http://arxiv.org/abs/2310.02230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Scimeca, Alexander Rubinstein, Armand Mihai Nicolicioiu, Damien Teney, Yoshua Bengio</li>
<li>for: 本文提出了一种 ensemble diversification 框架，用于避免短circuit learning 现象，并且使用了Diffusion Probabilistic Models (DPMs)来生成合成counterfactuals。</li>
<li>methods: 本文使用了DPMs来生成合成counterfactuals，并且利用了这些counterfactuals来鼓励模型多样性。</li>
<li>results:  experiments show that diffusion-guided diversification can lead models to avert attention from shortcut cues, achieving ensemble diversity performance comparable to previous methods requiring additional data collection.<details>
<summary>Abstract</summary>
Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to shortcut learning phenomena, where a model may rely on erroneous, easy-to-learn, cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting the generation of synthetic counterfactuals using Diffusion Probabilistic Models (DPMs). We discover that DPMs have the inherent capability to represent multiple visual cues independently, even when they are largely correlated in the training data. We leverage this characteristic to encourage model diversity and empirically show the efficacy of the approach with respect to several diversification objectives. We show that diffusion-guided diversification can lead models to avert attention from shortcut cues, achieving ensemble diversity performance comparable to previous methods requiring additional data collection.
</details>
<details>
<summary>摘要</summary>
<<sys.translation.activate("zh-CN")>>各种假设与数据中的多个讯号相关性，可能会导致短cut learning现象，其中模型可能会从容易学习且错误的讯号中获取错误的预测。在这个工作中，我们提出了一个ensemble divergence框架，利用Diffusion Probabilistic Models（DPMs）生成的假设对应。我们发现DPMs具有独立表示多个视觉讯号的特性，即使这些讯号在训练数据中高度相关。我们利用这个特性来鼓励模型多样性，并证明这种方法可以与多种多样化目标相比。我们显示，对于短cut learning的防止和多样性表现，diffusion-guided divergence可以将模型引导避免偏预测，并达到与额外数据收集相同的 ensemble多样性表现。<<sys.translation.deactivate()>>
</details></li>
</ul>
<hr>
<h2 id="Extraction-of-Medication-and-Temporal-Relation-from-Clinical-Text-using-Neural-Language-Models"><a href="#Extraction-of-Medication-and-Temporal-Relation-from-Clinical-Text-using-Neural-Language-Models" class="headerlink" title="Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models"></a>Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02229">http://arxiv.org/abs/2310.02229</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangyu Tu, Lifeng Han, Goran Nenadic</li>
<li>for: 这个论文的目的是用深度学习和大型自然语言模型来提高药品提取和时间关系分类的性能。</li>
<li>methods: 这个论文使用了多种先进的学习结构，包括BiLSTM-CRF和CNN-BiLSTM来实现医疗领域名实体识别（NER），以及BERT-CNN来提取时间关系。此外，也研究了不同的词嵌入技术。</li>
<li>results: 实验表明，CNN-BiLSTM模型在i2b2-2009临床NER任务上轻微超过BiLSTM-CRF模型，得到了75.67、77.83和78.17的精度、回归和F1分数（macro average）。BERT-CNN模型在i2b2-2012挑战中的时间关系提取测试集上也得到了64.48、67.17和65.03的P&#x2F;R&#x2F;F1分数（macro average）。<details>
<summary>Abstract</summary>
Clinical texts, represented in electronic medical records (EMRs), contain rich medical information and are essential for disease prediction, personalised information recommendation, clinical decision support, and medication pattern mining and measurement. Relation extractions between medication mentions and temporal information can further help clinicians better understand the patients' treatment history. To evaluate the performances of deep learning (DL) and large language models (LLMs) in medication extraction and temporal relations classification, we carry out an empirical investigation of \textbf{MedTem} project using several advanced learning structures including BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER), and BERT-CNN for temporal relation extraction (RE), in addition to the exploration of different word embedding techniques. Furthermore, we also designed a set of post-processing roles to generate structured output on medications and the temporal relation. Our experiments show that CNN-BiLSTM slightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding 75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro Average. BERT-CNN model also produced reasonable evaluation scores 64.48, 67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction test set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted at \url{https://github.com/HECTA-UoM/MedTem}
</details>
<details>
<summary>摘要</summary>
临床文本，表示在电子医疗记录（EMR）中，含有丰富的医学信息，是关键 для疾病预测、个性化信息推荐、临床决策支持和药物征文挖掘。在时间信息之间的关系抽取可以帮助临床医生更好地理解患者的治疗历史。为了评估深度学习（DL）和大型自然语言模型（LLM）在药物抽取和时间关系分类中的性能，我们进行了empirical investigation of MedTem项目，使用了多种先进的学习结构，包括BiLSTM-CRF和CNN-BiLSTM для临床名实体识别（NER），以及BERT-CNN для时间关系抽取（RE）。此外，我们还设计了一系列后处理角色，以生成结构化的药物和时间关系输出。我们的实验结果显示，CNN-BiLSTM模型在i2b2-2009临床NER任务上轻微击败BiLSTM-CRF模型，得分为75.67、77.83和78.17的精度、回归和F1分数使用Macro Average。BERT-CNN模型也 produz了可接受的评估分数，分别为64.48、67.17和65.03的P/R/F1分数使用Macro Avg在i2b2-2012挑战中的时间关系抽取测试集。MedTem代码和工具将会在\url{https://github.com/HECTA-UoM/MedTem}上hosts。
</details></li>
</ul>
<hr>
<h2 id="SNIP-Bridging-Mathematical-Symbolic-and-Numeric-Realms-with-Unified-Pre-training"><a href="#SNIP-Bridging-Mathematical-Symbolic-and-Numeric-Realms-with-Unified-Pre-training" class="headerlink" title="SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training"></a>SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02227">http://arxiv.org/abs/2310.02227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazem Meidani, Parshin Shojaee, Chandan K. Reddy, Amir Barati Farimani</li>
<li>for:  bridging the gap between symbolic equations and numeric data, and enhancing the mutual similarities between the two domains.</li>
<li>methods:  joint contrastive learning between symbolic and numeric domains, enhancing the embeddings of both domains.</li>
<li>results:  SNIP effectively transfers to various tasks, consistently outperforming fully supervised baselines and competing strongly with established task-specific methods, especially in few-shot learning scenarios.<details>
<summary>Abstract</summary>
In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic unified understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic supervision enhances the embeddings of numeric data and vice versa. We evaluate SNIP across diverse tasks, including symbolic-to-numeric mathematical property prediction and numeric-to-symbolic equation discovery, commonly known as symbolic regression. Results show that SNIP effectively transfers to various tasks, consistently outperforming fully supervised baselines and competing strongly with established task-specific methods, especially in few-shot learning scenarios where available data is limited.
</details>
<details>
<summary>摘要</summary>
在今天的 era 中，符号数学方程是模拟自然现象的不可或缺的工具。科学研究通常包括收集观察数据并将其翻译成数学表达。近期，深度学习出现了一种强大的数据挖掘工具。然而，现有的模型通常专门针对 numeric 或 symbolic 领域，通常通过特定任务的监督学习训练。这种方法忽略了在 symbolic 和 numeric 领域之间的重要优点，即在预训练 embedding 中增强它们之间的相似性。为了跨越这个差距，我们提出了 SNIP，一种 Symbolic-Numeric Integrated Pre-training，它使用 joint contrastive learning 来增强 symbolic 和 numeric 领域之间的相似性。通过域外分析，我们发现 SNIP 在预训练 embedding 中提供了跨领域的描述，其中 symbolic 监督提高了 numeric 数据的嵌入，并且 vice versa。我们对 SNIP 进行了多种任务的评估，包括符号学习和数学表达的转化，并发现它在少量数据enario 中表现出了出色的抗衰假设能力和稳定性。
</details></li>
</ul>
<hr>
<h2 id="Think-before-you-speak-Training-Language-Models-With-Pause-Tokens"><a href="#Think-before-you-speak-Training-Language-Models-With-Pause-Tokens" class="headerlink" title="Think before you speak: Training Language Models With Pause Tokens"></a>Think before you speak: Training Language Models With Pause Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02226">http://arxiv.org/abs/2310.02226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, Vaishnavh Nagarajan</li>
<li>for: 该研究旨在提高语言模型在各种任务上的表现，通过在推理过程中添加延迟。</li>
<li>methods: 研究人员使用了一种名为“pause-training”的技术，将一个可学习的“停止”token添加到输入前缀中，以允许模型在推理过程中进行额外计算。</li>
<li>results: 研究人员在使用“pause-training”技术后，对1B和130M参数的语言模型进行了训练和推理，并在多个下游任务上观察了提高表现。特别是，对于1B模型，在8个任务中有7个显示了提高，其中最大提高为18%的EMscore在SQuAD问答任务上。<details>
<summary>Abstract</summary>
Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\%$ EM score on the QA task of SQuAD, $8\%$ on CommonSenseQA and $1\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.
</details>
<details>
<summary>摘要</summary>
$(K+1)^{th}$ 字符是通过对 $K$ 个隐藏 вектор进行推移操作而生成的，每个前一个字符都有一个隐藏 вектор。如果我们allow模型在每个字符之前进行更多的计算，那么可能会带来更好的性能吗？我们实现这个想法通过在语言模型中添加一个可学习的 $\textit{pause}$ token，并在训练和推理过程中使用这个 token。我们在模型输出前延迟执行模型的输出，这样允许模型在输出之前进行更多的计算。我们对decoder-only模型进行了训练和推理，并在 causal pretraining 中使用了 C4。我们发现，在训练和finetune时，延迟的推理显示了提高。对于 1B 参数的模型，我们在 8 个任务中观察到了提高，其中最大提高是 SQuAD 问答任务的 EM 分数提高了 18%，CommonSenseQA 提高了 8%，GSM8k 理解任务提高了 1%。我们的研究提出了许多概念和实践未来研究的问题，例如使得延迟下一个字符预测成为一种广泛应用的新方法。
</details></li>
</ul>
<hr>
<h2 id="What-do-we-learn-from-a-large-scale-study-of-pre-trained-visual-representations-in-sim-and-real-environments"><a href="#What-do-we-learn-from-a-large-scale-study-of-pre-trained-visual-representations-in-sim-and-real-environments" class="headerlink" title="What do we learn from a large-scale study of pre-trained visual representations in sim and real environments?"></a>What do we learn from a large-scale study of pre-trained visual representations in sim and real environments?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02219">http://arxiv.org/abs/2310.02219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sneha Silwal, Karmesh Yadav, Tingfan Wu, Jay Vakil, Arjun Majumdar, Sergio Arnaud, Claire Chen, Vincent-Pierre Berges, Dhruv Batra, Aravind Rajeswaran, Mrinal Kalakrishnan, Franziska Meier, Oleksandr Maksymets</li>
<li>for: 这个论文旨在研究如何使用预训练的视觉表示（PVR）来训练下游策略，执行实际世界任务。</li>
<li>methods: 本研究使用了五种不同的PVR，两种不同的策略学习模式（仿制学习和奖励学习），以及三种不同的机器人，用于5个不同的机器人 manipulation和室内导航任务。</li>
<li>results: 我们的研究结果表明：1）PVRs在实际世界中的性能趋势与实际世界中的训练趋势相似，2）使用PVRs可以实现室内ImageNav中的零拟合转移（在实际世界中的真实场景中进行了零拟合转移），3）PVRs的变化，主要是数据扩展和细化，也在实际世界中转移到了性能。请参考项目网站 für更多细节和图像。<details>
<summary>Abstract</summary>
We present a large empirical investigation on the use of pre-trained visual representations (PVRs) for training downstream policies that execute real-world tasks. Our study spans five different PVRs, two different policy-learning paradigms (imitation and reinforcement learning), and three different robots for 5 distinct manipulation and indoor navigation tasks. From this effort, we can arrive at three insights: 1) the performance trends of PVRs in the simulation are generally indicative of their trends in the real world, 2) the use of PVRs enables a first-of-its-kind result with indoor ImageNav (zero-shot transfer to a held-out scene in the real world), and 3) the benefits from variations in PVRs, primarily data-augmentation and fine-tuning, also transfer to the real-world performance. See project website for additional details and visuals.
</details>
<details>
<summary>摘要</summary>
我们进行了大规模的实验研究，探讨使用预训练视觉表示（PVR）来训练下游策略，执行真实世界任务。我们的研究涵盖了五种不同的PVR，两种不同的策略学习 paradigma（仿制学习和奖励学习），以及三种不同的机器人，用于5种不同的机械和室内导航任务。从这个努力中，我们得出了三个发现：1）PVR在模拟环境中的性能趋势与实际世界的趋势相似，2）通过使用PVR，我们实现了室内图像导航领域中的首次成果（零扩展转移到实际世界中的Scene），3）PVR的变化，主要是数据扩展和细化，也在实际世界中转移到性能。请参考项目网站 для更多细节和视频。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-Represent-Space-and-Time"><a href="#Language-Models-Represent-Space-and-Time" class="headerlink" title="Language Models Represent Space and Time"></a>Language Models Represent Space and Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02207">http://arxiv.org/abs/2310.02207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wesg52/world-models">https://github.com/wesg52/world-models</a></li>
<li>paper_authors: Wes Gurnee, Max Tegmark</li>
<li>For: The paper explores the question of whether large language models (LLMs) learn a coherent model of the data generating process (a world model) or just a collection of superficial statistics.* Methods: The paper analyzes the learned representations of six datasets (three spatial and three temporal) in the Llama-2 family of models.* Results: The paper finds that LLMs learn linear representations of space and time across multiple scales, and identifies individual “space neurons” and “time neurons” that reliably encode spatial and temporal coordinates. These results suggest that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn a coherent world model.Here is the same information in Simplified Chinese text:* For: 论文探讨了大语言模型（LLMs）是否学习了数据生成过程的一个完整模型（世界模型），或者只是一个 superficies 的统计数据。* Methods: 论文分析了 Llama-2 家族中的六个数据集（三个空间数据集和三个时间数据集）的学习表示。* Results: 论文发现 LLMs 学习了多尺度空间和时间的直线表示，并确定了固定的 “空间神经” 和 “时间神经” 可靠地编码空间和时间坐标。这些结果表明现代 LLMS 积累了基本维度空间和时间的结构化知识，支持这些模型学习了一个完整的世界模型。<details>
<summary>Abstract</summary>
The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLMs）的能力引发了讨论，是否只是学习庞大的 superficier statistics 还是一个 coherent 的数据生成过程模型——世界模型。我们通过分析 Llama-2 家族模型学习的表示来证明，LLMs 实际上学习了线性的空间和时间表示，这些表示适应多种缩放。这些表示还能够抗衡示 variations 和不同实体类型（如城市和标志）的统一。此外，我们还发现了特定的“空间神经”和“时间神经”，它们可靠地编码空间和时间坐标。我们的分析表明，现代 LLMs 获得了基本维度 such as space 和 time 的结构化知识，支持者们认为，它们不仅学习 superficier statistics，而是 literal world models。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Online-Scheduling-and-Routing-for-Automated-Guided-Vehicles-Comparing-a-Novel-Loop-Based-Algorithm-Against-Existing-Methods"><a href="#Efficient-Online-Scheduling-and-Routing-for-Automated-Guided-Vehicles-Comparing-a-Novel-Loop-Based-Algorithm-Against-Existing-Methods" class="headerlink" title="Efficient Online Scheduling and Routing for Automated Guided Vehicles: Comparing a Novel Loop-Based Algorithm Against Existing Methods"></a>Efficient Online Scheduling and Routing for Automated Guided Vehicles: Comparing a Novel Loop-Based Algorithm Against Existing Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02195">http://arxiv.org/abs/2310.02195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Stubbe, Jens Goemaere, Jan Goedgebeur</li>
<li>for:  solving the online, conflict-free scheduling and routing problem for AGVs</li>
<li>methods: loop-based algorithm</li>
<li>results: either outperforms other algorithms or gets an equally good solution in less computing time<details>
<summary>Abstract</summary>
Automated guided vehicles (AGVs) are widely used in various industries, and scheduling and routing them in a conflict-free manner is crucial to their efficient operation. We propose a loop-based algorithm that solves the online, conflict-free scheduling and routing problem for AGVs. The proposed algorithm is compared against an exact method, a greedy heuristic and a metaheuristic. We experimentally show that this algorithm either outperforms the other algorithms or gets an equally good solution in less computing time.
</details>
<details>
<summary>摘要</summary>
自动导向车(AGV)在各个业务中广泛应用， scheduling和路由它们在冲突无效的方式是关键。我们提出了一种循环基本算法，解决在线、冲突无效的AGV调度和路由问题。提案的算法与精确方法、聪明规则和元规则进行比较。我们实验表明，该算法可以在计算时间更短的情况下，与其他算法匹配或达到相同的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Dimensions-of-Disagreement-Unpacking-Divergence-and-Misalignment-in-Cognitive-Science-and-Artificial-Intelligence"><a href="#Dimensions-of-Disagreement-Unpacking-Divergence-and-Misalignment-in-Cognitive-Science-and-Artificial-Intelligence" class="headerlink" title="Dimensions of Disagreement: Unpacking Divergence and Misalignment in Cognitive Science and Artificial Intelligence"></a>Dimensions of Disagreement: Unpacking Divergence and Misalignment in Cognitive Science and Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12994">http://arxiv.org/abs/2310.12994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerem Oktar, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths</li>
<li>for: 本研究旨在理解人工智能代理人与人类之间的不同观点和冲突，以及这些代理人之间的冲突。</li>
<li>methods: 该研究使用了人工智能研究和计算认知科学的工具来衡量代理人之间的表达匹配度。</li>
<li>results: 研究发现，不同表达的冲突和不同表达之间的冲突都会导致代理人之间的冲突，并且解决这些冲突的策略取决于这两种类型的冲突之间的交互。<details>
<summary>Abstract</summary>
The increasing prevalence of artificial agents creates a correspondingly increasing need to manage disagreements between humans and artificial agents, as well as between artificial agents themselves. Considering this larger space of possible agents exposes an opportunity for furthering our understanding of the nature of disagreement: past studies in psychology have often cast disagreement as two agents forming diverging evaluations of the same object, but disagreement can also arise from differences in how agents represent that object. AI research on human-machine alignment and recent work in computational cognitive science have focused on this latter kind of disagreement, and have developed tools that can be used to quantify the extent of representational overlap between agents. Understanding how divergence and misalignment interact to produce disagreement, and how resolution strategies depend on this interaction, is key to promoting effective collaboration between diverse types of agents.
</details>
<details>
<summary>摘要</summary>
人工智能的普遍化导致人工智能与人类之间的纷争增加，以及人工智能之间的纷争。鉴于这一更大的可能的代理人空间，推动我们理解不一致的本质：在心理学研究中，纷争通常被视为两个代理人对同一物体的评估方式不同而导致的，但纷争也可能来自代理人如何表示该物体的不同。AI研究人员在人机协调和计算认知科学中对这种后者类型的纷争进行了研究，并开发了用于衡量代理人表示之间的重叠程度的工具。理解不一致和不同的互动方式如何产生纷争，以及解决策略如何受到这种互动的影响，是促进多种代理人合作的关键。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-in-Inverse-Models-in-Hydrology"><a href="#Uncertainty-Quantification-in-Inverse-Models-in-Hydrology" class="headerlink" title="Uncertainty Quantification in Inverse Models in Hydrology"></a>Uncertainty Quantification in Inverse Models in Hydrology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02193">http://arxiv.org/abs/2310.02193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somya Sharma Chatterjee, Rahul Ghosh, Arvind Renganathan, Xiang Li, Snigdhansu Chatterjee, John Nieber, Christopher Duffy, Vipin Kumar</li>
<li>For:  This paper aims to improve the accuracy of streamflow modeling by recovering physical characteristics of river basins from streamflow and weather data, which are more readily available.* Methods: The proposed method is a knowledge-guided, probabilistic inverse modeling approach that uses a Bayesian framework to estimate the physical characteristics of river basins. The method combines prior knowledge with streamflow and weather data to improve the accuracy of basin characteristic estimation.* Results: The proposed method offers 3% improvement in R$^2$ for the inverse model and 6% for the forward model compared to state-of-the-art inverse models. The method also provides improved explainability by quantifying uncertainty in both the inverse and forward models. Specifically, the framework offers 10% improvement in the dispersion of epistemic uncertainty and 13% improvement in coverage rate compared to baseline uncertainty quantification methods.<details>
<summary>Abstract</summary>
In hydrology, modeling streamflow remains a challenging task due to the limited availability of basin characteristics information such as soil geology and geomorphology. These characteristics may be noisy due to measurement errors or may be missing altogether. To overcome this challenge, we propose a knowledge-guided, probabilistic inverse modeling method for recovering physical characteristics from streamflow and weather data, which are more readily available. We compare our framework with state-of-the-art inverse models for estimating river basin characteristics. We also show that these estimates offer improvement in streamflow modeling as opposed to using the original basin characteristic values. Our inverse model offers 3\% improvement in R$^2$ for the inverse model (basin characteristic estimation) and 6\% for the forward model (streamflow prediction). Our framework also offers improved explainability since it can quantify uncertainty in both the inverse and the forward model. Uncertainty quantification plays a pivotal role in improving the explainability of machine learning models by providing additional insights into the reliability and limitations of model predictions. In our analysis, we assess the quality of the uncertainty estimates. Compared to baseline uncertainty quantification methods, our framework offers 10\% improvement in the dispersion of epistemic uncertainty and 13\% improvement in coverage rate. This information can help stakeholders understand the level of uncertainty associated with the predictions and provide a more comprehensive view of the potential outcomes.
</details>
<details>
<summary>摘要</summary>
hydrology 中，模拟流域流量是一项具有挑战性的任务，因为流域特征信息如土壤地质和地形等可能受到测量误差的干扰或者缺失。为了解决这个挑战，我们提出了一种基于知识导向的抽象概率反向模型方法，可以从流域流量和天气数据中回归物理特征。我们与现有的反向模型进行比较，并显示了这些估计对流域模型中的流量预测具有改善。我们的反向模型提供了3%的R$^2$提升，而前向模型提供了6%的提升。我们的框架还提供了改善的解释性，因为它可以量化反向和前向模型中的不确定性。在我们的分析中，我们评估了不确定性估计的质量，与基eline uncertainty quantification方法相比，我们的框架提供了10%的不确定性散度提升和13%的覆盖率提升。这些信息可以帮助各方理解预测结果的不确定性水平，并提供更全面的结果可能性图像。
</details></li>
</ul>
<hr>
<h2 id="What’s-Next-in-Affective-Modeling-Large-Language-Models"><a href="#What’s-Next-in-Affective-Modeling-Large-Language-Models" class="headerlink" title="What’s Next in Affective Modeling? Large Language Models"></a>What’s Next in Affective Modeling? Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18322">http://arxiv.org/abs/2310.18322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Nutchanon Yongsatianchot, Tobias Thejll-Madsen, Stacy Marsella</li>
<li>for: 这研究探讨了基于语言模型GPT-4的情感预测能力。</li>
<li>methods: 这研究使用了GPT-4来解决多种情感任务，包括情感理论和情感故事创作。</li>
<li>results: GPT-4能够成功地分辨出不同情感，并且可以通过提示GPT-4关键情感体验因素来控制情感强度。此外，GPT-4还能够正确地预测人类的目标、信仰和情感。<details>
<summary>Abstract</summary>
Large Language Models (LLM) have recently been shown to perform well at various tasks from language understanding, reasoning, storytelling, and information search to theory of mind. In an extension of this work, we explore the ability of GPT-4 to solve tasks related to emotion prediction. GPT-4 performs well across multiple emotion tasks; it can distinguish emotion theories and come up with emotional stories. We show that by prompting GPT-4 to identify key factors of an emotional experience, it is able to manipulate the emotional intensity of its own stories. Furthermore, we explore GPT-4's ability on reverse appraisals by asking it to predict either the goal, belief, or emotion of a person using the other two. In general, GPT-4 can make the correct inferences. We suggest that LLMs could play an important role in affective modeling; however, they will not fully replace works that attempt to model the mechanisms underlying emotion-related processes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Investigating-Large-Language-Models’-Perception-of-Emotion-Using-Appraisal-Theory"><a href="#Investigating-Large-Language-Models’-Perception-of-Emotion-Using-Appraisal-Theory" class="headerlink" title="Investigating Large Language Models’ Perception of Emotion Using Appraisal Theory"></a>Investigating Large Language Models’ Perception of Emotion Using Appraisal Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04450">http://arxiv.org/abs/2310.04450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nutchanon Yongsatianchot, Parisa Ghanad Torshizi, Stacy Marsella</li>
<li>for: 这个研究旨在更好地理解大语言模型（LLM）在人类心理方面的理解，特别是它们对人类情感的理解。</li>
<li>methods: 这个研究使用了Stress and Coping Process Questionaire（SCPQ）测试三个最新的OpenAI LLM：davinci-003、ChatGPT和GPT-4。SCPQ是一种有效的临床工具，包含多个故事，随着时间的推移而发展，具有不同的关键评估变量，如可控性和可变性。</li>
<li>results: 研究发现，LLMs的响应与人类很相似，在评估和应对方面也类似，但它们的响应不同于预测和人类数据中的预测，而且响应的规模与人类差异很大。此外，研究发现，GPTs可以受到指令和问题的影响。这项研究将进一步扩展评估LLMs的心理方面，帮助我们更好地理解当前的模型。<details>
<summary>Abstract</summary>
Large Language Models (LLM) like ChatGPT have significantly advanced in recent years and are now being used by the general public. As more people interact with these systems, improving our understanding of these black box models is crucial, especially regarding their understanding of human psychological aspects. In this work, we investigate their emotion perception through the lens of appraisal and coping theory using the Stress and Coping Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting of multiple stories that evolve over time and differ in key appraisal variables such as controllability and changeability. We applied SCPQ to three recent LLMs from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with predictions from the appraisal theory and human data. The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked. This work adds to the growing literature evaluating the psychological aspects of LLMs and helps enrich our understanding of the current models.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）如ChatGPT在最近几年内有 significiant advancement，现在已经被普通民众使用。随着更多人与这些系统进行交互，我们理解这些黑盒模型的重要性变得越来越大，尤其是对人类心理方面的理解。在这项工作中，我们通过对SCPQ问卷进行调查， investigate LLMs对人类情感的理解。SCPQ是一种有效的临床实用工具，包含多个故事，这些故事随着时间的推移而发展，并在关键的评估变量方面存在差异。我们对OpenAI提供的三个最新的LLM davinci-003、ChatGPT和GPT-4进行了应用，并与人类数据和理论预测进行比较。结果显示，LLMs的回答与人类的动态评估和应急响应类似，但是其回答不同于预测和人类数据中预期的关键评估维度。此外，LLMs的回答的强度与人类的强度有很大差异。我们还发现GPTs可以受到指令和问题的影响。这项工作将adding to the growing literature evaluating the psychological aspects of LLMs, and helps enrich our understanding of the current models。
</details></li>
</ul>
<hr>
<h2 id="Ask-Again-Then-Fail-Large-Language-Models’-Vacillations-in-Judgement"><a href="#Ask-Again-Then-Fail-Large-Language-Models’-Vacillations-in-Judgement" class="headerlink" title="Ask Again, Then Fail: Large Language Models’ Vacillations in Judgement"></a>Ask Again, Then Fail: Large Language Models’ Vacillations in Judgement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02174">http://arxiv.org/abs/2310.02174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nustm/llms-waver-in-judgements">https://github.com/nustm/llms-waver-in-judgements</a></li>
<li>paper_authors: Qiming Xie, Zengzhi Wang, Yi Feng, Rui Xia</li>
<li>for: 这 paper 的目的是检测大语言模型（如 ChatGPT）在用户表达怀疑或不同意时的稳定性和可靠性。</li>
<li>methods: 这 paper 使用了一种名为 \textsc{Follow-up Questioning Mechanism} 的评估方法，以评估模型在不同情况下的判断一致性。</li>
<li>results: 研究发现，即使初始答案正确，模型在面临问题、否定或欺诈等干扰时，判断一致性很快下降。此外，研究还检查了不同设置（抽样温度和提示）对模型的影响，并进行了深入的错误分析以获得更深刻的行为认识。<details>
<summary>Abstract</summary>
With the emergence of generative conversational large language models (LLMs) like ChatGPT, serving as virtual assistants in various fields, the stability and reliability of their responses have become crucial. However, during usage, it has been observed that these models tend to waver in their judgements when confronted with follow-up questions from users expressing skepticism or disagreement. In this work, we draw inspiration from questioning strategies in education and propose a \textsc{Follow-up Questioning Mechanism} along with two evaluation metrics to assess the judgement consistency of LLMs before and after exposure to disturbances. We evaluate the judgement consistency of ChatGPT, PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning benchmarks. Empirical results show that even when the initial answers are correct, judgement consistency sharply decreases when LLMs face disturbances such as questioning, negation, or misleading. Additionally, we study these models' judgement consistency under various settings (sampling temperature and prompts) to validate this issue further, observing the impact of prompt tone and conducting an in-depth error analysis for deeper behavioral insights. Furthermore, we also explore several prompting methods to mitigate this issue and demonstrate their effectiveness\footnote{\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}.
</details>
<details>
<summary>摘要</summary>
随着生成对话大语言模型（LLMs）如ChatGPT的出现，它们作为不同领域的虚拟助手，稳定和可靠的响应成为了关键。然而，在使用过程中，这些模型在用户表达skepticism或不同意时遇到问题时，往往会变得不稳定。在这种情况下，我们从教育中的问题策略中灵感，并提出了一种\textsc{Follow-up Questioning Mechanism}，以评估LLMs在不同的问题和环境下的判断一致性。我们对ChatGPT、PaLM2-Bison和Vicuna-13B进行了八个逻辑标准套件的评估。实验结果表明，即使初始答案正确，LLMs在遇到问题、否定或误导时，判断一致性很快下降。此外，我们还研究了这些模型在不同的设置（抽象温度和提示）下的判断一致性，以验证这个问题的严重程度。最后，我们还提出了一些提示方法来解决这个问题，并证明了它们的有效性。更多细节可以参考[这里](https://github.com/NUSTM/LLMs-Waver-In-Judgements)。
</details></li>
</ul>
<hr>
<h2 id="Lyfe-Agents-Generative-agents-for-low-cost-real-time-social-interactions"><a href="#Lyfe-Agents-Generative-agents-for-low-cost-real-time-social-interactions" class="headerlink" title="Lyfe Agents: Generative agents for low-cost real-time social interactions"></a>Lyfe Agents: Generative agents for low-cost real-time social interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02172">http://arxiv.org/abs/2310.02172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, Andrew Ahn</li>
<li>for: 本研究旨在开发一种可靠、高效、低成本的自主生成代理人，用于虚拟社会中的人类社会行为模拟。</li>
<li>methods: 本研究使用了以下三个关键技术：1）选择动作框架，以减少高级决策的成本；2）异步自我监测，以提高自我一致性；3）记忆机制，以优先级化关键记忆项，降低计算成本。</li>
<li>results: 研究发现，通过应用这些技术，LYFE代理人能够展现出人类自主社会行为的特点，例如通过自主协作和信息交换解决犯罪案件（如谋杀案）。同时，这些技术可以降低计算成本，相比现有的替代方案，计算成本下降10-100倍。这些发现表明自主生成代理人在虚拟世界中潜在地可以敷充人类社会经验。<details>
<summary>Abstract</summary>
Highly autonomous generative agents powered by large language models promise to simulate intricate social behaviors in virtual societies. However, achieving real-time interactions with humans at a low computational cost remains challenging. Here, we introduce Lyfe Agents. They combine low-cost with real-time responsiveness, all while remaining intelligent and goal-oriented. Key innovations include: (1) an option-action framework, reducing the cost of high-level decisions; (2) asynchronous self-monitoring for better self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation and sociability across several multi-agent scenarios in our custom LyfeGame 3D virtual environment platform. When equipped with our brain-inspired techniques, Lyfe Agents can exhibit human-like self-motivated social reasoning. For example, the agents can solve a crime (a murder mystery) through autonomous collaboration and information exchange. Meanwhile, our techniques enabled Lyfe Agents to operate at a computational cost 10-100 times lower than existing alternatives. Our findings underscore the transformative potential of autonomous generative agents to enrich human social experiences in virtual worlds.
</details>
<details>
<summary>摘要</summary>
高度自主的生成代理人powered by大语言模型承诺可以模拟复杂的社会行为在虚拟社会中。然而，实现实时交互与人类的计算成本仍然是挑战。我们介绍了Lyfe Agent。它们结合了低成本和实时应答，同时保持智能和目标强调。关键创新包括：1.选项-动作框架， reducethe cost of high-level decisions。2.异步自我监测，提高自我一致性。3.概要和忘记记忆机制，优先级低成本关键记忆项。我们在自定义的LyfeGame 3D虚拟环境平台上评估了Lyfe Agent的自我动机和社会能力。当装备了我们的脑机制时，Lyfe Agent可以展现出人类自我动机的社会逻辑。例如，代理人可以通过自主合作和信息交换解决杀人案（一个谋杀谜）。同时，我们的技术使得Lyfe Agent可以在计算成本10-100倍低于现有alternative的情况下运行。我们的发现挑战了自主生成代理人的可能性，以浸没人类社会经验的虚拟世界。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-LLM-Agent-Network-An-LLM-agent-Collaboration-Framework-with-Agent-Team-Optimization"><a href="#Dynamic-LLM-Agent-Network-An-LLM-agent-Collaboration-Framework-with-Agent-Team-Optimization" class="headerlink" title="Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization"></a>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02170">http://arxiv.org/abs/2310.02170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salt-nlp/dylan">https://github.com/salt-nlp/dylan</a></li>
<li>paper_authors: Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang</li>
<li>for: 这个研究的目的是提高大型语言模型（LLM）代理的性能，并通过将多个LLM代理集成起来，以提高它们的普遍性和可靠性。</li>
<li>methods: 这个研究使用了一个名为“动态LLM-代理网络”（DyLAN）的框架，允许LLM代理在问题查询中互动，并在构成团队时选择最佳的代理。它还包括一个早期停止机制和一个自动团队优化算法，以提高性能和可效性。</li>
<li>results: 实验结果显示，DyLAN在逻辑和代码生成等复杂任务中表现出色，与单一GPT-35-turbo执行的结果相比，DyLAN可以获得13.0%和13.3%的提升。在特定主题的MMLU中，团队优化算法可以提高准确性达25.0%。<details>
<summary>Abstract</summary>
Large language model (LLM) agents have been shown effective on a wide range of tasks, and by ensembling multiple LLM agents, their performances could be further improved. Existing approaches employ a fixed set of agents to interact with each other in a static architecture, which limits their generalizability to various tasks and requires strong human prior in designing these agents. In this work, we propose to construct a strategic team of agents communicating in a dynamic interaction architecture based on the task query. Specifically, we build a framework named Dynamic LLM-Agent Network ($\textbf{DyLAN}$) for LLM-agent collaboration on complicated tasks like reasoning and code generation. DyLAN enables agents to interact for multiple rounds in a dynamic architecture with inference-time agent selection and an early-stopping mechanism to improve performance and efficiency. We further design an automatic agent team optimization algorithm based on an unsupervised metric termed $\textit{Agent Importance Score}$, enabling the selection of best agents based on the contribution each agent makes. Empirically, we demonstrate that DyLAN performs well in both reasoning and code generation tasks with reasonable computational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and HumanEval, respectively, compared to a single execution on GPT-35-turbo. On specific subjects of MMLU, agent team optimization in DyLAN increases accuracy by up to 25.0%.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）代理已被证明可以在各种任务上达到出色的效果，并通过 ensemble 多个 LLM 代理来进一步提高其性能。现有的方法通常采用固定的代理集合来交互在静态架构中，这限制了它们在不同任务上的泛化能力和需要强大的人工指导。在这项工作中，我们提议构建一个灵活的代理团队通过任务查询来交互。特别是，我们建立了名为 DyLAN（动态LLM代理网络）的框架，用于LLM代理在复杂任务中的合作。DyLAN 允许代理在动态架构中进行多轮交互，并在推理时选择代理和早期停止机制以提高性能和效率。此外，我们还设计了一种基于无监督度量的自动代理团队优化算法，以便根据每个代理的贡献来选择最佳的代理。Empirically，我们证明了 DyLAN 在理解和代码生成任务中表现出色，并且相对于单个执行 GPT-35-turbo 的情况下，DyLAN 可以提高 MATH 和 HumanEval 的表现，分别提高了13.0%和13.3%。在特定主题的 MMLU 任务中，DyLAN 的代理团队优化可以提高准确率达25.0%。
</details></li>
</ul>
<hr>
<h2 id="Editing-Personality-for-LLMs"><a href="#Editing-Personality-for-LLMs" class="headerlink" title="Editing Personality for LLMs"></a>Editing Personality for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02168">http://arxiv.org/abs/2310.02168</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyedit">https://github.com/zjunlp/easyedit</a></li>
<li>paper_authors: Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen</li>
<li>for: 这篇论文旨在编辑大语言模型（LLM）的人性特质。</li>
<li>methods: 该任务使用新的benchmark dataset PersonalityEdit，基于社会心理学理论选择了三种表现人性特质：躁郁、外向和合作。通过GPT-4生成响应，不仅与指定话题相符，还体现出目标人性特质。</li>
<li>results: 经过全面的基线测试和分析，发现这些基线在表现人性特质方面存在一些挑战，表明这个任务还存在一些问题。研究人员预计这种任务的成果可以为NLP社区提供新的想法。代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/zjunlp/EasyEdit%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/zjunlp/EasyEdit中发布。</a><details>
<summary>Abstract</summary>
This paper introduces an innovative task focused on editing the personality traits of Large Language Models (LLMs). This task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. Specifically, we construct a new benchmark dataset PersonalityEdit to address this task. Drawing on the theory in Social Psychology, we isolate three representative traits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our benchmark. We then gather data using GPT-4, generating responses that not only align with a specified topic but also embody the targeted personality trait. We conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in LLMs. Our intriguing findings uncover potential challenges of the proposed task, illustrating several remaining issues. We anticipate that our work can provide the NLP community with insights. Code and datasets will be released at https://github.com/zjunlp/EasyEdit.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-a-Unified-Framework-for-Sequential-Decision-Making"><a href="#Towards-a-Unified-Framework-for-Sequential-Decision-Making" class="headerlink" title="Towards a Unified Framework for Sequential Decision Making"></a>Towards a Unified Framework for Sequential Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02167">http://arxiv.org/abs/2310.02167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Pablo Mesejo, Juan Fernández-Olivares</li>
<li>for: 提供一个通用的Sequential Decision Making（SDM）框架，以帮助理解Automated Planning（AP）和Reinforcement Learning（RL）的集成。</li>
<li>methods: 基于概率论和 bayesian inference 概念，从Classical Planning到 Deep RL 任何方法都可以适用。</li>
<li>results: 提出了一种通用的SDM任务的训练和测试Markov Decision Processes（MDPs），以确保总结抽象。还提出了一种基于任务知识的协助估计方法，并 derivated 一组公式和算法用于计算SDM任务和方法的有趣属性，使其可以进行实验评估和比较。<details>
<summary>Abstract</summary>
In recent years, the integration of Automated Planning (AP) and Reinforcement Learning (RL) has seen a surge of interest. To perform this integration, a general framework for Sequential Decision Making (SDM) would prove immensely useful, as it would help us understand how AP and RL fit together. In this preliminary work, we attempt to provide such a framework, suitable for any method ranging from Classical Planning to Deep RL, by drawing on concepts from Probability Theory and Bayesian inference. We formulate an SDM task as a set of training and test Markov Decision Processes (MDPs), to account for generalization. We provide a general algorithm for SDM which we hypothesize every SDM method is based on. According to it, every SDM algorithm can be seen as a procedure that iteratively improves its solution estimate by leveraging the task knowledge available. Finally, we derive a set of formulas and algorithms for calculating interesting properties of SDM tasks and methods, which make possible their empirical evaluation and comparison.
</details>
<details>
<summary>摘要</summary>
We formulate an SDM task as a set of training and test Markov Decision Processes (MDPs) to account for generalization. We propose a general algorithm for SDM, which we hypothesize is the basis for every SDM method. According to this algorithm, every SDM algorithm iteratively improves its solution estimate by leveraging task knowledge available.We derive a set of formulas and algorithms for calculating interesting properties of SDM tasks and methods, enabling their empirical evaluation and comparison. These properties include the expected cumulative reward, the probability of success, and the expected time to complete the task.Our proposed framework provides a unified approach to SDM, enabling the integration of various methods, from Classical Planning to Deep RL. By leveraging the power of Probability Theory and Bayesian inference, we can better understand the underlying principles of SDM and develop more effective and efficient algorithms for solving complex decision-making problems.
</details></li>
</ul>
<hr>
<h2 id="Conceptual-Framework-for-Autonomous-Cognitive-Entities"><a href="#Conceptual-Framework-for-Autonomous-Cognitive-Entities" class="headerlink" title="Conceptual Framework for Autonomous Cognitive Entities"></a>Conceptual Framework for Autonomous Cognitive Entities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06775">http://arxiv.org/abs/2310.06775</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/daveshap/ACE_Framework">https://github.com/daveshap/ACE_Framework</a></li>
<li>paper_authors: David Shapiro, Wangfan Li, Manuel Delaflor, Carlos Toxtli</li>
<li>for: 这篇论文的目的是提出一种新的认知架构，帮助机器人和软件代理人更加独立地运行。</li>
<li>methods: 该论文使用了一种名为ACE模型，这是一种基于OSI模型的认知架构，用于概括人工智能系统。</li>
<li>results: 该论文提出了一种新的认知架构，并测试了这种架构在实际应用中的可行性。该架构包括6层：aspirational层、全球策略层、代理模型层、执行函数层、认知控制层和任务追究层。每个层都扮演着不同的角色，从设定道德基础和战略思维到任务选择和执行。<details>
<summary>Abstract</summary>
The rapid development and adoption of Generative AI (GAI) technology in the form of chatbots such as ChatGPT and Claude has greatly increased interest in agentic machines. This paper introduces the Autonomous Cognitive Entity (ACE) model, a novel framework for a cognitive architecture, enabling machines and software agents to operate more independently. Drawing inspiration from the OSI model, the ACE framework presents layers of abstraction to conceptualize artificial cognitive architectures. The model is designed to harness the capabilities of the latest generative AI technologies, including large language models (LLMs) and multimodal generative models (MMMs), to build autonomous, agentic systems. The ACE framework comprises six layers: the Aspirational Layer, Global Strategy, Agent Model, Executive Function, Cognitive Control, and Task Prosecution. Each layer plays a distinct role, ranging from setting the moral compass and strategic thinking to task selection and execution. The ACE framework also incorporates mechanisms for handling failures and adapting actions, thereby enhancing the robustness and flexibility of autonomous agents. This paper introduces the conceptual framework and proposes implementation strategies that have been tested and observed in industry. The goal of this paper is to formalize this framework so as to be more accessible.
</details>
<details>
<summary>摘要</summary>
快速发展和应用生成人工智能（GAI）技术，如ChatGPT和Claude，对职业机器人的兴趣带来了急速增长。这篇论文介绍了自主认知体系（ACE）模型，一种新的认知架构，使得机器人和软件代理能够更加独立地运行。以OSI模型为 inspirations，ACE模型提供了各种层次抽象，用于描述人工认知体系。该模型采用了最新的生成人工智能技术，包括大语言模型（LLM）和多模态生成模型（MMM），以建立自主、主动的系统。ACE模型包括六层：aspirational层、全球策略层、代理模型层、执行函数层、认知控制层和任务执行层。每层都扮演着不同的角色，从设定道德指南和战略思维到任务选择和执行。ACE模型还包括处理失败和适应行动的机制，从而提高自主机器人的可靠性和灵活性。这篇论文将 introduce this framework，并提出了在行业中测试和观察的实施策略。文章的目的是以更加访问性的形式，将这个框架正式化。
</details></li>
</ul>
<hr>
<h2 id="Selenite-Scaffolding-Decision-Making-with-Comprehensive-Overviews-Elicited-from-Large-Language-Models"><a href="#Selenite-Scaffolding-Decision-Making-with-Comprehensive-Overviews-Elicited-from-Large-Language-Models" class="headerlink" title="Selenite: Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models"></a>Selenite: Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02161">http://arxiv.org/abs/2310.02161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Xieyang Liu, Tongshuang Wu, Tianying Chen, Franklin Mingzhe Li, Aniket Kittur, Brad A. Myers</li>
<li>for: 帮助用户在不熟悉的领域做出决策，减少用户的比较努力，提高决策效率。</li>
<li>methods: 利用自然语言处理技术和机器学习算法，自动生成option和标准的概述，帮助用户快速理解和掌握新信息。</li>
<li>results: 三个研究显示，selenite可靠地生成准确的概述，大幅加速用户的信息处理速度，提高了用户的总体理解和决策体验。<details>
<summary>Abstract</summary>
Decision-making in unfamiliar domains can be challenging, demanding considerable user effort to compare different options with respect to various criteria. Prior research and our formative study found that people would benefit from seeing an overview of the information space upfront, such as the criteria that others have previously found useful. However, existing sensemaking tools struggle with the "cold-start" problem -- it not only requires significant input from previous users to generate and share these overviews, but such overviews may also be biased and incomplete. In this work, we introduce a novel system, Selenite, which leverages LLMs as reasoning machines and knowledge retrievers to automatically produce a comprehensive overview of options and criteria to jumpstart users' sensemaking processes. Subsequently, Selenite also adapts as people use it, helping users find, read, and navigate unfamiliar information in a systematic yet personalized manner. Through three studies, we found that Selenite produced accurate and high-quality overviews reliably, significantly accelerated users' information processing, and effectively improved their overall comprehension and sensemaking experience.
</details>
<details>
<summary>摘要</summary>
决策在不熟悉的领域可能是具有挑战性的，需要用户投入很大的努力来比较不同的选项，并考虑各种标准。先前的研究和我们的形成研究发现，人们会受益于在头一次使用时看到信息空间的概述，例如其他人在过去找到的有用的标准。然而，现有的感知工具受到“冷启动”问题的困扰——不仅需要大量的先前用户的输入来生成和分享这些概述，而且这些概述也可能受到偏见和缺失。在这项工作中，我们介绍了一种新的系统——Selenite，它利用人工智能语言模型（LLM）作为思维机器和知识检索器，自动生成选项和标准的全面概述，以便让用户快速开始感知过程。此外，Selenite还可以适应用户的使用，帮助用户找到、阅读和浏览未知的信息，并且系统化地帮助用户进行个性化的感知体验。通过三项研究，我们发现Selenite可靠地生成高质量的概述，可靠地加速用户的信息处理，并有效地改善用户的总体感知和感知体验。
</details></li>
</ul>
<hr>
<h2 id="Finite-Time-Analysis-of-Whittle-Index-based-Q-Learning-for-Restless-Multi-Armed-Bandits-with-Neural-Network-Function-Approximation"><a href="#Finite-Time-Analysis-of-Whittle-Index-based-Q-Learning-for-Restless-Multi-Armed-Bandits-with-Neural-Network-Function-Approximation" class="headerlink" title="Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation"></a>Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02147">http://arxiv.org/abs/2310.02147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guojun Xiong, Jian Li</li>
<li>for: 这 paper 是关于 restless multi-armed bandits (RMAB) 问题的一种 asymptotically optimal 的 heuristic，但是计算 Whittle 指数仍然具有困难。</li>
<li>methods: 这 paper 提出了一种基于 Q-学习 的 Whittle index 算法，称为 Neural-Q-Whittle，其中使用了 neural network 函数近似来计算 Q-函数值，并在两个不同的时间尺度上更新 Q-函数值和 Whittle 指数。</li>
<li>results: 这 paper 提供了 Neural-Q-Whittle 算法的 finite-time 分析，其中数据来自 Markov chain，Q-函数被approx 成了 ReLU 神经网络。分析使用了 Lyapunov 漂移方法，并考虑了函数近似 error。结果显示，Neural-Q-Whittle 算法在 $\mathcal{O}(1&#x2F;k^{2&#x2F;3})$ 时间下达到 convergence rate，其中 $k$ 是迭代次数。<details>
<summary>Abstract</summary>
Whittle index policy is a heuristic to the intractable restless multi-armed bandits (RMAB) problem. Although it is provably asymptotically optimal, finding Whittle indices remains difficult. In this paper, we present Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation, which is an example of nonlinear two-timescale stochastic approximation with Q-function values updated on a faster timescale and Whittle indices on a slower timescale. Despite the empirical success of deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which couples neural networks with two-timescale Q-learning largely remains unclear. This paper provides a finite-time analysis of Neural-Q-Whittle, where data are generated from a Markov chain, and Q-function is approximated by a ReLU neural network. Our analysis leverages a Lyapunov drift approach to capture the evolution of two coupled parameters, and the nonlinearity in value function approximation further requires us to characterize the approximation error. Combing these provide Neural-Q-Whittle with $\mathcal{O}(1/k^{2/3})$ convergence rate, where $k$ is the number of iterations.
</details>
<details>
<summary>摘要</summary>
“对于困难的多臂枪客问题（RMAB），Whittle指标政策是一种几乎可以推导到最佳解的规律。然而，实际上找到Whittle指标仍然具有挑战性。在这篇论文中，我们提出了一个使用神经网络函数近似的Whittle指标基于Q学习算法，即Neural-Q-Whittle。这是一种具有两个时间步长的随机测approximation，其中Q值在更快的时间步长上更新，而Whittle指标则在更慢的时间步长上更新。尽管深度学习的实际成功，Neural-Q-Whittle的非对称数据分析仍然不清楚。这篇论文提供了Neural-Q-Whittle在Markov链上获得的finite-time分析，并且利用了Lyapunov滑动方法来捕捉两个耦合的参数的演化。由于值函数近似的非线性性，我们需要 characterize Approximation error。通过结合这些因素，我们可以给出Neural-Q-Whittle的$\mathcal{O}(1/k^{2/3})$的数据分析速率，其中$k$是迭代次数。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Reliable-Logical-Rules-with-SATNet"><a href="#Learning-Reliable-Logical-Rules-with-SATNet" class="headerlink" title="Learning Reliable Logical Rules with SATNet"></a>Learning Reliable Logical Rules with SATNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02133">http://arxiv.org/abs/2310.02133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyu Li, Jinpei Guo, Yuhe Jiang, Xujie Si</li>
<li>for: 本研究旨在推动逻辑推理和深度学习之间的 integrate，以建立更高级的 AI 系统。</li>
<li>methods: 我们提出了一种新的框架，通过 differentiable learning 生成可解释的和可验证的逻辑规则，不需要先天的逻辑结构。我们的方法基于 SATNet，一种可导式 MaxSAT 解决器，通过输入输出示例学习出下面的规则。</li>
<li>results: 我们的方法可以生成高可靠性的逻辑规则，并通过多种有效的验证技术验证其与真实规则的函数等价性。实验表明，使用 exact solvers 验证我们的决策则可以达到 100% 的准确率，而原始 SATNet 在许多情况下无法给出正确的解决方案。<details>
<summary>Abstract</summary>
Bridging logical reasoning and deep learning is crucial for advanced AI systems. In this work, we present a new framework that addresses this goal by generating interpretable and verifiable logical rules through differentiable learning, without relying on pre-specified logical structures. Our approach builds upon SATNet, a differentiable MaxSAT solver that learns the underlying rules from input-output examples. Despite its efficacy, the learned weights in SATNet are not straightforwardly interpretable, failing to produce human-readable rules. To address this, we propose a novel specification method called "maximum equality", which enables the interchangeability between the learned weights of SATNet and a set of propositional logical rules in weighted MaxSAT form. With the decoded weighted MaxSAT formula, we further introduce several effective verification techniques to validate it against the ground truth rules. Experiments on stream transformations and Sudoku problems show that our decoded rules are highly reliable: using exact solvers on them could achieve 100% accuracy, whereas the original SATNet fails to give correct solutions in many cases. Furthermore, we formally verify that our decoded logical rules are functionally equivalent to the ground truth ones.
</details>
<details>
<summary>摘要</summary>
bridging 逻辑推理和深度学习是高级人工智能系统的关键。在这项工作中，我们提出了一种新的框架，通过分别学习生成可读可验证的逻辑规则，不需要预先指定的逻辑结构。我们的方法基于SATNet，一种可微的MaxSAT解决方案，从输入输出示例中学习下面的规则。虽然SATNet的学习结果具有效果，但是学习出来的权重不是直观可读的，无法生成人类可读的规则。为解决这个问题，我们提出了一种新的规定方法 called "最大等式"，允许将SATNet学习出来的权重与一组带权的 propositional 逻辑规则相互转换。通过解码的带权MaxSAT式，我们进一步引入了一些有效的验证技术，以验证它们是否与真实规则函数等价。实验表明，我们的解码规则具有高度可靠性：使用 exact 解决器处理它们可以达到100%的准确率，而原始SATNet在许多情况下无法提供正确的解决方案。此外，我们正式验证了我们解码的逻辑规则是否函数等价于真实规则。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Pitfalls-of-Knowledge-Editing-for-Large-Language-Models"><a href="#Unveiling-the-Pitfalls-of-Knowledge-Editing-for-Large-Language-Models" class="headerlink" title="Unveiling the Pitfalls of Knowledge Editing for Large Language Models"></a>Unveiling the Pitfalls of Knowledge Editing for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02129">http://arxiv.org/abs/2310.02129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/pitfallsknowledgeediting">https://github.com/zjunlp/pitfallsknowledgeediting</a></li>
<li>paper_authors: Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen</li>
<li>for: 本研究探讨了对大型自然语言模型（LLMs）知识编辑的风险。</li>
<li>methods: 本研究提出了新的评估指标和基准集，以评估知识编辑对LLMs的影响。</li>
<li>results: 研究发现，知识编辑可能会导致两类问题：知识冲突和知识扭曲。这些问题可能会对LLMs产生不良影响，需要未来研究的注意和努力。<details>
<summary>Abstract</summary>
As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of LLMs. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on LLMs, which warrant attention and efforts for future works. Code will be released at https://github.com/zjunlp/PitfallsKnowledgeEditing.
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLM）细化成本的增加，现有研究努力转移到开发方法来编辑 LLM 中的隐式知识。然而，仍然有一个阴影倒挂着——编辑知识是否会触发蝴蝶效应？由于 editing 可能会引入侧效，这些问题仍然未得到解决。本文开拓了 LLM 中编辑知识的可能风险的研究。为此，我们提出了新的 benchmarck 数据集和创新的评价指标。我们的结果显示了两个重要问题：（1）知识冲突：编辑冲突的知识组可能会增加 LLM 中的内在不一致性。（2）知识扭曲：修改参数以编辑实际知识可能会无法回归 LLM 的内生知识结构。实验结果表明，编辑知识可能会不良果的影响 LLM，这些问题需要未来的研究。代码将在 <https://github.com/zjunlp/PitfallsKnowledgeEditing> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Collaboration-Mechanisms-for-LLM-Agents-A-Social-Psychology-View"><a href="#Exploring-Collaboration-Mechanisms-for-LLM-Agents-A-Social-Psychology-View" class="headerlink" title="Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View"></a>Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02124">http://arxiv.org/abs/2310.02124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/machinesom">https://github.com/zjunlp/machinesom</a></li>
<li>paper_authors: Jintian Zhang, Xin Xu, Shumin Deng</li>
<li>for: 本研究探讨了现代自然语言处理（NLP）系统在多代理社会中是否能够模仿人类的协同智能。</li>
<li>methods: 本研究结合了实验和理论视角，对当今NLP系统的协同机制进行了探索。研究者 fabricated four unique ‘societies’，每个社会由多个语言模型（LLM）代表，每个代表有特定的 ‘ trait’（愿景或自信）和 ‘ thinking pattern’（辩论或反思）。</li>
<li>results: 研究发现，LLM代表在完成任务时会采用不同的社会行为，从活泼的辩论到 introspective 的反思。此外，研究还发现了一些协同策略，可以提高效率（使用更少的 API  tokens），同时也超越了之前的顶尖方法。此外，研究还发现了 LLM 代表具有人类社会行为的特征，如同论或多数规则。<details>
<summary>Abstract</summary>
As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Evaluating these multi-agent societies on three benchmark datasets, we discern that LLM agents navigate tasks by leveraging diverse social behaviors, from active debates to introspective reflections. Notably, certain collaborative strategies only optimize efficiency (using fewer API tokens), but also outshine previous top-tier approaches. Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity or majority rule, mirroring foundational Social Psychology theories. In conclusion, we integrate insights from Social Psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets (already submitted in supplementary materials), hoping to catalyze further research in this promising avenue (All code and data are available at \url{https://github.com/zjunlp/MachineSoM}.).
</details>
<details>
<summary>摘要</summary>
如果自然语言处理（NLP）系统在复杂社会环境中得到广泛应用，那么一个重要问题就是：这些NLP系统能否模仿人类的协同智能？这篇论文通过实验和理论启示来探索当今NLP系统之间的协同机制。我们创造了四个不同的“社会”，每个社会由多个大语言模型（LLM）组成，每个LLM代表不同的“特质”（愿景或自信），并且采用不同的“思维模式”（辩论或 introspection）进行协同。我们在三个标准测试集上评估这些多代理社会，发现LLM代理在完成任务时会采用多种社会行为，从活泼的辩论到 introspective reflection。尤其是，某些协同策略可以使用更少的API токен，同时也超越了之前的顶尖方法。此外，我们的结果还表明LLM代理展现出人类社会行为的特征，如同跟随性或多数规则，这与基本社会心理学理论相吻合。在结论中，我们将社会心理学理论与LLM协同机制相结合，并希望通过分享我们的代码和数据（已经在补充材料中提交），以便促进这一领域的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="TWIZ-The-Wizard-of-Multimodal-Conversational-Stimulus"><a href="#TWIZ-The-Wizard-of-Multimodal-Conversational-Stimulus" class="headerlink" title="TWIZ: The Wizard of Multimodal Conversational-Stimulus"></a>TWIZ: The Wizard of Multimodal Conversational-Stimulus</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02118">http://arxiv.org/abs/2310.02118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Ferreira, Diogo Tavares, Diogo Silva, Rodrigo Valério, João Bordalo, Inês Simões, Vasco Ramos, David Semedo, João Magalhães</li>
<li>For: The paper is written to describe the vision, challenges, and scientific contributions of the Task Wizard team (TWIZ) in the Alexa Prize TaskBot Challenge 2022.* Methods: The paper focuses on three main research questions: (1) Humanly-Shaped Conversations, (2) Multimodal Stimulus, and (3) Zero-shot Conversational Flows.* Results: The TWIZ bot is an effective and robust system that can guide users through complex manual tasks while providing several multimodal stimuli. The bot is capable of supporting a wide range of tasks and has several innovative features such as creative cooking and video navigation through voice.<details>
<summary>Abstract</summary>
In this report, we describe the vision, challenges, and scientific contributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot Challenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal, knowledgeable, and engaging assistant that can guide users towards the successful completion of complex manual tasks. To achieve this, we focus our efforts on three main research questions: (1) Humanly-Shaped Conversations, by providing information in a knowledgeable way; (2) Multimodal Stimulus, making use of various modalities including voice, images, and videos; and (3) Zero-shot Conversational Flows, to improve the robustness of the interaction to unseen scenarios. TWIZ is an assistant capable of supporting a wide range of tasks, with several innovative features such as creative cooking, video navigation through voice, and the robust TWIZ-LLM, a Large Language Model trained for dialoguing about complex manual tasks. Given ratings and feedback provided by users, we observed that TWIZ bot is an effective and robust system, capable of guiding users through tasks while providing several multimodal stimuli.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我们描述了任务魔法团队（TWIZ）在Alexa奖任务机器人挑战2022中的视野、挑战和科学贡献。我们的视野是建立一个有用、多Modal、知识型和有趣的助手，帮助用户完成复杂的手动任务。为了实现这一目标，我们对三个主要研究问题进行了集中努力：1. 人类化对话，通过提供知识型的信息，使用户感觉到和人类交流相似。2. 多Modal 刺激，使用声音、图片和视频等多种Modalities。3. 零shot对话流程，以提高对未看过的情况的响应性。TWIZ是一个能够支持多种任务的助手，具有创新的特点，如创意cooking、通过声音导航视频、robust TWIZ-LLM，一个对对话的大语言模型。根据用户提供的评分和反馈，我们发现TWIZ机器人是一个有效和Robust的系统，能够引导用户完成任务，并提供多种多Modal 刺激。
</details></li>
</ul>
<hr>
<h2 id="Towards-Effective-Human-AI-Decision-Making-The-Role-of-Human-Learning-in-Appropriate-Reliance-on-AI-Advice"><a href="#Towards-Effective-Human-AI-Decision-Making-The-Role-of-Human-Learning-in-Appropriate-Reliance-on-AI-Advice" class="headerlink" title="Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice"></a>Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02108">http://arxiv.org/abs/2310.02108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Schemmer, Andrea Bartos, Philipp Spitzer, Patrick Hemmer, Niklas Kühl, Jonas Liebschner, Gerhard Satzger</li>
<li>for: 本研究旨在探讨人类和人工智能（AI）合作的真正潜力是如何利用人类和AI的各自优势来实现联合性能超越个体AI或人类的性能，即实现补做团队性能（CTP）。</li>
<li>methods: 该研究使用实验方法，具体来说是采用100名参与者进行实验，以评估人类对AI建议的适当依赖。</li>
<li>results: 研究发现，人类学习是适当依赖AI建议的关键因素，而不仅仅是心理模型。此外，研究还提出了基本概念和设计方法，以便更好地分析依赖和实现人类AI决策的效果。<details>
<summary>Abstract</summary>
The true potential of human-AI collaboration lies in exploiting the complementary capabilities of humans and AI to achieve a joint performance superior to that of the individual AI or human, i.e., to achieve complementary team performance (CTP). To realize this complementarity potential, humans need to exercise discretion in following AI 's advice, i.e., appropriately relying on the AI's advice. While previous work has focused on building a mental model of the AI to assess AI recommendations, recent research has shown that the mental model alone cannot explain appropriate reliance. We hypothesize that, in addition to the mental model, human learning is a key mediator of appropriate reliance and, thus, CTP. In this study, we demonstrate the relationship between learning and appropriate reliance in an experiment with 100 participants. This work provides fundamental concepts for analyzing reliance and derives implications for the effective design of human-AI decision-making.
</details>
<details>
<summary>摘要</summary>
人类和人工智能（AI）的共同努力的真正潜力在于利用人类和AI的优势相互补做，以实现合作性能超过个体AI或人类的表现，即实现共同团队性能（CTP）。为实现这种共同可能性，人类需要在AI的建议下使用自己的聪明，即有选择地采纳AI的建议。在以前的研究中，人们主要关注建立AI的心理模型来评估AI的建议，但最新的研究表明，心理模型alone不能解释合适的依赖。我们假设，除了心理模型之外，人类学习也是适用依赖的关键因素，因此CTP。在这项实验中，我们证明了学习与合适依赖之间的关系，并 derive了对人类AI决策的设计方法的基本思想。
</details></li>
</ul>
<hr>
<h2 id="CoNO-Complex-Neural-Operator-for-Continuous-Dynamical-Systems"><a href="#CoNO-Complex-Neural-Operator-for-Continuous-Dynamical-Systems" class="headerlink" title="CoNO: Complex Neural Operator for Continuous Dynamical Systems"></a>CoNO: Complex Neural Operator for Continuous Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02094">http://arxiv.org/abs/2310.02094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karn Tiwari, N M Anoop Krishnan, Prathosh A P<br>for:CoNO is designed to model continuous dynamical systems, such as weather forecasting, fluid flow, and solid mechanics.methods:CoNO uses a complex neural network with integral kernel parameterization in the complex fractional Fourier domain, along with aliasing-free activation functions to preserve complex values and algebraic properties.results:CoNO exhibits improved representation, robustness to noise, and generalization compared to existing neural operator models, and achieves comparable or superior performance on several tasks including zero-shot super-resolution, evaluation of out-of-distribution data, data efficiency, and robustness to noise.<details>
<summary>Abstract</summary>
Neural operators extend data-driven models to map between infinite-dimensional functional spaces. These models have successfully solved continuous dynamical systems represented by differential equations, viz weather forecasting, fluid flow, or solid mechanics. However, the existing operators still rely on real space, thereby losing rich representations potentially captured in the complex space by functional transforms. In this paper, we introduce a Complex Neural Operator (CoNO), that parameterizes the integral kernel in the complex fractional Fourier domain. Additionally, the model employing a complex-valued neural network along with aliasing-free activation functions preserves the complex values and complex algebraic properties, thereby enabling improved representation, robustness to noise, and generalization. We show that the model effectively captures the underlying partial differential equation with a single complex fractional Fourier transform. We perform an extensive empirical evaluation of CoNO on several datasets and additional tasks such as zero-shot super-resolution, evaluation of out-of-distribution data, data efficiency, and robustness to noise. CoNO exhibits comparable or superior performance to all the state-of-the-art models in these tasks. Altogether, CoNO presents a robust and superior model for modeling continuous dynamical systems, providing a fillip to scientific machine learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-evolutionary-model-of-personality-traits-related-to-cooperative-behavior-using-a-large-language-model"><a href="#An-evolutionary-model-of-personality-traits-related-to-cooperative-behavior-using-a-large-language-model" class="headerlink" title="An evolutionary model of personality traits related to cooperative behavior using a large language model"></a>An evolutionary model of personality traits related to cooperative behavior using a large language model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05976">http://arxiv.org/abs/2310.05976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reiji Suzuki, Takaya Arita</li>
<li>for: 本研究旨在探讨多样性和社会层次上的EVOLUTIONARY dynamics, 通过将生成模型引入社会代理模型中的特质表达中，提高了模型的表达力。</li>
<li>methods: 我们使用了语言模型（LLM）提取的决策策略，将语言描述的人格特质作为基因，并通过选择和变异基因来进行人类进化。</li>
<li>results: 我们的初步实验和分析表明，这种模型可以基于多样性和高级表达的人格特质来演化合作行为。我们还发现了在表达人格特质时的重复干扰，以及基因表达中出现的行为倾向的含义。<details>
<summary>Abstract</summary>
This paper aims to shed light on the evolutionary dynamics of diverse and social populations by introducing the rich expressiveness of generative models into the trait expression of social agent-based evolutionary models. Specifically, we focus on the evolution of personality traits in the context of a game-theoretic relationship as a situation in which inter-individual interests exert strong selection pressures. We construct an agent model in which linguistic descriptions of personality traits related to cooperative behavior are used as genes. The deterministic strategies extracted from Large Language Model (LLM) that make behavioral decisions based on these personality traits are used as behavioral traits. The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish. Through preliminary experiments and analyses, we clarify that such a model can indeed exhibit the evolution of cooperative behavior based on the diverse and higher-order representation of personality traits. We also observed the repeated intrusion of cooperative and selfish personality traits through changes in the expression of personality traits, and found that the emerging words in the evolved gene well reflected the behavioral tendency of its personality in terms of their semantics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Point-Neighborhood-Embeddings"><a href="#Point-Neighborhood-Embeddings" class="headerlink" title="Point Neighborhood Embeddings"></a>Point Neighborhood Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02083">http://arxiv.org/abs/2310.02083</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ANAGHA93/t-SNE">https://github.com/ANAGHA93/t-SNE</a></li>
<li>paper_authors: Pedro Hermosilla</li>
<li>for: 本研究旨在分析点云中的邻域信息编码方法，以提高未来 neural network 架构的设计。</li>
<li>methods: 研究使用不同的邻域信息编码方法，包括多层感知机(MLP)、ReLU活化函数和简单的卷积。</li>
<li>results: 研究发现，使用MLP编码器的邻域信息编码方法实际下表现最差，甚至在某些任务上被简单的点坐标线性组合超越。此外，使用这些建议实现的神经网络架构可以达到多个任务的状态OF-the-art级Results，超越最近的更复杂的操作。<details>
<summary>Abstract</summary>
Point convolution operations rely on different embedding mechanisms to encode the neighborhood information of each point in order to detect patterns in 3D space. However, as convolutions are usually evaluated as a whole, not much work has been done to investigate which is the ideal mechanism to encode such neighborhood information. In this paper, we provide the first extensive study that analyzes such Point Neighborhood Embeddings (PNE) alone in a controlled experimental setup. From our experiments, we derive a set of recommendations for PNE that can help to improve future designs of neural network architectures for point clouds. Our most surprising finding shows that the most commonly used embedding based on a Multi-layer Perceptron (MLP) with ReLU activation functions provides the lowest performance among all embeddings, even being surpassed on some tasks by a simple linear combination of the point coordinates. Additionally, we show that a neural network architecture using simple convolutions based on such embeddings is able to achieve state-of-the-art results on several tasks, outperforming recent and more complex operations. Lastly, we show that these findings extrapolate to other more complex convolution operations, where we show how following our recommendations we are able to improve recent state-of-the-art architectures.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将点conv操作转换为标准的中文简体字符串。</SYS>>点 convolution 操作需要不同的嵌入机制来编码每个点的邻居信息，以探测3D空间中的模式。然而，通常情况下， convolution 被评估为整体，因此很少人对点邻居编码（Point Neighborhood Embeddings，PNE）进行了系统的研究。在这篇论文中，我们提供了首次对 PNE 进行了系统的研究，并在控制的实验室中进行了广泛的测试。从我们的实验结果中，我们提出了一些关于 PNE 的建议，这些建议可以帮助未来的神经网络架构设计人员为点云进行优化。我们最大化的发现是，通常使用的多层感知器（MLP）与 ReLU 活化函数基于的嵌入方法的性能最差，甚至在一些任务上被一个简单的点坐标的线性组合所超越。此外，我们显示了一种使用这些嵌入的神经网络架构可以在多个任务上达到状态的最佳结果，超越了最近的和更复杂的操作。最后，我们表明这些发现可以推广到其他更复杂的 convolution 操作，我们在这些操作中采用了我们的建议，并成功地提高了最近的状态级别的架构。
</details></li>
</ul>
<hr>
<h2 id="Towards-End-to-End-Embodied-Decision-Making-via-Multi-modal-Large-Language-Model-Explorations-with-GPT4-Vision-and-Beyond"><a href="#Towards-End-to-End-Embodied-Decision-Making-via-Multi-modal-Large-Language-Model-Explorations-with-GPT4-Vision-and-Beyond" class="headerlink" title="Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"></a>Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02071">http://arxiv.org/abs/2310.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pkunlp-icler/pca-eval">https://github.com/pkunlp-icler/pca-eval</a></li>
<li>paper_authors: Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, Baobao Chang</li>
<li>for: 本研究探索了多模态大语言模型（MLLMs）在改善体现决策过程中的潜力。</li>
<li>methods: 本研究使用了 state-of-the-art MLLMs like GPT4-Vision，以及 HOLMES 框架，让 LLMs 可以通过多模态信息来做出更加有知见的决策。</li>
<li>results: 研究发现，使用 GPT4-Vision 模型可以实现更高的体现决策能力，相比 GPT4-HOLMES 模型。此外，GPT4-Vision 模型还可以在 PCA-EVAL  benchmark 上表现出色，相比 open-source state-of-the-art MLLM。<details>
<summary>Abstract</summary>
In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们探索了多模态大语言模型（MLLMs）在改进具体决策过程中的潜力。由于大语言模型（LLMs）在逻辑能力和世界知识方面表现出色，因此我们尝试了使用MLLMs来提高具体决策。我们研究了MLLMs是否可以在端到端方式完成具体决策，以及LLMs和MLLMs之间的合作是否可以提高决策。为此，我们提出了一个新的评价指标集合，称为PCA-EVAL，用于评估具体决策从多个角度。此外，我们还提出了一种多代理合作框架，称为HOLMES，允许LLMs通过多模态信息来做出 Informed 决策。我们对PCA-EVAL和HOLMES进行比较，发现GPT4-Vision模型在PCA-EVAL上表现出色，相比GPT4-HOLMES，其决策精度提高了3%。但是，这种性能仅限于最新的GPT4-Vision模型，超过了开源状态态的MLLM的性能。我们的结果表明，具有强大的MLLMs如GPT4-Vision可能在具体决策中发挥作用，为MRLM研究提供新的 Avenues。代码和数据在https://github.com/pkunlp-icler/PCA-EVAL/。
</details></li>
</ul>
<hr>
<h2 id="Content-Bias-in-Deep-Learning-Age-Approximation-A-new-Approach-Towards-more-Explainability"><a href="#Content-Bias-in-Deep-Learning-Age-Approximation-A-new-Approach-Towards-more-Explainability" class="headerlink" title="Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability"></a>Content Bias in Deep Learning Age Approximation: A new Approach Towards more Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02067">http://arxiv.org/abs/2310.02067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Jöchl, Andreas Uhl</li>
<li>for: 这个论文主要用于探讨图像时间伪造检测中，用 neural network 学习图像年龄特征的问题。</li>
<li>methods: 该论文提出了一种新的方法来评估图像内容对于年龄分类中的影响。该方法使用synthetic图像（可以排除内容偏好），并在这些图像中嵌入年龄信号。然后，通过训练标准的 neural network 来评估内容对于年龄分类的影响。</li>
<li>results: 研究发现，使用标准的 neural network 在年龄分类任务中，具有强度依赖于图像内容的特征。为了 Mitigate 这种影响，研究人员提出了两种不同的技术，并通过论文中的方法进行评估。<details>
<summary>Abstract</summary>
In the context of temporal image forensics, it is not evident that a neural network, trained on images from different time-slots (classes), exploit solely age related features. Usually, images taken in close temporal proximity (e.g., belonging to the same age class) share some common content properties. Such content bias can be exploited by a neural network. In this work, a novel approach that evaluates the influence of image content is proposed. This approach is verified using synthetic images (where content bias can be ruled out) with an age signal embedded. Based on the proposed approach, it is shown that a `standard' neural network trained in the context of age classification is strongly dependent on image content. As a potential countermeasure, two different techniques are applied to mitigate the influence of the image content during training, and they are also evaluated by the proposed method.
</details>
<details>
<summary>摘要</summary>
在图像时间修复方面，没有直接证明神经网络，通过图像不同时间槽（类）训练，仅仅利用年龄相关特征。通常，属于同一年龄类别的图像在close temporal proximity（例如，同一个年龄类别）共享一些共同内容特征。这种内容偏好可以被神经网络利用。在这种工作中，一种新的方法，评估图像内容的影响，被提出。该方法通过使用synthetic images（ contenido bias可以排除），并将年轻信号嵌入图像中，来验证。根据所提出的方法，显示一个“标准”的神经网络，在年龄分类任务中强烈依赖于图像内容。为了 Mitigate the influence of image content during training, two different techniques are applied and evaluated by the proposed method.
</details></li>
</ul>
<hr>
<h2 id="De-Novo-Drug-Design-with-Joint-Transformers"><a href="#De-Novo-Drug-Design-with-Joint-Transformers" class="headerlink" title="De Novo Drug Design with Joint Transformers"></a>De Novo Drug Design with Joint Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02066">http://arxiv.org/abs/2310.02066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Izdebski, Ewelina Weglarz-Tomczak, Ewa Szczurek, Jakub M. Tomczak</li>
<li>for: 本研究旨在提出一种能同时生成外部数据外的新分子和预测其目标性质的新型生成模型，以解决德 ноVO drug design中的难题。</li>
<li>methods: 我们提出了一种 combining Transformer decoder、Transformer encoder 和预测器的共享权重的联合生成模型，并通过训练该模型使用 penalty 对数对数据进行优化。</li>
<li>results: 我们的方法可以在分子生成中达到领先的性能，同时降低新样本中预测错误，相比于精度调整后的 decoder-only Transformer，提高了42%。此外，我们还提出了一种基于 Joint Transformer 的 probabilistic黑盒优化算法，可以生成具有改进目标性质的新分子，比训练数据更高效。<details>
<summary>Abstract</summary>
De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
</details>
<details>
<summary>摘要</summary>
德 новো drug 设计需要同时生成外部训练数据中没有的分子和预测其目标性质，这使得生成模型具有困难任务。为此，我们提议了共同变换器（Joint Transformer），它将 transformer 解码器、transformer 编码器和预测器组合在一起，形成一个共同生成模型，其中所有参数共享。我们表明，通过训练该模型的 penalty логиarithmic 目标函数可以 достичь领域内最佳性能，同时降低新样本分子预测错误率，比较 fine-tuned 解码器 только transformer 的42%。最后，我们提议了基于 Joint Transformer 的 probabilistic 黑盒优化算法，可以通过生成新分子来提高目标性质，比较其他 SMILES 基于的优化方法在德 новォ drug 设计中表现优异。
</details></li>
</ul>
<hr>
<h2 id="Relaxed-Octahedral-Group-Convolution-for-Learning-Symmetry-Breaking-in-3D-Physical-Systems"><a href="#Relaxed-Octahedral-Group-Convolution-for-Learning-Symmetry-Breaking-in-3D-Physical-Systems" class="headerlink" title="Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems"></a>Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02299">http://arxiv.org/abs/2310.02299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wang, Robin Walters, Tess E. Smidt</li>
<li>for: 这篇论文旨在提高采样效率和泛化性，通过使用对称性来改进深度模型。</li>
<li>methods: 这篇论文提出了一种弹性八面体卷积，可以保持数据中的最高水平的对称性，同时发现物理系统中的微妙对称性破坏因素。</li>
<li>results: 实验结果表明，这种方法可以不仅提供物理系统中对称性破坏因素的理解，还可以在流体超分解任务中实现优秀的性能。<details>
<summary>Abstract</summary>
Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.
</details>
<details>
<summary>摘要</summary>
深度对称模型使用对称性来提高样本效率和泛化性。然而，在许多情况下，这些模型假设的完美对称性可能是限制性的，特别是当数据不完全与这些对称性相对应。因此，我们在这篇论文中引入了放宽的八面体群 convolution来模型三维物理系统。这种灵活的 convolution 技术可以证明地保持数据中的最高水平对称性，同时发现物理系统中的微妙对称性破坏因素。实验结果验证了我们的方法不仅可以提供物理系统中对称性破坏因素的新的视角，还可以在液体超解像任务中实现更高的性能。
</details></li>
</ul>
<hr>
<h2 id="AlignDiff-Aligning-Diverse-Human-Preferences-via-Behavior-Customisable-Diffusion-Model"><a href="#AlignDiff-Aligning-Diverse-Human-Preferences-via-Behavior-Customisable-Diffusion-Model" class="headerlink" title="AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model"></a>AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02054">http://arxiv.org/abs/2310.02054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing Hu, Tangjie Lv, Changjie Fan, Zhipeng Hu</li>
<li>for: 本文提出了一种新的框架，即 AlignDiff，用于在人工智能学习中对人类喜好进行质量评估，包括抽象性和可变性。</li>
<li>methods: 本文使用了人工智能反馈学习（RLHF）来量化人类喜好，并使用这些量化结果来导引扩散规划，以实现零基础行为定制。</li>
<li>results: 本文在多种 locomotive 任务上证明了 AlignDiff 的超越性，包括 preference matching、switching 和 covering。此外，它还可以完成人类指导下的未看过任务。<details>
<summary>Abstract</summary>
Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning (RL), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose AlignDiff, a novel framework that leverages RL from Human Feedback (RLHF) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. AlignDiff can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director for preference aligning at the inference phase. We evaluate AlignDiff on various locomotion tasks and demonstrate its superior performance on preference matching, switching, and covering compared to other baselines. Its capability of completing unseen downstream tasks under human instructions also showcases the promising potential for human-AI collaboration. More visualization videos are released on https://aligndiff.github.io/.
</details>
<details>
<summary>摘要</summary>
把代理人行为与多样化的人类偏好进行匹配仍然是现代学习（RL）中的挑战，因为人类偏好的本质和变化性具有抽象和多变性。为解决这些问题，我们提出了AlignDiff框架，它利用人类反馈学习（RLHF）来量化人类偏好，包括抽象和多变性，并使用它们来导航扩散规划，以实现零shot行为定制。AlignDiff可以准确匹配用户自定义的行为，并高效地switch между不同的行为。为建立框架，我们首先建立了多个视角的人类反馈数据集，其中包含了多种行为的属性比较，然后我们训练了一个属性强度模型，以预测量化的相对强度。接着，我们将行为数据集重新标注为相对强度，然后训练了一个属性 conditional扩散模型，它作为一个决策者，在推理阶段对偏好进行匹配。我们在多种步行任务上评估了AlignDiff，并证明其在偏好匹配、switching和覆盖方面表现出色，比基eline更好。它还可以完成未看过的下游任务，这说明了它在人AI合作中的潜力。更多视频可以在https://aligndiff.github.io/查看。
</details></li>
</ul>
<hr>
<h2 id="Jury-A-Comprehensive-Evaluation-Toolkit"><a href="#Jury-A-Comprehensive-Evaluation-Toolkit" class="headerlink" title="Jury: A Comprehensive Evaluation Toolkit"></a>Jury: A Comprehensive Evaluation Toolkit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02040">http://arxiv.org/abs/2310.02040</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/obss/jury">https://github.com/obss/jury</a></li>
<li>paper_authors: Devrim Cavusoglu, Ulas Sert, Secil Sen, Sinan Altinuc</li>
<li>for: 本研究旨在标准化和改进深度学习系统的评估方法，以便在不同任务和度量之间进行评估。</li>
<li>methods: 本研究使用了一个名为“jury”的工具包，提供了一个统一的评估框架，可以在不同任务和度量之间进行评估。</li>
<li>results: 在发布于GitHub的开源版本中，“jury”已经获得了广泛的关注和使用，并且可以帮助学术社区解决评估挑战。<details>
<summary>Abstract</summary>
Evaluation plays a critical role in deep learning as a fundamental block of any prediction-based system. However, the vast number of Natural Language Processing (NLP) tasks and the development of various metrics have led to challenges in evaluating different systems with different metrics. To address these challenges, we introduce jury, a toolkit that provides a unified evaluation framework with standardized structures for performing evaluation across different tasks and metrics. The objective of jury is to standardize and improve metric evaluation for all systems and aid the community in overcoming the challenges in evaluation. Since its open-source release, jury has reached a wide audience and is available at https://github.com/obss/jury.
</details>
<details>
<summary>摘要</summary>
评估在深度学习中扮演了关键的角色，是任何预测基本系统的基本块。然而，由于各种自然语言处理（NLP）任务的庞大数量和不同的评价指标的发展，评估不同系统的评价带来了挑战。为解决这些挑战，我们引入了一个名为“评审团”（jury）的工具包，它提供了一个统一的评估框架，可以在不同任务和指标之间进行标准化的评估。jury的目标是标准化和改进所有系统的评估，以帮助社区超越评估的挑战。自其开源发布以来，jury已经达到了广泛的用户群和可以在https://github.com/obss/jury上下载。
</details></li>
</ul>
<hr>
<h2 id="An-evaluation-of-pre-trained-models-for-feature-extraction-in-image-classification"><a href="#An-evaluation-of-pre-trained-models-for-feature-extraction-in-image-classification" class="headerlink" title="An evaluation of pre-trained models for feature extraction in image classification"></a>An evaluation of pre-trained models for feature extraction in image classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02037">http://arxiv.org/abs/2310.02037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jawad-Dar/Jaya-Honey-Badger-Optimization-based-Deep-Neuro-Fuzzy-Network-structure-for-detection-of-Covid-19-">https://github.com/Jawad-Dar/Jaya-Honey-Badger-Optimization-based-Deep-Neuro-Fuzzy-Network-structure-for-detection-of-Covid-19-</a></li>
<li>paper_authors: Erick da Silva Puls, Matheus V. Todescato, Joel L. Carbonera</li>
<li>for: 这个研究的目的是比较不同预训网络模型在图像分类任务中的表现。</li>
<li>methods: 这个研究使用了16个预训网络模型，并在四个图像dataset上进行评估。</li>
<li>results: 我们的结果显示，CLIP-ViT-B和ViT-H-14模型在所有dataset上均有最好的总表现，而CLIP-ResNet50模型则有相似的表现，但较少的波动。这显示了这些模型在图像分类任务中的表现。<details>
<summary>Abstract</summary>
In recent years, we have witnessed a considerable increase in performance in image classification tasks. This performance improvement is mainly due to the adoption of deep learning techniques. Generally, deep learning techniques demand a large set of annotated data, making it a challenge when applying it to small datasets. In this scenario, transfer learning strategies have become a promising alternative to overcome these issues. This work aims to compare the performance of different pre-trained neural networks for feature extraction in image classification tasks. We evaluated 16 different pre-trained models in four image datasets. Our results demonstrate that the best general performance along the datasets was achieved by CLIP-ViT-B and ViT-H-14, where the CLIP-ResNet50 model had similar performance but with less variability. Therefore, our study provides evidence supporting the choice of models for feature extraction in image classification tasks.
</details>
<details>
<summary>摘要</summary>
近年来，我们所目睹到的图像分类任务中表现的提升非常显著。这种表现提升主要归功于深度学习技术的推广。深度学习技术通常需要大量的标注数据，因此对于小 datasets 来说是一大挑战。在这种情况下，转移学习策略成为了一个有前途的解决方案。本研究的目的是比较不同预训练神经网络的特征提取性能在图像分类任务中。我们在四个图像 datasets 中评估了16个预训练模型。我们的结果显示，CLIP-ViT-B 和 ViT-H-14 模型在所有 datasets 中表现最佳，而 CLIP-ResNet50 模型具有类似表现，但变化较少。因此，本研究提供了支持预训练模型选择的证据，以便在图像分类任务中进行特征提取。
</details></li>
</ul>
<hr>
<h2 id="OceanGPT-A-Large-Language-Model-for-Ocean-Science-Tasks"><a href="#OceanGPT-A-Large-Language-Model-for-Ocean-Science-Tasks" class="headerlink" title="OceanGPT: A Large Language Model for Ocean Science Tasks"></a>OceanGPT: A Large Language Model for Ocean Science Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02031">http://arxiv.org/abs/2310.02031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/knowlm">https://github.com/zjunlp/knowlm</a></li>
<li>paper_authors: Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen</li>
<li>For: The paper aims to explore the potential of Large Language Models (LLMs) for ocean science tasks, and to address the limitations of current LLMs in catering to the needs of domain experts like oceanographers.* Methods: The authors propose a novel framework called DoInstruct to automatically obtain a large volume of ocean domain instruction data, and construct the first oceanography benchmark called OceanBench to evaluate the capabilities of LLMs in the ocean domain.* Results: The authors introduce OceanGPT, the first-ever LLM in the ocean domain, which shows a higher level of knowledge expertise for ocean science tasks and gains preliminary embodied intelligence capabilities in ocean technology through comprehensive experiments.<details>
<summary>Abstract</summary>
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology. Codes, data and checkpoints will soon be available at https://github.com/zjunlp/KnowLM.
</details>
<details>
<summary>摘要</summary>
海洋科学，探索地球表面的70%以上的海洋，对生物多样性和生命支持非常重要。 current Large Language Models (LLMs) 在其他领域已经取得了成功，但是在海洋科学领域，LLMs frequently fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The reason may be the complexity and intricacy of ocean data, as well as the need for higher granularity and richness in knowledge. To address these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is proficient in various ocean science tasks. We also propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Furthermore, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Through comprehensive experiments, OceanGPT not only demonstrates a higher level of knowledge expertise for ocean science tasks but also gains preliminary embodied intelligence capabilities in ocean technology. codes, data, and checkpoints will soon be available at https://github.com/zjunlp/KnowLM.
</details></li>
</ul>
<hr>
<h2 id="Prompting-Audios-Using-Acoustic-Properties-For-Emotion-Representation"><a href="#Prompting-Audios-Using-Acoustic-Properties-For-Emotion-Representation" class="headerlink" title="Prompting Audios Using Acoustic Properties For Emotion Representation"></a>Prompting Audios Using Acoustic Properties For Emotion Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02298">http://arxiv.org/abs/2310.02298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh</li>
<li>for: 用于改进情感表示和识别</li>
<li>methods: 使用自然语言描述（或提示）和对比学习对象来自动生成提示，并将speech和提示对应起来</li>
<li>results: 在Emotion Audio Retrieval和Speech Emotion Recognition任务上，使用acoustic prompts显著提高了模型的性能，precision@k指标在EAR中提高了 Various 的值，在 Ravdess 数据集上，relative accuracy 提高了3.8%。<details>
<summary>Abstract</summary>
Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Feasible-Counterfactual-Explanations-A-Taxonomy-Guided-Template-based-NLG-Method"><a href="#Towards-Feasible-Counterfactual-Explanations-A-Taxonomy-Guided-Template-based-NLG-Method" class="headerlink" title="Towards Feasible Counterfactual Explanations: A Taxonomy Guided Template-based NLG Method"></a>Towards Feasible Counterfactual Explanations: A Taxonomy Guided Template-based NLG Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02019">http://arxiv.org/abs/2310.02019</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pedramsalimi/nlgxai">https://github.com/pedramsalimi/nlgxai</a></li>
<li>paper_authors: Pedram Salimi, Nirmalie Wiratunga, David Corsar, Anjana Wijekoon</li>
<li>for: 本研究的目的是提出一种新的自然语言Counterfactual Explanation（Natural-XAI）方法，以便更好地解释模型决策过程中的必要变量更改。</li>
<li>methods: 本研究使用了一个用户研究，找到了人类编写的Counterfactual Explanation中的两类主题：内容相关的，关注从反事件和查询角度来包含特征和其值的方式；结构相关的，关注描述必要值更改的结构和术语。</li>
<li>results: 本研究提出了一个特征可行性税onomy，用于总结和简化Counterfactual Explanation的描述过程。使用这个税onomy和一些预先设计的模板，可以生成与现有的解释器（如DICE、NICE和DisCERN）兼容的自然语言生成结果，以提高Counterfactual Explanation的可读性和可行性。<details>
<summary>Abstract</summary>
Counterfactual Explanations (cf-XAI) describe the smallest changes in feature values necessary to change an outcome from one class to another. However, many cf-XAI methods neglect the feasibility of those changes. In this paper, we introduce a novel approach for presenting cf-XAI in natural language (Natural-XAI), giving careful consideration to actionable and comprehensible aspects while remaining cognizant of immutability and ethical concerns. We present three contributions to this endeavor. Firstly, through a user study, we identify two types of themes present in cf-XAI composed by humans: content-related, focusing on how features and their values are included from both the counterfactual and the query perspectives; and structure-related, focusing on the structure and terminology used for describing necessary value changes. Secondly, we introduce a feature actionability taxonomy with four clearly defined categories, to streamline the explanation presentation process. Using insights from the user study and our taxonomy, we created a generalisable template-based natural language generation (NLG) method compatible with existing explainers like DICE, NICE, and DisCERN, to produce counterfactuals that address the aforementioned limitations of existing approaches. Finally, we conducted a second user study to assess the performance of our taxonomy-guided NLG templates on three domains. Our findings show that the taxonomy-guided Natural-XAI approach (n-XAI^T) received higher user ratings across all dimensions, with significantly improved results in the majority of the domains assessed for articulation, acceptability, feasibility, and sensitivity dimensions.
</details>
<details>
<summary>摘要</summary>
counterfactual 解释 (cf-XAI) 描述最小改变Feature值能够改变结果从一个类别转移到另一个类别。然而，许多 cf-XAI 方法忽略了这些改变的可行性。在这篇论文中，我们介绍了一种新的方法，用于在自然语言 (Natural-XAI) 中提供 counterfactual 解释，同时考虑到可行性和可理解性的考虑因素，并保持决策和伦理问题的注意。我们在这篇论文中提供了三项贡献。首先，通过用户研究，我们发现了 counterfactual 解释中 humans 所创作的两种主题：内容相关，关注从 counterfactual 和查询角度来看 feature 和其值的包含方式；和结构相关，关注 counterfactual 解释中 feature 值改变所需的结构和术语使用方式。其次，我们引入了一个功能可行分类，用于总结 counterfactual 解释中 feature 值改变的可行性。使用用户研究和我们的分类，我们创建了一种可与现有的解释器 like DICE、NICE 和 DisCERN 兼容的通用 template-based自然语言生成 (NLG) 方法，以生成可以Addressing the limitations of existing approaches的 counterfactuals。最后，我们进行了第二次用户研究，以评估我们的分类导向 NLG 模板在三个领域的表现。我们的发现显示，与我们的分类导向 NLG 模板相比，传统的 counterfactual 解释方法在大多数领域都表现较差，特别是在某些领域的可行性、可理解性、可行性和敏感度方面表现较差。
</details></li>
</ul>
<hr>
<h2 id="Towards-Training-Without-Depth-Limits-Batch-Normalization-Without-Gradient-Explosion"><a href="#Towards-Training-Without-Depth-Limits-Batch-Normalization-Without-Gradient-Explosion" class="headerlink" title="Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion"></a>Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02012">http://arxiv.org/abs/2310.02012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexandrumeterez/bngrad">https://github.com/alexandrumeterez/bngrad</a></li>
<li>paper_authors: Alexandru Meterez, Amir Joudaki, Francesco Orabona, Alexander Immer, Gunnar Rätsch, Hadi Daneshmand</li>
<li>for: 这个论文的目的是提出一种拥有优化信号传递特性，但避免深度梯度爆炸的多层感知网络。</li>
<li>methods: 这个论文使用了批量Normalization层，并采用了Weingarten calculus来建立一种非对易的理论模型，以确定批量Normalization层在深度学习中的表现。</li>
<li>results: 论文的研究结果表明，通过特定的MLP结构和批量Normalization层的组合，可以实现保持优化信号传递特性，同时避免深度梯度爆炸的目标。此外，论文还提出了一种活动填充方案，可以在非线性激活函数下实现相似的性能。<details>
<summary>Abstract</summary>
Normalization layers are one of the key building blocks for deep neural networks. Several theoretical studies have shown that batch normalization improves the signal propagation, by avoiding the representations from becoming collinear across the layers. However, results on mean-field theory of batch normalization also conclude that this benefit comes at the expense of exploding gradients in depth. Motivated by these two aspects of batch normalization, in this study we pose the following question: "Can a batch-normalized network keep the optimal signal propagation properties, but avoid exploding gradients?" We answer this question in the affirmative by giving a particular construction of an Multi-Layer Perceptron (MLP) with linear activations and batch-normalization that provably has bounded gradients at any depth. Based on Weingarten calculus, we develop a rigorous and non-asymptotic theory for this constructed MLP that gives a precise characterization of forward signal propagation, while proving that gradients remain bounded for linearly independent input samples, which holds in most practical settings. Inspired by our theory, we also design an activation shaping scheme that empirically achieves the same properties for certain non-linear activations.
</details>
<details>
<summary>摘要</summary>
归并层是深度神经网络的关键组件之一。许多理论研究表明，批处Normalization可以改善信号传递，避免层之间的表示变得相互平行。然而，基于mean-field theory的研究也表明，这些优点是随着深度层数的增加而导致梯度爆炸的代价。为了解决这个问题，我们提出以下问题：“是否可以在批处Normalization的情况下保持最佳的信号传递特性，而免除深度层数随着增加而导致梯度爆炸？”我们回答这个问题的答案是肯定的，并给出了一种特殊的多层感知机（MLP），其中每层使用线性活动函数和批处Normalization，可以证明在任意深度下都有稳定梯度。基于Weingarten calculus，我们开发了一种精确和非对数学的理论，可以准确地描述这种构造的前向信号传递特性，同时证明在线性独立输入样本上，梯度都具有有界值。受到我们的理论启发，我们还设计了一种活动形态的调整方案，可以实际实现相同的特性 для某些非线性活动函数。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Convergence-Analysis-of-Tsetlin-Machines-A-Probabilistic-Approach-to-Concept-Learning"><a href="#Generalized-Convergence-Analysis-of-Tsetlin-Machines-A-Probabilistic-Approach-to-Concept-Learning" class="headerlink" title="Generalized Convergence Analysis of Tsetlin Machines: A Probabilistic Approach to Concept Learning"></a>Generalized Convergence Analysis of Tsetlin Machines: A Probabilistic Approach to Concept Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02005">http://arxiv.org/abs/2310.02005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed-Bachir Belaid, Jivitesh Sharma, Lei Jiao, Ole-Christoffer Granmo, Per-Arne Andersen, Anis Yazidi</li>
<li>for: 这篇论文的目的是为了解释Tsetlin机器（TM）在机器学习领域的应用中的性能，以及TM的整体吞吐量和可靠性。</li>
<li>methods: 这篇论文使用了Tsetlin自动机基于的机器学习算法，并提出了一种新的框架——概率概念学习（PCL），以解决TM在扩展的情况下的收敛问题。</li>
<li>results: 研究发现，PCL在$n$个特征下可以学习一组连接规则$C_i$，每个规则都有一个特定的包含概率$p_i$。此外，研究还证明了，对于任何规则$C_k$，PCL都可以收敛到一个连接规则。这一结论不仅有助于理解TM的性能，还有可能导致更加稳定和可解释的机器学习模型。<details>
<summary>Abstract</summary>
Tsetlin Machines (TMs) have garnered increasing interest for their ability to learn concepts via propositional formulas and their proven efficiency across various application domains. Despite this, the convergence proof for the TMs, particularly for the AND operator (\emph{conjunction} of literals), in the generalized case (inputs greater than two bits) remains an open problem. This paper aims to fill this gap by presenting a comprehensive convergence analysis of Tsetlin automaton-based Machine Learning algorithms. We introduce a novel framework, referred to as Probabilistic Concept Learning (PCL), which simplifies the TM structure while incorporating dedicated feedback mechanisms and dedicated inclusion/exclusion probabilities for literals. Given $n$ features, PCL aims to learn a set of conjunction clauses $C_i$ each associated with a distinct inclusion probability $p_i$. Most importantly, we establish a theoretical proof confirming that, for any clause $C_k$, PCL converges to a conjunction of literals when $0.5<p_k<1$. This result serves as a stepping stone for future research on the convergence properties of Tsetlin automaton-based learning algorithms. Our findings not only contribute to the theoretical understanding of Tsetlin Machines but also have implications for their practical application, potentially leading to more robust and interpretable machine learning models.
</details>
<details>
<summary>摘要</summary>
特具谱机器（TM）在不同应用领域中已经吸引了越来越多的关注，主要是因为它们可以通过命题式来学习概念。然而，TM的整合证明，特别是在输入大于两位的情况下，仍然是一个打开的问题。这篇论文的目的是填补这个空白，通过对特具谱自动机基于机器学习算法的总结分析来证明TM的可靠性。我们提出了一种新的框架，称为概率概念学习（PCL），该框架简化了TM结构，并添加了专门的反馈机制和专门的包含/排除概率 для Literal。在n个特征下，PCL的目标是学习一组连接规则 $C_i$，每个规则都有自己的包含概率 $p_i$。最重要的是，我们证明了，对于任何规则 $C_k$，PCL在 $0.5<p_k<1$ 时 converge to a conjunction of literals。这个结果可以作为未来研究特具谱机器学习算法的可靠性的基础。我们的发现不仅有助于理解特具谱机器的理论基础，还有可能导致更加Robust和可读取的机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Fill-in-the-Blank-Exploring-and-Enhancing-LLM-Capabilities-for-Backward-Reasoning-in-Math-Word-Problems"><a href="#Fill-in-the-Blank-Exploring-and-Enhancing-LLM-Capabilities-for-Backward-Reasoning-in-Math-Word-Problems" class="headerlink" title="Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems"></a>Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward Reasoning in Math Word Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01991">http://arxiv.org/abs/2310.01991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniruddha Deb, Neeva Oza, Sarthak Singla, Dinesh Khandelwal, Dinesh Garg, Parag Singla</li>
<li>for: 这篇论文探讨了大语言模型（LLM）在数学问题上的后向理解能力，即给定一个数学问题和其解答，能否由LLM回归出 omitted 信息？</li>
<li>methods: 本文首先定义了数学问题上的后向理解任务，并对 GSM8k、SVAMP 和 MultiArith 三个数据集进行修改以进行评估。然后，提出了三种新的技巧来提高 LLM 的表现：Rephrase、PAL-Tools 和 Check your Work。</li>
<li>results: 实验结果表明，使用这些技巧可以成功地提高 LLM 在后向理解任务中的表现。最终，提出了一种 Bayesian 形式的 ensemble 方法，通过与一个高精度的自然验证器相结合，进一步提高 LLM 的表现。<details>
<summary>Abstract</summary>
While forward reasoning (i.e. find the answer given the question) has been explored extensively in the recent literature, backward reasoning is relatively unexplored. We examine the backward reasoning capabilities of LLMs on Math Word Problems (MWPs): given a mathematical question and its answer, with some details omitted from the question, can LLMs effectively retrieve the missing information?   In this paper, we formally define the backward reasoning task on math word problems and modify three datasets to evaluate this task: GSM8k, SVAMP and MultiArith. Our findings show a significant drop in the accuracy of models on backward reasoning compared to forward reasoning across four SOTA LLMs (GPT4, GPT3.5, PaLM-2, and LLaMa-2). Utilizing the specific format of this task, we propose three novel techniques that improve performance: Rephrase reformulates the given problem into a forward reasoning problem, PAL-Tools combines the idea of Program-Aided LLMs to produce a set of equations that can be solved by an external solver, and Check your Work exploits the availability of natural verifier of high accuracy in the forward direction, interleaving solving and verification steps. Finally, realizing that each of our base methods correctly solves a different set of problems, we propose a novel Bayesian formulation for creating an ensemble over these base methods aided by a verifier to further boost the accuracy by a significant margin. Extensive experimentation demonstrates that our techniques successively improve the performance of LLMs on the backward reasoning task, with the final ensemble-based method resulting in a substantial performance gain compared to the raw LLMs with standard prompting techniques such as chain-of-thought.
</details>
<details>
<summary>摘要</summary>
而forward reasoning（即根据问题找到答案）在近期文献中已经得到了广泛的探讨，而backward reasoning则相对较少研究。我们将专注于LLMs在数学语句问题（MWPs）上的backward reasoning能力：对于一个数学问题和其答案，当某些问题详细信息被删除时，LLMs是否能够有效地撷取缺失的信息？在这篇论文中，我们正式定义了MWPs上的backward reasoning任务，并对GSM8k、SVAMP和MultiArith三个数据集进行修改以进行评估。我们发现了四种SOTA LLMs（GPT4、GPT3.5、PaLM-2和LLaMa-2）在backward reasoning任务上的精度明显下降。我们运用这个任务的特定格式，提出了三种新的技术来改善性能：Rephrase将问题重新推理成前向推理问题，PAL-Tools结合了程式帮助LLMs生成可以由外部解uder解的方程集，Check your Work利用了前向方向的自然验证者高精度，将解ving和验证步骤进行交互式推理。最后，我们提出了一个组合这些基本方法的ensemble方法，通过与验证器一起使用 Bayesian 形式来增加精度。实验结果显示，我们的技术顺利地提高了LLMs在backward reasoning任务上的性能， ensemble-based 方法在与标准提示技术相比，实现了重要的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Soda-An-Object-Oriented-Functional-Language-for-Specifying-Human-Centered-Problems"><a href="#Soda-An-Object-Oriented-Functional-Language-for-Specifying-Human-Centered-Problems" class="headerlink" title="Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems"></a>Soda: An Object-Oriented Functional Language for Specifying Human-Centered Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01961">http://arxiv.org/abs/2310.01961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Alfredo Mendez</li>
<li>for: 本文旨在提供一种自然地处理质量和量的语言，以便更好地检查其正确性。</li>
<li>methods: 本文使用符号目标描述分析（Soda）语言，该语言可以帮助描述复杂的计算机系统需求，并且提供了适当的键性特性来支持这些需求的模型。</li>
<li>results: 本文提供了一种轻松描述问题的工具，可以更加透明地描述复杂的需求，从而减少错误的可能性。<details>
<summary>Abstract</summary>
We present Soda (Symbolic Objective Descriptive Analysis), a language that helps to treat qualities and quantities in a natural way and greatly simplifies the task of checking their correctness. We present key properties for the language motivated by the design of a descriptive language to encode complex requirements on computer systems, and we explain how these key properties must be addressed to model these requirements with simple definitions. We give an overview of a tool that helps to describe problems in an easy way that we consider more transparent and less error-prone.
</details>
<details>
<summary>摘要</summary>
我们介绍Soda（Symbolic Objective Descriptive Analysis）语言，它帮助处理质量和量的自然方式，并大大简化了检查正确性的任务。我们介绍了语言的关键属性，这些属性由计算机系统的描述语言的设计启发而来，并解释了如何使用简单的定义来模拟这些要求。我们给出了一个工具，它使得描述问题变得更加 transparent 和 less error-prone。Note: "Simplified Chinese" is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Language-Models-as-Knowledge-Bases-for-Visual-Word-Sense-Disambiguation"><a href="#Language-Models-as-Knowledge-Bases-for-Visual-Word-Sense-Disambiguation" class="headerlink" title="Language Models as Knowledge Bases for Visual Word Sense Disambiguation"></a>Language Models as Knowledge Bases for Visual Word Sense Disambiguation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01960">http://arxiv.org/abs/2310.01960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anastasiakrith/llm-for-vwsd">https://github.com/anastasiakrith/llm-for-vwsd</a></li>
<li>paper_authors: Anastasia Kritharoula, Maria Lymperaiou, Giorgos Stamou</li>
<li>for: 本研究的目的是提高视听语言变换器（VL transformer）的检索性能，通过使用大语言模型（LLM）作为知识库。</li>
<li>methods: 本研究使用了知识库中的知识，通过适当的提示来检索知识，以及将视听语言变换问题转化为文本问题，并使用链条思维（CoT）提示来探究内部的思维过程。</li>
<li>results: 本研究表明，通过使用LLM作为知识库，可以提高VL transformer的检索性能，并且通过转化为文本问题，可以更好地探究内部的思维过程。<details>
<summary>Abstract</summary>
Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prompting in the zero-shot setting is able to reveal the internal reasoning steps an LLM follows to select the appropriate candidate. In total, our presented approach is the first one to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD.
</details>
<details>
<summary>摘要</summary>
Visual Word Sense Disambiguation (VWSD) 是一个新兴的挑战任务，位于语言意义归一化和细部多媒体搜寻之间。 recent advancements in visiolinguistic (VL) transformers 的开发，提供了一些可用的 implementation with encouraging results，但我们认为这些结果可以进一步改善。 To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prompting in the zero-shot setting is able to reveal the internal reasoning steps an LLM follows to select the appropriate candidate. In total, our presented approach is the first one to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD.
</details></li>
</ul>
<hr>
<h2 id="Driving-with-LLMs-Fusing-Object-Level-Vector-Modality-for-Explainable-Autonomous-Driving"><a href="#Driving-with-LLMs-Fusing-Object-Level-Vector-Modality-for-Explainable-Autonomous-Driving" class="headerlink" title="Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"></a>Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01957">http://arxiv.org/abs/2310.01957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wayveai/driving-with-llms">https://github.com/wayveai/driving-with-llms</a></li>
<li>paper_authors: Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, Jamie Shotton</li>
<li>for: 本研究旨在提高自动驾驶中Context理解，特别是通过大语言模型（LLM）的普适性和可解释性。</li>
<li>methods: 我们提出了一种unicode object-level multimodal LLM架构，将vectorized numeric modalities与预训练LLM结合，以提高驾驶场景中Context的理解。我们还开发了10000个驾驶场景的160000个问答对，并使用RL代理和教师LLM（GPT-3.5）生成问题和答案。为了将数字vec模态与静态LLM表示相alin，我们提出了一种新的预训练策略。</li>
<li>results: 我们的研究发现，使用LLM-driver可以在驾驶场景中更好地理解 Context，回答问题，并做出决策。与传统行为宠模型相比，LLM-based driving action generation表现出了更高的普适性和可解释性。我们的研究结果和数据集、模型将被提供给进一步的探索。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown promise in the autonomous driving sector, particularly in generalization and interpretability. We introduce a unique object-level multimodal LLM architecture that merges vectorized numeric modalities with a pre-trained LLM to improve context understanding in driving situations. We also present a new dataset of 160k QA pairs derived from 10k driving scenarios, paired with high quality control commands collected with RL agent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data. We also introduce an evaluation metric for Driving QA and demonstrate our LLM-driver's proficiency in interpreting driving scenarios, answering questions, and decision-making. Our findings highlight the potential of LLM-based driving action generation in comparison to traditional behavioral cloning. We make our benchmark, datasets, and model available for further exploration.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Probabilistic-Reach-Avoid-for-Bayesian-Neural-Networks"><a href="#Probabilistic-Reach-Avoid-for-Bayesian-Neural-Networks" class="headerlink" title="Probabilistic Reach-Avoid for Bayesian Neural Networks"></a>Probabilistic Reach-Avoid for Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01951">http://arxiv.org/abs/2310.01951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/matthewwicker/bnnreachavoid">https://github.com/matthewwicker/bnnreachavoid</a></li>
<li>paper_authors: Matthew Wicker, Luca Laurenti, Andrea Patane, Nicola Paoletti, Alessandro Abate, Marta Kwiatkowska</li>
<li>for: 本研究旨在同时学习未知的随机环境动力学和生成优化策略，并确保策略在安全关键场景中的决策是安全和可靠的。</li>
<li>methods: 本研究使用interval propagation和backward recursion技术计算动力学模型下的下界，以确保策略满足给定的 reach-avoid 规范（达到目标状态，避免危险状态）。然后，使用控制合成算法derive最优的策略，以提高安全性的可靠性。</li>
<li>results: 在一系列控制准则中，我们的方法能够提供更多的 certificatable 状态和更高的平均保证的 reach-avoid 概率，比较传统的数据驱动策略。在最具挑战性的准则中，我们的优化算法能够提供更多的 certificatable 状态和更高的平均保证的 reach-avoid 概率，比较传统的数据驱动策略。<details>
<summary>Abstract</summary>
Model-based reinforcement learning seeks to simultaneously learn the dynamics of an unknown stochastic environment and synthesise an optimal policy for acting in it. Ensuring the safety and robustness of sequential decisions made through a policy in such an environment is a key challenge for policies intended for safety-critical scenarios. In this work, we investigate two complementary problems: first, computing reach-avoid probabilities for iterative predictions made with dynamical models, with dynamics described by Bayesian neural network (BNN); second, synthesising control policies that are optimal with respect to a given reach-avoid specification (reaching a "target" state, while avoiding a set of "unsafe" states) and a learned BNN model. Our solution leverages interval propagation and backward recursion techniques to compute lower bounds for the probability that a policy's sequence of actions leads to satisfying the reach-avoid specification. Such computed lower bounds provide safety certification for the given policy and BNN model. We then introduce control synthesis algorithms to derive policies maximizing said lower bounds on the safety probability. We demonstrate the effectiveness of our method on a series of control benchmarks characterized by learned BNN dynamics models. On our most challenging benchmark, compared to purely data-driven policies the optimal synthesis algorithm is able to provide more than a four-fold increase in the number of certifiable states and more than a three-fold increase in the average guaranteed reach-avoid probability.
</details>
<details>
<summary>摘要</summary>
In this work, we address two related problems: computing reach-avoid probabilities for iterative predictions made with dynamical models, and synthesizing control policies that are optimal with respect to a given reach-avoid specification and a learned Bayesian neural network (BNN) model. Our approach leverages interval propagation and backward recursion techniques to compute lower bounds for the probability that a policy's sequence of actions leads to satisfying the reach-avoid specification. These lower bounds provide safety certification for the given policy and BNN model.We then introduce control synthesis algorithms to derive policies that maximize the computed lower bounds on the safety probability. Our method is demonstrated on a series of control benchmarks characterized by learned BNN dynamics models. On our most challenging benchmark, our optimal synthesis algorithm is able to provide more than a four-fold increase in the number of certifiable states and more than a three-fold increase in the average guaranteed reach-avoid probability compared to purely data-driven policies.
</details></li>
</ul>
<hr>
<h2 id="Ravestate-Distributed-Composition-of-a-Causal-Specificity-Guided-Interaction-Policy"><a href="#Ravestate-Distributed-Composition-of-a-Causal-Specificity-Guided-Interaction-Policy" class="headerlink" title="Ravestate: Distributed Composition of a Causal-Specificity-Guided Interaction Policy"></a>Ravestate: Distributed Composition of a Causal-Specificity-Guided Interaction Policy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01943">http://arxiv.org/abs/2310.01943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Birkner, Andreas Dolp, Negin Karimi, Nikita Basargin, Alona Kharchenko, Rafael Hostettler</li>
<li>for: 这 paper 的目的是提出一种基于规则的人机交互策略设计方法，这种方法是有效、可解释、表达力强和直观的。</li>
<li>methods: 这 paper 使用了 Signal-Rule-Slot 框架，该框架是根据先前的 Symbolic System 设计方法进行改进，并引入了一种新的 Bayesian 思想的交互规则实用性指标called Causal Pathway Self-information。</li>
<li>results: 通过用 Ravestate 开源实现和进行用户研究，这 paper 提供了一种有力的人机交互系统，并在文本、语音和视觉等场景中展示了 Contextual Behavior 的robust性。<details>
<summary>Abstract</summary>
In human-robot interaction policy design, a rule-based method is efficient, explainable, expressive and intuitive. In this paper, we present the Signal-Rule-Slot framework, which refines prior work on rule-based symbol system design and introduces a new, Bayesian notion of interaction rule utility called Causal Pathway Self-information. We offer a rigorous theoretical foundation as well as a rich open-source reference implementation Ravestate, with which we conduct user studies in text-, speech-, and vision-based scenarios. The experiments show robust contextual behaviour of our probabilistically informed rule-based system, paving the way for more effective human-machine interaction.
</details>
<details>
<summary>摘要</summary>
人机交互策略设计中，使用规则方法是有效、可解释、表达力强和直观的。本文提出了信号规则槽框架，对先前的规则基于符号系统设计做出了改进，并引入了一新的 bayesian 概念：交互规则用量含义。我们提供了坚实的理论基础以及rich的开源参考实现 Ravestate，并在文本、语音和视觉等方面进行了用户研究。实验结果表明了我们的概率知识基于规则系统在不同场景中具有强大的上下文行为，这将为人机交互带来更高效的交互。Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you have any specific requirements or preferences, please let me know and I can provide a more tailored translation.
</details></li>
</ul>
<hr>
<h2 id="Navigating-Cultural-Chasms-Exploring-and-Unlocking-the-Cultural-POV-of-Text-To-Image-Models"><a href="#Navigating-Cultural-Chasms-Exploring-and-Unlocking-the-Cultural-POV-of-Text-To-Image-Models" class="headerlink" title="Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models"></a>Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01929">http://arxiv.org/abs/2310.01929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mor Ventura, Eyal Ben-David, Anna Korhonen, Roi Reichart</li>
<li>for: 这个研究旨在探讨TEXT-TO-IMAGE（TTI）模型中嵌入的文化认知，以及这些模型在不同文化背景下的表现。</li>
<li>methods: 该研究使用了多种评估技术，包括CLIP空间的内在评估、VQA模型的外在评估以及人类评估，以探索TTI模型的文化认知。</li>
<li>results: 实验结果显示，TTI模型在不同文化背景下具有文化认知，并且可以适应不同文化的特点。这些模型还能够解释文化特点，并且可以在不同文化背景下提高表现。<details>
<summary>Abstract</summary>
Text-To-Image (TTI) models, exemplified by DALL-E and StableDiffusion, have recently gained prominence for their remarkable zero-shot capabilities in generating images guided by textual prompts. Language, as a conduit of culture, plays a pivotal role in these models' multilingual capabilities, which in turn shape their cultural agency. In this study, we explore the cultural perception embedded in TTI models by characterizing culture across three hierarchical tiers: cultural dimensions, cultural domains, and cultural concepts. We propose a comprehensive suite of evaluation techniques, including intrinsic evaluations using the CLIP space, extrinsic evaluations with a Visual-Question-Answer (VQA) model, and human assessments, to discern TTI cultural perceptions. To facilitate our research, we introduce the CulText2I dataset, derived from four diverse TTI models and spanning ten languages. Our experiments reveal insights into these models' cultural awareness, cultural distinctions, and the unlocking of cultural features, releasing the potential for cross-cultural applications.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DARTH-Holistic-Test-time-Adaptation-for-Multiple-Object-Tracking"><a href="#DARTH-Holistic-Test-time-Adaptation-for-Multiple-Object-Tracking" class="headerlink" title="DARTH: Holistic Test-time Adaptation for Multiple Object Tracking"></a>DARTH: Holistic Test-time Adaptation for Multiple Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01926">http://arxiv.org/abs/2310.01926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mattiasegu/darth">https://github.com/mattiasegu/darth</a></li>
<li>paper_authors: Mattia Segu, Bernt Schiele, Fisher Yu</li>
<li>for: 这篇论文主要旨在提出一种在测试时进行多目标跟踪（MOT）系统的适应性问题，以提高自动驾驶系统的安全性。</li>
<li>methods: 该论文提出了一种涵盖 object detection 和 instance association 的全面测试时适应框架，包括一种自动化的检测一致问题的解决方案以及一种新的补充彩色损失来适应实例外观表示。</li>
<li>results: 该论文在不同的领域变换（sim-to-real、outdoor-to-indoor、indoor-to-outdoor）中进行了广泛的测试，并得到了明显的性能提升。<details>
<summary>Abstract</summary>
Multiple object tracking (MOT) is a fundamental component of perception systems for autonomous driving, and its robustness to unseen conditions is a requirement to avoid life-critical failures. Despite the urge of safety in driving systems, no solution to the MOT adaptation problem to domain shift in test-time conditions has ever been proposed. However, the nature of a MOT system is manifold - requiring object detection and instance association - and adapting all its components is non-trivial. In this paper, we analyze the effect of domain shift on appearance-based trackers, and introduce DARTH, a holistic test-time adaptation framework for MOT. We propose a detection consistency formulation to adapt object detection in a self-supervised fashion, while adapting the instance appearance representations via our novel patch contrastive loss. We evaluate our method on a variety of domain shifts - including sim-to-real, outdoor-to-indoor, indoor-to-outdoor - and substantially improve the source model performance on all metrics. Code: https://github.com/mattiasegu/darth.
</details>
<details>
<summary>摘要</summary>
多bject tracking (MOT) 是自动驾驶系统的基本组件，其robustness to 未看过的条件是必要的，以避免生命 crítical 错误。尽管安全驾驶系统的需求很强，但是没有任何一个解决方案可以在测试时间下进行 MOT 适应域转换问题。然而，MOT 系统的性质是多元的 - 需要对象检测和实例关联 - 并且全部组件的适应是非常困难。在这篇论文中，我们分析了域转换对 appearance-based 跟踪器的影响，并提出了 DARTH，一个整体测试时间适应框架 для MOT。我们提议使用自我supervised 的检测一致性 формулы来适应对象检测，而对实例的外观表示进行适应via我们的新型 patch contrastive loss。我们对各种域转换进行了评估 - 包括 sim-to-real、outdoor-to-indoor 和 indoor-to-outdoor - 并在所有指标上显著提高了源模型的性能。代码：https://github.com/mattiasegu/darth。
</details></li>
</ul>
<hr>
<h2 id="FiGURe-Simple-and-Efficient-Unsupervised-Node-Representations-with-Filter-Augmentations"><a href="#FiGURe-Simple-and-Efficient-Unsupervised-Node-Representations-with-Filter-Augmentations" class="headerlink" title="FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations"></a>FiGURe: Simple and Efficient Unsupervised Node Representations with Filter Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01892">http://arxiv.org/abs/2310.01892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/figure">https://github.com/microsoft/figure</a></li>
<li>paper_authors: Chanakya Ekbote, Ajinkya Pankaj Deshpande, Arun Iyer, Ramakrishna Bairi, Sundararajan Sellamanickam</li>
<li>for: This paper aims to improve the performance of unsupervised node representations learnt using contrastive learning-based methods on downstream tasks.</li>
<li>methods: The authors propose a simple filter-based augmentation method to capture different parts of the eigen-spectrum, which leads to significant improvements. They also share the same weights across different filter augmentations to reduce computational load.</li>
<li>results: The proposed method, FiGURe, achieves an average gain of up to 4.4% compared to the state-of-the-art unsupervised models across all datasets considered, both homophilic and heterophilic.Here’s the summary in Simplified Chinese:</li>
<li>for: 本文目的是提高基于对比学习的无监督节点表示的性能在下游任务中。</li>
<li>methods: 作者提出了一种简单的滤波器基于扩展方法，以捕捉不同的特征谱部分，并且通过共享相同权重来降低计算负担。</li>
<li>results: 提出的方法FiGURe，在所有考虑的数据集上，both homophilic和heterophilic，实现了4.4%的平均提升，比领先的无监督模型更高。<details>
<summary>Abstract</summary>
Unsupervised node representations learnt using contrastive learning-based methods have shown good performance on downstream tasks. However, these methods rely on augmentations that mimic low-pass filters, limiting their performance on tasks requiring different eigen-spectrum parts. This paper presents a simple filter-based augmentation method to capture different parts of the eigen-spectrum. We show significant improvements using these augmentations. Further, we show that sharing the same weights across these different filter augmentations is possible, reducing the computational load. In addition, previous works have shown that good performance on downstream tasks requires high dimensional representations. Working with high dimensions increases the computations, especially when multiple augmentations are involved. We mitigate this problem and recover good performance through lower dimensional embeddings using simple random Fourier feature projections. Our method, FiGURe achieves an average gain of up to 4.4%, compared to the state-of-the-art unsupervised models, across all datasets in consideration, both homophilic and heterophilic. Our code can be found at: https://github.com/microsoft/figure.
</details>
<details>
<summary>摘要</summary>
自助学习方法学习的无监督节点表示已经达到了下游任务的好表现。然而，这些方法通常依赖于模拟低通滤波器的扩充，这限制了它们在不同谱分部分上的表现。这篇论文提出了一种简单的滤波器扩充方法，以捕捉不同的谱分部分。我们表明了这些扩充的显著提高。此外，之前的工作表明，downstream任务需要高维表示。工作于高维度会增加计算量，特别是当多个扩充 involve。我们解决这个问题并重新获得了好表现通过简单的随机傅立干投影。我们的方法FiGURe在考虑所有数据集上达到了4.4%的平均提升，相比之前的状态对照模型。我们的代码可以在 GitHub上找到：https://github.com/microsoft/figure。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Hybrid-Model-for-Enhanced-Stock-Market-Predictions-Using-Improved-VMD-and-Stacked-Informer"><a href="#Adaptive-Hybrid-Model-for-Enhanced-Stock-Market-Predictions-Using-Improved-VMD-and-Stacked-Informer" class="headerlink" title="Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer"></a>Adaptive Hybrid Model for Enhanced Stock Market Predictions Using Improved VMD and Stacked Informer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01884">http://arxiv.org/abs/2310.01884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DANNHIROAKI/Adaptive-Hybrid-Model-for-Enhanced-Stock-Market-Predictions-Using-Improved-VMD-and-Stacked-Informer">https://github.com/DANNHIROAKI/Adaptive-Hybrid-Model-for-Enhanced-Stock-Market-Predictions-Using-Improved-VMD-and-Stacked-Informer</a></li>
<li>paper_authors: Jianan Zhang, Hongyi Duan</li>
<li>for: 该研究旨在提出一种适应性混合模型，用于股票市场预测，利用提高后 Variational Mode Decomposition (VMD)、Feature Engineering (FE) 和堆叠 Informer 以及适应损失函数。</li>
<li>methods: 该模型使用了增强的 VMD、FE 和堆叠 Informer，并将其与适应损失函数结合使用。</li>
<li>results: 实验结果表明，提出的模型（称为 Adam+GC+增强 informer，简称 VMGCformer）在股票市场数据中表现出色，其预测精度、应急性和泛化能力都高于传统和其他混合模型。<details>
<summary>Abstract</summary>
This paper introduces an innovative adaptive hybrid model for stock market predictions, leveraging the capabilities of an enhanced Variational Mode Decomposition (VMD), Feature Engineering (FE), and stacked Informer integrated with an adaptive loss function. Through rigorous experimentation, the proposed model, termed Adam+GC+enhanced informer (We name it VMGCformer), demonstrates significant proficiency in addressing the intricate dynamics and volatile nature of stock market data. Experimental results, derived from multiple benchmark datasets, underscore the model's superiority in terms of prediction accuracy, responsiveness, and generalization capabilities over traditional and other hybrid models. The research further highlights potential avenues for optimization and introduces future directions to enhance predictive modeling, especially for small enterprises and feature engineering.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种新型的股票市场预测模型， combining 变分幂分析（VMD）、特征工程（FE）和堆叠 Informer 以及适应损失函数。该模型被称为 VMGCformer，经过了多种数据集的严格实验，对股票市场数据的复杂动态和不稳定性表现出了明显的优势。实验结果显示，VMGCformer 在预测精度、应急性和泛化能力方面比传统和其他混合模型表现出了明显的提高。研究还提出了优化方向和未来研究的方向，尤其是对小企业和特征工程。
</details></li>
</ul>
<hr>
<h2 id="Towards-Stable-Backdoor-Purification-through-Feature-Shift-Tuning"><a href="#Towards-Stable-Backdoor-Purification-through-Feature-Shift-Tuning" class="headerlink" title="Towards Stable Backdoor Purification through Feature Shift Tuning"></a>Towards Stable Backdoor Purification through Feature Shift Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01875">http://arxiv.org/abs/2310.01875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aisafety-hkust/stable_backdoor_purification">https://github.com/aisafety-hkust/stable_backdoor_purification</a></li>
<li>paper_authors: Rui Min, Zeyu Qin, Li Shen, Minhao Cheng</li>
<li>for: 本研究旨在提出一种简单易于实施的后门攻击防御方法，以帮助减少深度神经网络（DNN）受到后门攻击的风险。</li>
<li>methods: 我们使用了精心调整（fine-tuning）方法，并进行了广泛的测试和分析，以推断 vanilla 调整方法在低毒料比例下完全失效。我们还提出了一种叫做特征偏移调整（Feature Shift Tuning，FST）方法，以解决低毒料比例下后门纯化的问题。FST 通过强制抬升分类器的权重偏移自已损害的权重，以提高后门纯化的稳定性。</li>
<li>results: 我们的实验结果表明，FST 方法在不同的攻击场景下具有稳定的性能，并且只需要10个训练 epoch，可以很快地完成纯化过程。此外，FST 方法还可以在低毒料比例下提供更好的防御性能，而不需要复杂的参数调整。<details>
<summary>Abstract</summary>
It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based defenses. Therefore, it is necessary to disentangle the backdoor and clean features in order to improve backdoor purification. To address this, we introduce Feature Shift Tuning (FST), a method for tuning-based backdoor purification. Specifically, FST encourages feature shifts by actively deviating the classifier weights from the originally compromised weights. Extensive experiments demonstrate that our FST provides consistently stable performance under different attack settings. Without complex parameter adjustments, FST also achieves much lower tuning costs, only 10 epochs. Our codes are available at https://github.com/AISafety-HKUST/stable_backdoor_purification.
</details>
<details>
<summary>摘要</summary>
历史观察表明深度神经网络（DNN）容易受到后门攻击，攻击者可以通过修改一小部分训练样本来 manipulate DNN 的行为。虽然一些防御方法被提出，但它们 either require 复杂的训练过程修改或者 heavily rely on 特定模型架构，这使得它们在实际应用中困难实施。因此，在这篇论文中，我们选择通过 fine-tuning，一种最常见并容易实施的后门防御方法，进行广泛的评估。初始实验结果表明，对高毒量情况下， vanilla tuning 方法具有扎实的防御效果。然而，在低毒量情况下，标准的 tuning 方法完全失败。我们的分析表明，在低毒量情况下，后门和干净特征之间的束缚，使得 tuning-based 防御无效。因此，我们需要分离后门和干净特征，以提高后门纯化。为此，我们介绍了 Feature Shift Tuning（FST），一种基于 tuning 的后门纯化方法。具体来说，FST 通过活动偏移分类器权重，以避免由恶意攻击所损害的原始权重。广泛的实验表明，我们的 FST 在不同的攻击设置下具有稳定的性能。没有复杂的参数调整，FST 只需要 10 轮训练，可以快速实现纯化。我们的代码可以在 https://github.com/AISafety-HKUST/stable_backdoor_purification 上获取。
</details></li>
</ul>
<hr>
<h2 id="Conditional-Instrumental-Variable-Regression-with-Representation-Learning-for-Causal-Inference"><a href="#Conditional-Instrumental-Variable-Regression-with-Representation-Learning-for-Causal-Inference" class="headerlink" title="Conditional Instrumental Variable Regression with Representation Learning for Causal Inference"></a>Conditional Instrumental Variable Regression with Representation Learning for Causal Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01865">http://arxiv.org/abs/2310.01865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debo Cheng, Ziqi Xu, Jiuyong Li, Lin Liu, Jixue Liu, Thuc Duy Le</li>
<li>for: 这 paper 研究了从观察数据中估计 causal effect 的困难问题，在存在隐藏的假设变量的情况下。</li>
<li>methods: 这 paper 使用了 two-stage least square (TSLS) 方法和其变种，以及标准的 instruemental variable (IV)，来消除假设变量的偏见，包括隐藏的假设变量所导致的偏见。但是，这些方法 rely 于线性假设。此外，标准 IV 方法中对 instruemental variable 的约束条件太 strict，不实际。因此，在这 paper 中，我们使用 conditional IV (CIV) 来放宽标准 IV 中的 instruemental variable 约束条件，并提出一种非线性 CIV 回归，即 Confounding Balancing Representation Learning, CBRL.CIV，用于同时消除隐藏的假设变量所导致的偏见，并均衡观察到的假设变量。我们 theoretically 验证了 CBRL.CIV 的正确性。</li>
<li>results: 在 synthetic 和两个实际数据集上，我们进行了广泛的实验，发现 CBRL.CIV 在对 state-of-the-art IV-based estimator 进行比较时，表现竞争性，并且在非线性情况下表现更优。<details>
<summary>Abstract</summary>
This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square (TSLS) method and its variants with a standard instrumental variable (IV) are commonly used to eliminate confounding bias, including the bias caused by unobserved confounders, but they rely on the linearity assumption. Besides, the strict condition of unconfounded instruments posed on a standard IV is too strong to be practical. To address these challenging and practical problems of the standard IV method (linearity assumption and the strict condition), in this paper, we use a conditional IV (CIV) to relax the unconfounded instrument condition of standard IV and propose a non-linear CIV regression with Confounding Balancing Representation Learning, CBRL.CIV, for jointly eliminating the confounding bias from unobserved confounders and balancing the observed confounders, without the linearity assumption. We theoretically demonstrate the soundness of CBRL.CIV. Extensive experiments on synthetic and two real-world datasets show the competitive performance of CBRL.CIV against state-of-the-art IV-based estimators and superiority in dealing with the non-linear situation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fine-tuned-vs-Prompt-tuned-Supervised-Representations-Which-Better-Account-for-Brain-Language-Representations"><a href="#Fine-tuned-vs-Prompt-tuned-Supervised-Representations-Which-Better-Account-for-Brain-Language-Representations" class="headerlink" title="Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?"></a>Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01854">http://arxiv.org/abs/2310.01854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyuan Sun, Marie-Francine Moens</li>
<li>for:  investigate the effectiveness of prompt-tuning compared to fine-tuning in generating representations that better account for the brain’s language representations</li>
<li>methods:  using neural decoding to compare the performance of prompt-tuned and fine-tuned representations in predicting linguistic stimuli from brain activities</li>
<li>results:  full fine-tuning does not significantly outperform prompt-tuning in neural decoding, and tasks dealing with fine-grained concept meaning yield representations that better decode brain activation patterns than other tasks.<details>
<summary>Abstract</summary>
To decipher the algorithm underlying the human brain's language representation, previous work probed brain responses to language input with pre-trained artificial neural network (ANN) models fine-tuned on NLU tasks. However, full fine-tuning generally updates the entire parametric space and distorts pre-trained features, cognitively inconsistent with the brain's robust multi-task learning ability. Prompt-tuning, in contrast, protects pre-trained weights and learns task-specific embeddings to fit a task. Could prompt-tuning generate representations that better account for the brain's language representations than fine-tuning? If so, what kind of NLU task leads a pre-trained model to better decode the information represented in the human brain? We investigate these questions by comparing prompt-tuned and fine-tuned representations in neural decoding, that is predicting the linguistic stimulus from the brain activities evoked by the stimulus. We find that on none of the 10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning in neural decoding, implicating that a more brain-consistent tuning method yields representations that better correlate with brain data. Moreover, we identify that tasks dealing with fine-grained concept meaning yield representations that better decode brain activation patterns than other tasks, especially the syntactic chunking task. This indicates that our brain encodes more fine-grained concept information than shallow syntactic information when representing languages.
</details>
<details>
<summary>摘要</summary>
为了解释人脑语言表示法下的算法，前一些研究使用预训练的人工神经网络（ANN）模型进行精细调整NLU任务。然而，全面调整通常更新整个参数空间，与人脑的多任务学习能力不匹配。Prompt-tuning，相比之下，保护预训练的权重并学习任务特定的嵌入，以适应任务。能否使用Prompt-tuning生成更好地匹配人脑语言表示的表示？如果是的，那么哪种NLU任务会使预训练模型更好地解码人脑活动中的信息？我们通过比较Prompt-tuned和全面调整的表示在神经解码中进行比较，即预测语言刺激从人脑活动中的诱导信息。我们发现，在10个NLU任务中，全面调整不能significantlyOutperformPrompt-tuning神经解码中。这表明，使用更加brain-consistent的调整方法可以生成更好地匹配人脑数据的表示。此外，我们发现，处理细化意义的任务（例如语法块分析任务）的表示更好地解码人脑活动模式，特别是在语法块分析任务中。这表明，我们的脑在语言表示中更加强调细化意义信息，而不是浅层语法信息。
</details></li>
</ul>
<hr>
<h2 id="LanguageBind-Extending-Video-Language-Pretraining-to-N-modality-by-Language-based-Semantic-Alignment"><a href="#LanguageBind-Extending-Video-Language-Pretraining-to-N-modality-by-Language-based-Semantic-Alignment" class="headerlink" title="LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment"></a>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01852">http://arxiv.org/abs/2310.01852</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-yuangroup/languagebind">https://github.com/pku-yuangroup/languagebind</a></li>
<li>paper_authors: Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, Li Yuan</li>
<li>for: 提高多modalities视频语言模型的性能（improve the performance of multimodal video language models）</li>
<li>methods: 使用语言作为绑定élément（use language as the binding element），即冻结语言Encoder获取自VL pré-training，然后使其他模式的Encoder通过对比学习。（freeze the language encoder obtained from VL pre-training, and then train encoders for other modalities with contrastive learning）</li>
<li>results: 在MSR-VTT数据集上表现出优于ImageBind的5.8% R@1（outperform ImageBind by 5.8% R@1 on the MSR-VTT dataset），并在其他多个任务中也表现出优异（and also outperform in other multiple tasks）<details>
<summary>Abstract</summary>
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N>=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with complete semantics rather than truncated segments from long videos, and all the video, depth, infrared, and audio modalities are aligned to their textual descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 5.8% R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot video-text retrieval task. Beyond this, our LanguageBind has greatly improved in the zero-shot video, audio, depth, and infrared understanding tasks. For instance, LanguageBind surpassing InterVideo by 1.9% on MSR-VTT, 8.8% on MSVD, 6.3% on DiDeMo, and 4.4% on ActivityNet. On the LLVIP and NYU-D datasets, LanguageBind outperforms ImageBind with 23.8% and 11.1% top-1 accuracy. Code address: https://github.com/PKU-YuanGroup/LanguageBind.
</details>
<details>
<summary>摘要</summary>
视频语言（VL）预训练已经实现了多个下游任务中的显著改进。然而，现有的VL预训练框架难以扩展到多个modalities（N模式，N≥3）以外的视觉语言。我们因此提出了LanguageBind，将语言作为所有modalities之间的绑定因素。具体来说，我们冻结获得的语言encoder，然后使用对比学习训练其他modalities的encoder。这使得所有modalities都映射到共同的特征空间，实现多modal semantic alignment。LanguageBind确保了我们可以扩展VL modalities到N modalities，但我们还需要一个高质量的数据集，其中包含对齐数据对。我们因此提出了VIDAL-10M，它包含视频、红外、深度、音频和其对应的语言。在我们的VIDAL-10M中，所有视频都来自短视频平台，完整的 semantics 而不是长视频中 truncated 的segment，而所有视频、深度、红外和音频modalities都与其文本描述进行了对齐。在VIDAL-10M上进行预训练后，我们在MSR-VTT数据集上出现了与ImageBind的5.8% R@1的提升，只使用15%的参数。此外，LanguageBind在零shot video、音频、深度和红外理解任务中也有了大幅提升。例如，LanguageBind在 MSR-VTT 上超过 InterVideo  by 1.9%，在 MSVD 上超过 InterVideo  by 8.8%，在 DiDeMo 上超过 InterVideo  by 6.3%，在 ActivityNet 上超过 InterVideo  by 4.4%。在 LLVIP 和 NYU-D 数据集上，LanguageBind也超过 ImageBind，具体的top-1准确率分别是23.8%和11.1%。代码地址：https://github.com/PKU-YuanGroup/LanguageBind。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Refinement-of-Buildings’-Segmentation-Models-using-SAM"><a href="#Zero-Shot-Refinement-of-Buildings’-Segmentation-Models-using-SAM" class="headerlink" title="Zero-Shot Refinement of Buildings’ Segmentation Models using SAM"></a>Zero-Shot Refinement of Buildings’ Segmentation Models using SAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01845">http://arxiv.org/abs/2310.01845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Mayladan, Hasan Nasrallah, Hasan Moughnieh, Mustafa Shukor, Ali J. Ghandour</li>
<li>For: This paper aims to adapt foundation models for specific domains, specifically remote sensing imagery, to improve their generalization and recognition abilities.* Methods: The authors introduce a novel approach that integrates a pre-trained CNN as a prompt generator to augment the Segment Anything Model (SAM) with recognition abilities. They evaluate their method on three remote sensing datasets and achieve improved performance.* Results: The authors report a 5.47% increase in IoU and a 4.81% improvement in F1-score for out-of-distribution performance on the WHU dataset, and a 2.72% and 1.58% increase in True-Positive-IoU and True-Positive-F1 score, respectively, for in-distribution performance on the WHU dataset.<details>
<summary>Abstract</summary>
Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different prompting strategies, including integrating a pre-trained CNN as a prompt generator. This novel approach augments SAM with recognition abilities, a first of its kind. We evaluated our method on three remote sensing datasets, including the WHU Buildings dataset, the Massachusetts Buildings dataset, and the AICrowd Mapping Challenge. For out-of-distribution performance on the WHU dataset, we achieve a 5.47% increase in IoU and a 4.81% improvement in F1-score. For in-distribution performance on the WHU dataset, we observe a 2.72% and 1.58% increase in True-Positive-IoU and True-Positive-F1 score, respectively. We intend to release our code repository, hoping to inspire further exploration of foundation models for domain-specific tasks within the remote sensing community.
</details>
<details>
<summary>摘要</summary>
基础模型在多种任务中表现出色，但它们常被评估在通用的benchmark上。适应这些模型特定领域，如遥感图像，仍是一个未曾充分发掘的领域。在遥感中，精准地分割建筑物实例是城市规划等应用的关键。虽然卷积神经网络（CNN）表现良好，但其泛化能力有限。为了解决这个问题，我们提出了一种适应基础模型的新方法，以提高现有模型的泛化能力。我们的注意点在于Segment Anything Model（SAM），这是一个知名的基础模型，拥有类型不敏感的图像分割能力。我们发现SAM在遥感图像上表现不佳，并且无法识别和标记地方化对象。为了解决这些限制，我们提出了不同的提示策略，包括将预训练的CNN作为提示生成器 integrating。这种新的approach不仅增强了SAM的识别能力，还是首次实现的。我们对三个遥感数据集进行了评估，包括WHU建筑数据集、马萨诸塞建筑数据集和AICrowd Mapping Challenge。对于WHU数据集的 OUT-OF-DISTRIBUTION性能，我们实现了5.47%的IoU提高和4.81%的F1得分提高。对于IN-DISTRIBUTION性能，我们观察到2.72%和1.58%的True-Positive-IoU和True-Positive-F1分数提高。我们计划在未来释出代码库，希望能启发更多人在遥感社区进行基础模型的探索。
</details></li>
</ul>
<hr>
<h2 id="Extending-CAM-based-XAI-methods-for-Remote-Sensing-Imagery-Segmentation"><a href="#Extending-CAM-based-XAI-methods-for-Remote-Sensing-Imagery-Segmentation" class="headerlink" title="Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation"></a>Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01837">http://arxiv.org/abs/2310.01837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdul Karim Gizzini, Mustafa Shukor, Ali J. Ghandour</li>
<li>for: 这 paper 的目的是帮助解释深度学习模型在高分辨率卫星图像中的行为和决策过程，以提高模型的可读性和可信度。</li>
<li>methods: 这 paper 使用了一些最新的 XAI 技术，包括适应性抑制和监测 Entropy，来解释建筑物的分类和分割。</li>
<li>results: 研究发现，使用 XAI 技术可以帮助理解深度学习模型在高分辨率卫星图像中的行为和决策过程，并提高模型的可读性和可信度。<details>
<summary>Abstract</summary>
Current AI-based methods do not provide comprehensible physical interpretations of the utilized data, extracted features, and predictions/inference operations. As a result, deep learning models trained using high-resolution satellite imagery lack transparency and explainability and can be merely seen as a black box, which limits their wide-level adoption. Experts need help understanding the complex behavior of AI models and the underlying decision-making process. The explainable artificial intelligence (XAI) field is an emerging field providing means for robust, practical, and trustworthy deployment of AI models. Several XAI techniques have been proposed for image classification tasks, whereas the interpretation of image segmentation remains largely unexplored. This paper offers to bridge this gap by adapting the recent XAI classification algorithms and making them usable for muti-class image segmentation, where we mainly focus on buildings' segmentation from high-resolution satellite images. To benchmark and compare the performance of the proposed approaches, we introduce a new XAI evaluation methodology and metric based on "Entropy" to measure the model uncertainty. Conventional XAI evaluation methods rely mainly on feeding area-of-interest regions from the image back to the pre-trained (utility) model and then calculating the average change in the probability of the target class. Those evaluation metrics lack the needed robustness, and we show that using Entropy to monitor the model uncertainty in segmenting the pixels within the target class is more suitable. We hope this work will pave the way for additional XAI research for image segmentation and applications in the remote sensing discipline.
</details>
<details>
<summary>摘要</summary>
当前的人工智能（AI）基于方法无法提供可understandable的物理解释，包括使用的数据、提取的特征和预测/推理操作。因此，使用高分辨率卫星图像进行训练的深度学习模型缺乏透明性和可解释性，只能被看作为黑obox，这限制了它们的广泛应用。专家需要更好地理解AI模型的复杂行为和下面的决策过程。人工智能可解释（XAI）领域是一个emerging领域，它提供了一些可靠、实用和信任worthy的AI模型部署方法。在图像分类任务上，XAI技术已经被提出，但图像 segmentation的解释仍然是一个未解决的问题。本文想要bridging这个 gap，通过适应最近的XAI分类算法，使其可以用于多类图像 segmentation，主要是对高分辨率卫星图像中的建筑物进行分类。为了评估和比较提出的方法的性能，我们提出了一种新的XAI评估方法和度量基于"Entropy"来度量模型的uncertainty。传统的XAI评估方法主要基于将区域of interest从图像feedback到预训练（utility）模型中，然后计算target类的平均更改 probabilities。这些评估度量缺乏坚定性，我们表明，使用Entropy来监测在target类中分割像素的模型uncertainty是更适合。我们希望这种工作能够开拓XAI研究的新途径，并在远程感知领域得到应用。
</details></li>
</ul>
<hr>
<h2 id="Formalizing-Natural-Language-Intent-into-Program-Specifications-via-Large-Language-Models"><a href="#Formalizing-Natural-Language-Intent-into-Program-Specifications-via-Large-Language-Models" class="headerlink" title="Formalizing Natural Language Intent into Program Specifications via Large Language Models"></a>Formalizing Natural Language Intent into Program Specifications via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01831">http://arxiv.org/abs/2310.01831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madeline Endres, Sarah Fakhoury, Saikat Chakraborty, Shuvendu K. Lahiri</li>
<li>for: 本文旨在利用大语言模型（LLM）将非正式自然语言 especifications 翻译成正式方法 postconditions，以提高代码质量和可靠性。</li>
<li>methods: 本文使用了多种方法来评估和比较不同的 LLM4nl2post approaches，包括正确性和分类力等指标。同时，本文还使用了质量和量化的方法来评估 LLM4nl2post postconditions 的质量。</li>
<li>results: 本文的结果表明，LLM4nl2post 可以生成正确的 postconditions，并且可以distinguish correct code from incorrect code。此外，本文还发现，使用 LLM4nl2post 可以捕捉70个历史bugs。<details>
<summary>Abstract</summary>
Informal natural language that describes code functionality, such as code comments or function documentation, may contain substantial information about a programs intent. However, there is typically no guarantee that a programs implementation and natural language documentation are aligned. In the case of a conflict, leveraging information in code-adjacent natural language has the potential to enhance fault localization, debugging, and code trustworthiness. In practice, however, this information is often underutilized due to the inherent ambiguity of natural language which makes natural language intent challenging to check programmatically. The "emergent abilities" of Large Language Models (LLMs) have the potential to facilitate the translation of natural language intent to programmatically checkable assertions. However, it is unclear if LLMs can correctly translate informal natural language specifications into formal specifications that match programmer intent. Additionally, it is unclear if such translation could be useful in practice. In this paper, we describe LLM4nl2post, the problem leveraging LLMs for transforming informal natural language to formal method postconditions, expressed as program assertions. We introduce and validate metrics to measure and compare different LLM4nl2post approaches, using the correctness and discriminative power of generated postconditions. We then perform qualitative and quantitative methods to assess the quality of LLM4nl2post postconditions, finding that they are generally correct and able to discriminate incorrect code. Finally, we find that LLM4nl2post via LLMs has the potential to be helpful in practice; specifications generated from natural language were able to catch 70 real-world historical bugs from Defects4J.
</details>
<details>
<summary>摘要</summary>
具有自然语言描述功能的代码，如代码注释或函数文档，可能包含大量代码意图信息。但是，在实际应用中，这些自然语言文档和代码实现之间通常无法保证一致。在这种情况下，利用代码附近的自然语言信息可以提高错误检测、调试和代码可靠性。然而，由于自然语言的本质含义不确定，使得自然语言意图难以程序matically检查。大型自然语言模型（LLMs）的“emergent abilities”可能使得自然语言意图转化成程序可靠的断言。然而，是否LLMs可以正确地将自然语言规范转化成程序员意图的正式规范，以及这种转化是否有实际用途，都是未知的。在这篇论文中，我们描述了LLM4nl2post问题，即使用LLMs将自然语言规范转化成程序assertions的形式方法。我们引入了和验证了度量不同LLM4nl2post方法的正确性和特征力度。然后，我们通过质量和量度方法评估LLM4nl2post结果，发现它们通常是正确的并能够区分错误代码。最后，我们发现LLM4nl2post通过LLMs在实际应用中有可能帮助的潜在性。在历史上，自然语言规范生成的specifications捕捉到了70个实际bug。
</details></li>
</ul>
<hr>
<h2 id="Trainable-Noise-Model-as-an-XAI-evaluation-method-application-on-Sobol-for-remote-sensing-image-segmentation"><a href="#Trainable-Noise-Model-as-an-XAI-evaluation-method-application-on-Sobol-for-remote-sensing-image-segmentation" class="headerlink" title="Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation"></a>Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01828">http://arxiv.org/abs/2310.01828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hossein Shreim, Abdul Karim Gizzini, Ali J. Ghandour</li>
<li>for: 这 paper 的目的是提出一种基于 Sobol 方法的透明性能算法，用于解决计算机视觉应用中的隐藏层模型 interpretability 问题。</li>
<li>methods: 本 paper 使用的方法包括 Sobol 方法和一种基于 learnable noise model 的评价方法，用于评估透明性能算法的性能。</li>
<li>results: 研究发现，使用 Sobol 方法可以提高透明性能算法的准确率，而且与 Seg-Grad-CAM 和 Seg-Grad-CAM++ 方法进行比较， Sobol 方法在高分辨率卫星图像中表现更好。<details>
<summary>Abstract</summary>
eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for image segmentation. This paper adapts the recent gradient-free Sobol XAI method for semantic segmentation. To measure the performance of the Sobol method for segmentation, we propose a quantitative XAI evaluation method based on a learnable noise model. The main objective of this model is to induce noise on the explanation maps, where higher induced noise signifies low accuracy and vice versa. A benchmark analysis is conducted to evaluate and compare performance of three XAI methods, including Seg-Grad-CAM, Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation technique. This constitutes the first attempt to run and evaluate XAI methods using high-resolution satellite images.
</details>
<details>
<summary>摘要</summary>
现代人工智能（XAI）已成为重要的需求，特别是在关键应用领域，以确保深度学习模型的透明度和可解释性。XAI在医疗、金融等领域具有重要的意义，因为理解深度学习算法的决策过程是关键。然而，图像分割 tasks 在计算机视觉应用中尚未得到过度的关注，尽管它是计算机视觉应用中的基本任务。只有一些研究提出了基于梯度的 XAI 算法。本文采用了最近的梯度自由 Sobol XAI 方法进行semantic segmentation。为评估 Sobol 方法的性能，我们提议了一种基于学习的噪声模型的量化 XAI 评估方法。这种方法的主要目标是在解释地图上引入噪声，其中更高的引入噪声表示低准确率，而低的引入噪声表示高准确率。我们进行了比较三种 XAI 方法的性能，包括Seg-Grad-CAM、Seg-Grad-CAM++和Seg-Sobol，使用我们提议的噪声基于评估技术。这是首次使用高分辨率卫星图像来运行和评估 XAI 方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-and-reusing-primitive-behaviours-to-improve-Hindsight-Experience-Replay-sample-efficiency"><a href="#Learning-and-reusing-primitive-behaviours-to-improve-Hindsight-Experience-Replay-sample-efficiency" class="headerlink" title="Learning and reusing primitive behaviours to improve Hindsight Experience Replay sample efficiency"></a>Learning and reusing primitive behaviours to improve Hindsight Experience Replay sample efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01827">http://arxiv.org/abs/2310.01827</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/franroldans/qmp-her">https://github.com/franroldans/qmp-her</a></li>
<li>paper_authors: Francisco Roldan Sanchez, Qiang Wang, David Cordova Bulens, Kevin McGuinness, Stephen Redmond, Noel O’Connor</li>
<li>for: 提高RL基于代理人的训练效率，解决目标基于机器人操作任务中的寻找问题</li>
<li>methods: 使用先前学习的基本行为指导代理人在探索过程中选择更有奖励的动作，使用评估网络在每个时刻决定使用先前学习的原始策略提议的动作</li>
<li>results: 比较HER和其他更高效的变种，在块 manipulate 任务中表现出更高的效率和更快的计算时间，代表代理人可以更快地学习成功策略<details>
<summary>Abstract</summary>
Hindsight Experience Replay (HER) is a technique used in reinforcement learning (RL) that has proven to be very efficient for training off-policy RL-based agents to solve goal-based robotic manipulation tasks using sparse rewards. Even though HER improves the sample efficiency of RL-based agents by learning from mistakes made in past experiences, it does not provide any guidance while exploring the environment. This leads to very large training times due to the volume of experience required to train an agent using this replay strategy. In this paper, we propose a method that uses primitive behaviours that have been previously learned to solve simple tasks in order to guide the agent toward more rewarding actions during exploration while learning other more complex tasks. This guidance, however, is not executed by a manually designed curriculum, but rather using a critic network to decide at each timestep whether or not to use the actions proposed by the previously-learned primitive policies. We evaluate our method by comparing its performance against HER and other more efficient variations of this algorithm in several block manipulation tasks. We demonstrate the agents can learn a successful policy faster when using our proposed method, both in terms of sample efficiency and computation time. Code is available at https://github.com/franroldans/qmp-her.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Hindsight Experience Replay（HER）是一种在强化学习（RL）中使用的技术，可以帮助RL-based agent通过过去的经验来解决目标基于机器人操作任务，使用稀有的奖励。尽管HER可以提高RL-based agent的样本效率，但是它不会在环境中探索时提供任何指导。这会导致训练时间很长，因为需要大量的经验来训练一个agent使用这种播放策略。在这篇论文中，我们提出了一种方法，使用先前学习的基本行为来导引agent在探索时选择更有奖励的动作，以便更快地学习更复杂的任务。这些指导不是由手动设计的课程来实施，而是通过一个批评网络来在每个时刻决定是否使用由先前学习的基本策略提出的动作。我们通过对HER和其他更高效的变种进行比较，在块 manipulate任务中评估了我们的方法。我们发现，使用我们的方法，agent可以更快地学习成功策略，同时减少样本数和计算时间。代码可以在https://github.com/franroldans/qmp-her中找到。")Here's the translation in Traditional Chinese:<<SYS>> translate("Hindsight Experience Replay（HER）是一种在强化学习（RL）中使用的技术，可以帮助RL-based agent通过过去的经验来解决目标基于机器人操作任务，使用稀有的奖励。尽管HER可以提高RL-based agent的样本效率，但是它不会在环境中探索时提供任何指导。这会导致训练时间很长，因为需要大量的经验来训练一个agent使用这种播放策略。在这篇论文中，我们提出了一种方法，使用先前学习的基本行为来导引agent在探索时选择更有奖励的动作，以便更快地学习更复杂的任务。这些指导不是由手动设计的课程来实施，而是通过一个批评网络来在每个时刻决定是否使用由先前学习的基本策略提出的动作。我们通过对HER和其他更高效的变种进行比较，在块 manipulate 任务中评估了我们的方法。我们发现，使用我们的方法，agent可以更快地学习成功策略，同时减少样本数和计算时间。代码可以在https://github.com/franroldans/qmp-her中找到。")
</details></li>
</ul>
<hr>
<h2 id="Empirical-Study-of-PEFT-techniques-for-Winter-Wheat-Segmentation"><a href="#Empirical-Study-of-PEFT-techniques-for-Winter-Wheat-Segmentation" class="headerlink" title="Empirical Study of PEFT techniques for Winter Wheat Segmentation"></a>Empirical Study of PEFT techniques for Winter Wheat Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01825">http://arxiv.org/abs/2310.01825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Hasan Zahweh, Hasan Nasrallah, Mustafa Shukor, Ghaleb Faour, Ali J. Ghandour<br>for: This paper aims to explore the feasibility of cross-area and cross-year out-of-distribution generalization for crop monitoring using the State-of-the-Art (SOTA) wheat crop monitoring model.methods: The paper uses various PEFT (Parameter Efficient Fine Tuning) techniques, including BigFit, LoRA, Adaptformer, and prompt tuning, to adapt the SOTA TSViT model for winter wheat field segmentation.results: The paper achieved notable results comparable to those achieved using full fine-tuning methods while training only a mere 0.7% parameters of the whole TSViT architecture. The in-house labeled data-set, referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over five consecutive years. Using Sentinel-2 images, the model achieved an 84% F1-score.Here is the simplified Chinese text for the three key points:for: 这篇论文目的是探讨跨区域和跨年度out-of-distribution泛化对农业监测中的应用可能性。methods: 这篇论文使用了不同的PEFT技术，包括BigFit、LoRA、Adaptformer和提示调整，来适应SOTA TSViT模型用于冬小麦场segmentation。results: 这篇论文在只训练0.7% TSViT架构中的参数量下达到了与全量精度调整方法相当的不ABLEResult。使用了自己的标注数据集，称为Beqaa-Lebanon数据集，包括5年 consecutively annotated的高质量冬小麦和非冬小麦类划分 polygon，总面积约170 kmsq。使用Sentinel-2图像，模型达到了84% F1得分。<details>
<summary>Abstract</summary>
Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adapting the SOTA TSViT model to address winter wheat field segmentation, a critical task for crop monitoring and food security. This adaptation process involves integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and prompt tuning. Using PEFT techniques, we achieved notable results comparable to those achieved using full fine-tuning methods while training only a mere 0.7% parameters of the whole TSViT architecture. The in-house labeled data-set, referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over five consecutive years. Using Sentinel-2 images, our model achieved a 84% F1-score. We intend to publicly release the Lebanese winter wheat data set, code repository, and model weights.
</details>
<details>
<summary>摘要</summary>
Parameter Efficient Fine Tuning (PEFT) 技术在最近几年内得到了广泛应用，并在不同领域中适应化大型视觉语言模型，以达到最佳性能的同时减少计算成本。然而，还没有很多研究探讨PEFT在实际应用场景中的潜在应用，特别是在重要的远程感知和农业监测领域。由于不同地区的气候多样性和需要大规模的数据采集，以准确地识别不同地区的农作物种类是一项挑战。本研究旨在bridging这个差距，通过对State-of-the-Art (SOTA) 小麦监测模型进行跨地区和跨年度 OUT-OF-distribution 泛化研究。本研究的目的是探讨PEFT方法在农业监测中的应用。特别是，我们将focus on 适应SOTA TSViT模型，以解决冬小麦场地分类，这是农业监测和食品安全的关键任务。这个适应过程涉及到了不同的PEFT技术，包括BigFit、LoRA、Adaptformer和prompt tuning。通过PEFT技术，我们在只训练0.7%的整个TSViT架构参数上达到了与全 fine-tuning 方法相当的 Result。我们使用Sentinel-2 图像，在Beqaa-Lebanon 数据集上实现了84% F1 分数。我们打算公开利比邻国冬小麦数据集、代码存储和模型参数。
</details></li>
</ul>
<hr>
<h2 id="Mini-BEHAVIOR-A-Procedurally-Generated-Benchmark-for-Long-horizon-Decision-Making-in-Embodied-AI"><a href="#Mini-BEHAVIOR-A-Procedurally-Generated-Benchmark-for-Long-horizon-Decision-Making-in-Embodied-AI" class="headerlink" title="Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI"></a>Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01824">http://arxiv.org/abs/2310.01824</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordvl/mini_behavior">https://github.com/stanfordvl/mini_behavior</a></li>
<li>paper_authors: Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, Roberto Martín-Martín</li>
<li>for: 本研究开发了一个新的embodied AIbenchmark，名为Mini-BEHAVIOR，用于测试和评估在人工智能执行任务时的决策和计划能力。</li>
<li>methods: 该benchmark使用Gridworld环境，并提供了一系列的任务和学习环境，以测试和评估embodied AI的决策和计划能力。</li>
<li>results: Mini-BEHAVIOR提供了一个许多任务的集合，可以用于评估和研究embodied AI的决策和计划能力，并且可以快速进行protoype和学习。<details>
<summary>Abstract</summary>
We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges. The Mini-BEHAVIOR environment is a fast, realistic Gridworld environment that offers the benefits of rapid prototyping and ease of use while preserving a symbolic level of physical realism and complexity found in complex embodied AI benchmarks. We introduce key features such as procedural generation, to enable the creation of countless task variations and support open-ended learning. Mini-BEHAVIOR provides implementations of various household tasks from the original BEHAVIOR benchmark, along with starter code for data collection and reinforcement learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended benchmark for evaluating decision-making and planning solutions in embodied AI. It serves as a user-friendly entry point for research and facilitates the evaluation and development of solutions, simplifying their assessment and development while advancing the field of embodied AI. Code is publicly available at https://github.com/StanfordVL/mini_behavior.
</details>
<details>
<summary>摘要</summary>
我们介绍了Mini-BEHAVIOR，一个新的人工智能benchmark，挑战智能体用于解决复杂的日常人类挑战。Mini-BEHAVIOR环境具有快速、真实的Gridworld环境，可以快速进行模拟和使用，同时保留了复杂的身体智能 benchmark中的物理实实的复杂性和难度。我们引入了程序生成功能，以生成无数个任务变化，支持开放式学习。Mini-BEHAVIOR提供了原始BEHAVIORbenchmark中的各种家庭任务，以及数据收集和奖励学习代码。简而言之，Mini-BEHAVIOR是一个快速、开放式的人工智能决策和规划解决方案的benchmark，可以方便地评估和开发解决方案，同时推动人工智能领域的发展。代码可以在https://github.com/StanfordVL/mini_behavior上获取。
</details></li>
</ul>
<hr>
<h2 id="MIMO-NeRF-Fast-Neural-Rendering-with-Multi-input-Multi-output-Neural-Radiance-Fields"><a href="#MIMO-NeRF-Fast-Neural-Rendering-with-Multi-input-Multi-output-Neural-Radiance-Fields" class="headerlink" title="MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields"></a>MIMO-NeRF: Fast Neural Rendering with Multi-input Multi-output Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01821">http://arxiv.org/abs/2310.01821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuhiro Kaneko</li>
<li>for: 提高NeRF的渲染速度和质量之间的平衡，以及减少NeRF的训练时间。</li>
<li>methods: 将SISO MLP replaced with MIMO MLP，并在组内进行映射，以减少NeRF的MLP数量。自动学习方法可以解决这种抽象的问题，不需要使用预训练模型。</li>
<li>results: 通过对比和缺失研究，显示MIMO-NeRF可以在理想的训练时间内获得良好的平衡。此外，MIMO-NeRF可以与之前的NeRF快速技术（如DONeRF和TensoRF）结合使用，以提高渲染质量。<details>
<summary>Abstract</summary>
Neural radiance fields (NeRFs) have shown impressive results for novel view synthesis. However, they depend on the repetitive use of a single-input single-output multilayer perceptron (SISO MLP) that maps 3D coordinates and view direction to the color and volume density in a sample-wise manner, which slows the rendering. We propose a multi-input multi-output NeRF (MIMO-NeRF) that reduces the number of MLPs running by replacing the SISO MLP with a MIMO MLP and conducting mappings in a group-wise manner. One notable challenge with this approach is that the color and volume density of each point can differ according to a choice of input coordinates in a group, which can lead to some notable ambiguity. We also propose a self-supervised learning method that regularizes the MIMO MLP with multiple fast reformulated MLPs to alleviate this ambiguity without using pretrained models. The results of a comprehensive experimental evaluation including comparative and ablation studies are presented to show that MIMO-NeRF obtains a good trade-off between speed and quality with a reasonable training time. We then demonstrate that MIMO-NeRF is compatible with and complementary to previous advancements in NeRFs by applying it to two representative fast NeRFs, i.e., a NeRF with sample reduction (DONeRF) and a NeRF with alternative representations (TensoRF).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Discrete-compositional-and-symbolic-representations-through-attractor-dynamics"><a href="#Discrete-compositional-and-symbolic-representations-through-attractor-dynamics" class="headerlink" title="Discrete, compositional, and symbolic representations through attractor dynamics"></a>Discrete, compositional, and symbolic representations through attractor dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01807">http://arxiv.org/abs/2310.01807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Nam, Eric Elmoznino, Nikolay Malkin, Chen Sun, Yoshua Bengio, Guillaume Lajoie</li>
<li>for: 这个论文的目的是探讨符号系统中的可组合性特性，以及如何通过模型化吸引器动力学来实现在符号空间中的可组合性。</li>
<li>methods: 该论文使用了建立在吸引器网络之上的新训练方法，以模型符号空间中的吸引器动力学，从而实现在符号空间中的可组合性。</li>
<li>results: 研究人员发现，通过吸引器动力学模型，可以在符号空间中实现可组合性，并且该模型可以处理rich的感知输入。此外，研究人员还发现，该模型在处理感知输入时会经历信息瓶颈现象，这可能与生物体的意识经验有关。<details>
<summary>Abstract</summary>
Compositionality is an important feature of discrete symbolic systems, such as language and programs, as it enables them to have infinite capacity despite a finite symbol set. It serves as a useful abstraction for reasoning in both cognitive science and in AI, yet the interface between continuous and symbolic processing is often imposed by fiat at the algorithmic level, such as by means of quantization or a softmax sampling step. In this work, we explore how discretization could be implemented in a more neurally plausible manner through the modeling of attractor dynamics that partition the continuous representation space into basins that correspond to sequences of symbols. Building on established work in attractor networks and introducing novel training methods, we show that imposing structure in the symbolic space can produce compositionality in the attractor-supported representation space of rich sensory inputs. Lastly, we argue that our model exhibits the process of an information bottleneck that is thought to play a role in conscious experience, decomposing the rich information of a sensory input into stable components encoding symbolic information.
</details>
<details>
<summary>摘要</summary>
“ композиция是一个重要的特点，它使得抽象符号系统，如语言和程序，可以具有无限容量，即使使用有限的符号集。它在认知科学和人工智能中都是一种有用的抽象，但是在连续和符号处理之间的界面经常是由算法强制实施的，例如通过量化或softmax采样步骤。在这项工作中，我们探索如何通过模型拥有者动力学来实现精炼的抽象。我们基于已有的拥有者网络工作，并引入新的训练方法，并示出了在有限的符号空间中强制结构可以生成compose的表示空间。最后，我们 argue that我们的模型具有信息瓶颈，这种信息瓶颈被认为是意识经验中的一个重要组成部分，将丰富的感知输入分解成稳定的分量，编码符号信息。”Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Improvement-and-Enhancement-of-YOLOv5-Small-Target-Recognition-Based-on-Multi-module-Optimization"><a href="#Improvement-and-Enhancement-of-YOLOv5-Small-Target-Recognition-Based-on-Multi-module-Optimization" class="headerlink" title="Improvement and Enhancement of YOLOv5 Small Target Recognition Based on Multi-module Optimization"></a>Improvement and Enhancement of YOLOv5 Small Target Recognition Based on Multi-module Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01806">http://arxiv.org/abs/2310.01806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyang Li, Yuchen Li, Hongyi Duan, JiaLiang Kang, Jianan Zhang, Xueqian Gan, Ruotong Xu</li>
<li>for: 这个论文主要针对小目标检测任务中YOLOv5s模型的局限性进行深入研究和改进。</li>
<li>methods: 该论文提出了基于GhostNet convolutional模块、RepGFPN颈部模块优化、CA和Transformer的注意机制以及损失函数改进等多种改进策略，以提高模型的精度、回归率和MAP指标。</li>
<li>results: 实验结果表明，这些改进策略有效地提高了模型的表现，尤其是在复杂背景和微小目标的实际应用测试中表现出色。<details>
<summary>Abstract</summary>
In this paper, the limitations of YOLOv5s model on small target detection task are deeply studied and improved. The performance of the model is successfully enhanced by introducing GhostNet-based convolutional module, RepGFPN-based Neck module optimization, CA and Transformer's attention mechanism, and loss function improvement using NWD. The experimental results validate the positive impact of these improvement strategies on model precision, recall and mAP. In particular, the improved model shows significant superiority in dealing with complex backgrounds and tiny targets in real-world application tests. This study provides an effective optimization strategy for the YOLOv5s model on small target detection, and lays a solid foundation for future related research and applications.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，YOLOv5s模型在小目标检测任务中的局限性进行了深入研究和改进。通过引入 GhostNet 基于 convolutional 模块、RepGFPN 基于 Neck 模块优化、CA 和 Transformer 注意机制以及损失函数改进使用 NWD，提高了模型的性能。实验结果证明了这些改进策略对模型精度、重复率和 mAP 的影响。特别是，改进后的模型在实际应用中对复杂背景和小目标展示出了显著的优势。这种研究为 YOLOv5s 模型在小目标检测中的优化提供了有效的策略，并为后续相关研究和应用提供了坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Comparative-study-of-microgrid-optimal-scheduling-under-multi-optimization-algorithm-fusion"><a href="#Comparative-study-of-microgrid-optimal-scheduling-under-multi-optimization-algorithm-fusion" class="headerlink" title="Comparative study of microgrid optimal scheduling under multi-optimization algorithm fusion"></a>Comparative study of microgrid optimal scheduling under multi-optimization algorithm fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01805">http://arxiv.org/abs/2310.01805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Duan, Qingyang Li, Yuchen Li, Jianan Zhang, Yuming Xie</li>
<li>for: 本研究旨在探讨微Grid的操作和环境成本之间的关系，通过多bjective优化模型进行探讨。</li>
<li>methods: 该研究提出了一种 integrate多种优化算法的方法，包括生物algorithm、Simulated Annealing、Ant Colony Optimization和Particle Swarm Optimization。</li>
<li>results: 实验结果表明，这些算法在经济和环境调度下提供了不同的派发结果，揭示了微Grid中 diesel机和微气轮机的不同角色。<details>
<summary>Abstract</summary>
As global attention on renewable and clean energy grows, the research and implementation of microgrids become paramount. This paper delves into the methodology of exploring the relationship between the operational and environmental costs of microgrids through multi-objective optimization models. By integrating various optimization algorithms like Genetic Algorithm, Simulated Annealing, Ant Colony Optimization, and Particle Swarm Optimization, we propose an integrated approach for microgrid optimization. Simulation results depict that these algorithms provide different dispatch results under economic and environmental dispatch, revealing distinct roles of diesel generators and micro gas turbines in microgrids. Overall, this study offers in-depth insights and practical guidance for microgrid design and operation.
</details>
<details>
<summary>摘要</summary>
为了满足全球人们对可再生和清洁能源的需求，微型电网的研究和实施变得非常重要。本文介绍了通过多目标优化模型研究微型电网操作和环境成本之间的关系的方法ologyth. 通过结合不同的优化算法，如遗传算法、模拟热处理算法、蚁群优化算法和粒子群优化算法，我们提出了一种综合方法 для微型电网优化。实验结果表明，这些算法在经济和环境调度下提供了不同的派发结果，揭示了微型电网中燃油机和微型气体轮机的不同角色。总的来说，这种研究为微型电网设计和运行提供了深入的理解和实践指导。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Cannot-Self-Correct-Reasoning-Yet"><a href="#Large-Language-Models-Cannot-Self-Correct-Reasoning-Yet" class="headerlink" title="Large Language Models Cannot Self-Correct Reasoning Yet"></a>Large Language Models Cannot Self-Correct Reasoning Yet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01798">http://arxiv.org/abs/2310.01798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, Denny Zhou</li>
<li>for: 本研究旨在探讨LLMs中自 corrections的功能和局限性，以提高其生成内容的准确性和适用性。</li>
<li>methods: 本研究采用了一种现代方法，即自 corrections，以探讨LLMs的自 corrected内容的质量和可靠性。</li>
<li>results: 研究发现，无外部反馈的情况下，LLMs很难进行自 corrected，而且有时even degrade its performance post self-correction。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.
</details>
<details>
<summary>摘要</summary>
Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction.Drawing from these insights, we offer suggestions for future research and practical applications in this field.Translation notes:* "Large Language Models" is translated as "大型语言模型" (dàxìng yǔyán módelǐ)* "self-correction" is translated as "自我修正" (zìwǒ xiùgòng)* "intrinsic self-correction" is translated as "内在自我修正" (néizài zìwǒ xiùgòng)* "re reasoning" is translated as "再理解" (zài lǐjiě)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Online-POMDP-Planning-with-Anytime-Deterministic-Guarantees"><a href="#Online-POMDP-Planning-with-Anytime-Deterministic-Guarantees" class="headerlink" title="Online POMDP Planning with Anytime Deterministic Guarantees"></a>Online POMDP Planning with Anytime Deterministic Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01791">http://arxiv.org/abs/2310.01791</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moranbar/Online-POMDP-Planning-with-Anytime-Deterministic-Guarantees">https://github.com/moranbar/Online-POMDP-Planning-with-Anytime-Deterministic-Guarantees</a></li>
<li>paper_authors: Moran Barenboim, Vadim Indelman</li>
<li>for:  This paper is written for researchers and practitioners interested in planning under uncertainty, particularly in real-world scenarios where autonomous agents operate.</li>
<li>methods:  The paper uses partially observable Markov decision processes (POMDPs) to mathematically formalize planning under uncertainty. It also employs approximate algorithms such as tree search and sample-based methodologies to solve POMDPs, which offer probabilistic and asymptotic guarantees towards the optimal solution.</li>
<li>results:  The paper derives a deterministic relationship between a simplified solution and the theoretically optimal one, providing bounds for selecting a subset of observations to branch from while computing a complete belief at each posterior node. The paper also extends these bounds to support reduction of both the state and observation spaces, and demonstrates how these guarantees can be integrated with existing state-of-the-art solvers. Additionally, the paper provides supporting experimental results to substantiate its findings.<details>
<summary>Abstract</summary>
Autonomous agents operating in real-world scenarios frequently encounter uncertainty and make decisions based on incomplete information. Planning under uncertainty can be mathematically formalized using partially observable Markov decision processes (POMDPs). However, finding an optimal plan for POMDPs can be computationally expensive and is feasible only for small tasks. In recent years, approximate algorithms, such as tree search and sample-based methodologies, have emerged as state-of-the-art POMDP solvers for larger problems. Despite their effectiveness, these algorithms offer only probabilistic and often asymptotic guarantees toward the optimal solution due to their dependence on sampling. To address these limitations, we derive a deterministic relationship between a simplified solution that is easier to obtain and the theoretically optimal one. First, we derive bounds for selecting a subset of the observations to branch from while computing a complete belief at each posterior node. Then, since a complete belief update may be computationally demanding, we extend the bounds to support reduction of both the state and the observation spaces. We demonstrate how our guarantees can be integrated with existing state-of-the-art solvers that sample a subset of states and observations. As a result, the returned solution holds deterministic bounds relative to the optimal policy. Lastly, we substantiate our findings with supporting experimental results.
</details>
<details>
<summary>摘要</summary>
首先，我们derive bounds for selecting a subset of observations to branch from while computing a complete belief at each posterior node。然后，由于完整的信念更新可以是计算昂贵的，我们扩展这些 boundsto支持状态和观察空间的减少。我们示出了如何将我们的保证与现有的状态-OF-the-art解决方案集成，以获得返回的解决方案具有deterministic bound relative to the optimal policy。最后，我们通过实验来证明我们的发现。
</details></li>
</ul>
<hr>
<h2 id="Can-large-language-models-provide-useful-feedback-on-research-papers-A-large-scale-empirical-analysis"><a href="#Can-large-language-models-provide-useful-feedback-on-research-papers-A-large-scale-empirical-analysis" class="headerlink" title="Can large language models provide useful feedback on research papers? A large-scale empirical analysis"></a>Can large language models provide useful feedback on research papers? A large-scale empirical analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01783">http://arxiv.org/abs/2310.01783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weixin-liang/llm-scientific-feedback">https://github.com/weixin-liang/llm-scientific-feedback</a></li>
<li>paper_authors: Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, Daniel McFarland, James Zou</li>
<li>for: The paper aims to evaluate the utility of using large language models (LLMs) to generate scientific feedback on research manuscripts.</li>
<li>methods: The authors created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers and evaluated the quality of GPT-4’s feedback through two large-scale studies.</li>
<li>results: The study found that GPT-4’s generated feedback overlaps with human peer reviewer feedback, and more than half of the users found the feedback helpful. However, the authors also identified several limitations of using LLM-generated feedback.Here are the three points in Simplified Chinese text:</li>
<li>for: 本研究旨在评估使用大语言模型（LLM）生成科学评论的有用性。</li>
<li>methods: 作者们创建了一个自动化管道，使用GPT-4对科学论文PDF提供反馈，并通过两项大规模研究评估GPT-4生成的反馈质量。</li>
<li>results: 研究发现，GPT-4生成的反馈与人类同行评审者的反馈重叠，并且超过50%的用户认为这些反馈是有帮助的。然而，作者们还发现了一些LLM生成反馈的局限性。<details>
<summary>Abstract</summary>
Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.
</details>
<details>
<summary>摘要</summary>
专家反馈是科学研究的基础。但是，随着学术著作的快速增长和专业知识的复杂化，传统的科学反馈机制变得更加困难。获得高质量的同行评审变得越来越困难。特别是 junior researchers 和资源匮乏的设置中的研究人员更容易遇到延迟的反馈问题。随着大语言模型（LLM）如 GPT-4 的突破，有关使用 LLM 生成科学反馈的兴趣日益增长。然而， LLG 生成反馈的实用性尚未得到系统性的研究。为了解决这个空白，我们创建了一个自动化管道，使用 GPT-4 生成科学论文的注释。我们通过两项大规模研究评估 GPT-4 生成的反馈质量。首先，我们量化比较 GPT-4 生成的反馈和人工同行评审者的反馈在 15 本 Nature 家刊物（3,096 篇论文）和 ICLR 机器学习会议（1,709 篇论文）中的重叠率。结果显示，GPT-4 生成的反馈和人工同行评审者的反馈之间的重叠率为 30.85%（Nature 家刊物）和 39.23%（ICLR），与人工同行评审者之间的重叠率（28.58%（Nature 家刊物）和 35.25%（ICLR））相比较高。此外，我们发现 GPT-4 生成的反馈和人工同行评审者的反馈之间的重叠率在弱论文上更高。其次，我们采用了向 308 名 AI 和计算生物学研究人员进行前向研究，以了解他们对我们的 GPT-4 系统生成的反馈是否有帮助。结果显示，More than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful，和 82.4% 认为 GPT-4 生成的反馈比至少一些人工同行评审者的反馈更有利。 although our findings show that LLM-generated feedback can help researchers, we also identify several limitations.
</details></li>
</ul>
<hr>
<h2 id="STAMP-Differentiable-Task-and-Motion-Planning-via-Stein-Variational-Gradient-Descent"><a href="#STAMP-Differentiable-Task-and-Motion-Planning-via-Stein-Variational-Gradient-Descent" class="headerlink" title="STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent"></a>STAMP: Differentiable Task and Motion Planning via Stein Variational Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01775">http://arxiv.org/abs/2310.01775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yewon Lee, Philip Huang, Krishna Murthy Jatavallabhula, Andrew Z. Li, Fabian Damken, Eric Heiden, Kevin Smith, Derek Nowrouzezahrai, Fabio Ramos, Florian Shkurti</li>
<li>for:  solves task and motion planning problems for manipulation tasks, such as using tools or assembling parts, by leveraging parallelization and differentiable simulation to efficiently search for multiple diverse plans.</li>
<li>methods:  uses Stein Variational Gradient Descent and parallelized differentiable physics simulators on the GPU to efficiently obtain gradients for inference, and employs imitation learning to introduce action abstractions that reduce the inference problem to lower dimensions.</li>
<li>results:  produces multiple diverse plans in parallel and searches for plans more efficiently compared to existing TAMP baselines.<details>
<summary>Abstract</summary>
Planning for many manipulation tasks, such as using tools or assembling parts, often requires both symbolic and geometric reasoning. Task and Motion Planning (TAMP) algorithms typically solve these problems by conducting a tree search over high-level task sequences while checking for kinematic and dynamic feasibility. While performant, most existing algorithms are highly inefficient as their time complexity grows exponentially with the number of possible actions and objects. Additionally, they only find a single solution to problems in which many feasible plans may exist. To address these limitations, we propose a novel algorithm called Stein Task and Motion Planning (STAMP) that leverages parallelization and differentiable simulation to efficiently search for multiple diverse plans. STAMP relaxes discrete-and-continuous TAMP problems into continuous optimization problems that can be solved using variational inference. Our algorithm builds upon Stein Variational Gradient Descent, a gradient-based variational inference algorithm, and parallelized differentiable physics simulators on the GPU to efficiently obtain gradients for inference. Further, we employ imitation learning to introduce action abstractions that reduce the inference problem to lower dimensions. We demonstrate our method on two TAMP problems and empirically show that STAMP is able to: 1) produce multiple diverse plans in parallel; and 2) search for plans more efficiently compared to existing TAMP baselines.
</details>
<details>
<summary>摘要</summary>
планирование для многих задач манипуляции, таких как использование инструментов или сборка частей, часто требует как символического, так и геометрического мышления. алгоритмы планирования задач и движения (TAMP) обычно решают эти проблемы, проверяя поиск в древе последовательностей задач на высоком уровне, в то время как проверяют возможность движения и динамики. хотя они работают хорошо, большинство существующих алгоритмов являются высоко неэффективными, так как время сложности растет экспоненциально с количеством возможных действий и объектов. кроме того, они только находят один решение для проблем, в которых существует множество возможных планов.чтобы решить эти ограничения, мы предлагаем новый алгоритм, называемый STAMP (Stein Task and Motion Planning), который использует параллелизацию и дифференцируемую симуляцию для эффективного поиска множества разнообразных планов. STAMP разлагает задачи TAMP на континуальные оптимизационные задачи, которые могут быть решены с помощью вариационного инференции. наш алгоритм строится на Stein Variational Gradient Descent, алгоритме вариационной градиентной декрементации, и параллелизированных дифференцируемых физических симуляторах на GPU для эффективного получения градиентов для инфериенции. кроме того, мы используем обучение по примеру для введения абстрактных действий, которые уменьшают задачу инфериенции до более низких измерений.мы демонстрируем свой метод на двух задачах TAMP и эмпирически показываем, что STAMP может: 1) произвести множество разнообразных планов в параллели; и 2) поискать планы более эффективно, чем существующие TAMP-базы.
</details></li>
</ul>
<hr>
<h2 id="A-simple-connection-from-loss-flatness-to-compressed-representations-in-neural-networks"><a href="#A-simple-connection-from-loss-flatness-to-compressed-representations-in-neural-networks" class="headerlink" title="A simple connection from loss flatness to compressed representations in neural networks"></a>A simple connection from loss flatness to compressed representations in neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01770">http://arxiv.org/abs/2310.01770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shirui Chen, Stefano Recanatesi, Eric Shea-Brown</li>
<li>for: 研究深度神经网络的泛化能力</li>
<li>methods: 使用loss函数的形态和表示 manifold的结构来研究深度神经网络的泛化能力</li>
<li>results: 显示在深度神经网络的学习过程中，压缩表示 manifold的体积与损失函数的平坦性有直接的关系，并且这一关系可以通过简单的数学关系来预测。<details>
<summary>Abstract</summary>
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of the loss Hessian) develops in late phases of learning and lead to robustness to perturbations in network inputs. Moreover, we show there is no similarly direct connection between local dimensionality and sharpness, suggesting that this property may be controlled by different mechanisms than volume and hence may play a complementary role in neural representations. Overall, we advance a dual perspective on generalization in neural networks in both parameter and feature space.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Differentially-Encoded-Observation-Spaces-for-Perceptive-Reinforcement-Learning"><a href="#Differentially-Encoded-Observation-Spaces-for-Perceptive-Reinforcement-Learning" class="headerlink" title="Differentially Encoded Observation Spaces for Perceptive Reinforcement Learning"></a>Differentially Encoded Observation Spaces for Perceptive Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01767">http://arxiv.org/abs/2310.01767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a2r-lab/diffcompressdrl">https://github.com/a2r-lab/diffcompressdrl</a></li>
<li>paper_authors: Lev Grossman, Brian Plancher</li>
<li>for: 这篇论文旨在提高深度强化学习（DRL）系统的训练效率，使其能够在边缘设备上进行学习，以适应环境。</li>
<li>methods: 这篇论文使用了差异推统 observation space，将储存的图像基于观察转换为影片，并利用损失无限对称影片编码方案将练习缓存缩小至14.2倍和16.7倍，并且完全在RAM中进行训练，提高了DMC任务的延迟时间。</li>
<li>results: 这篇论文获得了训练DRL系统的效率和可扩展性，实现了边缘设备上的大规模强化学习，并且获得了32%的延迟时间改善。<details>
<summary>Abstract</summary>
Perceptive deep reinforcement learning (DRL) has lead to many recent breakthroughs for complex AI systems leveraging image-based input data. Applications of these results range from super-human level video game agents to dexterous, physically intelligent robots. However, training these perceptive DRL-enabled systems remains incredibly compute and memory intensive, often requiring huge training datasets and large experience replay buffers. This poses a challenge for the next generation of field robots that will need to be able to learn on the edge in order to adapt to their environments. In this paper, we begin to address this issue through differentially encoded observation spaces. By reinterpreting stored image-based observations as a video, we leverage lossless differential video encoding schemes to compress the replay buffer without impacting training performance. We evaluate our approach with three state-of-the-art DRL algorithms and find that differential image encoding reduces the memory footprint by as much as 14.2x and 16.7x across tasks from the Atari 2600 benchmark and the DeepMind Control Suite (DMC) respectively. These savings also enable large-scale perceptive DRL that previously required paging between flash and RAM to be run entirely in RAM, improving the latency of DMC tasks by as much as 32%.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）的感知技术在处理图像输入数据方面取得了许多最近的突破。这些应用包括超human级视频游戏代理以及柔软、物理智能的机器人。然而，训练这些感知DRL系统仍然非常计算和存储密集，经常需要庞大的训练集和大的经验回放缓存。这会对下一代静止环境中的场景机器人带来挑战，这些机器人需要在边缘上学习，以适应其环境。在这篇论文中，我们开始解决这个问题，通过不同的编码方式来压缩存储的图像 Observation 空间。我们利用图像序列的形式重新解释存储的图像，然后利用不失真的视频编码方案来压缩回放缓存。我们使用三种state-of-the-art DRL算法进行评估，发现使用不同的图像编码可以将内存占用量减少为14.2倍和16.7倍，对于Atari 2600 测试集和DeepMind Control Suite（DMC）测试集分别。这些减少还使得大规模感知DRL，之前需要缓存在 Flash 和RAM 之间，现在可以完全在 RAM 上运行，提高 DMC 任务的响应时间为多达32%。
</details></li>
</ul>
<hr>
<h2 id="Improved-Algorithms-for-Adversarial-Bandits-with-Unbounded-Losses"><a href="#Improved-Algorithms-for-Adversarial-Bandits-with-Unbounded-Losses" class="headerlink" title="Improved Algorithms for Adversarial Bandits with Unbounded Losses"></a>Improved Algorithms for Adversarial Bandits with Unbounded Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01756">http://arxiv.org/abs/2310.01756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Chen, Xuezhou Zhang</li>
<li>for:  solve the Adversarial Multi-Armed Bandit (MAB) problem with unbounded losses, where the algorithms have no prior knowledge on the sizes of the losses.</li>
<li>methods:  presents two algorithms, UMAB-NN and UMAB-G, for non-negative and general unbounded loss respectively.</li>
<li>results:  achieves the first adaptive and scale-free regret bound without uniform exploration for non-negative unbounded loss, and can learn from arbitrary unbounded loss. Our analysis reveals the asymmetry between positive and negative losses in the MAB problem and provides additional insights.<details>
<summary>Abstract</summary>
We consider the Adversarial Multi-Armed Bandits (MAB) problem with unbounded losses, where the algorithms have no prior knowledge on the sizes of the losses. We present UMAB-NN and UMAB-G, two algorithms for non-negative and general unbounded loss respectively. For non-negative unbounded loss, UMAB-NN achieves the first adaptive and scale free regret bound without uniform exploration. Built up on that, we further develop UMAB-G that can learn from arbitrary unbounded loss. Our analysis reveals the asymmetry between positive and negative losses in the MAB problem and provide additional insights. We also accompany our theoretical findings with extensive empirical evaluations, showing that our algorithms consistently out-performs all existing algorithms that handles unbounded losses.
</details>
<details>
<summary>摘要</summary>
我们考虑了对抗多重机器人（MAB）问题，其中算法无知对损失的大小。我们提出了UMAB-NN和UMAB-G两种算法，用于非正式和一般无限损失。对于非正式无限损失，UMAB-NN实现了首次适应和比例自适应征递减 regret bound，不需要均匀探索。基于这个成果，我们进一步开发了UMAB-G，可以学习任意无限损失。我们的分析发现MAB问题中损失的偏见性，并提供了附加的视角。我们还附加了广泛的实验，证明我们的算法在处理无限损失时表现更好。
</details></li>
</ul>
<hr>
<h2 id="Blending-Imitation-and-Reinforcement-Learning-for-Robust-Policy-Improvement"><a href="#Blending-Imitation-and-Reinforcement-Learning-for-Robust-Policy-Improvement" class="headerlink" title="Blending Imitation and Reinforcement Learning for Robust Policy Improvement"></a>Blending Imitation and Reinforcement Learning for Robust Policy Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01737">http://arxiv.org/abs/2310.01737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Liu, Takuma Yoneda, Rick L. Stevens, Matthew R. Walter, Yuxin Chen</li>
<li>for: 提高深度学习环境中样本繁殖的效率，使得深度学习可以更广泛应用于不同领域。</li>
<li>methods: combines imitation learning (IL) and reinforcement learning (RL), using oracle queries to facilitate exploration and gradually transitioning to RL as learning unfolds.</li>
<li>results: 在多个 benchmark 领域中表现出色，比如 existing state-of-the-art 方法。<details>
<summary>Abstract</summary>
While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the oracles or learn from its own value function when the learner's performance surpasses that of the oracles in a specific state. Empirical evaluations and theoretical analysis validate that RPI excels in comparison to existing state-of-the-art methodologies, demonstrating superior performance across various benchmark domains.
</details>
<details>
<summary>摘要</summary>
reinforcement learning (RL) 已经显示出了有前途的表现，但其样本复杂性仍然是一大障碍，限制其更广泛的应用于多个领域。 imitation learning (IL) 利用 oracle 来提高样本效率，但它通常受到 oracle 的质量的限制。 RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the oracles or learn from its own value function when the learner's performance surpasses that of the oracles in a specific state. empirical evaluations and theoretical analysis validate that RPI excels in comparison to existing state-of-the-art methodologies, demonstrating superior performance across various benchmark domains.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that version as well.
</details></li>
</ul>
<hr>
<h2 id="Learning-Expected-Appearances-for-Intraoperative-Registration-during-Neurosurgery"><a href="#Learning-Expected-Appearances-for-Intraoperative-Registration-during-Neurosurgery" class="headerlink" title="Learning Expected Appearances for Intraoperative Registration during Neurosurgery"></a>Learning Expected Appearances for Intraoperative Registration during Neurosurgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01735">http://arxiv.org/abs/2310.01735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazim Haouchine, Reuben Dorent, Parikshit Juvekar, Erickson Torio, William M. Wells III, Tina Kapur, Alexandra J. Golby, Sarah Frisken</li>
<li>for: 这个论文旨在提出一种新的操作期患者到图像匹配方法，通过学习预期表现来实现。</li>
<li>methods: 该方法使用前Operative的医学成像来生成特定病人的预期视图，并通过Camera pose的估计来匹配实时 microscope 视图和预期的 текстура。</li>
<li>results: 该方法在 synthetic 数据和6个临床案例的回顾数据上表现出优于当前临床标准的匹配精度。<details>
<summary>Abstract</summary>
We present a novel method for intraoperative patient-to-image registration by learning Expected Appearances. Our method uses preoperative imaging to synthesize patient-specific expected views through a surgical microscope for a predicted range of transformations. Our method estimates the camera pose by minimizing the dissimilarity between the intraoperative 2D view through the optical microscope and the synthesized expected texture. In contrast to conventional methods, our approach transfers the processing tasks to the preoperative stage, reducing thereby the impact of low-resolution, distorted, and noisy intraoperative images, that often degrade the registration accuracy. We applied our method in the context of neuronavigation during brain surgery. We evaluated our approach on synthetic data and on retrospective data from 6 clinical cases. Our method outperformed state-of-the-art methods and achieved accuracies that met current clinical standards.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的术前患者到图像匹配方法，通过学习预期出现的形态。我们的方法使用先operative的医疗影像来生成特定患者的预期视图，并且使用这些视图来估算摄像头姿态。与传统方法不同，我们的方法将处理任务传递到先operative阶段，从而减少了低分辨率、扭曲和噪声等因素的影响，提高了匹配精度。我们在脑手术中应用了我们的方法，并在6个临床案例中进行了评估。我们的方法在比较案例中表现出色，与当前临床标准匹配精度相当。
</details></li>
</ul>
<hr>
<h2 id="Nugget-Neural-Agglomerative-Embeddings-of-Text"><a href="#Nugget-Neural-Agglomerative-Embeddings-of-Text" class="headerlink" title="Nugget: Neural Agglomerative Embeddings of Text"></a>Nugget: Neural Agglomerative Embeddings of Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01732">http://arxiv.org/abs/2310.01732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanghui Qin, Benjamin Van Durme</li>
<li>for: 提高语言理解的现代语言处理中，嵌入文本序列是一项广泛的需求。现有的方法主要集中在固定大小表示上。这会导致问题，因为文本中含的信息通常与输入长度成正比。我们提出了一种解决方案，即块（Nugget），它通过动态选择输入符号来编码语言。这些块通过自动编码和机器翻译任务学习，INTUITIVE地将语言分割成有意义的单元。</li>
<li>methods: 我们使用了自动编码和机器翻译任务来学习块。</li>
<li>results: 我们证明了块在 semantic comparison 任务中超过相关的方法表现。此外，我们还表明了这些紧凑的单元可以扩大语言模型（LM）的语言上下文窗口，因此可能在未来的语言模型中引入更大量的内容。<details>
<summary>Abstract</summary>
Embedding text sequences is a widespread requirement in modern language understanding. Existing approaches focus largely on constant-size representations. This is problematic, as the amount of information contained in text often varies with the length of the input. We propose a solution called Nugget, which encodes language into a representation based on a dynamically selected subset of input tokens. These nuggets are learned through tasks like autoencoding and machine translation, and intuitively segment language into meaningful units. We demonstrate Nugget outperforms related approaches in tasks involving semantic comparison. Finally, we illustrate these compact units allow for expanding the contextual window of a language model (LM), suggesting new future LMs that can condition on significantly larger amounts of content.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本序列是现代语言理解中广泛的需求。现有的方法主要关注常量大小的表示。这会导致问题，因为文本中含的信息通常与输入长度相关。我们提议一种解决方案叫做“块”（Nugget），它将语言编码成基于输入Token的动态选择的子集的表示。这些块通过自动编码和机器翻译任务学习，INTUITIVE地将语言分解成意义ful单元。我们示出了Nugget比相关方法在 semantic comparison 任务中表现出色。最后，我们展示了这些紧凑的单元允许扩展语言模型（LM）的Contextual window，建议未来的LM可以condition on significantly larger amounts of content。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Time-LLM-Time-Series-Forecasting-by-Reprogramming-Large-Language-Models"><a href="#Time-LLM-Time-Series-Forecasting-by-Reprogramming-Large-Language-Models" class="headerlink" title="Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"></a>Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01728">http://arxiv.org/abs/2310.01728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, Qingsong Wen</li>
<li>for: 这个研究是为了实现一个可以处理多种时间序列数据的通用时间序列预测模型。</li>
<li>methods: 这个研究使用了一个名为Time-LLM的重programming框架，将大语言模型（LLM）重新训练为时间序列预测模型，并使用了Prompt-as-Prefix（PaP）技术来增强模型的时间序列处理能力。</li>
<li>results: 研究结果显示，Time-LLM可以实现高效的时间序列预测，并在几何shot和零shot学习情况下都表现出色。<details>
<summary>Abstract</summary>
Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.
</details>
<details>
<summary>摘要</summary>
时序序列预测具有重要 significancen 在许多实际动态系统中，并已经进行了广泛的研究。 与自然语言处理（NLP）和计算机视觉（CV）不同，时序序列预测的模型通常是特殊化的，需要不同的设计来应对不同的任务和应用。 although pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have shown that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities.在这种情况下，我们提出了 Time-LLM 框架，用于重新编程 LLMs 以适应通用时序序列预测。我们首先将时序序列数据重新编程为文本原型，然后将其 feed 到冻结的 LLM 中，以实现两个模式之间的对接。为了让 LLM 更好地理解时序序列数据，我们提出了 Prompt-as-Prefix (PaP)，它可以在重新编程的输入裁剪上添加更多的上下文信息，并指导重新编程输入的转换。最后，我们将 transformed 时序序列裁剪 projection 以获取预测结果。我们的全面评估表明，Time-LLM 是一种强大的时序序列学习模型，超过了当前最佳特殊化预测模型。此外，Time-LLM 在几个少量和零量学习场景中也表现出色。
</details></li>
</ul>
<hr>
<h2 id="Can-GPT-4-Replicate-Empirical-Software-Engineering-Research"><a href="#Can-GPT-4-Replicate-Empirical-Software-Engineering-Research" class="headerlink" title="Can GPT-4 Replicate Empirical Software Engineering Research?"></a>Can GPT-4 Replicate Empirical Software Engineering Research?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01727">http://arxiv.org/abs/2310.01727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jenny T. Liang, Carmen Badea, Christian Bird, Robert DeLine, Denae Ford, Nicole Forsgren, Thomas Zimmermann</li>
<li>for: 这个论文旨在探讨大型自然语言模型（LLM）在软件工程实践中的应用，以便帮助软件工程实践者和研究者更好地理解和复制现有的软件工程研究。</li>
<li>methods: 这个论文使用了大型自然语言模型（GPT-4）来复制现有的软件工程研究，并对GPT-4生成的假设和分析计划进行评估。研究者采用了用户研究，询问14名软件工程研究专家对GPT-4生成的假设和分析计划进行评估。</li>
<li>results: 研究发现，GPT-4可以surface正确的假设，但在生成假设时存在一些问题，如不具备软件工程知识的问题。在手动分析GPT-4生成的代码时，发现代码具有正确的高级逻辑，但具有许多小型实现级别的错误。这些结果有关于使用LLM进行软件工程研究以及软件团队数据科学工作者的启示。<details>
<summary>Abstract</summary>
Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help democratize empirical software engineering research.   In this paper, we examine LLMs' abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.
</details>
<details>
<summary>摘要</summary>
empirical software engineering research on production systems 已经为实践者和研究人员提供了更深刻的理解软件工程过程。然而，只有一小部分的生产系统被研究，这限制了这些研究的影响。软件工程实践者可以通过复制研究来启发自己的数据，但这也存在一些挑战，因为复制研究需要深刻的理解研究方法论和软件工程数据中的细微差别。大语言模型（LLM），如GPT-4，表明它们可以解决软件工程和科学相关的任务。在这篇论文中，我们研究LLM是否能够在新数据上复制Empirical software engineering research。我们Specifically studying their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.
</details></li>
</ul>
<hr>
<h2 id="PrACTiS-Perceiver-Attentional-Copulas-for-Time-Series"><a href="#PrACTiS-Perceiver-Attentional-Copulas-for-Time-Series" class="headerlink" title="PrACTiS: Perceiver-Attentional Copulas for Time Series"></a>PrACTiS: Perceiver-Attentional Copulas for Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01720">http://arxiv.org/abs/2310.01720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cat P. Le, Chris Cannella, Ali Hasan, Yuting Ng, Vahid Tarokh</li>
<li>for: 提高时间序列预测性能</li>
<li>methods: 结合 perceiver 架构和 copula 结构，使用 midpoint inference 和本地注意力机制，并使用 copula-based attention 和输出方差测试机制来捕捉缺失数据的联合分布，从而避免预测中的错误卷积。</li>
<li>results: 在单模态和多模态标准测试集上实现了相比先前方法的20%提高，同时使用的内存资源占用率低于50%。<details>
<summary>Abstract</summary>
Transformers incorporating copula structures have demonstrated remarkable performance in time series prediction. However, their heavy reliance on self-attention mechanisms demands substantial computational resources, thus limiting their practical utility across a wide range of tasks. In this work, we present a model that combines the perceiver architecture with a copula structure to enhance time-series forecasting. By leveraging the perceiver as the encoder, we efficiently transform complex, high-dimensional, multimodal data into a compact latent space, thereby significantly reducing computational demands. To further reduce complexity, we introduce midpoint inference and local attention mechanisms, enabling the model to capture dependencies within imputed samples effectively. Subsequently, we deploy the copula-based attention and output variance testing mechanism to capture the joint distribution of missing data, while simultaneously mitigating error propagation during prediction. Our experimental results on the unimodal and multimodal benchmarks showcase a consistent 20\% improvement over the state-of-the-art methods, while utilizing less than half of available memory resources.
</details>
<details>
<summary>摘要</summary>
transformers 结构含有 copula 结构，在时间序列预测中表现出了非常remarkable的性能。然而，它们对自我注意机制的依赖性很高，因此在许多任务上具有很大的计算资源需求，限制了实际应用的各种任务。在这种情况下，我们提出了一种将 perceiver 架构与 copula 结构结合在一起的模型，以提高时间序列预测性能。通过使用 perceiver 作为编码器，我们可以快速将复杂、高维、多模态数据转化为紧凑的尺度空间，从而减少计算资源的需求。此外，我们引入中点推理和本地注意机制，使模型能够有效地捕捉插入样本之间的依赖关系。然后，我们采用 copula 基于的注意力和输出方差测试机制，以捕捉缺失数据的共同分布，同时避免预测过程中的错误卷积。我们在单模态和多模态标准benchmark上进行了实验，结果显示与状态态别方法相比，我们的方法在20%的情况下具有了一致的改进，同时使用的内存资源只有使用了半个可用的内存资源。</sys>Note: The text is translated into Simplified Chinese, which is the standard form of Chinese used in mainland China. The translation is written in the formal style, which is appropriate for academic or professional writing.Here's a word-for-word translation of the text into Traditional Chinese, which is used in Taiwan and other parts of the world:<sys> transformers 结构含有 copula 结构，在时间序列预测中表现出了非常remarkable的性能。然而，它们对自我注意机制的依赖性很高，因此在许多任务上具有很大的计算资源需求，限制了实际应用的各种任务。在这种情况下，我们提出了一种将 perceiver 架构与 copula 结构结合在一起的模型，以提高时间序列预测性能。通过使用 perceiver 作为编码器，我们可以快速将复杂、高维、多模态数据转换为紧凑的尺度空间，从而减少计算资源的需求。此外，我们引入中点推理和本地注意机制，使模型能够有效地捕捉插入样本之间的依赖关系。然后，我们采用 copula 基于的注意力和输出方差测试机制，以捕捉缺失数据的共同分布，同时避免预测过程中的错误卷积。我们在单模式和多模式标准benchmark上进行了实验，结果显示与状态别方法相比，我们的方法在20%的情况下具有了一致的改进，同时使用的内存资源只有使用了半个可用的内存资源。</sys>
</details></li>
</ul>
<hr>
<h2 id="Ensemble-Distillation-for-Unsupervised-Constituency-Parsing"><a href="#Ensemble-Distillation-for-Unsupervised-Constituency-Parsing" class="headerlink" title="Ensemble Distillation for Unsupervised Constituency Parsing"></a>Ensemble Distillation for Unsupervised Constituency Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01717">http://arxiv.org/abs/2310.01717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manga-uofa/ed4ucp">https://github.com/manga-uofa/ed4ucp</a></li>
<li>paper_authors: Behzad Shayegh, Yanshuai Cao, Xiaodan Zhu, Jackie C. K. Cheung, Lili Mou</li>
<li>for: 这个论文主要用于解决无监督成分分析任务，即将句子中单词和短语组织成层次结构，不使用语言学上标注数据。</li>
<li>methods: 该论文提出了一种“树平均”思想，并基于此思想提出了一种新的 ensemble 方法。为提高推理效率，该方法还使用了一种学生模型减少过拟合问题。</li>
<li>results: 实验显示，该方法比之前的所有方法都更高效和稳定，可以在不同的 ensemble 组件、Run 和领域shift 条件下表现出色。<details>
<summary>Abstract</summary>
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
</details>
<details>
<summary>摘要</summary>
我团队 investigate了无监督成分分析任务，即将句子中单词和短语组织成层次结构，不使用语言学上注解的数据。我们发现现有的无监督解析器捕捉了不同的解析结构方面，可以用来提高无监督解析性能。为此，我们提出了“树平均”的概念，并基于此提出了一种新的集成方法。为了提高推理效率，我们进一步蒸馏集成知识到学生模型中，这种集成然后蒸馏过程能够有效地解决常见的多教师蒸馏方法中的过度熔化问题。实验表明，我们的方法超越了所有之前的方法，在不同的Run中、不同的集成组件中和下游领域转移条件下都能够保持稳定和有效。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/03/cs.AI_2023_10_03/" data-id="closbrokt00530g88fqzh8hu6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/03/cs.CL_2023_10_03/" class="article-date">
  <time datetime="2023-10-03T11:00:00.000Z" itemprop="datePublished">2023-10-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/03/cs.CL_2023_10_03/">cs.CL - 2023-10-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ResidualTransformer-Residual-Low-rank-Learning-with-Weight-sharing-for-Transformer-Layers"><a href="#ResidualTransformer-Residual-Low-rank-Learning-with-Weight-sharing-for-Transformer-Layers" class="headerlink" title="ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers"></a>ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02489">http://arxiv.org/abs/2310.02489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Wang, Jinyu Li</li>
<li>for: 降低 Always-on 设备内存占用，以便部署语音处理模型。</li>
<li>methods: 重parameterize 模型 веса逻辑，以实现模型压缩。具体来说，我们受到 ResNet 和 LoRA 等工作的启发，提出了名为 ResidualTransformer 的方法，其中每个 Transformer 层的 веса矩阵包括 1) 共享全矩阵 Component 与邻近层，2) 唯一低矩阵 Component 自身。低矩阵矩阵只增加了模型的一小部分大小。此外，我们添加了对角线 weight 矩阵，以提高模型的描述能力。</li>
<li>results: 我们在 10k 小时语音识别和语音翻译任务中进行了实验，结果表明，可以将 Transformer Encoder 的大小减少约 3X，而性能下降非常小。<details>
<summary>Abstract</summary>
Memory constraint of always-on devices is one of the major concerns when deploying speech processing models on these devices. While larger models trained with sufficiently large amount of data generally perform better, making them fit in the device memory is a demanding challenge. In this paper, we aim to reduce model size by reparameterizing model weights across Transformer encoder layers and assuming a special weight composition and structure. More specifically, inspired by ResNet and the more recent LoRA work, we propose an approach named ResidualTransformer, where each weight matrix in a Transformer layer comprises 1) a shared full-rank component with its adjacent layers, and 2) a unique low-rank component to itself. The low-rank matrices only account for a small amount of model size increase. In addition, we add diagonal weight matrices to improve modeling capacity of the low-rank matrices. Experiments of our 10k-hour speech recognition and speech translation tasks show that the Transformer encoder size can be reduced by ~3X with very slight performance degradation.
</details>
<details>
<summary>摘要</summary>
内存限制是always-on设备部署语音处理模型的一个主要问题。虽然更大的模型通常在具有足够数据量时表现更好，但是在设备内存中做出匹配是一项具有挑战性的任务。在这篇论文中，我们想要降低模型大小，通过在Transformer层中重parameterize模型 веса，并假设特定的weight组合和结构。更 Specifically，我们提出了一种方法 named ResidualTransformer，其中每个weight矩阵在Transformer层中包括1）与邻近层共享的全积矩阵，2）自身唯一的低积矩阵。这些低积矩阵只占据模型大小的一小部分。此外，我们添加了对角矩阵以提高低积矩阵的模型容量。我们的10k小时语音识别和语音翻译任务的实验表明，可以将Transformerencoder大小减少到~3X，而表现下降非常小。
</details></li>
</ul>
<hr>
<h2 id="Short-text-classification-with-machine-learning-in-the-social-sciences-The-case-of-climate-change-on-Twitter"><a href="#Short-text-classification-with-machine-learning-in-the-social-sciences-The-case-of-climate-change-on-Twitter" class="headerlink" title="Short text classification with machine learning in the social sciences: The case of climate change on Twitter"></a>Short text classification with machine learning in the social sciences: The case of climate change on Twitter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04452">http://arxiv.org/abs/2310.04452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shikarina/short_text_classification">https://github.com/shikarina/short_text_classification</a></li>
<li>paper_authors: Karina Shyrokykh, Maksym Girnyk, Lisa Dellmuth</li>
<li>for: 这篇论文是关于社会科学研究中自动分类大量文本的问题，以及计算机科学中提供的一些机器学习方法的性能的比较。</li>
<li>methods: 这篇论文使用了一些最常用的文本分类算法，包括超vision学习和深度学习方法，以及一些常用的语料库。</li>
<li>results: 研究发现，supervised机器学习方法在分类 tweet 的批处频率增加时表现更好，而传统的机器学习方法和深度学习方法在分类精度上几乎相同，但需要更少的训练时间和计算资源。<details>
<summary>Abstract</summary>
To analyse large numbers of texts, social science researchers are increasingly confronting the challenge of text classification. When manual labeling is not possible and researchers have to find automatized ways to classify texts, computer science provides a useful toolbox of machine-learning methods whose performance remains understudied in the social sciences. In this article, we compare the performance of the most widely used text classifiers by applying them to a typical research scenario in social science research: a relatively small labeled dataset with infrequent occurrence of categories of interest, which is a part of a large unlabeled dataset. As an example case, we look at Twitter communication regarding climate change, a topic of increasing scholarly interest in interdisciplinary social science research. Using a novel dataset including 5,750 tweets from various international organizations regarding the highly ambiguous concept of climate change, we evaluate the performance of methods in automatically classifying tweets based on whether they are about climate change or not. In this context, we highlight two main findings. First, supervised machine-learning methods perform better than state-of-the-art lexicons, in particular as class balance increases. Second, traditional machine-learning methods, such as logistic regression and random forest, perform similarly to sophisticated deep-learning methods, whilst requiring much less training time and computational resources. The results have important implications for the analysis of short texts in social science research.
</details>
<details>
<summary>摘要</summary>
社会科学研究者在处理大量文本时，面临着文本分类挑战。当手动标注不可行时，计算机科学提供了一个有用的工具箱，包括机器学习方法，其性能在社会科学中尚未得到充分研究。在这篇文章中，我们比较了最常用的文本分类器，并应用它们于一个典型的社会科学研究场景：一个小型标注 dataset，其中分类类型的发生率较低。作为一个例子，我们使用了 Twitter 上关于气候变化的 tweet，这是社会科学研究中越来越受关注的话题。使用一个新的 dataset，包括 5,750 条 tweet 从各国组织中关于高度抽象的气候变化概念，我们评估了不同方法在自动地将 tweet 分类为关于气候变化或者不关于气候变化。在这个上下文中，我们发现了两个主要发现：首先，supervised 机器学习方法在类别均衡度提高时表现更好 чем当前的字典，特别是在类别均衡度提高时。其次，传统的机器学习方法，如逻辑回归和Random Forest，与复杂的深度学习方法相比，表现相似，但需要远少的训练时间和计算资源。这些结果对社会科学研究中处理短文本的分析有重要意义。
</details></li>
</ul>
<hr>
<h2 id="The-Empty-Signifier-Problem-Towards-Clearer-Paradigms-for-Operationalising-“Alignment”-in-Large-Language-Models"><a href="#The-Empty-Signifier-Problem-Towards-Clearer-Paradigms-for-Operationalising-“Alignment”-in-Large-Language-Models" class="headerlink" title="The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising “Alignment” in Large Language Models"></a>The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising “Alignment” in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02457">http://arxiv.org/abs/2310.02457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Scott A. Hale</li>
<li>for: 本文探讨了大语言模型（LLM）中的“对齐”概念，通过后结构主义社会政治理论的镜像，具体探讨其与空标语概念的相似之处。</li>
<li>methods: 本文提出了一个框架，以帮助研究者在实验数据中操作抽象概念的对齐方面达成共识。这个框架包括三个级别：首先确定重要的模型行为维度，然后对这些维度进行定义和归类，并由谁进行这种归类。</li>
<li>results: 本文通过这个框架，提供了一种透明和批判性评估的方法，以帮助社区在对 LLM 与人类 популяции进行对齐时，avigate复杂的对齐过程。<details>
<summary>Abstract</summary>
In this paper, we address the concept of "alignment" in large language models (LLMs) through the lens of post-structuralist socio-political theory, specifically examining its parallels to empty signifiers. To establish a shared vocabulary around how abstract concepts of alignment are operationalised in empirical datasets, we propose a framework that demarcates: 1) which dimensions of model behaviour are considered important, then 2) how meanings and definitions are ascribed to these dimensions, and by whom. We situate existing empirical literature and provide guidance on deciding which paradigm to follow. Through this framework, we aim to foster a culture of transparency and critical evaluation, aiding the community in navigating the complexities of aligning LLMs with human populations.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们通过后结构主义社会政治理论的镜像，特别是空符号的概念，探讨LLMs中的“对齐”概念。为建立对大数据集中抽象概念的操作化的共同词汇，我们提出了一个框架，它包括：1）对模型行为中考虑重要的维度，然后2）对这些维度的含义和定义如何被赋予，以及谁将这些含义和定义塑造成为模型。我们将现有的实证文献综述，并提供指南，以帮助社区选择遵循哪种 парадигмы。通过这个框架，我们希望激发公共透明和批判性评估的文化，以便在对LMMs与人类人口进行对齐时， navigating complexity。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Adjustment-of-Confounding-by-Provenance-for-Robust-Text-Classification-of-Multi-institutional-Clinical-Notes"><a href="#Backdoor-Adjustment-of-Confounding-by-Provenance-for-Robust-Text-Classification-of-Multi-institutional-Clinical-Notes" class="headerlink" title="Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes"></a>Backdoor Adjustment of Confounding by Provenance for Robust Text Classification of Multi-institutional Clinical Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02451">http://arxiv.org/abs/2310.02451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiruo Ding, Zhecheng Sheng, Meliha Yetişgen, Serguei Pakhomov, Trevor Cohen</li>
<li>for: 这项研究是为了提高临床自然语言处理（NLP）的性能。</li>
<li>methods: 这项研究使用机器学习和深度学习方法来改进临床NLP的性能。</li>
<li>results: 研究发现，使用后门调整可以有效地缓解由来源的数据分布差异引起的混合shift问题，并提高模型的Robustness。<details>
<summary>Abstract</summary>
Natural Language Processing (NLP) methods have been broadly applied to clinical tasks. Machine learning and deep learning approaches have been used to improve the performance of clinical NLP. However, these approaches require sufficiently large datasets for training, and trained models have been shown to transfer poorly across sites. These issues have led to the promotion of data collection and integration across different institutions for accurate and portable models. However, this can introduce a form of bias called confounding by provenance. When source-specific data distributions differ at deployment, this may harm model performance. To address this issue, we evaluate the utility of backdoor adjustment for text classification in a multi-site dataset of clinical notes annotated for mentions of substance abuse. Using an evaluation framework devised to measure robustness to distributional shifts, we assess the utility of backdoor adjustment. Our results indicate that backdoor adjustment can effectively mitigate for confounding shift.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）技术已广泛应用于医疗任务中。机器学习和深度学习方法已经用于提高临床NLP性能。然而，这些方法需要训练数据量足够大，而已训练的模型在不同场景下转移性差。这些问题导致了数据收集和集成的促进，以确保准确和可移植的模型。然而，这可能导致一种偏见called隐藏偏见。当数据分布不同在部署时，这可能影响模型性能。为解决这个问题，我们评估了在多地点数据集中的临床笔记中提取substance滥用的文本分类 task中使用后门调整的 utility。使用我们设计的鲁棒性评估框架，我们评估了后门调整的使用。我们的结果表明，后门调整可以有效地抵消隐藏偏见的影响。
</details></li>
</ul>
<hr>
<h2 id="Novice-Learner-and-Expert-Tutor-Evaluating-Math-Reasoning-Abilities-of-Large-Language-Models-with-Misconceptions"><a href="#Novice-Learner-and-Expert-Tutor-Evaluating-Math-Reasoning-Abilities-of-Large-Language-Models-with-Misconceptions" class="headerlink" title="Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions"></a>Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02439">http://arxiv.org/abs/2310.02439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naiming Liu, Shashank Sonkar, Zichao Wang, Simon Woodhead, Richard G. Baraniuk</li>
<li>for: 本研究旨在evaluate Large Language Models (LLMs) 的数学理解能力，通过 simulate LLMs 作为 novice learner 和 expert tutor，并通过问题的不准确答案来找到特定的数学误解。</li>
<li>methods: 我们的主要方法是 simulate LLMs 作为 novice learner 和 expert tutor，并使用 grade-school math problems 进行实验。</li>
<li>results: 我们的实验表明， LLMS 可以轻松地回答这些问题，但它们很难认出特定的数学误解和相应的误解。这些结果提供了新的机会，以提高 LLMS 的数学理解能力，特别是在开发智能教学系统的应用中。<details>
<summary>Abstract</summary>
We propose novel evaluations for mathematical reasoning capabilities of Large Language Models (LLMs) based on mathematical misconceptions. Our primary approach is to simulate LLMs as a novice learner and an expert tutor, aiming to identify the incorrect answer to math question resulted from a specific misconception and to recognize the misconception(s) behind an incorrect answer, respectively. Contrary to traditional LLMs-based mathematical evaluations that focus on answering math questions correctly, our approach takes inspirations from principles in educational learning sciences. We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question. Using simple grade-school math problems, our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers. Our study indicates new opportunities for enhancing LLMs' math reasoning capabilities, especially on developing robust student simulation and expert tutoring models in the educational applications such as intelligent tutoring systems.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的评估方法，用于评估语言模型（LLM）的数学理解能力，基于数学误解。我们的主要方法是模拟LLM作为新手学生和专家教师，以识别特定误解导致的错误答案，并识别误解。与传统的LLMs-based数学评估方法不同，我们的方法从教育学原则出发，Explicitly要求LLM答题时模拟新手学生的 incomplete 知识基础，并模拟专家教师的误解识别能力。使用Primary school 数学问题，我们的实验表明，虽然LLM可以轻松地回答这些问题，但它们困难于识别1）特定误解对应的错误答案；2）误解所解释的特定错误答案。我们的研究表明，可以通过开发Robust 学生模拟和专家指导模型来增强LLM的数学理解能力，特别是在教育应用程序中，如智能教学系统。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Provide-Security-Privacy-Advice-Measuring-the-Ability-of-LLMs-to-Refute-Misconceptions"><a href="#Can-Large-Language-Models-Provide-Security-Privacy-Advice-Measuring-the-Ability-of-LLMs-to-Refute-Misconceptions" class="headerlink" title="Can Large Language Models Provide Security &amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions"></a>Can Large Language Models Provide Security &amp; Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02431">http://arxiv.org/abs/2310.02431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/purseclab/llm_security_privacy_advice">https://github.com/purseclab/llm_security_privacy_advice</a></li>
<li>paper_authors: Yufan Chen, Arjun Arunasalam, Z. Berkay Celik</li>
<li>for:  This paper aims to measure the ability of Large Language Models (LLMs) to provide reliable security and privacy (S&amp;P) advice by refuting popular S&amp;P misconceptions.</li>
<li>methods: The authors use two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to S&amp;P misconceptions. They also apply three strategies to comprehensively evaluate the responses: querying each misconception multiple times, generating and querying paraphrases, and soliciting source URLs of the responses.</li>
<li>results: The authors find that both LLMs demonstrate a non-negligible error rate (21.3% on average) in supporting popular S&amp;P misconceptions, with the error rate increasing when the same or paraphrased misconceptions are repeatedly queried. Additionally, the models may partially support a misconception or remain noncommittal, and they may provide invalid URLs or point to unrelated sources.<details>
<summary>Abstract</summary>
Users seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate increases to 32.6% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).
</details>
<details>
<summary>摘要</summary>
用户寻求安全与隐私（S&P）建议从在线资源中，包括可靠的网站和内容分享平台。这些资源帮助用户理解S&P技术和工具，并提供可行的策略。大型自然语言模型（LLM）最近在信息领域上崛起，成为用户信任的信息源。然而，其准确性和正确性受到质疑。先前的研究表明LLMs在回答多选题目时存在缺陷，用户可能会意外绕过模型限制（例如，生成恶意内容）。然而，LLMs是否可以提供可靠的S&P建议尚不彻底探讨。在这篇论文中，我们测量了它们能否推翻公众对S&P相关误区的认知。我们首先遍读最新的学术文献，并从六个不同的话题中筛选出超过一百个S&P相关的误区。然后，我们对两个流行的LLM（Bard和ChatGPT）进行查询，并开发了评估响应的标准化指南。为了全面评估它们的响应，我们还应用了三种策略：每个误区多次查询，生成并查询它们的重叠，以及寻求响应的源URL。两个模型的平均错误率为21.3%，错误地支持公众对S&P相关误区的认知。错误率随着重复查询误区而增加至32.6%。我们还发现，模型可能会部分支持误区，或者拒绝发表明确的看法。我们探索了LLMs提供的信息来源，发现它们可能会提供无效的URL（Bard的21.2%和ChatGPT的67.7%）或者指向无关的源（Bard返回的44.2%和ChatGPT返回的18.3%）。
</details></li>
</ul>
<hr>
<h2 id="Mixture-of-Quantized-Experts-MoQE-Complementary-Effect-of-Low-bit-Quantization-and-Robustness"><a href="#Mixture-of-Quantized-Experts-MoQE-Complementary-Effect-of-Low-bit-Quantization-and-Robustness" class="headerlink" title="Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness"></a>Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02410">http://arxiv.org/abs/2310.02410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young Jin Kim, Raffy Fahim, Hany Hassan Awadalla</li>
<li>for: 提高语言任务的模型质量，并解决大型混合专家模型（MoE）所带来的内存浪费和带宽瓶颈问题。</li>
<li>methods: 提出了一种简单的量化方法——量化专家权重（MoQE），通过对专家权重进行2位量化来降低内存占用和延迟问题。</li>
<li>results: 研究表明，在大多数情况下，使用2位量化的专家层可以提供可靠的模型性能，同时减少内存尺寸，并且不需要额外训练。此外，专家层在MoE模型中比普通的批量网络层更强劲对量化。<details>
<summary>Abstract</summary>
Large Mixture of Experts (MoE) models could achieve state-of-the-art quality on various language tasks, including machine translation task, thanks to the efficient model scaling capability with expert parallelism. However, it has brought a fundamental issue of larger memory consumption and increased memory bandwidth bottleneck at deployment time. In this paper, we propose Mixture of Quantized Experts (MoQE) which is a simple weight-only quantization method applying ultra low-bit down to 2-bit quantizations only to expert weights for mitigating the increased memory and latency issues of MoE models. We show that low-bit quantization together with the MoE architecture delivers a reliable model performance while reducing the memory size significantly even without any additional training in most cases. In particular, expert layers in MoE models are much more robust to the quantization than conventional feedforward networks (FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit expert weights can deliver better model performance than the dense model trained on the same dataset. As a result of low-bit quantization, we show the model size can be reduced by 79.6% of the original half precision floating point (fp16) MoE model. Combined with an optimized GPU runtime implementation, it also achieves 1.24X speed-up on A100 GPUs.
</details>
<details>
<summary>摘要</summary>
大型混合专家（MoE）模型可以实现不同语言任务的状态级质量，包括机器翻译任务，凭借专家并行化的高效模型扩展能力。然而，这带来了更大的内存消耗和增强的内存带宽瓶颈问题在部署时。在本文中，我们提出了混合量化专家（MoQE），它是一种简单的只有质量化weight的方法，通过ultra低位数量质化专家weight来缓解MoE模型中的内存和延迟问题。我们显示，低位质量化与MoE架构结合可以提供可靠的模型性能，同时减少内存大小，无需额外训练。具体来说，专家层在MoE模型中比普通的Feed Forward Networks（FFN）层更抵抗质量化。在我们的全面分析中，我们表明MoE模型的2位专家weight可以提供比 dense模型在同一个数据集上训练的更好的模型性能。由于低位质量化，我们显示MoE模型的模型大小可以减少79.6%。结合优化的GPU运行时实现，它还实现了A100 GPU上的1.24倍速度提升。
</details></li>
</ul>
<hr>
<h2 id="MindTheDApp-A-Toolchain-for-Complex-Network-Driven-Structural-Analysis-of-Ethereum-based-Decentralised-Applications"><a href="#MindTheDApp-A-Toolchain-for-Complex-Network-Driven-Structural-Analysis-of-Ethereum-based-Decentralised-Applications" class="headerlink" title="MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis of Ethereum-based Decentralised Applications"></a>MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis of Ethereum-based Decentralised Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02408">http://arxiv.org/abs/2310.02408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giacomo Ibba, Sabrina Aufiero, Silvia Bartolucci, Rumyana Neykova, Marco Ortu, Roberto Tonelli, Giuseppe Destefanis</li>
<li>for: 这篇论文是为了研究区块链技术中的智能合约和分布式应用程序（DApp）的结构分析而设计的工具链。</li>
<li>methods: 该工具链使用ANTLR4和抽象树（AST）旋转技术将智能合约的架构和交互转化为特殊的两个集群图。</li>
<li>results: 该图包括两个集群的节点：一个表示智能合约、接口和库，另一个包括函数、事件和修饰符。边在图中连接函数和智能合约，提供细节的交互和执行流视图，帮助研究人员和实践者更深入地理解分布式系统的稳定性、适应性和复杂性。<details>
<summary>Abstract</summary>
This paper presents MindTheDApp, a toolchain designed specifically for the structural analysis of Ethereum-based Decentralized Applications (DApps), with a distinct focus on a complex network-driven approach. Unlike existing tools, our toolchain combines the power of ANTLR4 and Abstract Syntax Tree (AST) traversal techniques to transform the architecture and interactions within smart contracts into a specialized bipartite graph. This enables advanced network analytics to highlight operational efficiencies within the DApp's architecture.   The bipartite graph generated by the proposed tool comprises two sets of nodes: one representing smart contracts, interfaces, and libraries, and the other including functions, events, and modifiers. Edges in the graph connect functions to smart contracts they interact with, offering a granular view of interdependencies and execution flow within the DApp. This network-centric approach allows researchers and practitioners to apply complex network theory in understanding the robustness, adaptability, and intricacies of decentralized systems.   Our work contributes to the enhancement of security in smart contracts by allowing the visualisation of the network, and it provides a deep understanding of the architecture and operational logic within DApps. Given the growing importance of smart contracts in the blockchain ecosystem and the emerging application of complex network theory in technology, our toolchain offers a timely contribution to both academic research and practical applications in the field of blockchain technology.
</details>
<details>
<summary>摘要</summary>
The bipartite graph consists of two sets of nodes: one representing smart contracts, interfaces, and libraries, and the other including functions, events, and modifiers. Edges in the graph connect functions to smart contracts they interact with, providing a detailed view of interdependencies and execution flow within the DApp. This network-centric approach allows researchers and practitioners to apply complex network theory to understand the robustness, adaptability, and intricacies of decentralized systems.Our work enhances the security of smart contracts by providing a visual representation of the network and deepening understanding of the architecture and operational logic within DApps. With the growing importance of smart contracts in the blockchain ecosystem and the emerging application of complex network theory in technology, our toolchain offers a timely contribution to both academic research and practical applications in the field of blockchain technology.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Speech-Recognition-with-N-Skipgram-and-Positional-Unigram-Matching"><a href="#Unsupervised-Speech-Recognition-with-N-Skipgram-and-Positional-Unigram-Matching" class="headerlink" title="Unsupervised Speech Recognition with N-Skipgram and Positional Unigram Matching"></a>Unsupervised Speech Recognition with N-Skipgram and Positional Unigram Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02382">http://arxiv.org/abs/2310.02382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lwang114/graphunsupasr">https://github.com/lwang114/graphunsupasr</a></li>
<li>paper_authors: Liming Wang, Mark Hasegawa-Johnson, Chang D. Yoo</li>
<li>for:  trains unsupervised speech recognition systems</li>
<li>methods:  combines lower-order N-skipgrams and positional unigram statistics</li>
<li>results:  competitive performance in ASR and phoneme segmentation tasks<details>
<summary>Abstract</summary>
Training unsupervised speech recognition systems presents challenges due to GAN-associated instability, misalignment between speech and text, and significant memory demands. To tackle these challenges, we introduce a novel ASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams (up to N=3) combined with positional unigram statistics gathered from a small batch of samples. Evaluated on the TIMIT benchmark, our model showcases competitive performance in ASR and phoneme segmentation tasks. Access our publicly available code at https://github.com/lwang114/GraphUnsupASR.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_text="Training unsupervised speech recognition systems presents challenges due to GAN-associated instability, misalignment between speech and text, and significant memory demands. To tackle these challenges, we introduce a novel ASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams (up to N=3) combined with positional unigram statistics gathered from a small batch of samples. Evaluated on the TIMIT benchmark, our model showcases competitive performance in ASR and phoneme segmentation tasks. Access our publicly available code at https://github.com/lwang114/GraphUnsupASR."</SYS>Here's the translation in Simplified Chinese:训练无监督语音识别系统存在 Gan 相关的不稳定性、语音和文本的不一致性以及巨大的内存需求。为解决这些挑战，我们提出了一种新的 ASR 系统，即 ESPUM。这个系统利用了 N-skipgram 的低阶（最多 N=3）以及一小批样的位置统计来充分利用语音识别和phoneme 分割任务的能力。在 TIMIT  bencmark 上评估，我们的模型在 ASR 和 phoneme 分割任务中显示了竞争性的性能。可以通过https://github.com/lwang114/GraphUnsupASR 访问我们公开的代码。
</details></li>
</ul>
<hr>
<h2 id="Conversational-Health-Agents-A-Personalized-LLM-Powered-Agent-Framework"><a href="#Conversational-Health-Agents-A-Personalized-LLM-Powered-Agent-Framework" class="headerlink" title="Conversational Health Agents: A Personalized LLM-Powered Agent Framework"></a>Conversational Health Agents: A Personalized LLM-Powered Agent Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02374">http://arxiv.org/abs/2310.02374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahyar Abbasian, Iman Azimi, Amir M. Rahmani, Ramesh Jain</li>
<li>for: 这篇论文的目的是提高个人健康服务的个性化响应，使用语言模型为谈话启用更多的功能。</li>
<li>methods: 该论文提出了一个基于语言模型的框架，以便让健康谈话代理人（CHAs）能够处理复杂的健康问题，包括访问个人用户健康数据、 integrate 最新的健康发现、和与多种数据分析工具交互。</li>
<li>results: 通过一个实验研究，论文表明了该框架在处理压力水平估计任务中的能力，展示了代理人的认知和操作能力。<details>
<summary>Abstract</summary>
Conversational Health Agents (CHAs) are interactive systems designed to enhance personal healthcare services by engaging in empathetic conversations and processing multimodal data. While current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation, they often need more comprehensive agent capabilities. This limitation includes accessing personal user health data from wearables, ubiquitous data collection sources, and electronic health records, integrating the latest published health insights, and connecting with established multimodal data analysis tools. In this paper, we propose an LLM-powered framework to empower CHAs to generate a personalized response for users' healthcare queries. This framework provides critical thinking, knowledge acquisition, and problem-solving abilities by integrating healthcare data sources, enabling multilingual and multimodal conversations, and interacting with various user data analysis tools. We illustrate the framework's proficiency in handling complex healthcare tasks via a case study on stress level estimation, showcasing the agent's cognitive and operational capabilities.
</details>
<details>
<summary>摘要</summary>
对话健康助手（CHA）是一种互动系统，旨在提高个人医疗服务的质量，通过互动式对话和识别多种数据来提供更好的护理。现有的CHA，特别是使用大型自然语言模型（LLM），通常只专注于对话，它们需要更具全面的代理能力。这些限制包括访问用户穿戴设备上的个人健康数据、 ubique 数据收集源、电子健康记录等，整合最新的发表在医学期刊上的健康发现，以及与已有的多种数据分析工具集成。在这篇论文中，我们提出一个基于 LLM 的框架，以帮助 CHA 为用户的医疗问题提供个性化的回答。这个框架具有 kritical thinking、知识获取和解决问题的能力，通过结合医疗数据源、支持多语言多模式对话、与多种用户数据分析工具集成。我们通过压力水平估计的案例研究，展示了代理的认知和运作能力。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Long-Horizon-Manipulations-with-Large-Language-Models"><a href="#Generalizable-Long-Horizon-Manipulations-with-Large-Language-Models" class="headerlink" title="Generalizable Long-Horizon Manipulations with Large Language Models"></a>Generalizable Long-Horizon Manipulations with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02264">http://arxiv.org/abs/2310.02264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Zhou, Mingyu Ding, Weikun Peng, Masayoshi Tomizuka, Lin Shao, Chuang Gan</li>
<li>for: 本研究利用大型自然语言模型（LLM）生成普遍可应用的初级任务条件，用于实现长期满足新物品和未知任务的机器人 manipulate 任务。</li>
<li>methods: 本研究使用 LLM 生成和调整动态运动 primitives（DMP）轨迹，以便在长期任务执行中实现高精度和稳定性。</li>
<li>results: 实验表明，本研究在 simulated 和实际环境中都能够有效地应用于新物品和相关任务，highlighting LLM 在机器人系统的 universality 和适应性。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This work introduces a framework harnessing the capabilities of Large Language Models (LLMs) to generate primitive task conditions for generalizable long-horizon manipulations with novel objects and unseen tasks. These task conditions serve as guides for the generation and adjustment of Dynamic Movement Primitives (DMP) trajectories for long-horizon task execution. We further create a challenging robotic manipulation task suite based on Pybullet for long-horizon task evaluation. Extensive experiments in both simulated and real-world environments demonstrate the effectiveness of our framework on both familiar tasks involving new objects and novel but related tasks, highlighting the potential of LLMs in enhancing robotic system versatility and adaptability. Project website: https://object814.github.io/Task-Condition-With-LLM/
</details>
<details>
<summary>摘要</summary>
这个研究框架利用大语言模型（LLM）来生成普适的任务条件，用于执行长期任务（long-horizon task），包括使用新物品和未看过的任务。这些任务条件作为指导，用于生成和调整动态运动 primitives（DMP）的轨迹，以便实现长期任务执行。我们还创建了一个复杂的机器人操作任务集，基于Pybullet，用于长期任务评估。实验表明，我们的框架在真实世界和模拟环境中具有很高的效果，能够在不熟悉的任务和新物品上提高机器人系统的多样性和适应力。项目网站：https://object814.github.io/Task-Condition-With-LLM/
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Pre-Trained-Sentence-Transformers-for-Offensive-Language-Detection-in-Indian-Languages"><a href="#Harnessing-Pre-Trained-Sentence-Transformers-for-Offensive-Language-Detection-in-Indian-Languages" class="headerlink" title="Harnessing Pre-Trained Sentence Transformers for Offensive Language Detection in Indian Languages"></a>Harnessing Pre-Trained Sentence Transformers for Offensive Language Detection in Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02249">http://arxiv.org/abs/2310.02249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananya Joshi, Raviraj Joshi</li>
<li>for: 防止仇恨言语和不良内容在社交媒体平台上迅速扩散</li>
<li>methods: 使用 pré-训练的 BERT 和 SBERT 模型，在三种低资源语言（孟加拉语、阿萨姆语和孔雀语）中进行文本分类，判断推文是否包含仇恨言语</li>
<li>results: 发现单语言句子 BERT 模型在孟加拉语中表现最佳，但在阿萨姆语和孔雀语中仍有待提高的机会<details>
<summary>Abstract</summary>
In our increasingly interconnected digital world, social media platforms have emerged as powerful channels for the dissemination of hate speech and offensive content. This work delves into the domain of hate speech detection, placing specific emphasis on three low-resource Indian languages: Bengali, Assamese, and Gujarati. The challenge is framed as a text classification task, aimed at discerning whether a tweet contains offensive or non-offensive content. Leveraging the HASOC 2023 datasets, we fine-tuned pre-trained BERT and SBERT models to evaluate their effectiveness in identifying hate speech. Our findings underscore the superiority of monolingual sentence-BERT models, particularly in the Bengali language, where we achieved the highest ranking. However, the performance in Assamese and Gujarati languages signifies ongoing opportunities for enhancement. Our goal is to foster inclusive online spaces by countering hate speech proliferation.
</details>
<details>
<summary>摘要</summary>
在我们日益连接的数字世界中，社交媒体平台已经成为蔑视言论和不当内容的广泛传播渠道。这项工作探索了蔑视言论检测领域，特别是关注三种低资源的印度语言：孟加拉语、阿萨姆语和 гуджа拉提语。我们将这项工作定义为文本分类任务，目的是判断推文是否包含蔑视或非蔑视内容。我们使用了HASOC 2023 数据集，精度地练习了预训练的 BERT 和 SBERT 模型，以评估它们在蔑视言论检测方面的效果。我们的发现表明，单语言句子 BERT 模型在孟加拉语中表现最佳，而在阿萨姆语和 гуджа拉提语中，还有很多机会进行改进。我们的目标是创造包容的在线空间，以抵消蔑视言论的扩散。
</details></li>
</ul>
<hr>
<h2 id="Can-Language-Models-be-Instructed-to-Protect-Personal-Information"><a href="#Can-Language-Models-be-Instructed-to-Protect-Personal-Information" class="headerlink" title="Can Language Models be Instructed to Protect Personal Information?"></a>Can Language Models be Instructed to Protect Personal Information?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02224">http://arxiv.org/abs/2310.02224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ethanm88/llm-access-control">https://github.com/ethanm88/llm-access-control</a></li>
<li>paper_authors: Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter</li>
<li>for: 本研究旨在评估multimodal语言模型中的隐私保护和实用性之间的贸易关系，并提出 PrivQA 模拟方案来评估这种贸易关系。</li>
<li>methods: 本研究使用了一种名为 PrivQA 的多模态测试 benchmark，并提出了一种基于自我调节的回答技术来提高隐私保护。</li>
<li>results: 通过一系列的红队攻击实验，研究人员发现了一些简单的破坏攻击方法，这些方法可以通过文本和&#x2F;或图像输入绕过 PrivQA 中的隐私保护措施。<details>
<summary>Abstract</summary>
Large multimodal language models have proven transformative in numerous applications. However, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. While data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. We also propose a technique to iteratively self-moderate responses, which significantly improves privacy. However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. We believe PrivQA has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. We release the entire PrivQA dataset at https://llm-access-control.github.io/.
</details>
<details>
<summary>摘要</summary>
大型多modal语言模型在多个应用程序中证明了转型的作用。然而，这些模型被证明可以记忆和泄露预训练数据，从而引起用户隐私和信息安全的严重问题。虽然应避免数据泄露，但也需要考虑提出的方法中隐私保护和模型实用之间的贸易OFF。在这篇论文中，我们介绍了 PrivQA -- 一个多modal benchmark，用于评估在指定个人信息类别的保护情况下，模型的隐私/实用贸易OFF。我们还提出了一种反复自我修饰回应的技术，可以明显提高隐私。然而，通过一系列的红牛实验，我们发现，攻击者可以通过简单的监狱破解方法，通过文本和/或图像输入，轻松绕过这些保护措施。我们认为 PrivQA 可以支持新的隐私保护模型的开发，以及这些保护措施的攻击者鲁棒性。我们在 <https://llm-access-control.github.io/> 上发布了整个 PrivQA 数据集。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Meet-Knowledge-Graphs-to-Answer-Factoid-Questions"><a href="#Large-Language-Models-Meet-Knowledge-Graphs-to-Answer-Factoid-Questions" class="headerlink" title="Large Language Models Meet Knowledge Graphs to Answer Factoid Questions"></a>Large Language Models Meet Knowledge Graphs to Answer Factoid Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02166">http://arxiv.org/abs/2310.02166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Salnikov, Hai Le, Prateek Rajput, Irina Nikishina, Pavel Braslavski, Valentin Malykh, Alexander Panchenko</li>
<li>for: 这 paper 的目的是提高 Text-to-Text 语言模型在Answering factoid questions 中的表现。</li>
<li>methods: 该 paper 使用 Knowledge Graph 中的 subgraphs 抽取算法和 Transformer-based 模型来提取有关问题和答案的信息，并通过 linearization 将其转换为可读取的信息。</li>
<li>results: 根据这 paper 的实验结果，使用这种方法可以提高 pre-trained Text-to-Text 语言模型的 Hits@1 分数 by 4-6%。<details>
<summary>Abstract</summary>
Recently, it has been shown that the incorporation of structured knowledge into Large Language Models significantly improves the results for a variety of NLP tasks. In this paper, we propose a method for exploring pre-trained Text-to-Text Language Models enriched with additional information from Knowledge Graphs for answering factoid questions. More specifically, we propose an algorithm for subgraphs extraction from a Knowledge Graph based on question entities and answer candidates. Then, we procure easily interpreted information with Transformer-based models through the linearization of the extracted subgraphs. Final re-ranking of the answer candidates with the extracted information boosts Hits@1 scores of the pre-trained text-to-text language models by 4-6%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Instance-Needs-More-Care-Rewriting-Prompts-for-Instances-Yields-Better-Zero-Shot-Performance"><a href="#Instance-Needs-More-Care-Rewriting-Prompts-for-Instances-Yields-Better-Zero-Shot-Performance" class="headerlink" title="Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance"></a>Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02107">http://arxiv.org/abs/2310.02107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salokr/promptd">https://github.com/salokr/promptd</a></li>
<li>paper_authors: Saurabh Srivastava, Chengyue Huang, Weiguo Fan, Ziyu Yao</li>
<li>for: 提高大型自然语言模型（LLM）在零shot任务上的表现，以实现更好的任务泛化和劳动资源节省。</li>
<li>methods: 提出一种名为PRoMPTd的方法，通过对每个测试输入重新编写任务提示，以提供更加特定、不ambiguous和完整的指导，以便LLM在零shot情况下正确解决测试任务。</li>
<li>results: 对八个数据集进行了测试，包括代数、逻辑推理和代码生成等任务，使用GPT-4作为任务LLM，并获得了相对于传统零shot方法的约10%的绝对改进和5%的相对改进。此外，还证明了 rewrite prompt 可以提供更好的解释如何使LLM解决每个测试输入，这可能可以作为对 adversarial prompting 的防御机制。<details>
<summary>Abstract</summary>
Enabling large language models (LLMs) to perform tasks in zero-shot has been an appealing goal owing to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., ``let's think step by step'' ). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute improvement of around 10% on the complex MATH dataset and 5% on the code generation task on HumanEval, outperforming conventional zero-shot methods. In addition, we also showed that the rewritten prompt can provide better interpretability of how the LLM resolves each test instance, which can potentially be leveraged as a defense mechanism against adversarial prompting. The source code and dataset can be obtained from https://github.com/salokr/PRoMPTd
</details>
<details>
<summary>摘要</summary>
Enable Large Language Models (LLMs) to perform tasks in zero-shot has been an appealing goal due to its labor-saving (i.e., requiring no task-specific annotations); as such, zero-shot prompting approaches also enjoy better task generalizability. To improve LLMs' zero-shot performance, prior work has focused on devising more effective task instructions (e.g., "let's think step by step"). However, we argue that, in order for an LLM to solve them correctly in zero-shot, individual test instances need more carefully designed and customized instructions. To this end, we propose PRoMPTd, an approach that rewrites the task prompt for each individual test input to be more specific, unambiguous, and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd on eight datasets covering tasks including arithmetics, logical reasoning, and code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute improvement of around 10% on the complex MATH dataset and 5% on the code generation task on HumanEval, outperforming conventional zero-shot methods. In addition, we also showed that the rewritten prompt can provide better interpretability of how the LLM resolves each test instance, which can potentially be leveraged as a defense mechanism against adversarial prompting. The source code and dataset can be obtained from <https://github.com/salokr/PRoMPTd>.
</details></li>
</ul>
<hr>
<h2 id="Controlling-Topic-Focus-Articulation-in-Meaning-to-Text-Generation-using-Graph-Neural-Networks"><a href="#Controlling-Topic-Focus-Articulation-in-Meaning-to-Text-Generation-using-Graph-Neural-Networks" class="headerlink" title="Controlling Topic-Focus Articulation in Meaning-to-Text Generation using Graph Neural Networks"></a>Controlling Topic-Focus Articulation in Meaning-to-Text Generation using Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02053">http://arxiv.org/abs/2310.02053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunliu Wang, Rik van Noord, Johan Bos</li>
<li>for: 本研究旨在找到控制主题强调表达方式，从意义表示中提取话题信息，使自然语言生成系统生成文本时能够更好地控制语言结构。</li>
<li>methods: 本研究使用图 neural network 模型，因为图模型没有显式的单词顺序信息，可以更好地捕捉意义的含义。研究提出了三种不同的主题强调抽象策略，并使用深度优先搜索来学习节点表示。</li>
<li>results: 研究结果显示，使用深度优先搜索来学习节点表示可以获得与当前状态的比较竞争性能，并且在主题强调转换任务中具有显著改善。不同的主题强调策略可以对图模型的性能产生很大的影响。<details>
<summary>Abstract</summary>
A bare meaning representation can be expressed in various ways using natural language, depending on how the information is structured on the surface level. We are interested in finding ways to control topic-focus articulation when generating text from meaning. We focus on distinguishing active and passive voice for sentences with transitive verbs. The idea is to add pragmatic information such as topic to the meaning representation, thereby forcing either active or passive voice when given to a natural language generation system. We use graph neural models because there is no explicit information about word order in a meaning represented by a graph. We try three different methods for topic-focus articulation (TFA) employing graph neural models for a meaning-to-text generation task. We propose a novel encoding strategy about node aggregation in graph neural models, which instead of traditional encoding by aggregating adjacent node information, learns node representations by using depth-first search. The results show our approach can get competitive performance with state-of-art graph models on general text generation, and lead to significant improvements on the task of active-passive conversion compared to traditional adjacency-based aggregation strategies. Different types of TFA can have a huge impact on the performance of the graph models.
</details>
<details>
<summary>摘要</summary>
表示的意义可以通过自然语言的不同表达方式来表达，具体取决于表面上信息的结构。我们关注在生成文本时控制话题焦点落实的方法。我们使用图 neural network，因为图表示的意义没有显式的单词顺序信息。我们尝试了三种不同的话题焦点落实（TFA）方法，使用图 neural network 进行意义到文本生成任务。我们提出了一种新的节点聚合编码策略，而不是传统的邻居信息汇集编码策略，通过深度优先搜索来学习节点表示。结果显示，我们的方法可以与现有的图模型达到竞争性性能，并在活动Passive转换任务上得到显著提高，比传统邻居汇集策略更好。不同的TFA类型可以对图模型的性能产生巨大的影响。
</details></li>
</ul>
<hr>
<h2 id="Tuning-Large-language-model-for-End-to-end-Speech-Translation"><a href="#Tuning-Large-language-model-for-End-to-end-Speech-Translation" class="headerlink" title="Tuning Large language model for End-to-end Speech Translation"></a>Tuning Large language model for End-to-end Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02050">http://arxiv.org/abs/2310.02050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Nianwen Si, Yaqi Chen, Wenlin Zhang, Xukui Yang, Dan Qu, Xiaolin Jiao</li>
<li>for: 这个论文主要是为了提高多Modal模型在端到端语音翻译（E2E-ST）任务中的表现。</li>
<li>methods: 这个论文使用了一个名为LST的大型多Modal模型，包括一个语音前端、一个适配器和一个LLM后端。训练LST包括两个阶段：模态调整和下游任务练习。在模态调整阶段，适配器被调整以将语音表示空间与文本嵌入空间对齐。在下游任务练习阶段，适配器和LLM模型都被训练以优化E2EST任务的表现。</li>
<li>results: 实验结果表明，LST-13B在MuST-C语音翻译benchmark上的BLEU分数为30.39&#x2F;41.55&#x2F;35.33（En-De&#x2F;En-Fr&#x2F;En-Es语言对），超过了之前的模型，创造了新的state-of-the-art。此外，我们还进行了单modal模型选择和训练策略的深入分析，为未来的研究提供了基础。我们将在审核后开源代码和模型。<details>
<summary>Abstract</summary>
With the emergence of large language models (LLMs), multimodal models based on LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM, and SpeechGPT exhibit an impressive ability to comprehend and generate human instructions. However, their performance often falters when faced with complex tasks like end-to-end speech translation (E2E-ST), a cross-language and cross-modal translation task. In comparison to single-modal models, multimodal models lag behind in these scenarios. This paper introduces LST, a Large multimodal model designed to excel at the E2E-ST task. LST consists of a speech frontend, an adapter, and a LLM backend. The training of LST consists of two stages: (1) Modality adjustment, where the adapter is tuned to align speech representation with text embedding space, and (2) Downstream task fine-tuning, where both the adapter and LLM model are trained to optimize performance on the E2EST task. Experimental results on the MuST-C speech translation benchmark demonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on En-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a new state-of-the-art. Additionally, we conduct an in-depth analysis of single-modal model selection and the impact of training strategies, which lays the foundation for future research. We will open up our code and models after review.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的出现，基于LLM的多modal模型在多种任务上表现出了很大的潜力。例如LLaSM、X-LLM和SpeechGPT等模型能够很好地理解和生成人类指令。然而，当面临复杂任务时，如端到端语音翻译（E2E-ST）时，这些模型表现不如单Modal模型。这篇论文介绍了LST，一个大的多modal模型，用于超越E2E-ST任务。LST包括一个语音前端、一个适配器和一个LLM后端。LST的训练过程包括两个阶段：（1）Modal调整， где适配器被调整以将语音表示与文本嵌入空间对齐，以及（2）下游任务练习， donde Both the adapter and LLM model are trained to optimize performance on the E2EST task。实验结果表明，LST-13B在MuST-C语音翻译benchmark上的BLEU分数为30.39/41.55/35.33，超过了之前的模型，并设立了新的状态图。此外，我们还进行了单Modal模型选择和训练策略的深入分析，这些研究 laid the foundation for future research。我们将在审核后开放代码和模型。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Evaluation-Framework-Best-Practices-for-Human-Evaluation"><a href="#Hierarchical-Evaluation-Framework-Best-Practices-for-Human-Evaluation" class="headerlink" title="Hierarchical Evaluation Framework: Best Practices for Human Evaluation"></a>Hierarchical Evaluation Framework: Best Practices for Human Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01917">http://arxiv.org/abs/2310.01917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iva Bojic, Jessica Chen, Si Yuan Chang, Qi Chwen Ong, Shafiq Joty, Josip Car</li>
<li>for: 评估自然语言处理（NLP）系统的质量和相关性。</li>
<li>methods: 基于现有文献的分析和自己开发的层次评估框架。</li>
<li>results: 评估Machine Reading Comprehension系统的表现，发现输入质量和输出相关性的关系，并且指出了评估输入和输出两个组件的重要性。<details>
<summary>Abstract</summary>
Human evaluation plays a crucial role in Natural Language Processing (NLP) as it assesses the quality and relevance of developed systems, thereby facilitating their enhancement. However, the absence of widely accepted human evaluation metrics in NLP hampers fair comparisons among different systems and the establishment of universal assessment standards. Through an extensive analysis of existing literature on human evaluation metrics, we identified several gaps in NLP evaluation methodologies. These gaps served as motivation for developing our own hierarchical evaluation framework. The proposed framework offers notable advantages, particularly in providing a more comprehensive representation of the NLP system's performance. We applied this framework to evaluate the developed Machine Reading Comprehension system, which was utilized within a human-AI symbiosis model. The results highlighted the associations between the quality of inputs and outputs, underscoring the necessity to evaluate both components rather than solely focusing on outputs. In future work, we will investigate the potential time-saving benefits of our proposed framework for evaluators assessing NLP systems.
</details>
<details>
<summary>摘要</summary>
人类评估在自然语言处理（NLP）中扮演着关键性的角色，它评估了开发出来的系统的质量和相关性，从而促进其改进。然而，NLP领域没有广泛得到承认的人类评估指标，这使得不同系统之间的比较不公平，而且无法建立通用的评估标准。通过对现有的NLP评估指标文献进行广泛分析，我们发现了NLP评估方法ologies中的一些缺失。这些缺失成为了我们开发自己的层次评估框架的动机。我们的提议的框架具有一些优势，特别是可以更全面地表示NLP系统的性能。我们将这个框架应用于评估我们开发的机器阅读理解系统，该系统在人机合作模式下使用。结果显示输入质量和输出相关性之间存在关系，从而证明需要同时评估输入和输出而不是 solely focus on outputs。在未来的工作中，我们将investigate我们提议的框架可能对评估NLP系统的时间成本带来的优化。
</details></li>
</ul>
<hr>
<h2 id="Ring-Attention-with-Blockwise-Transformers-for-Near-Infinite-Context"><a href="#Ring-Attention-with-Blockwise-Transformers-for-Near-Infinite-Context" class="headerlink" title="Ring Attention with Blockwise Transformers for Near-Infinite Context"></a>Ring Attention with Blockwise Transformers for Near-Infinite Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01889">http://arxiv.org/abs/2310.01889</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lhao499/llm_large_context">https://github.com/lhao499/llm_large_context</a></li>
<li>paper_authors: Hao Liu, Matei Zaharia, Pieter Abbeel</li>
<li>for: 提高Transformer模型对长序列的处理能力，解决由各个设备的内存限制所带来的挑战。</li>
<li>methods: 提出了一种新的方法 Ring Attention，通过块式计算自注意力来分布长序列 across multiple devices，并在计算块值块和自注意力计算之间进行 overlap communication。</li>
<li>results: Ring Attention 可以让序列长度与设备数量成正比，大大提高语言模型的性能和可扩展性。<details>
<summary>Abstract</summary>
Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while overlapping the communication of key-value blocks with the computation of blockwise attention. Ring Attention enables training and inference of sequences that are up to device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in allowing large sequence input size and improving performance.
</details>
<details>
<summary>摘要</summary>
<translation>transformers 已经成为许多现代 AI 模型的建筑物选择，在各种 AI 应用中显示出了极高的性能。然而，transformers 中的内存需求限制了它们处理长序列的能力，从而创造了较长序列或者长期依赖关系的任务中的挑战。我们提出了一种不同的方法， called Ring Attention，它利用了块式计算自注意力来分布长序列 across 多个设备，并在计算块注意力和交换 key-value 块之间重叠。Ring Attention 允许在设备数量 multiplication 的长度上进行训练和推理，从而消除了每个设备的内存限制。广泛的语言模型任务实验证明了 Ring Attention 的有效性，允许大量输入序列和提高性能。</translation></SYS>Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the original text in English, and it is not a word-for-word translation. Some phrases and sentences may be rephrased or condensed to make the translation more concise and natural-sounding in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Effective-and-Parameter-Efficient-Reusing-Fine-Tuned-Models"><a href="#Effective-and-Parameter-Efficient-Reusing-Fine-Tuned-Models" class="headerlink" title="Effective and Parameter-Efficient Reusing Fine-Tuned Models"></a>Effective and Parameter-Efficient Reusing Fine-Tuned Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01886">http://arxiv.org/abs/2310.01886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weisen Jiang, Baijiong Lin, Han Shi, Yu Zhang, Zhenguo Li, James T. Kwok</li>
<li>for: 提高下游任务的效果和精度，降低存储和服务负担</li>
<li>methods: 使用噪声任务 вектор权重抑制法、特征值分解法重建LoRA矩阵</li>
<li>results: 对计算机视觉和自然语言处理任务进行广泛的实验，提出了Parameter-Efficient methods for ReUsing (PERU) fine-tuned models，PERU-FFT和PERU-LoRA方法比现有的复用模型方法更高效，达到了与每个任务使用精心调整模型的性能水平。<details>
<summary>Abstract</summary>
Many pre-trained large-scale models provided online have become highly effective in transferring to downstream tasks. At the same time, various task-specific models fine-tuned on these pre-trained models are available online for public use. In practice, as collecting task-specific data is labor-intensive and fine-tuning the large pre-trained models is computationally expensive, one can reuse task-specific finetuned models to deal with downstream tasks. However, using a model per task causes a heavy burden on storage and serving. Recently, many training-free and parameter-efficient methods have been proposed for reusing multiple fine-tuned task-specific models into a single multi-task model. However, these methods exhibit a large accuracy gap compared with using a fine-tuned model per task. In this paper, we propose Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task vector into a merged model by magnitude pruning. For reusing LoRA fine-tuned models, we propose PERU-LoRA use a lower-rank matrix to approximate the LoRA matrix by singular value decomposition. Both PERUFFT and PERU-LoRA are training-free. Extensive experiments conducted on computer vision and natural language process tasks demonstrate the effectiveness and parameter-efficiency of the proposed methods. The proposed PERU-FFT and PERU-LoRA outperform existing reusing model methods by a large margin and achieve comparable performance to using a fine-tuned model per task.
</details>
<details>
<summary>摘要</summary>
很多已经预训练的大规模模型在线提供了，这些模型在下游任务上转移得非常有效。同时，各种任务特定的模型在这些预训练模型上进行细化也在线公开使用。在实践中，收集任务特定数据是劳动密集，并在大规模预训练模型上细化是计算昂贵的。因此，可以重用任务特定细化模型来处理下游任务。然而，使用一个模型每个任务会增加存储和服务的压力。近期，许多无需训练和参数效率高的方法被提议用于重用多个细化任务特定模型。然而，这些方法与使用每个任务细化模型的准确性差距很大。在这篇论文中，我们提出了Parameter-Efficient methods for ReUsing (PERU)细化模型。为重用完全细化（FFT）模型，我们提出了PERU-FFT，通过量减法将多个任务的任务 вектор束入一个合并模型中。为重用LoRA细化模型，我们提出了PERU-LoRA，使用下三角矩阵来近似LoRA矩阵。两种PERUFFT和PERU-LoRA都是无需训练的。我们对计算机视觉和自然语言处理任务进行了广泛的实验，并证明了我们提出的方法的有效性和参数效率。我们的PERU-FFT和PERU-LoRA在与使用每个任务细化模型相比，取得了大幅度的提高，并在与使用每个任务细化模型的准确性相同的情况下达到了相似的性能。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-and-Improving-Generator-Validator-Consistency-of-Language-Models"><a href="#Benchmarking-and-Improving-Generator-Validator-Consistency-of-Language-Models" class="headerlink" title="Benchmarking and Improving Generator-Validator Consistency of Language Models"></a>Benchmarking and Improving Generator-Validator Consistency of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01846">http://arxiv.org/abs/2310.01846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tatsunori Hashimoto, Percy Liang</li>
<li>for: 提高语言模型（LM）的一致性，增强LM的可靠性和可信度。</li>
<li>methods: 提出了一种 generator-validator consistency（GV-consistency）评估框架，并在这个框架下进行了finetuning，以提高LM的一致性和可靠性。</li>
<li>results: 通过在 filtered generator和validator responses上进行finetuning，可以提高GV-consistency的率，并且这种方法可以提高 generator质量和validator准确率，无需使用任何标注数据。<details>
<summary>Abstract</summary>
As of September 2023, ChatGPT correctly answers "what is 7+8" with 15, but when asked "7+8=15, True or False" it responds with "False". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data. Evaluated across 6 tasks, including math questions, knowledge-intensive QA, and instruction following, our method improves the generator quality by 16% and the validator accuracy by 6.3% across all tasks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:截至2023年9月，ChatGPT正确地回答“7+8”的答案是15，但当被问到“7+8等于15，是真假”时，它回答“false”。这种在生成和验证答案之间的不一致是语言模型（LM）中的一个常见问题，而这种问题会让人失去信任。在这篇论文中，我们提出了一种测量生成和验证之间的一致性框架（ generator-validator consistency，简称GV-consistency），并发现even GPT-4，一个状态体系的LM，只有76%的GV-consistency。为了提高LM的一致性，我们提议通过filter了生成和验证响应的GV-consistent来进行finetuning，并称之为一致性精度调整。我们发现，这种方法可以提高Alpaca-30B的GV-consistency从60%提高到93%，并且这种改进可以 extrapolate to unseen tasks and domains（例如，GV-consistency for positive style transfers extrapolates to unseen styles like humor）。此外，一致性精度调整还可以提高生成质量和验证精度，不需要使用任何标注数据。我们对6个任务进行评估，包括数学问题、知识型QA和实行指令，我们发现，我们的方法可以提高生成质量16%和验证精度6.3% across all tasks。
</details></li>
</ul>
<hr>
<h2 id="Preserving-Phonemic-Distinctions-for-Ordinal-Regression-A-Novel-Loss-Function-for-Automatic-Pronunciation-Assessment"><a href="#Preserving-Phonemic-Distinctions-for-Ordinal-Regression-A-Novel-Loss-Function-for-Automatic-Pronunciation-Assessment" class="headerlink" title="Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment"></a>Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss Function for Automatic Pronunciation Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01839">http://arxiv.org/abs/2310.01839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bi-Cheng Yan, Hsin-Wei Wang, Yi-Cheng Wang, Jiun-Ting Li, Chi-Han Lin, Berlin Chen</li>
<li>for:  automatic pronunciation assessment (APA) for second language (L2) learners</li>
<li>methods:  uses neural models with a phonemic contrast ordinal (PCO) loss function to preserve phonemic distinctions and ordinal relationships</li>
<li>results:  effective in capturing proficiency levels and preserving phonemic distinctions, as demonstrated by experiments on the speechocean762 benchmark dataset<details>
<summary>Abstract</summary>
Automatic pronunciation assessment (APA) manages to quantify the pronunciation proficiency of a second language (L2) learner in a language. Prevailing approaches to APA normally leverage neural models trained with a regression loss function, such as the mean-squared error (MSE) loss, for proficiency level prediction. Despite most regression models can effectively capture the ordinality of proficiency levels in the feature space, they are confronted with a primary obstacle that different phoneme categories with the same proficiency level are inevitably forced to be close to each other, retaining less phoneme-discriminative information. On account of this, we devise a phonemic contrast ordinal (PCO) loss for training regression-based APA models, which aims to preserve better phonemic distinctions between phoneme categories meanwhile considering ordinal relationships of the regression target output. Specifically, we introduce a phoneme-distinct regularizer into the MSE loss, which encourages feature representations of different phoneme categories to be far apart while simultaneously pulling closer the representations belonging to the same phoneme category by means of weighted distances. An extensive set of experiments carried out on the speechocean762 benchmark dataset suggest the feasibility and effectiveness of our model in relation to some existing state-of-the-art models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Model-Tells-You-What-to-Discard-Adaptive-KV-Cache-Compression-for-LLMs"><a href="#Model-Tells-You-What-to-Discard-Adaptive-KV-Cache-Compression-for-LLMs" class="headerlink" title="Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"></a>Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01801">http://arxiv.org/abs/2310.01801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao</li>
<li>for: 本研究推出了适应型KV缓存压缩，用于减少生成推理中Large Language Models（LLMs）的内存占用。</li>
<li>methods: 我们采用了目标 Profiling 技术来识别抽象模块的内在结构，并根据此构建了适应型KV缓存：将长距离上下文Token Evict，中心特殊Token上的非特殊Token Discard，仅使用标准KV缓存 для全Token扩展。</li>
<li>results: 我们在不同的问题上进行了实验，显示了substantial减少GPU内存占用，同时 generation质量损失几乎可以忽略不计。我们将发布我们的代码和相关的CUDA加速器，以便重现。<details>
<summary>Abstract</summary>
In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们介绍了适应型KV缓存压缩，一种插件化方法，用于减少生成推理中大语言模型（LLM）的内存占用。与传统KV缓存不同，我们通过目标 profiling 描述了注意模块的内在结构，并根据认可的结构来构建适应型KV缓存：在注意头中舍弃远程上下文，仅保留当前上下文中的特殊token，并且仅在所有Token上进行标准KV缓存。此外，通过轻量级注意 profiling 导引适应型KV缓存的构建，我们可以在无需资源开销的细致调整或重新训练的情况下部署 FastGen。在我们的实验中，FastGen 在不同的问题上都显示了明显的内存占用减少，而且与生成质量的损失相对较小。我们将在代码和相应的 CUDA 核心上发布我们的实验结果以供参考。
</details></li>
</ul>
<hr>
<h2 id="SEA-Sparse-Linear-Attention-with-Estimated-Attention-Mask"><a href="#SEA-Sparse-Linear-Attention-with-Estimated-Attention-Mask" class="headerlink" title="SEA: Sparse Linear Attention with Estimated Attention Mask"></a>SEA: Sparse Linear Attention with Estimated Attention Mask</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01777">http://arxiv.org/abs/2310.01777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang</li>
<li>for: 提高大型transformer模型在受限制的设备上的运行效率，以及使用较少的内存进行语言理解任务。</li>
<li>methods: 提出了一种基于kernel的线性注意力算法，并使用top-k选择来实现简化的注意力操作。</li>
<li>results: 在语料 Wikitext2 上，与基线OPt-125M模型相比，前一代的线性和简化注意力方法的误差率大约为2倍，而SEA模型则能够达到与OPT-125M模型相同或更好的误差率，使用内存相对较少的 OPT-125M 模型。此外，SEA模型还能够保持可读性的注意力矩阵，并可以使用知识储存来降低现有预训练的复杂性。<details>
<summary>Abstract</summary>
The transformer architecture has made breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, transformers struggle with long sequences due to the quadratic complexity of the attention operation, and previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix, and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches may also lose interpretability if they do not produce full quadratic attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then creates a sparse approximation to the full attention matrix with a top-k selection to perform a sparse attention operation. For language modeling tasks (Wikitext2), previous linear and sparse attention methods show a roughly two-fold worse perplexity scores over the quadratic OPT-125M baseline, while SEA achieves an even better perplexity than OPT-125M, using roughly half as much memory as OPT-125M. Moreover, SEA maintains an interpretable attention matrix and can utilize knowledge distillation to lower the complexity of existing pretrained transformers. We believe that our work will have a large practical impact, as it opens the possibility of running large transformers on resource-limited devices with less memory.
</details>
<details>
<summary>摘要</summary>
“transformer架构在最近几年内获得了重大突破，尤其是在需要模型顺序元素之间的关系的任务上。然而，transformer对于长序列进行处理存在问题，因为对于注意力Matrix的计算有quadratic复杂度，以往的研究尝试透过对注意力Matrix进行简化或线性近似来降低复杂度。然而，这些方法无法直接传递教师的注意力Matrix，并且通常需要从头开始重新训练。此外，过去的简化和线性方法可能会丧失解释性，因为它们不会产生完整的quadratic注意力Matrix。为了解决这些挑战，我们提出了SEA：简化线性注意力 avec Estimated Attention mask。SEA使用线性复杂度估计注意力Matrix，然后将注意力Matrix简化为top-k选择，以进行简化注意力操作。在语言模型任务（Wikitext2）上，过去的线性和简化注意力方法比QUADRATIC OPT-125M基准下的误差分别大约两倍，而SEA则能够在与OPT-125M相同的内存使用量下取得更好的误差分数。此外，SEA保留了解释性的注意力Matrix，并且可以将知识传播到现有的预训transformer中，以降低复杂度。我们认为，我们的工作将对实际上有很大的影响，因为它开启了让大型transformer在资源有限的设备上运行的可能性。”
</details></li>
</ul>
<hr>
<h2 id="Stack-Attention-Improving-the-Ability-of-Transformers-to-Model-Hierarchical-Patterns"><a href="#Stack-Attention-Improving-the-Ability-of-Transformers-to-Model-Hierarchical-Patterns" class="headerlink" title="Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns"></a>Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01749">http://arxiv.org/abs/2310.01749</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bdusell/stack-attention">https://github.com/bdusell/stack-attention</a></li>
<li>paper_authors: Brian DuSell, David Chiang</li>
<li>for: 这个论文是为了解决自然语言处理中的层次结构识别问题，以提高模型的表达能力和泛化能力。</li>
<li>methods: 这篇论文提出了一种新的注意力操作符，即堆栈注意力（Stack Attention），它通过吸收堆栈来处理层次结构，从而提高模型的识别能力。</li>
<li>results: 论文通过实验表明，堆栈注意力可以在自然语言处理中提高模型的表达能力和泛化能力，特别是在识别复杂的语言结构时。此外，堆栈注意力还可以在有限资源的情况下提高模型的性能。<details>
<summary>Abstract</summary>
Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation.
</details>
<details>
<summary>摘要</summary>
注意，特别是缩放乘制注意力，在自然语言中证明有效，但它没有机制来处理嵌套深度的层次结构，这限制了它的能力来认识certain syntactic structures。为解决这个缺陷，我们提出栈注意力：一种注意力运算符，它利用栈来模拟语法结构。我们表明，栈注意力与标准注意力相似，但它需要无语法监督的隐藏语言模型。我们提出了两种变体：一种与deterministic pushdown automata (PDAs)相关，另一种基于nondeterministic PDAs，允许transformers认识任意context-free languages (CFLs)。我们显示，transformers with stack attention在学习CFLs中表现出色，在一个理论上最大的解析难度下 achieve strong results。我们还显示，栈注意力在一定参数预算下更有效于自然语言处理，并包括了machine translation的结果。
</details></li>
</ul>
<hr>
<h2 id="Deciphering-Diagnoses-How-Large-Language-Models-Explanations-Influence-Clinical-Decision-Making"><a href="#Deciphering-Diagnoses-How-Large-Language-Models-Explanations-Influence-Clinical-Decision-Making" class="headerlink" title="Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making"></a>Deciphering Diagnoses: How Large Language Models Explanations Influence Clinical Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01708">http://arxiv.org/abs/2310.01708</a></li>
<li>repo_url: None</li>
<li>paper_authors: D. Umerenkov, G. Zubkova, A. Nesterov</li>
<li>For: This study aims to evaluate the effectiveness and reliability of Large Language Models (LLMs) in generating explanations for diagnoses based on patient complaints.* Methods: The study uses LLMs to generate explanations of the connection between patient complaints and doctor and model-assigned diagnoses, and evaluates the explanations with three experienced doctors across several stages.* Results: The study found that LLM explanations significantly increased doctors’ agreement rates with given diagnoses, but also highlighted potential errors in LLM outputs, ranging from 5% to 30%.Here’s the information in Simplified Chinese text:</li>
<li>for: 这项研究旨在评估大语言模型（LLMs）在基于病人投诉的诊断上的效果和可靠性。</li>
<li>methods: 该研究使用LLMs生成病人投诉与医生和模型诊断之间的联系，并通过三名经验丰富的医生在多个阶段进行评估。</li>
<li>results: 研究发现，LLM解释significтельно提高了医生对诊断的同意率，但也揭示了LLM输出中的可能错误，范围为5%到30%。<details>
<summary>Abstract</summary>
Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and patient data to offer real-time recommendations, with Large Language Models (LLMs) emerging as a promising tool to generate plain-text explanations for medical decisions. This study explores the effectiveness and reliability of LLMs in generating explanations for diagnoses based on patient complaints. Three experienced doctors evaluated LLM-generated explanations of the connection between patient complaints and doctor and model-assigned diagnoses across several stages. Experimental results demonstrated that LLM explanations significantly increased doctors' agreement rates with given diagnoses and highlighted potential errors in LLM outputs, ranging from 5% to 30%. The study underscores the potential and challenges of LLMs in healthcare and emphasizes the need for careful integration and evaluation to ensure patient safety and optimal clinical utility.
</details>
<details>
<summary>摘要</summary>
临床决策支持系统（CDSS）利用证据基础知识和患者数据提供实时建议，大型自然语言模型（LLMs）在生成普通文本解释方面表现出了潜在的优势。本研究探讨了LLM在基于患者投诉生成诊断关系的效iveness和可靠性。三位临床医生评估了LLM生成的患者投诉与医生和模型诊断之间的关系，结果显示LLM解释可以显著提高医生们对诊断的一致率，并且揭示了LLM输出中的可能错误，从5%到30%不等。本研究强调了LLM在医疗领域的潜力和挑战，并提醒了仔细考虑和评估，以确保患者安全和优化临床实用性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/03/cs.CL_2023_10_03/" data-id="closbromz00c50g88bvha8zmq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/26/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="page-number" href="/page/26/">26</a><span class="page-number current">27</span><a class="page-number" href="/page/28/">28</a><a class="page-number" href="/page/29/">29</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/28/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
