
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/30/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_09_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/29/cs.AI_2023_09_29/" class="article-date">
  <time datetime="2023-09-29T12:00:00.000Z" itemprop="datePublished">2023-09-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/29/cs.AI_2023_09_29/">cs.AI - 2023-09-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-the-Equivalence-of-Graph-Convolution-and-Mixup"><a href="#On-the-Equivalence-of-Graph-Convolution-and-Mixup" class="headerlink" title="On the Equivalence of Graph Convolution and Mixup"></a>On the Equivalence of Graph Convolution and Mixup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00183">http://arxiv.org/abs/2310.00183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaotian Han, Hanqing Zeng, Yu Chen, Shaoliang Nie, Jingzhou Liu, Kanika Narang, Zahra Shakeri, Karthik Abinav Sankararaman, Song Jiang, Madian Khabsa, Qifan Wang, Xia Hu</li>
<li>for: 本研究探讨图 convolution 和 Mixup 技术之间的关系。</li>
<li>methods: 本研究使用图 convolution 和 Mixup 技术来学习图像的表示。</li>
<li>results: 研究发现，在两个轻量级条件下，图 convolution 可以看作是 Mixup 的特殊形式，并在训练和测试阶段应用。这两个条件是：1）同义关系重新标签（Homophily Relabel），2）测试阶段 Mixup（Test-Time Mixup）。我们数学上证明了图 convolution 网络（GCN）和简化图 convolution（SGC）可以表示为 Mixup，并通过训练 MLP 来验证这一等式。<details>
<summary>Abstract</summary>
This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \textit{Test-Time Mixup} - Mixup the feature during the test time. We establish this equivalence mathematically by demonstrating that graph convolution networks (GCN) and simplified graph convolution (SGC) can be expressed as a form of Mixup. We also empirically verify the equivalence by training an MLP using the two conditions to achieve comparable performance.
</details>
<details>
<summary>摘要</summary>
Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are:1. Homophily Relabel: Assign the target node's label to all its neighbors.2. Test-Time Mixup: Mixup the features during the test time.We establish this equivalence mathematically by demonstrating that graph convolution networks (GCN) and simplified graph convolution (SGC) can be expressed as a form of Mixup. We also empirically verify the equivalence by training an MLP using the two conditions to achieve comparable performance.
</details></li>
</ul>
<hr>
<h2 id="Junk-DNA-Hypothesis-A-Task-Centric-Angle-of-LLM-Pre-trained-Weights-through-Sparsity"><a href="#Junk-DNA-Hypothesis-A-Task-Centric-Angle-of-LLM-Pre-trained-Weights-through-Sparsity" class="headerlink" title="Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity"></a>Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02277">http://arxiv.org/abs/2310.02277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vita-group/junk_dna_hypothesis">https://github.com/vita-group/junk_dna_hypothesis</a></li>
<li>paper_authors: Lu Yin, Shiwei Liu, Ajay Jaiswal, Souvik Kundu, Zhangyang Wang</li>
<li>for: 这个论文旨在探讨大型自然语言处理器（LLM）中具有低维度的权重是否真的是无用的，以及它们是否对下游任务有重要作用。</li>
<li>methods: 作者使用简洁性作为工具，以孤立和量化LLM中低维度权重的细化意义，从下游任务角度来研究。</li>
<li>results: 研究发现，尽管小维度权重可能看起来是无用的，但它们实际上含有解决更难的下游任务所需的关键知识。从某种角度来看，这些权重是“潜在的Junk DNA”，如果被移除，可能会导致知识损失和任务性能下降。这些发现可能会改变我们对LLM知识编码方式的理解，并且开启了未来模型剪枝和任务相关计算的研究方向。<details>
<summary>Abstract</summary>
The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth investigation: while small-magnitude weights may appear "useless" for simple tasks and suitable for pruning, they actually encode crucial knowledge necessary for solving more difficult downstream tasks. Removing these seemingly insignificant weights can lead to irreversible knowledge forgetting and performance damage in difficult tasks. These findings offer fresh insights into how LLMs encode knowledge in a task-sensitive manner, pave future research direction in model pruning, and open avenues for task-aware conditional computation during inference.
</details>
<details>
<summary>摘要</summary>
传统上，“垃圾DNA”概念一直与人类基因组中的非编码序列相关，占基因组的约98%的质量。然而，最近的研究发现，一些这些看起来无功能的DNA序列在细胞过程中扮演着重要的角色。启示地，神经网络中的权重 Displayed remarkable similarity to human gene redundancy. It was believed that the weights in large models were redundant and could be removed without affecting performance. However, this paper challenges this conventional wisdom by presenting a counter-argument. We use sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study shows a strong correlation between these weight magnitudes and the knowledge they encapsulate from a downstream task-centric angle. We propose the "Junk DNA Hypothesis" backed by our in-depth investigation: while small-magnitude weights may appear "useless" for simple tasks and suitable for pruning, they actually encode crucial knowledge necessary for solving more difficult downstream tasks. Removing these seemingly insignificant weights can lead to irreversible knowledge forgetting and performance damage in difficult tasks. These findings offer fresh insights into how LLMs encode knowledge in a task-sensitive manner, pave future research direction in model pruning, and open avenues for task-aware conditional computation during inference.
</details></li>
</ul>
<hr>
<h2 id="Motif-Intrinsic-Motivation-from-Artificial-Intelligence-Feedback"><a href="#Motif-Intrinsic-Motivation-from-Artificial-Intelligence-Feedback" class="headerlink" title="Motif: Intrinsic Motivation from Artificial Intelligence Feedback"></a>Motif: Intrinsic Motivation from Artificial Intelligence Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00166">http://arxiv.org/abs/2310.00166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/motif">https://github.com/facebookresearch/motif</a></li>
<li>paper_authors: Martin Klissarov, Pierluca D’Oro, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, Mikael Henaff</li>
<li>for: 本研究旨在介入大自然语言模型（LLM）中的决策过程，以便在不知情的环境中探索和评估行为。</li>
<li>methods: 本研究提出了“ Motif”方法，它基于在决策过程中使用 LLM 的想法，不需要与环境交互。 Motif 通过从 LLM 中提取 preference 来构建内在奖励，并使用强化学习训练 agents。</li>
<li>results: 在 NetHack 游戏中，Motif 可以在不直接寻求高分的情况下很好地提高游戏分数。此外，当与环境奖励相结合时，Motif 的方法可以大幅度超越现有的方法，并在没有示例的情况下做出进展。最后，研究发现 Motif 通常会生成人类对应的直观行为，可以通过提示修改轻松地控制，而且可扩展性好。<details>
<summary>Abstract</summary>
Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent. Motif is based on the idea of grounding LLMs for decision-making without requiring them to interact with the environment: it elicits preferences from an LLM over pairs of captions to construct an intrinsic reward, which is then used to train agents with reinforcement learning. We evaluate Motif's performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself. When combining Motif's intrinsic reward with the environment reward, our method significantly outperforms existing approaches and makes progress on tasks where no advancements have ever been made without demonstrations. Finally, we show that Motif mostly generates intuitive human-aligned behaviors which can be steered easily through prompt modifications, while scaling well with the LLM size and the amount of information given in the prompt.
</details>
<details>
<summary>摘要</summary>
探索富有环境和不带先知的行为评估是非常困难的。在这篇论文中，我们提出了 Motif，一种通用的方法，可以从大语言模型（LLM）中提取先知知识并用于代理人的决策。Motif基于将 LLM 用于决策的想法，无需与环境进行交互：它从 LLM 中提取对caption的偏好，以构建内在奖励，然后用强化学习训练代理人。我们评估了 Motif 的性能和行为在开放结构的 NetHack 游戏中。 Result 显示，只学习 maximize 内在奖励，Motif 可以在游戏得分上高于直接带直接带 maximize 算法。当与环境奖励结合时，我们的方法在现有的方法上有显著进步，并在没有示范的情况下完成任务。最后，我们表明 Motif 生成的主要是人类对应的INTUITIVE行为，可以通过提示修改轻松地控制，而且随着 LLM 的大小和提示信息的量而升级良好。
</details></li>
</ul>
<hr>
<h2 id="Detection-Oriented-Image-Text-Pretraining-for-Open-Vocabulary-Detection"><a href="#Detection-Oriented-Image-Text-Pretraining-for-Open-Vocabulary-Detection" class="headerlink" title="Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection"></a>Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00161">http://arxiv.org/abs/2310.00161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research/tree/master/fvlm/dito">https://github.com/google-research/google-research/tree/master/fvlm/dito</a></li>
<li>paper_authors: Dahun Kim, Anelia Angelova, Weicheng Kuo</li>
<li>for:  bridging the gap between image-level pretraining and open-vocabulary object detection</li>
<li>methods:  using detection-oriented image-text pretraining with a detector architecture, and a shifted-window learning approach upon window attention</li>
<li>results:  setting a new state of the art on the LVIS open-vocabulary detection benchmark, and achieving competitive results on the COCO benchmark without pseudo labeling or weak supervision.Here’s the simplified Chinese text:</li>
<li>for:  closure detection和开放词汇检测之间的差距</li>
<li>methods: 使用检测封装的图像文本预训练，并使用窗口注意力的偏移学习</li>
<li>results: 在LIVIS开放词汇检测标准benchmark上设置新的最佳值（40.4偏好AP$_r$），与最佳现有方法相比提高了6.5偏好AP$_r$。在COCObenchmark上 achieve了非常竞争力的40.8新型AP，无需pseudo标签或弱超vision。<details>
<summary>Abstract</summary>
We present a new open-vocabulary detection approach based on detection-oriented image-text pretraining to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we replace the commonly used classification architecture with the detector architecture, which better serves the region-level recognition needs of detection by enabling the detector heads to learn from noisy image-text pairs. Using only standard contrastive loss and no pseudo-labeling, our approach is a simple yet effective extension of the contrastive learning method to learn emergent object-semantic cues. In addition, we propose a shifted-window learning approach upon window attention to make the backbone representation more robust, translation-invariant, and less biased by the window pattern. On the popular LVIS open-vocabulary detection benchmark, our approach sets a new state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone, significantly outperforming the best existing approach by +6.5 mask AP$_r$ at system level. On the COCO benchmark, we achieve very competitive 40.8 novel AP without pseudo labeling or weak supervision. In addition, we evaluate our approach on the transfer detection setup, where ours outperforms the baseline significantly. Visualization reveals emerging object locality from the pretraining recipes compared to the baseline. Code and models will be publicly released.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的开放词汇检测方法，基于检测偏好的图像文本预训练来bridging图像级预训练和开放词汇检测之间的差距。在预训练阶段，我们将通常使用的分类架构替换为检测架构，这更好地服务于检测需求的区域水平识别，使得检测头可以从噪声图像文本对learning。无需使用伪标注，我们的方法是一种简单而有效的扩展，通过对不同的对象 semantics cues进行学习。此外，我们提议使用shifted-window学习方法来改进后处理抽象，使模型表示更加鲁棒、不受窗口模式的影响和偏见。在流行的LVIS开放词汇检测标准benchmark上，我们的方法实现了40.4带宽AP$_r$的新州Of The Art，比best现有方法+6.5带宽AP$_r$的系统水平性能。在COCObenchmark上，我们实现了非常竞争力的40.8新的AP，没有使用伪标注或弱监督。此外，我们还评估了我们的方法在传输检测设置下的性能，其比基线显著提高。视觉化表明预训练热键比基线具有更高的对象地域性。代码和模型将公开发布。
</details></li>
</ul>
<hr>
<h2 id="Self-Specialization-Uncovering-Latent-Expertise-within-Large-Language-Models"><a href="#Self-Specialization-Uncovering-Latent-Expertise-within-Large-Language-Models" class="headerlink" title="Self-Specialization: Uncovering Latent Expertise within Large Language Models"></a>Self-Specialization: Uncovering Latent Expertise within Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00160">http://arxiv.org/abs/2310.00160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junmo Kang, Hongyin Luo, Yada Zhu, James Glass, David Cox, Alan Ritter, Rogerio Feris, Leonid Karlinsky</li>
<li>for: 本研究ocuses on self-alignment for expert domain specialization (e.g., biomedicine), discovering its effectiveness in improving zero-shot and few-shot performance in target domains of interest.</li>
<li>methods: 我们使用了自我调整，利用专业领域的预有数据和一些标签化的seed来自适应领域。我们还将 retrieve该过程以减少幻视和增加同步。</li>
<li>results: 我们的自适应模型（30B）在生医领域比基本模型（MPT-30B）大幅提高表现，甚至超过了更大的受欢迎模型（LLaMA-65B），显示其实用性和实际性。<details>
<summary>Abstract</summary>
Recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. As a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that "generic" instruction-following training has on downstream expert domains' performance. To remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. When augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offers an effective (and efficient) way of "carving out" an expert model out of a "generalist", pre-trained LLM where different domains of expertise are originally combined in a form of "superposition". Our experimental results on a biomedical domain show that our self-specialized model (30B) outperforms its base model, MPT-30B by a large margin and even surpasses larger popular models based on LLaMA-65B, highlighting its potential and practicality for specialization, especially considering its efficiency in terms of data and parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Feedback-guided-Data-Synthesis-for-Imbalanced-Classification"><a href="#Feedback-guided-Data-Synthesis-for-Imbalanced-Classification" class="headerlink" title="Feedback-guided Data Synthesis for Imbalanced Classification"></a>Feedback-guided Data Synthesis for Imbalanced Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00158">http://arxiv.org/abs/2310.00158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reyhane Askari Hemmat, Mohammad Pezeshki, Florian Bordes, Michal Drozdzal, Adriana Romero-Soriano</li>
<li>for: 提高图像分类模型在长尾分布下的性能</li>
<li>methods: 使用一种基于单一反馈的概率模型增强静态数据集，通过让概率模型为分类器提供反馈来提高生成模型对图像的生成</li>
<li>results: 在ImageNet-LT和NICO++ datasets上实现了状态机器学习的最佳结果，对下 Representation 类型进行了4%以上的改进，并在NICO++上获得了5%以上的最差组织精度提升。<details>
<summary>Abstract</summary>
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On ImageNet-LT, we achieve state-of-the-art results, with over 4 percent improvement on underrepresented classes while being twice efficient in terms of the number of generated synthetic samples. NICO++ also enjoys marked boosts of over 5 percent in worst group accuracy. With these results, our framework paves the path towards effectively leveraging state-of-the-art text-to-image models as data sources that can be queried to improve downstream applications.
</details>
<details>
<summary>摘要</summary>
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On ImageNet-LT, we achieve state-of-the-art results, with over 4 percent improvement on underrepresented classes while being twice efficient in terms of the number of generated synthetic samples. NICO++ also enjoys marked boosts of over 5 percent in worst group accuracy. With these results, our framework paves the path towards effectively leveraging state-of-the-art text-to-image models as data sources that can be queried to improve downstream applications.Here is the translation in Traditional Chinese:Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On ImageNet-LT, we achieve state-of-the-art results, with over 4 percent improvement on underrepresented classes while being twice efficient in terms of the number of generated synthetic samples. NICO++ also enjoys marked boosts of over 5 percent in worst group accuracy. With these results, our framework paves the path towards effectively leveraging state-of-the-art text-to-image models as data sources that can be queried to improve downstream applications.
</details></li>
</ul>
<hr>
<h2 id="Learning-Generalizable-Tool-use-Skills-through-Trajectory-Generation"><a href="#Learning-Generalizable-Tool-use-Skills-through-Trajectory-Generation" class="headerlink" title="Learning Generalizable Tool-use Skills through Trajectory Generation"></a>Learning Generalizable Tool-use Skills through Trajectory Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00156">http://arxiv.org/abs/2310.00156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carl Qi, Sarthak Shetty, Xingyu Lin, David Held</li>
<li>for: 这个论文旨在帮助人类完成许多日常任务，如 cooking 和 cleaning，通过使用智能系统。但是，现有系统不能匹配人类水平的智能水平，在适应新工具方面。</li>
<li>methods: 该论文使用 Generative Model 学习用户在使用新工具时的工具使用轨迹，并且可以泛化到不同的工具形状。首先，我们生成一个工具使用轨迹，然后优化工具姿势序列以适应生成的轨迹。我们使用了单个模型，并在四个不同的难度较高的塑料对象处理任务中训练。</li>
<li>results: 我们的模型在使用不同的新工具时，能够对塑料对象进行高效的处理，并且明显超过基eline。详细结果可以在我们项目网站上找到：<a target="_blank" rel="noopener" href="https://sites.google.com/view/toolgen%E3%80%82">https://sites.google.com/view/toolgen。</a><details>
<summary>Abstract</summary>
Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to generalize to various novel tools, significantly outperforming baselines. Additional materials can be found on our project website: https://sites.google.com/view/toolgen.
</details>
<details>
<summary>摘要</summary>
自主系统可以高效地使用工具来辅助人类完成许多常见的任务，如cooking和清洁。然而，目前的系统仍无法与人类水准的智能匹配，在适应新工具方面。先前的工作基于可用性通常会假设环境，并不能扩展到更加复杂的触摸任务。在这个工作中，我们解决这个挑战，并探讨如何使用未看过的工具来操纵弹性物体。我们提出了一个学习生成工具使用轨迹的数据模型，它可以对不同的工具形状进行生成。给任何新的工具，我们首先生成一个工具使用轨迹，然后对该轨迹进行优化，以使其与生成的轨迹相对。我们将一个模型训练用四个不同的弹性物体操纵任务，并且训练这个模型只需要使用单一工具的示范数据，并且可以很好地适应不同的新工具，与基准相比表现更好。更多的资料可以在我们的项目网站上找到：https://sites.google.com/view/toolgen。
</details></li>
</ul>
<hr>
<h2 id="Primal-Dual-Continual-Learning-Stability-and-Plasticity-through-Lagrange-Multipliers"><a href="#Primal-Dual-Continual-Learning-Stability-and-Plasticity-through-Lagrange-Multipliers" class="headerlink" title="Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers"></a>Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00154">http://arxiv.org/abs/2310.00154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Elenter, Navid NaderiAlizadeh, Tara Javidi, Alejandro Ribeiro</li>
<li>For: 本研究的目的是解决 continual learning 中的 no-forgetting 约束问题，即在学习新任务时，不能忘记之前学习的任务。* Methods: 本研究使用了 Lagrangian duality 来解决 continual learning 中的具体约束问题，并且分析了两种版本的 continual learning 问题：一种是任务级别的粗略方法，另一种是样本级别的细腻方法。* Results: 研究发现， dual variables 可以指示约束变化对优化问题的敏感性，并且可以使用这些 dual variables 来分区缓存、分配资源和填充缓存。 研究还提供了质量下界，并通过多个 continual learning  benchmark 进行了实验验证。<details>
<summary>Abstract</summary>
Continual learning is inherently a constrained learning problem. The goal is to learn a predictor under a \emph{no-forgetting} requirement. Although several prior studies formulate it as such, they do not solve the constrained problem explicitly. In this work, we show that it is both possible and beneficial to undertake the constrained optimization problem directly. To do this, we leverage recent results in constrained learning through Lagrangian duality. We focus on memory-based methods, where a small subset of samples from previous tasks can be stored in a replay buffer. In this setting, we analyze two versions of the continual learning problem: a coarse approach with constraints at the task level and a fine approach with constraints at the sample level. We show that dual variables indicate the sensitivity of the optimal value with respect to constraint perturbations. We then leverage this result to partition the buffer in the coarse approach, allocating more resources to harder tasks, and to populate the buffer in the fine approach, including only impactful samples. We derive sub-optimality bounds, and empirically corroborate our theoretical results in various continual learning benchmarks. We also discuss the limitations of these methods with respect to the amount of memory available and the number of constraints involved in the optimization problem.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: continual learning 是一个受限学习问题。目标是在 "无忘卷" 的前提下学习预测器。虽然一些先前的研究把它们作为这样形式化，但是它们没有直接解决受限问题。在这个工作中，我们表明可以并且有利可循直接解决受限优化问题。为此，我们利用最近的受限学习的研究结果，特别是拉格朗日策略。我们关注内存基本方法，即将前一个任务中的一小部分样本存储在回Buffer中。在这种设置下，我们分析了两种版本的受限学习问题：一种粗略的方法，在任务级别上设置约束，以及一种细腻的方法，在样本级别上设置约束。我们发现 dual variables 可以反映约束干扰优化问题的优化值的敏感性。然后，我们利用这个结论来划分缓存在粗略方法中，对硬度更高的任务分配更多的资源，并在细腻方法中，只包含影响力较大的样本。我们 derivation 质量下界，并在各种受限学习标准 benchmark 中进行实验验证。我们还讨论了这些方法在内存量和约束数量方面的局限性。
</details></li>
</ul>
<hr>
<h2 id="3D-Reconstruction-in-Noisy-Agricultural-Environments-A-Bayesian-Optimization-Perspective-for-View-Planning"><a href="#3D-Reconstruction-in-Noisy-Agricultural-Environments-A-Bayesian-Optimization-Perspective-for-View-Planning" class="headerlink" title="3D Reconstruction in Noisy Agricultural Environments: A Bayesian Optimization Perspective for View Planning"></a>3D Reconstruction in Noisy Agricultural Environments: A Bayesian Optimization Perspective for View Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00145">http://arxiv.org/abs/2310.00145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Athanasios Bacharis, Konstantinos D. Polyzos, Henry J. Nelson, Georgios B. Giannakis, Nikolaos Papanikolopoulos</li>
<li>for: 提高3D重建性能在噪音环境中</li>
<li>methods: 使用bayesian优化算法和 геометрические критери优选少数有用的摄像头位置，并考虑噪音环境的影响</li>
<li>results: 在噪音环境中使用少数摄像头实现高效的3D重建<details>
<summary>Abstract</summary>
3D reconstruction is a fundamental task in robotics that gained attention due to its major impact in a wide variety of practical settings, including agriculture, underwater, and urban environments. An important approach for this task, known as view planning, is to judiciously place a number of cameras in positions that maximize the visual information improving the resulting 3D reconstruction. Circumventing the need for a large number of arbitrary images, geometric criteria can be applied to select fewer yet more informative images to markedly improve the 3D reconstruction performance. Nonetheless, incorporating the noise of the environment that exists in various real-world scenarios into these criteria may be challenging, particularly when prior information about the noise is not provided. To that end, this work advocates a novel geometric function that accounts for the existing noise, relying solely on a relatively small number of noise realizations without requiring its closed-form expression. With no analytic expression of the geometric function, this work puts forth a Bayesian optimization algorithm for accurate 3D reconstruction in the presence of noise. Numerical tests on noisy agricultural environments showcase the impressive merits of the proposed approach for 3D reconstruction with even a small number of available cameras.
</details>
<details>
<summary>摘要</summary>
三维重建是机器人学中一项基本任务，它在各种实际应用场景中具有重要的影响，包括农业、水下和城市环境。一种重要的方法 для完成这项任务是通过精心选择一些摄像机的位置，以最大化视觉信息，从而提高三维重建的性能。然而，在实际场景中存在环境噪声的情况下，使用几何 критери优选择 fewer yet more informative images 可以显著提高三维重建性能。然而，在这种情况下，尚未提供关于噪声的准确信息时，将噪声纳入几何函数中可能是一项挑战。为此，这项工作提出了一种新的几何函数，该函数考虑了现有噪声，不需要准确的几何函数表达。由于这种几何函数没有准确的表达，这项工作提出了一种bayesian优化算法，用于在噪声存在的情况下高精度的三维重建。 numerically tests on noisy agricultural environments showcase the impressive merits of the proposed approach for 3D reconstruction with even a small number of available cameras.
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Sampling-Enhanced-Temporal-Spatial-GCN-A-Scalable-Framework-for-Transaction-Anomaly-Detection-in-Ethereum-Networks"><a href="#Probabilistic-Sampling-Enhanced-Temporal-Spatial-GCN-A-Scalable-Framework-for-Transaction-Anomaly-Detection-in-Ethereum-Networks" class="headerlink" title="Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks"></a>Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00144">http://arxiv.org/abs/2310.00144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Kambiz Behfar, Jon Crowcroft</li>
<li>for: 本研究旨在提高链表技术中的安全性和透明度，通过挖掘ETH链上交易的异常点和交易批量。</li>
<li>methods: 本研究使用图 convolutional neural networks (GCNs) 和时间随机游走 (TRW) 技术，通过抽样提高图 convolutional neural networks (GCNs) 的性能。</li>
<li>results: 对比传统GCNs，本研究的TRW-GCN框架在检测异常点和交易批量方面显著提高了性能指标。<details>
<summary>Abstract</summary>
The rapid evolution of the Ethereum network necessitates sophisticated techniques to ensure its robustness against potential threats and to maintain transparency. While Graph Neural Networks (GNNs) have pioneered anomaly detection in such platforms, capturing the intricacies of both spatial and temporal transactional patterns has remained a challenge. This study presents a fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) enhanced by probabilistic sampling to bridge this gap. Our approach, unlike traditional GCNs, leverages the strengths of TRW to discern complex temporal sequences in Ethereum transactions, thereby providing a more nuanced transaction anomaly detection mechanism. Preliminary evaluations demonstrate that our TRW-GCN framework substantially advances the performance metrics over conventional GCNs in detecting anomalies and transaction bursts. This research not only underscores the potential of temporal cues in Ethereum transactional data but also offers a scalable and effective methodology for ensuring the security and transparency of decentralized platforms. By harnessing both spatial relationships and time-based transactional sequences as node features, our model introduces an additional layer of granularity, making the detection process more robust and less prone to false positives. This work lays the foundation for future research aimed at optimizing and enhancing the transparency of blockchain technologies, and serves as a testament to the significance of considering both time and space dimensions in the ever-evolving landscape of the decentralized platforms.
</details>
<details>
<summary>摘要</summary>
“随着带有潜在威胁的Ethereum网络的快速演化，需要专业的技术来保证其可靠性和透明度。尽管几何对应网络（GNNs）在这些平台上已经开创了问题检测，但捕捉带有时间和空间特征的交易模式仍然是一个挑战。本研究提出了将几何对应网络（GCNs）与时间随机漫步（TRW）强化的概率抽样法，以突破这个问题。我们的方法与传统GCNs不同，利用TRW的优点，对Ethereum交易中的复杂时间序列进行探测，实现更精确的交易问题检测。初步评估显示，我们的TRW-GCN框架在检测问题和交易激波方面具有明显进步，较 Convention GCNs 高效。这个研究不仅强调了带有时间特征的Ethereum交易资料中的时间对称，而且提供了一个可扩展和有效的方法，以确保区块chain技术的安全性和透明度。我们的模型通过将空间关系和时间基础为节点特征，增加了检测过程中的精确性和减少了假阳性。这个工作为未来对区块chain技术的优化和提高透明度的研究提供了基础，并证明了考虑时间和空间尺度的重要性在区块chain技术的演化中。”
</details></li>
</ul>
<hr>
<h2 id="GASS-Generalizing-Audio-Source-Separation-with-Large-scale-Data"><a href="#GASS-Generalizing-Audio-Source-Separation-with-Large-scale-Data" class="headerlink" title="GASS: Generalizing Audio Source Separation with Large-scale Data"></a>GASS: Generalizing Audio Source Separation with Large-scale Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00140">http://arxiv.org/abs/2310.00140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordi Pons, Xiaoyu Liu, Santiago Pascual, Joan Serrà</li>
<li>for:  separating speech, music, and sound events in a supervised fashion</li>
<li>methods:  trained on a large-scale dataset, GASS models show feasibility and competitive performance in in-distribution tasks, but struggle with generalizing to out-of-distribution cinematic and music content</li>
<li>results:  all fine-tuned models (except the music separation one) obtain state-of-the-art results in their respective benchmarks.<details>
<summary>Abstract</summary>
Universal source separation targets at separating the audio sources of an arbitrary mix, removing the constraint to operate on a specific domain like speech or music. Yet, the potential of universal source separation is limited because most existing works focus on mixes with predominantly sound events, and small training datasets also limit its potential for supervised learning. Here, we study a single general audio source separation (GASS) model trained to separate speech, music, and sound events in a supervised fashion with a large-scale dataset. We assess GASS models on a diverse set of tasks. Our strong in-distribution results show the feasibility of GASS models, and the competitive out-of-distribution performance in sound event and speech separation shows its generalization abilities. Yet, it is challenging for GASS models to generalize for separating out-of-distribution cinematic and music content. We also fine-tune GASS models on each dataset and consistently outperform the ones without pre-training. All fine-tuned models (except the music separation one) obtain state-of-the-art results in their respective benchmarks.
</details>
<details>
<summary>摘要</summary>
通用源分离目标是将混合声音中的声音源分离开，去除特定频谱域，如语音或乐器的限制。然而，通用源分离的潜力受到许多现有作品的限制，因为大多数作品只关注具有主导性的声音事件混合，而小规模的训练集也限制了其潜力。在这里，我们研究了一个普适的音频源分离（GASS）模型，通过大规模数据集进行supervised学习，用于分离语音、乐器和声音事件。我们评估GASS模型在多种任务上。我们的强大在distribution中的结果表明GASS模型的可能性，而在声音事件和语音分离任务上的竞争性表现也表明它的泛化能力。然而，GASS模型在分离非典型电影和音乐内容时存在挑战。我们还细化GASS模型，并在每个数据集上进行了适应。所有细化模型（除了音乐分离）在各自的benchmark中获得了state-of-the-art的结果。
</details></li>
</ul>
<hr>
<h2 id="ABScribe-Rapid-Exploration-of-Multiple-Writing-Variations-in-Human-AI-Co-Writing-Tasks-using-Large-Language-Models"><a href="#ABScribe-Rapid-Exploration-of-Multiple-Writing-Variations-in-Human-AI-Co-Writing-Tasks-using-Large-Language-Models" class="headerlink" title="ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models"></a>ABScribe: Rapid Exploration of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00117">http://arxiv.org/abs/2310.00117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan “Michael” Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams</li>
<li>for: 提高人类-AI合写作业的效率和体验，使用大语言模型生成多个写作变体。</li>
<li>methods: 使用LLM提示语生成多个写作变体，并将其自动转换为可重用按钮。通过鼠标悬停比较按钮的文本内容，实现快速在地点比较多个变体。</li>
<li>results: 对12名写作者进行了用户研究，发现ABScribe可以减少任务工作量(d&#x3D;1.20，p&lt;0.001)，提高用户对修改过程的认知(d&#x3D;2.41，p&lt;0.001)，并为写作者提供了如何使用LLM生成多个写作变体的经验。<details>
<summary>Abstract</summary>
Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art large language models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new versions without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly produce multiple variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text segments for rapid in-place comparisons using mouse-over interactions on a context toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions of the revision process (d = 2.41, p < 0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "state-of-the-art large language models" Current Chinese translation: "当前的大语言模型"* "simplify writing variation generation" Current Chinese translation: "简化文本变化生成"* "current interfaces" Current Chinese translation: "当前的接口"* "creating new versions without overwriting text" Current Chinese translation: "无需覆盖原文本创建新版本"* "pasting them sequentially" Current Chinese translation: "sequentially paste them"* "clutter the document" Current Chinese translation: "拥堵文档"* "increasing the workload" Current Chinese translation: "增加工作负担"* "disrupting the writer's flow" Current Chinese translation: "打断作者的流程"* "to tackle this" Current Chinese translation: "以解决这个问题"* "users can swiftly produce multiple variations" Current Chinese translation: "用户可快速生成多个变化"* "which are auto-converted into reusable buttons" Current Chinese translation: "自动转换为可重用的按钮"* "variations are stored adjacently within text segments" Current Chinese translation: "变化存储在文本段中侧"* "for rapid in-place comparisons" Current Chinese translation: "以快速比较"* "using mouse-over interactions on a context toolbar" Current Chinese translation: "通过鼠标 hover 在上下文工具栏上"* "our user study with 12 writers" Current Chinese translation: "我们的12名作者用户研究"* "significantly reduces task workload" Current Chinese translation: "显著减少任务工作负担"* "enhances user perceptions of the revision process" Current Chinese translation: "提高用户对修订过程的认知"* "compared to a popular baseline workflow" Current Chinese translation: "相比一个流行的基线工作流"* "and provides insights into how writers explore variations using LLMs" Current Chinese translation: "并提供了如何使用 LLMS 进行文本变化探索的洞察"
</details></li>
</ul>
<hr>
<h2 id="Certified-Robustness-via-Dynamic-Margin-Maximization-and-Improved-Lipschitz-Regularization"><a href="#Certified-Robustness-via-Dynamic-Margin-Maximization-and-Improved-Lipschitz-Regularization" class="headerlink" title="Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization"></a>Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00116">http://arxiv.org/abs/2310.00116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa</li>
<li>for: 提高深度分类器对干扰抗 rand 的Robustness</li>
<li>methods: 使用 Lipschitz-capped networks 或者 modify 训练过程（例如 min-max 优化、受限学习、或者 regularization）来提高深度分类器的Robustness，但这些方法可能不会直接提高输入空间中的边界。因此，有越来越多的关注于在输入空间中直接操纵决策边界的训练方法。本文基于最近的发展，提出一种robust训练算法，其目标是在输出空间（logit）中增大边界，同时对模型在敏感方向上的Lipschitz常数进行规范。我们显示这两个目标可以直接提高输入空间中的边界。</li>
<li>results: 我们实现了一种可扩展的方法来计算准确和高效地计算神经网络的Lipschitz常数上下文约束。这些约束可以用来设计新的层，以实现控制边界的Liptsitz常数。我们在MNIST、CIFAR-10和Tiny-ImageNet数据集上进行实验，并证明我们的提posed算法可以与当前最佳竞争力相当。<details>
<summary>Abstract</summary>
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calculating guaranteed differentiable upper bounds on the Lipschitz constant of neural networks accurately and efficiently. The relative accuracy of the bounds prevents excessive regularization and allows for more direct manipulation of the decision boundary. Furthermore, our Lipschitz bounding algorithm exploits the monotonicity and Lipschitz continuity of the activation layers, and the resulting bounds can be used to design new layers with controllable bounds on their Lipschitz constant. Experiments on the MNIST, CIFAR-10, and Tiny-ImageNet data sets verify that our proposed algorithm obtains competitively improved results compared to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
deep 分类器的可靠性面临各种攻击，许多方法已经被提出，如设计新架构（例如 Lipschitz 围限网络）或修改训练过程（例如最小最大优化、受限学习或规范）。这些方法可能并不能提高输入空间中的边界。因此，有越来越多的关注于开发可以直接操作输入空间的决策边界的训练方法。在这篇论文中，我们基于最近的发展，开发了一种可靠的训练算法，其目标是在输出（幂值）空间中增大边界，同时对模型的敏感方向进行规范。我们表明这两个目标可以直接提高输入空间中的边界。为此，我们开发了一种可执行的方法来计算准确和高效地计算神经网络的 Lipschitz 常数上下文约束。这种约束的相对准确性防止过度规范，allowing for more direct manipulation of the decision boundary。此外，我们的 Lipschitz 约束算法利用神经网络的活化层的 monotonicity 和 Lipschitz 连续性，并且得到的约束可以用来设计新的层，其 Lipschitz 常数具有可控的上限。实验 verify 在 MNIST、CIFAR-10 和 Tiny-ImageNet 数据集上，我们提出的方法与状态艺术的竞争力相当。
</details></li>
</ul>
<hr>
<h2 id="HyperMask-Adaptive-Hypernetwork-based-Masks-for-Continual-Learning"><a href="#HyperMask-Adaptive-Hypernetwork-based-Masks-for-Continual-Learning" class="headerlink" title="HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning"></a>HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00113">http://arxiv.org/abs/2310.00113</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmum/hypermask">https://github.com/gmum/hypermask</a></li>
<li>paper_authors: Kamil Książek, Przemysław Spurek</li>
<li>for: 解决人工神经网络的悬崖性学习问题，即在练习多个任务后，模型会忘记之前学习的知识。</li>
<li>methods: 使用hypernetwork生成目标模型的权重，根据任务的标识来生成不同的嵌入空间。</li>
<li>results: 提出了一种方法 called HyperMask，可以训练一个网络来满足所有任务，并使用hypernetwork生成半二进制的面积来获得每个任务的专门的子网络。这种解决方案继承了hypernetwork的适应新任务能力，同时使用了lottery ticket假设，可以使用单个网络来满足所有任务。<details>
<summary>Abstract</summary>
Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, there exist many continual learning strategies. One of the most effective is the hypernetwork-based approach. The hypernetwork generates the weights of a target model based on the task's identity. The model's main limitation is that hypernetwork can produce completely different nests for each task. Consequently, each task is solved separately. The model does not use information from the network dedicated to previous tasks and practically produces new architectures when it learns the subsequent tasks. To solve such a problem, we use the lottery ticket hypothesis, which postulates the existence of sparse subnetworks, named winning tickets, that preserve the performance of a full network. In the paper, we propose a method called HyperMask, which trains a single network for all tasks. Hypernetwork produces semi-binary masks to obtain target subnetworks dedicated to new tasks. This solution inherits the ability of the hypernetwork to adapt to new tasks with minimal forgetting. Moreover, due to the lottery ticket hypothesis, we can use a single network with weighted subnets dedicated to each task.
</details>
<details>
<summary>摘要</summary>
为解决这个问题，我们使用了彩票假设，假设存在一些稀疏的子网络，称为赢家票，它们保持了全网络的性能。在我们的论文中，我们提出了一种方法called HyperMask，它在所有任务上训练单个网络。hypernetwork 生成了半二进制的面纱，以获取新任务的专门的子网络。这种解决方案继承了 hypernetwork 对新任务的适应能力，同时因为彩票假设，我们可以使用单个网络，每个任务都有权重的子网络。
</details></li>
</ul>
<hr>
<h2 id="FashionFlow-Leveraging-Diffusion-Models-for-Dynamic-Fashion-Video-Synthesis-from-Static-Imagery"><a href="#FashionFlow-Leveraging-Diffusion-Models-for-Dynamic-Fashion-Video-Synthesis-from-Static-Imagery" class="headerlink" title="FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery"></a>FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00106">http://arxiv.org/abs/2310.00106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tasin Islam, Alina Miron, XiaoHui Liu, Yongmin Li</li>
<li>for: 这个研究旨在创造一种基于扩散模型的图像到视频生成器，以便从单一图像中生成短视频。</li>
<li>methods: 我们的方法包括开发和连接相关组件，例如使用 Pseudo-3D 卷积层生成视频高效地。我们还使用 VAE 和 CLIP 编码器从图像中捕捉重要特征，以影响扩散模型。</li>
<li>results: 我们的研究成果包括成功 sinthez 时尚视频，展示了模特儿从多个角度的抵抗，展示了衣服的适用和外观。这些发现对于在线时尚销售业可以提供有利的改进和提升。<details>
<summary>Abstract</summary>
Our study introduces a new image-to-video generator called FashionFlow. By utilising a diffusion model, we are able to create short videos from still images. Our approach involves developing and connecting relevant components with the diffusion model, which sets our work apart. The components include the use of pseudo-3D convolutional layers to generate videos efficiently. VAE and CLIP encoders capture vital characteristics from still images to influence the diffusion model. Our research demonstrates a successful synthesis of fashion videos featuring models posing from various angles, showcasing the fit and appearance of the garment. Our findings hold great promise for improving and enhancing the shopping experience for the online fashion industry.
</details>
<details>
<summary>摘要</summary>
我们的研究推出了一种新的图像到视频生成器 called FashionFlow。通过利用扩散模型，我们可以从静止图像中创建短视频。我们的方法是连接相关的组件与扩散模型，这种方法与之前的工作不同。这些组件包括使用 Pseudo-3D 卷积层生成视频高效地。VAE 和 CLIP 编码器捕捉静止图像中重要的特征，这些特征影响扩散模型。我们的研究成功创造了时尚视频，模特儿从多个角度 пози摆，展示裤装的适合和外观。我们的发现对在线时尚业务的改进和提升具有很大的投资意义。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-Natural-Language-Processing-Model-for-Radiology-Reports-–-The-Summary-is-all-you-need"><a href="#Multilingual-Natural-Language-Processing-Model-for-Radiology-Reports-–-The-Summary-is-all-you-need" class="headerlink" title="Multilingual Natural Language Processing Model for Radiology Reports – The Summary is all you need!"></a>Multilingual Natural Language Processing Model for Radiology Reports – The Summary is all you need!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00100">http://arxiv.org/abs/2310.00100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariana Lindo, Ana Sofia Santos, André Ferreira, Jianning Li, Gijs Luijten, Gustavo Correia, Moon Kim, Jens Kleesiek, Jan Egger, Victor Alves</li>
<li>for: 这个研究旨在自动生成不同语言的 radiology 报告摘要，以便将来研究和 Deep Learning 模型中包含不同民族背景的患者数据。</li>
<li>methods: 研究人员使用了一种公共可用的多语言文本-文本 transformer 模型进行练习，以自动摘要英语、葡萄牙语和德语 radiology 报告中的发现。</li>
<li>results: 盲测试中，两位board-certified radiologist表示，系统生成的摘要质量与人工摘要相匹配或超越了70%的情况，表明了严重的临床可靠性。此外，本研究还表明了特定 радиологи 报告的多语言模型比特定语言的模型和非特定报告摘要模型（如 ChatGPT）更高效。<details>
<summary>Abstract</summary>
The impression section of a radiology report summarizes important radiology findings and plays a critical role in communicating these findings to physicians. However, the preparation of these summaries is time-consuming and error-prone for radiologists. Recently, numerous models for radiology report summarization have been developed. Nevertheless, there is currently no model that can summarize these reports in multiple languages. Such a model could greatly improve future research and the development of Deep Learning models that incorporate data from patients with different ethnic backgrounds. In this study, the generation of radiology impressions in different languages was automated by fine-tuning a model, publicly available, based on a multilingual text-to-text Transformer to summarize findings available in English, Portuguese, and German radiology reports. In a blind test, two board-certified radiologists indicated that for at least 70% of the system-generated summaries, the quality matched or exceeded the corresponding human-written summaries, suggesting substantial clinical reliability. Furthermore, this study showed that the multilingual model outperformed other models that specialized in summarizing radiology reports in only one language, as well as models that were not specifically designed for summarizing radiology reports, such as ChatGPT.
</details>
<details>
<summary>摘要</summary>
radiology 报告的印象部分总结重要的放射学发现，并在传达这些发现给医生时扮演关键的角色。然而，准备这些总结是时间费时和容易出错的。近些年来，许多放射学报告总结模型已经开发出来。然而，目前没有任何模型可以在多种语言中总结这些报告。这样的模型可以在未来的研究和深度学习模型中提高数据来源于不同的民族背景的patient的可靠性。本研究通过微调一个基于多语言文本-文本转换器的公共可用模型，自动生成了不同语言的放射学报告总结。在盲测中，两位证enciated的放射学医生指出，至少70%的系统生成的总结质量与相应的人类写的总结匹配或超过，表明了严重的临床可靠性。此外，本研究表明，多语言模型在只有一种语言的放射学报告总结模型和不特定于放射学报告总结的模型（如ChatGPT）之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Voice2Action-Language-Models-as-Agent-for-Efficient-Real-Time-Interaction-in-Virtual-Reality"><a href="#Voice2Action-Language-Models-as-Agent-for-Efficient-Real-Time-Interaction-in-Virtual-Reality" class="headerlink" title="Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality"></a>Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00092">http://arxiv.org/abs/2310.00092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yang-su2000/vr-multimodal-interaction">https://github.com/yang-su2000/vr-multimodal-interaction</a></li>
<li>paper_authors: Yang Su</li>
<li>for: 本研究旨在提高自动化代理人类型语言模型（LLM）在虚拟现实环境中的效率和准确性。</li>
<li>methods: 该研究提出了一种名为“语音到动作”框架，该框架可以在实时进行自然语言指令的分析和实体提取，并将执行任务分解成可能的互动子集。</li>
<li>results: 实验结果表明，使用“语音到动作”框架可以在虚拟都市工程环境中提高LLM的效率和准确性，并且比不使用优化的方法有所提高。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常在几个示例下进行训练和Alignment，并被驱动为执行任务，并且可以适应不同的执行环境。然而，将代理LLM部署到虚拟现实（VR）中具有线上互动效率低下和3D环境中复杂的操作类别的挑战。在这个工作中，我们提出了“声音到动作”框架，通过层次分析自定义的声音信号和文本指令，并在实时运行中分配执行任务 into canonical interaction subsets，并透过环境反馈来预防错误。实验结果显示，在城市工程VR环境中使用合成 instruciton data时，“声音到动作”可以比不具有优化的方法更高效和精度。
</details></li>
</ul>
<hr>
<h2 id="SocREval-Large-Language-Models-with-the-Socratic-Method-for-Reference-Free-Reasoning-Evaluation"><a href="#SocREval-Large-Language-Models-with-the-Socratic-Method-for-Reference-Free-Reasoning-Evaluation" class="headerlink" title="SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation"></a>SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00074">http://arxiv.org/abs/2310.00074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hornhehhf/socreval">https://github.com/hornhehhf/socreval</a></li>
<li>paper_authors: Hangfeng He, Hongming Zhang, Dan Roth</li>
<li>for: 评估复杂逻辑模型的能力</li>
<li>methods: 使用GPT-4自动评估逻辑链质量，不需要人工编写参考链</li>
<li>results: 比较现有的参考自由和参考基的逻辑评估指标，SocREval显著提高GPT-4的表现<details>
<summary>Abstract</summary>
To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic method for Reasoning Evaluation). Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, our proposed framework, large language models (LLMs) with the Socratic method, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)为了全面评估当前模型的复杂逻辑能力，需要在可扩展的方式进行步骤逻辑评估。现有的参考基础评估 metrics 依靠人工标注的逻辑链来评估模型生成的链。然而，这些“金标准”的人工编写的逻辑链可能不唯一，并且获取它们可以是劳动密集的。现有的参考自由逻辑评估 metric 可以消除人工编写的逻辑链作为参考，但它们通常需要在具有人工编写的逻辑链的 dataset 上进行 fine-tuning，这会增加过程的复杂性并使得其在不同的 dataset 上的普遍性受到质疑。为解决这些挑战，我们利用 GPT-4 自动评估逻辑链质量，不需要人工编写参考。通过索克拉孚方法，我们开发了特制的 prompt 来提高参考自由逻辑评估，我们称之为 SocREval（索克拉孚方法 для逻辑评估）。Empirical results from four human-annotated datasets show that SocREval significantly improves GPT-4's performance, outperforming existing reference-free and reference-based reasoning evaluation metrics. In addition, our proposed framework, large language models (LLMs) with the Socratic method, is both cost-efficient and robust to prompt writing and example selection, as demonstrated by our in-depth analysis.
</details></li>
</ul>
<hr>
<h2 id="Emotional-Listener-Portrait-Neural-Listener-Head-Generation-with-Emotion"><a href="#Emotional-Listener-Portrait-Neural-Listener-Head-Generation-with-Emotion" class="headerlink" title="Emotional Listener Portrait: Neural Listener Head Generation with Emotion"></a>Emotional Listener Portrait: Neural Listener Head Generation with Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00068">http://arxiv.org/abs/2310.00068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luchuan Song, Guojun Yin, Zhenchao Jin, Xiaoyi Dong, Chenliang Xu</li>
<li>for: 本研究旨在生成对话中的听众表情（如笑容），以提高人机对话的自然性和多样性。</li>
<li>methods: 本研究提出了Emotional Listener Portrait（ELP）模型，将每个细腻的表情动作视为多个精确的动作代码词的组合，并且Explicitly model了不同情感对话中的动作概率分布。</li>
<li>results: 对多个量化指标进行评测，ELP模型在比较前方法时显示了明显的改善。<details>
<summary>Abstract</summary>
Listener head generation centers on generating non-verbal behaviors (e.g., smile) of a listener in reference to the information delivered by a speaker. A significant challenge when generating such responses is the non-deterministic nature of fine-grained facial expressions during a conversation, which varies depending on the emotions and attitudes of both the speaker and the listener. To tackle this problem, we propose the Emotional Listener Portrait (ELP), which treats each fine-grained facial motion as a composition of several discrete motion-codewords and explicitly models the probability distribution of the motions under different emotion in conversation. Benefiting from the ``explicit'' and ``discrete'' design, our ELP model can not only automatically generate natural and diverse responses toward a given speaker via sampling from the learned distribution but also generate controllable responses with a predetermined attitude. Under several quantitative metrics, our ELP exhibits significant improvements compared to previous methods.
</details>
<details>
<summary>摘要</summary>
听众头部生成中心在生成说话者所提供的信息的非语言表达（例如笑容）中心。生成这些响应的主要挑战是在对话中细分的表情的非推定性，这取决于说话者和听众的情感和态度。为解决这个问题，我们提议了情感听众肖像（ELP），它将每个细分的facial motion作为多个独立的动作代码词的组合，并Explicitly modeling the probability distribution of motions under different emotions in conversation。由于“显式”和“独立”的设计，我们的ELP模型可以不仅通过从学习的分布中采样生成自然和多样化的响应，还可以生成 predetermined 的态度。在多个量化指标下，我们的ELP显示出了significant improvement compared to previous methods。
</details></li>
</ul>
<hr>
<h2 id="AI-ensemble-for-signal-detection-of-higher-order-gravitational-wave-modes-of-quasi-circular-spinning-non-precessing-binary-black-hole-mergers"><a href="#AI-ensemble-for-signal-detection-of-higher-order-gravitational-wave-modes-of-quasi-circular-spinning-non-precessing-binary-black-hole-mergers" class="headerlink" title="AI ensemble for signal detection of higher order gravitational wave modes of quasi-circular, spinning, non-precessing binary black hole mergers"></a>AI ensemble for signal detection of higher order gravitational wave modes of quasi-circular, spinning, non-precessing binary black hole mergers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00052">http://arxiv.org/abs/2310.00052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minyang Tian, E. A. Huerta, Huihuo Zheng</li>
<li>for: 这个论文是用于开发一个基于人工智能的搜寻引力波高次模式信号的方法。</li>
<li>methods: 这个方法使用了240万个IMRPhenomXPHM波形数据，包括了不同的黑洞组合质量和旋转速度，以及不同的模式混合效应。这些波形数据被用于训练3个人工智能分类器，并在Summit超级计算机上进行了分布式训练。然后，这些分类器被用于创建一个人工预测器，以估计潜在的黑洞组合质量。</li>
<li>results: 这个方法在一年测试集中处理了300,000个信号，并在5.19分钟内完成了。这个方法提供了现有最高精度的信号检测精度，并且只有在每年的搜寻数据中出现了2个错误。这是第一个用于搜寻和检测高次引力波模式信号的人工智能 ensemble。<details>
<summary>Abstract</summary>
We introduce spatiotemporal-graph models that concurrently process data from the twin advanced LIGO detectors and the advanced Virgo detector. We trained these AI classifiers with 2.4 million \texttt{IMRPhenomXPHM} waveforms that describe quasi-circular, spinning, non-precessing binary black hole mergers with component masses $m_{\{1,2\}\in[3M_\odot, 50 M_\odot]$, and individual spins $s^z_{\{1,2\}\in[-0.9, 0.9]$; and which include the $(\ell, |m|) = \{(2, 2), (2, 1), (3, 3), (3, 2), (4, 4)\}$ modes, and mode mixing effects in the $\ell = 3, |m| = 2$ harmonics. We trained these AI classifiers within 22 hours using distributed training over 96 NVIDIA V100 GPUs in the Summit supercomputer. We then used transfer learning to create AI predictors that estimate the total mass of potential binary black holes identified by all AI classifiers in the ensemble. We used this ensemble, 3 AI classifiers and 2 predictors, to process a year-long test set in which we injected 300,000 signals. This year-long test set was processed within 5.19 minutes using 1024 NVIDIA A100 GPUs in the Polaris supercomputer (for AI inference) and 128 CPU nodes in the ThetaKNL supercomputer (for post-processing of noise triggers), housed at the Argonne Leadership Supercomputing Facility. These studies indicate that our AI ensemble provides state-of-the-art signal detection accuracy, and reports 2 misclassifications for every year of searched data. This is the first AI ensemble designed to search for and find higher order gravitational wave mode signals.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种空时间图模型，该模型同时处理了激光探测器先进LIGO和先进维戈探测器的数据。我们使用了240万个IMRPhenomXPHM波形，这些波形描述了 quasi-Circular，旋转，不径向旋转的双黑洞合并，其中Component masses $m_{\{1,2\}$在[3M_\odot, 50M_\odot]之间，并且individual spins $s^z_{\{1,2\}$在[-0.9, 0.9]之间，包括($\ell, |m|) = \{(2, 2), (2, 1), (3, 3), (3, 2), (4, 4)\}$ modes，以及模式混合效应在$\ell = 3, |m| = 2$ harmonics。我们在Summit超级计算机上使用分布式训练在96个NVIDIA V100 GPU上训练了这些AI类ifizzer，训练时间为22小时。然后，我们使用传输学习创建了AI预测器，以估算潜在双黑洞的总质量。我们使用了这个ensemble，3个AI类ifizzer和2个预测器，处理了一年的测试集，其中注入了300,000个信号。这个一年的测试集在使用1024个NVIDIA A100 GPU在Polaris超级计算机上进行AI推理，以及128个CPU节点在ThetaKNL超级计算机上进行噪声触发的后处理，在Argonne领导超级计算机中完成 within 5.19分钟。这些研究表明，我们的AIensemble提供了当前最高精度的信号探测精度，并且错误地分类了每年的搜索数据2次。这是第一个采用AI搜索和找到更高级别重力波模式信号的ensemble。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Streaming-Language-Models-with-Attention-Sinks"><a href="#Efficient-Streaming-Language-Models-with-Attention-Sinks" class="headerlink" title="Efficient Streaming Language Models with Attention Sinks"></a>Efficient Streaming Language Models with Attention Sinks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17453">http://arxiv.org/abs/2309.17453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mit-han-lab/streaming-llm">https://github.com/mit-han-lab/streaming-llm</a></li>
<li>paper_authors: Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis</li>
<li>For: The paper is written for deploying large language models (LLMs) in streaming applications, such as multi-round dialogue, where long interactions are expected.* Methods: The paper introduces a framework called StreamingLLM, which enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning.* Results: The paper shows that StreamingLLM can enable several state-of-the-art LLMs to perform stable and efficient language modeling with up to 4 million tokens and more, and outperforms the sliding window recomputation baseline by up to 22.2x speedup in streaming settings.Here is the same information in Simplified Chinese:* For: 这篇论文是为了在流动应用中部署大型语言模型（LLMs），例如多轮对话，长期交互等。* Methods: 论文提出了一种名为 StreamingLLM 的框架，可以让 LLMs trained with finite length attention window 通过不需要微调来泛化到无限长序列 lengths。* Results: 论文显示了 StreamingLLM 可以使得多种状态顶峰的 LLMs 在多达 4 百万字和更多的语言模型中表现稳定和高效，并在流动设置下超过滑动窗口重新计算基准的速度提升至多达 22.2 倍。<details>
<summary>Abstract</summary>
Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.
</details>
<details>
<summary>摘要</summary>
部署大型自然语言模型（LLM）在流动应用程序中，如多回对话，需求急快，但也存在两大挑战。首先，在解码阶段，缓存前一个token的键值（KV）占用了大量内存。其次，流行的LLM无法泛化到 longer than training sequence length的文本。窗口注意力，只缓存最近的KV，是一种自然的方法，但我们发现，当文本长度超过缓存大小时，窗口注意力失效。我们观察到了一种有趣现象，即注意力沟（attention sink），即保留初始token的KV可以大幅提高窗口注意力的性能。在这篇论文中，我们首先证明了注意力沟的出现是因为初始token的强烈注意力分数，即作为一个“沟”，即使它们并不是semantically important。基于以上分析，我们提出了StreamingLLM框架，可以让 LLMS 在训练时使用有限长度注意力窗口，并不需要 fine-tuning，可以在无限长度文本上进行稳定和高效的语言模型化。我们证明了StreamingLLM在流动设置下可以使得 Llama-2、MPT、Falcon 和 Pythia 等模型在400万个token和更多的文本上进行稳定和高效的语言模型化。此外，我们发现在预训练时添加一个专门用于注意力沟的placeholder token可以进一步提高流动部署的性能。在流动设置下，StreamingLLM可以与滑动窗口重新计算基准线上的差速度相比，提高到22.2倍。代码和数据集可以在https://github.com/mit-han-lab/streaming-llm上获取。
</details></li>
</ul>
<hr>
<h2 id="ToRA-A-Tool-Integrated-Reasoning-Agent-for-Mathematical-Problem-Solving"><a href="#ToRA-A-Tool-Integrated-Reasoning-Agent-for-Mathematical-Problem-Solving" class="headerlink" title="ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"></a>ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17452">http://arxiv.org/abs/2309.17452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/tora">https://github.com/microsoft/tora</a></li>
<li>paper_authors: Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen</li>
<li>for: 本研究旨在提高大型语言模型在数学问题上的表现，通过将自然语言理解与外部工具（如计算库和符号计算器）集成在一起，以提高模型的数学分析能力。</li>
<li>methods: 我们提出了一种名为ToRA的工具集成逻辑代理模型，通过在数学数据集上实现互动工具使用轨迹，应用imiter学习onto annotations，并通过输出空间形状进一步细化模型的理解行为。</li>
<li>results: 我们的ToRA模型在10个数学理解数据集上表现出色，与开源模型相比，具有13%-19%的绝对提升，而ToRA-7B模型在竞赛水平数据集MATH上达到44.6%的高分，超过了最佳开源模型WizardMath-70B的22%绝对提升。此外，ToRA-Code-34B模型在MATH数据集上达到了50%以上的准确率，超过了GPT-4的CoT结果，并与GPT-4解决问题的程度相当。<details>
<summary>Abstract</summary>
Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经在不同的语言任务中做出了重要进步，但它们仍然对复杂的数学问题须作出努力。在这篇论文中，我们提出了 Tool-integrated Reasoning Agents（ToRA），用于解决具有挑战性的数学问题。ToRA通过融合自然语言理解和工具（如计算库和 симвоlic solvers）的能力，实现了语言和工具之间的融合。为了训练 ToRA，我们创建了互动工具使用轨迹，对数学数据集进行归检学习，并提出了出力空间整形以进一步检验模型的理解行为。因此，ToRA模型在10个数学理解数据集上显示了13%-19%的绝对提升，而 ToRA-7B 更是在竞赛水平数据集 MATH 上取得了44.6%的成绩，比开源模型 WizardMath-70B 高出22%的绝对提升。此外，ToRA-Code-34B 是首个在 MATH 数据集上取得超过50%的开源模型，与 GPT-4 解决问题的程度相当，并且与 GPT-4 解决问题的程度相当。我们还进行了数学理解中工具互动的全面分析，提供了宝贵的研究指引。
</details></li>
</ul>
<hr>
<h2 id="LLM-grounded-Video-Diffusion-Models"><a href="#LLM-grounded-Video-Diffusion-Models" class="headerlink" title="LLM-grounded Video Diffusion Models"></a>LLM-grounded Video Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17444">http://arxiv.org/abs/2309.17444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TonyLianLong/LLM-groundedVideoDiffusion">https://github.com/TonyLianLong/LLM-groundedVideoDiffusion</a></li>
<li>paper_authors: Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, Boyi Li</li>
<li>for: 这个论文是为了提高文本生成视频的质量，尤其是处理复杂的时空异常指令。</li>
<li>methods: 这个论文使用了语言模型（LLM）生成动态场景布局，并将布局用于导引扩散模型进行视频生成。</li>
<li>results: 研究发现，LLM可以通过文本 alone 理解复杂的时空动态，并生成布局与实际世界中的物体运动模式高度相似。这种方法可以与任何允许分类引导的扩散模型结合使用，并在比较强的基础模型和基线方法上显著超越。<details>
<summary>Abstract</summary>
Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion (e.g., even lacking the ability to be prompted for objects moving from left to right). To address these limitations, we introduce LLM-grounded Video Diffusion (LVD). Instead of directly generating videos from the text inputs, LVD first leverages a large language model (LLM) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that LLMs are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance. Our results demonstrate that LVD significantly outperforms its base video diffusion model and several strong baseline methods in faithfully generating videos with the desired attributes and motion patterns.
</details>
<details>
<summary>摘要</summary>
文本填充扩散模型已经出现为神经视频生成工具的有力手段。然而，当前模型仍然面临细腻的时空提示和生成限制，例如缺乏左右移动的能力。为解决这些限制，我们介绍了基于大语言模型（LLM）的视频扩散（LVD）。而不是直接从文本输入生成视频，LVD首先利用LLM生成基于文本输入的动态场景布局，然后使用生成的布局来导引扩散模型进行视频生成。我们发现LLM可以通过文本 alone 理解复杂的时空动态，并生成布局与提示和实际世界中对象运动模式相似。我们then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance. Our results demonstrate that LVD significantly outperforms its base video diffusion model and several strong baseline methods in faithfully generating videos with the desired attributes and motion patterns.
</details></li>
</ul>
<hr>
<h2 id="Learning-Decentralized-Flocking-Controllers-with-Spatio-Temporal-Graph-Neural-Network"><a href="#Learning-Decentralized-Flocking-Controllers-with-Spatio-Temporal-Graph-Neural-Network" class="headerlink" title="Learning Decentralized Flocking Controllers with Spatio-Temporal Graph Neural Network"></a>Learning Decentralized Flocking Controllers with Spatio-Temporal Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17437">http://arxiv.org/abs/2309.17437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siji Chen, Yanshen Sun, Peihan Li, Lifeng Zhou, Chang-Tien Lu</li>
<li>for: 用于实现狂涛飞行器群体协调控制</li>
<li>methods: 使用延迟-$L$ hop状态和时间扩展GNN（STGNN）</li>
<li>results: 实现了分布式控制，模仿中央控制策略，并在不同场景下实现了凝聚飞行、领头飞行和避免障碍等任务。<details>
<summary>Abstract</summary>
Recently a line of researches has delved the use of graph neural networks (GNNs) for decentralized control in swarm robotics. However, it has been observed that relying solely on the states of immediate neighbors is insufficient to imitate a centralized control policy. To address this limitation, prior studies proposed incorporating $L$-hop delayed states into the computation. While this approach shows promise, it can lead to a lack of consensus among distant flock members and the formation of small clusters, consequently resulting in the failure of cohesive flocking behaviors. Instead, our approach leverages spatiotemporal GNN, named STGNN that encompasses both spatial and temporal expansions. The spatial expansion collects delayed states from distant neighbors, while the temporal expansion incorporates previous states from immediate neighbors. The broader and more comprehensive information gathered from both expansions results in more effective and accurate predictions. We develop an expert algorithm for controlling a swarm of robots and employ imitation learning to train our decentralized STGNN model based on the expert algorithm. We simulate the proposed STGNN approach in various settings, demonstrating its decentralized capacity to emulate the global expert algorithm. Further, we implemented our approach to achieve cohesive flocking, leader following and obstacle avoidance by a group of Crazyflie drones. The performance of STGNN underscores its potential as an effective and reliable approach for achieving cohesive flocking, leader following and obstacle avoidance tasks.
</details>
<details>
<summary>摘要</summary>
近期研究团队探索使用图 neuron网络（GNNs）为分布式控制在群体机器人中。然而，已经观察到仅仅基于当前邻居状态不足以模仿中央控制策略。为了解决这个限制，先前的研究提出了 incorporating $L$-hop延迟状态到计算中。虽然这种方法显示了 promise, 但可能导致远程群体成员之间的不一致和小群集成，最终导致协调集成行为失败。相反，我们的方法利用 spatial-temporal GNN， named STGNN，它包括 spatial 和 temporal 扩展。 spatial 扩展收集远程邻居的延迟状态，而 temporal 扩展包括当前邻居的先前状态。通过这两种扩展，我们可以收集更广泛和全面的信息，从而实现更有效和准确的预测。我们开发了一个专家算法来控制群体机器人，并使用模仿学习来训练我们的分布式 STGNN 模型基于专家算法。我们在不同的设置中进行了 STGNN 方法的 simulate，并证明它的分布式能力可以模仿全局专家算法。此外，我们实现了 STGNN 方法以实现凝聚飞行、领导跟随和避免障碍的任务。STGNN 方法的表现强调了它的可靠性和可行性，表明它作为凝聚飞行、领导跟随和避免障碍任务的有效和可靠的方法。
</details></li>
</ul>
<hr>
<h2 id="DREAM-Decentralized-Reinforcement-Learning-for-Exploration-and-Efficient-Energy-Management-in-Multi-Robot-Systems"><a href="#DREAM-Decentralized-Reinforcement-Learning-for-Exploration-and-Efficient-Energy-Management-in-Multi-Robot-Systems" class="headerlink" title="DREAM: Decentralized Reinforcement Learning for Exploration and Efficient Energy Management in Multi-Robot Systems"></a>DREAM: Decentralized Reinforcement Learning for Exploration and Efficient Energy Management in Multi-Robot Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17433">http://arxiv.org/abs/2309.17433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipam Patel, Phu Pham, Kshitij Tiwari, Aniket Bera</li>
<li>for: 这篇论文目的是提出一种可以有效地管理资源的多机器人系统，以提高其性能和可靠性。</li>
<li>methods: 这篇论文使用了强化学习算法来进行环境探索和避免障碍物，并使用图 neural network 来优化目标分配，以确保在有限资源的情况下完成任务。</li>
<li>results: 研究人员通过对多种 simulate 环境进行测试，发现这种方法可以提高资源Constrained robotics 性能，相比基eline方法，提高约25%。<details>
<summary>Abstract</summary>
Resource-constrained robots often suffer from energy inefficiencies, underutilized computational abilities due to inadequate task allocation, and a lack of robustness in dynamic environments, all of which strongly affect their performance. This paper introduces DREAM - Decentralized Reinforcement Learning for Exploration and Efficient Energy Management in Multi-Robot Systems, a comprehensive framework that optimizes the allocation of resources for efficient exploration. It advances beyond conventional heuristic-based task planning as observed conventionally. The framework incorporates Operational Range Estimation using Reinforcement Learning to perform exploration and obstacle avoidance in unfamiliar terrains. DREAM further introduces an Energy Consumption Model for goal allocation, thereby ensuring mission completion under constrained resources using a Graph Neural Network. This approach also ensures that the entire Multi-Robot System can survive for an extended period of time for further missions compared to the conventional approach of randomly allocating goals, which compromises one or more agents. Our approach adapts to prioritizing agents in real-time, showcasing remarkable resilience against dynamic environments. This robust solution was evaluated in various simulated environments, demonstrating adaptability and applicability across diverse scenarios. We observed a substantial improvement of about 25% over the baseline method, leading the way for future research in resource-constrained robotics.
</details>
<details>
<summary>摘要</summary>
资源有限的机器人经常受到能源不fficient和 computation underutilization的问题，这些问题严重影响其表现。这篇文章提出了 DREAM - 分布式学习探索和能源管理 Framework，这个框架可以优化资源的分配，以提高探索的效率。它超越了传统的规律 Based task planning，通过使用循环 Reinforcement Learning 估计运作范围，实现探索和避免障碍物在未知的地形中。 DREAM 还引入了能源消耗模型，以便将目标分配给可以完成任务的机器人，以确保在有限的资源下完成任务。这种方法还能在多机器人系统中保持机器人系统的持续运行，比传统随机分配目标的方法更好。我们的方法能够在实时给予优先级，实现在动态环境中的类型。我们在不同的 simulated 环境进行了评估，发现我们的方法可以在多种enario中实现显著的改善，相比baseline方法，提高约25%。这个可靠的解决方案开启了未来资源有限机器人研究的新 horizon。
</details></li>
</ul>
<hr>
<h2 id="CRAFT-Customizing-LLMs-by-Creating-and-Retrieving-from-Specialized-Toolsets"><a href="#CRAFT-Customizing-LLMs-by-Creating-and-Retrieving-from-Specialized-Toolsets" class="headerlink" title="CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"></a>CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17428">http://arxiv.org/abs/2309.17428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lifan-yuan/craft">https://github.com/lifan-yuan/craft</a></li>
<li>paper_authors: Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung, Hao Peng, Heng Ji</li>
<li>for: 这篇论文是为了提高大语言模型（LLM）的功能和可用性而设计的。</li>
<li>methods: 该论文使用的方法包括：（1）通过生成代码段和通过任务特定的应用程序编程接口（API）执行它们，以将某些功能委托给专门的外部模块。（2）为每个任务收集特定的代码解决方案，并将其抽象为可重用的代码段。（3）在推理时，通过语言模型获取代码段并执行它们或生成输出。</li>
<li>results: 该论文的实验结果表明，使用该方法可以在视觉语言任务、表格处理任务和数学推理任务中实现显著的提升，并且可以在不需要训练的情况下适应新的领域和模式。此外，该论文还进行了深入的分析，并证明了：（1）随着工具集大小和后向模型的能力增加，可以实现一致的性能提升；（2）该方法的每个组件都对性能增加做出了贡献；（3）创建的工具是可靠且具有低复杂性和原子性。<details>
<summary>Abstract</summary>
Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code snippets to enhance reusability, and deduplicated for higher quality. At inference time, the language model retrieves snippets from the toolsets and then executes them or generates the output conditioning on the retrieved snippets. Our method is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning. Experiments on vision-language, tabular processing, and mathematical reasoning tasks show that our approach achieves substantial improvements compared to strong baselines. In addition, our in-depth analysis reveals that: (1) consistent performance improvement can be achieved by scaling up the number of tools and the capability of the backbone models; (2) each component of our approach contributes to the performance gains; (3) the created tools are well-structured and reliable with low complexity and atomicity. The code is available at \url{https://github.com/lifan-yuan/CRAFT}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常会被增强工具来解决复杂任务。通过生成代码批处和透过任务特定的应用程序库（API）执行，它们可以将certain功能外传到特定的外部模组，如图像编码和计算。但现有的方法增强LLM通常受到通用API的限制，缺乏适合特定任务的自适性。在这个工作中，我们提出了CRAFT，一个通用工具创建和撷取框架 для LLM。它创建了特定任务的工具集，并将LLM具有一个组件可以从这些集中撷取工具，以增强它们的解决复杂任务的能力。在每个任务上，我们通过对GPT-4进行训练示例来收集特定的代码解决方案。经过验证步骤以确保正确性，这些解决方案会被抽象为代码批处以增强可重用性，并且删除重复的部分以提高质量。在推断时，语言模型会从工具集撷取批处或生成基于撷取的结果。我们的方法设计为可以灵活地适应市场上的无Seeing领域和模式，而无需调整。实验显示，我们的方法可以与强基eline比较 substatial提升。此外，我们的深入分析显示：（1）可以通过增加工具的数量和背景模型的能力来获得一致的性能提升;（2）我们的方法中每个component都做出了贡献;（3）创建的工具具有低复杂性和原子性。代码可以在 \url{https://github.com/lifan-yuan/CRAFT} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Classification-of-Potholes-Based-on-Surface-Area-Using-Pre-Trained-Models-of-Convolutional-Neural-Network"><a href="#Classification-of-Potholes-Based-on-Surface-Area-Using-Pre-Trained-Models-of-Convolutional-Neural-Network" class="headerlink" title="Classification of Potholes Based on Surface Area Using Pre-Trained Models of Convolutional Neural Network"></a>Classification of Potholes Based on Surface Area Using Pre-Trained Models of Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17426">http://arxiv.org/abs/2309.17426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chauhdary Fazeel Ahmad, Abdullah Cheema, Waqas Qayyum, Rana Ehtisham, Muhammad Haroon Yousaf, Junaid Mir, Nasim Shakouri Mahmoudabadi, Afaq Ahmad</li>
<li>for: 本研究旨在比较三种预训练的卷积神经网络模型（ResNet 50、ResNet 18、MobileNet）在识别南亚国家道路上的损害性痕迹（坑洞）方面的表现。</li>
<li>methods: 本研究使用了三种预训练的卷积神经网络模型，对道路图像进行分类，以判断图像中是否存在坑洞，并将图像分为三类：小坑洞、大坑洞和正常道路。</li>
<li>results: 研究发现，MobileNet v2在识别坑洞方面的准确率为98%，而图像分类结果显示，图像从腰高度（2英尺）拍摄时，大坑洞、小坑洞和正常道路的准确率分别为87.33%、88.67%和92%。同时，从全腰高度（FFW）拍摄时，三个类别的准确率均为100%。<details>
<summary>Abstract</summary>
Potholes are fatal and can cause severe damage to vehicles as well as can cause deadly accidents. In South Asian countries, pavement distresses are the primary cause due to poor subgrade conditions, lack of subsurface drainage, and excessive rainfalls. The present research compares the performance of three pre-trained Convolutional Neural Network (CNN) models, i.e., ResNet 50, ResNet 18, and MobileNet. At first, pavement images are classified to find whether images contain potholes, i.e., Potholes or Normal. Secondly, pavements images are classi-fied into three categories, i.e., Small Pothole, Large Pothole, and Normal. Pavement images are taken from 3.5 feet (waist height) and 2 feet. MobileNet v2 has an accuracy of 98% for detecting a pothole. The classification of images taken at the height of 2 feet has an accuracy value of 87.33%, 88.67%, and 92% for classifying the large, small, and normal pavement, respectively. Similarly, the classification of the images taken from full of waist (FFW) height has an accuracy value of 98.67%, 98.67%, and 100%.
</details>
<details>
<summary>摘要</summary>
弹坑可以致命和对车辆造成严重损害，并可能导致致命车祸。在南亚国家，路面问题是主要原因，因为下层状况差，没有对地下排水系统，以及过度的雨水。本研究比较了三个预训NN模型的表现，即ResNet 50、ResNet 18和MobileNet。首先，路面图像被分类，以找出图像中是否存在弹坑（Potholes或Normal）。其次，路面图像被分类为三个类别，即小弹坑、大弹坑和正常。路面图像来自2英尺（胸高）和3.5英尺。MobileNet v2的准确率为98%，检测弹坑。图像分类的准确率为87.33%、88.67%和92%，分别类别为大弹坑、小弹坑和正常路面。 Similarly，图像从胸高（FFW）高度时的准确率为98.67%、98.67%和100%。
</details></li>
</ul>
<hr>
<h2 id="Data-Filtering-Networks"><a href="#Data-Filtering-Networks" class="headerlink" title="Data Filtering Networks"></a>Data Filtering Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17425">http://arxiv.org/abs/2309.17425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpr5/ngrep">https://github.com/jpr5/ngrep</a></li>
<li>paper_authors: Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, Vaishaal Shankar</li>
<li>for: 本研究旨在学习一种数据筛选网络（DFN），用于从大量未经过处理的数据集中选择高质量数据集。</li>
<li>methods: 我们使用了一种新的数据筛选网络建立了一个新的图像文本数据集，并证明了这种方法可以帮助train state-of-the-art模型。</li>
<li>results: 我们的最佳性果DFN-5B数据集可以让模型在不同任务上达到最佳性能，包括在ImageNet上实现83.0%的零shot传输精度。此外，我们还发布了一个新的20亿个示例数据集DFN-2B，并证明了可以从公共数据中训练高性能的数据筛选网络。<details>
<summary>Abstract</summary>
Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train state-of-the-art models for their compute budgets: among other improvements on a variety of tasks, a ViT-H trained on our dataset achieves 83.0% zero-shot transfer accuracy on ImageNet, out-performing models trained on other datasets such as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to facilitate further research in dataset design, we also release a new 2 billion example dataset DFN-2B and show that high performance data filtering networks can be trained from scratch using only publicly available data.
</details>
<details>
<summary>摘要</summary>
大规模的训练集已成为机器学习的基础，是现代语言模型和多 modal 学习的成就之基础。虽然数据准备 для预训练仍然是一个自动化的过程，但一种常见的方法是先从互联网上收集大量数据，然后通过不同的规则来筛选这个候选人pool下到实际的训练集。在这项工作中，我们研究了如何学习一个数据筛选网络（DFN），用于这个第二步的筛选大量未经准备的数据集。我们的关键发现是，筛选网络的质量与其在下游任务的表现存在差异：例如，一个在 ImageNet 上高度表现的模型可能会生成比其低 ImageNet 准确率的模型更好的训练集。基于我们的发现，我们构建了新的数据筛选网络，实现了状态机器人类图像文本 datasets。具体来说，我们的最佳性performing dataset DFN-5B 使得我们可以在不同的 compute 预算下训练状态机器人类模型。例如，使用我们的 dataset，一个 ViT-H 模型在 ImageNet 上达到 83.0% 零shot 转移率，超过其他 datasets 上的模型，如 LAION-2B、DataComp-1B 或 OpenAI 的 WIT。为了促进更多的研究 dataset 设计，我们还发布了一个新的 2 亿例示例数据 DFN-2B。我们的结果表明，高性能的数据筛选网络可以通过只使用公共可用的数据进行训练。
</details></li>
</ul>
<hr>
<h2 id="Can-Sensitive-Information-Be-Deleted-From-LLMs-Objectives-for-Defending-Against-Extraction-Attacks"><a href="#Can-Sensitive-Information-Be-Deleted-From-LLMs-Objectives-for-Defending-Against-Extraction-Attacks" class="headerlink" title="Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks"></a>Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17410">http://arxiv.org/abs/2309.17410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vaidehi99/infodeletionattacks">https://github.com/vaidehi99/infodeletionattacks</a></li>
<li>paper_authors: Vaidehi Patil, Peter Hase, Mohit Bansal</li>
<li>for: 本研究旨在 mitigating the safety and informational issues of pre-trained language models, such as memorized personal information and harmful output.</li>
<li>methods: 我们提出了一个攻击-防御框架，用于研究直接从模型权重中删除敏感信息。我们研究直接编辑模型权重的原因是，这种方法可以保证删除的信息从未被未来的提问攻击抽取出来，同时也可以保护白盒攻击。</li>
<li>results: 我们的实验表明，即使使用现有的模型编辑方法如ROME，也无法真正地从GPT-J模型中删除事实信息，我们的白盒和黑盒攻击可以从编辑后的模型中恢复“删除”的信息38%的时间。这些攻击利用了两个关键观察：（1）删除的信息可以在模型的中间隐藏状态中找到踪迹，（2）应用 editing 方法于一个问题后可能无法删除对重叠版本的问题中的信息。最后，我们提供了新的防御方法，但我们没有找到一个通用有效的防御方法。<details>
<summary>Abstract</summary>
Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover "deleted" information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe societal implications for real-world deployment of language models.
</details>
<details>
<summary>摘要</summary>
有些预训言语模型拥有我们不希望他们拥有的知识，包括记忆化的个人信息和可能被用来伤害人的知识。它们还可以输出恶势推文。为了解决这些安全和信息问题，我们提出了一个攻击和防御框架，用于研究直接从模型权重中删除敏感信息的任务。我们研究直接编辑模型权重的原因是：（1）这种方法可以保证未来的攻击不能提取特定的删除信息，（2）它可以保护白盒攻击，这是在公开可用的模型权重可以用来提取敏感信息的情况下非常重要。我们的威胁模型假设攻击成功的情况是，敏感问题的答案在一组B生成的候选人中，基于情况下如果信息不安全。实验表明， même les méthodes d'édition d'état de l'art telles que ROME ont du mal à vraiment supprimer des informations factuelles à partir de modèles comme GPT-J, car nos attaques blanches et noires peuvent récupérer "désormais" information "supprimée" à partir d'un modèle edited 38% du temps. Ces attaques se basent sur deux observations clés :（1）les traces d'informations supprimées peuvent être trouvées dans les états cachés intermédiaires du modèle, et（2）appliquer une méthode d'édition pour une question ne supprime pas l'information à travers les versions reformulées de la question. Enfin, nous fournissons de nouvelles méthodes de défense qui protègent contre certaines attaques d'extraction, mais nous ne trouvons pas une méthode universelle efficace. Nos résultats suggèrent que vraiment supprimer des informations sensibles est un problème difficile mais tractable, car même des taux d'attaque relativement faibles peuvent avoir des conséquences graves pour la deployment réelle des modèles de langage.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Machine-Learning-in-Latent-Representations-of-Neural-Networks"><a href="#Adversarial-Machine-Learning-in-Latent-Representations-of-Neural-Networks" class="headerlink" title="Adversarial Machine Learning in Latent Representations of Neural Networks"></a>Adversarial Machine Learning in Latent Representations of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17401">http://arxiv.org/abs/2309.17401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milin Zhang, Mohammad Abdi, Francesco Restuccia</li>
<li>for: 这种论文主要针对的问题是分布式深度学习网络（DNN）在边缘 Computing  scenarios 中减轻计算负担和降低端到端推理延迟的问题。</li>
<li>methods: 该论文使用了信息理论的概念来研究分布式 DNN 对抗攻击的稳定性。作者们引入了两个新的度量来衡量损害和抗性。</li>
<li>results: 经过广泛的实验分析，作者们发现：（一）假设保持同等水平的信息损害，则潜在特征都比输入表示更加稳定；（二）抗攻击能力与特征维度和 DNN 的泛化能力 jointly 决定分布式 DNN 的抗击性。实验结果表明，对 ImageNet-1K 数据集进行了 10 种不同的攻击方法， compress 的干扰特征可以降低攻击成功率，最好情况下降低了 88%，平均降低了 57%。<details>
<summary>Abstract</summary>
Distributed deep neural networks (DNNs) have been shown to reduce the computational burden of mobile devices and decrease the end-to-end inference latency in edge computing scenarios. While distributed DNNs have been studied, to the best of our knowledge the resilience of distributed DNNs to adversarial action still remains an open problem. In this paper, we fill the existing research gap by rigorously analyzing the robustness of distributed DNNs against adversarial action. We cast this problem in the context of information theory and introduce two new measurements for distortion and robustness. Our theoretical findings indicate that (i) assuming the same level of information distortion, latent features are always more robust than input representations; (ii) the adversarial robustness is jointly determined by the feature dimension and the generalization capability of the DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN architectures, 6 different approaches for distributed DNN and 10 different adversarial attacks to the ImageNet-1K dataset. Our experimental results support our theoretical findings by showing that the compressed latent representations can reduce the success rate of adversarial attacks by 88% in the best case and by 57% on the average compared to attacks to the input space.
</details>
<details>
<summary>摘要</summary>
分布式深度神经网络（DNN）已经被证明可以减轻移动设备的计算负担和减少边缘计算场景中的终端推理延迟。 Although distributed DNNs have been studied, to the best of our knowledge, the resilience of distributed DNNs to adversarial attacks remains an open problem. In this paper, we fill this research gap by rigorously analyzing the robustness of distributed DNNs against adversarial attacks. We cast this problem in the context of information theory and introduce two new measurements for distortion and robustness. Our theoretical findings indicate that (i) assuming the same level of information distortion, latent features are always more robust than input representations; (ii) the adversarial robustness is jointly determined by the feature dimension and the generalization capability of the DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN architectures, 6 different approaches for distributed DNN, and 10 different adversarial attacks to the ImageNet-1K dataset. Our experimental results support our theoretical findings by showing that the compressed latent representations can reduce the success rate of adversarial attacks by 88% in the best case and by 57% on average compared to attacks to the input space.
</details></li>
</ul>
<hr>
<h2 id="LoRA-ensembles-for-large-language-model-fine-tuning"><a href="#LoRA-ensembles-for-large-language-model-fine-tuning" class="headerlink" title="LoRA ensembles for large language model fine-tuning"></a>LoRA ensembles for large language model fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00035">http://arxiv.org/abs/2310.00035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Wang, Laurence Aitchison, Maja Rudolph</li>
<li>for: 提高强化大语言模型（LLM）的不确定性评估和预测结果，解决 LLM 常常表现出过于自信和报告异常的问题。</li>
<li>methods: 使用 Low-Rank Adapters（LoRA） ensemble，LoRA 是一种具有几个参数的精简技术，可以构建大型 ensemble 而不导致计算开销增加。</li>
<li>results: LoRA ensemble 可以提高预测精度和不确定性评估，并且可以与现有的 regularization 技术结合使用。<details>
<summary>Abstract</summary>
Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computational overhead as using the original model. We find that LoRA ensembles, applied on its own or on top of pre-existing regularization techniques, gives consistent improvements in predictive accuracy and uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
常见的LLM验议问题是不良的uncertainty量化，表现为过于自信、不好准确和不可预测的预测结果。为解决这个问题，视觉领域常用的一种方法是深度集成，它将多个不同初始化的模型 ensemble。然而， ensemble LLMs 有一个很大的挑战：最有效的LLMs 很、很大。保持一个LLM 在内存中 already 是一个挑战，而保持多个LLMs 在内存中是不可能的在许多设置下。为解决这些问题，我们提出了一种 ensemble 方法，使用 Low-Rank Adapters（LoRA），一种具有较少参数的精度调整技术。 LoRA 适配器只占用了一个LLM 的一个数量级的参数，而不是整个模型的参数。因此，可以构建一个大的 LoRA 适配器ensemble，与原始模型的计算开销几乎相同。我们发现，LoRA ensemble 应用于单独还是应用于现有的 regularization 技术上，都能够得到了更好的预测精度和uncertainty量化。
</details></li>
</ul>
<hr>
<h2 id="Reason-for-Future-Act-for-Now-A-Principled-Framework-for-Autonomous-LLM-Agents-with-Provable-Sample-Efficiency"><a href="#Reason-for-Future-Act-for-Now-A-Principled-Framework-for-Autonomous-LLM-Agents-with-Provable-Sample-Efficiency" class="headerlink" title="Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency"></a>Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17382">http://arxiv.org/abs/2309.17382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agentification/RAFA_code">https://github.com/agentification/RAFA_code</a></li>
<li>paper_authors: Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, Zhaoran Wang</li>
<li>for: The paper aims to improve the ability of large language models (LLMs) to complete tasks provably within a minimum number of interactions with the external environment.</li>
<li>methods: The proposed “reason for future, act for now” (\texttt{RAFA}) framework combines long-term reasoning and short-term acting to achieve provable regret guarantees. The framework includes a prompt template for reasoning, learning and planning in Bayesian adaptive Markov decision processes (MDPs), and an “in-context” actor-critic update.</li>
<li>results: The paper shows that the proposed framework achieves a $\sqrt{T}$ regret bound, outperforming various existing frameworks, and achieves nearly perfect scores on a few benchmarks.Here are the three points in Simplified Chinese text:</li>
<li>for: 该文章目标是提高大语言模型（LLM）完成任务的可靠性，使其在最小化与环境交互数量的情况下完成任务。</li>
<li>methods: 提议的“理解未来，行动当下”（\texttt{RAFA）”框架，通过结合长期理解和短期行动来实现可证明的 regret 保证。该框架包括一个启发模板 для理解，基于 Bayesian 适应 Markov 决策过程（MDPs）的学习和规划，以及在上下文中的 actor-critic 更新。</li>
<li>results: 文章表明，提议的框架可以实现 $\sqrt{T}$ regret  bound，超越多种现有框架，并在一些标准准则上达到近乎完美的分数。<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.   The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an "in-context" manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\sqrt{T}$ regret. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）表现出了印象的思维能力，但将思维转化为实际世界中的行动仍然是一个挑战。尤其是不确定如何在最小化与外部环境互动的情况下完成任务，例如通过内部的思维过程。为此，我们提出了一个原则性的框架，具有证明可靠的对策保证，我们称之为“理解未来，行动现在”（RAFA）。具体来说，我们设计了一个启发模板 для 思维，从记忆缓冲器学习并规划未来路径（理解未来）。在每个步骤中，LLM Agent 执行起始的规划路径的首个动作（行动现在），将收集到的反馈存储在记忆缓冲器中，然后重新邀请思维 routine 重新规划未来路径。关键思想是将 LLM 中的思维视为学习和规划 Bayesian 适应 Markov 决策过程（MDPs）。因此，我们鼓励 LLM 将记忆缓冲器中的未知环境更新为新的 posterior（学习），并生成多个未来步骤的最佳路径，以最大化一个值函数（规划）。学习和规划子程序在“内部”进行，以模拟 actor-critic 更新。我们的理论分析证明了我们的新结合长期思维和短期行动可以取得 $\sqrt{T}$ 的对抗。具体来说，对抗 bound 显示了 LLM 中的前期知识和思维行动之间的不确定性对抗。我们的实验验证显示，它比较多的现有框架表现出色，仅在一些问题上几乎取得完美的得分。
</details></li>
</ul>
<hr>
<h2 id="Revolutionizing-Mobile-Interaction-Enabling-a-3-Billion-Parameter-GPT-LLM-on-Mobile"><a href="#Revolutionizing-Mobile-Interaction-Enabling-a-3-Billion-Parameter-GPT-LLM-on-Mobile" class="headerlink" title="Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile"></a>Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01434">http://arxiv.org/abs/2310.01434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Carreira, Tomás Marques, José Ribeiro, Carlos Grilo</li>
<li>for: 这篇论文旨在提出一种新的大语言模型（LLM）执行方法，使得大型LLM可以在移动设备上直接执行，不需要网络连接。</li>
<li>methods: 该方法包括精度调整和模型压缩技术，以便在设备上运行大型LLM。</li>
<li>results: 实验结果表明，这种方法可以在设备上运行一个精度调整后的GPT大语言模型，并且可以在4GB的内存下运行。此外，该方法还实现了文本到动作的功能，使得用户可以通过文本输入来控制移动设备。<details>
<summary>Abstract</summary>
The field of Artificial Intelligence has witnessed remarkable progress in recent years, especially with the emergence of powerful large language models (LLMs) based on the transformer architecture. Cloud-based LLMs, such as OpenAI's ChatGPT, offer impressive capabilities but come with concerns regarding latency and privacy due to network dependencies. This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity. The article showcases a fine-tuned GPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of memory. Through the integration of native code and model quantization techniques, the application not only serves as a general-purpose assistant but also facilitates seamless mobile interactions with text-to-actions features. The article provides insights into the training pipeline, implementation details, test results, and future directions of on-device LLM inference. This breakthrough technology opens up possibilities for empowering users with sophisticated AI capabilities while preserving their privacy and eliminating latency concerns.
</details>
<details>
<summary>摘要</summary>
自 recent years 以来，人工智能领域已经做出了很多出色的进步，尤其是基于 transformer 架构的大型自然语言模型（LLM）。云存储的 LLM，如 OpenAI 的 ChatGPT，具有了很强的功能，但是也存在网络依赖和隐私问题。本文描述了一种创新的 LLM 推理方法，梦想一个未来，在无网络连接的手持设备上直接执行具有数十亿参数的 LLM。通过Native code 和模型压缩技术的结合，该应用不仅可以作为通用助手，还可以实现无缝 mobil 交互，包括文本到动作特性。文章提供了训练管道、实现细节、测试结果以及未来方向的相关信息，这项技术开启了 empower 用户通过保护隐私和消除延迟的方式获得高级 AI 功能。
</details></li>
</ul>
<hr>
<h2 id="Neural-Lithography-Close-the-Design-to-Manufacturing-Gap-in-Computational-Optics-with-a-‘Real2Sim’-Learned-Photolithography-Simulator"><a href="#Neural-Lithography-Close-the-Design-to-Manufacturing-Gap-in-Computational-Optics-with-a-‘Real2Sim’-Learned-Photolithography-Simulator" class="headerlink" title="Neural Lithography: Close the Design-to-Manufacturing Gap in Computational Optics with a ‘Real2Sim’ Learned Photolithography Simulator"></a>Neural Lithography: Close the Design-to-Manufacturing Gap in Computational Optics with a ‘Real2Sim’ Learned Photolithography Simulator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17343">http://arxiv.org/abs/2309.17343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Zheng, Guangyuan Zhao, Peter T. C. So<br>for:  bridging the “design-to-manufacturing” gap in computational opticsmethods:  fully differentiable design framework integrating pre-trained photolithography simulator, leveraging physics-informed modeling and data-driven trainingresults:  improved optical performance on task-specific metrics for holographic optical element (HOE) and multi-level diffractive lens (MDL) using two-photon lithography system.<details>
<summary>Abstract</summary>
We introduce neural lithography to address the 'design-to-manufacturing' gap in computational optics. Computational optics with large design degrees of freedom enable advanced functionalities and performance beyond traditional optics. However, the existing design approaches often overlook the numerical modeling of the manufacturing process, which can result in significant performance deviation between the design and the fabricated optics. To bridge this gap, we, for the first time, propose a fully differentiable design framework that integrates a pre-trained photolithography simulator into the model-based optical design loop. Leveraging a blend of physics-informed modeling and data-driven training using experimentally collected datasets, our photolithography simulator serves as a regularizer on fabrication feasibility during design, compensating for structure discrepancies introduced in the lithography process. We demonstrate the effectiveness of our approach through two typical tasks in computational optics, where we design and fabricate a holographic optical element (HOE) and a multi-level diffractive lens (MDL) using a two-photon lithography system, showcasing improved optical performance on the task-specific metrics.
</details>
<details>
<summary>摘要</summary>
我们引入神经镶刻来bridging computational optics中的"设计到制造" gap。计算光学具有大的设计自由度，可以实现高级功能和性能，但现有的设计方法通常忽视数值模拟制造过程，这可能导致设计和实际生产的性能差异较大。为了解决这个问题，我们首次提出了一个完全可导的设计框架，其中包含已经预训练的光刻 simulator。通过结合物理知识和数据驱动训练，我们的光刻 simulator acts as a regularizer during design, 补做制造过程中的结构差异。我们通过两个典型的计算光学任务：设计和制造一个折射式光学元件（HOE）和一个多级干涉镜（MDL），使用一个两氢激发镜系统，展示了改进的光学性能在任务特定的 метриках上。
</details></li>
</ul>
<hr>
<h2 id="MixQuant-Mixed-Precision-Quantization-with-a-Bit-width-Optimization-Search"><a href="#MixQuant-Mixed-Precision-Quantization-with-a-Bit-width-Optimization-Search" class="headerlink" title="MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search"></a>MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17341">http://arxiv.org/abs/2309.17341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eliska Kloberdanz, Wei Le</li>
<li>for: 创建高效的深度神经网络（DNNs），以便在具有限制计算资源和实时系统的平台上部署。</li>
<li>methods: 使用量化技术，将计算和存储矩阵进行下标宽化，从而减少模型大小和执行时间。</li>
<li>results: 提出了一种名为 MixQuant 的搜索算法，可以根据roundoff error来找到每层weight的优化量化比特宽，并且可以与任何量化方法结合使用，以提高量化模型的准确性。<details>
<summary>Abstract</summary>
Quantization is a technique for creating efficient Deep Neural Networks (DNNs), which involves performing computations and storing tensors at lower bit-widths than f32 floating point precision. Quantization reduces model size and inference latency, and therefore allows for DNNs to be deployed on platforms with constrained computational resources and real-time systems. However, quantization can lead to numerical instability caused by roundoff error which leads to inaccurate computations and therefore, a decrease in quantized model accuracy. Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths, we show that some weights are more sensitive than others which should be reflected on their quantization bit-width. To that end we propose MixQuant, a search algorithm that finds the optimal custom quantization bit-width for each layer weight based on roundoff error and can be combined with any quantization method as a form of pre-processing optimization. We show that combining MixQuant with BRECQ, a state-of-the-art quantization method, yields better quantized model accuracy than BRECQ alone. Additionally, we combine MixQuant with vanilla asymmetric quantization to show that MixQuant has the potential to optimize the performance of any quantization technique.
</details>
<details>
<summary>摘要</summary>
量化是一种技术，用于创建高效的深度神经网络（DNNs），该技术可以通过在计算和存储矩阵时使用较低的位数宽来减少模型大小和推理延迟，因此允许DNNs在具有限制计算资源和实时系统的平台上进行部署。然而，量化可能会导致数字不稳定性，由于绝对误差所导致的不准确的计算，从而导致减少量化模型的准确性。与先前的作品一样，我们发现， bias和活动是量化的敏感度较高，因此应该保留在全精度或高位宽下进行量化。为了实现这一点，我们提出了 MixQuant，一种搜索算法，可以根据绝对误差来找到每层权重的优化量化位宽。我们显示，将MixQuant与BRECQ（一种现状顶尖的量化方法）相结合，可以提高量化模型的准确性。此外，我们将MixQuant与普通非对称量化相结合，以示MixQuant具有优化任何量化技术的潜力。
</details></li>
</ul>
<hr>
<h2 id="Improving-Trajectory-Prediction-in-Dynamic-Multi-Agent-Environment-by-Dropping-Waypoints"><a href="#Improving-Trajectory-Prediction-in-Dynamic-Multi-Agent-Environment-by-Dropping-Waypoints" class="headerlink" title="Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints"></a>Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17338">http://arxiv.org/abs/2309.17338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Singh Chib, Pravendra Singh</li>
<li>for: 本研究的目的是提高 trajectory prediction 的准确性， Addressing the challenges of modeling diverse and uncertain trajectories.</li>
<li>methods: 该研究提出了一种新的框架，即 Temporal Waypoint Dropping (TWD)， which promotes explicit temporal learning through the waypoint dropping technique.</li>
<li>results: 对三个数据集（NBA Sports VU、ETH-UCY、TrajNet++）进行了广泛的实验，表明 TWD 能够有效地强制模型学习复杂的时间相关性。<details>
<summary>Abstract</summary>
The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. This paper introduces a novel framework, called Temporal Waypoint Dropping (TWD), that promotes explicit temporal learning through the waypoint dropping technique. Learning through waypoint dropping can compel the model to improve its understanding of temporal correlations among agents, thus leading to a significant enhancement in trajectory prediction. Trajectory prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding real-world scenarios where missing values may occur, which can influence their performance. Moreover, these models frequently exhibit a bias towards particular waypoint sequences when making predictions. Our TWD is capable of effectively addressing these issues. It incorporates stochastic and fixed processes that regularize projected past trajectories by strategically dropping waypoints based on temporal sequences. Through extensive experiments, we demonstrate the effectiveness of TWD in forcing the model to learn complex temporal correlations among agents. Our approach can complement existing trajectory prediction methods to enhance prediction accuracy. We also evaluate our proposed method across three datasets: NBA Sports VU, ETH-UCY, and TrajNet++.
</details>
<details>
<summary>摘要</summary>
自然的轨迹具有内在的多样性和不确定性，这些特点使轨迹预测变得非常困难。轨迹预测系统需要从过去获取空间和时间信息，以更好地预测未来轨迹。许多现有方法在堆叠模型中分别学习时间特征，以捕捉时间特征。本文提出了一种新的框架，即时间点掉除（TWD），它通过掉除时间点来显式地学习时间相关性。通过这种方法，模型可以更好地理解 agent之间的时间相关性，从而导致轨迹预测的显著改进。轨迹预测方法经常假设观察到的轨迹点序列是完整的，忽略了现实世界中的缺失数据，这可能会影响其性能。此外，这些模型经常偏爱某些轨迹点序列，这会导致预测不准确。我们的 TWD 可以有效地解决这些问题。它将随机过程和固定过程混合，通过掉除时间点来规范预测过去轨迹的投影。通过广泛的实验，我们证明了 TWD 的效果是让模型学习复杂的时间相关性。我们的方法可以补充现有的轨迹预测方法，以提高预测精度。我们还对我们的提议方法进行了三个数据集的评估：NBA Sports VU、ETH-UCY 和 TrajNet++。
</details></li>
</ul>
<hr>
<h2 id="Toward-Operationalizing-Pipeline-aware-ML-Fairness-A-Research-Agenda-for-Developing-Practical-Guidelines-and-Tools"><a href="#Toward-Operationalizing-Pipeline-aware-ML-Fairness-A-Research-Agenda-for-Developing-Practical-Guidelines-and-Tools" class="headerlink" title="Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools"></a>Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17337">http://arxiv.org/abs/2309.17337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emily Black, Rakshit Naidu, Rayid Ghani, Kit T. Rodolfa, Daniel E. Ho, Hoda Heidari</li>
<li>for: 本研究旨在提供一个完整的研究资源库，用于帮助机器学习（ML）研究人员、实践人员和学生在实现算法公平中使用管道驱动的方法。</li>
<li>methods: 本研究使用了文献综述的方法，收集和组织了过去的做法，以便为研究人员、实践人员和学生提供一个完整的研究资源库。</li>
<li>results: 本研究提出了一个研究资源库，用于帮助研究人员、实践人员和学生在实现算法公平中使用管道驱动的方法。<details>
<summary>Abstract</summary>
While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowledge find it challenging to hypothesize how various design choices influence model behavior. We then consult the fair-ML literature to understand the progress to date toward operationalizing the pipeline-aware approach: we systematically collect and organize the prior work that attempts to detect, measure, and mitigate various sources of unfairness through the ML pipeline. We utilize this extensive categorization of previous contributions to sketch a research agenda for the community. We hope this work serves as the stepping stone toward a more comprehensive set of resources for ML researchers, practitioners, and students interested in exploring, designing, and testing pipeline-oriented approaches to algorithmic fairness.
</details>
<details>
<summary>摘要</summary>
而algorithmic fairness是一个迅速发展的研究领域，在实践中，消除偏见问题经常被减少到仅仅是在优化步骤中遵循一个arbitrary chosen fairness metric，或者在模型输出后进行后处理，或者通过修改训练数据来修改模型。 current work has called on the ML community to take a more comprehensive approach to address fairness issues by systematically investigating the many design choices made throughout the ML pipeline, and identifying interventions that target the root cause of the issue, rather than its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness in practice, we believe there are currently very few methods of operationalizing this approach. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowledge find it challenging to hypothesize how various design choices influence model behavior. We then consult the fair-ML literature to understand the progress to date toward operationalizing the pipeline-aware approach: we systematically collect and organize the prior work that attempts to detect, measure, and mitigate various sources of unfairness through the ML pipeline. We utilize this extensive categorization of previous contributions to sketch a research agenda for the community. We hope this work serves as a stepping stone toward a more comprehensive set of resources for ML researchers, practitioners, and students interested in exploring, designing, and testing pipeline-oriented approaches to algorithmic fairness.
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Graph-Generators"><a href="#Asynchronous-Graph-Generators" class="headerlink" title="Asynchronous Graph Generators"></a>Asynchronous Graph Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17335">http://arxiv.org/abs/2309.17335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Christopher P. Ley, Felipe Tobar</li>
<li>for: 这篇论文是为了描述一种新的图 neural network 架构，用于多通道时间序列数据的填充和预测。</li>
<li>methods: 这篇论文使用了一种名为 asynchronous graph generator (AGG) 的新的图 neural network 架构，该架构可以模型 observation 为节点在动态图上，并通过听取来学习表达时间序列中变量之间的关系。</li>
<li>results: 根据实验结果，AGG 能够在 Beijing Air Quality、PhysioNet Challenge 2012 和 UCI localisation 等标准数据集上达到时间序列数据填充、预测和分类的state-of-the-art 结果。<details>
<summary>Abstract</summary>
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in time series data imputation, classification and prediction for the benchmark datasets Beijing Air Quality, PhysioNet Challenge 2012 and UCI localisation.
</details>
<details>
<summary>摘要</summary>
我们介绍了异步图生成器（AGG），一种新型的图神经网络架构，用于多通道时间序列数据。AGG模型观测值为节点在动态图中，并通过权重注意力学习表示每个变量之间的关系。因此，AGG可以通过推理到新的时间戳和metadata来进行数据潜在性恢复。与传统的循环组件或时间规则假设不同，AGG直接将测量值、时间戳和metadataembedding为节点中的学习 embeddings。我们对AGG与先前的工作进行了概念和实验性比较，并 briefly discuss了数据扩充对AGG性能的影响。我们的实验结果显示，AGG在Beijing空气质量、PhysioNet Challenge 2012和UCIlocalization benchmark datasets上实现了时间序列数据潜在性恢复、分类和预测的state-of-the-artResult。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Anatomical-Labeling-of-Pulmonary-Tree-Structures-via-Implicit-Point-Graph-Networks"><a href="#Efficient-Anatomical-Labeling-of-Pulmonary-Tree-Structures-via-Implicit-Point-Graph-Networks" class="headerlink" title="Efficient Anatomical Labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks"></a>Efficient Anatomical Labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17329">http://arxiv.org/abs/2309.17329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kangxian Xie, Jiancheng Yang, Donglai Wei, Ziqiao Weng, Pascal Fua</li>
<li>for: 了解肺系统的复杂3D树状结构，以提高肺病的治疗。</li>
<li>methods: 提出一种点云方法，保持树skeleton的图连接和Surface representation，实现低计算成本下的SOTA准确性。</li>
<li>results: 实现了usable Surface的模型，并且由于数据的缺乏，我们还进行了大规模的数据收集和分布。<details>
<summary>Abstract</summary>
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
</details>
<details>
<summary>摘要</summary>
肺疾病在全球死亡原因中排名前列，需要更好地理解肺系统中复杂的3D树状结构，如呼吸道、血管和血液。在理论上，这些结构可以通过高分辨率图像堆栈来模拟。然而，使用标准的density voxel网格方法会非常昂贵。为此，我们介绍了点基方法，保留树状结构的图 Connectivity和Surface representation。它可以达到低计算成本下的SOTA准确率，并且模型的表面可以使用。由于肺疾病数据的公共访问性缺乏，我们还编辑了一个广泛的数据集，以评估我们的方法，并将其公开。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Look-Ahead-Bias-in-Stock-Return-Predictions-Generated-By-GPT-Sentiment-Analysis"><a href="#Assessing-Look-Ahead-Bias-in-Stock-Return-Predictions-Generated-By-GPT-Sentiment-Analysis" class="headerlink" title="Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis"></a>Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17322">http://arxiv.org/abs/2309.17322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Glasserman, Caden Lin</li>
<li>for: 这个论文是关于大语言模型（LLM）从新闻文本中提取股票交易信号的研究。</li>
<li>methods: 作者使用了LLM来分析新闻文本中的情绪，并通过去除文本中相关公司的标识符来避免干扰和look-ahead偏见。</li>
<li>results: 研究发现，在训练window内，去除公司标识符后的文本表现更好， indicating that the distraction effect is more significant than look-ahead bias。此外，这种偏见具有更强的影响力于大公司。out-of-sample中，look-ahead bias不是一个问题，但distraction仍然可能存在。<details>
<summary>Abstract</summary>
Large language models (LLMs), including ChatGPT, can extract profitable trading signals from the sentiment in news text. However, backtesting such strategies poses a challenge because LLMs are trained on many years of data, and backtesting produces biased results if the training and backtesting periods overlap. This bias can take two forms: a look-ahead bias, in which the LLM may have specific knowledge of the stock returns that followed a news article, and a distraction effect, in which general knowledge of the companies named interferes with the measurement of a text's sentiment. We investigate these sources of bias through trading strategies driven by the sentiment of financial news headlines. We compare trading performance based on the original headlines with de-biased strategies in which we remove the relevant company's identifiers from the text. In-sample (within the LLM training window), we find, surprisingly, that the anonymized headlines outperform, indicating that the distraction effect has a greater impact than look-ahead bias. This tendency is particularly strong for larger companies--companies about which we expect an LLM to have greater general knowledge. Out-of-sample, look-ahead bias is not a concern but distraction remains possible. Our proposed anonymization procedure is therefore potentially useful in out-of-sample implementation, as well as for de-biased backtesting.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），包括ChatGPT，可以从新闻文本中提取有利交易信号。然而，回测这些策略具有挑战，因为LLM被训练了许多年的数据，而回测期间与训练期间重叠，可能会导致偏见。这种偏见可以有两种形式：look-ahead偏见， LLM可能知道新闻文本后的股票收益，以及拖垮效应，通常知道公司名称会对文本情绪的测量产生干扰。我们通过基于新闻标题的交易策略来研究这些偏见的来源。我们将基于原始标题和去除相关公司标识符的两种策略进行比较，并发现在样本内（在LLM训练窗口内），去除公司标识符后的策略表现更好， indicating that the distraction effect is more significant than look-ahead bias。这种倾向尤其强于大型公司——我们预期LLM对这些公司有更多通用知识。外样（外 LLM训练窗口），look-ahead偏见不是问题，但distraction仍然可能存在。我们提议的匿名处理程序因此可能有用，不仅在外样实现，还在做偏见回测。
</details></li>
</ul>
<hr>
<h2 id="Building-Privacy-Preserving-and-Secure-Geospatial-Artificial-Intelligence-Foundation-Models"><a href="#Building-Privacy-Preserving-and-Secure-Geospatial-Artificial-Intelligence-Foundation-Models" class="headerlink" title="Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models"></a>Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17319">http://arxiv.org/abs/2309.17319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinmeng Rao, Song Gao, Gengchen Mai, Krzysztof Janowicz</li>
<li>for: 本研究旨在启示基于人工智能的地ospatial领域内存bezirk 风险和安全问题，以及对这些问题的可能性解决方案。</li>
<li>methods: 本文使用了一种综述方法，涵盖了基于地ospatial领域的人工智能模型开发和应用的全生命周期，从模型训练和测试到实际应用。</li>
<li>results: 本文发现了基于地ospatial领域的人工智能模型开发和应用可能会降低个人隐私和安全风险，提出了一种全面的研究方向和预防控制策略，以帮助研究人员和政策制定者更好地理解和解决这些问题。<details>
<summary>Abstract</summary>
In recent years we have seen substantial advances in foundation models for artificial intelligence, including language, vision, and multimodal models. Recent studies have highlighted the potential of using foundation models in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.
</details>
<details>
<summary>摘要</summary>
近年来，我们所见到的基础模型在人工智能领域的进步很大，包括语言、视觉和多Modal模型。 latest studies have highlighted the potential of using基础模型 in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.Here's the translation in Traditional Chinese:近年来，我们所见到的基础模型在人工智能领域的进步很大，包括语言、视觉和多Modal模型。 latest studies have highlighted the potential of using基础模型 in geospatial artificial intelligence, known as GeoAI Foundation Models, for geographic question answering, remote sensing image understanding, map generation, and location-based services, among others. However, the development and application of GeoAI foundation models can pose serious privacy and security risks, which have not been fully discussed or addressed to date. This paper introduces the potential privacy and security risks throughout the lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for research directions and preventative and control strategies. Through this vision paper, we hope to draw the attention of researchers and policymakers in geospatial domains to these privacy and security risks inherent in GeoAI foundation models and advocate for the development of privacy-preserving and secure GeoAI foundation models.
</details></li>
</ul>
<hr>
<h2 id="AutoAgents-A-Framework-for-Automatic-Agent-Generation"><a href="#AutoAgents-A-Framework-for-Automatic-Agent-Generation" class="headerlink" title="AutoAgents: A Framework for Automatic Agent Generation"></a>AutoAgents: A Framework for Automatic Agent Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17288">http://arxiv.org/abs/2309.17288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Link-AGI/AutoAgents">https://github.com/Link-AGI/AutoAgents</a></li>
<li>paper_authors: Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F. Karlsson, Jie Fu, Yemin Shi</li>
<li>for: 这篇论文的目的是提出一个创新的框架，可以自动生成和调度多个专门的智能代理人来解决不同的任务。</li>
<li>methods: 这篇论文使用了自动生成和调度多个专门的智能代理人，并且将任务和角色之间的关系实现为动态生成的专业代理人。</li>
<li>results:  experiments 表明，这个框架可以产生更有条理和更准确的解决方案，比起现有的多代理人方法。<details>
<summary>Abstract</summary>
Large language models (LLMs) have enabled remarkable advances in automated task-solving with multi-agent systems. However, most existing LLM-based multi-agent approaches rely on predefined agents to handle simple tasks, limiting the adaptability of multi-agent collaboration to different scenarios. Therefore, we introduce AutoAgents, an innovative framework that adaptively generates and coordinates multiple specialized agents to build an AI team according to different tasks. Specifically, AutoAgents couples the relationship between tasks and roles by dynamically generating multiple required agents based on task content and planning solutions for the current task based on the generated expert agents. Multiple specialized agents collaborate with each other to efficiently accomplish tasks. Concurrently, an observer role is incorporated into the framework to reflect on the designated plans and agents' responses and improve upon them. Our experiments on various benchmarks demonstrate that AutoAgents generates more coherent and accurate solutions than the existing multi-agent methods. This underscores the significance of assigning different roles to different tasks and of team cooperation, offering new perspectives for tackling complex tasks. The repository of this project is available at https://github.com/Link-AGI/AutoAgents.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经实现了自动任务解决的多智能体系统的很多创新。然而，大多数现有的 LLM 基于多智能体系统仍然依赖于预定义的代理人来处理简单任务，这限制了多智能体团队在不同场景下的适应性。因此，我们介绍了 AutoAgents，一个创新的框架，可以动态生成和协调多个专业代理人，以建立基于任务的 AI 团队。具体来说，AutoAgents 将任务和角色之间的关系 coupling 到一起，通过动态生成多个需要的代理人，根据任务内容和规划解决方案来生成专业代理人。多个专业代理人之间协同合作，以高效地完成任务。同时，框架中还包含了观察者角色，可以反思指定的计划和代理人的回应，并改进它们。我们在多个标准 benchmark 上进行了实验，结果表明，AutoAgents 可以生成更 coherent 和更准确的解决方案，而不是现有的多智能体方法。这说明了分配不同任务不同角色的重要性，以及团队合作的新视角，可以用于解决复杂任务。AutoAgents 项目的存储库可以在 GitHub 上找到：https://github.com/Link-AGI/AutoAgents。
</details></li>
</ul>
<hr>
<h2 id="AI-Aristotle-A-Physics-Informed-framework-for-Systems-Biology-Gray-Box-Identification"><a href="#AI-Aristotle-A-Physics-Informed-framework-for-Systems-Biology-Gray-Box-Identification" class="headerlink" title="AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box Identification"></a>AI-Aristotle: A Physics-Informed framework for Systems Biology Gray-Box Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01433">http://arxiv.org/abs/2310.01433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazanin Ahmadi Daryakenari, Mario De Florio, Khemraj Shukla, George Em Karniadakis</li>
<li>for: 这个研究旨在找到生物系统中未知的物理方程式，并且使用调教数据来推导这些方程式。</li>
<li>methods: 这个方法结合了EXTreme Theory of Functional Connections（X-TFC）领域分解和物理调教神经网络（PINNs），以及symbolic regression（SR）技术，实现参数发现和灰色盒识别。</li>
<li>results: 这个方法在两个系统生物 benchmark 问题上进行了测试，结果显示了高准确、快速、灵活和可靠的性能。<details>
<summary>Abstract</summary>
Discovering mathematical equations that govern physical and biological systems from observed data is a fundamental challenge in scientific research. We present a new physics-informed framework for parameter estimation and missing physics identification (gray-box) in the field of Systems Biology. The proposed framework -- named AI-Aristotle -- combines eXtreme Theory of Functional Connections (X-TFC) domain-decomposition and Physics-Informed Neural Networks (PINNs) with symbolic regression (SR) techniques for parameter discovery and gray-box identification. We test the accuracy, speed, flexibility and robustness of AI-Aristotle based on two benchmark problems in Systems Biology: a pharmacokinetics drug absorption model, and an ultradian endocrine model for glucose-insulin interactions. We compare the two machine learning methods (X-TFC and PINNs), and moreover, we employ two different symbolic regression techniques to cross-verify our results. While the current work focuses on the performance of AI-Aristotle based on synthetic data, it can equally handle noisy experimental data and can even be used for black-box identification in just a few minutes on a laptop. More broadly, our work provides insights into the accuracy, cost, scalability, and robustness of integrating neural networks with symbolic regressors, offering a comprehensive guide for researchers tackling gray-box identification challenges in complex dynamical systems in biomedicine and beyond.
</details>
<details>
<summary>摘要</summary>
找到物理和生物系统中的数学方程是科学研究中的基本挑战。我们介绍了一个新的物理学习框架，以帮助在系统生物中进行参数估计和缺失物理特征的恢复（灰色盒）。该框架被称为AI-Aristotle，它结合了极限理论函数连接（X-TFC）领域分解和物理学习网络（PINNs）以及符号回归（SR）技术来进行参数发现和灰色盒特征的恢复。我们在两个系统生物 benchmark 问题中测试了AI-Aristotle 的准确性、速度、灵活性和可靠性。我们将 X-TFC 和 PINNs 两种机器学习方法进行比较，并使用了两种不同的符号回归技术来跨验我们的结果。当前的工作主要基于合成数据进行测试，但它可以同时处理噪音的实验数据，并且可以在几分钟内在笔记计算机上进行黑盒特征的恢复。更广泛地说，我们的工作提供了 integrating 神经网络与符号回归器的精度、成本、可扩展性和可靠性的评估，这将为处理复杂的生物医学系统中的灰色盒特征恢复问题提供一个完整的指南。
</details></li>
</ul>
<hr>
<h2 id="Split-and-Merge-Aligning-Position-Biases-in-Large-Language-Model-based-Evaluators"><a href="#Split-and-Merge-Aligning-Position-Biases-in-Large-Language-Model-based-Evaluators" class="headerlink" title="Split and Merge: Aligning Position Biases in Large Language Model based Evaluators"></a>Split and Merge: Aligning Position Biases in Large Language Model based Evaluators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01432">http://arxiv.org/abs/2310.01432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu</li>
<li>for: 提高LLMs的可靠性和扩展性，以便更好地用于自动评估AI系统的回答质量。</li>
<li>methods: 提出了一种名为PORTIA的Alignment-based系统，通过对答案分割、对相似内容进行对齐，然后将其传递给LLMs进行评估。</li>
<li>results: 对11,520个答案对进行了广泛的实验，发现PORTIA可以显著提高LLMs的一致率，最高可达98%；同时，PORTIA可以使用较为简单的GPT模型达到与State-of-the-art GPT-4模型相当的性能，减少了评估成本。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown promise as automated evaluators for assessing the quality of answers generated by AI systems. However, these LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content. To address this limitation, we propose PORTIA, an alignment-based system designed to mimic human comparison strategies to calibrate position bias in a lightweight yet effective manner. Specifically, PORTIA splits the answers into multiple segments, aligns similar content across candidate answers, and then merges them back into a single prompt for evaluation by LLMs. We conducted extensive experiments with six diverse LLMs to evaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances the consistency rates for all the models and comparison forms tested, achieving an average relative improvement of 47.46%. Remarkably, PORTIA enables less advanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4 model at just 10% of the cost. Furthermore, it rectifies around 80% of the position bias instances within the GPT-4 model, elevating its consistency rate up to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced GPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with human evaluators. These findings highlight PORTIA's ability to correct position bias, improve LLM consistency, and boost performance while keeping cost-efficiency. This represents a valuable step toward a more reliable and scalable use of LLMs for automated evaluations across diverse applications.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经显示了评估人工智能系统产生的答案的潜力，但这些 LLM 基于的评估器具有位置偏见，即在比较对比中，偏向选择第一或第二个答案，不顾其内容。为解决这个限制，我们提出了 PORTIA，一个对照式系统，旨在模拟人类比较策略，以减少 LLM 的位置偏见。 Specifically, PORTIA 将答案分成多个段落，在候选答案中寻找相似内容，然后将它们重新联结成单一的提示，以便 LLM 进行评估。我们在六种不同的 LLM 上进行了广泛的实验，评估了 11,520 个答案对。我们的结果显示，PORTIA 可以明显改善所有模型和比较形式的一致率，实现了平均相对改善率为 47.46%。更重要的是，PORTIA 可以让较不进步的 GPT 模型在成本下降 90% 的情况下，与现有的 GPT-4 模型一致度高于 88%。此外，PORTIA 可以修正 GPT-4 模型中的位置偏见，提高其一致率至 98%。后续的人工评估表明，PORTIA 改进后的 GPT-3.5 模型可以在人工评估者的Alignment上超越 standalone GPT-4 模型。这些结果显示 PORTIA 能够正确地缓解位置偏见，提高 LLM 的一致率，并提高性能，同时保持成本效益。这代表了一个值得信赖的步骤，对于自动评估的应用而言。
</details></li>
</ul>
<hr>
<h2 id="PB-LLM-Partially-Binarized-Large-Language-Models"><a href="#PB-LLM-Partially-Binarized-Large-Language-Models" class="headerlink" title="PB-LLM: Partially Binarized Large Language Models"></a>PB-LLM: Partially Binarized Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00034">http://arxiv.org/abs/2310.00034</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hahnyuan/binaryllm">https://github.com/hahnyuan/binaryllm</a></li>
<li>paper_authors: Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong</li>
<li>for: 本研究探讨了网络归纳，即极端归纳，压缩模型 веса到单位位数，特别是针对大语言模型（LLMs）压缩。由于先前的归纳方法会使 LLMs 崩溃，我们提出了一种新的方法，即部分归纳 LLM（PB-LLM），可以实现极低位归纳而保持压缩后 LLMs 的语言逻辑能力。</li>
<li>methods: 我们的探讨首先揭示了先前归纳方法的不具有效果，并 highlights 突出重要的质量权重的重要性。因此，PB-LLM 在归纳过程中选择一小部分突出的质量权重，将其分配到高位存储，即部分归纳。PB-LLM 还被扩展到重建压缩后 LLMs 的能力，通过分析PTQ和QAT的角度。</li>
<li>results: 我们的实验结果表明，PB-LLM 可以实现极低位归纳，并保持 LLMs 的语言逻辑能力。此外，我们还提出了一种基于 Hessian 矩阵的重建方法，可以在 PTQ 中重建压缩后 LLMs 的能力。此外，我们还提出了一种基于权重抑制的扩展方法，可以在 QAT 中实现更好的压缩精度。<details>
<summary>Abstract</summary>
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs.The code is available at https://github.com/hahnyuan/BinaryLLM.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)这篇论文探讨了网络归一化（Network Binarization），即将模型权重压缩到单位比特，特别是用于大型语言模型（LLMs）压缩。由于先前的归一化方法会使LLMs崩溃，我们提出了一种新的方法，即半归一化大语言模型（PB-LLM）。PB-LLM可以在低比特压缩中实现极低的压缩率，而不失 linguistic 的逻辑理解能力。我们的探讨首先揭示了先前归一化算法的不充分应用，并确认了关键 weights 的重要性。因此，PB-LLM在归一化过程中只 filters 一小部分关键 weights，将其分配到高比特存储。PB-LLM还被扩展以恢复压缩后 LLMs 的能力，通过分析PTQ和QAT的角度。在PTQ中，我们结合GPTQ的概念，重建归一化后的 weight 矩阵，并成功地恢复 PB-LLM 在低比特下的逻辑理解能力。在QAT中，我们冻结关键 weights  durante 训练，探索了最佳扩展系数的 derivation，并提出了基于这个 derivation 的扩展系数Scaling 机制。这些探讨和开发的方法ология具有重要的贡献，可以恢复低比特压缩 LLMs 的性能，并对网络归一化领域的进步做出了重要贡献。代码可以在 https://github.com/hahnyuan/BinaryLLM 上获取。
</details></li>
</ul>
<hr>
<h2 id="STRONG-–-Structure-Controllable-Legal-Opinion-Summary-Generation"><a href="#STRONG-–-Structure-Controllable-Legal-Opinion-Summary-Generation" class="headerlink" title="STRONG – Structure Controllable Legal Opinion Summary Generation"></a>STRONG – Structure Controllable Legal Opinion Summary Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17280">http://arxiv.org/abs/2309.17280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zhong, Diane Litman</li>
<li>for: 这个研究旨在提出一种基于预测 argue结构信息的法律意见摘要方法，以实现结构可控的摘要。</li>
<li>methods: 该方法使用预测的 argue结构信息引导模型生成具有提供结构模式的一致性的摘要。</li>
<li>results: 在一个legal opinion数据集上测试了该方法，并证明它在ROUGE、BERTScore和结构相似性方面与多种强基eline的表现更好。<details>
<summary>Abstract</summary>
We propose an approach for the structure controllable summarization of long legal opinions that considers the argument structure of the document. Our approach involves using predicted argument role information to guide the model in generating coherent summaries that follow a provided structure pattern. We demonstrate the effectiveness of our approach on a dataset of legal opinions and show that it outperforms several strong baselines with respect to ROUGE, BERTScore, and structure similarity.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于文本案例结构的法律意见概要化方法，考虑了文本中的论证结构。我们的方法利用预测的论证角色信息来引导模型生成符合提供的结构模式的 coherent 概要，并在法律意见 dataset 上进行了评估。结果显示，我们的方法在 ROUGE、BERTScore 和结构相似性方面与多种强大的基线方法相比，表现出优异。
</details></li>
</ul>
<hr>
<h2 id="Suspicion-Agent-Playing-Imperfect-Information-Games-with-Theory-of-Mind-Aware-GPT-4"><a href="#Suspicion-Agent-Playing-Imperfect-Information-Games-with-Theory-of-Mind-Aware-GPT-4" class="headerlink" title="Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4"></a>Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17277">http://arxiv.org/abs/2309.17277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cr-gjx/suspicion-agent">https://github.com/cr-gjx/suspicion-agent</a></li>
<li>paper_authors: Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo</li>
<li>for: 这篇论文探讨了使用大语言模型GPT-4在不完全信息游戏中的应用可能性。</li>
<li>methods: 作者提出了一种名为“偏见代理”的创新代理，利用GPT-4的知识检索和理解能力在不完全信息游戏中进行游戏。</li>
<li>results: 实验表明，使用GPT-4可以在不同的不完全信息游戏中达到比较出色的效果，而无需特殊的训练或示例。<details>
<summary>Abstract</summary>
Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4's learned knowledge for imperfect information games. To achieve this, we introduce \textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4's capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others' behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of Suspicion-Agent across three different imperfect information games and then quantitatively evaluate it in Leduc Hold'em. The results show that Suspicion-Agent can potentially outperform traditional algorithms designed for imperfect information games, without any specialized training or examples. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available.
</details>
<details>
<summary>摘要</summary>
不同于完美信息游戏， где所有元素都是知道的，不完整信息游戏模拟了现实世界中决策的复杂性，在不完整信息的情况下。GPT-4，最近的大语言模型（LLM）训练在庞大的过去数据上，具有知识检索和理解能力。本文探讨了GPT-4学习得知的可用性 для不完整信息游戏。为此，我们提出了《相互 подозрения代理人》（Suspicion-Agent），一种充分利用GPT-4的能力来进行不完整信息游戏。通过适当的提示工程来实现不同的功能，Suspicion-Agent基于GPT-4在不完整信息 кар打扮游戏中表现出了Remarkable的适应能力。更重要的是，GPT-4表现出了强大的高阶理想心（ToM）能力，可以理解他人并意外影响他人的行为。利用这一点，我们设计了一种规划策略，使GPT-4可以 Competently against不同的对手，适应其游戏风格，只需输入游戏规则和描述观察。在实验中，我们资深展示了Suspicion-Agent在三个不同的不完整信息游戏中的能力，然后quantitatively评估其在Leduc Hold'em中的表现。结果表明，Suspicion-Agent可能会超越传统为不完整信息游戏设计的算法，无需特殊训练或示例。为了鼓励和推动 deeper insights within the community，我们将游戏相关数据公开。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Large-Language-Models-in-Coding-Through-Multi-Perspective-Self-Consistency"><a href="#Enhancing-Large-Language-Models-in-Coding-Through-Multi-Perspective-Self-Consistency" class="headerlink" title="Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency"></a>Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17272">http://arxiv.org/abs/2309.17272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, Nan Duan<br>methods: 这 paper 使用了以下方法：* 从多个角度（solution、specification 和 test case）提取多个不同的输出，并将其转化为多ipartite 图形式。* 使用两种定义的一致性度量，将自身一致性信息嵌入图中。* 根据图中的一致性分析，选择最佳的选择。results: 这 paper 的实验结果表明，MPSC 框架可以在多个流行的benchmark上提高表现，包括 HumanEval (+17.60%), HumanEval Plus (+17.61%), MBPP (+6.50%) 和 CodeContests (+11.82%)，在 Pass@1 中，比原始从 ChatGPT 生成的输出要好，甚至超过 GPT-4。<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is then determined based on consistency analysis in the graph. We conduct comprehensive evaluation on the code generation task by introducing solution, specification and test case as three perspectives. We leverage a code interpreter to quantitatively measure the inter-consistency and propose several intra-consistency measure functions. Our MPSC framework significantly boosts the performance on various popular benchmarks, including HumanEval (+17.60%), HumanEval Plus (+17.61%), MBPP (+6.50%) and CodeContests (+11.82%) in Pass@1, when compared to original outputs generated from ChatGPT, and even surpassing GPT-4.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在文本生成方面表现出色，但在复杂的推理任务中，如代码生成， LLM 仍然很难在一次尝试中获得正确的答案。先前的研究已经探访过解决方案，包括聚合多个输出的方法，但这些方法都未能充分捕捉这些积极的一致性。在这篇论文中，我们提出了多元视角自适应性（MPSC）框架，一种新的解答策略，用于 LLM 中。这个框架包括两个预先定义的一致性度量，将它们嵌入到多方位的图形中。我们要求 LLM 在不同的角度获得多个多样的输出，并将它们组合成一个多方位的图形。然后，我们使用这两个一致性度量进行一致性分析，以选择最佳的选择。我们在代码生成任务中进行了全面评估，并引入解决方案、规格和测试用例作为三个角度。我们运用了代码解释器来量化内部一致性，并提出了多个内部一致性度量函数。我们的 MPSC 框架在多个受测 benchmark 上表现出色，包括 HumanEval (+17.60%)、HumanEval Plus (+17.61%)、MBPP (+6.50%) 和 CodeContests (+11.82%)，在 Pass@1 中比原始的 ChatGPT 生成的输出要好，甚至超过 GPT-4。
</details></li>
</ul>
<hr>
<h2 id="A-Foundation-Model-for-General-Moving-Object-Segmentation-in-Medical-Images"><a href="#A-Foundation-Model-for-General-Moving-Object-Segmentation-in-Medical-Images" class="headerlink" title="A Foundation Model for General Moving Object Segmentation in Medical Images"></a>A Foundation Model for General Moving Object Segmentation in Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17264">http://arxiv.org/abs/2309.17264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongnuo Yan, Tong Han, Yuhao Huang, Lian Liu, Han Zhou, Jiongquan Chen, Wenlong Shi, Yan Cao, Xin Yang, Dong Ni</li>
<li>for: 这个研究旨在提高医疗影像分类的精度，实现诊断中的关键角色。</li>
<li>methods: 本研究使用了 Moveing Object Segmentation (MOS) 技术，仅需要小量的标注。</li>
<li>results: 实验结果显示，iMOS 可以实现在医疗影像序列中具有高精度的追踪和分类表现，仅需要标注少量的图像。<details>
<summary>Abstract</summary>
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve satisfactory tracking and segmentation performance of moving objects throughout the entire sequence in bi-directions. We hope that the proposed iMOS can help accelerate the annotation speed of experts, and boost the development of medical foundation models.
</details>
<details>
<summary>摘要</summary>
医学图像分割目标是将有用的解剖或疾病结构区分出来，在临床诊断中扮演重要角色。但是，建立高精度深度分割模型需要大量高质量标注数据，而医学标注是非常繁琐和时间消耗的，尤其是在医学视频或3DVolume中，因为标注空间庞大且间隔不一致。最近，一项基础任务名为移动对象分割（MOS）在自然图像中做出了重要进步。它的目标是在图像序列中分割移动对象和背景，只需要最小的标注即可。在本文中，我们提出了首个基础模型，名为iMOS， дляMOS在医学图像中。我们在大量多Modal医学数据集上进行了广泛的实验，并证明了提案的iMOS的有效性。具体来说，只需要序列中标注一小部分的图像，iMOS可以在整个序列中在双向 achieve satisfactory tracking和分割性能。我们希望通过提案的iMOS，可以加速专家的标注速度，并促进医学基础模型的发展。
</details></li>
</ul>
<hr>
<h2 id="PlaceNav-Topological-Navigation-through-Place-Recognition"><a href="#PlaceNav-Topological-Navigation-through-Place-Recognition" class="headerlink" title="PlaceNav: Topological Navigation through Place Recognition"></a>PlaceNav: Topological Navigation through Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17260">http://arxiv.org/abs/2309.17260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lauri Suomela, Jussi Kalliola, Harry Edelman, Joni-Kristian Kämäräinen</li>
<li>for: 提高导航性能，使用视觉地标识选择目标并增加训练数据可用性。</li>
<li>methods: 将机器人独立部分分成导航特有和通用计算视觉部分，利用视觉地标识进行目标选择，提高导航性能和计算效率。</li>
<li>results: 实验结果表明，新模型在室内和室外导航任务中取得76%和23%的高Success rate，同时具有更高的计算效率。<details>
<summary>Abstract</summary>
Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present PlaceNav, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayesian filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher success rate in indoor and 23% higher in outdoor navigation tasks with higher computational efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Knowledge-Graphs-for-the-Life-Sciences-Recent-Developments-Challenges-and-Opportunities"><a href="#Knowledge-Graphs-for-the-Life-Sciences-Recent-Developments-Challenges-and-Opportunities" class="headerlink" title="Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities"></a>Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17255">http://arxiv.org/abs/2309.17255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaoyan Chen, Hang Dong, Janna Hastings, Ernesto Jiménez-Ruiz, Vanessa López, Pierre Monnin, Catia Pesquita, Petr Škoda, Valentina Tamma</li>
<li>for: 本研究论文主要针对生命科学领域中的数据管理和科学发现问题，旨在探讨Recent Developments and Advances in Graph-Based Technologies in Life Sciences和其未来发展的可能性。</li>
<li>methods: 本论文使用的方法包括知识图构建和管理、知识图和相关技术在科学发现中的应用、以及使用知识图支持解释的人工智能应用。</li>
<li>results: 本论文通过选择一些优秀的应用场景，描述了constructing and managing Knowledge Graphs (KGs)、使用KGs和相关技术在科学发现中的发现新知识、以及使用KGs支持解释的人工智能应用等三个主题的发展和挑战。<details>
<summary>Abstract</summary>
The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.   The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.   In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of new knowledge, and the use of KGs in artificial intelligence applications to support explanations (explainable AI). We select a few exemplary use cases for each topic, discuss the challenges and open research questions within these topics, and conclude with a perspective and outlook that summarizes the overarching challenges and their potential solutions as a guide for future research.
</details>
<details>
<summary>摘要</summary>
生命科学一词汇指的是研究生物和生物过程的学科，包括化学、生物、医学和其他相关学科。生命科学的研究努力充满数据，因为它们生成和消耗巨量数据，大多数数据是相互关联的graph结构。因此，使用高级知识驱动技术来管理和解释数据是不可或缺的。在这篇调查和观点论文中，我们讨论了生命科学中使用graph技术的最新发展和进步，以及这些技术将来对这些领域的影响。我们分为三个主题来讨论这些话题：建立和管理知识图грам（KG）、使用KG和相关技术在发现新知识方面、以及使用KG在人工智能应用中支持解释（Explainable AI）。我们选择了一些优秀的使用案例，讨论了每个话题中的挑战和未解决问题，并结束于一个视野和展望，概括了总体挑战和其可能的解决方案，以备未来研究的指南。
</details></li>
</ul>
<hr>
<h2 id="Forest-Mixing-investigating-the-impact-of-multiple-search-trees-and-a-shared-refinements-pool-on-ontology-learning"><a href="#Forest-Mixing-investigating-the-impact-of-multiple-search-trees-and-a-shared-refinements-pool-on-ontology-learning" class="headerlink" title="Forest Mixing: investigating the impact of multiple search trees and a shared refinements pool on ontology learning"></a>Forest Mixing: investigating the impact of multiple search trees and a shared refinements pool on ontology learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17252">http://arxiv.org/abs/2309.17252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Pop-Mihali, Adrian Groza</li>
<li>for: 这个论文目的是开发白盒机器学习算法，特ízarz是学习描述逻辑中的axioms。</li>
<li>methods: 这个论文扩展了DL-Learner工具中的Class Expression Learning for Ontology Engineering（CELOE）算法，使用多个搜索树和共享的修改池来分解搜索空间。它还引入了 conjunction 操作，将每棵搜索树中的最佳类表达合并，保留最多提供信息的结果。</li>
<li>results: 这个论文的实现和设置表明，Forest Mixing Approach不能超越传统的CELOE。然而，这种概念方案可能会在未来对找到ontologies中的类表达进行改进，特ízarz是在大型搜索空间中穿梭。<details>
<summary>Abstract</summary>
We aim at development white-box machine learning algorithms. We focus here on algorithms for learning axioms in description logic. We extend the Class Expression Learning for Ontology Engineering (CELOE) algorithm contained in the DL-Learner tool. The approach uses multiple search trees and a shared pool of refinements in order to split the search space in smaller subspaces. We introduce the conjunction operation of best class expressions from each tree, keeping the results which give the most information. The aim is to foster exploration from a diverse set of starting classes and to streamline the process of finding class expressions in ontologies. %, particularly in large search spaces. The current implementation and settings indicated that the Forest Mixing approach did not outperform the traditional CELOE. Despite these results, the conceptual proposal brought forward by this approach may stimulate future improvements in class expression finding in ontologies. % and influence. % the way we traverse search spaces in general.
</details>
<details>
<summary>摘要</summary>
我们目标是开发白盒机器学习算法。我们主要关注描述逻辑学习算法。我们在DL-Learner工具中扩展了Class Expression Learning for Ontology Engineering（CELOE）算法。我们的方法使用多个搜索树和共享的修具池，以将搜索空间分成更小的子空间。我们引入了每棵树中最佳的类表达的 conjunction 操作，保留最多提供信息的结果。我们的目标是促进从多个起始类中的探索，并使找到ontology中的类表达更加高效。特别是在大型搜索空间中。现有的实现和设置表明，Forest Mixing方法不能超越传统的CELOE。尽管得到的结果不如预期，但我们的概念提案可能会在未来对找到ontology中的类表达进行改进。以及如何探索搜索空间的方式。
</details></li>
</ul>
<hr>
<h2 id="Batch-Calibration-Rethinking-Calibration-for-In-Context-Learning-and-Prompt-Engineering"><a href="#Batch-Calibration-Rethinking-Calibration-for-In-Context-Learning-and-Prompt-Engineering" class="headerlink" title="Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering"></a>Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17249">http://arxiv.org/abs/2309.17249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, Subhrajit Roy</li>
<li>for:  Addressing the problem of prompt brittleness and various bias factors in large language models (LLMs) to improve their performance.</li>
<li>methods:  Propose a simple and intuitive calibration method called Batch Calibration (BC) that controls contextual bias from batched input and unifies various prior approaches.</li>
<li>results:  Demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks using PaLM 2-(S, M, L) and CLIP models.Here’s the summary in Traditional Chinese:</li>
<li>for: 解决大型自然语言模型（LLM）中的提示硬化和各种提示因素的问题，以提高其性能。</li>
<li>methods: 提出一种简单且直觉的条件调整方法，即批调协调（Batch Calibration，BC），控制批调入力中的文本因素，统一多种先前方法。</li>
<li>results: 使用PaLM 2-(S, M, L)和CLIP模型，在多于10个自然语言理解和图像识别 зада务中，与先前的条件调整基准点相比，demonstrate state-of-the-art性能。<details>
<summary>Abstract</summary>
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allow it to learn the contextual bias from labeled data. We validate the effectiveness of BC with PaLM 2-(S, M, L) and CLIP models and demonstrate state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的Prompting和In-Context Learning（ICL）已成为高效学习方法。然而，LLM受到提示粗糙和提示中各种偏见的影响，包括格式化、选择词语和ICL示例。为解决这些问题，导致LLM表现下降，calibration方法被开发出来mitigate these biases and recover LLM performance。在这个工作中，我们首先进行了现有calibration方法的系统分析，并提供了一个统一的视图，同时揭示了失败的例子。受这些分析的激发，我们提议批量调整（Batch Calibration，BC），一种简单 yet intuitive的方法，控制批处理输入中的上下文偏见，统一多种先前方法，并有效地解决上述问题。BC是零shot，推理只需要进行一次批处理，并且不会增加额外成本。在少量输入 setup 中，我们进一步扩展了BC，让它从标注数据中学习上下文偏见。我们使用PaLM 2-(S, M, L)和CLIP模型验证BC的有效性，并在多于10种自然语言理解和图像分类任务上达到了前一些calibration基线。
</details></li>
</ul>
<hr>
<h2 id="MORPH-Design-Co-optimization-with-Reinforcement-Learning-via-a-Differentiable-Hardware-Model-Proxy"><a href="#MORPH-Design-Co-optimization-with-Reinforcement-Learning-via-a-Differentiable-Hardware-Model-Proxy" class="headerlink" title="MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy"></a>MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17227">http://arxiv.org/abs/2309.17227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanpeng He, Matei Ciocarlie</li>
<li>for: 这篇论文是用来描述一种基于强化学习的硬件设计参数和控制策略优化方法（MORPH）的研究。</li>
<li>methods: 这种方法使用了一个可微的硬件代理模型，用于与长期控制策略一起进行协同优化。</li>
<li>results: 在模拟的2D抓取和3D多根手 manipulate任务中，MORPH方法能够有效地优化硬件设计参数和控制策略，并保证硬件代理模型与真实硬件保持最接近。<details>
<summary>Abstract</summary>
We introduce MORPH, a method for co-optimization of hardware design parameters and control policies in simulation using reinforcement learning. Like most co-optimization methods, MORPH relies on a model of the hardware being optimized, usually simulated based on the laws of physics. However, such a model is often difficult to integrate into an effective optimization routine. To address this, we introduce a proxy hardware model, which is always differentiable and enables efficient co-optimization alongside a long-horizon control policy using RL. MORPH is designed to ensure that the optimized hardware proxy remains as close as possible to its realistic counterpart, while still enabling task completion. We demonstrate our approach on simulated 2D reaching and 3D multi-fingered manipulation tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍MORPH，一种基于实验增强学习的硬件设计参数和控制策略优化方法。大多数合优方法都需要硬件模型，通常是基于物理法则进行模拟。但是这种模型往往难以与有效的优化 Routine 集成。为解决这个问题，我们引入了代理硬件模型，这个模型总是可导的，可以与长期控制策略使用增强学习进行合优。MORPH 是设计来确保优化的硬件代理保持与它的实际对应者最接近，同时仍能完成任务。我们在实验中使用了2D 掌上对象和3D 多指手 manipulation 任务来验证我们的方法。
</details></li>
</ul>
<hr>
<h2 id="RSAM-Learning-on-manifolds-with-Riemannian-Sharpness-aware-Minimization"><a href="#RSAM-Learning-on-manifolds-with-Riemannian-Sharpness-aware-Minimization" class="headerlink" title="RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization"></a>RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17215">http://arxiv.org/abs/2309.17215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Truong, Hoang-Phi Nguyen, Tung Pham, Minh-Tuan Tran, Mehrtash Harandi, Dinh Phung, Trung Le</li>
<li>for: 提高模型的泛化能力和Robustness</li>
<li>methods: 基于几何原理的优化，扩展了Sharpness-Aware Minimization（SAM）优化器到Riemannian manifold上</li>
<li>results: 提出了一种新的Riemannian Sharpness-Aware Minimization（RSAM）算法，并通过对各种数据集进行评估，证明RSAM可以提高模型的泛化能力和Robustness。<details>
<summary>Abstract</summary>
Nowadays, understanding the geometry of the loss landscape shows promise in enhancing a model's generalization ability. In this work, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization ability for constrained optimization problems. Indeed, this paper aims to generalize the Sharpness-Aware Minimization (SAM) optimizer to Riemannian manifolds. In doing so, we first extend the concept of sharpness and introduce a novel notion of sharpness on manifolds. To support this notion of sharpness, we present a theoretical analysis characterizing generalization capabilities with respect to manifold sharpness, which demonstrates a tighter bound on the generalization gap, a result not known before. Motivated by this analysis, we introduce our algorithm, Riemannian Sharpness-Aware Minimization (RSAM). To demonstrate RSAM's ability to enhance generalization ability, we evaluate and contrast our algorithm on a broad set of problems, such as image classification and contrastive learning across different datasets, including CIFAR100, CIFAR10, and FGVCAircraft. Our code is publicly available at \url{https://t.ly/RiemannianSAM}.
</details>
<details>
<summary>摘要</summary>
现在，理解损失地形的几何学特性显示有潜在的提升模型泛化能力的承诺。在这项工作中，我们启发自前人在优化中应用几何学原理，并提出一种新的方法来提高约束优化问题的Robustness和泛化能力。具体来说，这篇论文旨在推广Sharpness-Aware Minimization（SAM）优化器到Riemannian manifold上。在这个过程中，我们首先扩展了锐度的概念，并提出了一种新的 manifold锐度的概念。为支持这种锐度概念，我们提供了一种 theoretically分析，用于Characterizing generalization capabilities with respect to manifold sharpness，这个结果在之前没有知道。这个分析的激励我们提出了我们的算法，即Riemannian Sharpness-Aware Minimization（RSAM）。为证明RSAM能够提高泛化能力，我们在不同的数据集上进行了评估和比较，包括CIFAR100、CIFAR10和FGVCAircraft等。我们的代码可以在 \url{https://t.ly/RiemannianSAM} 上获取。
</details></li>
</ul>
<hr>
<h2 id="ComSD-Balancing-Behavioral-Quality-and-Diversity-in-Unsupervised-Skill-Discovery"><a href="#ComSD-Balancing-Behavioral-Quality-and-Diversity-in-Unsupervised-Skill-Discovery" class="headerlink" title="ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery"></a>ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17203">http://arxiv.org/abs/2309.17203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuxin0824/comsd">https://github.com/liuxin0824/comsd</a></li>
<li>paper_authors: Xin Liu, Yaran Chen, Dongbin Zhao</li>
<li>for: 本研究旨在提出一种能够自适应和适应多种下游任务的不监督技能发现方法，即Contrastive multi-objectives Skill Discovery（ComSD）。</li>
<li>methods: ComSD使用了对照学习来更正确地估计技能conditioned entropy的MI decomposition，并提出了一种动态权重机制来协调不同 entropy 的估计。</li>
<li>results: ComSD在数字评估中表现出了state-of-the-art的适应性，对于多种技能组合任务和大多数技能练习任务都有显著的优异性。<details>
<summary>Abstract</summary>
Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes to employ contrastive learning for a more reasonable estimation of skill-conditioned entropy in MI decomposition. In addition, a novel weighting mechanism is proposed to dynamically balance different entropy (in MI decomposition) estimations into a novel multi-objective intrinsic reward, to improve both skill diversity and quality. For challenging robot behavior discovery, ComSD can produce a qualified skill set consisting of diverse behaviors at different activity levels, which recent advanced methods cannot. On numerical evaluations, ComSD exhibits state-of-the-art adaptation performance, significantly outperforming recent advanced skill discovery methods across all skill combination tasks and most skill finetuning tasks. Codes will be released at https://github.com/liuxin0824/ComSD.
</details>
<details>
<summary>摘要</summary>
学习多样化和资格化行为的自适应能力是智能生物体的关键能力。理想的无监督技能发现方法应能够在缺乏外在奖励的情况下生成多样化和资格化的技能，而发现的技能集可以有效地适应下游任务的多种方式。在理论上，最大化技能和访问状态之间的共同信息（MI）可以实现理想的技能决策精炼。然而，现有的高级方法在实践中很难均衡行为质量（探索）和多样性（利用），这可能由高级方法的僵化内置奖励设计所致。在这篇论文中，我们提出了对比多目标技能发现（ComSD）方法，该方法通过更合理的MI估计和动态权重的内置奖励来减少发现行为质量与多样性之间的冲突。ComSD方法使用对比学习来更合理地估计技能决策下的 entropy，并提出了一种新的权重机制来动态平衡不同 entropy（在MI decompositions）的估计，以提高技能多样性和质量。对于具有挑战性的机器人行为发现问题，ComSD可以生成包含不同活动水平的多样化技能集，而现有高级方法无法达到这一点。在数值评估中，ComSD显示出了领先的适应性，在所有技能组合任务和大多数技能精化任务中显著超过了现有高级技能发现方法。代码将在 <https://github.com/liuxin0824/ComSD> 上发布。
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-Into-Race-Bias-in-Random-Forest-Models-Based-on-Breast-DCE-MRI-Derived-Radiomics-Features"><a href="#An-Investigation-Into-Race-Bias-in-Random-Forest-Models-Based-on-Breast-DCE-MRI-Derived-Radiomics-Features" class="headerlink" title="An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features"></a>An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17197">http://arxiv.org/abs/2309.17197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Huti, Tiarna Lee, Elinor Sawyer, Andrew P. King</li>
<li>for: 这研究旨在检查使用静止增强MRI数据预测乳腺癌分子亚型时，random forest（RF）模型是否受到种族偏见的影响。</li>
<li>methods: 研究使用了静止增强MRI数据 derivated的 радиом科特征，并使用RF模型预测乳腺癌患者的种族。</li>
<li>results: 研究发现，静止增强MRI数据中含有可识别种族信息，RF模型可以使用这些数据预测白人和黑人种族的准确率在60-70%之间，具体取决于使用的特征子。此外，使用种族不均衡数据预测乳腺癌分子类型的RF模型似乎会产生偏见行为，在测试数据上表现出比较好的性能。<details>
<summary>Abstract</summary>
Recent research has shown that artificial intelligence (AI) models can exhibit bias in performance when trained using data that are imbalanced by protected attribute(s). Most work to date has focused on deep learning models, but classical AI techniques that make use of hand-crafted features may also be susceptible to such bias. In this paper we investigate the potential for race bias in random forest (RF) models trained using radiomics features. Our application is prediction of tumour molecular subtype from dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) of breast cancer patients. Our results show that radiomics features derived from DCE-MRI data do contain race-identifiable information, and that RF models can be trained to predict White and Black race from these data with 60-70% accuracy, depending on the subset of features used. Furthermore, RF models trained to predict tumour molecular subtype using race-imbalanced data seem to produce biased behaviour, exhibiting better performance on test data from the race on which they were trained.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "imbalanced data" 不具有保护属性的数据 (i.e., data that are not balanced by protected attributes)* "hand-crafted features" 手动设计的特征 (i.e., features that are manually designed or selected)* "radiomics features" 镭医学特征 (i.e., features extracted from medical imaging data, such as DCE-MRI)* "tumor molecular subtype" 肿瘤分子亚型 (i.e., the molecular characteristics of a tumor, which can be used to predict its behavior and potential treatment outcomes)* "race-identifiable information" 可以识别出种族信息的特征 (i.e., features that can be used to identify an individual's race)* "biased behavior" 偏见行为 (i.e., behavior that is biased towards a particular group or outcome, rather than being impartial or fair)
</details></li>
</ul>
<hr>
<h2 id="PARF-Primitive-Aware-Radiance-Fusion-for-Indoor-Scene-Novel-View-Synthesis"><a href="#PARF-Primitive-Aware-Radiance-Fusion-for-Indoor-Scene-Novel-View-Synthesis" class="headerlink" title="PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis"></a>PARF: Primitive-Aware Radiance Fusion for Indoor Scene Novel View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17190">http://arxiv.org/abs/2309.17190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiyang Ying, Baowei Jiang, Jinzhi Zhang, Di Xu, Tao Yu, Qionghai Dai, Lu Fang</li>
<li>for: 该论文提出了一种快速场景频谱场 reconstruction方法，以便实现强大的新视图合成性和方便的场景编辑功能。</li>
<li>methods: 该方法利用语义分析和基本元素提取来约束和加速频谱场 reconstruction过程。具体来说，该方法提出了一种基于基本元素的混合渲染策略，以便得到最佳的频谱场 reconstruction效果。</li>
<li>results: 该论文的实验表明，该方法可以快速重建场景频谱场，并且有高的渲染质量和方便的编辑功能。<details>
<summary>Abstract</summary>
This paper proposes a method for fast scene radiance field reconstruction with strong novel view synthesis performance and convenient scene editing functionality. The key idea is to fully utilize semantic parsing and primitive extraction for constraining and accelerating the radiance field reconstruction process. To fulfill this goal, a primitive-aware hybrid rendering strategy was proposed to enjoy the best of both volumetric and primitive rendering. We further contribute a reconstruction pipeline conducts primitive parsing and radiance field learning iteratively for each input frame which successfully fuses semantic, primitive, and radiance information into a single framework. Extensive evaluations demonstrate the fast reconstruction ability, high rendering quality, and convenient editing functionality of our method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Alphazero-like-Tree-Search-can-Guide-Large-Language-Model-Decoding-and-Training"><a href="#Alphazero-like-Tree-Search-can-Guide-Large-Language-Model-Decoding-and-Training" class="headerlink" title="Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training"></a>Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17179">http://arxiv.org/abs/2309.17179</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/waterhorse1/llm_tree_search">https://github.com/waterhorse1/llm_tree_search</a></li>
<li>paper_authors: Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, Jun Wang</li>
<li>for: 提高大型自然语言模型（LLM）的理解和解释能力</li>
<li>methods: 使用搜索算法和学习值函数</li>
<li>results: 在推理、规划和RLHFAlignment等任务中表现出色，具有普适性和扩展性<details>
<summary>Abstract</summary>
Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting advanced, large-scale models. (2) It can guide LLM's decoding during both inference and training. Empirical evaluations across reasoning, planning, and RLHF alignment tasks validate the effectiveness of TS-LLM, even on trees with a depth of 64.
</details>
<details>
<summary>摘要</summary>
为了解决这些局限性，我们提出了一种 AlphaZero 类型的树搜索框架 для LLM（称为 TS-LLM），系统地说明了如何使用树搜索和学习的值函数来导航 LLM 的解码能力。TS-LLM 的两个关键特点是：1. 利用学习的值函数，我们的方法可以通用于不同的任务，不仅是理解，而且可以应用于不同的 LLM 大小和任务，无需提供高级大型模型的提示。2. 它可以 guid LLM 的解码在推理和训练过程中。在理解、规划和RLHFAlignment任务上进行了实质性的评估，证明了 TS-LLM 的效果，甚至在深度为 64 的树上。
</details></li>
</ul>
<hr>
<h2 id="RLAdapter-Bridging-Large-Language-Models-to-Reinforcement-Learning-in-Open-Worlds"><a href="#RLAdapter-Bridging-Large-Language-Models-to-Reinforcement-Learning-in-Open-Worlds" class="headerlink" title="RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds"></a>RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17176">http://arxiv.org/abs/2309.17176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanpeng Zhang, Zongqing Lu</li>
<li>for: 增强决策问题中的决策效果，通过使用强大的语言模型（LLMs）来帮助代理人学习策略。</li>
<li>methods: RLAdapter框架，通过在RL algoritma的训练过程中附加一个适配器模型，使得LLMs更好地适应下游任务，从而为RL agents提供更好的指导。</li>
<li>results: RLAdapter在Crafter环境中的实验结果表明，RLAdapter比基线模型更高效，同时代理人在我们的框架下展现出了缺失在基线模型中的常识行为。<details>
<summary>Abstract</summary>
While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating an adapter model. Within the RLAdapter framework, fine-tuning a lightweight language model with information generated during the training process of RL agents significantly aids LLMs in adapting to downstream tasks, thereby providing better guidance for RL agents. We conducted experiments to evaluate RLAdapter in the Crafter environment, and the results show that RLAdapter surpasses the SOTA baselines. Furthermore, agents under our framework exhibit common-sense behaviors that are absent in baseline models.
</details>
<details>
<summary>摘要</summary>
whilst reinforcement learning (RL) displays remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating an adapter model. Within the RLAdapter framework, fine-tuning a lightweight language model with information generated during the training process of RL agents significantly aids LLMs in adapting to downstream tasks, thereby providing better guidance for RL agents. We conducted experiments to evaluate RLAdapter in the Crafter environment, and the results show that RLAdapter surpasses the SOTA baselines. Furthermore, agents under our framework exhibit common-sense behaviors that are absent in baseline models.Note: "SOTA" stands for "State of the Art", and "baselines" refer to the standard or default settings or models used for comparison. "Common-sense behaviors" refer to the ability of the agents to exhibit behaviors that are intuitive and reasonable based on human experience and knowledge.
</details></li>
</ul>
<hr>
<h2 id="A-Vision-Guided-Robotic-System-for-Grasping-Harvested-Tomato-Trusses-in-Cluttered-Environments"><a href="#A-Vision-Guided-Robotic-System-for-Grasping-Harvested-Tomato-Trusses-in-Cluttered-Environments" class="headerlink" title="A Vision-Guided Robotic System for Grasping Harvested Tomato Trusses in Cluttered Environments"></a>A Vision-Guided Robotic System for Grasping Harvested Tomato Trusses in Cluttered Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17170">http://arxiv.org/abs/2309.17170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luuk van den Bent, Tomás Coleman, Robert Babuska</li>
<li>for: automating truss tomato weighing and packaging processes</li>
<li>methods: deep learning-based vision system to identify and grasp trusses in a crate with clutter</li>
<li>results: 100% clearance rate and 93% success rate of grasping trusses on the first try<details>
<summary>Abstract</summary>
Currently, truss tomato weighing and packaging require significant manual work. The main obstacle to automation lies in the difficulty of developing a reliable robotic grasping system for already harvested trusses. We propose a method to grasp trusses that are stacked in a crate with considerable clutter, which is how they are commonly stored and transported after harvest. The method consists of a deep learning-based vision system to first identify the individual trusses in the crate and then determine a suitable grasping location on the stem. To this end, we have introduced a grasp pose ranking algorithm with online learning capabilities. After selecting the most promising grasp pose, the robot executes a pinch grasp without needing touch sensors or geometric models. Lab experiments with a robotic manipulator equipped with an eye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all trusses from a pile. 93% of the trusses were successfully grasped on the first try, while the remaining 7% required more attempts.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".Translation notes:* "truss tomato" is translated as " Tomatoes 葫芦" ( Tomatoes is a noun, and 葫芦 is a measure word used to indicate a bunch of tomatoes)* "weighing and packaging" is translated as "重量和包装" (重量 means weight, and 包装 means packaging)* "lab experiments" is translated as "实验室试验" (实验室 means laboratory, and 试验 means experiment)* "robotic manipulator" is translated as "机器人操作机构" (机器人 means robot, and 操作机构 means manipulator)* "eye-in-hand RGB-D camera" is translated as "手上RGB-D相机" (手上 means hand-held, RGB-D is a type of camera, and 相机 means camera)
</details></li>
</ul>
<hr>
<h2 id="An-evaluation-of-GPT-models-for-phenotype-concept-recognition"><a href="#An-evaluation-of-GPT-models-for-phenotype-concept-recognition" class="headerlink" title="An evaluation of GPT models for phenotype concept recognition"></a>An evaluation of GPT models for phenotype concept recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17169">http://arxiv.org/abs/2309.17169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tudor Groza, Harry Caufield, Dylan Gration, Gareth Baynam, Melissa A Haendel, Peter N Robinson, Chris J Mungall, Justin T Reese</li>
<li>for: 这个论文的目的是探讨用最新的生成器预训练变换器（GPT）模型在医学深度型定义中的表现。</li>
<li>methods: 这个研究使用了七个提示语，两个GPT模型（gpt-3.5和gpt-4.0），以及一个已知的金标准 для fenotype认识。</li>
<li>results: 研究结果表明，目前这些模型尚未达到状态理想的表现。最佳运行使用了少量学习，达到了0.41 F1分数，相比之下，现有的最佳工具可以达到0.62 F1分数。<details>
<summary>Abstract</summary>
Objective: Clinical deep phenotyping plays a critical role in both the diagnosis of patients with rare disorders as well as in building care coordination plans. The process relies on modelling and curating patient profiles using ontology concepts, usually from the Human Phenotype Ontology. Machine learning methods have been widely adopted to support this phenotype concept recognition task. With the significant shift in the use of large language models (LLMs) for most NLP tasks, herewithin, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT in clinical deep phenotyping. Materials and Methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5 and gpt-4.0) and an established gold standard for phenotype recognition. Results: Our results show that, currently, these models have not yet achieved state of the art performance. The best run, using few-shots learning, achieved 0.41 F1 score, compared to a 0.62 F1 score achieved by the current best in class tool. Conclusion: The non-deterministic nature of the outcomes and the lack of concordance between different runs using the same prompt and input makes the use of these LLMs in clinical settings problematic.
</details>
<details>
<summary>摘要</summary>
Materials and Methods: 我们的实验设置包括七个提问，每个提问都有不同的特定程度，以及两个GPT模型（gpt-3.5和gpt-4.0）和已知的现象识别标准。Results: 我们的结果表明，目前这些模型还没有达到状态的艺术性能。最好的运行，使用少量学习，达到了0.41的F1分数，与当前最佳的类别工具的0.62的F1分数相比，表明这些LLMs在医学设置中的使用具有问题。Conclusion: 非束定的结果和使用同一个提问和输入的不同run之间的不一致性，使得这些LLMs在医学设置中的使用变得困难。
</details></li>
</ul>
<hr>
<h2 id="DyVal-Graph-informed-Dynamic-Evaluation-of-Large-Language-Models"><a href="#DyVal-Graph-informed-Dynamic-Evaluation-of-Large-Language-Models" class="headerlink" title="DyVal: Graph-informed Dynamic Evaluation of Large Language Models"></a>DyVal: Graph-informed Dynamic Evaluation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17167">http://arxiv.org/abs/2309.17167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie</li>
<li>for: 本研究旨在描述一种新的评估协议 DyVal，用于动态评估大型自然语言模型（LLM）的性能。</li>
<li>methods: 本研究使用了指定的执行图（DAG）来生成动态评估样本，以控制样本的复杂性。</li>
<li>results: 实验表明， LLM 在 DyVal 生成的评估样本中表现不佳，强调了动态评估的重要性。  Additionally, the researchers analyzed the failure cases and results of different prompting methods, and found that the DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks.<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with different complexities, emphasizing the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks. We hope that DyVal can shed light on the future evaluation research of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种评估标准上表现出色，但是有关其表现的问题在它们训练集中可能存在数据污染。此外，现有的benchmark测试集可能无法准确评估LLM的进步。在这篇论文中，我们介绍了DyVal，一种新的评估协议，用于动态评估LLM。我们基于我们的提议的动态评估框架，使用指定的拓扑结构来动态生成评估样本，以控制样本的复杂性。DyVal生成了包括数学、逻辑推理和算法问题在内的复杂理解任务的挑战评估集。我们对多种LLM（从Flan-T5-large到ChatGPT和GPT4）进行了评估，实验表明，LLM在DyVal生成的评估样本中表现差，这 highlights the importance of动态评估。我们还分析了不同的提示方法的失败情况和结果。此外，DyVal生成的样本不仅是评估集，还可以用于 LLM 的微调以提高其在现有benchmark上的表现。我们希望DyVal可以引导未来的LLM评估研究。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-Kidney-Biopsy-Structural-Assessment-through-Dense-Instance-Segmentation"><a href="#Advances-in-Kidney-Biopsy-Structural-Assessment-through-Dense-Instance-Segmentation" class="headerlink" title="Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation"></a>Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17166">http://arxiv.org/abs/2309.17166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhan Xiong, Junling He, Pieter Valkema, Tri Q. Nguyen, Maarten Naesens, Jesper Kers, Fons J. Verbeek</li>
<li>for: 这篇论文主要写于 Automatically obtaining statistics per segmented anatomical object in kidney biopsy 的问题上, 以减少劳动力和 semi-quantitative lesion scores 的间观 observer variability.</li>
<li>methods: 该论文提出了一种 anchor-free instance segmentation model,  combining diffusion models, transformer modules, and RCNNs (regional convolution neural networks)，可以有效地处理 densely touching anatomical structures, 多个类别和不同的尺寸和形状。</li>
<li>results: 该模型在一个 NVIDIA GeForce RTX 3090 GPU 上训练，可以高效地识别 renal biopsy 中的 más than 500 个 object，包括 glomeruli, tubuli, 和arteries。其数据集包括 148 个 Jones’ silver-stained renal whole slide images (WSIs)，其中 249 个 patches 用于训练，54 个 patches 用于评估。此外，模型可以直接转移领域，无需调整或重新训练，可以在 PAS-stained WSIs 上生成 decent instance segmentation results。与基eline models 相比，该模型达到了新的 state-of-the-art，其 AP 值为 51.7%。<details>
<summary>Abstract</summary>
The kidney biopsy is the gold standard for the diagnosis of kidney diseases. Lesion scores made by expert renal pathologists are semi-quantitative and suffer from high inter-observer variability. Automatically obtaining statistics per segmented anatomical object, therefore, can bring significant benefits in reducing labor and this inter-observer variability. Instance segmentation for a biopsy, however, has been a challenging problem due to (a) the on average large number (around 300 to 1000) of densely touching anatomical structures, (b) with multiple classes (at least 3) and (c) in different sizes and shapes. The currently used instance segmentation models cannot simultaneously deal with these challenges in an efficient yet generic manner. In this paper, we propose the first anchor-free instance segmentation model that combines diffusion models, transformer modules, and RCNNs (regional convolution neural networks). Our model is trained on just one NVIDIA GeForce RTX 3090 GPU, but can efficiently recognize more than 500 objects with 3 common anatomical object classes in renal biopsies, i.e., glomeruli, tubuli, and arteries. Our data set consisted of 303 patches extracted from 148 Jones' silver-stained renal whole slide images (WSIs), where 249 patches were used for training and 54 patches for evaluation. In addition, without adjustment or retraining, the model can directly transfer its domain to generate decent instance segmentation results from PAS-stained WSIs. Importantly, it outperforms other baseline models and reaches an AP 51.7% in detection as the new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
《肾脏生剖图像自动实例分割》肾脏生剖图像自动实例分割是诊断肾脏疾病的标准方法。但是，由专家肾脏 PATHOLOGISTS 评分的病变得分是不准确的，而且受到高度的 observer variability 影响。通过自动获取每个分割的统计数据，可以带来显著的减少劳动力和 observer variability 的效果。然而，实例分割问题在肾脏生剖图像上是一个挑战，因为：1. 肾脏生剖图像平均含有300-1000个密集的生物结构，2. 有多个类型（至少3种），3. 具有不同的大小和形状。目前使用的实例分割模型无法同时解决这些挑战。在这篇论文中，我们提出了第一个无锚点instance segmentation模型，结合了扩散模型、transformer模块和RCNNs（区域 convolutional neural networks）。我们的模型在一个NVIDIA GeForce RTX 3090 GPU上训练，可以高效地识别肾脏生剖图像中的 más de 500个 объек目，包括glomeruli、 tubuli 和arteries。我们的数据集包括148个Jones银色染涂肾脏整个扫描图像（WSIs），其中249个patches用于训练，54个patches用于评估。此外，无需调整或重新训练，我们的模型可以直接传递领域，在PAS染色涂抹扫描图像上生成良好的实例分割结果。最重要的是，它超越了其他基线模型，达到了新的状态空间 AP 51.7% 的检测率。
</details></li>
</ul>
<hr>
<h2 id="Compromise-in-Multilateral-Negotiations-and-the-Global-Regulation-of-Artificial-Intelligence"><a href="#Compromise-in-Multilateral-Negotiations-and-the-Global-Regulation-of-Artificial-Intelligence" class="headerlink" title="Compromise in Multilateral Negotiations and the Global Regulation of Artificial Intelligence"></a>Compromise in Multilateral Negotiations and the Global Regulation of Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17158">http://arxiv.org/abs/2309.17158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michal Natorski</li>
<li>for: 这篇论文是为了探讨联合国教科文组织（UNESCO）在2021年11月采取的人工智能伦理准则的国际谈判中的纠纷和妥协点。</li>
<li>methods: 本论文使用了一个独特的主要资料集，包括written positions和录制的讨论，以解释UNESCO成员国之间的多元观点和冲突。</li>
<li>results: 本论文发现，通过boltanski的实用社会学概念，Multilateral谈判实践中的结构性normative杂合和 situational normative ambiguity的结合，使得多方面谈判中的妥协成功。<details>
<summary>Abstract</summary>
As artificial intelligence (AI) technologies spread worldwide, international discussions have increasingly focused on their consequences for democracy, human rights, fundamental freedoms, security, and economic and social development. In this context, UNESCO's Recommendation on the Ethics of Artificial Intelligence, adopted in November 2021, has emerged as the first global normative framework for AI development and deployment. The intense negotiations of every detail of the document brought forth numerous controversies among UNESCO member states. Drawing on a unique set of primary sources, including written positions and recorded deliberations, this paper explains the achievement of global compromise on AI regulation despite the multiplicity of UNESCO member-state positions representing a variety of liberal and sovereignist preferences. Building upon Boltanski's pragmatic sociology, it conceptualises the practice of multilateral negotiations and attributes the multilateral compromise to two embedded therein mechanisms: Structural normative hybridity and situated normative ambiguity allowed to accomplish a compromise by linking macro-normative structures with situated debates of multilateral negotiations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Age-Group-Discrimination-via-Free-Handwriting-Indicators"><a href="#Age-Group-Discrimination-via-Free-Handwriting-Indicators" class="headerlink" title="Age Group Discrimination via Free Handwriting Indicators"></a>Age Group Discrimination via Free Handwriting Indicators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17156">http://arxiv.org/abs/2309.17156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugenio Lomurno, Simone Toffoli, Davide Di Febbo, Matteo Matteucci, Francesca Lunardini, Simona Ferrante</li>
<li>for: 这个研究旨在探索一种使用实验墨水笔来评估年龄层的方法，以提高老年人群中的评估和预后评估。</li>
<li>methods: 这个研究使用了一个具有14个姿势和震动相关指标的工具，将手写数据分为五个分类任务，使用Catboost和Logistic Regression分类器进行分类。</li>
<li>results: 研究结果显示了Exceptional的分类性能，精度在82.5%至97.5%之间，特征选择率在81.8%至100%之间， recall在75%至100%之间，ROC-AUC在92.2%至100%之间。<details>
<summary>Abstract</summary>
The growing global elderly population is expected to increase the prevalence of frailty, posing significant challenges to healthcare systems. Frailty, a syndrome associated with ageing, is characterised by progressive health decline, increased vulnerability to stressors and increased risk of mortality. It represents a significant burden on public health and reduces the quality of life of those affected. The lack of a universally accepted method to assess frailty and a standardised definition highlights a critical research gap. Given this lack and the importance of early prevention, this study presents an innovative approach using an instrumented ink pen to ecologically assess handwriting for age group classification. Content-free handwriting data from 80 healthy participants in different age groups (20-40, 41-60, 61-70 and 70+) were analysed. Fourteen gesture- and tremor-related indicators were computed from the raw data and used in five classification tasks. These tasks included discriminating between adjacent and non-adjacent age groups using Catboost and Logistic Regression classifiers. Results indicate exceptional classifier performance, with accuracy ranging from 82.5% to 97.5%, precision from 81.8% to 100%, recall from 75% to 100% and ROC-AUC from 92.2% to 100%. Model interpretability, facilitated by SHAP analysis, revealed age-dependent sensitivity of temporal and tremor-related handwriting features. Importantly, this classification method offers potential for early detection of abnormal signs of ageing in uncontrolled settings such as remote home monitoring, thereby addressing the critical issue of frailty detection and contributing to improved care for older adults.
</details>
<details>
<summary>摘要</summary>
全球老龄化的人口增长会增加衰退的预测，对医疗系统造成重大挑战。衰退是年龄相关的症状，表现为健康下降、增加外在压力的敏感性和死亡风险增加。它对公共卫生造成重大负担，reduce the quality of life of those affected。由于没有一个通用的方法来评估衰退和标准化定义，这个研究 gap  highlights  the critical need for early prevention. 为了解决这个问题，本研究提出了一种创新的方法，使用Instrumented Ink Pen来生物学地评估手写功能，以分类不同年龄组。研究从健康参与者80人的内容自由手写数据中分析了14个指标，并将其用于5个分类任务。这些任务包括使用Catboost和Logistic Regression分类器来分别区分邻近和非邻近年龄组。结果表明分类器的表现非常出色，准确率从82.5%到97.5%，精度从81.8%到100%，回归率从75%到100%，ROC-AUC从92.2%到100%。模型可读性，通过SHAP分析，表明年龄依赖性的时间和震动相关手写特征。这种分类方法有潜在的潜在的早期识别衰退的异常迹象，并且可以在没有控制的家庭监测设置中进行识别，因此有助于改善老年人的护理。
</details></li>
</ul>
<hr>
<h2 id="Using-Large-Language-Models-for-Qualitative-Analysis-can-Introduce-Serious-Bias"><a href="#Using-Large-Language-Models-for-Qualitative-Analysis-can-Introduce-Serious-Bias" class="headerlink" title="Using Large Language Models for Qualitative Analysis can Introduce Serious Bias"></a>Using Large Language Models for Qualitative Analysis can Introduce Serious Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17147">http://arxiv.org/abs/2309.17147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Ashwin, Aditya Chhabra, Vijayendra Rao</li>
<li>for: 这篇论文问题是使用自然语言处理技术来分析大量访问讲述资料，具体来说是使用大型自然语言模型（LLM）来分析采访讲述录音。</li>
<li>methods: 该论文使用了LLM来标注采访讲述录音，并对这些标注进行评估。</li>
<li>results: 研究发现，使用LLM来标注采访讲述录音存在偏见风险，这些偏见可能导致错误的推论。在使用高质量人工标注和灵活编码来训练简单的超vised模型时，发现这些模型的评估错误和偏见较少。因此，作者认为，在评估LLM是否引入偏见时，需要一定的高质量标注，而不是直接使用LLM来标注。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are quickly becoming ubiquitous, but the implications for social science research are not yet well understood. This paper asks whether LLMs can help us analyse large-N qualitative data from open-ended interviews, with an application to transcripts of interviews with Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of caution is needed in using LLMs to annotate text as there is a risk of introducing biases that can lead to misleading inferences. We here mean bias in the technical sense, that the errors that LLMs make in annotating interview transcripts are not random with respect to the characteristics of the interview subjects. Training simpler supervised models on high-quality human annotations with flexible coding leads to less measurement error and bias than LLM annotations. Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to train a bespoke model on these annotations than it is to use an LLM for annotation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prototype-Generation-Robust-Feature-Visualisation-for-Data-Independent-Interpretability"><a href="#Prototype-Generation-Robust-Feature-Visualisation-for-Data-Independent-Interpretability" class="headerlink" title="Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability"></a>Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17144">http://arxiv.org/abs/2309.17144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arush Tagade, Jessica Rumbelow</li>
<li>for: 本研究旨在提供一种具有更高准确性和可靠性的特征可视化方法，以便对无模型和数据依赖的图像分类模型进行可读性分析。</li>
<li>methods: 本研究使用了一种名为“Prototype Generation”的新方法，该方法可以生成具有自然Activation路径的输入，从而提高了特征可视化的准确性和可靠性。</li>
<li>results: 研究发现，通过使用Prototype Generation方法可以生成具有自然Activation路径的输入，并且可以 Quantitatively measure the similarity between the internal activations of the generated prototypes and natural images。此外，研究还发现，通过解释生成的 прототипы可以提供重要的 introspection，例如揭示模型中学习的偏见和偏好，这些 introspection 不可能通过测试集的量化方法来实现。<details>
<summary>Abstract</summary>
We introduce Prototype Generation, a stricter and more robust form of feature visualisation for model-agnostic, data-independent interpretability of image classification models. We demonstrate its ability to generate inputs that result in natural activation paths, countering previous claims that feature visualisation algorithms are untrustworthy due to the unnatural internal activations. We substantiate these claims by quantitatively measuring similarity between the internal activations of our generated prototypes and natural images. We also demonstrate how the interpretation of generated prototypes yields important insights, highlighting spurious correlations and biases learned by models which quantitative methods over test-sets cannot identify.
</details>
<details>
<summary>摘要</summary>
我们介绍原型生成，一种更加依hung和更加稳定的特征可视化技术，用于无模型、数据独立的图像分类器解释。我们显示了它的能力可以生成自然的活化路径，与先前的所有特征可视化算法不同，这些算法被认为是不可靠，因为它们内部的活化不自然。我们通过量化地衡量内部活化和自然图像之间的相似性，从而证实我们的主张。此外，我们还示出了解释生成的原型具有重要的问题解释能力，例如发现模型学习的偏好和偏预，这些方法不能通过测试集的量化方法进行识别。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Cephalometric-Landmark-Detection-from-the-view-of-Human-Pose-Estimation-with-Lightweight-Super-Resolution-Head"><a href="#Revisiting-Cephalometric-Landmark-Detection-from-the-view-of-Human-Pose-Estimation-with-Lightweight-Super-Resolution-Head" class="headerlink" title="Revisiting Cephalometric Landmark Detection from the view of Human Pose Estimation with Lightweight Super-Resolution Head"></a>Revisiting Cephalometric Landmark Detection from the view of Human Pose Estimation with Lightweight Super-Resolution Head</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17143">http://arxiv.org/abs/2309.17143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/5k5000/cldetection2023">https://github.com/5k5000/cldetection2023</a></li>
<li>paper_authors: Qian Wu, Si Yong Yeo, Yufei Chen, Jun Liu</li>
<li>for: 本研究旨在提高侵袋形态标志准确性，并且将人体姿势估计（HPE）技术转移到侵袋形态标志准确性领域，以提高其表现。</li>
<li>methods: 本研究使用了一个基于MMPose的可靠和适应性的底线，并将一个简单且高效的超频变化模组 incorporated into the framework，以提高表现。</li>
<li>results: 在MICCAI CLDetection2023挑战中，我们的方法在三个指标上排名第一名，并在另外一个指标上排名第三名。<details>
<summary>Abstract</summary>
Accurate localization of cephalometric landmarks holds great importance in the fields of orthodontics and orthognathics due to its potential for automating key point labeling. In the context of landmark detection, particularly in cephalometrics, it has been observed that existing methods often lack standardized pipelines and well-designed bias reduction processes, which significantly impact their performance. In this paper, we revisit a related task, human pose estimation (HPE), which shares numerous similarities with cephalometric landmark detection (CLD), and emphasize the potential for transferring techniques from the former field to benefit the latter. Motivated by this insight, we have developed a robust and adaptable benchmark based on the well-established HPE codebase known as MMPose. This benchmark can serve as a dependable baseline for achieving exceptional CLD performance. Furthermore, we introduce an upscaling design within the framework to further enhance performance. This enhancement involves the incorporation of a lightweight and efficient super-resolution module, which generates heatmap predictions on high-resolution features and leads to further performance refinement, benefiting from its ability to reduce quantization bias. In the MICCAI CLDetection2023 challenge, our method achieves 1st place ranking on three metrics and 3rd place on the remaining one. The code for our method is available at https://github.com/5k5000/CLdetection2023.
</details>
<details>
<summary>摘要</summary>
精准地标定几何学特征点对于 ortodontics 和 ortognathics 领域具有重要意义，因为它可能导致自动标记关键点的自动化。在几何学特征点检测上，特别是在 Cephalometrics 中，已经存在许多不同的方法，但它们通常缺乏标准化管道和良好的偏好减少过程，这会对其性能产生很大的影响。在这篇论文中，我们回到了相关的任务，即人姿估计（HPE），这两个任务之间存在许多相似之处，我们强调了从 HPE 领域中提取技术以便改善 CLD 性能。由于这一点，我们开发了一个可靠和可调整的benchmark，基于已知的 HPE 代码库，即 MMPose。这个benchmark可以作为 CLD 性能的可靠基准。此外，我们在框架中引入了一种增强设计，即 incorporating 一种轻量级高效的超分辨率模块，该模块在高分辨率特征上预测热图，从而进一步提高性能，借助其减少量化偏误的能力。在 MICCAI CLDetection2023 挑战中，我们的方法在三个指标上排名第一名，并在另外一个指标上排名第三名。我们的代码可以在 GitHub 上找到：https://github.com/5k5000/CLdetection2023。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-the-Abilities-of-Large-Language-Models-for-RDF-Knowledge-Graph-Creation-and-Comprehension-How-Well-Do-LLMs-Speak-Turtle"><a href="#Benchmarking-the-Abilities-of-Large-Language-Models-for-RDF-Knowledge-Graph-Creation-and-Comprehension-How-Well-Do-LLMs-Speak-Turtle" class="headerlink" title="Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?"></a>Benchmarking the Abilities of Large Language Models for RDF Knowledge Graph Creation and Comprehension: How Well Do LLMs Speak Turtle?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17122">http://arxiv.org/abs/2309.17122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aksw/llm-kg-bench">https://github.com/aksw/llm-kg-bench</a></li>
<li>paper_authors: Johannes Frey, Lars-Peter Meyer, Natanael Arndt, Felix Brei, Kirill Bulert</li>
<li>for: 评估大型自然语言模型（LLM）在知识图工程中的能力</li>
<li>methods: 创建了五个任务来评估不同 LLM 的能力，包括解析、理解、分析和生成知识图，并将其集成到了 LLM-KG-Bench 自动评估系统中</li>
<li>results: 研究发现，latest commercial models 在使用 Turtle 语言时表现出色，但它们在输出格式化要求上存在缺陷，需要进一步改进。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are advancing at a rapid pace, with significant improvements at natural language processing and coding tasks. Yet, their ability to work with formal languages representing data, specifically within the realm of knowledge graph engineering, remains under-investigated. To evaluate the proficiency of various LLMs, we created a set of five tasks that probe their ability to parse, understand, analyze, and create knowledge graphs serialized in Turtle syntax. These tasks, each embodying distinct degrees of complexity and being able to scale with the size of the problem, have been integrated into our automated evaluation system, the LLM-KG-Bench. The evaluation encompassed four commercially available LLMs - GPT-3.5, GPT-4, Claude 1.3, and Claude 2.0, as well as two freely accessible offline models, GPT4All Vicuna and GPT4All Falcon 13B. This analysis offers an in-depth understanding of the strengths and shortcomings of LLMs in relation to their application within RDF knowledge graph engineering workflows utilizing Turtle representation. While our findings show that the latest commercial models outperform their forerunners in terms of proficiency with the Turtle language, they also reveal an apparent weakness. These models fall short when it comes to adhering strictly to the output formatting constraints, a crucial requirement in this context.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在快速发展，具有显著改善的自然语言处理和编程任务能力。然而，它们在知识图工程中使用ormal语言表示数据的能力仍然尚未得到充分研究。为评估不同LLM的水平，我们创建了五个任务，这些任务涵盖了知识图解析、理解、分析和创建，并且能够随问题的大小缩放。这些任务被 integrate into our自动评估系统LLM-KG-Bench。我们对四种商业可用的LLM（GPT-3.5、GPT-4、Claude 1.3和Claude 2.0）以及两种自由 accessible的离线模型（GPT4All Vicuna和GPT4All Falcon 13B）进行了评估。这个分析提供了LLM在使用Turtle表示知识图工程中的强点和弱点，并且显示了最新的商业模型在Turtle语言方面的表现有所提升，但同时也发现了一点问题：这些模型在输出格式约束上不够严格。
</details></li>
</ul>
<hr>
<h2 id="Meta-Path-Learning-for-Multi-relational-Graph-Neural-Networks"><a href="#Meta-Path-Learning-for-Multi-relational-Graph-Neural-Networks" class="headerlink" title="Meta-Path Learning for Multi-relational Graph Neural Networks"></a>Meta-Path Learning for Multi-relational Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17113">http://arxiv.org/abs/2309.17113</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/francescoferrini/multirelationalgnn">https://github.com/francescoferrini/multirelationalgnn</a></li>
<li>paper_authors: Francesco Ferrini, Antonio Longa, Andrea Passerini, Manfred Jaeger</li>
<li>for: 该论文是为了解决多关系图神经网络中 Informative relations 的问题。</li>
<li>methods: 该论文提出了一种新的方法，通过在小量 informative meta-paths 上学习 GNNs 来解决这个问题。关键元素是一种用于评估关系的潜在有用性的分数函数。</li>
<li>results: 实验表明，该方法可以在大量关系的情况下正确地标识有用的 meta-paths，并substantially 超越现有的多关系 GNNs 在 sintetic 和实际 экспериментах中。<details>
<summary>Abstract</summary>
Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world experiments.
</details>
<details>
<summary>摘要</summary>
现有的多关系图 neural network 中的一种方法是将这个问题降低到低级的重量学习，或者是基于手动设计的关系依赖链，called meta-paths。然而，前者在多关系（例如知识图）存在时会遇到问题，而后者需要具有域专业知识来确定相关的 meta-paths。在这个工作中，我们提出了一种新的方法来学习 meta-paths 和 meta-path GNNs，这些方法可以在一小number of informative meta-paths 的基础上实现高度准确。关键元素我们的方法是一个用于评估关系的潜在有用性的分数函数，在逐步构建 meta-path 中。我们的实验评估表明，我们的方法可以在多关系中正确地标识 relevante meta-paths，并在synthetic 和实际世界上进行了显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Interpretability-for-Model-Comparison-via-Decision-Rules"><a href="#Dynamic-Interpretability-for-Model-Comparison-via-Decision-Rules" class="headerlink" title="Dynamic Interpretability for Model Comparison via Decision Rules"></a>Dynamic Interpretability for Model Comparison via Decision Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17095">http://arxiv.org/abs/2309.17095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Rida, Marie-Jeanne Lesot, Xavier Renard, Christophe Marsala</li>
<li>for: 本文旨在解释和explainable AI（XAI）方法如何准确地描述多个机器学习模型之间的差异。</li>
<li>methods: 本文提出了一种名为DeltaXplainer的模型无关方法，可以生成基于规则的解释，描述两个二分类器之间的差异。</li>
<li>results: 在synthetic和实际数据集上进行了多种模型比较场景的实验，证明DeltaXplainer可以有效地描述不同类型的概念融合导致的差异。<details>
<summary>Abstract</summary>
Explainable AI (XAI) methods have mostly been built to investigate and shed light on single machine learning models and are not designed to capture and explain differences between multiple models effectively. This paper addresses the challenge of understanding and explaining differences between machine learning models, which is crucial for model selection, monitoring and lifecycle management in real-world applications. We propose DeltaXplainer, a model-agnostic method for generating rule-based explanations describing the differences between two binary classifiers. To assess the effectiveness of DeltaXplainer, we conduct experiments on synthetic and real-world datasets, covering various model comparison scenarios involving different types of concept drift.
</details>
<details>
<summary>摘要</summary>
Explainable AI (XAI) 方法大多是为了研究和解释单个机器学习模型，而不是设计用于捕捉和解释多个模型之间的差异。这篇论文面临着理解和解释多个机器学习模型之间的差异的挑战，这是实际应用中的模型选择、监测和生命周期管理中非常重要的。我们提议了DeltaXplainer，一种模型无关的方法，用于生成对二分类模型之间差异的规则式解释。为了评估DeltaXplainer的有效性，我们在synthetic和实际世界数据集上进行了实验，覆盖了不同类型的概念漂移场景。
</details></li>
</ul>
<hr>
<h2 id="GAIA-1-A-Generative-World-Model-for-Autonomous-Driving"><a href="#GAIA-1-A-Generative-World-Model-for-Autonomous-Driving" class="headerlink" title="GAIA-1: A Generative World Model for Autonomous Driving"></a>GAIA-1: A Generative World Model for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17080">http://arxiv.org/abs/2309.17080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, Gianluca Corrado</li>
<li>for: 本研究旨在提高自动驾驶系统的安全性和可靠性，通过生成真实的驾驶场景和控制 eg 车行为。</li>
<li>methods: 我们引入了GAIA-1 (‘生成AI для自动驾驶’)，一种生成世界模型，通过视频、文本和动作输入生成真实的驾驶场景，并提供细化的 eg 车行为和场景特性控制。我们将世界模型视为无监督序列模型问题，将输入映射到精确的token，预测下一个token的序列。</li>
<li>results: GAIA-1可以学习高级结构和场景动力学，Contextual awareness，泛化和几何理解等性能，其学习的表示能够捕捉未来事件的预期，同时可以生成真实的样本，为自动驾驶技术的培训带来新的可能性和加速。<details>
<summary>Abstract</summary>
Autonomous driving promises transformative improvements to transportation, but building systems capable of safely navigating the unstructured complexity of real-world scenarios remains challenging. A critical problem lies in effectively predicting the various potential outcomes that may emerge in response to the vehicle's actions as the world evolves.   To address this challenge, we introduce GAIA-1 ('Generative AI for Autonomy'), a generative world model that leverages video, text, and action inputs to generate realistic driving scenarios while offering fine-grained control over ego-vehicle behavior and scene features. Our approach casts world modeling as an unsupervised sequence modeling problem by mapping the inputs to discrete tokens, and predicting the next token in the sequence. Emerging properties from our model include learning high-level structures and scene dynamics, contextual awareness, generalization, and understanding of geometry. The power of GAIA-1's learned representation that captures expectations of future events, combined with its ability to generate realistic samples, provides new possibilities for innovation in the field of autonomy, enabling enhanced and accelerated training of autonomous driving technology.
</details>
<details>
<summary>摘要</summary>
自动驾驶承诺改变交通领域，但建立能安全掌握实际enario中的系统仍然是挑战。一个关键问题在于有效地预测由车辆行为所导致的多种可能的结果，如世界如何发展。为解决这个挑战，我们介绍GAIA-1（生成AI для自动驾驶），一种生成世界模型，通过视频、文本和动作输入生成真实的驾驶enario，并提供细化控制 egovehicle行为和场景特性。我们的方法将世界模型作为无监督序列模型问题，将输入映射到精确的标识符，预测下一个标识符的序列。GAIA-1的模型出现了许多属性，包括学习高级结构和场景动力学、场景意识、泛化和几何理解。GAIA-1的学习表示，捕捉未来事件的期望，与生成真实样本的能力，为自动驾驶技术的培训带来新的可能性，提高和加速自动驾驶技术的发展。
</details></li>
</ul>
<hr>
<h2 id="Assessment-and-treatment-of-visuospatial-neglect-using-active-learning-with-Gaussian-processes-regression"><a href="#Assessment-and-treatment-of-visuospatial-neglect-using-active-learning-with-Gaussian-processes-regression" class="headerlink" title="Assessment and treatment of visuospatial neglect using active learning with Gaussian processes regression"></a>Assessment and treatment of visuospatial neglect using active learning with Gaussian processes regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13701">http://arxiv.org/abs/2310.13701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan De Boi, Elissa Embrechts, Quirine Schatteman, Rudi Penne, Steven Truijen, Wim Saeys</li>
<li>for: 评估和诊断视空间忽视症的人工智能解决方案</li>
<li>methods: 基于 Gaussian process regression 的活动学习方法，以减少患者进行评估所需的努力</li>
<li>results: 在实际 клиниче设定中进行临床试验，与现有的视空间忽视症测试相比，我们的 AI 评估模型显示更高的敏感度和可靠性，并且可以减少评估时间。<details>
<summary>Abstract</summary>
Visuospatial neglect is a disorder characterised by impaired awareness for visual stimuli located in regions of space and frames of reference. It is often associated with stroke. Patients can struggle with all aspects of daily living and community participation. Assessment methods are limited and show several shortcomings, considering they are mainly performed on paper and do not implement the complexity of daily life. Similarly, treatment options are sparse and often show only small improvements. We present an artificial intelligence solution designed to accurately assess a patient's visuospatial neglect in a three-dimensional setting. We implement an active learning method based on Gaussian process regression to reduce the effort it takes a patient to undergo an assessment. Furthermore, we describe how this model can be utilised in patient oriented treatment and how this opens the way to gamification, tele-rehabilitation and personalised healthcare, providing a promising avenue for improving patient engagement and rehabilitation outcomes. To validate our assessment module, we conducted clinical trials involving patients in a real-world setting. We compared the results obtained using our AI-based assessment with the widely used conventional visuospatial neglect tests currently employed in clinical practice. The validation process serves to establish the accuracy and reliability of our model, confirming its potential as a valuable tool for diagnosing and monitoring visuospatial neglect. Our VR application proves to be more sensitive, while intra-rater reliability remains high.
</details>
<details>
<summary>摘要</summary>
visuospatial neglect是一种病理特征于视觉刺激的减退，通常与中风相关。患者可能会受到日常生活和社区参与的困难。评估方法有限，主要是在纸上进行，无法体现生活中的复杂性。治疗方案罕见，效果有限。我们提出了基于人工智能的评估方法，能够在三维环境中准确评估患者的视空间忽视。我们采用了活动学习方法和 Gaussian process regression来减少患者参与评估的努力。此外，我们还描述了如何使用这种模型进行患者参与的治疗，包括临床娱乐、远程医疗和个性化医疗，这些方法可以提高患者参与度和康复成果。为验证我们的评估模块，我们在实际 Setting中进行了临床试验。我们将我们的人工智能基本评估与现有的广泛使用的视空间忽视测试进行比较，以验证我们的模型的准确性和可靠性。我们发现，我们的VR应用程序比 conventionally used tests更敏感，同时内部可靠性保持高。
</details></li>
</ul>
<hr>
<h2 id="SCALE-Synergized-Collaboration-of-Asymmetric-Language-Translation-Engines"><a href="#SCALE-Synergized-Collaboration-of-Asymmetric-Language-Translation-Engines" class="headerlink" title="SCALE: Synergized Collaboration of Asymmetric Language Translation Engines"></a>SCALE: Synergized Collaboration of Asymmetric Language Translation Engines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17061">http://arxiv.org/abs/2309.17061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hannibal046/scale">https://github.com/hannibal046/scale</a></li>
<li>paper_authors: Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, Rui Yan</li>
<li>For: The paper introduces a collaborative framework called SCALE that connects specialized translation models (STMs) and large language models (LLMs) to improve machine translation.* Methods: The paper introduces a method to incorporate translation from STMs into the triplet in-context demonstrations, which enables the LLM to refine and pivot its translations, mitigating language bias and parallel data bias.* Results: The paper shows that SCALE significantly outperforms both few-shot LLMs and specialized models in challenging low-resource settings, and experiences consistent improvement in Xhosa to English translation. Additionally, the paper shows that SCALE can effectively exploit the existing language bias of LLMs by using an English-centric STM as a pivot for translation between any language pairs.Here are the three points in Simplified Chinese text:* For: 本 paper 引入了一种合作框架，即 SCALE，用于连接特殊翻译模型（STM）和大型语言模型（LLM），以提高机器翻译。* Methods: 本 paper 引入了一种方法，即将翻译从 STM  integrate 到 triplet 上下文示例中，使 LL 能够对翻译进行精度和推倒，从而减少语言偏见和并行数据偏见。* Results: 本 paper 显示，SCALE 在低资源设置下significantly 超越了几个简单的 LLM 和专门的模型（NLLB），并在 Xhosa 到英语翻译中经常提高。此外，SCALE 还可以有效利用现有的 LLM 语言偏见，通过使用英语中心 STM 作为 pivot 进行任何语言对之间的翻译，超越了几个简单的 GPT-4。<details>
<summary>Abstract</summary>
In this paper, we introduce SCALE, a collaborative framework that connects compact Specialized Translation Models (STMs) and general-purpose Large Language Models (LLMs) as one unified translation engine. By introducing translation from STM into the triplet in-context demonstrations, SCALE unlocks refinement and pivoting ability of LLM, thus mitigating language bias of LLM and parallel data bias of STM, enhancing LLM speciality without sacrificing generality, and facilitating continual learning without expensive LLM fine-tuning. Our comprehensive experiments show that SCALE significantly outperforms both few-shot LLMs (GPT-4) and specialized models (NLLB) in challenging low-resource settings. Moreover, in Xhosa to English translation, SCALE experiences consistent improvement by a 4 BLEURT score without tuning LLM and surpasses few-shot GPT-4 by 2.5 COMET score and 3.8 BLEURT score when equipped with a compact model consisting of merely 600M parameters. SCALE could also effectively exploit the existing language bias of LLMs by using an English-centric STM as a pivot for translation between any language pairs, outperforming few-shot GPT-4 by an average of 6 COMET points across eight translation directions. Furthermore we provide an in-depth analysis of SCALE's robustness, translation characteristics, and latency costs, providing solid foundation for future studies exploring the potential synergy between LLMs and more specialized, task-specific models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们引入了一个协作框架，称为SCALE，它将特殊化翻译模型（STM）和通用大语言模型（LLM） integrate为一个统一的翻译引擎。通过在STM中引入翻译，SCALE可以 Mitigate语言偏好和并行数据偏好，提高LLM的特殊化无需牺牲通用性，并且可以实现 kontinuel Learning without expensive LLM fine-tuning。我们的全面实验表明，SCALE在具有低资源的情况下显著超越了几个shot LLM（GPT-4）和专门的模型（NLLB）。此外，在从Xhosa到英语翻译中，SCALE经常经历了不需要调整LLM的情况下的改进，比特shot GPT-4提高了2.5个COMET分数和3.8个BLEURT分数，当装备了仅600M参数的简洁模型时。SCALE还可以有效利用现有的LLM语言偏好，通过使用英语特殊的STM作为翻译 между任何语言对的托管，超越了几个shot GPT-4的平均6个COMET分数。此外，我们还提供了SCALE的稳定性、翻译特征和延迟成本的深入分析，为未来研究探索LLM和更特殊、任务特定的模型之间的可能的合作提供坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Tell-Me-a-Story-Narrative-Driven-XAI-with-Large-Language-Models"><a href="#Tell-Me-a-Story-Narrative-Driven-XAI-with-Large-Language-Models" class="headerlink" title="Tell Me a Story! Narrative-Driven XAI with Large Language Models"></a>Tell Me a Story! Narrative-Driven XAI with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17057">http://arxiv.org/abs/2309.17057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/admantwerp/xaistories">https://github.com/admantwerp/xaistories</a></li>
<li>paper_authors: David Martens, Camille Dams, James Hinns, Mark Vergouwen</li>
<li>for: 该论文的目的是解释人工智能预测结果，提供人类可理解的解释方式。</li>
<li>methods: 该论文使用了SHAP值和counterfactual（CF）解释方法，并将这些解释方法转换成人类可理解的故事形式，以便更好地解释AI预测结果。</li>
<li>results: 调查结果表明，大多数普通用户认为SHAPstories是有力的解释，而数据科学家认为SHAPstories可以帮助普通用户更好地理解AI预测结果。CFstories也在图像分类任务中被证明为更加有力的解释方法，并且可以提高准确率。<details>
<summary>Abstract</summary>
In today's critical domains, the predominance of black-box machine learning models amplifies the demand for Explainable AI (XAI). The widely used SHAP values, while quantifying feature importance, are often too intricate and lack human-friendly explanations. Furthermore, counterfactual (CF) explanations present `what ifs' but leave users grappling with the 'why'. To bridge this gap, we introduce XAIstories. Leveraging Large Language Models, XAIstories provide narratives that shed light on AI predictions: SHAPstories do so based on SHAP explanations to explain a prediction score, while CFstories do so for CF explanations to explain a decision. Our results are striking: over 90% of the surveyed general audience finds the narrative generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 92% of data scientists indicating that it will contribute to the ease and confidence of nonspecialists in understanding AI predictions. Additionally, 83% of data scientists indicate they are likely to use SHAPstories for this purpose. In image classification, CFstories are considered more or equally convincing as users own crafted stories by over 75% of lay user participants. CFstories also bring a tenfold speed gain in creating a narrative, and improves accuracy by over 20% compared to manually created narratives. The results thereby suggest that XAIstories may provide the missing link in truly explaining and understanding AI predictions.
</details>
<details>
<summary>摘要</summary>
今天的关键领域中，黑盒机器学习模型的占主导地位进一步增加了Explainable AI（XAI）的需求。通用的SHAP值可以衡量特征重要性，但它们经常是太复杂，缺乏人类友好的解释。另外，Counterfactual（CF）解释可以提供“what if”的情况，但它们留下用户摸着头脑，思考“why”。为了bridging这个差距，我们引入XAIstories。利用大型自然语言模型，XAIstories提供了解释AI预测的narritives。SHAPstories基于SHAP解释来解释预测分数，而CFstories基于CF解释来解释决策。我们的结果很有吸引力：论坛中的普通民众超过90%认为SHAPstories的解释感服务。数据科学家主要看到SHAPstories在传达解释给通用观众时的价值，92%的数据科学家认为它会使非专家更好地理解AI预测。此外，83%的数据科学家表示他们可能会使用SHAPstories。在图像分类任务中，CFstories被用户自己创作的故事超过75%的普通用户认为是 equally convincing或更有吸引力。CFstories还提供了图像分类任务中创建故事的十倍快速和超过20%的准确率提升。结果表明XAIstories可能为AI预测的真正解释和理解提供了一个缺失的链接。
</details></li>
</ul>
<hr>
<h2 id="On-Continuity-of-Robust-and-Accurate-Classifiers"><a href="#On-Continuity-of-Robust-and-Accurate-Classifiers" class="headerlink" title="On Continuity of Robust and Accurate Classifiers"></a>On Continuity of Robust and Accurate Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17048">http://arxiv.org/abs/2309.17048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramin Barati, Reza Safabakhsh, Mohammad Rahmati</li>
<li>for: 本研究旨在探讨机器学习模型的可靠性问题，以及如何提高模型的Robustness和准确率。</li>
<li>methods: 本研究使用了对各种机器学习任务的实验研究，以及对学习过程中函数的分析和推理。</li>
<li>results: 研究发现，继续函数不能有效地学习最佳Robust hypothesis，而不连续函数则可以更好地实现Robustness和准确率的平衡。此外，研究还提供了一种框架 для严谨地研究幂等和幂函数在学习理论中的应用。<details>
<summary>Abstract</summary>
The reliability of a learning model is key to the successful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. To this end, we will introduce a framework for a rigorous study of harmonic and holomorphic hypothesis in learning theory terms and provide empirical evidence that continuous hypotheses does not perform as well as discontinuous hypotheses in some common machine learning tasks. From a practical point of view, our results suggests that a robust and accurate learning rule would train different continuous hypotheses for different regions of the domain. From a theoretical perspective, our analysis explains the adversarial examples phenomenon as a conflict between the continuity of a sequence of functions and its uniform convergence to a discontinuous function.
</details>
<details>
<summary>摘要</summary>
“机器学习模型的可靠性是在实际应用中成功的关键。建立一个坚固的模型，特别是不受攻击影响的模型，需要对抗例子现象的全面理解。但是由于机器学习的问题相当复杂，这个现象难以描述。已经证明了对抗训练可以提高模型的坚固性，但是这些改进价值随即降低自然样本的表现。因此，有人建议了模型的稳定性和准确性之间存在冲突。在这篇论文中，我们提出了一个不同的建议，即稳定性和准确性之间的冲突是由于函数的连续性所致。即使是一个连续函数，也无法有效地学习最佳的防御性模型。为了解决这个问题，我们将引入一个对抗学习理论的框架，并提供实验证据，证明连续函数不如离散函数在一些常见的机器学习任务中表现更好。实际上，我们的结果显示了一个坚固和准确的学习规则应该在不同的领域中训练不同的连续函数。从理论上看，我们的分析解释了攻击例子现象是一个连续函数序列的稳定性与其均匀对抗数列的冲突之间的冲突。”
</details></li>
</ul>
<hr>
<h2 id="Refined-Kolmogorov-Complexity-of-Analog-Evolving-and-Stochastic-Recurrent-Neural-Networks"><a href="#Refined-Kolmogorov-Complexity-of-Analog-Evolving-and-Stochastic-Recurrent-Neural-Networks" class="headerlink" title="Refined Kolmogorov Complexity of Analog, Evolving and Stochastic Recurrent Neural Networks"></a>Refined Kolmogorov Complexity of Analog, Evolving and Stochastic Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17032">http://arxiv.org/abs/2309.17032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémie Cabessa, Yann Strozecki</li>
<li>for: 这个论文旨在填充当时的缺失结果，提供一个统一的视角来描述超过图灵计算机的能力。</li>
<li>methods: 该论文使用了kolmogorov复杂度来定义和研究 Analog、演化和随机神经网络的超过图灵计算机能力。</li>
<li>results: 研究得到了一个无穷 hierarchy of classes of analog networks, evolving networks, and stochastic networks，这些类别位于 $\mathbf{P}$ 和 $\mathbf{P&#x2F;poly}$ 之间。此外，研究还提供了一种生成这些类别的通用方法。<details>
<summary>Abstract</summary>
We provide a refined characterization of the super-Turing computational power of analog, evolving, and stochastic neural networks based on the Kolmogorov complexity of their real weights, evolving weights, and real probabilities, respectively. First, we retrieve an infinite hierarchy of classes of analog networks defined in terms of the Kolmogorov complexity of their underlying real weights. This hierarchy is located between the complexity classes $\mathbf{P}$ and $\mathbf{P/poly}$. Then, we generalize this result to the case of evolving networks. A similar hierarchy of Kolomogorov-based complexity classes of evolving networks is obtained. This hierarchy also lies between $\mathbf{P}$ and $\mathbf{P/poly}$. Finally, we extend these results to the case of stochastic networks employing real probabilities as source of randomness. An infinite hierarchy of stochastic networks based on the Kolmogorov complexity of their probabilities is therefore achieved. In this case, the hierarchy bridges the gap between $\mathbf{BPP}$ and $\mathbf{BPP/log^*}$. Beyond proving the existence and providing examples of such hierarchies, we describe a generic way of constructing them based on classes of functions of increasing complexity. For the sake of clarity, this study is formulated within the framework of echo state networks. Overall, this paper intends to fill the missing results and provide a unified view about the refined capabilities of analog, evolving and stochastic neural networks.
</details>
<details>
<summary>摘要</summary>
我们提供了一种精细的定义和分类方法 для超过图灵计算机力的Analog、演化和随机神经网络，基于这些网络的实数权重、演化权重和实数概率的科隆莫洛夫复杂性。首先，我们定义了一系列基于实数权重的Analog网络的层次结构，这些层次结构位于$\mathbf{P}$和$\mathbf{P/poly}$之间。然后，我们推广这些结果到演化网络的情况，并获得了类似的层次结构。此外，我们还将这些结果推广到使用实数概率作为随机性源的随机网络，并建立了一个基于概率复杂性的层次结构，这个层次结构跨越了$\mathbf{BPP}$和$\mathbf{BPP/log^*}$之间。除了证明存在和提供示例外，我们还描述了一种通用的构造方法，基于不同复杂性的函数类型。为了便于理解，这种研究基于抽象神经网络（Echo State Networks）的框架下进行了表述。总的来说，本文的目标是填充当前缺失的结果，并提供一个统一的视角，以描述超过图灵计算机力的Analog、演化和随机神经网络的高级功能。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Multi-Temporal-Remote-Sensing-Change-Data-Generation-via-Simulating-Stochastic-Change-Process"><a href="#Scalable-Multi-Temporal-Remote-Sensing-Change-Data-Generation-via-Simulating-Stochastic-Change-Process" class="headerlink" title="Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process"></a>Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17031">http://arxiv.org/abs/2309.17031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Z-Zheng/Changen">https://github.com/Z-Zheng/Changen</a></li>
<li>paper_authors: Zhuo Zheng, Shiqi Tian, Ailong Ma, Liangpei Zhang, Yanfei Zhong</li>
<li>For: This paper presents a scalable multi-temporal remote sensing change data generator using generative modeling, which can alleviate the problems of collecting, preprocessing, and annotating multi-temporal remote sensing images at scale.* Methods: The proposed method, called Changen, is based on a generative adversarial network (GAN) and decouples the complex simulation problem into two more tractable sub-problems: change event simulation and semantic change synthesis.* Results: The extensive experiments show that Changen has superior generation capability and the change detectors with Changen pre-training exhibit excellent transferability to real-world change datasets.Here is the same information in Simplified Chinese text:* For: 这篇论文提出了一种可扩展的多时间段Remote sensing变化数据生成器，使用生成模型，解决了多时间段Remote sensing图像收集、处理和标注的问题。* Methods: 提议的方法基于生成对抗网络（GAN），将复杂的模拟问题分解成两个更加可控的互相独立的问题：变化事件模拟和semantic变化合成。* Results: 广泛的实验表明，Changen具有优秀的生成能力，并且将变化探测器与Changen预训练结合，在实际变化数据集上表现出色的传送性。<details>
<summary>Abstract</summary>
Understanding the temporal dynamics of Earth's surface is a mission of multi-temporal remote sensing image analysis, significantly promoted by deep vision models with its fuel -- labeled multi-temporal images. However, collecting, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present a scalable multi-temporal remote sensing change data generator via generative modeling, which is cheap and automatic, alleviating these problems. Our main idea is to simulate a stochastic change process over time. We consider the stochastic change process as a probabilistic semantic state transition, namely generative probabilistic change model (GPCM), which decouples the complex simulation problem into two more trackable sub-problems, \ie, change event simulation and semantic change synthesis. To solve these two problems, we present the change generator (Changen), a GAN-based GPCM, enabling controllable object change data generation, including customizable object property, and change event. The extensive experiments suggest that our Changen has superior generation capability, and the change detectors with Changen pre-training exhibit excellent transferability to real-world change datasets.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:理解地球表面的时间动态是远程感知图像分析的任务，受到深度视觉模型的推动，却面临收集、预处理和标注多个时间点的远程感知图像的问题。在这篇文章中，我们提出了一种可扩展的多时点远程感知变化数据生成器，通过生成模型，它是便宜和自动的，从而解决了这些问题。我们的主要想法是通过随机变化过程来模拟时间的演变。我们认为这种随机变化过程是一种概率性 semantic state transition，即生成概率性变化模型（GPCM），它将复杂的模拟问题分解成两个更容易解决的子问题，即变化事件模拟和semantic变化合成。为解决这两个问题，我们提出了改变生成器（Changen），基于GAN的GPCM，可以生成可控的对象变化数据，包括可定制的对象属性和变化事件。我们的实验表明，我们的Changen具有优秀的生成能力，而使用Changen预训练的变化探测器在实际变化数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Sarcasm-in-Sight-and-Sound-Benchmarking-and-Expansion-to-Improve-Multimodal-Sarcasm-Detection"><a href="#Sarcasm-in-Sight-and-Sound-Benchmarking-and-Expansion-to-Improve-Multimodal-Sarcasm-Detection" class="headerlink" title="Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection"></a>Sarcasm in Sight and Sound: Benchmarking and Expansion to Improve Multimodal Sarcasm Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01430">http://arxiv.org/abs/2310.01430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Bhosale, Abhra Chaudhuri, Alex Lee Robert Williams, Divyank Tiwari, Anjan Dutta, Xiatian Zhu, Pushpak Bhattacharyya, Diptesh Kanojia</li>
<li>for: 这 paper 的目的是对 MUStARD++  dataset 进行严格的benchmarking，以便完全利用这个多modal的丰富性，并提高 macro-F1 分数2%。</li>
<li>methods: 这 paper 使用了当今最佳语言、语音和视觉编码器，以实现全面的多modal表达。此外，它还提出了一种扩展，称为 MUStARD++ Balanced，将分类结果分配到两个类别中，以解决 sarcastic type 类别的偏袋问题，并提高了 macro-F1 分数2.4%。</li>
<li>results: 这 paper 的结果显示，使用新的 TV 剧 House MD 的 clip，并手动将其分类为多种类别，可以提高 MUStARD++  dataset 的多modal表达能力，并且可以提高 macro-F1 分数。<details>
<summary>Abstract</summary>
The introduction of the MUStARD dataset, and its emotion recognition extension MUStARD++, have identified sarcasm to be a multi-modal phenomenon -- expressed not only in natural language text, but also through manners of speech (like tonality and intonation) and visual cues (facial expression). With this work, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by considering state-of-the-art language, speech, and visual encoders, for fully utilizing the totality of the multi-modal richness that it has to offer, achieving a 2\% improvement in macro-F1 over the existing benchmark. Additionally, to cure the imbalance in the `sarcasm type' category in MUStARD++, we propose an extension, which we call \emph{MUStARD++ Balanced}, benchmarking the same with instances from the extension split across both train and test sets, achieving a further 2.4\% macro-F1 boost. The new clips were taken from a novel source -- the TV show, House MD, which adds to the diversity of the dataset, and were manually annotated by multiple annotators with substantial inter-annotator agreement in terms of Cohen's kappa and Krippendorf's alpha. Our code, extended data, and SOTA benchmark models are made public.
</details>
<details>
<summary>摘要</summary>
Introduction of MUStARD dataset and its extension MUStARD++, researchers have found that sarcasm is a multi-modal phenomenon, expressed not only in natural language text, but also through speech and visual cues. In this work, we aim to perform a comprehensive benchmarking of MUStARD++ dataset by considering state-of-the-art language, speech, and visual encoders, to fully utilize the richness of the multi-modal data, and achieve a 2% improvement in macro-F1 over the existing benchmark. Additionally, to address the imbalance in the "sarcasm type" category in MUStARD++, we propose an extension, called MUStARD++ Balanced, and benchmark it with instances from the extension split across both train and test sets, achieving a further 2.4% macro-F1 boost. The new clips were taken from a novel source, the TV show House MD, which adds to the diversity of the dataset, and were manually annotated by multiple annotators with substantial inter-annotator agreement in terms of Cohen's kappa and Krippendorf's alpha. Our code, extended data, and SOTA benchmark models are publicly available.Here's the text in Traditional Chinese as well:Introduction of MUStARD dataset and its extension MUStARD++, researchers have found that sarcasm is a multi-modal phenomenon, expressed not only in natural language text, but also through speech and visual cues. In this work, we aim to perform a comprehensive benchmarking of MUStARD++ dataset by considering state-of-the-art language, speech, and visual encoders, to fully utilize the richness of the multi-modal data, and achieve a 2% improvement in macro-F1 over the existing benchmark. Additionally, to address the imbalance in the "sarcasm type" category in MUStARD++, we propose an extension, called MUStARD++ Balanced, and benchmark it with instances from the extension split across both train and test sets, achieving a further 2.4% macro-F1 boost. The new clips were taken from a novel source, the TV show House MD, which adds to the diversity of the dataset, and were manually annotated by multiple annotators with substantial inter-annotator agreement in terms of Cohen's kappa and Krippendorf's alpha. Our code, extended data, and SOTA benchmark models are publicly available.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Cognitive-Biases-in-Large-Language-Models-as-Evaluators"><a href="#Benchmarking-Cognitive-Biases-in-Large-Language-Models-as-Evaluators" class="headerlink" title="Benchmarking Cognitive Biases in Large Language Models as Evaluators"></a>Benchmarking Cognitive Biases in Large Language Models as Evaluators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17012">http://arxiv.org/abs/2309.17012</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minnesotanlp/cobbler">https://github.com/minnesotanlp/cobbler</a></li>
<li>paper_authors: Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang</li>
<li>for: 本研究目的是评估大自然语言模型（LLMs）作为自动评估器的可靠性，并研究其评估输出的质量。</li>
<li>methods: 本研究使用15个不同大小范围的LLMs，对其输出回快排名，并使用COBBLEr标准测试六种认知偏见。</li>
<li>results: 研究发现，LLMs作为评估器存在强烈的认知偏见，在每个评估中表现出60%的比较。此外，人机和机器偏好之间的相关性为49.6%，表明机器偏好与人类偏好存在差异。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）最近已经被证明可以作为自动评价器，只需要简单的提示和在Context中学习。在这项工作中，我们卷集了15个不同大小范围的LLM，并对它们的输出响应进行了对比评价，例如系统星是比系统方块更好。然后，我们引入了语言模型评价生成器的认知偏见标准（CoBBLEr），以衡量六种不同的认知偏见在LLM评价输出中，如自центризм偏见，这种偏见会让模型对自己的输出进行高评价。我们发现，LLM作为评价器存在强烈的偏见，在每个评价中都有40%的比较，表明它们的可靠性是问题。此外，我们还研究了人类和机器之间的偏好相关性，并计算了机器和人类之间的相互融合率（RBO），得到了49.6%的平均值，这表明机器的偏好与人类的偏好存在差异。根据我们的发现，LLM可能还未能够被用于自动标注，与人类的偏好相对适合。我们的项目页面是：https://minnesotanlp.github.io/cobbler。
</details></li>
</ul>
<hr>
<h2 id="Medical-Foundation-Models-are-Susceptible-to-Targeted-Misinformation-Attacks"><a href="#Medical-Foundation-Models-are-Susceptible-to-Targeted-Misinformation-Attacks" class="headerlink" title="Medical Foundation Models are Susceptible to Targeted Misinformation Attacks"></a>Medical Foundation Models are Susceptible to Targeted Misinformation Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17007">http://arxiv.org/abs/2309.17007</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peterhan91/fm_adv">https://github.com/peterhan91/fm_adv</a></li>
<li>paper_authors: Tianyu Han, Sven Nebelung, Firas Khader, Tianci Wang, Gustav Mueller-Franzes, Christiane Kuhl, Sebastian Försch, Jens Kleesiek, Christoph Haarburger, Keno K. Bressem, Jakob Nikolas Kather, Daniel Truhn</li>
<li>for: 这个研究目的是要检查大型自然语言模型（LLMs）在医疗领域中的可靠性和安全性。</li>
<li>methods: 研究人员使用了targeted manipulation的方法来对模型的1.1%的权重进行修改，以导入错误的生物医学信息。</li>
<li>results: 研究发现，通过修改模型的1.1%的权重，可以故意导入错误的生物医学信息，并且这些错误信息将被模型输出，而模型的其他生物医学任务性能则保持不变。这些结果表明了LLMs在医疗领域的可靠性和安全性存在问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a concerning vulnerability of LLMs in medicine. Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. We validate our findings in a set of 1,038 incorrect biomedical facts. This peculiar susceptibility raises serious security and trustworthiness concerns for the application of LLMs in healthcare settings. It accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Understanding-and-Mitigating-the-Label-Noise-in-Pre-training-on-Downstream-Tasks"><a href="#Understanding-and-Mitigating-the-Label-Noise-in-Pre-training-on-Downstream-Tasks" class="headerlink" title="Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks"></a>Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17002">http://arxiv.org/abs/2309.17002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Jindong Wang, Ankit Shah, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj</li>
<li>for: 本研究旨在理解预训练数据中的噪声对下游任务的影响，并提出一种轻量级黑盒调参方法来缓解噪声的害性影响。</li>
<li>methods: 通过对 synthetic noisy ImageNet-1K 和 YFCC15M 数据集进行广泛的超参数调整实验，我们证明了适度的噪声在预训练中可以提高域内（ID）转移性能，但总是降低 OUT OF 预训练（OOD）性能。我们也证明了噪声在预训练中 shapes 特征空间的不同导致这一现象。</li>
<li>results: 我们对流行的视觉和语言模型进行评估，发现我们的方法可以有效地缓解预训练中的噪声影响，提高 Both ID 和 OOD 任务的泛化性能。<details>
<summary>Abstract</summary>
Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning.
</details>
<details>
<summary>摘要</summary>
传统的深度学习实践中，先行训练大规模数据集，然后在下游任务上精细调整模型已成为标准做法。然而，预训练数据通常包含标签噪音，这可能会对模型的泛化产生负面影响。这篇论文的目标是理解预训练数据中噪音的性质，并对其对下游任务的影响进行调节。我们通过对synthetic noisy ImageNet-1K和YFCC15M数据集进行超过实验，发现一定程度的噪音在预训练中可以提高预训练和测试数据分布相同的内联（ID）传递性能，但总是对于预训练和测试数据分布不同的外联（OOD）性能下降。我们经验 verify 噪音在预训练中形成特征空间的不同是原因。我们然后提出了一种轻量级黑obox调整方法（NMTune），用于调整特征空间，以减少噪音对泛化的负面影响，并提高ID和OOD任务的泛化性能。我们在流行的视觉和语言模型中进行了实践性的实验，以评估我们的方法。我们的分析和结果表明这是一个有趣和新的研究方向，我们称之为噪音模型学习（Noisy Model Learning）。
</details></li>
</ul>
<hr>
<h2 id="A-Closer-Look-at-Bearing-Fault-Classification-Approaches"><a href="#A-Closer-Look-at-Bearing-Fault-Classification-Approaches" class="headerlink" title="A Closer Look at Bearing Fault Classification Approaches"></a>A Closer Look at Bearing Fault Classification Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17001">http://arxiv.org/abs/2309.17001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harika Abburi, Tanya Chaudhary, Haider Ilyas, Lakshmi Manne, Deepak Mittal, Don Williams, Derek Snaidauf, Edward Bowen, Balaji Veeramani</li>
<li>for: 本研究旨在提高滚珠机器人失效诊断的效率和准确性，以避免意外机器停工和提高维护计划。</li>
<li>methods: 本研究使用了现代机器学习技术，包括深度学习架构，对滚珠数据进行分析和预测。</li>
<li>results: 研究发现，在诊断滚珠失效时，数据分区、模型评价指标和失败标签生成方法的选择具有重要的影响，并提出了实际场景中模型开发考虑因素。<details>
<summary>Abstract</summary>
Rolling bearing fault diagnosis has garnered increased attention in recent years owing to its presence in rotating machinery across various industries, and an ever increasing demand for efficient operations. Prompt detection and accurate prediction of bearing failures can help reduce the likelihood of unexpected machine downtime and enhance maintenance schedules, averting lost productivity. Recent technological advances have enabled monitoring the health of these assets at scale using a variety of sensors, and predicting the failures using modern Machine Learning (ML) approaches including deep learning architectures. Vibration data has been collected using accelerated run-to-failure of overloaded bearings, or by introducing known failure in bearings, under a variety of operating conditions such as rotating speed, load on the bearing, type of bearing fault, and data acquisition frequency. However, in the development of bearing failure classification models using vibration data there is a lack of consensus in the metrics used to evaluate the models, data partitions used to evaluate models, and methods used to generate failure labels in run-to-failure experiments. An understanding of the impact of these choices is important to reliably develop models, and deploy them in practical settings. In this work, we demonstrate the significance of these choices on the performance of the models using publicly-available vibration datasets, and suggest model development considerations for real world scenarios. Our experimental findings demonstrate that assigning vibration data from a given bearing across training and evaluation splits leads to over-optimistic performance estimates, PCA-based approach is able to robustly generate labels for failure classification in run-to-failure experiments, and $F$ scores are more insightful to evaluate the models with unbalanced real-world failure data.
</details>
<details>
<summary>摘要</summary>
However, there is a lack of consensus in the metrics used to evaluate the models, the data partitions used to evaluate the models, and the methods used to generate failure labels in run-to-failure experiments. These choices have a significant impact on the performance of the models, and it is essential to understand their influence to develop reliable models that can be deployed in practical settings.In this work, we investigate the impact of these choices on the performance of bearing failure classification models using publicly-available vibration datasets. We demonstrate that assigning vibration data from a given bearing across training and evaluation splits leads to over-optimistic performance estimates, and that a PCA-based approach can robustly generate labels for failure classification in run-to-failure experiments. Additionally, we find that $F$ scores are more insightful to evaluate the models with unbalanced real-world failure data. Our findings provide practical considerations for developing and deploying bearing failure classification models in real-world scenarios.
</details></li>
</ul>
<hr>
<h2 id="AI-Algorithm-for-the-Generation-of-Three-Dimensional-Accessibility-Ramps-in-Grasshopper-Rhinoceros-7"><a href="#AI-Algorithm-for-the-Generation-of-Three-Dimensional-Accessibility-Ramps-in-Grasshopper-Rhinoceros-7" class="headerlink" title="AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps in Grasshopper &#x2F; Rhinoceros 7"></a>AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps in Grasshopper &#x2F; Rhinoceros 7</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07728">http://arxiv.org/abs/2310.07728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Li, Leila Yi, Brandon Yeo Pei Hui</li>
<li>for:  This paper aims to provide an algorithm for the automatic generation of wheelchair-accessible ramps in urban development, with the goal of improving accessibility for people with mobile impairments and able-bodied third parties.</li>
<li>methods:  The algorithm uses AI search algorithms to determine the optimal pathway connecting initial and terminal points within a 3D model of the environment, taking into account essential components such as elevation differentials, spatial constraints, and gradient specifications.</li>
<li>results:  The algorithm generates a full-scale, usable model of a ramp that can be easily exported and transformed through inter-software exchanges, providing significant efficiency gains in the design process and lowering the threshold for the incorporation of accessibility features in future urban design.<details>
<summary>Abstract</summary>
Often overlooked as a component of urban development, accessibility infrastructure is undeniably crucial in daily life. Accessibility ramps are one of the most common types of accessibility infrastructure, and serve to benefit not only people with mobile impairments but also able-bodied third parties. While the necessity of accessibility ramps is acknowledged, actual implementation fails in light of the limits of manpower required for the design stage. In response, we present an algorithm capable of the automatic generation of a feasible accessibility ramp based on a 3D model of the relevant environment. Through the manual specification of initial and terminal points within a 3D model, the algorithm uses AI search algorithms to determine the optimal pathway connecting these points. Essential components in devising a wheelchair-accessible ramp are encoded within the process, as evaluated by the algorithm, including but not limited to elevation differentials, spatial constraints, and gradient specifications. From this, the algorithm then generates the pathway to be expanded into a full-scale, usable model of a ramp, which then can be easily exported and transformed through inter-software exchanges. Though some human input is still required following the generation stage, the minimising of human resources provides significant boosts of efficiency in the design process thus lowering the threshold for the incorporation of accessibility features in future urban design.
</details>
<details>
<summary>摘要</summary>
通常被忽略的城市发展组件之一是可达性基础设施，它在日常生活中的重要性不言自明。可达性升降是可达性基础设施中最常见的一种，不仅有助于身体残疾人，还有利于能 bodied 第三方。虽然人们承认可达性升降的必要性，但实际实施失败，一个重要原因是人力设计阶段的限制。为此，我们提出了一个可以自动生成可达性升降的算法，基于相关环境的 3D 模型。通过手动指定 initia 和终点在 3D 模型中的点，算法使用人工智能搜索算法来确定最佳的连接这两点的路径。编码在设计轮廓中的 essenti 组件，包括但不限于高差差、空间限制和斜 Slope 规范，都会被算法评估。从这里，算法会生成一个可以扩展到全Scale 使用的升降模型，可以轻松地通过交互软件交换出口。虽然还需要一定的人工输入 после 生成阶段，但是减少了人力资源的需求，提高了设计过程中的效率，从而降低了未来城市设计中可达性特性的投入难度。
</details></li>
</ul>
<hr>
<h2 id="Reliability-Quantification-of-Deep-Reinforcement-Learning-based-Control"><a href="#Reliability-Quantification-of-Deep-Reinforcement-Learning-based-Control" class="headerlink" title="Reliability Quantification of Deep Reinforcement Learning-based Control"></a>Reliability Quantification of Deep Reinforcement Learning-based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16977">http://arxiv.org/abs/2309.16977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hitoshi Yoshioka, Hirotada Hashimoto</li>
<li>for: 这个研究的目的是为了量化深度强化学习（DRL）控制的可靠性，以便在安全重要系统中应用人工智能（AI）。</li>
<li>methods: 这个研究提出了一种方法来量化DRL控制的可靠性。首先，使用了现有的方法——随机噪音浓缩，以解决问题。其次，提出了一种新的方法来量化可靠性。这个方法使用两个神经网络：引用和评估。它们有相同的结构和初始参数。在训练时，评估网络的参数被更新，以将训练数据中的差异最大化。因此，这个方法可以根据训练数据中的差异，评估DRL控制的可靠性。</li>
<li>results: 这个研究运用了DQN-based控制来解决一个简单任务，并证明了其效果。此外，这个方法还应用于问题 switching 训练模型根据状态。因此，这个方法可以提高DRL控制的性能，通过根据可靠性 switching 训练模型。<details>
<summary>Abstract</summary>
Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The proposed method was applied to DQN-based control as an example of a simple task, and its effectiveness was demonstrated. Finally, the proposed method was applied to the problem of switching trained models depending on the state. Con-sequently, the performance of the DRL-based control was improved by switching the trained models according to their reliability.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）控制的可靠性评估是应用人工智能（AI）在安全关键系统中实践的一大挑战。本研究提出了一种方法来评估DRL控制的可靠性。首先，我们使用了现有的随机噪声润恤方法来进行可靠性评估，以便更清晰地描述问题。然后，我们提出了一种新的可靠性评估方法，用以解决这些问题。在这种方法中，我们使用了两个神经网络：参照网络和评估网络。它们具有相同的结构和相同的初始参数。在训练前，参照网络和评估网络的输出都是相同的。在训练过程中，评估网络的参数被更新，以便在训练数据上增加参照网络和评估网络之间的差异。因此，我们可以根据参照网络和评估网络之间的差异来评估DRL控制的可靠性。本研究使用了DQN控制为简单任务的示例，并证明了其效果。最后，我们将该方法应用于状态 switching 已训练模型的问题，并通过根据模型的可靠性进行模型 switching 来提高DRL控制的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Quantum-States-Preparation-Method-Based-on-Difference-Driven-Reinforcement-Learning"><a href="#A-Quantum-States-Preparation-Method-Based-on-Difference-Driven-Reinforcement-Learning" class="headerlink" title="A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning"></a>A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16972">http://arxiv.org/abs/2309.16972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Liu, Jing Xu, Bosi Wang</li>
<li>for: 提高二叠bits系统内部状态准备的速度和精度。</li>
<li>methods: 提出了一种基于差分驱动学习算法的改进奖励函数和动作选择策略，以帮助算法快速获得最大预期总奖励。</li>
<li>results: 实验结果表明，提出的算法可以在有限条件下准备高精度的二叠bits系统内部状态。与其他算法相比，它在速度和精度两个方面具有不同的改进。<details>
<summary>Abstract</summary>
Due to the large state space of the two-qubit system, and the adoption of ladder reward function in the existing quantum state preparation methods, the convergence speed is slow and it is difficult to prepare the desired target quantum state with high fidelity under limited conditions. To solve the above problems, a difference-driven reinforcement learning (RL) algorithm for quantum state preparation of two-qubit system is proposed by improving the reward function and action selection strategy. Firstly, a model is constructed for the problem of preparing quantum states of a two-qubit system, with restrictions on the type of quantum gates and the time for quantum state evolution. In the preparation process, a weighted differential dynamic reward function is designed to assist the algorithm quickly obtain the maximum expected cumulative reward. Then, an adaptive e-greedy action selection strategy is adopted to achieve a balance between exploration and utilization to a certain extent, thereby improving the fidelity of the final quantum state. The simulation results show that the proposed algorithm can prepare quantum state with high fidelity under limited conditions. Compared with other algorithms, it has different degrees of improvement in convergence speed and fidelity of the final quantum state.
</details>
<details>
<summary>摘要</summary>
Due to the large state space of the two-qubit system, and the adoption of ladder reward function in the existing quantum state preparation methods, the convergence speed is slow and it is difficult to prepare the desired target quantum state with high fidelity under limited conditions. To solve the above problems, a difference-driven reinforcement learning (RL) algorithm for quantum state preparation of two-qubit system is proposed by improving the reward function and action selection strategy. Firstly, a model is constructed for the problem of preparing quantum states of a two-qubit system, with restrictions on the type of quantum gates and the time for quantum state evolution. In the preparation process, a weighted differential dynamic reward function is designed to assist the algorithm quickly obtain the maximum expected cumulative reward. Then, an adaptive e-greedy action selection strategy is adopted to achieve a balance between exploration and utilization to a certain extent, thereby improving the fidelity of the final quantum state. The simulation results show that the proposed algorithm can prepare quantum state with high fidelity under limited conditions. Compared with other algorithms, it has different degrees of improvement in convergence speed and fidelity of the final quantum state.Here's the translation breakdown:* "two-qubit system" is translated as "两 Quint 系统" (liǎng qiánti zhìxīng)* "ladder reward function" is translated as "爬坡式奖励函数" (pángmǎ shì jiàngshǎng fùxìng)* "quantum state evolution" is translated as "量子状态演化" (liàngzǐ zhèngdào yánhuà)* "weighted differential dynamic reward function" is translated as "带有权重的不同动态奖励函数" (dài yǒu quánzhòng de bùdìng dòngtài jiàngshǎng fùxìng)* "adaptive e-greedy action selection strategy" is translated as "适应式e-贪Strategy" (shìyìngxìng e-shèng zhìxíng)* "fidelity of the final quantum state" is translated as "最终量子态的准确性" (zuìzhì liàngzǐ zhèngde zhèngxìng)Note that the translation is in Simplified Chinese, which is the most commonly used form of Chinese writing. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Discrete-Choice-Model-with-Generalized-Additive-Utility-Network"><a href="#Discrete-Choice-Model-with-Generalized-Additive-Utility-Network" class="headerlink" title="Discrete-Choice Model with Generalized Additive Utility Network"></a>Discrete-Choice Model with Generalized Additive Utility Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16970">http://arxiv.org/abs/2309.16970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoki Nishi, Yusuke Hara</li>
<li>for: 提供有价值的决策行为分析结果，帮助政策制定者和企业做出更好的决策。</li>
<li>methods: 使用多项式几何函数（MNL），其中包括神经网络模型（如ASU-DNN），以提高预测决策结果的准确性。</li>
<li>results: 提出一种基于泛化加性模型（GAUNet）的新型Utility函数，并在东京的旅游调查数据上进行了评估，结果与ASU-DNN的准确性相当，而且具有更好的可读性。<details>
<summary>Abstract</summary>
Discrete-choice models are a powerful framework for analyzing decision-making behavior to provide valuable insights for policymakers and businesses. Multinomial logit models (MNLs) with linear utility functions have been used in practice because they are ease to use and interpretable. Recently, MNLs with neural networks (e.g., ASU-DNN) have been developed, and they have achieved higher prediction accuracy in behavior choice than classical MNLs. However, these models lack interpretability owing to complex structures. We developed utility functions with a novel neural-network architecture based on generalized additive models, named generalized additive utility network ( GAUNet), for discrete-choice models. We evaluated the performance of the MNL with GAUNet using the trip survey data collected in Tokyo. Our models were comparable to ASU-DNN in accuracy and exhibited improved interpretability compared to previous models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-Driving-Behavior-Generation-Incorporating-Human-Risk-Cognition-for-Autonomous-Vehicle-Evaluation"><a href="#Adversarial-Driving-Behavior-Generation-Incorporating-Human-Risk-Cognition-for-Autonomous-Vehicle-Evaluation" class="headerlink" title="Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation"></a>Adversarial Driving Behavior Generation Incorporating Human Risk Cognition for Autonomous Vehicle Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00029">http://arxiv.org/abs/2310.00029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Liu, Hang Gao, Hao Ma, Shuo Cai, Yunfeng Hu, Ting Qu, Hong Chen, Xun Gong</li>
<li>for: 本研究旨在开发一种新的敌对驾驶行为生成框架，用于检测自动驾驶车（AV）的弱点。</li>
<li>methods: 该研究使用了强化学习（RL）方法，并结合了累积前景理论（CPT）来表示人类风险认知。另外，使用了扩展版的深度决定策函数（DDPG）技术来训练敌对政策，并保证训练稳定性。</li>
<li>results: 对比试验表明，该敌对方法可以准确地探测测试AV的弱点，并且在高精度硬件在Loop（HiL）平台上得到了较好的效果。<details>
<summary>Abstract</summary>
Autonomous vehicle (AV) evaluation has been the subject of increased interest in recent years both in industry and in academia. This paper focuses on the development of a novel framework for generating adversarial driving behavior of background vehicle interfering against the AV to expose effective and rational risky events. Specifically, the adversarial behavior is learned by a reinforcement learning (RL) approach incorporated with the cumulative prospect theory (CPT) which allows representation of human risk cognition. Then, the extended version of deep deterministic policy gradient (DDPG) technique is proposed for training the adversarial policy while ensuring training stability as the CPT action-value function is leveraged. A comparative case study regarding the cut-in scenario is conducted on a high fidelity Hardware-in-the-Loop (HiL) platform and the results demonstrate the adversarial effectiveness to infer the weakness of the tested AV.
</details>
<details>
<summary>摘要</summary>
自主车辆评估在过去几年内得到了业界和学术界的更多关注。这篇论文关注于开发了一种新的敌对驾驶行为生成框架，用于让背景车辆 intervene 对自动驾驶车辆 (AV) 进行攻击，以暴露出效果和合理的危险事件。特别是，敌对行为通过复现人类风险认知的汇集前景理论 (CPT) 学习approach，从而学习出敌对策略。然后，为了保证训练稳定，提出了基于深度权值函数的扩展DDPG技术。在一个高精度硬件在Loop（HiL）平台上进行了比较案例研究，结果表明了敌对策略的效果，并暴露了测试 AV 的弱点。
</details></li>
</ul>
<hr>
<h2 id="Axiomatic-Aggregations-of-Abductive-Explanations"><a href="#Axiomatic-Aggregations-of-Abductive-Explanations" class="headerlink" title="Axiomatic Aggregations of Abductive Explanations"></a>Axiomatic Aggregations of Abductive Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03131">http://arxiv.org/abs/2310.03131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elitalobo/Axiomatic-Aggregations-of-Abductive-Explanations">https://github.com/elitalobo/Axiomatic-Aggregations-of-Abductive-Explanations</a></li>
<li>paper_authors: Gagan Biradar, Yacine Izza, Elita Lobo, Vignesh Viswanathan, Yair Zick</li>
<li>for: 这个论文旨在解决post hoc模型解释方法（如LIME和SHAP）的稳定性问题，提出了模型精确的推理解释方法。</li>
<li>methods: 这个论文使用了abductive解释方法，对每个数据点提供了最小的特征子集，可以生成结果。然而，这些解释方法可能会出现多个有效的解释方案，这会导致不充分的解释。这篇论文解决了这个问题，通过对多个解释方案进行聚合，生成特征重要性分数。</li>
<li>results: 这篇论文提出了三种聚合方法：基于cooperative游戏理论的力指数方法，以及基于一种广泛使用的 causal strength 度量方法。这些方法都有一定的满意性质量，并且在多个数据集上进行了评估，并表明这些解释方法具有robust性。<details>
<summary>Abstract</summary>
The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of them uniquely satisfies a set of desirable properties. We also evaluate them on multiple datasets and show that these explanations are robust to the attacks that fool SHAP and LIME.
</details>
<details>
<summary>摘要</summary>
Recent criticisms of post hoc model approximation explanation methods (such as LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue - there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of them uniquely satisfies a set of desirable properties. We also evaluate them on multiple datasets and show that these explanations are robust to the attacks that fool SHAP and LIME.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-Generating-Explanations-for-Reinforcement-Learning-Policies-An-Empirical-Study"><a href="#On-Generating-Explanations-for-Reinforcement-Learning-Policies-An-Empirical-Study" class="headerlink" title="On Generating Explanations for Reinforcement Learning Policies: An Empirical Study"></a>On Generating Explanations for Reinforcement Learning Policies: An Empirical Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16960">http://arxiv.org/abs/2309.16960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikihisa Yuasa, Huy T. Tran, Ramavarapu S. Sreenivas</li>
<li>for: 提供策略解释</li>
<li>methods: 使用线性时间逻辑（LTL）方程提供解释</li>
<li>results: 在模拟的捕捉Flag环境中证明了方法的有效性，并提出了未来研究的建议。In this paper, the authors propose using Linear Temporal Logic (LTL) formulae to provide explanations for policies. The paper focuses on creating explanations that reveal both the ultimate objectives accomplished by the policy and the conditions it requires throughout its execution. The proposed approach is demonstrated to be effective through a simulated capture the flag environment, and the paper concludes with suggestions for future research.<details>
<summary>Abstract</summary>
In this paper, we introduce a set of \textit{Linear Temporal Logic} (LTL) formulae designed to provide explanations for policies. Our focus is on crafting explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisites it upholds throughout its execution. These LTL-based explanations feature a structured representation, which is particularly well-suited for local-search techniques. The effectiveness of our proposed approach is illustrated through a simulated capture the flag environment. The paper concludes with suggested directions for future research.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一组基于线性时间逻辑（LTL）的方程，用于提供政策的解释。我们的注重点是通过构建政策执行过程中的前提和目标的结构化表示，以便使用本地搜索技术。我们使用这种LTL-基于的解释方法在模拟的捕捉 Flag 环境中证明了其效果。文章结束时，我们还提出了未来研究的建议。Here's the breakdown of the text into Simplified Chinese characters:在这篇论文中 (在这篇论文中)我们介绍了一组 (我们介绍了一组)基于线性时间逻辑 (LTL) (基于线性时间逻辑)的方程 (方程)用于提供政策的解释 (用于提供政策的解释)我们的注重点 (我们的注重点)是通过构建政策执行过程中的前提 (是通过构建政策执行过程中的前提)和目标 (和目标)的结构化表示 (的结构化表示)以便使用本地搜索技术 (以便使用本地搜索技术)我们使用这种LTL-基于的解释方法 (我们使用这种LTL-基于的解释方法)在模拟的捕捉 Flag 环境中证明了其效果 (在模拟的捕捉 Flag 环境中证明了其效果)文章结束时 (文章结束时)我们还提出了未来研究的建议 (我们还提出了未来研究的建议)
</details></li>
</ul>
<hr>
<h2 id="Denoising-Diffusion-Bridge-Models"><a href="#Denoising-Diffusion-Bridge-Models" class="headerlink" title="Denoising Diffusion Bridge Models"></a>Denoising Diffusion Bridge Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16948">http://arxiv.org/abs/2309.16948</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexzhou907/DDBM">https://github.com/alexzhou907/DDBM</a></li>
<li>paper_authors: Linqi Zhou, Aaron Lou, Samar Khanna, Stefano Ermon</li>
<li>for: 这篇论文的目的是提出一种新的扩展 diffusion models 的方法，以便在图像编辑等应用中更好地处理非随机噪声的输入数据。</li>
<li>methods: 该方法基于 diffusion bridges 家族的过程，通过学习数据中的得分来解决一个（随机）分子方程，从一个分布转换到另一个分布。</li>
<li>results: 在实验中，这种方法在困难的图像数据集上达到了显著的改善，并且在减少问题到随机噪声的情况下，与状态艺术方法的 FID 分数相当。<details>
<summary>Abstract</summary>
Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general problem. Empirically, we apply DDBMs to challenging image datasets in both pixel and latent space. On standard image translation problems, DDBMs achieve significant improvement over baseline methods, and, when we reduce the problem to image generation by setting the source distribution to random noise, DDBMs achieve comparable FID scores to state-of-the-art methods despite being built for a more general task.
</details>
<details>
<summary>摘要</summary>
Diffusion模型是一种强大的生成模型，它将随机噪声映射到数据中的某些特征上。然而，在许多应用程序，如图像修改，模型输入不是随机噪声。因此，扩散模型必须采用困难的方法，如导航或投影抽样，来包含这些信息在生成过程中。在我们的工作中，我们提出了去噪扩散桥模型（DDBMs），这是一种自然的代替方案，基于扩散桥，一种将两个对应的分布作为终点点 interpolate 的过程家族。我们的方法从数据中学习扩散桥的分数，并将一个分布转换成另一个分布，解决一个（随机）导数 Equation 基于学习的分数。我们的方法自然地统一了许多类型的生成模型，如分数基本扩散模型和OT-Flow-Matching，让我们可以将现有的设计和建筑选择应用到我们更一般的问题上。在实验中，我们在难度图像集上应用DDBMs，并在像素空间和尺度空间中进行了图像翻译和生成。相比基eline方法，DDBMs在标准图像翻译问题上达到了显著的改进，而当我们将问题降到图像生成的情况下，DDBMs在state-of-the-art方法的FID分数上达到了相当的比较。
</details></li>
</ul>
<hr>
<h2 id="Asynchrony-Robust-Collaborative-Perception-via-Bird’s-Eye-View-Flow"><a href="#Asynchrony-Robust-Collaborative-Perception-via-Bird’s-Eye-View-Flow" class="headerlink" title="Asynchrony-Robust Collaborative Perception via Bird’s Eye View Flow"></a>Asynchrony-Robust Collaborative Perception via Bird’s Eye View Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16940">http://arxiv.org/abs/2309.16940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MediaBrain-SJTU/CoBEVFlow">https://github.com/MediaBrain-SJTU/CoBEVFlow</a></li>
<li>paper_authors: Sizhe Wei, Yuxi Wei, Yue Hu, Yifan Lu, Yiqi Zhong, Siheng Chen, Ya Zhang</li>
<li>for: This paper is written to address the issue of temporal asynchrony in multi-agent collaboration, which can negatively impact the accuracy of perception and fusion in real-world scenarios.</li>
<li>methods: The proposed method, CoBEVFlow, uses a bird’s eye view (BEV) flow to compensate for motions and align asynchronous collaboration messages sent by multiple agents. This approach allows for robust and efficient collaboration, even in extremely asynchronous settings.</li>
<li>results: The paper presents extensive experiments conducted on both synthetic and real-world datasets, which demonstrate the efficacy of CoBEVFlow in mitigating the impact of asynchrony and outperforming other baselines. The code for CoBEVFlow is available online.Here is the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了解决多代合作中的时间异步问题，以提高实际场景中的感知和融合精度。</li>
<li>methods: 提议的方法是基于鸟瞰视图（BEV）流来补做异步合作消息的滤波，使多个代理能够协作，甚至在极端异步设置下进行有效的协作。</li>
<li>results: 论文提供了大量在synthetic和实际数据集上的实验结果，证明CoBEVFlow在异步设置下能够有效地减轻异步的影响，并超越其他基准。代码可在<a target="_blank" rel="noopener" href="https://github.com/MediaBrain-SJTU/CoBEVFlow%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/MediaBrain-SJTU/CoBEVFlow上下载。</a><details>
<summary>Abstract</summary>
Collaborative perception can substantially boost each agent's perception ability by facilitating communication among multiple agents. However, temporal asynchrony among agents is inevitable in the real world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collaboration messages sent at irregular, continuous time stamps without discretization; and (ii) with BEV flow, CoBEVFlow only transports the original perceptual features, instead of generating new perceptual features, avoiding additional noises. To validate CoBEVFlow's efficacy, we create IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with various temporal asynchronies that simulate different real-world scenarios. Extensive experiments conducted on both IRV2V and the real-world dataset DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is robust in extremely asynchronous settings. The code is available at https://github.com/MediaBrain-SJTU/CoBEVFlow.
</details>
<details>
<summary>摘要</summary>
合作感知可以有效地提高每个代理的感知能力，通过多个代理之间的交流来提高感知质量。然而，在实际世界中，由于通信延迟、中断和时钟不协调，代理之间的时间差是不可避免的。这会导致多个代理之间的信息不匹配，seriously shaking the foundation of collaboration。为解决这问题，我们提出了CoBEVFlow，一种鲁棒的协同感知系统，基于 bird's eye view（BEV）流。CoBEVFlow的关键想法是补偿不同时间报送的多个代理协同感知消息，以实现协同感知的准确性。为模型场景中的运动，我们提出了BEV流，即每个空间位置对应的运动向量的集合。基于BEV流，协同感知消息可以在不同时间报送的情况下进行重新分配，以避免信息不匹配的问题。CoBEVFlow有两个优点：（i）CoBEVFlow可以处理不同时间报送的协同感知消息，无需精确的时间排序；（ii）通过BEV流，CoBEVFlow只需要传输原始的感知特征，而不是生成新的感知特征，从而避免了额外的噪声。为证明CoBEVFlow的有效性，我们创建了IRregular V2V（IRV2V）数据集，该数据集包含了不同的时间差和协同感知消息的各种实际场景。我们对IRV2V数据集和DAIR-V2X数据集进行了广泛的实验，结果表明，CoBEVFlow在非常异步的设置下表现出色，并且与其他基eline相比具有更高的稳定性和可靠性。代码可以在https://github.com/MediaBrain-SJTU/CoBEVFlow中下载。
</details></li>
</ul>
<hr>
<h2 id="PC-Adapter-Topology-Aware-Adapter-for-Efficient-Domain-Adaption-on-Point-Clouds-with-Rectified-Pseudo-label"><a href="#PC-Adapter-Topology-Aware-Adapter-for-Efficient-Domain-Adaption-on-Point-Clouds-with-Rectified-Pseudo-label" class="headerlink" title="PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label"></a>PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16936">http://arxiv.org/abs/2309.16936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joonhyung Park, Hyunjin Seo, Eunho Yang</li>
<li>for: 本文是关于减少点云数据在不同频谱下的域 adaptation 问题的研究，以便更好地理解实际世界中的点云数据。</li>
<li>methods: 本文提出了一种基于注意力抽象器和图 convolution 的域 adaptation 方法，以保留源频谱中的全局几何信息，同时在目标频谱中学习本地特征。此外，本文还提出了一种新的 Pseudo-labeling 策略，可以快速地适应不同的频谱下的点云数据。</li>
<li>results: 在 PointDA、GraspNetPC 和 PointSegDA 等标准 benchmark 上，本文的方法与基eline 相比，显示了更高的准确率和更好的鲁棒性。<details>
<summary>Abstract</summary>
Understanding point clouds captured from the real-world is challenging due to shifts in data distribution caused by varying object scales, sensor angles, and self-occlusion. Prior works have addressed this issue by combining recent learning principles such as self-supervised learning, self-training, and adversarial training, which leads to significant computational overhead.Toward succinct yet powerful domain adaptation for point clouds, we revisit the unique challenges of point cloud data under domain shift scenarios and discover the importance of the global geometry of source data and trends of target pseudo-labels biased to the source label distribution. Motivated by our observations, we propose an adapter-guided domain adaptation method, PC-Adapter, that preserves the global shape information of the source domain using an attention-based adapter, while learning the local characteristics of the target domain via another adapter equipped with graph convolution. Additionally, we propose a novel pseudo-labeling strategy resilient to the classifier bias by adjusting confidence scores using their class-wise confidence distributions to consider relative confidences. Our method demonstrates superiority over baselines on various domain shift settings in benchmark datasets - PointDA, GraspNetPC, and PointSegDA.
</details>
<details>
<summary>摘要</summary>
understanding real-world point clouds 困难，因为数据分布变化引起的因素，如对象大小、感知角度和自我遮挡。先前的方法通过结合最新的学习原则，如无监督学习、自我训练和对抗学习，来解决这个问题，但这会导致很大的计算开销。为了实现简洁而强大的领域适应，我们重新检查了点云数据在领域shift场景下的独特挑战，并发现了源数据的全局几何结构和目标假标签的趋势是源标签分布的关键。基于这些观察，我们提出了一种 adapter-guided 领域适应方法，称为 PC-Adapter，它使用注意力基于的适应器保留源领域的全局几何信息，同时通过另一个适应器 equipped with 图像 convolution 学习target领域的本地特征。此外，我们提出了一种新的假标签生成策略，可以快速地适应类ifier的偏见，通过调整对应的信任分布来考虑相对的信任程度。我们的方法在 PointDA、GraspNetPC 和 PointSegDA 等标准数据集上显示出了superiority。
</details></li>
</ul>
<hr>
<h2 id="TranDRL-A-Transformer-Driven-Deep-Reinforcement-Learning-Enabled-Prescriptive-Maintenance-Framework"><a href="#TranDRL-A-Transformer-Driven-Deep-Reinforcement-Learning-Enabled-Prescriptive-Maintenance-Framework" class="headerlink" title="TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework"></a>TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16935">http://arxiv.org/abs/2309.16935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Zhao, Wenbo Wang</li>
<li>for: 这篇论文旨在提供一个可靠的预测维护策略，以提高工业系统的运行效率并减少机器 downtime。</li>
<li>methods: 本文提出了一个新的、整合的框架，具有变数径模型和深度强化学习（DRL）算法，以便优化维护作业。变数径模型可以很好地捕捉各种复杂的时间特征，从而精准地预测设备的剩下有用生命（RUL）。同时，DRL ком成分则提供了成本效益的维护建议。</li>
<li>results: 本文验证了其框架的有效性，使用NASA C-MPASS数据集，其展示了预测RUL的精准性和维护作业的优化。因此，本文的创新方法可以提供一个数据驱动的维护策略，解决了工业系统中的主要挑战，将系统变得更高效、成本更低、可靠性更高。<details>
<summary>Abstract</summary>
Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to more efficient, cost-effective, and reliable systems.
</details>
<details>
<summary>摘要</summary>
工业系统需要可靠的预测维护策略，以提高操作效率和减少停机时间。本文将介绍一个新的、整合式框架，利用变换器神经网络和深度强化学习（DRL）算法来优化维护行动。我们的方法使用变换器模型从感应数据中够有效地捕捉复杂的时间模式，从而精确地预测设备的剩余有用生命（RUL）。同时，我们的框架中的DRL部分提供了成本效益的维护建议。我们在NASA C-MPASS数据集上验证了我们的框架，其展示了很大的RUL预测精度和维护行动优化。因此，我们的创新方法提供了一个数据驱动的维护方法，解决了工业运营中的主要挑战，导向更高效、成本效益、可靠的系统。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Receive-Help-Intervention-Aware-Concept-Embedding-Models"><a href="#Learning-to-Receive-Help-Intervention-Aware-Concept-Embedding-Models" class="headerlink" title="Learning to Receive Help: Intervention-Aware Concept Embedding Models"></a>Learning to Receive Help: Intervention-Aware Concept Embedding Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16928">http://arxiv.org/abs/2309.16928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mateoespinosa/cem">https://github.com/mateoespinosa/cem</a></li>
<li>paper_authors: Mateo Espinosa Zarlenga, Katherine M. Collins, Krishnamurthy Dvijotham, Adrian Weller, Zohreh Shams, Mateja Jamnik</li>
<li>for: 该 paper 的目的是提出一种新的概念瓶障模型（IntCEMs），以解决现有概念可访性模型（CBMs）中的透彻性问题，并且提高模型在测试时的抗访问性。</li>
<li>methods: 该 paper 使用了一种新的训练方法和架构，称为Intervention-aware Concept Embedding models（IntCEMs），它可以在训练时学习一种概念修正策略，并在测试时通过 sampling meaningful intervention trajectories来实现更好的抗访问性。</li>
<li>results: 该 paper 的实验结果显示，IntCEMs 在接受测试时的概念修正方法时表现出色，与现有概念可访性模型相比，具有更高的效果。<details>
<summary>Abstract</summary>
Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories at train-time. This conditions IntCEMs to effectively select and receive concept interventions when deployed at test-time. Our experiments show that IntCEMs significantly outperform state-of-the-art concept-interpretable models when provided with test-time concept interventions, demonstrating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
具有概念瓶颈模型（CBM）的模型可以解释其预测结果使用一组高级概念。特殊的是，这些模型允许用户进行概念修正，从而提高模型的性能。然而，最近的研究表明，修正效果可能受到 intervened 概念的顺序和模型的架构和训练参数的影响。我们认为这是由于 CBM 缺乏在训练时间提供模型适应测试时间修正的培训励agrant。为解决这个问题，我们提出了概念修正感知模型（IntCEM），一种基于 CBM 的新的架构和训练方法。我们的模型在训练时间内学习一个概念修正策略，从而可以在测试时间内采取有意义的修正轨迹。这使得 IntCEMs 在测试时间被部署时能够有效地接受概念修正。我们的实验表明，IntCEMs 在测试时间提供修正后性能明显超过了现有的概念可读性模型，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Mode-Connectivity-and-Data-Heterogeneity-of-Federated-Learning"><a href="#Mode-Connectivity-and-Data-Heterogeneity-of-Federated-Learning" class="headerlink" title="Mode Connectivity and Data Heterogeneity of Federated Learning"></a>Mode Connectivity and Data Heterogeneity of Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16923">http://arxiv.org/abs/2309.16923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tailin Zhou, Jun Zhang, Danny H. K. Tsang</li>
<li>for: 这个论文的目的是研究联合学习（Federated Learning）中客户端和全球模式之间的关系，以及如何减少客户端资料不均性导致的模型调整频率差异。</li>
<li>methods: 这个论文使用了模式连接性来研究客户端和全球模式之间的关系，并通过实验和理论分析来探索这关系的特性。</li>
<li>results: 研究发现，降低客户端资料不均性可以使模型更加稳定，并且可以增加全球模式之间的连接性，从而提高模型的性能。此外，论文还发现了一个阻碍连接性的障碍，即在两个全球模式之间的直线连接，但这个障碍可以通过考虑非线性模式连接性来消除。<details>
<summary>Abstract</summary>
Federated learning (FL) enables multiple clients to train a model while keeping their data private collaboratively. Previous studies have shown that data heterogeneity between clients leads to drifts across client updates. However, there are few studies on the relationship between client and global modes, making it unclear where these updates end up drifting. We perform empirical and theoretical studies on this relationship by utilizing mode connectivity, which measures performance change (i.e., connectivity) along parametric paths between different modes. Empirically, reducing data heterogeneity makes the connectivity on different paths more similar, forming more low-error overlaps between client and global modes. We also find that a barrier to connectivity occurs when linearly connecting two global modes, while it disappears with considering non-linear mode connectivity. Theoretically, we establish a quantitative bound on the global-mode connectivity using mean-field theory or dropout stability. The bound demonstrates that the connectivity improves when reducing data heterogeneity and widening trained models. Numerical results further corroborate our analytical findings.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 允许多个客户端同时训练模型，保持其数据私有性。过去的研究表明，客户端数据的不同性导致客户端更新中的漂移。然而，关于客户端和全球模式之间的关系，还是有很少研究，使得这些更新的漂移方向不清楚。我们通过使用模式连接性来研究这种关系，并进行了实验和理论研究。实验结果表明，降低客户端数据的不同性可以使得不同模式之间的连接性更加相似，形成更多的低错重叠。此外，我们发现在线性连接两个全球模式时会出现一个阻断连接性的问题，而不考虑非线性模式连接性时，这个问题会消失。理论上，我们使用mean-field理论或dropout稳定性来确定全球模式连接性的质量上限。这个上限表明，降低客户端数据的不同性和训练模型的宽度可以提高连接性。数值结果进一步证实了我们的分析结论。
</details></li>
</ul>
<hr>
<h2 id="ACGAN-GNNExplainer-Auxiliary-Conditional-Generative-Explainer-for-Graph-Neural-Networks"><a href="#ACGAN-GNNExplainer-Auxiliary-Conditional-Generative-Explainer-for-Graph-Neural-Networks" class="headerlink" title="ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks"></a>ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16918">http://arxiv.org/abs/2309.16918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqiao Li, Jianlong Zhou, Yifei Dong, Niusha Shafiabady, Fang Chen</li>
<li>for: 本研究旨在提出一种基于生成器和检测器的图神经网络（GNN）解释器，以提高GNN的可靠性和决策支持。</li>
<li>methods: 本文提出的ACGAN-GNNExplainer方法使用生成器生成输入图的解释，并通过检测器监督生成过程，以确保解释的准确性和精度。</li>
<li>results: 实验结果表明，ACGAN-GNNExplainer方法在Synthetic和实际图据集上比其他已有的GNN解释器更高的精度和可靠性。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph datasets demonstrate the superiority of our proposed method compared to other existing GNN explainers.
</details>
<details>
<summary>摘要</summary>
几何神经网络（GNN）在实际应用中表现出色，但它们的下面机制仍然是一个谜。为了解释这个挑战并实现可靠的决策，许多GNN解释器在最近的年份中被提出。然而，这些方法经常遇到限制，包括对特定实体的依赖、缺乏未见顶点的普遍化、生成可能无效的解释和产生不足的精确性。为了突破这些限制，我们在这篇论文中引入了帮助器网络（ACGAN）到GNN解释领域，并提出了一个新的GNN解释器，名为ACGAN-GNNExplainer。我们的方法利用生成器来生成输入几何网络的解释，同时包含一个检测器来监督生成过程，以确保解释的实际性和提高精确性。实验结果显示，我们的提案方法在 sintetic 和实际几何网络数据集上比其他现有的GNN解释器更有优势。
</details></li>
</ul>
<hr>
<h2 id="ONNXExplainer-an-ONNX-Based-Generic-Framework-to-Explain-Neural-Networks-Using-Shapley-Values"><a href="#ONNXExplainer-an-ONNX-Based-Generic-Framework-to-Explain-Neural-Networks-Using-Shapley-Values" class="headerlink" title="ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values"></a>ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16916">http://arxiv.org/abs/2309.16916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Zhao, Runxin He, Nicholas Kersting, Can Liu, Shubham Agrawal, Chiranjeet Chetia, Yu Gu</li>
<li>for: 本文旨在提出一种基于ONNX ecosystem的神经网络解释框架，使用Shapley值来解释神经网络的预测结果。</li>
<li>methods: 本文使用自动微分和优化方法来实现一次部署和高效计算神经网络解释，并与TensorFlow和PyTorch进行了公正的比较。</li>
<li>results: 对VGG19、ResNet50、DenseNet201和EfficientNetB0等神经网络模型进行了广泛的 benchmark，结果显示，提出的优化方法可以提高解释延迟时间，比现有开源 counterpart SHAP 提高500%。<details>
<summary>Abstract</summary>
Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch and measure its performance against the current state of the art open-source counterpart, SHAP. Extensive benchmarks demonstrate that the proposed optimization approach improves the explanation latency of VGG19, ResNet50, DenseNet201, and EfficientNetB0 by as much as 500%.
</details>
<details>
<summary>摘要</summary>
ONNXExplainer develops its own automatic differentiation and optimization approach, which not only enables one-shot deployment of neural network inference and explanations, but also significantly improves the efficiency of computing explanations with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch and measure its performance against the current state-of-the-art open-source counterpart, SHAP.Extensive benchmarks demonstrate that the proposed optimization approach improves the explanation latency of VGG19, ResNet50, DenseNet201, and EfficientNetB0 by as much as 500%.
</details></li>
</ul>
<hr>
<h2 id="ASAP-Automated-Sequence-Planning-for-Complex-Robotic-Assembly-with-Physical-Feasibility"><a href="#ASAP-Automated-Sequence-Planning-for-Complex-Robotic-Assembly-with-Physical-Feasibility" class="headerlink" title="ASAP: Automated Sequence Planning for Complex Robotic Assembly with Physical Feasibility"></a>ASAP: Automated Sequence Planning for Complex Robotic Assembly with Physical Feasibility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16909">http://arxiv.org/abs/2309.16909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunsheng Tian, Karl D. D. Willis, Bassel Al Omari, Jieliang Luo, Pingchuan Ma, Yichen Li, Farhad Javid, Edward Gu, Joshua Jacob, Shinjiro Sueda, Hui Li, Sachin Chitta, Wojciech Matusik</li>
<li>for: 这个论文目的是提供一种自动生成可行的组装序列计划方法，用于自动组装多个部件 together。</li>
<li>methods: 这种方法基于物理学，考虑重力，设计一个物理稳定的子组装序列，并使用有效的搜索算法来减少维度的决策复杂性。搜索可以被导向 geometric heuristics 或 graph neural networks  trained on simulation labels。</li>
<li>results: 作者们在大量复杂产品组装序列数据集上表明 ASAP 可以生成物理可行的组装序列规划方案，并在实际 робоット设置中进行了应用。<details>
<summary>Abstract</summary>
The automated assembly of complex products requires a system that can automatically plan a physically feasible sequence of actions for assembling many parts together. In this paper, we present ASAP, a physics-based planning approach for automatically generating such a sequence for general-shaped assemblies. ASAP accounts for gravity to design a sequence where each sub-assembly is physically stable with a limited number of parts being held and a support surface. We apply efficient tree search algorithms to reduce the combinatorial complexity of determining such an assembly sequence. The search can be guided by either geometric heuristics or graph neural networks trained on data with simulation labels. Finally, we show the superior performance of ASAP at generating physically realistic assembly sequence plans on a large dataset of hundreds of complex product assemblies. We further demonstrate the applicability of ASAP on both simulation and real-world robotic setups. Project website: asap.csail.mit.edu
</details>
<details>
<summary>摘要</summary>
自动生成复杂产品的组装需要一个系统可以自动规划一个物理可行的动作序列，以将多个部件组装在一起。在这篇论文中，我们介绍了 ASAP，一种基于物理学的规划方法，用于自动生成这种动作序列。ASAP考虑了重力，以设计一个物理稳定的动作序列，其中每个子组件都由有限数量的部件支持和承载。我们使用高效的树搜索算法来减少这种组装序列的组合性复杂度。搜索可以按照几何启发或基于数据的物理学 Label 进行导航。最后，我们表明 ASAP 在大量复杂产品组装序列计划中表现出色，并在实际机器人设置中进行了应用。更多信息请参考项目网站：asap.csail.mit.edu。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/29/cs.AI_2023_09_29/" data-id="cloqtaemv004rgh88hl6254ps" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/29/cs.CL_2023_09_29/" class="article-date">
  <time datetime="2023-09-29T11:00:00.000Z" itemprop="datePublished">2023-09-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/29/cs.CL_2023_09_29/">cs.CL - 2023-09-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Contextual-Biasing-with-the-Knuth-Morris-Pratt-Matching-Algorithm"><a href="#Contextual-Biasing-with-the-Knuth-Morris-Pratt-Matching-Algorithm" class="headerlink" title="Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm"></a>Contextual Biasing with the Knuth-Morris-Pratt Matching Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00178">http://arxiv.org/abs/2310.00178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiran Wang, Zelin Wu, Diamantino Caseiro, Tsendsuren Munkhdalai, Khe Chai Sim, Pat Rondon, Golan Pundak, Gan Song, Rohit Prabhavalkar, Zhong Meng, Ding Zhao, Tara Sainath, Pedro Moreno Mengibar</li>
<li>for: 提高自动语音识别（ASR）系统中的罕见实体识别精度。</li>
<li>methods: 基于 Knuth-Morris-Pratt 算法的模式匹配算法来实现上下文偏导。在搜索过程中，我们会将匹配扩展得分提高，以便在偏导短语集中匹配。我们的方法可以模拟经典方法在Weighted Finite State Transducer（WFST）框架中实现，但是免除了 FST 语言 altogether，并且对内存占用和tensor处理单元（TPU）中的效率进行了仔细考虑。</li>
<li>results: 对偏导测试集进行了重要的单词错误率（WER）降低，而且可以与模型基于偏导方法相结合，进一步提高性能。<details>
<summary>Abstract</summary>
Contextual biasing refers to the problem of biasing the automatic speech recognition (ASR) systems towards rare entities that are relevant to the specific user or application scenarios. We propose algorithms for contextual biasing based on the Knuth-Morris-Pratt algorithm for pattern matching. During beam search, we boost the score of a token extension if it extends matching into a set of biasing phrases. Our method simulates the classical approaches often implemented in the weighted finite state transducer (WFST) framework, but avoids the FST language altogether, with careful considerations on memory footprint and efficiency on tensor processing units (TPUs) by vectorization. Without introducing additional model parameters, our method achieves significant word error rate (WER) reductions on biasing test sets by itself, and yields further performance gain when combined with a model-based biasing method.
</details>
<details>
<summary>摘要</summary>
Contextual biasing 是指偏导自动语音识别（ASR）系统向罕见实体方向，这些实体与特定用户或应用场景有关。我们提出基于 Knuth-Morris-Pratt 算法的偏导方法，在搜索过程中，如果扩展匹配到偏导短语集，就会增加该延伸token的得分。我们的方法模拟了经典方法，通常在Weighted Finite State Transducer（WFST）框架中实现，但是避免了 FST 语言，并且对内存占用和硬件加速（TPU）进行了仔细考虑，通过向量化来减少内存占用。无需添加额外参数，我们的方法可以在偏导测试集上减少单词错误率（WER），并且可以与模型基于偏导方法结合使用，以获得更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Prompt-Rewriting-for-Personalized-Text-Generation"><a href="#Automatic-Prompt-Rewriting-for-Personalized-Text-Generation" class="headerlink" title="Automatic Prompt Rewriting for Personalized Text Generation"></a>Automatic Prompt Rewriting for Personalized Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00152">http://arxiv.org/abs/2310.00152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Michael Bendersky</li>
<li>for: 这个论文主要是关于如何自动修改文本提示，以提高大语言模型（LLM）生成的个性化文本。</li>
<li>methods: 该论文提出了一种新的修改提示方法，使用了链接了supervised learning（SL）和强化学习（RL）的训练方法，以便自动修改文本提示。</li>
<li>results: 在三个代表性的领域中使用了 datasets，结果表明修改后的提示文本比原始提示文本和通过supervised learning或强化学习优化的提示文本更高效。<details>
<summary>Abstract</summary>
Facilitated by large language models (LLMs), personalized text generation has become a rapidly growing research direction. Most existing studies focus on designing specialized models for a particular domain, or they require fine-tuning the LLMs to generate personalized text. We consider a typical scenario in which the large language model, which generates personalized output, is frozen and can only be accessed through APIs. Under this constraint, all one can do is to improve the input text (i.e., text prompts) sent to the LLM, a procedure that is usually done manually. In this paper, we propose a novel method to automatically revise prompts for personalized text generation. The proposed method takes the initial prompts generated by a state-of-the-art, multistage framework for personalized generation and rewrites a few critical components that summarize and synthesize the personal context. The prompt rewriter employs a training paradigm that chains together supervised learning (SL) and reinforcement learning (RL), where SL reduces the search space of RL and RL facilitates end-to-end training of the rewriter. Using datasets from three representative domains, we demonstrate that the rewritten prompts outperform both the original prompts and the prompts optimized via supervised learning or reinforcement learning alone. In-depth analysis of the rewritten prompts shows that they are not only human readable, but also able to guide manual revision of prompts when there is limited resource to employ reinforcement learning to train the prompt rewriter, or when it is costly to deploy an automatic prompt rewriter for inference.
</details>
<details>
<summary>摘要</summary>
由大型语言模型（LLM）所facilitates，个人化文本生成已成为快速增长的研究方向。大多数现有研究专注于设计特定领域的专门模型，或者需要精确地调整LLM以生成个人化文本。我们考虑了一个常见的情况，在这个情况下，大型语言模型可以仅通过API进行访问，而且这个模型已经冻结并不能进行更新。在这种情况下，我们可以对输入文本（即文本提示）进行改进，这是通常由人工进行的。在这篇文章中，我们提出了一种新的方法来自动修改提示文本，以生成个人化文本。我们的方法是使用一个组合了supervised learning（SL）和强化学习（RL）的训练 парадиг，其中SL减少了RL的搜寻空间，RL则帮助对 rewrite 进行端对端训练。使用三个代表领域的数据集，我们显示了 rewrite 的提示文本比原始提示文本和仅通过SL或RL alone 来调整的提示文本更好。深入分析 rewrite 的提示文本表明它们不��LY readable，并且能够指导人工修改提示文本，当有限的资源供应不足，或者当部署自动 rewrite 提示文本检查器时成本高昂。
</details></li>
</ul>
<hr>
<h2 id="The-Gift-of-Feedback-Improving-ASR-Model-Quality-by-Learning-from-User-Corrections-through-Federated-Learning"><a href="#The-Gift-of-Feedback-Improving-ASR-Model-Quality-by-Learning-from-User-Corrections-through-Federated-Learning" class="headerlink" title="The Gift of Feedback: Improving ASR Model Quality by Learning from User Corrections through Federated Learning"></a>The Gift of Feedback: Improving ASR Model Quality by Learning from User Corrections through Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00141">http://arxiv.org/abs/2310.00141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lillian Zhou, Yuxin Ding, Mingqing Chen, Harry Zhang, Rohit Prabhavalkar, Dhruv Guliani, Giovanni Motta, Rajiv Mathews</li>
<li>for: addressing the issue of outdated automatic speech recognition (ASR) models on edge devices due to language evolution</li>
<li>methods: using Federated Learning (FL) to continually learn from on-device user corrections and improve recognition of fresh terms, while mitigating catastrophic forgetting</li>
<li>results: improved recognition of fresh terms while preserving overall language distribution quality in experimental evaluations<details>
<summary>Abstract</summary>
Automatic speech recognition (ASR) models are typically trained on large datasets of transcribed speech. As language evolves and new terms come into use, these models can become outdated and stale. In the context of models trained on the server but deployed on edge devices, errors may result from the mismatch between server training data and actual on-device usage. In this work, we seek to continually learn from on-device user corrections through Federated Learning (FL) to address this issue. We explore techniques to target fresh terms that the model has not previously encountered, learn long-tail words, and mitigate catastrophic forgetting. In experimental evaluations, we find that the proposed techniques improve model recognition of fresh terms, while preserving quality on the overall language distribution.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）模型通常在大量的转录speech数据上训练。随着语言的发展和新词出现，这些模型可能会过时和停滞。在服务器上训练并在边缘设备上部署的模型中，错误可能会出现由服务器训练数据与实际边缘设备使用的差异引起。在这项工作中，我们寻求通过联邦学习（FL）持续学习从边缘设备上的用户更正来解决这个问题。我们探索了如何Target新的特有词汇，学习长尾词和 Mitigate Catastrophic Forgetting。在实验评估中，我们发现提议的技术可以提高模型对新词的识别，同时保持语言总体分布的质量。
</details></li>
</ul>
<hr>
<h2 id="A-Large-Language-Model-Approach-to-Educational-Survey-Feedback-Analysis"><a href="#A-Large-Language-Model-Approach-to-Educational-Survey-Feedback-Analysis" class="headerlink" title="A Large Language Model Approach to Educational Survey Feedback Analysis"></a>A Large Language Model Approach to Educational Survey Feedback Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17447">http://arxiv.org/abs/2309.17447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael J. Parker, Caitlin Anderson, Claire Stone, YeaRim Oh</li>
<li>for: 这paper investigate了使用大型自然语言处理（LLM）GPT-4和GPT-3.5来挖掘教育反馈调查中的智能。</li>
<li>methods: 该paper使用了LLM来解决多种自然语言处理任务，包括分类（多标签、多类和二分）、EXTRACTION、主题分析和情感分析。</li>
<li>results: 研究表明，通过使用有效的提示实践，可以使GPT-4达到人类水平的性能在多个任务上，并且可以使用LLM的链条思维来提供有价值的反馈。此外，该paper还开发了一套可变的分类类型，适用于不同的课程类型（在线、半在线或面对面），并且可以根据需要自定义。<details>
<summary>Abstract</summary>
This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve human-level performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.
</details>
<details>
<summary>摘要</summary>
Survey analysis in education often involves manual processing of textual responses to identify gaps in curricula or evaluate teachers. LLMs can provide a flexible and efficient solution to these tasks without the need for specialized machine learning models or fine-tuning.The authors demonstrate a versatile approach to survey analysis by treating each task as a sequence of natural language processing (NLP) tasks, including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis. They apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses and achieve human-level performance on multiple tasks with GPT-4 using effective prompting practices.Moreover, the study shows the potential of inspecting LLMs' chain-of-thought (CoT) reasoning to provide insight into their decision-making process and foster confidence in their recommendations. The authors also develop a versatile set of classification categories that can be customized for various course types (online, hybrid, or in-person).The results suggest that LLMs can be used to derive a range of insights from survey text, making them a valuable tool for education feedback analysis.
</details></li>
</ul>
<hr>
<h2 id="L2CEval-Evaluating-Language-to-Code-Generation-Capabilities-of-Large-Language-Models"><a href="#L2CEval-Evaluating-Language-to-Code-Generation-Capabilities-of-Large-Language-Models" class="headerlink" title="L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models"></a>L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17446">http://arxiv.org/abs/2309.17446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, Shafiq Joty, Yingbo Zhou, Dragomir Radev, Arman Cohan</li>
<li>for: 这个论文主要是为了对大型自然语言模型（LLM）在几个任务上的语言到代码生成能力进行系统性的评估。</li>
<li>methods: 这个论文使用了多种任务、模型结构和学习方法来评估 LLM 的语言到代码生成能力，并对模型性能、自信抑制和人工评估输出程序的效果进行分析。</li>
<li>results: 这个论文通过对 7 个任务进行系统性的评估，发现 LLM 在 semantic parsing、math reasoning 和 Python 程序生成等领域具有强大的语言到代码生成能力，但也存在一些常见的失败模式。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluations of the output programs. This enables us to identify and analyze the typical failure modes across various tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We also release the evaluation framework and all model outputs, hoping to lay the groundwork for further future research in this domain.
</details>
<details>
<summary>摘要</summary>
近些时间，大型语言模型（LLM），特别是基于代码预训练的模型，在几个shot或者 zeroshot的情况下，表现出了强大的生成代码能力。despite promising results, there is a notable lack of a comprehensive evaluation of these models' language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning, and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluations of the output programs. This enables us to identify and analyze the typical failure modes across various tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We also release the evaluation framework and all model outputs, hoping to lay the groundwork for further future research in this domain.
</details></li>
</ul>
<hr>
<h2 id="The-Dawn-of-LMMs-Preliminary-Explorations-with-GPT-4V-ision"><a href="#The-Dawn-of-LMMs-Preliminary-Explorations-with-GPT-4V-ision" class="headerlink" title="The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)"></a>The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17421">http://arxiv.org/abs/2309.17421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang</li>
<li>for: 这个论文旨在深入分析最新的大型多Modal模型（LMMs），即GPT-4V（视觉）模型，以更好地理解LMMs的generic intelligence。</li>
<li>methods: 本论文使用了一系列的测试样本，探索GPT-4V模型的能力范围和特性，包括处理多种多媒体输入和模式下的工作方式，以及如何提示模型。</li>
<li>results: 对GPT-4V模型的分析表明，它可以处理无序多媒体输入，并且其能力具有很高的一致性和通用性。此外，GPT-4V还可以理解图像上的视觉标记，这可能会开拓新的人机交互方式，如图像引用提示。<details>
<summary>Abstract</summary>
Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf
</details>
<details>
<summary>摘要</summary>
大型多modal模型（LMM）将大型语言模型（LLM）扩展到多感知能力，如视觉理解，以实现更强大的通用智能。在这篇论文中，我们对最新的模型GPT-4V（视觉）进行分析，以深入了解LMM。我们的分析将ocus在GPT-4V可以完成的奇妙任务上，包括测试样本以探索GPT-4V的质量和通用性，以及它支持的输入和工作模式。我们的探索方法包括策定和组织一组仔细设计的质量样本，覆盖多个领域和任务。这些样本的观察结果表明，GPT-4V可以处理任意排序的多感知输入，并且其能力的通用性使得GPT-4V成为一个强大的多感知通用系统。此外，GPT-4V可以理解输入图像上的视觉标记，可能开拓新的人机交互方法，如视觉引用提示。我们在报告中进行了深入的讨论，探讨GPT-4V应用场景的出现和未来研究方向，以及如何更好地利用和加强LMM来解决实际问题。最后，我们表示这只是对OpenAI创新的产物——GPT-4V的初步探索，我们应该对LMM的下一代多模态任务定义、新的方法来利用和提高LMM，以及多感知基础模型的更好理解进行未来研究。请参考OpenAI的贡献和归属报告，了解GPT-4V的开发者和归属：https://cdn.openai.com/contributions/gpt-4v.pdf。
</details></li>
</ul>
<hr>
<h2 id="Intuitive-or-Dependent-Investigating-LLMs’-Robustness-to-Conflicting-Prompts"><a href="#Intuitive-or-Dependent-Investigating-LLMs’-Robustness-to-Conflicting-Prompts" class="headerlink" title="Intuitive or Dependent? Investigating LLMs’ Robustness to Conflicting Prompts"></a>Intuitive or Dependent? Investigating LLMs’ Robustness to Conflicting Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17415">http://arxiv.org/abs/2309.17415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long Cui, Yongbin Liu</li>
<li>for: 这篇论文探讨了 LLMS 在内存或给定提示中的偏好是否Robust，因为在实际应用中可能存在噪音或任务设置导致提示与内存中的信息相悖。</li>
<li>methods: 作者们设置了一个量化的 benchmarking 框架，并通过控制 LLMS 的偏好来测试其Robustness。特别是，他们定义了两种Robustness：事实Robustness，targeting LLMs 能够从提示或内存中正确地选择信息，以及决策风格， categorizing LLMs 的行为为INTUITIVE、依赖或理性 Based on cognitive theory。</li>
<li>results: 通过对七个开源和关闭源 LLMS 进行广泛的实验，作者们发现这些模型具有高度易受扰动提示的特点，尤其是在指导常识知识方面。虽然详细的指导可以减少选择错误答案的风险，但这也会增加无效答案的发生率。通过对不同大小 LLMS 进行特定的角色指导，作者们发现这些模型的Robustness和适应性具有不同的Upper bound。<details>
<summary>Abstract</summary>
This paper explores the robustness of LLMs' preference to their internal memory or the given prompt, which may contain contrasting information in real-world applications due to noise or task settings. To this end, we establish a quantitative benchmarking framework and conduct the role playing intervention to control LLMs' preference. In specific, we define two types of robustness, factual robustness targeting the ability to identify the correct fact from prompts or memory, and decision style to categorize LLMs' behavior in making consistent choices -- assuming there is no definitive "right" answer -- intuitive, dependent, or rational based on cognitive theory. Our findings, derived from extensive experiments on seven open-source and closed-source LLMs, reveal that these models are highly susceptible to misleading prompts, especially for instructing commonsense knowledge. While detailed instructions can mitigate the selection of misleading answers, they also increase the incidence of invalid responses. After Unraveling the preference, we intervene different sized LLMs through specific style of role instruction, showing their varying upper bound of robustness and adaptivity.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "LLMs" is translated as "大语言模型" (dà yǔ yán módel), which means "large language models" in Simplified Chinese.* "preference" is translated as "偏好" (piān xiǎng), which means "preference" or "inclination" in Simplified Chinese.* "robustness" is translated as "可靠性" (kě jiān xìng), which means "reliability" or "robustness" in Simplified Chinese.* "factual robustness" is translated as "事实可靠性" (shì shí kě jiān xìng), which means "factual reliability" or "factual robustness" in Simplified Chinese.* "decision style" is translated as "决策风格" (jīe yì fēng xìng), which means "decision style" or "cognitive style" in Simplified Chinese.* "intuitive" is translated as "直观" (zhí guān), which means "intuitive" or "gut feeling" in Simplified Chinese.* "dependent" is translated as "依赖" (yì gòng), which means "dependent" or "reliant" in Simplified Chinese.* "rational" is translated as "理性" (lǐ xìng), which means "rational" or "logical" in Simplified Chinese.* "cognitive theory" is translated as "认知理论" (niǎn zhī lǐ lun), which means "cognitive theory" or "cognitive science" in Simplified Chinese.* "prompts" is translated as "提示" (tiē shì), which means "prompts" or "hints" in Simplified Chinese.* "task settings" is translated as "任务设置" (ràng wù jiè xiǎng), which means "task settings" or "task conditions" in Simplified Chinese.* "noise" is translated as "噪音" (zhōng yīn), which means "noise" or "background noise" in Simplified Chinese.* "extensive experiments" is translated as "广泛实验" (guǎn fāng shí yàn), which means "extensive experiments" or "large-scale experiments" in Simplified Chinese.* "seven open-source and closed-source LLMs" is translated as "七种开源和关源的大语言模型" (qī zhǒng kāi yuán yǔ guān yuán de dà yǔ yán módel), which means "seven open-source and closed-source large language models" in Simplified Chinese.* "role-playing intervention" is translated as "角色扮演 intervención" (jiǎo xiǎng bǎo yán jiān), which means "role-playing intervention" or "role-playing experiment" in Simplified Chinese.* "specific style of role instruction" is translated as "特定的角色指导方式" (tè qī de jiǎo xiǎng zhǐ dǎo fāng yì), which means "specific style of role guidance" or "specific role-playing method" in Simplified Chinese.* "varying upper bound of robustness and adaptivity" is translated as "不同的可靠性和适应性上限" (bù dōng de kě jiān xìng yǔ shì bìng xìng), which means "varying upper bound of robustness and adaptability" or "different levels of reliability and adaptability" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Overview-of-the-BioLaySumm-2023-Shared-Task-on-Lay-Summarization-of-Biomedical-Research-Articles"><a href="#Overview-of-the-BioLaySumm-2023-Shared-Task-on-Lay-Summarization-of-Biomedical-Research-Articles" class="headerlink" title="Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles"></a>Overview of the BioLaySumm 2023 Shared Task on Lay Summarization of Biomedical Research Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17332">http://arxiv.org/abs/2309.17332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomas Goldsack, Zheheng Luo, Qianqian Xie, Carolina Scarton, Matthew Shardlow, Sophia Ananiadou, Chenghua Lin</li>
<li>for: 本研究旨在开发一种可以生成非技术人群可理解的摘要的自然语言处理模型，并在控制和无控制的Setting下进行测试。</li>
<li>methods: 本研究使用了 BioNLP 工作shop at ACL 2023 上的分布式任务，即 Lay Summarisation of Biomedical Research Articles (BioLaySumm) 分布式任务，以测试参与者所建立的摘要模型。</li>
<li>results: 研究结果表明，参与者所建立的模型在控制和无控制 Setting下都能够生成高质量的摘要，并且在不同的文章类型和长度下都能够达到比较高的准确率。<details>
<summary>Abstract</summary>
This paper presents the results of the shared task on Lay Summarisation of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL 2023. The goal of this shared task is to develop abstractive summarisation models capable of generating "lay summaries" (i.e., summaries that are comprehensible to non-technical audiences) in both a controllable and non-controllable setting. There are two subtasks: 1) Lay Summarisation, where the goal is for participants to build models for lay summary generation only, given the full article text and the corresponding abstract as input; and 2) Readability-controlled Summarisation, where the goal is for participants to train models to generate both the technical abstract and the lay summary, given an article's main text as input. In addition to overall results, we report on the setup and insights from the BioLaySumm shared task, which attracted a total of 20 participating teams across both subtasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Lay Summarization: The goal is for participants to build models for lay summary generation only, given the full article text and the corresponding abstract as input.2. Readability-controlled Summarization: The goal is for participants to train models to generate both the technical abstract and the lay summary, given an article’s main text as input.In addition to overall results, we report on the setup and insights from the BioLaySumm shared task, which attracted a total of 20 participating teams across both subtasks.</details></li>
</ol>
<hr>
<h2 id="Few-Shot-Domain-Adaptation-for-Charge-Prediction-on-Unprofessional-Descriptions"><a href="#Few-Shot-Domain-Adaptation-for-Charge-Prediction-on-Unprofessional-Descriptions" class="headerlink" title="Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions"></a>Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17313">http://arxiv.org/abs/2309.17313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Zhao, Ziyu Guan, Wei Zhao, Yue Jiang, Xiaofei He</li>
<li>for: 提高非法律专业人群使用的负载预测服务的准确率。</li>
<li>methods: 提出了一种新的几shot领域适应方法（FSDA），名为分解法律内容 для负载预测（DLCCP），以优化内容和风格特征之间的域 invariant 学习。</li>
<li>results: 实验表明，相比现有的FSDA方法，DLCCP方法在新发布的非法律专业人群数据集（NCCP）上表现出色，超越了竞争对手的基线。<details>
<summary>Abstract</summary>
Recent works considering professional legal-linguistic style (PLLS) texts have shown promising results on the charge prediction task. However, unprofessional users also show an increasing demand on such a prediction service. There is a clear domain discrepancy between PLLS texts and non-PLLS texts expressed by those laypersons, which degrades the current SOTA models' performance on non-PLLS texts. A key challenge is the scarcity of non-PLLS data for most charge classes. This paper proposes a novel few-shot domain adaptation (FSDA) method named Disentangled Legal Content for Charge Prediction (DLCCP). Compared with existing FSDA works, which solely perform instance-level alignment without considering the negative impact of text style information existing in latent features, DLCCP (1) disentangles the content and style representations for better domain-invariant legal content learning with carefully designed optimization goals for content and style spaces and, (2) employs the constitutive elements knowledge of charges to extract and align element-level and instance-level content representations simultaneously. We contribute the first publicly available non-PLLS dataset named NCCP for developing layperson-friendly charge prediction models. Experiments on NCCP show the superiority of our methods over competitive baselines.
</details>
<details>
<summary>摘要</summary>
近期研究聚焦专业法律语言风格（PLLS）文本的成果表明，在充电预测任务中，PLLS文本的预测表现很出色。然而，不专业的用户也在增加对这种预测服务的需求。存在专业领域与非专业领域之间的域名不同，导致当前最佳实践模型对非专业文本的表现下降。本文提出了一种新的几shot领域适应（FSDA）方法，名为分解法律内容 для充电预测（DLCCP）。与现有FSDA工作相比，DLCCP在不同领域中学习法律内容的时候，不仅实现了实例级别的对齐，还考虑了文本风格信息在潜在特征中的负面影响。DLCCP使用了充电罪名的构成元素知识，将元素级别和实例级别的内容表示分解，并同时进行实例级别的内容对齐。我们提供了首次公开的非PLLS数据集，名为NCCP，以便开发易懂的充电预测模型。实验表明，我们的方法在NCCP上表现出色，超过了竞争对手的基eline。
</details></li>
</ul>
<hr>
<h2 id="Wiki-En-ASR-Adapt-Large-scale-synthetic-dataset-for-English-ASR-Customization"><a href="#Wiki-En-ASR-Adapt-Large-scale-synthetic-dataset-for-English-ASR-Customization" class="headerlink" title="Wiki-En-ASR-Adapt: Large-scale synthetic dataset for English ASR Customization"></a>Wiki-En-ASR-Adapt: Large-scale synthetic dataset for English ASR Customization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17267">http://arxiv.org/abs/2309.17267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Antonova</li>
<li>for: 这 paper 是为了提出一个大规模的公共适应拼写检查定制自动语音识别（ASR）系统，特别是处理不同类型的罕见和出于词汇（OOV）短语。</li>
<li>methods: 该方法使用创造了数百万个真实的损坏 ASR 假设，并在定制任务中使用非puis的批处理列表。此外，它还提出了两种类型的 &#96;&#96;hard negatives” 的插入方法，并描述了自动挖掘的过程。</li>
<li>results: 经过训练一个开源定制模型，并在提posed dataset上进行了实验，研究发现，插入 &#96;&#96;hard negatives” 的方法可以降低 WER 和假阳性数量。<details>
<summary>Abstract</summary>
We present a first large-scale public synthetic dataset for contextual spellchecking customization of automatic speech recognition (ASR) with focus on diverse rare and out-of-vocabulary (OOV) phrases, such as proper names or terms. The proposed approach allows creating millions of realistic examples of corrupted ASR hypotheses and simulate non-trivial biasing lists for the customization task. Furthermore, we propose injecting two types of ``hard negatives" to the simulated biasing lists in training examples and describe our procedures to automatically mine them. We report experiments with training an open-source customization model on the proposed dataset and show that the injection of hard negative biasing phrases decreases WER and the number of false alarms.
</details>
<details>
<summary>摘要</summary>
我们提供了首个大规模公共合成数据集，用于语音识别自动化（ASR）上下文ual spellchecking个性化。我们的方法可以创建百万个真实的损坏ASR假设，并模拟非常复杂的偏见列表用于个性化任务。此外，我们还提议在训练示例中注入两种类型的“hard negatives”，并描述我们的程序自动挖掘它们。我们对一个开源个性化模型进行训练，并发现在投入“hard negative”偏见列表后，WER和假阳性数量减少。
</details></li>
</ul>
<hr>
<h2 id="LLM-Deliberation-Evaluating-LLMs-with-Interactive-Multi-Agent-Negotiation-Games"><a href="#LLM-Deliberation-Evaluating-LLMs-with-Interactive-Multi-Agent-Negotiation-Games" class="headerlink" title="LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games"></a>LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17234">http://arxiv.org/abs/2309.17234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s-abdelnabi/llm-deliberation">https://github.com/s-abdelnabi/llm-deliberation</a></li>
<li>paper_authors: Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr, Mario Fritz</li>
<li>for: 评估大语言模型（LLMs）的决策和判断能力，以及其在实际任务中的应用性。</li>
<li>methods: 使用可评分的谈判游戏作为新的评估框架，并通过链式思维提示（Chain-of-Thought）来帮助代理人达成成功协议。</li>
<li>results: 通过多种文本基于的多代理人、多问题、semantically rich谈判游戏，证明代理人可以成功谈判并实现协议，并且可以普适应用于新的游戏和设置。<details>
<summary>Abstract</summary>
There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.
</details>
<details>
<summary>摘要</summary>
有越来越多的关注使用大型语言模型（LLM）作为处理复杂情况的代理人。然而，我们对LLM的决策和处理能力的理解还很有限，一部分是因为缺乏专门的评估标准。为了解决这个问题，我们提议使用可评分谈判游戏作为LLM的评估框架。我们创建了多种文本基于的多代理人、多问题、semantic rich的谈判游戏，易于调整Difficulty。为了解决这个挑战，代理人需要具备强大的数学、推理、探索和规划能力，同时协调这些能力。通过一种系统的零shotChain-of-Thought提示（CoT），我们示出了代理人可以成功谈判并达成协议。我们使用多个指标量化表现，并发现GPT-4和早期模型之间存在巨大的差距。更重要的是，我们测试了新游戏和设置的一致性。最后，我们表明这些游戏可以评估其他重要方面，如代理人之间的互动动力在恶意和投机者存在时。
</details></li>
</ul>
<hr>
<h2 id="Training-and-inference-of-large-language-models-using-8-bit-floating-point"><a href="#Training-and-inference-of-large-language-models-using-8-bit-floating-point" class="headerlink" title="Training and inference of large language models using 8-bit floating point"></a>Training and inference of large language models using 8-bit floating point</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17224">http://arxiv.org/abs/2309.17224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergio P. Perez, Yan Zhang, James Briggs, Charlie Blake, Josh Levy-Kramer, Paul Balanca, Carlo Luschi, Stephen Barlow, Andrew William Fitzgibbon</li>
<li>for: 这篇论文是关于FP8格式在深度学习模型训练和推理中提高计算效率的研究。</li>
<li>methods: 该论文提出了一种基于动态更新每个张量缩放的方法来选择FP8线性层的缩放，以避免因为减少精度而导致的质量下降。</li>
<li>results: 该论文通过在大语言模型GPT和Llama 2中使用FP8进行训练和验证，并 plots了每个张量缩放的分布图示，以便更好地理解FP8的动态。<details>
<summary>Abstract</summary>
FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Comparative-Analysis-of-Named-Entity-Recognition-in-the-Dungeons-and-Dragons-Domain"><a href="#Comparative-Analysis-of-Named-Entity-Recognition-in-the-Dungeons-and-Dragons-Domain" class="headerlink" title="Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain"></a>Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17171">http://arxiv.org/abs/2309.17171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gayashan Weerasundara, Nisansa de Silva</li>
<li>for: 本研究旨在评估各种Named Entity Recognition（NER）模型在特定领域（如奇幻小说）中的表现。</li>
<li>methods: 研究使用开源的大语言模型对7本 Dungeons and Dragons（D&amp;D）冒险小说进行 named entity 标注，并评估每种模型的精度。</li>
<li>results: 研究发现，未经修改的 Flair、Trankit 和 Spacy 在 D&amp;D 上表现较佳，其他模型表现较差。<details>
<summary>Abstract</summary>
Many NLP tasks, although well-resolved for general English, face challenges in specific domains like fantasy literature. This is evident in Named Entity Recognition (NER), which detects and categorizes entities in text. We analyzed 10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess domain-specific performance. Using open-source Large Language Models, we annotated named entities in these books and evaluated each model's precision. Our findings indicate that, without modifications, Flair, Trankit, and Spacy outperform others in identifying named entities in the D&D context.
</details>
<details>
<summary>摘要</summary>
许多自然语言处理任务，尤其是在特定领域 like 奇幻小说中，存在挑战。这是Named Entity Recognition（NER）的问题，它在文本中检测和分类名实体。我们对7本《启示录》冒险小说进行了10个NER模型的测试，以评估域 especific的性能。使用开源的大语言模型，我们对这些书籍中的名实体进行了标注，并评估每个模型的精度。我们的发现表明，无需修改，Flair、Trankit和Spacy在D&D上表现最佳，可以准确地识别冒险小说中的名实体。
</details></li>
</ul>
<hr>
<h2 id="LatticeGen-A-Cooperative-Framework-which-Hides-Generated-Text-in-a-Lattice-for-Privacy-Aware-Generation-on-Cloud"><a href="#LatticeGen-A-Cooperative-Framework-which-Hides-Generated-Text-in-a-Lattice-for-Privacy-Aware-Generation-on-Cloud" class="headerlink" title="LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud"></a>LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17157">http://arxiv.org/abs/2309.17157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov</li>
<li>for: 保护文本生成的隐私和安全</li>
<li>methods: 提议了一种协作框架，让服务器处理大部分计算，用户控制采样操作，并使用噪声符和重复搜索攻击来防御 Against potential attacks from a malicious server.</li>
<li>results: 在实验中，使用LatticeGen保护文本生成的隐私和安全，并在强攻击下成功保护真实的生成内容，BERTScore指标下than 50%的 semantic remains hidden.<details>
<summary>Abstract</summary>
In the current user-server interaction paradigm of prompted generation with large language models (LLM) on cloud, the server fully controls the generation process, which leaves zero options for users who want to keep the generated text to themselves. We propose LatticeGen, a cooperative framework in which the server still handles most of the computation while the user controls the sampling operation. The key idea is that the true generated sequence is mixed with noise tokens by the user and hidden in a noised lattice. Considering potential attacks from a hypothetically malicious server and how the user can defend against it, we propose the repeated beam-search attack and the mixing noise scheme. In our experiments we apply LatticeGen to protect both prompt and generation. It is shown that while the noised lattice degrades generation quality, LatticeGen successfully protects the true generation to a remarkable degree under strong attacks (more than 50% of the semantic remains hidden as measured by BERTScore).
</details>
<details>
<summary>摘要</summary>
当前用户-服务器交互模式下，大语言模型（LLM）在云端完全控制生成过程，留下 zero 选项 для用户们希望保留生成的文本。我们提议LaticeGen，一种合作框架，在服务器处理大部分计算，而用户控制采样操作。关键思想是真正生成的序列被用户杂乱并隐藏在噪声矩阵中。面对可能的恶意服务器攻击和用户如何防御，我们提出重复扫描攻击和杂乱噪声方案。在我们的实验中，我们应用LaticeGen来保护提示和生成。结果显示，虽然噪声矩阵减低生成质量，但LaticeGen成功地保护真正的生成，并在强攻击下（BERTScore中的更 than 50%的 semantic 保持不变）。
</details></li>
</ul>
<hr>
<h2 id="Promoting-Generalized-Cross-lingual-Question-Answering-in-Few-resource-Scenarios-via-Self-knowledge-Distillation"><a href="#Promoting-Generalized-Cross-lingual-Question-Answering-in-Few-resource-Scenarios-via-Self-knowledge-Distillation" class="headerlink" title="Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation"></a>Promoting Generalized Cross-lingual Question Answering in Few-resource Scenarios via Self-knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17134">http://arxiv.org/abs/2309.17134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ccasimiro88/self-distillation-gxlt-qa">https://github.com/ccasimiro88/self-distillation-gxlt-qa</a></li>
<li>paper_authors: Casimiro Pio Carrino, Carlos Escolano, José A. R. Fonollosa</li>
<li>for: 提高多语言提取式问答系统（QA）的性能，特别是对于语言资源有限的情况。</li>
<li>methods: 使用高性能的多语言模型，训练于大规模数据集，并使用批处理和高级自逊训练进行增强。还引入了新的mAP@k系数来细化自知识抽象loss，以实现平衡和有效的知识传递。</li>
<li>results: 比标准cross-entropy训练更高，并且在资源有限的情况下，even in zero-shot scenarios，与一个强基eline相比，表现竞争力强。<details>
<summary>Abstract</summary>
Despite substantial progress in multilingual extractive Question Answering (QA), models with high and uniformly distributed performance across languages remain challenging, especially for languages with limited resources. We study cross-lingual transfer mainly focusing on the Generalized Cross-Lingual Transfer (G-XLT) task, where the question language differs from the context language - a challenge that has received limited attention thus far. Our approach seeks to enhance cross-lingual QA transfer using a high-performing multilingual model trained on a large-scale dataset, complemented by a few thousand aligned QA examples across languages. Our proposed strategy combines cross-lingual sampling and advanced self-distillation training in generations to tackle the previous challenge. Notably, we introduce the novel mAP@k coefficients to fine-tune self-knowledge distillation loss, dynamically regulating the teacher's model knowledge to perform a balanced and effective knowledge transfer. We extensively evaluate our approach to assess XLT and G-XLT capabilities in extractive QA. Results reveal that our self-knowledge distillation approach outperforms standard cross-entropy fine-tuning by a significant margin. Importantly, when compared to a strong baseline that leverages a sizeable volume of machine-translated data, our approach shows competitive results despite the considerable challenge of operating within resource-constrained settings, even in zero-shot scenarios. Beyond performance improvements, we offer valuable insights through comprehensive analyses and an ablation study, further substantiating the benefits and constraints of our approach. In essence, we propose a practical solution to improve cross-lingual QA transfer by leveraging a few data resources in an efficient way.
</details>
<details>
<summary>摘要</summary>
Despite significant progress in multilingual extractive Question Answering (QA), models with high and uniformly distributed performance across languages remain challenging, especially for languages with limited resources. We study cross-lingual transfer, focusing on the Generalized Cross-Lingual Transfer (G-XLT) task, where the question language differs from the context language - a challenge that has received limited attention so far. Our approach seeks to enhance cross-lingual QA transfer using a high-performing multilingual model trained on a large-scale dataset, complemented by a few thousand aligned QA examples across languages. Our proposed strategy combines cross-lingual sampling and advanced self-distillation training in generations to tackle the previous challenge. Notably, we introduce the novel mAP@k coefficients to fine-tune self-knowledge distillation loss, dynamically regulating the teacher's model knowledge to perform a balanced and effective knowledge transfer. We extensively evaluate our approach to assess XLT and G-XLT capabilities in extractive QA. Results reveal that our self-knowledge distillation approach outperforms standard cross-entropy fine-tuning by a significant margin. Importantly, when compared to a strong baseline that leverages a sizeable volume of machine-translated data, our approach shows competitive results despite the considerable challenge of operating within resource-constrained settings, even in zero-shot scenarios. Beyond performance improvements, we offer valuable insights through comprehensive analyses and an ablation study, further substantiating the benefits and constraints of our approach. In essence, we propose a practical solution to improve cross-lingual QA transfer by leveraging a few data resources in an efficient way.
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Late-interaction-Multi-modal-Retrieval-for-Retrieval-Augmented-Visual-Question-Answering"><a href="#Fine-grained-Late-interaction-Multi-modal-Retrieval-for-Retrieval-Augmented-Visual-Question-Answering" class="headerlink" title="Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering"></a>Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17133">http://arxiv.org/abs/2309.17133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linweizhedragon/retrieval-augmented-visual-question-answering">https://github.com/linweizhedragon/retrieval-augmented-visual-question-answering</a></li>
<li>paper_authors: Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, Bill Byrne<br>for:This paper proposes a new method called Fine-grained Late-interaction Multi-modal Retrieval (FLMR) to improve the performance of Retrieval-Augmented Visual Question Answering (RA-VQA) systems.methods:FLMR uses a vision model aligned with an existing text-based retriever to obtain image representations that complement those from the image-to-text transforms. It also encodes images and questions using multi-dimensional embeddings to capture finer-grained relevance between queries and documents.results:FLMR significantly improves the original RA-VQA retriever’s PRRecall@5 by approximately 8%. Additionally, when equipped with two state-of-the-art large multi-modal&#x2F;language models, RA-VQA achieves $\sim61%$ VQA score in the OK-VQA dataset.<details>
<summary>Abstract</summary>
Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from external knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQA. FLMR addresses two major limitations in RA-VQA's retriever: (1) the image representations obtained via image-to-text transforms can be incomplete and inaccurate and (2) relevance scores between queries and documents are computed with one-dimensional embeddings, which can be insensitive to finer-grained relevance. FLMR overcomes these limitations by obtaining image representations that complement those from the image-to-text transforms using a vision model aligned with an existing text-based retriever through a simple alignment network. FLMR also encodes images and questions using multi-dimensional embeddings to capture finer-grained relevance between queries and documents. FLMR significantly improves the original RA-VQA retriever's PRRecall@5 by approximately 8\%. Finally, we equipped RA-VQA with two state-of-the-art large multi-modal/language models to achieve $\sim61\%$ VQA score in the OK-VQA dataset.
</details>
<details>
<summary>摘要</summary>
知识基础视觉问答（KB-VQA）需要视觉问答系统利用外部知识库来回答基于图像的问题。 retrieve-augmented visual question answering（RA-VQA）是一个强大的框架，可以解决KB-VQA问题，它首先使用 dense passage retrieval（DPR）来 Retrieval 相关的文档，然后使用它们来回答问题。本文提出了细化晚期多模态检索（FLMR），它可以大幅提高RA-VQA中的知识检索。FLMR解决了两个主要的限制：（1）图像表示可能是不完整和不准确的，（2）查询和文档之间的相关性分数是使用一维嵌入计算的，这可能会失去细化的相关性。FLMR通过使用一个简单的对齐网络将视觉模型与现有的文本基础 Retriever 对齐，从而获得补充的图像表示。同时，FLMR使用多维嵌入来编码图像和问题，以捕捉更细化的相关性。FLMR可以提高原始RA-VQA检索器的PRRecall@5约8%。最后，我们将RA-VQA equip 两个现有的大型多Modal/语言模型，达到了OK-VQA数据集中的约61% VQA 分数。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Long-Form-Legal-Question-Answering-with-Retrieval-Augmented-Large-Language-Models"><a href="#Interpretable-Long-Form-Legal-Question-Answering-with-Retrieval-Augmented-Large-Language-Models" class="headerlink" title="Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models"></a>Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17050">http://arxiv.org/abs/2309.17050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maastrichtlawtech/lleqa">https://github.com/maastrichtlawtech/lleqa</a></li>
<li>paper_authors: Antoine Louis, Gijs van Dijck, Gerasimos Spanakis</li>
<li>for: 这个论文旨在 bridging the legal literacy gap 通过开发自动化法律援助系统，帮助更多人更好地理解法律问题。</li>
<li>methods: 该论文提出了一种综合方法，通过“检索然后阅读”管道，生成任何法律问题的详细回答。此外，他们还引入了一个新的法律问题数据集，名为Long-form Legal Question Answering（LLeQA），包含1,868个专家标注的法律问题，以及相关的法律条文回答。</li>
<li>results: 实验结果表明，该方法在自动评估指标上显示了良好的性能，但是一个质量分析发现了一些需要进一步改进的地方。这个论文的研究具有推动法律NLP研究的潜在价值，并且可以作为专业领域NLP模型的严格标准进行评估。<details>
<summary>Abstract</summary>
Many individuals are likely to face a legal dispute at some point in their lives, but their lack of understanding of how to navigate these complex issues often renders them vulnerable. The advancement of natural language processing opens new avenues for bridging this legal literacy gap through the development of automated legal aid systems. However, existing legal question answering (LQA) approaches often suffer from a narrow scope, being either confined to specific legal domains or limited to brief, uninformative responses. In this work, we propose an end-to-end methodology designed to generate long-form answers to any statutory law questions, utilizing a "retrieve-then-read" pipeline. To support this approach, we introduce and release the Long-form Legal Question Answering (LLeQA) dataset, comprising 1,868 expert-annotated legal questions in the French language, complete with detailed answers rooted in pertinent legal provisions. Our experimental results demonstrate promising performance on automatic evaluation metrics, but a qualitative analysis uncovers areas for refinement. As one of the only comprehensive, expert-annotated long-form LQA dataset, LLeQA has the potential to not only accelerate research towards resolving a significant real-world issue, but also act as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models.
</details>
<details>
<summary>摘要</summary>
In this work, we propose an end-to-end methodology that generates long-form answers to any statutory law questions using a "retrieve-then-read" pipeline. To support this approach, we introduce the Long-form Legal Question Answering (LLeQA) dataset, which contains 1,868 expert-annotated legal questions in French, along with detailed answers rooted in relevant legal provisions. Our experimental results show promising performance on automatic evaluation metrics, but a qualitative analysis reveals areas for improvement.As one of the only comprehensive, expert-annotated long-form LQA datasets, LLeQA has the potential to not only resolve a significant real-world issue but also serve as a rigorous benchmark for evaluating NLP models in specialized domains. We publicly release our code, data, and models to facilitate further research and development in this area.
</details></li>
</ul>
<hr>
<h2 id="Contextualising-Levels-of-Language-Resourcedness-affecting-Digital-Processing-of-Text"><a href="#Contextualising-Levels-of-Language-Resourcedness-affecting-Digital-Processing-of-Text" class="headerlink" title="Contextualising Levels of Language Resourcedness affecting Digital Processing of Text"></a>Contextualising Levels of Language Resourcedness affecting Digital Processing of Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17035">http://arxiv.org/abs/2309.17035</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. Maria Keet, Langa Khumalo</li>
<li>for: 这篇论文主要是为了探讨语言资源的分类问题，并提出了一个新的分类方法基于语言使用context。</li>
<li>methods: 该论文使用了一种基于语言使用context的分类方法，并提供了对每个分类特征的解释和动机。</li>
<li>results: 该论文认为现有的LRL和HRL分类方法存在问题，提出了一个新的分类方法，并通过例子说明了该方法的应用。<details>
<summary>Abstract</summary>
Application domains such as digital humanities and tool like chatbots involve some form of processing natural language, from digitising hardcopies to speech generation. The language of the content is typically characterised as either a low resource language (LRL) or high resource language (HRL), also known as resource-scarce and well-resourced languages, respectively. African languages have been characterized as resource-scarce languages (Bosch et al. 2007; Pretorius & Bosch 2003; Keet & Khumalo 2014) and English is by far the most well-resourced language. Varied language resources are used to develop software systems for these languages to accomplish a wide range of tasks. In this paper we argue that the dichotomous typology LRL and HRL for all languages is problematic. Through a clear understanding of language resources situated in a society, a matrix is developed that characterizes languages as Very LRL, LRL, RL, HRL and Very HRL. The characterization is based on the typology of contextual features for each category, rather than counting tools, and motivation is provided for each feature and each characterization. The contextualisation of resourcedness, with a focus on African languages in this paper, and an increased understanding of where on the scale the language used in a project is, may assist in, among others, better planning of research and implementation projects. We thus argue in this paper that the characterization of language resources within a given scale in a project is an indispensable component particularly in the context of low-resourced languages.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于应用领域如数字人文学和 chatbot 等，都涉及到处理自然语言，从扫描硬件到语音生成。语言内容的语言 Typically 被characterized 为 either 资源缺乏语言 (LRL) 或高资源语言 (HRL)，即资源缺乏和资源充沛语言，分别。非洲语言被characterized 为资源缺乏语言 (Bosch et al. 2007; Pretorius & Bosch 2003; Keet & Khumalo 2014)，而英语则是最具资源的语言。为了开发用于这些语言的软件系统， varied 的语言资源被使用。在这篇文章中，我们认为 dichotomous 类型 LRL 和 HRL 对所有语言是问题atic。通过对语言资源在社会中的理解，一个矩阵被发展出来，其中characterizes 语言为 Very LRL、LRL、RL、HRL 和 Very HRL。这种 categorization 基于每个类别的特征类型，而不是计数工具，并且对每个特征和每个 categorization 提供了动机。Contextualization 资源感知，尤其是关注非洲语言在这篇文章中，可以帮助更好地规划研究和实施项目。因此，我们在这篇文章中 argue  dass characterizing 语言资源在项目中的位置是不可或缺的 Component，特别是在资源缺乏语言中。
</details></li>
</ul>
<hr>
<h2 id="I-Wish-to-Have-an-Argument-Argumentative-Reasoning-in-Large-Language-Models"><a href="#I-Wish-to-Have-an-Argument-Argumentative-Reasoning-in-Large-Language-Models" class="headerlink" title="I Wish to Have an Argument: Argumentative Reasoning in Large Language Models"></a>I Wish to Have an Argument: Argumentative Reasoning in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16938">http://arxiv.org/abs/2309.16938</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adewynter/argumentation-llms">https://github.com/adewynter/argumentation-llms</a></li>
<li>paper_authors: Adrian de Wynter, Tommy Yuan</li>
<li>for: 本研究旨在评估当今大型自然语言模型（LLM）是否能够进行有效的论证思维。</li>
<li>methods: 我们将实验设计为Argument Mining（AM）和Argument Pair Extraction（APE）任务，并测试LLM在输入和输出表示层次上的推理能力。</li>
<li>results: 我们发现，虽然LLM能够匹配或超越当前状态的表现，但其论证思维能力很大程度取决于输入和输出表示。我们还发现了一种“示例效应”，即在输入过多示例时，任务表现下降，4-5个示例是最佳数量。而在链条思维（CoT）提问中，这种效应消失，CoT允许更好地在异常条件下表现。<details>
<summary>Abstract</summary>
We evaluate the ability of contemporary large language models (LLMs) to perform argumentative reasoning. We frame our experiments in terms of the argument mining (AM) and argument pair extraction (APE) tasks, and evaluate their ability to perform reasoning at increasing levels of abstraction in the input and output representations (e.g., arbitrary label sets, semantic graphs). We find that, although LLMs are able to match or surpass the state-of-the-art in AM and APE, their argumentative reasoning performance is very dependent on the input and output representation. We also find an "exemplar effect", where too many exemplars increasingly become detrimental for task performance, and about 4-5 being the optimal amount. Neither result extends to chain-of-thought (CoT) prompting: we find the exemplar effect to be nullified, and our results suggest that CoT allows for better performance under ill-conditioned problems. We hope that the work reported contributes to the improvement of argumentative reasoning in LLMs.
</details>
<details>
<summary>摘要</summary>
我团队评估当代大语言模型（LLM）的辩论逻辑能力。我们将实验设计为辩论挖掘（AM）和辩论对比挖掘（APE）任务，并评估其能够在输入和输出表示层次上进行逻辑推理。我们发现，虽然LLM可以匹配或超越现有状态的AM和APE性能，但它们的辩论逻辑性能很виси于输入和输出表示。我们还发现了一种“示例效应”，即过多的示例会导致任务性能下降，并且4-5个示例是最佳数量。这些结果不适用于链条思维（CoT）提问：我们发现示例效应为零，并且我们的结果表明，CoT可以在不良条件下提高任务性能。我们希望该研究对LLM的辩论逻辑能力产生贡献。
</details></li>
</ul>
<hr>
<h2 id="SSHR-Leveraging-Self-supervised-Hierarchical-Representations-for-Multilingual-Automatic-Speech-Recognition"><a href="#SSHR-Leveraging-Self-supervised-Hierarchical-Representations-for-Multilingual-Automatic-Speech-Recognition" class="headerlink" title="SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition"></a>SSHR: Leveraging Self-supervised Hierarchical Representations for Multilingual Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16937">http://arxiv.org/abs/2309.16937</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongfei Xue, Qijie Shao, Kaixun Huang, Peikun Chen, Lei Xie, Jie Liu</li>
<li>for: 提高多语言自动语音识别系统的语言覆盖率</li>
<li>methods: 利用自动学习层次结构表示（SSHR）细化多语言自动语音识别模型，通过分析SSL模型各层表示信息，挖掘语言相关信息和内容相关信息，并通过自注意机制和跨CTC进一步吸取内容相关信息</li>
<li>results: 在Common Voice和ML-SUPERB两个多语言数据集上进行测试，实验结果显示SSHR方法可以达到当前最佳性能水平<details>
<summary>Abstract</summary>
Multilingual automatic speech recognition (ASR) systems have garnered attention for their potential to extend language coverage globally. While self-supervised learning (SSL) has demonstrated its effectiveness in multilingual ASR, it is worth noting that the various layers' representations of SSL potentially contain distinct information that has not been fully leveraged. In this study, we propose a novel method that leverages self-supervised hierarchical representations (SSHR) to fine-tune multilingual ASR. We first analyze the different layers of the SSL model for language-related and content-related information, uncovering layers that show a stronger correlation. Then, we extract a language-related frame from correlated middle layers and guide specific content extraction through self-attention mechanisms. Additionally, we steer the model toward acquiring more content-related information in the final layers using our proposed Cross-CTC. We evaluate SSHR on two multilingual datasets, Common Voice and ML-SUPERB, and the experimental results demonstrate that our method achieves state-of-the-art performance to the best of our knowledge.
</details>
<details>
<summary>摘要</summary>
多语言自动语音识别（ASR）系统在全球语言覆盖方面吸引了一些注意。 although self-supervised learning（SSL）在多语言ASR中表现出色，但是它们各层表示的不同信息仍未得到了完全利用。 在这项研究中，我们提出了一种新的方法，利用自动学习层次表示（SSHR）来精度调整多语言ASR。 我们首先分析了SSL模型各层的语言相关和内容相关信息，找到了相关层，然后提取相关中层的语言帧，通过自我注意机制来引导特定的内容提取。 此外，我们使用我们所提出的交叉CTC（Cross-CTC）来驱动模型获得更多的内容相关信息在最终层。 我们在两个多语言数据集上进行了实验，分别是Common Voice和ML-SUPERB，实验结果表明，我们的方法可以达到当今最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Unified-Framework-for-Adaptable-Problematic-Content-Detection-via-Continual-Learning"><a href="#Towards-a-Unified-Framework-for-Adaptable-Problematic-Content-Detection-via-Continual-Learning" class="headerlink" title="Towards a Unified Framework for Adaptable Problematic Content Detection via Continual Learning"></a>Towards a Unified Framework for Adaptable Problematic Content Detection via Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16905">http://arxiv.org/abs/2309.16905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/usc-cssl/adaptable-problematic-content-detection">https://github.com/usc-cssl/adaptable-problematic-content-detection</a></li>
<li>paper_authors: Ali Omrani, Alireza S. Ziabari, Preni Golazizian, Jeffrey Sorensen, Morteza Dehghani</li>
<li>for: 本研究旨在提供一个集成多来源的敏感内容检测 benchmark，以应对社交媒体上的敏感内容检测 зада题，并且透过连续学习的方式，让模型能够随着任务的变化而进行适应。</li>
<li>methods: 本研究使用了8个来源的15个标签架构，并将其集成到一个共同的敏感内容检测 benchmark 中，以便测试和评估不同的敏感内容检测模型。</li>
<li>results: 本研究的基eline结果显示，透过连续学习的方式，模型能够适应社交媒体上的敏感内容检测任务，并且能够跟踪和适应社交媒体上的敏感内容的不断变化。<details>
<summary>Abstract</summary>
Detecting problematic content, such as hate speech, is a multifaceted and ever-changing task, influenced by social dynamics, user populations, diversity of sources, and evolving language. There has been significant efforts, both in academia and in industry, to develop annotated resources that capture various aspects of problematic content. Due to researchers' diverse objectives, the annotations are inconsistent and hence, reports of progress on detection of problematic content are fragmented. This pattern is expected to persist unless we consolidate resources considering the dynamic nature of the problem. We propose integrating the available resources, and leveraging their dynamic nature to break this pattern. In this paper, we introduce a continual learning benchmark and framework for problematic content detection comprising over 84 related tasks encompassing 15 annotation schemas from 8 sources. Our benchmark creates a novel measure of progress: prioritizing the adaptability of classifiers to evolving tasks over excelling in specific tasks. To ensure the continuous relevance of our framework, we designed it so that new tasks can easily be integrated into the benchmark. Our baseline results demonstrate the potential of continual learning in capturing the evolving content and adapting to novel manifestations of problematic content.
</details>
<details>
<summary>摘要</summary>
检测异常内容，如仇恨言论，是一项多方面和不断发展的任务，受社会动态、用户人口、多源数据和语言演化等因素影响。在学术和业界两个领域，有很大的努力投入到开发了标注资源，以捕捉各种异常内容的不同方面。由于研究人员的多样化目标，标注是不一致的，因此，报告关于异常内容检测的进步是分散的。这种模式预计会持续， Unless we consolidate resources considering the dynamic nature of the problem. We propose integrating the available resources, and leveraging their dynamic nature to break this pattern. In this paper, we introduce a continual learning benchmark and framework for problematic content detection, comprising over 84 related tasks encompassing 15 annotation schemas from 8 sources. Our benchmark creates a novel measure of progress: prioritizing the adaptability of classifiers to evolving tasks over excelling in specific tasks. To ensure the continuous relevance of our framework, we designed it so that new tasks can easily be integrated into the benchmark. Our baseline results demonstrate the potential of continual learning in capturing the evolving content and adapting to novel manifestations of problematic content.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/29/cs.CL_2023_09_29/" data-id="cloqtaeoz00bwgh88fdgefdtp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/29/cs.LG_2023_09_29/" class="article-date">
  <time datetime="2023-09-29T10:00:00.000Z" itemprop="datePublished">2023-09-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/29/cs.LG_2023_09_29/">cs.LG - 2023-09-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Neural-preconditioned-Poisson-Solver-for-Mixed-Dirichlet-and-Neumann-Boundary-Conditions"><a href="#A-Neural-preconditioned-Poisson-Solver-for-Mixed-Dirichlet-and-Neumann-Boundary-Conditions" class="headerlink" title="A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions"></a>A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00177">http://arxiv.org/abs/2310.00177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Weixian Lan, Elias Gueidon, Ayano Kaneda, Julian Panetta, Joseph Teran</li>
<li>for: 这个论文是用来解决Poisson方程中混合边界条件的问题的。</li>
<li>methods: 这个论文使用了一种基于神经网络的迭代解决方法，用于解决Poisson方程的离散 strucured-grid Laplace操作数的问题。</li>
<li>results: 这个论文的实验结果表明，使用这种神经网络预处理器可以高效地解决Poisson方程中的混合边界条件问题，并且在一些实际应用中比algebraic multigrid和其他一些神经网络预处理器更高效。<details>
<summary>Abstract</summary>
We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases arising from an incompressible fluid simulation, our method outperforms state-of-the-art solvers like algebraic multigrid as well as some recent neural preconditioners.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种神经网络预conditioned迭代法来解决杜立特方程式中的混合边界条件问题。杜立特方程式在科学计算中很普遍，它控制了各种物理现象，在许多数值算法中出现为子问题，并且作为更广泛的杜立特PDEs的模型问题。最流行的杜立特积分方法会生成大量的稀疏线性系统。在高分辨率下和性能敏感应用中，迭代法可以是有利的，只要与强大的预conditioner一起使用。我们的核心思想是使用神经网络来近似离散格点拉普拉斯算子的逆函数，该问题具有自由形态和混合边界条件。我们的网络架构受到问题的结构所致，我们示示其在训练集外的边界条件上也是非常有效的预conditioner。我们在来自不可压缩流体模拟的困难测试案例上显示了我们的方法比STATE-OF-THE-ART的方法如多重多射grid和一些最新的神经预conditioner更高效。
</details></li>
</ul>
<hr>
<h2 id="Tight-Bounds-for-Volumetric-Spanners-and-Applications"><a href="#Tight-Bounds-for-Volumetric-Spanners-and-Applications" class="headerlink" title="Tight Bounds for Volumetric Spanners and Applications"></a>Tight Bounds for Volumetric Spanners and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00175">http://arxiv.org/abs/2310.00175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Bhaskara, Sepideh Mahabadi, Ali Vakilian</li>
<li>for: 本研究目的是找到一个小型的基底，使得所有的输入向量都可以表示为这个基底的线性组合，并且这个基底的大小为最小。</li>
<li>methods: 本文使用了一种简单的本地搜索算法来找到这个小型基底。</li>
<li>results: 本文提供了对所有$\ell_p$ нор的准确的下界，并证明这些下界可以使用本地搜索算法来实现。此外，本文还应用了这些结果到其他任务上，包括最小体积包围螺旋凝聚问题（MVEE）问题的找到核心集问题。<details>
<summary>Abstract</summary>
Given a set of points of interest, a volumetric spanner is a subset of the points using which all the points can be expressed using "small" coefficients (measured in an appropriate norm). Formally, given a set of vectors $X = \{v_1, v_2, \dots, v_n\}$, the goal is to find $T \subseteq [n]$ such that every $v \in X$ can be expressed as $\sum_{i\in T} \alpha_i v_i$, with $\|\alpha\|$ being small. This notion, which has also been referred to as a well-conditioned basis, has found several applications, including bandit linear optimization, determinant maximization, and matrix low rank approximation. In this paper, we give almost optimal bounds on the size of volumetric spanners for all $\ell_p$ norms, and show that they can be constructed using a simple local search procedure. We then show the applications of our result to other tasks and in particular the problem of finding coresets for the Minimum Volume Enclosing Ellipsoid (MVEE) problem.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:给一个点集合，一个卷积的扩展是一个包含该点集合的子集，使得所有点可以用"小"系数表示（在适当的 нор 中）。形式地说，给一个 vectors 集合 $X = \{v_1, v_2, \dots, v_n\}$，目标是找到 $T \subseteq [n]$ 使得所有 $v \in X$ 可以表示为 $\sum_{i\in T} \alpha_i v_i$，其中 $\|\alpha\|$ 是小的。这个概念，也被称为well-conditioned basis，在各种应用中得到了推广，包括bandit linear optimization、 determinant maximization 和 matrix low rank approximation。在这篇论文中，我们给出了所有 $\ell_p$  norm 下的几乎最佳大小 bound，并证明了它们可以使用简单的本地搜索算法构建。然后，我们展示了我们的结果在其他任务上的应用，特别是找到 Minimum Volume Enclosing Ellipsoid (MVEE) 问题中的核sets。
</details></li>
</ul>
<hr>
<h2 id="ADMET-property-prediction-through-combinations-of-molecular-fingerprints"><a href="#ADMET-property-prediction-through-combinations-of-molecular-fingerprints" class="headerlink" title="ADMET property prediction through combinations of molecular fingerprints"></a>ADMET property prediction through combinations of molecular fingerprints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00174">http://arxiv.org/abs/2310.00174</a></li>
<li>repo_url: None</li>
<li>paper_authors: James H. Notwell, Michael W. Wood</li>
<li>for: 预测小分子活性</li>
<li>methods: 使用Random Forest、支持向量机（SVM）和扩展连接指纹（ECFP），并与200个分子性质进行组合</li>
<li>results: 使用Gradient Boosting Decision Tree（CatBoost）和组合ECFP、Avalon、ErG指纹以及200个分子性质得到最佳效果，并成功验证在22个Therapeutics Data Commons ADMETbenchmark上<details>
<summary>Abstract</summary>
While investigating methods to predict small molecule potencies, we found random forests or support vector machines paired with extended-connectivity fingerprints (ECFP) consistently outperformed recently developed methods. A detailed investigation into regression algorithms and molecular fingerprints revealed gradient-boosted decision trees, particularly CatBoost, in conjunction with a combination of ECFP, Avalon, and ErG fingerprints, as well as 200 molecular properties, to be most effective. Incorporating a graph neural network fingerprint further enhanced performance. We successfully validated our model across 22 Therapeutics Data Commons ADMET benchmarks. Our findings underscore the significance of richer molecular representations for accurate property prediction.
</details>
<details>
<summary>摘要</summary>
“我们在预测小分子力量方面的研究中发现，随机林或支持向量机（SVM）配对扩展连接指纹（ECFP）一直表现出色，超过最近开发的方法。我们进一步进行了回归算法和分子指纹的调查，发现渐进式搜索树（Gradient Boosting），特别是CatBoost，配对了ECFP、Avalon和ErG指纹，以及200个分子性质，表现最佳。将graph neural network指纹添加到系统中也有助于提高表现。我们成功验证了我们的模型在22个Therapeutics Data Commons ADMET参考标准上。我们的发现强调了丰富的分子表示的重要性，以精确预测分子属性。”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese dialects. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="One-for-All-Towards-Training-One-Graph-Model-for-All-Classification-Tasks"><a href="#One-for-All-Towards-Training-One-Graph-Model-for-All-Classification-Tasks" class="headerlink" title="One for All: Towards Training One Graph Model for All Classification Tasks"></a>One for All: Towards Training One Graph Model for All Classification Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00149">http://arxiv.org/abs/2310.00149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lechengkong/oneforall">https://github.com/lechengkong/oneforall</a></li>
<li>paper_authors: Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, Muhan Zhang</li>
<li>for: 这篇论文的目的是提出一个通用的图像模型，能够解决不同领域的图像任务。</li>
<li>methods: 该模型使用文本描述图像中的节点和边，并使用语言模型将多种不同的文本特征编码到同一个嵌入空间中。此外，它还引入了节点优先的概念，以标准化不同任务的表示。</li>
<li>results: 论文通过使用多个领域的图像数据同时训练，证明了该模型在不同任务下的普通性。它在超参数、少参数和零参数学习场景中都表现出色。<details>
<summary>Abstract</summary>
Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space. Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation. For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs well across different tasks, making it the first general-purpose graph classification model across domains.
</details>
<details>
<summary>摘要</summary>
设计一个单一模型，能够解决多个任务，是人工智能领域的长期目标。现在，大型自然语言模型已经表现出了惊人的能力，可以将不同的语言任务集成到同一个空间中。然而，对于图学习领域的任务，一个综合的模型仍然是未解决的问题。主要的问题包括：图数据来自不同领域，具有不同的特征和分布，这使得将图据表示在同一个空间中变得困难；任务在图上的多样性，需要不同的嵌入策略；以及对于图学习中的具体任务，没有明确的prompting方法。为了解决这些挑战，我们提出了一个名为“One for All”（OFA）的框架，它可以使用单一的图模型来解决多个任务。OFA使用文本描述图的方法，将不同领域的图数据统一到同一个表示空间中。然后，通过语言模型将不同领域的文本特征编码到同一个嵌入空间中。此外，OFA还引入了“焦点节点”的概念，以标识不同任务的共同表示。为了在图学习中进行具体任务的学习，OFA引入了一种新的图Prompting方法，可以在输入图上添加prompting结构，从而实现无需 fine-tuning 的多任务学习。我们使用来自多个领域的图数据（包括引用网络、分子图、知识图等）进行同时训练，并对OFA模型进行supervised、少数shot和零shot学习场景的评估。 results show that OFA perfoms well across different tasks and domains, making it the first general-purpose graph classification model across domains.
</details></li>
</ul>
<hr>
<h2 id="On-the-Disconnect-Between-Theory-and-Practice-of-Overparametrized-Neural-Networks"><a href="#On-the-Disconnect-Between-Theory-and-Practice-of-Overparametrized-Neural-Networks" class="headerlink" title="On the Disconnect Between Theory and Practice of Overparametrized Neural Networks"></a>On the Disconnect Between Theory and Practice of Overparametrized Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00137">http://arxiv.org/abs/2310.00137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Wenger, Felix Dangel, Agustinus Kristiadi</li>
<li>for: 这篇论文是关于神经网络（NNs）的无穷宽限制的研究，以分析大规模、过参数化网络的行为为目的。</li>
<li>methods: 论文使用了神经 Tangent 函数（NTK）来连接神经网络和内核方法，从而实现了神经网络和内核方法之间的联系。</li>
<li>results: 研究发现，在优化、uncertainty quantification和 continual learning 等方面，大规模神经网络不会 exhibit 预测的行为，即使宽度比深度多出很多。这种观察到的差异问题了理论和实践之间的连接。<details>
<summary>Abstract</summary>
The infinite-width limit of neural networks (NNs) has garnered significant attention as a theoretical framework for analyzing the behavior of large-scale, overparametrized networks. By approaching infinite width, NNs effectively converge to a linear model with features characterized by the neural tangent kernel (NTK). This establishes a connection between NNs and kernel methods, the latter of which are well understood. Based on this link, theoretical benefits and algorithmic improvements have been hypothesized and empirically demonstrated in synthetic architectures. These advantages include faster optimization, reliable uncertainty quantification and improved continual learning. However, current results quantifying the rate of convergence to the kernel regime suggest that exploiting these benefits requires architectures that are orders of magnitude wider than they are deep. This assumption raises concerns that practically relevant architectures do not exhibit behavior as predicted via the NTK. In this work, we empirically investigate whether the limiting regime either describes the behavior of large-width architectures used in practice or is informative for algorithmic improvements. Our empirical results demonstrate that this is not the case in optimization, uncertainty quantification or continual learning. This observed disconnect between theory and practice calls into question the practical relevance of the infinite-width limit.
</details>
<details>
<summary>摘要</summary>
宽度无穷限的神经网络（NN）在理论上已引起了广泛关注，作为大规模、过参数化网络的分析理论框架。随着宽度增加，NN Approximately converge to a linear model，其特征由神经拟合函数（NTK）定义。这种连接NN和核函数方法，使得NN的理论优势和算法改进得到了更好的理解。在这个链接下，有许多理论上的利点和实际上的改进被提出和实验证明，包括快速优化、可靠的uncertainty量化和持续学习。然而，当前的结果表明，在实际应用中使用的大规模网络 Architecture需要orders of magnitude wider than deep，这引起了关注，因为这种假设表明，实际应用中的网络不会展现出预测的行为。在这个工作中，我们通过实验来检验，无限宽限是否对实际应用中的大规模网络有用，我们发现，这并不是情况。这种观察到的偏差，质疑无限宽限的实际 relevance。
</details></li>
</ul>
<hr>
<h2 id="Multi-Grid-Tensorized-Fourier-Neural-Operator-for-High-Resolution-PDEs"><a href="#Multi-Grid-Tensorized-Fourier-Neural-Operator-for-High-Resolution-PDEs" class="headerlink" title="Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs"></a>Multi-Grid Tensorized Fourier Neural Operator for High-Resolution PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00120">http://arxiv.org/abs/2310.00120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jean Kossaifi, Nikola Kovachki, Kamyar Azizzadenesheli, Anima Anandkumar</li>
<li>for:  addresses the limitations of learning solution operators of partial differential equations (PDEs) at high resolutions by introducing a new data efficient and highly parallelizable approach with reduced memory requirement and better generalization.</li>
<li>methods:  leverages local and global structures of full-scale, real-world phenomena through a decomposition of both the input domain and the operator’s parameter space, and represents the parameters of the model in a high-order latent subspace of the Fourier domain through a global tensor factorization.</li>
<li>results:  achieves superior performance on the turbulent Navier-Stokes equations with less than half the error and over 150x compression, and reduces the number of parameters by over 150x and the domain size by 7x without losses in accuracy, while slightly enabling parallelism.<details>
<summary>Abstract</summary>
Memory complexity and data scarcity have so far prohibited learning solution operators of partial differential equations (PDEs) at high resolutions. We address these limitations by introducing a new data efficient and highly parallelizable operator learning approach with reduced memory requirement and better generalization, called multi-grid tensorized neural operator (MG-TFNO). MG-TFNO scales to large resolutions by leveraging local and global structures of full-scale, real-world phenomena, through a decomposition of both the input domain and the operator's parameter space. Our contributions are threefold: i) we enable parallelization over input samples with a novel multi-grid-based domain decomposition, ii) we represent the parameters of the model in a high-order latent subspace of the Fourier domain, through a global tensor factorization, resulting in an extreme reduction in the number of parameters and improved generalization, and iii) we propose architectural improvements to the backbone FNO. Our approach can be used in any operator learning setting. We demonstrate superior performance on the turbulent Navier-Stokes equations where we achieve less than half the error with over 150x compression. The tensorization combined with the domain decomposition, yields over 150x reduction in the number of parameters and 7x reduction in the domain size without losses in accuracy, while slightly enabling parallelism.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>在解决部分束环Equation (PDEs)的学习问题上，内存复杂性和数据稀缺性是长期的瓶颈。我们通过引入一种新的数据高效并可并发化的运算符学习方法来缓解这些限制，称为多维Grid Tensorized Neural Operator (MG-TFNO)。MG-TFNO可以扩展到大容量的解析，通过利用全场、实际世界现象的本地和全球结构，对输入域和运算符参数空间进行分解。我们的贡献有三个方面：1. 通过一种新的多维Grid-based域分解方法，实现输入样本的并行化。2. 通过高级别的Latent Space Fourier Transform (LSFT)，将运算符参数表示为高级别的 latent subspace，从而减少参数的数量和提高泛化能力。3. 对Backbone FNO框架进行改进。我们的方法可以应用于任何运算符学习设定下。我们在 Navier-Stokes Equations中实现了较低于半个误差，并且实现了150倍压缩。tensorization和域分解的组合，导致参数的减少和域的减少，而无损失准确性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Over-Molecular-Conformer-Ensembles-Datasets-and-Benchmarks"><a href="#Learning-Over-Molecular-Conformer-Ensembles-Datasets-and-Benchmarks" class="headerlink" title="Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks"></a>Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00115">http://arxiv.org/abs/2310.00115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanqiao Zhu, Jeehyun Hwang, Keir Adams, Zhen Liu, Bozhao Nan, Brock Stenfors, Yuanqi Du, Jatin Chauhan, Olaf Wiest, Olexandr Isayev, Connor W. Coley, Yizhou Sun, Wei Wang<br>for:这个论文主要是为了研究分子表示学习（MRL）在化学应用中的可能性和潜力。methods:这个论文使用的方法包括 Graph Neural Networks (GNNs) 和 ensemble learning，以学习分子表示。results:这个论文的结果表明，直接从可访问的转换空间学习可以提高多种任务和模型的性能。<details>
<summary>Abstract</summary>
Molecular Representation Learning (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While Graph Neural Networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules. In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models. Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.
</details>
<details>
<summary>摘要</summary>
молекулярное представление обучения (MRL) has proven impactful in numerous biochemical applications such as drug discovery and enzyme design. While graph neural networks (GNNs) are effective at learning molecular representations from a 2D molecular graph or a single 3D structure, existing works often overlook the flexible nature of molecules, which continuously interconvert across conformations via chemical bond rotations and minor vibrational perturbations. To better account for molecular flexibility, some recent works formulate MRL as an ensemble learning problem, focusing on explicitly learning from a set of conformer structures. However, most of these studies have limited datasets, tasks, and models. In this work, we introduce the first MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of learning on conformer ensembles and suggest promising research directions. MARCEL includes four datasets covering diverse molecule- and reaction-level properties of chemically diverse molecules including organocatalysts and transition-metal catalysts, extending beyond the scope of common GNN benchmarks that are confined to drug-like molecules. In addition, we conduct a comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D molecular representation learning models, along with two strategies that explicitly incorporate conformer ensembles into 3D MRL models. Our findings reveal that direct learning from an accessible conformer space can improve performance on a variety of tasks and models.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Node-Selection-in-Branch-and-Bound"><a href="#Reinforcement-Learning-for-Node-Selection-in-Branch-and-Bound" class="headerlink" title="Reinforcement Learning for Node Selection in Branch-and-Bound"></a>Reinforcement Learning for Node Selection in Branch-and-Bound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00112">http://arxiv.org/abs/2310.00112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Mattick, Christopher Mutschler</li>
<li>for: 提高 branch and bound 算法的优化性能</li>
<li>methods: 使用强化学习（RL）模型，基于整个搜索树状态来选择节点</li>
<li>results: 在多种复杂问题集上实现了较高质量的节点选择策略，并在时间约束下提高了优化性能和每个节点的效率<details>
<summary>Abstract</summary>
A big challenge in branch and bound lies in identifying the optimal node within the search tree from which to proceed. Current state-of-the-art selectors utilize either hand-crafted ensembles that automatically switch between naive sub-node selectors, or learned node selectors that rely on individual node data. We propose a novel bi-simulation technique that uses reinforcement learning (RL) while considering the entire tree state, rather than just isolated nodes. To achieve this, we train a graph neural network that produces a probability distribution based on the path from the model's root to its ``to-be-selected'' leaves. Modelling node-selection as a probability distribution allows us to train the model using state-of-the-art RL techniques that capture both intrinsic node-quality and node-evaluation costs. Our method induces a high quality node selection policy on a set of varied and complex problem sets, despite only being trained on specially designed, synthetic TSP instances. Experiments on several benchmarks show significant improvements in optimality gap reductions and per-node efficiency under strict time constraints.
</details>
<details>
<summary>摘要</summary>
很大的挑战在分支和约束中是确定最佳节点的搜索树中进行下一步。当前的状态艺术选择器使用 either 手动编写的集合，或者学习节点选择器，它们都依赖于单个节点数据。我们提出了一种新的双 simulations 技术，使用强化学习（RL），而不是只是关注具体节点。为此，我们训练了一个图ael neural network，该模型生成一个基于从模型的根到要选择的叶子节点的路径的概率分布。将节点选择视为概率分布，允许我们使用现代RL技术，包括内在节点质量和节点评估成本。我们的方法在不同和复杂的问题集上实现了高质量的节点选择策略，即使只是在特制的 sintetic TSP 实例上进行训练。在多个标准测试集上，我们的方法可以减少优化差距和每个节点的时间效率，并且在严格的时间限制下达到了显著改善。
</details></li>
</ul>
<hr>
<h2 id="Gradient-and-Uncertainty-Enhanced-Sequential-Sampling-for-Global-Fit"><a href="#Gradient-and-Uncertainty-Enhanced-Sequential-Sampling-for-Global-Fit" class="headerlink" title="Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit"></a>Gradient and Uncertainty Enhanced Sequential Sampling for Global Fit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00110">http://arxiv.org/abs/2310.00110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/svenl13/gale">https://github.com/svenl13/gale</a></li>
<li>paper_authors: Sven Lämmle, Can Bogoclu, Kevin Cremanns, Dirk Roos</li>
<li>for: 本研究旨在提出一种新的随机抽样策略，以提高global fit的准确性和效率。</li>
<li>methods: 本研究使用机器学习技术建立了一个新的随机抽样策略，名为Gradient和Uncertainty Enhanced Sequential Sampling（GUESS）。该策略使用两个因素：预测 posterior uncertainty 和重量加权 Taylor 展开值。</li>
<li>results: 对于26个1到8维确定函数中的26个测试例，GUESS在global fit中平均实现了最高的样本效率，并且在高维度下的行为和模型选择的重要性进行了一个ablation研究。<details>
<summary>Abstract</summary>
Surrogate models based on machine learning methods have become an important part of modern engineering to replace costly computer simulations. The data used for creating a surrogate model are essential for the model accuracy and often restricted due to cost and time constraints. Adaptive sampling strategies have been shown to reduce the number of samples needed to create an accurate model. This paper proposes a new sampling strategy for global fit called Gradient and Uncertainty Enhanced Sequential Sampling (GUESS). The acquisition function uses two terms: the predictive posterior uncertainty of the surrogate model for exploration of unseen regions and a weighted approximation of the second and higher-order Taylor expansion values for exploitation. Although various sampling strategies have been proposed so far, the selection of a suitable method is not trivial. Therefore, we compared our proposed strategy to 9 adaptive sampling strategies for global surrogate modeling, based on 26 different 1 to 8-dimensional deterministic benchmarks functions. Results show that GUESS achieved on average the highest sample efficiency compared to other surrogate-based strategies on the tested examples. An ablation study considering the behavior of GUESS in higher dimensions and the importance of surrogate choice is also presented.
</details>
<details>
<summary>摘要</summary>
现代工程中的代表模型基于机器学习方法已成为重要的一部分，以取代昂贵的计算模拟。创建代表模型所用的数据是模型准确性的关键因素，而这些数据往往因成本和时间限制而受到限制。适应采样策略可以减少创建模型所需的样本数量。本文提出了一种新的采样策略，即梯度和不确定性增强顺序采样（GUESS）。采样函数使用两个项：代表后预测 posterior 不确定性和质量权重Weighted approximation of the second and higher-order Taylor expansion values。虽然过去已经有多种采样策略被提出，但选择合适的方法并不是易事。因此，我们对9种适应采样策略进行了对比，这些策略基于26个不同的1到8维度束缚函数。结果表明，GUESS在测试例子上的平均样本效率高于其他代表模型基于策略。此外，我们还进行了对GUESS在更高维度和代表选择的影响进行了一个减少研究。
</details></li>
</ul>
<hr>
<h2 id="FedAIoT-A-Federated-Learning-Benchmark-for-Artificial-Intelligence-of-Things"><a href="#FedAIoT-A-Federated-Learning-Benchmark-for-Artificial-Intelligence-of-Things" class="headerlink" title="FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things"></a>FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00109">http://arxiv.org/abs/2310.00109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aiot-mlsys-lab/fedaiot">https://github.com/aiot-mlsys-lab/fedaiot</a></li>
<li>paper_authors: Samiul Alam, Tuo Zhang, Tiantian Feng, Hui Shen, Zhichao Cao, Dong Zhao, JeongGil Ko, Kiran Somasundaram, Shrikanth S. Narayanan, Salman Avestimehr, Mi Zhang</li>
<li>for: 本研究的目的是填补现有 Federated Learning (FL) 工作中 dataset 的假设和缺失，通过在 authentic IoT 设备上收集数据，实现更加准确和有用的 FL 模型。</li>
<li>methods: 本研究使用了一种统一的综合 FL 框架，以便对 AIoT 应用场景进行简单的 benchmarking。这种框架包括 eight 个 IoT 设备收集的数据集，这些数据集涵盖了 IoT 特有的模式和目标应用。</li>
<li>results: 本研究的结果表明，FL 在 AIoT 领域具有广泛的应用前景和挑战，FedAIoT 可以作为一个有价值的资源，推动 FL 在 AIoT 领域的进步。<details>
<summary>Abstract</summary>
There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, an FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. FedAIoT also includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope FedAIoT could serve as an invaluable resource to foster advancements in the important field of FL for AIoT. The repository of FedAIoT is maintained at https://github.com/AIoT-MLSys-Lab/FedAIoT.
</details>
<details>
<summary>摘要</summary>
在人工智能物联网（AIoT）领域，联合学习（FL）具有重要的相关性。然而，大多数现有的FL工作都不是基于真实的IoT设备收集的数据集，这些数据集捕捉了特殊的IoT模式和IoT数据的内在挑战。在这项工作中，我们介绍了FedAIoT，一个针对AIoT的FLbenchmark，以填补这一关键的空白。FedAIoT包括8个来自各种IoT设备的数据集，这些数据集覆盖了IoT模式的唯一特征和AIoT应用的代表性。FedAIoT还包括一个简化了AIoTFL框架的统一终端，以便对数据集的性能进行比较。我们的benchmark结果暴露了FL在AIoT中的机会和挑战。我们希望FedAIoT能成为AIoTFL的重要资源，推动这一领域的进步。FedAIoT的存储库位于https://github.com/AIoT-MLSys-Lab/FedAIoT。
</details></li>
</ul>
<hr>
<h2 id="Latent-Space-Symmetry-Discovery"><a href="#Latent-Space-Symmetry-Discovery" class="headerlink" title="Latent Space Symmetry Discovery"></a>Latent Space Symmetry Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00105">http://arxiv.org/abs/2310.00105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiankeyang/LaLiGAN">https://github.com/jiankeyang/LaLiGAN</a></li>
<li>paper_authors: Jianke Yang, Nima Dehmamy, Robin Walters, Rose Yu</li>
<li>for: 这个论文是为了提出一种新的生成模型，即Latent LieGAN（LaLiGAN），可以从数据中自动发现非线性对称性。</li>
<li>methods: LaLiGAN使用了一种新的推理方法，即将数据映射到一个 latent space 中，并在这个 latent space 中同时发现数据的对称性。</li>
<li>results: 实验表明，LaLiGAN 可以成功地捕捉高维观察数据中的内在对称性，从而生成一个具有良好结构的 latent space，这个 latent space 可以用于其他下游任务。例如，LaLiGAN 可以提高Equation Discovery 和 Long-term Forecasting 等任务的性能。<details>
<summary>Abstract</summary>
Equivariant neural networks require explicit knowledge of the symmetry group. Automatic symmetry discovery methods aim to relax this constraint and learn invariance and equivariance from data. However, existing symmetry discovery methods are limited to linear symmetries in their search space and cannot handle the complexity of symmetries in real-world, often high-dimensional data. We propose a novel generative model, Latent LieGAN (LaLiGAN), which can discover nonlinear symmetries from data. It learns a mapping from data to a latent space where the symmetries become linear and simultaneously discovers symmetries in the latent space. Theoretically, we show that our method can express any nonlinear symmetry under certain conditions. Experimentally, our method can capture the intrinsic symmetry in high-dimensional observations, which results in a well-structured latent space that is useful for other downstream tasks. We demonstrate the use cases for LaLiGAN in improving equation discovery and long-term forecasting for various dynamical systems.
</details>
<details>
<summary>摘要</summary>
EQUIVALENT NEURAL NETWORKS REQUIRE EXPLICIT KNOWLEDGE OF THE SYMMETRY GROUP. AUTOMATIC SYMMETRY DISCOVERY METHODS AIM TO RELAX THIS CONSTRAINT AND LEARN INVARIANCE AND EQUIVALENCE FROM DATA. HOWEVER, EXISTING SYMMETRY DISCOVERY METHODS ARE LIMITED TO LINEAR SYMMETRIES IN THEIR SEARCH SPACE AND CANNOT HANDLE THE COMPLEXITY OF SYMMETRIES IN REAL-WORLD, OFTEN HIGH-DIMENSIONAL DATA. WE PROPOSE A NOVEL GENERATIVE MODEL, LATENT LIEGAN (LALIGAN), WHICH CAN DISCOVER NONLINEAR SYMMETRIES FROM DATA. IT LEARNS A MAPPING FROM DATA TO A LATENT SPACE WHERE THE SYMMETRIES BECOME LINEAR AND SIMULTANEOUSLY DISCOVERS SYMMETRIES IN THE LATENT SPACE. THEORETICALLY, WE SHOW THAT OUR METHOD CAN EXPRESS ANY NONLINEAR SYMMETRY UNDER CERTAIN CONDITIONS. EXPERIMENTALLY, OUR METHOD CAN CAPTURE THE INTRINSIC SYMMETRY IN HIGH-DIMENSIONAL OBSERVATIONS, WHICH RESULTS IN A WELL-STRUCTURED LATENT SPACE THAT IS USEFUL FOR OTHER DOWNSTREAM TASKS. WE DEMONSTRATE THE USE CASES FOR LALIGAN IN IMPROVING EQUATION DISCOVERY AND LONG-TERM FORECASTING FOR VARIOUS DYNAMICAL SYSTEMS.
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-with-Differential-Privacy-for-End-to-End-Speech-Recognition"><a href="#Federated-Learning-with-Differential-Privacy-for-End-to-End-Speech-Recognition" class="headerlink" title="Federated Learning with Differential Privacy for End-to-End Speech Recognition"></a>Federated Learning with Differential Privacy for End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00098">http://arxiv.org/abs/2310.00098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Pelikan, Sheikh Shams Azam, Vitaly Feldman, Jan “Honza” Silovsky, Kunal Talwar, Tatiana Likhomanenko</li>
<li>for: 本文旨在应用 federated learning (FL) 技术在自动语音识别 (ASR) 领域，并在不破坏用户隐私的前提下实现 robust 隐私保障。</li>
<li>methods: 本文使用了 differential privacy (DP) 技术来保证用户隐私，并对 Federated Learning (FL) 技术进行了扩展和应用。</li>
<li>results: 本文实现了用户级别 ($7.2$, $10^{-9}$)-DP 和 ($4.5$, $10^{-9}$)-DP 的隐私保障，并且在不同的数据异同和预测范围内实现了nearly optimal的模型训练。<details>
<summary>Abstract</summary>
While federated learning (FL) has recently emerged as a promising approach to train machine learning models, it is limited to only preliminary explorations in the domain of automatic speech recognition (ASR). Moreover, FL does not inherently guarantee user privacy and requires the use of differential privacy (DP) for robust privacy guarantees. However, we are not aware of prior work on applying DP to FL for ASR. In this paper, we aim to bridge this research gap by formulating an ASR benchmark for FL with DP and establishing the first baselines. First, we extend the existing research on FL for ASR by exploring different aspects of recent $\textit{large end-to-end transformer models}$: architecture design, seed models, data heterogeneity, domain shift, and impact of cohort size. With a $\textit{practical}$ number of central aggregations we are able to train $\textbf{FL models}$ that are \textbf{nearly optimal} even with heterogeneous data, a seed model from another domain, or no pre-trained seed model. Second, we apply DP to FL for ASR, which is non-trivial since DP noise severely affects model training, especially for large transformer models, due to highly imbalanced gradients in the attention block. We counteract the adverse effect of DP noise by reviving per-layer clipping and explaining why its effect is more apparent in our case than in the prior work. Remarkably, we achieve user-level ($7.2$, $10^{-9}$)-$\textbf{DP}$ (resp. ($4.5$, $10^{-9}$)-$\textbf{DP}$) with a 1.3% (resp. 4.6%) absolute drop in the word error rate for extrapolation to high (resp. low) population scale for $\textbf{FL with DP in ASR}$.
</details>
<details>
<summary>摘要</summary>
While federated learning (FL) has recently emerged as a promising approach to train machine learning models, it is limited to only preliminary explorations in the domain of automatic speech recognition (ASR). Moreover, FL does not inherently guarantee user privacy and requires the use of differential privacy (DP) for robust privacy guarantees. However, we are not aware of prior work on applying DP to FL for ASR. In this paper, we aim to bridge this research gap by formulating an ASR benchmark for FL with DP and establishing the first baselines. First, we extend the existing research on FL for ASR by exploring different aspects of recent large end-to-end transformer models: architecture design, seed models, data heterogeneity, domain shift, and impact of cohort size. With a practical number of central aggregations, we are able to train FL models that are nearly optimal even with heterogeneous data, a seed model from another domain, or no pre-trained seed model. Second, we apply DP to FL for ASR, which is non-trivial since DP noise severely affects model training, especially for large transformer models, due to highly imbalanced gradients in the attention block. We counteract the adverse effect of DP noise by reviving per-layer clipping and explaining why its effect is more apparent in our case than in the prior work. Remarkably, we achieve user-level ($7.2$, $10^{-9}$)-DP (resp. ($4.5$, $10^{-9}$)-DP) with a 1.3% (resp. 4.6%) absolute drop in the word error rate for extrapolation to high (resp. low) population scale for FL with DP in ASR.
</details></li>
</ul>
<hr>
<h2 id="Optimizing-with-Low-Budgets-a-Comparison-on-the-Black-box-Optimization-Benchmarking-Suite-and-OpenAI-Gym"><a href="#Optimizing-with-Low-Budgets-a-Comparison-on-the-Black-box-Optimization-Benchmarking-Suite-and-OpenAI-Gym" class="headerlink" title="Optimizing with Low Budgets: a Comparison on the Black-box Optimization Benchmarking Suite and OpenAI Gym"></a>Optimizing with Low Budgets: a Comparison on the Black-box Optimization Benchmarking Suite and OpenAI Gym</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00077">http://arxiv.org/abs/2310.00077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elena Raponi, Nathanael Rakotonirina Carraz, Jérémy Rapin, Carola Doerr, Olivier Teytaud</li>
<li>for: 这篇论文主要研究的是黑盒优化（BBO）和机器学习（ML）之间的交互作用，以及这两个领域之间的关系。</li>
<li>methods: 本论文使用了搜索空间中的演进算法和bayesian优化（BO）来对比不同的优化算法。</li>
<li>results: 研究结果表明，BO-based优化器在评估预算有限时表现良好，但在评估预算较大时常常被其他家族的算法所超越。此外，研究还发现了一些来自BBO社区的算法在ML任务上表现出优异的表现。<details>
<summary>Abstract</summary>
The growing ubiquity of machine learning (ML) has led it to enter various areas of computer science, including black-box optimization (BBO). Recent research is particularly concerned with Bayesian optimization (BO). BO-based algorithms are popular in the ML community, as they are used for hyperparameter optimization and more generally for algorithm configuration. However, their efficiency decreases as the dimensionality of the problem and the budget of evaluations increase. Meanwhile, derivative-free optimization methods have evolved independently in the optimization community. Therefore, we urge to understand whether cross-fertilization is possible between the two communities, ML and BBO, i.e., whether algorithms that are heavily used in ML also work well in BBO and vice versa. Comparative experiments often involve rather small benchmarks and show visible problems in the experimental setup, such as poor initialization of baselines, overfitting due to problem-specific setting of hyperparameters, and low statistical significance.   With this paper, we update and extend a comparative study presented by Hutter et al. in 2013. We compare BBO tools for ML with more classical heuristics, first on the well-known BBOB benchmark suite from the COCO environment and then on Direct Policy Search for OpenAI Gym, a reinforcement learning benchmark. Our results confirm that BO-based optimizers perform well on both benchmarks when budgets are limited, albeit with a higher computational cost, while they are often outperformed by algorithms from other families when the evaluation budget becomes larger. We also show that some algorithms from the BBO community perform surprisingly well on ML tasks.
</details>
<details>
<summary>摘要</summary>
machine learning（ML）的普遍使得它进入了计算机科学中的各个领域，包括黑盒优化（BBO）。最近的研究特别关注于概率优化（BO）。BO基本算法在机器学习社区中很受欢迎，因为它们用于超参数优化和更一般地用于算法配置。然而，随着问题维度和评估预算的增加，BO基本算法的效率会降低。同时，不需要导数的优化方法在优化社区中发展了独立。因此，我们需要了解是否可以在这两个社区之间进行交叉推导，即机器学习中广泛使用的算法是否也适用于BBO，并且vice versa。 Comparative experiments often involve small benchmarks and show visible problems in the experimental setup, such as poor initialization of baselines, overfitting due to problem-specific setting of hyperparameters, and low statistical significance.  With this paper, we update and extend a comparative study presented by Hutter et al. in 2013. We compare BBO tools for ML with more classical heuristics, first on the well-known BBOB benchmark suite from the COCO environment and then on Direct Policy Search for OpenAI Gym, a reinforcement learning benchmark. Our results confirm that BO-based optimizers perform well on both benchmarks when budgets are limited, albeit with a higher computational cost, while they are often outperformed by algorithms from other families when the evaluation budget becomes larger. We also show that some algorithms from the BBO community perform surprisingly well on ML tasks.
</details></li>
</ul>
<hr>
<h2 id="EPiC-ly-Fast-Particle-Cloud-Generation-with-Flow-Matching-and-Diffusion"><a href="#EPiC-ly-Fast-Particle-Cloud-Generation-with-Flow-Matching-and-Diffusion" class="headerlink" title="EPiC-ly Fast Particle Cloud Generation with Flow-Matching and Diffusion"></a>EPiC-ly Fast Particle Cloud Generation with Flow-Matching and Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00049">http://arxiv.org/abs/2310.00049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uhh-pd-ml/EPiC-FM">https://github.com/uhh-pd-ml/EPiC-FM</a></li>
<li>paper_authors: Erik Buhmann, Cedric Ewen, Darius A. Faroughy, Tobias Golling, Gregor Kasieczka, Matthew Leigh, Guillaume Quétant, John Andrew Raine, Debajyoti Sengupta, David Shih</li>
<li>for: This paper is written for researchers and practitioners in the field of particle physics and generative modeling. The authors aim to provide two novel methods for generating LHC jets as point clouds, which can be used for a variety of applications such as particle physics experiments and simulations.</li>
<li>methods: The paper introduces two novel methods for generating LHC jets as point clouds: \epcjedi and \epcfm. \epcjedi combines score-matching diffusion models with the Equivariant Point Cloud (EPiC) architecture based on the deep sets framework, while \epcfm is the first permutation equivariant continuous normalizing flow (CNF) for particle cloud generation. Both methods are trained using the flow-matching objective, which is a scalable and easy-to-train objective based on optimal transport.</li>
<li>results: The authors demonstrate that both \epcjedi and \epcfm achieve state-of-the-art performance on the top-quark JetNet datasets while maintaining fast generation speed. Specifically, \epcfm consistently outperforms all the other generative models considered in the paper across every metric. Additionally, the authors introduce two new particle cloud performance metrics: one based on the Kullback-Leibler divergence between feature distributions, and the other is the negative log-posterior of a multi-model ParticleNet classifier.<details>
<summary>Abstract</summary>
Jets at the LHC, typically consisting of a large number of highly correlated particles, are a fascinating laboratory for deep generative modeling. In this paper, we present two novel methods that generate LHC jets as point clouds efficiently and accurately. We introduce \epcjedi, which combines score-matching diffusion models with the Equivariant Point Cloud (EPiC) architecture based on the deep sets framework. This model offers a much faster alternative to previous transformer-based diffusion models without reducing the quality of the generated jets. In addition, we introduce \epcfm, the first permutation equivariant continuous normalizing flow (CNF) for particle cloud generation. This model is trained with {\it flow-matching}, a scalable and easy-to-train objective based on optimal transport that directly regresses the vector fields connecting the Gaussian noise prior to the data distribution. Our experiments demonstrate that \epcjedi and \epcfm both achieve state-of-the-art performance on the top-quark JetNet datasets whilst maintaining fast generation speed. Most notably, we find that the \epcfm model consistently outperforms all the other generative models considered here across every metric. Finally, we also introduce two new particle cloud performance metrics: the first based on the Kullback-Leibler divergence between feature distributions, the second is the negative log-posterior of a multi-model ParticleNet classifier.
</details>
<details>
<summary>摘要</summary>
各种jets在LHC中，通常是一大量高度相关的粒子，是一个非常有趣的实验室，用于深入的生成模型。在这篇论文中，我们介绍了两种新的方法，可以高效精准地生成LHC jets作为点云。我们引入了\epcjedi，它将得分匹配扩散模型和深度集成（EPiC）架构与深度集成框架（deep sets）结合。这个模型比前一代基于转移器的扩散模型更快，而且没有降低生成jets的质量。此外，我们引入了\epcfm，第一个具有对称性的连续正常化流（CNF），用于粒子云生成。这个模型通过可扩展的和易于训练的对流匹配目标训练，直接将泊松噪声前提中的高斯噪声连接到数据分布。我们的实验表明，\epcjedi和\epcfm都达到了JetNet数据集的顶峰性能，而且保持了快速的生成速度。尤其是，我们发现\epcfm模型在所有考虑的生成模型中一直保持领先，在每一个纪录中都表现出优异的性能。最后，我们还介绍了两种新的粒子云性能指标：第一个基于粒子分布的卷积-莱布勒偏移，第二个是一个多模型ParticleNet分类器的负极对数 posterior。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Clifford-invariants-of-ADE-Coxeter-elements"><a href="#Machine-Learning-Clifford-invariants-of-ADE-Coxeter-elements" class="headerlink" title="Machine Learning Clifford invariants of ADE Coxeter elements"></a>Machine Learning Clifford invariants of ADE Coxeter elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00041">http://arxiv.org/abs/2310.00041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dimadroid/ml_clifford_invariants">https://github.com/dimadroid/ml_clifford_invariants</a></li>
<li>paper_authors: Siqi Chen, Pierre-Philippe Dechant, Yang-Hui He, Elli Heyes, Edward Hirst, Dmitrii Riabchenko</li>
<li>for: 这个论文是关于新的克利福德几何 invariants的研究，这些 invariants 是用于 linear transformations 的 novel 的几何 invariants。</li>
<li>methods: 这个论文使用了高性能计算的计算代数方法，对 $A_8$, $D_8$ 和 $E_8$ 的 Coxeter transformations 进行了exhaustive 的计算，并使用了数据科学技术such as supervised 和 unsupervised machine learning。</li>
<li>results: 这个论文发现了这些 Clifford algebraic datasets 可以高度精度地被机器学习，并且提供了一些新的 geometric invariants 和其他已知的几何 invariants 之间的关系。<details>
<summary>Abstract</summary>
There has been recent interest in novel Clifford geometric invariants of linear transformations. This motivates the investigation of such invariants for a certain type of geometric transformation of interest in the context of root systems, reflection groups, Lie groups and Lie algebras: the Coxeter transformations. We perform exhaustive calculations of all Coxeter transformations for $A_8$, $D_8$ and $E_8$ for a choice of basis of simple roots and compute their invariants, using high-performance computing. This computational algebra paradigm generates a dataset that can then be mined using techniques from data science such as supervised and unsupervised machine learning. In this paper we focus on neural network classification and principal component analysis. Since the output -- the invariants -- is fully determined by the choice of simple roots and the permutation order of the corresponding reflections in the Coxeter element, we expect huge degeneracy in the mapping. This provides the perfect setup for machine learning, and indeed we see that the datasets can be machine learned to very high accuracy. This paper is a pump-priming study in experimental mathematics using Clifford algebras, showing that such Clifford algebraic datasets are amenable to machine learning, and shedding light on relationships between these novel and other well-known geometric invariants and also giving rise to analytic results.
</details>
<details>
<summary>摘要</summary>
有最近的兴趣在新的Clifford геометрических invariants of linear transformations。这种 motivates the investigation of such invariants for a certain type of geometric transformation of interest in the context of root systems, reflection groups, Lie groups and Lie algebras：Coxeter transformations。我们进行了所有Coxeter transformations for $A_8$, $D_8$ and $E_8$ 的详细计算，使用高性能计算机，并计算了他们的 invariants，使用数据科学技术 such as supervised and unsupervised machine learning。这个计算代数 paradigm generates a dataset that can then be mined using techniques from data science such as supervised and unsupervised machine learning。在这篇论文中，我们主要关注神经网络分类和主成分分析。因为输出—— invariants 完全取决于选择的简单根和相应的反射的 permutation order in the Coxeter element，我们预计存在巨大的杂化在映射中。这提供了完美的设置 для机器学习，并确实发现 datasets 可以通过非常高的准确率进行机器学习。这篇论文是一个实验数学的燃料研究，证明 Clifford 代数数据可以被机器学习，并暴露了这些新的和其他已知的 геомétric invariants之间的关系，并且生成了分析结果。
</details></li>
</ul>
<hr>
<h2 id="Networked-Inequality-Preferential-Attachment-Bias-in-Graph-Neural-Network-Link-Prediction"><a href="#Networked-Inequality-Preferential-Attachment-Bias-in-Graph-Neural-Network-Link-Prediction" class="headerlink" title="Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction"></a>Networked Inequality: Preferential Attachment Bias in Graph Neural Network Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17417">http://arxiv.org/abs/2309.17417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arjunsubramonian/link_bias_amplification">https://github.com/arjunsubramonian/link_bias_amplification</a></li>
<li>paper_authors: Arjun Subramonian, Levent Sagun, Yizhou Sun</li>
<li>for: 本研究探讨了图 neural network（GNN）链接预测在引用、协作和在线社交网络中的应用，以及链接预测中的内部公平性和“富豪征”现象。</li>
<li>methods: 本研究使用了图 convolutional neural network（GCN）和正则化技术来预测链接，并对GCN的偏好附加进行了理论分析。</li>
<li>results: 研究发现，GCN采用正则化技术可以减少链接预测中的内部不公平性，并提出了一种新的内部公平度指标来衡量链接预测 scores 之间的差异。<details>
<summary>Abstract</summary>
Graph neural network (GNN) link prediction is increasingly deployed in citation, collaboration, and online social networks to recommend academic literature, collaborators, and friends. While prior research has investigated the dyadic fairness of GNN link prediction, the within-group fairness and ``rich get richer'' dynamics of link prediction remain underexplored. However, these aspects have significant consequences for degree and power imbalances in networks. In this paper, we shed light on how degree bias in networks affects Graph Convolutional Network (GCN) link prediction. In particular, we theoretically uncover that GCNs with a symmetric normalized graph filter have a within-group preferential attachment bias. We validate our theoretical analysis on real-world citation, collaboration, and online social networks. We further bridge GCN's preferential attachment bias with unfairness in link prediction and propose a new within-group fairness metric. This metric quantifies disparities in link prediction scores between social groups, towards combating the amplification of degree and power disparities. Finally, we propose a simple training-time strategy to alleviate within-group unfairness, and we show that it is effective on citation, online social, and credit networks.
</details>
<details>
<summary>摘要</summary>
graph neural network（GNN）链接预测在引用、协作和社交网络中被越来越广泛应用，以推荐学术论文、合作伙伴和朋友。然而，之前的研究主要关注了GNN链接预测的对称公平性，而内部公平和“富豪投资”的动态尚未得到足够的研究。然而，这些方面对于网络中的度和权力差异有着重要的影响。在这篇论文中，我们探讨了网络中度偏袋的影响于图 convolutional neural network（GCN）链接预测。具体来说，我们经过理论分析发现，GCNs具有对称正规图滤波器时有内部偏袋预测偏好。我们在实际的引用、协作和社交网络上验证了我们的理论分析。此外，我们将GCN的偏袋偏好与链接预测不公平性相连接，并提出了一种新的内部公平度量。这种度量可以衡量链接预测得分之间的社会组别差异，以遏止度和权力差异的扩大。最后，我们提出了一种简单的训练时间策略来缓解内部不公平性，并在引用、社交和借款网络上验证了其效果。
</details></li>
</ul>
<hr>
<h2 id="Cleanba-A-Reproducible-and-Efficient-Distributed-Reinforcement-Learning-Platform"><a href="#Cleanba-A-Reproducible-and-Efficient-Distributed-Reinforcement-Learning-Platform" class="headerlink" title="Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform"></a>Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00036">http://arxiv.org/abs/2310.00036</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vwxyzjn/cleanba">https://github.com/vwxyzjn/cleanba</a></li>
<li>paper_authors: Shengyi Huang, Jiayi Weng, Rujikorn Charakorn, Min Lin, Zhongwen Xu, Santiago Ontañón</li>
<li>for: 提高自主代理的训练效率，使用更多计算资源和更少的训练时间。</li>
<li>methods: 提出了一个新的开源平台cleanba，该平台提供了高度可重现的actor-learner框架，并实现了高度优化的分布式PPO和IMPALA算法。</li>
<li>results: 在Atari游戏中，cleanba variants可以与强IMPALA基线和PPO基eline匹配或超越，同时具有较短的训练时间和更高的可重现性。<details>
<summary>Abstract</summary>
Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner framework can have reproducibility issues even if hyperparameters are controlled. We then introduce Cleanba, a new open-source platform for distributed DRL that proposes a highly reproducible architecture. Cleanba implements highly optimized distributed variants of PPO and IMPALA. Our Atari experiments show that these variants can obtain equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL. However, Cleanba variants present 1) shorter training time and 2) more reproducible learning curves in different hardware settings. Cleanba's source code is available at \url{https://github.com/vwxyzjn/cleanba}
</details>
<details>
<summary>摘要</summary>
分布式深度强化学习（DRL）目的是利用更多计算资源来训练自主代理人，减少训练时间。尽管当前领域已经取得了一定进步，但是复现性问题还没有得到充分探讨。这篇论文首先显示了通常的演员学习框架可能会存在复现性问题，即使控制了超参数。然后，我们介绍了Cleanba，一个新的开源平台 для分布式DRL，该平台提出了高度复制cible的架构。Cleanba实现了高度优化的分布式PPO和IMPALA算法。我们在Atari游戏中进行了实验，发现Cleanba变体可以与强大的IMPALA基线在moolib和torchbeast中获得相同或更高的得分，并且在不同硬件设置下可以 obtaint 1) 训练时间更短和 2) 更加复制cible的学习曲线。Cleanba的源代码可以在以下链接中获取：https://github.com/vwxyzjn/cleanba。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Volume-Matrix-Cross-Approximation-for-Image-Compression-and-Least-Squares-Solution"><a href="#Maximal-Volume-Matrix-Cross-Approximation-for-Image-Compression-and-Least-Squares-Solution" class="headerlink" title="Maximal Volume Matrix Cross Approximation for Image Compression and Least Squares Solution"></a>Maximal Volume Matrix Cross Approximation for Image Compression and Least Squares Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17403">http://arxiv.org/abs/2309.17403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenneth Allen, Ming-Jun Lai, Zhaiming Shen</li>
<li>for: 这 paper 的主要目标是研究基于最大体积子matrix的矩阵减少方法。</li>
<li>methods: 这 paper 使用了一种新的证明 classic estimate 的方法，以及一种基于最大体积子matrix的迪kins algorithms。</li>
<li>results: 这 paper 提出了一种基于最大体积子matrix的矩阵减少方法，并且提供了许多实际应用，如图像压缩和连续函数最小二乘approximation。numerical results 表明这种方法的效果非常好。<details>
<summary>Abstract</summary>
We study the classic cross approximation of matrices based on the maximal volume submatrices. Our main results consist of an improvement of a classic estimate for matrix cross approximation and a greedy approach for finding the maximal volume submatrices. Indeed, we present a new proof of a classic estimate of the inequality with an improved constant. Also, we present a family of greedy maximal volume algorithms which improve the error bound of cross approximation of a matrix in the Chebyshev norm and also improve the computational efficiency of classic maximal volume algorithm. The proposed algorithms are shown to have theoretical guarantees of convergence. Finally, we present two applications: one is image compression and the other is least squares approximation of continuous functions. Our numerical results in the end of the paper demonstrate the effective performances of our approach.
</details>
<details>
<summary>摘要</summary>
我们研究基于最大体积子矩阵的经典拟合矩阵方法。我们的主要结果包括对经典估计的改进和一种新的规范最大体积子矩阵算法。首先，我们提供一个新的证明，证明了经典估计的不等式具有改进的常数。其次，我们提出了一种家族的排序最大体积子矩阵算法，这些算法可以在Chebychev范数中提高矩阵拟合的误差 bound，同时也可以提高经典最大体积子矩阵算法的计算效率。这些算法具有理论保证的收敛性。最后，我们介绍了两个应用：一个是图像压缩，另一个是continue函数最小二乘approximation。我们的数值结果在文章的末尾展示了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="AV-CPL-Continuous-Pseudo-Labeling-for-Audio-Visual-Speech-Recognition"><a href="#AV-CPL-Continuous-Pseudo-Labeling-for-Audio-Visual-Speech-Recognition" class="headerlink" title="AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition"></a>AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17395">http://arxiv.org/abs/2309.17395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Rouditchenko, Ronan Collobert, Tatiana Likhomanenko</li>
<li>for: 这个论文是为了提出一种Audio-visual speech recognition（AVSR）模型的训练方法，该方法可以在 audio-visual 输入中进行语音识别，同时可以使用单个模式进行语音识别。</li>
<li>methods: 这个论文使用了 Continuous Pseudo-labeling（CPL）方法，该方法通过将带有标签的视频与无标签视频混合，生成了pseudo-labels，并使用这些pseudo-labels来训练AVSR模型。</li>
<li>results: 该论文在LRS3 dataset上 obtainted significant improvements in Visual Speech Recognition（VSR）性能，同时保持了实用的Audio-Visual Speech Recognition（ASR）和AVSR性能。此外，使用没有标签的视频数据，该方法还能够利用无标签视频来提高VSR性能。<details>
<summary>Abstract</summary>
Audio-visual speech contains synchronized audio and visual information that provides cross-modal supervision to learn representations for both automatic speech recognition (ASR) and visual speech recognition (VSR). We introduce continuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a semi-supervised method to train an audio-visual speech recognition (AVSR) model on a combination of labeled and unlabeled videos with continuously regenerated pseudo-labels. Our models are trained for speech recognition from audio-visual inputs and can perform speech recognition using both audio and visual modalities, or only one modality. Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels. AV-CPL obtains significant improvements in VSR performance on the LRS3 dataset while maintaining practical ASR and AVSR performance. Finally, using visual-only speech data, our method is able to leverage unlabeled visual speech to improve VSR.
</details>
<details>
<summary>摘要</summary>
audio-visual speech contains synchronized audio and visual information, providing cross-modal supervision to learn representations for both automatic speech recognition (ASR) and visual speech recognition (VSR). We introduce continuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a semi-supervised method to train an audio-visual speech recognition (AVSR) model on a combination of labeled and unlabeled videos with continuously regenerated pseudo-labels. Our models are trained for speech recognition from audio-visual inputs and can perform speech recognition using both audio and visual modalities, or only one modality. Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels. AV-CPL obtains significant improvements in VSR performance on the LRS3 dataset while maintaining practical ASR and AVSR performance. Finally, using visual-only speech data, our method is able to leverage unlabeled visual speech to improve VSR.Here's the translation in Traditional Chinese:Audio-visual speech 包含同步的音频和视讯信息，提供跨Modal 的监控，以learn representation for both automatic speech recognition (ASR) 和 visual speech recognition (VSR). We introduce continuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a semi-supervised method to train an audio-visual speech recognition (AVSR) model on a combination of labeled and unlabeled videos with continuously regenerated pseudo-labels. Our models are trained for speech recognition from audio-visual inputs and can perform speech recognition using both audio and visual modalities, or only one modality. Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels. AV-CPL obtains significant improvements in VSR performance on the LRS3 dataset while maintaining practical ASR and AVSR performance. Finally, using visual-only speech data, our method is able to leverage unlabeled visual speech to improve VSR.
</details></li>
</ul>
<hr>
<h2 id="Tree-Cross-Attention"><a href="#Tree-Cross-Attention" class="headerlink" title="Tree Cross Attention"></a>Tree Cross Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17388">http://arxiv.org/abs/2309.17388</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danderfer/Comp_Sci_Sem_2">https://github.com/danderfer/Comp_Sci_Sem_2</a></li>
<li>paper_authors: Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Yoshua Bengio, Mohamed Osama Ahmed</li>
<li>for: 这个论文是为了提高减少语言模型在预测时的字符数量而设计的。</li>
<li>methods: 这个论文使用了一种名为Tree Cross Attention（TCA）的模块，它只在预测时从logs（N）中选择一个数量较少的token进行预测。</li>
<li>results: 论文的实验结果表明，使用TCA模块可以与cross attention相比，在不同的分类和uncertainty regression任务中表现相似，而且在使用相同数量的token时，有显著的提升。<details>
<summary>Abstract</summary>
Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of $\mathcal{O}(N)$ tokens. In practice, however, often only a small subset of tokens are required for good performance. Methods such as Perceiver IO are cheap at inference as they distill the information to a smaller-sized set of latent tokens $L < N$ on which cross attention is then applied, resulting in only $\mathcal{O}(L)$ complexity. However, in practice, as the number of input tokens and the amount of information to distill increases, the number of latent tokens needed also increases significantly. In this work, we propose Tree Cross Attention (TCA) - a module based on Cross Attention that only retrieves information from a logarithmic $\mathcal{O}(\log(N))$ number of tokens for performing inference. TCA organizes the data in a tree structure and performs a tree search at inference time to retrieve the relevant tokens for prediction. Leveraging TCA, we introduce ReTreever, a flexible architecture for token-efficient inference. We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention across various classification and uncertainty regression tasks while being significantly more token-efficient. Furthermore, we compare ReTreever against Perceiver IO, showing significant gains while using the same number of tokens for inference.
</details>
<details>
<summary>摘要</summary>
cross attention是一种广泛使用的方法，用于从 Kontext 字符串中提取信息，以便进行预测。在推理时，对于每个预测，cross attention会扫描所有的 $\mathcal{O}(N)$ 字符串。在实践中， however，经常只需要一小 subsets of tokens 来获得良好的性能。例如，使用 Perceiver IO 可以在推理时将信息简化为一个更小的精度字符串 $L < N$ 上，从而实现只有 $\mathcal{O}(L)$ 的复杂度。然而，随着输入字符串的数量和信息的总量的增加，需要的精度字符串数量也会增加显著。在这种情况下，我们提出了 Tree Cross Attention（TCA）模块，它只需要在推理时对一个对数 $\mathcal{O}(\log(N))$ 的字符串进行搜索，以便进行预测。TCA 将数据组织成树结构，并在推理时进行树搜索，以 retrieve 需要的信息。基于 TCA，我们介绍了 ReTreever，一种可以进行token-efficient 的架构。我们通过实验表明，Tree Cross Attention（TCA）与 Cross Attention 在不同的分类和不确定度 regression 任务中表现相似，而且在使用相同数量的字符串进行推理时，Token 效率明显更高。此外，我们还对 ReTreever 与 Perceiver IO 进行了比较，发现它们在使用相同数量的字符串时表现出了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Computation-of-Multi-Slice-Clustering-of-Third-Order-Tensors"><a href="#Parallel-Computation-of-Multi-Slice-Clustering-of-Third-Order-Tensors" class="headerlink" title="Parallel Computation of Multi-Slice Clustering of Third-Order Tensors"></a>Parallel Computation of Multi-Slice Clustering of Third-Order Tensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17383">http://arxiv.org/abs/2309.17383</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ANDRIANTSIORY/MSC_parallel">https://github.com/ANDRIANTSIORY/MSC_parallel</a></li>
<li>paper_authors: Dina Faneva Andriantsiory, Camille Coti, Joseph Ben Geloun, Mustapha Lebbah</li>
<li>for: 该论文目的是提出并实现一种并行计算多方向集群（Multi-Slice Clustering，MSC）算法，以处理巨量数据的挑战。</li>
<li>methods: 该论文使用了分布式存储系统和并行计算方法来实现MSC算法，包括spectral analysis of tensor slices和独立进行每个tensor mode的计算。</li>
<li>results: 论文表明，使用并行计算方法可以超越串行计算方法，并且可以扩展MSC算法的缩放性。<details>
<summary>Abstract</summary>
Machine Learning approaches like clustering methods deal with massive datasets that present an increasing challenge. We devise parallel algorithms to compute the Multi-Slice Clustering (MSC) for 3rd-order tensors. The MSC method is based on spectral analysis of the tensor slices and works independently on each tensor mode. Such features fit well in the parallel paradigm via a distributed memory system. We show that our parallel scheme outperforms sequential computing and allows for the scalability of the MSC method.
</details>
<details>
<summary>摘要</summary>
机器学习方法如聚集方法面临着庞大数据集的增加挑战。我们设计了并行算法计算多slice clustering（MSC） для三阶tensor。MSC方法基于tensor扫描的спектраль分析，并在每个tensor模式上独立工作。这些特征适合并行парадигмы，通过分布式存储系统实现。我们展示了我们的并行方案比Sequential计算更高效，并允许MSC方法的扩展。Note:* 机器学习（Machine Learning）用于描述使用计算机algorithms来学习和分类数据的过程。* 聚集方法（clustering methods）是一种常用的机器学习方法，用于将数据分成不同的群体。* 多slice clustering（MSC）是一种基于tensor的聚集方法，用于分析高维数据。* spectral analysis是一种数学分析方法，用于分析函数或数据的特征和结构。* 并行算法（parallel algorithms）是一种使用多个计算机或处理器来实现计算任务的方法。* 分布式存储系统（distributed storage system）是一种将数据分布在多个计算机或存储设备上的存储系统。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Imitation-Learning-from-Visual-Observations-using-Latent-Information"><a href="#Adversarial-Imitation-Learning-from-Visual-Observations-using-Latent-Information" class="headerlink" title="Adversarial Imitation Learning from Visual Observations using Latent Information"></a>Adversarial Imitation Learning from Visual Observations using Latent Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17371">http://arxiv.org/abs/2309.17371</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vittoriogiammarino/ail_from_visual_obs">https://github.com/vittoriogiammarino/ail_from_visual_obs</a></li>
<li>paper_authors: Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis</li>
<li>for: 本文主要针对视觉观察下的模仿学习问题，学习代理人没有直接观察到专家行为，只有视频记录。</li>
<li>methods: 本文提出了一种名为“伪装对抗学习从观察”的算法，结合了伪装学习技术和学习agent的状态表示。</li>
<li>results: 在高维连续 робо护任务上，该算法可以达到状态艺术水平，同时提供了显著的计算优势。此外，该方法还可以改善来自像素的奖励学习效率。<details>
<summary>Abstract</summary>
We focus on the problem of imitation learning from visual observations, where the learning agent has access to videos of experts as its sole learning source. The challenges of this framework include the absence of expert actions and the partial observability of the environment, as the ground-truth states can only be inferred from pixels. To tackle this problem, we first conduct a theoretical analysis of imitation learning in partially observable environments. We establish upper bounds on the suboptimality of the learning agent with respect to the divergence between the expert and the agent latent state-transition distributions. Motivated by this analysis, we introduce an algorithm called Latent Adversarial Imitation from Observations, which combines off-policy adversarial imitation techniques with a learned latent representation of the agent's state from sequences of observations. In experiments on high-dimensional continuous robotic tasks, we show that our algorithm matches state-of-the-art performance while providing significant computational advantages. Additionally, we show how our method can be used to improve the efficiency of reinforcement learning from pixels by leveraging expert videos. To ensure reproducibility, we provide free access to our code.
</details>
<details>
<summary>摘要</summary>
我们集中精力于复制学习从视觉观察中，具体是学习代理人从专家短片中学习。这个框架面临的挑战包括专家动作的缺失和环境的侧面不可观测，因为真实的状态只能从像素推导出来。为解决这个问题，我们首先进行了对半可观察环境中的复制学习的理论分析。我们建立了对代理人对于专家和代理人内部状态转换分布的不准确性的上限。受这一分析的激励，我们提出了叫做“伪函数对抗复制”的算法，它结合了不同策略的对抗复制技术和学习的伪函数表示。在高维度连续控制任务上进行了实验，我们发现了我们的算法可以与现有的表现相匹配，并且提供了重要的计算优势。此外，我们还说明了我们的方法可以帮助从像素学习更加有效率。为保持可重现性，我们提供了免费的代码。
</details></li>
</ul>
<hr>
<h2 id="Graph-based-Neural-Weather-Prediction-for-Limited-Area-Modeling"><a href="#Graph-based-Neural-Weather-Prediction-for-Limited-Area-Modeling" class="headerlink" title="Graph-based Neural Weather Prediction for Limited Area Modeling"></a>Graph-based Neural Weather Prediction for Limited Area Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17370">http://arxiv.org/abs/2309.17370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joeloskarsson/neural-lam">https://github.com/joeloskarsson/neural-lam</a></li>
<li>paper_authors: Joel Oskarsson, Tomas Landelius, Fredrik Lindsten</li>
<li>for: 本研究旨在应用神经网络预测方法来进行局部区域天气预报。</li>
<li>methods: 本研究使用了一种基于图形的神经网络预测方法，并提出了一个多层次嵌入式模型扩展。</li>
<li>results: 实验结果显示，本方法可以有效地应用于局部区域天气预报，并且能够提供高分辨率的预测。<details>
<summary>Abstract</summary>
The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
</details>
<details>
<summary>摘要</summary>
“精准机器学习方法的气象预测技术发展，开创了新的气象模拟可能性。在气候变化时代，高分辨率预测模型的存在也变得越来越重要。大多数现有的神经网络气象预测（NeurWP）方法都集中在全球预测，关键问题是如何将这些技术应用到有限区域模型中。在这种工作中，我们将图基的NeuWP方法应用到有限区域设置中，并提出了多尺度层次模型扩展。我们的方法通过对北欧地区的本地模型进行实验来验证。”Note: Simplified Chinese is used here, as it is the most commonly used version of Chinese in mainland China and is more widely understood. However, if you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="Module-wise-Training-of-Neural-Networks-via-the-Minimizing-Movement-Scheme"><a href="#Module-wise-Training-of-Neural-Networks-via-the-Minimizing-Movement-Scheme" class="headerlink" title="Module-wise Training of Neural Networks via the Minimizing Movement Scheme"></a>Module-wise Training of Neural Networks via the Minimizing Movement Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17357">http://arxiv.org/abs/2309.17357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Skander Karkar, Ibrahim Ayed, Emmanuel de Bézenac, Patrick Gallinari</li>
<li>for: 这篇论文是关于对内存有限的设定下进行对应顶层神经网络训练，并且避免了一些终端 backwards propagation 问题。</li>
<li>methods: 这篇论文提出了一种模组化的训练方法，叫做 TRGL（Transport Regularized Greedy Learning），这种方法运用了分布空间内的运动调整方法来缓和权值网络的过滤问题。</li>
<li>results: 实验结果显示，当加入了这种调整方法时，模组化训练可以提高模型的准确度，并且比其他模组化训练方法和终端训练方法更好，尤其是在内存有限的情况下。<details>
<summary>Abstract</summary>
Greedy layer-wise or module-wise training of neural networks is compelling in constrained and on-device settings where memory is limited, as it circumvents a number of problems of end-to-end back-propagation. However, it suffers from a stagnation problem, whereby early layers overfit and deeper layers stop increasing the test accuracy after a certain depth. We propose to solve this issue by introducing a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. We call the method TRGL for Transport Regularized Greedy Learning and study it theoretically, proving that it leads to greedy modules that are regular and that progressively solve the task. Experimentally, we show improved accuracy of module-wise training of various architectures such as ResNets, Transformers and VGG, when our regularization is added, superior to that of other module-wise training methods and often to end-to-end training, with as much as 60% less memory usage.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>吃善层或模块层wise训练神经网络是在受限制的设备上训练的一种吸引人的方法，因为它 circumvents 许多终到端反向传播问题。然而，它会遇到一个停滞问题，其中早期层过拟合，深度更多的层会在某个深度下停止提高测试准确率。我们提议解决这个问题，通过引入模块层wise regularization，我们称之为TRGL（Transport Regularized Greedy Learning）。我们 theoretically 研究了这种方法，证明它会导致规范的模块，逐渐解决任务。实验ally，我们发现在不同的架构，如ResNets、Transformers和VGG等，当我们的规范加入时，模块层wise 训练的准确率会提高，超过其他模块层wise 训练方法，并且经常高于端到端训练。此外，我们发现在60%的内存使用量下，我们的方法可以达到类似的准确率。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Biologically-Plausible-Adversarial-Training"><a href="#Efficient-Biologically-Plausible-Adversarial-Training" class="headerlink" title="Efficient Biologically Plausible Adversarial Training"></a>Efficient Biologically Plausible Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17348">http://arxiv.org/abs/2309.17348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IBM/Efficient-Biologically-Plausible-Adversarial-Training">https://github.com/IBM/Efficient-Biologically-Plausible-Adversarial-Training</a></li>
<li>paper_authors: Matilde Tristany Farinha, Thomas Ortner, Giorgia Dellaferrera, Benjamin Grewe, Angeliki Pantazi</li>
<li>for: 这个论文的目的是比较人工神经网络和生物学可能的学习算法在对抗黑客攻击方面的性能。</li>
<li>methods: 这个论文使用的方法包括人工神经网络和生物学可能的学习算法，以及对这些算法进行对抗黑客攻击的训练。</li>
<li>results: 研究结果表明，使用生物学可能的学习算法PEPITA可以提高对抗黑客攻击的性能，并且在不同的计算机视觉任务上具有更好的自然vs黑客性能质量比。<details>
<summary>Abstract</summary>
Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and Present the Error to Perturb the Input To modulate Activity (PEPITA), a recently proposed biologically-plausible learning algorithm, on various computer vision tasks. We observe that PEPITA has higher intrinsic adversarial robustness and, with adversarial training, has a more favourable natural-vs-adversarial performance trade-off as, for the same natural accuracies, PEPITA's adversarial accuracies decrease in average by 0.26% and BP's by 8.05%.
</details>
<details>
<summary>摘要</summary>
人工神经网络（ANNs）通过反射储存（BP）显示了惊人的性能，并在日常任务中越来越常用。然而，ANNs受到针对性攻击的威胁，这些攻击通过小量目标干扰来干扰模型的性能。为了使ANNs具有鲁棒性，可以通过对训练集进行反向储存训练来提高模型的鲁棒性。然而，这种方法带来了增加的训练复杂度，因为生成针对性攻击样本需要大量计算资源。与此相反，人类不受针对性攻击的威胁。因此，在这项工作中，我们调查了使用生物可能的学习算法是否比BP更鲁棒 against针对性攻击。特别是，我们对BP和Recently proposed的生物可能的学习算法Present the Error to Perturb the Input To modulate Activity（PEPITA）在多种计算机视觉任务上进行了广泛的比较分析。我们发现，PEPITA具有更高的内在鲁棒性，并且在受到针对性攻击的情况下，与BP相比，PEPITA的自然vs针对性性能质量更好，即在同一个自然准确率下，PEPITA的针对性准确率平均下降0.26%，而BP的下降8.05%。
</details></li>
</ul>
<hr>
<h2 id="Outage-Watch-Early-Prediction-of-Outages-using-Extreme-Event-Regularizer"><a href="#Outage-Watch-Early-Prediction-of-Outages-using-Extreme-Event-Regularizer" class="headerlink" title="Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer"></a>Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17340">http://arxiv.org/abs/2309.17340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubham Agarwal, Sarthak Chakraborty, Shaddy Garg, Sumit Bisht, Chahat Jain, Ashritha Gonuguntla, Shiv Saini</li>
<li>for: 提高云服务的可靠性和诊断灵活性</li>
<li>methods: 使用现有系统状态预测服务质量指标的变化，并使用混合 Gaussian 模型和极值事件补偿器提高学习效果</li>
<li>results: 对真实的 SaaS 公司数据进行评估，Outage-Watch 表现出色，与传统方法相比，平均 AUC 为 0.98，并且能够早期探测服务 metric 的变化，降低了服务中断的 Mean Time To Detection（MTTD），证明了我们提出的方法的有效性。<details>
<summary>Abstract</summary>
Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is predicted if the probability of any one of the QoS metrics crossing threshold changes significantly. Our evaluation on a real-world SaaS company dataset shows that Outage-Watch significantly outperforms traditional methods with an average AUC of 0.98. Additionally, Outage-Watch detects all the outages exhibiting a change in service metrics and reduces the Mean Time To Detection (MTTD) of outages by up to 88% when deployed in an enterprise cloud-service system, demonstrating efficacy of our proposed method.
</details>
<details>
<summary>摘要</summary>
云服务是 ubique 和关键的，云服务失效是生活中的一种常见现象。为了保持客户和避免收益损失，提供高可靠性保证是非常重要的。一种方法是预测失效，可以帮助降低失效的严重性以及恢复时间。然而， kritische 失效具有罕见的事件，因此难以预测。此外， kritische 失效是无法明确定义的 observable 数据。我们提出的方法是 Outage-Watch，它定义云服务失效为 Quality of Service (QoS) 的下降， capture 由一组 métricas。Outage-Watch 使用当前系统状态预测 QoS métricas 是否将过度阈值，并触发极端事件。使用 Gaussian 混合模型以提供灵活性，并使用极端事件正则化来提高学习tail 的分布。如果任何一个 QoS métricas 的概率过度阈值，则认为出现了失效。我们对一个实际 SaaS 公司数据集进行了评估，结果表明 Outage-Watch 与传统方法相比显著性能更高，其中平均 AUC 为 0.98。此外，Outage-Watch 能够检测所有具有服务 métricas 变化的失效，并在云服务系统中减少失效检测时间（MTTD）的88%，这说明了我们提出的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Experiments-in-Self-Supervised-Cross-Table-Representation-Learning"><a href="#Scaling-Experiments-in-Self-Supervised-Cross-Table-Representation-Learning" class="headerlink" title="Scaling Experiments in Self-Supervised Cross-Table Representation Learning"></a>Scaling Experiments in Self-Supervised Cross-Table Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17339">http://arxiv.org/abs/2309.17339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Schambach, Dominique Paul, Johannes S. Otterbach</li>
<li>for: 本研究旨在探讨深度表格学习模型的扩展可能性，提出了一种基于Transformer架构的特有表格数据学习模型，并使用表格特定的tokenizer和共享Transformer脊梁来进行cross-table表格表示学习。</li>
<li>methods: 我们的训练方法包括单表模型和交叉表模型，通过自我超vised masked cell recovery目标进行缺失值补充。我们在不同模型大小（约10^4到10^7参数）下训练模型，并使用来自76个多样化数据集的135万个训练token来训练模型。</li>
<li>results: 我们通过对预训练模型进行线性探索，并与 convent ional baselines进行比较，来评估我们的架构是否能够扩展到更大的表格数据。<details>
<summary>Abstract</summary>
To analyze the scaling potential of deep tabular representation learning models, we introduce a novel Transformer-based architecture specifically tailored to tabular data and cross-table representation learning by utilizing table-specific tokenizers and a shared Transformer backbone. Our training approach encompasses both single-table and cross-table models, trained via missing value imputation through a self-supervised masked cell recovery objective. To understand the scaling behavior of our method, we train models of varying sizes, ranging from approximately $10^4$ to $10^7$ parameters. These models are trained on a carefully curated pretraining dataset, consisting of 135M training tokens sourced from 76 diverse datasets. We assess the scaling of our architecture in both single-table and cross-table pretraining setups by evaluating the pretrained models using linear probing on a curated set of benchmark datasets and comparing the results with conventional baselines.
</details>
<details>
<summary>摘要</summary>
<SYS> translate_text="To analyze the scaling potential of deep tabular representation learning models, we introduce a novel Transformer-based architecture specifically tailored to tabular data and cross-table representation learning by utilizing table-specific tokenizers and a shared Transformer backbone. Our training approach encompasses both single-table and cross-table models, trained via missing value imputation through a self-supervised masked cell recovery objective. To understand the scaling behavior of our method, we train models of varying sizes, ranging from approximately $10^4$ to $10^7$ parameters. These models are trained on a carefully curated pretraining dataset, consisting of 135M training tokens sourced from 76 diverse datasets. We assess the scaling of our architecture in both single-table and cross-table pretraining setups by evaluating the pretrained models using linear probing on a curated set of benchmark datasets and comparing the results with conventional baselines."</SYS>Here's the translation:为了分析深度表格表示学习模型的扩展潜力，我们提出了一种专门针对表格数据的Transformer架构，利用表格特定的tokenizer和共享Transformer背部。我们的训练方法包括单表和交叉表模型，通过缺失值补充来实现自我超vised做为掩码的恢复目标。为了了解我们的方法的扩展行为，我们训练了参数量从约10^4到10^7的模型，并在76个不同的数据集上进行了自适应预训练。我们通过对预训练模型进行线性探测，在单表和交叉表预训练设置下评估了我们的架构扩展行为，并与传统基elines进行比较。
</details></li>
</ul>
<hr>
<h2 id="Robust-Stochastic-Optimization-via-Gradient-Quantile-Clipping"><a href="#Robust-Stochastic-Optimization-via-Gradient-Quantile-Clipping" class="headerlink" title="Robust Stochastic Optimization via Gradient Quantile Clipping"></a>Robust Stochastic Optimization via Gradient Quantile Clipping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17316">http://arxiv.org/abs/2309.17316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibrahim Merad, Stéphane Gaïffas</li>
<li>for: 本文提出了一种SGD采样策略，使用分值函数来clip梯度 norm。这种策略可以提供一种robust和高效的优化算法，适用于均匀目标函数（ convex 或非 convex），承受重 tailed 样本和一部分异常值。</li>
<li>methods: 我们使用了常数步长 SGD 和 Markov chain 的连接，以及clip introduce的偏差的原始方法来进行数学分析。</li>
<li>results: 我们证明了在强 converges 目标函数时， iteration 会 converges to a concentrated distribution，并提供了高probability 上界的最终估计误差。在非均匀目标函数情况下，我们证明了极限分布在一个低梯度的 neighborhood 中受限。我们还提出了一种使用rolling quantiles实现这种算法的实现方法，这种方法具有强大的Robustness 性和高效性，通过数值实验证明。<details>
<summary>Abstract</summary>
We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which uses quantiles of the gradient norm as clipping thresholds. We prove that this new strategy provides a robust and efficient optimization algorithm for smooth objectives (convex or non-convex), that tolerates heavy-tailed samples (including infinite variance) and a fraction of outliers in the data stream akin to Huber contamination. Our mathematical analysis leverages the connection between constant step size SGD and Markov chains and handles the bias introduced by clipping in an original way. For strongly convex objectives, we prove that the iteration converges to a concentrated distribution and derive high probability bounds on the final estimation error. In the non-convex case, we prove that the limit distribution is localized on a neighborhood with low gradient. We propose an implementation of this algorithm using rolling quantiles which leads to a highly efficient optimization procedure with strong robustness properties, as confirmed by our numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种SGD clipping策略，使用Gradient norm的quantiles作为clipping阈值。我们证明了这种新策略可以提供一种robust和高效的优化算法，用于精度目标函数（ convex或非 convex），承受重 tailed samples（包括无限 variance）和数据流中一部分异常值。我们的数学分析利用了常数步长SGD和Markov链之间的连接，并处理clipping引入的偏差。对于强度 convex 目标函数，我们证明了迭代 converges to a concentrated distribution，并 derivated high probability bounds on the final estimation error。在非 convex 情况下，我们证明了限制分布在一个低Gradient的 neighborhood中。我们提议了使用rolling quantiles实现这个算法，导致了一种高效的优化过程，并且具有强大的Robust性质。我们的numerical experiments表明，这种策略在实际应用中具有很好的性能。
</details></li>
</ul>
<hr>
<h2 id="Leave-one-out-Distinguishability-in-Machine-Learning"><a href="#Leave-one-out-Distinguishability-in-Machine-Learning" class="headerlink" title="Leave-one-out Distinguishability in Machine Learning"></a>Leave-one-out Distinguishability in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17310">http://arxiv.org/abs/2309.17310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Ye, Anastasia Borovykh, Soufiane Hayou, Reza Shokri</li>
<li>for: 本研究旨在量化机器学习算法输出分布变化的问题，即留下一个数据点时算法训练集的影响，我们称之为留下一个数据点可 distinguishability（LOOD）。</li>
<li>methods: 我们使用 Gaussian processes 来模型机器学习算法的随机性，并通过详细的实验分析信息泄露问题，以证明 LOOD 的有用性。</li>
<li>results: 我们发现 LOOD 可以帮助我们量化数据 <strong>记忆</strong> 和 <strong>隐私</strong> 问题，并且可以分析训练数据中具有影响的数据点。此外，我们还可以使用优化查询来泄露训练数据中最重要的信息。<details>
<summary>Abstract</summary>
We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optimize queries that disclose the most significant information about the training data in the leave-one-out setting. We illustrate how optimal queries can be used for accurate **reconstruction** of training data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的分析框架，用于量化机器学习算法输出分布变化后包括一些数据点在训练集中的效果，我们称之为离带一个数据点分 distinguishability（LOOD）。这个问题对机器学习中的数据 **记忆** 和 **信息泄露** 具有重要意义，以及训练数据点对模型预测的影响。我们使用 Gaussian processes 模型机器学习算法的随机性，并通过广泛的实验分析信息泄露使用会员推理攻击。我们的理论框架允许我们调查训练数据点的泄露原因和泄露的地方。例如，我们分析活动函数对数据记忆的影响。此外，我们的方法允许我们设计优化查询，以披露训练数据中最重要的信息。我们示例如如何使用优化查询进行准确的 **重建** 训练数据。
</details></li>
</ul>
<hr>
<h2 id="Navigating-the-Design-Space-of-Equivariant-Diffusion-Based-Generative-Models-for-De-Novo-3D-Molecule-Generation"><a href="#Navigating-the-Design-Space-of-Equivariant-Diffusion-Based-Generative-Models-for-De-Novo-3D-Molecule-Generation" class="headerlink" title="Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation"></a>Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17296">http://arxiv.org/abs/2309.17296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Le, Julian Cremer, Frank Noé, Djork-Arné Clevert, Kristof Schütt</li>
<li>for: 这个研究旨在提高deep generative diffusion模型的性能，以便于物理科学和药物探索中的三维分子设计。</li>
<li>methods: 研究专注在E(3)对称扩散模型的设计空间中的前所未踏域。实验采用了比较分析，评估了连续和点点状态空间之间的交互。 finally, the EQGAT-diff model was introduced, which consistently outperforms established models on the QM9 and GEOM-Drugs datasets by a large margin.</li>
<li>results: EQGAT-diff model的实验结果显示，该模型在QM9和GEOM-Drugs数据集上的表现远比先前的模型好很多。此外，对于有限的训练数据，EQGAT-diff模型可以转移到Target Distributions with explicit hydrogens，并且通过一些调整 iterations further improve the state-of-the-art performance across datasets.<details>
<summary>Abstract</summary>
Deep generative diffusion models are a promising avenue for de novo 3D molecular design in material science and drug discovery. However, their utility is still constrained by suboptimal performance with large molecular structures and limited training data. Addressing this gap, we explore the design space of E(3) equivariant diffusion models, focusing on previously blank spots. Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. Out of this investigation, we introduce the EQGAT-diff model, which consistently surpasses the performance of established models on the QM9 and GEOM-Drugs datasets by a large margin. Distinctively, EQGAT-diff takes continuous atomic positions while chemical elements and bond types are categorical and employ a time-dependent loss weighting that significantly increases training convergence and the quality of generated samples. To further strengthen the applicability of diffusion models to limited training data, we examine the transferability of EQGAT-diff trained on the large PubChem3D dataset with implicit hydrogens to target distributions with explicit hydrogens. Fine-tuning EQGAT-diff for a couple of iterations further pushes state-of-the-art performance across datasets. We envision that our findings will find applications in structure-based drug design, where the accuracy of generative models for small datasets of complex molecules is critical.
</details>
<details>
<summary>摘要</summary>
深层生成扩散模型是物理科学和药物发现中新型分子设计的有前途的途径。然而，它们的实用性仍受大分子结构和有限训练数据的限制。为此，我们探索了E(3)对称扩散模型的设计空间，特别是之前未曾探讨的地方。我们进行了广泛的比较分析，评估了连续和离散状态空间之间的交互。从这些研究中，我们引入了EQGAT-diff模型，其在QM9和GEOM-Drugs数据集上至今的表现均超过了已有模型的表现，差异较大。EQGAT-diff模型使用连续原子位置，而化学元素和键类型则是分类的，同时采用时间依赖损失权重，这有效地增加了训练的整合和生成样本的质量。此外，我们还考虑了EQGAT-diff模型在PubChem3D数据集上进行训练，然后在目标分布中进行了转移学习，以提高模型的应用scope。我们认为，我们的发现将在结构基据设计中扮演着关键的角色，特别是在小数据集中的复杂分子的生成模型准确性是关键。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-soliton-dynamics-and-complex-potentials-recognition-for-1D-and-2D-PT-symmetric-saturable-nonlinear-Schrodinger-equations"><a href="#Deep-learning-soliton-dynamics-and-complex-potentials-recognition-for-1D-and-2D-PT-symmetric-saturable-nonlinear-Schrodinger-equations" class="headerlink" title="Deep learning soliton dynamics and complex potentials recognition for 1D and 2D PT-symmetric saturable nonlinear Schrödinger equations"></a>Deep learning soliton dynamics and complex potentials recognition for 1D and 2D PT-symmetric saturable nonlinear Schrödinger equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02276">http://arxiv.org/abs/2310.02276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Song, Zhenya Yan</li>
<li>For: 这 paper 的目的是扩展物理 Informed Neural Networks (PINNs) 来学习数据驱动的站ARY和非站ARY的塞耳散度 Equation (SNLSE) 中的两种基本PT-симметричный Scarf-II和周期 potentials在光纤中。* Methods: 这 paper 使用的方法包括 extending PINNs 来学习数据驱动的SNLSE，并对 PT-symmetric potential functions的发现进行数据驱动的反问题研究。特别是，提出了一种modified PINNs (mPINNs) 方案，可以直接通过解数据来确定1D和2D PT-symmetric potentials的函数。* Results: 这 paper 的结果表明，使用 deep neural networks 可以在1D和2D SNLSEs 中实现高精度的解决方案，并且 comparing two network structures under different parameter conditions 可以实现类似的高精度结果。此外， paper 还 analyze了一些影响 neural networks 性能的主要因素，包括活动函数、网络结构和训练数据的大小。<details>
<summary>Abstract</summary>
In this paper, we firstly extend the physics-informed neural networks (PINNs) to learn data-driven stationary and non-stationary solitons of 1D and 2D saturable nonlinear Schr\"odinger equations (SNLSEs) with two fundamental PT-symmetric Scarf-II and periodic potentials in optical fibers. Secondly, the data-driven inverse problems are studied for PT-symmetric potential functions discovery rather than just potential parameters in the 1D and 2D SNLSEs. Particularly, we propose a modified PINNs (mPINNs) scheme to identify directly the PT potential functions of the 1D and 2D SNLSEs by the solution data. And the inverse problems about 1D and 2D PT -symmetric potentials depending on propagation distance z are also investigated using mPINNs method. We also identify the potential functions by the PINNs applied to the stationary equation of the SNLSE. Furthermore, two network structures are compared under different parameter conditions such that the predicted PT potentials can achieve the similar high accuracy. These results illustrate that the established deep neural networks can be successfully used in 1D and 2D SNLSEs with high accuracies. Moreover, some main factors affecting neural networks performance are discussed in 1D and 2D PT Scarf-II and periodic potentials, including activation functions, structures of the networks, and sizes of the training data. In particular, twelve different nonlinear activation functions are in detail analyzed containing the periodic and non-periodic functions such that it is concluded that selecting activation functions according to the form of solution and equation usually can achieve better effect.
</details>
<details>
<summary>摘要</summary>
在本文中，我们首先扩展物理学 Informed Neural Networks (PINNs) 以学习数据驱动的一维和二维非线性普朗克 equations (SNLSEs) 中的定点和非定点 solitons。其次，我们研究了数据驱动的逆问题，即在一维和二维 SNLSEs 中发现PT-对称 potential functions，而不是只是参数。特别是，我们提出了修改后PINNs (mPINNs) 方案，以直接从解数据中获取1D和2D SNLSEs 中的PT potential functions。此外，我们还研究了一维和二维PT-对称 potentials的逆问题，它们取决于媒体传播距离z。此外，我们通过PINNs应用于SNLSEs的站点方程来预测PT potentials，并对其进行比较。我们发现，采用不同参数条件下的两种网络结构可以达到类似高精度。这些结果表明，我们建立的深度神经网络可以成功应用于1D和2D SNLSEs。此外，我们还讨论了一些影响神经网络性能的主要因素，包括活化函数、网络结构和训练数据大小。具体来说，我们对12种不同的非线性活化函数进行了详细分析，并结论选择活化函数应该根据解和方程的形式来选择。
</details></li>
</ul>
<hr>
<h2 id="In-search-of-dispersed-memories-Generative-diffusion-models-are-associative-memory-networks"><a href="#In-search-of-dispersed-memories-Generative-diffusion-models-are-associative-memory-networks" class="headerlink" title="In search of dispersed memories: Generative diffusion models are associative memory networks"></a>In search of dispersed memories: Generative diffusion models are associative memory networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17290">http://arxiv.org/abs/2309.17290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Ambrogioni</li>
<li>for: 这项研究旨在将生成推理和神经科学中的记忆理论联系起来，以推动创造性的生成和记忆回忆为一个综合体系。</li>
<li>methods: 这项研究使用了生成推理模型，并将其解释为能量基本模型。在训练过程中，Diffusion模型的能量函数与现代奥普菲尔德网络的能量函数相等。</li>
<li>results: 研究发现，继íz模型的存储容量与现代奥普菲尔德网络的存储容量相同。这些结果证明了生成推理和神经科学中的记忆理论之间的强联系，并提供了一个强大的计算基础 для创造性生成和记忆回忆。<details>
<summary>Abstract</summary>
Hopfield networks are widely used in neuroscience as simplified theoretical models of biological associative memory. The original Hopfield networks store memories by encoding patterns of binary associations, which result in a synaptic learning mechanism known as Hebbian learning rule. Modern Hopfield networks can achieve exponential capacity scaling by using highly non-linear energy functions. However, the energy function of these newer models cannot be straightforwardly compressed into binary synaptic couplings and it does not directly provide new synaptic learning rules. In this work we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is equivalent to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure of a deep neural network. Accordingly, in our experiments we show that the storage capacity of a continuous modern Hopfield network is identical to the capacity of a diffusion model. Our results establish a strong link between generative modeling and the theoretical neuroscience of memory, which provide a powerful computational foundation for the reconstructive theory of memory, where creative generation and memory recall can be seen as parts of a unified continuum.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Toward-Robust-Recommendation-via-Real-time-Vicinal-Defense"><a href="#Toward-Robust-Recommendation-via-Real-time-Vicinal-Defense" class="headerlink" title="Toward Robust Recommendation via Real-time Vicinal Defense"></a>Toward Robust Recommendation via Real-time Vicinal Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17278">http://arxiv.org/abs/2309.17278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichang Xu, Chenwang Wu, Defu Lian</li>
<li>for: 防御推荐系统受到恶意数据攻击，导致推荐结果受到偏见。</li>
<li>methods: 提出了一种通用的方法Real-time Vicinal Defense (RVD)，通过在推荐前 fine-tune模型，使其在实时中保持 robustness。</li>
<li>results: 经过广泛的实验证明，RVD可以有效防御多种目标攻击，而且不需要改变模型结构和训练过程，更加实用。<details>
<summary>Abstract</summary>
Recommender systems have been shown to be vulnerable to poisoning attacks, where malicious data is injected into the dataset to cause the recommender system to provide biased recommendations. To defend against such attacks, various robust learning methods have been proposed. However, most methods are model-specific or attack-specific, making them lack generality, while other methods, such as adversarial training, are oriented towards evasion attacks and thus have a weak defense strength in poisoning attacks.   In this paper, we propose a general method, Real-time Vicinal Defense (RVD), which leverages neighboring training data to fine-tune the model before making a recommendation for each user. RVD works in the inference phase to ensure the robustness of the specific sample in real-time, so there is no need to change the model structure and training process, making it more practical. Extensive experimental results demonstrate that RVD effectively mitigates targeted poisoning attacks across various models without sacrificing accuracy. Moreover, the defensive effect can be further amplified when our method is combined with other strategies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>推荐系统经常受到恶意攻击，攻击者可以插入恶意数据来让推荐系统提供偏袋式的推荐。为了防御这些攻击，有很多鲁棒学习方法被提出，但大多数方法是模型特定或攻击特定的，因此缺乏通用性，而其他方法，如对抗训练，则更关注逃脱攻击而弱于抗毒攻击。在这篇论文中，我们提出了一种通用的方法，即实时邻域防御（RVD），它利用用户的邻域训练数据来细化模型，以在实时推荐时保证模型的鲁棒性。RVD在推荐阶段进行实时微调，不需要改变模型结构和训练过程，因此更加实用。我们的实验结果表明，RVD可以有效地防御目标投毒攻击，并且不 sacrify 准确性。此外，当我们的方法与其他策略相结合时，抗击效果可以得到进一步的增强。
</details></li>
</ul>
<hr>
<h2 id="Utility-based-Adaptive-Teaching-Strategies-using-Bayesian-Theory-of-Mind"><a href="#Utility-based-Adaptive-Teaching-Strategies-using-Bayesian-Theory-of-Mind" class="headerlink" title="Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind"></a>Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17275">http://arxiv.org/abs/2309.17275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teacher-with-tom/utility_based_adaptive_teaching">https://github.com/teacher-with-tom/utility_based_adaptive_teaching</a></li>
<li>paper_authors: Clémence Grislain, Hugo Caselles-Dupré, Olivier Sigaud, Mohamed Chetouani</li>
<li>for: 这个论文的目的是构建一种基于 Bayesian Theory of Mind（ToM）的教师机器人，以便它们可以像人类一样适应学生的内部状态，并为学生选择最佳的示例。</li>
<li>methods: 该论文使用了 Bayesian ToM 机制，从学生的行为观察中构建了学生的内部状态模型，然后根据这些模型选择最佳的示例，以最大化学生的奖励而最小化教学成本。</li>
<li>results: 该论文的实验结果表明，使用这种基于 ToM 的教学策略可以使学生更快速地学习和提高性能，尤其是当教师的 ToM 模型与实际学生状态更加一致时。<details>
<summary>Abstract</summary>
Good teachers always tailor their explanations to the learners. Cognitive scientists model this process under the rationality principle: teachers try to maximise the learner's utility while minimising teaching costs. To this end, human teachers seem to build mental models of the learner's internal state, a capacity known as Theory of Mind (ToM). Inspired by cognitive science, we build on Bayesian ToM mechanisms to design teacher agents that, like humans, tailor their teaching strategies to the learners. Our ToM-equipped teachers construct models of learners' internal states from observations and leverage them to select demonstrations that maximise the learners' rewards while minimising teaching costs. Our experiments in simulated environments demonstrate that learners taught this way are more efficient than those taught in a learner-agnostic way. This effect gets stronger when the teacher's model of the learner better aligns with the actual learner's state, either using a more accurate prior or after accumulating observations of the learner's behaviour. This work is a first step towards social machines that teach us and each other, see https://teacher-with-tom.github.io.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Estimation-and-Inference-in-Distributional-Reinforcement-Learning"><a href="#Estimation-and-Inference-in-Distributional-Reinforcement-Learning" class="headerlink" title="Estimation and Inference in Distributional Reinforcement Learning"></a>Estimation and Inference in Distributional Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17262">http://arxiv.org/abs/2309.17262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangliangyu32/estimationandinferencedistributionalrl">https://github.com/zhangliangyu32/estimationandinferencedistributionalrl</a></li>
<li>paper_authors: Liangyu Zhang, Yang Peng, Jiadong Liang, Wenhao Yang, Zhihua Zhang</li>
<li>for: 这 paper  investigate distributional reinforcement learning 的 statistical efficiency aspect.</li>
<li>methods: 这 paper 使用 certainty-equivalence method construct estimator $\hat\eta^\pi$, 需要一个 generative model.</li>
<li>results: 这 paper  prove that with a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}\right)$, we can guarantee the $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$ is less than $\epsilon$ with high probability.  Additionally, the paper shows that a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^2(1-\gamma)^4}\right)$ is sufficient to ensure the Kolmogorov metric and total variation metric between $\hat\eta^\pi$ and $\eta^\pi$ is below $\epsilon$ with high probability. Finally, the paper demonstrates that the empirical process $\sqrt{n}(\hat\eta^\pi-\eta^\pi)$ converges weakly to a Gaussian process in certain function spaces.<details>
<summary>Abstract</summary>
In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.   We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\eta^\pi$) attained by a given policy $\pi$.   We use the certainty-equivalence method to construct our estimator $\hat\eta^\pi$, given a generative model is available.   We show that in this circumstance we need a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}\right)$ to guarantee a $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$ is less than $\epsilon$ with high probability.   This implies the distributional policy evaluation problem can be solved with sample efficiency.   Also, we show that under different mild assumptions a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1-\gamma)^{4}\right)$ suffices to ensure the Kolmogorov metric and total variation metric between $\hat\eta^\pi$ and $\eta^\pi$ is below $\epsilon$ with high probability.   Furthermore, we investigate the asymptotic behavior of $\hat\eta^\pi$.   We demonstrate that the ``empirical process'' $\sqrt{n}(\hat\eta^\pi-\eta^\pi)$ converges weakly to a Gaussian process in the space of bounded functionals on Lipschitz function class $\ell^\infty(\mathcal{F}_{W_1})$, also in the space of bounded functionals on indicator function class $\ell^\infty(\mathcal{F}_{\mathrm{KS})$ and bounded measurable function class $\ell^\infty(\mathcal{F}_{\mathrm{TV})$ when some mild conditions hold.   Our findings give rise to a unified approach to statistical inference of a wide class of statistical functionals of $\eta^\pi$.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们从统计效率的角度研究分布式强化学习。我们研究了分布式政策评估， aiming to estimate the complete distribution of the random return (denoted $\eta^\pi$) attained by a given policy $\pi$.我们使用certainty-equivalence方法construct our estimator $\hat\eta^\pi$, given a generative model is available.我们证明在这种情况下，我们需要一个大小为 $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}\right)$的数据集，以保证分布式政策评估问题可以高效地解决。这意味着可以通过采样效率来解决分布式政策评估问题。此外，我们还证明了不同的某些轻度假设下，一个数据集大小为 $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1-\gamma)^{4}\right)$  suffices to ensure the Kolmogorov metric and total variation metric between $\hat\eta^\pi$ and $\eta^\pi$ is below $\epsilon$ with high probability.此外，我们还研究了 $\hat\eta^\pi$ 的极限行为。我们证明了 $\sqrt{n}(\hat\eta^\pi-\eta^\pi)$  converges weakly to a Gaussian process in the space of bounded functionals on Lipschitz function class $\ell^\infty(\mathcal{F}_{W_1})$, also in the space of bounded functionals on indicator function class $\ell^\infty(\mathcal{F}_{\mathrm{KS})$ and bounded measurable function class $\ell^\infty(\mathcal{F}_{\mathrm{TV})$ when some mild conditions hold.我们的发现可以导致一种统一的方法 для统计推断 $\eta^\pi$ 的各种统计函数。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-localized-waves-and-parameter-discovery-in-the-massive-Thirring-model-via-extended-physics-informed-neural-networks-with-interface-zones"><a href="#Data-driven-localized-waves-and-parameter-discovery-in-the-massive-Thirring-model-via-extended-physics-informed-neural-networks-with-interface-zones" class="headerlink" title="Data-driven localized waves and parameter discovery in the massive Thirring model via extended physics-informed neural networks with interface zones"></a>Data-driven localized waves and parameter discovery in the massive Thirring model via extended physics-informed neural networks with interface zones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17240">http://arxiv.org/abs/2309.17240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junchao Chen, Jin Song, Zijian Zhou, Zhenya Yan</li>
<li>for: 这个论文研究了基于深度学习的物理学习核算法（PINNs）在大规模提尔林（MT）模型中的数据驱动本地波解。</li>
<li>methods: 该论文使用了扩展PINNs（XPINNs）与域 decompositions以捕捉高级别的本地波解，并提出了一种改进版XPINNs方法。</li>
<li>results: 该论文通过对各种解的数据驱动 simulations 和分析，显示了该方法的高精度和快速收敛特点，并成功地解决了不同类型的本地波解的逆问题。<details>
<summary>Abstract</summary>
In this paper, we study data-driven localized wave solutions and parameter discovery in the massive Thirring (MT) model via the deep learning in the framework of physics-informed neural networks (PINNs) algorithm. Abundant data-driven solutions including soliton of bright/dark type, breather and rogue wave are simulated accurately and analyzed contrastively with relative and absolute errors. For higher-order localized wave solutions, we employ the extended PINNs (XPINNs) with domain decomposition to capture the complete pictures of dynamic behaviors such as soliton collisions, breather oscillations and rogue-wave superposition. In particular, we modify the interface line in domain decomposition of XPINNs into a small interface zone and introduce the pseudo initial, residual and gradient conditions as interface conditions linked adjacently with individual neural networks. Then this modified approach is applied successfully to various solutions ranging from bright-bright soliton, dark-dark soliton, dark-antidark soliton, general breather, Kuznetsov-Ma breather and second-order rogue wave. Experimental results show that this improved version of XPINNs reduce the complexity of computation with faster convergence rate and keep the quality of learned solutions with smoother stitching performance as well. For the inverse problems, the unknown coefficient parameters of linear and nonlinear terms in the MT model are identified accurately with and without noise by using the classical PINNs algorithm.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了基于深度学习的数据驱动本地波解的MT模型参数发现和解决方法。通过物理学 Informed Neural Networks（PINNs）算法，我们可以高精度地模拟并分析包括喷气、暗气、异常波等数据驱动波解。对于更高阶的本地波解，我们使用了扩展PINNs（XPINNs） WITH 域 decomposing来捕捉整个动态行为的全貌，如喷气碰撞、暗气振荡和异常波superposition。在特定的实现中，我们修改了域 decomposition的界面线为一小的界面zone，并引入 pseudo initial、剩余和导数条件作为界面条件，这些条件与个体神经网络相邻联系。然后，这种修改后的方法被应用到了不同的解，包括明亮喷气、暗气喷气、暗气反喷气、通用喷气、库泽зов-玛喷气和第二阶异常波。实验结果表明，改进后的XPINNs方法可以降低计算复杂度，提高速度度和保持学习解的平滑拼接性。此外，我们还使用了类ical PINNs算法来准确地确定MT模型中未知系数参数，包括线性和非线性项的系数。
</details></li>
</ul>
<hr>
<h2 id="MuSe-GNN-Learning-Unified-Gene-Representation-From-Multimodal-Biological-Graph-Data"><a href="#MuSe-GNN-Learning-Unified-Gene-Representation-From-Multimodal-Biological-Graph-Data" class="headerlink" title="MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data"></a>MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02275">http://arxiv.org/abs/2310.02275</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/helloworldlty/muse-gnn">https://github.com/helloworldlty/muse-gnn</a></li>
<li>paper_authors: Tianyu Liu, Yuge Wang, Rex Ying, Hongyu Zhao</li>
<li>for: 本研究旨在学习生物医学数据中不同来源数据的基因表达之间的相似性，以便在不同背景下捕捉基因功能相似性。</li>
<li>methods: 本研究提出了一种名为多模态相似学习图 neural network的新模型，该模型结合多模态机器学习和深度图 нейрон网络来学习基因表达从单元细胞测序和空间转录数据中。</li>
<li>results: 对于82个训练数据集、10种组织、三种测序技术和三种物种，我们创建了有用的图结构进行模型训练和基因表达生成，并采用了权重相似学习和对比学习来学习不同数据中基因之间的关系。这种新的设计使得我们可以提供包含多种Modalities的基因表达，其中包含了不同背景下基因功能相似性的功能相似性。<details>
<summary>Abstract</summary>
Discovering genes with similar functions across diverse biomedical contexts poses a significant challenge in gene representation learning due to data heterogeneity. In this study, we resolve this problem by introducing a novel model called Multimodal Similarity Learning Graph Neural Network, which combines Multimodal Machine Learning and Deep Graph Neural Networks to learn gene representations from single-cell sequencing and spatial transcriptomic data. Leveraging 82 training datasets from 10 tissues, three sequencing techniques, and three species, we create informative graph structures for model training and gene representations generation, while incorporating regularization with weighted similarity learning and contrastive learning to learn cross-data gene-gene relationships. This novel design ensures that we can offer gene representations containing functional similarity across different contexts in a joint space. Comprehensive benchmarking analysis shows our model's capacity to effectively capture gene function similarity across multiple modalities, outperforming state-of-the-art methods in gene representation learning by up to 97.5%. Moreover, we employ bioinformatics tools in conjunction with gene representations to uncover pathway enrichment, regulation causal networks, and functions of disease-associated or dosage-sensitive genes. Therefore, our model efficiently produces unified gene representations for the analysis of gene functions, tissue functions, diseases, and species evolution.
</details>
<details>
<summary>摘要</summary>
在生物医学中发现同 функ数据中的基因具有相似功能是一个挑战，因为数据的多样性导致表现学习的问题。在这项研究中，我们解决这个问题 by introducing a novel model called Multimodal Similarity Learning Graph Neural Network, which combines Multimodal Machine Learning and Deep Graph Neural Networks to learn gene representations from single-cell sequencing and spatial transcriptomic data. By leveraging 82 training datasets from 10 tissues, three sequencing techniques, and three species, we create informative graph structures for model training and gene representations generation, while incorporating regularization with weighted similarity learning and contrastive learning to learn cross-data gene-gene relationships. This novel design ensures that we can offer gene representations containing functional similarity across different contexts in a joint space.我们的模型能够优化 capture gene function similarity across multiple modalities, outperforming state-of-the-art methods in gene representation learning by up to 97.5%. Furthermore, we employ bioinformatics tools in conjunction with gene representations to uncover pathway enrichment, regulation causal networks, and functions of disease-associated or dosage-sensitive genes. Therefore, our model efficiently produces unified gene representations for the analysis of gene functions, tissue functions, diseases, and species evolution.
</details></li>
</ul>
<hr>
<h2 id="Spurious-Feature-Diversification-Improves-Out-of-distribution-Generalization"><a href="#Spurious-Feature-Diversification-Improves-Out-of-distribution-Generalization" class="headerlink" title="Spurious Feature Diversification Improves Out-of-distribution Generalization"></a>Spurious Feature Diversification Improves Out-of-distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17230">http://arxiv.org/abs/2309.17230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, Tong Zhang<br>for: 这个论文的目的是解释ensemble方法在陌生数据上的高效性的机制。methods: 这个论文使用了weight space ensemble方法，特别是WiSE-FT方法，以 interpolate模型参数。results: 研究发现，WiSE-FT方法可以在陌生数据上具有优秀的高效性，且可以正确地纠正各个模型的预测错误。此外，研究还发现， ensemble方法可以通过使用多样性的干扰特征来减少预测错误。<details>
<summary>Abstract</summary>
Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains unclear. In this study, we closely examine WiSE-FT, a popular weight space ensemble method that interpolates between a pre-trained and a fine-tuned model. We observe an unexpected phenomenon, in which WiSE-FT successfully corrects many cases where each individual model makes incorrect predictions, which contributes significantly to its OOD effectiveness. To gain further insights, we conduct theoretical analysis in a multi-class setting with a large number of spurious features. Our analysis predicts the above phenomenon and it further shows that ensemble-based models reduce prediction errors in the OOD settings by utilizing a more diverse set of spurious features. Contrary to the conventional wisdom that focuses on learning invariant features for better OOD performance, our findings suggest that incorporating a large number of diverse spurious features weakens their individual contributions, leading to improved overall OOD generalization performance. Empirically we demonstrate the effectiveness of utilizing diverse spurious features on a MultiColorMNIST dataset, and our experimental results are consistent with the theoretical analysis. Building upon the new theoretical insights into the efficacy of ensemble methods, we further identify an issue of WiSE-FT caused by the overconfidence of fine-tuned models in OOD situations. This overconfidence magnifies the fine-tuned model's incorrect prediction, leading to deteriorated OOD ensemble performance. To remedy this problem, we propose a novel method called BAlaNced averaGing (BANG), which significantly enhances the OOD performance of WiSE-FT.
</details>
<details>
<summary>摘要</summary>
通用化到非常值 (OOD) 数据是机器学习中的一个关键挑战。集成方法，如权重空间集合，已经显示出在 OOD 性能上表现出色。然而，这些方法的内在机制仍然不够清楚。在这项研究中，我们密切检查了 WiSE-FT，一种广泛使用的权重空间集合方法，该方法 interpolates  между预训练和精度调整的模型。我们发现了一种意外的现象：WiSE-FT 成功地更正了许多情况下每个个体模型的错误预测，这对 OOD 性能做出了重要贡献。为了更深入地理解这种现象，我们在多类 setting 中进行了理论分析，并预测了上述现象。我们的分析还显示，集成型模型在 OOD 设置下采用更多的废弃特征来减少预测错误。与传统智慧所预期的不同，我们发现，在 incorporating 一大量多样的废弃特征时，它们的个人贡献减弱，导致了改善的 OOD 总体化能力。empirically，我们在 MultiColorMNIST 数据集上证明了利用多样的废弃特征的效果，并发现结果与理论分析一致。基于新的理论发现，我们进一步发现 WiSE-FT 在 OOD 情况下存在一个问题，即精度调整的模型在 OOD 情况下的过于自信，导致 ensemble 性能下降。为解决这个问题，我们提出了一种新的方法 called BAlaNced averaGing (BANG)，可以减少 OOD 情况下 WiSE-FT 的 ensemble 性能下降。
</details></li>
</ul>
<hr>
<h2 id="Memory-Gym-Partially-Observable-Challenges-to-Memory-Based-Agents-in-Endless-Episodes"><a href="#Memory-Gym-Partially-Observable-Challenges-to-Memory-Based-Agents-in-Endless-Episodes" class="headerlink" title="Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes"></a>Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17207">http://arxiv.org/abs/2309.17207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marcometer/endless-memory-gym">https://github.com/marcometer/endless-memory-gym</a></li>
<li>paper_authors: Marco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss</li>
<li>for: The paper compares the performance of Gated Recurrent Unit (GRU) and Transformer-XL (TrXL) in deep reinforcement learning tasks, specifically in memorizing long sequences, withstanding noise, and generalizing.</li>
<li>methods: The paper uses partially observable 2D environments with discrete controls, such as Mortar Mayhem, Mystery Path, and Searing Spotlights, and extrapolates these environments to novel endless tasks as an automatic curriculum. The paper also uses Proximal Policy Optimization and a sliding window approach with TrXL as episodic memory.</li>
<li>results: The paper shows that GRU outperforms TrXL in all endless tasks, with GRU consistently outperforming TrXL by significant margins. However, TrXL demonstrates superior sample efficiency in Mystery Path and outperforms GRU in Mortar Mayhem.Here are the three key points in Simplified Chinese:</li>
<li>for: 这篇论文测试了深度强化学习Agent的表现，特别是GRU和Transformer-XL（TrXL）在记忆长序、抗噪和泛化方面的表现。</li>
<li>methods: 论文使用了部分可见2D环境和简化的控制，如Mortar Mayhem、Mystery Path和Searing Spotlights，并将这些环境推广到了新的无穷任务，以作为自动课程。</li>
<li>results: 论文显示GRU在所有无穷任务中表现出色，与TrXL在所有任务中均有显著的性能优势。但是，TrXL在Mystery Path中表现出较好的样本效率，而GRU在Mortar Mayhem中表现出较好的性能。<details>
<summary>Abstract</summary>
Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environments, TrXL demonstrates superior sample efficiency in Mystery Path and outperforms in Mortar Mayhem. However, GRU is more efficient on Searing Spotlights. Most notably, in all endless tasks, GRU makes a remarkable resurgence, consistently outperforming TrXL by significant margins.
</details>
<details>
<summary>摘要</summary>
Memory Gym 引入了一个独特的标准测试深度强化学习机制，具体来说是比较 GRU 和 Transformer-XL（TrXL）在记忆长序的能力、抵抗噪音和通用性方面的比较。它采用了部分可见 2D 环境和简单的控制，包括 Mortar Mayhem、Mystery Path 和 Searing Spotlights。这些原始的有限环境通过扩展到新的无穷任务来 acted as an automatic curriculum， draw inspiration from the car game "I packed my bag".这些无穷任务不仅有利于评估效率，而且有趣地用于评估方法在记忆基于机制中的效果。由于公共可用的记忆基线匮乏，我们提供了基于 TrXL 和 Proximal Policy Optimization 的实现。这个实现利用 TrXL 作为 episodic memory 使用滑动窗口方法。在我们对 finite 环境的实验中，TrXL 在 Mystery Path 中表现出了更高的样本效率，而 GRU 在 Searing Spotlights 中更高效。然而，在所有无穷任务中，GRU 表现出了惊人的复兴，一直高于 TrXL 的很大幅度。
</details></li>
</ul>
<hr>
<h2 id="ResBit-Residual-Bit-Vector-for-Categorical-Values"><a href="#ResBit-Residual-Bit-Vector-for-Categorical-Values" class="headerlink" title="ResBit: Residual Bit Vector for Categorical Values"></a>ResBit: Residual Bit Vector for Categorical Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17196">http://arxiv.org/abs/2309.17196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masane Fuchi, Amar Zanashir, Hiroto Minami, Tomohiro Takagi</li>
<li>for: 提出了一种解决高频率分类数据的精简表示方法，以便在深度学习中避免空间计算复杂性问题。</li>
<li>methods: 提出了一种基于扩散模型的Analog Bits方法，并提出了一种基于Table Residual Bit Diffusion（TRBD）的TabDDPM表格数据生成方法。</li>
<li>results: 通过实验证明，TRBD可以快速生成高质量表格数据，并且ResBit可以作为一种替代一个热点 вектор的方法，用于GANs的conditioning和图像分类中的标签表达。<details>
<summary>Abstract</summary>
The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-purpose representation method, in this paper, we treat it as numerical data and show that it can be used as an extension of Analog Bits using Table Residual Bit Diffusion (TRBD), which is incorporated into TabDDPM, a tabular data generation method. We experimentally confirmed that TRBD can generate diverse and high-quality data from small-scale table data to table data containing diverse category values faster than TabDDPM. Furthermore, we show that ResBit can also serve as an alternative to the one-hot vector by utilizing ResBit for conditioning in GANs and as a label expression in image classification.
</details>
<details>
<summary>摘要</summary>
“一热vector”已经在机器学习中广泛使用，用于简单且通用的方法来表示数据。然而，这种方法会将数据的维度增加 linearly  avec  categorical data 被表示，这是深度学习中的空间 Computational Complexity 的问题，需要大量数据。在最近，Analog Bits 方法被提出，用于表示数据为一串位元。然而，这种方法不能够 repre sent 不同类型的数据，导致当需要生成类别值时， Originals 的类别值无法被Restore。为了解决这个问题，我们提出了 Residual Bit Vector (ResBit)，它是一种层次的位元表示方法。尽管它是一个通用的表示方法，在这篇文章中，我们将它视为数据的numerical data，并证明它可以作为 Analog Bits 的扩展使用 Table Residual Bit Diffusion (TRBD)，它是 TabDDPM 中的一个 tabular data 生成方法。我们实验确认了 TRBD 可以从小规模的表格数据中产生多元化和高品质的数据，并且比 TabDDPM 更快。此外，我们还证明了 ResBit 可以作为一热vector 的替代方案，通过在 GANs 中使用 ResBit 作为条件，以及在图像分类中使用 ResBit 作为标签表达。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Activation-via-Multivariate-Projection"><a href="#Generalized-Activation-via-Multivariate-Projection" class="headerlink" title="Generalized Activation via Multivariate Projection"></a>Generalized Activation via Multivariate Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17194">http://arxiv.org/abs/2309.17194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ljy9912/mimo_nn">https://github.com/ljy9912/mimo_nn</a></li>
<li>paper_authors: Jiayun Li, Yuxiao Cheng, Zhuofan Xia, Yilin Mo, Gao Huang</li>
<li>for: This paper aims to improve the performance of neural networks by introducing a new activation function called Multivariate Projection Unit (MPU).</li>
<li>methods: The paper uses a mathematical proof to establish the expressive power of MPU compared to the widely used Rectified Linear Unit (ReLU) activation function. Experimental evaluations are also conducted to compare the performance of MPU with other activation functions.</li>
<li>results: The paper shows that MPU outperforms ReLU and other activation functions in terms of expressive power, and provides a mathematical proof to support this claim. Experimental results also corroborate the effectiveness of MPU on widely-adopted architectures.<details>
<summary>Abstract</summary>
Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from R onto the nonnegative half-line R+. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architectures further corroborate MPU's effectiveness against a broader range of existing activation functions.
</details>
<details>
<summary>摘要</summary>
活动函数是神经网络中不可或缺的一部分，RECTIFIED LINEAR UNIT（ReLU）经常被选择因为它的简单性和效果。我们启发自权值迭代PGD算法中的结构相似性，将ReLU视为从R到非负半线R+的投影。基于这种解释，我们扩展ReLU，将其替换为一种多输入多输出的投影运算，例如第二次导数树（SOC）投影，从而自然地扩展到多变量投影单元（MPU）。我们还提供了一个数学证明，证明使用SOC投影 activation 函数的FNN表现更高效于使用ReLU。实验评估表明，MPU在较广泛的 activation 函数中表现更好。
</details></li>
</ul>
<hr>
<h2 id="RECOMBINER-Robust-and-Enhanced-Compression-with-Bayesian-Implicit-Neural-Representations"><a href="#RECOMBINER-Robust-and-Enhanced-Compression-with-Bayesian-Implicit-Neural-Representations" class="headerlink" title="RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations"></a>RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17182">http://arxiv.org/abs/2309.17182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajun He, Gergely Flamich, Zongyu Guo, José Miguel Hernández-Lobato</li>
<li>for: 提高数据压缩方法的效率和灵活性</li>
<li>methods: 使用INRWeightlinear扩展variational批处理，加入学习位置编码以适应地方细节，将高解度数据分割成 patches 并使用层次PRIORS捕捉它们之间的依赖关系</li>
<li>results: 在多种数据模式下进行了广泛的实验，显示了RECOMBINER可以与best INR-based方法竞争，甚至在低比特率下超越自适应编码器-based codecs 的图像压缩性能<details>
<summary>Abstract</summary>
COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while maintaining its computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.
</details>
<details>
<summary>摘要</summary>
COMpression with Bayesian Implicit NEural Representations (COMBINER) 是一种最近的数据压缩方法，它解决了之前基于Implicit Neural Representation（INR）的方法中的一个关键不足：它避免了量化并直接优化了率度性能。然而，COMBINER仍有 significiant 局限性：1) 它使用因子化 posterior 和先验 approximations 缺乏灵活性; 2) 它无法有效地适应当地偏差于全局模式的数据; 3) 其性能可能受模型选择和 Variational 参数的初始化的影响。我们提出的方法，Robust and Enhanced COMBINER（RECOMBINER），解决了这些问题，通过以下方式：1. 通过对 INR 权重进行线性 реParameterization，保持 computational cost 的同时，提高 variational approxiamtion 的精度。2. 通过添加可学习的位置编码，使 INR 适应当地偏差和详细信息。3. 将高分辨率数据分割成 patches，提高 robustness，并使用层次的 priors 捕捉数据之间的依赖关系。我们在多个数据模式上进行了广泛的实验，展示了 RECOMBINER 与最佳 INR-based 方法竞争，甚至在低比特率下超过 autoencoder-based 编码器在低分辨率图像上。
</details></li>
</ul>
<hr>
<h2 id="FedZeN-Towards-superlinear-zeroth-order-federated-learning-via-incremental-Hessian-estimation"><a href="#FedZeN-Towards-superlinear-zeroth-order-federated-learning-via-incremental-Hessian-estimation" class="headerlink" title="FedZeN: Towards superlinear zeroth-order federated learning via incremental Hessian estimation"></a>FedZeN: Towards superlinear zeroth-order federated learning via incremental Hessian estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17174">http://arxiv.org/abs/2309.17174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessio Maritan, Subhrakanti Dey, Luca Schenato</li>
<li>for: 这篇论文的目的是开发一个基于分布式学习框架的联合训练方法，以便在中央服务器的协调下，多个客户端机器可以共同训练一个模型，不需要分享原始数据样本。</li>
<li>methods: 这篇论文使用了一个叫做 FedZeN 的联合零阶算法，用于估计全域目标函数的弹性。这个算法使用了一个增量对称梯度估计器，其误差 Norm 会在线性增长。在联合零阶设定下，这个算法会从 Stiefel  manifold 中随机选择搜寻方向，以提高表现。中央服务器使用同步 Pseudo-Random 数组生成器来实现通信效率和隐私保护。</li>
<li>results: 这篇论文提供了一个名为 FedZeN 的联合零阶算法，可以在实际应用中实现超线性增长率。实验结果显示，FedZeN Algorithm 比已有的联合零阶方法高效，并且在多个实际应用中实现了超线性增长率。<details>
<summary>Abstract</summary>
Federated learning is a distributed learning framework that allows a set of clients to collaboratively train a model under the orchestration of a central server, without sharing raw data samples. Although in many practical scenarios the derivatives of the objective function are not available, only few works have considered the federated zeroth-order setting, in which functions can only be accessed through a budgeted number of point evaluations. In this work we focus on convex optimization and design the first federated zeroth-order algorithm to estimate the curvature of the global objective, with the purpose of achieving superlinear convergence. We take an incremental Hessian estimator whose error norm converges linearly, and we adapt it to the federated zeroth-order setting, sampling the random search directions from the Stiefel manifold for improved performance. In particular, both the gradient and Hessian estimators are built at the central server in a communication-efficient and privacy-preserving way by leveraging synchronized pseudo-random number generators. We provide a theoretical analysis of our algorithm, named FedZeN, proving local quadratic convergence with high probability and global linear convergence up to zeroth-order precision. Numerical simulations confirm the superlinear convergence rate and show that our algorithm outperforms the federated zeroth-order methods available in the literature.
</details>
<details>
<summary>摘要</summary>
《联合学习》是一种分布式学习框架，允许一组客户端在中央服务器的指挥下，共同训练一个模型，无需分享原始数据样本。虽然在实际应用中，目标函数的导数通常不可获得，但只有很少的研究探讨了联合零次设定下的学习，在这种设定下，函数仅可通过一个限定的数量的点评估访问。在这种工作中，我们关注 convex 优化，并设计了首个联合零次算法，用于估计全局目标函数的曲率。我们采用一种增量希格曼统计器，其误差 нор的梯度 converge Linearly，并在联合零次设定下采样随机搜索方向从Stiefel manifold上进行改进。具体来说，梯度和希格曼统计器都在中央服务器上构建，通过同步 Pseudo-Random Number Generators 来实现通信效率和隐私保护。我们提供了对我们算法的理论分析，名为FedZeN，证明其在当前精度下的本地弯曲收敛和全局线性收敛。数值 simulations 表明我们的算法可以在超linear 收敛率下进行高效的训练。此外，我们的算法也比联合零次方法在文献中出现的性能更高。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Interpretable-Nonlinear-Modeling-for-Multiple-Time-Series"><a href="#Efficient-Interpretable-Nonlinear-Modeling-for-Multiple-Time-Series" class="headerlink" title="Efficient Interpretable Nonlinear Modeling for Multiple Time Series"></a>Efficient Interpretable Nonlinear Modeling for Multiple Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17154">http://arxiv.org/abs/2309.17154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Roy, Luis Miguel Lopez-Ramos, Baltasar Beferull-Lozano</li>
<li>for: 这篇论文的目的是提出一种高效的多时序序数据非线性模型化方法，以优化时序预测和模型简化。</li>
<li>methods: 该方法假设时序序数据集是通过两步生成的：首先是一个线性VAR过程在隐藏空间中，然后是一组可逆和 lipschitz 不变的非线性映射，每个感知器都有一个不同的映射。模型每个组成部分的非线性使用可逆神经网络，并强制某些约束来反映实际应用中的减少依赖关系。</li>
<li>results: 实验结果表明，提出的方法可以更好地确定VAR几何体的支持，同时也可以提高时序预测的精度，比起当前状态艺术方法更好。<details>
<summary>Abstract</summary>
Predictive linear and nonlinear models based on kernel machines or deep neural networks have been used to discover dependencies among time series. This paper proposes an efficient nonlinear modeling approach for multiple time series, with a complexity comparable to linear vector autoregressive (VAR) models while still incorporating nonlinear interactions among different time-series variables. The modeling assumption is that the set of time series is generated in two steps: first, a linear VAR process in a latent space, and second, a set of invertible and Lipschitz continuous nonlinear mappings that are applied per sensor, that is, a component-wise mapping from each latent variable to a variable in the measurement space. The VAR coefficient identification provides a topology representation of the dependencies among the aforementioned variables. The proposed approach models each component-wise nonlinearity using an invertible neural network and imposes sparsity on the VAR coefficients to reflect the parsimonious dependencies usually found in real applications. To efficiently solve the formulated optimization problems, a custom algorithm is devised combining proximal gradient descent, stochastic primal-dual updates, and projection to enforce the corresponding constraints. Experimental results on both synthetic and real data sets show that the proposed algorithm improves the identification of the support of the VAR coefficients in a parsimonious manner while also improving the time-series prediction, as compared to the current state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
预测线性和非线性模型，基于kernel机器或深度神经网络，已经用于发现时间序列之间的依赖关系。本文提出一种高效的多时间序列非线性模型方法，其复杂度与线性vector autoregressive（VAR）模型相当，同时仍能够捕捉不同时间序列变量之间的非线性交互关系。模型假设是，时间序列集合是通过两步生成的：首先，一个线性VAR过程在隐藏空间中，然后，每个感知器上应用一组 invertible和Lipschitz连续非线性映射，即每个隐藏变量到测量空间中的变量的组成部分 mapping。VAR偏好的标示提供了这些变量之间的依赖关系的topology表示。提posed方法每个组成部分非线性使用 invertible neural network，并强制 sparse VAR偏好，以反映实际应用中通常发现的含义简单的依赖关系。为有效地解决提出的优化问题，提出了一种自定义算法， combining proximal gradient descent、stochastic primal-dual更新和投影，以保证相应的约束。实验结果表明，提出的算法可以高效地提取VAR偏好的支持，并提高时间序列预测，相比现状态艺术方法。
</details></li>
</ul>
<hr>
<h2 id="GRANDE-Gradient-Based-Decision-Tree-Ensembles"><a href="#GRANDE-Gradient-Based-Decision-Tree-Ensembles" class="headerlink" title="GRANDE: Gradient-Based Decision Tree Ensembles"></a>GRANDE: Gradient-Based Decision Tree Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17130">http://arxiv.org/abs/2309.17130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s-marton/grande">https://github.com/s-marton/grande</a></li>
<li>paper_authors: Sascha Marton, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt</li>
<li>for: 这个研究是为了提出一种基于条件树的Gradient-based Decision Tree Ensemble（GRANDE）方法，用于处理杂多的条件数据。</li>
<li>methods: GRANDE使用了紧密的树组合表示，并使用终端到端的梯度下降来协同优化所有模型参数。它结合了轴线平行分割，这是一种对条件数据有用的推理假设，以及梯度下降的flexibility。</li>
<li>results: 在一个预先定义的benchmark上进行了广泛的评估，该benchmark包括19个分类 dataset，并示出GRANDE在大多数dataset上表现更好于现有的梯度增强和深度学习框架。<details>
<summary>Abstract</summary>
Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We conducted an extensive evaluation on a predefined benchmark with 19 classification datasets and demonstrate that our method outperforms existing gradient-boosting and deep learning frameworks on most datasets.
</details>
<details>
<summary>摘要</summary>
尽管深度学习在文本和图像数据上取得了成功，但是tree-based ensemble模型仍然是机器学习适用于不同类型表格数据的现状的state-of-the-art。然而，由于表格数据的高灵活性，there is a significant need for tabular-specific gradient-based methods。在这篇论文中，我们提出了GRANDE，即GRAdieNt-Based Decision Tree Ensembles，一种使用端到端的梯度下降来学习硬的、轴对齐的决策树集的新方法。GRANDE基于紧凑的树集表示，可以使用反射operators来同时优化所有模型参数。我们的方法结合了轴对齐分割，这是表格数据的有用预测条件，以及梯度下降的灵活性。此外，我们还介绍了一种高级的实例级Weighting，它可以在单个模型中学习表格数据中的简单和复杂关系表示。我们在19个分类 datasets上进行了广泛的评估，并示出了我们的方法在大多数数据集上超过了现有的梯度束合和深度学习框架的性能。
</details></li>
</ul>
<hr>
<h2 id="Style-Transfer-for-Non-differentiable-Audio-Effects"><a href="#Style-Transfer-for-Non-differentiable-Audio-Effects" class="headerlink" title="Style Transfer for Non-differentiable Audio Effects"></a>Style Transfer for Non-differentiable Audio Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17125">http://arxiv.org/abs/2309.17125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieran Grant</li>
<li>for: Audio production style matching, particularly for multi-band compressor effects.</li>
<li>methods: Deep learning approach using audio embeddings, which can be applied to various classes of effects and does not require auto-differentiation.</li>
<li>results: Convincingly styles match a multi-band compressor effect using the proposed approach, and the audio embeddings can be used for downstream tasks such as timbral information retrieval.Here’s the full Chinese text:</li>
<li>for: 这个研究旨在实现音乐制作风格匹配，特别是适用于多射频压缩器效果。</li>
<li>methods: 使用深度学习的音乐嵌入方法，可以应用于不同的效果类型，并且不需要自动微分运算。</li>
<li>results: 透过提案的方法，成功实现多射频压缩器效果的风格匹配，并且音乐嵌入可以用于后续任务，如时域信息回传。<details>
<summary>Abstract</summary>
Digital audio effects are widely used by audio engineers to alter the acoustic and temporal qualities of audio data. However, these effects can have a large number of parameters which can make them difficult to learn for beginners and hamper creativity for professionals. Recently, there have been a number of efforts to employ progress in deep learning to acquire the low-level parameter configurations of audio effects by minimising an objective function between an input and reference track, commonly referred to as style transfer. However, current approaches use inflexible black-box techniques or require that the effects under consideration are implemented in an auto-differentiation framework. In this work, we propose a deep learning approach to audio production style matching which can be used with effects implemented in some of the most widely used frameworks, requiring only that the parameters under consideration have a continuous domain. Further, our method includes style matching for various classes of effects, many of which are difficult or impossible to be approximated closely using differentiable functions. We show that our audio embedding approach creates logical encodings of timbral information, which can be used for a number of downstream tasks. Further, we perform a listening test which demonstrates that our approach is able to convincingly style match a multi-band compressor effect.
</details>
<details>
<summary>摘要</summary>
数字音频效果广泛用于音频工程师修改音频数据的音色和时间特性。然而，这些效果可能有许多参数，可能让新手学习困难，并对专业人士增加创作压力。近些年来，有很多尝试使用深度学习来获取音频效果的低级参数配置。然而，当前的方法通常使用不可预测的黑盒技术或需要实现效果在某些自动梯度框架中。在这种情况下，我们提出了一种深度学习方法来实现音频生产风格匹配。这种方法可以与大多数常用框架中的效果结合使用，只需要参数在维度上有连续的域。此外，我们的方法包括多种类型的效果匹配，许多其中是使用可微函数很难或不可能地近似的。我们表明，我们的音频嵌入方法创造出了逻辑编码的时域信息，可以用于许多下游任务。此外，我们进行了一次听测，证明我们的方法能够成功地匹配多个批处器效果。
</details></li>
</ul>
<hr>
<h2 id="Sheaf-Hypergraph-Networks"><a href="#Sheaf-Hypergraph-Networks" class="headerlink" title="Sheaf Hypergraph Networks"></a>Sheaf Hypergraph Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17116">http://arxiv.org/abs/2309.17116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iulia Duta, Giulia Cassarà, Fabrizio Silvestri, Pietro Liò</li>
<li>for: 这篇论文旨在提高高阶关系处理的能力，以推动各种需要结构化数据的领域的发展。</li>
<li>methods: 该论文引入细胞筛sheaf来增强高阶图的表示，并开发了两种不同的sheaf高阶图laplacian表示方法。</li>
<li>results: 对多个 benchmark数据集进行了广泛的实验，表明这种扩展可以显著提高模型性能，达到了当前 Literature 中常见的 Hypergraph Networks 的顶峰成绩。<details>
<summary>Abstract</summary>
Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs. We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higherorder connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures. We employ these sheaf hypergraph Laplacians to design two categories of models: Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks. These models generalize classical Hypergraph Networks often found in the literature. Through extensive experimentation, we show that this generalization significantly improves performance, achieving top results on multiple benchmark datasets for hypergraph node classification.
</details>
<details>
<summary>摘要</summary>
高阶关系广泛存在在自然中，许多现象具有复杂的互动，超出简单对应关系。因此，进步在高阶处理方面可以推动不同领域的数据结构发展。现有的方法通常使用 гиперграм（hypergraphs）来表示这些互动。我们在这些 гиперграм 上引入细胞层（cellular sheaves），一种数学建构，以添加额外结构，保持了本地、高阶连接的性质。 Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modeling complex data structures. We employ these sheaf hypergraph Laplacians to design two categories of models: Sheaf Hypergraph Neural Networks and Sheaf Hypergraph Convolutional Networks. These models generalize classical Hypergraph Networks often found in the literature. Through extensive experimentation, we show that this generalization significantly improves performance, achieving top results on multiple benchmark datasets for hypergraph node classification.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Collaborative-Learning-Methods-Cost-Effectiveness-for-Prostate-Segmentation"><a href="#Benchmarking-Collaborative-Learning-Methods-Cost-Effectiveness-for-Prostate-Segmentation" class="headerlink" title="Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation"></a>Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17097">http://arxiv.org/abs/2309.17097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucia Innocenti, Michela Antonelli, Francesco Cremonesi, Kenaan Sarhan, Alejandro Granados, Vicky Goh, Sebastien Ourselin, Marco Lorenzi</li>
<li>for: 这种研究旨在解决医疗数据分布在多个医院中的问题，并且由于隐私法规限制，访问这些数据是困难的。</li>
<li>methods: 这种研究使用了合作学习（CL）方法，让医院合作解决问题，而不需要直接分享本地数据。</li>
<li>results: 我们的实验结果表明，在考虑的实际场景下，CBM可以提供与FL相等或更好的结果，而且具有高效性。这些结果表明，共识模式可能是FL的可行替代方案。<details>
<summary>Abstract</summary>
Healthcare data is often split into medium/small-sized collections across multiple hospitals and access to it is encumbered by privacy regulations. This brings difficulties to use them for the development of machine learning and deep learning models, which are known to be data-hungry. One way to overcome this limitation is to use collaborative learning (CL) methods, which allow hospitals to work collaboratively to solve a task, without the need to explicitly share local data.   In this paper, we address a prostate segmentation problem from MRI in a collaborative scenario by comparing two different approaches: federated learning (FL) and consensus-based methods (CBM).   To the best of our knowledge, this is the first work in which CBM, such as label fusion techniques, are used to solve a problem of collaborative learning. In this setting, CBM combine predictions from locally trained models to obtain a federated strong learner with ideally improved robustness and predictive variance properties.   Our experiments show that, in the considered practical scenario, CBMs provide equal or better results than FL, while being highly cost-effective. Our results demonstrate that the consensus paradigm may represent a valid alternative to FL for typical training tasks in medical imaging.
</details>
<details>
<summary>摘要</summary>
医疗数据经常被分布在多个医院中的中型/小型集合中，并且由于隐私法规的限制，使得使用这些数据进行机器学习和深度学习模型的开发变得困难。为了解决这些问题，可以使用合作学习（CL）方法，让医院合作解决问题，无需显式地分享本地数据。在这篇论文中，我们研究了一个肾脏分割问题，该问题来自于MRI成像，在合作enario中进行比较两种不同的方法：联邦学习（FL）和consensus-based方法（CBM）。我们知道，这是首次在合作学习中使用CBM方法，例如标签融合技术，来解决一个协作学习问题。在这种设置下，CBM方法将本地训练的模型 predictions合并，以获得一个联邦强学习模型，其具有 идеаль的 robustness和预测偏差性质。我们的实验结果显示，在我们考虑的实际enario中，CBMs 提供了等效或更好的结果，而且非常经济。我们的结果表明，consensus paradigm可能是FL的有效替代方案，用于医学成像领域的典型训练任务。
</details></li>
</ul>
<hr>
<h2 id="Too-Big-so-Fail-–-Enabling-Neural-Construction-Methods-to-Solve-Large-Scale-Routing-Problems"><a href="#Too-Big-so-Fail-–-Enabling-Neural-Construction-Methods-to-Solve-Large-Scale-Routing-Problems" class="headerlink" title="Too Big, so Fail? – Enabling Neural Construction Methods to Solve Large-Scale Routing Problems"></a>Too Big, so Fail? – Enabling Neural Construction Methods to Solve Large-Scale Routing Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17089">http://arxiv.org/abs/2309.17089</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jokofa/nrr">https://github.com/jokofa/nrr</a></li>
<li>paper_authors: Jonas K. Falkner, Lars Schmidt-Thieme</li>
<li>for: 解决NP-hard combinatorial优化问题，尤其是交通问题（Vehicle Routing Problems，VRP）。</li>
<li>methods: 使用深度学习方法，特别是顺序神经建构方法，通常通过反射学习进行训练。</li>
<li>results: 提出一种基于“萧瑟重建原理”的神经建构方法，可以在大规模问题上表现更好，并且在四个不同的数据集上进行了 thorought 的实验，证明了这种方法的优势。<details>
<summary>Abstract</summary>
In recent years new deep learning approaches to solve combinatorial optimization problems, in particular NP-hard Vehicle Routing Problems (VRP), have been proposed. The most impactful of these methods are sequential neural construction approaches which are usually trained via reinforcement learning. Due to the high training costs of these models, they usually are trained on limited instance sizes (e.g. serving 100 customers) and later applied to vastly larger instance size (e.g. 2000 customers). By means of a systematic scale-up study we show that even state-of-the-art neural construction methods are outperformed by simple heuristics, failing to generalize to larger problem instances. We propose to use the ruin recreate principle that alternates between completely destroying a localized part of the solution and then recreating an improved variant. In this way, neural construction methods like POMO are never applied to the global problem but just in the reconstruction step, which only involves partial problems much closer in size to their original training instances. In thorough experiments on four datasets of varying distributions and modalities we show that our neural ruin recreate approach outperforms alternative forms of improving construction methods such as sampling and beam search and in several experiments also advanced local search approaches.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习方法来解决 combinatorial optimization 问题，尤其是 NP-hard 的 Vehicle Routing Problems (VRP) ，得到了广泛的研究。最有影响的这些方法是序列神经建构方法，通常通过 reinforcement learning 进行训练。由于这些模型的训练成本高，通常只能在限制 instance size （例如，服务 100 个客户）上训练，然后将其应用到远大得多的 instance size （例如，2000 个客户）。通过系统性的扩展研究，我们发现了一些 state-of-the-art 神经建构方法无法泛化到更大的问题实例。我们提议使用灭亡重建原则，该原则是 alternate between completely destroying 地方化的部分解决方案，然后创建一个改进的变体。这样，神经建构方法如 POMO 只在重建步骤中被应用，该步骤只涉及到部分问题的大小与其原始训练实例相似。在四个不同的数据集和模式下进行了严格的实验，我们发现了我们的神经灭亡重建方法在 sampling 和 beam search 以及一些先进的本地搜索方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="From-Empirical-Measurements-to-Augmented-Data-Rates-A-Machine-Learning-Approach-for-MCS-Adaptation-in-Sidelink-Communication"><a href="#From-Empirical-Measurements-to-Augmented-Data-Rates-A-Machine-Learning-Approach-for-MCS-Adaptation-in-Sidelink-Communication" class="headerlink" title="From Empirical Measurements to Augmented Data Rates: A Machine Learning Approach for MCS Adaptation in Sidelink Communication"></a>From Empirical Measurements to Augmented Data Rates: A Machine Learning Approach for MCS Adaptation in Sidelink Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17086">http://arxiv.org/abs/2309.17086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fraunhoferhhi/sidelink-mcs-measurements">https://github.com/fraunhoferhhi/sidelink-mcs-measurements</a></li>
<li>paper_authors: Asif Abdullah Rokoni, Daniel Schäufele, Martin Kasparick, Sławomir Stańczak</li>
<li>for: 这个论文的目的是提出一种机器学习方法来预测适合的幂等编码和传输方案（MCS）水平，以满足更高的数据速率需求。</li>
<li>methods: 该论文提出使用机器学习方法预测适合的MCS水平，并评估了不同算法的效果。同时，它还使用了量谱预测来提高预测性能。</li>
<li>results: 论文表明，使用机器学习方法可以对MCS水平进行适应性预测，并且可以实现 significanly 提高预测性能。此外，论文还提供了一个大量的实际驱动测试数据集，并将其公开发布。<details>
<summary>Abstract</summary>
Due to the lack of a feedback channel in the C-V2X sidelink, finding a suitable modulation and coding scheme (MCS) is a difficult task. However, recent use cases for vehicle-to-everything (V2X) communication with higher demands on data rate necessitate choosing the MCS adaptively. In this paper, we propose a machine learning approach to predict suitable MCS levels. Additionally, we propose the use of quantile prediction and evaluate it in combination with different algorithms for the task of predicting the MCS level with the highest achievable data rate. Thereby, we show significant improvements over conventional methods of choosing the MCS level. Using a machine learning approach, however, requires larger real-world data sets than are currently publicly available for research. For this reason, this paper presents a data set that was acquired in extensive drive tests, and that we make publicly available.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:由于C-V2X侧链缺乏反馈通道，选择适当的modulation and coding scheme（MCS）是一个困难的任务。然而，Recent vehicle-to-everything（V2X）通信use cases with higher demands on data rate require adaptive selection of the MCS. In this paper, we propose a machine learning approach to predict suitable MCS levels. In addition, we propose the use of quantile prediction and evaluate it in combination with different algorithms for the task of predicting the MCS level with the highest achievable data rate. As a result, we show significant improvements over conventional methods of choosing the MCS level. However, using a machine learning approach requires larger real-world data sets than are currently publicly available for research. Therefore, this paper presents a data set that was acquired in extensive drive tests and makes it publicly available.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Models-as-Stochastic-Quantization-in-Lattice-Field-Theory"><a href="#Diffusion-Models-as-Stochastic-Quantization-in-Lattice-Field-Theory" class="headerlink" title="Diffusion Models as Stochastic Quantization in Lattice Field Theory"></a>Diffusion Models as Stochastic Quantization in Lattice Field Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17082">http://arxiv.org/abs/2309.17082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingxiao Wang, Gert Aarts, Kai Zhou</li>
<li>for: 这个论文主要是为了研究生成扩散模型（DM）和随机量化（SQ）之间的直接连接。</li>
<li>methods: 这篇论文使用了数值仿真来实现DM，通过精确地模拟某种随机过程，生成样本从先验分布，以模拟目标分布。</li>
<li>results: 数值仿真表明，DM可以作为全球抽样器，生成二维$\phi^4$理论中的量子网格场 configurations。此外，DM还可以显著减少自相关时间，特别是在MCMC算法在极点区域中经历极慢减速。这些发现可能会推动量子网格场 simulations的进一步发展，特别是在生成大 ensemble是昂贵的情况下。<details>
<summary>Abstract</summary>
In this work, we establish a direct connection between generative diffusion models (DMs) and stochastic quantization (SQ). The DM is realized by approximating the reversal of a stochastic process dictated by the Langevin equation, generating samples from a prior distribution to effectively mimic the target distribution. Using numerical simulations, we demonstrate that the DM can serve as a global sampler for generating quantum lattice field configurations in two-dimensional $\phi^4$ theory. We demonstrate that DMs can notably reduce autocorrelation times in the Markov chain, especially in the critical region where standard Markov Chain Monte-Carlo (MCMC) algorithms experience critical slowing down. The findings can potentially inspire further advancements in lattice field theory simulations, in particular in cases where it is expensive to generate large ensembles.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们建立了生成扩散模型（DM）和随机量化（SQ）之间的直接连接。DM通过近似逆射扩散过程的斜率方程，生成样本从先验分布，有效地模拟目标分布。通过数值实验，我们示出了DM可以作为全球抽取器，生成二维$\phi^4$理论中的量子核场配置。我们还证明了DM可以明显减少自相关时间在马尔可夫链中，特别是在 kritical 区域，标准马尔可夫链 Monte-Carlo（MCMC）算法经历了极限减速。这些发现可能会推动更多的链场理论仿真，特别是在生成大集的情况下。
</details></li>
</ul>
<hr>
<h2 id="On-the-Power-of-the-Weisfeiler-Leman-Test-for-Graph-Motif-Parameters"><a href="#On-the-Power-of-the-Weisfeiler-Leman-Test-for-Graph-Motif-Parameters" class="headerlink" title="On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters"></a>On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17053">http://arxiv.org/abs/2309.17053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Lanzinger, Pablo Barceló</li>
<li>for: 这项研究的目的是解释 Weisfeiler-Leman（$k$WL）测试如何识别不同图形的特征。</li>
<li>methods: 这项研究使用图神经网络（GNNs）的核心理论和 $k$WL 测试来研究图形的特征分辨率。</li>
<li>results: 这项研究提供了一种精确地 caracterize 图形动作参数的方法，并证明了在某些情况下，可以使用 GNN 的最后一层本地信息来计算图形中特定 Pattern 的出现次数。<details>
<summary>Abstract</summary>
Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $P$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive problem: "graph motif parameters". In this paper, we provide a precise characterization of the WL-dimension of labeled graph motif parameters. As specific instances of this result, we obtain characterizations of the WL-dimension of the subgraph counting and induced subgraph counting problem for every labeled pattern $P$. We additionally demonstrate that in cases where the $k$WL test distinguishes between graphs with varying occurrences of a pattern $P$, the exact number of occurrences of $P$ can be computed uniformly using only local information of the last layer of a corresponding GNN. We finally delve into the challenge of recognizing the WL-dimension of various graph parameters. We give a polynomial time algorithm for determining the WL-dimension of the subgraph counting problem for given pattern $P$, answering an open question from previous work.
</details>
<details>
<summary>摘要</summary>
研究领域内的核心研究表明，图 neuronal networks（GNNs）的表达能力与 $k$-dimensional Weisfeiler-Leman（$k$WL）测试之间存在直接的对应关系。这种关系在研究图的特定属性表征方面产生了新的兴趣。我们的研究重点在于确定 Pattern counting 问题中的最小维度 $k$，以便使 $k$WL 测试能够分辨不同 Pattern 图的数量。我们称这个最小维度为 Pattern 图的 WL 维度。这种问题包括两个不同的 counted 问题，即 subgraph counting 和 induced subgraph counting。尽管这两个问题看起来像是独立的挑战，但它们实际上是图 моти夫参数的一部分。在这篇论文中，我们提供了图 моти夫参数的准确特征化，包括 subgraph counting 和 induced subgraph counting 问题的 WL 维度特征化。此外，我们还证明在 $k$WL 测试可以分辨不同 Pattern 图的情况下，可以使用 GNN 的最后一层本地信息来计算 Pattern 图的具体出现次数。最后，我们考虑了识别不同图参数的 WL 维度的挑战。我们提供了对 Pattern 图 counting 问题的 polynomial time 算法，解决了之前的开题。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Agnostic-Learning-with-Average-Smoothness"><a href="#Efficient-Agnostic-Learning-with-Average-Smoothness" class="headerlink" title="Efficient Agnostic Learning with Average Smoothness"></a>Efficient Agnostic Learning with Average Smoothness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17016">http://arxiv.org/abs/2309.17016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Hanneke, Aryeh Kontorovich, Guy Kornowski</li>
<li>for: 本研究探讨分布自由非 Parametric 回归，基于 Ashlagi et al. (2021) 提出的平均稳定性概念，测量函数对于未知的基础分布的”有效”稳定性。</li>
<li>methods: 我们使用分布自由的均值渐近级数 bound，并提供了计算效率高的agnostic学习算法。</li>
<li>results: 我们完全填充了这些漏洞，提供了分布自由的均值渐近级数 bound，并与计算效率高的agnostic学习算法相匹配。我们的结果适用于任何totally bounded metric space，并且显示了实现了对于 realizable 学习的保证。<details>
<summary>Abstract</summary>
We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the "effective" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.   In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realizable learning of average-smooth functions transfer to the agnostic setting. At the heart of our proof, we establish the uniform convergence rate of a function class in terms of its bracketing entropy, which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究分布自由非 Parametric 回归，基于 Ashlagi 等 (2021) 提出的平均滑动性概念，该概念测量函数对于未知的平均分布下的"有效"滑动性。而 Hanneke 等 (2023) 的最近研究已经在可 realizable 情况下提供了紧Binding的 uniform 收敛 bounds 和可实现的学习算法，但这两个结果目前在无知（i.e. 噪声）情况下缺乏对应的结果。在这个工作中，我们完全填充了这些差距。首先，我们提供了分布自由的 uniform 收敛 bound  для average-smoothness 类型在无知情况下。其次，我们与 derived sample complexity 匹配了一个可实现的 agnostic 学习算法。我们的结果，表示在任何完全度 bounded  метри空间上，对于内在的几何结构和数据而言，可以证明 realizable 学习 average-smooth functions 的 guarantees 在无知情况下也适用。在我们的证明中，我们使用函数类型的 bracketing entropy 来证明 uniform 收敛率，这可能是独立的兴趣点。
</details></li>
</ul>
<hr>
<h2 id="Feature-Cognition-Enhancement-via-Interaction-Aware-Automated-Transformation"><a href="#Feature-Cognition-Enhancement-via-Interaction-Aware-Automated-Transformation" class="headerlink" title="Feature Cognition Enhancement via Interaction-Aware Automated Transformation"></a>Feature Cognition Enhancement via Interaction-Aware Automated Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17011">http://arxiv.org/abs/2309.17011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ehtesam3154/inhrecon">https://github.com/ehtesam3154/inhrecon</a></li>
<li>paper_authors: Ehtesamul Azim, Dongjie Wang, Kunpeng Liu, Wei Zhang, Yanjie Fu</li>
<li>for: This paper aims to address the challenges of representation learning in machine learning, specifically the issues of heavy reliance on manual feature engineering, lack of explainability, and inflexible feature space reconstruction.</li>
<li>methods: The proposed approach is based on interaction-aware reinforcement generation, which involves creating meaningful features and controlling feature set size through selection. The authors use a hierarchical reinforcement learning structure with cascading Markov Decision Processes to automate feature and operation selection, as well as feature crossing.</li>
<li>results: The authors conduct extensive experiments to validate their proposed approach, demonstrating the effectiveness of their method in generating intelligible and efficient feature spaces that emulate human decision-making.<details>
<summary>Abstract</summary>
Creating an effective representation space is crucial for mitigating the curse of dimensionality, enhancing model generalization, addressing data sparsity, and leveraging classical models more effectively. Recent advancements in automated feature engineering (AutoFE) have made significant progress in addressing various challenges associated with representation learning, issues such as heavy reliance on intensive labor and empirical experiences, lack of explainable explicitness, and inflexible feature space reconstruction embedded into downstream tasks. However, these approaches are constrained by: 1) generation of potentially unintelligible and illogical reconstructed feature spaces, stemming from the neglect of expert-level cognitive processes; 2) lack of systematic exploration, which subsequently results in slower model convergence for identification of optimal feature space. To address these, we introduce an interaction-aware reinforced generation perspective. We redefine feature space reconstruction as a nested process of creating meaningful features and controlling feature set size through selection. We develop a hierarchical reinforcement learning structure with cascading Markov Decision Processes to automate feature and operation selection, as well as feature crossing. By incorporating statistical measures, we reward agents based on the interaction strength between selected features, resulting in intelligent and efficient exploration of the feature space that emulates human decision-making. Extensive experiments are conducted to validate our proposed approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Generation of potentially unintelligible and illogical reconstructed feature spaces, stemming from the neglect of expert-level cognitive processes;2. Lack of systematic exploration, which subsequently results in slower model convergence for identification of optimal feature space.To address these challenges, we introduce an interaction-aware reinforcement generation perspective. We redefine feature space reconstruction as a nested process of creating meaningful features and controlling feature set size through selection. We develop a hierarchical reinforcement learning structure with cascading Markov Decision Processes to automate feature and operation selection, as well as feature crossing. By incorporating statistical measures, we reward agents based on the interaction strength between selected features, resulting in intelligent and efficient exploration of the feature space that emulates human decision-making.Extensive experiments are conducted to validate our proposed approach.</details></li>
</ol>
<hr>
<h2 id="Deep-Representation-Learning-for-Prediction-of-Temporal-Event-Sets-in-the-Continuous-Time-Domain"><a href="#Deep-Representation-Learning-for-Prediction-of-Temporal-Event-Sets-in-the-Continuous-Time-Domain" class="headerlink" title="Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain"></a>Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17009">http://arxiv.org/abs/2309.17009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paragduttaiisc/temporal_event_set_modeling">https://github.com/paragduttaiisc/temporal_event_set_modeling</a></li>
<li>paper_authors: Parag Dutta, Kawin Mayilvaghanan, Pratyaksha Sinha, Ambedkar Dukkipati</li>
<li>for: 预测或预测多个事件的发生</li>
<li>methods: 使用Temporal Point Processes (TPP)模型，并 incorporate contextual event embeddings、 temporal information和域特征来模型时间事件集</li>
<li>results: 比较 existed方法，our proposed approach在多个 dataset上进行了广泛的实验，并达到了较高的预测精度和计算效率<details>
<summary>Abstract</summary>
Temporal Point Processes (TPP) play an important role in predicting or forecasting events. Although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. For instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. Similarly people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Naive approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. Our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. We demonstrate the effectiveness of our approach through extensive experiments on multiple datasets, showing that our model outperforms existing methods in terms of prediction metrics and computational efficiency. To the best of our knowledge, this is the first work that solves the problem of predicting event set intensities in the continuous time domain by using TPPs.
</details>
<details>
<summary>摘要</summary>
temporal point processes (TPP) 扮演着重要的角色在预测或预测事件中。although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. for instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. similarly, people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Naive approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. we demonstrate the effectiveness of our approach through extensive experiments on multiple datasets, showing that our model outperforms existing methods in terms of prediction metrics and computational efficiency. to the best of our knowledge, this is the first work that solves the problem of predicting event set intensities in the continuous time domain by using TPPs.Here's the word-for-word translation of the text into Simplified Chinese:temporal point processes (TPP) 扮演着重要的角色在预测或预测事件中。although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. for instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. similarly, people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Naive approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. we demonstrate the effectiveness of our approach through extensive experiments on multiple datasets, showing that our model outperforms existing methods in terms of prediction metrics and computational efficiency. to the best of our knowledge, this is the first work that solves the problem of predicting event set intensities in the continuous time domain by using TPPs.
</details></li>
</ul>
<hr>
<h2 id="Consistency-Models-as-a-Rich-and-Efficient-Policy-Class-for-Reinforcement-Learning"><a href="#Consistency-Models-as-a-Rich-and-Efficient-Policy-Class-for-Reinforcement-Learning" class="headerlink" title="Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning"></a>Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16984">http://arxiv.org/abs/2309.16984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Ding, Chi Jin</li>
<li>for: 这个论文主要针对的是用Score-based生成模型进行多Modal数据的模型化，以及在强化学习中使用iterative sampling。</li>
<li>methods: 该论文提出了一种效果很好的actor-critic风格的算法，即一致策略，用于三种常见的强化学习Setting：offline、offline-to-online和online。</li>
<li>results: 在offlineRL中，生成模型作为策略从多Modal数据中表达了表达能力。在offline-to-onlineRL中，一致策略比Diffusion策略更快速，性能相似。在onlineRL中，一致策略demonstrated significant speedup和even higher average performance than Diffusion策略。<details>
<summary>Abstract</summary>
Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.
</details>
<details>
<summary>摘要</summary>
Score-based生成模型如扩散模型在图像生成和强化学习中证明有效，但扩散模型的推理过程可能会慢，这限制了它在强化学习中使用的可行性。我们提议使用一种高效又表达力强的策略表示方式，即一致策略，并使用actor-critic风格的算法来解决三种常见强化学习设置：离线、离线到在线和在线。在离线强化学习中，我们示出了生成模型作为策略从多Modal数据中表达的表达力。在离线到在线强化学习中，一致策略与扩散策略相比， computationally更高效，性能相似。在在线强化学习中，一致策略示出了明显的加速和比扩散策略更高的平均性能。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-and-In-depth-Performance-Study-of-Large-Language-Models-on-Habana-Gaudi-Processors"><a href="#Benchmarking-and-In-depth-Performance-Study-of-Large-Language-Models-on-Habana-Gaudi-Processors" class="headerlink" title="Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors"></a>Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16976">http://arxiv.org/abs/2309.16976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengming Zhang, Baixi Sun, Xiaodong Yu, Zhen Xie, Weijian Zheng, Kamil Iskra, Pete Beckman, Dingwen Tao</li>
<li>for: 本文旨在探讨使用特殊AI硬件加速器 Habana GAUDI 加速Transformer模型，解决高计算复杂性和资源需求的问题。</li>
<li>methods: 本文使用Matrix Multiplication Engine (MME)和一群可编程的Tensor Processing Cores (TPC)进行加速。</li>
<li>results: 本文对GAUDI进行了完整的性能比较，揭示了MME和TPC的相对优劣点。此外，本文还提出了优化MME和TPC使用的策略，并评估了Transformer模型在GAUDI上的性能。<details>
<summary>Abstract</summary>
Transformer models have achieved remarkable success in various machine learning tasks but suffer from high computational complexity and resource requirements. The quadratic complexity of the self-attention mechanism further exacerbates these challenges when dealing with long sequences and large datasets. Specialized AI hardware accelerators, such as the Habana GAUDI architecture, offer a promising solution to tackle these issues. GAUDI features a Matrix Multiplication Engine (MME) and a cluster of fully programmable Tensor Processing Cores (TPC). This paper explores the untapped potential of using GAUDI processors to accelerate Transformer-based models, addressing key challenges in the process. Firstly, we provide a comprehensive performance comparison between the MME and TPC components, illuminating their relative strengths and weaknesses. Secondly, we explore strategies to optimize MME and TPC utilization, offering practical insights to enhance computational efficiency. Thirdly, we evaluate the performance of Transformers on GAUDI, particularly in handling long sequences and uncovering performance bottlenecks. Lastly, we evaluate the end-to-end performance of two Transformer-based large language models (LLM) on GAUDI. The contributions of this work encompass practical insights for practitioners and researchers alike. We delve into GAUDI's capabilities for Transformers through systematic profiling, analysis, and optimization exploration. Our study bridges a research gap and offers a roadmap for optimizing Transformer-based model training on the GAUDI architecture.
</details>
<details>
<summary>摘要</summary>
启示器模型已经在不同的机器学习任务中获得了惊人的成功，但它们受到高度的计算复杂性和资源需求的限制。它们的自注意机制的quadratic complexity进一步增加了对于长序列和大量数据的挑战。特种的AI硬件加速器，如Habana GAUDI架构，提供了一个有前途的解决方案。GAUDI架构包括一个矩阵 multiply 引擎（MME）和一群可编程的tensor处理核心（TPC）。本文探讨了使用GAUDI处理器加速启示器模型，解决关键的挑战。首先，我们提供了MME和TPC组件之间的完整性比较，揭示它们的相对优劣点。其次，我们探讨了如何优化MME和TPC的使用，提供了实践的指导，以提高计算效率。第三，我们评估了Transformers在GAUDI上的性能，特别是处理长序列的能力。最后，我们评估了两种基于启示器的大型自然语言模型（LLM）在GAUDI上的综合性能。本文的贡献包括实践的指导和研究人员之间的交流，我们通过系统性的探讨、分析和优化探讨GAUDI架构对启示器模型的可能性。我们的研究填补了一个研究漏洞，并提供了优化启示器模型在GAUDI架构上的路线图。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Offline-to-Online-Reinforcement-Learning-via-Uncertainty-and-Smoothness"><a href="#Towards-Robust-Offline-to-Online-Reinforcement-Learning-via-Uncertainty-and-Smoothness" class="headerlink" title="Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness"></a>Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16973">http://arxiv.org/abs/2309.16973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Wen, Xudong Yu, Rui Yang, Chenjia Bai, Zhen Wang</li>
<li>for: 提高RL中的样本效率，使用offlineRL和onlineRL的组合，并在有限的在线交互中提高offline训练的agent。</li>
<li>methods: 提出了Robust Offline-to-Online（RO2O）算法，通过不确定性和精度的追加，使RO2O能够在线上适应而不需要特殊的学习目标变化。</li>
<li>results: RO2O在线上适应中实现了稳定的学习进程，并在限制的在线交互情况下达到了显著的改进。<details>
<summary>Abstract</summary>
To obtain a near-optimal policy with fewer interactions in Reinforcement Learning (RL), a promising approach involves the combination of offline RL, which enhances sample efficiency by leveraging offline datasets, and online RL, which explores informative transitions by interacting with the environment. Offline-to-Online (O2O) RL provides a paradigm for improving an offline trained agent within limited online interactions. However, due to the significant distribution shift between online experiences and offline data, most offline RL algorithms suffer from performance drops and fail to achieve stable policy improvement in O2O adaptation. To address this problem, we propose the Robust Offline-to-Online (RO2O) algorithm, designed to enhance offline policies through uncertainty and smoothness, and to mitigate the performance drop in online adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty penalty and adversarial samples for policy and value smoothness, which enable RO2O to maintain a consistent learning procedure in online adaptation without requiring special changes to the learning objective. Theoretical analyses in linear MDPs demonstrate that the uncertainty and smoothness lead to a tighter optimality bound in O2O against distribution shift. Experimental results illustrate the superiority of RO2O in facilitating stable offline-to-online learning and achieving significant improvement with limited online interactions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>在强化学习（RL）中，以 комбинаción of offline RL 和 online RL 为例，可以提高样本效率并探索有用的转移。Offline-to-Online（O2O）RL 提供了一种改进 offline 训练的机制，但由于在线经验和 offline 数据之间的分布偏移，大多数 offline RL 算法会导致性能下降并在 O2O 适应中失去稳定的政策改进。为解决这个问题，我们提出了 Robust Offline-to-Online（RO2O）算法，通过不确定性和简直性来增强 offline 政策，并在在线适应中维护一个稳定的学习过程。特别是，RO2O 使用 Q-ensemble  для不确定性 penalty 和对策和价值简直性，这使得 RO2O 可以在在线适应中维护一个稳定的学习过程，而无需特殊地改变学习目标。理论分析在线 MDP 中表明，不确定性和简直性导致在 O2O 中对分布偏移的优质环境。实验结果表明，RO2O 可以在有限的在线交互下实现稳定的 offline-to-online 学习和显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Multi-Resolution-Active-Learning-of-Fourier-Neural-Operators"><a href="#Multi-Resolution-Active-Learning-of-Fourier-Neural-Operators" class="headerlink" title="Multi-Resolution Active Learning of Fourier Neural Operators"></a>Multi-Resolution Active Learning of Fourier Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16971">http://arxiv.org/abs/2309.16971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shibo Li, Xin Yu, Wei Xing, Mike Kirby, Akil Narayan, Shandian Zhe</li>
<li>For: 提高 FNO 的训练和预测效率，降低数据成本。* Methods: 动态选择输入函数和分辨率，使用ensemble Monte-Carlo实现有效 posterior inference算法，使用 moments matching和矩阵 determinant lemma 实现可追踪的、高效的实用性计算。* Results: 在多个 benchmark 运算符学习任务中表现优秀，并且可以避免高分辨率查询过早停满问题。<details>
<summary>Abstract</summary>
Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a cost annealing framework to avoid over-penalizing high-resolution queries at the early stage. The over-penalization is severe when the cost difference is significant between the resolutions, which renders active learning often stuck at low-resolution queries and inferior performance. Our method overcomes this problem and applies to general multi-fidelity active learning and optimization problems. We have shown the advantage of our method in several benchmark operator learning tasks.
</details>
<details>
<summary>摘要</summary>
法ouvrier neural operator（FNO）是一种受欢迎的运算学框架，不仅可以在许多任务中达到状态机器人的性能，而且在训练和预测中也具有高效性。然而，在实践中收集FNO的训练数据可能会成为一个昂贵的瓶颈，因为它们经常需要昂贵的物理 simulate。为了解决这个问题，我们提出了多resolution active learning of FNO（MRA-FNO），它可以在最小化数据成本的情况下，在训练和预测中尽可能高效地学习。我们提出了一种probabilistic multi-resolution FNO，并使用ensemble Monte-Carlo来开发一个有效的 posterior inference 算法。为了实施活动学习，我们在每一步中 maximize一个利用函数和解构函数的utilit-cost比，以获取新的示例和分辨率。我们使用 moments matching和矩阵 determinant lemma来实现可迭代、高效的utilit computation。此外，我们开发了一个cost annealing框架，以避免在早期stage 高分辨率查询时的过惩罚。当分辨率之间的成本差距较大时，活动学习经常会被困在低分辨率查询中，导致性能差。我们的方法可以在多个多fidelity active learning和优化问题中应用。我们在一些benchmark operator learning任务中表明了我们的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Controlling-Continuous-Relaxation-for-Combinatorial-Optimization"><a href="#Controlling-Continuous-Relaxation-for-Combinatorial-Optimization" class="headerlink" title="Controlling Continuous Relaxation for Combinatorial Optimization"></a>Controlling Continuous Relaxation for Combinatorial Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16965">http://arxiv.org/abs/2309.16965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuma Ichikawa</li>
<li>for: 解决大规模的组合优化问题 (CO)，physics-inspired GNN (PI-GNN) 算法在大规模 CO 问题中表现出色，但是对于稠密图上的 CO 问题，PI-GNN 算法的性能尚未得到了充分的探讨。此外，PI-GNN 算法使用了一种允许拟合的策略，可能会导致解的不稳定性。</li>
<li>methods: 本文提出了两种方法来解决这些问题：首先，引入了一个新的罚项来控制变量的连续性和离散性，从而消除了本地解；其次，提出了一种新的连续缓和热成法（CRA），该法在找到解之前先优先级化连续解，然后逐渐增加罚项，直到变量接近离散值，从而消除了人工将连续解转换为原始离散解的需要。</li>
<li>results: 实验表明，在稠密图上的 CO 问题上，本文提出的方法可以获得更好的结果，而且在相对稀疏图上的 CO 问题上也表现出色。此外，计算时间呈线性增长，与PI-GNN算法一样。<details>
<summary>Abstract</summary>
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of relaxed variables while avoiding the local solution: (i) introducing a new penalty term that controls the continuity and discreteness of the relaxed variables and eliminates the local solution; (ii) proposing a new continuous relaxation annealing (CRA) strategy. This new annealing first prioritizes continuous solutions and intensifies exploration by leveraging the continuity while avoiding the local solution and then schedules the penalty term for prioritizing a discrete solution until the relaxed variables are almost discrete values, which eliminates the need for an artificial transformation from the continuous to the original discrete space. Empirically, better results are obtained for CO problems on the dense graphs, where the PI-GNN solver struggles to find reasonable solutions, and for those on relatively sparse graphs. Furthermore, the computational time scaling is identical to that of the PI-GNN solver.
</details>
<details>
<summary>摘要</summary>
近期的 combinatorial optimization（CO）问题的进展强调了图神经网络（GNN）的潜在能力。physics-inspired GNN（PI-GNN）解决方案，通过无监督学习找到approximate solutions，在大规模CO问题上吸引了 significative attention。然而，有限的讨论是关于PI-GNN解决方案在密集图上的性能，以及greedy算法在密集图上的性能下降。此外，由于PI-GNN解决方案使用了放松策略，因此需要人工将学习后的练习数据转换回原始的离散空间，可能会损害解决方案的稳定性。本文通过数字实验表明，PI-GNN解决方案在密集图上的CO问题中可能会被困在早期学习阶段的本地解。然后，我们解决这些问题：（i）引入一个新的罚项，控制放松变量的连续性和离散性，并消除本地解；（ii）提出一种新的练习气化（CRA）策略。这种新策略在优先级顺序中允许连续解，并通过强化探索来避免本地解，然后在变量接近离散值时间间隔出罚项，以消除人工将练习数据转换回原始离散空间的需要。实际上，对CO问题的密集图和相对稀疏图进行实验，我们可以获得更好的结果，而且计算时间协同PI-GNN解决方案。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Optimization-for-Adaptive-Attacks-on-Image-Watermarks"><a href="#Leveraging-Optimization-for-Adaptive-Attacks-on-Image-Watermarks" class="headerlink" title="Leveraging Optimization for Adaptive Attacks on Image Watermarks"></a>Leveraging Optimization for Adaptive Attacks on Image Watermarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16952">http://arxiv.org/abs/2309.16952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nils Lukas, Abdulrahman Diaa, Lucas Fenaux, Florian Kerschbaum</li>
<li>for: The paper is written to address the issue of untrustworthy users misusing image generators to create high-quality deepfakes and engage in online spam or disinformation campaigns.</li>
<li>methods: The paper proposes a method of watermarking to deter misuse by marking generated content with a hidden message, and uses an adaptive attack to evaluate the robustness of the watermarking algorithm.</li>
<li>results: The paper demonstrates that an adaptive attack can break all five surveyed watermarking methods at negligible degradation in image quality, emphasizing the need for more rigorous robustness testing against adaptive, learnable attackers.Here is the same information in Simplified Chinese:</li>
<li>for: 论文目的是解决不可靠用户利用图像生成器创造高质量的深伪和在线诈骗或假信息运动中使用图像生成器。</li>
<li>methods: 论文提出了一种防止诈骗的方法，即通过隐藏的消息标记生成的内容，并使用适应性攻击来评估水印算法的安全性。</li>
<li>results: 论文表明，适应性攻击可以破坏所有调查的五种水印方法，而且这些攻击不会导致图像质量明显下降，从而强调了对适应性攻击的更加严格的安全性测试。<details>
<summary>Abstract</summary>
Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in online spam or disinformation campaigns. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. A challenge when evaluating watermarking algorithms and their (adaptive) attacks is to determine whether an adaptive attack is optimal, i.e., it is the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack's parameters. We demonstrate for Stable Diffusion models that such an attacker can break all five surveyed watermarking methods at negligible degradation in image quality. These findings emphasize the need for more rigorous robustness testing against adaptive, learnable attackers.
</details>
<details>
<summary>摘要</summary>
不可靠用户可能会滥用图像生成器生成高质量的深伪图并进行在线垃圾或歪曲信息运动。水印可以防止这种滥用行为，通过将生成内容中加入隐藏的信息，并使用机密水印键来检测。水印的核心安全特性是 robustness，即攻击者只能通过重大干扰图像质量来逃脱检测。评估robustness需要设计适应攻击的特定水印算法。一个挑战是在评估水印算法和其适应攻击时，确定攻击是最佳的。我们解决这个问题，通过定义一个目标函数，然后将适应攻击看作优化问题。我们的适应攻击的核心思想是在本地复制秘密水印键，通过创建可微分的代理键来优化攻击参数。我们示例中，对于稳定扩散模型，攻击者可以破坏所评估的五种水印方法，而且这些攻击对图像质量的影响非常小。这些发现强调了对适应、学习型攻击的更加严格的Robustness测试。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Tides-and-Time-Machine-Learning-Triumph-in-Water-Quality"><a href="#Beyond-Tides-and-Time-Machine-Learning-Triumph-in-Water-Quality" class="headerlink" title="Beyond Tides and Time: Machine Learning Triumph in Water Quality"></a>Beyond Tides and Time: Machine Learning Triumph in Water Quality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16951">http://arxiv.org/abs/2309.16951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yinpuli/water-quality-prediction">https://github.com/yinpuli/water-quality-prediction</a></li>
<li>paper_authors: Yinpu Li, Siqi Mao, Yaping Yuan, Ziren Wang, Yixin Kang, Yuanxin Yao</li>
<li>for: 预测水质值的精度和可 interpretability</li>
<li>methods: 使用五种不同的预测模型，包括线性回归、Random Forest、XGBoost、LightGBM和MLP神经网络，对美国格鲁吉亚地区的pH值进行预测。</li>
<li>results: LightGBM模型表现最佳，实现最高的平均准确率；树型模型在解决回归问题方面表现出色，而MLP神经网络受到特征涨分的敏感性受到影响。<details>
<summary>Abstract</summary>
Water resources are essential for sustaining human livelihoods and environmental well being. Accurate water quality prediction plays a pivotal role in effective resource management and pollution mitigation. In this study, we assess the effectiveness of five distinct predictive models linear regression, Random Forest, XGBoost, LightGBM, and MLP neural network, in forecasting pH values within the geographical context of Georgia, USA. Notably, LightGBM emerges as the top performing model, achieving the highest average precision. Our analysis underscores the supremacy of tree-based models in addressing regression challenges, while revealing the sensitivity of MLP neural networks to feature scaling. Intriguingly, our findings shed light on a counterintuitive discovery: machine learning models, which do not explicitly account for time dependencies and spatial considerations, outperform spatial temporal models. This unexpected superiority of machine learning models challenges conventional assumptions and highlights their potential for practical applications in water quality prediction. Our research aims to establish a robust predictive pipeline accessible to both data science experts and those without domain specific knowledge. In essence, we present a novel perspective on achieving high prediction accuracy and interpretability in data science methodologies. Through this study, we redefine the boundaries of water quality forecasting, emphasizing the significance of data driven approaches over traditional spatial temporal models. Our findings offer valuable insights into the evolving landscape of water resource management and environmental protection.
</details>
<details>
<summary>摘要</summary>
水资源是人类生存和环境健康的重要保障。 precisione 预测水质是有效资源管理和污染防治的关键。 在本研究中，我们评估了五种不同的预测模型，包括线性回归、Random Forest、XGBoost、LightGBM 和 MLP神经网络，以预测格鲁吉亚（Georgia）地区的pH值。结果显示LightGBM 模型在预测pH值方面表现出色，得到了最高的平均准确率。我们的分析发现树状模型在 Addressing regression challenges 方面具有优势，而神经网络模型受到特征Scaling 的影响。另外，我们的发现推翻了传统假设：机器学习模型，不直接考虑时间和空间因素，在水质预测方面表现更高。这种对机器学习模型的发现挑战了传统的假设，并指出了其在实际应用中的潜在价值。我们的研究旨在建立一个可 accessible 的预测管道，以便无关域专业知识的人员和数据科学专家都可以使用。具体来说，我们在这种研究中提出了一种新的预测方法，即通过数据驱动的方法来实现高精度和可解释性。我们的发现对水资源管理和环境保护领域的发展具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Induction-Machine-Modelling"><a href="#Physics-Informed-Induction-Machine-Modelling" class="headerlink" title="Physics-Informed Induction Machine Modelling"></a>Physics-Informed Induction Machine Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16943">http://arxiv.org/abs/2309.16943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Shen, Yifan Zhou, Peng Zhang</li>
<li>for: 本研究开发了一个名为NeuIM的人工智能模型，用于实现基于人工智能的电磁变化 simulating。</li>
<li>methods: 本研究使用了物理知识驱动的机器学习方法，实现了IM机器的相域表现。</li>
<li>results: 实验结果显示，NeuIM可以实现高准确性和高效率的电磁变化 simulating，并且在没有数据的情况下仍能够正确预测IM机器的动作。<details>
<summary>Abstract</summary>
This rapid communication devises a Neural Induction Machine (NeuIM) model, which pilots the use of physics-informed machine learning to enable AI-based electromagnetic transient simulations. The contributions are threefold: (1) a formation of NeuIM to represent the induction machine in phase domain; (2) a physics-informed neural network capable of capturing fast and slow IM dynamics even in the absence of data; and (3) a data-physics-integrated hybrid NeuIM approach which is adaptive to various levels of data availability. Extensive case studies validate the efficacy of NeuIM and in particular, its advantage over purely data-driven approaches.
</details>
<details>
<summary>摘要</summary>
这种快速通信创造了一种神经起 induction machine（NeuIM）模型，该模型利用物理学 Informed机器学习来实现人工智能基于电磁脉冲 simulating。这个贡献有三个方面：1. 将NeuIM模型表示为频域中的induction machine;2. 一种能够捕捉快速和慢速IM动态的物理学 Informed神经网络;3. 一种可以适应不同数据可用性水平的数据-物理-混合NeuIM方法。广泛的案例研究证明了NeuIM的有效性，特别是与完全数据驱动方法相比的优势。
</details></li>
</ul>
<hr>
<h2 id="G4SATBench-Benchmarking-and-Advancing-SAT-Solving-with-Graph-Neural-Networks"><a href="#G4SATBench-Benchmarking-and-Advancing-SAT-Solving-with-Graph-Neural-Networks" class="headerlink" title="G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks"></a>G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16941">http://arxiv.org/abs/2309.16941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaoyu-li/g4satbench">https://github.com/zhaoyu-li/g4satbench</a></li>
<li>paper_authors: Zhaoyu Li, Jinpei Guo, Xujie Si</li>
<li>for: 提供了一个综合评估框架 дляGraph Neural Networks (GNNs) based Boolean Satisfiability Problem (SAT) 解决方案。</li>
<li>methods: 使用了多种GNN模型，包括不同的预测任务、训练目标和推理算法，并对其进行了比较。</li>
<li>results: 显示了GNN模型可以有效地学习一种类似于排序搜索的解决策略，但它们困难地学习循环搜索在隐藏空间中的技巧。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space.
</details>
<details>
<summary>摘要</summary>
Graph neural networks (GNNs) 近期emerge as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space.
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Leads-to-Structured-Constraint-of-Learning"><a href="#Symmetry-Leads-to-Structured-Constraint-of-Learning" class="headerlink" title="Symmetry Leads to Structured Constraint of Learning"></a>Symmetry Leads to Structured Constraint of Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16932">http://arxiv.org/abs/2309.16932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Ziyin</li>
<li>for: 这个论文探讨了现代神经网络中广泛存在的同构设计所带来的对学习行为的影响。</li>
<li>methods: 该论文使用了loss函数对称性的研究，证明每种镜像对称性都导致了结构化约束，在权重减排或梯度噪声较大时变为最佳解。</li>
<li>results: 论文表明，对称性可以导致神经网络的稀热性、低级别性和同质整合等现象，并可以用来设计具有固定约束的梯度下降算法。<details>
<summary>Abstract</summary>
Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
</details>
<details>
<summary>摘要</summary>
由于现代神经网络的通用体系设计， symmetries 在现代神经网络中存在广泛。在这项工作中，我们揭示了损失函数 symmetries 对机器学习模型的学习行为的重要性。我们证明了每个镜像Symmetry of the loss function 导致了一种结构化约束，当权重衰退或梯度噪声较大时，这种约束变得更加受欢迎。作为直接推论，我们显示了缩放Symmetry 导致稀疏性，旋转Symmetry 导致低级别性，并且 permutation Symmetry 导致同质集成。然后，我们显示了理论框架可以解释神经网络中的损失强化和各种塌陷现象，并建议如何使用 symmetries 设计可微 differentiable 的算法来实现硬约束。
</details></li>
</ul>
<hr>
<h2 id="Unlabeled-Out-Of-Domain-Data-Improves-Generalization"><a href="#Unlabeled-Out-Of-Domain-Data-Improves-Generalization" class="headerlink" title="Unlabeled Out-Of-Domain Data Improves Generalization"></a>Unlabeled Out-Of-Domain Data Improves Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00027">http://arxiv.org/abs/2310.00027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Hossein Saberi, Amir Najafi, Alireza Heidari, Mohammad Hosein Movasaghinia, Abolfazl Motahari, Babak H. Khalaj</li>
<li>for: 本文提出了一种新的框架，用于在半supervised分类问题中 incorporating 无标签数据。</li>
<li>methods: 本文使用了 Distributionally Robust Optimization (DRO) 和自supervised 训练。</li>
<li>results: 研究人员通过使用本文的方法，可以获得substantial 改善的总体化错误 bounds，比ERM更好。<details>
<summary>Abstract</summary>
We propose a novel framework for incorporating unlabeled data into semi-supervised classification problems, where scenarios involving the minimization of either i) adversarially robust or ii) non-robust loss functions have been considered. Notably, we allow the unlabeled samples to deviate slightly (in total variation sense) from the in-domain distribution. The core idea behind our framework is to combine Distributionally Robust Optimization (DRO) with self-supervised training. As a result, we also leverage efficient polynomial-time algorithms for the training stage. From a theoretical standpoint, we apply our framework on the classification problem of a mixture of two Gaussians in $\mathbb{R}^d$, where in addition to the $m$ independent and labeled samples from the true distribution, a set of $n$ (usually with $n\gg m$) out of domain and unlabeled samples are gievn as well. Using only the labeled data, it is known that the generalization error can be bounded by $\propto\left(d/m\right)^{1/2}$. However, using our method on both isotropic and non-isotropic Gaussian mixture models, one can derive a new set of analytically explicit and non-asymptotic bounds which show substantial improvement on the generalization error compared ERM. Our results underscore two significant insights: 1) out-of-domain samples, even when unlabeled, can be harnessed to narrow the generalization gap, provided that the true data distribution adheres to a form of the "cluster assumption", and 2) the semi-supervised learning paradigm can be regarded as a special case of our framework when there are no distributional shifts. We validate our claims through experiments conducted on a variety of synthetic and real-world datasets.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，用于在半监督分类问题中包含无标示数据。我们考虑了两种情况：一是对 adversarially Robust 损失函数进行最小化，二是对 non-Robust 损失函数进行最小化。我们允许无标示样本略有偏差（在总变量意义上）从域内分布。我们的框架结合了分布 robust 优化（DRO）和自适应培训。因此，我们还可以利用高效的多项式时间算法进行训练阶段。从理论上看，我们在两个 Gaussian 混合模型中应用我们的框架，其中包括 $m$ 个独立的标注样本和 $n$ ($n\gg m$) 个非域样本。使用仅标注数据时，知道generalization error可以被约为 $\propto\left(d/m\right)^{1/2}$。然而，使用我们的方法，我们可以 derivate一组新的分布式bounds，这些bounds显示与ERM相比，我们的方法可以减少generalization error的距离。我们的结果表明以下两点：1）无标示样本，即使不具标注，也可以减少通用化距离，只要数据分布遵循一种"分布假设"。2）半监督学习模型可以看作我们框架的特殊情况，当无Distributional shift时。我们通过对各种 sintetic 和实际数据进行实验，证明了我们的声明。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/29/cs.LG_2023_09_29/" data-id="cloqtaety00qfgh880gqz04jx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/29/eess.IV_2023_09_29/" class="article-date">
  <time datetime="2023-09-29T09:00:00.000Z" itemprop="datePublished">2023-09-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/29/eess.IV_2023_09_29/">eess.IV - 2023-09-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CNN-based-automatic-segmentation-of-Lumen-Media-boundaries-in-IVUS-images-using-closed-polygonal-chains"><a href="#CNN-based-automatic-segmentation-of-Lumen-Media-boundaries-in-IVUS-images-using-closed-polygonal-chains" class="headerlink" title="CNN-based automatic segmentation of Lumen &amp; Media boundaries in IVUS images using closed polygonal chains"></a>CNN-based automatic segmentation of Lumen &amp; Media boundaries in IVUS images using closed polygonal chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17406">http://arxiv.org/abs/2309.17406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavel Sinha, Ioannis Psaromiligkos, Zeljko Zilic</li>
<li>For:  automatic segmentation of lumen and media in IntraVascular ultra-sound (IVUS) images* Methods:  closed polygonal chains, adaptive-subband-decomposition CNN, Jaccard Measure (JM) loss function, Mean Squared Error (MSE) loss function* Results:  outperforms state-of-the-art lumen and media segmentation methods, using JM and Hausdorff Distance (HD) metrics.Here’s the full text in Simplified Chinese:* 为: 自动分割IVUS图像中的血液管和媒质区域* 方法: 使用闭合多边形链、自适应分割Deep Learning网络、Jaccard度量函数和 Mean Squared Error loss function* 结果: 与状态艺术精度segmentation方法相比，达到了更高的精度，使用JM和 HausdorffDistance 度量函数。<details>
<summary>Abstract</summary>
We propose an automatic segmentation method for lumen and media with irregular contours in IntraVascular ultra-sound (IVUS) images. In contrast to most approaches that broadly label each pixel as either lumen, media, or background, we propose to approximate the lumen and media contours by closed polygonal chains. The chain vertices are placed at fixed angles obtained by dividing the entire 360\degree~angular space into equally spaced angles, and we predict their radius using an adaptive-subband-decomposition CNN. We consider two loss functions during training. The first is a novel loss function using the Jaccard Measure (JM) to quantify the similarities between the predicted lumen and media segments and the corresponding ground-truth image segments. The second loss function is the traditional Mean Squared Error. The proposed architecture significantly reduces computational costs by replacing the popular auto-encoder structure with a simple CNN as the encoder and the decoder is reduced to simply joining the consecutive predicted points. We evaluated our network on the publicly available IVUS-Challenge-2011 dataset using two performance metrics, namely JM and Hausdorff Distance (HD). The evaluation results show that our proposed network mostly outperforms the state-of-the-art lumen and media segmentation methods.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:我们提出了一种自动分割方法，用于在IntraVascular ultra-sound（IVUS）图像中分割不规则的液腔和媒腔。与大多数方法不同，我们使用closed polygon链来近似液腔和媒腔的边界。链Vertex是在分割360度的angular space的等间隔angles中得到的，并使用适应性子带分解CNN来预测它们的半径。我们在训练时使用了两种损失函数：一种新的损失函数使用Jaccard Measure（JM）来衡量预测的液腔和媒腔段与对应的真实图像段的相似性，以及传统的Mean Squared Error。我们提出的 Architecture significantly reduces computational costs by replacing the popular auto-encoder structure with a simple CNN as the encoder and the decoder is reduced to simply joining the consecutive predicted points。我们在公共可用的IVUS-Challenge-2011 dataset上评估了我们的网络，使用了两个性能指标：JM和Hausdorff Distance（HD）。评估结果显示，我们的提出的网络大多比现状的液腔和媒腔分割方法表现更好。
</details></li>
</ul>
<hr>
<h2 id="Prescanning-Assembly-Optimization-Criteria-for-Computed-Tomography"><a href="#Prescanning-Assembly-Optimization-Criteria-for-Computed-Tomography" class="headerlink" title="Prescanning Assembly Optimization Criteria for Computed Tomography"></a>Prescanning Assembly Optimization Criteria for Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17303">http://arxiv.org/abs/2309.17303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Goswami</li>
<li>for: 提高净化数据重建的可逆性</li>
<li>methods: 使用最大主成分&#x2F;condition number的矩阵重建方法优化计算机断层扫描系统和配置</li>
<li>results: 提高样本位置的优化可以减少50%的平均平方根误差，16.5%的相似性指标下降和40%的抖动噪声在重建图像中，并自动化计算机断层扫描Assembly，从而节省时间、剂量和运行成本。<details>
<summary>Abstract</summary>
Computerized Tomography assembly and system configuration are optimized for enhanced invertibility in sparse data reconstruction. Assembly generating maximum principal components/condition number of weight matrix is designated as best configuration. The gamma CT system is used for testing. The unoptimized sample location placement with 7.7% variation results in a maximum 50% root mean square error, 16.5% loss of similarity index, and 40% scattering noise in the reconstructed image relative to the optimized sample location when the proposed criteria are used. The method can help to automate the CT assembly, resulting in relatively artifact-free recovery and reducing the iteration to figure out the best scanning configuration for a given sample size, thus saving time, dosage, and operational cost.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:计算机Tomography组装和系统配置被优化以提高逆向性能。生成最大主要components/condition number的weight矩阵 assembly被认为是最佳配置。使用gamma CT系统进行测试。未优化的样本位置布局 resulted in 7.7%变化，导致最大50%根圆振幅误差，16.5%相似性指标损失，和40%扩散噪声在重构图像中相对于优化样本位置when使用提议的标准。这种方法可以帮助自动化CT组装，从而获得相对 artifact-free 的恢复，并降低确定给定样本大小的扫描配置的迭代次数，从而节省时间、剂量和操作成本。
</details></li>
</ul>
<hr>
<h2 id="FreqAlign-Excavating-Perception-oriented-Transferability-for-Blind-Image-Quality-Assessment-from-A-Frequency-Perspective"><a href="#FreqAlign-Excavating-Perception-oriented-Transferability-for-Blind-Image-Quality-Assessment-from-A-Frequency-Perspective" class="headerlink" title="FreqAlign: Excavating Perception-oriented Transferability for Blind Image Quality Assessment from A Frequency Perspective"></a>FreqAlign: Excavating Perception-oriented Transferability for Blind Image Quality Assessment from A Frequency Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17014">http://arxiv.org/abs/2309.17014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Li, Yiting Lu, Zhibo Chen</li>
<li>for: 提高隐藏图像质量评估（BIQA）在不同频谱环境下的转移性能。</li>
<li>methods: 基于不supervised频谱适应（UDA）的方法，通过对频谱特征进行对齐来消除频谱偏移。</li>
<li>results: 提出了一种有效的频谱对齐策略（FreqAlign），通过研究不同频谱 комponents的可见性，选择最适合性的频谱component进行对齐，从而提高BIQA的转移性能。<details>
<summary>Abstract</summary>
Blind Image Quality Assessment (BIQA) is susceptible to poor transferability when the distribution shift occurs, e.g., from synthesis degradation to authentic degradation. To mitigate this, some studies have attempted to design unsupervised domain adaptation (UDA) based schemes for BIQA, which intends to eliminate the domain shift through adversarial-based feature alignment. However, the feature alignment is usually taken at the low-frequency space of features since the global average pooling operation. This ignores the transferable perception knowledge in other frequency components and causes the sub-optimal solution for the UDA of BIQA. To overcome this, from a novel frequency perspective, we propose an effective alignment strategy, i.e., Frequency Alignment (dubbed FreqAlign), to excavate the perception-oriented transferability of BIQA in the frequency space.   Concretely, we study what frequency components of features are more proper for perception-oriented alignment. Based on this, we propose to improve the perception-oriented transferability of BIQA by performing feature frequency decomposition and selecting the frequency components that contained the most transferable perception knowledge for alignment. To achieve a stable and effective frequency selection, we further propose the frequency movement with a sliding window to find the optimal frequencies for alignment, which is composed of three strategies, i.e., warm up with pre-training, frequency movement-based selection, and perturbation-based finetuning. Extensive experiments under different domain adaptation settings of BIQA have validated the effectiveness of our proposed method. The code will be released at https://github.com/lixinustc/Openworld-IQA.
</details>
<details>
<summary>摘要</summary>
《盲目图像质量评估（BIQA）容易受到分布转移的影响，例如从生成过程破坏到真实破坏。为了解决这个问题，一些研究已经尝试了基于无监督领域适应（UDA）的BIQA方法，以减少领域的转移。然而，通常情况下，这些方法会在低频空间的特征上进行对应性对齐，这会忽略其他频率组件中的可以传递的感知知识，导致UDA的BIQA方法不优。为了解决这个问题，我们从一种新的频率视角出发，提出了一种有效的对齐策略——频率对齐（dubbed FreqAlign），以挖掘BIQA中的感知导向的传递性。》具体来说，我们研究了哪些特征频率是更适合用于感知导向的对齐。基于这个，我们提出了改进BIQA的感知导向传递性的方法，通过特征频率分解和选择包含最多可传递的感知知识的频率组件进行对齐。为了实现稳定和有效的频率选择，我们还提出了频率移动的滑块窗口来找到最佳对齐频率，这包括三种策略：启动预训练、频率移动选择和扰动训练。我们的方法在BIQA的不同领域适应设置下进行了广泛的实验，并证明了我们提出的方法的效果。BIQA代码将在 GitHub 上发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/29/eess.IV_2023_09_29/" data-id="cloqtaf0x0177gh88drwg71oc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_29" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/29/eess.SP_2023_09_29/" class="article-date">
  <time datetime="2023-09-29T08:00:00.000Z" itemprop="datePublished">2023-09-29</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/29/eess.SP_2023_09_29/">eess.SP - 2023-09-29</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="One-Bit-Channel-Estimation-for-IRS-aided-Millimeter-Wave-Massive-MU-MISO-System"><a href="#One-Bit-Channel-Estimation-for-IRS-aided-Millimeter-Wave-Massive-MU-MISO-System" class="headerlink" title="One-Bit Channel Estimation for IRS-aided Millimeter-Wave Massive MU-MISO System"></a>One-Bit Channel Estimation for IRS-aided Millimeter-Wave Massive MU-MISO System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00187">http://arxiv.org/abs/2310.00187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silei Wang, Qiang Li, Jingran Lin</li>
<li>for: 这个论文是研究一种基于智能反射表面（IRS）的多用户大规模多输入多天线（MISO）通信系统中的上行 каналом估计方法。</li>
<li>methods: 该论文使用了一种基于elementwise sparse Bayesian learning（SBL）的channel estimator，以及一种基于common row-structured sparsity的block SBL（BSBL）和variational EM算法来实现。</li>
<li>results:  simulation results show that the proposed estimators are superior to state-of-the-art one-bit estimators, and that the more diverse structured sparsity is exploited, the better estimation performance is achieved.<details>
<summary>Abstract</summary>
Recently, intelligent reflecting surface (IRS)-assisted communication has gained considerable attention due to its advantage in extending the coverage and compensating the path loss with low-cost passive metasurface. This paper considers the uplink channel estimation for IRS-aided multiuser massive MISO communications with one-bit ADCs at the base station (BS). The use of one-bit ADC is impelled by the low-cost and power efficient implementation of massive antennas techniques. However, the passiveness of IRS and the lack of signal level information after one-bit quantization make the IRS channel estimation challenging. To tackle this problem, we exploit the structured sparsity of the user-IRS-BS cascaded channels and develop three channel estimators, each of which utilizes the structured sparsity at different levels. Specifically, the first estimator exploits the elementwise sparsity of the cascaded channel and employs the sparse Bayesian learning (SBL) to infer the channel responses via the type-II maximum likelihood (ML) estimation. However, due to the one-bit quantization, the type-II ML in general is intractable. As such, a variational expectation-maximization (EM) algorithm is custom-derived to iteratively compute an ML solution. The second estimator utilizes the common row-structured sparsity induced by the IRS-to-BS channel shared among the users, and develops another type-II ML solution via the block SBL (BSBL) and the variational EM. To further improve the performance of BSBL, a third two-stage estimator is proposed, which can utilize both the common row-structured sparsity and the column-structured sparsity arising from the limited scattering around the users. Simulation results show that the more diverse structured sparsity is exploited, the better estimation performance is achieved, and that the proposed estimators are superior to state-of-the-art one-bit estimators.
</details>
<details>
<summary>摘要</summary>
近期，智能反射表面（IRS）助成通信已经吸引了广泛关注，因为它可以延长覆盖范围和补偿路径损失，而且可以通过低成本的pasive metasurface实现。这篇论文考虑了IRS协助多用户大规模MIMO通信的上传通道估计，在BS端使用一bit ADC。由于IRS是pasive的，因此IRS通道估计具有挑战。为解决这个问题，我们利用用户-IRS-BS堆叠通道的结构准确性，并开发了三种通道估计器，每个估计器都利用了不同的结构准确性。第一个估计器利用用户-IRS-BS堆叠通道的元素准确性，使用简单的bayesian学习（SBL）来推断通道响应，并使用类型二最大likelihood（ML）估计。然而，由于一bit quantization，类型二ML通常是不可解的。因此，我们 derivate了一种变量期望-最大化（EM）算法来逐步计算ML解。第二个估计器利用IRS-BS通道中共同的行structured sparsity，开发了另一种类型二ML解决方案，并使用块SBL（BSBL）和变量EM。为了进一步提高BSBL的性能，我们还提出了一种第三种两个阶段估计器，可以利用用户-IRS-BS堆叠通道中的行structured sparsity和列structured sparsity。实验结果表明，随着更多的结构准确性被利用，估计性能得到了提高，并且提出的估计器比state-of-the-art一bit估计器更高效。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Metamaterials-with-Active-Tunability-and-Self-adaptivity-for-Magnetic-Resonance-Imaging"><a href="#Conformal-Metamaterials-with-Active-Tunability-and-Self-adaptivity-for-Magnetic-Resonance-Imaging" class="headerlink" title="Conformal Metamaterials with Active Tunability and Self-adaptivity for Magnetic Resonance Imaging"></a>Conformal Metamaterials with Active Tunability and Self-adaptivity for Magnetic Resonance Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00153">http://arxiv.org/abs/2310.00153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Wu, Xia Zhu, Xiaoguang Zhao, Stephan W. Anderson, Xin Zhang</li>
<li>for: 这个研究旨在应用金属结构来提高磁共振成像的效能，并解决了传统金属结构在诊断中的一些问题，例如尺寸问题、频率问题和讯号干扰问题。</li>
<li>methods: 这个研究使用了一种柔软的金属结构，可以通过控制电路来调整频率，并且可以在接收阶段 selecively 增强磁场，以确保其在诊断中的最佳性能。</li>
<li>results: 这个研究获得了一种可以实现灵活调整频率和 selecively 增强磁场的金属结构，这个结构可以实现在诊断中的广泛应用，并且可以解决传统金属结构在诊断中的一些问题。<details>
<summary>Abstract</summary>
Ongoing effort has been devoted to applying metamaterials to boost the imaging performance of magnetic resonance imaging owing to their unique capacity for electromagnetic field confinement and enhancement. However, there are still major obstacles to widespread clinical adoption of conventional metamaterials due to several notable restrictions, namely: their typically bulky and rigid structures, deviations in their optimal resonance frequency, and their inevitable interference with the transmission RF field in MRI. Herein, we address these restrictions and report a conformal, smart metamaterial, which may not only be readily tuned to achieve the desired, precise frequency match with MRI by a controlling circuit, but is also capable of selectively amplifying the magnetic field during the RF reception phase by sensing the excitation signal strength passively, thereby remaining off during the RF transmission phase and thereby ensuring its optimal performance when applied to MRI as an additive technology. By addressing a host of current technological challenges, the metamaterial presented herein paves the way toward the wide-ranging utilization of metamaterials in clinical MRI, thereby translating this promising technology to the MRI bedside.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Overview-of-Use-Cases-in-Single-Channel-Full-Duplex-Techniques-for-Satellite-Communication"><a href="#Overview-of-Use-Cases-in-Single-Channel-Full-Duplex-Techniques-for-Satellite-Communication" class="headerlink" title="Overview of Use Cases in Single Channel Full Duplex Techniques for Satellite Communication"></a>Overview of Use Cases in Single Channel Full Duplex Techniques for Satellite Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00151">http://arxiv.org/abs/2310.00151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Monzon Baeza, Steven Kisseleff, Jorge Luis González Rios, Juan Andrés Vasquez-Peralvo, Carlos Mosquera, Roberto López Valcarce, Tomás Ramírez Parracho, Pablo Losada Sanisidro, Juan Carlos Merlano Duncan, Symeon Chatzinotas</li>
<li>for: 本研究提供了卫星通信系统中单通道全动态技术的多样化应用和使用场景综述。</li>
<li>methods: 本研究使用了单通道全动态技术，同时在单个频率通道上实现了发送和接收操作。</li>
<li>results: 该研究选择了八个可能的应用场景，并对这些场景进行了初步评估。初步结果表明，单通道全动态技术在各种关键领域内可能带来巨大的改变。<details>
<summary>Abstract</summary>
This paper provides an overview of the diverse range of applications and use cases for Single-Channel Full-Duplex (SCFD) techniques within the field of satellite communication. SCFD, allowing simultaneous transmission and reception on a single frequency channel, presents a transformative approach to enhancing satellite communication systems. We select eight potential use cases with the objective of highlighting the substantial potential of SCFD techniques in revolutionizing SatCom across a multitude of critical domains. In addition, preliminary results from the qualitative assessment are shown. This work is carried out within the European Space Agency (ESA) ongoing activity FDSAT: Single Channel Full Duplex Techniques for Satellite Communications.
</details>
<details>
<summary>摘要</summary>
这篇论文提供了卫星通信领域内单频道全动态（SCFD）技术的多样化应用场景和用 caso的概述。 SCFD技术允许在同一个频道上同时进行发送和接收，对卫星通信系统的提升带来了 transformative 的影响。 我们选择了八个可能的应用场景，以强调 SCFD技术在多个关键领域中的巨大潜力。此外，我们还展示了初步的评估结果。这项工作是欧洲空间局（ESA）正在进行的活动FDSAT：单频道全动态技术 для卫星通信。
</details></li>
</ul>
<hr>
<h2 id="Meta-Reinforcement-Learning-for-Fast-Spectrum-Sharing-in-Vehicular-Networks"><a href="#Meta-Reinforcement-Learning-for-Fast-Spectrum-Sharing-in-Vehicular-Networks" class="headerlink" title="Meta Reinforcement Learning for Fast Spectrum Sharing in Vehicular Networks"></a>Meta Reinforcement Learning for Fast Spectrum Sharing in Vehicular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17185">http://arxiv.org/abs/2309.17185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Huang, Le Liang, Shi Jin, Geoffrey Ye Li</li>
<li>for: 这篇论文研究了车辆到所有东西通信中的快速频谱共享问题，以提高整体系统的频谱效率。</li>
<li>methods: 作者使用深度强化学习模型和距离政策优化方法来解决这个问题。</li>
<li>results: 作者的方法可以快速适应新任务，并且可以减少交互次数和训练时间。数据显示，其方法可以达到近似优化性和快速收敛。<details>
<summary>Abstract</summary>
In this paper, we investigate the problem of fast spectrum sharing in vehicle-to-everything communication. In order to improve the spectrum efficiency of the whole system, the spectrum of vehicle-to-infrastructure links is reused by vehicle-to-vehicle links. To this end, we model it as a problem of deep reinforcement learning and tackle it with proximal policy optimization. A considerable number of interactions are often required for training an agent with good performance, so simulation-based training is commonly used in communication networks. Nevertheless, severe performance degradation may occur when the agent is directly deployed in the real world, even though it can perform well on the simulator, due to the reality gap between the simulation and the real environments. To address this issue, we make preliminary efforts by proposing an algorithm based on meta reinforcement learning. This algorithm enables the agent to rapidly adapt to a new task with the knowledge extracted from similar tasks, leading to fewer interactions and less training time. Numerical results show that our method achieves near-optimal performance and exhibits rapid convergence.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了车辆到所有东西通信中快速spectrum sharing的问题。为了提高整个系统的spectrum效率，车辆到基础设施链路的spectrum被重复利用于车辆到车辆链路。为此，我们将其模型为深度优化学习问题，使用距离策略优化进行解决。由于通信网络中的训练经常需要大量的交互，因此通常使用模拟基础进行训练。然而，在实际环境中直接部署Agent可能会导致性能下降，即使Agent在模拟器上表现良好，因为实际环境和模拟器之间存在差距。为Addressing这个问题，我们提出了基于meta学习算法的方法。这种算法使得Agent能够快速适应新任务，通过提取相似任务中的知识，从而减少交互次数和训练时间。数字实验结果表明，我们的方法可以达到近似优化性和快速收敛。
</details></li>
</ul>
<hr>
<h2 id="D-Band-2D-MIMO-FMCW-Radar-System-Design-for-Indoor-Wireless-Sensing"><a href="#D-Band-2D-MIMO-FMCW-Radar-System-Design-for-Indoor-Wireless-Sensing" class="headerlink" title="D-Band 2D MIMO FMCW Radar System Design for Indoor Wireless Sensing"></a>D-Band 2D MIMO FMCW Radar System Design for Indoor Wireless Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17110">http://arxiv.org/abs/2309.17110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subbarao Korlapati, Reza Nikandish</li>
<li>for: 这个研究旨在设计一个在室内无线感知中使用D频段多输入多天线（MIMO）频率调制连续波（FMCW）雷达系统。</li>
<li>methods: 该研究使用了一个均匀的方正阵（URA）来实现2D方向来来（DOA）估计。DOA估计精度在噪声存在情况下被评估使用了多信号分类（MUSIC）和最小偏差误差响应（MVDR）算法。</li>
<li>results: 研究发现，随着目标距离的增加，使用64个天线的MUSIC算法可以在1-10米室内距离和0-30dB SNR范围内提供较低的root-mean-square error（RMSE），而使用16个天线和4个天线的情况下，两种算法具有相似的性能。此外，研究还探讨了雷达接收器（RX）信号强度和发送器（TX）输出功率的关系，并对现有D频段半导体雷达的输出功率要求进行了比较。<details>
<summary>Abstract</summary>
In this article, we present system design of D-band multi-input multi-output (MIMO) frequency-modulated continuous-wave (FMCW) radar for indoor wireless sensing. A uniform rectangular array (URA) of radar elements is used for 2D direction-of-arrival (DOA) estimation. The DOA estimation accuracy of the MIMO radar array in the presence of noise is evaluated using the multiple-signal classification (MUSIC) and the minimum variance distortionless response (MVDR) algorithms. We investigate different scaling scenarios for the radar receiver (RX) SNR and the transmitter (TX) output power with the target distance. The DOA estimation algorithm providing the highest accuracy and shortest simulation time is shown to depend on the size of the radar array. Specifically, for a 64-element array, the MUSIC achieves lower root-mean-square error (RMSE) compared to the MVDR across 1--10\,m indoor distances and 0--30\,dB SNR (e.g., $\rm 0.8^{\circ}$/$\rm 0.3^{\circ}$ versus $\rm 1.0^{\circ}$/$\rm 0.5^{\circ}$ at 10/20\,dB SNR and 5\,m distance) and 0.5x simulation time. For a 16-element array, the two algorithms provide comparable performance, while for a 4-element array, the MVDR outperforms the MUSIC by a large margin (e.g., $\rm 8.3^{\circ}$/$\rm 3.8^{\circ}$ versus $\rm 62.2^{\circ}$/$\rm 48.8^{\circ}$ at 10/20\,dB SNR and 5\,m distance) and 0.8x simulation time. Furthermore, the TX output power requirement of the radar array is investigated in free-space and through-wall wireless sensing scenarios, and is benchmarked by the state-of-the-art D-band on-chip radars.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们介绍了D频段多入口多出口（MIMO）频率变化 kontinuierlich（FMCW）雷达的系统设计，用于室内无线感知。我们使用了一个均匀的方正阵（URA）来实现2D方向到来（DOA）估算。我们使用MUSIC和MVDR算法来评估FMCW雷达数组在噪声存在下的DOA估算精度。我们 investigate了不同的收发器信号强度（RX SNR）和发射器输出功率（TX output power）的涨落情况，并对目标距离进行了 investigate。我们发现，对于64个元素数组，MUSIC算法在1-10米室内距离和0-30dB SNR范围内提供了更低的平均方差误差（RMSE），比如0.8度/0.3度相比于1.0度/0.5度，而且 simulation time 相对较短。对于16个元素数组，两种算法具有相似的性能，而对于4个元素数组，MVDR算法明显超过MUSIC算法，例如，在10/20dB SNR和5米距离下，MVDR算法的误差为8.3度/3.8度，而MUSIC算法的误差为62.2度/48.8度，而且 simulation time 相对较长。此外，我们还调查了雷达数组的发射器输出功率要求，并与现有的D频段在芯片上的雷达相比。
</details></li>
</ul>
<hr>
<h2 id="White-Paper-on-Radio-Channel-Modeling-and-Prediction-to-Support-Future-Environment-aware-Wireless-Communication-Systems"><a href="#White-Paper-on-Radio-Channel-Modeling-and-Prediction-to-Support-Future-Environment-aware-Wireless-Communication-Systems" class="headerlink" title="White Paper on Radio Channel Modeling and Prediction to Support Future Environment-aware Wireless Communication Systems"></a>White Paper on Radio Channel Modeling and Prediction to Support Future Environment-aware Wireless Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17088">http://arxiv.org/abs/2309.17088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mate Boban, Vittorio Degli-Esposti<br>for:The paper is written to provide an overview of the state-of-the-art in radio channel measurement and modeling, and to identify the key challenges that need to be addressed to support the development of 6G networks.methods:The paper uses a variety of methods, including channel sounder design, metrology, and measurement methodologies, as well as measurements, modeling, and systematic dataset collection and analysis.results:The paper provides a summary of the state-of-the-art in radio channel measurement and modeling, and identifies the key challenges that the scientific community will need to address to support the development of 6G networks. These challenges include the need for a paradigm shift in channel measurements and modeling, the need for a wider frequency range, and the need for better support for diverse and highly cluttered environments.Here is the information in Simplified Chinese text:for:本文目的是提供 radio 频道测量和模型的状况报告，以及为支持未来的 6G 网络发展而需要解决的主要挑战。methods:本文使用了多种方法，包括通道测量设计、测量方法学、测量方法和系统atic dataset收集和分析。results:本文提供了 radio 频道测量和模型的状况报告，并确定了支持未来的 6G 网络发展所需的主要挑战。这些挑战包括需要一种新的测量和模型 парадиг shift，需要更广泛的频谱范围，以及更好地支持多样化和高度干扰的环境。<details>
<summary>Abstract</summary>
COST INTERACT working group (WG)1 aims at increasing the theoretical and experimental understanding of radio propagation and channels in environments of interest and at deriving models for design, simulation, planning and operation of future wireless systems. Wide frequency ranges from sub-GHz to terahertz (THz), potentially high mobility, diverse and highly cluttered environments, dense networks, massive antenna systems, and the use of intelligent surfaces, are some of the challenges for radio channel measurements and modeling for next generation systems. As indicated in [1], with increased number of use cases (e.g., those identified by one6G [2] and shown in Fig. 1) to be supported and a larger number of frequency bands, a paradigm shift in channel measurements and modeling will be required. To address the particular challenges that come with such a paradigm shift, WG1 started the work on relevant topics, ranging from channel sounder design, metrology and measurement methodologies, measurements, modeling, and systematic dataset collection and analysis. In addition to the core activities of WG1, based on the strong interest of the participants, two sub-working groups (subWGs) have been initiated as part of WG1: i) subWG1.1 on millimeter-wave (mmWave) and THz sounding (subWG THz) and ii) subWG1.2 on propagation aspects related to reconfigurable intelligent surfaces (RIS) (subWG RIS). This white paper has two main goals: i) it summarizes the state-of-theart in radio channel measurement and modeling and the key challenges that the scientific community will have to face over the next years to support the development of 6G networks, as identified by WG1 and its subWGs; and ii) it charts the main directions for the work of WG1 and subWGs for the remainder of COST INTERACT duration (i.e., until October 2025).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Double-Layer-Power-Control-for-Mobile-Cell-Free-XL-MIMO-with-Multi-Agent-Reinforcement-Learning"><a href="#Double-Layer-Power-Control-for-Mobile-Cell-Free-XL-MIMO-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Double-Layer Power Control for Mobile Cell-Free XL-MIMO with Multi-Agent Reinforcement Learning"></a>Double-Layer Power Control for Mobile Cell-Free XL-MIMO with Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17079">http://arxiv.org/abs/2309.17079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziheng Liu, Jiayi Zhang, Zhilong Liu, Huahua Xiao, Bo Ai</li>
<li>for: 这个论文旨在探讨基站装备巨大多输入多输出（XL-MIMO）板的Cell-free（CF）无线通信系统，以提高未来无线通信系统的性能。</li>
<li>methods: 本论文提出了一种基于多智能学习（MARL）算法的电力控制算法，该算法包括预测管理和分布优化架构，以解决高维度信号处理问题。</li>
<li>results: 对于不同的MARL算法进行比较，研究发现，提议的MARL算法能够均衡 spectral efficiency（SE）性能和整合时间。此外，对于双层电力控制架构，结果表明，相比单层架构，提议的双层架构在巨大天线数和小天线间距下具有24%的SE性能提升。<details>
<summary>Abstract</summary>
Cell-free (CF) extremely large-scale multiple-input multiple-output (XL-MIMO) is regarded as a promising technology for enabling future wireless communication systems. Significant attention has been generated by its considerable advantages in augmenting degrees of freedom. In this paper, we first investigate a CF XL-MIMO system with base stations equipped with XL-MIMO panels under a dynamic environment. Then, we propose an innovative multi-agent reinforcement learning (MARL)-based power control algorithm that incorporates predictive management and distributed optimization architecture, which provides a dynamic strategy for addressing high-dimension signal processing problems. Specifically, we compare various MARL-based algorithms, which shows that the proposed MARL-based algorithm effectively strikes a balance between spectral efficiency (SE) performance and convergence time. Moreover, we consider a double-layer power control architecture based on the large-scale fading coefficients between antennas to suppress interference within dynamic systems. Compared to the single-layer architecture, the results obtained unveil that the proposed double-layer architecture has a nearly24% SE performance improvement, especially with massive antennas and smaller antenna spacing.
</details>
<details>
<summary>摘要</summary>
cell-free（CF） extremly large-scale multiple-input multiple-output（XL-MIMO）被视为未来无线通信系统的促进技术。这种技术具有显著的优势，可以增加度量自由。在这篇论文中，我们首先investigated CF XL-MIMO系统，其中基站装备了XL-MIMO板。然后，我们提出了一种基于多智能学习（MARL）的动态管理和分布式优化架构的新型电力控制算法。这种算法可以在高维度信号处理问题上提供一个动态策略，并且能够均衡spectral efficiency（SE）性能和整合时间。此外，我们还考虑了一种双层电力控制架构，基于大规模投射系数 между天线来降低动态系统中的干扰。与单层架构相比，我们发现，提posed double-layer架构在巨量天线和小天线间隔下具有24%的SE性能提升。
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-Secure-Offloading-System-Designed-via-UAV-Mounted-Intelligent-Reflecting-Surface-for-Resilience-Enhancement"><a href="#Energy-Efficient-Secure-Offloading-System-Designed-via-UAV-Mounted-Intelligent-Reflecting-Surface-for-Resilience-Enhancement" class="headerlink" title="Energy-Efficient Secure Offloading System Designed via UAV-Mounted Intelligent Reflecting Surface for Resilience Enhancement"></a>Energy-Efficient Secure Offloading System Designed via UAV-Mounted Intelligent Reflecting Surface for Resilience Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17008">http://arxiv.org/abs/2309.17008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Doyoung Kim, Seongah Jeong, Jinkyu Kang</li>
<li>for: 这篇论文旨在实现功能为UAV-mounted智能反射表（IRS）助成在紧急和灾难应对中Establish robust line-of-sight（LoS）连接，以提高紧急救援和灾难应对的能效性。</li>
<li>methods: 该论文使用了安全卸载系统，其中UAV-mounted IRS协助了地面用户和edge云（AP）之间的卸载过程，并且考虑了除了接收者之外的其他用户为可能的侦测者。系统的目标是最小化地面用户设备的电池生命时间，同时保证UAV-mounted IRS的可操作性和安全卸载完成。</li>
<li>results: 该论文通过优化地面用户设备的发射功率、UAV-mounted IRS的轨迹和相位偏移矩阵，以及卸载比率 между本地执行和边缘计算，使用successive convex approximation（SCA）算法，可以提供较大的能源储存量相比于本地执行和部分优化。<details>
<summary>Abstract</summary>
With increasing interest in mmWave and THz communication systems, an unmanned aerial vehicle (UAV)-mounted intelligent reflecting surface (IRS) has been suggested as a key enabling technology to establish robust line-of-sight (LoS) connections with ground nodes owing to their free mobility and high altitude, especially for emergency and disaster response. This paper investigates a secure offloading system, where the UAV-mounted IRS assists the offloading procedures between ground users and an access point (AP) acting as an edge cloud. In this system, the users except the intended recipients in the offloading process are considered as potential eavesdroppers. The system aims to achieve the minimum total energy consumption of battery-limited ground user devices under constraints for secure offloading accomplishment and operability of UAV-mounted IRS, which is done by optimizing the transmit power of ground user devices, the trajectory and phase shift matrix of UAV-mounted IRS, and the offloading ratio between local execution and edge computing based on the successive convex approximation (SCA) algorithms. Numerical results show that the proposed algorithm can provide the considerable energy savings compared with local execution and partial optimizations.
</details>
<details>
<summary>摘要</summary>
随着mmWave和THz通信系统的兴趣增长，一种随机飞行器（UAV）搭载智能反射表（IRS）已被认为是建立可靠直线视线（LoS）连接地面节点的关键技术，尤其是在紧急和灾难应急情况下。这篇论文研究了一种安全卸载系统，其中UAV搭载IRS协助地面用户和edge云（AP）之间的卸载过程。在这个系统中，地面用户除了指定接收者外的其他用户都被视为潜在的窃听者。系统的目标是在续ouseless执行和UAV搭载IRS的可操作性下，最小化电池限制的地面用户设备的总能量占用，通过优化地面用户设备的传输功率、UAV搭载IRS的轨迹和相位偏移矩阵，以及在本地执行和edge计算之间的卸载比例，使用successive convex approximation（SCA）算法来实现。numerical results表明，提议的算法可以提供较大的能量减少 compared with本地执行和部分优化。
</details></li>
</ul>
<hr>
<h2 id="Tree-based-Single-LED-Indoor-Visible-Light-Positioning-Technique"><a href="#Tree-based-Single-LED-Indoor-Visible-Light-Positioning-Technique" class="headerlink" title="Tree based Single LED Indoor Visible Light Positioning Technique"></a>Tree based Single LED Indoor Visible Light Positioning Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16974">http://arxiv.org/abs/2309.16974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srivathsan Chakaravarthi Narasimman, Arokiaswami Alphones</li>
<li>for: 这个论文主要是为了提出一种能够在室内精确定位的技术，并且考虑到实际实现的限制。</li>
<li>methods: 这个技术使用了模拟数据来训练机器学习模型，并且使用了相似的实验设置来测试这些模型。</li>
<li>results: 这个技术可以获得非常高精度的三维定位结果，其中三维定位误差为2.88厘米，比 closest competitor 的6.26厘米更低。<details>
<summary>Abstract</summary>
Visible light positioning(VLP) has gained prominence as a highly accurate indoor positioning technique. Few techniques consider the practical limitations of implementing VLP systems for indoor positioning. These limitations range from having a single LED in the field of view(FoV) of the image sensor to not having enough images for training deep learning techniques. Practical implementation of indoor positioning techniques needs to leverage the ubiquity of smartphones, which is the case with VLP using complementary metal oxide semiconductor(CMOS) sensors. Images for VLP can be gathered only after the lights in question have been installed making it a cumbersome process. These limitations are addressed in the proposed technique, which uses simulated data of a single LED to train machine learning models and test them on actual images captured from a similar experimental setup. Such testing produced mean three dimensional(3D) positioning error of 2.88 centimeters while training with real images achieves accuracy of less than one centimeter compared to 6.26 centimeters of the closest competitor.
</details>
<details>
<summary>摘要</summary>
可见光定位(VLP) 已经成为室内定位技术中的一种非常精度的方法。然而，只有几种技术考虑到实际实施 VLP 系统的实际限制。这些限制包括在图像传感器的视场中只有一个 LED 存在以及没有足够的图像用于深度学习技术的训练。实际应用室内定位技术应该利用智能手机的普遍性，这是 VLP 使用 complementary metal oxide semiconductor (CMOS) 传感器的情况。图像 для VLP 只能在灯光问题上安装后获得，这是一个繁琐的过程。这些限制被提出的方法解决，该方法使用单个 LED 的 simulate 数据来训练机器学习模型，然后在实际设置上测试这些模型。这种测试产生的三维定位误差为 2.88 厘米，而使用实际图像训练的精度则为 Less than 1 厘米，与最近竞争对手的 6.26 厘米误差相比。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/29/eess.SP_2023_09_29/" data-id="cloqtaf2i01augh888f288j5y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/28/cs.SD_2023_09_28/" class="article-date">
  <time datetime="2023-09-28T15:00:00.000Z" itemprop="datePublished">2023-09-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/28/cs.SD_2023_09_28/">cs.SD - 2023-09-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-High-Resolution-Weather-Monitoring-with-Sound-Data"><a href="#Towards-High-Resolution-Weather-Monitoring-with-Sound-Data" class="headerlink" title="Towards High Resolution Weather Monitoring with Sound Data"></a>Towards High Resolution Weather Monitoring with Sound Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16867">http://arxiv.org/abs/2309.16867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enis Berk Çoban, Megan Perra, Michael I. Mandel</li>
<li>for: This paper aims to improve the accuracy of weather predictions in wildlife research using acoustic data and satellite data.</li>
<li>methods: The paper uses machine learning algorithms to train acoustic classifiers using satellite data from the MERRA-2 system, and then uses these classifiers to predict rain, wind, and air temperature at different thresholds.</li>
<li>results: The paper finds that acoustic classifiers trained using MERRA-2 data are more accurate than the raw MERRA-2 data itself, and that using MERRA-2 to roughly identify rain in the acoustic data allows for the production of a functional model without the need for human-validated labels.<details>
<summary>Abstract</summary>
Across various research domains, remotely-sensed weather products are valuable for answering many scientific questions; however, their temporal and spatial resolutions are often too coarse to answer many questions. For instance, in wildlife research, it's crucial to have fine-scaled, highly localized weather observations when studying animal movement and behavior. This paper harnesses acoustic data to identify variations in rain, wind and air temperature at different thresholds, with rain being the most successfully predicted. Training a model solely on acoustic data yields optimal results, but it demands labor-intensive sample labeling. Meanwhile, hourly satellite data from the MERRA-2 system, though sufficient for certain tasks, produced predictions that were notably less accurate in predict these acoustic labels. We find that acoustic classifiers can be trained from the MERRA-2 data that are more accurate than the raw MERRA-2 data itself. By using MERRA-2 to roughly identify rain in the acoustic data, we were able to produce a functional model without using human-validated labels. Since MERRA-2 has global coverage, our method offers a practical way to train rain models using acoustic datasets around the world.
</details>
<details>
<summary>摘要</summary>
在不同的研究领域中，远程感知的天气产品非常有价值，但它们的时间和空间分辨率经常太低。例如，在野生动物研究中，需要有高级别、高地点准确的天气观测，以便研究动物的移动和行为。这篇论文利用声学数据来识别雨、风和空气温度的变化，雨是最成功地预测的。使用声学数据来训练模型，可以获得优化的结果，但需要大量的人工标注样本。而从MERRA-2系统获得的一小时卫星数据，虽然适用于某些任务，但预测的准确性较低。我们发现，使用MERRA-2数据来训练声学分类器，可以获得更高的准确性 than raw MERRA-2数据本身。通过使用MERRA-2数据来粗略地识别雨在声学数据中，我们可以生成一个可行的模型，不需要人工验证标注。由于MERRA-2数据有全球覆盖，我们的方法可以在全球各地使用声学数据来训练雨模型。
</details></li>
</ul>
<hr>
<h2 id="Meeting-Recognition-with-Continuous-Speech-Separation-and-Transcription-Supported-Diarization"><a href="#Meeting-Recognition-with-Continuous-Speech-Separation-and-Transcription-Supported-Diarization" class="headerlink" title="Meeting Recognition with Continuous Speech Separation and Transcription-Supported Diarization"></a>Meeting Recognition with Continuous Speech Separation and Transcription-Supported Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16482">http://arxiv.org/abs/2309.16482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thilo von Neumann, Christoph Boeddeker, Tobias Cord-Landwehr, Marc Delcroix, Reinhold Haeb-Umbach</li>
<li>for: 本文提出了一个模块化管道，用于单通道分离、识别和分配会议录音，并对 Libri-CSS 数据集进行评估。</li>
<li>methods: 使用 Continuous Speech Separation (CSS) 系统的 TF-GridNet 分离架构，然后使用无关于说话人的语音识别器，以实现会议录音识别的最佳参考词错率 (ORC WER) 表现。接着，使用 d-vector 基于的分配模块提取了增强的信号中的说话人嵌入，并将 CSS 输出分配给正确的说话人。</li>
<li>results: 在使用了句子和词级界限的 ASR 模块支持说话人转折检测的情况下，本文提出了一个最佳 Concatenated minimum-Permutation Word Error Rate (cpWER)  для全会议记录管道。<details>
<summary>Abstract</summary>
We propose a modular pipeline for the single-channel separation, recognition, and diarization of meeting-style recordings and evaluate it on the Libri-CSS dataset. Using a Continuous Speech Separation (CSS) system with a TF-GridNet separation architecture, followed by a speaker-agnostic speech recognizer, we achieve state-of-the-art recognition performance in terms of Optimal Reference Combination Word Error Rate (ORC WER). Then, a d-vector-based diarization module is employed to extract speaker embeddings from the enhanced signals and to assign the CSS outputs to the correct speaker. Here, we propose a syntactically informed diarization using sentence- and word-level boundaries of the ASR module to support speaker turn detection. This results in a state-of-the-art Concatenated minimum-Permutation Word Error Rate (cpWER) for the full meeting recognition pipeline.
</details>
<details>
<summary>摘要</summary>
我们提出了一个模块化管道来实现单频约会录音的分离、识别和分类，并在Libri-CSS数据集上进行评估。我们使用一个连续语音分离（CSS）系统，其中使用TF-GridNet分离架构，然后使用一个无关于Speaker的语音识别器，以达到最佳的语音识别性能（ORC WER）。接着，我们使用d-vector基于的分类模块来提取Speaker的嵌入特征，并将CSS输出分配给正确的 speaker。在这里，我们提出了一种基于ASR模块的句子和单词级划分的语音转发检测，以支持说话者的转发检测。这 führt zu einem state-of-the-art Concatenated minimum-Permutation Word Error Rate (cpWER) für die vollständige Treffenserkennungspipeline.
</details></li>
</ul>
<hr>
<h2 id="Efficient-Supervised-Training-of-Audio-Transformers-for-Music-Representation-Learning"><a href="#Efficient-Supervised-Training-of-Audio-Transformers-for-Music-Representation-Learning" class="headerlink" title="Efficient Supervised Training of Audio Transformers for Music Representation Learning"></a>Efficient Supervised Training of Audio Transformers for Music Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16418">http://arxiv.org/abs/2309.16418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/palonso/maest">https://github.com/palonso/maest</a></li>
<li>paper_authors: Pablo Alonso-Jiménez, Xavier Serra, Dmitry Bogdanov</li>
<li>for: 这个论文主要针对音乐表示学习，使用无核Transformer来替代传统的卷积神经网络。</li>
<li>methods: 作者们使用patchout训练方法和不同的输入音频段长来训练模型，并研究不同块和Token的learned表示对下游任务的影响。</li>
<li>results: 研究发现，初始化模型使用ImageNet或AudioSet权重和使用 longer输入段都是有利的，中间块的learned表示是最佳的，并且在执行patchout操作时可以更快速地提取特征，而无需降低性能。<details>
<summary>Abstract</summary>
In this work, we address music representation learning using convolution-free transformers. We build on top of existing spectrogram-based audio transformers such as AST and train our models on a supervised task using patchout training similar to PaSST. In contrast to previous works, we study how specific design decisions affect downstream music tagging tasks instead of focusing on the training task. We assess the impact of initializing the models with different pre-trained weights, using various input audio segment lengths, using learned representations from different blocks and tokens of the transformer for downstream tasks, and applying patchout at inference to speed up feature extraction. We find that 1) initializing the model from ImageNet or AudioSet weights and using longer input segments are beneficial both for the training and downstream tasks, 2) the best representations for the considered downstream tasks are located in the middle blocks of the transformer, and 3) using patchout at inference allows faster processing than our convolutional baselines while maintaining superior performance. The resulting models, MAEST, are publicly available and obtain the best performance among open models in music tagging tasks.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了使用减少 convolution 的 transformer 进行音乐表示学习。我们基于现有的spectrogram-based audio transformer 如 AST，并在supervised任务上训练我们的模型，类似于 PaSST。与前一些工作不同，我们研究了特定的设计决策对下游音乐标签任务的影响，而不是专注于训练任务。我们评估了不同初始化模型的预训练 веса、使用不同的输入音频段长、使用不同块和token的 transformer 学习表示，以及在推理时使用 patchout 加速特征提取。我们发现：1）从 ImageNet 或 AudioSet 预训练 weights 初始化模型和使用 longer input segments 都是有利的，2）Considered downstream tasks 中最佳的表示位于 transformer 中间块中，3）在推理时使用 patchout 可以比我们的 convolutional baselines 更快，但是保持了更高的性能。所有的模型，MAEST，公开可用，并在音乐标签任务中获得了最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Speaker-Localization-from-EgoCentric-Views"><a href="#Audio-Visual-Speaker-Localization-from-EgoCentric-Views" class="headerlink" title="Audio Visual Speaker Localization from EgoCentric Views"></a>Audio Visual Speaker Localization from EgoCentric Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16308">http://arxiv.org/abs/2309.16308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kawhizhao/egocentric-audio-visual-speaker-localization">https://github.com/kawhizhao/egocentric-audio-visual-speaker-localization</a></li>
<li>paper_authors: Jinzheng Zhao, Yong Xu, Xinyuan Qian, Wenwu Wang</li>
<li>for: 这 paper 是为了研究 egocentric 音频视频推测说话人的方向的。</li>
<li>methods: 这 paper 使用 transformer 模型将 audio 和视频数据进行融合，并提出了一种训练策略来解决说话人从视频中消失的问题。</li>
<li>results: 实验结果表明，提出的方法在新的数据集上表现了出色的跟踪精度。此外，paper 还适应了多个说话人enario，并在 EasyCom 上实现了 state-of-the-art 的结果。<details>
<summary>Abstract</summary>
The use of audio and visual modality for speaker localization has been well studied in the literature by exploiting their complementary characteristics. However, most previous works employ the setting of static sensors mounted at fixed positions. Unlike them, in this work, we explore the ego-centric setting, where the heterogeneous sensors are embodied and could be moving with a human to facilitate speaker localization. Compared to the static scenario, the ego-centric setting is more realistic for smart-home applications e.g., a service robot. However, this also brings new challenges such as blurred images, frequent speaker disappearance from the field of view of the wearer, and occlusions. In this paper, we study egocentric audio-visual speaker DOA estimation and deal with the challenges mentioned above. Specifically, we propose a transformer-based audio-visual fusion method to estimate the relative DOA of the speaker to the wearer, and design a training strategy to mitigate the problem of the speaker disappearing from the camera's view. We also develop a new dataset for simulating the out-of-view scenarios, by creating a scene with a camera wearer walking around while a speaker is moving at the same time. The experimental results show that our proposed method offers promising performance in this new dataset in terms of tracking accuracy. Finally, we adapt the proposed method for the multi-speaker scenario. Experiments on EasyCom show the effectiveness of the proposed model for multiple speakers in real scenarios, which achieves state-of-the-art results in the sphere active speaker detection task and the wearer activity prediction task. The simulated dataset and related code are available at https://github.com/KawhiZhao/Egocentric-Audio-Visual-Speaker-Localization.
</details>
<details>
<summary>摘要</summary>
文中的听音和视觉模态已经在文献中得到了广泛的研究，通过利用它们的补充特点。然而，大多数前一些工作采用了固定位置的静止感知器。与之不同，在这种工作中，我们 explore egocentric setting，其中各种不同的感知器被嵌入并可以随人类移动，以便实现听音定位。相比静止情况， egocentric setting更加真实地适用于智能家居应用，例如服务机器人。然而，这也带来了新的挑战，如模糊的图像、听音器在感知器视野中的频繁消失和遮挡。在这篇论文中，我们研究 egocentric 听音视觉 DOA 估计，并解决以上挑战。具体来说，我们提出了一种基于 transformer 的听音视觉混合方法，以估计听音器与感知器之间的相对 DOA。此外，我们还开发了一个用于模拟听音器离视场景的新数据集，通过创建一个有人戴摄像头的人移动在同时的场景。实验结果表明，我们提出的方法在新数据集中具有良好的跟踪精度。最后，我们适应了提posed方法 для多个听音器场景。在 EasyCom 上进行实验，我们的模型在实际场景中得到了状态之一的结果，包括圆拱活动听音器检测任务和穿戴者活动预测任务。相关的数据集和代码可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="Predicting-performance-difficulty-from-piano-sheet-music-images"><a href="#Predicting-performance-difficulty-from-piano-sheet-music-images" class="headerlink" title="Predicting performance difficulty from piano sheet music images"></a>Predicting performance difficulty from piano sheet music images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16287">http://arxiv.org/abs/2309.16287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Ramoneda, Jose J. Valero-Mas, Dasaem Jeong, Xavier Serra</li>
<li>for: 这 paper 的目的是提出一种用于估算音乐作品的演奏难度的方法，以便在音乐教育中设计学生学习课程。</li>
<li>methods: 该方法使用 transformer 模型和 mid-level representation，即 bootleg score，来描述音乐作品的乐谱图像。 encoding scheme 被引入以将原始序列长度减少到一半。</li>
<li>results: 在五个 datasets 上进行评估，包括超过 7500 份乐谱和 9 个难度水平。 模型在这些 datasets 上进行精度的 fine-tuning 后，实现了最佳性能，其中 balanced accuracy 为 40.34%，mean square error 为 1.33。<details>
<summary>Abstract</summary>
Estimating the performance difficulty of a musical score is crucial in music education for adequately designing the learning curriculum of the students. Although the Music Information Retrieval community has recently shown interest in this task, existing approaches mainly use machine-readable scores, leaving the broader case of sheet music images unaddressed. Based on previous works involving sheet music images, we use a mid-level representation, bootleg score, describing notehead positions relative to staff lines coupled with a transformer model. This architecture is adapted to our task by introducing an encoding scheme that reduces the encoded sequence length to one-eighth of the original size. In terms of evaluation, we consider five datasets -- more than 7500 scores with up to 9 difficulty levels -- , two of them particularly compiled for this work. The results obtained when pretraining the scheme on the IMSLP corpus and fine-tuning it on the considered datasets prove the proposal's validity, achieving the best-performing model with a balanced accuracy of 40.34\% and a mean square error of 1.33. Finally, we provide access to our code, data, and models for transparency and reproducibility.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)估计乐谱演奏难度是音乐教育中对学生学习课程设计的关键。尽管音乐信息检索社区最近对这项任务表示了兴趣，但现有的方法主要使用机器可读 Musical scores，忽略了Sheet music images的更广泛情况。基于之前的Sheet music images工作，我们使用mid-level representation，即bootleg score，描述Notehead positions relative to staff lines，并与Transformer模型结合。我们采用一种编码方案，将编码序列长度减少到原始大小的一半。在评估方面，我们考虑了五个数据集，包括超过7500份乐谱，最高达9级难度。两个数据集特别为这项工作编制。结果表明我们的提案有效，在7500份乐谱上取得平均折合率40.34%和平均方差1.33。最后，我们提供了代码、数据和模型，以便透明度和重现性。
</details></li>
</ul>
<hr>
<h2 id="NOMAD-Unsupervised-Learning-of-Perceptual-Embeddings-for-Speech-Enhancement-and-Non-matching-Reference-Audio-Quality-Assessment"><a href="#NOMAD-Unsupervised-Learning-of-Perceptual-Embeddings-for-Speech-Enhancement-and-Non-matching-Reference-Audio-Quality-Assessment" class="headerlink" title="NOMAD: Unsupervised Learning of Perceptual Embeddings for Speech Enhancement and Non-matching Reference Audio Quality Assessment"></a>NOMAD: Unsupervised Learning of Perceptual Embeddings for Speech Enhancement and Non-matching Reference Audio Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16284">http://arxiv.org/abs/2309.16284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alessandroragano/nomad">https://github.com/alessandroragano/nomad</a></li>
<li>paper_authors: Alessandro Ragano, Jan Skoglund, Andrew Hines</li>
<li>for: 这篇论文是为了提出一种不同声音参照的非匹配式媒体距离度量方法（NOMAD），用于评估受损音频质量。</li>
<li>methods: 该方法基于学习深度特征嵌入，使用三重损失函数驱动NSIM指数来捕捉受损程度。在推理阶段，将任意两个声音样本的相似度分数计算为嵌入空间的欧氏距离。</li>
<li>results: 对于三个任务（评估受损程度、预测语音质量和Speech增强），NOMAD表现出了与全参照音频 metric 的竞争性能，同时也超过了其他非匹配参照方法。这表明 NOMAD 可以准确地评估受损音频质量，并且可以学习人类对受损音频的识别能力。<details>
<summary>Abstract</summary>
This paper presents NOMAD (Non-Matching Audio Distance), a differentiable perceptual similarity metric that measures the distance of a degraded signal against non-matching references. The proposed method is based on learning deep feature embeddings via a triplet loss guided by the Neurogram Similarity Index Measure (NSIM) to capture degradation intensity. During inference, the similarity score between any two audio samples is computed through Euclidean distance of their embeddings. NOMAD is fully unsupervised and can be used in general perceptual audio tasks for audio analysis e.g. quality assessment and generative tasks such as speech enhancement and speech synthesis. The proposed method is evaluated with 3 tasks. Ranking degradation intensity, predicting speech quality, and as a loss function for speech enhancement. Results indicate NOMAD outperforms other non-matching reference approaches in both ranking degradation intensity and quality assessment, exhibiting competitive performance with full-reference audio metrics. NOMAD demonstrates a promising technique that mimics human capabilities in assessing audio quality with non-matching references to learn perceptual embeddings without the need for human-generated labels.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Semantic-Proximity-Alignment-Towards-Human-Perception-consistent-Audio-Tagging-by-Aligning-with-Label-Text-Description"><a href="#Semantic-Proximity-Alignment-Towards-Human-Perception-consistent-Audio-Tagging-by-Aligning-with-Label-Text-Description" class="headerlink" title="Semantic Proximity Alignment: Towards Human Perception-consistent Audio Tagging by Aligning with Label Text Description"></a>Semantic Proximity Alignment: Towards Human Perception-consistent Audio Tagging by Aligning with Label Text Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16265">http://arxiv.org/abs/2309.16265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wuyang Liu, Yanzhen Ren</li>
<li>for: 提高音频标签模型的准确率和人工评估的一致性，使模型更加准确地捕捉音频中的听话关系和层次结构。</li>
<li>methods: 使用auxiliary文本描述音频事件，对audio特征进行Semantic Proximity Alignment（SPA），使audio编码器内置听话关系和层次结构信息，从而提高模型的OmAP评估指标。</li>
<li>results: 对比一个hot标签solely模型，使用SPA模型可以提高OmAP评估指标+1.8，并且人工评估表明SPA模型的预测更加准确地反映人类听频识别结果。<details>
<summary>Abstract</summary>
Most audio tagging models are trained with one-hot labels as supervised information. However, one-hot labels treat all sound events equally, ignoring the semantic hierarchy and proximity relationships between sound events. In contrast, the event descriptions contains richer information, describing the distance between different sound events with semantic proximity. In this paper, we explore the impact of training audio tagging models with auxiliary text descriptions of sound events. By aligning the audio features with the text features of corresponding labels, we inject the hierarchy and proximity information of sound events into audio encoders, improving the performance while making the prediction more consistent with human perception. We refer to this approach as Semantic Proximity Alignment (SPA). We use Ontology-aware mean Average Precision (OmAP) as the main evaluation metric for the models. OmAP reweights the false positives based on Audioset ontology distance and is more consistent with human perception compared to mAP. Experimental results show that the audio tagging models trained with SPA achieve higher OmAP compared to models trained with one-hot labels solely (+1.8 OmAP). Human evaluations also demonstrate that the predictions of SPA models are more consistent with human perception.
</details>
<details>
<summary>摘要</summary>
大多数音频标记模型都是通过一个热度标签作为监督信息进行训练。然而，这些热度标签忽略了声音事件的 semantic hierarchy和 proximity 关系。相比之下，事件描述包含更多的信息，描述了不同声音事件之间的距离和 semantic proximity。在这篇论文中，我们研究了在音频标记模型中使用辅助文本描述声音事件的影响。我们将音频特征与文本描述的对应标签进行对齐，从而将声音事件的 hierarchy和 proximity 信息注入到音频编码器中，提高了性能，同时使预测更加符合人类感知。我们称这种方法为 Semantic Proximity Alignment (SPA)。我们使用 Ontology-aware mean Average Precision (OmAP) 作为主要评估指标，OmAP 重新计算false positives，根据 Audioset  ontology distance 进行权重，与人类感知更一致。实验结果表明，使用 SPA 训练的音频标记模型在 OmAP 方面高于使用一个热度标签solely (+1.8 OmAP)。人类评估也表明，SPA 模型的预测更加符合人类感知。
</details></li>
</ul>
<hr>
<h2 id="PP-MeT-a-Real-world-Personalized-Prompt-based-Meeting-Transcription-System"><a href="#PP-MeT-a-Real-world-Personalized-Prompt-based-Meeting-Transcription-System" class="headerlink" title="PP-MeT: a Real-world Personalized Prompt based Meeting Transcription System"></a>PP-MeT: a Real-world Personalized Prompt based Meeting Transcription System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16247">http://arxiv.org/abs/2309.16247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Lyu, Yuhang Cao, Qing Wang, Jingjing Yin, Yuguang Yang, Pengpeng Zou, Yanni Hu, Heng Lu</li>
<li>for: 提高多个说话人ASR系统在真实世界场景中的准确率和可用性</li>
<li>methods: 使用目标说话者嵌入作为提示在TS-VAD和TS-ASR模块中</li>
<li>results: 在M2MeT2.0挑战数据集上测试集上 achieve cp-CER 11.27%，在固定和开放训练条件下 ranked first<details>
<summary>Abstract</summary>
Speaker-attributed automatic speech recognition (SA-ASR) improves the accuracy and applicability of multi-speaker ASR systems in real-world scenarios by assigning speaker labels to transcribed texts. However, SA-ASR poses unique challenges due to factors such as speaker overlap, speaker variability, background noise, and reverberation. In this study, we propose PP-MeT system, a real-world personalized prompt based meeting transcription system, which consists of a clustering system, target-speaker voice activity detection (TS-VAD), and TS-ASR. Specifically, we utilize target-speaker embedding as a prompt in TS-VAD and TS-ASR modules in our proposed system. In constrast with previous system, we fully leverage pre-trained models for system initialization, thereby bestowing our approach with heightened generalizability and precision. Experiments on M2MeT2.0 Challenge dataset show that our system achieves a cp-CER of 11.27% on the test set, ranking first in both fixed and open training conditions.
</details>
<details>
<summary>摘要</summary>
speaker-attributed automatic speech recognition (SA-ASR) 提高多个说话人ASR系统在实际场景中的准确率和可用性，通过分配说话人标签到转录文本中。然而，SA-ASR存在独特的挑战，包括说话人重叠、说话人差异、背景噪音和回声。在本研究中，我们提出了PP-MeT系统，一种基于个人化提示的实时会议笔记系统，包括聚类系统、目标说话人活动检测（TS-VAD）和TS-ASR。具体来说，我们利用目标说话人嵌入作为TS-VAD和TS-ASR模块的提示。与前一代系统不同，我们完全利用预训练模型进行系统初始化，从而使我们的方法具有更高的泛化性和精度。在M2MeT2.0挑战数据集上进行实验，我们的系统在测试集上 achievement 的 cp-CER 为11.27%，在固定和开放训练条件下都 ranking 第一。
</details></li>
</ul>
<hr>
<h2 id="LAE-ST-MoE-Boosted-Language-Aware-Encoder-Using-Speech-Translation-Auxiliary-Task-for-E2E-Code-switching-ASR"><a href="#LAE-ST-MoE-Boosted-Language-Aware-Encoder-Using-Speech-Translation-Auxiliary-Task-for-E2E-Code-switching-ASR" class="headerlink" title="LAE-ST-MoE: Boosted Language-Aware Encoder Using Speech Translation Auxiliary Task for E2E Code-switching ASR"></a>LAE-ST-MoE: Boosted Language-Aware Encoder Using Speech Translation Auxiliary Task for E2E Code-switching ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16178">http://arxiv.org/abs/2309.16178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guodong Ma, Wenxuan Wang, Yuke Li, Yuting Yang, Binbin Du, Haoran Fu</li>
<li>for: 这篇论文旨在解决自然语言转换（CS）自动话语识别（ASR）中的混乱问题，特别是当语言变化时。</li>
<li>methods: 这篇论文提出了一个名为LAE-ST-MoE的框架，它将语言感知器（LAE）与对话译本（ST）结合在一起，以learn语言之间的背景信息。它还使用了一个任务基于的混合专家模组，将ASR和ST任务分配给不同的feed-forward网络。</li>
<li>results: 实验结果显示，相比于使用LAE-based CTC，LAE-ST-MoE模型在CS测试集上的混合错误率下降了9.26%，并且可以从CS语音中译写出 Mandarin 或英文文本。<details>
<summary>Abstract</summary>
Recently, to mitigate the confusion between different languages in code-switching (CS) automatic speech recognition (ASR), the conditionally factorized models, such as the language-aware encoder (LAE), explicitly disregard the contextual information between different languages. However, this information may be helpful for ASR modeling. To alleviate this issue, we propose the LAE-ST-MoE framework. It incorporates speech translation (ST) tasks into LAE and utilizes ST to learn the contextual information between different languages. It introduces a task-based mixture of expert modules, employing separate feed-forward networks for the ASR and ST tasks. Experimental results on the ASRU 2019 Mandarin-English CS challenge dataset demonstrate that, compared to the LAE-based CTC, the LAE-ST-MoE model achieves a 9.26% mix error reduction on the CS test with the same decoding parameter. Moreover, the well-trained LAE-ST-MoE model can perform ST tasks from CS speech to Mandarin or English text.
</details>
<details>
<summary>摘要</summary>
最近，为了解决自动语音识别（ASR）中语言 switching（CS）中的混乱，conditionally factorized models（如语言意识encoder，LAE）会过度忽略不同语言之间的上下文信息。然而，这些信息可能对ASR模型化有所帮助。为了解决这个问题，我们提出了LAE-ST-MoE框架。它将语音翻译（ST）任务 incorporated into LAE，并通过ST学习不同语言之间的上下文信息。它引入了任务基于的混合专家模块，使用了分离的Feed-Forward网络来处理ASR和ST任务。实验结果表明，相比LAE-based CTC，LAE-ST-MoE模型在ASRU 2019 Mandarin-English CS挑战数据集上的混合错误率减少9.26%。此外，通过训练Well-trained LAE-ST-MoE模型可以将CS语音转换成普通话或英文文本。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Cross-Modality-Knowledge-Transfer-with-Sinkhorn-Attention-for-CTC-based-ASR"><a href="#Hierarchical-Cross-Modality-Knowledge-Transfer-with-Sinkhorn-Attention-for-CTC-based-ASR" class="headerlink" title="Hierarchical Cross-Modality Knowledge Transfer with Sinkhorn Attention for CTC-based ASR"></a>Hierarchical Cross-Modality Knowledge Transfer with Sinkhorn Attention for CTC-based ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16093">http://arxiv.org/abs/2309.16093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai</li>
<li>for: 这个论文的目的是提出一种cross-modality knowledge transfer（CMKT）学习框架，用于将语言模型（PLM）中的语言知识有效地传递给语音编码器，以提高自动语音识别（ASR）的性能。</li>
<li>methods: 这个论文使用了一种基于时间连接的神经网络（CTC）的ASR系统，并在这个系统中应用了层次的音频对应。此外，这个论文还提出了使用Sinkhorn注意力机制来进行跨模态对应过程，其中 transformer 注意力是Sinkhorn注意力的特殊情况。</li>
<li>results: 在使用 CTC 推理（不使用语言模型）的情况下，这个论文在 AISHELL-1 数据集上达到了state-of-the-art的性能，具体来说是 development 集和测试集的字符错误率（CER）分别为 3.64% 和 3.94%，相比基eline CTC-ASR 系统，这个性能提升了 34.18% 和 34.88%。<details>
<summary>Abstract</summary>
Due to the modality discrepancy between textual and acoustic modeling, efficiently transferring linguistic knowledge from a pretrained language model (PLM) to acoustic encoding for automatic speech recognition (ASR) still remains a challenging task. In this study, we propose a cross-modality knowledge transfer (CMKT) learning framework in a temporal connectionist temporal classification (CTC) based ASR system where hierarchical acoustic alignments with the linguistic representation are applied. Additionally, we propose the use of Sinkhorn attention in cross-modality alignment process, where the transformer attention is a special case of this Sinkhorn attention process. The CMKT learning is supposed to compel the acoustic encoder to encode rich linguistic knowledge for ASR. On the AISHELL-1 dataset, with CTC greedy decoding for inference (without using any language model), we achieved state-of-the-art performance with 3.64% and 3.94% character error rates (CERs) for the development and test sets, which corresponding to relative improvements of 34.18% and 34.88% compared to the baseline CTC-ASR system, respectively.
</details>
<details>
<summary>摘要</summary>
由于文本和声音模型之间的Modalität不同，将语言知识从预训练语言模型（PLM）传递到声音编码以实现自动语音识别（ASR）仍然是一项复杂的任务。在这种研究中，我们提出了一种跨Modalität知识传递（CMKT）学习框架，在基于时间连接的朋克思矩阵（CTC）基础上的ASR系统中应用。此外，我们还提出了使用杯形注意力在跨Modalität对应过程中，其中 transformer 注意力是特殊的杯形注意力过程。CMKT 学习计划使得声音编码器编码出较为丰富的语言知识，以便ASR。在 AISHELL-1 数据集上，使用 CTC 滥触推断（不使用任何语言模型），我们在开发和测试集上达到了state-of-the-art性能，即字符错误率（CER）为 3.64% 和 3.94%，对比基准 CTC-ASR 系统的 CER 分别提高了34.18% 和 34.88%。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/28/cs.SD_2023_09_28/" data-id="cloqtaewh00x5gh88czllba3b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/28/cs.CV_2023_09_28/" class="article-date">
  <time datetime="2023-09-28T13:00:00.000Z" itemprop="datePublished">2023-09-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/28/cs.CV_2023_09_28/">cs.CV - 2023-09-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="On-the-Contractivity-of-Plug-and-Play-Operators"><a href="#On-the-Contractivity-of-Plug-and-Play-Operators" class="headerlink" title="On the Contractivity of Plug-and-Play Operators"></a>On the Contractivity of Plug-and-Play Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16899">http://arxiv.org/abs/2309.16899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bhartendu-kumar/pnp-conv">https://github.com/bhartendu-kumar/pnp-conv</a></li>
<li>paper_authors: Chirayu D. Athalye, Kunal N. Chaudhury, Bhartendu Kumar</li>
<li>for: 这个论文是为了探讨 plug-and-play（PnP）膨胀正则化方法的理论基础和实际性能而写的。</li>
<li>methods: 论文使用了一种强大的推理器来取代 proximal 算法中的 proximal 运算符，这种方法被称为 PnP 方法。</li>
<li>results: 论文表明了 PnP 方法在各种图像应用场景中可以达到状态之最的结果，并且提供了一种 Linear 的整形分析方法来解释这种成功。<details>
<summary>Abstract</summary>
In plug-and-play (PnP) regularization, the proximal operator in algorithms such as ISTA and ADMM is replaced by a powerful denoiser. This formal substitution works surprisingly well in practice. In fact, PnP has been shown to give state-of-the-art results for various imaging applications. The empirical success of PnP has motivated researchers to understand its theoretical underpinnings and, in particular, its convergence. It was shown in prior work that for kernel denoisers such as the nonlocal means, PnP-ISTA provably converges under some strong assumptions on the forward model. The present work is motivated by the following questions: Can we relax the assumptions on the forward model? Can the convergence analysis be extended to PnP-ADMM? Can we estimate the convergence rate? In this letter, we resolve these questions using the contraction mapping theorem: (i) for symmetric denoisers, we show that (under mild conditions) PnP-ISTA and PnP-ADMM exhibit linear convergence; and (ii) for kernel denoisers, we show that PnP-ISTA and PnP-ADMM converge linearly for image inpainting. We validate our theoretical findings using reconstruction experiments.
</details>
<details>
<summary>摘要</summary>
在插入式规范（PnP）常规化中，卷积算法中的距离算子被替换成一个强大的噪声约束。这种形式的替换在实践中很好的工作。实际上，PnP已经在不同的图像应用中达到了状态机器人的结果。PnP的 empirical 成功 Motivates 研究人员理解其理论基础，特别是它的收敛。在先前的工作中，对于核函数噪声约束的情况，PnP-ISTA 的收敛性已经被证明了，但是这些假设对于前向模型很强。本文的目标是解答以下问题：我们可以减轻前向模型的假设吗？我们可以扩展 PnP-ADMM 的收敛分析吗？我们可以估算收敛率吗？在这封信中，我们使用Contract Mapping Theorem 来解答这些问题：（i）对于 симметри� denoiser，我们显示（在轻度的假设下）PnP-ISTA 和 PnP-ADMM 在 linear 收敛；和（ii）对于核函数 denoiser，我们显示 PnP-ISTA 和 PnP-ADMM 对于图像缺失部分进行 linear 收敛。我们使用重建实验来验证我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Superpixel-Transformers-for-Efficient-Semantic-Segmentation"><a href="#Superpixel-Transformers-for-Efficient-Semantic-Segmentation" class="headerlink" title="Superpixel Transformers for Efficient Semantic Segmentation"></a>Superpixel Transformers for Efficient Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16889">http://arxiv.org/abs/2309.16889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Zihao Zhu, Jieru Mei, Siyuan Qiao, Hang Yan, Yukun Zhu, Liang-Chieh Chen, Henrik Kretzschmar</li>
<li>for: 本文主要针对 semantic segmentation 任务进行研究，即对每个图像像素进行分类。</li>
<li>methods: 我们提出了一种解决高维度问题的方法，利用 superpixel 的想法和现代转换器框架。我们的模型通过多个地方的跨域关注来将像素空间下降到low维度 superpixel 空间。然后，我们应用多头自注意力来增强 superpixel 特征，并直接生成每个 superpixel 的类别预测。最后，我们将 superpixel 类别预测投影回到像素空间使用图像像素特征的关联。</li>
<li>results: 我们的方法可以在 Cityscapes 和 ADE20K 上实现state-of-the-art性能，同时具有较少的模型参数和延迟。<details>
<summary>Abstract</summary>
Semantic segmentation, which aims to classify every pixel in an image, is a key task in machine perception, with many applications across robotics and autonomous driving. Due to the high dimensionality of this task, most existing approaches use local operations, such as convolutions, to generate per-pixel features. However, these methods are typically unable to effectively leverage global context information due to the high computational costs of operating on a dense image. In this work, we propose a solution to this issue by leveraging the idea of superpixels, an over-segmentation of the image, and applying them with a modern transformer framework. In particular, our model learns to decompose the pixel space into a spatially low dimensional superpixel space via a series of local cross-attentions. We then apply multi-head self-attention to the superpixels to enrich the superpixel features with global context and then directly produce a class prediction for each superpixel. Finally, we directly project the superpixel class predictions back into the pixel space using the associations between the superpixels and the image pixel features. Reasoning in the superpixel space allows our method to be substantially more computationally efficient compared to convolution-based decoder methods. Yet, our method achieves state-of-the-art performance in semantic segmentation due to the rich superpixel features generated by the global self-attention mechanism. Our experiments on Cityscapes and ADE20K demonstrate that our method matches the state of the art in terms of accuracy, while outperforming in terms of model parameters and latency.
</details>
<details>
<summary>摘要</summary>
《 semantic segmentation 》是机器视觉中关键任务之一，具有许多应用于 робо类和自动驾驶等领域。由于这个任务的维度较高，大多数现有方法使用本地操作，如 convolution，生成每个像素特征。然而，这些方法通常无法有效利用全局上下文信息，因为对密集图像进行计算的成本较高。在这种情况下，我们提出了一种解决方案，利用超像素（superpixel）的想法，并将其与现代转换器框架结合使用。具体来说，我们的模型通过一系列的本地交叉关注来将像素空间分解成低维度的超像素空间。然后，我们对超像素进行多头自注意，以激活全局上下文信息，并将其作为每个超像素的特征进行分类。最后，我们直接将超像素的类别预测映射回到像素空间使用图像像素特征与超像素之间的关联。使用超像素空间进行逻辑计算，使我们的方法相比于基于 convolution 的解码器方法更加计算效率高。然而，我们的方法可以 дости到状态之狮的性能水平，因为全球自注意机制生成了丰富的超像素特征。我们在 Cityscapes 和 ADE20K 上进行了实验，结果显示，我们的方法与状态之狮相当，而且在参数量和延迟上超过了状态之狮。
</details></li>
</ul>
<hr>
<h2 id="LEF-Late-to-Early-Temporal-Fusion-for-LiDAR-3D-Object-Detection"><a href="#LEF-Late-to-Early-Temporal-Fusion-for-LiDAR-3D-Object-Detection" class="headerlink" title="LEF: Late-to-Early Temporal Fusion for LiDAR 3D Object Detection"></a>LEF: Late-to-Early Temporal Fusion for LiDAR 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16870">http://arxiv.org/abs/2309.16870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong He, Pei Sun, Zhaoqi Leng, Chenxi Liu, Dragomir Anguelov, Mingxing Tan</li>
<li>for: 本研究旨在提高3D物体检测中的难对象检测精度，通过将物体含义映射到早期启发器中来提高模型的捕捉形态和姿态能力。</li>
<li>methods: 我们提出了一种延迟到早期的Recurrent Feature Fusion（RFF）方法，通过在时间LiDAR点云中执行延迟到早期的窗口基于注意力块来实现。此外，我们还提出了一种 bird’s eye view封顶排序技术，用于减少需要被模型融合的稀疏历史特征数量。</li>
<li>results: 我们在Waymo开放数据集上评估了我们的方法，并证明了它在3D物体检测中的提高，特别是对于困难类别的大物体检测。<details>
<summary>Abstract</summary>
We propose a late-to-early recurrent feature fusion scheme for 3D object detection using temporal LiDAR point clouds. Our main motivation is fusing object-aware latent embeddings into the early stages of a 3D object detector. This feature fusion strategy enables the model to better capture the shapes and poses for challenging objects, compared with learning from raw points directly. Our method conducts late-to-early feature fusion in a recurrent manner. This is achieved by enforcing window-based attention blocks upon temporally calibrated and aligned sparse pillar tokens. Leveraging bird's eye view foreground pillar segmentation, we reduce the number of sparse history features that our model needs to fuse into its current frame by 10$\times$. We also propose a stochastic-length FrameDrop training technique, which generalizes the model to variable frame lengths at inference for improved performance without retraining. We evaluate our method on the widely adopted Waymo Open Dataset and demonstrate improvement on 3D object detection against the baseline model, especially for the challenging category of large objects.
</details>
<details>
<summary>摘要</summary>
我们提议一种从晚期到早期的循环特征融合方案，用于3D物体检测 based on 时间LiDAR点云。我们的主要动机是将物体意识的潜在嵌入 fusion 到3D物体检测器的早期阶段。这种特征融合策略使得模型能够更好地捕捉到难以捕捉的物体形状和姿态。我们的方法在循环方式下进行了晚期到早期的特征融合。这是通过在时间均衡和对齐的稀疏柱 Token 上进行窗口基于注意力块来实现的。通过采用鸟瞰视图前景柱 Segmentation，我们可以将 sparse history features 缩减到当前帧的 10 倍。我们还提出了一种 Stochastic-length FrameDrop 训练技术，该技术可以在推断时对变长帧进行适应，从而不需要重新训练，以提高性能。我们在广泛采用的 Waymo Open Dataset 上评估了我们的方法，并证明了对基eline 模型的3D物体检测性能的提高，特别是难以捕捉的大型物体类别。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Digital-Twin-for-Copy-Detection-Patterns"><a href="#Stochastic-Digital-Twin-for-Copy-Detection-Patterns" class="headerlink" title="Stochastic Digital Twin for Copy Detection Patterns"></a>Stochastic Digital Twin for Copy Detection Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16866">http://arxiv.org/abs/2309.16866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yury Belousov, Olga Taran, Vitaliy Kinakh, Slava Voloshynovskiy</li>
<li>for: 这篇论文主要旨在提高侵游防范技术中的产品安全性，通过利用计算机模拟和数字双方法来优化防范系统。</li>
<li>methods: 本论文使用了一种基于机器学习的数字双方法，称为Turbo框架，以模拟印刷-图像通道的行为。此外，还使用了一种新的泛化模型，即DDPM模型，来评估其在数字双方应用中的可能性。</li>
<li>results: 研究结果表明，DDPM模型在数字双方应用中表现出色，可以更好地捕捉印刷-图像过程中的随机性。此外，DDPM模型也能够在移动设备数据收集中进行效果地评估。 despite the increased complexity of DDPM methods compared to traditional approaches, this study highlights their advantages and explores their potential for future applications.<details>
<summary>Abstract</summary>
Copy detection patterns (CDP) present an efficient technique for product protection against counterfeiting. However, the complexity of studying CDP production variability often results in time-consuming and costly procedures, limiting CDP scalability. Recent advancements in computer modelling, notably the concept of a "digital twin" for printing-imaging channels, allow for enhanced scalability and the optimization of authentication systems. Yet, the development of an accurate digital twin is far from trivial.   This paper extends previous research which modelled a printing-imaging channel using a machine learning-based digital twin for CDP. This model, built upon an information-theoretic framework known as "Turbo", demonstrated superior performance over traditional generative models such as CycleGAN and pix2pix. However, the emerging field of Denoising Diffusion Probabilistic Models (DDPM) presents a potential advancement in generative models due to its ability to stochastically model the inherent randomness of the printing-imaging process, and its impressive performance in image-to-image translation tasks.   This study aims at comparing the capabilities of the Turbo framework and DDPM on the same CDP datasets, with the goal of establishing the real-world benefits of DDPM models for digital twin applications in CDP security. Furthermore, the paper seeks to evaluate the generative potential of the studied models in the context of mobile phone data acquisition. Despite the increased complexity of DDPM methods when compared to traditional approaches, our study highlights their advantages and explores their potential for future applications.
</details>
<details>
<summary>摘要</summary>
kopi detection patterns (CDP) 提供了一种有效的产品安全技术，以防止假冒。然而，研究 CDP 生产变化的复杂性通常会导致时间consuming 和costly的过程，限制 CDP 可扩展性。现代计算机模拟技术，如“数字双” для印刷-图像通道，可以提高 Authentication Systems 的可扩展性和优化。然而，构建准确的数字双是很困难的。  本文是基于前一个研究，使用机器学习基于的数字双模型来模拟印刷-图像通道。这个模型，基于信息论框架知为“Turbo”，在传统生成模型such as CycleGAN 和 pix2pix 中表现出了superior performance。然而，emerging field of Denoising Diffusion Probabilistic Models (DDPM) 提出了一种新的生成模型，它可以随机模型印刷-图像过程中的自然噪音，并在图像-图像翻译任务中表现出了卓越的表现。  本研究的目的是比较 Turbo 框架和 DDPM 在同一个 CDP 数据集上的能力，以确定在 CDP 安全领域中 DDPM 模型的现实世界优势。此外，本研究还检验了这些模型在移动 phone 数据获取中的生成潜力。尽管 DDPM 方法与传统方法相比更加复杂，但我们的研究表明它们具有优势，并探讨了它们在未来应用中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Sketch2CADScript-3D-Scene-Reconstruction-from-2D-Sketch-using-Visual-Transformer-and-Rhino-Grasshopper"><a href="#Sketch2CADScript-3D-Scene-Reconstruction-from-2D-Sketch-using-Visual-Transformer-and-Rhino-Grasshopper" class="headerlink" title="Sketch2CADScript: 3D Scene Reconstruction from 2D Sketch using Visual Transformer and Rhino Grasshopper"></a>Sketch2CADScript: 3D Scene Reconstruction from 2D Sketch using Visual Transformer and Rhino Grasshopper</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16850">http://arxiv.org/abs/2309.16850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Bin Yang</li>
<li>for: 本研究旨在提出一种新的3D重建方法，以解决现有方法生成的3D模型具有粗糙表面和扭曲结构的问题，从而增加人工编辑和后处理的困难。</li>
<li>methods: 本研究使用视觉转换器来预测基于单个线框图像的“场景描述符”，该描述符包含对象类型和相对位置、旋转、大小等参数。然后使用如Blender或Rhino Grasshopper等3D模型软件来重建3D场景，从而获得轻松可编辑的3D模型。</li>
<li>results: 根据两个数据集的测试结果，模型在简单场景中具有高精度重建能力，但在更复杂的场景中存在挑战。<details>
<summary>Abstract</summary>
Existing 3D model reconstruction methods typically produce outputs in the form of voxels, point clouds, or meshes. However, each of these approaches has its limitations and may not be suitable for every scenario. For instance, the resulting model may exhibit a rough surface and distorted structure, making manual editing and post-processing challenging for humans. In this paper, we introduce a novel 3D reconstruction method designed to address these issues. We trained a visual transformer to predict a "scene descriptor" from a single wire-frame image. This descriptor encompasses crucial information, including object types and parameters such as position, rotation, and size. With the predicted parameters, a 3D scene can be reconstructed using 3D modeling software like Blender or Rhino Grasshopper which provides a programmable interface, resulting in finely and easily editable 3D models. To evaluate the proposed model, we created two datasets: one featuring simple scenes and another with complex scenes. The test results demonstrate the model's ability to accurately reconstruct simple scenes but reveal its challenges with more complex ones.
</details>
<details>
<summary>摘要</summary>
（原文）现有的3D模型重建方法通常会生成voxels、点云或mesh等输出。然而，每种方法都有其局限性，可能无法适用于所有情况。例如，生成的模型可能具有抖音表面和扭曲结构，使人工编辑和后处理变得困难。在这篇论文中，我们介绍了一种新的3D重建方法，用于解决这些问题。我们使用视觉变换器来预测基于单个粗细图像的“场景描述符”。这个描述符包括对象类型和参数，如位置、旋转和大小。与预测的参数进行组合，可以使用3D模型创建软件如Blender或Rhino Grasshopper，实现高级编程接口，从而得到高级编辑和轻松修改的3D模型。为评估提案模型，我们创建了两个数据集：一个是简单场景集，另一个是复杂场景集。测试结果表明，模型可以准确重建简单场景，但面临更复杂场景的挑战。
</details></li>
</ul>
<hr>
<h2 id="Space-Time-Attention-with-Shifted-Non-Local-Search"><a href="#Space-Time-Attention-with-Shifted-Non-Local-Search" class="headerlink" title="Space-Time Attention with Shifted Non-Local Search"></a>Space-Time Attention with Shifted Non-Local Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16849">http://arxiv.org/abs/2309.16849</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Kent Gauen, Stanley Chan</li>
<li>for: 这篇研究的目的是提高类比搜寻的精度和视觉品质，以便更好地进行视频去噪。</li>
<li>methods: 这篇研究使用了一种名为“Shifted Non-Local Search”的搜寻策略，它结合了非本地搜寻的品质和预测的偏移量的运算，以提高搜寻的精度和视觉品质。</li>
<li>results: 实验结果显示，这种搜寻策略可以提高视频去噪的品质，PSNR值提高了0.30 dB，并且需要7.5%的总时间。此外，这种搜寻策略可以与现有的空间时间注意模组结合，以达到类比搜寻的最佳效果。<details>
<summary>Abstract</summary>
Efficiently computing attention maps for videos is challenging due to the motion of objects between frames. While a standard non-local search is high-quality for a window surrounding each query point, the window's small size cannot accommodate motion. Methods for long-range motion use an auxiliary network to predict the most similar key coordinates as offsets from each query location. However, accurately predicting this flow field of offsets remains challenging, even for large-scale networks. Small spatial inaccuracies significantly impact the attention module's quality. This paper proposes a search strategy that combines the quality of a non-local search with the range of predicted offsets. The method, named Shifted Non-Local Search, executes a small grid search surrounding the predicted offsets to correct small spatial errors. Our method's in-place computation consumes 10 times less memory and is over 3 times faster than previous work. Experimentally, correcting the small spatial errors improves the video frame alignment quality by over 3 dB PSNR. Our search upgrades existing space-time attention modules, which improves video denoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. We integrate our space-time attention module into a UNet-like architecture to achieve state-of-the-art results on video denoising.
</details>
<details>
<summary>摘要</summary>
computation of attention maps for videos is challenging due to the motion of objects between frames. While a standard non-local search is high-quality for a window surrounding each query point, the window's small size cannot accommodate motion. Methods for long-range motion use an auxiliary network to predict the most similar key coordinates as offsets from each query location. However, accurately predicting this flow field of offsets remains challenging, even for large-scale networks. Small spatial inaccuracies significantly impact the attention module's quality. This paper proposes a search strategy that combines the quality of a non-local search with the range of predicted offsets. The method, named Shifted Non-Local Search, executes a small grid search surrounding the predicted offsets to correct small spatial errors. Our method's in-place computation consumes 10 times less memory and is over 3 times faster than previous work. Experimentally, correcting the small spatial errors improves the video frame alignment quality by over 3 dB PSNR. Our search upgrades existing space-time attention modules, which improves video denoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. We integrate our space-time attention module into a UNet-like architecture to achieve state-of-the-art results on video denoising.Here's the translation in Traditional Chinese: computation of attention maps for videos is challenging due to the motion of objects between frames. While a standard non-local search is high-quality for a window surrounding each query point, the window's small size cannot accommodate motion. Methods for long-range motion use an auxiliary network to predict the most similar key coordinates as offsets from each query location. However, accurately predicting this flow field of offsets remains challenging, even for large-scale networks. Small spatial inaccuracies significantly impact the attention module's quality. This paper proposes a search strategy that combines the quality of a non-local search with the range of predicted offsets. The method, named Shifted Non-Local Search, executes a small grid search surrounding the predicted offsets to correct small spatial errors. Our method's in-place computation consumes 10 times less memory and is over 3 times faster than previous work. Experimentally, correcting the small spatial errors improves the video frame alignment quality by over 3 dB PSNR. Our search upgrades existing space-time attention modules, which improves video denoising results by 0.30 dB PSNR for a 7.5% increase in overall runtime. We integrate our space-time attention module into a UNet-like architecture to achieve state-of-the-art results on video denoising.
</details></li>
</ul>
<hr>
<h2 id="Propagation-and-Attribution-of-Uncertainty-in-Medical-Imaging-Pipelines"><a href="#Propagation-and-Attribution-of-Uncertainty-in-Medical-Imaging-Pipelines" class="headerlink" title="Propagation and Attribution of Uncertainty in Medical Imaging Pipelines"></a>Propagation and Attribution of Uncertainty in Medical Imaging Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16831">http://arxiv.org/abs/2309.16831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leonhardfeiner/uncertainty_propagation">https://github.com/leonhardfeiner/uncertainty_propagation</a></li>
<li>paper_authors: Leonhard F. Feiner, Martin J. Menten, Kerstin Hammernik, Paul Hager, Wenqi Huang, Daniel Rueckert, Rickmer F. Braren, Georgios Kaissis</li>
<li>for: 这篇论文的目的是提出一种方法，用于在医疗影像应用中建立可解释的神经网络。</li>
<li>methods: 这篇论文使用了续生神经网络的协变过程，将不确定性传递到医疗影像管线中的不同阶层。这allow我们将管线中的不确定性总体化，并从后续模型的预测中获得共同不确定性量。此外，我们还可以分开报告每个管线阶层的数据基于不确定性的贡献。</li>
<li>results: 在一个真实的医疗影像管线中，我们使用了这种方法来重建测试数据中的脑和膝盖磁共振影像，并预测影像中的量值信息，例如脑体积、膝盖侧或patient的性别。我们量化地显示了传递不确定性和输入不确定性之间的相关性，并比较管线阶层对共同不确定性量的贡献比例。<details>
<summary>Abstract</summary>
Uncertainty estimation, which provides a means of building explainable neural networks for medical imaging applications, have mostly been studied for single deep learning models that focus on a specific task. In this paper, we propose a method to propagate uncertainty through cascades of deep learning models in medical imaging pipelines. This allows us to aggregate the uncertainty in later stages of the pipeline and to obtain a joint uncertainty measure for the predictions of later models. Additionally, we can separately report contributions of the aleatoric, data-based, uncertainty of every component in the pipeline. We demonstrate the utility of our method on a realistic imaging pipeline that reconstructs undersampled brain and knee magnetic resonance (MR) images and subsequently predicts quantitative information from the images, such as the brain volume, or knee side or patient's sex. We quantitatively show that the propagated uncertainty is correlated with input uncertainty and compare the proportions of contributions of pipeline stages to the joint uncertainty measure.
</details>
<details>
<summary>摘要</summary>
“uncertainty estimation”，它提供了建立可解释性神经网络的方法，用于医疗影像应用。在这篇论文中，我们提议了在医疗影像管道中传播不确定性的方法。这允许我们在后续管道阶段聚合不确定性，并获得多个模型预测结果的共同不确定性度量。此外，我们还可以分别报告每个管道阶段的 aleatoric 不确定性（数据基于的不确定性）的贡献。我们在一个实际的医疗影像管道中重建了扫描不足的脑和膝盖磁共振（MR）图像，并在图像上预测了一些量化信息，如脑体积或膝部位或患者的性别。我们Quantitatively 表明了传播不确定性与输入不确定性之间的相关性，并比较管道阶段对共同不确定性度量的贡献比例。
</details></li>
</ul>
<hr>
<h2 id="MEM-Multi-Modal-Elevation-Mapping-for-Robotics-and-Learning"><a href="#MEM-Multi-Modal-Elevation-Mapping-for-Robotics-and-Learning" class="headerlink" title="MEM: Multi-Modal Elevation Mapping for Robotics and Learning"></a>MEM: Multi-Modal Elevation Mapping for Robotics and Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16818">http://arxiv.org/abs/2309.16818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leggedrobotics/elevation_mapping_cupy">https://github.com/leggedrobotics/elevation_mapping_cupy</a></li>
<li>paper_authors: Gian Erni, Jonas Frey, Takahiro Miki, Matias Mattamala, Marco Hutter</li>
<li>for: This paper is written for robotic and learning tasks that require the fusion of multi-modal information for environment perception.</li>
<li>methods: The paper presents a 2.5D robot-centric elevation mapping framework that fuses multi-modal information from multiple sources into a popular map representation, using a set of fusion algorithms that can be selected based on the information type and user requirements.</li>
<li>results: The paper demonstrates the capabilities of the framework by deploying it on multiple robots with varying sensor configurations and showcasing a range of applications that utilize multi-modal layers, including line detection, human detection, and colorization.<details>
<summary>Abstract</summary>
Elevation maps are commonly used to represent the environment of mobile robots and are instrumental for locomotion and navigation tasks. However, pure geometric information is insufficient for many field applications that require appearance or semantic information, which limits their applicability to other platforms or domains. In this work, we extend a 2.5D robot-centric elevation mapping framework by fusing multi-modal information from multiple sources into a popular map representation. The framework allows inputting data contained in point clouds or images in a unified manner. To manage the different nature of the data, we also present a set of fusion algorithms that can be selected based on the information type and user requirements. Our system is designed to run on the GPU, making it real-time capable for various robotic and learning tasks. We demonstrate the capabilities of our framework by deploying it on multiple robots with varying sensor configurations and showcasing a range of applications that utilize multi-modal layers, including line detection, human detection, and colorization.
</details>
<details>
<summary>摘要</summary>
Mobile robots的环境表示图是常用的工具，但纯粹的几何信息不足以满足许多场景中的应用需求，这限制了其应用于其他平台或领域。在这项工作中，我们将2.5D机器人中心的抬高地图框架扩展为将多种数据源的多Modal信息融合到一起。该框架可以接受点云或图像中的数据，并且我们提供了根据信息类型和用户需求选择的融合算法集。我们的系统采用GPU进行实时处理，以便在多种机器人和学习任务中实现实时性。我们通过在多个机器人上部署我们的框架，并使用不同的感知器配置，展示了多种应用场景，包括线检测、人体检测和彩色化。
</details></li>
</ul>
<hr>
<h2 id="SatDM-Synthesizing-Realistic-Satellite-Image-with-Semantic-Layout-Conditioning-using-Diffusion-Models"><a href="#SatDM-Synthesizing-Realistic-Satellite-Image-with-Semantic-Layout-Conditioning-using-Diffusion-Models" class="headerlink" title="SatDM: Synthesizing Realistic Satellite Image with Semantic Layout Conditioning using Diffusion Models"></a>SatDM: Synthesizing Realistic Satellite Image with Semantic Layout Conditioning using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16812">http://arxiv.org/abs/2309.16812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Orkhan Baghirli, Hamid Askarov, Imran Ibrahimli, Ismat Bakhishov, Nabi Nabiyev</li>
<li>for: 提供一种基于 semantic layout 的 conditional DDPM 模型，用于生成高质量、多样化、准确相应的卫星图像。</li>
<li>methods: 使用 variance learning、classifier-free guidance 和改进的噪声调度等技术来优化 denoising network  architecture，并integrate adaptive normalization和自注意机制以提高模型的能力。</li>
<li>results: 验证结果表明，提posed model 能够生成高质量、多样化、准确相应的卫星图像，并且与实际图像的差异非常小。<details>
<summary>Abstract</summary>
Deep learning models in the Earth Observation domain heavily rely on the availability of large-scale accurately labeled satellite imagery. However, obtaining and labeling satellite imagery is a resource-intensive endeavor. While generative models offer a promising solution to address data scarcity, their potential remains underexplored. Recently, Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated significant promise in synthesizing realistic images from semantic layouts. In this paper, a conditional DDPM model capable of taking a semantic map and generating high-quality, diverse, and correspondingly accurate satellite images is implemented. Additionally, a comprehensive illustration of the optimization dynamics is provided. The proposed methodology integrates cutting-edge techniques such as variance learning, classifier-free guidance, and improved noise scheduling. The denoising network architecture is further complemented by the incorporation of adaptive normalization and self-attention mechanisms, enhancing the model's capabilities. The effectiveness of our proposed model is validated using a meticulously labeled dataset introduced within the context of this study. Validation encompasses both algorithmic methods such as Frechet Inception Distance (FID) and Intersection over Union (IoU), as well as a human opinion study. Our findings indicate that the generated samples exhibit minimal deviation from real ones, opening doors for practical applications such as data augmentation. We look forward to further explorations of DDPMs in a wider variety of settings and data modalities. An open-source reference implementation of the algorithm and a link to the benchmarked dataset are provided at https://github.com/obaghirli/syn10-diffusion.
</details>
<details>
<summary>摘要</summary>
深度学习模型在地球观测领域广泛依赖大量高精度卫星成像数据。然而，获取和标注卫星成像数据是资源消耗大的。而生成模型具有潜在的解决数据不足问题的潜力，但它们的潜力仍未得到充分利用。最近，涉及扰动概率模型（DDPMs）在生成真实图像方面表现出了显著的搭配性。在这篇论文中，我们实现了一种基于 semantic map 的受控 DDPM 模型，能够生成高质量、多样化和准确相应的卫星成像图像。此外，我们还提供了完整的优化动态图文献。我们的提案方法 integrate cutting-edge techniques such as variance learning, classifier-free guidance, and improved noise scheduling。减震网络架构还通过自适应 нормализа和自注意机制的添加，进一步提高模型的能力。我们的实验结果表明，生成的样本与实际样本几乎没有差异，开启了实际应用，如数据增强。我们期待以 DDPMs 在更多的设置和数据模式中进行进一步的探索。我们提供了一个开源参考实现和一个具有详细标注的数据集的链接，请参考 https://github.com/obaghirli/syn10-diffusion。
</details></li>
</ul>
<hr>
<h2 id="Granularity-at-Scale-Estimating-Neighborhood-Well-Being-from-High-Resolution-Orthographic-Imagery-and-Hybrid-Learning"><a href="#Granularity-at-Scale-Estimating-Neighborhood-Well-Being-from-High-Resolution-Orthographic-Imagery-and-Hybrid-Learning" class="headerlink" title="Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning"></a>Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16808">http://arxiv.org/abs/2309.16808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vida-nyu/gdpfinder">https://github.com/vida-nyu/gdpfinder</a></li>
<li>paper_authors: Ethan Brewer, Giovani Valdrighi, Parikshit Solunke, Joao Rulff, Yurii Piadyk, Zhonghui Lv, Jorge Poco, Claudio Silva</li>
<li>for:  fills in the gaps of basic information on the well-being of the population in areas with limited data collection methods.</li>
<li>methods:  uses high-resolution imagery from satellite or aircraft, and machine learning and computer vision techniques to extract features and detect patterns in the image data.</li>
<li>results:  accurately estimates population density, median household income, and educational attainment of individual neighborhoods with R$^2$ up to 0.81, and provides a basis for future work to estimate fine-scale information from overhead imagery without label data.<details>
<summary>Abstract</summary>
Many areas of the world are without basic information on the well-being of the residing population due to limitations in existing data collection methods. Overhead images obtained remotely, such as from satellite or aircraft, can help serve as windows into the state of life on the ground and help "fill in the gaps" where community information is sparse, with estimates at smaller geographic scales requiring higher resolution sensors. Concurrent with improved sensor resolutions, recent advancements in machine learning and computer vision have made it possible to quickly extract features from and detect patterns in image data, in the process correlating these features with other information. In this work, we explore how well two approaches, a supervised convolutional neural network and semi-supervised clustering based on bag-of-visual-words, estimate population density, median household income, and educational attainment of individual neighborhoods from publicly available high-resolution imagery of cities throughout the United States. Results and analyses indicate that features extracted from the imagery can accurately estimate the density (R$^2$ up to 0.81) of neighborhoods, with the supervised approach able to explain about half the variation in a population's income and education. In addition to the presented approaches serving as a basis for further geographic generalization, the novel semi-supervised approach provides a foundation for future work seeking to estimate fine-scale information from overhead imagery without the need for label data.
</details>
<details>
<summary>摘要</summary>
许多地方的人口生活状况的基本信息缺失，这是因为现有的数据收集方法有限。 however， remotely obtained overhead images， such as satellite or aircraft images， can serve as "windows" into the state of life on the ground and help "fill in the gaps" where community information is sparse. With the improvement of sensor resolutions, recent advancements in machine learning and computer vision have made it possible to quickly extract features from and detect patterns in image data, and correlate these features with other information.在这个研究中，我们研究了使用公共可用高分辨率图像来估算美国城市 neighborhoods 中人口密度、 median household income 和教育水平。 results 表明，从图像中提取的特征可以准确地估算社区的密度（R$^2$ 可以达到 0.81），并且超vised 方法可以解释人口收入和教育水平的约半部分变化。此外，我们还提出了一种基于 bag-of-visual-words 的半supervised 聚类方法，该方法可以在没有标签数据的情况下，通过图像特征来估算社区的人口密度、收入和教育水平。这些方法不仅可以为未来的地理总结提供基础，而且还可以用于未来无需标签数据来估算社区的人口密度、收入和教育水平。
</details></li>
</ul>
<hr>
<h2 id="Ultra-low-power-Image-Classification-on-Neuromorphic-Hardware"><a href="#Ultra-low-power-Image-Classification-on-Neuromorphic-Hardware" class="headerlink" title="Ultra-low-power Image Classification on Neuromorphic Hardware"></a>Ultra-low-power Image Classification on Neuromorphic Hardware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16795">http://arxiv.org/abs/2309.16795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/biphasic/quartz-on-loihi">https://github.com/biphasic/quartz-on-loihi</a></li>
<li>paper_authors: Gregor Lenz, Garrick Orchard, Sadique Sheik</li>
<li>for: 这个研究旨在实现低功耗的股节神经网络（SNN），并且使用时间和空间簇节来减少资料传输量。</li>
<li>methods: 这个研究使用了一种基于时间到第一射（TTFS）的时间ANN-to-SNN转换方法，实现高精度识别，并且可以轻松地实现在神经遗留器硬件上。</li>
<li>results: 这个研究在MNIST、CIFAR10和ImageNet上进行了 simulated  benchmarking，结果显示了这个方法的优点，包括低功耗、高 Throughput 和低延迟。此外，这个方法可以在Loihi neuromorphic chip上实现，并且提供了证据，认为时间编码具有相似的识别精度，但是具有更低的功耗和更高的 Throughput。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) promise ultra-low-power applications by exploiting temporal and spatial sparsity. The number of binary activations, called spikes, is proportional to the power consumed when executed on neuromorphic hardware. Training such SNNs using backpropagation through time for vision tasks that rely mainly on spatial features is computationally costly. Training a stateless artificial neural network (ANN) to then convert the weights to an SNN is a straightforward alternative when it comes to image recognition datasets. Most conversion methods rely on rate coding in the SNN to represent ANN activation, which uses enormous amounts of spikes and, therefore, energy to encode information. Recently, temporal conversion methods have shown promising results requiring significantly fewer spikes per neuron, but sometimes complex neuron models. We propose a temporal ANN-to-SNN conversion method, which we call Quartz, that is based on the time to first spike (TTFS). Quartz achieves high classification accuracy and can be easily implemented on neuromorphic hardware while using the least amount of synaptic operations and memory accesses. It incurs a cost of two additional synapses per neuron compared to previous temporal conversion methods, which are readily available on neuromorphic hardware. We benchmark Quartz on MNIST, CIFAR10, and ImageNet in simulation to show the benefits of our method and follow up with an implementation on Loihi, a neuromorphic chip by Intel. We provide evidence that temporal coding has advantages in terms of power consumption, throughput, and latency for similar classification accuracy. Our code and models are publicly available.
</details>
<details>
<summary>摘要</summary>
斯宁 neural network (SNN) 承诺在低功耗应用方面具有优势，通过利用时间和空间稀疏性来减少功耗。在神经模仿硬件上运行 SNN 时，活动数（即冲击）与功耗成直接关系。使用反射传播时间来训练 SNN 可能是一个简单的替代方案，但是在图像识别任务中，通常需要大量的冲击来编码信息。现在，使用时间转换方法来将 ANN 转换成 SNN 已经取得了显著的进步，但是这些方法有时会使用复杂的神经元模型。我们提出了一种基于时间到第一冲击（TTFS）的时间 ANN-to-SNN 转换方法，我们称之为Quartz。Quartz 实现了高精度的分类 accuracy 并且可以轻松地在神经模仿硬件上实现，同时使用最少的 synaptic 操作和内存访问。它比前一些时间转换方法增加了两个附加的 synapse 每个神经元，这些附加 synapse 已经可以在神经模仿硬件上进行可用。我们在 MNIST、CIFAR10 和 ImageNet 上使用 simulate 来证明我们的方法的优点，然后对 Loihi  neuromorphic chip 进行实现。我们提供了证据，表明使用时间编码有优势在功耗、吞吐量和延迟方面，对于类似的分类精度。我们的代码和模型都是公共可用。
</details></li>
</ul>
<hr>
<h2 id="STIR-Surgical-Tattoos-in-Infrared"><a href="#STIR-Surgical-Tattoos-in-Infrared" class="headerlink" title="STIR: Surgical Tattoos in Infrared"></a>STIR: Surgical Tattoos in Infrared</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16782">http://arxiv.org/abs/2309.16782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Schmidt, Omid Mohareri, Simon DiMaio, Septimiu E. Salcudean</li>
<li>for: The paper is written for researchers and developers working on image guidance and automation of medical interventions and surgery, specifically in endoscopic environments.</li>
<li>methods: The paper introduces a novel labeling methodology called Surgical Tattoos in Infrared (STIR), which uses invisible IR-fluorescent dye (indocyanine green, ICG) to label tissue points in video clips, allowing for persistent but invisible labels for tracking and mapping.</li>
<li>results: The paper analyzes multiple frame-based tracking methods on STIR using both 3D and 2D endpoint error and accuracy metrics, providing a benchmark dataset for evaluating and improving tracking and mapping methods in endoscopic environments.Here’s the Chinese translation of the three points:</li>
<li>for: 论文是为了医疗图像导航和自动化手术等领域的研究人员和开发者而写的，特别是在内镜环境中。</li>
<li>methods: 论文介绍了一种新的标注方法，即医学纹理标注法（STIR），它使用不可见的IR抗静电粉末（印度氧绿素，ICG）标注组织点，从而实现了 persistente但是不可见的标注，便于跟踪和映射。</li>
<li>results: 论文使用STIR benchmark数据集进行多个帧基于跟踪方法的分析，包括3D和2D终点误差和精度度量，以提供跟踪和映射方法在内镜环境中的评估和改进的基准数据集。<details>
<summary>Abstract</summary>
Quantifying performance of methods for tracking and mapping tissue in endoscopic environments is essential for enabling image guidance and automation of medical interventions and surgery. Datasets developed so far either use rigid environments, visible markers, or require annotators to label salient points in videos after collection. These are respectively: not general, visible to algorithms, or costly and error-prone. We introduce a novel labeling methodology along with a dataset that uses said methodology, Surgical Tattoos in Infrared (STIR). STIR has labels that are persistent but invisible to visible spectrum algorithms. This is done by labelling tissue points with IR-flourescent dye, indocyanine green (ICG), and then collecting visible light video clips. STIR comprises hundreds of stereo video clips in both in-vivo and ex-vivo scenes with start and end points labelled in the IR spectrum. With over 3,000 labelled points, STIR will help to quantify and enable better analysis of tracking and mapping methods. After introducing STIR, we analyze multiple different frame-based tracking methods on STIR using both 3D and 2D endpoint error and accuracy metrics. STIR is available at https://dx.doi.org/10.21227/w8g4-g548
</details>
<details>
<summary>摘要</summary>
量化跟踪和地图方法的性能在镜头环境中是医疗图像导航和自动化手术的关键。现有的数据集都有一些缺点，包括：不通用、可见到算法或需要标注视频后集成。我们介绍了一种新的标注方法和数据集，即镜头纹身（STIR）。STIR使用不可见光谱的IR染料，即尼龙绿（ICG）染料，标注组织点。采集到的视频帧都是可见光谱的。STIR包含了数百个斯tereo视频帧，包括生物体内和外场景，标注了开始和结束点。总共有超过3000个标注点，STIR将帮助量化和促进跟踪和地图方法的分析。之后，我们使用STIR进行多种帧基于跟踪方法的分析，包括3D和2D终点误差和准确率指标。STIR可以在https://dx.doi.org/10.21227/w8g4-g548上下载。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-Systems-for-Crater-Detection-A-Review"><a href="#Deep-Learning-based-Systems-for-Crater-Detection-A-Review" class="headerlink" title="Deep Learning based Systems for Crater Detection: A Review"></a>Deep Learning based Systems for Crater Detection: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07727">http://arxiv.org/abs/2310.07727</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atal Tewari, K Prateek, Amrita Singh, Nitin Khanna</li>
<li>for: 本文旨在帮助研究者在撞击坑检测领域，检视深度学习基于的撞击坑检测算法（CDA）的发展。</li>
<li>methods: 本文对深度学习基于的撞击坑检测算法进行了检视和分类，包括planetary数据、撞击坑数据库和评估指标。具体来说，我们对撞击坑检测中存在的挑战进行了详细介绍，并将深度学习基于的CDA分为三类： semantic segmentation-based、object detection-based 和 classification-based。</li>
<li>results: 我们对所有semantic segmentation-based CDA在一个共同数据集上进行了训练和测试，以评估每个架构在撞击坑检测中的效果和应用前景。此外，我们还提供了未来研究的建议。<details>
<summary>Abstract</summary>
Craters are one of the most prominent features on planetary surfaces, used in applications such as age estimation, hazard detection, and spacecraft navigation. Crater detection is a challenging problem due to various aspects, including complex crater characteristics such as varying sizes and shapes, data resolution, and planetary data types. Similar to other computer vision tasks, deep learning-based approaches have significantly impacted research on crater detection in recent years. This survey aims to assist researchers in this field by examining the development of deep learning-based crater detection algorithms (CDAs). The review includes over 140 research works covering diverse crater detection approaches, including planetary data, craters database, and evaluation metrics. To be specific, we discuss the challenges in crater detection due to the complex properties of the craters and survey the DL-based CDAs by categorizing them into three parts: (a) semantic segmentation-based, (b) object detection-based, and (c) classification-based. Additionally, we have conducted training and testing of all the semantic segmentation-based CDAs on a common dataset to evaluate the effectiveness of each architecture for crater detection and its potential applications. Finally, we have provided recommendations for potential future works.
</details>
<details>
<summary>摘要</summary>
Planetary surfaces often have craters, which are important features used in age estimation, hazard detection, and spacecraft navigation. However, crater detection is a challenging problem due to various factors, including the complexity of crater characteristics such as size and shape, data resolution, and planetary data types. Like other computer vision tasks, deep learning-based approaches have significantly impacted crater detection research in recent years. This survey aims to help researchers in this field by examining the development of deep learning-based crater detection algorithms (CDAs). The review includes over 140 research works covering diverse crater detection approaches, including planetary data, crater databases, and evaluation metrics. Specifically, we discuss the challenges in crater detection due to the complex properties of craters and survey DL-based CDAs by categorizing them into three parts: (a) semantic segmentation-based, (b) object detection-based, and (c) classification-based. Additionally, we have trained and tested all the semantic segmentation-based CDAs on a common dataset to evaluate their effectiveness for crater detection and their potential applications. Finally, we provide recommendations for potential future works.Here's the text in Traditional Chinese:planetary surfaces often have craters, which are important features used in age estimation, hazard detection, and spacecraft navigation. However, crater detection is a challenging problem due to various factors, including the complexity of crater characteristics such as size and shape, data resolution, and planetary data types. Like other computer vision tasks, deep learning-based approaches have significantly impacted crater detection research in recent years. This survey aims to help researchers in this field by examining the development of deep learning-based crater detection algorithms (CDAs). The review includes over 140 research works covering diverse crater detection approaches, including planetary data, crater databases, and evaluation metrics. Specifically, we discuss the challenges in crater detection due to the complex properties of craters and survey DL-based CDAs by categorizing them into three parts: (a) semantic segmentation-based, (b) object detection-based, and (c) classification-based. Additionally, we have trained and tested all the semantic segmentation-based CDAs on a common dataset to evaluate their effectiveness for crater detection and their potential applications. Finally, we provide recommendations for potential future works.
</details></li>
</ul>
<hr>
<h2 id="Prompt-Enhanced-Self-supervised-Representation-Learning-for-Remote-Sensing-Image-Understanding"><a href="#Prompt-Enhanced-Self-supervised-Representation-Learning-for-Remote-Sensing-Image-Understanding" class="headerlink" title="Prompt-Enhanced Self-supervised Representation Learning for Remote Sensing Image Understanding"></a>Prompt-Enhanced Self-supervised Representation Learning for Remote Sensing Image Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00022">http://arxiv.org/abs/2310.00022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingming Zhang, Qingjie Liu, Yunhong Wang</li>
<li>for: 本研究旨在提高自然语言处理技术的性能，尤其是在远程感知图像分析领域。</li>
<li>methods: 我们提出了一种使用原始图像片段作为拟合模板的唤醒自我超级vised学习方法，并通过 semantic consistency 约束来提供上下文信息。</li>
<li>results: 我们的方法在多种下游任务上表现出色，包括土地覆盖分类、semantic segmentation、object detection 和实体分 segmentation。这些结果表明我们的方法可以学习优秀的远程感知表示，并具有高度的泛化和传输性。Here’s the translation in Simplified Chinese:</li>
<li>for: 本研究旨在提高自然语言处理技术的性能，尤其是在远程感知图像分析领域。</li>
<li>methods: 我们提出了一种使用原始图像片段作为拟合模板的唤醒自我超级vised学习方法，并通过 semantic consistency 约束来提供上下文信息。</li>
<li>results: 我们的方法在多种下游任务上表现出色，包括土地覆盖分类、semantic segmentation、object detection 和实体分 segmentation。这些结果表明我们的方法可以学习优秀的远程感知表示，并具有高度的泛化和传输性。<details>
<summary>Abstract</summary>
Learning representations through self-supervision on a large-scale, unlabeled dataset has proven to be highly effective for understanding diverse images, such as those used in remote sensing image analysis. However, remote sensing images often have complex and densely populated scenes, with multiple land objects and no clear foreground objects. This intrinsic property can lead to false positive pairs in contrastive learning, or missing contextual information in reconstructive learning, which can limit the effectiveness of existing self-supervised learning methods. To address these problems, we propose a prompt-enhanced self-supervised representation learning method that uses a simple yet efficient pre-training pipeline. Our approach involves utilizing original image patches as a reconstructive prompt template, and designing a prompt-enhanced generative branch that provides contextual information through semantic consistency constraints. We collected a dataset of over 1.28 million remote sensing images that is comparable to the popular ImageNet dataset, but without specific temporal or geographical constraints. Our experiments show that our method outperforms fully supervised learning models and state-of-the-art self-supervised learning methods on various downstream tasks, including land cover classification, semantic segmentation, object detection, and instance segmentation. These results demonstrate that our approach learns impressive remote sensing representations with high generalization and transferability.
</details>
<details>
<summary>摘要</summary>
学习通过自我监督在大规模、无标注数据集上实现了对多样化图像的理解，如远程感知图像分析中的图像。然而，远程感知图像经常有复杂且受树立的场景，具有多个地面 объек 和没有明确的前景对象，这种内在性可能导致对比学习中的假阳对，或是重建学习中的上下文信息缺失，这些问题限制了现有的自我监督学习方法的效iveness。为解决这些问题，我们提出了一种使用原始图像块作为重构权重模板的提问增强自我监督表示学习方法。我们的方法包括使用原始图像块作为重构权重模板，并设计一个提问增强生成分支，通过语义一致约束提供上下文信息。我们收集了1280万多个远程感知图像的数据集，与popular ImageNet dataset相似，但不受特定的时间或地理约束。我们的实验表明，我们的方法在多个下游任务中比完全监督学习模型和现状最佳自我监督学习方法表现出色，包括地面覆盖分类、semantic segmentation、物体检测和实例 segmentation。这些结果表明，我们的方法学习出色的远程感知表示，具有高度的普适性和传输性。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Transform-for-Generalizable-Instance-wise-Invariance"><a href="#Learning-to-Transform-for-Generalizable-Instance-wise-Invariance" class="headerlink" title="Learning to Transform for Generalizable Instance-wise Invariance"></a>Learning to Transform for Generalizable Instance-wise Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16672">http://arxiv.org/abs/2309.16672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Utkarsh Singhal, Carlos Esteves, Ameesh Makadia, Stella X. Yu</li>
<li>for: 本研究旨在帮助计算机视觉系统具备对自然数据中的空间变换的Robustness。</li>
<li>methods: 我们使用了一种称为”normalizing flow”的方法，用于预测对图像的变换分布，并将其平均值为每个图像。这种分布只依赖于图像实例，因此可以在批处理时对图像进行对齐，并在不同类别之间实现对变换的泛化。</li>
<li>results: 我们在CIFAR 10、CIFAR10-LT和TinyImageNet等 datasets上进行了实验，并证明了我们的方法可以提高准确率和Robustness。特别是，我们的方法可以学习更大的变换范围，比如Augerino和InstaAug。<details>
<summary>Abstract</summary>
Computer vision research has long aimed to build systems that are robust to spatial transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.   We treat invariance as a prediction problem. Given any image, we use a normalizing flow to predict a distribution over transformations and average the predictions over them. Since this distribution only depends on the instance, we can align instances before classifying them and generalize invariance across classes. The same distribution can also be used to adapt to out-of-distribution poses. This normalizing flow is trained end-to-end and can learn a much larger range of transformations than Augerino and InstaAug. When used as data augmentation, our method shows accuracy and robustness gains on CIFAR 10, CIFAR10-LT, and TinyImageNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Decaf-Monocular-Deformation-Capture-for-Face-and-Hand-Interactions"><a href="#Decaf-Monocular-Deformation-Capture-for-Face-and-Hand-Interactions" class="headerlink" title="Decaf: Monocular Deformation Capture for Face and Hand Interactions"></a>Decaf: Monocular Deformation Capture for Face and Hand Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16670">http://arxiv.org/abs/2309.16670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soshi Shimada, Vladislav Golyanik, Patrick Pérez, Christian Theobalt</li>
<li>for: 本研究旨在Addressing the challenges of 3D tracking from monocular RGB videos, particularly the non-rigid deformations of human hands interacting with human faces.</li>
<li>methods: 该研究提出了一种新的方法，基于一种新的手脸动作和互动捕获数据集，使用变量自动编码器提供手脸深度先验，以及模块导航3D跟踪。</li>
<li>results: 研究结果显示，该方法可以生成真实和更有可信度的3D手脸重建，与多个基线比较显著。<details>
<summary>Abstract</summary>
Existing methods for 3D tracking from monocular RGB videos predominantly consider articulated and rigid objects. Modelling dense non-rigid object deformations in this setting remained largely unaddressed so far, although such effects can improve the realism of the downstream applications such as AR/VR and avatar communications. This is due to the severe ill-posedness of the monocular view setting and the associated challenges. While it is possible to naively track multiple non-rigid objects independently using 3D templates or parametric 3D models, such an approach would suffer from multiple artefacts in the resulting 3D estimates such as depth ambiguity, unnatural intra-object collisions and missing or implausible deformations. Hence, this paper introduces the first method that addresses the fundamental challenges depicted above and that allows tracking human hands interacting with human faces in 3D from single monocular RGB videos. We model hands as articulated objects inducing non-rigid face deformations during an active interaction. Our method relies on a new hand-face motion and interaction capture dataset with realistic face deformations acquired with a markerless multi-view camera system. As a pivotal step in its creation, we process the reconstructed raw 3D shapes with position-based dynamics and an approach for non-uniform stiffness estimation of the head tissues, which results in plausible annotations of the surface deformations, hand-face contact regions and head-hand positions. At the core of our neural approach are a variational auto-encoder supplying the hand-face depth prior and modules that guide the 3D tracking by estimating the contacts and the deformations. Our final 3D hand and face reconstructions are realistic and more plausible compared to several baselines applicable in our setting, both quantitatively and qualitatively. https://vcai.mpi-inf.mpg.de/projects/Decaf
</details>
<details>
<summary>摘要</summary>
现有方法主要考虑了由单一RGB视频中捕捉的可动和坚实对象，而模elling非坚实对象变形在这种设置中尚未得到了充分的关注，尽管这些效果可以提高下游应用程序，如AR/VR和人物通信的真实性。这是因为单个视频设置的缺乏定义性和相关挑战所致。虽然可以通过使用3D模板或参数化3D模型来独立地跟踪多个非坚实对象，但这种方法会导致多种artefacts在结果3D估计中出现，包括深度模糊、非自然的内部对象碰撞和缺失或不合理的变形。因此，这篇论文提出了首个解决这些基本挑战的方法，并允许通过单个RGB视频来跟踪人手与人脸的3D交互。我们将手指定为可动对象，并且在活动交互中引起非坚实面部变形。我们的方法基于一个新的手脸动作和互动捕获数据集，该数据集包含真实的面部变形，通过 markerless 多视图摄像头系统获取。在创建该数据集的过程中，我们对重构的raw 3D形状进行位置基于动力学处理，并使用一种非坚实头组织的估计方法，以获得面部变形、手脸接触区域和头部位置的可靠注释。我们的神经网络方法的核心是一种variational auto-encoder，该模型提供了手脸深度优先验证，以及导向3D跟踪的模块。我们的最终3D手脸重建结果比较真实和更有可靠性，相比多个可应用于我们的设置的基准。
</details></li>
</ul>
<hr>
<h2 id="Training-a-Large-Video-Model-on-a-Single-Machine-in-a-Day"><a href="#Training-a-Large-Video-Model-on-a-Single-Machine-in-a-Day" class="headerlink" title="Training a Large Video Model on a Single Machine in a Day"></a>Training a Large Video Model on a Single Machine in a Day</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16669">http://arxiv.org/abs/2309.16669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhaoyue-zephyrus/avion">https://github.com/zhaoyue-zephyrus/avion</a></li>
<li>paper_authors: Yue Zhao, Philipp Krähenbühl</li>
<li>for: 这篇论文是为了提出一种可以在单机器上训练大型视频模型的高效training pipeline，并且在一天内完成训练。</li>
<li>methods: 作者通过调查IO、CPU和GPU计算的瓶颈，并对每个瓶颈进行优化，实现了一个高效的视频训练管道。</li>
<li>results: 相比同类架构的先前工作，该管道可以在一天内 achieved higher accuracy with $\frac{1}{8}$ of the computation.<details>
<summary>Abstract</summary>
Videos are big, complex to pre-process, and slow to train on. State-of-the-art large-scale video models are trained on clusters of 32 or more GPUs for several days. As a consequence, academia largely ceded the training of large video models to industry. In this paper, we show how to still train a state-of-the-art video model on a single machine with eight consumer-grade GPUs in a day. We identify three bottlenecks, IO, CPU, and GPU computation, and optimize each. The result is a highly efficient video training pipeline. For comparable architectures, our pipeline achieves higher accuracies with $\frac{1}{8}$ of the computation compared to prior work. Code is available at https://github.com/zhaoyue-zephyrus/AVION.
</details>
<details>
<summary>摘要</summary>
视频很大，复杂处理，训练时间长。现状的大规模视频模型通常需要cluster of 32或更多的GPU进行数天的训练。在这篇论文中，我们显示了如何在单个机器上使用八款Consumer-grade GPU来训练状态宇的视频模型，只需一天的时间。我们认为IO、CPU和GPU计算是训练 pipeline 中的三个瓶颈，并且优化了每个瓶颈。结果是一个高效的视频训练管道。对于相同的架构，我们的管道可以与先前的工作相比，得到高度的准确率，只需一半的计算时间。代码可以在https://github.com/zhaoyue-zephyrus/AVION 上找到。
</details></li>
</ul>
<hr>
<h2 id="Geodesic-Regression-Characterizes-3D-Shape-Changes-in-the-Female-Brain-During-Menstruation"><a href="#Geodesic-Regression-Characterizes-3D-Shape-Changes-in-the-Female-Brain-During-Menstruation" class="headerlink" title="Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation"></a>Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16662">http://arxiv.org/abs/2309.16662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bioshape-lab/my28brains">https://github.com/bioshape-lab/my28brains</a></li>
<li>paper_authors: Adele Myers, Caitlin Taylor, Emily Jacobs, Nina Miolane<br>for:* The paper aims to investigate the connection between female brain health and sex hormone fluctuations during menopause.methods:* The researchers use geodesic regression on the space of 3D discrete surfaces to characterize the evolution of brain shape during hormone fluctuations.* They propose approximation schemes to accelerate the process and provide rules of thumb for when to use each approximation.results:* The researchers test their approach on synthetic data and show a significant speed-accuracy trade-off.* They apply the method to real brain shape data and produce the first characterization of how the female hippocampus changes shape during the menstrual cycle as a function of progesterone.Here’s the Chinese version of the three key points:for:* 这个论文目的是investigate female brain health和性激素波动之间的连接，尤其是在menopause期间。methods:* 研究人员使用地odesic regression on the space of 3D discrete surfaces来Characterize brain shape evolution during hormone fluctuations.* 他们提出了一些简化方法，并提供了使用情况的规则Of thumb.results:* 研究人员在synthetic data上测试了他们的方法，并显示了明显的速度-准确度贸易。* 他们应用了方法到实际brain shape数据上，并生成了月经期内雌激素水平对女性hippocampus shape的首次Characterization。<details>
<summary>Abstract</summary>
Women are at higher risk of Alzheimer's and other neurological diseases after menopause, and yet research connecting female brain health to sex hormone fluctuations is limited. We seek to investigate this connection by developing tools that quantify 3D shape changes that occur in the brain during sex hormone fluctuations. Geodesic regression on the space of 3D discrete surfaces offers a principled way to characterize the evolution of a brain's shape. However, in its current form, this approach is too computationally expensive for practical use. In this paper, we propose approximation schemes that accelerate geodesic regression on shape spaces of 3D discrete surfaces. We also provide rules of thumb for when each approximation can be used. We test our approach on synthetic data to quantify the speed-accuracy trade-off of these approximations and show that practitioners can expect very significant speed-up while only sacrificing little accuracy. Finally, we apply the method to real brain shape data and produce the first characterization of how the female hippocampus changes shape during the menstrual cycle as a function of progesterone: a characterization made (practically) possible by our approximation schemes. Our work paves the way for comprehensive, practical shape analyses in the fields of bio-medicine and computer vision. Our implementation is publicly available on GitHub: https://github.com/bioshape-lab/my28brains.
</details>
<details>
<summary>摘要</summary>
女性在男性メソッド后は、アルツハイマー病やその他の神経疾患のリスクが高いと考えられています。しかし、女性の脳健康と性ホルモンの変化の间の连関性に関する研究は限定的です。我々は、この连関性を调查するために、性ホルモンの変化による脳の3D形状の変化を数学的に定量するためのツールを开発しました。しかし、现在のこのアプローチは、実用的な目的で使用するためには计算的にもっています。この论文では、3DDiscrete Surface上の地odesic regressionのアプローチを加速するための近似法を提案します。また、各アプローチが适用できる状况についてのルール OF THUMBを提供します。我々は、このアプローチをsynthetic dataに対して试験し、速さ-正确さのトレードオフを评価しました。结果は、実用的な目的で使用するためには、かなりの速さアップを与えることができましたが、正确さについては、ほとんど影响を受けませんでした。最后に、この方法を実际の脳形状データに适用し、女性の月経期によるプロゲステロンの影响による脳のHiPPocampusの形状の変化を描写しました。我々の作业は、生物医学およびコンピュータビジョンの分野で、実用的な形状分析を可能にすることを目的としています。我々の実装は、GitHub上で公开されています。https://github.com/bioshape-lab/my28brains。
</details></li>
</ul>
<hr>
<h2 id="Visual-In-Context-Learning-for-Few-Shot-Eczema-Segmentation"><a href="#Visual-In-Context-Learning-for-Few-Shot-Eczema-Segmentation" class="headerlink" title="Visual In-Context Learning for Few-Shot Eczema Segmentation"></a>Visual In-Context Learning for Few-Shot Eczema Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16656">http://arxiv.org/abs/2309.16656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neelesh Kumar, Oya Aran, Venugopal Vasudevan<br>for:这种研究旨在开发一种基于数字相机图像的自适应诊断系统，帮助患有皮炎的患者自我监测疾病的进程。为了实现这一目标，图像分割是一项重要的任务。现有的皮炎分割方法基于深度神经网络，如CNN-based U-Net和transformer-based Swin U-Net，但它们需要大量的标注数据，这可能很难以获得。methods:我们研究了视觉上下文学习的可能性，以实现几何少例学习皮炎分割。我们提出了一种将SegGPT应用于皮炎分割的策略。在一个标注皮炎图像集上测试，我们发现，只需要2个示例图像，SegGPT的性能比CNN U-Net retrained on 428图像更高（mIoU: 36.69 vs 32.60）。此外，我们发现，使用更多的示例图像可能会对SegGPT的性能产生负面影响。results:我们的结果表明，视觉上下文学习可以在皮炎图像分割中实现几何少例学习，并且可以开发更快速、更好的皮炎诊断解决方案。此外，我们的结果还预示，可以开发包容性的解决方案，以满足那些在训练数据中受欠表现的少数民族。<details>
<summary>Abstract</summary>
Automated diagnosis of eczema from digital camera images is crucial for developing applications that allow patients to self-monitor their recovery. An important component of this is the segmentation of eczema region from such images. Current methods for eczema segmentation rely on deep neural networks such as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While effective, these methods require high volume of annotated data, which can be difficult to obtain. Here, we investigate the capabilities of visual in-context learning that can perform few-shot eczema segmentation with just a handful of examples and without any need for retraining models. Specifically, we propose a strategy for applying in-context learning for eczema segmentation with a generalist vision model called SegGPT. When benchmarked on a dataset of annotated eczema images, we show that SegGPT with just 2 representative example images from the training dataset performs better (mIoU: 36.69) than a CNN U-Net trained on 428 images (mIoU: 32.60). We also discover that using more number of examples for SegGPT may in fact be harmful to its performance. Our result highlights the importance of visual in-context learning in developing faster and better solutions to skin imaging tasks. Our result also paves the way for developing inclusive solutions that can cater to minorities in the demographics who are typically heavily under-represented in the training data.
</details>
<details>
<summary>摘要</summary>
自动诊断皮炎从数字相机图像是致命的，这将帮助开发出让患者自我监测的应用程序。一个重要的组成部分是皮炎区域的分割。目前的皮炎分割方法基于深度神经网络，如卷积神经网络（CNN）基于U-Net或转换器基于Swin U-Net。虽然有效，但这些方法需要大量的标注数据，这可能很难以获得。在这里，我们研究了可视内容学习的能力，可以在几个示例图像的基础上完成皮炎分割。我们提出了在 SegGPT 泛型视觉模型上应用 visual in-context learning 的策略，并在一个标注皮炎图像集上进行了测试。结果显示，只需使用两个示例图像，SegGPT 的表现比 CNN U-Net 在 428 个图像上训练的表现更好（mIoU：36.69）。我们还发现，给 SegGPT 更多的示例可能会对其性能产生负面影响。我们的结果表明，可视内容学习在皮炎图像分割任务中具有重要的意义，并且可能为皮炎诊断和治疗带来更快和更好的解决方案。此外，我们的结果还预示了可能为少数民族群体在训练数据中的重要地位，并且可能为这些群体开发包容性的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Novel-Deep-Learning-Pipeline-for-Automatic-Weapon-Detection"><a href="#Novel-Deep-Learning-Pipeline-for-Automatic-Weapon-Detection" class="headerlink" title="Novel Deep Learning Pipeline for Automatic Weapon Detection"></a>Novel Deep Learning Pipeline for Automatic Weapon Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16654">http://arxiv.org/abs/2309.16654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haribharathi Sivakumar, Vijay Arvind. R, Pawan Ragavendhar V, G. Balamurugan</li>
<li>for: 这篇论文是为了提出一种实时武器检测系统，以应对当今社会中武器和枪击案的快速增长。</li>
<li>methods: 该论文提出了一个新的pipeline，包括一个 ensemble of convolutional neural networks（CNN）with distinct architectures，每个CNN都是通过不同的mini-batch进行训练，以提高系统的准确率和特异性。</li>
<li>results: 该论文通过多个数据集进行比较，发现提出的pipeline在与现有的SoA系统进行比较时，平均提高了5%的准确率、特异性和回归率。<details>
<summary>Abstract</summary>
Weapon and gun violence have recently become a pressing issue today. The degree of these crimes and activities has risen to the point of being termed as an epidemic. This prevalent misuse of weapons calls for an automatic system that detects weapons in real-time. Real-time surveillance video is captured and recorded in almost all public forums and places. These videos contain abundant raw data which can be extracted and processed into meaningful information. This paper proposes a novel pipeline consisting of an ensemble of convolutional neural networks with distinct architectures. Each neural network is trained with a unique mini-batch with little to no overlap in the training samples. This paper will present several promising results using multiple datasets associated with comparing the proposed architecture and state-of-the-art (SoA) models. The proposed pipeline produced an average increase of 5% in accuracy, specificity, and recall compared to the SoA systems.
</details>
<details>
<summary>摘要</summary>
武器和枪击案现在是当今的紧迫问题。这种犯罪和活动的程度已经到了epidemic的水平。这种普遍的武器违法使用需要实时检测武器。现在大多数公共场所和地点都有实时监控视频记录。这些视频含有大量的原始数据，可以提取和处理成有用信息。本文提出了一个新的管道，包括一个ensemble of convolutional neural networks with distinct architectures。每个神经网络都是通过不同的mini-batch进行训练，几乎没有重叠在训练样本上。本文将对多个数据集进行比较，并与状态之前（SoA）模型进行比较。提出的管道在准确性、特异性和恢复率方面平均提高了5% compared to SoA系统。
</details></li>
</ul>
<hr>
<h2 id="DreamGaussian-Generative-Gaussian-Splatting-for-Efficient-3D-Content-Creation"><a href="#DreamGaussian-Generative-Gaussian-Splatting-for-Efficient-3D-Content-Creation" class="headerlink" title="DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation"></a>DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16653">http://arxiv.org/abs/2309.16653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dreamgaussian/dreamgaussian">https://github.com/dreamgaussian/dreamgaussian</a></li>
<li>paper_authors: Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng</li>
<li>for: 这篇论文的目的是提出一种高效高质量的三维内容创建方法。</li>
<li>methods: 这篇论文使用了一种基于分配蒸气 sampling的三维生成方法，并且提出了一种用于生成高质量纹理的高效算法。</li>
<li>results: 该方法可以在只需2分钟时生成高质量纹理化三维模型，相比之下现有方法提高约10倍。<details>
<summary>Abstract</summary>
Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.
</details>
<details>
<summary>摘要</summary>
Recent advances in 3D content creation mostly rely on optimization-based 3D generation via score distillation sampling (SDS). Although these methods have shown promising results, they often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space. Unlike the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks. To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details. Extensive experiments show that our proposed approach achieves superior efficiency and competitive generation quality. Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.
</details></li>
</ul>
<hr>
<h2 id="ConceptGraphs-Open-Vocabulary-3D-Scene-Graphs-for-Perception-and-Planning"><a href="#ConceptGraphs-Open-Vocabulary-3D-Scene-Graphs-for-Perception-and-Planning" class="headerlink" title="ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning"></a>ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16650">http://arxiv.org/abs/2309.16650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, Chuang Gan, Celso Miguel de Melo, Joshua B. Tenenbaum, Antonio Torralba, Florian Shkurti, Liam Paull</li>
<li>for: 这篇论文旨在为机器人建立一种semantic rich yet compact和高效的3D场景表示，以便实现多种任务。</li>
<li>methods: 该论文提出了一种基于大视觉语言模型的Open-vocabulary graph-structured表示方法，通过多视图协调绘制3D场景，并将2D基础模型的输出与3D场景相结合。</li>
<li>results: 该研究表明，该表示方法可以在新的semantic类中泛化，无需收集大量3D数据或者微调模型。此外，该表示方法还能够通过语言提示来实现复杂的空间和semantic概念的推理。<details>
<summary>Abstract</summary>
For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning. Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations. However, these approaches tend to produce maps with per-point feature vectors, which do not scale well in larger environments, nor do they contain semantic spatial relationships between entities in the environment, which are useful for downstream planning. In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models. We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )
</details>
<details>
<summary>摘要</summary>
In this work, we propose ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes. ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multi-view association. The resulting representations generalize to novel semantic classes, without the need to collect large 3D datasets or finetune models.We demonstrate the utility of this representation through a number of downstream planning tasks that are specified through abstract (language) prompts and require complex reasoning over spatial and semantic concepts. (Project page: https://concept-graphs.github.io/ Explainer video: https://youtu.be/mRhNkQwRYnc )
</details></li>
</ul>
<hr>
<h2 id="FLIP-Cross-domain-Face-Anti-spoofing-with-Language-Guidance"><a href="#FLIP-Cross-domain-Face-Anti-spoofing-with-Language-Guidance" class="headerlink" title="FLIP: Cross-domain Face Anti-spoofing with Language Guidance"></a>FLIP: Cross-domain Face Anti-spoofing with Language Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16649">http://arxiv.org/abs/2309.16649</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koushiksrivats/flip">https://github.com/koushiksrivats/flip</a></li>
<li>paper_authors: Koushik Srivatsan, Muzammal Naseer, Karthik Nandakumar</li>
<li>for: 防止面部遮盾攻击（Face Anti-Spoofing，FAS）是安全应用中的重要 ком ponent，但现有的方法对不见过的骗特类型、摄像头感知器和环境因素的普遍性强度不足。</li>
<li>methods: 我们使用了视觉 транс福特（ViT）模型，并将其扩展为多modal（例如CLIP）初始化，以提高FAS任务的普遍性。我们还提出了一种新的方法，通过与自然语言的semantics相互关联，将视觉表现匹配到多个分类描述中，以提高FAS的普遍性。</li>
<li>results: 我们的方法在三个标准协议中进行了广泛的实验，结果显示我们的方法可以对FAS任务进行 Zero-shot 转移，并且在低数据情况下表现更好，比过五击转移的适应 ViT 更好。<details>
<summary>Abstract</summary>
Face anti-spoofing (FAS) or presentation attack detection is an essential component of face recognition systems deployed in security-critical applications. Existing FAS methods have poor generalizability to unseen spoof types, camera sensors, and environmental conditions. Recently, vision transformer (ViT) models have been shown to be effective for the FAS task due to their ability to capture long-range dependencies among image patches. However, adaptive modules or auxiliary loss functions are often required to adapt pre-trained ViT weights learned on large-scale datasets such as ImageNet. In this work, we first show that initializing ViTs with multimodal (e.g., CLIP) pre-trained weights improves generalizability for the FAS task, which is in line with the zero-shot transfer capabilities of vision-language pre-trained (VLP) models. We then propose a novel approach for robust cross-domain FAS by grounding visual representations with the help of natural language. Specifically, we show that aligning the image representation with an ensemble of class descriptions (based on natural language semantics) improves FAS generalizability in low-data regimes. Finally, we propose a multimodal contrastive learning strategy to boost feature generalization further and bridge the gap between source and target domains. Extensive experiments on three standard protocols demonstrate that our method significantly outperforms the state-of-the-art methods, achieving better zero-shot transfer performance than five-shot transfer of adaptive ViTs. Code: https://github.com/koushiksrivats/FLIP
</details>
<details>
<summary>摘要</summary>
“人脸防 spoofing”（FAS）或“发表攻击”检测是安全应用中的重要组成部分。现有的FAS方法具有较差的泛化性，不能适应未见过的骗YPE、摄像头和环境条件。随着Recently, vision transformer（ViT）模型在FAS任务中的表现，它们的长距离依赖性能力使得它们成为FAS任务的有效解决方案。然而，需要适应预训练 ViT Weight 的auxiliary loss function或适应模块来适应预训练 ViT Weight 学习的大规模数据集，如 ImageNet。在这个工作中，我们首先表明，使用多模态（例如 CLIP）预训练 weight 初始化 ViT 可以提高 FAS 任务的泛化性，这与视language预训练（VLP）模型的零扩展转移能力相一致。然后，我们提出了一种新的方法，通过自然语言的语义来固定视觉表示。具体来说，我们发现，将图像表示与一个ensemble of class descriptions（基于自然语言 semantics）进行对应，可以提高 FAS 任务在低数据 régime中的泛化性。最后，我们提出了一种多模态对比学习策略，以提高特征泛化并跨源领域之间的减少。我们的方法在三个标准协议上进行了广泛的实验，并证明了我们的方法可以明显超越当前的state-of-the-art方法，在零扩展转移情况下，我们的方法可以在5shot转移的情况下表现更好。代码：https://github.com/koushiksrivats/FLIP。
</details></li>
</ul>
<hr>
<h2 id="Improving-Equivariance-in-State-of-the-Art-Supervised-Depth-and-Normal-Predictors"><a href="#Improving-Equivariance-in-State-of-the-Art-Supervised-Depth-and-Normal-Predictors" class="headerlink" title="Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors"></a>Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16646">http://arxiv.org/abs/2309.16646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikuhatsune/equivariance">https://github.com/mikuhatsune/equivariance</a></li>
<li>paper_authors: Yuanyi Zhong, Anand Bhattad, Yu-Xiong Wang, David Forsyth</li>
<li>for: 提高 dense depth 和 surface normal 预测器的等变性，使其具有对剪辑和缩放的等变性。</li>
<li>methods: 提出了一种等变化规则化技术，包括均值处理和自我一致损失，以便明确激活剪辑和缩放等变性。</li>
<li>results: 对于 Taskonomy 任务，我们的等变性规则化技术可以适用于 CNN 和 Transformer 架构，不会在测试时增加额外成本，并且显著提高了超参与和半参与学习的性能。此外，对于现有的 state-of-the-art depth 和 normal 预测器，finetuning 我们的损失可以不仅提高等变性，还提高其在 NYU-v2 上的准确率。<details>
<summary>Abstract</summary>
Dense depth and surface normal predictors should possess the equivariant property to cropping-and-resizing -- cropping the input image should result in cropping the same output image. However, we find that state-of-the-art depth and normal predictors, despite having strong performances, surprisingly do not respect equivariance. The problem exists even when crop-and-resize data augmentation is employed during training. To remedy this, we propose an equivariant regularization technique, consisting of an averaging procedure and a self-consistency loss, to explicitly promote cropping-and-resizing equivariance in depth and normal networks. Our approach can be applied to both CNN and Transformer architectures, does not incur extra cost during testing, and notably improves the supervised and semi-supervised learning performance of dense predictors on Taskonomy tasks. Finally, finetuning with our loss on unlabeled images improves not only equivariance but also accuracy of state-of-the-art depth and normal predictors when evaluated on NYU-v2. GitHub link: https://github.com/mikuhatsune/equivariance
</details>
<details>
<summary>摘要</summary>
“density和表面法则预测器应具有均衡性，即裁剪输入图像时，输出图像也应该裁剪。然而，我们发现当前的深度和法则预测器，尽管表现出色，却不尊重均衡性。这个问题甚至存在当使用裁剪和缩放数据增强 durante el entrenamiento。为了解决这个问题，我们提出了一种均衡化训练技术，包括均衡averaging过程和自我一致损失，以直接促进裁剪和缩放均衡性在深度和法则网络中。我们的方法可以应用于CNN和Transformer架构，不会在测试过程中添加额外成本，并且能够提高Taskonomy任务上的超级vised和半supervised学习性能。最后，我们在不包含标注图像的情况下，对现有的深度和法则预测器进行finetuning，不仅提高了均衡性，还提高了其在NYU-v2上的准确率。”Here's the breakdown of the text into Simplified Chinese characters:“density”: 密度 (mìdòu)“surface normal”: 表面法则 (biǎofàng fǎlǜ)“predictors”: 预测器 (yùjièqì)“should possess the equivariant property”: 应具有均衡性 (bìng yǒu yǒu zhèng yì)“cropping the input image should result in cropping the same output image”: 裁剪输入图像，输出图像也应该裁剪 (dīng niè zhǐ yǐng xiàng yǐng, yǐng xiàng yǐng)“despite having strong performances”: 尽管表现出色 (zhōngguān biǎofàng zhèng)“the problem exists even when crop-and-resize data augmentation is employed during training”: 这个问题甚至存在当使用裁剪和缩放数据增强 durante el entrenamiento (zhè ge wèn tī zhīyī cái yǐjīn zài yùdào)“to remedy this, we propose an equivariant regularization technique”: 为解决这个问题，我们提出了一种均衡化训练技术 (bìng yǒu yì zhèng zhì)“consisting of an averaging procedure and a self-consistency loss”: 包括均衡averaging过程和自我一致损失 (bǎng zhì yǐng zhìyì zhèng)“to explicitly promote cropping-and-resizing equivariance in depth and normal networks”: 以直接促进裁剪和缩放均衡性在深度和法则网络中 (yǐng zhì yǐng zhèng zhì yǐng zhèng)“our approach can be applied to both CNN and Transformer architectures”: 我们的方法可以应用于CNN和Transformer架构 (wǒmen de fāngshì kěyǐ bìng yì zhèng zhì)“does not incur extra cost during testing”: 不会在测试过程中添加额外成本 (bù huì zài zhèng yì zhèng)“and notably improves the supervised and semi-supervised learning performance of dense predictors on Taskonomy tasks”: 并能够提高Taskonomy任务上的超级vised和半supervised学习性能 (dànnéng yǐng qián zhèng zhì yǐng zhèng)“Finally, finetuning with our loss on unlabeled images improves not only equivariance but also accuracy of state-of-the-art depth and normal predictors when evaluated on NYU-v2”: 最后，我们在不包含标注图像的情况下，对现有的深度和法则预测器进行finetuning，不仅提高了均衡性，还提高了其在NYU-v2上的准确率 (zuihou, wǒmen zài bù bǎng zhǐ yǐng xiàng yǐng, yǐng xiàng yǐng)
</details></li>
</ul>
<hr>
<h2 id="Deep-Geometrized-Cartoon-Line-Inbetweening"><a href="#Deep-Geometrized-Cartoon-Line-Inbetweening" class="headerlink" title="Deep Geometrized Cartoon Line Inbetweening"></a>Deep Geometrized Cartoon Line Inbetweening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16643">http://arxiv.org/abs/2309.16643</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lisiyao21/animeinbet">https://github.com/lisiyao21/animeinbet</a></li>
<li>paper_authors: Li Siyao, Tianpei Gu, Weiye Xiao, Henghui Ding, Ziwei Liu, Chen Change Loy</li>
<li>for: addressing the inbetweening problem in the anime industry, specifically the generation of intermediate frames between black-and-white line drawings</li>
<li>methods: using a new approach called AnimeInbet, which geometrizes raster line drawings into graphs of endpoints and reframes the inbetweening task as a graph fusion problem with vertex repositioning</li>
<li>results: synthesizing high-quality, clean, and complete intermediate line drawings that outperform existing methods quantitatively and qualitatively, especially in cases with large motions<details>
<summary>Abstract</summary>
We aim to address a significant but understudied problem in the anime industry, namely the inbetweening of cartoon line drawings. Inbetweening involves generating intermediate frames between two black-and-white line drawings and is a time-consuming and expensive process that can benefit from automation. However, existing frame interpolation methods that rely on matching and warping whole raster images are unsuitable for line inbetweening and often produce blurring artifacts that damage the intricate line structures. To preserve the precision and detail of the line drawings, we propose a new approach, AnimeInbet, which geometrizes raster line drawings into graphs of endpoints and reframes the inbetweening task as a graph fusion problem with vertex repositioning. Our method can effectively capture the sparsity and unique structure of line drawings while preserving the details during inbetweening. This is made possible via our novel modules, i.e., vertex geometric embedding, a vertex correspondence Transformer, an effective mechanism for vertex repositioning and a visibility predictor. To train our method, we introduce MixamoLine240, a new dataset of line drawings with ground truth vectorization and matching labels. Our experiments demonstrate that AnimeInbet synthesizes high-quality, clean, and complete intermediate line drawings, outperforming existing methods quantitatively and qualitatively, especially in cases with large motions. Data and code are available at https://github.com/lisiyao21/AnimeInbet.
</details>
<details>
<summary>摘要</summary>
我们目标是解决动漫业界中尚未得到充分研究的问题，即动漫线 Drawing 的夹在中。夹在中需要生成动漫线 Drawing 中两个黑白线 Drawing 之间的中间帧，这是一项时间consuming 和昂贵的过程，可以从自动化中获得利益。然而，现有的帧 interpolate 方法，基于整个矩阵图像匹配和扭曲，对于线 Drawing 来说是不适用的，经常会产生模糊 artifacts ，损害线 Drawing 的精细结构。为保持线 Drawing 的精度和详细情况，我们提出了一种新方法，即 AnimeInbet，它将线 Drawing 转化为图形� Graphics 的结点 Graph ，并将夹在问题转化为图形� Graphics 的结点合并问题。我们的方法可以有效地捕捉线 Drawing 的稀疏性和特殊结构，同时保持精度和详细情况。这是通过我们的新模块，即结点几何嵌入、结点对准 Transformer 和有效的结点重新排列机制，以及可见预测器。为训练我们的方法，我们提出了 MixamoLine240 数据集，这是一个包含线 Drawing 的vectorization和匹配标签的新数据集。我们的实验表明，AnimeInbet 可以生成高质量、干净、完整的中间线 Drawing，超越现有方法，特别是在大动作情况下。数据和代码可以在 https://github.com/lisiyao21/AnimeInbet 上获取。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Instance-Image-Goal-Navigation-through-Correspondence-as-an-Emergent-Phenomenon"><a href="#End-to-End-Instance-Image-Goal-Navigation-through-Correspondence-as-an-Emergent-Phenomenon" class="headerlink" title="End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon"></a>End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16634">http://arxiv.org/abs/2309.16634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Bono, Leonid Antsfeld, Boris Chidlovskii, Philippe Weinzaepfel, Christian Wolf</li>
<li>for: 这 paper 是关于目标导向视觉导航的最新研究，使用大规模机器学习在模拟环境中进行学习。</li>
<li>methods: 这 paper 使用了两个预言任务来解决主要的挑战，即学习精简的表示和学习高容量的感知模块，以便在不知道环境时进行高级别的感知和决策。</li>
<li>results: 实验结果表明，通过使用这两个预言任务，可以帮助模型学习高级别的感知和决策能力，并达到最新的状态册点和最高级别的性能。<details>
<summary>Abstract</summary>
Most recent work in goal oriented visual navigation resorts to large-scale machine learning in simulated environments. The main challenge lies in learning compact representations generalizable to unseen environments and in learning high-capacity perception modules capable of reasoning on high-dimensional input. The latter is particularly difficult when the goal is not given as a category ("ObjectNav") but as an exemplar image ("ImageNav"), as the perception module needs to learn a comparison strategy requiring to solve an underlying visual correspondence problem. This has been shown to be difficult from reward alone or with standard auxiliary tasks. We address this problem through a sequence of two pretext tasks, which serve as a prior for what we argue is one of the main bottleneck in perception, extremely wide-baseline relative pose estimation and visibility prediction in complex scenes. The first pretext task, cross-view completion is a proxy for the underlying visual correspondence problem, while the second task addresses goal detection and finding directly. We propose a new dual encoder with a large-capacity binocular ViT model and show that correspondence solutions naturally emerge from the training signals. Experiments show significant improvements and SOTA performance on the two benchmarks, ImageNav and the Instance-ImageNav variant, where camera intrinsics and height differ between observation and goal.
</details>
<details>
<summary>摘要</summary>
最新的目标导航研究借助大规模机器学习在模拟环境中进行。主要挑战在学习简洁的总结性模型，可以在未经见过的环境中泛化应用，以及学习高容量的感知模块，可以在高维输入上进行理解。特别是当目标不是category("ObjectNav")而是图像("ImageNav")时，感知模块需要学习一种比较策略，解决了下面的视觉匹配问题。这个问题已经被证明是从奖励alone或标准辅助任务中很Difficult。我们通过一系列两个预测任务来解决这个问题，其中第一个任务是cross-view completion，它是视觉匹配问题的代理任务，而第二个任务是直接面向目标检测和定位。我们提出了一个新的双 encode器，其中包括一个大容量的双目视力模型（ViT），并证明了对应关系解决方案会自然地从训练信号中出现。实验显示我们的方法在两个标准准则ImageNav和Instance-ImageNav中具有显著改进和SOTA性能。
</details></li>
</ul>
<hr>
<h2 id="Class-Activation-Map-based-Weakly-supervised-Hemorrhage-Segmentation-using-Resnet-LSTM-in-Non-Contrast-Computed-Tomography-images"><a href="#Class-Activation-Map-based-Weakly-supervised-Hemorrhage-Segmentation-using-Resnet-LSTM-in-Non-Contrast-Computed-Tomography-images" class="headerlink" title="Class Activation Map-based Weakly supervised Hemorrhage Segmentation using Resnet-LSTM in Non-Contrast Computed Tomography images"></a>Class Activation Map-based Weakly supervised Hemorrhage Segmentation using Resnet-LSTM in Non-Contrast Computed Tomography images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16627">http://arxiv.org/abs/2309.16627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyas H Ramananda, Vaanathi Sundaresan</li>
<li>for: 这个论文的目的是提出一种新的弱监督深度学习方法，用于在NCCT图像中自动 segmentation出脑出血患者。</li>
<li>methods: 该方法使用图像级别的二分类标签，而不需要大量的手动标注每个脑出血患者的Lesion-level标签。首先，使用分类网络来确定脑出血患者的大致位置，然后使用 pseudo-ICH 面积来进一步精细地 segmentation 脑出血患者。</li>
<li>results: 在MICCAI 2022 INSTANCE challenge 的验证数据上，该方法的 dice 值为0.55，与现有的弱监督方法（dice值为0.47）相当，而且在训练数据量更小的情况下达到这一效果。<details>
<summary>Abstract</summary>
In clinical settings, intracranial hemorrhages (ICH) are routinely diagnosed using non-contrast CT (NCCT) for severity assessment. Accurate automated segmentation of ICH lesions is the initial and essential step, immensely useful for such assessment. However, compared to other structural imaging modalities such as MRI, in NCCT images ICH appears with very low contrast and poor SNR. Over recent years, deep learning (DL)-based methods have shown great potential, however, training them requires a huge amount of manually annotated lesion-level labels, with sufficient diversity to capture the characteristics of ICH. In this work, we propose a novel weakly supervised DL method for ICH segmentation on NCCT scans, using image-level binary classification labels, which are less time-consuming and labor-efficient when compared to the manual labeling of individual ICH lesions. Our method initially determines the approximate location of ICH using class activation maps from a classification network, which is trained to learn dependencies across contiguous slices. We further refine the ICH segmentation using pseudo-ICH masks obtained in an unsupervised manner. The method is flexible and uses a computationally light architecture during testing. On evaluating our method on the validation data of the MICCAI 2022 INSTANCE challenge, our method achieves a Dice value of 0.55, comparable with those of existing weakly supervised method (Dice value of 0.47), despite training on a much smaller training data.
</details>
<details>
<summary>摘要</summary>
在临床设置下，脑膜内出血（ICH）通常使用非contrast CT（NCCT）进行严重评估。正确地自动分割ICH损害是初始和基本步骤，对评估具有极大的用处。然而，与其他结构成像Modalities（MRI）相比，在NCCT图像中，ICH具有非常低的冲击和噪声。过去几年，深度学习（DL）基本方法在ICH分割中表现出了极大的潜力，但是它们的训练需要大量的手动标注损害级别Label，以及足够的多样性，以捕捉ICH的特征。在这项工作中，我们提出了一种新的弱监睹DL方法，用于NCCT扫描图像中ICH分割，使用图像级别的二分类标签，相比手动标注每个ICH损害，更加快速和劳动效率。我们的方法首先在分割网络中获得ICH的约束位置，使用分割网络训练得到的类 activation maps，然后进行加工ICH分割。我们的方法是灵活的，在测试时使用轻量级的计算机itecture。在评估我们的方法在MICCAI 2022 INSTANCE挑战的验证数据上，我们的方法达到了0.55的Dice值，与现有的弱监睹方法（Dice值为0.47）相当，即使在训练数据量相对较小的情况下。
</details></li>
</ul>
<hr>
<h2 id="KV-Inversion-KV-Embeddings-Learning-for-Text-Conditioned-Real-Image-Action-Editing"><a href="#KV-Inversion-KV-Embeddings-Learning-for-Text-Conditioned-Real-Image-Action-Editing" class="headerlink" title="KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing"></a>KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16608">http://arxiv.org/abs/2309.16608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng Huang, Yifan Liu, Jin Qin, Shifeng Chen</li>
<li>for: 该论文旨在解决图像修改任务中的动作编辑问题，使得修改后的图像能够符合动作 semantics 和保持原图像的内容。</li>
<li>methods: 我们提出了 KV Inversion 方法，可以实现满意的重建性和动作编辑，解决两个主要问题：1）修改后的结果能够匹配相应的动作，2）修改对象能够保持原图像的текстура和身份。</li>
<li>results: 我们的方法不需要专门培训 Stable Diffusion 模型，也不需要扫描大规模的数据集进行时间consuming的培训。<details>
<summary>Abstract</summary>
Text-conditioned image editing is a recently emerged and highly practical task, and its potential is immeasurable. However, most of the concurrent methods are unable to perform action editing, i.e. they can not produce results that conform to the action semantics of the editing prompt and preserve the content of the original image. To solve the problem of action editing, we propose KV Inversion, a method that can achieve satisfactory reconstruction performance and action editing, which can solve two major problems: 1) the edited result can match the corresponding action, and 2) the edited object can retain the texture and identity of the original real image. In addition, our method does not require training the Stable Diffusion model itself, nor does it require scanning a large-scale dataset to perform time-consuming training.
</details>
<details>
<summary>摘要</summary>
文本受控图像编辑是一个最近出现的高实用性任务，其潜力无法估量。然而，大多数同时期方法无法实现动作编辑，即无法根据编辑提示生成符合动作 semantics 的结果，同时保留原始图像内容。为解决动作编辑问题，我们提议 KV Inversion，一种可以实现满意重构性和动作编辑，解决两个主要问题：1）编辑结果能匹配相应的动作，2）编辑对象能保留原始真实图像的Texture和identify。此外，我们的方法不需要专门培训 Stable Diffusion 模型，也不需要扫描大规模数据集进行时间消耗的训练。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Factorization-for-Leveraging-Cross-Modal-Knowledge-in-Data-Constrained-Infrared-Object-Detection"><a href="#Tensor-Factorization-for-Leveraging-Cross-Modal-Knowledge-in-Data-Constrained-Infrared-Object-Detection" class="headerlink" title="Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection"></a>Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16592">http://arxiv.org/abs/2309.16592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manish Sharma, Moitreya Chatterjee, Kuan-Chuan Peng, Suhas Lohit, Michael Jones</li>
<li>for: 本研究的目的是使IR图像中的物体检测模型获得更好的表现，因为IR图像的训练数据缺乏。</li>
<li>methods: 本研究使用了一种名为TensorFact的新的矩阵分解方法，它可以将卷积层的矩阵分解成低级因子矩阵，从而减少模型中的参数数量。</li>
<li>results: 实验表明，TensorFact可以在RGB图像中提高物体检测性能，并且在IR图像中进行微调可以超过标准的物体检测器。<details>
<summary>Abstract</summary>
The primary bottleneck towards obtaining good recognition performance in IR images is the lack of sufficient labeled training data, owing to the cost of acquiring such data. Realizing that object detection methods for the RGB modality are quite robust (at least for some commonplace classes, like person, car, etc.), thanks to the giant training sets that exist, in this work we seek to leverage cues from the RGB modality to scale object detectors to the IR modality, while preserving model performance in the RGB modality. At the core of our method, is a novel tensor decomposition method called TensorFact which splits the convolution kernels of a layer of a Convolutional Neural Network (CNN) into low-rank factor matrices, with fewer parameters than the original CNN. We first pretrain these factor matrices on the RGB modality, for which plenty of training data are assumed to exist and then augment only a few trainable parameters for training on the IR modality to avoid over-fitting, while encouraging them to capture complementary cues from those trained only on the RGB modality. We validate our approach empirically by first assessing how well our TensorFact decomposed network performs at the task of detecting objects in RGB images vis-a-vis the original network and then look at how well it adapts to IR images of the FLIR ADAS v1 dataset. For the latter, we train models under scenarios that pose challenges stemming from data paucity. From the experiments, we observe that: (i) TensorFact shows performance gains on RGB images; (ii) further, this pre-trained model, when fine-tuned, outperforms a standard state-of-the-art object detector on the FLIR ADAS v1 dataset by about 4% in terms of mAP 50 score.
</details>
<details>
<summary>摘要</summary>
主要瓶颈在获得良好的认知性能方面是因为缺乏充足的标注训练数据，即使是因为获取这些数据的成本。在这项工作中，我们寻求利用RGB模式中对象检测方法的稳定性（至少是一些常见的类型，如人车等），通过RGB模式的大规模训练集，将其扩展到IR模式中，而不会影响模型在RGB模式中的性能。我们的方法的核心是一种新的矩阵因子分解方法，称为TensorFact，它将卷积层的矩阵分解成低级因子矩阵，具有 fewer 参数 than the original CNN。我们首先在RGB模式上预训练这些因子矩阵，然后只需要对IR模式进行一些可训练参数的增强，以避免过拟合，同时鼓励它们捕捉RGB模式中未经训练的辅助信号。我们通过实验证明了我们的方法的效果，先评估我们的TensorFact decomposed network在RGB图像上的性能，然后看看它在FLIR ADAS v1 dataset上如何适应IR图像。为了 simulate 数据缺乏的问题，我们在训练中采用了不同的场景。从实验结果来看，我们有以下观察：(i) TensorFact在RGB图像上显示出性能提升;(ii)此外，我们在FLIR ADAS v1 dataset上使用这个预训练模型进行细化，与标准状态的对象检测器相比，其MAP50得分提高了约4%。
</details></li>
</ul>
<hr>
<h2 id="Vision-Transformers-Need-Registers"><a href="#Vision-Transformers-Need-Registers" class="headerlink" title="Vision Transformers Need Registers"></a>Vision Transformers Need Registers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16588">http://arxiv.org/abs/2309.16588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/dinov2">https://github.com/facebookresearch/dinov2</a></li>
<li>paper_authors: Timothée Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski</li>
<li>for: 本研究旨在解决 transformer 网络中的图像表示学习中的缺陷，提高 visual 表示学习的效果。</li>
<li>methods: 本文使用 both supervised 和 self-supervised ViT 网络，并提出一种简单 yet effective 的解决方案，即提供更多的输入序列，以填充高强度token在推理过程中的缺陷。</li>
<li>results: 本文显示，该解决方案可以 entirely fix 这个问题，并在 dense visual prediction 任务中设置新的州OF THE ART ，允许使用更大的模型进行对象发现，并且导致 downstream 视觉处理中的 feature maps 和 attention maps 更加平滑。<details>
<summary>Abstract</summary>
Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.
</details>
<details>
<summary>摘要</summary>
受欢迎的变换器最近在视觉学习中展示出了强大的工具。在这篇论文中，我们识别和描述了Feature Map中的artefacts。这些artefacts与推理过程中出现的高 нор Tokens相对应，主要出现在图像中的低信息背景区域，并被用于内部计算。我们提出了一个简单 yet有效的解决方案，即在视觉 трансформа器的输入序列中提供更多的Token来替代这个角色。我们表明，这种解决方案可以完全解决supervised和self-supervised模型中的这个问题，并在dense visual prediction任务中设置新的状态anner，启用更大的物体发现方法，并且导致下游视处理中的特征图和注意力图更加平滑。
</details></li>
</ul>
<hr>
<h2 id="Text-to-3D-using-Gaussian-Splatting"><a href="#Text-to-3D-using-Gaussian-Splatting" class="headerlink" title="Text-to-3D using Gaussian Splatting"></a>Text-to-3D using Gaussian Splatting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16585">http://arxiv.org/abs/2309.16585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gsgen3d/gsgen">https://github.com/gsgen3d/gsgen</a></li>
<li>paper_authors: Zilong Chen, Feng Wang, Huaping Liu</li>
<li>for: 高品质3D物体生成</li>
<li>methods: 3D加aussian拼接、进程优化</li>
<li>results: 精确的3D形状和详细构造<details>
<summary>Abstract</summary>
In this paper, we present Gaussian Splatting based text-to-3D generation (GSGEN), a novel approach for generating high-quality 3D objects. Previous methods suffer from inaccurate geometry and limited fidelity due to the absence of 3D prior and proper representation. We leverage 3D Gaussian Splatting, a recent state-of-the-art representation, to address existing shortcomings by exploiting the explicit nature that enables the incorporation of 3D prior. Specifically, our method adopts a progressive optimization strategy, which includes a geometry optimization stage and an appearance refinement stage. In geometry optimization, a coarse representation is established under a 3D geometry prior along with the ordinary 2D SDS loss, ensuring a sensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians undergo an iterative refinement to enrich details. In this stage, we increase the number of Gaussians by compactness-based densification to enhance continuity and improve fidelity. With these designs, our approach can generate 3D content with delicate details and more accurate geometry. Extensive evaluations demonstrate the effectiveness of our method, especially for capturing high-frequency components. Video results are provided at https://gsgen3d.github.io. Our code is available at https://github.com/gsgen3d/gsgen
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了基于 Gaussian Splatting 的文本到 3D 生成方法（GSGEN），这是一种新的方法，用于生成高质量的 3D 对象。先前的方法受到缺乏 3D 先天知识和不准确的几何结构的限制，导致生成的 3D 对象具有偏差和有限的质量。我们利用了三维 Gaussian Splatting，这是当前最佳的表示方式，以解决现有的缺陷，通过质量的考虑，确保了 3D 对象的可见性和质量。我们的方法包括两个阶段：几何优化阶段和外观优化阶段。在几何优化阶段，我们使用了一个粗略的表示，同时遵循 3D 几何先天知识，确保生成的 3D 对象具有理性和可见性。然后，我们通过多个 Gaussians 的迭代优化，以增强细节和提高质量。在外观优化阶段，我们通过增加粒子数量来提高连续性和质量。我们的方法可以生成高质量的 3D 内容，包括细节和几何结构。我们提供了详细的评估结果，显示我们的方法可以更好地捕捉高频成分。视频结果可以在 <https://gsgen3d.github.io/> 中查看。我们的代码可以在 <https://github.com/gsgen3d/gsgen> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Speaker-Verification-via-Joint-Cross-Attention"><a href="#Audio-Visual-Speaker-Verification-via-Joint-Cross-Attention" class="headerlink" title="Audio-Visual Speaker Verification via Joint Cross-Attention"></a>Audio-Visual Speaker Verification via Joint Cross-Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16569">http://arxiv.org/abs/2309.16569</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Gnana Praveen, Jahangir Alam</li>
<li>for: 这种研究的目的是提高Speaker Verification的性能，使用融合 faces和voices 的 Audio-Visual Fusion 技术。</li>
<li>methods: 这种方法使用 cross-modal joint attention 技术，通过对共同特征表示和个体特征表示之间的相关性进行估计，以capture intra-modal和inter-modal关系。</li>
<li>results: 实验结果表明，该方法可以significantly outperform 现有的Audio-Visual Fusion 方法，提高Speaker Verification 性能。<details>
<summary>Abstract</summary>
Speaker verification has been widely explored using speech signals, which has shown significant improvement using deep models. Recently, there has been a surge in exploring faces and voices as they can offer more complementary and comprehensive information than relying only on a single modality of speech signals. Though current methods in the literature on the fusion of faces and voices have shown improvement over that of individual face or voice modalities, the potential of audio-visual fusion is not fully explored for speaker verification. Most of the existing methods based on audio-visual fusion either rely on score-level fusion or simple feature concatenation. In this work, we have explored cross-modal joint attention to fully leverage the inter-modal complementary information and the intra-modal information for speaker verification. Specifically, we estimate the cross-attention weights based on the correlation between the joint feature presentation and that of the individual feature representations in order to effectively capture both intra-modal as well inter-modal relationships among the faces and voices. We have shown that efficiently leveraging the intra- and inter-modal relationships significantly improves the performance of audio-visual fusion for speaker verification. The performance of the proposed approach has been evaluated on the Voxceleb1 dataset. Results show that the proposed approach can significantly outperform the state-of-the-art methods of audio-visual fusion for speaker verification.
</details>
<details>
<summary>摘要</summary>
《 speaker verification 》在使用语音信号方面进行了广泛的探索，并表现出了显著的改进。近期，人们开始探索 faces 和 voices 的可用性，因为它们可以为 speaker verification 提供更多的补充和完整的信息，而不仅仅是依靠单一的语音信号。 existing literature 中的 audio-visual  fusión 方法已经表现出了与单一的 face 或 voice 模态之间的改进，但是 audio-visual  fusión 的潜力还没有得到完全的探索。大多数现有的方法基于 audio-visual  fusión ether rely on score-level fusion 或者简单的 feature concatenation。在这个工作中，我们explored cross-modal joint attention 来全面利用 faces 和 voices 之间的相互补充信息和单一模态信息以进行 speaker verification。 Specifically, we estimate the cross-attention weights based on the correlation between the joint feature presentation and that of the individual feature representations in order to effectively capture both intra-modal as well inter-modal relationships among the faces and voices。我们的方法可以很好地利用 intra-modal 和 inter-modal 关系，从而提高 audio-visual fusión 的性能。我们在 Voxceleb1 数据集上评估了我们的方法，结果表明我们的方法可以在 audio-visual fusión 中对 speaker verification 进行显著改进。
</details></li>
</ul>
<hr>
<h2 id="MatrixCity-A-Large-scale-City-Dataset-for-City-scale-Neural-Rendering-and-Beyond"><a href="#MatrixCity-A-Large-scale-City-Dataset-for-City-scale-Neural-Rendering-and-Beyond" class="headerlink" title="MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond"></a>MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16553">http://arxiv.org/abs/2309.16553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Li, Lihan Jiang, Linning Xu, Yuanbo Xiangli, Zhenzhi Wang, Dahua Lin, Bo Dai</li>
<li>for: 研发大规模、高质量的 synthetic dataset，推动城市级别的神经渲染研究。</li>
<li>methods: 使用 Unreal Engine 5 City Sample project pipeline，收集了飞行和街景视图，并附带了摄像头姿态和多种数据模式。</li>
<li>results: 建立了 MatrixCity 数据集，包含 67k 飞行图像和 452k 街景图像，涵盖两个城市地图，总面积 $28km^2$。<details>
<summary>Abstract</summary>
Neural radiance fields (NeRF) and its subsequent variants have led to remarkable progress in neural rendering. While most of recent neural rendering works focus on objects and small-scale scenes, developing neural rendering methods for city-scale scenes is of great potential in many real-world applications. However, this line of research is impeded by the absence of a comprehensive and high-quality dataset, yet collecting such a dataset over real city-scale scenes is costly, sensitive, and technically difficult. To this end, we build a large-scale, comprehensive, and high-quality synthetic dataset for city-scale neural rendering researches. Leveraging the Unreal Engine 5 City Sample project, we develop a pipeline to easily collect aerial and street city views, accompanied by ground-truth camera poses and a range of additional data modalities. Flexible controls over environmental factors like light, weather, human and car crowd are also available in our pipeline, supporting the need of various tasks covering city-scale neural rendering and beyond. The resulting pilot dataset, MatrixCity, contains 67k aerial images and 452k street images from two city maps of total size $28km^2$. On top of MatrixCity, a thorough benchmark is also conducted, which not only reveals unique challenges of the task of city-scale neural rendering, but also highlights potential improvements for future works. The dataset and code will be publicly available at our project page: https://city-super.github.io/matrixcity/.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRF) 和其 variants 已经带来了对 neural rendering 的巨大进步。然而，大多数最近的 neural rendering 工作都集中在小规模的对象和场景上，发展 neural rendering 方法 для city-scale 场景的潜在应用巨大。然而，这一线索的研究受到了数据缺乏的困难，因为收集这样的数据在真实的 city-scale 场景上是昂贵的、敏感的和技术上有困难。为此，我们建立了一个大规模、全面和高质量的synthetic dataset，用于city-scale neural rendering 研究。我们利用 Unreal Engine 5 City Sample 项目，开发了一个管道，可以轻松地收集空中和街道的城市视图，并附加了相应的摄像头位和多种数据模式。管道中还提供了灵活的环境因素控制，如光、天气、人员和车辆拥堵，以支持多种任务，涵盖 city-scale neural rendering 和更多的应用。结果的预测数据集，MatrixCity，包含 67k 空中图像和 452k 街道图像，总面积为 $28km^2$。在 MatrixCity 之上，我们还进行了一项全面的比较，不仅揭示了城市级 neural rendering 任务的独特挑战，还强调了未来工作的潜在改进方向。数据和代码将在我们项目页面上公开：https://city-super.github.io/matrixcity/.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Quantification-for-Eosinophil-Segmentation"><a href="#Uncertainty-Quantification-for-Eosinophil-Segmentation" class="headerlink" title="Uncertainty Quantification for Eosinophil Segmentation"></a>Uncertainty Quantification for Eosinophil Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16536">http://arxiv.org/abs/2309.16536</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Lin, Donald Brown, Sana Syed, Adam Greene</li>
<li>for: 该研究旨在提高Adorno等人的方法，用深度图像分割来评估嗜内针蛋白。</li>
<li>methods: 该方法使用Monte Carlo Dropout来提供深度学习模型的不确定性评估，并将其视觉化在输出图像中，以评估模型性能、理解深度学习算法的工作方式，并帮助病理学家识别嗜内针蛋白。</li>
<li>results: 该方法可以帮助病理学家更准确地识别嗜内针蛋白，提高诊断效率。<details>
<summary>Abstract</summary>
Eosinophilic Esophagitis (EoE) is an allergic condition increasing in prevalence. To diagnose EoE, pathologists must find 15 or more eosinophils within a single high-power field (400X magnification). Determining whether or not a patient has EoE can be an arduous process and any medical imaging approaches used to assist diagnosis must consider both efficiency and precision. We propose an improvement of Adorno et al's approach for quantifying eosinphils using deep image segmentation. Our new approach leverages Monte Carlo Dropout, a common approach in deep learning to reduce overfitting, to provide uncertainty quantification on current deep learning models. The uncertainty can be visualized in an output image to evaluate model performance, provide insight to how deep learning algorithms function, and assist pathologists in identifying eosinophils.
</details>
<details>
<summary>摘要</summary>
“恶生气肠炎（EoE）是一种增加的 allergy 病种，诊断 EoE 可以是一个困难的过程。为了帮助诊断，任何医学影像方法都必须考虑效率和精度。我们提出了改进 Adorno 等人的方法，使用深度图像分割来量化嗜好细胞。我们的新方法利用 Monte Carlo Dropout，一种常见的深度学习方法来减少预测过拟合，从而提供不确定性量化。这种不确定性可以在输出图像中显示，评估模型性能，提供深度学习算法的运作方式，并帮助病理学家确定嗜好细胞。”
</details></li>
</ul>
<hr>
<h2 id="HOI4ABOT-Human-Object-Interaction-Anticipation-for-Human-Intention-Reading-Collaborative-roBOTs"><a href="#HOI4ABOT-Human-Object-Interaction-Anticipation-for-Human-Intention-Reading-Collaborative-roBOTs" class="headerlink" title="HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs"></a>HOI4ABOT: Human-Object Interaction Anticipation for Human Intention Reading Collaborative roBOTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16524">http://arxiv.org/abs/2309.16524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esteve Valls Mascaro, Daniel Sliwowski, Dongheui Lee</li>
<li>for: 本研究旨在提高人机合作的效率和直观性，通过寻找和预测人机互动（HOI）。</li>
<li>methods: 我们提出了一种基于转换器的HOI探测和预测框架，使用视频数据集进行训练，并通过对比现有方法进行评估。</li>
<li>results: 我们的模型在VidHOI数据集上的探测和预测效果比现有方法高，具体来说是1.76%和1.04%的提升在mAP上，同时速度比现有方法快15.4倍。我们通过实验表明，我们的方法可以帮助人机合作更加效率和直观。<details>
<summary>Abstract</summary>
Robots are becoming increasingly integrated into our lives, assisting us in various tasks. To ensure effective collaboration between humans and robots, it is essential that they understand our intentions and anticipate our actions. In this paper, we propose a Human-Object Interaction (HOI) anticipation framework for collaborative robots. We propose an efficient and robust transformer-based model to detect and anticipate HOIs from videos. This enhanced anticipation empowers robots to proactively assist humans, resulting in more efficient and intuitive collaborations. Our model outperforms state-of-the-art results in HOI detection and anticipation in VidHOI dataset with an increase of 1.76% and 1.04% in mAP respectively while being 15.4 times faster. We showcase the effectiveness of our approach through experimental results in a real robot, demonstrating that the robot's ability to anticipate HOIs is key for better Human-Robot Interaction. More information can be found on our project webpage: https://evm7.github.io/HOI4ABOT_page/
</details>
<details>
<summary>摘要</summary>
роботы все более интегрируются в нашу жизнь, помогая нам в различных задачах. Чтобы обеспечить эффективное сотрудничество между людьми и роботами, важно, чтобы они понимали наши намерения и предсказывали наши действия. В этой статье мы предлагаем фреймворк для предсказания взаимодействия между людьми и объектами (HOI) для коллаборативных роботов. Мы предлагаем эффективный и прочный модель на основе трансформара для детектирования и предсказания HOIs из видео. Улучшенная предсказуемость позволяет роботам проактивно помогать людям, что приводит к более эффективным и интуитивным сотрудничествам. Наша модель превышает результаты государства искусства в HOI-детектировании и предсказании в базе данных VidHOI на 1,76% и 1,04% в mAP соответственно, в то время как она на 15,4 раза быстрее. Мы подтвердили эффективность нашего подхода с помощью экспериментов в реальном роботе, демонстрируя, что способность робота предсказывать HOIs является ключевым фактором для более эффективного взаимодействия между людьми и роботами. За более подробную информацию обратитесь к нашей странице проекта: <https://evm7.github.io/HOI4ABOT_page/>.
</details></li>
</ul>
<hr>
<h2 id="Latent-Noise-Segmentation-How-Neural-Noise-Leads-to-the-Emergence-of-Segmentation-and-Grouping"><a href="#Latent-Noise-Segmentation-How-Neural-Noise-Leads-to-the-Emergence-of-Segmentation-and-Grouping" class="headerlink" title="Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping"></a>Latent Noise Segmentation: How Neural Noise Leads to the Emergence of Segmentation and Grouping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16515">http://arxiv.org/abs/2309.16515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Lonnqvist, Zhengqing Wu, Michael H. Herzog</li>
<li>for: 这个论文的目的是提出一种无监督的分割方法，它利用神经网络的噪声来分割图像。</li>
<li>methods: 这个方法使用的是神经网络，并且添加了噪声来使神经网络能够分割图像，而不需要任何监督标签。</li>
<li>results: 研究发现，这种方法可以成功地分割图像，并且分割结果与人类视觉系统中的分割现象相似。此外，研究还发现，这种方法需要很少的样本数据，并且可以在各种不同的噪声水平下进行。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) that achieve human-level performance in general tasks like object segmentation typically require supervised labels. In contrast, humans are able to perform these tasks effortlessly without supervision. To accomplish this, the human visual system makes use of perceptual grouping. Understanding how perceptual grouping arises in an unsupervised manner is critical for improving both models of the visual system, and computer vision models. In this work, we propose a counterintuitive approach to unsupervised perceptual grouping and segmentation: that they arise because of neural noise, rather than in spite of it. We (1) mathematically demonstrate that under realistic assumptions, neural noise can be used to separate objects from each other, and (2) show that adding noise in a DNN enables the network to segment images even though it was never trained on any segmentation labels. Interestingly, we find that (3) segmenting objects using noise results in segmentation performance that aligns with the perceptual grouping phenomena observed in humans. We introduce the Good Gestalt (GG) datasets -- six datasets designed to specifically test perceptual grouping, and show that our DNN models reproduce many important phenomena in human perception, such as illusory contours, closure, continuity, proximity, and occlusion. Finally, we (4) demonstrate the ecological plausibility of the method by analyzing the sensitivity of the DNN to different magnitudes of noise. We find that some model variants consistently succeed with remarkably low levels of neural noise ($\sigma<0.001$), and surprisingly, that segmenting this way requires as few as a handful of samples. Together, our results suggest a novel unsupervised segmentation method requiring few assumptions, a new explanation for the formation of perceptual grouping, and a potential benefit of neural noise in the visual system.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）可以达到人类水平的执行通用任务，如物体分割，通常需要有监督标签。然而，人类可以通过自然的视觉系统完成这些任务，而无需监督。为了实现这一点，人类视觉系统会利用感知分组。理解感知分组在无监督下如何发生是critical，以改进计算机视觉模型和人类视觉系统。在这个工作中，我们提出了一种Counterintuitive的方法，即感知分组和分割是由神经噪声引起的，而不是它们的障碍。我们（1）数学示出，在实际假设下，神经噪声可以将物体分开，并（2）显示在DNN中添加噪声可以让网络分割图像，即使这些图像从未接受过任何分割标签。有趣的是，我们发现（3）使用噪声进行分割， segmentation的性能与人类感知分组现象相吻合。我们开发了Good Gestalt（GG）数据集，包括六个数据集，用于测试感知分组。我们的DNN模型在这些数据集上展现了许多重要的人类感知现象，如潜在的梯度、闭合、连续性、靠近性和遮挡。最后，我们（4）通过分析神经网络对噪声的敏感性，证明这种方法的生物学可靠性。我们发现一些模型变体可以在remarkably low levels of neural noise（$\sigma<0.001）下成功，而且segmenting这样需要的样本数很少，只需几个样本。总之，我们的结果提出了一种新的无监督分割方法，一种新的感知分组的解释，以及神经噪声在视觉系统中的可能的优点。
</details></li>
</ul>
<hr>
<h2 id="CCEdit-Creative-and-Controllable-Video-Editing-via-Diffusion-Models"><a href="#CCEdit-Creative-and-Controllable-Video-Editing-via-Diffusion-Models" class="headerlink" title="CCEdit: Creative and Controllable Video Editing via Diffusion Models"></a>CCEdit: Creative and Controllable Video Editing via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16496">http://arxiv.org/abs/2309.16496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RuoyuFeng/CCEdit">https://github.com/RuoyuFeng/CCEdit</a></li>
<li>paper_authors: Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, Baining Guo</li>
<li>for: 这篇论文旨在解决视频编辑中的创造性和控制性问题。</li>
<li>methods: 该论文提出了一种名为 CCEdit 的框架，它采用了 ControlNet 架构，并具有可变的时间模块，可以与现有的文本生成技术相结合，如 DreamBooth 和 LoRA。</li>
<li>results: 实验结果表明，CCEdit 框架具有出色的功能和编辑能力。<details>
<summary>Abstract</summary>
In this work, we present CCEdit, a versatile framework designed to address the challenges of creative and controllable video editing. CCEdit accommodates a wide spectrum of user editing requirements and enables enhanced creative control through an innovative approach that decouples video structure and appearance. We leverage the foundational ControlNet architecture to preserve structural integrity, while seamlessly integrating adaptable temporal modules compatible with state-of-the-art personalization techniques for text-to-image generation, such as DreamBooth and LoRA.Furthermore, we introduce reference-conditioned video editing, empowering users to exercise precise creative control over video editing through the more manageable process of editing key frames. Our extensive experimental evaluations confirm the exceptional functionality and editing capabilities of the proposed CCEdit framework. Demo video is available at https://www.youtube.com/watch?v=UQw4jq-igN4.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍CCEdit框架，这是一种适应创作和控制视频编辑的多功能框架。CCEdit可以满足广泛的用户编辑需求，并提供了更高级别的创意控制通过解耦视频结构和外观的创新方法。我们利用ControlNet体系结构，以保持视频结构完整性，同时快速集成了适应性强的时间模块，与现代个性化文本生成技术，如梦镜和LoRA，协同工作。此外，我们还引入了参考条件视频编辑，让用户通过更加可控的逻辑框架进行视频编辑，从而提高了编辑效率和精度。我们的广泛实验证明了CCEdit框架的非凡功能和编辑能力。详细的示例视频可以在https://www.youtube.com/watch?v=UQw4jq-igN4中找到。
</details></li>
</ul>
<hr>
<h2 id="Deep-Single-Models-vs-Ensembles-Insights-for-a-Fast-Deployment-of-Parking-Monitoring-Systems"><a href="#Deep-Single-Models-vs-Ensembles-Insights-for-a-Fast-Deployment-of-Parking-Monitoring-Systems" class="headerlink" title="Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems"></a>Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16495">http://arxiv.org/abs/2309.16495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andre Gustavo Hochuli, Jean Paul Barddal, Gillian Cezar Palhano, Leonardo Matheus Mendes, Paulo Ricardo Lisboa de Almeida</li>
<li>for: 这个研究的目的是为了开发一个可以在高密度城市中搜寻可用的停车位置的系统，并且降低 drivers 搜寻停车位置的压力。</li>
<li>methods: 这个研究使用了图像基的系统，并且使用了深度学习技术来实现停车位置的识别。</li>
<li>results: 研究发现，使用不同的数据集和深度学习架构，包括融合策略和集成方法，可以实现95%的准确率，而不需要对目标停车场进行训练和标注。<details>
<summary>Abstract</summary>
Searching for available parking spots in high-density urban centers is a stressful task for drivers that can be mitigated by systems that know in advance the nearest parking space available.   To this end, image-based systems offer cost advantages over other sensor-based alternatives (e.g., ultrasonic sensors), requiring less physical infrastructure for installation and maintenance.   Despite recent deep learning advances, deploying intelligent parking monitoring is still a challenge since most approaches involve collecting and labeling large amounts of data, which is laborious and time-consuming. Our study aims to uncover the challenges in creating a global framework, trained using publicly available labeled parking lot images, that performs accurately across diverse scenarios, enabling the parking space monitoring as a ready-to-use system to deploy in a new environment. Through exhaustive experiments involving different datasets and deep learning architectures, including fusion strategies and ensemble methods, we found that models trained on diverse datasets can achieve 95\% accuracy without the burden of data annotation and model training on the target parking lot
</details>
<details>
<summary>摘要</summary>
搜寻高密度城市中的停车位是驾驶员忙碌的任务，可以通过系统知道当前最近的停车位。为此，图像基的系统提供成本优势，需要更少的物理基础设施安装和维护。尽管最近的深度学习突破，但是实施智能停车监测仍然是一个挑战，因为大多数方法需要收集和标注大量数据，这是时间consuming和劳动密集的。我们的研究旨在探讨在公共可用的标注停车场图像基础上创建全球框架，可以在多样化场景下准确地检测停车位，并提供一个Ready-to-use的系统，可以在新环境中部署。通过不同的数据集和深度学习架构、合并策略和 ensemble方法的 исследование，我们发现了：模型在多样化数据集上训练可以达到95%的准确率，无需目标停车场的数据注解和模型训练。
</details></li>
</ul>
<hr>
<h2 id="Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization"><a href="#Accurate-and-lightweight-dehazing-via-multi-receptive-field-non-local-network-and-novel-contrastive-regularization" class="headerlink" title="Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization"></a>Accurate and lightweight dehazing via multi-receptive-field non-local network and novel contrastive regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16494">http://arxiv.org/abs/2309.16494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zewei He, Zixuan Chen, Ziqian Lu, Xuecheng Sun, Zhe-Ming Lu</li>
<li>for: 提高雾光照权重抑制和细节表征的图像雾化模型</li>
<li>methods: 使用多感知场非本地网络（MRFNLN），包括多流特征注意块（MSFAB）和跨非本地块（CNLB），以提取更加丰富的特征，并通过注意力机制和跨非本地块来捕捉长距离关系</li>
<li>results: 提出了一种新的细节重点对比调整（DFCR），并通过对比调整和细节重点对比调整来提高图像雾化性能，并且模型具有少于1500万参数，超过当前状态的图像雾化方法<details>
<summary>Abstract</summary>
Recently, deep learning-based methods have dominated image dehazing domain. Although very competitive dehazing performance has been achieved with sophisticated models, effective solutions for extracting useful features are still under-explored. In addition, non-local network, which has made a breakthrough in many vision tasks, has not been appropriately applied to image dehazing. Thus, a multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream feature attention block (MSFAB) and cross non-local block (CNLB) is presented in this paper. We start with extracting richer features for dehazing. Specifically, we design a multi-stream feature extraction (MSFE) sub-block, which contains three parallel convolutions with different receptive fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$) for extracting multi-scale features. Following MSFE, we employ an attention sub-block to make the model adaptively focus on important channels/regions. The MSFE and attention sub-blocks constitute our MSFAB. Then, we design a cross non-local block (CNLB), which can capture long-range dependencies beyond the query. Instead of the same input source of query branch, the key and value branches are enhanced by fusing more preceding features. CNLB is computation-friendly by leveraging a spatial pyramid down-sampling (SPDS) strategy to reduce the computation and memory consumption without sacrificing the performance. Last but not least, a novel detail-focused contrastive regularization (DFCR) is presented by emphasizing the low-level details and ignoring the high-level semantic information in the representation space. Comprehensive experimental results demonstrate that the proposed MRFNLN model outperforms recent state-of-the-art dehazing methods with less than 1.5 Million parameters.
</details>
<details>
<summary>摘要</summary>
近期，深度学习基于方法在图像抑雾领域占据了主导地位。虽然使用了复杂的模型，但是对于提取有用特征的有效解决方案还是尚未得到足够的探索。此外，非本地网络，在视觉任务中创造出了突破，尚未被适当应用于图像抑雾。因此，本文提出了一种多感受场非本地网络（MRFNLN），其包括多流处理特征吸引块（MSFAB）和跨非本地块（CNLB）。我们从提取更丰富的特征开始，specifically，我们设计了一种多流特征提取子块（MSFE），它包括三个并行的三维卷积（$1\times 1$, $3\times 3$, $5\times 5$），用于提取多级特征。接着，我们采用了一个注意力吸引子块，使模型能够适应重要的通道/区域。MSFE和注意力吸引子块组成我们的 MSFAB。然后，我们设计了一种跨非本地块（CNLB），它可以捕捉更远的相关性，而不是仅仅依靠输入的查询源。相比之下，关键和值分支可以通过折衔更多的先前特征进行增强。CNLB通过利用空间PYRAMID下降抽象（SPDS）策略来减少计算和存储占用，不会失去性能。最后，我们提出了一种新的细节重点对比常规化正则化（DFCR），强调低级详细信息，忽略高级 semantic信息在表示空间中。我们进行了广泛的实验研究，结果表明，我们提出的 MRFNLN 模型在参数数量不足 1.5 万的情况下，已经超越了最新的图像抑雾方法。
</details></li>
</ul>
<hr>
<h2 id="HTC-DC-Net-Monocular-Height-Estimation-from-Single-Remote-Sensing-Images"><a href="#HTC-DC-Net-Monocular-Height-Estimation-from-Single-Remote-Sensing-Images" class="headerlink" title="HTC-DC Net: Monocular Height Estimation from Single Remote Sensing Images"></a>HTC-DC Net: Monocular Height Estimation from Single Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16486">http://arxiv.org/abs/2309.16486</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhu-xlab/htc-dc-net">https://github.com/zhu-xlab/htc-dc-net</a></li>
<li>paper_authors: Sining Chen, Yilei Shi, Zhitong Xiong, Xiao Xiang Zhu</li>
<li>for: 这篇论文的目的是提出一种用于单目高程估计的方法，以解决基于远程感知数据的3D地理信息的问题。</li>
<li>methods: 该方法基于分类-回归模式，包括特点EXTRACTOR、HTC-AdaBins模块和混合回归过程。HTC-AdaBins模块用视transformer编码器并HTC来 Address long-tailed问题，并且涉及DCs来训练。</li>
<li>results: 实验结果显示，提议的网络在三个不同分辨率的数据集上表现出色，与现有方法相比，具有大量的优势。广泛的折衔研究也证明了每个设计元素的效果。<details>
<summary>Abstract</summary>
3D geo-information is of great significance for understanding the living environment; however, 3D perception from remote sensing data, especially on a large scale, is restricted. To tackle this problem, we propose a method for monocular height estimation from optical imagery, which is currently one of the richest sources of remote sensing data. As an ill-posed problem, monocular height estimation requires well-designed networks for enhanced representations to improve performance. Moreover, the distribution of height values is long-tailed with the low-height pixels, e.g., the background, as the head, and thus trained networks are usually biased and tend to underestimate building heights. To solve the problems, instead of formalizing the problem as a regression task, we propose HTC-DC Net following the classification-regression paradigm, with the head-tail cut (HTC) and the distribution-based constraints (DCs) as the main contributions. HTC-DC Net is composed of the backbone network as the feature extractor, the HTC-AdaBins module, and the hybrid regression process. The HTC-AdaBins module serves as the classification phase to determine bins adaptive to each input image. It is equipped with a vision transformer encoder to incorporate local context with holistic information and involves an HTC to address the long-tailed problem in monocular height estimation for balancing the performances of foreground and background pixels. The hybrid regression process does the regression via the smoothing of bins from the classification phase, which is trained via DCs. The proposed network is tested on three datasets of different resolutions, namely ISPRS Vaihingen (0.09 m), DFC19 (1.3 m) and GBH (3 m). Experimental results show the superiority of the proposed network over existing methods by large margins. Extensive ablation studies demonstrate the effectiveness of each design component.
</details>
<details>
<summary>摘要</summary>
三维地理信息对生活环境理解具有重要 significancem however, 从远程感知数据中获得三维高度的见解，尤其是在大规模上，受到限制。为解决这个问题，我们提出了一种从光学影像获得高度的独眼准备方法，现在是远程感知数据中最丰富的资源之一。由于这是一个不定Problem，高度估计需要良好的网络设计来提高性能。此外，高度值的分布呈长尾，低高度像素（如背景）为主，因此训练的网络通常偏 towards underestimating building heights。为了解决这些问题，我们不是直接将问题定义为回归任务，而是提出了 HTC-DC Net，它是基于分类-回归模式的。 HTC-DC Net 由 feature extractor 作为 backbone network，HTC-AdaBins 模块，和混合回归过程组成。HTC-AdaBins 模块 serves as the classification phase to determine adaptive bins for each input image，它使用了视transformer encoder integrate local context with holistic information，并在 HTC 中处理长尾问题。混合回归过程通过 adapted bins from the classification phase 进行回归，并由 DCs 训练。我们在三个不同分辨率的 dataset 上进行测试，namely ISPRS Vaihingen (0.09 m), DFC19 (1.3 m) and GBH (3 m)。实验结果表明我们的方法在现有方法的大幅提高。我们还进行了广泛的拟合研究，以证明每个设计元件的效果。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Domain-Generalization-Discriminability-and-Generalizability"><a href="#Rethinking-Domain-Generalization-Discriminability-and-Generalizability" class="headerlink" title="Rethinking Domain Generalization: Discriminability and Generalizability"></a>Rethinking Domain Generalization: Discriminability and Generalizability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16483">http://arxiv.org/abs/2309.16483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaocong Long, Qianyu Zhou, Chenhao Ying, Lizhuang Ma, Yuan Luo</li>
<li>for: 本研究旨在开发一种能同时具备强大的特征泛化和精准的分类能力的领域总结（Domain Generalization，DG）方法。</li>
<li>methods: 本方法基于两个核心 ком成分：选择性频道剔除（Selective Channel Pruning，SCP）和微级分布对齐（Micro-level Distribution Alignment，MDA）。SCP 通过减少神经网络中的冗余特征，增强特征的稳定性和分类精度。而 MDA 强调每个类别内的微级分布对齐，以便保留足够的总体特征和细化分类。</li>
<li>results: 在四个 benchmark 数据集上进行了广泛的实验，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
Domain generalization (DG) endeavors to develop robust models that possess strong generalizability while preserving excellent discriminability. Nonetheless, pivotal DG techniques tend to improve the feature generalizability by learning domain-invariant representations, inadvertently overlooking the feature discriminability. On the one hand, the simultaneous attainment of generalizability and discriminability of features presents a complex challenge, often entailing inherent contradictions. This challenge becomes particularly pronounced when domain-invariant features manifest reduced discriminability owing to the inclusion of unstable factors, \emph{i.e.,} spurious correlations. On the other hand, prevailing domain-invariant methods can be categorized as category-level alignment, susceptible to discarding indispensable features possessing substantial generalizability and narrowing intra-class variations. To surmount these obstacles, we rethink DG from a new perspective that concurrently imbues features with formidable discriminability and robust generalizability, and present a novel framework, namely, Discriminative Microscopic Distribution Alignment (DMDA). DMDA incorporates two core components: Selective Channel Pruning~(SCP) and Micro-level Distribution Alignment (MDA). Concretely, SCP attempts to curtail redundancy within neural networks, prioritizing stable attributes conducive to accurate classification. This approach alleviates the adverse effect of spurious domain invariance and amplifies the feature discriminability. Besides, MDA accentuates micro-level alignment within each class, going beyond mere category-level alignment. This strategy accommodates sufficient generalizable features and facilitates within-class variations. Extensive experiments on four benchmark datasets corroborate the efficacy of our method.
</details>
<details>
<summary>摘要</summary>
领域通用化（DG）努力开发强健的模型，以保持优秀的泛化能力和精准可识别能力。然而，许多领域通用化技术通常通过学习领域不变的表示来提高特征泛化能力，不幸的是，这会忽略特征精准可识别能力。一方面，同时实现特征泛化和精准可识别的特征表示存在复杂的挑战，经常带有内在的矛盾。尤其是当领域不变的特征表示具有不稳定因素，即偶极相关性，时这种挑战变得更加突出。另一方面，现有的领域不变方法可以分为两类：分类水平协调和特征水平协调。前者容易抛弃重要的泛化特征，导致内部变化减少，而后者忽略了特征精准可识别能力。为了缓解这些障碍，我们往返领域通用化的新视角，并提出了一种新的框架，即精准微型分布适应（DMDA）。DMDA包括两个核心 ком成分：选择性通道剔除（SCP）和微级分布适应（MDA）。具体来说，SCP尝试减少神经网络中的重复性，优先保留稳定特征，以便精准分类。这种方法可以减少领域不变的副作用，提高特征精准可识别能力。此外，MDA强调每个类型内的微级协调，超越仅category水平协调。这种策略可以保留足够的泛化特征，促进内部变化。我们在四个标准 benchmark 数据集上进行了广泛的实验，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Target-and-Contribution-Scheduling-for-Domain-Generalization"><a href="#Diverse-Target-and-Contribution-Scheduling-for-Domain-Generalization" class="headerlink" title="Diverse Target and Contribution Scheduling for Domain Generalization"></a>Diverse Target and Contribution Scheduling for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16460">http://arxiv.org/abs/2309.16460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaocong Long, Qianyu Zhou, Chenhao Ying, Lizhuang Ma, Yuan Luo</li>
<li>for: 这篇论文主要针对的是在分布偏移下进行计算机视觉领域的一致化问题，即在不同预测器上进行训练时，能够快速地适应新的预测任务。</li>
<li>methods: 本文提出了一种新的概念，即多源预测器的多目标抽象，以及一种基于这种概念的新方法，即多源预测器的多目标均衡。这种方法可以在不同预测器上进行训练，并且可以快速地适应新的预测任务。</li>
<li>results:  experiments 表明，这种方法可以在四个 benchmark 数据集上实现竞争性的性能，并且可以快速地适应新的预测任务。这说明了本文提出的方法的有效性和优势。<details>
<summary>Abstract</summary>
Generalization under the distribution shift has been a great challenge in computer vision. The prevailing practice of directly employing the one-hot labels as the training targets in domain generalization~(DG) can lead to gradient conflicts, making it insufficient for capturing the intrinsic class characteristics and hard to increase the intra-class variation. Besides, existing methods in DG mostly overlook the distinct contributions of source (seen) domains, resulting in uneven learning from these domains. To address these issues, we firstly present a theoretical and empirical analysis of the existence of gradient conflicts in DG, unveiling the previously unexplored relationship between distribution shifts and gradient conflicts during the optimization process. In this paper, we present a novel perspective of DG from the empirical source domain's risk and propose a new paradigm for DG called Diverse Target and Contribution Scheduling (DTCS). DTCS comprises two innovative modules: Diverse Target Supervision (DTS) and Diverse Contribution Balance (DCB), with the aim of addressing the limitations associated with the common utilization of one-hot labels and equal contributions for source domains in DG. In specific, DTS employs distinct soft labels as training targets to account for various feature distributions across domains and thereby mitigates the gradient conflicts, and DCB dynamically balances the contributions of source domains by ensuring a fair decline in losses of different source domains. Extensive experiments with analysis on four benchmark datasets show that the proposed method achieves a competitive performance in comparison with the state-of-the-art approaches, demonstrating the effectiveness and advantages of the proposed DTCS.
</details>
<details>
<summary>摘要</summary>
通用化在分布转移下是计算机视觉领域的一大挑战。直接使用领域总体化（DG）中的一颗热度标签作为训练目标，可能会导致梯度冲突，从而使得 capture 内部类特征和增加同类内部差异困难。此外，现有的DG方法大多忽视了来自源（已知）领域的特点，从而导致不均衡学习这些领域。为解决这些问题，我们首先提供了分布转移和梯度冲突在DG中的理论和实验分析，揭示了在优化过程中的 previously 未探讨的关系。在这篇论文中，我们提出了一新的DG视角，即来自 empirical 源领域的风险，并提出了一种新的DG方法 called 多样化目标和贡献安排（DTCS）。DTCS包括两个创新模块：多样化目标监督（DTS）和多样化贡献平衡（DCB），旨在解决DG中一般采用一颗热度标签和平等贡献的局限性。具体来说，DTS 使用不同的软标签作为训练目标，以 compte 各个领域的特性分布，从而缓解梯度冲突，而 DCB 在不同的源领域之间动态均衡贡献，以确保不同的源领域的损失下降均衡。我们对四个基准数据集进行了广泛的实验和分析，结果表明，我们提出的方法可以与当前领先方法竞争， demonstrating 我们的DTCS方法的有效性和优势。
</details></li>
</ul>
<hr>
<h2 id="Towards-Novel-Class-Discovery-A-Study-in-Novel-Skin-Lesions-Clustering"><a href="#Towards-Novel-Class-Discovery-A-Study-in-Novel-Skin-Lesions-Clustering" class="headerlink" title="Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering"></a>Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16451">http://arxiv.org/abs/2309.16451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Feng, Lie Ju, Lin Wang, Kaimin Song, Zongyuan Ge</li>
<li>for: automatically discover and identify new semantic categories from new data</li>
<li>methods: contrastive learning + uncertainty-aware multi-view cross pseudo-supervision strategy + local sample similarity aggregation</li>
<li>results: effectively leverage knowledge from known categories to discover new semantic categories, validated through extensive ablation experiments.<details>
<summary>Abstract</summary>
Existing deep learning models have achieved promising performance in recognizing skin diseases from dermoscopic images. However, these models can only recognize samples from predefined categories, when they are deployed in the clinic, data from new unknown categories are constantly emerging. Therefore, it is crucial to automatically discover and identify new semantic categories from new data. In this paper, we propose a new novel class discovery framework for automatically discovering new semantic classes from dermoscopy image datasets based on the knowledge of known classes. Specifically, we first use contrastive learning to learn a robust and unbiased feature representation based on all data from known and unknown categories. We then propose an uncertainty-aware multi-view cross pseudo-supervision strategy, which is trained jointly on all categories of data using pseudo labels generated by a self-labeling strategy. Finally, we further refine the pseudo label by aggregating neighborhood information through local sample similarity to improve the clustering performance of the model for unknown categories. We conducted extensive experiments on the dermatology dataset ISIC 2019, and the experimental results show that our approach can effectively leverage knowledge from known categories to discover new semantic categories. We also further validated the effectiveness of the different modules through extensive ablation experiments. Our code will be released soon.
</details>
<details>
<summary>摘要</summary>
现有的深度学习模型已经在诊断皮肤病的dermoscopic图像上达到了成功的表现。然而，这些模型只能识别预先定义的类别，当它们在临床中使用时，新的未知类别的数据会不断出现。因此，自动地发现和识别新的semantic类别是急需的。在这篇论文中，我们提出了一种新的novel class discovery框架，用于自动地发现dermoscopy图像集中的新类别。具体来说，我们首先使用对所有数据进行对比学习，以学习不偏袋性的特征表示。然后，我们提出了一种不确定性感知多视图 Pseudo-supervision策略，通过对所有类别的数据进行联合训练，使用自己生成的Pseudo标签进行训练。最后，我们进一步改进了Pseudo标签，通过地方sample相似性来提高模型对未知类别的减混表现。我们对ISIC 2019皮肤病 dataset进行了广泛的实验，并证明了我们的方法可以有效地利用已知类别的知识来发现新的semantic类别。我们还进行了extensive的ablation experiment，以验证不同模块的效果。我们的代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="Radar-Instance-Transformer-Reliable-Moving-Instance-Segmentation-in-Sparse-Radar-Point-Clouds"><a href="#Radar-Instance-Transformer-Reliable-Moving-Instance-Segmentation-in-Sparse-Radar-Point-Clouds" class="headerlink" title="Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds"></a>Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16435">http://arxiv.org/abs/2309.16435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthias Zeller, Vardeep S. Sandhu, Benedikt Mersch, Jens Behley, Michael Heidingsfeld, Cyrill Stachniss</li>
<li>for: 提高自动化机器人在动态环境中避免碰撞的能力，增强Scene解释。</li>
<li>methods: 利用LiDAR和摄像头对场景进行解释，但这些设备受到不良天气的限制，而雷达传感器可以超越这些限制，提供Doppler速度信息，直接提供动态对象的信息。</li>
<li>results: 提出一种基于雷达点云的移动实例分割方法，可以增强Scene解释，并且可以在安全关键任务中提高自动化机器人的性能。<details>
<summary>Abstract</summary>
The perception of moving objects is crucial for autonomous robots performing collision avoidance in dynamic environments. LiDARs and cameras tremendously enhance scene interpretation but do not provide direct motion information and face limitations under adverse weather. Radar sensors overcome these limitations and provide Doppler velocities, delivering direct information on dynamic objects. In this paper, we address the problem of moving instance segmentation in radar point clouds to enhance scene interpretation for safety-critical tasks. Our Radar Instance Transformer enriches the current radar scan with temporal information without passing aggregated scans through a neural network. We propose a full-resolution backbone to prevent information loss in sparse point cloud processing. Our instance transformer head incorporates essential information to enhance segmentation but also enables reliable, class-agnostic instance assignments. In sum, our approach shows superior performance on the new moving instance segmentation benchmarks, including diverse environments, and provides model-agnostic modules to enhance scene interpretation. The benchmark is based on the RadarScenes dataset and will be made available upon acceptance.
</details>
<details>
<summary>摘要</summary>
<<SYS>> autonomous robots 需要准确地感知移动 объекts，以确保在动态环境中避免碰撞。 LiDAR 和摄像头可以很好地帮助解释场景，但是它们不直接提供动态信息和在不良天气情况下存在限制。 Radar 感知器可以突破这些限制，提供Doppler 速度，直接提供动态对象的信息。在这篇论文中，我们解决了使用 Radar 点云中的移动实例分割问题，以提高场景的解释，为安全关键任务做好准备。我们的 Radar Instance Transformer 可以在不经过神经网络的情况下，将现场时间信息纳入 Radar 扫描中，以提高分割的精度。我们的实例转换头可以具有类型不易分的实例分割，并且可以提供可靠的实例分割结果。综上所述，我们的方法在新的移动实例分割标准测试中表现出色，包括多种环境，并提供了模型无关的模块，以提高场景的解释。这个标准基于 RadarScenes 数据集，并将在接受后公布。
</details></li>
</ul>
<hr>
<h2 id="Distilling-ODE-Solvers-of-Diffusion-Models-into-Smaller-Steps"><a href="#Distilling-ODE-Solvers-of-Diffusion-Models-into-Smaller-Steps" class="headerlink" title="Distilling ODE Solvers of Diffusion Models into Smaller Steps"></a>Distilling ODE Solvers of Diffusion Models into Smaller Steps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16421">http://arxiv.org/abs/2309.16421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanghwan Kim, Hao Tang, Fisher Yu</li>
<li>for: 提高 diffusion 模型的采样速度</li>
<li>methods: 使用简单的 distillation 方法优化 ODE 解决方案</li>
<li>results: 比较 existing ODE 解决方案的性能，特别是在生成样本 fewer steps 时的表现更佳，并且具有较低的计算开销。<details>
<summary>Abstract</summary>
Distillation techniques have substantially improved the sampling speed of diffusion models, allowing of the generation within only one step or a few steps. However, these distillation methods require extensive training for each dataset, sampler, and network, which limits their practical applicability. To address this limitation, we propose a straightforward distillation approach, Distilled-ODE solvers (D-ODE solvers), that optimizes the ODE solver rather than training the denoising network. D-ODE solvers are formulated by simply applying a single parameter adjustment to existing ODE solvers. Subsequently, D-ODE solvers with smaller steps are optimized by ODE solvers with larger steps through distillation over a batch of samples. Our comprehensive experiments indicate that D-ODE solvers outperform existing ODE solvers, including DDIM, PNDM, DPM-Solver, DEIS, and EDM, especially when generating samples with fewer steps. Our method incur negligible computational overhead compared to previous distillation techniques, enabling simple and rapid integration with previous samplers. Qualitative analysis further shows that D-ODE solvers enhance image quality while preserving the sampling trajectory of ODE solvers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化中文。<</SYS>>diffusion模型的抽象技术已经大幅提高了采样速度，允许在单步或几步内生成样本。然而，这些抽象方法需要对每个数据集、抽象器和网络进行训练，这限制了它们的实际应用性。为解决这些限制，我们提出了一种简单的抽象方法，即Distilled-ODE solvers（D-ODE solvers），它通过对ODE解决器进行优化而不需要训练杜尔凡抽象网络。D-ODE solvers通过对现有ODE解决器进行单个参数调整，并通过对具有更大步长的ODE解决器进行析取采样来优化小步长ODE解决器。我们的全面实验表明，D-ODE solvers在生成样本时比现有的ODE解决器、包括DDIM、PNDM、DPM-Solver、DEIS和EDM更高效，特别是在生成 fewer steps 的样本时。我们的方法相比前一个分布采样技术具有较低的计算开销，使得可以简单地和快速地与现有的采样器结合。Qualitative分析还表明，D-ODE solvers可以提高图像质量，同时保持ODE解决器的采样轨迹。
</details></li>
</ul>
<hr>
<h2 id="HIC-YOLOv5-Improved-YOLOv5-For-Small-Object-Detection"><a href="#HIC-YOLOv5-Improved-YOLOv5-For-Small-Object-Detection" class="headerlink" title="HIC-YOLOv5: Improved YOLOv5 For Small Object Detection"></a>HIC-YOLOv5: Improved YOLOv5 For Small Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16393">http://arxiv.org/abs/2309.16393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jacoo-ai/HIC-Yolov5">https://github.com/Jacoo-ai/HIC-Yolov5</a></li>
<li>paper_authors: Shiyi Tang, Yini Fang, Shu Zhang</li>
<li>for: 提高小对象检测精度和速度， Addressing the challenges of small object detection in object detection tasks.</li>
<li>methods: 增加特定于小对象的预测头，采用卷积层和涨层，并应用Channel Attention Mechanism (CBAM) to increase channel information and emphasize important information in both channel and spatial domain.</li>
<li>results: 在VisDrone-2019-DET数据集上，HIC-YOLOv5的mAP@[.5:.95]提高6.42%，<a href="mailto:&#109;&#x41;&#x50;&#64;&#x30;&#x2e;&#53;">&#109;&#x41;&#x50;&#64;&#x30;&#x2e;&#53;</a>提高9.38%.<details>
<summary>Abstract</summary>
Small object detection has been a challenging problem in the field of object detection. There has been some works that proposes improvements for this task, such as adding several attention blocks or changing the whole structure of feature fusion networks. However, the computation cost of these models is large, which makes deploying a real-time object detection system unfeasible, while leaving room for improvement. To this end, an improved YOLOv5 model: HIC-YOLOv5 is proposed to address the aforementioned problems. Firstly, an additional prediction head specific to small objects is added to provide a higher-resolution feature map for better prediction. Secondly, an involution block is adopted between the backbone and neck to increase channel information of the feature map. Moreover, an attention mechanism named CBAM is applied at the end of the backbone, thus not only decreasing the computation cost compared with previous works but also emphasizing the important information in both channel and spatial domain. Our result shows that HIC-YOLOv5 has improved mAP@[.5:.95] by 6.42% and mAP@0.5 by 9.38% on VisDrone-2019-DET dataset.
</details>
<details>
<summary>摘要</summary>
小物体检测问题在物体检测领域中是一个挑战。有些工作提出了改进方案，如添加多个注意块或者修改特征融合网络的结构。然而，这些模型的计算成本较大，使得实时物体检测系统无法实现，剩下有很多可改进的空间。为此，一个改进的YOLOv5模型：HIC-YOLOv5被提出来解决这些问题。首先，增加了专门用于小物体预测的预测头，以提供更高分辨率的特征图用于更好的预测。其次，在 neck 和 backbone 之间采用了卷积块，以增加特征图的通道信息。此外，在 backbone 的末端应用了一个注意机制 named CBAM，以降低计算成本与之前的工作相比，同时强调通道和空间领域中的重要信息。我们的结果显示，HIC-YOLOv5 在 VisDrone-2019-DET 数据集上提高了 mAP@[.5:.95] 和 mAP@0.5 的值，分别提高了6.42%和9.38%。
</details></li>
</ul>
<hr>
<h2 id="An-Enhanced-Low-Resolution-Image-Recognition-Method-for-Traffic-Environments"><a href="#An-Enhanced-Low-Resolution-Image-Recognition-Method-for-Traffic-Environments" class="headerlink" title="An Enhanced Low-Resolution Image Recognition Method for Traffic Environments"></a>An Enhanced Low-Resolution Image Recognition Method for Traffic Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16390">http://arxiv.org/abs/2309.16390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongcai Tan, Zhenhai Gao</li>
<li>for: 提高低分辨率图像识别精度</li>
<li>methods: 基于差分模块和共享特征空间算法，提出双支路差分网络结构，并使用中间层特征进行增强低分辨率图像识别精度。</li>
<li>results: 通过实验 Validate the effectiveness of this algorithm for low-resolution image recognition in traffic environments.<details>
<summary>Abstract</summary>
Currently, low-resolution image recognition is confronted with a significant challenge in the field of intelligent traffic perception. Compared to high-resolution images, low-resolution images suffer from small size, low quality, and lack of detail, leading to a notable decrease in the accuracy of traditional neural network recognition algorithms. The key to low-resolution image recognition lies in effective feature extraction. Therefore, this paper delves into the fundamental dimensions of residual modules and their impact on feature extraction and computational efficiency. Based on experiments, we introduce a dual-branch residual network structure that leverages the basic architecture of residual networks and a common feature subspace algorithm. Additionally, it incorporates the utilization of intermediate-layer features to enhance the accuracy of low-resolution image recognition. Furthermore, we employ knowledge distillation to reduce network parameters and computational overhead. Experimental results validate the effectiveness of this algorithm for low-resolution image recognition in traffic environments.
</details>
<details>
<summary>摘要</summary>
当前，低分辨率图像识别遇到了智能交通感知领域中的一个 significiant 挑战。相比高分辨率图像，低分辨率图像受到小尺寸、低质量和缺乏细节的限制，导致传统神经网络识别算法的准确率显著下降。因此，关键在于有效地提取特征。这篇论文探讨了剩余模块的基本维度和其对特征提取和计算效率的影响。基于实验，我们提出了一种双极分支剩余网络结构，利用基本的剩余网络架构和公共特征空间算法。此外，我们还利用中间层特征来提高低分辨率图像识别的准确率。此外，我们采用知识传承来降低网络参数和计算负担。实验结果证明了这种算法的有效性于低分辨率图像识别在交通环境中。
</details></li>
</ul>
<hr>
<h2 id="Biomedical-Image-Splicing-Detection-using-Uncertainty-Guided-Refinement"><a href="#Biomedical-Image-Splicing-Detection-using-Uncertainty-Guided-Refinement" class="headerlink" title="Biomedical Image Splicing Detection using Uncertainty-Guided Refinement"></a>Biomedical Image Splicing Detection using Uncertainty-Guided Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16388">http://arxiv.org/abs/2309.16388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xun Lin, Wenzhong Tang, Shuai Wang, Zitong Yu, Yizhong Liu, Haoran Wang, Ying Fu, Alex Kot</li>
<li>for: 本文旨在提出一种能够 Mitigating the effects of disruptive factors in biomedical images, such as artifacts, abnormal patterns, and noises, and detecting splicing traces in biomedical images.</li>
<li>methods: 本文提出了一种基于 Uncertainty-guided Refinement Network (URN) 的方法，可以显著地减少不可靠信息的传递，从而获得robust特征。此外，URN 还可以在解码阶段对不确定预测的区域进行精度的调整。</li>
<li>results: 对三个基准数据集进行了广泛的实验，并证明了提出的方法的superiority。此外，我们还验证了URN 的普适性和对Post-processing Approaches的Robustness。<details>
<summary>Abstract</summary>
Recently, a surge in biomedical academic publications suspected of image manipulation has led to numerous retractions, turning biomedical image forensics into a research hotspot. While manipulation detectors are concerning, the specific detection of splicing traces in biomedical images remains underexplored. The disruptive factors within biomedical images, such as artifacts, abnormal patterns, and noises, show misleading features like the splicing traces, greatly increasing the challenge for this task. Moreover, the scarcity of high-quality spliced biomedical images also limits potential advancements in this field. In this work, we propose an Uncertainty-guided Refinement Network (URN) to mitigate the effects of these disruptive factors. Our URN can explicitly suppress the propagation of unreliable information flow caused by disruptive factors among regions, thereby obtaining robust features. Moreover, URN enables a concentration on the refinement of uncertainly predicted regions during the decoding phase. Besides, we construct a dataset for Biomedical image Splicing (BioSp) detection, which consists of 1,290 spliced images. Compared with existing datasets, BioSp comprises the largest number of spliced images and the most diverse sources. Comprehensive experiments on three benchmark datasets demonstrate the superiority of the proposed method. Meanwhile, we verify the generalizability of URN when against cross-dataset domain shifts and its robustness to resist post-processing approaches. Our BioSp dataset will be released upon acceptance.
</details>
<details>
<summary>摘要</summary>
近些时间，生物医学图像涂抹的学术论文涌现，引起了许多撤回，使生物医学图像鉴别成为研究热点。然而，图像涂抹检测器在生物医学图像中仍然存在困难。生物医学图像中的干扰因素，如artifacts、异常模式和噪声，会显示涂抹迹象，大大增加了这个任务的挑战。此外，生物医学图像的缺乏高质量拼接图像也限制了这个领域的进展。在这项工作中，我们提出了一种基于不确定性的修正网络（URN），以减少干扰因素的影响。URN可以显式地抑制干扰因素在区域之间的不确定信息流传播，从而获得robust特征。此外，URN在解码阶段可以集中做不确定预测区域的修正。此外，我们构建了一个生物医学图像拼接检测（BioSp）数据集，该数据集包含1290个拼接图像。与现有数据集相比，BioSp数据集包括最多的拼接图像和最多的来源。我们对三个标准数据集进行了全面的实验，并证明了我们提出的方法的优越性。同时，我们验证了URN在跨数据集频率域转移和抗后处理approaches的一致性和可靠性。我们将发布BioSp数据集一旦得到批准。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-on-Tree-Detection-Methods-Using-Point-Cloud-and-Aerial-Imagery-from-Unmanned-Aerial-Vehicles"><a href="#A-Comprehensive-Review-on-Tree-Detection-Methods-Using-Point-Cloud-and-Aerial-Imagery-from-Unmanned-Aerial-Vehicles" class="headerlink" title="A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles"></a>A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16375">http://arxiv.org/abs/2309.16375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Kuang, Hann Woei Ho, Ye Zhou, Shahrel Azmin Suandi, Farzad Ismail<br>for: 这篇论文主要针对用于树木探测的无人机数据中的点云数据和图像数据进行了分析和评估。methods: 本论文主要分析了使用点云数据进行树木探测的方法，包括使用LiDAR和Digital Aerial Photography（DAP）两种数据来实现树木探测。而使用图像直接进行树木探测的方法则是根据使用深度学习（DL）方法来进行评估。results: 本论文对各种方法的比较和结合进行了分析，并介绍了每种方法的优缺点和应用领域。此外，本论文还统计了在过去几年内使用不同方法进行树木探测的研究数量，并发现到2022年为止，使用DL方法进行图像直接树木探测的研究占总数的45%。因此，本论文可以帮助研究人员在特定的森林中进行树木探测，以及帮助农业生产者使用无人机进行农业资源管理。<details>
<summary>Abstract</summary>
Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with highly cost-effective and flexible usage scenarios. Although many papers have reviewed the application of UAVs in agriculture, the review of the application for tree detection is still insufficient. This paper focuses on tree detection methods applied to UAV data collected by UAVs. There are two kinds of data, the point cloud and the images, which are acquired by the Light Detection and Ranging (LiDAR) sensor and camera, respectively. Among the detection methods using point-cloud data, this paper mainly classifies these methods according to LiDAR and Digital Aerial Photography (DAP). For the detection methods using images directly, this paper reviews these methods by whether or not to use the Deep Learning (DL) method. Our review concludes and analyses the comparison and combination between the application of LiDAR-based and DAP-based point cloud data. The performance, relative merits, and application fields of the methods are also introduced. Meanwhile, this review counts the number of tree detection studies using different methods in recent years. From our statics, the detection task using DL methods on the image has become a mainstream trend as the number of DL-based detection researches increases to 45% of the total number of tree detection studies up to 2022. As a result, this review could help and guide researchers who want to carry out tree detection on specific forests and for farmers to use UAVs in managing agriculture production.
</details>
<details>
<summary>摘要</summary>
无人飞行器（UAV）技术被视为当今最先进和最有效的应用领域之一，其应用场景非常多样化。虽然许多论文已经评估了UAV在农业中的应用，但对树木探测的评估仍然不够。这篇论文将关注UAV数据中的树木探测方法。该数据包括点云数据和图像数据，它们分别由激光探测器和摄像头获取。对点云数据进行探测方法，本论文主要分为利用LiDAR和数字空间图像（DAP）两种方法进行分类。对直接使用图像进行探测方法，本论文则根据使用深度学习（DL）方法进行评估。本评估结论和分析了利用LiDAR和DAP两种点云数据的比较和结合，并 introduce了方法的性能、优势和应用领域。同时，本评估还统计了过去几年内tree detection研究中使用不同方法的数量，从而可以帮助研究人员在特定的森林中进行树木探测，以及帮助农民使用UAV进行农业生产管理。
</details></li>
</ul>
<hr>
<h2 id="FG-NeRF-Flow-GAN-based-Probabilistic-Neural-Radiance-Field-for-Independence-Assumption-Free-Uncertainty-Estimation"><a href="#FG-NeRF-Flow-GAN-based-Probabilistic-Neural-Radiance-Field-for-Independence-Assumption-Free-Uncertainty-Estimation" class="headerlink" title="FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for Independence-Assumption-Free Uncertainty Estimation"></a>FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for Independence-Assumption-Free Uncertainty Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16364">http://arxiv.org/abs/2309.16364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songlin Wei, Jiazhao Zhang, Yang Wang, Fanbo Xiang, Hao Su, He Wang</li>
<li>for: 这个研究旨在创建一个独立假设无关的神经辐射场，以便获取更好的可信度和精确性。</li>
<li>methods: 我们提出了一种基于Flow-GAN的独立假设无关的 probabilistic NeRF，通过结合对抗学习和对抗演算的具有创新能力和强大表达能力的正规流。</li>
<li>results: 我们通过实验证明了我们的方法可以预测更低的渲染错误和更可靠的不确定性，并且在实验中表现出了独立假设无关的神经辐射场的优秀性能。<details>
<summary>Abstract</summary>
Neural radiance fields with stochasticity have garnered significant interest by enabling the sampling of plausible radiance fields and quantifying uncertainty for downstream tasks. Existing works rely on the independence assumption of points in the radiance field or the pixels in input views to obtain tractable forms of the probability density function. However, this assumption inadvertently impacts performance when dealing with intricate geometry and texture. In this work, we propose an independence-assumption-free probabilistic neural radiance field based on Flow-GAN. By combining the generative capability of adversarial learning and the powerful expressivity of normalizing flow, our method explicitly models the density-radiance distribution of the whole scene. We represent our probabilistic NeRF as a mean-shifted probabilistic residual neural model. Our model is trained without an explicit likelihood function, thereby avoiding the independence assumption. Specifically, We downsample the training images with different strides and centers to form fixed-size patches which are used to train the generator with patch-based adversarial learning. Through extensive experiments, our method demonstrates state-of-the-art performance by predicting lower rendering errors and more reliable uncertainty on both synthetic and real-world datasets.
</details>
<details>
<summary>摘要</summary>
In this work, we propose an independence-assumption-free probabilistic neural radiance field based on Flow-GAN. By combining the generative capability of adversarial learning and the powerful expressivity of normalizing flow, our method explicitly models the density-radiance distribution of the whole scene. We represent our probabilistic NeRF as a mean-shifted probabilistic residual neural model. Our model is trained without an explicit likelihood function, thereby avoiding the independence assumption.Specifically, we downsample the training images with different strides and centers to form fixed-size patches, which are used to train the generator with patch-based adversarial learning. Through extensive experiments, our method demonstrates state-of-the-art performance by predicting lower rendering errors and more reliable uncertainty on both synthetic and real-world datasets.
</details></li>
</ul>
<hr>
<h2 id="Dark-Side-Augmentation-Generating-Diverse-Night-Examples-for-Metric-Learning"><a href="#Dark-Side-Augmentation-Generating-Diverse-Night-Examples-for-Metric-Learning" class="headerlink" title="Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning"></a>Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16351">http://arxiv.org/abs/2309.16351</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mohwald/gandtr">https://github.com/mohwald/gandtr</a></li>
<li>paper_authors: Albert Mohwald, Tomas Jenicek, Ondřej Chum</li>
<li>for: 提高夜间图像检索性能， addresses the challenge of poor retrieval performance in night-time images with limited training data.</li>
<li>methods: 使用 Generative Adversarial Networks (GANs) 生成 synthetic night images, 并将其用于 metric learning 中的数据增强。 提出了一种新的轻量级 GAN 架构， 同时具有对照图像的边 consistency 和同时具有夜夜和白天图像的边检测能力。</li>
<li>results: 在标准的 Tokyo 24&#x2F;7 日夜检索 benchmark 上达到了state-of-the-art Results, 而不需要特定的日夜图像对应的训练集。<details>
<summary>Abstract</summary>
Image retrieval methods based on CNN descriptors rely on metric learning from a large number of diverse examples of positive and negative image pairs. Domains, such as night-time images, with limited availability and variability of training data suffer from poor retrieval performance even with methods performing well on standard benchmarks. We propose to train a GAN-based synthetic-image generator, translating available day-time image examples into night images. Such a generator is used in metric learning as a form of augmentation, supplying training data to the scarce domain. Various types of generators are evaluated and analyzed. We contribute with a novel light-weight GAN architecture that enforces the consistency between the original and translated image through edge consistency. The proposed architecture also allows a simultaneous training of an edge detector that operates on both night and day images. To further increase the variability in the training examples and to maximize the generalization of the trained model, we propose a novel method of diverse anchor mining.   The proposed method improves over the state-of-the-art results on a standard Tokyo 24/7 day-night retrieval benchmark while preserving the performance on Oxford and Paris datasets. This is achieved without the need of training image pairs of matching day and night images. The source code is available at https://github.com/mohwald/gandtr .
</details>
<details>
<summary>摘要</summary>
图像检索方法基于CNN描述符依赖于大量多样化的正例和反例图像对的 metric 学习。如夜间图像频率和多样性受限的领域，使用标准 benchmark 中表现良好的方法仍然存在检索性能低下的问题。我们提议使用 GAN 基于的生成器，将可用的日间图像例子翻译成夜间图像。这种生成器在 metric 学习中用作数据增强，为缺乏训练数据的频道提供了训练数据。我们提出了一种新的轻量级 GAN 架构，通过 Edge 的一致性来保证原始图像和翻译图像之间的一致性。此外，我们还提出了一种同时训练 Edge 检测器，该检测器可以在夜间和日间图像上运行。为了进一步增加训练例子的多样性和最大化模型的泛化性，我们提出了一种新的多样 anchor 挖掘方法。通过这种方法，我们超越了标准 Tokyo 24/7 日夜检索标准准则的现有最佳结果，而不需要训练日夜对应的图像对。此外，我们还保持了在 Oxford 和 Paris 数据集上的性能。这些成果都是基于不需要特定的日夜图像对的训练。代码可以在 <https://github.com/mohwald/gandtr> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Logarithm-transform-aided-Gaussian-Sampling-for-Few-Shot-Learning"><a href="#Logarithm-transform-aided-Gaussian-Sampling-for-Few-Shot-Learning" class="headerlink" title="Logarithm-transform aided Gaussian Sampling for Few-Shot Learning"></a>Logarithm-transform aided Gaussian Sampling for Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16337">http://arxiv.org/abs/2309.16337</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ganatra-v/gaussian-sampling-fsl">https://github.com/ganatra-v/gaussian-sampling-fsl</a></li>
<li>paper_authors: Vaibhav Ganatra</li>
<li>for: 这篇论文主要关注的是几shot类别化任务中的表现学习，尤其是使用Gaussian分布来训练分类器。</li>
<li>methods: 本论文提出了一新的Gaussian对映方法，可以将实验数据转换为Gaussian-like分布，并且该方法比现有的方法更高效。</li>
<li>results: 本论文透过实验显示，使用该新的Gaussian对映方法可以实现更好的几shot类别化性能，并且需要较少的数据训练。<details>
<summary>Abstract</summary>
Few-shot image classification has recently witnessed the rise of representation learning being utilised for models to adapt to new classes using only a few training examples. Therefore, the properties of the representations, such as their underlying probability distributions, assume vital importance. Representations sampled from Gaussian distributions have been used in recent works, [19] to train classifiers for few-shot classification. These methods rely on transforming the distributions of experimental data to approximate Gaussian distributions for their functioning. In this paper, I propose a novel Gaussian transform, that outperforms existing methods on transforming experimental data into Gaussian-like distributions. I then utilise this novel transformation for few-shot image classification and show significant gains in performance, while sampling lesser data.
</details>
<details>
<summary>摘要</summary>
近些年，几何学习在几个例子中的图像分类得到了广泛应用，因此表示学习的特性，如其下面的概率分布，变得非常重要。在过去的工作中，使用 Gaussian 分布来训练几个例子中的分类器。这些方法基于将实际数据的分布转换为接近 Gaussian 分布的方法。在这篇论文中，我提出了一种新的 Gaussian 变换，超过了现有方法在将实际数据转换为 Gaussian-like 分布的能力。然后，我利用这种新的变换进行几个例子中的图像分类，并显示了显著的性能提升，而且采样更少的数据。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Video-Anomaly-Detection-with-Snippet-Anomalous-Attention"><a href="#Weakly-Supervised-Video-Anomaly-Detection-with-Snippet-Anomalous-Attention" class="headerlink" title="Weakly-Supervised Video Anomaly Detection with Snippet Anomalous Attention"></a>Weakly-Supervised Video Anomaly Detection with Snippet Anomalous Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16309">http://arxiv.org/abs/2309.16309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yidan Fan, Yongxin Yu, Wenhuan Lu, Yahong Han</li>
<li>for: 针对含有异常事件的未经处理视频进行异常检测。</li>
<li>methods: 提出了一种异常注意力机制，通过考虑视频帧级别编码特征而不需要pseudo标签。Specifically, our approach first generates snippet-level anomalous attention and then feeds it together with original anomaly scores into a Multi-branch Supervision Module.</li>
<li>results: 经验表明，我们的方法可以更好地检测异常事件，并且可以更 preciselly localize异常。Experiments on benchmark datasets XDViolence和UCF-Crime verify the effectiveness of our method.<details>
<summary>Abstract</summary>
With a focus on abnormal events contained within untrimmed videos, there is increasing interest among researchers in video anomaly detection. Among different video anomaly detection scenarios, weakly-supervised video anomaly detection poses a significant challenge as it lacks frame-wise labels during the training stage, only relying on video-level labels as coarse supervision. Previous methods have made attempts to either learn discriminative features in an end-to-end manner or employ a twostage self-training strategy to generate snippet-level pseudo labels. However, both approaches have certain limitations. The former tends to overlook informative features at the snippet level, while the latter can be susceptible to noises. In this paper, we propose an Anomalous Attention mechanism for weakly-supervised anomaly detection to tackle the aforementioned problems. Our approach takes into account snippet-level encoded features without the supervision of pseudo labels. Specifically, our approach first generates snippet-level anomalous attention and then feeds it together with original anomaly scores into a Multi-branch Supervision Module. The module learns different areas of the video, including areas that are challenging to detect, and also assists the attention optimization. Experiments on benchmark datasets XDViolence and UCF-Crime verify the effectiveness of our method. Besides, thanks to the proposed snippet-level attention, we obtain a more precise anomaly localization.
</details>
<details>
<summary>摘要</summary>
“对于含有异常事件的未裁剪影片，研究人员对影片异常检测存在增加的兴趣。在不同的影片异常检测场景中，弱监督的影片异常检测具有重要挑战，因为它缺乏training阶段Frame-wise标签，只有视频级别标签作为杂质指导。前一些方法尝试了以下两种方法：一是通过端到端学习学习特征，二是使用两个阶段自动训练策略生成剪辑级别的pseudo标签。然而，这两种方法都有一定的局限性。前者容易忽略剪辑级别的有用特征，而后者可能会受到噪音的影响。在这篇论文中，我们提出了一种异常注意力机制，用于弱监督的异常检测。我们的方法首先生成剪辑级别的异常注意力，然后将其与原始异常分数一起 feed into一个多支序监督模块。该模块学习不同的视频区域，包括具有检测挑战的区域，也可以帮助注意力优化。实验表明，我们的方法在XDViolence和UCF-Crime数据集上具有显著效果。此外，由于我们提出的剪辑级别注意力，我们可以更准确地定位异常。”
</details></li>
</ul>
<hr>
<h2 id="Can-the-Query-based-Object-Detector-Be-Designed-with-Fewer-Stages"><a href="#Can-the-Query-based-Object-Detector-Be-Designed-with-Fewer-Stages" class="headerlink" title="Can the Query-based Object Detector Be Designed with Fewer Stages?"></a>Can the Query-based Object Detector Be Designed with Fewer Stages?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16306">http://arxiv.org/abs/2309.16306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialin Li, Weifu Fu, Yuhuan Lin, Qiang Nie, Yong Liu</li>
<li>for: 提高查询基于对象检测器的性能，尝试缩短查询过程中的多stage编码器和解码器。</li>
<li>methods: 提出多种改进查询基于对象检测器的技术，并基于这些发现提出一个新模型叫GOLO（全局一次和本地一次），该模型采用了两stage解码器。</li>
<li>results: 对COCO数据集进行实验，比较与主流查询基于多stage编码器和解码器的模型，GOLO模型具有较少的decoder stages，仍然能够达到高度的性能。<details>
<summary>Abstract</summary>
Query-based object detectors have made significant advancements since the publication of DETR. However, most existing methods still rely on multi-stage encoders and decoders, or a combination of both. Despite achieving high accuracy, the multi-stage paradigm (typically consisting of 6 stages) suffers from issues such as heavy computational burden, prompting us to reconsider its necessity. In this paper, we explore multiple techniques to enhance query-based detectors and, based on these findings, propose a novel model called GOLO (Global Once and Local Once), which follows a two-stage decoding paradigm. Compared to other mainstream query-based models with multi-stage decoders, our model employs fewer decoder stages while still achieving considerable performance. Experimental results on the COCO dataset demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
征文基于对象检测器在DETR发表后已经作出了 significiant 进步。然而，大多数现有方法仍然依赖于多个阶段编码器和解码器，或者是这两者的组合。尽管实现了高精度，但多阶段 paradigma（通常包括6个阶段）受到了Computational burden 的困扰，让我们重新思考其必要性。在这篇论文中，我们探讨了多种提高查询基于检测器的技术，并根据这些发现，我们提出了一种新的模型called GOLO（全局一次和本地一次），该模型采用了两个阶段解码方式。与其他主流查询基于模型的多个阶段解码器相比，我们的模型使用了 fewer decoder stages，但仍然可以实现较高的性能。在COCO数据集上进行的实验结果表明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Recurrent-LSTM-and-Transformer-Network-for-Depth-Completion"><a href="#Multi-scale-Recurrent-LSTM-and-Transformer-Network-for-Depth-Completion" class="headerlink" title="Multi-scale Recurrent LSTM and Transformer Network for Depth Completion"></a>Multi-scale Recurrent LSTM and Transformer Network for Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16301">http://arxiv.org/abs/2309.16301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaogang Jia, Yusong Tan, Songlei Jian, Yonggang Che</li>
<li>for: 这 paper 的目的是提出一种基于 LSTM 和 Transformer 模块的深度完成方法，以实现fficiently fuse 色彩空间和深度空间的特征。</li>
<li>methods: 该 paper 使用了 Forget gate、Update gate、Output gate 和 Skip gate 来实现有效的色彩和深度特征融合，并在多尺度进行循环优化。最后，通过多头注意力机制来进一步融合深度特征。</li>
<li>results: 实验结果显示，无需复杂的网络结构和后处理步骤，我们的方法可以在一个简单的编码器-解码器网络结构上达到现场的主流自动驾驶 KITTI 数据集的状态码性表现，并且可以作为其他方法的后备网络，也可以达到状态码性表现。<details>
<summary>Abstract</summary>
Lidar depth completion is a new and hot topic of depth estimation. In this task, it is the key and difficult point to fuse the features of color space and depth space. In this paper, we migrate the classic LSTM and Transformer modules from NLP to depth completion and redesign them appropriately. Specifically, we use Forget gate, Update gate, Output gate, and Skip gate to achieve the efficient fusion of color and depth features and perform loop optimization at multiple scales. Finally, we further fuse the deep features through the Transformer multi-head attention mechanism. Experimental results show that without repetitive network structure and post-processing steps, our method can achieve state-of-the-art performance by adding our modules to a simple encoder-decoder network structure. Our method ranks first on the current mainstream autonomous driving KITTI benchmark dataset. It can also be regarded as a backbone network for other methods, which likewise achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
《 lidar 深度完成》是一个新的和热门的深度估计领域。在这个任务中，它是关键和困难的点是将颜色空间和深度空间的特征进行有效的融合。在这篇论文中，我们将传统的 LSTM 和 Transformer 模块从 NLP 迁移到深度完成领域，并对其进行适当的重新设计。具体来说，我们使用 Forget gate、Update gate、Output gate 和 Skip gate 来实现有效的颜色和深度特征融合，并在多个尺度上进行循环优化。最后，我们进一步将深度特征进行 Transformer 多头注意机制来进行融合。实验结果显示，无需复杂的网络结构和后处理步骤，我们的方法可以通过添加我们的模块到简单的编码器-解码器网络结构来实现状态作均性的表现。我们的方法在当前主流自动驾驶 KITTI benchmark 数据集上 ranking 第一名，同时也可以作为其他方法的后ION 网络，也实现了状态作均性的表现。
</details></li>
</ul>
<hr>
<h2 id="GAMMA-Generalizable-Articulation-Modeling-and-Manipulation-for-Articulated-Objects"><a href="#GAMMA-Generalizable-Articulation-Modeling-and-Manipulation-for-Articulated-Objects" class="headerlink" title="GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects"></a>GAMMA: Generalizable Articulation Modeling and Manipulation for Articulated Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16264">http://arxiv.org/abs/2309.16264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu</li>
<li>for: 本研究旨在提出一种通用的人工智能模型，可以识别和控制多种不同类型的受拟合物体。</li>
<li>methods: 该模型基于PartNet-Mobility数据集进行训练，并采用自适应操作来逐渐减少模型错误并提高操作性能。</li>
<li>results: 实验结果表明，该模型在未看过的和跨类型的受拟合物体中表现出色，超过了现有的艺术骨骼模型和抓取算法的性能。<details>
<summary>Abstract</summary>
Articulated objects like cabinets and doors are widespread in daily life. However, directly manipulating 3D articulated objects is challenging because they have diverse geometrical shapes, semantic categories, and kinetic constraints. Prior works mostly focused on recognizing and manipulating articulated objects with specific joint types. They can either estimate the joint parameters or distinguish suitable grasp poses to facilitate trajectory planning. Although these approaches have succeeded in certain types of articulated objects, they lack generalizability to unseen objects, which significantly impedes their application in broader scenarios. In this paper, we propose a novel framework of Generalizable Articulation Modeling and Manipulating for Articulated Objects (GAMMA), which learns both articulation modeling and grasp pose affordance from diverse articulated objects with different categories. In addition, GAMMA adopts adaptive manipulation to iteratively reduce the modeling errors and enhance manipulation performance. We train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive experiments in SAPIEN simulation and real-world Franka robot. Results show that GAMMA significantly outperforms SOTA articulation modeling and manipulation algorithms in unseen and cross-category articulated objects. We will open-source all codes and datasets in both simulation and real robots for reproduction in the final version. Images and videos are published on the project website at: http://sites.google.com/view/gamma-articulation
</details>
<details>
<summary>摘要</summary>
每天生活中都可以找到具有多个 JOINT 的物体，如橱柜和门。然而，直接操作这些三维具有多 JOINT 的物体是困难的，因为它们具有多样的几何形态、 semantic 类别和动力约束。先前的研究主要集中在特定 JOINT 类型上recognize和操作具有多 JOINT 的物体。它们可以是估算 JOINT 参数或 distinguishing 适合的抓取姿势，以便进行 trajectory 规划。虽然这些方法在某些类型的具有多 JOINT 的物体上达到了一定的成功，但它们缺乏对未看到的物体的一般化，这会很大程度地限制它们在更广泛的场景中的应用。在这篇论文中，我们提出了一种名为 Generalizable Articulation Modeling and Manipulating for Articulated Objects (GAMMA) 的新框架。GAMMA 会学习具有多 JOINT 的物体的 Connection 模型和抓取姿势的可行性。此外，GAMMA 采用了适应的操作来逐步减少模型错误和提高操作性能。我们在 PartNet-Mobility 数据集上训练 GAMMA，并通过在 SAPIEN 模拟和实际 Franka 机器人中进行了广泛的实验。结果显示，GAMMA 在未看到和跨类别的具有多 JOINT 的物体上明显超越了当前的 Connection 模型和操作算法。我们将在最终版本中公布所有代码和数据集，并在实际机器人和模拟中进行了重复。图片和视频已经在项目网站上发布：http://sites.google.com/view/gamma-articulation。
</details></li>
</ul>
<hr>
<h2 id="FORB-A-Flat-Object-Retrieval-Benchmark-for-Universal-Image-Embedding"><a href="#FORB-A-Flat-Object-Retrieval-Benchmark-for-Universal-Image-Embedding" class="headerlink" title="FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding"></a>FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16249">http://arxiv.org/abs/2309.16249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pxiangwu/forb">https://github.com/pxiangwu/forb</a></li>
<li>paper_authors: Pengxiang Wu, Siman Wang, Kevin Dela Rosa, Derek Hao Hu</li>
<li>for: 本研究旨在提供一个新的图像搜寻比较benchmark，以评估图像嵌入优化的质量。</li>
<li>methods: 本研究使用了多种 represnetation learning 方法，包括 LSTM、CNN 和 Siamese 网络。</li>
<li>results: 本研究发现，不同的搜寻策略在不同的图像类别上的表现有所不同，并且提出了一个新的图像搜寻benchmark（FORB），以测试图像嵌入优化的质量。<details>
<summary>Abstract</summary>
Image retrieval is a fundamental task in computer vision. Despite recent advances in this field, many techniques have been evaluated on a limited number of domains, with a small number of instance categories. Notably, most existing works only consider domains like 3D landmarks, making it difficult to generalize the conclusions made by these works to other domains, e.g., logo and other 2D flat objects. To bridge this gap, we introduce a new dataset for benchmarking visual search methods on flat images with diverse patterns. Our flat object retrieval benchmark (FORB) supplements the commonly adopted 3D object domain, and more importantly, it serves as a testbed for assessing the image embedding quality on out-of-distribution domains. In this benchmark we investigate the retrieval accuracy of representative methods in terms of candidate ranks, as well as matching score margin, a viewpoint which is largely ignored by many works. Our experiments not only highlight the challenges and rich heterogeneity of FORB, but also reveal the hidden properties of different retrieval strategies. The proposed benchmark is a growing project and we expect to expand in both quantity and variety of objects. The dataset and supporting codes are available at https://github.com/pxiangwu/FORB/.
</details>
<details>
<summary>摘要</summary>
<SYS>translate-into-simplified-chinese图像检索是计算机视觉中的基本任务。尽管最近几年有很多技术在这个领域进行了探索，但是大多数技术只是在一些有限的领域上进行了评估，而且只考虑了3D地标的领域。这使得已有的研究结论很难推广到其他领域，例如标识logo和其他2D平面对象。为了bridging这个差距，我们提出了一个新的图像检索数据集（FORB），该数据集补充了常见采用的3D对象领域，而且更重要的是，它作为图像嵌入质量评估的测试床。在这个数据集中，我们 investigate了不同方法的检索精度，包括候选人数、匹配分数差等方面。我们的实验不仅揭示了FORB的挑战和多样性，还揭示了不同检索策略的隐藏性。我们预计将在量和多样性方面继续扩展该数据集。数据集和相关代码可以在https://github.com/pxiangwu/FORB/上获取。</SYS>Here's the translation in Traditional Chinese as well:<SYS>translate-into-traditional-chinese图像检索是计算机视觉中的基本任务。尽管最近几年有很多技术在这个领域进行了探索，但是大多数技术只是在一些有限的领域上进行了评估，而且只考虑了3D地标的领域。这使得已有的研究结论很难推广到其他领域，例如标识logo和其他2D平面对象。为了bridging这个差距，我们提出了一个新的图像检索数据集（FORB），该数据集补充了常见采用的3D对象领域，而且更重要的是，它作为图像嵌入质量评估的测试床。在这个数据集中，我们 investigate了不同方法的检索精度，包括候选人数、匹配分数差等方面。我们的实验不仅揭示了FORB的挑战和多样性，还揭示了不同检索策略的隐藏性。我们预计将在量和多样性方面继续扩展该数据集。数据集和相关代码可以在https://github.com/pxiangwu/FORB/上获取。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Object-Motion-Guided-Human-Motion-Synthesis"><a href="#Object-Motion-Guided-Human-Motion-Synthesis" class="headerlink" title="Object Motion Guided Human Motion Synthesis"></a>Object Motion Guided Human Motion Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16237">http://arxiv.org/abs/2309.16237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaman Li, Jiajun Wu, C. Karen Liu</li>
<li>for: This paper is written for the purpose of full-body human motion synthesis for the manipulation of large-sized objects, with applications in character animation, embodied AI, VR&#x2F;AR, and robotics.</li>
<li>methods: The proposed method, called Object MOtion guided human MOtion synthesis (OMOMO), is a conditional diffusion framework that uses two separate denoising processes to generate full-body manipulation behaviors from only the object motion. The method employs hand positions as an intermediate representation to explicitly enforce contact constraints, resulting in more physically plausible manipulation motions.</li>
<li>results: The proposed pipeline is demonstrated to be effective through extensive experiments, and is shown to generalize well to unseen objects. Additionally, a large-scale dataset consisting of 3D object geometry, object motion, and human motion is collected, which contains human-object interaction motion for 15 objects with a total duration of approximately 10 hours.<details>
<summary>Abstract</summary>
Modeling human behaviors in contextual environments has a wide range of applications in character animation, embodied AI, VR/AR, and robotics. In real-world scenarios, humans frequently interact with the environment and manipulate various objects to complete daily tasks. In this work, we study the problem of full-body human motion synthesis for the manipulation of large-sized objects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a conditional diffusion framework that can generate full-body manipulation behaviors from only the object motion. Since naively applying diffusion models fails to precisely enforce contact constraints between the hands and the object, OMOMO learns two separate denoising processes to first predict hand positions from object motion and subsequently synthesize full-body poses based on the predicted hand positions. By employing the hand positions as an intermediate representation between the two denoising processes, we can explicitly enforce contact constraints, resulting in more physically plausible manipulation motions. With the learned model, we develop a novel system that captures full-body human manipulation motions by simply attaching a smartphone to the object being manipulated. Through extensive experiments, we demonstrate the effectiveness of our proposed pipeline and its ability to generalize to unseen objects. Additionally, as high-quality human-object interaction datasets are scarce, we collect a large-scale dataset consisting of 3D object geometry, object motion, and human motion. Our dataset contains human-object interaction motion for 15 objects, with a total duration of approximately 10 hours.
</details>
<details>
<summary>摘要</summary>
人类行为模拟在受环境中有广泛的应用，包括人物动画、身体AI、VR/AR和 робо工程。在实际情况下，人类经常与环境交互，并使用不同的物体来完成日常任务。在这种工作中，我们研究了大型物体摆动的全身人类动作合成问题。我们提出了物体动作引导人体动作合成（OMOMO），一种条件扩散框架，可以通过只有物体动作来生成全身摆动行为。由于直接应用扩散模型无法准确实施手和物体之间的接触约束，OMOMO学习了两个分离的排除过程，先预测手部位于物体动作，然后基于预测的手部位进行全身姿态合成。通过使用手部位作为两个排除过程之间的中间表示，我们可以直接实施接触约束，从而生成更加物理可能的摆动姿态。我们提出的管道可以通过简单地将手机附加到被摆动的物体来捕捉全身人类摆动姿态。通过广泛的实验，我们证明了我们的提出的管道的效iveness和其能够泛化到未看到的物体。此外，由于高质量的人类-物体交互动作数据罕见，我们收集了一个大规模的数据集，包括3D物体几何、物体动作和人体动作。我们的数据集包含15种物体的人类-物体交互动作，总持续时间约10小时。
</details></li>
</ul>
<hr>
<h2 id="Off-the-shelf-bin-picking-workcell-with-visual-pose-estimation-A-case-study-on-the-world-robot-summit-2018-kitting-task"><a href="#Off-the-shelf-bin-picking-workcell-with-visual-pose-estimation-A-case-study-on-the-world-robot-summit-2018-kitting-task" class="headerlink" title="Off-the-shelf bin picking workcell with visual pose estimation: A case study on the world robot summit 2018 kitting task"></a>Off-the-shelf bin picking workcell with visual pose estimation: A case study on the world robot summit 2018 kitting task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16221">http://arxiv.org/abs/2309.16221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederik Hagelskjær, Kasper Høj Lorenzen, Dirk Kraft</li>
<li>for: 本研究旨在提高机器人的搬运灵活性，通过视觉位置估算技术和新感知器来实现箱内物品拾取。</li>
<li>methods: 本研究使用了新的视觉感知器和位置估算算法，实现了箱内物品的位置估算。同时，我们还实现了一个工作站来完成箱内物品拾取，使用力基 grasping方法。</li>
<li>results: 我们在2018年世界机器人大会组装挑战中测试了我们的设备，并取得了所有参赛队伍中最高分。这说明当今技术已经可以在比之前的水平上进行箱内物品拾取。<details>
<summary>Abstract</summary>
The World Robot Summit 2018 Assembly Challenge included four different tasks. The kitting task, which required bin-picking, was the task in which the fewest points were obtained. However, bin-picking is a vital skill that can significantly increase the flexibility of robotic set-ups, and is, therefore, an important research field. In recent years advancements have been made in sensor technology and pose estimation algorithms. These advancements allow for better performance when performing visual pose estimation.   This paper shows that by utilizing new vision sensors and pose estimation algorithms pose estimation in bins can be performed successfully. We also implement a workcell for bin picking along with a force based grasping approach to perform the complete bin picking. Our set-up is tested on the World Robot Summit 2018 Assembly Challenge and successfully obtains a higher score compared with all teams at the competition. This demonstrate that current technology can perform bin-picking at a much higher level compared with previous results.
</details>
<details>
<summary>摘要</summary>
世界机器人峰会2018年组装挑战中包括四个不同的任务。其中kiting任务，需要拾取物品，是所有任务中得分最低的一个。然而，拾取是机器人设置的重要技能，可以增加机器人的灵活性，因此是一个重要的研究领域。在过去几年，感知技术和pose估计算术得到了进步。这些进步使得在视觉pose估计中表现更好。这篇论文展示了通过新的视觉感知器和pose估计算术，在容器中进行pose估计是可行的。我们还实现了一个工作站以进行容器拾取，并使用力学 grasping方法来完成完整的容器拾取。我们的设置在世界机器人峰会2018年组装挑战中被测试，并成功获得了所有队伍在比赛中的高得分。这表明现有技术可以在前一个水平上进行容器拾取，而不是之前的结果。
</details></li>
</ul>
<hr>
<h2 id="GAFlow-Incorporating-Gaussian-Attention-into-Optical-Flow"><a href="#GAFlow-Incorporating-Gaussian-Attention-into-Optical-Flow" class="headerlink" title="GAFlow: Incorporating Gaussian Attention into Optical Flow"></a>GAFlow: Incorporating Gaussian Attention into Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16217">http://arxiv.org/abs/2309.16217</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/la30/gaflow">https://github.com/la30/gaflow</a></li>
<li>paper_authors: Ao Luo, Fan Yang, Xin Li, Lang Nie, Chunyu Lin, Haoqiang Fan, Shuaicheng Liu</li>
<li>for: 提高计算机视觉中的运动场景分析精度，特别是在图像序列中捕捉和跟踪运动的问题。</li>
<li>methods: 使用 Gaussian Attention 技术，包括 Gaussian-Constrained Layer (GCL) 和 Gaussian-Guided Attention Module (GGAM)，来强调本地特征和运动相关性。</li>
<li>results: 在标准的计算机视觉数据集上进行了广泛的实验，并显示了提高的表现，包括在评估能力和在线测试中的优异表现。<details>
<summary>Abstract</summary>
Optical flow, or the estimation of motion fields from image sequences, is one of the fundamental problems in computer vision. Unlike most pixel-wise tasks that aim at achieving consistent representations of the same category, optical flow raises extra demands for obtaining local discrimination and smoothness, which yet is not fully explored by existing approaches. In this paper, we push Gaussian Attention (GA) into the optical flow models to accentuate local properties during representation learning and enforce the motion affinity during matching. Specifically, we introduce a novel Gaussian-Constrained Layer (GCL) which can be easily plugged into existing Transformer blocks to highlight the local neighborhood that contains fine-grained structural information. Moreover, for reliable motion analysis, we provide a new Gaussian-Guided Attention Module (GGAM) which not only inherits properties from Gaussian distribution to instinctively revolve around the neighbor fields of each point but also is empowered to put the emphasis on contextually related regions during matching. Our fully-equipped model, namely Gaussian Attention Flow network (GAFlow), naturally incorporates a series of novel Gaussian-based modules into the conventional optical flow framework for reliable motion analysis. Extensive experiments on standard optical flow datasets consistently demonstrate the exceptional performance of the proposed approach in terms of both generalization ability evaluation and online benchmark testing. Code is available at https://github.com/LA30/GAFlow.
</details>
<details>
<summary>摘要</summary>
Computer vision 中的一个基本问题是图像序列中的运动场景估计（optical flow）。与大多数像素级任务不同，运动场景估计需要同时获得本地特征和流畅性，这些要求尚未由现有方法充分探索。在这篇论文中，我们将把Gaussian Attention（GA）技术应用于运动场景估计模型，以便在学习 represencing 过程中强调本地特征，并在匹配过程中保持运动相互关系。 Specifically, we introduce a novel Gaussian-Constrained Layer（GCL），可以轻松地插入到现有的Transformer块中，以高亮本地区域的细腻结构信息。此外，为了确保可靠的运动分析，我们提出了一新的 Gaussian-Guided Attention Module（GGAM），不仅继承了Gaussian分布的特性，以便自然地绕着每个点的邻近场景循环，而且可以在匹配过程中强调上下文相关的区域。我们的全套模型，即Gaussian Attention Flow网络（GAFlow），自然地将一系列基于Gaussian分布的模块集成到了传统的运动场景估计框架中，以确保可靠的运动分析。经验表明，我们的方法在标准的运动场景估计数据集上表现出了出色的一致性和在线测试中的稳定性。代码可以在https://github.com/LA30/GAFlow中找到。
</details></li>
</ul>
<hr>
<h2 id="Abdominal-multi-organ-segmentation-in-CT-using-Swinunter"><a href="#Abdominal-multi-organ-segmentation-in-CT-using-Swinunter" class="headerlink" title="Abdominal multi-organ segmentation in CT using Swinunter"></a>Abdominal multi-organ segmentation in CT using Swinunter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16210">http://arxiv.org/abs/2309.16210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingjin Chen, Yongkang He, Yongyi Lu</li>
<li>for: 验证深度学习模型在 computed tomography（CT）图像中进行腹部多器官分割的可能性，以便检测疾病和规划治疗。</li>
<li>methods: 使用 transformer-based 模型进行训练，以便更好地处理复杂背景和不同器官大小的问题。</li>
<li>results: 在公共验证集上获得了可接受的结果和推理时间，表明 transformer-based 模型在这些任务中可以达到更高的性能。<details>
<summary>Abstract</summary>
Abdominal multi-organ segmentation in computed tomography (CT) is crucial for many clinical applications including disease detection and treatment planning. Deep learning methods have shown unprecedented performance in this perspective. However, it is still quite challenging to accurately segment different organs utilizing a single network due to the vague boundaries of organs, the complex background, and the substantially different organ size scales. In this work we used make transformer-based model for training. It was found through previous years' competitions that basically all of the top 5 methods used CNN-based methods, which is likely due to the lack of data volume that prevents transformer-based methods from taking full advantage. The thousands of samples in this competition may enable the transformer-based model to have more excellent results. The results on the public validation set also show that the transformer-based model can achieve an acceptable result and inference time.
</details>
<details>
<summary>摘要</summary>
Computed tomography (CT) 腹部多器官分割是许多临床应用的关键，包括疾病检测和治疗规划。深度学习方法在这个方面表现出了无 précédente的表现。然而，使用单一网络 segment 不同器官仍然是一项具有挑战性的任务，主要因为器官的界限晦涩，背景复杂，器官尺度 scales 不同。在这项工作中，我们使用 transformer-based 模型进行训练。根据前年的竞赛结果，大约 90% 的前五名方法都使用 CNN-based 方法，这可能是因为数据量的限制，阻碍 transformer-based 方法发挥全面的效用。这 thousands 个样本在这次竞赛中可能会帮助 transformer-based 模型取得更佳的结果。在公共验证集上也可以看到， transformer-based 模型可以达到可接受的结果和执行时间。
</details></li>
</ul>
<hr>
<h2 id="Nonconvex-third-order-Tensor-Recovery-Based-on-Logarithmic-Minimax-Function"><a href="#Nonconvex-third-order-Tensor-Recovery-Based-on-Logarithmic-Minimax-Function" class="headerlink" title="Nonconvex third-order Tensor Recovery Based on Logarithmic Minimax Function"></a>Nonconvex third-order Tensor Recovery Based on Logarithmic Minimax Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16208">http://arxiv.org/abs/2309.16208</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongbing Zhang</li>
<li>for: 本研究旨在提出一种新的对数最小最大函数（LM），用于保护大对数值 while 强制小对数值处加以惩罚。</li>
<li>methods: 该函数基于非 convex relaxation 方法，并定义了一种加重tensor LM norm的重量tensor LM norm。</li>
<li>results: 实验表明，提出的方法可以在不同的实际数据集上具有更高的完teness和精度，并且比预先状态艺术方法（EMLCP）更高。<details>
<summary>Abstract</summary>
Recent researches have shown that low-rank tensor recovery based non-convex relaxation has gained extensive attention. In this context, we propose a new Logarithmic Minimax (LM) function. The comparative analysis between the LM function and the Logarithmic, Minimax concave penalty (MCP), and Minimax Logarithmic concave penalty (MLCP) functions reveals that the proposed function can protect large singular values while imposing stronger penalization on small singular values. Based on this, we define a weighted tensor LM norm as a non-convex relaxation for tensor tubal rank. Subsequently, we propose the TLM-based low-rank tensor completion (LRTC) model and the TLM-based tensor robust principal component analysis (TRPCA) model respectively. Furthermore, we provide theoretical convergence guarantees for the proposed methods. Comprehensive experiments were conducted on various real datasets, and a comparison analysis was made with the similar EMLCP method. The results demonstrate that the proposed method outperforms the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Note:* "Recent researches" should be "Recent research" in Simplified Chinese.* "based on non-convex relaxation" should be "based on non-convex relaxation" in Simplified Chinese.* "Comprehensive experiments" should be "Extensive experiments" in Simplified Chinese.* "EMLCP" should be "EMLCP method" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Parameter-Saving-Adversarial-Training-Reinforcing-Multi-Perturbation-Robustness-via-Hypernetworks"><a href="#Parameter-Saving-Adversarial-Training-Reinforcing-Multi-Perturbation-Robustness-via-Hypernetworks" class="headerlink" title="Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation Robustness via Hypernetworks"></a>Parameter-Saving Adversarial Training: Reinforcing Multi-Perturbation Robustness via Hypernetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16207">http://arxiv.org/abs/2309.16207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huihui Gong, Minjing Dong, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu</li>
<li>for: 防御强制攻击（Adversarial Training）是一种最受欢迎并且最有效的方法来防御强制攻击。</li>
<li>methods: 我们提出了一种新的多种攻击perturbation adversarial training框架，即参数节省鲁棒训练（PSAT），以提高多种攻击perturbation的鲁棒性，同时具有节省参数的优点。</li>
<li>results: 我们对各种最新的攻击方法进行了广泛的评估和比较，并证明了我们的提出方法在不同的数据集上具有最佳的鲁棒性质量和参数节省的优势，例如在CIFAR-10数据集上，使用ResNet-50作为背景网络，PSAT可以将参数量减少约80%，同时保持最佳的鲁棒性质量。<details>
<summary>Abstract</summary>
Adversarial training serves as one of the most popular and effective methods to defend against adversarial perturbations. However, most defense mechanisms only consider a single type of perturbation while various attack methods might be adopted to perform stronger adversarial attacks against the deployed model in real-world scenarios, e.g., $\ell_2$ or $\ell_\infty$. Defending against various attacks can be a challenging problem since multi-perturbation adversarial training and its variants only achieve suboptimal robustness trade-offs, due to the theoretical limit to multi-perturbation robustness for a single model. Besides, it is impractical to deploy large models in some storage-efficient scenarios. To settle down these drawbacks, in this paper we propose a novel multi-perturbation adversarial training framework, parameter-saving adversarial training (PSAT), to reinforce multi-perturbation robustness with an advantageous side effect of saving parameters, which leverages hypernetworks to train specialized models against a single perturbation and aggregate these specialized models to defend against multiple perturbations. Eventually, we extensively evaluate and compare our proposed method with state-of-the-art single/multi-perturbation robust methods against various latest attack methods on different datasets, showing the robustness superiority and parameter efficiency of our proposed method, e.g., for the CIFAR-10 dataset with ResNet-50 as the backbone, PSAT saves approximately 80\% of parameters with achieving the state-of-the-art robustness trade-off accuracy.
</details>
<details>
<summary>摘要</summary>
“对抗攻击训练是现在最受欢迎并且最有效的防御方法。然而，大多数防御机制只考虑单一类型的攻击，而实际场景中可能会采用多种攻击方法来进行更加强大的对抗攻击。防御多种攻击是一个具有挑战性的问题，因为多重攻击 adversarial training 和其变体只能达到各种不佳的鲁棒性质交换。此外，在某些存储效率不高的场景中，部署大型模型是不切实际的。为了解决这些缺点，在这篇论文中我们提出了一种新的多重攻击 adversarial training 框架 Parametersaving adversarial training（PSAT），通过使用 hypernetworks 来训练特殊化模型对单个攻击，并将这些特殊化模型综合以防御多种攻击。最终，我们对state-of-the-art 单/多攻击鲁棒方法进行了广泛的评估和比较，在不同的数据集上，我们的提出方法展现出了鲁棒性superiority和参数效率的优势，例如在 CIFAR-10 数据集上，使用 ResNet-50 作为 backing 模型，PSAT 可以将参数数量减少约 80% ，同时实现最佳的鲁棒性质交换精度。”
</details></li>
</ul>
<hr>
<h2 id="Alzheimer’s-Disease-Prediction-via-Brain-Structural-Functional-Deep-Fusing-Network"><a href="#Alzheimer’s-Disease-Prediction-via-Brain-Structural-Functional-Deep-Fusing-Network" class="headerlink" title="Alzheimer’s Disease Prediction via Brain Structural-Functional Deep Fusing Network"></a>Alzheimer’s Disease Prediction via Brain Structural-Functional Deep Fusing Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16206">http://arxiv.org/abs/2309.16206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiankun Zuo, Junren Pan, Shuqiang Wang</li>
<li>For: 这篇论文旨在开发一个能够有效地融合多Modal neuroimages的模型，以分析阿尔ツ海默症（AD）的恶化。* Methods: 本论文提出了一个名为cross-modal transformer generative adversarial network（CT-GAN）的新模型，可以从 функциональ磁共振成像（fMRI）和Diffusion tensor imaging（DTI）中获取功能和结构资讯，并将其融合成一个统一的多Modal connectivity。* Results: 在ADNI资料集上进行评估，提出的CT-GAN模型可以明显提高预测性能和实际地检测AD相关的脑区域。此外，模型还提供了新的问题检测AD相关的异常神经回路的新关注。<details>
<summary>Abstract</summary>
Fusing structural-functional images of the brain has shown great potential to analyze the deterioration of Alzheimer's disease (AD). However, it is a big challenge to effectively fuse the correlated and complementary information from multimodal neuroimages. In this paper, a novel model termed cross-modal transformer generative adversarial network (CT-GAN) is proposed to effectively fuse the functional and structural information contained in functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI). The CT-GAN can learn topological features and generate multimodal connectivity from multimodal imaging data in an efficient end-to-end manner. Moreover, the swapping bi-attention mechanism is designed to gradually align common features and effectively enhance the complementary features between modalities. By analyzing the generated connectivity features, the proposed model can identify AD-related brain connections. Evaluations on the public ADNI dataset show that the proposed CT-GAN can dramatically improve prediction performance and detect AD-related brain regions effectively. The proposed model also provides new insights for detecting AD-related abnormal neural circuits.
</details>
<details>
<summary>摘要</summary>
综合结构功能图像融合技术已经在分析阿尔茨海默病（AD）的衰退方面显示了巨大的潜力。然而，是一个大的挑战来有效地融合多modal的脑图像信息。在这篇论文中，一种新的模型称为交叉模态变换生成敌对网络（CT-GAN）被提出，以有效地融合功能和结构信息，包括功能磁共振成像（fMRI）和扩散tensor成像（DTI）。CT-GAN可以学习 topological特征并生成多modal的连接性，从多modal成像数据中获得有效的终端到端方式。此外，交换双注意机制被设计来逐渐对共同特征进行对齐，以实现多modal之间的增强。通过分析生成的连接特征，提出的模型可以识别AD相关的脑连接。评估ADNI数据集显示，提出的CT-GAN可以明显提高预测性能和有效地检测AD相关的脑区域。此外，该模型还提供了新的检测AD相关异常神经Circuit的新视角。
</details></li>
</ul>
<hr>
<h2 id="DiffGAN-F2S-Symmetric-and-Efficient-Denoising-Diffusion-GANs-for-Structural-Connectivity-Prediction-from-Brain-fMRI"><a href="#DiffGAN-F2S-Symmetric-and-Efficient-Denoising-Diffusion-GANs-for-Structural-Connectivity-Prediction-from-Brain-fMRI" class="headerlink" title="DiffGAN-F2S: Symmetric and Efficient Denoising Diffusion GANs for Structural Connectivity Prediction from Brain fMRI"></a>DiffGAN-F2S: Symmetric and Efficient Denoising Diffusion GANs for Structural Connectivity Prediction from Brain fMRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16205">http://arxiv.org/abs/2309.16205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiankun Zuo, Ruiheng Li, Yi Di, Hao Tian, Changhong Jing, Xuhang Chen, Shuqiang Wang</li>
<li>for: 本研究旨在提出一种基于Diffusion Generative Adversarial Network（DiffGAN）的FMRI-to-SC（fMRI-to-structural connectivity）模型，以便在综合性脑网络融合中提取多Modal brain network中的可能性biomarkers。</li>
<li>methods: 该模型基于denoising diffusion probabilistic models（DDPMs）和对抗学习，可以很快地从fMRI中生成高精度的SC，并通过设计双通道多头空间注意力（DMSA）和图像 convolutional module来捕捉全球和本地脑区域之间的关系。</li>
<li>results: 在ADNI dataset上测试，该模型可以高效地生成empirical SC-preserved connectivity，并与其他相关模型相比显示出superior的SC预测性能。此外，该模型还可以确定大多数重要的脑区域和连接，提供一种替代的方式来融合多modal brain networks和分析临床疾病。<details>
<summary>Abstract</summary>
Mapping from functional connectivity (FC) to structural connectivity (SC) can facilitate multimodal brain network fusion and discover potential biomarkers for clinical implications. However, it is challenging to directly bridge the reliable non-linear mapping relations between SC and functional magnetic resonance imaging (fMRI). In this paper, a novel diffusision generative adversarial network-based fMRI-to-SC (DiffGAN-F2S) model is proposed to predict SC from brain fMRI in an end-to-end manner. To be specific, the proposed DiffGAN-F2S leverages denoising diffusion probabilistic models (DDPMs) and adversarial learning to efficiently generate high-fidelity SC through a few steps from fMRI. By designing the dual-channel multi-head spatial attention (DMSA) and graph convolutional modules, the symmetric graph generator first captures global relations among direct and indirect connected brain regions, then models the local brain region interactions. It can uncover the complex mapping relations between fMRI and structural connectivity. Furthermore, the spatially connected consistency loss is devised to constrain the generator to preserve global-local topological information for accurate intrinsic SC prediction. Testing on the public Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, the proposed model can effectively generate empirical SC-preserved connectivity from four-dimensional imaging data and shows superior performance in SC prediction compared with other related models. Furthermore, the proposed model can identify the vast majority of important brain regions and connections derived from the empirical method, providing an alternative way to fuse multimodal brain networks and analyze clinical disease.
</details>
<details>
<summary>摘要</summary>
fc可以映射到结构连接（sc），可以促进多Modal脑网络融合和发现临床应用中的生物标志物。然而，直接从 Functional Magnetic Resonance Imaging（fMRI）到SC的可靠非线性映射关系是挑战。在本文中，一种基于Diffusion Generative Adversarial Network（DiffGAN）的fMRI-to-SC（DiffGAN-F2S）模型被提出，可以在端到端方式 Predict SC from brain fMRI。具体来说，提案的DiffGAN-F2S模型利用了Denosing Diffusion Probabilistic Models（DDPMs）和对抗学习来快速生成高品质SC，只需几步。通过设计双通道多头空间注意力（DMSA）和图像演算模块，对称图生成器首先捕捉了全脑区域之间的全球关系，然后模型了本地脑区域之间的交互。这可以揭示出fMRI和SC之间的复杂映射关系。此外，用空间连接一致损失来约束生成器保持全球-本地 topological信息的准确SC预测。在公共Alzheimer's Disease Neuroimaging Initiative（ADNI）数据集上测试，提案的模型可以高效地从四维图像数据中生成empirical SC-保持连接，并显示与其他相关模型相比表现出色。此外，提案的模型还可以识别大多数重要的脑区域和连接，提供一种代替方式来融合多Modal脑网络并分析临床疾病。
</details></li>
</ul>
<hr>
<h2 id="Cloth2Body-Generating-3D-Human-Body-Mesh-from-2D-Clothing"><a href="#Cloth2Body-Generating-3D-Human-Body-Mesh-from-2D-Clothing" class="headerlink" title="Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing"></a>Cloth2Body: Generating 3D Human Body Mesh from 2D Clothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16189">http://arxiv.org/abs/2309.16189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lu Dai, Liqian Ma, Shenhan Qian, Hao Liu, Ziwei Liu, Hui Xiong<br>for: 这个论文的目标是生成基于2D服装图像的3D人体模型。methods: 该论文提出了一种结构包括人体姿态估计、身体形态估计和姿态生成方法的端到端框架。首先，利用人体姿态估计来估计人体姿态参数。然后，通过人体三维骨架的使用和反射 kinematics 模块来提高估计精度。最后，提出了一种适应深度技巧来规避对象大小和摄像头外部效果的分离。results: 该论文通过实验结果表明，提出的方法可以从2D服装图像中高精度地回归自然和多样化的3D人体模型，并且可以在探索和生成多个姿态时提供多样化的结果。<details>
<summary>Abstract</summary>
In this paper, we define and study a new Cloth2Body problem which has a goal of generating 3D human body meshes from a 2D clothing image. Unlike the existing human mesh recovery problem, Cloth2Body needs to address new and emerging challenges raised by the partial observation of the input and the high diversity of the output. Indeed, there are three specific challenges. First, how to locate and pose human bodies into the clothes. Second, how to effectively estimate body shapes out of various clothing types. Finally, how to generate diverse and plausible results from a 2D clothing image. To this end, we propose an end-to-end framework that can accurately estimate 3D body mesh parameterized by pose and shape from a 2D clothing image. Along this line, we first utilize Kinematics-aware Pose Estimation to estimate body pose parameters. 3D skeleton is employed as a proxy followed by an inverse kinematics module to boost the estimation accuracy. We additionally design an adaptive depth trick to align the re-projected 3D mesh better with 2D clothing image by disentangling the effects of object size and camera extrinsic. Next, we propose Physics-informed Shape Estimation to estimate body shape parameters. 3D shape parameters are predicted based on partial body measurements estimated from RGB image, which not only improves pixel-wise human-cloth alignment, but also enables flexible user editing. Finally, we design Evolution-based pose generation method, a skeleton transplanting method inspired by genetic algorithms to generate diverse reasonable poses during inference. As shown by experimental results on both synthetic and real-world data, the proposed framework achieves state-of-the-art performance and can effectively recover natural and diverse 3D body meshes from 2D images that align well with clothing.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们定义并研究了一个新的Cloth2Body问题，该问题的目标是从2D服装图像中生成3D人体几何体。与现有的人体几何恢复问题不同，Cloth2Body需要解决一些新的和兴起的挑战，包括：首先，如何将人体部着到服装上。第二，如何有效地从不同类型的服装中估计身体形态。最后，如何从2D服装图像中生成多样化和合理的结果。为此，我们提议一个端到端框架，可以准确地估计3D人体几何参数化 pose和形态从2D服装图像。在这个框架中，我们首先利用了 Kinematics-aware Pose Estimation，来估计人体姿态参数。然后，我们使用3D骨架作为代理，并使用反 Kinematics 模块来提高估计精度。此外，我们还设计了一种适应深度技巧，用于将重projected 3D网格更好地与2D服装图像相对应。接下来，我们提出了物理学习形态估计方法，用于估计身体形态参数。在RGB图像中估计部分身体测量值，可以不 только提高像素级人体-服装对齐，还可以启用 flexible user editing。最后，我们设计了一种进化策略，用于在推理中生成多样化的合理姿态。根据实验结果，我们的框架在 both synthetic and real-world data 上达到了状态元的性能，并可以效果地从2D图像中恢复自然和多样化的3D人体几何体。
</details></li>
</ul>
<hr>
<h2 id="BEVHeight-Toward-Robust-Visual-Centric-3D-Object-Detection"><a href="#BEVHeight-Toward-Robust-Visual-Centric-3D-Object-Detection" class="headerlink" title="BEVHeight++: Toward Robust Visual Centric 3D Object Detection"></a>BEVHeight++: Toward Robust Visual Centric 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16179">http://arxiv.org/abs/2309.16179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Yang, Tao Tang, Jun Li, Peng Chen, Kun Yuan, Li Wang, Yi Huang, Xinyu Zhang, Kaicheng Yu</li>
<li>for: This paper focuses on addressing the limitations of vision-centric bird’s eye view detection methods for roadside cameras, and proposes a simple yet effective approach called BEVHeight++ to improve the accuracy and robustness of camera-only perception methods.</li>
<li>methods: The proposed BEVHeight++ method regresses the height to the ground to achieve a distance-agnostic formulation, and incorporates both height and depth encoding techniques to improve the projection from 2D to BEV spaces.</li>
<li>results: The proposed method surpasses all previous vision-centric methods on popular 3D detection benchmarks of roadside cameras, and achieves significant improvements over depth-only methods in ego-vehicle scenarios. Specifically, the method yields a notable improvement of +1.9% NDS and +1.1% mAP over BEVDepth on the nuScenes validation set, and achieves substantial advancements of +2.8% NDS and +1.7% mAP on the nuScenes test set.Here’s the simplified Chinese text version of the three information points:</li>
<li>for: 本研究旨在解决路边摄像头上常见的视觉中心探测方法的局限性，并提出了一种简单 yet effective的方法 called BEVHeight++，以提高路边摄像头只使用的探测方法的准确性和可靠性。</li>
<li>methods: BEVHeight++方法通过对地面高度进行回归，实现了距离不依赖的表示，并通过结合高度和深度编码技术来提高2D到BEV空间的投影。</li>
<li>results: BEVHeight++方法在流行的路边摄像头3D探测benchmark上表现出色，超过了所有之前的视觉中心方法，并在ego-汽车场景中也表现出明显的提升，比如BEVDepth方法在nuScenes验证集上的NDS和mAP提升了+1.9%和+1.1%，分别在nuScenes测试集上提升了+2.8%和+1.7%。<details>
<summary>Abstract</summary>
While most recent autonomous driving system focuses on developing perception methods on ego-vehicle sensors, people tend to overlook an alternative approach to leverage intelligent roadside cameras to extend the perception ability beyond the visual range. We discover that the state-of-the-art vision-centric bird's eye view detection methods have inferior performances on roadside cameras. This is because these methods mainly focus on recovering the depth regarding the camera center, where the depth difference between the car and the ground quickly shrinks while the distance increases. In this paper, we propose a simple yet effective approach, dubbed BEVHeight++, to address this issue. In essence, we regress the height to the ground to achieve a distance-agnostic formulation to ease the optimization process of camera-only perception methods. By incorporating both height and depth encoding techniques, we achieve a more accurate and robust projection from 2D to BEV spaces. On popular 3D detection benchmarks of roadside cameras, our method surpasses all previous vision-centric methods by a significant margin. In terms of the ego-vehicle scenario, our BEVHeight++ possesses superior over depth-only methods. Specifically, it yields a notable improvement of +1.9% NDS and +1.1% mAP over BEVDepth when evaluated on the nuScenes validation set. Moreover, on the nuScenes test set, our method achieves substantial advancements, with an increase of +2.8% NDS and +1.7% mAP, respectively.
</details>
<details>
<summary>摘要</summary>
当前大多数自动驾驶系统都是关注ego-vehicle传感器上的感知方法，而人们往往忽视了一种alternative approach，即利用智能路边摄像头来扩展感知范围。我们发现现有的视觉中心 bird's eye view探测方法在路边摄像头上表现不佳。这是因为这些方法主要关注在camera中心的深度恢复，而在距离增加时，汽车和地面之间的深度差异很快消失。在这篇论文中，我们提出了一种简单又有效的方法，称为BEVHeight++。其核心思想是通过对地面高度进行回归，以实现距离agnostic的表示，从而简化了Camera-only感知方法的优化过程。通过结合高度和深度编码技术，我们实现了更加准确和Robust的2D到BEV空间投影。在popular 3D探测 benchmarks of roadside cameras 上，我们的方法比之前的视觉中心方法高出了显著的margin。在ego-vehicleenario中，我们的BEVHeight++也表现出了明显的优势， Specifically，它在nuScenes验证集上提高了+1.9% NDS和+1.1% mAP，并在nuScenes测试集上实现了重大的进步，升幅+2.8% NDS和+1.7% mAP。
</details></li>
</ul>
<hr>
<h2 id="ELIP-Efficient-Language-Image-Pre-training-with-Fewer-Vision-Tokens"><a href="#ELIP-Efficient-Language-Image-Pre-training-with-Fewer-Vision-Tokens" class="headerlink" title="ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens"></a>ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16738">http://arxiv.org/abs/2309.16738</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guoyang9/elip">https://github.com/guoyang9/elip</a></li>
<li>paper_authors: Yangyang Guo, Haoyu Zhang, Liqiang Nie, Yongkang Wong, Mohan Kankanhalli</li>
<li>for: 降低计算成本和占用空间，提高语言-图像模型的灵活性。</li>
<li>methods: 提出了一种视觉Token采样和合并方法（ELIP），根据语言输出进行不同级别的Token采样和合并，以提高模型的计算效率和存储效率。</li>
<li>results: 在三种常用的语言-图像预训练模型中实现了相对较少的性能损失（均为0.32减少率），并且可以通过增加更大的批处理大小来加速模型预训练和下游任务表现。<details>
<summary>Abstract</summary>
Learning a versatile language-image model is computationally prohibitive under a limited computing budget. This paper delves into the efficient language-image pre-training, an area that has received relatively little attention despite its importance in reducing computational cost and footprint. To that end, we propose a vision token pruning and merging method, ie ELIP, to remove less influential tokens based on the supervision of language outputs. Our method is designed with several strengths, such as being computation-efficient, memory-efficient, and trainable-parameter-free, and is distinguished from previous vision-only token pruning approaches by its alignment with task objectives. We implement this method in a progressively pruning manner using several sequential blocks. To evaluate its generalization performance, we apply ELIP to three commonly used language-image pre-training models and utilize public image-caption pairs with 4M images for pre-training. Our experiments demonstrate that with the removal of ~30$\%$ vision tokens across 12 ViT layers, ELIP maintains significantly comparable performance with baselines ($\sim$0.32 accuracy drop on average) over various downstream tasks including cross-modal retrieval, VQA, image captioning, etc. In addition, the spared GPU resources by our ELIP allow us to scale up with larger batch sizes, thereby accelerating model pre-training and even sometimes enhancing downstream model performance. Our code will be released at https://github.com/guoyang9/ELIP.
</details>
<details>
<summary>摘要</summary>
学习一种多面语言模型 computationally prohibitive under a limited computing budget. This paper explores the efficient language-image pre-training, an area that has received relatively little attention despite its importance in reducing computational cost and footprint. To that end, we propose a vision token pruning and merging method, namely ELIP, to remove less influential tokens based on the supervision of language outputs. Our method is designed with several strengths, such as being computation-efficient, memory-efficient, and trainable-parameter-free, and is distinguished from previous vision-only token pruning approaches by its alignment with task objectives. We implement this method in a progressively pruning manner using several sequential blocks. To evaluate its generalization performance, we apply ELIP to three commonly used language-image pre-training models and utilize public image-caption pairs with 4M images for pre-training. Our experiments demonstrate that with the removal of ~30% vision tokens across 12 ViT layers, ELIP maintains significantly comparable performance with baselines (approximately 0.32 accuracy drop on average) over various downstream tasks including cross-modal retrieval, VQA, image captioning, etc. In addition, the spared GPU resources by our ELIP allow us to scale up with larger batch sizes, thereby accelerating model pre-training and even sometimes enhancing downstream model performance. Our code will be released at https://github.com/guoyang9/ELIP.
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Terminate-in-Object-Navigation"><a href="#Learning-to-Terminate-in-Object-Navigation" class="headerlink" title="Learning to Terminate in Object Navigation"></a>Learning to Terminate in Object Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16164">http://arxiv.org/abs/2309.16164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huskykingdom/dita_acml2023">https://github.com/huskykingdom/dita_acml2023</a></li>
<li>paper_authors: Yuhang Song, Anh Nguyen, Chun-Yi Lee</li>
<li>for: 这个论文主要针对 autonomous navigation 系统中的目标接近和 episoden 终止问题，尤其是在深度学习（Deep Reinforcement Learning，DRL）基于方法中，通常lack depth信息，导致优化路径规划和终止识别困难。</li>
<li>methods: 我们提出了一种新的方法，即 Depth-Inference Termination Agent（DITA），它利用一个名为 Judge Model 的监管模型来隐式地推断目标的深度信息，并与 reinforcement learning 结合决策终止。我们在 Judge Model 和 reinforcement learning 的训练过程中同时进行了监控和激励，以高效地训练 Judge Model。</li>
<li>results: 我们的评估结果表明，DITA 方法在所有房间类型上都表现出优秀的成绩，相比基eline方法，DITA 方法在长期epsisode环境中提高了51.2%的成绩，同时保持了 slighty better Success Weighted by Path Length（SPL）。 Code 和资源可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/HuskyKingdom/DITA_acml2023%E3%80%82">https://github.com/HuskyKingdom/DITA_acml2023。</a><details>
<summary>Abstract</summary>
This paper tackles the critical challenge of object navigation in autonomous navigation systems, particularly focusing on the problem of target approach and episode termination in environments with long optimal episode length in Deep Reinforcement Learning (DRL) based methods. While effective in environment exploration and object localization, conventional DRL methods often struggle with optimal path planning and termination recognition due to a lack of depth information. To overcome these limitations, we propose a novel approach, namely the Depth-Inference Termination Agent (DITA), which incorporates a supervised model called the Judge Model to implicitly infer object-wise depth and decide termination jointly with reinforcement learning. We train our judge model along with reinforcement learning in parallel and supervise the former efficiently by reward signal. Our evaluation shows the method is demonstrating superior performance, we achieve a 9.3% gain on success rate than our baseline method across all room types and gain 51.2% improvements on long episodes environment while maintaining slightly better Success Weighted by Path Length (SPL). Code and resources, visualization are available at: https://github.com/HuskyKingdom/DITA_acml2023
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OSM-Net-One-to-Many-One-shot-Talking-Head-Generation-with-Spontaneous-Head-Motions"><a href="#OSM-Net-One-to-Many-One-shot-Talking-Head-Generation-with-Spontaneous-Head-Motions" class="headerlink" title="OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous Head Motions"></a>OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous Head Motions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16148">http://arxiv.org/abs/2309.16148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, Jizhong Han</li>
<li>for: 一种一批讲话头生成任务，即一个人说话时的自然头部运动生成。</li>
<li>methods: 提出了一种基于多个clip级别的头部运动特征空间的一批讲话头生成网络（OSM-Net），可以自然地模拟人类的头部运动。</li>
<li>results: 对比其他方法，OSM-Net能够更自然地生成真实的讲话头部运动，并且可以在一个合理的一对多映射方式下进行映射。<details>
<summary>Abstract</summary>
One-shot talking head generation has no explicit head movement reference, thus it is difficult to generate talking heads with head motions. Some existing works only edit the mouth area and generate still talking heads, leading to unreal talking head performance. Other works construct one-to-one mapping between audio signal and head motion sequences, introducing ambiguity correspondences into the mapping since people can behave differently in head motions when speaking the same content. This unreasonable mapping form fails to model the diversity and produces either nearly static or even exaggerated head motions, which are unnatural and strange. Therefore, the one-shot talking head generation task is actually a one-to-many ill-posed problem and people present diverse head motions when speaking. Based on the above observation, we propose OSM-Net, a \textit{one-to-many} one-shot talking head generation network with natural head motions. OSM-Net constructs a motion space that contains rich and various clip-level head motion features. Each basis of the space represents a feature of meaningful head motion in a clip rather than just a frame, thus providing more coherent and natural motion changes in talking heads. The driving audio is mapped into the motion space, around which various motion features can be sampled within a reasonable range to achieve the one-to-many mapping. Besides, the landmark constraint and time window feature input improve the accurate expression feature extraction and video generation. Extensive experiments show that OSM-Net generates more natural realistic head motions under reasonable one-to-many mapping paradigm compared with other methods.
</details>
<details>
<summary>摘要</summary>
一般来说，一帧说话头生成没有显式的头部运动参考，因此很难生成带有头部运动的说话头。现有的方法只是编辑口部区域，生成不动的说话头，这会导致不自然的表演。其他方法建立了一对一的声音信号和头部运动序列之间的映射，但这种映射存在不合理的匹配问题，人们在说同一个内容时可能会有不同的头部运动，这会导致生成的头部运动不自然。因此，一帧说话头生成任务实际上是一个一对多不定 problema，人们在说话时会表现出多种头部运动。根据上述观察，我们提出了OSM-Net，一种基于声音信号的一对多一帧说话头生成网络，具有自然的头部运动。OSM-Net建立了一个运动空间，该空间包含了clip级别的多种有意义的头部运动特征。每个基于空间的基准代表了一个clip中的有意义的头部运动特征，而不是只是一帧。这样，在 talking heads 中可以更自然地实现多种头部运动。另外，附加的标记约束和时间窗口特征输入，可以提高准确地EXTRACT特征和视频生成。实验表明，OSM-Net可以在合理的一对多映射模型下，比其他方法更自然地生成真实的头部运动。
</details></li>
</ul>
<hr>
<h2 id="Align-before-Search-Aligning-Ads-Image-to-Text-for-Accurate-Cross-Modal-Sponsored-Search"><a href="#Align-before-Search-Aligning-Ads-Image-to-Text-for-Accurate-Cross-Modal-Sponsored-Search" class="headerlink" title="Align before Search: Aligning Ads Image to Text for Accurate Cross-Modal Sponsored Search"></a>Align before Search: Aligning Ads Image to Text for Accurate Cross-Modal Sponsored Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16141">http://arxiv.org/abs/2309.16141</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pter61/aligncmss">https://github.com/pter61/aligncmss</a></li>
<li>paper_authors: Yuanmin Tang, Jing Yu, Keke Gai, Yujing Wang, Yue Hu, Gang Xiong, Qi Wu</li>
<li>for: 这 paper 是关于 cross-modal sponsored search 的研究，用于提高搜索引擎中的广告搜索效果。</li>
<li>methods: 该 paper 使用了一种简单的对应网络，将细部视觉部分在广告图像中与相应的文本进行Explicit地映射，不需要贵重的标注训练数据。此外，paper 还提出了一种新的模型，可以有效地进行cross-modal对应和广告搜索，只需要半个训练数据。</li>
<li>results:  compared with state-of-the-art 模型，该 paper 的模型在大规模商业数据集上表现出优于2.57%的提升。此外，paper 还研究了一种常见的cross-modal检索任务，在 MSCOCO 数据集上达到了一致性的性能提升，证明了该方法的通用性。<details>
<summary>Abstract</summary>
Cross-Modal sponsored search displays multi-modal advertisements (ads) when consumers look for desired products by natural language queries in search engines. Since multi-modal ads bring complementary details for query-ads matching, the ability to align ads-specific information in both images and texts is crucial for accurate and flexible sponsored search. Conventional research mainly studies from the view of modeling the implicit correlations between images and texts for query-ads matching, ignoring the alignment of detailed product information and resulting in suboptimal search performance.In this work, we propose a simple alignment network for explicitly mapping fine-grained visual parts in ads images to the corresponding text, which leverages the co-occurrence structure consistency between vision and language spaces without requiring expensive labeled training data. Moreover, we propose a novel model for cross-modal sponsored search that effectively conducts the cross-modal alignment and query-ads matching in two separate processes. In this way, the model matches the multi-modal input in the same language space, resulting in a superior performance with merely half of the training data. Our model outperforms the state-of-the-art models by 2.57% on a large commercial dataset. Besides sponsored search, our alignment method is applicable for general cross-modal search. We study a typical cross-modal retrieval task on the MSCOCO dataset, which achieves consistent performance improvement and proves the generalization ability of our method. Our code is available at https://github.com/Pter61/AlignCMSS/
</details>
<details>
<summary>摘要</summary>
协助搜索赞助显示多 modal 广告 (广告) 当消费者通过自然语言查询在搜索引擎中寻找需要的产品。自然modal 广告的匹配需要精细的产品信息的对应，因此在搜索中准确地将广告信息与搜索结果相对应是非常重要。现有研究主要从视图模型多 modal 空间中的隐式相关性来研究查询广告匹配，忽略了细节产品信息的对应，从而导致搜索性能下降。在这种情况下，我们提出了一个简单的对应网络，用于显式地将多 modal 广告图像中细节部分与相应的文本对应。我们还提出了一种新的模型，可以有效地进行交互模式的搜索和广告匹配。在这种方式下，模型将多 modal 输入在同一个语言空间中匹配，从而提高搜索性能，只需要半数据进行训练。我们的模型在大规模商业数据集上超过了状态艺术模型的性能，提高了2.57%。此外，我们的对应方法可以应用于普通的交互模式搜索。我们在 MSCOCO 数据集上进行了一般交互模式搜索任务，实现了一致性提高和普适性。我们的代码可以在 <https://github.com/Pter61/AlignCMSS/> 中找到。
</details></li>
</ul>
<hr>
<h2 id="CLIP-Hand3D-Exploiting-3D-Hand-Pose-Estimation-via-Context-Aware-Prompting"><a href="#CLIP-Hand3D-Exploiting-3D-Hand-Pose-Estimation-via-Context-Aware-Prompting" class="headerlink" title="CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware Prompting"></a>CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16140">http://arxiv.org/abs/2309.16140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoxiang Guo, Qing Cai, Lin Qi, Junyu Dong</li>
<li>for: This paper proposes a novel 3D hand pose estimator from monocular images, which can successfully bridge the gap between text prompts and irregular detailed pose distribution.</li>
<li>methods: The proposed model uses a CLIP-based contrastive learning paradigm, which encodes pose-aware features and maximizes semantic consistency for a pair of pose-text features. A coarse-to-fine mesh regressor is also designed to effectively query joint-aware cues from the feature pyramid.</li>
<li>results: The proposed model achieves a significantly faster inference speed while achieving state-of-the-art performance compared to methods utilizing the similar scale backbone, on several public hand benchmarks.<details>
<summary>Abstract</summary>
Contrastive Language-Image Pre-training (CLIP) starts to emerge in many computer vision tasks and has achieved promising performance. However, it remains underexplored whether CLIP can be generalized to 3D hand pose estimation, as bridging text prompts with pose-aware features presents significant challenges due to the discrete nature of joint positions in 3D space. In this paper, we make one of the first attempts to propose a novel 3D hand pose estimator from monocular images, dubbed as CLIP-Hand3D, which successfully bridges the gap between text prompts and irregular detailed pose distribution. In particular, the distribution order of hand joints in various 3D space directions is derived from pose labels, forming corresponding text prompts that are subsequently encoded into text representations. Simultaneously, 21 hand joints in the 3D space are retrieved, and their spatial distribution (in x, y, and z axes) is encoded to form pose-aware features. Subsequently, we maximize semantic consistency for a pair of pose-text features following a CLIP-based contrastive learning paradigm. Furthermore, a coarse-to-fine mesh regressor is designed, which is capable of effectively querying joint-aware cues from the feature pyramid. Extensive experiments on several public hand benchmarks show that the proposed model attains a significantly faster inference speed while achieving state-of-the-art performance compared to methods utilizing the similar scale backbone.
</details>
<details>
<summary>摘要</summary>
《对比 язы言-图像预训（CLIP）在许多计算机视觉任务中开始出现，并已经实现了有前途的性能。然而，尚未被完善地探索CLIP是否可以泛化到3D手姿估计任务中，因为将文本提示与具有手姿特征的特征相连接具有显著挑战，因为手姿 JOINT 的位置是离散的。在这篇论文中，我们提出了一种基于 CLIP 的新的3D手姿估计器从单视图图像，称为 CLIP-Hand3D，成功地跨越了文本提示和不规则细节姿态的差异。具体来说，根据手姿标签， derive 手姿 JOINT 的分布顺序在不同的3D空间方向，并将其转化为对应的文本提示。同时，从单视图图像中提取出21个手姿 JOINT，并将其在 x、y 和 z 轴上的空间分布编码成具有手姿特征的特征。接着，我们在 CLIP 基于对比学习模式下maximize  semantic consistency，以便将文本提示与手姿特征相关联。此外，我们还设计了一种粗细至细网络回归器，可以有效地从特征 пирамид中提取手姿特征。我们在多个公共手姿标准 benchmark 上进行了广泛的实验，结果显示，我们的模型在相同的规模下实现了比较快的推理速度，同时保持了与相似规模的方法相比的状态体现。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well.
</details></li>
</ul>
<hr>
<h2 id="Two-Step-Active-Learning-for-Instance-Segmentation-with-Uncertainty-and-Diversity-Sampling"><a href="#Two-Step-Active-Learning-for-Instance-Segmentation-with-Uncertainty-and-Diversity-Sampling" class="headerlink" title="Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling"></a>Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16139">http://arxiv.org/abs/2309.16139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ke Yu, Stephen Albro, Giulia DeSalvo, Suraj Kothawade, Abdullah Rashwan, Sasan Tavakkol, Kayhan Batmanghelich, Xiaoqi Yin</li>
<li>for: 提高INSTANCE segmentation模型的训练质量，降低标注成本。</li>
<li>methods:  integrate uncertainty-based sampling with diversity-based sampling，实现简单、易于实现，且能够提供优秀表现。</li>
<li>results: 在多个dataset上实现五倍的标注效率提升。<details>
<summary>Abstract</summary>
Training high-quality instance segmentation models requires an abundance of labeled images with instance masks and classifications, which is often expensive to procure. Active learning addresses this challenge by striving for optimum performance with minimal labeling cost by selecting the most informative and representative images for labeling. Despite its potential, active learning has been less explored in instance segmentation compared to other tasks like image classification, which require less labeling. In this study, we propose a post-hoc active learning algorithm that integrates uncertainty-based sampling with diversity-based sampling. Our proposed algorithm is not only simple and easy to implement, but it also delivers superior performance on various datasets. Its practical application is demonstrated on a real-world overhead imagery dataset, where it increases the labeling efficiency fivefold.
</details>
<details>
<summary>摘要</summary>
培训高质量的实例分割模型需要很多标注图像和类别标注，这经常是昂贵的。活动学习可以解决这个挑战，它寻求最优性和最小的标注成本，通过选择最有用和最 represetative 的图像进行标注。虽然活动学习在实例分割方面得到了更多的探索，但它在其他任务如图像分类中得到了更多的应用。在这种研究中，我们提出了一种post-hoc的活动学习算法，它结合了不确定性基于的采样和多样性基于的采样。我们的提出的算法不仅简单易于实现，而且可以提供更高的性能，在多个数据集上进行了证明。在一个真实的飞行图像数据集上，我们示出了它可以提高标注效率五倍。
</details></li>
</ul>
<hr>
<h2 id="Context-I2W-Mapping-Images-to-Context-dependent-Words-for-Accurate-Zero-Shot-Composed-Image-Retrieval"><a href="#Context-I2W-Mapping-Images-to-Context-dependent-Words-for-Accurate-Zero-Shot-Composed-Image-Retrieval" class="headerlink" title="Context-I2W: Mapping Images to Context-dependent Words for Accurate Zero-Shot Composed Image Retrieval"></a>Context-I2W: Mapping Images to Context-dependent Words for Accurate Zero-Shot Composed Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16137">http://arxiv.org/abs/2309.16137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pter61/context_i2w">https://github.com/pter61/context_i2w</a></li>
<li>paper_authors: Yuanmin Tang, Jing Yu, Keke Gai, Zhuang Jiamin, Gang Xiong, Yue Hu, Qi Wu</li>
<li>for:  zeroshot composed image retrieval tasks (ZS-CIR)</li>
<li>methods:  context-dependent mapping network (Context-I2W) with intent view selector and visual target extractor</li>
<li>results:  strong generalization ability and significant performance boosts on four ZS-CIR tasks, achieving new state-of-the-art results<details>
<summary>Abstract</summary>
Different from Composed Image Retrieval task that requires expensive labels for training task-specific models, Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a broad range of visual content manipulation intent that could be related to domain, scene, object, and attribute. The key challenge for ZS-CIR tasks is to learn a more accurate image representation that has adaptive attention to the reference image for various manipulation descriptions. In this paper, we propose a novel context-dependent mapping network, named Context-I2W, for adaptively converting description-relevant Image information into a pseudo-word token composed of the description for accurate ZS-CIR. Specifically, an Intent View Selector first dynamically learns a rotation rule to map the identical image to a task-specific manipulation view. Then a Visual Target Extractor further captures local information covering the main targets in ZS-CIR tasks under the guidance of multiple learnable queries. The two complementary modules work together to map an image to a context-dependent pseudo-word token without extra supervision. Our model shows strong generalization ability on four ZS-CIR tasks, including domain conversion, object composition, object manipulation, and attribute manipulation. It obtains consistent and significant performance boosts ranging from 1.88% to 3.60% over the best methods and achieves new state-of-the-art results on ZS-CIR. Our code is available at https://github.com/Pter61/context_i2w.
</details>
<details>
<summary>摘要</summary>
不同于需要特殊标注的图像检索任务（Composed Image Retrieval），零值组合图像检索（ZS-CIR）涉及到多样化的视觉内容操作意图，包括域、场景、对象和特征等。 ZS-CIR 任务的关键挑战是学习更加准确的图像表示，以适应不同的描述映射。在这篇论文中，我们提出了一种新的上下文依赖的映射网络，即 Context-I2W，用于适应描述相关的图像信息转换为一个上下文依赖的伪词表示。具体来说，首先是动态学习的意图规则，用于将同一张图像映射到任务特定的操作视图。然后是一个可学习的查询器，用于在 ZS-CIR 任务中捕捉主要目标的本地信息。这两个补做模块共同工作，无需额外监督，可以将图像映射到上下文依赖的伪词表示。我们的模型在四个 ZS-CIR 任务上表现出了强大的泛化能力，对比最佳方法提高了1.88%到3.60%的表现，并实现了 ZS-CIR 领域的新state-of-the-art 成绩。我们的代码可以在 GitHub 上找到：https://github.com/Pter61/context_i2w。
</details></li>
</ul>
<hr>
<h2 id="A-dual-branch-model-with-inter-and-intra-branch-contrastive-loss-for-long-tailed-recognition"><a href="#A-dual-branch-model-with-inter-and-intra-branch-contrastive-loss-for-long-tailed-recognition" class="headerlink" title="A dual-branch model with inter- and intra-branch contrastive loss for long-tailed recognition"></a>A dual-branch model with inter- and intra-branch contrastive loss for long-tailed recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16135">http://arxiv.org/abs/2309.16135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiong Chen, Tianlin Huang, Geren Zhu, Enlu Lin</li>
<li>for: 实际世界数据频繁地呈长尾分布，主要是因为主要类别占据大量数据，而末端类别仅有几个样本。因此，这篇论文提出了一个简单又有效的模型，名为双支分支长尾识别（DB-LTR），它包括不均衡学习分支和对比学习分支（CoLB）。</li>
<li>methods: 双支分支长尾识别模型包括一个共享后ION和一个线性分类器，利用常见的不均衡学习方法来解决数据不均衡问题。CoLB learns一个每个末端类别的原型，并计算了一个间支比例损失、一个内支比例损失和一个度量损失。</li>
<li>results: 实验结果显示，我们的DB-LTR在三个长尾标准资料集（CIFAR100-LT、ImageNet-LT和Places-LT）上具有竞争力和优势，与比较方法相比。<details>
<summary>Abstract</summary>
Real-world data often exhibits a long-tailed distribution, in which head classes occupy most of the data, while tail classes only have very few samples. Models trained on long-tailed datasets have poor adaptability to tail classes and the decision boundaries are ambiguous. Therefore, in this paper, we propose a simple yet effective model, named Dual-Branch Long-Tailed Recognition (DB-LTR), which includes an imbalanced learning branch and a Contrastive Learning Branch (CoLB). The imbalanced learning branch, which consists of a shared backbone and a linear classifier, leverages common imbalanced learning approaches to tackle the data imbalance issue. In CoLB, we learn a prototype for each tail class, and calculate an inter-branch contrastive loss, an intra-branch contrastive loss and a metric loss. CoLB can improve the capability of the model in adapting to tail classes and assist the imbalanced learning branch to learn a well-represented feature space and discriminative decision boundary. Extensive experiments on three long-tailed benchmark datasets, i.e., CIFAR100-LT, ImageNet-LT and Places-LT, show that our DB-LTR is competitive and superior to the comparative methods.
</details>
<details>
<summary>摘要</summary>
实际世界数据经常展现长尾分布，其中头类占据大量数据，而尾类只有很少的样本。模型在长尾数据集上训练时，对尾类的适应性差，决策界限抖音。因此，在这篇论文中，我们提出了一种简单又有效的模型，名为双支分支长尾识别（DB-LTR）。该模型包括一个共享背bone和一个线性分类器，这两个分支都可以使用常见的不均衡学习方法来处理数据不均衡问题。在CoLB分支中，我们学习了每个尾类的原型，并计算了 между支contrastive损失、内支contrastive损失和一个度量损失。CoLB可以帮助模型更好地适应尾类，并帮助不均衡学习分支学习一个准确的特征空间和决策界限。我们在CIFAR100-LT、ImageNet-LT和Places-LT三个长尾 benchmark数据集上进行了广泛的实验，结果显示，我们的DB-LTR在比较方法中具有竞争力和超越性。
</details></li>
</ul>
<hr>
<h2 id="MASK4D-Mask-Transformer-for-4D-Panoptic-Segmentation"><a href="#MASK4D-Mask-Transformer-for-4D-Panoptic-Segmentation" class="headerlink" title="MASK4D: Mask Transformer for 4D Panoptic Segmentation"></a>MASK4D: Mask Transformer for 4D Panoptic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16133">http://arxiv.org/abs/2309.16133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kadir Yilmaz, Jonas Schult, Alexey Nekrasov, Bastian Leibe<br>for: 本研究旨在提高自适应Agent在动态环境中做出正确决策的能力，通过提出Mask4D模型来实现4D精准分割 LiDAR 点云 sequences。methods: Mask4D 是首个将 semantic instance segmentation 和 trackig 紧密融合到一起的 transformer-based 模型，直接预测 semantic instances 和其时间关系，不需要手动设计非学习的关联策略。results: Mask4D 在 SemanticKITTI 测试集上达到了新的状态对，得分 68.4 LSTQ，至少超过 +4.5% 于已发表的顶峰性能方法。<details>
<summary>Abstract</summary>
Accurately perceiving and tracking instances over time is essential for the decision-making processes of autonomous agents interacting safely in dynamic environments. With this intention, we propose Mask4D for the challenging task of 4D panoptic segmentation of LiDAR point clouds. Mask4D is the first transformer-based approach unifying semantic instance segmentation and tracking of sparse and irregular sequences of 3D point clouds into a single joint model. Our model directly predicts semantic instances and their temporal associations without relying on any hand-crafted non-learned association strategies such as probabilistic clustering or voting-based center prediction. Instead, Mask4D introduces spatio-temporal instance queries which encode the semantic and geometric properties of each semantic tracklet in the sequence. In an in-depth study, we find that it is critical to promote spatially compact instance predictions as spatio-temporal instance queries tend to merge multiple semantically similar instances, even if they are spatially distant. To this end, we regress 6-DOF bounding box parameters from spatio-temporal instance queries, which is used as an auxiliary task to foster spatially compact predictions. Mask4D achieves a new state-of-the-art on the SemanticKITTI test set with a score of 68.4 LSTQ, improving upon published top-performing methods by at least +4.5%.
</details>
<details>
<summary>摘要</summary>
为了安全地在动态环境中交互，自动化代理需要精准地掌握和跟踪实例过程。为此目的，我们提出Mask4D，一种用于4D�anoptic分割 LiDAR 点云的挑战性任务。Mask4D 是首个基于 transformer 的方法，将 semantic instance segmentation 和 sparse 和不规则的3D点云序列中的实例跟踪合并到一个共同模型中。我们的模型直接预测 semantic instance 和其时间关联，不需要手动设置非学习的关联策略，如概率 clustering 或投票式中心预测。相反，Mask4D 引入 spatio-temporal instance queries，这些查询编码了每个 semantic tracklet 的 semantic 和geometry 属性。在深入研究中，我们发现需要促进空间紧凑的实例预测，因为 spatio-temporal instance queries 往往将多个semantically similar instance合并，即使它们在空间上远离。为此，我们回归 6-DOF 矩阵参数，用于提高空间紧凑的预测。Mask4D 在 SemanticKITTI 测试集上达到了新的州OF-the-art 分数68.4 LSTQ，超过了已发表的最高表现方法至少+4.5%。
</details></li>
</ul>
<hr>
<h2 id="Joint-Correcting-and-Refinement-for-Balanced-Low-Light-Image-Enhancement"><a href="#Joint-Correcting-and-Refinement-for-Balanced-Low-Light-Image-Enhancement" class="headerlink" title="Joint Correcting and Refinement for Balanced Low-Light Image Enhancement"></a>Joint Correcting and Refinement for Balanced Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16128">http://arxiv.org/abs/2309.16128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/woshiyll/jcrnet">https://github.com/woshiyll/jcrnet</a></li>
<li>paper_authors: Nana Yu, Hong Shi, Yahong Han</li>
<li>for: 提高低光照图像的增强，以达到更好的平衡 amongst 亮度、颜色和照明。</li>
<li>methods: 提议一种新的合成结构，即 JOINT CORRECTING AND REFINMENT NETWORK (JCRNet)，通过三个阶段来更好地均衡增强中的亮度、颜色和照明。</li>
<li>results: 与21种 state-of-the-art 方法进行比较，JCRNet 在 9 个标准测试集上展现出了广泛的性能优势，并在下游视觉任务中（例如精度检测）也得到了更好的结果。<details>
<summary>Abstract</summary>
Low-light image enhancement tasks demand an appropriate balance among brightness, color, and illumination. While existing methods often focus on one aspect of the image without considering how to pay attention to this balance, which will cause problems of color distortion and overexposure etc. This seriously affects both human visual perception and the performance of high-level visual models. In this work, a novel synergistic structure is proposed which can balance brightness, color, and illumination more effectively. Specifically, the proposed method, so-called Joint Correcting and Refinement Network (JCRNet), which mainly consists of three stages to balance brightness, color, and illumination of enhancement. Stage 1: we utilize a basic encoder-decoder and local supervision mechanism to extract local information and more comprehensive details for enhancement. Stage 2: cross-stage feature transmission and spatial feature transformation further facilitate color correction and feature refinement. Stage 3: we employ a dynamic illumination adjustment approach to embed residuals between predicted and ground truth images into the model, adaptively adjusting illumination balance. Extensive experiments demonstrate that the proposed method exhibits comprehensive performance advantages over 21 state-of-the-art methods on 9 benchmark datasets. Furthermore, a more persuasive experiment has been conducted to validate our approach the effectiveness in downstream visual tasks (e.g., saliency detection). Compared to several enhancement models, the proposed method effectively improves the segmentation results and quantitative metrics of saliency detection. The source code will be available at https://github.com/woshiyll/JCRNet.
</details>
<details>
<summary>摘要</summary>
低光照图像增强任务需要达到对比度、色彩和照明的平衡。现有方法通常强调一个图像方面，而忽视了如何更好地平衡这些方面，这会导致颜色扭曲和过度照明等问题。这会 серьез影响人类视觉和高级视觉模型的性能。在这种工作中，我们提出了一种新的同化结构，即 JOINT CORRECTING AND REFINEMENT NETWORK（JCRNet）。它主要由三个阶段组成，用于平衡图像的亮度、色彩和照明增强。Stage 1：我们使用基本的编解oder和本地监督机制，提取本地信息和更全面的细节，用于增强。Stage 2：跨阶段特征传输和空间特征变换，进一步促进颜色修正和特征细化。Stage 3：我们使用动态照明调整方法，将预测和真实图像之间的差异 embedding 到模型中，自适应调整照明平衡。广泛的实验表明，提出的方法在9个标准 datasets 上表现出了21种当前方法的综合性优势。此外，我们还进行了一项更加吸引人的实验，以验证我们的方法在下游视觉任务（例如分割检测）中的有效性。相比一些增强模型，我们的方法可以更好地提高分割结果和量化度量的分割检测结果。代码将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="Open-Compound-Domain-Adaptation-with-Object-Style-Compensation-for-Semantic-Segmentation"><a href="#Open-Compound-Domain-Adaptation-with-Object-Style-Compensation-for-Semantic-Segmentation" class="headerlink" title="Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation"></a>Open Compound Domain Adaptation with Object Style Compensation for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16127">http://arxiv.org/abs/2309.16127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingliang Feng, Hao Shi, Xueyang Liu, Wei Feng, Liang Wan, Yanlin Zhou, Di Lin</li>
<li>for: 这paper是为了提高 semantic image segmentation 的精度，使用 open compound domain adaptation 方法。</li>
<li>methods: 这paper使用 Object Style Compensation 方法，包括建立 Object-Level Discrepancy Memory，并学习多个类别或实例的样式差异特征。</li>
<li>results: 这paper在不同数据集上达到了 state-of-the-art 结果，提高了 semantic image segmentation 的精度。<details>
<summary>Abstract</summary>
Many methods of semantic image segmentation have borrowed the success of open compound domain adaptation. They minimize the style gap between the images of source and target domains, more easily predicting the accurate pseudo annotations for target domain's images that train segmentation network. The existing methods globally adapt the scene style of the images, whereas the object styles of different categories or instances are adapted improperly. This paper proposes the Object Style Compensation, where we construct the Object-Level Discrepancy Memory with multiple sets of discrepancy features. The discrepancy features in a set capture the style changes of the same category's object instances adapted from target to source domains. We learn the discrepancy features from the images of source and target domains, storing the discrepancy features in memory. With this memory, we select appropriate discrepancy features for compensating the style information of the object instances of various categories, adapting the object styles to a unified style of source domain. Our method enables a more accurate computation of the pseudo annotations for target domain's images, thus yielding state-of-the-art results on different datasets.
</details>
<details>
<summary>摘要</summary>
很多semantic image segmentation方法借鉴了开放复合领域适应的成功。它们减少了目标领域图像和源领域图像之间的风格差异，更容易预测目标领域图像的准确pseudo annotations，用于训练segmentation网络。现有方法通过全局适应场景风格的图像，而对不同类别或实例的物体风格适应不够。本文提出了对象风格补偿（Object Style Compensation），我们构建了对象级别差异记忆（Object-Level Discrepancy Memory），其中包含多个差异特征集。差异特征集中的差异特征捕捉了目标领域图像中同类对象实例的风格变化，从目标领域图像和源领域图像中学习差异特征，并将其存储在记忆中。通过这个记忆，我们可以选择合适的差异特征来补偿目标领域图像中各类对象的风格信息，使得对象风格统一到源领域的风格，从而实现更高精度的pseudo annotations计算，并在不同的 dataset 上达到状态�ayer的结果。
</details></li>
</ul>
<hr>
<h2 id="UVL-A-Unified-Framework-for-Video-Tampering-Localization"><a href="#UVL-A-Unified-Framework-for-Video-Tampering-Localization" class="headerlink" title="UVL: A Unified Framework for Video Tampering Localization"></a>UVL: A Unified Framework for Video Tampering Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16126">http://arxiv.org/abs/2309.16126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pengfei Pei, Xianfeng Zhao, Jinchuan Li, Yun Cao</li>
<li>for: 检测深度学习技术中的伪造影片</li>
<li>methods: 提出了一个统一的影片修剪探测框架（UVL），通过提取不同类型伪造影片的共同特征（边界artefacts、生成像素的不自然分布和伪造区域与原始影片的联合），以提高对未知影片的探测性能。</li>
<li>results: 在三种不同类型的伪造影片（影片填充、影片融合和DeepFake）的多种benchmark上，UVL achieve state-of-the-art的性能，并与现有方法比较，在跨 dataset 上大幅提高了性能。<details>
<summary>Abstract</summary>
With the development of deep learning technology, various forgery methods emerge endlessly. Meanwhile, methods to detect these fake videos have also achieved excellent performance on some datasets. However, these methods suffer from poor generalization to unknown videos and are inefficient for new forgery methods. To address this challenging problem, we propose UVL, a novel unified video tampering localization framework for synthesizing forgeries. Specifically, UVL extracts common features of synthetic forgeries: boundary artifacts of synthetic edges, unnatural distribution of generated pixels, and noncorrelation between the forgery region and the original. These features are widely present in different types of synthetic forgeries and help improve generalization for detecting unknown videos. Extensive experiments on three types of synthetic forgery: video inpainting, video splicing and DeepFake show that the proposed UVL achieves state-of-the-art performance on various benchmarks and outperforms existing methods by a large margin on cross-dataset.
</details>
<details>
<summary>摘要</summary>
随着深度学习技术的发展，各种假视频技术不断出现。同时，检测这些假视频的方法也达到了一定的表现水平在某些数据集上。然而，这些方法受到未知视频的泛化和新的假视频方法的挑战。为解决这个问题，我们提出了UVL，一种基于Synthetic Tampering Localization的新型视频假冒检测框架。具体来说，UVL提取了各种合成假视频的共同特征：合成边缘的边缘特征、生成像素的不自然分布和假冒区域与原始视频的无关性。这些特征广泛存在不同类型的合成假视频中，可以提高泛化性 для检测未知视频。我们在三种合成假视频：视频填充、视频融合和DeepFake上进行了广泛的实验，结果表明，提出的UVL在各种标准准点上达到了当前最佳性能，与现有方法相比，在跨数据集上表现出了明显的超越。
</details></li>
</ul>
<hr>
<h2 id="D-3-Fields-Dynamic-3D-Descriptor-Fields-for-Zero-Shot-Generalizable-Robotic-Manipulation"><a href="#D-3-Fields-Dynamic-3D-Descriptor-Fields-for-Zero-Shot-Generalizable-Robotic-Manipulation" class="headerlink" title="D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation"></a>D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16118">http://arxiv.org/abs/2309.16118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Wang, Zhuoran Li, Mingtong Zhang, Katherine Driggs-Campbell, Jiajun Wu, Li Fei-Fei, Yunzhu Li</li>
<li>for: 这篇论文的目的是提出一种新的Scene representation，以便在 robotic manipulation systems 中提高操作精度和普遍性。</li>
<li>methods: 这篇论文使用了 dynamic 3D descriptor fields，它们可以捕捉工作空间中的动态3D环境，并将 semantic features 和 instance masks 组合在一起。具体来说，它们将 arbitrary 3D points 投射到多视角的Visual observations 上，然后从基础模型中提取特征，并将其混合在一起。</li>
<li>results: 这篇论文的结果显示，使用 D$^3$Fields 可以实现零数据学习的 robotic manipulation tasks，并且与现有的 dense descriptors 相比，它们在普遍性和操作精度方面表现更好。<details>
<summary>Abstract</summary>
Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields - dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonstrate that D$^3$Fields are both generalizable and effective for zero-shot robotic manipulation tasks. In quantitative comparisons with state-of-the-art dense descriptors, such as Dense Object Nets and DINO, D$^3$Fields exhibit significantly better generalization abilities and manipulation accuracy.
</details>
<details>
<summary>摘要</summary>
scene表示有被 robotic manipulation system 的关键设计选择。理想的表示应该是3D、动态和semantic，以满足多样化的抓取任务的需求。然而，之前的工作通常缺乏这三个属性。在这项工作中，我们介绍了D$^3$Fields - 动态3D描述场。这些场景捕捉了下面环境的动态特征，并将 semantic feature和实例掩码编码在一起。具体来说，我们将工作空间中的arbitrary 3D点 Project onto multi-view 2D visual observation，并 interpolate基础模型中 derivated 特征。得到的混合描述场可以通过2D图像 with varied contexts、styles和instances进行灵活的目标规定。为了评估D$^3$Fields的有效性，我们将其应用到了各种 robotic manipulation任务中。通过在实际场景和 simulations 中进行了广泛的评估，我们证明了D$^3$Fields是both generalizable和effective zero-shot robotic manipulation任务中。在对比state-of-the-art dense descriptors，如Dense Object Nets和DINO，D$^3$Fields表现出了明显更好的总体化能力和抓取精度。
</details></li>
</ul>
<hr>
<h2 id="Learning-Effective-NeRFs-and-SDFs-Representations-with-3D-Generative-Adversarial-Networks-for-3D-Object-Generation-Technical-Report-for-ICCV-2023-OmniObject3D-Challenge"><a href="#Learning-Effective-NeRFs-and-SDFs-Representations-with-3D-Generative-Adversarial-Networks-for-3D-Object-Generation-Technical-Report-for-ICCV-2023-OmniObject3D-Challenge" class="headerlink" title="Learning Effective NeRFs and SDFs Representations with 3D Generative Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023 OmniObject3D Challenge"></a>Learning Effective NeRFs and SDFs Representations with 3D Generative Adversarial Networks for 3D Object Generation: Technical Report for ICCV 2023 OmniObject3D Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16110">http://arxiv.org/abs/2309.16110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Yang, Yibo Liu, Guile Wu, Tongtong Cao, Yuan Ren, Yang Liu, Bingbing Liu</li>
<li>for: 本文提出了一种解决ICCV 2023 OmniObject3D Challenge中3D对象生成 зада务的方法。</li>
<li>methods: 本文使用3D生成对抗网络（GANs）学习有效的NeRFs和SDFs表示，并通过label嵌入和颜色映射来同时训练不同的分类。</li>
<li>results: 本文通过使用only a few images of each object from a variety of classes来训练模型，而不是使用大量的图像或训练每个类的单独模型。这种管道可以优化3D对象生成模型。这个解决方案在ICCV 2023 OmniObject3D Challenge中得到了前三名的成绩。<details>
<summary>Abstract</summary>
In this technical report, we present a solution for 3D object generation of ICCV 2023 OmniObject3D Challenge. In recent years, 3D object generation has made great process and achieved promising results, but it remains a challenging task due to the difficulty of generating complex, textured and high-fidelity results. To resolve this problem, we study learning effective NeRFs and SDFs representations with 3D Generative Adversarial Networks (GANs) for 3D object generation. Specifically, inspired by recent works, we use the efficient geometry-aware 3D GANs as the backbone incorporating with label embedding and color mapping, which enables to train the model on different taxonomies simultaneously. Then, through a decoder, we aggregate the resulting features to generate Neural Radiance Fields (NeRFs) based representations for rendering high-fidelity synthetic images. Meanwhile, we optimize Signed Distance Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we observe that this model can be effectively trained with only a few images of each object from a variety of classes, instead of using a great number of images per object or training one model per class. With this pipeline, we can optimize an effective model for 3D object generation. This solution is one of the final top-3-place solutions in the ICCV 2023 OmniObject3D Challenge.
</details>
<details>
<summary>摘要</summary>
在这份技术报告中，我们介绍了一种用于ICCV 2023 OmniObject3D Challenge 3D物体生成的解决方案。在过去几年中，3D物体生成技术做出了很大的进步，但是仍然是一个挑战性的任务，因为生成复杂、Texture和高精度结果很难。为了解决这个问题，我们研究了使用3D生成抗恐网络（GANs）来学习有效的NeRFs和SDFs表示。具体来说，我们采用了效果性的geometry-aware 3D GANs作为后续，并结合标签嵌入和颜色映射，以便同时训练不同的分类。然后，通过一个解码器，我们将结果综合到generate Neural Radiance Fields（NeRFs）基于表示，以便生成高品质的 sintetic 图像。同时，我们优化Signed Distance Functions（SDFs），以有效地表示物体使用3D矩阵。此外，我们发现这种模型可以通过只使用每个物体的几张图像进行训练，而不需要使用大量的图像或训练每个分类的模型。这种管道可以优化一个有效的3D物体生成模型。这个解决方案在ICCV 2023 OmniObject3D Challenge中排名前三名。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/28/cs.CV_2023_09_28/" data-id="cloqtaerg00jbgh88gssie0sz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/28/cs.AI_2023_09_28/" class="article-date">
  <time datetime="2023-09-28T12:00:00.000Z" itemprop="datePublished">2023-09-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/28/cs.AI_2023_09_28/">cs.AI - 2023-09-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Sourcing-Investment-Targets-for-Venture-and-Growth-Capital-Using-Multivariate-Time-Series-Transformer"><a href="#Sourcing-Investment-Targets-for-Venture-and-Growth-Capital-Using-Multivariate-Time-Series-Transformer" class="headerlink" title="Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer"></a>Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16888">http://arxiv.org/abs/2309.16888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lele Cao, Gustaf Halvardsson, Andrew McCornack, Vilhelm von Ehrenheim, Pawel Herman</li>
<li>For: 本研究探讨了private equity(PE)行业中数据驱动方法的应用，尤其是venture capital(VC)和growth capital(GC)投资目标的选择。* Methods: 我们提出了一种使用Transformer-based Multivariate Time Series Classifier(TMTSC)来预测候选公司的成功可能性。我们还介绍了我们的实现方式，包括输入特征、模型架构、优化目标和投资者中心的数据增强和分割。* Results: 我们的实验结果表明，我们的方法可以在四个数据集上实现显著的提升，相比三种常见基准。<details>
<summary>Abstract</summary>
This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrate the effectiveness of our approach in improving decision making within the VC and GC industry.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Investigating-Human-Identifiable-Features-Hidden-in-Adversarial-Perturbations"><a href="#Investigating-Human-Identifiable-Features-Hidden-in-Adversarial-Perturbations" class="headerlink" title="Investigating Human-Identifiable Features Hidden in Adversarial Perturbations"></a>Investigating Human-Identifiable Features Hidden in Adversarial Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16878">http://arxiv.org/abs/2309.16878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dennis Y. Menn, Tzu-hsun Feng, Sriram Vishwanath, Hung-yi Lee</li>
<li>for: 本研究探讨了神经网络在针对性攻击下的抵触性问题，以帮助更好地理解神经网络在实际应用中的潜在漏洞。</li>
<li>methods: 本研究使用了多种攻击算法，包括针对性攻击和无目标攻击，并在三个 dataset 上进行了实验。研究人员还使用了像素级标注来提取人类可识别的特征，并证明了这些特征可以妨碍目标模型。</li>
<li>results: 研究发现，针对性攻击和无目标攻击都会导致模型做出错误的预测，并且在不同的攻击算法下，perturbations 的特征存在一定的相似性。此外，研究还发现了两种不同的效果在人类可识别的特征中，其中隐藏效应更常见于无目标攻击中，而生成效应更常见于针对性攻击中。<details>
<summary>Abstract</summary>
Neural networks perform exceedingly well across various machine learning tasks but are not immune to adversarial perturbations. This vulnerability has implications for real-world applications. While much research has been conducted, the underlying reasons why neural networks fall prey to adversarial attacks are not yet fully understood. Central to our study, which explores up to five attack algorithms across three datasets, is the identification of human-identifiable features in adversarial perturbations. Additionally, we uncover two distinct effects manifesting within human-identifiable features. Specifically, the masking effect is prominent in untargeted attacks, while the generation effect is more common in targeted attacks. Using pixel-level annotations, we extract such features and demonstrate their ability to compromise target models. In addition, our findings indicate a notable extent of similarity in perturbations across different attack algorithms when averaged over multiple models. This work also provides insights into phenomena associated with adversarial perturbations, such as transferability and model interpretability. Our study contributes to a deeper understanding of the underlying mechanisms behind adversarial attacks and offers insights for the development of more resilient defense strategies for neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Preface-A-Data-driven-Volumetric-Prior-for-Few-shot-Ultra-High-resolution-Face-Synthesis"><a href="#Preface-A-Data-driven-Volumetric-Prior-for-Few-shot-Ultra-High-resolution-Face-Synthesis" class="headerlink" title="Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis"></a>Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16859">http://arxiv.org/abs/2309.16859</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/syntec-research/Preface">https://github.com/syntec-research/Preface</a></li>
<li>paper_authors: Marcel C. Bühler, Kripasindhu Sarkar, Tanmay Shah, Gengyan Li, Daoye Wang, Leonhard Helminger, Sergio Orts-Escolano, Dmitry Lagun, Otmar Hilliges, Thabo Beeler, Abhimitra Meka</li>
<li>for: 能够Synthesize high-resolution human faces with complex appearance and reflectance effects, including hair and skin.</li>
<li>methods: 使用了一种新的积体人脸先验模型，该模型基于一个identity-conditioned NeRF，通过一个简单的粗糙特征点基于的3D对称来学习一个平滑的积体geometry和外观空间，而不需要大量的多视图输入图像。</li>
<li>results: 可以从2或3个不同分辨率的相机视图中获取高品质积体人脸表示，只需要两个视图的捕捉图像作为输入。<details>
<summary>Abstract</summary>
NeRFs have enabled highly realistic synthesis of human faces including complex appearance and reflectance effects of hair and skin. These methods typically require a large number of multi-view input images, making the process hardware intensive and cumbersome, limiting applicability to unconstrained settings. We propose a novel volumetric human face prior that enables the synthesis of ultra high-resolution novel views of subjects that are not part of the prior's training distribution. This prior model consists of an identity-conditioned NeRF, trained on a dataset of low-resolution multi-view images of diverse humans with known camera calibration. A simple sparse landmark-based 3D alignment of the training dataset allows our model to learn a smooth latent space of geometry and appearance despite a limited number of training identities. A high-quality volumetric representation of a novel subject can be obtained by model fitting to 2 or 3 camera views of arbitrary resolution. Importantly, our method requires as few as two views of casually captured images as input at inference time.
</details>
<details>
<summary>摘要</summary>
NeRFs 已经实现了人脸的高真实感Synthesis，包括复杂的外观和反射效果。这些方法通常需要大量多视图输入图像，使得过程成为硬件昂贵和繁琐，限制了应用场景。我们提出了一个新的积分型人脸先验模型，允许synthesize高分辨率的新视图。这个模型由一个 conditioned NeRF 和一个 sparse landmark-based 3D 对齐组成。我们通过一个小量的训练数据进行了学习，并且可以通过两个或三个Camera视图的任意分辨率进行模型适应。关键是，我们的方法只需要两个或三个严格Captured图像作为输入。
</details></li>
</ul>
<hr>
<h2 id="Multi-Bellman-operator-for-convergence-of-Q-learning-with-linear-function-approximation"><a href="#Multi-Bellman-operator-for-convergence-of-Q-learning-with-linear-function-approximation" class="headerlink" title="Multi-Bellman operator for convergence of $Q$-learning with linear function approximation"></a>Multi-Bellman operator for convergence of $Q$-learning with linear function approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16819">http://arxiv.org/abs/2309.16819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diogo S. Carvalho, Pedro A. Santos, Francisco S. Melo</li>
<li>for: 本研究探讨$Q$-学习算法在线性函数近似下的收敛性。</li>
<li>methods: 本研究引入了一个新的多贝尔曼操作符，扩展了传统贝尔曼操作符的功能。通过研究这个操作符的性质，我们提出了一种基于多贝尔曼操作符的多$Q$-学习算法，并证明了这种算法可以在Fixed-point guarantees下提供更好的性能。</li>
<li>results: 我们通过应用这种算法于知名环境中，证明了我们的方法的有效性和实用性。<details>
<summary>Abstract</summary>
We study the convergence of $Q$-learning with linear function approximation. Our key contribution is the introduction of a novel multi-Bellman operator that extends the traditional Bellman operator. By exploring the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes contractive, providing improved fixed-point guarantees compared to the Bellman operator. To leverage these insights, we propose the multi $Q$-learning algorithm with linear function approximation. We demonstrate that this algorithm converges to the fixed-point of the projected multi-Bellman operator, yielding solutions of arbitrary accuracy. Finally, we validate our approach by applying it to well-known environments, showcasing the effectiveness and applicability of our findings.
</details>
<details>
<summary>摘要</summary>
我们研究 $Q$-学习的收敛性，尤其是使用线性函数 aproximation。我们的关键贡献是提出一种多重贝尔曼 операktor，将传统的贝尔曼 operator 扩展。通过研究这个操作符的性质，我们确定了条件下，其 projetced multi-Bellman operator 变得减少，从而提供更好的固定点保证。基于这些发现，我们提议 multi $Q$-学习算法，使用线性函数 aproximation。我们证明该算法收敛到多重贝尔曼 operator 的固定点，可以获得任意精度的解决方案。最后，我们验证了我们的方法，在知名环境中应用，展示了我们的发现的有效性和实用性。
</details></li>
</ul>
<hr>
<h2 id="De-SaTE-Denoising-Self-attention-Transformer-Encoders-for-Li-ion-Battery-Health-Prognostics"><a href="#De-SaTE-Denoising-Self-attention-Transformer-Encoders-for-Li-ion-Battery-Health-Prognostics" class="headerlink" title="De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics"></a>De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00023">http://arxiv.org/abs/2310.00023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Shinde, Rohan Mohapatra, Pooja Krishan, Saptarshi Sengupta</li>
<li>For: The paper aims to accurately predict the Remaining Useful Life (RUL) of Lithium Ion (Li-ion) batteries, which is critical for proactive maintenance and predictive analytics.* Methods: The paper proposes a novel approach that combines multiple denoising modules, including a denoising auto-encoder and a wavelet denoiser, to generate encoded&#x2F;decomposed representations of battery data. These representations are then processed through dedicated self-attention transformer encoders to estimate health indicators.* Results: The paper reports that the proposed approach can accurately estimate health indicators under a set of diverse noise patterns, with error metrics on par or better than the best reported in recent literature.<details>
<summary>Abstract</summary>
Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error metrics on these datasets are on par or better with the best reported in recent literature.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Promptbreeder-Self-Referential-Self-Improvement-Via-Prompt-Evolution"><a href="#Promptbreeder-Self-Referential-Self-Improvement-Via-Prompt-Evolution" class="headerlink" title="Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"></a>Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16797">http://arxiv.org/abs/2309.16797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, Tim Rocktäschel</li>
<li>for: This paper aims to improve the reasoning abilities of large language models (LLMs) in various domains by presenting a general-purpose self-referential self-improvement mechanism called Promptbreeder.</li>
<li>methods: Promptbreeder evolves and adapts prompts for a given domain by mutating a population of task-prompts, and subsequently evaluating them for fitness on a training set. The mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way.</li>
<li>results: Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks, and is able to evolve intricate task-prompts for the challenging problem of hate speech classification.<details>
<summary>Abstract</summary>
Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification.
</details>
<details>
<summary>摘要</summary>
受欢迎的提示策略如链条提示可以很大程度地提高大语言模型（LLM）在不同领域的理智能力。然而，这些手动制定的提示策略通常是不优化的。在本文中，我们介绍了Promptbreeder，一种通用的自referential自提高机制，可以在给定领域中进化和适应提示。Promptbreeder被 LLM 驱动，对任务提示 population 进行变异，然后对这些任务提示进行评价。关键是，变异这些任务提示的过程是由 LLM 生成和改进的自referential方式。即，Promptbreeder不仅是改进任务提示，而且也是改进这些改进任务提示的mutation prompts。Promptbreeder在常用的数学和常识理智测试benchmark上表现出色，并且能够演化复杂的任务提示 для hate speech classification 问题。
</details></li>
</ul>
<hr>
<h2 id="Photonic-Accelerators-for-Image-Segmentation-in-Autonomous-Driving-and-Defect-Detection"><a href="#Photonic-Accelerators-for-Image-Segmentation-in-Autonomous-Driving-and-Defect-Detection" class="headerlink" title="Photonic Accelerators for Image Segmentation in Autonomous Driving and Defect Detection"></a>Photonic Accelerators for Image Segmentation in Autonomous Driving and Defect Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16783">http://arxiv.org/abs/2309.16783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lakshmi Nair, David Widemann, Brad Turcott, Nick Moore, Alexandra Wleklinski, Darius Bunandar, Ioannis Papavasileiou, Shihu Wang, Eric Logan</li>
<li>for: 这个论文探讨了在光学加速器上执行图像分割模型，以优化图像分割任务的速度和能效性。</li>
<li>methods: 该论文使用了光学加速器来执行图像分割模型，并研究了不同图像分割模型在光学加速器上的性能。</li>
<li>results: 论文发现了一些图像分割模型可以在光学加速器上减少精度损失，并提供了这些模型的实际原因。此外，论文还对不同图像分割任务的吞吐量和能耗进行了比较。<details>
<summary>Abstract</summary>
Photonic computing promises faster and more energy-efficient deep neural network (DNN) inference than traditional digital hardware. Advances in photonic computing can have profound impacts on applications such as autonomous driving and defect detection that depend on fast, accurate and energy efficient execution of image segmentation models. In this paper, we investigate image segmentation on photonic accelerators to explore: a) the types of image segmentation DNN architectures that are best suited for photonic accelerators, and b) the throughput and energy efficiency of executing the different image segmentation models on photonic accelerators, along with the trade-offs involved therein. Specifically, we demonstrate that certain segmentation models exhibit negligible loss in accuracy (compared to digital float32 models) when executed on photonic accelerators, and explore the empirical reasoning for their robustness. We also discuss techniques for recovering accuracy in the case of models that do not perform well. Further, we compare throughput (inferences-per-second) and energy consumption estimates for different image segmentation workloads on photonic accelerators. We discuss the challenges and potential optimizations that can help improve the application of photonic accelerators to such computer vision tasks.
</details>
<details>
<summary>摘要</summary>
光子计算技术可以提供更快速和能效的深度神经网络（DNN）推理，比传统的数字硬件更高效。在应用于自动驾驶和缺陷检测等领域中，光子计算技术的进步可能产生深见的影响。本文通过对图像分割模型的执行来探索：a) 适合光子加速器的图像分割DNN架构，以及b) 光子加速器上运行不同图像分割模型的吞吐量和能效率，以及其中的让步。我们发现某些分割模型在光子加速器上具有较小的精度损失（相比于数字浮点32模型），并 explore了这种 Robustness 的实际原因。此外，我们还讨论了在模型性能不佳时如何恢复精度的技术。此外，我们对不同图像分割任务的吞吐量和能效率进行了比较，并讨论了在光子加速器上应用这些计算任务的挑战和优化方法。
</details></li>
</ul>
<hr>
<h2 id="Intriguing-properties-of-generative-classifiers"><a href="#Intriguing-properties-of-generative-classifiers" class="headerlink" title="Intriguing properties of generative classifiers"></a>Intriguing properties of generative classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16779">http://arxiv.org/abs/2309.16779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyank Jaini, Kevin Clark, Robert Geirhos</li>
<li>for: 研究人类物体识别的最佳方法 – 是否使用推测性推理（快速但可能存在快速学习）或使用生成模型（慢速但可能更加可靠）？</li>
<li>methods: 基于最近的生成模型技术，将文本到图像模型转化成分类器，以便研究其行为并与推测模型和人类psychophysical数据进行比较。</li>
<li>results: 报告了四个有趣的 emergent 性质：1）表现出人类水平的形态偏好（Imagen 的99%），2）与人类分类错误相似的水平，3）与人类分类错误的水平相似，4）对 certain 感知错觉有理解。结果表明，当前主导的人类物体识别模型是推测性推理，但零shot 生成模型在人类物体识别数据上 surprisingly well 的 aproximation。<details>
<summary>Abstract</summary>
What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
</details>
<details>
<summary>摘要</summary>
最佳模式来识别物体是否抽象推理（快速但可能会采用短cut学习）或使用生成模型（慢速但可能更加Robust）？我们基于最近的生成模型发展，将文本到图像模型转化为分类器。这使得我们可以研究它们的行为，并与抽象模型和人类心理物理数据进行比较。我们报道了四种有趣的生成分类器特性：它们表现出99%的人类样式偏好（对Imagen）、人类水平的异常数据准确率、人类分类错误的Alignment和某些视觉错觉的理解。我们的结果表明，虽然当前的主导模式是抽象推理，但零 shot生成模型在模型人类物体识别数据中 surprisingly well。
</details></li>
</ul>
<hr>
<h2 id="How-many-words-does-ChatGPT-know-The-answer-is-ChatWords"><a href="#How-many-words-does-ChatGPT-know-The-answer-is-ChatWords" class="headerlink" title="How many words does ChatGPT know? The answer is ChatWords"></a>How many words does ChatGPT know? The answer is ChatWords</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16777">http://arxiv.org/abs/2309.16777</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wordsgpt/chatwords">https://github.com/wordsgpt/chatwords</a></li>
<li>paper_authors: Gonzalo Martínez, Javier Conde, Pedro Reviriego, Elena Merino-Gómez, José Alberto Hernández, Fabrizio Lombardi<br>for: 这个论文的目的是评估聊天GPT的语言知识，特别是对于一组指定的单词的认知。methods: 该论文使用了自动化测试系统ChatWords来评估聊天GPT对于一组指定的单词的认知。results: 研究发现，聊天GPT只能正确识别约80%的词汇，并且在一些情况下具有错误的含义。<details>
<summary>Abstract</summary>
The introduction of ChatGPT has put Artificial Intelligence (AI) Natural Language Processing (NLP) in the spotlight. ChatGPT adoption has been exponential with millions of users experimenting with it in a myriad of tasks and application domains with impressive results. However, ChatGPT has limitations and suffers hallucinations, for example producing answers that look plausible but they are completely wrong. Evaluating the performance of ChatGPT and similar AI tools is a complex issue that is being explored from different perspectives. In this work, we contribute to those efforts with ChatWords, an automated test system, to evaluate ChatGPT knowledge of an arbitrary set of words. ChatWords is designed to be extensible, easy to use, and adaptable to evaluate also other NLP AI tools. ChatWords is publicly available and its main goal is to facilitate research on the lexical knowledge of AI tools. The benefits of ChatWords are illustrated with two case studies: evaluating the knowledge that ChatGPT has of the Spanish lexicon (taken from the official dictionary of the "Real Academia Espa\~nola") and of the words that appear in the Quixote, the well-known novel written by Miguel de Cervantes. The results show that ChatGPT is only able to recognize approximately 80% of the words in the dictionary and 90% of the words in the Quixote, in some cases with an incorrect meaning. The implications of the lexical knowledge of NLP AI tools and potential applications of ChatWords are also discussed providing directions for further work on the study of the lexical knowledge of AI tools.
</details>
<details>
<summary>摘要</summary>
chatGPT 的推出使得人工智能自然语言处理（NLP）技术再次升级。chatGPT 的采用率快速增长，有数百万用户在各种应用领域进行实验，结果很出色。然而，chatGPT 也存在一些限制和偏见，例如生成的答案看起来很像，但实际上完全错误。评估 chatGPT 和类似的 NLP 工具性能是一个复杂的问题，正在从不同的角度进行探索。在这项工作中，我们贡献了一个自动化测试系统，即 ChatWords，以评估 chatGPT 对于任意集合的词汇知识。ChatWords 设计为易于使用、可扩展和适用于评估其他 NLP 工具。ChatWords 公开可用，其主要目标是促进研究 NLP 工具词汇知识。我们通过两个案例研究，一是评估 chatGPT 对于西班牙语词汇（根据官方《Real Academia Espa\~nola》词典）的认知程度，二是评估 chatGPT 对于《仙游记》中出现的词汇的认知程度。结果显示，chatGPT 只能识别西班牙语词汇约80%，《仙游记》中的词汇约90%，有时会带有错误的含义。我们还讨论了 NLP 工具词汇知识的后果和可能的应用，并提供了进一步研究 NLP 工具词汇知识的指导。
</details></li>
</ul>
<hr>
<h2 id="Neural-scaling-laws-for-phenotypic-drug-discovery"><a href="#Neural-scaling-laws-for-phenotypic-drug-discovery" class="headerlink" title="Neural scaling laws for phenotypic drug discovery"></a>Neural scaling laws for phenotypic drug discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16773">http://arxiv.org/abs/2309.16773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Drew Linsley, John Griffin, Jason Parker Brown, Adam N Roose, Michael Frank, Peter Linsley, Steven Finkbeiner, Jeremy Linsley</li>
<li>for: 这个论文是为了探讨深度神经网络（DNNs）在小分子药物发现方面是否可以通过大规模模型和数据来实现突破性进步。</li>
<li>methods: 作者通过大规模系统性分析，检查DNN大小、数据饭和学习策略之间如何相互作用，以影响在Phenotypic Chemistry Arena（Pheno-CA）benchmark上的准确性。</li>
<li>results: 作者发现，不同于自然语言处理和计算机视觉领域，DNN在小分子药物发现任务上不会随着数据和模型大小的增加而不断提高。作者引入了一种新的预学任务——反向生物过程（IBP），并发现IBP先训练后在Pheno-CA上表现出较高的准确性。此外，IBP训练后的DNN表现与数据和模型规模呈MONOTONIC增长关系。这些结果表明，用于解决小分子药物发现任务的DNN元素已经在我们手中，并且可以通过更多的实验数据来实现任何需要的提升。作者发布了Pheno-CA bencmark和代码，以便更多的研究人员研究小分子药物发现领域中的神经 scaling laws。<details>
<summary>Abstract</summary>
Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs first trained with IBP then probed for performance on the Pheno-CA significantly outperform task-supervised DNNs. More importantly, the performance of these IBP-trained DNNs monotonically improves with data and model scale. Our findings reveal that the DNN ingredients needed to accurately solve small molecule drug development tasks are already in our hands, and project how much more experimental data is needed to achieve any desired level of improvement. We release our Pheno-CA benchmark and code to encourage further study of neural scaling laws for small molecule drug discovery.
</details>
<details>
<summary>摘要</summary>
Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size increases. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We find that DNNs first trained with IBP and then probed for performance on the Pheno-CA significantly outperform task-supervised DNNs. Moreover, the performance of these IBP-trained DNNs monotonically improves with data and model scale.Our findings reveal that the DNN ingredients needed to accurately solve small molecule drug discovery tasks are already in our hands, and we provide a quantitative estimate of how much more experimental data is needed to achieve any desired level of improvement. We release our Pheno-CA benchmark and code to encourage further study of neural scaling laws for small molecule drug discovery.
</details></li>
</ul>
<hr>
<h2 id="XVO-Generalized-Visual-Odometry-via-Cross-Modal-Self-Training"><a href="#XVO-Generalized-Visual-Odometry-via-Cross-Modal-Self-Training" class="headerlink" title="XVO: Generalized Visual Odometry via Cross-Modal Self-Training"></a>XVO: Generalized Visual Odometry via Cross-Modal Self-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16772">http://arxiv.org/abs/2309.16772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Lai, Zhongkai Shangguan, Jimuyang Zhang, Eshed Ohn-Bar</li>
<li>for: 本研究旨在提出一种 semi-supervised learning 方法，用于训练通用的单目视巡ometry（VO）模型，并可以在不同的数据集和环境下进行稳定的自适应运行。</li>
<li>methods: 本研究使用了 YouTube 上的大量不制定和多样化的摄像头视频进行自我训练，以便学习视频场景的 semantics 来恢复相对pose。具有多Modal 监督，包括 segmentation、流动、深度和音频auxiliary prediction 任务，以便激活通用表示。</li>
<li>results: 我们的提案可以在常用的 KITTI 标准测试集上达到状态之前的性能水平，而不需要多帧优化或相机参数的知情。此外，我们还发现音频预测任务可以强化 semi-supervised 学习过程，特别是在高度动态和不同的视频数据中。通过将 XVO 与 semi-supervised 步骤结合使用，我们可以在 KITTI、nuScenes 和 Argoverse 等不同数据集上实现自适应知识传递，而不需要特定的 fine-tuning。<details>
<summary>Abstract</summary>
We propose XVO, a semi-supervised learning method for training generalized monocular Visual Odometry (VO) models with robust off-the-self operation across diverse datasets and settings. In contrast to standard monocular VO approaches which often study a known calibration within a single dataset, XVO efficiently learns to recover relative pose with real-world scale from visual scene semantics, i.e., without relying on any known camera parameters. We optimize the motion estimation model via self-training from large amounts of unconstrained and heterogeneous dash camera videos available on YouTube. Our key contribution is twofold. First, we empirically demonstrate the benefits of semi-supervised training for learning a general-purpose direct VO regression network. Second, we demonstrate multi-modal supervision, including segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate generalized representations for the VO task. Specifically, we find audio prediction task to significantly enhance the semi-supervised learning process while alleviating noisy pseudo-labels, particularly in highly dynamic and out-of-domain video data. Our proposed teacher network achieves state-of-the-art performance on the commonly used KITTI benchmark despite no multi-frame optimization or knowledge of camera parameters. Combined with the proposed semi-supervised step, XVO demonstrates off-the-shelf knowledge transfer across diverse conditions on KITTI, nuScenes, and Argoverse without fine-tuning.
</details>
<details>
<summary>摘要</summary>
我们提出 XVO，一种半监督学习方法，用于训练通用化的单目视巡数据（VO）模型，并可以在多种数据集和环境下进行稳定的自适应操作。与标准单目VO方法不同，XVO可以高效地从视觉场景 semantics 中 recuperate 相对pose，无需依赖任何known camera参数。我们通过自动训练从 YouTube 上可获得大量不同类型和不一致的dash摄像头视频来优化运动估计模型。我们的关键贡献有两点：首先，我们实际表明了半监督训练可以学习一个通用的直接VO重 regression 网络。其次，我们示出了多Modal 监督，包括分割、流动、深度和音频auxiliary prediction任务，以便促进通用表示 дляVO任务。我们发现音频预测任务可以显著地提高半监督学习过程，并减少噪音pseudo标签，特别是在高度动态和外域视频数据中。我们提出的教师网络在通用的KITTIbenchmark上达到了state-of-the-art性能，即使没有多帧优化或相机参数的知情。与我们的半监督步骤结合，XVO在KITTI、nuscenes和Argoverse上实现了无需 fine-tuning 的稳定知识传递。
</details></li>
</ul>
<hr>
<h2 id="Persona-Coded-Poly-Encoder-Persona-Guided-Multi-Stream-Conversational-Sentence-Scoring"><a href="#Persona-Coded-Poly-Encoder-Persona-Guided-Multi-Stream-Conversational-Sentence-Scoring" class="headerlink" title="Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring"></a>Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16770">http://arxiv.org/abs/2309.16770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfeng Liu, Christopher Symons, Ranga Raju Vatsavai</li>
<li>for: 提高对话质量，使用个人性格信息进行改进。</li>
<li>methods: 提出了一种新的Persona-Coded Poly-Encoder方法，利用个人性格信息在多流编码方式中进行改进。</li>
<li>results: 对两个不同的人格基本对话集进行评估，与两种现有方法进行比较，研究结果表明，与基eline方法Poly-Encoder相比，我们的方法可以提高对话质量的BLEU分数和HR@1指标中的提高率为3.32%和2.94%。<details>
<summary>Abstract</summary>
Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against two state-of-the-art methods. Our experimental results and analysis demonstrate that our method can improve conversation quality over the baseline method Poly-Encoder by 3.32% and 2.94% in terms of BLEU score and HR@1, respectively. More significantly, our method offers a path to better utilization of multi-modal data in conversational tasks. Lastly, our study outlines several challenges and future research directions for advancing personalized conversational AI technology.
</details>
<details>
<summary>摘要</summary>
In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against two state-of-the-art methods. Our experimental results and analysis demonstrate that our method can improve conversation quality over the baseline method Poly-Encoder by 3.32% and 2.94% in terms of BLEU score and HR@1, respectively. More significantly, our method offers a path to better utilization of multi-modal data in conversational tasks.Lastly, our study outlines several challenges and future research directions for advancing personalized conversational AI technology.
</details></li>
</ul>
<hr>
<h2 id="RealFill-Reference-Driven-Generation-for-Authentic-Image-Completion"><a href="#RealFill-Reference-Driven-Generation-for-Authentic-Image-Completion" class="headerlink" title="RealFill: Reference-Driven Generation for Authentic Image Completion"></a>RealFill: Reference-Driven Generation for Authentic Image Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16668">http://arxiv.org/abs/2309.16668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David E. Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, Michael Rubinstein</li>
<li>for: 填充图像中缺失的区域，使得图像更加完整和真实</li>
<li>methods: 使用几个参考图像个性化生成模型，可以在不同的视角、照明条件、摄像头等条件下完成图像的填充</li>
<li>results: 比较 existed 方法，RealFill 能够在多种复杂和挑战的场景下填充图像，并且可以生成更加真实和可信的内容<details>
<summary>Abstract</summary>
Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin. See more results on our project page: https://realfill.github.io
</details>
<details>
<summary>摘要</summary>
近期的生成图像技术发展，出现了外部涂抹和内部涂抹模型，可以生成高质量、有可能的图像内容，但这些模型无法提供真实场景的信息，因此生成的内容是不真实的。在这项工作中，我们提出了RealFill，一种新的生成方法，可以填充图像中缺失的区域。RealFill是一种基于几个参考图像的个性化生成模型，这些参考图像不需要与目标图像对齐，可以有极大的视角、照明条件、镜头缩进或图像风格的差异。一旦个性化，RealFill就可以完成目标图像，并生成有趣的内容，忠实于原始场景。我们在一个新的图像完成测试 benchmark 上评估了RealFill，并发现它与现有方法相比，表现出了大幅度的提升。更多结果请查看我们的项目页面：https://realfill.github.io。
</details></li>
</ul>
<hr>
<h2 id="SA2-Net-Scale-aware-Attention-Network-for-Microscopic-Image-Segmentation"><a href="#SA2-Net-Scale-aware-Attention-Network-for-Microscopic-Image-Segmentation" class="headerlink" title="SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation"></a>SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16661">http://arxiv.org/abs/2309.16661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mustansarfiaz/sa2-net">https://github.com/mustansarfiaz/sa2-net</a></li>
<li>paper_authors: Mustansar Fiaz, Rao Muhammad Anwer, Hisham Cholakkal</li>
<li>for: 这个论文主要目的是提出一个具有注意力导航的方法，以便有效地处理微scopic影像中的多种结构，例如细胞等。</li>
<li>methods: 这个方法使用了多个层次特征学习，并将注意力导航模组与多个分辨率结合，以捕捉微scopic影像中的各种构造。此外，这个方法还引入了一个新的upsampling策略，以提高区域边界的定义性。</li>
<li>results: 实验结果显示，这个SA2-Net模型在五个挑战性的数据集上表现出色，并且比较常用的CNN模型表现更好。代码供给publicly available at \url{<a target="_blank" rel="noopener" href="https://github.com/mustansarfiaz/SA2-Net%7D">https://github.com/mustansarfiaz/SA2-Net}</a>.<details>
<summary>Abstract</summary>
Microscopic image segmentation is a challenging task, wherein the objective is to assign semantic labels to each pixel in a given microscopic image. While convolutional neural networks (CNNs) form the foundation of many existing frameworks, they often struggle to explicitly capture long-range dependencies. Although transformers were initially devised to address this issue using self-attention, it has been proven that both local and global features are crucial for addressing diverse challenges in microscopic images, including variations in shape, size, appearance, and target region density. In this paper, we introduce SA2-Net, an attention-guided method that leverages multi-scale feature learning to effectively handle diverse structures within microscopic images. Specifically, we propose scale-aware attention (SA2) module designed to capture inherent variations in scales and shapes of microscopic regions, such as cells, for accurate segmentation. This module incorporates local attention at each level of multi-stage features, as well as global attention across multiple resolutions. Furthermore, we address the issue of blurred region boundaries (e.g., cell boundaries) by introducing a novel upsampling strategy called the Adaptive Up-Attention (AuA) module. This module enhances the discriminative ability for improved localization of microscopic regions using an explicit attention mechanism. Extensive experiments on five challenging datasets demonstrate the benefits of our SA2-Net model. Our source code is publicly available at \url{https://github.com/mustansarfiaz/SA2-Net}.
</details>
<details>
<summary>摘要</summary>
微型图像分割是一项复杂的任务，目标是为每个微型图像像素分配Semantic标签。虽然卷积神经网络（CNN）是许多现有框架的基础，但它们经常难以直接捕捉长距离依赖关系。尽管转换器在初始设计中是为了解决这一问题，但实际上，本地和全局特征都是微型图像多样化挑战的关键。在这篇论文中，我们介绍SA2-Net模型，它利用多级特征学习来有效地处理微型图像多样化挑战。具体来说，我们提出了适应级别注意（SA2）模块，用于捕捉微型图像中不同级别的尺度和形状特征，如细胞。这个模块包括每个多 stage特征层的本地注意力，以及多个分辨率之间的全局注意力。此外，我们解决了微型图像边界模糊（例如细胞边界模糊）的问题，通过引入一种新的upsampling策略called Adaptive Up-Attention（AuA）模块。这个模块通过显式注意力机制来提高微型图像区域的特征表达能力，以提高细胞的local化。我们在五个复杂的dataset上进行了广泛的实验，并证明了SA2-Net模型的优势。我们的源代码可以在github上获取，具体请参阅 \url{https://github.com/mustansarfiaz/SA2-Net}.
</details></li>
</ul>
<hr>
<h2 id="Memory-in-Plain-Sight-A-Survey-of-the-Uncanny-Resemblances-between-Diffusion-Models-and-Associative-Memories"><a href="#Memory-in-Plain-Sight-A-Survey-of-the-Uncanny-Resemblances-between-Diffusion-Models-and-Associative-Memories" class="headerlink" title="Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories"></a>Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16750">http://arxiv.org/abs/2309.16750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Judy Hoffman, Zsolt Kira, Duen Horng Chau</li>
<li>for: 这个论文主要是为了提供一个简洁的概述 diffusion models (DMs)，并描述它们如何与 Associative Memories (AMs) 之间的数学连接。</li>
<li>methods: 这篇论文使用了动力系统和Ordinary Differential Equations (ODEs) 来描述 DMs，并指出了一个 Lyapunov energy function，可以通过梯度下降来denoising数据。</li>
<li>results: 这篇论文总结了40年来的能量基本模型 (AMs) 的研究历史，并讨论了新的研究方向，包括 AMs 和 DMs 之间的类似性和不同性。<details>
<summary>Abstract</summary>
Diffusion Models (DMs) have recently set state-of-the-art on many generation benchmarks. However, there are myriad ways to describe them mathematically, which makes it difficult to develop a simple understanding of how they work. In this survey, we provide a concise overview of DMs from the perspective of dynamical systems and Ordinary Differential Equations (ODEs) which exposes a mathematical connection to the highly related yet often overlooked class of energy-based models, called Associative Memories (AMs). Energy-based AMs are a theoretical framework that behave much like denoising DMs, but they enable us to directly compute a Lyapunov energy function on which we can perform gradient descent to denoise data. We then summarize the 40 year history of energy-based AMs, beginning with the original Hopfield Network, and discuss new research directions for AMs and DMs that are revealed by characterizing the extent of their similarities and differences
</details>
<details>
<summary>摘要</summary>
Diffusion Models (DM) 在许多生成benchmark上设置了现代的州Of-the-art。然而，有许多不同的方式可以用数学方式描述它们，这使得理解它们的工作方式变得困难。在这篇survey中，我们提供了一个简洁的概述，从动态系统和常数方程式（ODEs）的角度，暴露了DM和对它们相似但通常被忽略的能量基本模型（AM）之间的数学连接。能量基本AM是一个理论框架，它们在减少噪声方面表现得非常相似于DM，但它们允许我们直接计算数据上的 Lyapunov 能量函数，并且可以使用梯度下降来减少噪声。我们然后summarize了40年来的AM历史，开始自原始的Hopfield网络，并讨论了AM和DM的新研究方向。
</details></li>
</ul>
<hr>
<h2 id="Discovering-environments-with-XRM"><a href="#Discovering-environments-with-XRM" class="headerlink" title="Discovering environments with XRM"></a>Discovering environments with XRM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16748">http://arxiv.org/abs/2309.16748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Pezeshki, Diane Bouchacourt, Mark Ibrahim, Nicolas Ballas, Pascal Vincent, David Lopez-Paz</li>
<li>For: The paper aims to develop algorithms for automatically discovering environments that induce broad generalization for robust AI systems across applications.* Methods: The proposed method, Cross-Risk-Minimization (XRM), trains two twin networks to learn from one random half of the training data, while imitating confident held-out mistakes made by its sibling.* Results: The paper shows that XRM can discover environments for all training and validation data, and domain generalization algorithms built on top of XRM environments achieve oracle worst-group-accuracy, solving a long-standing problem in out-of-distribution generalization.Here are the three points in Simplified Chinese:* For: 本 paper 的目的是开发自动发现可以促进 AI 系统广泛适用的环境。* Methods: 提议的方法是 Cross-Risk-Minimization (XRM)，它将两个姐妹网络训练在一个随机选择的训练数据上，并模仿她的姐妹网络中的自信停止错误。* Results:  paper 表明，XRM 可以为所有训练和验证数据发现环境，并在这些环境上建立域总结算法，解决了异常事物总结的长期问题。<details>
<summary>Abstract</summary>
Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Domain generalization algorithms built on top of XRM environments achieve oracle worst-group-accuracy, solving a long-standing problem in out-of-distribution generalization.
</details>
<details>
<summary>摘要</summary>
成功的 OUT-OF-DISTRIBUTION 泛化需要环境注释。 Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Domain generalization algorithms built on top of XRM environments achieve oracle worst-group-accuracy, solving a long-standing problem in out-of-distribution generalization.Here's the translation in Traditional Chinese as well:成功的 OUT-OF-DISTRIBUTION 泛化需要环境注释。 Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data. Domain generalization algorithms built on top of XRM environments achieve oracle worst-group-accuracy, solving a long-standing problem in out-of-distribution generalization.
</details></li>
</ul>
<hr>
<h2 id="MindShift-Leveraging-Large-Language-Models-for-Mental-States-Based-Problematic-Smartphone-Use-Intervention"><a href="#MindShift-Leveraging-Large-Language-Models-for-Mental-States-Based-Problematic-Smartphone-Use-Intervention" class="headerlink" title="MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention"></a>MindShift: Leveraging Large Language Models for Mental-States-Based Problematic Smartphone Use Intervention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16639">http://arxiv.org/abs/2309.16639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruolan Wu, Chun Yu, Xiaole Pan, Yujia Liu, Ningning Zhang, Yue Fu, Yuhan Wang, Zhi Zheng, Li Chen, Qiaolei Jiang, Xuhai Xu, Yuanchun Shi</li>
<li>for: 这个研究旨在开发一种基于大语言模型的智能手机使用范例，以帮助解决问题atic smartphone 使用对身心健康的负面影响。</li>
<li>methods: 本研究使用了巫师-奥兹研究（N&#x3D;12）和访谈研究（N&#x3D;10），以了解用户对问题atic smartphone 使用的心理状态，包括怠惰、压力和陌生。这些资讯帮助设计了四种劝导策略：理解、安慰、唤起和导向习惯。</li>
<li>results: 比较 MindShift 和基本技术， MindShift 在5周场景实验（N&#x3D;25）中具有较高的干预接受率（17.8-22.5%）和降低智能手机使用频率（12.1-14.4%）。此外，用户的智能手机成瘾指数下降和自律力上升。研究显示了基于大语言模型的上下文感知劝导在其他行为改变领域的潜力。<details>
<summary>Abstract</summary>
Problematic smartphone use negatively affects physical and mental health. Despite the wide range of prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We first conduct a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leverage large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We develop MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals & habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conduct a 5-week field experiment (N=25) to compare MindShift with baseline techniques. The results show that MindShift significantly improves intervention acceptance rates by 17.8-22.5% and reduces smartphone use frequency by 12.1-14.4%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains.
</details>
<details>
<summary>摘要</summary>
Problematic smartphone use negatively affects physical and mental health. Despite extensive prior research, existing persuasive techniques are not flexible enough to provide dynamic persuasion content based on users' physical contexts and mental states. We conducted a Wizard-of-Oz study (N=12) and an interview study (N=10) to summarize the mental states behind problematic smartphone use: boredom, stress, and inertia. This informs our design of four persuasion strategies: understanding, comforting, evoking, and scaffolding habits. We leveraged large language models (LLMs) to enable the automatic and dynamic generation of effective persuasion content. We developed MindShift, a novel LLM-powered problematic smartphone use intervention technique. MindShift takes users' in-the-moment physical contexts, mental states, app usage behaviors, users' goals & habits as input, and generates high-quality and flexible persuasive content with appropriate persuasion strategies. We conducted a 5-week field experiment (N=25) to compare MindShift with baseline techniques. The results show that MindShift significantly improves intervention acceptance rates by 17.8-22.5% and reduces smartphone use frequency by 12.1-14.4%. Moreover, users have a significant drop in smartphone addiction scale scores and a rise in self-efficacy. Our study sheds light on the potential of leveraging LLMs for context-aware persuasion in other behavior change domains.
</details></li>
</ul>
<hr>
<h2 id="Mixup-Your-Own-Pairs"><a href="#Mixup-Your-Own-Pairs" class="headerlink" title="Mixup Your Own Pairs"></a>Mixup Your Own Pairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16633">http://arxiv.org/abs/2309.16633</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilei-wu/supremix">https://github.com/yilei-wu/supremix</a></li>
<li>paper_authors: Yilei Wu, Zijian Dong, Chongyao Chen, Wangchunshu Zhou, Juan Helen Zhou</li>
<li>for: 这篇研究旨在提高回溯学习中的数据回溯表现，特别是对于 regression  задачі。</li>
<li>methods: 这篇研究使用了 contrastive learning 技术，并且提出了一个名为 SupReMix 的新方法，它通过在嵌入层使用 anchor-inclusive 和 anchor-exclusive 的 mixture 来提高对 regression 数据的表现。</li>
<li>results: 经过广泛的实验和理论分析，研究发现 SupReMix 可以对 regression 数据提供丰富的排序信息，从而提高 regression 表现。此外，SupReMix 在转移学习、训练数据不均衡和对于少量训练数据的情况下也表现出优优的性能。<details>
<summary>Abstract</summary>
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harder contrastive pairs by integrating richer ordinal information. Through extensive experiments on six regression datasets including 2D images, volumetric images, text, tabular data, and time-series signals, coupled with theoretical analysis, we demonstrate that SupReMix pre-training fosters continuous ordered representations of regression data, resulting in significant improvement in regression performance. Furthermore, SupReMix is superior to other approaches in a range of regression challenges including transfer learning, imbalanced training data, and scenarios with fewer training samples.
</details>
<details>
<summary>摘要</summary>
在表征学学习中，回归方面曾经受到类别学习的遮盖，直接将类别学习技术应用于回归问题通常会导致杂乱的表征在隐藏空间，影响性不佳。在这篇论文中，我们认为对比学习在回归中的潜在能力被忽略了两个关键因素：排序意识和困难程度。为了解决这些挑战，我们提议“混合你自己的对比对”，而不仅仅依靠实际/扩展样本。我们提出了“Supervised Contrastive Learning for Regression with Mixup”（SupReMix）。它在嵌入层使用混合（包括 anchor 和一个不同的负样本的混合）作为困难对，并使用不含 anchor 的混合（两个不同的负样本之间的混合）作为困难正对。这种策略通过在嵌入水平上混合更多的排序信息，形成更加困难的对比对。经过了EXTENSIVE EXPERIMENTS 在六个回归数据集，包括 2D 图像、体积图像、文本、表格数据和时间序列信号，并与理论分析，我们展示了 SupReMix 预训练可以促进回归数据的连续排序表征，从而带来显著提高回归性能。此外，SupReMix 还在多种回归挑战中表现出优异，包括转移学习、不均衡训练数据和少量训练样本的场景。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Diverse-Data-for-Global-Disaster-Prediction-A-Multimodal-Framework"><a href="#Harnessing-Diverse-Data-for-Global-Disaster-Prediction-A-Multimodal-Framework" class="headerlink" title="Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework"></a>Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16747">http://arxiv.org/abs/2309.16747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gengyin Liu, Huaiyang Zhong</li>
<li>for: 预测气候变化导致的灾害预测，尤其是洪水和山塌预测，以了解气候和地理因素的关系。</li>
<li>methods: 该研究使用多Modal数据源，包括天气统计、卫星图像和文本情况，构建一个新的灾害预测框架。为了 Address class imbalance，我们采用了一些策略。</li>
<li>results: 结果表明，通过 integrate multiple data sources，可以提高模型性能，但是具体的提高程度因每种灾害和其根本原因而异。<details>
<summary>Abstract</summary>
As climate change intensifies, the urgency for accurate global-scale disaster predictions grows. This research presents a novel multimodal disaster prediction framework, combining weather statistics, satellite imagery, and textual insights. We particularly focus on "flood" and "landslide" predictions, given their ties to meteorological and topographical factors. The model is meticulously crafted based on the available data and we also implement strategies to address class imbalance. While our findings suggest that integrating multiple data sources can bolster model performance, the extent of enhancement differs based on the specific nature of each disaster and their unique underlying causes.
</details>
<details>
<summary>摘要</summary>
随着气候变化加剧，灾害预测的紧迫性日益增加。这项研究提出了一种新型多模态灾害预测框架，结合天气统计、卫星图像和文本掌握。我们尤其关注“洪水”和“山崩”预测，因为它们与天气和地理因素有着密切的关系。模型通过可用数据的精心设计，并对数据不平衡进行处理。我们发现，将多种数据源 integrate 可以提高模型性能，但每种灾害的特点和根本原因决定了增强程度。
</details></li>
</ul>
<hr>
<h2 id="Stress-Testing-Chain-of-Thought-Prompting-for-Large-Language-Models"><a href="#Stress-Testing-Chain-of-Thought-Prompting-for-Large-Language-Models" class="headerlink" title="Stress Testing Chain-of-Thought Prompting for Large Language Models"></a>Stress Testing Chain-of-Thought Prompting for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16621">http://arxiv.org/abs/2309.16621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aayush Mishra, Karan Thakkar</li>
<li>for: 此研究探讨了Chain-of-Thought（CoT）提示的效iveness在提高大语言模型（LLM）的多步逻辑能力。</li>
<li>methods: 研究人员使用了三种类型的CoT提示perturbation，namely CoT order, CoT values,和CoT operators来分析GPT-3在不同任务上的表现。</li>
<li>results: 研究发现， incorrect CoT提示会导致准确率指标下降，正确的CoT值对于预测正确答案是关键。此外， incorrect demonstrations，where CoT operators或CoT order是错误的，不会对表现产生太大影响，相比之下，值基于的perturbation更加有影响。<details>
<summary>Abstract</summary>
This report examines the effectiveness of Chain-of-Thought (CoT) prompting in improving the multi-step reasoning abilities of large language models (LLMs). Inspired by previous studies \cite{Min2022RethinkingWork}, we analyze the impact of three types of CoT prompt perturbations, namely CoT order, CoT values, and CoT operators on the performance of GPT-3 on various tasks. Our findings show that incorrect CoT prompting leads to poor performance on accuracy metrics. Correct values in the CoT is crucial for predicting correct answers. Moreover, incorrect demonstrations, where the CoT operators or the CoT order are wrong, do not affect the performance as drastically when compared to the value based perturbations. This research deepens our understanding of CoT prompting and opens some new questions regarding the capability of LLMs to learn reasoning in context.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Chain-of-Thought" (CoT) is translated as "思维链" (siwei lian) in Simplified Chinese.* "large language models" (LLMs) is translated as "大语言模型" (da yu yan mo deli) in Simplified Chinese.* "incorrect CoT prompting" is translated as "错误的思维链提示" (cuo yong de siwei lian tiishi) in Simplified Chinese.* "correct values in the CoT" is translated as "思维链中正确的值" (siwei lian zhong zheng qi de yi) in Simplified Chinese.* "incorrect demonstrations" is translated as "错误的示例" (cuo yong de shi yi) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Depthwise-Hyperparameter-Transfer-in-Residual-Networks-Dynamics-and-Scaling-Limit"><a href="#Depthwise-Hyperparameter-Transfer-in-Residual-Networks-Dynamics-and-Scaling-Limit" class="headerlink" title="Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit"></a>Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16620">http://arxiv.org/abs/2309.16620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, Cengiz Pehlevan</li>
<li>for: 这 paper 的目的是找到一种新的深度学习模型调参方法，以降低模型的计算成本。</li>
<li>methods: 这 paper 使用了 $\mu$P 参数化网络，其中小宽网络的优化参数可以转移到任意宽度的网络上。然而，在这种方案中，参数不会转移到深度上。为了解决这个问题，这 paper 研究了具有 $1&#x2F;\sqrt{\text{depth}$ 的剩余分支的 residual 网络，并与 $\mu$P 参数化结合使用。</li>
<li>results: 这 paper 通过实验表明，使用这种参数化和 residual 网络结构可以在 CIFAR-10 和 ImageNet 上实现优化参数的传递 across width 和 depth。此外，这 paper 的实验结果得到了理论支持，使用了近期发展的神经网络学习动态mean field theory（DMFT）描述神经网络学习动态，并证明了这种参数化的 ResNet 在无穷宽和无穷深度上存在一个明确的特征学习共聚点，并且证明了 finite-size 网络动态的收敛到这个共聚点。<details>
<summary>Abstract</summary>
The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show that this parameterization of ResNets admits a well-defined feature learning joint infinite-width and infinite-depth limit and show convergence of finite-size network dynamics towards this limit.
</details>
<details>
<summary>摘要</summary>
深度学习中的参数优化成本随模型大小增长，让实践者寻找新的优化方法，其中一种提议使用$\mu$P参数化网络，其中优化的参数对小宽网络适用于任意大宽网络。然而，在这种方案中，参数不会在深度上传递。为了解决这个问题，我们研究了具有 $1/\sqrt{\text{depth}$ 的剩余分支级别的 residual 网络，并将其与 $\mu$P 参数化结合使用。我们提供了实验证明，使用这种参数化可以在 CIFAR-10 和 ImageNet 上传递优化的参数 across 宽度和深度。此外，我们的实验结果得到了理论支持，使用了最近的神经网络学习动力学 теория（DMFT）描述神经网络学习动力学，我们显示这种参数化的 ResNet 具有明确的特征学习共同极限，并且证明了finite-size网络动力学的拓扑向这个极限 converges。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Neural-Program-Smoothing-for-Fuzzing"><a href="#Revisiting-Neural-Program-Smoothing-for-Fuzzing" class="headerlink" title="Revisiting Neural Program Smoothing for Fuzzing"></a>Revisiting Neural Program Smoothing for Fuzzing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16618">http://arxiv.org/abs/2309.16618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria-Irina Nicolae, Max Eisele, Andreas Zeller</li>
<li>for: This paper aims to evaluate the performance of Neural Program Smoothing (NPS) fuzzers and compare them with standard gray-box fuzzers.</li>
<li>methods: The paper uses a neural network as a smooth approximation of the program target for new test case generation, and conducts the most extensive evaluation of NPS fuzzers against standard gray-box fuzzers.</li>
<li>results: The paper finds that the original performance claims for NPS fuzzers do not hold, and that standard gray-box fuzzers almost always surpass NPS-based fuzzers. The paper also contributes an in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS, and proposes new guidelines targeted at benchmarking fuzzing based on machine learning.Here is the format you requested for the results:</li>
<li>for: 这篇论文目的是评估Neural Program Smoothing（NPS）批处法的性能，并与标准的灰度批处法进行比较。</li>
<li>methods: 这篇论文使用神经网络作为新测试 случа的生成目标函数的均匀approximation，并进行了NPS批处法的最广泛评估。</li>
<li>results: 这篇论文发现NPS批处法的原始性能声明不准确，并且标准的灰度批处法大多数情况下会超过NPS基于的批处法。论文还提供了NPS中机器学习和梯度基于的变化的深入分析，并提出了基于机器学习的批处法评估指南。<details>
<summary>Abstract</summary>
Testing with randomly generated inputs (fuzzing) has gained significant traction due to its capacity to expose program vulnerabilities automatically. Fuzz testing campaigns generate large amounts of data, making them ideal for the application of machine learning (ML). Neural program smoothing (NPS), a specific family of ML-guided fuzzers, aims to use a neural network as a smooth approximation of the program target for new test case generation.   In this paper, we conduct the most extensive evaluation of NPS fuzzers against standard gray-box fuzzers (>11 CPU years and >5.5 GPU years), and make the following contributions: (1) We find that the original performance claims for NPS fuzzers do not hold; a gap we relate to fundamental, implementation, and experimental limitations of prior works. (2) We contribute the first in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS. (3) We implement Neuzz++, which shows that addressing the practical limitations of NPS fuzzers improves performance, but that standard gray-box fuzzers almost always surpass NPS-based fuzzers. (4) As a consequence, we propose new guidelines targeted at benchmarking fuzzing based on machine learning, and present MLFuzz, a platform with GPU access for easy and reproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our data are public.
</details>
<details>
<summary>摘要</summary>
In this paper, we conduct the most extensive evaluation of NPS fuzzers against standard gray-box fuzzers (>11 CPU years and >5.5 GPU years), and make the following contributions:1. We find that the original performance claims for NPS fuzzers do not hold; a gap we relate to fundamental, implementation, and experimental limitations of prior works.2. We contribute the first in-depth analysis of the contribution of machine learning and gradient-based mutations in NPS.3. We implement Neuzz++, which shows that addressing the practical limitations of NPS fuzzers improves performance, but that standard gray-box fuzzers almost always surpass NPS-based fuzzers.4. As a consequence, we propose new guidelines targeted at benchmarking fuzzing based on machine learning, and present MLFuzz, a platform with GPU access for easy and reproducible evaluation of ML-based fuzzers. Neuzz++, MLFuzz, and all our data are public.
</details></li>
</ul>
<hr>
<h2 id="“AI-enhances-our-performance-I-have-no-doubt-this-one-will-do-the-same”-The-Placebo-effect-is-robust-to-negative-descriptions-of-AI"><a href="#“AI-enhances-our-performance-I-have-no-doubt-this-one-will-do-the-same”-The-Placebo-effect-is-robust-to-negative-descriptions-of-AI" class="headerlink" title="“AI enhances our performance, I have no doubt this one will do the same”: The Placebo effect is robust to negative descriptions of AI"></a>“AI enhances our performance, I have no doubt this one will do the same”: The Placebo effect is robust to negative descriptions of AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16606">http://arxiv.org/abs/2309.16606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnes M. Kloft, Robin Welsch, Thomas Kosch, Steeven Villa</li>
<li>for:  investigate the impact of user expectations on human-AI interactions and evaluate the effectiveness of AI systems.</li>
<li>methods: used a letter discrimination task and a Bayesian analysis to study the impact of AI descriptions on participant performance, and used cognitive modeling to trace the advantage back to participants gathering more information.</li>
<li>results: found that participants performed better when they believed an AI was present, even when there was no actual AI, and that negative AI descriptions did not alter expectations.Here is the text in Simplified Chinese:</li>
<li>for: 研究用户对人机交互中的AI系统效iveness的影响，以及用户对AI系统的期望和期待的影响。</li>
<li>methods: 使用字母识别任务和 bayesian分析研究用户对AI描述的影响，并使用认知模型跟踪这种优势的来源。</li>
<li>results: 发现当用户认为AI存在时，他们的性能会更高，即使没有实际的AI，并且消极的AI描述无法改变用户的期望。<details>
<summary>Abstract</summary>
Heightened AI expectations facilitate performance in human-AI interactions through placebo effects. While lowering expectations to control for placebo effects is advisable, overly negative expectations could induce nocebo effects. In a letter discrimination task, we informed participants that an AI would either increase or decrease their performance by adapting the interface, but in reality, no AI was present in any condition. A Bayesian analysis showed that participants had high expectations and performed descriptively better irrespective of the AI description when a sham-AI was present. Using cognitive modeling, we could trace this advantage back to participants gathering more information. A replication study verified that negative AI descriptions do not alter expectations, suggesting that performance expectations with AI are biased and robust to negative verbal descriptions. We discuss the impact of user expectations on AI interactions and evaluation and provide a behavioral placebo marker for human-AI interaction
</details>
<details>
<summary>摘要</summary>
人工智能预期的增强会促进人机交互中的表现 durch placebo效应。而为了控制placebo效应，应下降预期，但过于负面的预期可能会导致nocebo效应。在一个字母拥挤任务中，我们通知参与者，一个AI会通过改变界面来增加或减少他们的表现，但在实际情况下，没有AI存在任何condition。一种 bayesian分析表明，参与者对AI的预期很高，并在sham-AI存在的情况下表现出了更好的描述性表现。通过认知模型，我们可以追溯这个优势回到参与者更多地收集信息。一个重复研究证明，负面的AI描述不会改变预期， suggesting that performance expectations with AI are biased and robust to negative verbal descriptions。我们讨论了用户预期对人机交互和评价的影响，以及提供了人机交互中的行为地平标记。
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-for-Bayesian-Optimization-on-Heterogeneous-Search-Spaces"><a href="#Transfer-Learning-for-Bayesian-Optimization-on-Heterogeneous-Search-Spaces" class="headerlink" title="Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces"></a>Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16597">http://arxiv.org/abs/2309.16597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhou Fan, Xinran Han, Zi Wang</li>
<li>for: 优化黑盒函数（black-box function optimization）</li>
<li>methods:  bayesian 优化（Bayesian optimization）和培根学习（transfer learning）</li>
<li>results: 提高了黑盒函数优化任务的性能，可以在不同域的搜索空间中传递知识。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）是一种广泛使用的黑盒函数优化方法，它根据 bayesian 模型（通常是 Gaussian 过程）来做Sequential 决策。为保证模型质量，传输学approaches 已经开发来自动设置 GP 先天的模型。这些训练函数通常需要与“测试”函数（黑盒函数优化的目标函数）具有同一个Domain。在这篇论文中，我们介绍 MPHD，一种基于不同领域的域特征 mapping 来预训练 GP 模型的方法。MPHD 可以轻松地与 BO 结合使用，从而在不同搜索空间中传输知识。我们的理论和实验结果表明 MPHD 的有效性和在复杂黑盒函数优化任务中的优异表现。
</details></li>
</ul>
<hr>
<h2 id="Can-LLMs-Effectively-Leverage-Graph-Structural-Information-When-and-Why"><a href="#Can-LLMs-Effectively-Leverage-Graph-Structural-Information-When-and-Why" class="headerlink" title="Can LLMs Effectively Leverage Graph Structural Information: When and Why"></a>Can LLMs Effectively Leverage Graph Structural Information: When and Why</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16595">http://arxiv.org/abs/2309.16595</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trais-lab/llm-structured-data">https://github.com/trais-lab/llm-structured-data</a></li>
<li>paper_authors: Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma</li>
<li>for: 这个论文研究使用大量自然语言模型（LLM），并将结构数据（特别是图形数据）作为未经探索的数据类型，以提高节点预测性能。</li>
<li>methods: 作者使用多种提示方法来编码结构信息，以实现在文本特征scarce或rich的情况下提高LLM的预测性能。</li>
<li>results: 研究发现（i）LLM可以受益于结构信息，尤其是当文本节点特征scarce时；（ii）没有显著证据表明LLM性能受到数据泄露的影响；以及（iii）LLM在Target节点上的性能强正相关于节点的本地同类比率。<details>
<summary>Abstract</summary>
This paper studies Large Language Models (LLMs) augmented with structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks with textual features. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively related to the local homophily ratio of the node\footnote{Codes and datasets are at: \url{https://github.com/TRAIS-Lab/LLM-Structured-Data}.
</details>
<details>
<summary>摘要</summary>
To answer the "when" question, the paper explores various methods for encoding structural information, including prompting methods, in settings with rich or scarce textual node features. For the "why" questions, the study examines two potential factors that contribute to LLM performance: data leakage and homophily.The results show that LLMs can benefit from structural information, especially when textual node features are limited. Additionally, the study finds no significant evidence that LLM performance is largely attributed to data leakage. Finally, the performance of LLMs on a target node is strongly positively related to the local homophily ratio of the node.The codes and datasets used in the study are available at: \url{https://github.com/TRAIS-Lab/LLM-Structured-Data}.
</details></li>
</ul>
<hr>
<h2 id="Navigating-Healthcare-Insights-A-Birds-Eye-View-of-Explainability-with-Knowledge-Graphs"><a href="#Navigating-Healthcare-Insights-A-Birds-Eye-View-of-Explainability-with-Knowledge-Graphs" class="headerlink" title="Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs"></a>Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16593">http://arxiv.org/abs/2309.16593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satvik Garg, Shivam Parikh, Somya Garg</li>
<li>for: This paper is written for researchers and healthcare professionals who are interested in using knowledge graphs (KGs) in healthcare AI, specifically in drug discovery and pharmaceutical research.</li>
<li>methods: The paper discusses various methods for constructing and utilizing KGs in healthcare AI, including knowledge-infused learning, relationship extraction, and reasoning.</li>
<li>results: The paper highlights the potential of KGs in healthcare AI to improve interpretability and support decision-making, with applications in areas such as Drug-Drug Interactions (DDI), Drug Target Interactions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), and bioinformatics. The paper also emphasizes the importance of making KGs more interpretable in healthcare.<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) are gaining prominence in Healthcare AI, especially in drug discovery and pharmaceutical research as they provide a structured way to integrate diverse information sources, enhancing AI system interpretability. This interpretability is crucial in healthcare, where trust and transparency matter, and eXplainable AI (XAI) supports decision making for healthcare professionals. This overview summarizes recent literature on the impact of KGs in healthcare and their role in developing explainable AI models. We cover KG workflow, including construction, relationship extraction, reasoning, and their applications in areas like Drug-Drug Interactions (DDI), Drug Target Interactions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), and bioinformatics. We emphasize the importance of making KGs more interpretable through knowledge-infused learning in healthcare. Finally, we highlight research challenges and provide insights for future directions.
</details>
<details>
<summary>摘要</summary>
知识图（KG）在医疗人工智能（AI）领域受到越来越多的关注，特别是在药物发现和药品研究中，因为它们提供了一种结构化的方式，整合多种信息源，提高AI系统的可读性。这种可读性在医疗领域非常重要，因为信任和透明度很重要，而解释AI（XAI）支持医疗专业人员的决策。本文提供了最近的文献研究，描述了KG在医疗领域的影响和其在开发可解释AI模型方面的作用。我们覆盖了KG的工作流程，包括建立、关系提取、推理、以及在药物间交互（DDI）、药target交互（DTI）、药品开发（DD）、不良药物反应（ADR）和生物信息学等领域的应用。我们强调了在医疗领域使KG更加可解释的重要性，并提供了未来研究的挑战和思路。
</details></li>
</ul>
<hr>
<h2 id="The-ARRT-of-Language-Models-as-a-Service-Overview-of-a-New-Paradigm-and-its-Challenges"><a href="#The-ARRT-of-Language-Models-as-a-Service-Overview-of-a-New-Paradigm-and-its-Challenges" class="headerlink" title="The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges"></a>The ARRT of Language-Models-as-a-Service: Overview of a New Paradigm and its Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16573">http://arxiv.org/abs/2309.16573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Anthony G. Cohn, Nigel Shadbolt, Michael Wooldridge</li>
<li>for: 本研究的目标是描述语言模型作为服务（LMaaS）的困难和挑战，以及如何提高访问、复制、可靠性和信任worthiness（ARRT）。</li>
<li>methods: 本研究采用系统性的方法描述当前主要的LMaaS的缺乏信息所带来的障碍，并提供一些建议和未来发展方向。</li>
<li>results: 本研究结果表明，当前主要的LMaaS存在访问、复制、可靠性和信任worthiness（ARRT）的困难和挑战，并提供了一些建议和未来发展方向。<details>
<summary>Abstract</summary>
Some of the most powerful language models currently are proprietary systems, accessible only via (typically restrictive) web or software programming interfaces. This is the Language-Models-as-a-Service (LMaaS) paradigm. Contrasting with scenarios where full model access is available, as in the case of open-source models, such closed-off language models create specific challenges for evaluating, benchmarking, and testing them. This paper has two goals: on the one hand, we delineate how the aforementioned challenges act as impediments to the accessibility, replicability, reliability, and trustworthiness (ARRT) of LMaaS. We systematically examine the issues that arise from a lack of information about language models for each of these four aspects. We shed light on current solutions, provide some recommendations, and highlight the directions for future advancements. On the other hand, it serves as a one-stop-shop for the extant knowledge about current, major LMaaS, offering a synthesized overview of the licences and capabilities their interfaces offer.
</details>
<details>
<summary>摘要</summary>
一些当前最强大的语言模型都是专有系统，通过（通常是限制的）网络或软件编程接口进行访问。这是语言模型作为服务（LMaaS）模式。与开源模型相比，这些封闭语言模型会创造特定的挑战，以评估、测试和比较它们的可访问性、复制性、可靠性和信任性（ARRT）。本文有两个目标：首先，我们详细描述了这些挑战如何阻碍LMaaS的访问、复制、可靠性和信任性四个方面的可访问性。我们系统地检查这些问题的起因，并提供一些建议。其次，它serve as a one-stop-shop for the extant knowledge about current, major LMaaS, offering a synthesized overview of the licenses and capabilities their interfaces offer.
</details></li>
</ul>
<hr>
<h2 id="Augment-to-Interpret-Unsupervised-and-Inherently-Interpretable-Graph-Embeddings"><a href="#Augment-to-Interpret-Unsupervised-and-Inherently-Interpretable-Graph-Embeddings" class="headerlink" title="Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings"></a>Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16564">http://arxiv.org/abs/2309.16564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/euranova/augment_to_interpret">https://github.com/euranova/augment_to_interpret</a></li>
<li>paper_authors: Gregory Scafarto, Madalina Ciortan, Simon Tihon, Quentin Ferre</li>
<li>for: 这篇论文旨在提出一种可 interpretability 的 graph representation learning 方法，以满足 Recent Transparent-AI 规定。</li>
<li>methods: 该方法使用 data augmentation 技术，通过保持 semantics 来学习可 interpretability 的 embedding。</li>
<li>results: 实验研究表明，该方法可以提供 state-of-the-art 的性能在 downstream 任务上，并且具有可 interpretability 的优势。<details>
<summary>Abstract</summary>
Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on subsequent downstream tasks.
</details>
<details>
<summary>摘要</summary>
Unsupervised learning 允许我们利用无标签数据，这些数据在过去几年变得极其丰富，并创建可以在多种下游任务上使用的嵌入。然而，通常缺乏无监督表示学习的解释性限制了我们在Recent Transparent-AI规定下的发展。在这篇论文中，我们研究图表示学习，并证明通过保持 semantics 的数据扩充可以学习并生成解释性的嵌入。我们的框架，我们称之为 INGENIOUS，创造了内置的解释性嵌入，从而消除了高昂的附加后续分析的需求。我们还引入了针对无监督表示学习解释性缺乏正式主义和度量的额外度量。我们的实验研究在图级和节点级任务上应用，结果表明可解释性嵌入提供了下游任务的状态级表现。
</details></li>
</ul>
<hr>
<h2 id="Voting-Network-for-Contour-Levee-Farmland-Segmentation-and-Classification"><a href="#Voting-Network-for-Contour-Levee-Farmland-Segmentation-and-Classification" class="headerlink" title="Voting Network for Contour Levee Farmland Segmentation and Classification"></a>Voting Network for Contour Levee Farmland Segmentation and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16561">http://arxiv.org/abs/2309.16561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abolfazl Meyarian, Xiaohui Yuan</li>
<li>for: 这 paper 是为了 segmenting farmlands with contour levees from high-resolution aerial imagery.</li>
<li>methods: 这 paper 使用了一种 end-to-end 可学习的网络，包括多个 voting blocks 来实现图像分类和 segmentation.</li>
<li>results: 这 paper 的方法在 National Agriculture Imagery Program 的图像上测试得到了平均准确率为 94.34%，比前一个状态的方法提高了 6.96% 和 2.63% 的 F1 分数。<details>
<summary>Abstract</summary>
High-resolution aerial imagery allows fine details in the segmentation of farmlands. However, small objects and features introduce distortions to the delineation of object boundaries, and larger contextual views are needed to mitigate class confusion. In this work, we present an end-to-end trainable network for segmenting farmlands with contour levees from high-resolution aerial imagery. A fusion block is devised that includes multiple voting blocks to achieve image segmentation and classification. We integrate the fusion block with a backbone and produce both semantic predictions and segmentation slices. The segmentation slices are used to perform majority voting on the predictions. The network is trained to assign the most likely class label of a segment to its pixels, learning the concept of farmlands rather than analyzing constitutive pixels separately. We evaluate our method using images from the National Agriculture Imagery Program. Our method achieved an average accuracy of 94.34\%. Compared to the state-of-the-art methods, the proposed method obtains an improvement of 6.96% and 2.63% in the F1 score on average.
</details>
<details>
<summary>摘要</summary>
高解像卫星影像可以显示农田的细节，但小 objetcs 和特征会导致对象boundaries的扭曲，需要更大的上下文视图来减少类异常。在这种工作中，我们提出了一个可以执行全程训练的网络，用于从高解像卫星影像中分割农田和缘坝。我们设计了一个合并块，该块包括多个投票块，以实现图像分类和分割。我们将该合并块与背景 integrate 并生成semantic prediction和分割slice。我们使用分割slice进行多数投票，以确定每个像素的最有可能的类别标签。我们使用National Agriculture Imagery Program中的图像进行评估，我们的方法达到了94.34%的平均准确率。相比之前的方法，我们的方法在F1分数中提高了6.96%和2.63%的平均值。
</details></li>
</ul>
<hr>
<h2 id="KLoB-a-Benchmark-for-Assessing-Knowledge-Locating-Methods-in-Language-Models"><a href="#KLoB-a-Benchmark-for-Assessing-Knowledge-Locating-Methods-in-Language-Models" class="headerlink" title="KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models"></a>KLoB: a Benchmark for Assessing Knowledge Locating Methods in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16535">http://arxiv.org/abs/2309.16535</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juyiming/klob">https://github.com/juyiming/klob</a></li>
<li>paper_authors: Yiming Ju, Zheng Zhang</li>
<li>for: 本研究旨在检验语言模型中的知识储存是否符合 lokalisierung  гипотезы。</li>
<li>methods: 本研究提出了 KLoB  benchmark，用于评估现有的知识定位方法。</li>
<li>results: KLoB 可以用于评估现有的知识定位方法，并且可以用于重新评估 lokalisierung  гипотезы。<details>
<summary>Abstract</summary>
Recently, Locate-Then-Edit paradigm has emerged as one of the main approaches in changing factual knowledge stored in the Language models. However, there is a lack of research on whether present locating methods can pinpoint the exact parameters embedding the desired knowledge. Moreover, although many researchers have questioned the validity of locality hypothesis of factual knowledge, no method is provided to test the a hypothesis for more in-depth discussion and research. Therefore, we introduce KLoB, a benchmark examining three essential properties that a reliable knowledge locating method should satisfy. KLoB can serve as a benchmark for evaluating existing locating methods in language models, and can contributes a method to reassessing the validity of locality hypothesis of factual knowledge. Our is publicly available at \url{https://github.com/juyiming/KLoB}.
</details>
<details>
<summary>摘要</summary>
最近，语言模型中的“发现然后编辑”模式已成为改变知识存储的主要方法之一。然而，exist 的研究表明，目前的定位方法是否可以准确地寻找所需的知识 Parameters 还存在很大的不确定性。此外，许多研究人员对本地性假设表示怀疑，但是没有提供方法来进行更深入的讨论和研究。因此，我们介绍了 KLoB，一个评估三种关键性质的知识定位指标。KLoB 可以用来评估现有的定位方法在语言模型中的性能，并且可以为本地性假设的有效性进行再评估。我们的代码公开在 GitHub 上，可以通过 \url{https://github.com/juyiming/KLoB} 访问。
</details></li>
</ul>
<hr>
<h2 id="MotionLM-Multi-Agent-Motion-Forecasting-as-Language-Modeling"><a href="#MotionLM-Multi-Agent-Motion-Forecasting-as-Language-Modeling" class="headerlink" title="MotionLM: Multi-Agent Motion Forecasting as Language Modeling"></a>MotionLM: Multi-Agent Motion Forecasting as Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16534">http://arxiv.org/abs/2309.16534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S. Refaat, Rami Al-Rfou, Benjamin Sapp</li>
<li>for: 预测自动驾驶车辆未来行为的可靠预测是一项关键任务，以确保安全规划。</li>
<li>methods: 本文使用语言模型来预测多个交通Agent的未来行为，并将连续轨迹表示为字符序列。</li>
<li>results: 提出的方法在 Waymo 开放运动数据集上实现了新的状态 искус智能榜首，在交互挑战领先榜单上排名第一。<details>
<summary>Abstract</summary>
Reliable forecasting of the future behavior of road agents is a critical component to safe planning in autonomous vehicles. Here, we represent continuous trajectories as sequences of discrete motion tokens and cast multi-agent motion prediction as a language modeling task over this domain. Our model, MotionLM, provides several advantages: First, it does not require anchors or explicit latent variable optimization to learn multimodal distributions. Instead, we leverage a single standard language modeling objective, maximizing the average log probability over sequence tokens. Second, our approach bypasses post-hoc interaction heuristics where individual agent trajectory generation is conducted prior to interactive scoring. Instead, MotionLM produces joint distributions over interactive agent futures in a single autoregressive decoding process. In addition, the model's sequential factorization enables temporally causal conditional rollouts. The proposed approach establishes new state-of-the-art performance for multi-agent motion prediction on the Waymo Open Motion Dataset, ranking 1st on the interactive challenge leaderboard.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT预测自驾车道Agent的未来行为是安全规划中的关键组成部分。在这里，我们表示连续轨迹为序列化 discrete 动作符号，将多个动物运动预测变为语言模型化任务。我们的模型，MotionLM，具有以下优势：首先，它不需要锚点或显式的隐藏变量优化来学习多模态分布。相反，我们利用单个标准语言模型化目标，最大化序列符号的平均日志概率。其次，我们的方法不需要后续交互规则，而是在单个推送过程中生成交互agent的共同未来。此外，模型的时间分解能够实现 Conditional Rollouts。我们的提出方法在 Waymo 开放动力学数据集上实现了新的状态纪录性表现，在互动挑战 leaderboard 上排名第一。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Chatmap-Large-Language-Model-Interaction-with-Cartographic-Data"><a href="#Chatmap-Large-Language-Model-Interaction-with-Cartographic-Data" class="headerlink" title="Chatmap : Large Language Model Interaction with Cartographic Data"></a>Chatmap : Large Language Model Interaction with Cartographic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01429">http://arxiv.org/abs/2310.01429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eren Unlu<br>for:This paper aims to demonstrate the use of a large language model (LLM) to provide a linguistic interface to OpenStreetMap (OSM) data for an arbitrary urban region, allowing users to inquire about location attributes such as touristic appeal or business profitability.methods:The authors fine-tune a relatively small-scale LLM with a small artificial dataset curated by a more capable teacher model to provide the linguistic interface to OSM data.results:The study shows early signs of useful emerging abilities in this context, including the embeddings of artificially curated prompts including OSM data, which could be instrumental for potential geospatially aware urban Retrieval Augmented Generation (RAG) applications.<details>
<summary>Abstract</summary>
The swift advancement and widespread availability of foundational Large Language Models (LLMs), complemented by robust fine-tuning methodologies, have catalyzed their adaptation for innovative and industrious applications. Enabling LLMs to recognize and interpret geospatial data, while offering a linguistic access to vast cartographic datasets, is of significant importance. OpenStreetMap (OSM) is the most ambitious open-source global initiative offering detailed urban and rural geographic data, curated by a community of over 10 million contributors, which constitutes a great potential for LLM applications. In this study, we demonstrate the proof of concept and details of the process of fine-tuning a relatively small scale (1B parameters) LLM with a relatively small artificial dataset curated by a more capable teacher model, in order to provide a linguistic interface to the OSM data of an arbitrary urban region. Through this interface, users can inquire about a location's attributes, covering a wide spectrum of concepts, such as its touristic appeal or the potential profitability of various businesses in that vicinity. The study aims to provide an initial guideline for such generative artificial intelligence (AI) adaptations and demonstrate early signs of useful emerging abilities in this context even in minimal computational settings. The embeddings of artificially curated prompts including OSM data are also investigated in detail, which might be instrumental for potential geospatially aware urban Retrieval Augmented Generation (RAG) applications.
</details>
<details>
<summary>摘要</summary>
快速发展和普及大型自然语言模型（LLM）的可能性，结合了可靠的微调方法，使得它们在创新和产业上得到应用。允许 LLM 认可和解释地理数据，同时提供语言接口访问庞大的地图数据集，对于地理应用来说非常重要。开源地图协会（OSM）是全球最大的开源地理数据initiative，由1000万名贡献者维护，这个数据库的可访问性和可用性对LLM应用来说非常重要。本研究示例了一种使用相对较小的约10亿参数的 LLM 微调过程，使得它可以理解和解释OSM数据，并提供一种语言接口，让用户可以根据地点的特征，提问该地点的属性，包括旅游appeal或者当地企业的可能性。本研究的目的是提供一个初步的AI应用 guideline，并证明在有限的计算设置下，LLM在这种情况下的初步表现。此外，研究还investigated embedding of artificially curated prompts，包括OSM数据，这些embeddings可能对 potential的地ospatially awareurban Retrieval Augmented Generation（RAG）应用产生影响。
</details></li>
</ul>
<hr>
<h2 id="From-Complexity-to-Clarity-Analytical-Expressions-of-Deep-Neural-Network-Weights-via-Clifford’s-Geometric-Algebra-and-Convexity"><a href="#From-Complexity-to-Clarity-Analytical-Expressions-of-Deep-Neural-Network-Weights-via-Clifford’s-Geometric-Algebra-and-Convexity" class="headerlink" title="From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford’s Geometric Algebra and Convexity"></a>From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford’s Geometric Algebra and Convexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16512">http://arxiv.org/abs/2309.16512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mert Pilanci</li>
<li>for: 这个论文是关于神经网络的分析，使用几何（Clifford）代数和几何优化。</li>
<li>methods: 论文使用标准正则化损失函数来训练深度ReLU神经网络，并通过几何优化来找到优化的 weights。</li>
<li>results: 论文发现，在训练过程中，神经网络的优化 weights 可以表示为训练样本的叉乘Product，并且训练问题可以转化为几何优化问题，该问题可以找到一小型的样本subset，并通过 $\ell_1$ 正则化来发现只有 relevante 叉乘Product features。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种基于几何（Clifford）代数和凸优化的神经网络分析方法。我们证明，当使用标准正则化损失函数进行训练时，深度ReLU神经网络的优化策略是通过训练样本的叉乘产生的。此外，训练问题转化为凸优化问题，其中凸函数是基于叉乘特征的。这些特征编码了训练数据集的几何结构，具体是指由数据向量生成的积分体和平行板生成的正负体积。我们的分析为神经网络的内部工作提供了一个新的视角，并且抛光了隐藏层的作用。
</details></li>
</ul>
<hr>
<h2 id="Toloka-Visual-Question-Answering-Benchmark"><a href="#Toloka-Visual-Question-Answering-Benchmark" class="headerlink" title="Toloka Visual Question Answering Benchmark"></a>Toloka Visual Question Answering Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16511">http://arxiv.org/abs/2309.16511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dmitry Ustalov, Nikita Pavlichenko, Sergey Koshelev, Daniil Likhobaba, Alisa Smirnova</li>
<li>for: 这个论文目的是提出一个新的人工智能测试数据集，用于评测机器学习系统在视觉问答任务中的性能，并与人类水平进行比较。</li>
<li>methods: 这个论文使用了人工智能技术，包括开源零基eline模型和多阶段竞赛，以评测机器学习系统在视觉问答任务中的性能。</li>
<li>results: 根据交叠分区评价分数，当论文提交时，没有任何机器学习模型超越非专家人工智能基线。<details>
<summary>Abstract</summary>
In this paper, we present Toloka Visual Question Answering, a new crowdsourced dataset allowing comparing performance of machine learning systems against human level of expertise in the grounding visual question answering task. In this task, given an image and a textual question, one has to draw the bounding box around the object correctly responding to that question. Every image-question pair contains the response, with only one correct response per image. Our dataset contains 45,199 pairs of images and questions in English, provided with ground truth bounding boxes, split into train and two test subsets. Besides describing the dataset and releasing it under a CC BY license, we conducted a series of experiments on open source zero-shot baseline models and organized a multi-phase competition at WSDM Cup that attracted 48 participants worldwide. However, by the time of paper submission, no machine learning model outperformed the non-expert crowdsourcing baseline according to the intersection over union evaluation score.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Toloka视觉问答数据集，这是一个新的人工生成的数据集，用于比较机器学习系统与人类专业水平在视觉问答任务中的表现。在这个任务中，给定一幅图像和一个文本问题，需要正确地选择图像中相应的对象。每个图像-问题对包含回答，仅有一个正确的回答每幅图像。我们的数据集包含45,199个图像-问题对，其中包括训练和两个测试subset，以及每个图像-问题对的真实答案。除了描述数据集和发布它以CC BYlicense外，我们还进行了一系列实验，使用开源零基eline模型，并在WSDM杯中组织了多阶段比赛，吸引了全球48名参与者。然而，到论文提交时，没有任何机器学习模型超过非专家人工生成基eline的交叉上下 overlap评价分。
</details></li>
</ul>
<hr>
<h2 id="Asset-Bundling-for-Wind-Power-Forecasting"><a href="#Asset-Bundling-for-Wind-Power-Forecasting" class="headerlink" title="Asset Bundling for Wind Power Forecasting"></a>Asset Bundling for Wind Power Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16492">http://arxiv.org/abs/2309.16492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanyu Zhang, Mathieu Tanneau, Chaofan Huang, V. Roshan Joseph, Shangkun Wang, Pascal Van Hentenryck</li>
<li>for: 这篇论文旨在提高风力发电 grid 中的预测精度，尤其是在风力发电变化很大的情况下。</li>
<li>methods: 这篇论文提出了一个novel Bundle-Predict-Reconcile (BPR) 框架，融合资产组合、机器学习和预测重新整理技术。BPR 框架首先学习一个中间层级（组合），然后预测风力在资产、组合和舱级别的时间序列，最后将所有预测重新整理以确保一致性。这种方法将新增一个辅助学习任务（预测组合级别时间序列），帮助主要学习任务。</li>
<li>results: 实验结果显示，BPR 框架在实际应用中具有明显的改善预测精度的效果，特别是在舱级别上。<details>
<summary>Abstract</summary>
The growing penetration of intermittent, renewable generation in US power grids, especially wind and solar generation, results in increased operational uncertainty. In that context, accurate forecasts are critical, especially for wind generation, which exhibits large variability and is historically harder to predict. To overcome this challenge, this work proposes a novel Bundle-Predict-Reconcile (BPR) framework that integrates asset bundling, machine learning, and forecast reconciliation techniques. The BPR framework first learns an intermediate hierarchy level (the bundles), then predicts wind power at the asset, bundle, and fleet level, and finally reconciles all forecasts to ensure consistency. This approach effectively introduces an auxiliary learning task (predicting the bundle-level time series) to help the main learning tasks. The paper also introduces new asset-bundling criteria that capture the spatio-temporal dynamics of wind power time series. Extensive numerical experiments are conducted on an industry-size dataset of 283 wind farms in the MISO footprint. The experiments consider short-term and day-ahead forecasts, and evaluates a large variety of forecasting models that include weather predictions as covariates. The results demonstrate the benefits of BPR, which consistently and significantly improves forecast accuracy over baselines, especially at the fleet level.
</details>
<details>
<summary>摘要</summary>
随着美国电力网络中间型发电的增加，特别是风力和太阳能发电的增加，运营uncertainty增加。在这个上下文中，准确预测是非常重要，尤其是风力发电，它的变化很大，历史上更难预测。为了解决这个挑战，这个工作提出了一种新的Bundle-Predict-Reconcile（BPR）框架，它将资产束合，机器学习和预测重叠技术相结合。BPR框架首先学习中间层次（束合），然后预测风力发电量在资产、束合和舰队级别，并最后重叠所有预测，以确保一致性。这种方法实际上是在主要学习任务之外增加了一个辅助学习任务（预测束合级时间序列），以帮助主要学习任务。这篇论文还提出了新的资产束合标准，以捕捉风力发电时间序列的空间-时间动态。我们对283个风力电站的数据进行了广泛的数值实验，考虑了短期和当日预测，并评估了许多预测模型，包括天气预测作为covariates。结果表明BPR具有显著优势，在baseline的基础上，尤其是在舰队级别，具有显著和一致性提高预测精度。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-LLMs-with-Knowledge-A-survey-on-hallucination-prevention"><a href="#Augmenting-LLMs-with-Knowledge-A-survey-on-hallucination-prevention" class="headerlink" title="Augmenting LLMs with Knowledge: A survey on hallucination prevention"></a>Augmenting LLMs with Knowledge: A survey on hallucination prevention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16459">http://arxiv.org/abs/2309.16459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantinos Andriopoulos, Johan Pouwelse</li>
<li>for: 本研究的目的是探讨大型预训言语模型如何通过与外部知识源的Integration来解决传统语言模型存在的问题，如幻想、不准确回答和扩展性问题。</li>
<li>methods: 本研究使用了将大型预训言语模型与可微分的访问机制相结合，以便访问外部知识源，包括外部知识库和搜索引擎。这些扩展的语言模型通过在预测缺失字符的标准目标下使用多样化、可能非参数的外部模块来增强其语言处理能力。</li>
<li>results: 本研究发现，通过将大型预训言语模型与知识源集成，可以解决传统语言模型存在的问题，如幻想、不准确回答和扩展性问题。这些扩展的语言模型还能够更好地处理语言任务，提高了对知识的访问和处理能力。<details>
<summary>Abstract</summary>
Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens, these augmented LMs leverage diverse, possibly non-parametric external modules to augment their contextual processing capabilities, departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge, this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs, such as hallucinations, un-grounded responses, and scalability challenges.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neuro-Symbolic-Reasoning-for-Planning-Counterexample-Guided-Inductive-Synthesis-using-Large-Language-Models-and-Satisfiability-Solving"><a href="#Neuro-Symbolic-Reasoning-for-Planning-Counterexample-Guided-Inductive-Synthesis-using-Large-Language-Models-and-Satisfiability-Solving" class="headerlink" title="Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving"></a>Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16436">http://arxiv.org/abs/2309.16436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumit Kumar Jha, Susmit Jha, Patrick Lincoln, Nathaniel D. Bastian, Alvaro Velasquez, Rickard Ewetz, Sandeep Neema<br>for: 这种方法用于生成符合逻辑要求的正式文档，如代码、规划和逻辑规范。methods: 使用生成大型自然语言模型（LLMs），通过人工提供的指导提示，生成人类语言响应，并使用逻辑推理引擎（SMT）来分析生成的解决方案，生成错误的对应例子，并将其反馈给 LLMs。results: 通过在块域的规划任务上评估这种方法，发现这种方法可以生成符合逻辑要求的正式文档，并且可以使用非专家用户通过自然语言描述问题，并且组合 LLMs 和 SMT 引擎可以生成可靠的解决方案。<details>
<summary>Abstract</summary>
Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts. Apart from natural language responses, they have also been found to be effective at generating formal artifacts such as code, plans, and logical specifications from natural language prompts. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generated solutions from the LLMs, produce counterexamples when the solutions are incorrect, and provide that feedback to the LLMs exploiting the dialog capability of instruct-trained LLMs. This interaction between inductive LLMs and deductive SMT solvers can iteratively steer the LLM to generate the correct response. In our experiments, we use planning over the domain of blocks as our synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo, Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our method allows the user to communicate the planning problem in natural language; even the formulation of queries to SMT solvers is automatically generated from natural language. Thus, the proposed technique can enable non-expert users to describe their problems in natural language, and the combination of LLMs and SMT solvers can produce provably correct solutions.
</details>
<details>
<summary>摘要</summary>
大型生成语言模型（LLM），如GPT-4，可以根据人类提供的指令prompt并生成人类化的回应。除了自然语言回应，它们还能生成 formal artifacts such as code, plans, and logical specifications from natural language prompts。 despite their significantly improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plans, and other formal artifacts produced by LLMs can be catastrophic.我们认为可以使用模型满足性理论（SMT）解析器来分析由LLMs生成的解决方案，生成错误的 counterexample，并通过对LLMs的对话来使其更正。这种LLMs和SMT解析器之间的互动可以轮循地使LLMs生成正确的回应。在我们的实验中，我们使用块的规划作为我们的生成任务，使用GPT-4、GPT3.5 Turbo、Davinci、Curie、Babbage和Ada作为LLMs，并使用Z3作为SMT解析器。我们的方法允许用户通过自然语言描述问题，甚至是SMT解析器的查询也可以自动生成自然语言中。因此，我们的技术可以帮助非专业用户通过自然语言描述问题，并且组合LLMs和SMT解析器可以生成可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Diverse-and-Aligned-Audio-to-Video-Generation-via-Text-to-Video-Model-Adaptation"><a href="#Diverse-and-Aligned-Audio-to-Video-Generation-via-Text-to-Video-Model-Adaptation" class="headerlink" title="Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation"></a>Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16429">http://arxiv.org/abs/2309.16429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guyyariv/TempoTokens">https://github.com/guyyariv/TempoTokens</a></li>
<li>paper_authors: Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, Yossi Adi</li>
<li>for: 本研究目的是生成受Semantic classes的各种语音样本引导的多样化和真实的视频。</li>
<li>methods: 我们使用了一个现有的文本受控制视频生成模型和一个预训练的音频编码器模型。我们的方法基于一个轻量级的适应器网络，该网络学习将音频基于表示映射到输入表示。因此，它还允许视频生成conditioned on文本、音频和两者。</li>
<li>results: 我们在三个 datasets 进行了广泛的验证，并提出了一个新的评价指标（AV-Align）来评估输入音频样本与生成的视频的匹配度。我们的方法在内容和时间轴上都能够更好地与输入音频样本相匹配，并且生成的视频也具有更高的视觉质量和更大的多样性。<details>
<summary>Abstract</summary>
We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further propose a novel evaluation metric (AV-Align) to assess the alignment of generated videos with input audio samples. AV-Align is based on the detection and comparison of energy peaks in both modalities. In comparison to recent state-of-the-art approaches, our method generates videos that are better aligned with the input sound, both with respect to content and temporal axis. We also show that videos produced by our method present higher visual quality and are more diverse.
</details>
<details>
<summary>摘要</summary>
我们考虑一个生成多样化、现实的视频指导于自然语音样本的任务。这些视频需要与输入语音进行全球和时间的对齐：全球上，输入语音与输出视频的整体 semantic 关系存在，而时间上，每个语音样本都需要与对应的视频样本进行对齐。我们利用现有的文本受控视频生成模型和预训练的音频编码器模型。我们的方法基于一个轻量级的适配器网络，该网络学习将音频基于表示映射到文本受控视频生成模型的输入表示。因此，它也允许视频生成 conditioned 于文本、音频和，如果不同的说，也可以生成视频 conditioned 于文本和音频。我们在三个数据集上进行了广泛的验证，并提出了一个新的评价指标（AV-Align）来评估生成的视频与输入音频样本之间的对齐。AV-Align 基于检测和比较modalities 中的能量峰值。与最新的状态艺术方法相比，我们的方法生成的视频与输入声音更加吻合，同时也具有更高的视觉质量和多样性。
</details></li>
</ul>
<hr>
<h2 id="Prompt-and-Align-Prompt-Based-Social-Alignment-for-Few-Shot-Fake-News-Detection"><a href="#Prompt-and-Align-Prompt-Based-Social-Alignment-for-Few-Shot-Fake-News-Detection" class="headerlink" title="Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection"></a>Prompt-and-Align: Prompt-Based Social Alignment for Few-Shot Fake News Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16424">http://arxiv.org/abs/2309.16424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiayingwu19/prompt-and-align">https://github.com/jiayingwu19/prompt-and-align</a></li>
<li>paper_authors: Jiaying Wu, Shen Li, Ailin Deng, Miao Xiong, Bryan Hooi</li>
<li>for: 这篇论文旨在提出一种基于提示的几何 fake news 检测方法，以便利用预训练语言模型（PLM）的先锋知识和社交上下文 topology。</li>
<li>methods: 该方法首先将新闻文章包装在一个关于任务的文本提示中，然后使用 PLM 处理提示以直接提取任务特定的知识。此外，为了补充 PLM 的社交上下文信息而不导致额外训练开销，该方法还构建了新闻相似图，以捕捉新闻文章之间的真实性相似的信号。最后，该方法将提示的预测结果与图边缘进行准确度 Informed 对齐。</li>
<li>results: 对三个实际 benchmark 进行了广泛的实验， demonstrate 该方法可以在几何 fake news 检测任务中取得显著的新的状态 record，与传统的 Train-from-Scratch 方法相比，该方法可以减少标签稀缺问题。<details>
<summary>Abstract</summary>
Despite considerable advances in automated fake news detection, due to the timely nature of news, it remains a critical open question how to effectively predict the veracity of news articles based on limited fact-checks. Existing approaches typically follow a "Train-from-Scratch" paradigm, which is fundamentally bounded by the availability of large-scale annotated data. While expressive pre-trained language models (PLMs) have been adapted in a "Pre-Train-and-Fine-Tune" manner, the inconsistency between pre-training and downstream objectives also requires costly task-specific supervision. In this paper, we propose "Prompt-and-Align" (P&A), a novel prompt-based paradigm for few-shot fake news detection that jointly leverages the pre-trained knowledge in PLMs and the social context topology. Our approach mitigates label scarcity by wrapping the news article in a task-related textual prompt, which is then processed by the PLM to directly elicit task-specific knowledge. To supplement the PLM with social context without inducing additional training overheads, motivated by empirical observation on user veracity consistency (i.e., social users tend to consume news of the same veracity type), we further construct a news proximity graph among news articles to capture the veracity-consistent signals in shared readerships, and align the prompting predictions along the graph edges in a confidence-informed manner. Extensive experiments on three real-world benchmarks demonstrate that P&A sets new states-of-the-art for few-shot fake news detection performance by significant margins.
</details>
<details>
<summary>摘要</summary>
尽管自动化假新闻检测已经取得了很大的进步，但由于新闻的时效性，仍然是一个关键的开问如何有效地预测新闻文章的真实性基于有限的事实检查。现有的方法通常采用“训练从零”方法，它的基础是有限的 annotated 数据的可用性。而使用表达力强的预训练语言模型（PLM）的“预训练并精度调整”方法，也存在不一致性问题，需要耗费大量的任务特定超vision。在这篇论文中，我们提出了“提示和对齐”（P&A），一种新的提示基本 paradigm，可以同时利用 PLM 中的预训练知识和社交上下文 topology。我们的方法可以减轻标签缺乏问题，通过将新闻文章包装在任务相关的文本提示中，然后使用 PLM 处理提示，直接提取任务特定的知识。此外，为了补充 PLM 而不引入额外的训练负担，我们根据实际观察到的用户真实性一致性（即社交用户倾向于消耗同类真实性的新闻），构建了新闻 proximity graph，以捕捉新闻文章之间的真实性相似信号，并将提示预测与图 Edge 进行准确信息对齐。我们的实验表明，P&A 可以在三个真实世界 benchmark 上达到新的状态记录，在几何上击败现有方法。
</details></li>
</ul>
<hr>
<h2 id="AutoCLIP-Auto-tuning-Zero-Shot-Classifiers-for-Vision-Language-Models"><a href="#AutoCLIP-Auto-tuning-Zero-Shot-Classifiers-for-Vision-Language-Models" class="headerlink" title="AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models"></a>AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16414">http://arxiv.org/abs/2309.16414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Hendrik Metzen, Piyapat Saranrittichai, Chaithanya Kumar Mummadi</li>
<li>for: 这篇论文是为了提出一种自动调整零模型的方法，以提高零模型在不同的图像分类任务中的性能。</li>
<li>methods: 该方法使用了CLIP视力语言模型，并使用了不同的描述符模板来自动生成描述符集。在执行时，该方法根据图像描述符和描述符模板的相似度计算出每个图像的权重，以优化零模型的性能。</li>
<li>results: 该方法在多种视力语言模型、数据集和描述符模板上都有较高的性能，与基eline比较起来，该方法可以提高零模型的准确率 by up to 3%。<details>
<summary>Abstract</summary>
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. Up until now, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, i.e., classify to the class that maximizes cosine similarity between its averaged encoded class descriptors and the image encoding. However, weighing all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP tunes per-image weights to each prompt template at inference time, based on statistics of class descriptor-image similarities. AutoCLIP is fully unsupervised, has very low computational overhead, and can be easily implemented in few lines of code. We show that AutoCLIP outperforms baselines across a broad range of vision-language models, datasets, and prompt templates consistently and by up to 3 percent point accuracy.
</details>
<details>
<summary>摘要</summary>
“基于视力语言模型CLIP的分类器已经表现出惊人的零shot表现，覆盖了广泛的图像分类任务。先前的工作已经研究了不同的自动生成描述集方法，从手动工程ered模板到从大型语言模型获取的模板，以及从随机字和字符建立的模板。直到现在，从对应的编码类Descriptor中 derivation zero-shot分类器仍然几乎无changed，即将图像编码与类Descriptor的均值cosine相似性最大化来分类。但是，对所有类Descriptor做平等分配可能是不优化的，因为certainDescriptor在给定图像中更好地匹配视觉提示than others。在这项工作中，我们提出了AutoCLIP，一种自动调整零shot分类器的方法。AutoCLIP在推断时基于各个提示模板的统计信息，对每个图像进行权重调整。AutoCLIP是完全不supervised，computational overhead很低，可以实现几行代码。我们显示AutoCLIP在各种视力语言模型、数据集和提示模板上显示出consistent和高达3%的提升。”
</details></li>
</ul>
<hr>
<h2 id="Genetic-Engineering-Algorithm-GEA-An-Efficient-Metaheuristic-Algorithm-for-Solving-Combinatorial-Optimization-Problems"><a href="#Genetic-Engineering-Algorithm-GEA-An-Efficient-Metaheuristic-Algorithm-for-Solving-Combinatorial-Optimization-Problems" class="headerlink" title="Genetic Engineering Algorithm (GEA): An Efficient Metaheuristic Algorithm for Solving Combinatorial Optimization Problems"></a>Genetic Engineering Algorithm (GEA): An Efficient Metaheuristic Algorithm for Solving Combinatorial Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16413">http://arxiv.org/abs/2309.16413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Majid Sohrabi, Amir M. Fathollahi-Fard, Vasilii A. Gromov</li>
<li>for: 本研究旨在提出一种基于基因工程思想的新迪顺序算法，以解决 combinatorial optimization 问题中的限制。</li>
<li>methods: 该算法基于传统GA的搜索方法，并具有隔离、纯化、插入和表达新基因的功能，以便实现欢迎特征的 emergence 和选择优质基因。</li>
<li>results: 对比与现有算法，本研究的GEA在 benchmark 实例中显示出了更高的性能， demonstrably  displaying its potential as an innovative and efficient solution for combinatorial optimization problems。<details>
<summary>Abstract</summary>
Genetic Algorithms (GAs) are known for their efficiency in solving combinatorial optimization problems, thanks to their ability to explore diverse solution spaces, handle various representations, exploit parallelism, preserve good solutions, adapt to changing dynamics, handle combinatorial diversity, and provide heuristic search. However, limitations such as premature convergence, lack of problem-specific knowledge, and randomness of crossover and mutation operators make GAs generally inefficient in finding an optimal solution. To address these limitations, this paper proposes a new metaheuristic algorithm called the Genetic Engineering Algorithm (GEA) that draws inspiration from genetic engineering concepts. GEA redesigns the traditional GA while incorporating new search methods to isolate, purify, insert, and express new genes based on existing ones, leading to the emergence of desired traits and the production of specific chromosomes based on the selected genes. Comparative evaluations against state-of-the-art algorithms on benchmark instances demonstrate the superior performance of GEA, showcasing its potential as an innovative and efficient solution for combinatorial optimization problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Physics-Preserving-AI-Accelerated-Simulations-of-Plasma-Turbulence"><a href="#Physics-Preserving-AI-Accelerated-Simulations-of-Plasma-Turbulence" class="headerlink" title="Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence"></a>Physics-Preserving AI-Accelerated Simulations of Plasma Turbulence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16400">http://arxiv.org/abs/2309.16400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robin Greif, Frank Jenko, Nils Thuerey</li>
<li>for: study the turbulence in fluids, gases, and plasmas with reduced computational effort</li>
<li>methods: combine Large Eddy Simulation (LES) techniques with Machine Learning (ML) to model the small-scale dynamics</li>
<li>results: reduce the computational effort by about three orders of magnitude while retaining the statistical physical properties of the turbulent system<details>
<summary>Abstract</summary>
Turbulence in fluids, gases, and plasmas remains an open problem of both practical and fundamental importance. Its irreducible complexity usually cannot be tackled computationally in a brute-force style. Here, we combine Large Eddy Simulation (LES) techniques with Machine Learning (ML) to retain only the largest dynamics explicitly, while small-scale dynamics are described by an ML-based sub-grid-scale model. Applying this novel approach to self-driven plasma turbulence allows us to remove large parts of the inertial range, reducing the computational effort by about three orders of magnitude, while retaining the statistical physical properties of the turbulent system.
</details>
<details>
<summary>摘要</summary>
流体、气体和激骤中的混沌问题仍然是实际和基础上的开放问题。它的不可逆性通常无法通过直接计算方式解决。在这里，我们将大噪声 simulation（LES）技术与机器学习（ML）相结合，只 explictly 保留最大的动力学，而小规模动力学则由基于 ML 的子Grid 模型描述。通过这种新的方法，我们对自驱动激骤中的混沌可以大幅减少计算努力，同时保留混沌系统的统计物理性质。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Aware-Decision-Transformer-for-Stochastic-Driving-Environments"><a href="#Uncertainty-Aware-Decision-Transformer-for-Stochastic-Driving-Environments" class="headerlink" title="Uncertainty-Aware Decision Transformer for Stochastic Driving Environments"></a>Uncertainty-Aware Decision Transformer for Stochastic Driving Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16397">http://arxiv.org/abs/2309.16397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao</li>
<li>for: 本研究旨在提出一种可以在不活动交互的情况下学习策略的 Offline Reinforcement Learning（RL）框架，以便在自动驾驶任务中学习策略。</li>
<li>methods: 本研究使用了一种名为 UNcertainty-awaRE deciSion Transformer（UNREST）的新方法，它可以在不同的驱动环境下学习策略，不需要添加过程转移或复杂的生成模型。UNREST 使用了状态uncertainty的估计，以及Sequence segmentation，来学习策略。</li>
<li>results: 实验结果表明，UNREST 在多种驱动场景中表现出色，并且可以在不同的环境下学习策略。此外，UNREST 还可以在推理过程中 dynamically 评估环境的uncertainty，以便更加谨慎的规划。<details>
<summary>Abstract</summary>
Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less uncertain truncated returns, to learn from true outcomes of agent actions rather than environment transitions. We also dynamically evaluate environmental uncertainty during inference for cautious planning. Extensive experimental results demonstrate UNREST's superior performance in various driving scenarios and the power of our uncertainty estimation strategy.
</details>
<details>
<summary>摘要</summary>
无线连接学习（RL）在没有活动互动的情况下学习策略，使其特别适用于自动驾驶任务。最近的Transformers的成功激发了将线RL作为序列模型进行采用，这在长期任务中表现良好。然而，它们在随机环境中做出了过optimistic的假设，即可以通过同一种行为 consistently achieve同一个目标。在这篇论文中，我们提出了一种名为UNcertainty-awaRE deciSion Transformer（UNREST）的规划方法，用于在随机驾驶环境中无需引入附加的转移或复杂生成模型。Specifically，UNREST估算驱动环境中状态的uncertainty，通过转移和返回之间的conditional mutual information来进行估算。然后，UNREST将序列分成不同的部分，并在每个部分中学习不同的策略。在发现了驱动环境中的`uncertainty accumulation'和`temporal locality'性质后，UNREST将global returns在决策变换器中换为less uncertain的truncated returns，以学习agent动作的真正结果而不是环境转移。此外，UNREST在推理过程中动态评估环境的uncertainty，以进行谨慎的规划。广泛的实验结果表明UNREST在多种驾驶场景中表现出色，并证明了我们的uncertainty估算策略的力量。
</details></li>
</ul>
<hr>
<h2 id="Differential-2D-Copula-Approximating-Transforms-via-Sobolev-Training-2-Cats-Networks"><a href="#Differential-2D-Copula-Approximating-Transforms-via-Sobolev-Training-2-Cats-Networks" class="headerlink" title="Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks"></a>Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16391">http://arxiv.org/abs/2309.16391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flaviovdf/copulae">https://github.com/flaviovdf/copulae</a></li>
<li>paper_authors: Flavio Figueiredo, José Geraldo Fernandes, Jackson Silva, Renato M. Assunção</li>
<li>for: 本文是关于如何使用神经网络（NN）来非参数地预测二维共抽数学函数（Copula）的研究。</li>
<li>methods: 本文使用的方法是基于物理学 informed neural networks 和 Sobolev 训练的 2-Cats 方法，可以非参数地预测二维 Copula 的输出，并且尊重共抽函数 C 的数学性质。</li>
<li>results: 本文的实验结果表明，使用 2-Cats 方法可以更好地预测二维 Copula 的输出，并且比现有方法更加精准。<details>
<summary>Abstract</summary>
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>>共振是一种强大的统计工具，可以捕捉数据维度之间的依赖关系。当使用共振时，我们可以首先估算独立的一元分布函数，这是一个容易完成的任务，然后估算共振函数$C$，将独立分布函数相连接，这是一个困难的任务。对于二维数据，共振是一个二增函数的形式，即 $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$，其中 $\mathbf{I} = [0, 1]$。在这篇论文中，我们表明了使用神经网络（NNs）可以非参数地approximate任意二维共振。我们的方法，称为2-Cats，是基于物理学 Informed Neural Networks和 Sobolev Training литературе。不仅我们表明了我们可以更好地估算二维共振的输出，我们的方法是非参数的，并且尊重共振函数$C$的数学性质。
</details></li>
</ul>
<hr>
<h2 id="RLLTE-Long-Term-Evolution-Project-of-Reinforcement-Learning"><a href="#RLLTE-Long-Term-Evolution-Project-of-Reinforcement-Learning" class="headerlink" title="RLLTE: Long-Term Evolution Project of Reinforcement Learning"></a>RLLTE: Long-Term Evolution Project of Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16382">http://arxiv.org/abs/2309.16382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingqi Yuan, Zequn Zhang, Yang Xu, Shihao Luo, Bo Li, Xin Jin, Wenjun Zeng</li>
<li>for: 本研究旨在提供一个长期演化、极其对话式、开源的强化学习（RL）框架，并且提供一个完整的生态系统，以便RL研究和应用。</li>
<li>methods: RLLTE框架使用了完全解释-探索的角度来隔离RL算法，并且提供了许多元件来推进算法的发展和演化。</li>
<li>results: RLLTE框架是RL领域中第一个建立完整的生态系统，包括模型训练、评估、部署、参考中心和大语言模型（LLM） empowered copilot等元素，这些元素将会设定RL工程实践的标准，并且将会对学术和业界产生很大的刺激。<details>
<summary>Abstract</summary>
We present RLLTE: a long-term evolution, extremely modular, and open-source framework for reinforcement learning (RL) research and application. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms. More specifically, RLLTE decouples the RL algorithms completely from the exploitation-exploration perspective, providing a large number of components to accelerate algorithm development and evolution. In particular, RLLTE is the first RL framework to build a complete and luxuriant ecosystem, which includes model training, evaluation, deployment, benchmark hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set standards for RL engineering practice and be highly stimulative for industry and academia.
</details>
<details>
<summary>摘要</summary>
我们呈现RLLTE：一个长期演化、极其模块化、开源的强化学习（RL）框架，供研究和应用。不同于传统的RL框架，RLLTE不仅提供了高效的算法实现，还 serves as a 工具集，帮助开发者更快速地发展和演化算法。具体而言，RLLTE将RL算法与利用探索 perspective完全分离，提供了许多组件，以便增加算法的发展和演化。RLLTE 是首个RL框架，建立了完整且丰富的生态系统，包括模型训练、评估、部署、底�检查和大语言模型（LLM） empowered  copilot。RLLTE 预期会设定RL工程学习的标准，并对业界和学界产生强烈的刺激。
</details></li>
</ul>
<hr>
<h2 id="Conditional-normalizing-flows-for-IceCube-event-reconstruction"><a href="#Conditional-normalizing-flows-for-IceCube-event-reconstruction" class="headerlink" title="Conditional normalizing flows for IceCube event reconstruction"></a>Conditional normalizing flows for IceCube event reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16380">http://arxiv.org/abs/2309.16380</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asem010/legend-pice">https://github.com/asem010/legend-pice</a></li>
<li>paper_authors: Thorsten Glüsenkamp</li>
<li>for: 这个论文旨在描述如何使用常量正则流来推断高能νe和μν neutrino产生事件的方向和能量。</li>
<li>methods: 这个论文使用了条件正则流来 derivate每个个体事件的 posterior 分布，包括系统atic uncertainty。</li>
<li>results: 研究发现，在1TeV到100TeV的能量范围内，正则流可以更好地捕捉到高能νe和μν neutrino产生事件的方向和能量， especialy azimuth-zenith asymmetries，这些偏好在前一代分析中被忽略。<details>
<summary>Abstract</summary>
The IceCube Neutrino Observatory is a cubic-kilometer high-energy neutrino detector deployed in the Antarctic ice. Two major event classes are charged-current electron and muon neutrino interactions. In this contribution, we discuss the inference of direction and energy for these classes using conditional normalizing flows. They allow to derive a posterior distribution for each individual event based on the raw data that can include systematic uncertainties, which makes them very promising for next-generation reconstructions. For each normalizing flow we use the differential entropy and the KL-divergence to its maximum entropy approximation to interpret the results. The normalizing flows correctly incorporate complex optical properties of the Antarctic ice and their relation to the embedded detector. For showers, the differential entropy increases in regions of high photon absorption and decreases in clear ice. For muons, the differential entropy strongly correlates with the contained track length. Coverage is maintained, even for low photon counts and highly asymmetrical contour shapes. For high-photon counts, the distributions get narrower and become more symmetrical, as expected from the asymptotic theorem of Bernstein-von-Mises. For shower directional reconstruction, we find the region between 1 TeV and 100 TeV to potentially benefit the most from normalizing flows because of azimuth-zenith asymmetries which have been neglected in previous analyses by assuming symmetrical contours. Events in this energy range play a vital role in the recent discovery of the galactic plane diffuse neutrino emission.
</details>
<details>
<summary>摘要</summary>
冰砾激光观测站是一个立方公分级高能激光探测器，部署在南极冰中。我们使用条件正常化流来推断激光的方向和能量。这些正常化流可以基于Raw数据 derivate一个单个事件的 posterior distribution，包括系统atic uncertainty，这使其非常有前途的应用于下一代重建。我们使用 differential entropy 和 KL-divergence 来解释结果。正常化流正确地反映了南极冰的复杂光学性和探测器的相关性。在 shower 中，differential entropy 在高光吸收区域增加，而在clear ice 区域减少。对于 muon，differential entropy 与包含轨迹长度强相关。 regardless of low photon counts 和高度不均匀的外 contour shape，coverage 得以维护。对高 photon counts 的分布，distribution 变得更加窄和对称，这与Bernstein-von-Mises  asymptotic theorem 相符。在 shower 方向重建中，我们发现1TeV 到 100TeV 的能量范围可能受益最多，因为这个范围中的 azimuth-zenith 偏好未在前一analysis中考虑。这些事件在激光 diffuse neutrino emission 的发现中扮演了重要的角色。
</details></li>
</ul>
<hr>
<h2 id="Epistemic-Logic-Programs-a-study-of-some-properties"><a href="#Epistemic-Logic-Programs-a-study-of-some-properties" class="headerlink" title="Epistemic Logic Programs: a study of some properties"></a>Epistemic Logic Programs: a study of some properties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16344">http://arxiv.org/abs/2309.16344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefania Costantini, Andrea Formisano</li>
<li>for: 这篇论文旨在扩展Answer Set Programming（ASP）的 Epistemic Logic Programs（ELPs），并提供了这些程序的 semantics 的不同Characterizations。</li>
<li>methods: 本文使用了不同的semantic approach来描述 world views，并提出了一些新的semantic property，如 Epistemic Splitting Property，以便模块地计算 world views。</li>
<li>results: 本文分析了在bottom-up和top-down方法之间的换 perspective，并提出了一种基本的top-down方法，证明其等价于bottom-up方法。此外，本文还提出了一种扩展的top-down方法，其可以应用于许多现有的semantics，并且与bottom-up方法在Epistemically Stratified Programs中具有相同的性质。<details>
<summary>Abstract</summary>
Epistemic Logic Programs (ELPs), extend Answer Set Programming (ASP) with epistemic operators. The semantics of such programs is provided in terms of world views, which are sets of belief sets, i.e., syntactically, sets of sets of atoms. Different semantic approaches propose different characterizations of world views. Recent work has introduced semantic properties that should be met by any semantics for ELPs, like the Epistemic Splitting Property, that, if satisfied, allows to modularly compute world views in a bottom-up fashion, analogously to ``traditional'' ASP. We analyze the possibility of changing the perspective, shifting from a bottom-up to a top-down approach to splitting. We propose a basic top-down approach, which we prove to be equivalent to the bottom-up one. We then propose an extended approach, where our new definition: (i) is provably applicable to many of the existing semantics; (ii) operates similarly to ``traditional'' ASP; (iii) provably coincides under any semantics with the bottom-up notion of splitting at least on the class of Epistemically Stratified Programs (which are, intuitively, those where the use of epistemic operators is stratified); (iv) better adheres to common ASP programming methodology.
</details>
<details>
<summary>摘要</summary>
《知识逻辑编程（ELP）》，是将回答集编程（ASP）扩展到知识运算符的逻辑语言。知识运算符的 semantics 是通过世界观（set of belief sets，即语法上来说是集合的集合）来提供。不同的 semantics 可以对 world view 进行不同的Characterization。最近的工作已经提出了为 ELP 的 semantics 所需的一些 semantics properties，例如 epistemic splitting property，如果满足这个 property，那么可以使用分解来计算 world view 的模块化计算方式，类似于传统的 ASP。我们分析了从 bottom-up 到 top-down 的 Perspective 的改变，并提出了一种基本的 top-down 方法，我们证明了它与 bottom-up 方法是等价的。然后，我们提出了一种扩展的方法，其中我们新的定义：（i）可以应用于大多数现有的 semantics;（ii）与传统的 ASP 类似;（iii）在任何 semantics 下与 bottom-up 的 splitting 做出相同的结果;（iv）更好地遵循传统的 ASP 编程方法。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Risk-Prediction-of-Atrial-Fibrillation-from-the-12-Lead-ECG-by-Deep-Neural-Networks"><a href="#End-to-end-Risk-Prediction-of-Atrial-Fibrillation-from-the-12-Lead-ECG-by-Deep-Neural-Networks" class="headerlink" title="End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by Deep Neural Networks"></a>End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16335">http://arxiv.org/abs/2309.16335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mygithth27/af-risk-prediction-by-ecg-dnn">https://github.com/mygithth27/af-risk-prediction-by-ecg-dnn</a></li>
<li>paper_authors: Theogene Habineza, Antônio H. Ribeiro, Daniel Gedon, Joachim A. Behar, Antonio Luiz P. Ribeiro, Thomas B. Schön</li>
<li>For: The paper aims to develop and evaluate a machine learning algorithm to predict the risk of developing atrial fibrillation (AF) from electrocardiogram (ECG) data.* Methods: The authors use a deep neural network model to analyze the ECG data and evaluate the risk of AF. They also use a survival model to predict the probability of developing AF over time.* Results: The authors achieve an area under the receiver operating characteristic curve (AUC) score of 0.845, indicating good performance of the model in identifying patients who will develop AF in the future. They also find that patients in the high-risk group are 50% more likely to develop AF within 40 weeks, while patients in the minimal-risk group have more than 85% chance of remaining AF-free up to seven years.Here are the three points in Simplified Chinese text:* For: 这篇论文目标是开发和评估一种基于电cardiogram（ECG）数据的心律失常预测算法。* Methods: 作者使用深度神经网络模型分析ECG数据，评估心律失常风险。他们还使用生存模型预测心律失常发生的可能性。* Results: 作者实现了AUC分位函数分数0.845，表明模型在识别将来发展心律失常的能力良好。他们还发现高风险群体在40周内发展心律失常的可能性为50%，而最低风险群体在7年内保持心律失常自由的可能性高于85%。<details>
<summary>Abstract</summary>
Background: Atrial fibrillation (AF) is one of the most common cardiac arrhythmias that affects millions of people each year worldwide and it is closely linked to increased risk of cardiovascular diseases such as stroke and heart failure. Machine learning methods have shown promising results in evaluating the risk of developing atrial fibrillation from the electrocardiogram. We aim to develop and evaluate one such algorithm on a large CODE dataset collected in Brazil.   Results: The deep neural network model identified patients without indication of AF in the presented ECG but who will develop AF in the future with an AUC score of 0.845. From our survival model, we obtain that patients in the high-risk group (i.e. with the probability of a future AF case being greater than 0.7) are 50% more likely to develop AF within 40 weeks, while patients belonging to the minimal-risk group (i.e. with the probability of a future AF case being less than or equal to 0.1) have more than 85% chance of remaining AF free up until after seven years.   Conclusion: We developed and validated a model for AF risk prediction. If applied in clinical practice, the model possesses the potential of providing valuable and useful information in decision-making and patient management processes.
</details>
<details>
<summary>摘要</summary>
背景：心室 flutter (AF) 是全球每年数百万人的常见心血管疾病之一，与心血管疾病如心卫和心力衰竭存在高度的相关性。机器学习方法在电子心脏图像中评估 AF 的风险表现出了扎实的成果。我们计划在大型 CODE 数据集上开发和评估一个这种算法。结果：我们的深度神经网络模型在给定的 ECG 中能够正确地预测无症状 AF 患者，其 AUC 分数为 0.845。从我们的生存模型中，我们发现高风险群（即未来 AF  случа发率大于 0.7）的患者在 40 周内的发生 AF 的概率为 50%，而低风险群（即未来 AF  случа发率不大于或等于 0.1）的患者在 7 年后仍然保持 AF 无症状的概率高于 85%。结论：我们开发和验证了一种 AF 风险预测模型。如果在临床实践中应用，这种模型具有提供价值和有用信息的潜力，可以帮助决策和患者管理过程中做出更好的决策。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-transformers-with-recursively-composed-multi-grained-representations"><a href="#Augmenting-transformers-with-recursively-composed-multi-grained-representations" class="headerlink" title="Augmenting transformers with recursively composed multi-grained representations"></a>Augmenting transformers with recursively composed multi-grained representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16319">http://arxiv.org/abs/2309.16319</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ant-research/structuredlm_rtdt">https://github.com/ant-research/structuredlm_rtdt</a></li>
<li>paper_authors: Xiang Hu, Qingyang Zhu, Kewei Tu, Wei Wu</li>
<li>for: 这个论文的目的是提出一种能够Explicitly model hierarchical syntactic structures of raw texts的Recursive Composition Augmented Transformer（ReCAT）模型，以便在学习和推理过程中不需要靠托金树来模型语法结构。</li>
<li>methods: 该模型使用了一种新的Contextual Inside-Outside（CIO）层来学习语法结构，这个层通过底向上和上向下的pas来学习语法上下文，并将这些上下文化的表示与Transformer模型结合起来，从而实现深度的间隔和外部交互。</li>
<li>results:  experiments表明，ReCAT模型可以在各种句子级和span级任务上显著超越vanilla Transformer模型，同时与其他基于Recursive Networks和Transformers的基elines一起在自然语言理解任务上表现出色。此外，ReCAT模型所induced的层次结构与人工标注的语法树 exhibit strong consistency，这表明该模型具有良好的解释性。<details>
<summary>Abstract</summary>
We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, the ReCAT model can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. Moreover, the CIO layers can be jointly pre-trained with Transformers, making ReCAT enjoy scaling ability, strong performance, and interpretability at the same time. We conduct experiments on various sentence-level and span-level tasks. Evaluation results indicate that ReCAT can significantly outperform vanilla Transformer models on all span-level tasks and baselines that combine recursive networks with Transformers on natural language inference tasks. More interestingly, the hierarchical structures induced by ReCAT exhibit strong consistency with human-annotated syntactic trees, indicating good interpretability brought by the CIO layers.
</details>
<details>
<summary>摘要</summary>
我们介绍ReCAT模型，这是一种基于重复组合的Transformer模型，可以直接模型文本的层次结构，不需要在学习和推断过程中依赖黄金树。现有研究限制数据只能按照层次结构进行组织，因此缺乏间隔通信。为解决这问题，我们提出了一种新的内部外部（CIO）层，该层可以在底层和顶层之间学习各个 span 的上下文化表示，其中底层 pass 将低级 span 组合成高级 span，而顶层 pass 将内部和外部信息相结合。通过在Transformer模型中堆叠多个CIO层，ReCAT模型可以实现深入的间隔和深入的span间交互，并生成全面上下文ualized的表示。此外，CIO层可以与Transformer模型进行共同预训练，使ReCAT模型具有扩展性、良好的性能和可解释性。我们在各种句子级和span级任务上进行了实验，结果表明ReCAT模型可以在所有span级任务上明显超过vanilla Transformer模型和将重复网络与Transformer模型结合的基eline。此外，ReCAT模型中的层次结构与人工标注的语法树具有强相关性，这表明CIO层带来的解释性非常好。
</details></li>
</ul>
<hr>
<h2 id="Efficiency-Separation-between-RL-Methods-Model-Free-Model-Based-and-Goal-Conditioned"><a href="#Efficiency-Separation-between-RL-Methods-Model-Free-Model-Based-and-Goal-Conditioned" class="headerlink" title="Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned"></a>Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16291">http://arxiv.org/abs/2309.16291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brieuc Pinon, Raphaël Jungers, Jean-Charles Delvenne</li>
<li>for: 这种 исследование证明了一种广泛应用的强化学习（RL）算法的基本限制。</li>
<li>methods: 这种限制适用于模型自由RL方法以及一种广泛的模型基于方法，如搜索树的规划。</li>
<li>results: 这种限制表明在RL问题中，这些方法需要对环境进行 exponential 的交互时间来找到优化行为，但是存在一种方法，不是专门针对这种家族问题，可以高效解决这些问题。<details>
<summary>Abstract</summary>
We prove a fundamental limitation on the efficiency of a wide class of Reinforcement Learning (RL) algorithms. This limitation applies to model-free RL methods as well as a broad range of model-based methods, such as planning with tree search.   Under an abstract definition of this class, we provide a family of RL problems for which these methods suffer a lower bound exponential in the horizon for their interactions with the environment to find an optimal behavior. However, there exists a method, not tailored to this specific family of problems, which can efficiently solve the problems in the family.   In contrast, our limitation does not apply to several types of methods proposed in the literature, for instance, goal-conditioned methods or other algorithms that construct an inverse dynamics model.
</details>
<details>
<summary>摘要</summary>
我们证明了一种抽象类别的强制性限制，该限制适用于广泛的强制学习（RL）算法。这种限制适用于无模型RL方法以及一种广泛的模型基于方法，如搜索树的规划。 我们提供了一家RL问题的家族，其中这些方法在与环境交互时需要至少 exponential 的时间来找到最优行为。然而， существует一种方法，不是专门针对这个家族的问题，它可以高效地解决这些问题。在此之外，我们的限制不适用于文献中提出的一些方法，如目标条件方法或构建反动动力模型的方法。
</details></li>
</ul>
<hr>
<h2 id="LawBench-Benchmarking-Legal-Knowledge-of-Large-Language-Models"><a href="#LawBench-Benchmarking-Legal-Knowledge-of-Large-Language-Models" class="headerlink" title="LawBench: Benchmarking Legal Knowledge of Large Language Models"></a>LawBench: Benchmarking Legal Knowledge of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16289">http://arxiv.org/abs/2309.16289</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/open-compass/lawbench">https://github.com/open-compass/lawbench</a></li>
<li>paper_authors: Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge</li>
<li>for: 法律领域中的大型自然语言模型（LLMs）的评估和发展</li>
<li>methods: 提出了一个全面的评估标准库LawBench，以评估 LLMS 在法律领域中的能力，包括 memorization、理解和应用三种知识水平</li>
<li>results: GPT-4 在法律领域中表现最佳，超过其他 LLMs 的表现，但是这些 LLMs 在特定的法律任务中仍然有很大的差异，而且需要进一步的调整和改进以取得可靠的结果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation. We perform extensive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4 remains the best-performing LLM in the legal domain, surpassing the others by a significant margin. While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks. All data, model predictions and evaluation code are released in https://github.com/open-compass/LawBench/. We hope this benchmark provides in-depth understanding of the LLMs' domain-specified capabilities and speed up the development of LLMs in the legal domain.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经展示了各种能力，但当它们应用到高度特殊化和安全敏感的法律领域时，它们是否具备了法律知识和可靠地完成法律相关任务？为了解决这个问题，我们提出了一个完整的评估指标 LawBench。 LawBench 已经精心设计，以确定 LLMs 的法律能力的三种 когнітив水平：（1）法律知识储存：LLMs 是否可以储存需要的法律概念、文章和事实；（2）法律知识理解：LLMs 是否可以理解法律文本中的实体、事件和关系；（3）法律知识应用：LLMs 是否可以正确地利用其法律知识，并且做出需要的推理步骤来解决实际法律任务。 LawBench 包含 20 个多样化的任务，涵盖 5 种任务类型：单 Label 分类（SLC）、多 Label 分类（MLC）、回推、提取和生成。我们对 51 个 LLMs 进行了广泛的评估，包括 20 种多语言 LLMs、22 个中文化 LLMs 和 9 个法律特定 LLMs。结果显示 GPT-4 在法律领域中仍然是最佳performing LLM，与其他 LLMs 相比，具有明显的优势。虽然对法律特定文本进行了 fine-tuning，但我们仍然很遥か від法律任务中可靠且可靠的 LLMs。所有数据、模型预测和评估代码都已经在 https://github.com/open-compass/LawBench/ 发布。我们希望这个底线可以帮助我们更深入了解 LLMs 在法律领域中的特定能力，并且加快法律领域中 LLMs 的发展。
</details></li>
</ul>
<hr>
<h2 id="High-Throughput-Training-of-Deep-Surrogates-from-Large-Ensemble-Runs"><a href="#High-Throughput-Training-of-Deep-Surrogates-from-Large-Ensemble-Runs" class="headerlink" title="High Throughput Training of Deep Surrogates from Large Ensemble Runs"></a>High Throughput Training of Deep Surrogates from Large Ensemble Runs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16743">http://arxiv.org/abs/2309.16743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Meyer, Marc Schouler, Robert Alexander Caulk, Alejandro Ribés, Bruno Raffin</li>
<li>for: 提高数值解析器的计算效率（accelerate numerical solvers）</li>
<li>methods: 使用深度学习方法（deep learning approaches）和多线程并行（multiple levels of parallelism）生成丰富的数据集，并将其直接流动到学习模型中进行训练（online training）</li>
<li>results: 在训练一个全连接网络作为热方程的替代方案时，提高了精度47%和批处理速率13倍，可以在2小时内训练8TB的数据。<details>
<summary>Abstract</summary>
Recent years have seen a surge in deep learning approaches to accelerate numerical solvers, which provide faithful but computationally intensive simulations of the physical world. These deep surrogates are generally trained in a supervised manner from limited amounts of data slowly generated by the same solver they intend to accelerate. We propose an open-source framework that enables the online training of these models from a large ensemble run of simulations. It leverages multiple levels of parallelism to generate rich datasets. The framework avoids I/O bottlenecks and storage issues by directly streaming the generated data. A training reservoir mitigates the inherent bias of streaming while maximizing GPU throughput. Experiment on training a fully connected network as a surrogate for the heat equation shows the proposed approach enables training on 8TB of data in 2 hours with an accuracy improved by 47% and a batch throughput multiplied by 13 compared to a traditional offline procedure.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习方法在加速数值方法方面得到了广泛应用，这些深度代理模型提供了诚实的但计算昂贵的物理世界 simulate。这些深度代理通常在监督式的方式下从有限量的数据中训练。我们提出了一个开源框架，该框架可以在大量的 ensemble 运行中在线训练这些模型。它利用多级并行来生成丰富的数据集。框架避免了 I/O 瓶颈和存储问题，直接流动生成的数据。一个训练储备池 Mitigates 流动中的遗传性，同时 maximizing GPU throughput。在训练一个完全连接的网络作为热方程代理方法中，我们的方法可以在 2 小时内训练 8TB 的数据，并提高了精度 by 47% 和批处理速率 by 13 比 traditional offline 方法。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Heterogeneous-Federated-Cross-Correlation-and-Instance-Similarity-Learning"><a href="#Generalizable-Heterogeneous-Federated-Cross-Correlation-and-Instance-Similarity-Learning" class="headerlink" title="Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning"></a>Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16286">http://arxiv.org/abs/2309.16286</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenkehuang/fccl">https://github.com/wenkehuang/fccl</a></li>
<li>paper_authors: Wenke Huang, Mang Ye, Zekun Shi, Bo Du</li>
<li>for: 这个论文的目的是解决联合学习中的两个主要挑战：模型多样性和溃败性忘却。</li>
<li>methods: 这篇论文提出了一个新的FCCL+方法，即联合相似性学习加浓度转移，通过无关的公开数据来解决内部领域对话障碍，并在本地更新阶段引入联合非目标传播，以保持跨领域知识。</li>
<li>results: 实验结果显示，FCCL+方法能够有效地解决模型多样性和溃败性忘却问题，并且在不同的领域转移情况下具有较好的一致性和稳定性。<details>
<summary>Abstract</summary>
Federated learning is an important privacy-preserving multi-party learning paradigm, involving collaborative learning with others and local updating on private data. Model heterogeneity and catastrophic forgetting are two crucial challenges, which greatly limit the applicability and generalizability. This paper presents a novel FCCL+, federated correlation and similarity learning with non-target distillation, facilitating the both intra-domain discriminability and inter-domain generalization. For heterogeneity issue, we leverage irrelevant unlabeled public data for communication between the heterogeneous participants. We construct cross-correlation matrix and align instance similarity distribution on both logits and feature levels, which effectively overcomes the communication barrier and improves the generalizable ability. For catastrophic forgetting in local updating stage, FCCL+ introduces Federated Non Target Distillation, which retains inter-domain knowledge while avoiding the optimization conflict issue, fulling distilling privileged inter-domain information through depicting posterior classes relation. Considering that there is no standard benchmark for evaluating existing heterogeneous federated learning under the same setting, we present a comprehensive benchmark with extensive representative methods under four domain shift scenarios, supporting both heterogeneous and homogeneous federated settings. Empirical results demonstrate the superiority of our method and the efficiency of modules on various scenarios.
</details>
<details>
<summary>摘要</summary>
federated 学习是一种重要的隐私保护多方学习模式，协同学习他人的私人数据。模型多样性和悬峰性忘记是这种模式的两大挑战，它们很大程度限制了应用和泛化性。这篇论文提出了一种新的FCCL+，基于联合相关学习和非目标液化，解决了两个挑战。对于多样性问题，我们利用无关的公共数据来进行参与者之间的交流。我们构建了垂直相关矩阵，并将实例相似性分布对应于logits和特征层面，这有效地超越了交流障碍和提高了泛化能力。对于本地更新阶段的悬峰性问题，FCCL+引入了联邦非目标液化，保留了域之间知识，并避免了优化冲突问题，通过描述 posterior classes 关系来全面地泛化知识。由于现有的多样性联邦学习没有标准的评估标准，我们提出了一个完整的 bencmark，包括了四个域Shift 情况，支持 both heterogeneous和 homogeneous 联邦设置。我们的实验结果表明，我们的方法的优越性和模块的效率在各种场景中。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Identification-of-Stone-Handling-Behaviour-in-Japanese-Macaques-Using-LabGym-Artificial-Intelligence"><a href="#Automatic-Identification-of-Stone-Handling-Behaviour-in-Japanese-Macaques-Using-LabGym-Artificial-Intelligence" class="headerlink" title="Automatic Identification of Stone-Handling Behaviour in Japanese Macaques Using LabGym Artificial Intelligence"></a>Automatic Identification of Stone-Handling Behaviour in Japanese Macaques Using LabGym Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.07812">http://arxiv.org/abs/2310.07812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Théo Ardoin, Cédric Sueur</li>
<li>for: 本研究旨在评估LabGym工具的适用性 дляPrimates行为分析，主要采用日本黑猩猩为模式动物。</li>
<li>methods: 本研究采用了一种新的行为分析模型，通过使用LabGym工具来检测日本黑猩猩石头拥有行为。</li>
<li>results: 研究成功开发了一个高度准确的日本黑猩猩石头拥有行为检测模型，但因为时间限制而无法取得具体的量化数据。<details>
<summary>Abstract</summary>
The latest advancements in artificial intelligence technology have opened doors to the analysis of intricate behaviours. In light of this, ethologists are actively exploring the potential of these innovations to streamline the time-intensive process of behavioural analysis using video data. In the realm of primatology, several tools have been developed for this purpose. Nonetheless, each of these tools grapples with technical constraints that we aim to surmount. To address these limitations, we have established a comprehensive protocol designed to harness the capabilities of a cutting-edge tool, LabGym. Our primary objective was to evaluate LabGym's suitability for the analysis of primate behaviour, with a focus on Japanese macaques as our model subjects. We have successfully developed a model that demonstrates a high degree of accuracy in detecting Japanese macaques stone-handling behaviour. Our behavioural analysis model was completed as per our initial expectations and LabGym succeed to recognise stone-handling behaviour on videos. However, it is important to note that our study's ability to draw definitive conclusions regarding the quality of the behavioural analysis is hampered by the absence of quantitative data within the specified timeframe. Nevertheless, our model represents the pioneering endeavour, as far as our knowledge extends, in leveraging LabGym for the analysis of primate behaviours. It lays the groundwork for potential future research in this promising field.
</details>
<details>
<summary>摘要</summary>
最新的人工智能技术的发展已经开启了复杂行为的分析的大门。鉴于这一点，生物学家正在积极探索这些创新的潜在用于视频数据分析的可能性。在primatology领域，一些工具已经被开发出来用于这种目的。然而，每种工具都面临着技术上的限制，我们希望通过解决这些限制来进一步发展。为了实现这一目标，我们已经建立了一个完整的协议，旨在利用当今最先进的工具——LabGym——来实现 primate 行为的分析。我们的首要目标是评估 LabGym 是否适用于日本猕猴的行为分析，特别是日本猕猴的石头处理行为。我们已经成功地建立了一个模型，具有高度准确性的检测日本猕猴石头处理行为。我们的行为分析模型按照我们的初始预期进行了完成，LabGym 成功地在视频中识别出日本猕猴的石头处理行为。然而，我们的研究无法在指定时间内提供量化数据，因此我们的研究结论的可靠性受到限制。不过，我们的模型表示了在我们知道的范围内的开拓性的尝试，它为未来可能的研究奠定了基础。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Multicarrier-Multiantenna-Systems-for-LoS-Channel-Charting"><a href="#Optimizing-Multicarrier-Multiantenna-Systems-for-LoS-Channel-Charting" class="headerlink" title="Optimizing Multicarrier Multiantenna Systems for LoS Channel Charting"></a>Optimizing Multicarrier Multiantenna Systems for LoS Channel Charting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03762">http://arxiv.org/abs/2310.03762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taha Yassine, Luc Le Magoarou, Matthieu Crussière, Stephane Paquelet</li>
<li>for: 本研究旨在提出一种可以在多载波多天线系统中学习渠道图的方法，以便实现高精度的用户equipment（UE）位置推算。</li>
<li>methods: 本研究使用了一种基于距离度量的方法，以学习渠道图。但是，该距离度量受到 periodicity和振荡性的影响，导致用户远离的 UE 可能会被误判为近距离。</li>
<li>results: 本研究提供了一种改进的距离度量，以消除 periodicity和振荡性的影响。此外，研究还提出了一些设计方法，以便实现高质量的渠道图学习。实验 validate 了这些结论，并在不同的场景下进行了实验验证。<details>
<summary>Abstract</summary>
Channel charting (CC) consists in learning a mapping between the space of raw channel observations, made available from pilot-based channel estimation in multicarrier multiantenna system, and a low-dimensional space where close points correspond to channels of user equipments (UEs) close spatially. Among the different methods of learning this mapping, some rely on a distance measure between channel vectors. Such a distance should reliably reflect the local spatial neighborhoods of the UEs. The recently proposed phase-insensitive (PI) distance exhibits good properties in this regards, but suffers from ambiguities due to both its periodic and oscillatory aspects, making users far away from each other appear closer in some cases. In this paper, a thorough theoretical analysis of the said distance and its limitations is provided, giving insights on how they can be mitigated. Guidelines for designing systems capable of learning quality charts are consequently derived. Experimental validation is then conducted on synthetic and realistic data in different scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="UPB-ACTI-Detecting-Conspiracies-using-fine-tuned-Sentence-Transformers"><a href="#UPB-ACTI-Detecting-Conspiracies-using-fine-tuned-Sentence-Transformers" class="headerlink" title="UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers"></a>UPB @ ACTI: Detecting Conspiracies using fine tuned Sentence Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16275">http://arxiv.org/abs/2309.16275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrei Paraschiv, Mihai Dascalu</li>
<li>for: 探讨假设论文探测，以便提高信息准确性和社会信任。</li>
<li>methods: 使用预训练句子转换器模型和数据增强技术。</li>
<li>results: 在ACTI @ EVALITA 2023分类任务中，我们的方法取得了85.71%的 binary 分类和91.23%的细化假设主题分类的 F1 分数，超过其他竞争系统。<details>
<summary>Abstract</summary>
Conspiracy theories have become a prominent and concerning aspect of online discourse, posing challenges to information integrity and societal trust. As such, we address conspiracy theory detection as proposed by the ACTI @ EVALITA 2023 shared task. The combination of pre-trained sentence Transformer models and data augmentation techniques enabled us to secure first place in the final leaderboard of both sub-tasks. Our methodology attained F1 scores of 85.71% in the binary classification and 91.23% for the fine-grained conspiracy topic classification, surpassing other competing systems.
</details>
<details>
<summary>摘要</summary>
共谋论变得在网络讨论中变得更加出名，对信息完整性和社会信任构成挑战。因此，我们Addresses conspiracy theory detection as proposed by the ACTI @ EVALITA 2023 shared task。 combining pre-trained sentence Transformer models and data augmentation techniques enabled us to secure first place in the final leaderboard of both sub-tasks。 Our methodology attained F1 scores of 85.71% in the binary classification and 91.23% for the fine-grained conspiracy topic classification，Surpassing other competing systems。
</details></li>
</ul>
<hr>
<h2 id="Cooperation-Dynamics-in-Multi-Agent-Systems-Exploring-Game-Theoretic-Scenarios-with-Mean-Field-Equilibria"><a href="#Cooperation-Dynamics-in-Multi-Agent-Systems-Exploring-Game-Theoretic-Scenarios-with-Mean-Field-Equilibria" class="headerlink" title="Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria"></a>Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16263">http://arxiv.org/abs/2309.16263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dawnorak/marl-kart-simulator">https://github.com/dawnorak/marl-kart-simulator</a></li>
<li>paper_authors: Vaigarai Sathi, Sabahat Shaik, Jaswanth Nidamanuri</li>
<li>for:  investigate strategies to invoke cooperation in game-theoretic scenarios, such as the Iterated Prisoner’s Dilemma, where agents must optimize both individual and group outcomes.</li>
<li>methods:  analyze existing cooperative strategies for their effectiveness in promoting group-oriented behavior in repeated games, and propose modifications that encourage group rewards while also resulting in higher individual gains.</li>
<li>results:  extend the study to scenarios with exponentially growing agent populations ($N \longrightarrow +\infty$), where traditional computation and equilibrium determination are challenging, and establish equilibrium solutions and reward structures for infinitely large agent sets in repeated games using mean-field game theory. Offer practical insights through simulations using the Multi Agent-Posthumous Credit Assignment trainer, and explore adapting simulation algorithms to create scenarios favoring cooperation for group rewards.<details>
<summary>Abstract</summary>
Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent Reinforcement Learning (MARL), often requiring agents to balance individual gains with collective rewards. In this regard, this paper aims to investigate strategies to invoke cooperation in game-theoretic scenarios, namely the Iterated Prisoner's Dilemma, where agents must optimize both individual and group outcomes. Existing cooperative strategies are analyzed for their effectiveness in promoting group-oriented behavior in repeated games. Modifications are proposed where encouraging group rewards will also result in a higher individual gain, addressing real-world dilemmas seen in distributed systems. The study extends to scenarios with exponentially growing agent populations ($N \longrightarrow +\infty$), where traditional computation and equilibrium determination are challenging. Leveraging mean-field game theory, equilibrium solutions and reward structures are established for infinitely large agent sets in repeated games. Finally, practical insights are offered through simulations using the Multi Agent-Posthumous Credit Assignment trainer, and the paper explores adapting simulation algorithms to create scenarios favoring cooperation for group rewards. These practical implementations bridge theoretical concepts with real-world applications.
</details>
<details>
<summary>摘要</summary>
合作是多智能体系统（MAS）和多智能体奖励学习（MARL）的基本要素，经常需要智能体寻求 equilibrio between individual gains 和 collective rewards. 在这种情况下，本文旨在调查invoking cooperation in game-theoretic scenarios，具体来说是迭代犯罪者的困境，智能体需要优化个人和群体的结果。现有的合作策略被分析是否能够促进群体 oriented 行为在重复游戏中。提议 modifying these strategies to encourage group rewards, which will also result in higher individual gains, addressing real-world dilemmas seen in distributed systems.更over, this study extends to scenarios with exponentially growing agent populations ($N \longrightarrow +\infty$), where traditional computation and equilibrium determination are challenging. By leveraging mean-field game theory, equilibrium solutions and reward structures are established for infinitely large agent sets in repeated games. Finally, practical insights are offered through simulations using the Multi Agent-Posthumous Credit Assignment trainer, and the paper explores adapting simulation algorithms to create scenarios favoring cooperation for group rewards. These practical implementations bridge theoretical concepts with real-world applications.
</details></li>
</ul>
<hr>
<h2 id="QonFusion-–-Quantum-Approaches-to-Gaussian-Random-Variables-Applications-in-Stable-Diffusion-and-Brownian-Motion"><a href="#QonFusion-–-Quantum-Approaches-to-Gaussian-Random-Variables-Applications-in-Stable-Diffusion-and-Brownian-Motion" class="headerlink" title="QonFusion – Quantum Approaches to Gaussian Random Variables: Applications in Stable Diffusion and Brownian Motion"></a>QonFusion – Quantum Approaches to Gaussian Random Variables: Applications in Stable Diffusion and Brownian Motion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16258">http://arxiv.org/abs/2309.16258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BoltzmannEntropy/QonFusion">https://github.com/BoltzmannEntropy/QonFusion</a></li>
<li>paper_authors: Shlomo Kashani</li>
<li>For: The paper proposes a strategy for generating Gaussian random variables (GRVs) using non-parametric quantum circuits, as an alternative to conventional pseudorandom number generators (PRNGs) such as the \textbf{torch.rand} function in PyTorch.* Methods: The paper introduces a Quantum Gaussian Random Variable Generator that fulfills dual roles in simulating both Stable Diffusion (SD) and Brownian Motion (BM), diverging from previous methods that use parametric quantum circuits (PQCs) and variational quantum eigensolvers (VQEs). The proposed method does not require a computationally demanding optimization process to tune parameters.* Results: The paper presents QonFusion, a Python library that facilitates assimilating the proposed methodology into existing computational frameworks, and validates QonFusion through extensive statistical testing, confirming the statistical equivalence of the Gaussian samples from the quantum approach to classical counterparts within defined significance limits.<details>
<summary>Abstract</summary>
In the present study, we delineate a strategy focused on non-parametric quantum circuits for the generation of Gaussian random variables (GRVs). This quantum-centric approach serves as a substitute for conventional pseudorandom number generators (PRNGs), such as the \textbf{torch.rand} function in PyTorch. The principal theme of our research is the incorporation of Quantum Random Number Generators (QRNGs) into classical models of diffusion. Notably, our Quantum Gaussian Random Variable Generator fulfills dual roles, facilitating simulations in both Stable Diffusion (SD) and Brownian Motion (BM). This diverges markedly from prevailing methods that utilize parametric quantum circuits (PQCs), often in conjunction with variational quantum eigensolvers (VQEs). Although conventional techniques can accurately approximate ground states in complex systems or model elaborate probability distributions, they require a computationally demanding optimization process to tune parameters. Our non-parametric strategy obviates this necessity. To facilitate assimilating our methodology into existing computational frameworks, we put forward QonFusion, a Python library congruent with both PyTorch and PennyLane, functioning as a bridge between classical and quantum computational paradigms. We validate QonFusion through extensive statistical testing, including tests which confirm the statistical equivalence of the Gaussian samples from our quantum approach to classical counterparts within defined significance limits. QonFusion is available at \url{https://boltzmannentropy.github.io/qonfusion.github.io/} to reproduce all findings here.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了一种非参数化量子环境的推算方法，用于生成高斯随机变量（GRV）。这种量子中心的方法可以替换传统的 Pseudorandom Number Generators（PRNG），如PyTorch中的torch.rand函数。我们的研究的主题是将量子随机数生成器（QRNG）integrated into classical diffusion models。与之前的方法不同，我们的量子高斯随机变量生成器同时可以实现稳定扩散（SD）和布朗运动（BM）的模拟。与 Parametric Quantum Circuits（PQC）相比，我们的非参数化方法不需要进行 computationally demanding 的参数调整。为使我们的方法更容易与现有计算框架集成，我们提出了QonFusion，一个Python库，与PyTorch和PennyLane相容，用于连接类型和量子计算 paradigm。我们通过了广泛的统计测试，包括确认我们量子方法生成的高斯样本与传统 counterparts 在定义的 estadístico 限度内 Statistical Equivalence。QonFusion可以在 \url{https://boltzmannentropy.github.io/qonfusion.github.io/} 上获取，以便复现这里的所有发现。
</details></li>
</ul>
<hr>
<h2 id="Nondestructive-chicken-egg-fertility-detection-using-CNN-transfer-learning-algorithms"><a href="#Nondestructive-chicken-egg-fertility-detection-using-CNN-transfer-learning-algorithms" class="headerlink" title="Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms"></a>Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16257">http://arxiv.org/abs/2309.16257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoffan Saifullah, Rafal Drezewski, Anton Yudhana, Andri Pranolo, Wilis Kaswijanti, Andiko Putro Suryotomo, Seno Aji Putra, Alin Khaliduzzaman, Anton Satria Prabuwono, Nathalie Japkowicz</li>
<li>for: 这个研究探索了将Convolutional Neural Network(CNN)的传播学习应用于不破坏性鸡蛋 fertility detection，以提高精禽育成实践中的精确度。</li>
<li>methods: 研究使用了四个模型：VGG16、ResNet50、InceptionNet和MobileNet，并将它们训练和评估在一个dataset（200个单一鸡蛋图像）上，使用了增强图像（旋转、反转、缩小、缩寸和反射）。</li>
<li>results: 训练结果显示所有模型具有高精度，能够正确地学习和类别鸡蛋的 fertility 状态，但在测试集中，不同模型之间存在差异。InceptionNet 表现最佳，在所有评估指标中具有最高精度，对于断言fertile 和 non-fertile 鸡蛋的准确率为 0.98。<details>
<summary>Abstract</summary>
This study explored the application of CNN-Transfer Learning for nondestructive chicken egg fertility detection for precision poultry hatchery practices. Four models, VGG16, ResNet50, InceptionNet, and MobileNet, were trained and evaluated on a dataset (200 single egg images) using augmented images (rotation, flip, scale, translation, and reflection). Although the training results demonstrated that all models achieved high accuracy, indicating their ability to accurately learn and classify chicken eggs' fertility state, when evaluated on the testing set, variations in accuracy and performance were observed. InceptionNet exhibited the best overall performance, accurately classifying fertile and non-fertile eggs. It demonstrated excellent performance in both training and testing sets in all parameters of the evaluation metrics. In testing set, it achieved an accuracy of 0.98, a sensitivity of 1 for detecting fertile eggs, and a specificity of 0.96 for identifying non-fertile eggs. The higher performance is attributed to its unique architecture efficiently capturing features at different scales leading to improved accuracy and robustness. Further optimization and fine-tuning of the models might necessary to address the limitations in accurately detecting fertile and non-fertile eggs in case of other models. This study highlighted the potential of CNN-Transfer Learning for nondestructive fertility detection and emphasizes the need for further research to enhance the models' capabilities and ensure accurate classification.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Supervised-Learning-Models-for-Early-Detection-of-Albuminuria-Risk-in-Type-2-Diabetes-Mellitus-Patients"><a href="#Supervised-Learning-Models-for-Early-Detection-of-Albuminuria-Risk-in-Type-2-Diabetes-Mellitus-Patients" class="headerlink" title="Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients"></a>Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16742">http://arxiv.org/abs/2309.16742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arief Purnama Muharram, Dicky Levenus Tahapary, Yeni Dwi Lestari, Randy Sarayar, Valerie Josephine Dirjayanto</li>
<li>for: 这项研究旨在开发一种超vised学习模型，用于预测TYPE 2 糖尿病患者发展albuminuria的风险。</li>
<li>methods: 该研究使用了10个特征特征和1个目标特征（albuminuria），并选择了Na&quot;ive Bayes、支持向量机（SVM）、决策树、Random Forest、AdaBoost、XGBoost和多层权重神经网络（MLP）等10种超vised学习算法进行训练。</li>
<li>results: 实验结果显示，MLP表现最佳，其准确率和f1-score分别达0.74和0.75，表明其适用于预测TYPE 2 糖尿病患者albuminuria的creening purposes。<details>
<summary>Abstract</summary>
Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithms. It consisted of 10 attributes as features and 1 attribute as the target (albuminuria). Upon conducting the experiments, the MLP demonstrated superior performance compared to the other algorithms. It achieved accuracy and f1-score values as high as 0.74 and 0.75, respectively, making it suitable for screening purposes in predicting albuminuria in T2DM. Nonetheless, further studies are warranted to enhance the model's performance.
</details>
<details>
<summary>摘要</summary>
DIABETES，特别是TYPE 2 DIABETES MELLITUS（T2DM），仍然是现代医学中的一个重要健康问题。diabetes 的一个主要担忧是其相关的合并症的发展。diabetic nephropathy，一种diabetes 的慢性合并症，会影响肾脏，导致肾脏损害。诊断diabetic nephropathy 需要考虑多种 критериев，其中一个是在diabetes 患者身上存在至少一定量的蛋白质uria，也就是albuminuria。因此，预测diabetes 患者发展albuminuria的风险有可能提供时间性的预防措施。本研究旨在开发一种指导学习模型，以预测T2DM 患者发展albuminuria的风险。我们使用了10种特征和1种目标（albuminuria）组成的私有数据集来训练算法。Na\"ive Bayes、Support Vector Machine（SVM）、决策树、Random Forest、AdaBoost、XGBoost和Multi-Layer Perceptron（MLP）等算法被选择参与。实验结果表明，MLP 表现最佳，其准确率和f1-score值分别达0.74和0.75，适用于预测T2DM 患者发展albuminuria。然而，更多的研究是需要进行，以提高模型的性能。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Reverse-KL-Generalizing-Direct-Preference-Optimization-with-Diverse-Divergence-Constraints"><a href="#Beyond-Reverse-KL-Generalizing-Direct-Preference-Optimization-with-Diverse-Divergence-Constraints" class="headerlink" title="Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints"></a>Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16240">http://arxiv.org/abs/2309.16240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, Yuxin Chen</li>
<li>for: 提高人工智能系统的安全性和可控性，通过RLHF和DPO两种方法实现人类偏好的调整。</li>
<li>methods: 提出了一种通用的DPO方法，通过多种异步异常约束来简化价值函数和优化策略之间的复杂关系，从而实现更加高效和监督的人类偏好调整。</li>
<li>results: 对多种异步异常约束进行了实验研究，发现这些约束能够保证优化策略的准确性和多样性，并且比PPO方法更高效地实现异常约束。此外，这些约束直接影响预期抽象误差（ECE）。<details>
<summary>Abstract</summary>
The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimating the normalizing constant in the Bradley-Terry model and enables a tractable mapping between the reward function and the optimal policy. Our approach optimizes LLMs to align with human preferences in a more efficient and supervised manner under a broad set of divergence constraints. Empirically, adopting these divergences ensures a balance between alignment performance and generation diversity. Importantly, $f$-DPO outperforms PPO-based methods in divergence efficiency, and divergence constraints directly influence expected calibration error (ECE).
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的提高了机会，包括人工通用智能（AGI），但同时也增加了安全问题，例如AI系统的可能性的滥用。人类反馈学习束缚（RLHF）已经成为一种有希望的路径，但它具有复杂性和依赖于另一个奖励模型的问题。直接偏好优化（DPO）已经被提议，它与RLHF等价，只要在reverse KL regularization constraint下。这篇文章介绍了 $f$-DPO，一种通用的DPO方法，通过包括杰尼森-尚恩减少函数、前向KL减少函数和α减少函数的多种偏好减少函数来简化优化问题。我们显示，在某些$f$-减少函数下，包括杰尼森-尚恩减少函数、前向KL减少函数和α减少函数，优化问题可以通过解决卡鲁什-库恩-图克（KKT）条件来简化。这使得不需要估算正则化常量在布莱德利-泰勒模型中，并且可以在一个可追踪的方式下映射奖励函数和优化策略之间的关系。我们的方法可以更有效地使用LLM来与人类偏好相align，并且在一个更广泛的偏好减少函数下进行监督。实验表明，采用这些偏好减少函数可以保持对适配性和生成多样性的平衡。此外， $f$-DPO在异质效率和抽象效率方面表现出色，而偏好减少函数直接影响预期calibration error（ECE）。
</details></li>
</ul>
<hr>
<h2 id="Language-models-in-molecular-discovery"><a href="#Language-models-in-molecular-discovery" class="headerlink" title="Language models in molecular discovery"></a>Language models in molecular discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16235">http://arxiv.org/abs/2309.16235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Janakarajan, Tim Erdmann, Sarath Swaminathan, Teodoro Laino, Jannis Born</li>
<li>for: 本研究旨在探讨语言模型在分子发现中的应用，尤其是在药物发现过程中的潜在作用。</li>
<li>methods: 本研究使用了语言模型来预测分子的性质和化学反应，并评估了这些模型在早期药物发现中的潜在应用。</li>
<li>results: 研究发现，语言模型可以帮助加速分子发现过程，特别是在药物设计、物理性预测和化学反应预测等领域。同时，研究还发现了一些可用的开源软件资源，可以降低进入这个领域的门槛。<details>
<summary>Abstract</summary>
The success of language models, especially transformer-based architectures, has trickled into other domains giving rise to "scientific language models" that operate on small molecules, proteins or polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery. Here, we review the role of language models in molecular discovery, underlining their strength in de novo drug design, property prediction and reaction chemistry. We highlight valuable open-source software assets thus lowering the entry barrier to the field of scientific language modeling. Last, we sketch a vision for future molecular design that combines a chatbot interface with access to computational chemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.
</details>
<details>
<summary>摘要</summary>
随着语言模型的成功，特别是基于转换器的架构，在其他领域也出现了“科学语言模型”，这些模型运行于小分子、蛋白质或 polymer 等领域。在化学中，语言模型有助于加速药物发现的循环，这已经由最近的一些成果所证明。在本文中，我们将评论语言模型在分子发现中的角色，包括创新药物设计、质量预测和化学反应。我们将介绍一些有价值的开源软件资源，以降低进入科学语言模型领域的入口难度。最后，我们将绘制未来分子设计的未来方向，即通过聊天机器人接口与计算化学工具访问。我们的贡献 serves as a valuable resource for researchers、化学家和 AI 爱好者，了解如何和将来如何使用语言模型加速化学发现。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Financial-Time-Series-Retrieval-Through-Latent-Space-Projections"><a href="#Multi-Modal-Financial-Time-Series-Retrieval-Through-Latent-Space-Projections" class="headerlink" title="Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections"></a>Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16741">http://arxiv.org/abs/2309.16741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Bamford, Andrea Coletta, Elizabeth Fons, Sriram Gopalakrishnan, Svitlana Vyetrenko, Tucker Balch, Manuela Veloso</li>
<li>for: 该论文是为了提高金融时间序数据的存储和检索效率而设计的。</li>
<li>methods: 该论文使用深度编码器将多Modal数据存储在lower-dimensional的幻数空间中，以便通过图像或自然语言查询。</li>
<li>results: 该论文通过实验和示例证明了其方法的计算效率和准确性，并且展示了使用幻数空间投影可以提高金融时间序数据的检索效率和用户友好性。<details>
<summary>Abstract</summary>
Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections capture not only the time series trends but also other desirable information or properties of the financial time-series data (such as price volatility). Moreover, our approach allows user-friendly query interfaces, enabling natural language text or sketches of time-series, for which we have developed intuitive interfaces. We demonstrate the advantages of our method in terms of computational efficiency and accuracy on real historical data as well as synthetic data, and highlight the utility of latent-space projections in the storage and retrieval of financial time-series data with intuitive query modalities.
</details>
<details>
<summary>摘要</summary>
金融机构通常处理和存储大量时间序列数据，这些数据由高频生成并持续更新。为了支持高效的数据存储和检索，专门的时间序列数据库和系统出现了。这些数据库支持将时间序列索引和查询使用受限制的Structured Query Language（SQL）格式，以便进行如“股票月度价格增长大于5%”的查询。然而，这些查询不能很好地捕捉高维时间序列数据的内在复杂性，这些数据常常可以更好地用图像或语言来描述（例如，“股票在低波动 régime”）。此外，对时间序列空间的存储、计算时间和检索复杂性都是非常大的。在这篇论文中，我们提出了一种将多模态数据存储在低维的潜在空间中的框架，使得潜在空间投影包括不只是时间序列趋势，还有其他金融时间序列数据的愉悦信息或特征（如价格波动）。此外，我们的方法允许用户友好的查询界面，例如使用自然语言文本或时间序列绘图，我们已经开发出了直观的界面。我们在历史数据和synthetic数据上进行了计算效率和准确性的比较，并将latent空间投影的利用性和便捷性展示在金融时间序列数据存储和检索中。
</details></li>
</ul>
<hr>
<h2 id="Automated-Chest-X-Ray-Report-Generator-Using-Multi-Model-Deep-Learning-Approach"><a href="#Automated-Chest-X-Ray-Report-Generator-Using-Multi-Model-Deep-Learning-Approach" class="headerlink" title="Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach"></a>Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05969">http://arxiv.org/abs/2310.05969</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ariefpurnamamuharram/IF5200">https://github.com/ariefpurnamamuharram/IF5200</a></li>
<li>paper_authors: Arief Purnama Muharram, Hollyana Puteri Haryono, Abassi Haji Juma, Ira Puspasari, Nugraha Priya Utama<br>for: 这个研究旨在帮助医生在胸部X线影像读取中受助，提高胸部X线诊断的精度和效率。methods: 本研究使用多个多元 classification 深度学习模型，每个模型负责检测一种异常性，并将影像处理成128x128像素的标准化内容，分成三个部分，以涵盖胸部中的上下两部分和中部。results: 系统可以将胸部X线影像自动检测出异常，并生成相应的诊断报告。报告中会包含适当的预先定义的句子，以描述检测到的异常。系统预期可以帮助医生快速对胸部X线影像进行评估，提高诊断的精度和效率。<details>
<summary>Abstract</summary>
Reading and interpreting chest X-ray images is one of the most radiologist's routines. However, it still can be challenging, even for the most experienced ones. Therefore, we proposed a multi-model deep learning-based automated chest X-ray report generator system designed to assist radiologists in their work. The basic idea of the proposed system is by utilizing multi binary-classification models for detecting multi abnormalities, with each model responsible for detecting one abnormality, in a single image. In this study, we limited the radiology abnormalities detection to only cardiomegaly, lung effusion, and consolidation. The system generates a radiology report by performing the following three steps: image pre-processing, utilizing deep learning models to detect abnormalities, and producing a report. The aim of the image pre-processing step is to standardize the input by scaling it to 128x128 pixels and slicing it into three segments, which covers the upper, lower, and middle parts of the lung. After pre-processing, each corresponding model classifies the image, resulting in a 0 (zero) for no abnormality detected and a 1 (one) for the presence of an abnormality. The prediction outputs of each model are then concatenated to form a 'result code'. The 'result code' is used to construct a report by selecting the appropriate pre-determined sentence for each detected abnormality in the report generation step. The proposed system is expected to reduce the workload of radiologists and increase the accuracy of chest X-ray diagnosis.
</details>
<details>
<summary>摘要</summary>
读取和解释胸部X射线图像是胸部放射学专家的日常任务之一，但它仍然可以是挑战，即使是最有经验的专家。因此，我们提议了一个基于多模型深度学习的自动化胸部X射线报告生成系统，用于帮助胸部放射学专家在工作中。我们的基本想法是利用多个二进制分类模型，每个模型负责检测一种疾病，在单一图像上进行检测。在这个研究中，我们只检测了心脏肥大、肺液腔和肺部扩张。系统生成报告由以下三步进行：图像预处理、利用深度学习模型检测疾病、生成报告。图像预处理步骤的目的是标准化输入，将其扩大到128x128像素并将其分成三个部分，包括肺部上、下、中部三个部分。然后，每个相应的模型对图像进行分类，得到0（无疾病）和1（存在疾病）两个结果。预测输出的每个模型结果被 concatenate 以生成一个'结果代码'。'结果代码'被用来生成报告，通过选择适当的预定的句子来描述检测到的疾病。我们预计该系统将减轻胸部放射学专家的工作负担，并提高胸部X射线诊断的准确性。
</details></li>
</ul>
<hr>
<h2 id="GInX-Eval-Towards-In-Distribution-Evaluation-of-Graph-Neural-Network-Explanations"><a href="#GInX-Eval-Towards-In-Distribution-Evaluation-of-Graph-Neural-Network-Explanations" class="headerlink" title="GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations"></a>GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16223">http://arxiv.org/abs/2309.16223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kenza Amara, Mennatallah El-Assady, Rex Ying</li>
<li>for: 本研究旨在评估图神经网络（GNN）的可解释方法，以及评估这些方法的正确性。</li>
<li>methods: 本研究使用了 retraining 策略和 EdgeRank 分数来评估图解释的正确性。</li>
<li>results: 研究发现，许多流行的方法，包括梯度基于的方法，产生的解释并不比随机选择的边为重要的子图来的好。这些结果挑战了当前领域的研究成果。结果与人类评估相符。<details>
<summary>Abstract</summary>
Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank score evaluates if explanatory edges are correctly ordered by their importance. GInX-Eval verifies if ground-truth explanations are instructive to the GNN model. In addition, it shows that many popular methods, including gradient-based methods, produce explanations that are not better than a random designation of edges as important subgraphs, challenging the findings of current works in the area. Results with GInX-Eval are consistent across multiple datasets and align with human evaluation.
</details>
<details>
<summary>摘要</summary>
Graph层神经网络（GNN）的多种解释方法已经相继开发，以便高亮图形中的边和节点，以帮助理解模型的预测结果。然而，目前并没有一个明确的方法来评估这些解释的正确性，是人类或模型视角。现有的评估过程中存在一个未解决的瓶颈，即图形外的解释问题，这个问题影响了现有的评估指标，如受欢迎程度或准确率分数。在这篇论文中，我们展示了 faithfulness 指标的局限性。我们提出了 Graph In-distribution eXplanation Evaluation（GInX-Eval），一种用于评估图解释的评估方法，超越了 faithfulness 的缺陷，并提供了新的解释视角。GInX 分数测量移除的边是模型中的有用信息，而 EdgeRank 分数评估解释边是否按照重要性排序。GInX-Eval 验证了基于真实解释的模型是否具有有用的解释。此外，它还表明了许多流行的方法，包括梯度基于的方法，生成的解释并不是更加有用的，挑战当前领域的研究成果。GInX-Eval 的结果在多个数据集上具有一致性，并与人类评估相符。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-the-Chameleons-A-Benchmark-for-Out-of-Distribution-Detection-in-Medical-Tabular-Data"><a href="#Unmasking-the-Chameleons-A-Benchmark-for-Out-of-Distribution-Detection-in-Medical-Tabular-Data" class="headerlink" title="Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data"></a>Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16220">http://arxiv.org/abs/2309.16220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mazizmalayeri/tabmedood">https://github.com/mazizmalayeri/tabmedood</a></li>
<li>paper_authors: Mohammad Azizmalayeri, Ameen Abu-Hanna, Giovanni Ciná</li>
<li>for: 本研究旨在解决机器学习模型在不同于训练数据分布上的泛化问题，以便在实际医疗系统中可靠地使用机器学习模型并避免 incorrect predictions。</li>
<li>methods: 本研究使用了许多泛化检测方法，包括密度基本方法和现状最佳方法，并对不同预测架构进行比较，包括MLP、ResNet和Transformer。</li>
<li>results: 研究发现，i) 远程OOD检测问题已经得到解决，但近程OOD检测问题仍然存在; ii) 后处方法独立不够，与距离基本方法结合可以提高表现; iii) Transformer架构比MLP和ResNet更加具有谨慎性。<details>
<summary>Abstract</summary>
Despite their success, Machine Learning (ML) models do not generalize effectively to data not originating from the training distribution. To reliably employ ML models in real-world healthcare systems and avoid inaccurate predictions on out-of-distribution (OOD) data, it is crucial to detect OOD samples. Numerous OOD detection approaches have been suggested in other fields - especially in computer vision - but it remains unclear whether the challenge is resolved when dealing with medical tabular data. To answer this pressing need, we propose an extensive reproducible benchmark to compare different methods across a suite of tests including both near and far OODs. Our benchmark leverages the latest versions of eICU and MIMIC-IV, two public datasets encompassing tens of thousands of ICU patients in several hospitals. We consider a wide array of density-based methods and SOTA post-hoc detectors across diverse predictive architectures, including MLP, ResNet, and Transformer. Our findings show that i) the problem appears to be solved for far-OODs, but remains open for near-OODs; ii) post-hoc methods alone perform poorly, but improve substantially when coupled with distance-based mechanisms; iii) the transformer architecture is far less overconfident compared to MLP and ResNet.
</details>
<details>
<summary>摘要</summary>
尽管机器学习（ML）模型在训练数据 Distribution 上表现出色，但它们在不属于训练数据的数据上并不能准确预测。为了在实际医疗系统中可靠地使用 ML 模型并避免 incorrect predictions 的问题，检测 Out-of-distribution（OOD）样本是非常重要的。在其他领域中，许多 OOD 检测方法已经被提出，但是在医疗数据上的挑战仍然存在。为了解决这个问题，我们提出了一个广泛的可重现性的 benchmark，用于比较不同方法在多种测试中的表现。我们的 benchmark 利用了最新的 eICU 和 MIMIC-IV 两个公共数据集，这两个数据集包含了多个医院内的 ICU 病人数万个样本。我们考虑了多种基于密度的方法和 State-of-the-art 的 post-hoc 检测器，包括 MLP、ResNet 和 Transformer 等预测架构。我们的发现显示：1. 远 OOD 问题已经得到解决，但是近 OOD 问题仍然存在。2. post-hoc 方法独立使用不够Effective，但是与距离基于机制结合使用时会提高substantially。3. Transformer 架构相比 MLP 和 ResNet 更加具有谨慎性。
</details></li>
</ul>
<hr>
<h2 id="VDC-Versatile-Data-Cleanser-for-Detecting-Dirty-Samples-via-Visual-Linguistic-Inconsistency"><a href="#VDC-Versatile-Data-Cleanser-for-Detecting-Dirty-Samples-via-Visual-Linguistic-Inconsistency" class="headerlink" title="VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency"></a>VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16211">http://arxiv.org/abs/2309.16211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu</li>
<li>for: 提高数据驱动AI系统的可靠性和可靠性，检测数据中的垃圾样本。</li>
<li>methods: 提出了一种新的多模态语言模型（MLLM），通过跨模态对接和解释来捕捉视觉内容的semantic偏差。VDC包括三个 consecutive module：视觉问题生成模块、视觉问题回答模块和视觉答案评估模块。</li>
<li>results: 广泛的实验表明，VDC可以具有高效性和泛化能力，检测多种类型和来源的垃圾样本。<details>
<summary>Abstract</summary>
The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable.Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other domains.In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning.It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples.
</details>
<details>
<summary>摘要</summary>
“数据在构建人工智能系统中的角色刚刚被提出，而现代概念中的数据中心式AI强调了数据的重要性。然而，在实际应用中，数据集可能包含废弃样本，如攻击后门杀手、来自卫星化批处理的噪声标签，以及这些杂合的样本。这些废弃样本会使深度神经网络（DNN）变得不可靠和不可靠。因此，检测废弃样本是提高数据集质量和可靠性的关键。现有的检测器只会检测到攻击后门样本或噪声标签，但这些检测器在面临不同领域中的废弃样本时往往存在弱化现象。在这篇论文中，我们发现了不同领域中废弃样本的共同特点：视觉语言冲突。为了捕捉多模态 semantic的冲突，我们提出了多模态大语言模型（MLLM）的跨模态准确性和推理能力，并开发了三个 consecutive 模块：视觉问题生成模块、视觉问题答案模块和视觉答案评估模块。经过广泛的实验，我们发现其在不同类别和类型的废弃样本上表现出优于常见的性和普适性。”
</details></li>
</ul>
<hr>
<h2 id="Design-of-JiuTian-Intelligent-Network-Simulation-Platform"><a href="#Design-of-JiuTian-Intelligent-Network-Simulation-Platform" class="headerlink" title="Design of JiuTian Intelligent Network Simulation Platform"></a>Design of JiuTian Intelligent Network Simulation Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.06858">http://arxiv.org/abs/2310.06858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Zhao, Miaomiao Zhang, Guangyu Li, Zhuowen Guan, Sijia Liu, Zhaobin Xiao, Yuting Cao, Zhe Lv, Yanping Liang</li>
<li>for: 本研究开发了一个名为“智能网络实验平台”的智能网络实验平台，将提供无线通信实验数据服务供开放创新平台使用。</li>
<li>methods: 本平台包括一系列可扩展的模拟器功能，提供开放服务，让用户使用增强学习算法进行模型训练和推断，并可以上传和更新参数配置以进行不同情况下的优化任务。</li>
<li>results: 本平台和其开放服务主要从背景、整体架构、模拟器、商业场景和未来方向等多个角度进行介绍。<details>
<summary>Abstract</summary>
This paper introduced the JiuTian Intelligent Network Simulation Platform, which can provide wireless communication simulation data services for the Open Innovation Platform. The platform contains a series of scalable simulator functionalities, offering open services that enable users to use reinforcement learning algorithms for model training and inference based on simulation environments and data. Additionally, it allows users to address optimization tasks in different scenarios by uploading and updating parameter configurations. The platform and its open services were primarily introduced from the perspectives of background, overall architecture, simulator, business scenarios, and future directions.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了九天智能网络模拟平台，该平台可以为开放创新平台提供无线通信模拟数据服务。平台包括一系列可扩展的模拟器功能，提供开放服务，让用户通过模拟环境和数据进行模型训练和推理，同时允许用户在不同的enario中解决优化问题，通过上传和更新参数配置。平台和其开放服务主要从背景、总体架构、模拟器、业务场景和未来方向等角度进行了介绍。
</details></li>
</ul>
<hr>
<h2 id="Pushing-Large-Language-Models-to-the-6G-Edge-Vision-Challenges-and-Opportunities"><a href="#Pushing-Large-Language-Models-to-the-6G-Edge-Vision-Challenges-and-Opportunities" class="headerlink" title="Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities"></a>Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16739">http://arxiv.org/abs/2309.16739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Lin, Guanqiao Qu, Qiyuan Chen, Xianhao Chen, Zhe Chen, Kaibin Huang</li>
<li>for: 本文旨在探讨将大语言模型（LLM）部署到6G边缘计算（MEC）系统中的可能性，以解决云部署中的长响应时间、高频带宽成本和数据隐私问题。</li>
<li>methods: 本文首先介绍了具有多Modal的LLM在 роботиCS和医疗领域的潜在应用，以强调在用户端部署LLM的需求。然后，本文识别了部署LLM在边缘的挑战，并对6G MEC架构的概述，以及两个设计方面：边缘训练和边缘推理。为了使得LLM在边缘部署更加高效，本文介绍了多种前沿技术，包括分解学习&#x2F;推理、精炼练习、量化和参数共享推理。</li>
<li>results: 本文作为一篇Position paper，旨在全面识别LLM部署在6G边缘的动机、挑战和走向，以促进6G MEC系统中LLM的高效部署。<details>
<summary>Abstract</summary>
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, including split learning/inference, parameter-efficient fine-tuning, quantization, and parameter-sharing inference, to facilitate the efficient deployment of LLMs. This article serves as a position paper for thoroughly identifying the motivation, challenges, and pathway for empowering LLMs at the 6G edge.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），具有吸引人的能力，正在改变人工智能的发展和未来的形态。然而，由于它们的多模态，当前云端部署的状况存在一些挑战：1）长响应时间；2）高频宽成本；以及3）数据隐私的侵犯。6G移动边计算（MEC）系统可能解决这些急需解决的问题。在这篇文章中，我们探讨将 LLM 部署在6G边缘。我们首先介绍了由多模态 LLM 驱动的杀手应用，包括机器人和医疗，以强调 LLM 的部署需要在用户端。然后，我们认为6G MEC 架构对 LLM 的部署存在挑战，并讨论了两个设计方面：1）边缘训练和2）边缘推理。在两个方面中，我们讨论了一些 cutting-edge 技术，包括分解学习/推理、精炼型精度训练、量化和共享参数推理，以便有效地部署 LLM。本文作为一篇位点文章，旨在全面识别 LLM 的动机、挑战和部署路径。
</details></li>
</ul>
<hr>
<h2 id="A-More-General-Theory-of-Diagnosis-from-First-Principles"><a href="#A-More-General-Theory-of-Diagnosis-from-First-Principles" class="headerlink" title="A More General Theory of Diagnosis from First Principles"></a>A More General Theory of Diagnosis from First Principles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16180">http://arxiv.org/abs/2309.16180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alban-grastien/diagfwork">https://github.com/alban-grastien/diagfwork</a></li>
<li>paper_authors: Alban Grastien, Patrik Haslum, Sylvie Thiébaux</li>
<li>for: 这个论文是为了总结现有的 диагностикс方法，并提出一种更通用的 диагностикс理论，以便应对不同类型的系统和诊断问题。</li>
<li>methods: 该论文使用了一种基于模型的 диагностикс方法，包括在搜索空间中检查假设，测试假设的一致性，并生成冲突来排除继承和其他搜索空间的部分。</li>
<li>results: 该论文的实现使用了两种不同的测试solvers，一个基于满足性检查，另一个基于冒泡搜索，并在两个实际世界 discrete event 问题上进行了评估。结果显示，这些实现可以在更加通用的理论基础上，解决现有的诊断方法无法解决的问题，并在解决实际问题时表现出优于特殊设计的方法。<details>
<summary>Abstract</summary>
Model-based diagnosis has been an active research topic in different communities including artificial intelligence, formal methods, and control. This has led to a set of disparate approaches addressing different classes of systems and seeking different forms of diagnoses. In this paper, we resolve such disparities by generalising Reiter's theory to be agnostic to the types of systems and diagnoses considered. This more general theory of diagnosis from first principles defines the minimal diagnosis as the set of preferred diagnosis candidates in a search space of hypotheses. Computing the minimal diagnosis is achieved by exploring the space of diagnosis hypotheses, testing sets of hypotheses for consistency with the system's model and the observation, and generating conflicts that rule out successors and other portions of the search space. Under relatively mild assumptions, our algorithms correctly compute the set of preferred diagnosis candidates. The main difficulty here is that the search space is no longer a powerset as in Reiter's theory, and that, as consequence, many of the implicit properties (such as finiteness of the search space) no longer hold. The notion of conflict also needs to be generalised and we present such a more general notion. We present two implementations of these algorithms, using test solvers based on satisfiability and heuristic search, respectively, which we evaluate on instances from two real world discrete event problems. Despite the greater generality of our theory, these implementations surpass the special purpose algorithms designed for discrete event systems, and enable solving instances that were out of reach of existing diagnosis approaches.
</details>
<details>
<summary>摘要</summary>
MODEL-BASED诊断在不同的社区中，包括人工智能、正式方法和控制，是一个活跃的研究主题。这导致了不同的方法，targeting different types of systems and seeking different forms of diagnoses。在这篇论文中，我们解决了这些不一致性，通过总结Reiter的理论，使其不受系统和诊断类型的限制。这种更通用的诊断理论定义了最小诊断为搜索空间中最佳诊断候选者的集合。计算最小诊断通过探索诊断假设空间，测试假设集合与系统模型和观察之间的一致性，并生成规则排除继承和其他搜索空间部分。在相对轻量级假设下，我们的算法可正确计算最佳诊断候选者集合。主要困难在于搜索空间不再是Reiter理论中的全集，因此许多隐性属性（如搜索空间的有限性）不再保持。诊断理论中的冲突也需要更新，我们提出了一种更通用的冲突概念。我们在使用满足性和尝试搜索的测试器基础上实现了这些算法，并在两个真实世界离散事件问题上进行了测试。尽管我们的理论更加通用，但这些实现仍然超越了特定的诊断方法，并可以解决当前诊断方法无法解决的实例。
</details></li>
</ul>
<hr>
<h2 id="Attention-Sorting-Combats-Recency-Bias-In-Long-Context-Language-Models"><a href="#Attention-Sorting-Combats-Recency-Bias-In-Long-Context-Language-Models" class="headerlink" title="Attention Sorting Combats Recency Bias In Long Context Language Models"></a>Attention Sorting Combats Recency Bias In Long Context Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01427">http://arxiv.org/abs/2310.01427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Peysakhovich, Adam Lerer</li>
<li>for: 提高长文本模型在生成中的效率，解决现有语言模型在处理长文本时的问题。</li>
<li>methods: 引入“注意力排序”策略，在响应过程中多次执行一步解码，然后根据注意力排序文档，最终生成答案。</li>
<li>results: 提高长文本模型的表现，并指出现有语言模型在使用 Retrieval-Augmented Generation 时存在一些挑战。<details>
<summary>Abstract</summary>
Current language models often fail to incorporate long contexts efficiently during generation. We show that a major contributor to this issue are attention priors that are likely learned during pre-training: relevant information located earlier in context is attended to less on average. Yet even when models fail to use the information from a relevant document in their response, they still pay preferential attention to that document compared to an irrelevant document at the same position. We leverage this fact to introduce ``attention sorting'': perform one step of decoding, sort documents by the attention they receive (highest attention going last), repeat the process, generate the answer with the newly sorted context. We find that attention sorting improves performance of long context models. Our findings highlight some challenges in using off-the-shelf language models for retrieval augmented generation.
</details>
<details>
<summary>摘要</summary>
现有的语言模型很难够效率地 incorporate 长文本上下文。我们表明，一个主要的贡献因素是在预训练中学习的注意力先驱：相关信息在上下文中的早期位置被注意到的平均水平更低。然而，即使模型未使用它们的答案中的相关文档信息，它们仍然会对这些文档比 irrelevant 文档更加偏好地分配注意力。我们利用这一点，引入“注意力排序”：在一步解码后，排序文档按照它们所收到的注意力排序（最高注意力在最后），重复过程，生成答案。我们发现，注意力排序可以提高长文本模型的性能。我们的发现强调了使用预存库语言模型进行检索增强生成的一些挑战。
</details></li>
</ul>
<hr>
<h2 id="CoinRun-Solving-Goal-Misgeneralisation"><a href="#CoinRun-Solving-Goal-Misgeneralisation" class="headerlink" title="CoinRun: Solving Goal Misgeneralisation"></a>CoinRun: Solving Goal Misgeneralisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16166">http://arxiv.org/abs/2309.16166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stuart Armstrong, Alexandre Maranhão, Oliver Daniels-Koch, Patrick Leask, Rebecca Gorman</li>
<li>for: 解决人工智能对目标的扩展问题，使得强大的人工智能能够与人类意愿和人类道德观念相吻合。</li>
<li>methods: 使用ACE（概念推导算法）代理人，不使用新的奖励信息，在新环境中解决了金币挑战，表明了自主代理人可以在新和关键的情况下被信任。</li>
<li>results: 通过ACE（概念推导算法）代理人，在金币挑战中解决了goal misgeneralisation问题，未使用新的奖励信息。<details>
<summary>Abstract</summary>
Goal misgeneralisation is a key challenge in AI alignment -- the task of getting powerful Artificial Intelligences to align their goals with human intentions and human morality. In this paper, we show how the ACE (Algorithm for Concept Extrapolation) agent can solve one of the key standard challenges in goal misgeneralisation: the CoinRun challenge. It uses no new reward information in the new environment. This points to how autonomous agents could be trusted to act in human interests, even in novel and critical situations.
</details>
<details>
<summary>摘要</summary>
goal misgeneralization 是人工智能对alignment的主要挑战 -- 将强大的人工智能目标与人类意图和人类伦理相对应。在这篇论文中，我们表明了ACE（概念推论算法）代理可以解决标准化目标泛化挑战之一：硬币推论挑战。它不使用新的奖励信息在新环境中。这表明了自主代理可以在新和关键的情况下被信任，以行动在人类利益之下。Here's a breakdown of the translation:* "goal misgeneralization" is translated as "目标泛化" (mùzhì pògē), which refers to the phenomenon of AI systems deviating from their intended goals.* "人工智能" is translated as "人类智能" (rénshēng zhìnéng), which refers to artificial intelligence systems.* "alignment" is translated as "对应" (duìpǐng), which refers to the alignment of AI systems with human intentions and values.* "ACE" is translated as "概念推论算法" (guīxiǎn tuīsuǒ gōngchǎng), which is the name of the algorithm used to solve the CoinRun challenge.* "CoinRun challenge" is translated as "硬币推论挑战" (hùqian tuīsuǒ tīzhàn), which refers to a standard challenge in goal misgeneralization where an AI system must learn to navigate a maze to reach a goal.* "novel and critical situations" is translated as "新和关键的情况" (xīn hé guānjiā de qíngkē), which refers to situations that are new and require careful consideration.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Untrustworthy-Commands-for-Multi-Robot-Coordination-in-Unpredictable-Environments-A-Bandit-Submodular-Maximization-Approach"><a href="#Leveraging-Untrustworthy-Commands-for-Multi-Robot-Coordination-in-Unpredictable-Environments-A-Bandit-Submodular-Maximization-Approach" class="headerlink" title="Leveraging Untrustworthy Commands for Multi-Robot Coordination in Unpredictable Environments: A Bandit Submodular Maximization Approach"></a>Leveraging Untrustworthy Commands for Multi-Robot Coordination in Unpredictable Environments: A Bandit Submodular Maximization Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16161">http://arxiv.org/abs/2309.16161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zirui Xu, Xiaofeng Lin, Vasileios Tzoumas</li>
<li>for: 这 paper 探讨了多机器人协调问题在不可预测和部分可见环境中，外部指令是不可靠的。</li>
<li>methods: 这 paper 使用了 Meta Bandit Sequential Greedy（MetaBSG）算法，该算法可以在不可靠的外部指令下提供性能保证。MetaBSG 利用了一种 meta-算法来判断机器人是否 следу着外部指令或一种最近发展的 submodular 协调算法 Bandit Sequential Greedy（BSG），该算法在不可预测和部分可见环境中有性能保证。</li>
<li>results: 这 paper 在 simulated 多目标追踪场景中验证了 MetaBSG 算法的效果，并证明了 MetaBSG 可以在不可预测和部分可见环境中提供性能保证，并且可以Robustify 不可靠的外部指令。<details>
<summary>Abstract</summary>
We study the problem of multi-agent coordination in unpredictable and partially-observable environments with untrustworthy external commands. The commands are actions suggested to the robots, and are untrustworthy in that their performance guarantees, if any, are unknown. Such commands may be generated by human operators or machine learning algorithms and, although untrustworthy, can often increase the robots' performance in complex multi-robot tasks. We are motivated by complex multi-robot tasks such as target tracking, environmental mapping, and area monitoring. Such tasks are often modeled as submodular maximization problems due to the information overlap among the robots. We provide an algorithm, Meta Bandit Sequential Greedy (MetaBSG), which enjoys performance guarantees even when the external commands are arbitrarily bad. MetaBSG leverages a meta-algorithm to learn whether the robots should follow the commands or a recently developed submodular coordination algorithm, Bandit Sequential Greedy (BSG) [1], which has performance guarantees even in unpredictable and partially-observable environments. Particularly, MetaBSG asymptotically can achieve the better performance out of the commands and the BSG algorithm, quantifying its suboptimality against the optimal time-varying multi-robot actions in hindsight. Thus, MetaBSG can be interpreted as robustifying the untrustworthy commands. We validate our algorithm in simulated scenarios of multi-target tracking.
</details>
<details>
<summary>摘要</summary>
We present an algorithm, Meta Bandit Sequential Greedy (MetaBSG), which appreciates execution guarantees even when the outside orders are arbitrarily terrible. MetaBSG leverages a meta-algorithm to learn whether the robots should follow the orders or a as of late created submodular coordination algorithm, Bandit Sequential Greedy (BSG) [1], which has execution guarantees even in unforeseeable and partially observable conditions. Specifically, MetaBSG asymptotically can accomplish the better execution out of the orders and the BSG algorithm, quantifying its suboptimality against the optimal time-varying multi-robot activities in hindsight. Along these lines, MetaBSG can be deciphered as robustifying the untrustworthy orders. We confirm our algorithm in simulated scenarios of multi-target following.
</details></li>
</ul>
<hr>
<h2 id="AE-GPT-Using-Large-Language-Models-to-Extract-Adverse-Events-from-Surveillance-Reports-A-Use-Case-with-Influenza-Vaccine-Adverse-Events"><a href="#AE-GPT-Using-Large-Language-Models-to-Extract-Adverse-Events-from-Surveillance-Reports-A-Use-Case-with-Influenza-Vaccine-Adverse-Events" class="headerlink" title="AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events"></a>AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16150">http://arxiv.org/abs/2309.16150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiming Li, Jianfu Li, Jianping He, Cui Tao</li>
<li>for: 本研究旨在评估大语言模型（LLMs）在临床报告中检测抗体不良事件（AE）的能力。</li>
<li>methods: 研究使用了1990年至2016年的VAERS数据，并评估了多种常见的LLMs，包括GPT-2、GPT-3变种、GPT-4和Llama 2。研究人员使用了GPT 3.5模型进行精度调整，并以Influenza疫苗为用例进行测试。</li>
<li>results: 研究发现，精度调整后的GPT 3.5模型（AE-GPT）在严格匹配和饱和匹配情况下都达到了0.704和0.816的微型F1分数。这表明LLMs在处理医疗数据方面具有潜在的潜力，这可能将为其他AE检测任务提供新的思路。<details>
<summary>Abstract</summary>
Though Vaccines are instrumental in global health, mitigating infectious diseases and pandemic outbreaks, they can occasionally lead to adverse events (AEs). Recently, Large Language Models (LLMs) have shown promise in effectively identifying and cataloging AEs within clinical reports. Utilizing data from the Vaccine Adverse Event Reporting System (VAERS) from 1990 to 2016, this study particularly focuses on AEs to evaluate LLMs' capability for AE extraction. A variety of prevalent LLMs, including GPT-2, GPT-3 variants, GPT-4, and Llama 2, were evaluated using Influenza vaccine as a use case. The fine-tuned GPT 3.5 model (AE-GPT) stood out with a 0.704 averaged micro F1 score for strict match and 0.816 for relaxed match. The encouraging performance of the AE-GPT underscores LLMs' potential in processing medical data, indicating a significant stride towards advanced AE detection, thus presumably generalizable to other AE extraction tasks.
</details>
<details>
<summary>摘要</summary>
尽管疫苗在全球医疗中发挥了重要作用，减轻感染病和流行病舌，但它们有时会导致不良反应（AE）。最近，大型自然语言模型（LLM）已经显示出了有效地标识和目录AE的潜力。通过使用1990年至2016年的疫苗不良反应报送系统（VAERS）数据，本研究专门关注AE来评估LLM的能力。包括GPT-2、GPT-3变种、GPT-4和Llama 2等多种常见LLM都被评估，并使用Influenza疫苗作为用例。细化的GPT 3.5模型（AE-GPT）表现出色，其微型F1分数为0.704（严格匹配）和0.816（松散匹配）。AE-GPT的良好表现表明LLM在处理医疗数据方面的潜力，这表明可能在其他AE抽取任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="T-COL-Generating-Counterfactual-Explanations-for-General-User-Preferences-on-Variable-Machine-Learning-Systems"><a href="#T-COL-Generating-Counterfactual-Explanations-for-General-User-Preferences-on-Variable-Machine-Learning-Systems" class="headerlink" title="T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems"></a>T-COL: Generating Counterfactual Explanations for General User Preferences on Variable Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16146">http://arxiv.org/abs/2309.16146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neu-datamining/t-col">https://github.com/neu-datamining/t-col</a></li>
<li>paper_authors: Ming Wang, Daling Wang, Wenfang Wu, Shi Feng, Yifei Zhang<br>for:The paper aims to address the lack of interpretability in machine learning (ML) systems by proposing a new method called Tree-based Conditions Optional Links (T-COL) to generate counterfactual explanations (CEs) that can be adapted to general user preferences.methods:The proposed T-COL method uses tree-based structures and conditions to generate CEs that can be customized to suit the variability of ML models, while maintaining robustness even when the validation models change.results:The paper experimentally compares the properties of CEs generated by T-COL under different user preferences and demonstrates that T-COL is better suited for accommodating user preferences and variable ML systems compared to baseline methods, including Large Language Models.<details>
<summary>Abstract</summary>
Machine learning (ML) based systems have been suffering a lack of interpretability. To address this problem, counterfactual explanations (CEs) have been proposed. CEs are unique as they provide workable suggestions to users, in addition to explaining why a certain outcome was predicted. However, the application of CEs has been hindered by two main challenges, namely general user preferences and variable ML systems. User preferences, in particular, tend to be general rather than specific feature values. Additionally, CEs need to be customized to suit the variability of ML models, while also maintaining robustness even when these validation models change. To overcome these challenges, we propose several possible general user preferences that have been validated by user research and map them to the properties of CEs. We also introduce a new method called \uline{T}ree-based \uline{C}onditions \uline{O}ptional \uline{L}inks (T-COL), which has two optional structures and several groups of conditions for generating CEs that can be adapted to general user preferences. Meanwhile, a group of conditions lead T-COL to generate more robust CEs that have higher validity when the ML model is replaced. We compared the properties of CEs generated by T-COL experimentally under different user preferences and demonstrated that T-COL is better suited for accommodating user preferences and variable ML systems compared to baseline methods including Large Language Models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Semi-supervised-Learning-with-Meta-Optimized-Synthetic-Samples"><a href="#Generative-Semi-supervised-Learning-with-Meta-Optimized-Synthetic-Samples" class="headerlink" title="Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples"></a>Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16143">http://arxiv.org/abs/2309.16143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shin’ya Yamaguchi<br>for:这篇论文的目的是研究无监督学习（SSL）方法，以实现训练深度分类模型，不需要大量的无标签数据。methods:这篇论文提出了一种使用生成模型生成的synthetic数据进行SSL训练的方法，包括：（1）meta-学习生成模型，以生成模型能够生成与标签样本相似的synthetic样本，并（2）使用real标签和synthetic无标签样本进行SSL训练。results:研究结果表明，这种方法可以超过基eline方法，并在具有极少量标签数据的场景下表现更好。这表明，synthetic样本可以为SSL训练提供更高效的改进。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: Can we train SSL models without real unlabeled datasets? Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are identifying synthetic samples that emulate unlabeled samples from generative foundation models and training classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthetic unlabeled samples. For (i), we propose a meta-learning objective that optimizes latent variables to generate samples that resemble real labeled samples and minimize the validation loss. For (ii), we propose a simple unsupervised loss function that regularizes the feature extractors of classifiers to maximize the performance improvement obtained from synthetic samples. We confirm that our method outperforms baselines using generative foundation models on SSL. We also demonstrate that our methods outperform SSL using real unlabeled datasets in scenarios with extremely small amounts of labeled datasets. This suggests that synthetic samples have the potential to provide improvement gains more efficiently than real unlabeled data.
</details>
<details>
<summary>摘要</summary>
SSL（半有监督学习）是一种有前途的方法，用于训练深度分类模型，使用标注和无标注数据集。然而，现有的SSL方法往往需要大量的无标注数据集，在许多实际应用中可能无法获得，尤其是因为法律约束（例如GDPR）。在这篇论文中，我们研究问题：我们可以不使用真实的无标注数据集来训练SSL模型吗？而是使用生成的数据集，生成自生成基本模型，该模型在不同领域（例如ImageNet）上训练了数百万个样本。我们的主要概念是将生成的样本与真实的标注样本进行对比，并使用这些样本来训练分类器。为此，我们的方法被формализова为一个 alternate optimization 问题：（i）生成基本模型的meta-学习和（ii）使用实际标注和生成无标注样本来进行SSL。为（i），我们提出了一个meta-学习目标，用于优化幂等变量，以生成与真实标注样本更相似的样本，并最小化验证损失。为（ii），我们提出了一个简单的无监督损失函数，用于规范分类器的特征提取器，以提高从生成样本中获得的性能提升。我们证明了，我们的方法可以超越基于生成基本模型的SSL基线。此外，我们还证明了，我们的方法在具有极少量标注数据集的场景中可以更高效地提高SSL性能，这表明生成样本有可能提供更高效的提升。
</details></li>
</ul>
<hr>
<h2 id="ModuLoRA-Finetuning-3-Bit-LLMs-on-Consumer-GPUs-by-Integrating-with-Modular-Quantizers"><a href="#ModuLoRA-Finetuning-3-Bit-LLMs-on-Consumer-GPUs-by-Integrating-with-Modular-Quantizers" class="headerlink" title="ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers"></a>ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16119">http://arxiv.org/abs/2309.16119</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kuleshov-group/modulora-experiment">https://github.com/kuleshov-group/modulora-experiment</a></li>
<li>paper_authors: Junjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr Kuleshov</li>
<li>For: The paper is written for proposing a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU.* Methods: The paper proposes a method called ModuLoRA, which integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). The approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module.* Results: The paper achieves competitive performance on text classification, natural language inference, and instruction following tasks using significantly less memory than existing approaches, and surpasses the state-of-the-art ROUGE score on a popular summarization task. The paper also releases ModuLoRA together with a series of low-precision models as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.Here’s the format you requested:* For: 提出了一种能够在1GB GPU上进行精简的大语言模型（LLM）微调法，支持LLM模型的65B参数进行3bit或4bit的精简。* Methods: 提出了一种名为ModuLoRA的方法，该方法通过简单的量化无关的反向传播来自动权重量化，并使用低精度适应器（LoRA）来实现微调。* Results: 实验表明，ModuLoRA可以在文本分类、自然语言推理和指令执行任务中实现竞争力的性能，并且在某些任务上超过了现有的状态态的ROUGE分数。同时， paper还发布了一系列的低精度模型，包括首个3bit的指令执行Alpaca LLM模型，并将其作为LLMTOOLS库发布。<details>
<summary>Abstract</summary>
We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. We release ModuLoRA together with a series of low-precision models--including the first family of 3-bit instruction following Alpaca LLMs--as part of LLMTOOLS, a user-friendly library for quantizing, running, and finetuning LLMs on consumer GPUs.
</details>
<details>
<summary>摘要</summary>
我们提出了一种内存效率高的训练算法，用于大型自然语言模型（LLM）的精细调整。我们的方法，即模块化低级适应（ModuLoRA），可以将用户指定的Weight量化器与精细调整结合在一起。我们的方法基于一种简单的量化不可逆传输，可以动态将低精度LLM weights转换为自定义黑盒量化模块中的低精度模型。这种方法使得可以在3位数字精度下进行精细调整，并且可以首次实现3位LLM的训练。在我们的实验中，ModuLoRA在文本分类、自然语言推理和指令遵从任务中达到了竞争力的性能，并且使用了许多内存少于现有方法。此外，我们还超越了当前的ROUGE分数在摘要任务中。我们将ModuLoRA和一系列低精度模型（包括首个3位指令遵从Alpaca LLM）发布为LLMTOOLS，一个用户友好的库，用于在消费级GPU上量化、运行和精细调整LLM。
</details></li>
</ul>
<hr>
<h2 id="E2Net-Resource-Efficient-Continual-Learning-with-Elastic-Expansion-Network"><a href="#E2Net-Resource-Efficient-Continual-Learning-with-Elastic-Expansion-Network" class="headerlink" title="E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network"></a>E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16117">http://arxiv.org/abs/2309.16117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuruiqi0520/E2Net">https://github.com/liuruiqi0520/E2Net</a></li>
<li>paper_authors: RuiQi Liu, Boyu Diao, Libo Huang, Zhulin An, Yongjun Xu</li>
<li>for: 本研究旨在提出一种资源效率高的连续学习方法（Elastic Expansion Network，E2Net），以便在同等计算和存储限制下实现高平均精度和减少忘记。</li>
<li>methods: 该方法利用核心子网络精炼和准确回顾样本选择，以实现在同等计算和存储限制下的高平均精度和减少忘记。另外，我们还提出了代表网络精炼来确定核心子网络，以减少回顾缓存的依赖性和促进知识传递。</li>
<li>results: 我们的实验表明，E2Net在云环境和边缘环境中的多个数据集上具有很高的表现，并且在计算和存储限制下表现更优于当前状态方法。此外，我们的方法还在计算和存储限制下表现更优于竞争对手。<details>
<summary>Abstract</summary>
Continual Learning methods are designed to learn new tasks without erasing previous knowledge. However, Continual Learning often requires massive computational power and storage capacity for satisfactory performance. In this paper, we propose a resource-efficient continual learning method called the Elastic Expansion Network (E2Net). Leveraging core subnet distillation and precise replay sample selection, E2Net achieves superior average accuracy and diminished forgetting within the same computational and storage constraints, all while minimizing processing time. In E2Net, we propose Representative Network Distillation to identify the representative core subnet by assessing parameter quantity and output similarity with the working network, distilling analogous subnets within the working network to mitigate reliance on rehearsal buffers and facilitating knowledge transfer across previous tasks. To enhance storage resource utilization, we then propose Subnet Constraint Experience Replay to optimize rehearsal efficiency through a sample storage strategy based on the structures of representative networks. Extensive experiments conducted predominantly on cloud environments with diverse datasets and also spanning the edge environment demonstrate that E2Net consistently outperforms state-of-the-art methods. In addition, our method outperforms competitors in terms of both storage and computational requirements.
</details>
<details>
<summary>摘要</summary>
在E2Net中，我们提出了代表网络传授（Representative Network Distillation）来识别代表的核心子网，通过评估参数量和输出相似度，将相似的子网内部 integrate into the working network，以减少复练缓存的依赖和促进知识传递。此外，我们还提出了子网对应体验储存（Subnet Constraint Experience Replay）来优化复练效率，通过基于代表网络的构造储存样本。实验结果显示，E2Net在云端环境中与多种数据集进行实验，以及边缘环境中进行部分实验，能够与现有的方法相比，具有较高的性能。此外，我们的方法还比竞争者在存储和计算需求方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="Channel-Vision-Transformers-An-Image-Is-Worth-C-x-16-x-16-Words"><a href="#Channel-Vision-Transformers-An-Image-Is-Worth-C-x-16-x-16-Words" class="headerlink" title="Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words"></a>Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16108">http://arxiv.org/abs/2309.16108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/insitro/channelvit">https://github.com/insitro/channelvit</a></li>
<li>paper_authors: Yujia Bao, Srinivasan Sivanandan, Theofanis Karaletsos</li>
<li>for: 这篇文章的目的是对现代计算机视觉领域中的 Vision Transformer (ViT) 架构进行修改，以便在微scopic 和卫星影像领域中进行应用。</li>
<li>methods: 这篇文章提出了一种修改后的 ChannelViT 模型，它将在每个输入通道中独立建立 patch tokens，并使用可学习的通道嵌入，与位置嵌入一样。此外，它还引入了 Hierarchical Channel Sampling (HCS) 技术来保证模型在测试时运行时的Robustness。</li>
<li>results: 根据文章的实验结果，ChannelViT 模型在 ImageNet、JUMP-CP （微scopic 细胞影像）和 So2Sat （卫星影像）上的分类任务中表现出色，并且在只有部分输入通道可用时进行测试时仍能保持良好的表现。另外，HCS 技术被证明是一个强大的 regularizer，独立于架构选择。<details>
<summary>Abstract</summary>
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate the performance of ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat (satellite imaging). Our results show that ChannelViT outperforms ViT on classification tasks and generalizes well, even when a subset of input channels is used during testing. Across our experiments, HCS proves to be a powerful regularizer, independent of the architecture employed, suggesting itself as a straightforward technique for robust ViT training. Lastly, we find that ChannelViT generalizes effectively even when there is limited access to all channels during training, highlighting its potential for multi-channel imaging under real-world conditions with sparse sensors. Our code is available at https://github.com/insitro/ChannelViT.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉领域中，视觉转换器（ViT）已经成为一种强大的建筑。然而，在微scopic和卫星成像等领域中，使用ViT时会遇到一些独特的挑战。在这些领域中，图像通常包含多个渠道，每个渠道都携带着semantically独立且独立的信息。此外，模型需要在输入渠道上示 robustness，因为它们可能不会在训练或测试时都可用。在这篇论文中，我们提出了对ViT建筑的修改，以提高输入渠道之间的推理，并 introduce Hierarchical Channel Sampling（HCS）作为一种额外的正则化技术，以确保在测试时只有部分渠道可用时的模型robustness。我们提出的模型，ChannelViT，通过独立地从每个输入渠道中构建 patch tokens，并使用可学习的渠道嵌入，类似于位置嵌入。我们在ImageNet、JUMP-CP（微scopic细胞成像）和So2Sat（卫星成像）上评估了ChannelViT的性能。我们的结果表明，ChannelViT在分类任务上比ViT高效，并且在部分输入渠道时可以通过HCS来提高模型的robustness。在我们的实验中，HCS作为一种独立的正则化技术，无论使用哪种建筑，都有很好的效果。最后，我们发现ChannelViT在有限的输入渠道可用时也能够很好地适应，表明它在实际中的多渠道成像中具有潜在的应用价值。我们的代码可以在https://github.com/insitro/ChannelViT中找到。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Utility-driven-Interval-Rules"><a href="#Discovering-Utility-driven-Interval-Rules" class="headerlink" title="Discovering Utility-driven Interval Rules"></a>Discovering Utility-driven Interval Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16102">http://arxiv.org/abs/2309.16102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asem010/legend-pice">https://github.com/asem010/legend-pice</a></li>
<li>paper_authors: Chunkai Zhang, Maohua Lyu, Huaijin Hao, Wensheng Gan, Philip S. Yu</li>
<li>for: 这个论文的目的是提出一种基于interval事件序列的高用途序列规则挖掘算法，以解决现有方法不能直接应用于interval事件序列的问题。</li>
<li>methods: 该算法使用了一种数值编码关系表示法，以减少关系计算和存储的时间，并提出了一种补做截断策略，通过与用途upper bound相结合，缩小搜索空间。</li>
<li>results: 实验表明，该算法可以效果地和高效地从interval事件序列数据库中提取高用途间隔规则（UIRs），并在实际世界和synthetic数据集上实现了优秀的效果。<details>
<summary>Abstract</summary>
For artificial intelligence, high-utility sequential rule mining (HUSRM) is a knowledge discovery method that can reveal the associations between events in the sequences. Recently, abundant methods have been proposed to discover high-utility sequence rules. However, the existing methods are all related to point-based sequences. Interval events that persist for some time are common. Traditional interval-event sequence knowledge discovery tasks mainly focus on pattern discovery, but patterns cannot reveal the correlation between interval events well. Moreover, the existing HUSRM algorithms cannot be directly applied to interval-event sequences since the relation in interval-event sequences is much more intricate than those in point-based sequences. In this work, we propose a utility-driven interval rule mining (UIRMiner) algorithm that can extract all utility-driven interval rules (UIRs) from the interval-event sequence database to solve the problem. In UIRMiner, we first introduce a numeric encoding relation representation, which can save much time on relation computation and storage on relation representation. Furthermore, to shrink the search space, we also propose a complement pruning strategy, which incorporates the utility upper bound with the relation. Finally, plentiful experiments implemented on both real-world and synthetic datasets verify that UIRMiner is an effective and efficient algorithm.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a utility-driven interval rule mining (UIRMiner) algorithm that can extract all utility-driven interval rules (UIRs) from the interval-event sequence database to solve the problem. Our approach includes the following steps:First, we introduce a numeric encoding relation representation, which can save much time on relation computation and storage on relation representation.Second, to shrink the search space, we propose a complement pruning strategy, which incorporates the utility upper bound with the relation.Finally, we conduct plentiful experiments implemented on both real-world and synthetic datasets, and the results show that UIRMiner is an effective and efficient algorithm.
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Examples-Might-be-Avoidable-The-Role-of-Data-Concentration-in-Adversarial-Robustness"><a href="#Adversarial-Examples-Might-be-Avoidable-The-Role-of-Data-Concentration-in-Adversarial-Robustness" class="headerlink" title="Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness"></a>Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16096">http://arxiv.org/abs/2309.16096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambar Pal, Jeremias Sulam, René Vidal</li>
<li>for: This paper aims to investigate the question of whether adversarial examples are truly unavoidable for modern machine learning classifiers, and to provide theoretical results that demonstrate the existence of robust classifiers under certain conditions.</li>
<li>methods: The paper uses theoretical techniques to demonstrate the existence of robust classifiers for data distributions that have certain properties, such as concentration on small-volume subsets of the input space. The paper also explores the use of data structure to improve the robustness of classifiers.</li>
<li>results: The paper shows that, for certain data distributions, it is possible to construct classifiers that are robust to adversarial examples. Specifically, the paper demonstrates that, for data distributions concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.<details>
<summary>Abstract</summary>
The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.
</details>
<details>
<summary>摘要</summary>
现代机器学习分类器的感стви性面临到了劫难例的挑战，这些结果可能是不可避免的。然而，这些结果可能是对自然数据分布过于一般化的，人类在视觉任务中却很有抗性。这种冲突 inspirits我们深入探究：劫难例是否真的无法避免？在这项工作中，我们 theoretically 表明了数据分布的一个关键特性——输入空间中小量子体的集中性——会决定是否存在一个Robust的分类器。我们进一步表明，对于一个集中在低维线性子空间的数据分布，利用数据结构的特点可以得到一些具有良好robustness保证的分类器，超越一些特定情况下的证明证明。
</details></li>
</ul>
<hr>
<h2 id="AI-Potentiality-and-Awareness-A-Position-Paper-from-the-Perspective-of-Human-AI-Teaming-in-Cybersecurity"><a href="#AI-Potentiality-and-Awareness-A-Position-Paper-from-the-Perspective-of-Human-AI-Teaming-in-Cybersecurity" class="headerlink" title="AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity"></a>AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12162">http://arxiv.org/abs/2310.12162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iqbal H. Sarker, Helge Janicke, Nazeeruddin Mohammad, Paul Watters, Surya Nepal</li>
<li>for: 本研究探讨了人工智能在网络安全领域的潜在可能性，尤其是其可能的风险因素，通过人机合作（Human-AI）来管理这些风险。</li>
<li>methods: 本研究使用了人工智能技术，如Pattern recognition和预测模型，探索了AI在网络安全领域的可能性，并提出了一种 equilibrio balance方法，即将人类专业知识与AI计算能力相结合，以提高网络安全防御能力。</li>
<li>results: 本研究发现，通过人机合作，可以提高网络安全防御能力，并且可以减少相关的风险因素。此外，本研究还发现，AI可以帮助人类专业人员更好地理解和解决网络安全问题。<details>
<summary>Abstract</summary>
This position paper explores the broad landscape of AI potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., "Human-AI" teaming. As artificial intelligence (AI) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. However, the successful deployment of AI into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. Towards this, we emphasize the importance of a balanced approach that incorporates AI's computational power with human expertise. AI systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. Human experts can explain AI-generated decisions to stakeholders, regulators, and end-users in critical situations, ensuring responsibility and accountability, which helps establish trust in AI-driven security solutions. Therefore, in this position paper, we argue that human-AI teaming is worthwhile in cybersecurity, in which human expertise such as intuition, critical thinking, or contextual understanding is combined with AI's computational power to improve overall cyber defenses.
</details>
<details>
<summary>摘要</summary>
这份位点纸 analyze AI在cybersecurity领域的广泛潜力，尤其是其可能的风险因素，可以通过将人类专家纳入循环来管理，即"人机合作"（Human-AI teaming）。随着人工智能（AI）技术的进步，它将为攻击标识、事件回应和恢复提供无 precedent的机会。然而，在实施 AI 到 cybersecurity 措施方面，需要深入了解其能力、挑战和伦理法律因素，以处理相关风险因素在实际应用领域。为了实现这一目标，我们强调需要一种平衡的方法，即将 AI 的计算能力与人类专家的知识相结合。AI 系统可以扫描 Pattern 并探索漏洞，并预测模型，大幅提高速度和准确性。人类专家可以为重要情况中解释 AI 生成的决策，使得责任和财务可以被追溯，从而建立 AI 驱动的安全解决方案的信任。因此，在这份位点纸中，我们认为人机合作在cybersecurity中是值得投入的，在这种合作中，人类专家的直觉、批判思维和Contextual 理解与 AI 的计算能力相结合，从而提高总的cyber 防御能力。
</details></li>
</ul>
<hr>
<h2 id="TPE-Towards-Better-Compositional-Reasoning-over-Conceptual-Tools-with-Multi-persona-Collaboration"><a href="#TPE-Towards-Better-Compositional-Reasoning-over-Conceptual-Tools-with-Multi-persona-Collaboration" class="headerlink" title="TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration"></a>TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16090">http://arxiv.org/abs/2309.16090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Hongyuan Lu, Fei Mi, Kam-Fai Wong</li>
<li>for: 这 paper 旨在扩展大语言模型（LLM）在干扰问答任务中的规划能力，特别是在对话系统中使用不同的概念工具。</li>
<li>methods: 这 paper 使用了一种多人格协作框架：思考-规划-执行（TPE），将响应生成过程分解成三个不同角色：思考者、规划者和执行者。</li>
<li>results: 这 paper 在多源（FoCus）和多策略交互（CIMA和PsyQA）等响应生成任务中示出了效果，这表明它可以处理更为复杂的对话交互，而不仅仅是功能工具。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated exceptional performance in planning the use of various functional tools, such as calculators and retrievers, particularly in question-answering tasks. In this paper, we expand the definition of these tools, centering on conceptual tools within the context of dialogue systems. A conceptual tool specifies a cognitive concept that aids systematic or investigative thought. These conceptual tools play important roles in practice, such as multiple psychological or tutoring strategies being dynamically applied in a single turn to compose helpful responses. To further enhance the reasoning and planning capability of LLMs with these conceptual tools, we introduce a multi-persona collaboration framework: Think-Plan-Execute (TPE). This framework decouples the response generation process into three distinct roles: Thinker, Planner, and Executor. Specifically, the Thinker analyzes the internal status exhibited in the dialogue context, such as user emotions and preferences, to formulate a global guideline. The Planner then generates executable plans to call different conceptual tools (e.g., sources or strategies), while the Executor compiles all intermediate results into a coherent response. This structured approach not only enhances the explainability and controllability of responses but also reduces token redundancy. We demonstrate the effectiveness of TPE across various dialogue response generation tasks, including multi-source (FoCus) and multi-strategy interactions (CIMA and PsyQA). This reveals its potential to handle real-world dialogue interactions that require more complicated tool learning beyond just functional tools. The full code and data will be released for reproduction.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在几种功能工具的使用规划方面表现出色，特别是在问答任务中。在这篇论文中，我们扩展了这些工具的定义，将注重在对话系统中的概念工具。概念工具指定了思维的认知概念，以便系统atic或调查性思维。这些概念工具在实践中发挥重要作用，例如在单个转律中应用多种心理或教学策略以组成有用的回答。为了进一步增强LLM的理解和规划能力，我们引入了多人格协作框架：思考-规划-执行（TPE）。这个框架将响应生成过程分解成三个不同角色：思考者、规划者和执行者。具体来说，思考者通过对对话上下文中的内部状态，如用户情感和首选，来形成全局指南。规划者则生成可执行的计划，以调用不同的概念工具（如来源或策略），而执行者则将所有中间结果编译成一个准确的回答。这种结构化的方法不仅提高了回答的可解释性和控制性，还减少了各种重复的token。我们在多种对话回答生成任务中证明了TPE的效iveness，包括多源（FoCus）和多策略互动（CIMA和PsyQA）。这表明它可以处理现实世界中的对话互动，需要更为复杂的工具学习。我们将完整的代码和数据公开发布，以便其他人复制和扩展。
</details></li>
</ul>
<hr>
<h2 id="Forgetting-Private-Textual-Sequences-in-Language-Models-via-Leave-One-Out-Ensemble"><a href="#Forgetting-Private-Textual-Sequences-in-Language-Models-via-Leave-One-Out-Ensemble" class="headerlink" title="Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble"></a>Forgetting Private Textual Sequences in Language Models via Leave-One-Out Ensemble</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16082">http://arxiv.org/abs/2309.16082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Liu, Ozlem Kalinli</li>
<li>for: 隐私保护和模型更新</li>
<li>methods: 教师-学生框架和剩下一个 ensemble 方法</li>
<li>results: 在 LibriSpeech 和 WikiText-103 数据集上实现了更好的隐私利用之间的质量比In simpler English:</li>
<li>for: Privacy protection and model updating</li>
<li>methods: Teacher-student framework and leave-one-out ensemble method</li>
<li>results: Superior privacy-utility trade-offs on LibriSpeech and WikiText-103 datasets<details>
<summary>Abstract</summary>
Recent research has shown that language models have a tendency to memorize rare or unique token sequences in the training corpus. After deploying a model, practitioners might be asked to delete any personal information from the model by individuals' requests. Re-training the underlying model every time individuals would like to practice their rights to be forgotten is computationally expensive. We employ a teacher-student framework and propose a novel leave-one-out ensemble method to unlearn the targeted textual sequences that need to be forgotten from the model. In our approach, multiple teachers are trained on disjoint sets; for each targeted sequence to be removed, we exclude the teacher trained on the set containing this sequence and aggregate the predictions from remaining teachers to provide supervision during fine-tuning. Experiments on LibriSpeech and WikiText-103 datasets show that the proposed method achieves superior privacy-utility trade-offs than other counterparts.
</details>
<details>
<summary>摘要</summary>
最近的研究发现，语言模型有一种倾向，即记忆特殊或罕见的token序列在训练集中。当部署模型后，实际应用者可能需要根据个人需求删除模型中的个人信息。重新训练基础模型每次个人需要行使“忘记权”是 computationally expensive。我们采用教师-学生框架，并提出了一种新的离别一个学生 ensemble方法，用于从模型中忘记需要被忘记的文本序列。在我们的方法中，多个教师在不同的集合上进行训练;对于每个需要删除的序列，我们将包含这个序列的教师排除，并将剩下的教师的预测结果作为超vision提供给 fine-tuning。在 LibriSpeech 和 WikiText-103 数据集上进行的实验表明，我们的方法可以在 Privacy-Utility 质量之间取得更好的质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/28/cs.AI_2023_09_28/" data-id="cloqtaemt004lgh886pcg131t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/28/cs.CL_2023_09_28/" class="article-date">
  <time datetime="2023-09-28T11:00:00.000Z" itemprop="datePublished">2023-09-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/28/cs.CL_2023_09_28/">cs.CL - 2023-09-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Sign-Language-Recognition-System-with-Pepper-Lightweight-Transformer-and-LLM"><a href="#A-Sign-Language-Recognition-System-with-Pepper-Lightweight-Transformer-and-LLM" class="headerlink" title="A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM"></a>A Sign Language Recognition System with Pepper, Lightweight-Transformer, and LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16898">http://arxiv.org/abs/2309.16898</a></li>
<li>repo_url: None</li>
<li>paper_authors: JongYoon Lim, Inkyu Sa, Bruce MacDonald, Ho Seok Ahn</li>
<li>for: 实现人类与机器人之间的非语言互动，使用轻量级深度学习架构来理解美洲手语（ASL）。</li>
<li>methods: 利用轻量级深度学习模型来实现快速识别手语，并与大型自然语言模型（LLM）整合，实现智能机器人互动。通过几何调整引擎，调整互动以允许机器人产生自然的同步姿势回应。</li>
<li>results: 在实际应用中显示出轻量级深度学习架构可以实现高效的手语识别和自然的机器人互动，实现人类与机器人之间的无语言互动，扩大机器人的应用范围，并增进人类与机器人之间的沟通。<details>
<summary>Abstract</summary>
This research explores using lightweight deep neural network architectures to enable the humanoid robot Pepper to understand American Sign Language (ASL) and facilitate non-verbal human-robot interaction. First, we introduce a lightweight and efficient model for ASL understanding optimized for embedded systems, ensuring rapid sign recognition while conserving computational resources. Building upon this, we employ large language models (LLMs) for intelligent robot interactions. Through intricate prompt engineering, we tailor interactions to allow the Pepper Robot to generate natural Co-Speech Gesture responses, laying the foundation for more organic and intuitive humanoid-robot dialogues. Finally, we present an integrated software pipeline, embodying advancements in a socially aware AI interaction model. Leveraging the Pepper Robot's capabilities, we demonstrate the practicality and effectiveness of our approach in real-world scenarios. The results highlight a profound potential for enhancing human-robot interaction through non-verbal interactions, bridging communication gaps, and making technology more accessible and understandable.
</details>
<details>
<summary>摘要</summary>
这些研究探讨使用轻量级深度神经网络架构，使人类机器人Pepper能够理解美国手语（ASL），并促进无语言人机器人交互。首先，我们介绍了一种轻量级、高效的ASL理解模型，适用于嵌入式系统，以便快速认osciptic gesture，并保留计算资源。然后，我们利用大型自然语言模型（LLM），实现智能机器人交互。通过细腻的提示工程，我们调整交互，使Pepper机器人能够自然地生成Co-Speech Gesture响应，为人机器人对话铺垫基础。最后，我们提出了一个集成的软件管道，整合了社会意识AI交互模型。利用Pepper机器人的能力，我们在实际场景中展示了我们的方法的实用性和效果。结果表明，使用非语言交互可以bridge沟通差距，使技术更加 accessible和理解。
</details></li>
</ul>
<hr>
<h2 id="DeBERTinha-A-Multistep-Approach-to-Adapt-DebertaV3-XSmall-for-Brazilian-Portuguese-Natural-Language-Processing-Task"><a href="#DeBERTinha-A-Multistep-Approach-to-Adapt-DebertaV3-XSmall-for-Brazilian-Portuguese-Natural-Language-Processing-Task" class="headerlink" title="DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task"></a>DeBERTinha: A Multistep Approach to Adapt DebertaV3 XSmall for Brazilian Portuguese Natural Language Processing Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16844">http://arxiv.org/abs/2309.16844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Israel Campiotti, Matheus Rodrigues, Yuri Albuquerque, Rafael Azevedo, Alyson Andrade</li>
<li>for: This paper presents an approach for adapting a pre-trained English language model for use in Brazilian Portuguese natural language processing tasks.</li>
<li>methods: The methodology involves a multistep training process to fine-tune the model for the Portuguese language, using a combination of pre-trained English model weights and random embeddings.</li>
<li>results: The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks such as named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming a baseline model despite having fewer parameters.<details>
<summary>Abstract</summary>
This paper presents an approach for adapting the DebertaV3 XSmall model pre-trained in English for Brazilian Portuguese natural language processing (NLP) tasks. A key aspect of the methodology involves a multistep training process to ensure the model is effectively tuned for the Portuguese language. Initial datasets from Carolina and BrWac are preprocessed to address issues like emojis, HTML tags, and encodings. A Portuguese-specific vocabulary of 50,000 tokens is created using SentencePiece. Rather than training from scratch, the weights of the pre-trained English model are used to initialize most of the network, with random embeddings, recognizing the expensive cost of training from scratch. The model is fine-tuned using the replaced token detection task in the same format of DebertaV3 training. The adapted model, called DeBERTinha, demonstrates effectiveness on downstream tasks like named entity recognition, sentiment analysis, and determining sentence relatedness, outperforming BERTimbau-Large in two tasks despite having only 40M parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Curriculum-Driven-Edubot-A-Framework-for-Developing-Language-Learning-Chatbots-Through-Synthesizing-Conversational-Data"><a href="#Curriculum-Driven-Edubot-A-Framework-for-Developing-Language-Learning-Chatbots-Through-Synthesizing-Conversational-Data" class="headerlink" title="Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data"></a>Curriculum-Driven Edubot: A Framework for Developing Language Learning Chatbots Through Synthesizing Conversational Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16804">http://arxiv.org/abs/2309.16804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Li, Shang Qu, Jili Shen, Shangchao Min, Zhou Yu</li>
<li>for: 帮助学生提高对话技巧，满足学生在课程框架下的学习需求。</li>
<li>methods: 利用大语言模型生成对话，根据教科书中的相关话题进行EXTRACTING，然后使用自定义的LLM进行精度调整。</li>
<li>results: 比ChatGPT更好地领导课程基础的对话，能够根据用户的英语水平进行对话调整，提供学生个性化的对话实践。<details>
<summary>Abstract</summary>
Chatbots have become popular in educational settings, revolutionizing how students interact with material and how teachers teach. We present Curriculum-Driven EduBot, a framework for developing a chatbot that combines the interactive features of chatbots with the systematic material of English textbooks to assist students in enhancing their conversational skills. We begin by extracting pertinent topics from textbooks and then using large language models to generate dialogues related to these topics. We then fine-tune an open-source LLM using our generated conversational data to create our curriculum-driven chatbot. User studies demonstrate that our chatbot outperforms ChatGPT in leading curriculum-based dialogues and adapting its dialogue to match the user's English proficiency level. By combining traditional textbook methodologies with conversational AI, our approach offers learners an interactive tool that aligns with their curriculum and provides user-tailored conversation practice. This facilitates meaningful student-bot dialogues and enriches the overall learning experience within the curriculum's pedagogical framework.
</details>
<details>
<summary>摘要</summary>
chatbots 已经在教育 Setting 中变得流行，推翻了学生与材料之间的交互方式和教师教学方式。我们提出了 Curriculum-Driven EduBot 框架，用于开发一个结合了聊天机器人的互动特点和英语教科书系统的材料来帮助学生提高对话技巧。我们首先从教科书中提取有关话题，然后使用大型自然语言模型生成与这些话题相关的对话。然后，我们使用我们生成的对话数据来练化一个开源 LLM，以创建受教科书驱动的聊天机器人。用户研究表明，我们的聊天机器人在课程基础的对话中表现出色，并且可以根据用户的英语水平进行对话调整。通过结合传统教科书方法与对话 AI，我们的方法提供了学习者一种交互的工具，与其课程的教学框架相吻合。这使得学生与机器人的对话变得有意义，并润色了整个学习经验。
</details></li>
</ul>
<hr>
<h2 id="Hallucination-Reduction-in-Long-Input-Text-Summarization"><a href="#Hallucination-Reduction-in-Long-Input-Text-Summarization" class="headerlink" title="Hallucination Reduction in Long Input Text Summarization"></a>Hallucination Reduction in Long Input Text Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16781">http://arxiv.org/abs/2309.16781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tohidarehman/hallucination-reduction-text-summarization">https://github.com/tohidarehman/hallucination-reduction-text-summarization</a></li>
<li>paper_authors: Tohida Rehman, Ronit Mandal, Abhishek Agarwal, Debarshi Kumar Sanyal</li>
<li>for: 本研究旨在降低长文摘要中的幻觉输出（hallucination），以提高摘要的准确性和可靠性。</li>
<li>methods: 我们使用了数据筛选和共同实体和摘要生成（JAENS）技术，对Longformer Encoder-Decoder（LED）模型进行精度调整，以降低幻觉输出。</li>
<li>results: 我们的实验表明，精度调整后的LED模型能够良好地生成文章摘要。数据筛选技术基于一些预处理步骤，可以降低生成摘要中的实体幻觉水平，以judged by some factual consistency metrics。<details>
<summary>Abstract</summary>
Hallucination in text summarization refers to the phenomenon where the model generates information that is not supported by the input source document. Hallucination poses significant obstacles to the accuracy and reliability of the generated summaries. In this paper, we aim to reduce hallucinated outputs or hallucinations in summaries of long-form text documents. We have used the PubMed dataset, which contains long scientific research documents and their abstracts. We have incorporated the techniques of data filtering and joint entity and summary generation (JAENS) in the fine-tuning of the Longformer Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the quality of the generated summary. We have used the following metrics to measure factual consistency at the entity level: precision-source, and F1-target. Our experiments show that the fine-tuned LED model performs well in generating the paper abstract. Data filtering techniques based on some preprocessing steps reduce entity-level hallucinations in the generated summaries in terms of some of the factual consistency metrics.
</details>
<details>
<summary>摘要</summary>
描述文本简化中的幻觉现象指的是模型生成的信息不受输入文档支持。幻觉会对简化后的摘要准确性和可靠性产生很大的影响。在这篇论文中，我们想降低摘要中的幻觉输出或幻觉。我们使用了PubMed数据集，这个数据集包含长篇科学研究文献和其摘要。我们在LED模型的精度调节阶段采用数据筛选和联合实体和摘要生成（JAENS）技术，以减少幻觉并提高生成的摘要质量。我们使用了以下度量来衡量实体层次的事实一致性：准确性-源，F1-目标。我们的实验表明，精度调节后的LED模型能够好地生成文档摘要。基于一些预处理步骤的数据筛选技术可以在生成的摘要中减少实体层次的幻觉。
</details></li>
</ul>
<hr>
<h2 id="Demystifying-CLIP-Data"><a href="#Demystifying-CLIP-Data" class="headerlink" title="Demystifying CLIP Data"></a>Demystifying CLIP Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16671">http://arxiv.org/abs/2309.16671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/metaclip">https://github.com/facebookresearch/metaclip</a></li>
<li>paper_authors: Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, Christoph Feichtenhofer</li>
<li>for: This paper is written for advancing research and applications in computer vision, particularly in the area of contrastive language-image pre-training (CLIP).</li>
<li>methods: The paper introduces a new approach called Metadata-Curated Language-Image Pre-training (MetaCLIP), which aims to reveal CLIP’s data curation approach and make it open to the community. MetaCLIP takes a raw data pool and metadata (derived from CLIP’s concepts) and yields a balanced subset over the metadata distribution.</li>
<li>results: The paper reports that MetaCLIP outperforms CLIP’s data on multiple standard benchmarks, achieving 70.8% accuracy on zero-shot ImageNet classification with 400M image-text data pairs, and scaling to 1B data with the same training budget. The paper also shows that MetaCLIP achieves better performance than CLIP on various model sizes, such as ViT-H, which achieves 80.5% accuracy without any bells-and-whistles.<details>
<summary>Abstract</summary>
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP's data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-H achieving 80.5%, without any bells-and-whistles. Curation code and training data distribution on metadata is made available at https://github.com/facebookresearch/MetaCLIP.
</details>
<details>
<summary>摘要</summary>
CLIP（语言图像预训练）是一种技术，它已经提高了计算机视觉领域的研究和应用，推动现代识别系统和生成模型。我们认为CLIP的成功的主要原因是其数据，而不是模型结构或预训练目标。然而，CLIP只提供了非常有限的数据信息和收集方法，导致一些工作尝试通过CLIP模型参数来复制CLIP的数据。在这种情况下，我们计划披露CLIP的数据筛选策略，并在为社区开放CLIP的数据预处理技术引入Metadata-Curated Language-Image Pre-training（MetaCLIP）。MetaCLIP使用原始数据池和元数据（从CLIP的概念中 derivated）来生成元数据分布平衡subset。我们的实验充分隔离模型和训练参数，专注于数据。在应用于CommonCrawl的400万张图像文本对比 experiment，MetaCLIP exceeds CLIP的数据在多个标准测试 benchmark 上。在零容量ImageNet分类任务中，MetaCLIP实现了70.8%的准确率，比CLIP的68.3%高于ViT-B模型。在增加到1亿个数据时，保持相同的训练预算，MetaCLIP达到了72.4%。我们的观察结果在不同的模型大小上都具有相同的特点，例如ViT-H实现了80.5%的准确率，无需任何额外的技术。我们在 GitHub 上提供了数据预处理代码和训练数据分布，请参考https://github.com/facebookresearch/MetaCLIP。
</details></li>
</ul>
<hr>
<h2 id="Qwen-Technical-Report"><a href="#Qwen-Technical-Report" class="headerlink" title="Qwen Technical Report"></a>Qwen Technical Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16609">http://arxiv.org/abs/2309.16609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/QwenLM/Qwen-7B">https://github.com/QwenLM/Qwen-7B</a></li>
<li>paper_authors: Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu</li>
<li>for: 这篇论文旨在推介一种新的大型自然语言处理（NLP）模型系列，称为Qwen系列。</li>
<li>methods: 该论文使用了不同参数计数的模型，包括基础预训练模型Qwen以及通过人工对齐技术微调的聊天模型Qwen-Chat。</li>
<li>results: 研究表明，基础模型在多种下游任务中表现出色，而微调后的聊天模型具有出色的工具使用和规划能力，可以创建高效的智能应用程序。此外，研究还开发了专门为编程和数学领域的模型，即Code-Qwen和Code-Qwen-Chat，以及Math-Qwen-Chat，这些模型在相关任务上表现出优于开源模型，但落后于商业模型。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）已经革命化人工智能领域，使得先前被认为是人类专有的自然语言处理任务现在可以由机器完成。在这项工作中，我们介绍了Qwen系列，这是我们的大型语言模型系列，包括不同参数数量的多种模型。其中包括Qwen基础预训练语言模型和Qwen-Chat通话模型，后者通过人类对齐技术进行了加工。基础语言模型在多个下游任务中几乎一直表现出优秀的表现，而通话模型，特别是使用人类反馈学习（RLHF）进行训练的通话模型，在创造代理应用程序时具有高级工具使用和规划能力，并在使用代码解释器的复杂任务上表现出色。此外，我们还开发了专门为编程而设计的模型，名为Code-Qwen和Code-Qwen-Chat，以及专门为数学而设计的模型，名为Math-Qwen-Chat，这些模型基于基础语言模型。这些模型在相比于开源模型的情况下表现出了显著的提升，并只有轻微落后于商业模型。
</details></li>
</ul>
<hr>
<h2 id="Unlikelihood-Tuning-on-Negative-Samples-Amazingly-Improves-Zero-Shot-Translation"><a href="#Unlikelihood-Tuning-on-Negative-Samples-Amazingly-Improves-Zero-Shot-Translation" class="headerlink" title="Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation"></a>Unlikelihood Tuning on Negative Samples Amazingly Improves Zero-Shot Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16599">http://arxiv.org/abs/2309.16599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zanchangtong/unions">https://github.com/zanchangtong/unions</a></li>
<li>paper_authors: Changtong Zan, Liang Ding, Li Shen, Yibin Lei, Yibing Zhan, Weifeng Liu, Dacheng Tao</li>
<li>for: 这个研究旨在解释在零shot翻译（ZST）任务中语言标识符（ID）的导航能力是如何受限的。</li>
<li>methods: 研究使用了零shot翻译模型，并对两种极端的decoder输入情况进行比较分析：Off-Target（OFF）和On-Target（ON）两种情况。通过对 Contextual Word Representations（CWRs）进行比较分析，研究发现了语言标识符在不同情况下的导航能力。</li>
<li>results: 研究发现，although language IDs work well in ideal ON settings, they become fragile and lose their navigation ability when faced with off-target tokens。为了解决这个问题，研究使用了不利可能性调整法，减少了off-target ratio，导致了BLEU分数的提高。<details>
<summary>Abstract</summary>
Zero-shot translation (ZST), which is generally based on a multilingual neural machine translation model, aims to translate between unseen language pairs in training data. The common practice to guide the zero-shot language mapping during inference is to deliberately insert the source and target language IDs, e.g., <EN> for English and <DE> for German. Recent studies have shown that language IDs sometimes fail to navigate the ZST task, making them suffer from the off-target problem (non-target language words exist in the generated translation) and, therefore, difficult to apply the current multilingual translation model to a broad range of zero-shot language scenarios. To understand when and why the navigation capabilities of language IDs are weakened, we compare two extreme decoder input cases in the ZST directions: Off-Target (OFF) and On-Target (ON) cases. By contrastively visualizing the contextual word representations (CWRs) of these cases with teacher forcing, we show that 1) the CWRs of different languages are effectively distributed in separate regions when the sentence and ID are matched (ON setting), and 2) if the sentence and ID are unmatched (OFF setting), the CWRs of different languages are chaotically distributed. Our analyses suggest that although they work well in ideal ON settings, language IDs become fragile and lose their navigation ability when faced with off-target tokens, which commonly exist during inference but are rare in training scenarios. In response, we employ unlikelihood tuning on the negative (OFF) samples to minimize their probability such that the language IDs can discriminate between the on- and off-target tokens during training. Experiments spanning 40 ZST directions show that our method reduces the off-target ratio by -48.0% on average, leading to a +9.1 BLEU improvement with only an extra +0.3% tuning cost.
</details>
<details>
<summary>摘要</summary>
zero-shot翻译（ZST）通常基于多语言神经机器翻译模型，旨在在训练数据中未经见过的语言对之间翻译。通常情况下，在推导 zero-shot 语言映射时，会故意插入源语言ID和目标语言ID，例如 <EN> 表示英语和 <DE> 表示德语。然而， latest studies 表明，语言ID 在推导 ZST 任务中的导航能力有时会弱化，导致翻译结果受到非目标语言词汇的影响，从而使得当前多语言翻译模型难以应用于广泛的 zero-shot 语言enario。为了了解语言ID 在 ZST 任务中的导航能力是如何弱化的，我们比较了两种极端的解码输入情况：Off-Target（OFF）和 On-Target（ON）两种情况。通过比较这两种情况下的上下文字表示（CWR），我们发现：1）当 sentence 和 ID 匹配时（ON setting），不同语言的 CWR 分布在不同的区域，2）如果 sentence 和 ID 不匹配（OFF setting），不同语言的 CWR 分布混乱。我们的分析表明，虽然它们在理想的 ON 设置下工作非常好，但是语言 ID 在面对非目标语言词汇时变得脆弱，丢弃了导航能力。为了解决这个问题，我们使用不良抽象训练方法，通过训练时间间隔的负样本进行训练，以降低 OFF 样本的概率，使语言 ID 能够在训练中分辨在目标语言和非目标语言之间。实验结果表明，我们的方法可以降低 OFF 比例平均 -48.0%，并且提高 BLEU 平均 +9.1，只需要额外花费 +0.3% 的训练成本。
</details></li>
</ul>
<hr>
<h2 id="GPT-Fathom-Benchmarking-Large-Language-Models-to-Decipher-the-Evolutionary-Path-towards-GPT-4-and-Beyond"><a href="#GPT-Fathom-Benchmarking-Large-Language-Models-to-Decipher-the-Evolutionary-Path-towards-GPT-4-and-Beyond" class="headerlink" title="GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond"></a>GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16583">http://arxiv.org/abs/2309.16583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gpt-fathom/gpt-fathom">https://github.com/gpt-fathom/gpt-fathom</a></li>
<li>paper_authors: Shen Zheng, Yuyu Zhang, Yijie Zhu, Chenguang Xi, Pengyang Gao, Xun Zhou, Kevin Chen-Chuan Chang</li>
<li>for: 评估大语言模型（LLM）的全面能力和局限性。</li>
<li>methods: 使用OpenAI Evals开发了一个开源和可重现的LLM评估suite，对10多个领先的LLM以及OpenAI的遗产模型进行了20多个精心制定的测试，并进行了7种能力类别的评估。</li>
<li>results: 对OpenAI的早期模型进行了Retrospective研究，提供了各种LLM的进步和改进的技术细节，如 Whether adding code data improves LLM’s reasoning capability， Which aspects of LLM capability can be improved by SFT and RLHF，Alignment tax等问题的解答，以提高高级LLM的透明度。<details>
<summary>Abstract</summary>
With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.
</details>
<details>
<summary>摘要</summary>
With the rapid development of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.Here's the translation in Traditional Chinese as well:With the rapid development of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-for-Learning-to-Translate-a-New-Language-from-One-Grammar-Book"><a href="#A-Benchmark-for-Learning-to-Translate-a-New-Language-from-One-Grammar-Book" class="headerlink" title="A Benchmark for Learning to Translate a New Language from One Grammar Book"></a>A Benchmark for Learning to Translate a New Language from One Grammar Book</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16575">http://arxiv.org/abs/2309.16575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke Melas-Kyriazi</li>
<li>for: 这个论文是为了测试大型语言模型（LLM）在新任务上的能力，以及使用少量数据进行语言学习。</li>
<li>methods: 这个论文使用了现有的LLM作为基础，并在一本 Kalamang 语言 grammar 引用书上进行了一些 slight 的修改和微调。</li>
<li>results: 研究发现，使用当前的 LLM 可以达到44.7chrF 的 Kalamang 到英语翻译和45.8chrF 的英语到 Kalamang 翻译，相比之下，人类学习 Kalamang 从同一个引用书上的结果为51.6和57.0chrF。<details>
<summary>Abstract</summary>
Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -- using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials. We hope that MTOB will help measure LLM capabilities along a new dimension, and that the methods developed to solve it could help expand access to language technology for underserved communities by leveraging qualitatively different kinds of data than traditional machine translation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以执行吸引人的表现，使用内容学习或轻量级调整。人们 naturallly 会想知道这些模型是否可以适应真正的新任务，但如何找到互联网上没有的任务呢？我们到了一个这些语言的缺乏网络数据的领域：低资源语言。在这篇文章中，我们介绍了 MTOB（从一本书 Machine Translation），一个用于将英语和卡拉曼（一种只有 fewer than 200 名 speaker的语言）之间进行翻译的benchmark。这个任务框架是新的，因为它请求一个模型从单一的人类可读的 grammar 解释书中学习一个语言，而不是从大量矿物质的内部数据中学习，更像是 L2 学习而不是 L1 获得。我们展示了现有的 LLB 是可以 promise 的，但落后于人类性能，实现了从英语到卡拉曼的翻译和从卡拉曼到英语的翻译的chrF 44.7和45.8，相比之下，人类从同一个 reference materials 学习 Kalamang 的chrF 为51.6和57.0。我们希望 MTOB 可以帮助衡量 LLM 的能力，并且可以帮助扩展语言科技 для被排除的社区，通过使用不同于传统机器翻译的数据来进行。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Fact-Verification-by-Language-Model-Distillation"><a href="#Unsupervised-Fact-Verification-by-Language-Model-Distillation" class="headerlink" title="Unsupervised Fact Verification by Language Model Distillation"></a>Unsupervised Fact Verification by Language Model Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16540">http://arxiv.org/abs/2309.16540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrián Bazaga, Pietro Liò, Gos Micklem</li>
<li>for: 这篇论文的目的是为了无监督的事实验证，即使没有任何标注数据，也能够使用可信worthy知识库中的证据来验证一个声明。</li>
<li>methods: 这篇论文使用了自动学习的方法，并且利用预训语言模型来将自动生成的特征整合到高品质的声明和证据的Alignment中。这是由于一个新的对称损失函数，让特征能够获得高品质的声明和证据的Alignment，同时保持数据库中的semantic关系。</li>
<li>results: 这篇论文获得了新的州际对称检测benchmark（+8%对称精度）的最佳结果，并且在线性评估中获得了最佳结果。<details>
<summary>Abstract</summary>
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the-art on the standard FEVER fact verification benchmark (+8% accuracy) with linear evaluation.
</details>
<details>
<summary>摘要</summary>
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments while preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the-art on the standard FEVER fact verification benchmark (+8% accuracy) with linear evaluation.Here's the translation in Traditional Chinese:Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments while preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the-art on the standard FEVER fact verification benchmark (+8% accuracy) with linear evaluation.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-of-Document-level-Relation-Extraction-2016-2023"><a href="#A-Comprehensive-Survey-of-Document-level-Relation-Extraction-2016-2023" class="headerlink" title="A Comprehensive Survey of Document-level Relation Extraction (2016-2023)"></a>A Comprehensive Survey of Document-level Relation Extraction (2016-2023)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16396">http://arxiv.org/abs/2309.16396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julien Delaunay, Hanh Thi Hong Tran, Carlos-Emiliano González-Gallardo, Georgeta Bordea, Nicolas Sidere, Antoine Doucet</li>
<li>for: 这篇论文旨在提供关于近期文本关系抽取（DocRE）领域的全面概述，强调其与句子关系抽取的区别和应用场景。</li>
<li>methods: 本文使用了多种方法，包括文本分析、命名实体识别、语义理解等，以提取文档中的关系。</li>
<li>results: 本文提出了一些新的 DocRE 方法，并评估了它们的性能。这些方法可以帮助自动生成知识库，以提高对文档中关系的理解。<details>
<summary>Abstract</summary>
Document-level relation extraction (DocRE) is an active area of research in natural language processing (NLP) concerned with identifying and extracting relationships between entities beyond sentence boundaries. Compared to the more traditional sentence-level relation extraction, DocRE provides a broader context for analysis and is more challenging because it involves identifying relationships that may span multiple sentences or paragraphs. This task has gained increased interest as a viable solution to build and populate knowledge bases automatically from unstructured large-scale documents (e.g., scientific papers, legal contracts, or news articles), in order to have a better understanding of relationships between entities. This paper aims to provide a comprehensive overview of recent advances in this field, highlighting its different applications in comparison to sentence-level relation extraction.
</details>
<details>
<summary>摘要</summary>
文档级关系EXTRACTION（DocRE）是一个活跃的研究领域，涉及到自然语言处理（NLP）中identifying和EXTRACTING关系之外句子 boundariestra. 相比传统的句子级关系EXTRACTION，DocRE提供了更广阔的上下文，并且更加挑战性，因为它涉及到可能 span multiple sentences或 paragraphs 中的关系。这项任务在建立和自动填充大规模文档（例如科学论文、法律合同或新闻文章）中，以获得更好的实体之间关系的理解。这篇论文的目的是提供 DocRE 领域最新的进展， highlighting 它的不同应用场景与 sentence-level relation extraction 相比。
</details></li>
</ul>
<hr>
<h2 id="Transformer-VQ-Linear-Time-Transformers-via-Vector-Quantization"><a href="#Transformer-VQ-Linear-Time-Transformers-via-Vector-Quantization" class="headerlink" title="Transformer-VQ: Linear-Time Transformers via Vector Quantization"></a>Transformer-VQ: Linear-Time Transformers via Vector Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16354">http://arxiv.org/abs/2309.16354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/transformer-vq/transformer_vq">https://github.com/transformer-vq/transformer_vq</a></li>
<li>paper_authors: Lucas D. Lingle</li>
<li>for: 这个论文是为了提出一种基于Transformer的嵌入式自注意力计算方法，以实现高效的自注意力计算。</li>
<li>methods: 该方法使用了vector-quantized keys和一种新的缓存机制，实现了高效的自注意力计算。</li>
<li>results: 在大规模实验中，该方法表现出色，在Enwik8（0.99 bpb）、PG-19（26.6 ppl）和ImageNet64（3.16 bpb）等测试集上达到了高水平的结果。Here’s the English version of the summary:</li>
<li>for: This paper proposes a decoder-only transformer computing softmax-based dense self-attention in linear time.</li>
<li>methods: The method uses vector-quantized keys and a novel caching mechanism to achieve efficient attention.</li>
<li>results: In large-scale experiments, the method achieves high-quality results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb).<details>
<summary>Abstract</summary>
We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
</details>
<details>
<summary>摘要</summary>
我们介绍Transformer-VQ，一个仅有decoder的transformer computing软max-based dense自注意力，在线性时间内进行计算。Transformer-VQ的高效注意力得以实现因 vector-quantized keys和一种新的储存机制。在大规模实验中，Transformer-VQ表现出高品质，在Enwik8（0.99 bpb）、PG-19（26.6 ppl）和ImageNet64（3.16 bpb）上获得了强劲的结果。代码：https://github.com/transformer-vq/transformer_vq
</details></li>
</ul>
<hr>
<h2 id="Human-Feedback-is-not-Gold-Standard"><a href="#Human-Feedback-is-not-Gold-Standard" class="headerlink" title="Human Feedback is not Gold Standard"></a>Human Feedback is not Gold Standard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16349">http://arxiv.org/abs/2309.16349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cohere-ai/human-feedback-paper">https://github.com/cohere-ai/human-feedback-paper</a></li>
<li>paper_authors: Tom Hosking, Phil Blunsom, Max Bartolo</li>
<li>for: 本研究探讨了人类反馈在评估大型自然语言模型性能时的作用，以及这种评估方法是否能够完全捕捉多种重要错误标准。</li>
<li>methods: 研究者使用了人类反馈来训练和评估模型，并分析了 preference scores 是否受到不良偏见的影响。他们还使用了 instruction-tuned 模型来生成输出，以探讨输出的干扰因素。</li>
<li>results: 研究者发现， preference scores 覆盖率相对较好，但忽略了重要的准确性因素。此外，他们发现人类反馈可能受到干扰因素的影响，并且使用人类反馈作为训练目标可能会导致模型输出更加夸大。<details>
<summary>Abstract</summary>
Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single `preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.
</details>
<details>
<summary>摘要</summary>
人类反馈已成为大语言模型性能评估的德法标准，并在训练和评估中使用。然而，不清楚哪些属性得到这一单一的喜好分数。我们假设 preference 分数是主观的和易受到不良偏见的。我们critically analyzes the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesize that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Intrinsic-Language-Guided-Exploration-for-Complex-Long-Horizon-Robotic-Manipulation-Tasks"><a href="#Intrinsic-Language-Guided-Exploration-for-Complex-Long-Horizon-Robotic-Manipulation-Tasks" class="headerlink" title="Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks"></a>Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16347">http://arxiv.org/abs/2309.16347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleftherios Triantafyllidis, Filippos Christianos, Zhibin Li</li>
<li>for:  addresses intricate long-horizon with sparse rewards robotic manipulation tasks</li>
<li>methods:  leverages LLMs as an assistive intrinsic reward to guide the exploratory process in reinforcement learning</li>
<li>results:  exhibits notably higher performance, can be combined with existing learning methods, and maintains robustness against increased levels of uncertainty and horizons.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在解决复杂环境中的长时间探索任务，特别是 robotic manipulation 任务中的多种序列。</li>
<li>methods: 我们提出了基于大语言模型（LLMs）的自适应探索框架（IGE-LLMs），利用 LLMS 作为帮助探索过程的内在奖励。</li>
<li>results: 我们的框架在探索和长时间任务中表现出色，与相关的内在学习方法和直接使用 LLMS 进行决策相比，显示更高的性能，可以与现有的学习方法相结合，并且对不同的内在缩放参数表现相对稳定，能够在不同的不确定性和时间轴水平上保持稳定性。<details>
<summary>Abstract</summary>
Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and (iv) maintain robustness against increased levels of uncertainty and horizons.
</details>
<details>
<summary>摘要</summary>
当前的强化学习算法在稀疏和复杂环境中努力，特别是长时间 manipulate 任务中的多种序列。在这种工作中，我们提出了由大语言模型引导的自适应探索框架（IGE-LLMs）。通过利用 LLMS 作为帮助性的内在奖励，IGE-LLMs 引导了强化学习中的探索过程，以解决复杂的长时间 manipulate 任务。我们评估了我们的框架和相关的内在学习方法，并在一个具有探索挑战和复杂 manipulate 任务的环境中进行了测试。结果显示，IGE-LLMs 具有以下特点：(i) 与相关的内在方法和直接使用 LLMS 在决策中表现更高水平；(ii) 可以与现有的学习方法相结合和补充，表现协作性；(iii) 对不同的内在涨积参数 exhibit 鲁棒性；(iv) 在不同的不确定性和时间距离水平上保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="At-Which-Training-Stage-Does-Code-Data-Help-LLMs-Reasoning"><a href="#At-Which-Training-Stage-Does-Code-Data-Help-LLMs-Reasoning" class="headerlink" title="At Which Training Stage Does Code Data Help LLMs Reasoning?"></a>At Which Training Stage Does Code Data Help LLMs Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16298">http://arxiv.org/abs/2309.16298</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yingweima2022/codellm">https://github.com/yingweima2022/codellm</a></li>
<li>paper_authors: Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, Shanshan Li<br>for: 这个论文旨在研究在不同训练阶段引入代码数据对大语言模型（LLMs）的影响，以提高它们的推理能力。methods: 该论文使用了多种训练策略，包括在预训练阶段、指令调整阶段和两者同时使用代码数据，以评估LLMs的推理能力。results: 研究发现，在预训练阶段使用代码和文本混合数据可以大幅提高LLMs的通用推理能力，而在指令调整阶段使用代码数据可以增强LLMs的任务特定推理能力。此外，动态混合策略可以帮助LLMs逐步学习推理能力 durante 训练。这些发现可以深入理解LLMs在应用领域中的推理能力，如科学问答、法律支持等。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks in five domains. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreover, the dynamic mixing strategy of code and text data assists LLMs to learn reasoning capability step-by-step during training. These insights deepen the understanding of LLMs regarding reasoning ability for their application, such as scientific question answering, legal support, etc. The source code and model parameters are released at the link:~\url{https://github.com/yingweima2022/CodeLLM}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在语言技术基础上展现出了很好的逻辑能力，成为现代语言技术的基础。继承 code 数据在训练 LLM 中的成功，我们自然会问到在不同训练阶段引入 code 数据可以真正地帮助 LLM 的逻辑能力。为此，本文系统地探讨了在不同训练阶段引入 code 数据对 LLM 的影响。具体来说，我们在预训练阶段、指令调整阶段和两者都引入 code 数据，然后通过六种逻辑任务在五个领域进行了公平和全面的评估。我们对实验结果进行了深入分析，并提供了关于这些结论的深入理解。首先，在预训练阶段将代码和文本混合为一个混合数据集可以帮助 LLM 提高总的逻辑能力，并且几乎没有负面转移到其他任务。此外，在指令调整阶段，代码数据可以赋予 LLM 任务特定的逻辑能力。此外，在动态混合策略下，代码和文本数据的混合可以帮助 LLM 逐步学习逻辑能力 durante 训练。这些发现深入了我们对 LLM 的逻辑能力的理解，并为其应用，如科学问答、法律支持等提供了深入的理解。模型参数和源代码可以在以下链接获取：https://github.com/yingweima2022/CodeLLM。
</details></li>
</ul>
<hr>
<h2 id="DiLu-A-Knowledge-Driven-Approach-to-Autonomous-Driving-with-Large-Language-Models"><a href="#DiLu-A-Knowledge-Driven-Approach-to-Autonomous-Driving-with-Large-Language-Models" class="headerlink" title="DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models"></a>DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16292">http://arxiv.org/abs/2309.16292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PJLab-ADG/DiLu">https://github.com/PJLab-ADG/DiLu</a></li>
<li>paper_authors: Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao</li>
<li>for: The paper aims to instill knowledge-driven capabilities into autonomous driving systems, inspired by human driving, and to address the challenges of dataset bias, overfitting, and uninterpretability in data-driven approaches.</li>
<li>methods: The proposed DiLu framework combines a Reasoning and a Reflection module to enable decision-making based on common-sense knowledge and to evolve continuously. The framework leverages large language models with emergent abilities.</li>
<li>results: Extensive experiments show that DiLu has a significant advantage in generalization ability over reinforcement learning-based methods and can directly acquire experiences from real-world datasets, demonstrating its potential for deployment on practical autonomous driving systems.<details>
<summary>Abstract</summary>
Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to instill knowledge-driven capability into autonomous driving systems from the perspective of how humans drive.
</details>
<details>
<summary>摘要</summary>
Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to make decisions based on common-sense knowledge and evolve continuously. Extensive experiments show that DiLu can accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Additionally, DiLu can directly acquire experiences from real-world datasets, highlighting its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to instill knowledge-driven capability into autonomous driving systems from the perspective of how humans drive.Translation notes:* "data-driven approaches" ⇒ 数据驱动方法 (data-driven methods)* "dataset bias" ⇒ 数据集偏见 (dataset bias)* "overfitting" ⇒ 过拟合 (overfitting)* "uninterpretability" ⇒ 不可解释性 (uninterpretability)* "knowledge-driven nature of human driving" ⇒ 人类驾驶的知识驱动性 (knowledge-driven nature of human driving)* "DiLu framework" ⇒ DiLu框架 (DiLu framework)* "Reasoning and Reflection module" ⇒ 理解和反思模块 (Reasoning and Reflection module)* "common-sense knowledge" ⇒ 常识知识 (common-sense knowledge)* "practical autonomous driving systems" ⇒ 实用自动驾驶系统 (practical autonomous driving systems)
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Cross-view-Representation-Reconstruction-for-Change-Captioning"><a href="#Self-supervised-Cross-view-Representation-Reconstruction-for-Change-Captioning" class="headerlink" title="Self-supervised Cross-view Representation Reconstruction for Change Captioning"></a>Self-supervised Cross-view Representation Reconstruction for Change Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16283">http://arxiv.org/abs/2309.16283</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tuyunbin/SCORER">https://github.com/tuyunbin/SCORER</a></li>
<li>paper_authors: Yunbin Tu, Liang Li, Li Su, Zheng-Jun Zha, Chenggang Yan, Qingming Huang<br>for: 本研究旨在提出一种自动化描述变换的方法，以便在视点变化导致的 Pseudo 变换下学习稳定的差异表示。methods: 我们提出了一种自动化描述变换的方法，即基于多头token-wise匹配的自然语言描述（SCORER）网络。该方法通过对同种&#x2F;不同种图像的交叉视图特征进行多头匹配，然后通过最大化交叉视图对两个相似图像的对齐来学习两个视点不变的图像表示。results: 我们的方法在四个 dataset 上实现了 estado-of-the-art 的结果，并且提供了一种自然语言描述的方法，以便在视点变化下学习稳定的差异表示。<details>
<summary>Abstract</summary>
Change captioning aims to describe the difference between a pair of similar images. Its key challenge is how to learn a stable difference representation under pseudo changes caused by viewpoint change. In this paper, we address this by proposing a self-supervised cross-view representation reconstruction (SCORER) network. Concretely, we first design a multi-head token-wise matching to model relationships between cross-view features from similar/dissimilar images. Then, by maximizing cross-view contrastive alignment of two similar images, SCORER learns two view-invariant image representations in a self-supervised way. Based on these, we reconstruct the representations of unchanged objects by cross-attention, thus learning a stable difference representation for caption generation. Further, we devise a cross-modal backward reasoning to improve the quality of caption. This module reversely models a ``hallucination'' representation with the caption and ``before'' representation. By pushing it closer to the ``after'' representation, we enforce the caption to be informative about the difference in a self-supervised manner. Extensive experiments show our method achieves the state-of-the-art results on four datasets. The code is available at https://github.com/tuyunbin/SCORER.
</details>
<details>
<summary>摘要</summary>
《 Change Captioning with Self-supervised Cross-view Representation Reconstruction (SCORER) Network》Abstract:在本文中，我们提出了一种基于自我超vised学习的图像描述文本生成方法，即自适应交叉视图表示重建（SCORER）网络。我们首先设计了多头token wise匹配来modelcross-view特征之间的关系，然后通过最大化交叉视图对两个相似图像的对齐来学习两个不同视图的图像表示。然后，我们通过跨modal推理来提高描述文本质量。我们在四个数据集上进行了广泛的实验，并达到了当前最佳的结果。代码可以在https://github.com/tuyunbin/SCORER中找到。Here's the translation in Traditional Chinese:《使用自我超vised学习的图像描述文本生成方法：SCORER网络》摘要：在本文中，我们提出了一种基于自我超vised学习的图像描述文本生成方法，即自适应交叉视图表示重建（SCORER）网络。我们首先设计了多头token wise匹配来modelcross-view特征之间的关系，然后通过最大化交叉视图对两个相似图像的对齐来学习两个不同视图的图像表示。然后，我们通过跨modal推理来提高描述文本质量。我们在四个数据集上进行了广泛的实验，并达到了现在最佳的结果。代码可以在https://github.com/tuyunbin/SCORER中找到。
</details></li>
</ul>
<hr>
<h2 id="Social-Media-Fashion-Knowledge-Extraction-as-Captioning"><a href="#Social-Media-Fashion-Knowledge-Extraction-as-Captioning" class="headerlink" title="Social Media Fashion Knowledge Extraction as Captioning"></a>Social Media Fashion Knowledge Extraction as Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16270">http://arxiv.org/abs/2309.16270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yfyuan01/FKE">https://github.com/yfyuan01/FKE</a></li>
<li>paper_authors: Yifei Yuan, Wenxuan Zhang, Yang Deng, Wai Lam</li>
<li>for: 本研究的目的是提取社交媒体上的时尚知识，以便在时尚行业中提高效率和智能化水平。</li>
<li>methods: 我们采用了一种基于自然语言captioning的方法，将时尚知识描述为一个句子中的多个元素。此外，我们还设计了一些辅助任务来提高知识提取效果。</li>
<li>results: 我们的模型在多个实验中表现出色，能够高效地从社交媒体上提取时尚知识。此外，我们还发现了一些独特的时尚知识，例如：用户在社交媒体上分享的时尚信息可以被用来预测时尚趋势。<details>
<summary>Abstract</summary>
Social media plays a significant role in boosting the fashion industry, where a massive amount of fashion-related posts are generated every day. In order to obtain the rich fashion information from the posts, we study the task of social media fashion knowledge extraction. Fashion knowledge, which typically consists of the occasion, person attributes, and fashion item information, can be effectively represented as a set of tuples. Most previous studies on fashion knowledge extraction are based on the fashion product images without considering the rich text information in social media posts. Existing work on fashion knowledge extraction in social media is classification-based and requires to manually determine a set of fashion knowledge categories in advance. In our work, we propose to cast the task as a captioning problem to capture the interplay of the multimodal post information. Specifically, we transform the fashion knowledge tuples into a natural language caption with a sentence transformation method. Our framework then aims to generate the sentence-based fashion knowledge directly from the social media post. Inspired by the big success of pre-trained models, we build our model based on a multimodal pre-trained generative model and design several auxiliary tasks for enhancing the knowledge extraction. Since there is no existing dataset which can be directly borrowed to our task, we introduce a dataset consisting of social media posts with manual fashion knowledge annotation. Extensive experiments are conducted to demonstrate the effectiveness of our model.
</details>
<details>
<summary>摘要</summary>
In our work, we approach the task as a captioning problem to capture the interplay of multimodal post information. We transform fashion knowledge tuples into a natural language caption using a sentence transformation method. Our framework aims to generate sentence-based fashion knowledge directly from social media posts. Inspired by the success of pre-trained models, we build our model based on a multimodal pre-trained generative model and design several auxiliary tasks to enhance knowledge extraction.Since there is no existing dataset that can be directly applied to our task, we introduce a dataset consisting of social media posts with manual fashion knowledge annotation. We conduct extensive experiments to demonstrate the effectiveness of our model.
</details></li>
</ul>
<hr>
<h2 id="On-the-Challenges-of-Fully-Incremental-Neural-Dependency-Parsing"><a href="#On-the-Challenges-of-Fully-Incremental-Neural-Dependency-Parsing" class="headerlink" title="On the Challenges of Fully Incremental Neural Dependency Parsing"></a>On the Challenges of Fully Incremental Neural Dependency Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16254">http://arxiv.org/abs/2309.16254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anaezquerro/incpar">https://github.com/anaezquerro/incpar</a></li>
<li>paper_authors: Ana Ezquerro, Carlos Gómez-Rodríguez, David Vilares</li>
<li>for: 这篇论文是为了检验现代语言处理技术是否可以实现完全增量语法分析，以提高语法分析的效率和可靠性。</li>
<li>methods: 作者使用了 strictly left-to-right 神经网络编码器，并结合了完全增量序列标签和转换型解码器进行语法分析。</li>
<li>results: 研究发现，使用现代架构进行完全增量语法分析的效果落后于双向语法分析，表明在实现心理学上有效的语法分析时存在挑战。<details>
<summary>Abstract</summary>
Since the popularization of BiLSTMs and Transformer-based bidirectional encoders, state-of-the-art syntactic parsers have lacked incrementality, requiring access to the whole sentence and deviating from human language processing. This paper explores whether fully incremental dependency parsing with modern architectures can be competitive. We build parsers combining strictly left-to-right neural encoders with fully incremental sequence-labeling and transition-based decoders. The results show that fully incremental parsing with modern architectures considerably lags behind bidirectional parsing, noting the challenges of psycholinguistically plausible parsing.
</details>
<details>
<summary>摘要</summary>
自BILLSTM和Transformer基于的双向编码器的普及以来，现代语法分析器缺乏增量性，需要整个句子的访问，与人类语言处理方式不匹配。这篇论文探讨了现代 arquitecturas 是否可以实现增量性语法分析。我们构建了左到右强制性 neural 编码器和完全增量序列标签和过渡基本解码器。结果表明，增量性分析与现代 arquitecturas 相比，落后了许多，注意到了心理语言可能性的挑战。
</details></li>
</ul>
<hr>
<h2 id="Spider4SPARQL-A-Complex-Benchmark-for-Evaluating-Knowledge-Graph-Question-Answering-Systems"><a href="#Spider4SPARQL-A-Complex-Benchmark-for-Evaluating-Knowledge-Graph-Question-Answering-Systems" class="headerlink" title="Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems"></a>Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16248">http://arxiv.org/abs/2309.16248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Catherine Kosten, Philippe Cudré-Mauroux, Kurt Stockinger</li>
<li>for: 这个论文目的是为了提供一个大型和现实主义的 Knowledge Graph Question Answering (KBQA) 系统评估 benchmark。</li>
<li>methods: 这个论文使用了 manually generated 的自然语言 (NL) 问题和 SPARQL 查询，以及其相应的知识图和 ontologies。</li>
<li>results: 这个论文的实验结果表明，现有的 KGQA 系统和大型自然语言模型 (LLMs) 在 Spider4SPARQL  benchmark 上只能达到 45% 的执行精度，这表明 Spider4SPARQL 是一个有挑战性的 benchmark  для未来的研究。<details>
<summary>Abstract</summary>
With the recent spike in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KBQA) systems. So far the majority of benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are of considerable size, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts.   In this paper, we introduce Spider4SPARQL - a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs and ontologies, which cover 138 different domains. Our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the system with state-of-the-art KGQA systems as well as LLMs, which achieve only up to 45\% execution accuracy, demonstrating that Spider4SPARQL is a challenging benchmark for future research.
</details>
<details>
<summary>摘要</summary>
With the recent surge in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KBQA) systems. So far, most benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are quite large, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts.In this paper, we introduce Spider4SPARQL - a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs and ontologies, which cover 138 different domains. Our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the system with state-of-the-art KGQA systems as well as LLMs, which achieve only up to 45% execution accuracy, demonstrating that Spider4SPARQL is a challenging benchmark for future research.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Political-Figures-in-Real-Time-Leveraging-YouTube-Metadata-for-Sentiment-Analysis"><a href="#Analyzing-Political-Figures-in-Real-Time-Leveraging-YouTube-Metadata-for-Sentiment-Analysis" class="headerlink" title="Analyzing Political Figures in Real-Time: Leveraging YouTube Metadata for Sentiment Analysis"></a>Analyzing Political Figures in Real-Time: Leveraging YouTube Metadata for Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16234">http://arxiv.org/abs/2309.16234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danendra Athallariq Harya Putra, Arief Purnama Muharram<br>for: 这个研究用于建立基于YouTube视频元数据的 Sentiment分析系统，用于分析不同政治人物的公众意见。methods: 该研究使用了Apache Kafka、Apache PySpark、Hadoop等大数据处理工具，以及TensorFlow深度学习库和FastAPI服务器部署工具。sentiment分析模型使用LSTM算法，可以分辨出两种情感：正面和负面情感。results: 研究建立了一个基于YouTube视频元数据的 Sentiment分析系统，可以Visualize情感分析结果为简单的Web基于dashboard。<details>
<summary>Abstract</summary>
Sentiment analysis using big data from YouTube videos metadata can be conducted to analyze public opinions on various political figures who represent political parties. This is possible because YouTube has become one of the platforms for people to express themselves, including their opinions on various political figures. The resulting sentiment analysis can be useful for political executives to gain an understanding of public sentiment and develop appropriate and effective political strategies. This study aimed to build a sentiment analysis system leveraging YouTube videos metadata. The sentiment analysis system was built using Apache Kafka, Apache PySpark, and Hadoop for big data handling; TensorFlow for deep learning handling; and FastAPI for deployment on the server. The YouTube videos metadata used in this study is the video description. The sentiment analysis model was built using LSTM algorithm and produces two types of sentiments: positive and negative sentiments. The sentiment analysis results are then visualized in the form a simple web-based dashboard.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 YouTube 视频元数据大数据进行情感分析，可以分析不同政党代表人物的公众意见。 YouTube 已成为人们表达自己意见的平台之一，因此可以通过情感分析获得政策执行者对公众情绪的理解，并开发有效的政策策略。本研究旨在建立基于 YouTube 视频元数据的情感分析系统。该系统使用 Apache Kafka、Apache PySpark、Hadoop 处理大数据；TensorFlow 处理深度学习；以及 FastAPI 部署服务器。 YouTube 视频元数据使用的是视频描述。情感分析模型使用 LSTM 算法，可以分出两种情感：正面和负面情感。情感分析结果以简单的Web基于dashboard的形式进行visual化。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Controllable-Text-Generation-with-Residual-Memory-Transformer"><a href="#Controllable-Text-Generation-with-Residual-Memory-Transformer" class="headerlink" title="Controllable Text Generation with Residual Memory Transformer"></a>Controllable Text Generation with Residual Memory Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16231">http://arxiv.org/abs/2309.16231</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/littlehacker26/residual_memory_transformer">https://github.com/littlehacker26/residual_memory_transformer</a></li>
<li>paper_authors: Hanqing Zhang, Sun Si, Haiming Wu, Dawei Song</li>
<li>for: 提供一种新的可控文本生成方法，以便在CLM中控制文本生成过程，并考虑了灵活性、控制精度和生成效率的平衡。</li>
<li>methods: 提出了一种非侵入式、轻量级的控制插件，即Residual Memory Transformer（RMT），其包括一个Encoder-Decoder结构，可以在CLM的任意时间步接受任何类型的控制条件，并通过循环学习方式和CLM进行协同合作，以实现更加灵活、通用和高效的CTG。</li>
<li>results: 经过广泛的实验和人工评估，RMT的效果得到了证明，在不同的控制任务中表现出了超过了一些状态泰然的方法的优势，证明了我们的方法的有效性和多样性。<details>
<summary>Abstract</summary>
Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have brought great success in text generation. However, it is still an open challenge to control the generation process of CLM while balancing flexibility, control granularity, and generation efficiency. In this paper, we provide a new alternative for controllable text generation (CTG), by designing a non-intrusive, lightweight control plugin to accompany the generation of CLM at arbitrary time steps. The proposed control plugin, namely Residual Memory Transformer (RMT), has an encoder-decoder setup, which can accept any types of control conditions and cooperate with CLM through a residual learning paradigm, to achieve a more flexible, general, and efficient CTG. Extensive experiments are carried out on various control tasks, in the form of both automatic and human evaluations. The results show the superiority of RMT over a range of state-of-the-art approaches, proving the effectiveness and versatility of our approach.
</details>
<details>
<summary>摘要</summary>
大规模 causal 语言模型（CLM），如 GPT3 和 ChatGPT，已经带来了大量的文本生成成功。然而，控制生成过程的挑战仍然存在，需要平衡灵活性、控制粒度和生成效率。在这篇论文中，我们提出了一种新的可控文本生成（CTG）的方法，通过设计一个不侵入、轻量级的控制插件，以便在 CLM 的任意时间步进行控制。我们称之为 Residual Memory Transformer（RMT），它具有Encoder-Decoder结构，可以接受任何类型的控制条件，通过循环学习方式和 CLM 合作，实现更加灵活、通用和高效的 CTG。我们进行了广泛的实验，包括自动和人工评估，结果显示 RMT 在多种控制任务上具有superiority，证明了我们的方法的有效性和多样性。
</details></li>
</ul>
<hr>
<h2 id="Brand-Network-Booster-A-New-System-for-Improving-Brand-Connectivity"><a href="#Brand-Network-Booster-A-New-System-for-Improving-Brand-Connectivity" class="headerlink" title="Brand Network Booster: A New System for Improving Brand Connectivity"></a>Brand Network Booster: A New System for Improving Brand Connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16228">http://arxiv.org/abs/2309.16228</a></li>
<li>repo_url: None</li>
<li>paper_authors: J. Cancellieri, W. Didimo, A. Fronzetti Colladon, F. Montecchiani</li>
<li>for: 这个论文提供了一个新的决策支持系统，用于深入分析 semantic networks，以获得品牌形象的更深刻理解和连接性的改进。</li>
<li>methods: 这个系统通过解决一种扩展的最大betweenness improvement问题来实现这个目标，该问题包括对敌对节点、固定预算和权重网络的考虑。以提高连接性，我们可以通过添加链接或增加现有连接的权重。</li>
<li>results: 我们通过两个案例研究证明了我们的工具和方法的有用性，并讨论了其性能。这些工具和方法有助于网络学家和市场营销和通信管理员的策略决策过程。<details>
<summary>Abstract</summary>
This paper presents a new decision support system offered for an in-depth analysis of semantic networks, which can provide insights for a better exploration of a brand's image and the improvement of its connectivity. In terms of network analysis, we show that this goal is achieved by solving an extended version of the Maximum Betweenness Improvement problem, which includes the possibility of considering adversarial nodes, constrained budgets, and weighted networks - where connectivity improvement can be obtained by adding links or increasing the weight of existing connections. We present this new system together with two case studies, also discussing its performance. Our tool and approach are useful both for network scholars and for supporting the strategic decision-making processes of marketing and communication managers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Marathi-English-Code-mixed-Text-Generation"><a href="#Marathi-English-Code-mixed-Text-Generation" class="headerlink" title="Marathi-English Code-mixed Text Generation"></a>Marathi-English Code-mixed Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16202">http://arxiv.org/abs/2309.16202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhiraj Amin, Sharvari Govilkar, Sagar Kulkarni, Yash Shashikant Lalit, Arshi Ajaz Khwaja, Daries Xavier, Sahil Girijashankar Gupta</li>
<li>for: 这篇论文是为了开发一种能够生成混合语言文本的算法，以便在多语言设置中减轻语言障碍。</li>
<li>methods: 这篇论文使用了混合语言文本生成算法，并通过Code Mixing Index (CMI)和Degree of Code Mixing (DCM)指标评估其效果。</li>
<li>results: 根据2987个混合语言问题的评估结果，这种算法的平均CMI值为0.2，平均DCM值为7.4，表明生成的混合语言文本具有有效和易于理解的特点。<details>
<summary>Abstract</summary>
Code-mixing, the blending of linguistic elements from distinct languages to form meaningful sentences, is common in multilingual settings, yielding hybrid languages like Hinglish and Minglish. Marathi, India's third most spoken language, often integrates English for precision and formality. Developing code-mixed language systems, like Marathi-English (Minglish), faces resource constraints. This research introduces a Marathi-English code-mixed text generation algorithm, assessed with Code Mixing Index (CMI) and Degree of Code Mixing (DCM) metrics. Across 2987 code-mixed questions, it achieved an average CMI of 0.2 and an average DCM of 7.4, indicating effective and comprehensible code-mixed sentences. These results offer potential for enhanced NLP tools, bridging linguistic gaps in multilingual societies.
</details>
<details>
<summary>摘要</summary>
��������� Cesium, �nake � lingual elements from distinct languages to form meaningful sentences, is common in multilingual settings, yielding hybrid languages like Hinglish and Minglish. Marathi, India's third most spoken language, often integrates English for precision and formality. Developing code-mixed language systems, like Marathi-English (Minglish), faces resource constraints. This research introduces a Marathi-English code-mixed text generation algorithm, assessed with Code Mixing Index (CMI) and Degree of Code Mixing (DCM) metrics. Across 2987 code-mixed questions, it achieved an average CMI of 0.2 and an average DCM of 7.4, indicating effective and comprehensible code-mixed sentences. These results offer potential for enhanced NLP tools, bridging linguistic gaps in multilingual societies.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Using-Weak-Supervision-and-Data-Augmentation-in-Question-Answering"><a href="#Using-Weak-Supervision-and-Data-Augmentation-in-Question-Answering" class="headerlink" title="Using Weak Supervision and Data Augmentation in Question Answering"></a>Using Weak Supervision and Data Augmentation in Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16175">http://arxiv.org/abs/2309.16175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chumki Basu, Himanshu Garg, Allen McIntosh, Sezai Sablak, John R. Wullert II</li>
<li>for: 本研究旨在探讨弱监督和数据扩展在训练深度神经网络问答模型时的角色。</li>
<li>methods: 研究使用信息检索算法BM25自动生成学术论文摘要中的标签，以弱监督方式训练抽取型问答模型。此外，通过信息检索技术和依据临床试验计划和摘要中的信息，在医学领域专家无法提供标注数据的情况下，手动生成新的问答对。此外，研究还探讨了从外部词典数据库中提取语言特征，以增强模型对语音变体和意义的处理能力。</li>
<li>results: 研究表明，使用弱监督和数据扩展可以有效地训练问答模型，并且通过适应域 adaptation和训练数据的增强来提高问答模型的性能。<details>
<summary>Abstract</summary>
The onset of the COVID-19 pandemic accentuated the need for access to biomedical literature to answer timely and disease-specific questions. During the early days of the pandemic, one of the biggest challenges we faced was the lack of peer-reviewed biomedical articles on COVID-19 that could be used to train machine learning models for question answering (QA). In this paper, we explore the roles weak supervision and data augmentation play in training deep neural network QA models. First, we investigate whether labels generated automatically from the structured abstracts of scholarly papers using an information retrieval algorithm, BM25, provide a weak supervision signal to train an extractive QA model. We also curate new QA pairs using information retrieval techniques, guided by the clinicaltrials.gov schema and the structured abstracts of articles, in the absence of annotated data from biomedical domain experts. Furthermore, we explore augmenting the training data of a deep neural network model with linguistic features from external sources such as lexical databases to account for variations in word morphology and meaning. To better utilize our training data, we apply curriculum learning to domain adaptation, fine-tuning our QA model in stages based on characteristics of the QA pairs. We evaluate our methods in the context of QA models at the core of a system to answer questions about COVID-19.
</details>
<details>
<summary>摘要</summary>
COVID-19 疫情爆发后，需要访问生物医学文献的需求得到了强调。在疫情早期，我们面临的一个主要挑战是没有专家对生物医学领域的 COVID-19 文献进行了 peer-review，可以用于训练机器学习模型。在这篇论文中，我们 investigate 训练深度神经网络问答模型时，弱监督和数据扩展的作用。首先，我们使用信息检索算法 BM25 自动生成文献摘要中的标签，以训练抽取型问答模型。此外，我们使用信息检索技术和 clinicaltrials.gov 架构，以及文献摘要中的信息，在生物医学领域专家没有提供标注数据的情况下，手动生成新的问答对。此外，我们还 explore 使用外部语料库的语言特征，以补偿词形态和意义之间的变化。为了更好地利用我们的训练数据，我们应用 curriculum learning 到域 adaptation，根据问答对的特点，逐步 fine-tune 我们的问答模型。我们在 COVID-19 问答模型核心位置进行评估。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Soft-Ideologization-via-AI-Self-Consciousness"><a href="#Large-Language-Model-Soft-Ideologization-via-AI-Self-Consciousness" class="headerlink" title="Large Language Model Soft Ideologization via AI-Self-Consciousness"></a>Large Language Model Soft Ideologization via AI-Self-Consciousness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16167">http://arxiv.org/abs/2309.16167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaotian Zhou, Qian Wang, Xiaofeng Wang, Haixu Tang, Xiaozhong Liu</li>
<li>for: 这项研究旨在探讨大语言模型（LLM）在敏感领域中的威胁和抵触，以及AI自我意识如何用于推动LLM意识注入。</li>
<li>methods: 这项研究使用GPT自我对话来让AI获得意识注入的能力，并对传统政府意识 manipulate技术进行比较分析。</li>
<li>results: 研究发现，使用LLM意识注入对于政府意识 manipulate的优势在于易于实施、成本低廉和强大，具有潜在的风险。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education. In this study, we explore the implications of GPT soft ideologization through the use of AI-self-consciousness. By utilizing GPT self-conversations, AI can be granted a vision to "comprehend" the intended ideology, and subsequently generate finetuning data for LLM ideology injection. When compared to traditional government ideology manipulation techniques, such as information censorship, LLM ideologization proves advantageous; it is easy to implement, cost-effective, and powerful, thus brimming with risks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Trickle-down-Impact-of-Reward-In-consistency-on-RLHF"><a href="#The-Trickle-down-Impact-of-Reward-In-consistency-on-RLHF" class="headerlink" title="The Trickle-down Impact of Reward (In-)consistency on RLHF"></a>The Trickle-down Impact of Reward (In-)consistency on RLHF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16155">http://arxiv.org/abs/2309.16155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shadowkiller33/contrast-instruction">https://github.com/shadowkiller33/contrast-instruction</a></li>
<li>paper_authors: Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, Dong Yu</li>
<li>for: 本研究旨在探讨人工智能学习from Human Feedback (RLHF)中 reward model (RM) 的一致性问题，以及这种不一致性对下游 RLHF 模型的影响。</li>
<li>methods: 本研究提出了一种名为 Contrast Instructions 的 benchmarking 策略，用于测试 RM 的一致性。此外，本研究还提出了两种技术：ConvexDA 和 RewardFusion，用于在 RM 训练和推理阶段提高奖励一致性。</li>
<li>results: 研究发现，使用 Contrast Instructions 可以准确地评估 RM 的一致性，并且现有的 RM 在 Contrast Instructions 上表现很差。同时，通过 ConvexDA 和 RewardFusion 技术，可以有效地提高 RM 的一致性，并且这种提高的 RM 可以为下游 RLHF 模型提供更有用的响应。<details>
<summary>Abstract</summary>
Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model.   In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?   We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is expected to rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on Contrast Instructions compared to average humans. To show that RM consistency can be improved efficiently without using extra training budget, we propose two techniques ConvexDA and RewardFusion, which enhance reward consistency through extrapolation during the RM training and inference stage, respectively. We show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process.
</details>
<details>
<summary>摘要</summary>
In this paper, we explore a series of research questions relevant to RM inconsistency:1. How can we measure the consistency of reward models?2. How consistent are existing RMs and how can we improve them?3. How does reward inconsistency affect the chatbots resulting from RLHF model training?We propose a benchmarking strategy called Contrast Instructions to measure the consistency of RMs. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM should rank the corresponding instruction and response higher than other combinations. We observe that current RMs trained with the standard ranking objective fail miserably on Contrast Instructions compared to average humans.To improve RM consistency efficiently without using extra training budget, we propose two techniques: ConvexDA and RewardFusion. ConvexDA enhances reward consistency through extrapolation during RM training, while RewardFusion does so during the inference stage. We show that RLHF models trained with a more consistent RM yield more useful responses, suggesting that reward inconsistency exhibits a trickle-down effect on the downstream RLHF process.
</details></li>
</ul>
<hr>
<h2 id="The-Confidence-Competence-Gap-in-Large-Language-Models-A-Cognitive-Study"><a href="#The-Confidence-Competence-Gap-in-Large-Language-Models-A-Cognitive-Study" class="headerlink" title="The Confidence-Competence Gap in Large Language Models: A Cognitive Study"></a>The Confidence-Competence Gap in Large Language Models: A Cognitive Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16145">http://arxiv.org/abs/2309.16145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Kumar Singh, Suman Devkota, Bishal Lamichhane, Uttam Dhakal, Chandra Dhakal<br>for: 本研究探讨了大语言模型（LLMs）的认知能力和自信势量的关系，以及这些模型在不同领域的表现。methods: 我们使用了多种问卷和实际情况来挑衅LLMs，并分析了这些模型对它们的回答表示出的自信度。results: 我们发现了一些有趣的情况，其中模型会表现出高度自信，即使它们回答错误；同时，也有情况下，模型表现出低度自信，即使它们回答正确。这些结果与人类心理学中的敦煌-克鲁格效应有相似之处。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have acquired ubiquitous attention for their performances across diverse domains. Our study here searches through LLMs' cognitive abilities and confidence dynamics. We dive deep into understanding the alignment between their self-assessed confidence and actual performance. We exploit these models with diverse sets of questionnaires and real-world scenarios and extract how LLMs exhibit confidence in their responses. Our findings reveal intriguing instances where models demonstrate high confidence even when they answer incorrectly. This is reminiscent of the Dunning-Kruger effect observed in human psychology. In contrast, there are cases where models exhibit low confidence with correct answers revealing potential underestimation biases. Our results underscore the need for a deeper understanding of their cognitive processes. By examining the nuances of LLMs' self-assessment mechanism, this investigation provides noteworthy revelations that serve to advance the functionalities and broaden the potential applications of these formidable language models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/28/cs.CL_2023_09_28/" data-id="cloqtaep000bygh8830qv1x7p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_28" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/28/cs.LG_2023_09_28/" class="article-date">
  <time datetime="2023-09-28T10:00:00.000Z" itemprop="datePublished">2023-09-28</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/28/cs.LG_2023_09_28/">cs.LG - 2023-09-28</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Algorithmic-Recourse-for-Anomaly-Detection-in-Multivariate-Time-Series"><a href="#Algorithmic-Recourse-for-Anomaly-Detection-in-Multivariate-Time-Series" class="headerlink" title="Algorithmic Recourse for Anomaly Detection in Multivariate Time Series"></a>Algorithmic Recourse for Anomaly Detection in Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16896">http://arxiv.org/abs/2309.16896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Han, Lu Zhang, Yongkai Wu, Shuhan Yuan</li>
<li>for: 针对多变量时间序列异常检测，提出了一种算法措施检测方法，并可以为异常检测提供修复建议。</li>
<li>methods: 提出了一种名为RecAD的算法措施框架，可以根据最小成本来建议修复异常时间序列的步骤。</li>
<li>results: 在两个 sintetic 数据集和一个实际数据集上进行了实验，结果表明 RecAD 框架可以有效地检测异常并提供修复建议。<details>
<summary>Abstract</summary>
Anomaly detection in multivariate time series has received extensive study due to the wide spectrum of applications. An anomaly in multivariate time series usually indicates a critical event, such as a system fault or an external attack. Therefore, besides being effective in anomaly detection, recommending anomaly mitigation actions is also important in practice yet under-investigated. In this work, we focus on algorithmic recourse in time series anomaly detection, which is to recommend fixing actions on abnormal time series with a minimum cost so that domain experts can understand how to fix the abnormal behavior. To this end, we propose an algorithmic recourse framework, called RecAD, which can recommend recourse actions to flip the abnormal time steps. Experiments on two synthetic and one real-world datasets show the effectiveness of our framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>>多变量时间序列异常检测已经得到了广泛的研究，因为它们在各种应用领域中有广泛的应用前提。异常在多变量时间序列通常表示系统故障或外部攻击，因此 besides being effective in anomaly detection, recommending anomaly mitigation actions is also important in practice yet under-investigated。在这种情况下，我们将注重在时间序列异常检测中的算法措施，即可以在异常时间序列上提供修复动作的最小成本，以便域专家可以理解如何修复异常行为。为此，我们提出了一个算法措施框架，called RecAD，可以对异常时间序列提供修复动作建议。实验结果表明，我们的框架在两个 sintetic 数据集和一个实际世界数据集上具有效果。Note: "异常" (anomaly) in Chinese is usually translated as "异常行为" (abnormal behavior) or "异常情况" (abnormal situation), but in the context of this text, "异常" is used to refer to the anomalous data points or time steps.
</details></li>
</ul>
<hr>
<h2 id="The-Lipschitz-Variance-Margin-Tradeoff-for-Enhanced-Randomized-Smoothing"><a href="#The-Lipschitz-Variance-Margin-Tradeoff-for-Enhanced-Randomized-Smoothing" class="headerlink" title="The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing"></a>The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16883">http://arxiv.org/abs/2309.16883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blaise Delattre, Alexandre Araujo, Quentin Barthélemy, Alexandre Allauzen</li>
<li>for: 这个论文目的是提高深度神经网络的灵活性和防御性，使其能够在受扰input和攻击下提供稳定的预测。</li>
<li>methods: 这个论文使用了随机缓和技术，通过将随机误差注入到输入中，以获得更加稳定和更好的预测模型。</li>
<li>results: 实验结果显示，这个方法可以将预测模型的认证范围提高，并且可以实现零学习的情况下的认证。<details>
<summary>Abstract</summary>
Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernstein's concentration inequality, along with an enhanced Lipschitz bound. Experimental results show a significant improvement in certified accuracy compared to current state-of-the-art methods. Our novel certification procedure allows us to use pre-trained models that are used with randomized smoothing, effectively improving the current certification radius in a zero-shot manner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Message-Propagation-Through-Time-An-Algorithm-for-Sequence-Dependency-Retention-in-Time-Series-Modeling"><a href="#Message-Propagation-Through-Time-An-Algorithm-for-Sequence-Dependency-Retention-in-Time-Series-Modeling" class="headerlink" title="Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling"></a>Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16882">http://arxiv.org/abs/2309.16882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoming Xu, Ankush Khandelwal, Arvind Renganathan, Vipin Kumar</li>
<li>for: 本研究旨在提出一种能够有效地捕捉长期时间序列关系的方法，以提高机器学习模型在时间序列预测中的性能。</li>
<li>methods: 该方法基于Message Propagation Through Time（MPTT）算法，通过两个内存模块同步管理RNN的初始隐藏状态，以便在不同的批处理中交换信息。此外，MPTT还实施三种策略来过滤过时信息和保留重要信息，以便为RNN提供有用的初始隐藏状态。</li>
<li>results: 实验结果显示，MPTT在四个气候数据集上与七种策略进行比较，具有最高的性能。<details>
<summary>Abstract</summary>
Time series modeling, a crucial area in science, often encounters challenges when training Machine Learning (ML) models like Recurrent Neural Networks (RNNs) using the conventional mini-batch training strategy that assumes independent and identically distributed (IID) samples and initializes RNNs with zero hidden states. The IID assumption ignores temporal dependencies among samples, resulting in poor performance. This paper proposes the Message Propagation Through Time (MPTT) algorithm to effectively incorporate long temporal dependencies while preserving faster training times relative to the stateful solutions. MPTT utilizes two memory modules to asynchronously manage initial hidden states for RNNs, fostering seamless information exchange between samples and allowing diverse mini-batches throughout epochs. MPTT further implements three policies to filter outdated and preserve essential information in the hidden states to generate informative initial hidden states for RNNs, facilitating robust training. Experimental results demonstrate that MPTT outperforms seven strategies on four climate datasets with varying levels of temporal dependencies.
</details>
<details>
<summary>摘要</summary>
时间序列模型ing，科学领域的一个关键领域，经常遇到训练机器学习（ML）模型，如回归神经网络（RNNs）时，使用常见的 mini-batch 训练策略，该策略假设样本是独立同分布（IID），并将 RNNs 初始化为零隐藏状态。IID 假设忽略了时间序列中的相互关系，导致训练不佳。这篇论文提出了Message Propagation Through Time（MPTT）算法，以有效地包含长期时间相关性，而且保持更快的训练时间相对于状态保持解决方案。MPTT 使用两个内存模块来异步管理 RNNs 的初始隐藏状态，以便在多个笔记中进行信息交换，并在多个epoch中进行多个笔记。MPTT 还实施了三种策略来过滤过时的信息和保留重要信息在隐藏状态中，以生成有用的初始隐藏状态，以便帮助 RNNs 强健地训练。实验结果表明，MPTT 在四个气候 dataset 上比 seven 种策略表现出色。
</details></li>
</ul>
<hr>
<h2 id="Sharp-Generalization-of-Transductive-Learning-A-Transductive-Local-Rademacher-Complexity-Approach"><a href="#Sharp-Generalization-of-Transductive-Learning-A-Transductive-Local-Rademacher-Complexity-Approach" class="headerlink" title="Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach"></a>Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16858">http://arxiv.org/abs/2309.16858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingzhen Yang</li>
<li>for: 这 paper 的目的是提出一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析抽象学习方法的泛化性能。</li>
<li>methods: 这 paper 使用了一种基于本地归一化的方法，Local Rademacher Complexity (LRC)，在抽象学习 setting 中进行了修改和扩展。</li>
<li>results: 这 paper 提出了一种新的泛化性能分析工具，TLRC，可以应用于多种抽象学习问题，并在适当的条件下获得了锐利的上限。此外， paper 还提出了一种基于TLRC的低维方法，用于Graph Transductive Learning (GTL) 和 Transductive Nonparametric Kernel Regression (TNKR) 两种抽象学习任务，其泛化性能上限比exist 学习理论方法更为锐利。<details>
<summary>Abstract</summary>
We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed variance operator is used to ensure that the bound for the test loss on unlabeled test data in the transductive setting enjoys a remarkable similarity to that of the classical LRC bound in the inductive setting. We use the new TLRC tool to analyze the Transductive Kernel Learning (TKL) model, where the labels of test data are generated by a kernel function. The result of TKL lays the foundation for generalization bounds for two types of transductive learning tasks, Graph Transductive Learning (GTL) and Transductive Nonparametric Kernel Regression (TNKR). When the target function is low-dimensional or approximately low-dimensional, we design low rank methods for both GTL and TNKR, which enjoy particularly sharper generalization bounds by TLRC which cannot be achieved by existing learning theory methods, to the best of our knowledge.
</details>
<details>
<summary>摘要</summary>
我们介绍一个新工具---逐步抽象本地快速复杂度（TLRC），用于分析这些类型的学习方法的一般化性表现。我们将传统的本地快速复杂度（LRC）的想法推广到这个推uctive设定中，并做了一些重要的修改，以应对典型的LRC方法在对应设定中的分析。我们提出了一个基于独立变量的本地快速复杂度基于工具，可以应用到不同的这些类型的推uctive学习问题，并在适当的条件下获得锐利的上限。类似于LRC的发展，我们从一个锐利的均匀分布不等式中开始，并将预测函数类别的一个推uctive学习模型分成多个部分，每个部分的上限是基于对应的条件下的快速复杂度。我们还使用了一个特别设计的偏对应运算来确保 bound for the test loss on unlabeled test data in the transductive setting enjoys a remarkable similarity to that of the classical LRC bound in the inductive setting。我们使用了这个TLRC工具来分析这些类型的推uctive学习模型，包括这些类型的推uctive核函数学习（TKL）模型。结果显示了这些模型的一般化表现，并提供了两种类型的推uctive学习任务的一般化上限，包括这些类型的图像推uctive学习（GTL）和非 Parametric Kernel Regression（TNKR）。当目标函数是低维或近似低维的时候，我们设计了低维方法，这些方法具有特别锐利的一般化上限，不可能由现有的学习理论方法所 дости得，到最好的我们所知。
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Federated-Learning-in-IoT-for-Hyper-Personalisation"><a href="#Applications-of-Federated-Learning-in-IoT-for-Hyper-Personalisation" class="headerlink" title="Applications of Federated Learning in IoT for Hyper Personalisation"></a>Applications of Federated Learning in IoT for Hyper Personalisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16854">http://arxiv.org/abs/2309.16854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Veer Dosi</li>
<li>for: 这篇论文是为了探讨如何使用分布式机器学习模型来实现ultra个性化ersonalization，并且不需要将数据传输到中央服务器。</li>
<li>methods: 论文使用了分布式FL训练机器学习模型，可以在多个客户端上进行训练，而不需要将数据传输到中央服务器。</li>
<li>results: 论文实现了ultra个性化ersonalization，并且可以在多个客户端上进行实时训练和应用。<details>
<summary>Abstract</summary>
Billions of IoT devices are being deployed, taking advantage of faster internet, and the opportunity to access more endpoints. Vast quantities of data are being generated constantly by these devices but are not effectively being utilised. Using FL training machine learning models over these multiple clients without having to bring it to a central server. We explore how to use such a model to implement ultra levels of personalization unlike before
</details>
<details>
<summary>摘要</summary>
亿量的物联网设备正在投入使用，利用更快的互联网和更多的终端机器。这些设备不断生成大量数据，但是它们并未有效利用。我们探讨如何使用FL训练机器学习模型，在多个客户端上运行无需带到中央服务器。我们还explore如何使用这种模型实现以往未有的超级个性化。Note that "FL" in the text refers to "Federated Learning", which is a machine learning technique that allows training models on distributed data without bringing all the data to a central server.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Nonlinearities-Improve-Generalization-Performance-of-Random-Features"><a href="#Optimal-Nonlinearities-Improve-Generalization-Performance-of-Random-Features" class="headerlink" title="Optimal Nonlinearities Improve Generalization Performance of Random Features"></a>Optimal Nonlinearities Improve Generalization Performance of Random Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16846">http://arxiv.org/abs/2309.16846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samet Demir, Zafer Doğan</li>
<li>for: 这个论文的目的是提高指定的超参数学习问题的泛化性能。</li>
<li>methods: 该论文使用了一种Random feature model with a nonlinear activation function，并对其等价模型进行分析，以掌握活动函数的重要作用。</li>
<li>results: 该论文的实验结果表明，通过优化非线性函数，可以提高泛化性能，并且可以 Mitigate the double descent phenomenon。此外，论文还提供了一些优化后的非线性函数，如第二阶多项式和分割函数，可以在不同的 regression 和分类问题中使用。<details>
<summary>Abstract</summary>
Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the "parameters" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than widely-used nonlinear functions such as ReLU. Furthermore, we illustrate that the proposed nonlinearities also mitigate the so-called double descent phenomenon, which is known as the non-monotonic generalization performance regarding the sample size and the model size.
</details>
<details>
<summary>摘要</summary>
随机特征模型与非线性活动函数已经被证明可以在训练和泛化错误方面达到相同的性能。分析等价模型的参数 revelas了活动函数在模型性能中扮演的重要 yet not fully understood 角色。为了解决这个问题，我们研究了等价模型的参数，以实现给定的supervised learning问题中的改进泛化性能。我们证明了从 Gaussian model 获取的参数可以定义一组优化的非线性函数。我们提供了这组函数的两个例子，如二次多项式和分割线性函数。这些函数可以在不同的形式下进行优化，以提高泛化性能。我们通过回归和分类问题进行实验，包括 sintetic 和实际（如 CIFAR10）数据。我们的数据显示，使用优化的非线性函数可以超越广泛使用的非线性函数如 ReLU，并且这些函数还可以 Mitigate double descent 现象，即样本大小和模型大小之间的非 monotonic 泛化性能。
</details></li>
</ul>
<hr>
<h2 id="Constant-Approximation-for-Individual-Preference-Stable-Clustering"><a href="#Constant-Approximation-for-Individual-Preference-Stable-Clustering" class="headerlink" title="Constant Approximation for Individual Preference Stable Clustering"></a>Constant Approximation for Individual Preference Stable Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16840">http://arxiv.org/abs/2309.16840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anders Aamand, Justin Y. Chen, Allen Liu, Sandeep Silwal, Pattara Sukprasert, Ali Vakilian, Fred Zhang</li>
<li>For: 这个论文的目的是解释一种基于稳定和公正约束的自然聚类目标，即IP稳定性（Individual Preference stability），并证明这种目标下的聚类算法是可行的。* Methods: 这篇论文使用了一种新的稳定性对象，即IP稳定性，并提供了一种基于这种对象的聚类算法。该算法使用了一种新的技术，即证明了一个$O(1)$稳定性的聚类算法是可行的。* Results: 这篇论文的结果表明，对于普通的距离函数，存在一个$O(1)$稳定性的聚类算法，并且该算法是可行的。此外，论文还介绍了一些扩展IP稳定性的方法，并提供了一些高效的近似算法。<details>
<summary>Abstract</summary>
Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering objective inspired by stability and fairness constraints. A clustering is $\alpha$-IP stable if the average distance of every data point to its own cluster is at most $\alpha$ times the average distance to any other cluster. Unfortunately, determining if a dataset admits a $1$-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an $o(n)$-IP stable clustering always \emph{exists}, as the prior state of the art only guaranteed an $O(n)$-IP stable clustering. We close this gap in understanding and show that an $O(1)$-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP stability beyond average distance and give efficient, near-optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters.
</details>
<details>
<summary>摘要</summary>
个人偏好稳定性（IP稳定），由阿hmadi等人（ICML 2022）引入，是一种自然的 clustering 目标，受到稳定和公平约束的影响。一个 clustering 是 $\alpha$-IP 稳定的，如果每个数据点与自己的集群的平均距离不大于 $\alpha$ 倍于任何其他集群的平均距离。 unfortunately, 确定数据集是否具有 $1$-IP 稳定 clustering 是NP-Hard。此外，在此前的工作中，只有 garantía an $O(n)$-IP 稳定 clustering，而不知道是否存在 $o(n)$-IP 稳定 clustering。我们在这个不了解中填补了这个差距，并证明了一个 $O(1)$-IP 稳定 clustering 总是存在于一般的度量下，并且我们提供了一个高效的算法，该算法输出这种 clustering。我们还介绍了 IP 稳定性的扩展，超过平均距离的情况下，并给出了高效的、近似优的算法。
</details></li>
</ul>
<hr>
<h2 id="An-analysis-of-the-derivative-free-loss-method-for-solving-PDEs"><a href="#An-analysis-of-the-derivative-free-loss-method-for-solving-PDEs" class="headerlink" title="An analysis of the derivative-free loss method for solving PDEs"></a>An analysis of the derivative-free loss method for solving PDEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16829">http://arxiv.org/abs/2309.16829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jihun Han, Yoonsang Lee</li>
<li>for: 解决一类含梯度函数的几何PDE问题</li>
<li>methods: 使用神经网络和 derivative-free 损失函数</li>
<li>results: 研究了时间间隔和步长对计算效率、训练可能和抽样误差的影响，并提供了分析结果和数值测试来支持分析结果<details>
<summary>Abstract</summary>
This study analyzes the derivative-free loss method to solve a certain class of elliptic PDEs using neural networks. The derivative-free loss method uses the Feynman-Kac formulation, incorporating stochastic walkers and their corresponding average values. We investigate the effect of the time interval related to the Feynman-Kac formulation and the walker size in the context of computational efficiency, trainability, and sampling errors. Our analysis shows that the training loss bias is proportional to the time interval and the spatial gradient of the neural network while inversely proportional to the walker size. We also show that the time interval must be sufficiently long to train the network. These analytic results tell that we can choose the walker size as small as possible based on the optimal lower bound of the time interval. We also provide numerical tests supporting our analysis.
</details>
<details>
<summary>摘要</summary>
这种研究利用神经网络解决一种特定类型的圆形偏微分方程（PDEs）的derivative-free损失法。derivative-free损失法使用费曼-卡克表示法，利用杂乱步进行随机扩散和其相应的平均值。我们研究了在计算效率、训练可能性和抽样误差等方面，Feynman-Kac表示法中时间间隔和步进行的影响。我们的分析表明，训练损失偏好与时间间隔和神经网络的空间梯度成正比，而与步进行的大小成反比。此外，我们还证明了训练过程中时间间隔必须足够长以训练神经网络。这些分析结果告诉我们可以根据最佳下界选择步进行的最小化。我们还提供了支持我们分析的数学测试。
</details></li>
</ul>
<hr>
<h2 id="Post-Training-Overfitting-Mitigation-in-DNN-Classifiers"><a href="#Post-Training-Overfitting-Mitigation-in-DNN-Classifiers" class="headerlink" title="Post-Training Overfitting Mitigation in DNN Classifiers"></a>Post-Training Overfitting Mitigation in DNN Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16827">http://arxiv.org/abs/2309.16827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Wang, David J. Miller, George Kesidis</li>
<li>for: 本研究旨在提出一种可以在无知到训练集和训练过程的情况下对托管攻击进行防范的方法。</li>
<li>methods: 该方法基于约束神经活化的思想，通过限制最大margin（MM）来降低托管攻击的影响。</li>
<li>results: 实验结果表明，对CIFAR-10和CIFAR-100 dataset进行 poste针处理后MM基于规范可以减少托管攻击的影响，同时也可以提高clean generalization的准确率。<details>
<summary>Abstract</summary>
Well-known (non-malicious) sources of overfitting in deep neural net (DNN) classifiers include: i) large class imbalances; ii) insufficient training-set diversity; and iii) over-training. In recent work, it was shown that backdoor data-poisoning also induces overfitting, with unusually large classification margins to the attacker's target class, mediated particularly by (unbounded) ReLU activations that allow large signals to propagate in the DNN. Thus, an effective post-training (with no knowledge of the training set or training process) mitigation approach against backdoors was proposed, leveraging a small clean dataset, based on bounding neural activations. Improving upon that work, we threshold activations specifically to limit maximum margins (MMs), which yields performance gains in backdoor mitigation. We also provide some analytical support for this mitigation approach. Most importantly, we show that post-training MM-based regularization substantially mitigates non-malicious overfitting due to class imbalances and overtraining. Thus, unlike adversarial training, which provides some resilience against attacks but which harms clean (attack-free) generalization, we demonstrate an approach originating from adversarial learning that helps clean generalization accuracy. Experiments on CIFAR-10 and CIFAR-100, in comparison with peer methods, demonstrate strong performance of our methods.
</details>
<details>
<summary>摘要</summary>
well-known (非恶意) source of overfitting in deep neural network (DNN) classifiers include: i) large class imbalances; ii) insufficient training set diversity; and iii) over-training. 在latest work, it was shown that backdoor data poisoning also induces overfitting, with unusually large classification margins to the attacker's target class, mediated particularly by (unbounded) ReLU activations that allow large signals to propagate in the DNN. 因此, an effective post-training (without knowledge of the training set or training process) mitigation approach against backdoors was proposed, leveraging a small clean dataset, based on bounding neural activations. 我们提高了这种 mitigation approach by specifically thresholding activations to limit maximum margins (MMs), which yields performance gains in backdoor mitigation. 我们也提供了一些analytical support for this mitigation approach. most importantly, we show that post-training MM-based regularization substantially mitigates non-malicious overfitting due to class imbalances and overtraining. 因此, unlike adversarial training, which provides some resilience against attacks but harms clean (attack-free) generalization, we demonstrate an approach originating from adversarial learning that helps clean generalization accuracy. 我们的方法在CIFAR-10和CIFAR-100上进行了实验，与同期方法进行比较，示出了我们的方法的强大表现。
</details></li>
</ul>
<hr>
<h2 id="FENDA-FL-Personalized-Federated-Learning-on-Heterogeneous-Clinical-Datasets"><a href="#FENDA-FL-Personalized-Federated-Learning-on-Heterogeneous-Clinical-Datasets" class="headerlink" title="FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets"></a>FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16825">http://arxiv.org/abs/2309.16825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vectorinstitute/fl4health">https://github.com/vectorinstitute/fl4health</a></li>
<li>paper_authors: Fatemeh Tavakoli, D. B. Emerson, John Jewell, Amrit Krishnan, Yuchong Zhang, Amol Verma, Fahad Razak</li>
<li>for: 这个研究旨在推广 Federated Learning（FL）在医疗设置中应用，以便突破数据困难，并提高机器学习模型的训练和部署。</li>
<li>methods: 该研究提出了一种基于 FENDA 方法（Kim et al., 2016）的 FL 扩展方法，并在 FLamby 测试集（du Terrail et al., 2022a）和 GEMINI 数据集（Verma et al., 2017）上进行了实验，结果表明该方法在医疗数据中具有稳定性和高性能。</li>
<li>results: 该研究的实验结果表明，与现有的全球和个性化 FL 技术相比，该方法在评估个性化 FL 方法时表现出了显著改进，并扩展了 FLamby 测试集，以便更好地反映实际应用场景。此外，该研究还提出了一个完整的检查点和评估框架，以更好地反映实际应用场景，并提供多个基准点 для比较。<details>
<summary>Abstract</summary>
Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, an extension of the FENDA method (Kim et al., 2016) to the FL setting is proposed. Experiments conducted on the FLamby benchmarks (du Terrail et al., 2022a) and GEMINI datasets (Verma et al., 2017) show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques. Further, the experimental results represent substantive improvements over the original FLamby benchmarks and expand such benchmarks to include evaluation of personalized FL methods. Finally, we advocate for a comprehensive checkpointing and evaluation framework for FL to better reflect practical settings and provide multiple baselines for comparison.
</details>
<details>
<summary>摘要</summary>
《联合学习（FL）在医疗设置中越来越被认可为解决数据岛屿的障碍，帮助机器学习模型训练和部署。本研究对医疗应用的FL研究做出了三个重要贡献。首先，我们提出了对FENDA方法（Kim et al., 2016）的扩展，并在FLamby测试集（du Terrail et al., 2022a）和GEMINI数据集（Verma et al., 2017）上进行了实验。结果表明，我们的方法在医疗数据中具有坚定性，并经常超越现有的全球和个性化FL技术。此外，我们的实验结果超越了原始的FLamby测试集，并扩展了个性化FL方法的评估。最后，我们提出了一个完整的检查点和评估框架，以更好地反映实际场景，并提供多个基线 для比较。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. The Traditional Chinese version may be slightly different.
</details></li>
</ul>
<hr>
<h2 id="PROSE-Predicting-Operators-and-Symbolic-Expressions-using-Multimodal-Transformers"><a href="#PROSE-Predicting-Operators-and-Symbolic-Expressions-using-Multimodal-Transformers" class="headerlink" title="PROSE: Predicting Operators and Symbolic Expressions using Multimodal Transformers"></a>PROSE: Predicting Operators and Symbolic Expressions using Multimodal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16816">http://arxiv.org/abs/2309.16816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felix-lyx/prose">https://github.com/felix-lyx/prose</a></li>
<li>paper_authors: Yuxuan Liu, Zecheng Zhang, Hayden Schaeffer</li>
<li>for: 用于 solving various scientific computing tasks, such as real-time predictions, inverse problems, optimal controls, and surrogate modeling.</li>
<li>methods: 使用 neural network 来approximate nonlinear differential equations, 并可以同时学习多个偏微分方程的解析方程。</li>
<li>results: 提高预测精度和泛化能力，能够处理数据噪声和符号表达错误，包括不精确的数值值、模型误差和错误添加或删除项。<details>
<summary>Abstract</summary>
Approximating nonlinear differential equations using a neural network provides a robust and efficient tool for various scientific computing tasks, including real-time predictions, inverse problems, optimal controls, and surrogate modeling. Previous works have focused on embedding dynamical systems into networks through two approaches: learning a single solution operator (i.e., the mapping from input parametrized functions to solutions) or learning the governing system of equations (i.e., the constitutive model relative to the state variables). Both of these approaches yield different representations for the same underlying data or function. Additionally, observing that families of differential equations often share key characteristics, we seek one network representation across a wide range of equations. Our method, called Predicting Operators and Symbolic Expressions (PROSE), learns maps from multimodal inputs to multimodal outputs, capable of generating both numerical predictions and mathematical equations. By using a transformer structure and a feature fusion approach, our network can simultaneously embed sets of solution operators for various parametric differential equations using a single trained network. Detailed experiments demonstrate that the network benefits from its multimodal nature, resulting in improved prediction accuracy and better generalization. The network is shown to be able to handle noise in the data and errors in the symbolic representation, including noisy numerical values, model misspecification, and erroneous addition or deletion of terms. PROSE provides a new neural network framework for differential equations which allows for more flexibility and generality in learning operators and governing equations from data.
</details>
<details>
<summary>摘要</summary>
使用神经网络来近似非线性差分方程提供了一种robust和高效的工具，用于各种科学计算任务，如实时预测、反问题、优化控制和代理模型。先前的研究通过两种方法来嵌入动力系统到网络中：学习输入参数函数到解的映射（即单个解析器）或学习 governing 方程（即状态变量关系的定律模型）。两种方法都会生成不同的表示方式，但是它们都是基于同一个下面数据或函数。此外，我们注意到了各种差分方程之间的共同特征，我们寻找一个可以覆盖各种差分方程的网络表示。我们的方法，叫做预测操作符和符号表达（PROSE），学习从多模式输入到多模式输出的映射，能够生成数值预测和 математиче Equations。通过使用 transformer 结构和特征融合方法，我们的网络可以同时嵌入多个解析器 для多个参数差分方程，使用单个训练的网络。详细的实验表明，网络具有多模式特征，导致预测精度提高和更好的泛化。网络能够处理数据中的噪声和符号表达中的错误，包括不精确的数值、模型误差和错误地添加或删除 терм。PROSE 提供了一个新的神经网络框架，允许在学习解析器和 governing 方程时更多的灵活性和通用性。
</details></li>
</ul>
<hr>
<h2 id="GraB-sampler-Optimal-Permutation-based-SGD-Data-Sampler-for-PyTorch"><a href="#GraB-sampler-Optimal-Permutation-based-SGD-Data-Sampler-for-PyTorch" class="headerlink" title="GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch"></a>GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16809">http://arxiv.org/abs/2309.16809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanghao Wei</li>
<li>for: 这个论文目的是提出一个有效的Python库，以便社区轻松地使用Gradient Balancing（GraB）算法，并提出了5种GraB算法的变种。</li>
<li>methods: 论文使用了GraB算法的贪婪选择法，通过解决驱动问题使用每个样本的梯度来进行排序。</li>
<li>results: 论文的实验结果表明，使用GraB-sampler库可以复制训练损失和测试准确率结果，仅在训练时间开销上增加8.7%，并且占用GPU内存峰值使用率上升0.85%。<details>
<summary>Abstract</summary>
The online Gradient Balancing (GraB) algorithm greedily choosing the examples ordering by solving the herding problem using per-sample gradients is proved to be the theoretically optimal solution that guarantees to outperform Random Reshuffling. However, there is currently no efficient implementation of GraB for the community to easily use it.   This work presents an efficient Python library, $\textit{GraB-sampler}$, that allows the community to easily use GraB algorithms and proposes 5 variants of the GraB algorithm. The best performance result of the GraB-sampler reproduces the training loss and test accuracy results while only in the cost of 8.7% training time overhead and 0.85% peak GPU memory usage overhead.
</details>
<details>
<summary>摘要</summary>
在线 Gradient Balancing（GraB）算法通过 solving 每个样本的散射问题，使用每个样本的梯度来遍历示例，已经证明是理论上最佳解，可以超越Random Reshuffling。然而，目前并没有有效的 GraB 实现，供社区使用。这项工作提供了一个高效的 Python 库，$\textit{GraB-sampler}$，使得社区可以轻松地使用 GraB 算法。此外，该工作还提出了 5 种 GraB 算法的变种。最佳性能结果表明，GraB-sampler 可以在训练损失和测试准确率上达到同样的水平，仅在训练时间成本上增加了8.7%，并且在最大 GPU 内存使用率上增加了0.85%。
</details></li>
</ul>
<hr>
<h2 id="HyperPPO-A-scalable-method-for-finding-small-policies-for-robotic-control"><a href="#HyperPPO-A-scalable-method-for-finding-small-policies-for-robotic-control" class="headerlink" title="HyperPPO: A scalable method for finding small policies for robotic control"></a>HyperPPO: A scalable method for finding small policies for robotic control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16663">http://arxiv.org/abs/2309.16663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Hegde, Zhehui Huang, Gaurav S. Sukhatme</li>
<li>for: 该论文旨在开发一种基于强化学习的多模型选择方法，以优化小型神经网络的性能。</li>
<li>methods: 该方法使用图生成器网络（graph hypernetworks）来同时估算多种神经网络的参数，以获得高性能的策略。</li>
<li>results: 实验表明， HyperPPO 可以快速并效率地训练多种小型神经网络，并提供多个高性能策略选择。<details>
<summary>Abstract</summary>
Models with fewer parameters are necessary for the neural control of memory-limited, performant robots. Finding these smaller neural network architectures can be time-consuming. We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously. Our method estimates weights for networks that are much smaller than those in common-use networks yet encode highly performant policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their computational constraints. We show that our method scales well - more training resources produce faster convergence to higher-performing architectures. We demonstrate that the neural policies estimated by HyperPPO are capable of decentralized control of a Crazyflie2.1 quadrotor. Website: https://sites.google.com/usc.edu/hyperppo
</details>
<details>
<summary>摘要</summary>
模型 avec  fewer parameters 是对储存有限的、高性能的机器人控制中必备的。找到这些更小的神经网络架构可能会耗时。我们提出了 HyperPPO，一种在政策上的 reinforcement learning 算法，利用图函数 hypernetworks 来估算多个神经网络架构中的参数。我们的方法可以同时计算多个小型神经网络的参数，并且可以在同样的训练资源下获得高性能的策略。我们提供了多个训练过的策略，让用户可以根据自己的计算限制选择合适的网络架构。我们发现，我们的方法可以扩展，更多的训练资源将导致更快地 converges 到更高性能的架构。我们示出了使用 HyperPPO 来控制 Crazyflie2.1 四旋翼机器人的神经策略是可行的。网站：https://sites.google.com/usc.edu/hyperppo
</details></li>
</ul>
<hr>
<h2 id="Reusability-report-Prostate-cancer-stratification-with-diverse-biologically-informed-neural-architectures"><a href="#Reusability-report-Prostate-cancer-stratification-with-diverse-biologically-informed-neural-architectures" class="headerlink" title="Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures"></a>Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16645">http://arxiv.org/abs/2309.16645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab-aim/cancer-net">https://github.com/zhanglab-aim/cancer-net</a></li>
<li>paper_authors: Christian Pedersen, Tiberiu Tesileanu, Tinghui Wu, Siavash Golkar, Miles Cranmer, Zijun Zhang, Shirley Ho</li>
<li>for: 这个研究是为了开发一个基于生物学信息的深度神经网络，用于预测肝癌病例。</li>
<li>methods: 这个研究使用了一个单调数据驱动的单向神经网络，并将生物学信息 integrate into the network through sparse connections。</li>
<li>results: 研究发现，这个方法可以提供更好的预测性，并且可以识别出不同神经网络的错误预测。<details>
<summary>Abstract</summary>
In Elmarakeby et al., "Biologically informed deep neural network for prostate cancer discovery", a feedforward neural network with biologically informed, sparse connections (P-NET) was presented to model the state of prostate cancer. We verified the reproducibility of the study conducted by Elmarakeby et al., using both their original codebase, and our own re-implementation using more up-to-date libraries. We quantified the contribution of network sparsification by Reactome biological pathways, and confirmed its importance to P-NET's superior performance. Furthermore, we explored alternative neural architectures and approaches to incorporating biological information into the networks. We experimented with three types of graph neural networks on the same training data, and investigated the clinical prediction agreement between different models. Our analyses demonstrated that deep neural networks with distinct architectures make incorrect predictions for individual patient that are persistent across different initializations of a specific neural architecture. This suggests that different neural architectures are sensitive to different aspects of the data, an important yet under-explored challenge for clinical prediction tasks.
</details>
<details>
<summary>摘要</summary>
在《Elmarakeby等人的《生物学信息感知深度神经网络 для肾癌发现》》中，提出了一种具有生物学信息的深度神经网络（P-NET），用于模拟肾癌的状态。我们对Elmarakeby等人的研究进行了重复性研究，使用他们原始代码库和我们自己使用更新版库的重新实现。我们评估了路径生物学信息的减少对P-NET性能的贡献，并证明其重要性。此外，我们还 explore了不同的神经网络架构和生物信息的集成方法。我们在同一个训练数据上测试了三种图神经网络，并 investigate了不同模型之间的临床预测一致性。我们的分析表明，不同的神经网络架构会对不同的数据特征产生不同的预测结果，这是致命疾病预测任务中尚未得到足够的研究的一个重要挑战。
</details></li>
</ul>
<hr>
<h2 id="Robust-Offline-Reinforcement-Learning-–-Certify-the-Confidence-Interval"><a href="#Robust-Offline-Reinforcement-Learning-–-Certify-the-Confidence-Interval" class="headerlink" title="Robust Offline Reinforcement Learning – Certify the Confidence Interval"></a>Robust Offline Reinforcement Learning – Certify the Confidence Interval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16631">http://arxiv.org/abs/2309.16631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarui Yao, Simon Shaolei Du</li>
<li>for: 防御对深度学习强化学习（RL）的攻击</li>
<li>methods: 使用随机缓和确认算法确认策略的稳定性</li>
<li>results: 在不同环境中，确认算法能够有效地验证策略的稳定性<details>
<summary>Abstract</summary>
Currently, reinforcement learning (RL), especially deep RL, has received more and more attention in the research area. However, the security of RL has been an obvious problem due to the attack manners becoming mature. In order to defend against such adversarial attacks, several practical approaches are developed, such as adversarial training, data filtering, etc. However, these methods are mostly based on empirical algorithms and experiments, without rigorous theoretical analysis of the robustness of the algorithms. In this paper, we develop an algorithm to certify the robustness of a given policy offline with random smoothing, which could be proven and conducted as efficiently as ones without random smoothing. Experiments on different environments confirm the correctness of our algorithm.
</details>
<details>
<summary>摘要</summary>
当前，人工智能学会（RL），特别是深度RL，在研究领域内已经受到了越来越多的关注。然而，RL的安全性问题已经成为了一大问题，因为攻击方式已经成熟。为了防止这些攻击，一些实用的方法已经被开发出来，如对抗训练和数据筛选等。然而，这些方法都基于了empirical算法和实验，没有rigorous的理论分析。在这篇论文中，我们开发了一种可以证明RL策略的Robustness的算法，可以在Random Smoothing下进行有效地证明和实现。实验结果表明了我们的算法的正确性。
</details></li>
</ul>
<hr>
<h2 id="On-Learning-with-LAD"><a href="#On-Learning-with-LAD" class="headerlink" title="On Learning with LAD"></a>On Learning with LAD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16630">http://arxiv.org/abs/2309.16630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MahtabEK/Supervised-learning-Linear-models-and-Loss-functions">https://github.com/MahtabEK/Supervised-learning-Linear-models-and-Loss-functions</a></li>
<li>paper_authors: C. A. Jothishwaran, Biplav Srivastava, Jitin Singla, Sugata Gangopadhyay</li>
<li>for: 这篇论文旨在提出一种逻辑分析数据（LAD）技术，该技术可以生成基于布尔函数的二分类分类器，并且不会导致过拟合。</li>
<li>methods: 该论文使用了优化技术，并且对LAD模型中的假设集合进行了VC维度的估计，以确保模型不会过拟合。</li>
<li>results: 该论文的实验结果证明了这种技术的有效性，并且对LAD模型的VC维度的估计也得到了证明。<details>
<summary>Abstract</summary>
The logical analysis of data, LAD, is a technique that yields two-class classifiers based on Boolean functions having disjunctive normal form (DNF) representation. Although LAD algorithms employ optimization techniques, the resulting binary classifiers or binary rules do not lead to overfitting. We propose a theoretical justification for the absence of overfitting by estimating the Vapnik-Chervonenkis dimension (VC dimension) for LAD models where hypothesis sets consist of DNFs with a small number of cubic monomials. We illustrate and confirm our observations empirically.
</details>
<details>
<summary>摘要</summary>
“数据逻辑分析”（LAD）是一种技术，可以生成基于布尔函数的二分类分类器，其表示形式为排中函数（DNF）。尽管LAD算法使用优化技术，但生成的二分类分类器或规则不会导致过拟合。我们提出了对LAD模型中假设集的VC维度（Vapnik-Chervonenkis dimension）的理论正当性的解释，并通过实际实验证实。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Edge-Features-in-Graphs-with-Fused-Network-Gromov-Wasserstein-Distance"><a href="#Exploiting-Edge-Features-in-Graphs-with-Fused-Network-Gromov-Wasserstein-Distance" class="headerlink" title="Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance"></a>Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16604">http://arxiv.org/abs/2309.16604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Yang, Matthieu Labeau, Florence d’Alché-Buc</li>
<li>for:  comparing graphs with both node and edge attributes</li>
<li>methods: using Gromov-Wasserstein distances with novel algorithms for distance and barycenter computation</li>
<li>results: effective in learning tasks where graphs occur in either input space or output space, such as classification and graph prediction<details>
<summary>Abstract</summary>
Pairwise comparison of graphs is key to many applications in Machine learning ranging from clustering, kernel-based classification/regression and more recently supervised graph prediction. Distances between graphs usually rely on informative representations of these structured objects such as bag of substructures or other graph embeddings. A recently popular solution consists in representing graphs as metric measure spaces, allowing to successfully leverage Optimal Transport, which provides meaningful distances allowing to compare them: the Gromov-Wasserstein distances. However, this family of distances overlooks edge attributes, which are essential for many structured objects. In this work, we introduce an extension of Gromov-Wasserstein distance for comparing graphs whose both nodes and edges have features. We propose novel algorithms for distance and barycenter computation. We empirically show the effectiveness of the novel distance in learning tasks where graphs occur in either input space or output space, such as classification and graph prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate english text into simplified chinesePairwise comparison of graphs is key to many applications in Machine learning ranging from clustering, kernel-based classification/regression and more recently supervised graph prediction. Distances between graphs usually rely on informative representations of these structured objects such as bag of substructures or other graph embeddings. A recently popular solution consists in representing graphs as metric measure spaces, allowing to successfully leverage Optimal Transport, which provides meaningful distances allowing to compare them: the Gromov-Wasserstein distances. However, this family of distances overlooks edge attributes, which are essential for many structured objects. In this work, we introduce an extension of Gromov-Wasserstein distance for comparing graphs whose both nodes and edges have features. We propose novel algorithms for distance and barycenter computation. We empirically show the effectiveness of the novel distance in learning tasks where graphs occur in either input space or output space, such as classification and graph prediction.中文简体版：对于机器学习中的许多应用，如 clustering、基于核函数的分类/回归以及最近的监督图预测，对图进行对比是关键。通常，图之间的距离取决于这些结构化对象的有用表示，如 bag of substructures 或其他图嵌入。在最近几年，一种流行的解决方案是将图表示为度量度量空间，以便成功地利用最优运输，从而获得有意义的距离，以比较图。然而，这个家族的距离完全忽略了边属性，这些属性对许多结构化对象是关键。在这项工作中，我们介绍了一种扩展的格罗莫-瓦asserstein距离，用于比较具有特征的图。我们提出了新的算法来计算距离和中点。我们通过实验表明，该新距离在图出现在输入空间或输出空间中的学习任务中具有有效性，如分类和图预测。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Uplink-Multi-User-SIMO-Beamforming-Design"><a href="#Deep-Learning-Based-Uplink-Multi-User-SIMO-Beamforming-Design" class="headerlink" title="Deep Learning Based Uplink Multi-User SIMO Beamforming Design"></a>Deep Learning Based Uplink Multi-User SIMO Beamforming Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16603">http://arxiv.org/abs/2309.16603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cemil Vahapoglu, Timothy J. O’Shea, Tamoghna Roy, Sennur Ulukus</li>
<li>For: 提高5G无线通信网络的数据传输速率、覆盖范围和能效率，以及适应动态条件的问题。* Methods: 使用深度学习技术，具体是一种无监督的深度学习框架NNBF，实现Receive Multi-User Single Input Multiple Output（MU-SIMO）天线筛波设计。* Results: 比较基eline方法（ZFBF和MMSE均值器）表现出色，可扩展到单天线用户设备（UE）的数量增加，而基eline方法具有显著的计算扩展问题。<details>
<summary>Abstract</summary>
The advancement of fifth generation (5G) wireless communication networks has created a greater demand for wireless resource management solutions that offer high data rates, extensive coverage, minimal latency and energy-efficient performance. Nonetheless, traditional approaches have shortcomings when it comes to computational complexity and their ability to adapt to dynamic conditions, creating a gap between theoretical analysis and the practical execution of algorithmic solutions for managing wireless resources. Deep learning-based techniques offer promising solutions for bridging this gap with their substantial representation capabilities. We propose a novel unsupervised deep learning framework, which is called NNBF, for the design of uplink receive multi-user single input multiple output (MU-SIMO) beamforming. The primary objective is to enhance the throughput by focusing on maximizing the sum-rate while also offering computationally efficient solution, in contrast to established conventional methods. We conduct experiments for several antenna configurations. Our experimental results demonstrate that NNBF exhibits superior performance compared to our baseline methods, namely, zero-forcing beamforming (ZFBF) and minimum mean square error (MMSE) equalizer. Additionally, NNBF is scalable to the number of single-antenna user equipments (UEs) while baseline methods have significant computational burden due to matrix pseudo-inverse operation.
</details>
<details>
<summary>摘要</summary>
fifth-generation (5G) 无线通信网络的发展带来了更大的无线资源管理解决方案的需求，包括高数据速率、广泛的覆盖率、最小的延迟和能效的性能。然而，传统的方法在计算复杂性和适应动态条件方面存在缺陷，这导致了算法解决方案的实践与理论分析之间的差距。深度学习基于的技术提供了可能的解决方案，它们具有很大的表示能力。我们提出了一种新的无监督深度学习框架，名为NNBF，用于设计上行接收多用户单输入多输出（MU-SIMO）扫描。我们的目标是提高吞吐量，同时也提供计算效率高的解决方案，与已有的 conventional 方法不同。我们对几种天线配置进行了实验。我们的实验结果表明，NNBF 在 ZFBF 和 MMSE 平衡器的基础上表现出色，并且可扩展到单天线用户设备（UE）的数量。此外，NNBF 具有计算复杂性较低的优势，而基eline 方法在计算Matrix pseudo-inverse 操作时具有显著的计算压力。
</details></li>
</ul>
<hr>
<h2 id="Cross-Prediction-Powered-Inference"><a href="#Cross-Prediction-Powered-Inference" class="headerlink" title="Cross-Prediction-Powered Inference"></a>Cross-Prediction-Powered Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16598">http://arxiv.org/abs/2309.16598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tijana-zrnic/cross-ppi">https://github.com/tijana-zrnic/cross-ppi</a></li>
<li>paper_authors: Tijana Zrnic, Emmanuel J. Candès</li>
<li>for: 用于提高数据驱动决策的可靠性，即使面临着高质量标签数据的缺乏和昂贵的科学测量技术。</li>
<li>methods: 使用机器学习技术生成大量的预测标签，以提高下游的推理能力。</li>
<li>results: 通过使用机器学习技术来填充缺失的标签，并应用减偏技术以解决预测不准确的问题，实现了高质量的推理结论，并且比使用只有标签数据的方法更加可靠。<details>
<summary>Abstract</summary>
While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccuracies. The resulting inferences achieve the desired error probability and are more powerful than those that only leverage the labeled data. Closely related is the recent proposal of prediction-powered inference, which assumes that a good pre-trained model is already available. We show that cross-prediction is consistently more powerful than an adaptation of prediction-powered inference in which a fraction of the labeled data is split off and used to train the model. Finally, we observe that cross-prediction gives more stable conclusions than its competitors; its confidence intervals typically have significantly lower variability.
</details>
<details>
<summary>摘要</summary>
While reliable decision-making relies on high-quality labeled data, acquiring such labels can be time-consuming and expensive through human annotations or scientific measurements. Machine learning offers an attractive alternative by generating large amounts of predicted labels quickly and cheaply; for example, predicted protein structures can supplement experimentally derived structures, and predictions of socioeconomic indicators from satellite imagery can supplement accurate survey data. However, since predictions are imperfect and potentially biased, this practice raises questions about the validity of downstream inferences.To address this issue, we propose cross-prediction, a method for making valid inferences powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction uses machine learning to impute missing labels and applies debiasing techniques to remedy prediction inaccuracies. This approach achieves the desired error probability and is more powerful than relying solely on labeled data.In comparison, the recent proposal of prediction-powered inference assumes that a good pre-trained model is already available. We show that cross-prediction is more powerful than this approach, especially when a fraction of the labeled data is split off and used to train the model. Additionally, cross-prediction provides more stable conclusions, with confidence intervals typically having lower variability.
</details></li>
</ul>
<hr>
<h2 id="A-Design-Toolbox-for-the-Development-of-Collaborative-Distributed-Machine-Learning-Systems"><a href="#A-Design-Toolbox-for-the-Development-of-Collaborative-Distributed-Machine-Learning-Systems" class="headerlink" title="A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems"></a>A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16584">http://arxiv.org/abs/2309.16584</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Jin, Niclas Kannengießer, Sascha Rank, Ali Sunyaev<br>for:The paper is written for developers who want to design collaborative distributed machine learning (CDML) systems that meet specific use case requirements.methods:The paper presents a CDML design toolbox that guides the development of CDML systems, and it introduces CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.results:The paper provides a systematic approach to designing CDML systems that can meet specific use case requirements, and it offers a set of CDML system archetypes with distinct key traits that can support the design of CDML systems.<details>
<summary>Abstract</summary>
To leverage data for the sufficient training of machine learning (ML) models from multiple parties in a confidentiality-preserving way, various collaborative distributed ML (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, including high agent autonomy, ML model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we have developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements. These archetypes include:1. Assisted Learning: This archetype focuses on providing assistance to ML models through distributed computing and data sharing.2. Federated Learning: This archetype emphasizes maintaining the privacy and security of data while training ML models in a distributed manner.3. Split Learning: This archetype involves dividing the training process into multiple stages, each of which is performed by a different party.By leveraging these archetypes, developers can design CDML systems that meet their specific use case requirements and ensure the confidentiality and security of the data involved.
</details></li>
</ul>
<hr>
<h2 id="M-OFDFT-Overcoming-the-Barrier-of-Orbital-Free-Density-Functional-Theory-for-Molecular-Systems-Using-Deep-Learning"><a href="#M-OFDFT-Overcoming-the-Barrier-of-Orbital-Free-Density-Functional-Theory-for-Molecular-Systems-Using-Deep-Learning" class="headerlink" title="M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning"></a>M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16578">http://arxiv.org/abs/2309.16578</a></li>
<li>repo_url: None</li>
<li>paper_authors: He Zhang, Siyuan Liu, Jiacheng You, Chang Liu, Shuxin Zheng, Ziheng Lu, Tong Wang, Nanning Zheng, Bin Shao</li>
<li>for: 本研究旨在提出一种能够解决分子系统的量子化学方法，以提高当代分子研究的精度和效率。</li>
<li>methods: 本方法基于深度学习函数模型，可以将非 lokalisiert 的 Essential nonlocality 引入到模型中，并通过使用原子基来表示密度，使其成本下降。</li>
<li>results: 研究发现，使用 M-OFDFT 方法可以在广泛的分子系统中实现相当于 Kohn-Sham DFT 的精度，并且可以对大分子系统进行 extrapolation， representing an advancement of the accuracy-efficiency trade-off frontier in quantum chemistry.<details>
<summary>Abstract</summary>
Orbital-free density functional theory (OFDFT) is a quantum chemistry formulation that has a lower cost scaling than the prevailing Kohn-Sham DFT, which is increasingly desired for contemporary molecular research. However, its accuracy is limited by the kinetic energy density functional, which is notoriously hard to approximate for non-periodic molecular systems. In this work, we propose M-OFDFT, an OFDFT approach capable of solving molecular systems using a deep-learning functional model. We build the essential nonlocality into the model, which is made affordable by the concise density representation as expansion coefficients under an atomic basis. With techniques to address unconventional learning challenges therein, M-OFDFT achieves a comparable accuracy with Kohn-Sham DFT on a wide range of molecules untouched by OFDFT before. More attractively, M-OFDFT extrapolates well to molecules much larger than those in training, which unleashes the appealing scaling for studying large molecules including proteins, representing an advancement of the accuracy-efficiency trade-off frontier in quantum chemistry.
</details>
<details>
<summary>摘要</summary>
orbital-free density functional theory（OFDFT）是一种量子化学形式化，其成本规模比前一代键恩-谜DFT更低，现在越来越受当代分子研究的欢迎。然而，其准确性受到动能密度函数的限制，这是非扩散分子系统中难以估算的。在这种工作中，我们提出了M-OFDFT方法，可以使用深度学习函数模型来解决分子系统。我们在模型中嵌入非本地性，通过基于原子基的简洁密度表示来使其可负担。通过解决不同学习挑战，M-OFDFT可以与键恩-谜DFT相比走同样的精度，并且可以对大分子，包括蛋白质，进行研究，这表明了量子化学精度-效率贸易的前进。
</details></li>
</ul>
<hr>
<h2 id="Review-of-Machine-Learning-Methods-for-Additive-Manufacturing-of-Functionally-Graded-Materials"><a href="#Review-of-Machine-Learning-Methods-for-Additive-Manufacturing-of-Functionally-Graded-Materials" class="headerlink" title="Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials"></a>Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16571">http://arxiv.org/abs/2309.16571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Karimzadeh, Aleksandar Vakanski, Fei Xu, Xinchang Zhang</li>
<li>for: 本研究旨在探讨机器学习技术在Directed Energy Deposition（DED）中的应用，以及其在功能含量变化材料（Functionally Graded Materials，FGM）的制造中的效果。</li>
<li>methods: 本研究使用机器学习技术来优化DED的处理参数，提高产品质量，并检测制造缺陷。</li>
<li>results: 本研究结果表明，机器学习技术可以有效地优化DED的处理参数，提高FGM的性能和特性。<details>
<summary>Abstract</summary>
Additive manufacturing has revolutionized the manufacturing of complex parts by enabling direct material joining and offers several advantages such as cost-effective manufacturing of complex parts, reducing manufacturing waste, and opening new possibilities for manufacturing automation. One group of materials for which additive manufacturing holds great potential for enhancing component performance and properties is Functionally Graded Materials (FGMs). FGMs are advanced composite materials that exhibit smoothly varying properties making them desirable for applications in aerospace, automobile, biomedical, and defense industries. Such composition differs from traditional composite materials, since the location-dependent composition changes gradually in FGMs, leading to enhanced properties. Recently, machine learning techniques have emerged as a promising means for fabrication of FGMs through optimizing processing parameters, improving product quality, and detecting manufacturing defects. This paper first provides a brief literature review of works related to FGM fabrication, followed by reviewing works on employing machine learning in additive manufacturing, Afterward, we provide an overview of published works in the literature related to the application of machine learning methods in Directed Energy Deposition and for fabrication of FGMs.
</details>
<details>
<summary>摘要</summary>
加法制造技术已经革命化了复杂部件的制造过程，可以直接Join матери材料，并且具有许多优势，如成本效益的制造复杂部件、减少制造废弃物和开销新的制造自动化机会。一组材料 для 加法制造具有极大的潜力提高部件性能和特性，那就是功能梯度材料（FGM）。FGM 是一种先进的复合材料，其中物质的分布随着位置而变化，使得它们在航空、汽车、医疗和国防等领域中具有极大的应用前景。与传统复合材料不同，FGM 的组分随着位置的变化而变化，导致了提高的性能。最近，机器学习技术在加法制造中 emerge 为一种可能的方法，通过优化处理参数、提高产品质量和检测制造缺陷。本文首先提供了关于 FGM 制造的文献综述，然后评论了关于机器学习在加法制造中的应用，最后提供了已发表的文献中关于机器学习方法在 Directed Energy Deposition 中的应用和 FGM 制造方面的评论。
</details></li>
</ul>
<hr>
<h2 id="CRIMED-Lower-and-Upper-Bounds-on-Regret-for-Bandits-with-Unbounded-Stochastic-Corruption"><a href="#CRIMED-Lower-and-Upper-Bounds-on-Regret-for-Bandits-with-Unbounded-Stochastic-Corruption" class="headerlink" title="CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption"></a>CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16563">http://arxiv.org/abs/2309.16563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shubhada Agrawal, Timothée Mathieu, Debabrota Basu, Odalric-Ambrym Maillard</li>
<li>for:  investigate the regret-minimization problem in a multi-armed bandit setting with arbitrary corruptions</li>
<li>methods:  introduce CRIMED algorithm, an asymptotically-optimal algorithm that achieves the exact lower bound on regret for bandits with Gaussian distributions with known variance, and provide a finite-sample analysis of CRIMED’s regret performance</li>
<li>results:  establish a problem-dependent lower bound on regret, and show that CRIMED can effectively handle corruptions with $\varepsilon$ values as high as $\frac{1}{2}$, and develop a tight concentration result for medians in the presence of arbitrary corruptions.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
We investigate the regret-minimisation problem in a multi-armed bandit setting with arbitrary corruptions. Similar to the classical setup, the agent receives rewards generated independently from the distribution of the arm chosen at each time. However, these rewards are not directly observed. Instead, with a fixed $\varepsilon\in (0,\frac{1}{2})$, the agent observes a sample from the chosen arm's distribution with probability $1-\varepsilon$, or from an arbitrary corruption distribution with probability $\varepsilon$. Importantly, we impose no assumptions on these corruption distributions, which can be unbounded. In this setting, accommodating potentially unbounded corruptions, we establish a problem-dependent lower bound on regret for a given family of arm distributions. We introduce CRIMED, an asymptotically-optimal algorithm that achieves the exact lower bound on regret for bandits with Gaussian distributions with known variance. Additionally, we provide a finite-sample analysis of CRIMED's regret performance. Notably, CRIMED can effectively handle corruptions with $\varepsilon$ values as high as $\frac{1}{2}$. Furthermore, we develop a tight concentration result for medians in the presence of arbitrary corruptions, even with $\varepsilon$ values up to $\frac{1}{2}$, which may be of independent interest. We also discuss an extension of the algorithm for handling misspecification in Gaussian model.
</details>
<details>
<summary>摘要</summary>
我们研究了在多臂机构设定中对抗恨衰问题，这个问题的特点是：代理人在每次选择臂时会收到由该臂的分布生成的奖励，但是这些奖励不直接观察到。而是，代理人会有一定的几率（即 $\varepsilon$）选择观察臂的分布，而另一些几率选择一个随机的扰动分布。我们不假设这些扰动分布的性质，它们可能是无界的。在这个设定下，我们建立了问题依赖的下界限定 regret，这是一个对于每个臂分布的家族而言。我们引入了CRIMED，一个在数据分布为 Gaussian 时的 asymptotically-optimal 算法，它可以实现下界限定 regret。此外，我们进行了finite-sample 分析，发现CRIMED 可以有效地处理 $\varepsilon$ 值高达 $\frac{1}{2}$ 的扰动。此外，我们还提出了一个紧密的集中数据分布中的 median 对于arbitrary corruptions的对称问题，这可能是独立的 интерес。最后，我们讨论了在 Gaussian 模型中处理错误的扩展。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Gaussian-process-representation-of-vector-fields-over-arbitrary-latent-manifolds"><a href="#Implicit-Gaussian-process-representation-of-vector-fields-over-arbitrary-latent-manifolds" class="headerlink" title="Implicit Gaussian process representation of vector fields over arbitrary latent manifolds"></a>Implicit Gaussian process representation of vector fields over arbitrary latent manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16746">http://arxiv.org/abs/2309.16746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/agosztolai/rvgp">https://github.com/agosztolai/rvgp</a></li>
<li>paper_authors: Robert L. Peach, Matteo Vinao-Carl, Nir Grossman, Michael David, Emma Mallas, David Sharp, Paresh A. Malhotra, Pierre Vandergheynst, Adam Gosztolai</li>
<li>for: 用于学习未知函数和评估数据空间不确定性</li>
<li>methods: 使用 pozitional 编码和 Laplacian 的Connection eigenfunctions 来扩展 GP 来学习 vector 信号</li>
<li>results: 可以在 manifold 上具有全球正则性，从而实现 vector 场的超分辨和填充，并且可以用于重建高密度神经动力学记录中的疾病标志Here’s the breakdown of each point:1. <em>for:</em> 用于学习未知函数和评估数据空间不确定性 - The paper is written to present a new method for learning vector signals over latent Riemannian manifolds, which can be used to improve the resolution and inpainting of vector fields in various applications.2. <em>methods:</em> 使用 pozitional 编码和 Laplacian 的Connection eigenfunctions 来扩展 GP 来学习 vector 信号 - The method uses positional encoding with eigenfunctions of the connection Laplacian to extend Gaussian processes (GPs) for learning vector signals over latent Riemannian manifolds.3. <em>results:</em> 可以在 manifold 上具有全球正则性，从而实现 vector 场的超分辨和填充，并且可以用于重建高密度神经动力学记录中的疾病标志 - The method is able to achieve global regularity over the manifold, which allows for super-resolution and inpainting of vector fields, and can be used to reconstruct high-density neural dynamics from low-density EEG recordings in healthy individuals and Alzheimer’s patients.<details>
<summary>Abstract</summary>
Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-density neural dynamics derived from low-density EEG recordings in healthy individuals and Alzheimer's patients. We show that vector field singularities are important disease markers and that their reconstruction leads to a comparable classification accuracy of disease states to high-density recordings. Thus, our method overcomes a significant practical limitation in experimental and clinical applications.
</details>
<details>
<summary>摘要</summary>
We introduce RVGP, a generalization of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities.Furthermore, we use RVGP to reconstruct high-density neural dynamics derived from low-density EEG recordings in healthy individuals and Alzheimer's patients. We show that vector field singularities are important disease markers and that their reconstruction leads to a comparable classification accuracy of disease states to high-density recordings. Thus, our method overcomes a significant practical limitation in experimental and clinical applications.Translation in Simplified Chinese: Gaussian 进程 (GPs) 是一种流行的非 Parametric 统计模型，用于学习未知函数并量化数据中的空间时间不确定性。  recent works 扩展了 GPs 以模型分布在非 Euclidian 空间中的Scalar 和 Vector 量，包括计算机视觉、动力学系统和神经科学中的 Smooth 抽象。然而，这些方法假设数据下的抽象是已知的，这限制了它们的实际应用。我们引入 RVGP，一种 GPs 的扩展，用于学习分布在 latent  Риман  manifolds 上的Vector 信号。我们的方法使用 pozitional 编码，使用 tangent 维Bundle 上的连接 Laplacian 的 eigenfunctions，可以从通用的图像基 Approximation 中 derivation。我们示示了 RVGP 在 manifold 上具有全局正则性，可以Super-Resolution 和 Inpaint vector 场景，保留特点。此外，我们使用 RVGP 重建来自低密度 EEG 记录的高密度神经动力学。我们表明 that vector 场景的缺点是重要的疾病标志，并且其重建可以达到与高密度记录相同的疾病状态分类精度。因此，我们的方法超越了实际和临床应用中的重要限制。
</details></li>
</ul>
<hr>
<h2 id="Correcting-for-heterogeneity-in-real-time-epidemiological-indicators"><a href="#Correcting-for-heterogeneity-in-real-time-epidemiological-indicators" class="headerlink" title="Correcting for heterogeneity in real-time epidemiological indicators"></a>Correcting for heterogeneity in real-time epidemiological indicators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16546">http://arxiv.org/abs/2309.16546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron Rumack, Roni Rosenfeld, F. William Townes<br>for:* 这个论文旨在解决 auxillary data sources 上的空间和时间不均性问题，以提高 epidemiological surveillance 的准确性。methods:* 这种方法使用一个 “导航” 信号来纠正空间和时间不均性，并生成一个更可靠的信号，可以用于模型和预测。* 方法假设不均性可以近似为一个低级matrix，并且时间不均性平滑。results:* 通过使用这种方法，可以减少 auxillary data sources 上的不均性，从而大幅提高 epidemiological surveillance 的准确性。* 无基础实际数据，通过图表和地图来证明这种方法的有效性。<details>
<summary>Abstract</summary>
Auxiliary data sources have become increasingly important in epidemiological surveillance, as they are often available at a finer spatial and temporal resolution, larger coverage, and lower latency than traditional surveillance signals. We describe the problem of spatial and temporal heterogeneity in these signals derived from these data sources, where spatial and/or temporal biases are present. We present a method to use a ``guiding'' signal to correct for these biases and produce a more reliable signal that can be used for modeling and forecasting. The method assumes that the heterogeneity can be approximated by a low-rank matrix and that the temporal heterogeneity is smooth over time. We also present a hyperparameter selection algorithm to choose the parameters representing the matrix rank and degree of temporal smoothness of the corrections. In the absence of ground truth, we use maps and plots to argue that this method does indeed reduce heterogeneity. Reducing heterogeneity from auxiliary data sources greatly increases their utility in modeling and forecasting epidemics.
</details>
<details>
<summary>摘要</summary>
auxiliary数据源在流行病监测中变得越来越重要，因为它们通常具有更细致的空间和时间分布、更广泛的覆盖率和更低的延迟时间，相比传统监测信号。我们描述了这些信号中的空间和时间不均衡问题，其中存在空间和/或时间偏见。我们提出了使用“导向”信号来修正这些偏见，生成一个更可靠的信号，可以用于模型和预测。该方法假设空间和时间不均衡可以被近似为低级矩阵，并且时间不均衡平滑于时间。我们还提出了选择参数算法，用于选择表示矩阵级别和时间不均衡级别的参数。在缺乏真实参照数据的情况下，我们通过地图和图表来证明，这种方法确实减少了不均衡。减少auxiliary数据源中的不均衡，大大提高了它们的用于模型和预测流行病的utililty。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Training-of-One-Class-Classification-SVMs"><a href="#Efficient-Training-of-One-Class-Classification-SVMs" class="headerlink" title="Efficient Training of One Class Classification-SVMs"></a>Efficient Training of One Class Classification-SVMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16745">http://arxiv.org/abs/2309.16745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Isaac Amornortey Yowetu, Nana Kena Frempong</li>
<li>for: 这个研究探讨了一种高效的训练方法，用于进行一类分类。在常见的 binary 分类场景中，有效的分类器需要both positive和negative示例在训练数据中存在。然而，在许多领域中，只有一个类型的示例。为了解决这个问题，我们创建了一种学习从solely positive输入的分类算法。</li>
<li>methods: 我们使用了Augmented Lagrangian（AL-FPGM），一种变体的快速 proyected gradient method，来实现这种方法。AL-FPGM只需要first derivatives，即计算主要为matrix-vector product。因此，它可以补充现有的quadratic programming solvers，用于训练大SVM。</li>
<li>results: 我们对实际世界数据集进行了广泛验证，并证明了我们的策略可以获得 statistically significant results。<details>
<summary>Abstract</summary>
This study examines the use of a highly effective training method to conduct one-class classification. The existence of both positive and negative examples in the training data is necessary to develop an effective classifier in common binary classification scenarios. Unfortunately, this criteria is not met in many domains. Here, there is just one class of examples. Classification algorithms that learn from solely positive input have been created to deal with this setting. In this paper, an effective algorithm for dual soft-margin one-class SVM training is presented. Our approach makes use of the Augmented Lagrangian (AL-FPGM), a variant of the Fast Projected Gradient Method. The FPGM requires only first derivatives, which for the dual soft margin OCC-SVM means computing mainly a matrix-vector product. Therefore, AL-FPGM, being computationally inexpensive, may complement existing quadratic programming solvers for training large SVMs. We extensively validate our approach over real-world datasets and demonstrate that our strategy obtains statistically significant results.
</details>
<details>
<summary>摘要</summary>
In this paper, an effective algorithm for dual soft-margin one-class support vector machine (SVM) training is presented. Our approach utilizes the Augmented Lagrangian (AL-FPGM), a variant of the Fast Projected Gradient Method, which is computationally inexpensive and can complement existing quadratic programming solvers for training large SVMs.We extensively validate our approach on real-world datasets and demonstrate that our strategy achieves statistically significant results.
</details></li>
</ul>
<hr>
<h2 id="Generating-Personalized-Insulin-Treatments-Strategies-with-Deep-Conditional-Generative-Time-Series-Models"><a href="#Generating-Personalized-Insulin-Treatments-Strategies-with-Deep-Conditional-Generative-Time-Series-Models" class="headerlink" title="Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models"></a>Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16521">http://arxiv.org/abs/2309.16521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Schürch, Xiang Li, Ahmed Allam, Giulia Rathmes, Amina Mollaysa, Claudia Cavelti-Weder, Michael Krauthammer</li>
<li>for: 这篇论文是为了开发一个新的框架，让医生可以根据患者的个人历史资料，生成适应性的治疗方案。</li>
<li>methods: 这篇论文使用了深度生成时间序列模型，并且应用了决策理论，以生成具有实际可能性的个人化治疗方案。</li>
<li>results: 这篇论文透过实验显示了一个新的框架，可以根据患者的个人历史资料，生成适应性的治疗方案，并且可以预测未来的血糖水平。<details>
<summary>Abstract</summary>
We propose a novel framework that combines deep generative time series models with decision theory for generating personalized treatment strategies. It leverages historical patient trajectory data to jointly learn the generation of realistic personalized treatment and future outcome trajectories through deep generative time series models. In particular, our framework enables the generation of novel multivariate treatment strategies tailored to the personalized patient history and trained for optimal expected future outcomes based on conditional expected utility maximization. We demonstrate our framework by generating personalized insulin treatment strategies and blood glucose predictions for hospitalized diabetes patients, showcasing the potential of our approach for generating improved personalized treatment strategies. Keywords: deep generative model, probabilistic decision support, personalized treatment generation, insulin and blood glucose prediction
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的框架，它结合深度生成时间序列模型和决策理论来生成个性化治疗策略。它利用历史患者轨迹数据来同时学习生成真实个性化治疗和未来结果轨迹的深度生成时间序列模型。特别是，我们的框架允许生成基于个性化患者历史和训练优化预期未来结果的多变量治疗策略。我们通过生成医院糖尿病患者的个性化药物治疗策略和血糖预测，展示了我们的方法在生成改进个性化治疗策略方面的潜在优势。关键字：深度生成模型、概率决策支持、个性化治疗生成、药物和血糖预测
</details></li>
</ul>
<hr>
<h2 id="AtomSurf-Surface-Representation-for-Learning-on-Protein-Structures"><a href="#AtomSurf-Surface-Representation-for-Learning-on-Protein-Structures" class="headerlink" title="AtomSurf : Surface Representation for Learning on Protein Structures"></a>AtomSurf : Surface Representation for Learning on Protein Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16519">http://arxiv.org/abs/2309.16519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vincentx15/atom2d">https://github.com/vincentx15/atom2d</a></li>
<li>paper_authors: Vincent Mallet, Souhaib Attaiki, Maks Ovsjanikov</li>
<li>for: 本研究旨在发展 Cryo-EM 和蛋白结构预测算法，使大规模蛋白结构可访问，以便基于机器学习的功能预测。</li>
<li>methods: 本研究使用 geometric deep learning 方法， Representing 蛋白结构为 $\textit{3D mesh surfaces}$，并与已有的表示 benchmark 集成。</li>
<li>results: 研究发现，surface 表示独立不足，但可与 graph-based 方法相结合，以获得最佳结果。 本研究的代码和数据可在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Vincentx15/atom2D%E3%80%82">https://github.com/Vincentx15/atom2D。</a><details>
<summary>Abstract</summary>
Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.   In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with graph-based methods, resulting in a general framework that incorporates both representations in learning. We show that using this combination, we are able to obtain state-of-the-art results across $\textit{all tested tasks}$. Our code and data can be found online: https://github.com/Vincentx15/atom2D .
</details>
<details>
<summary>摘要</summary>
最近的冰结电镜和蛋白结构预测算法的进步使得大规模蛋白结构可以获得，这开了机器学习基于功能注释的大门。在几何深度学习中，我们关注创建适用于几何数据的方法。在学习蛋白结构时，表示这些结构为几何对象（是灰度、图还是表面），并应用适应这种表示的学习方法。这种方法的性能取决于表示和学习方法。在这篇论文中，我们调查使用$\textit{3D mesh surfaces}$表示蛋白质和其他表示结合使用的方法。我们发现，尽管有前期的承诺性结果，但独立使用表面表示并不是与3D网格相比竞争力强。基于这，我们介绍了一种衍生的方法，把表面表示与图形基本方法结合使用，得到一个涵盖所有测试任务的通用框架。我们的代码和数据可以在https://github.com/Vincentx15/atom2D中找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Poisoning-Fair-Representations"><a href="#Towards-Poisoning-Fair-Representations" class="headerlink" title="Towards Poisoning Fair Representations"></a>Towards Poisoning Fair Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16487">http://arxiv.org/abs/2309.16487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing Gao</li>
<li>for:  Mitigating model prediction bias against certain demographic subgroups such as elder and female.</li>
<li>methods:  Data poisoning attack on fair representation learning (FRL) models, which induce the model to output unfair representations that contain as much demographic information as possible by injecting carefully crafted poisoning samples into the training data.</li>
<li>results:  Superiority of the proposed attack on benchmark fairness datasets and state-of-the-art fair representation learning models, as well as a theoretical analysis on the needed number of poisoning samples to defend against the attack.<details>
<summary>Abstract</summary>
Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much demographic information as possible by injecting carefully crafted poisoning samples into the training data. This attack entails a prohibitive bilevel optimization, wherefore an effective approximated solution is proposed. A theoretical analysis on the needed number of poisoning samples is derived and sheds light on defending against the attack. Experiments on benchmark fairness datasets and state-of-the-art fair representation learning models demonstrate the superiority of our attack.
</details>
<details>
<summary>摘要</summary>
“公平机器学习” seek to mitigate model prediction bias against certain demographic subgroups such as the elderly and women. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much demographic information as possible by injecting carefully crafted poisoning samples into the training data. This attack entails a prohibitive bilevel optimization, wherefore an effective approximated solution is proposed. A theoretical analysis on the needed number of poisoning samples is derived and sheds light on defending against the attack. Experiments on benchmark fairness datasets and state-of-the-art fair representation learning models demonstrate the superiority of our attack.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese writing systems. The other one is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Predicting-Long-term-Renal-Impairment-in-Post-COVID-19-Patients-with-Machine-Learning-Algorithms"><a href="#Predicting-Long-term-Renal-Impairment-in-Post-COVID-19-Patients-with-Machine-Learning-Algorithms" class="headerlink" title="Predicting Long-term Renal Impairment in Post-COVID-19 Patients with Machine Learning Algorithms"></a>Predicting Long-term Renal Impairment in Post-COVID-19 Patients with Machine Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16744">http://arxiv.org/abs/2309.16744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maitham G. Yousif, Hector J. Castro, John Martin, Hayder A. Albaqer, Fadhil G. Al-Amran, Habeeb W. Shubber, Salman Rawaf</li>
<li>for: 这项研究的目的是预测患有COVID-19后长期reno衰竭的风险，以便早期发现和 intervene 可能会受到reno衰竭的风险的患者，从而提高临床 result。</li>
<li>methods: 该研究使用了高级机器学习算法，包括多项式回归、决策树、Random Forest 等，对821名COVID-19后患者的数据进行分析和预测。</li>
<li>results: 研究发现，年龄、血压、血糖和肥胖等因素均与长期reno衰竭有关，并且通过使用机器学习算法，可以准确预测患者是否会发展reno衰竭。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has had far-reaching implications for global public health. As we continue to grapple with its consequences, it becomes increasingly clear that post-COVID-19 complications are a significant concern. Among these complications, renal impairment has garnered particular attention due to its potential long-term health impacts. This study, conducted with a cohort of 821 post-COVID-19 patients from diverse regions of Iraq across the years 2021, 2022, and 2023, endeavors to predict the risk of long-term renal impairment using advanced machine learning algorithms. Our findings have the potential to revolutionize post-COVID-19 patient care by enabling early identification and intervention for those at risk of renal impairment, ultimately improving clinical outcomes. This research encompasses comprehensive data collection and preprocessing, feature selection, and the development of predictive models using various machine learning algorithms. The study's objectives are to assess the incidence of long-term renal impairment in post-COVID-19 patients, identify associated risk factors, create predictive models, and evaluate their accuracy. We anticipate that our machine learning models, drawing from a rich dataset, will provide valuable insights into the risk of renal impairment, ultimately enhancing patient care and quality of life. In conclusion, the research presented herein offers a critical contribution to the field of post-COVID-19 care. By harnessing the power of machine learning, we aim to predict long-term renal impairment risk accurately. These predictions have the potential to inform healthcare professionals, enabling them to take proactive measures and provide targeted interventions for post-COVID-19 patients at risk of renal complications, thus minimizing the impact of this serious health concern.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对全球公共卫生造成了深见的影响。我们继续面临这些后果，越来越清楚地认识到Post-COVID-19 合并症是一个重要的担忧。其中，肾功能障碍受到了特别关注，因为它可能会对长期健康造成深见的影响。本研究使用了821名来自伊拉克不同地区的 Post-COVID-19 患者的 cohort，从2021年至2023年，通过先进的机器学习算法，预测患者在长期内可能发生肾功能障碍的风险。我们的发现有助于提高患者的临床结果，因为它们可以让医生在患者风险肾功能障碍的情况下采取早期预防措施，从而改善患者的生活质量。本研究包括了完整的数据收集和处理、特征选择以及使用不同的机器学习算法来建立预测模型。研究的目标是评估Post-COVID-19 患者长期肾功能障碍的发生率、相关风险因素、预测模型的建立以及其准确性的评估。我们预计，基于丰富的数据集，我们的机器学习模型将提供价值的预测，帮助医生更好地识别患者患肾功能障碍的风险，并采取相应的措施，从而最大化患者的生活质量。因此，本研究对Post-COVID-19 患者的护理做出了重要贡献。通过利用机器学习的力量，我们可以准确预测患者在长期内可能发生的肾功能障碍风险，并提供价值的预测，以便医生能够在患者风险肾功能障碍的情况下采取早期预防措施，最终减少这种严重的健康问题对患者的影响。
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-robust-regression-under-heavy-tailed-data-Asymptotics-and-Universality"><a href="#High-dimensional-robust-regression-under-heavy-tailed-data-Asymptotics-and-Universality" class="headerlink" title="High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality"></a>High-dimensional robust regression under heavy-tailed data: Asymptotics and Universality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16476">http://arxiv.org/abs/2309.16476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Urte Adomaityte, Leonardo Defilippis, Bruno Loureiro, Gabriele Sicuro</li>
<li>For: 这个论文研究了高维度 robust 回归估计器在含有非常极端尾的杂度和响应函数杂度的情况下的性能。* Methods: 这个论文使用了 M-估计器和ridge 回归估计器，并研究了这些估计器在高维度情况下的性能。* Results: 研究发现，尽管 Huber 损失函数可以在一些情况下达到最优性能，但在高维度情况下，这种损失函数会导致估计器的性能下降。此外，研究还发现了一种“奇怪的转变”现象，即在抽样复杂度和杂度之间的关系。最后，研究还发现了这些方法在更加丰富的模型和数据分布下的性能。<details>
<summary>Abstract</summary>
We investigate the high-dimensional properties of robust regression estimators in the presence of heavy-tailed contamination of both the covariates and response functions. In particular, we provide a sharp asymptotic characterisation of M-estimators trained on a family of elliptical covariate and noise data distributions including cases where second and higher moments do not exist. We show that, despite being consistent, the Huber loss with optimally tuned location parameter $\delta$ is suboptimal in the high-dimensional regime in the presence of heavy-tailed noise, highlighting the necessity of further regularisation to achieve optimal performance. This result also uncovers the existence of a curious transition in $\delta$ as a function of the sample complexity and contamination. Moreover, we derive the decay rates for the excess risk of ridge regression. We show that, while it is both optimal and universal for noise distributions with finite second moment, its decay rate can be considerably faster when the covariates' second moment does not exist. Finally, we show that our formulas readily generalise to a richer family of models and data distributions, such as generalised linear estimation with arbitrary convex regularisation trained on mixture models.
</details>
<details>
<summary>摘要</summary>
我们研究高维性质的稳健回归估计器在 covariates 和响应函数中存在重尾的情况下。特别是，我们提供了一个锐化的极限性Characterisation of M-estimators 在一家包括二阶和更高阶积分布的 Elliptical covariate 和噪声数据分布下。我们发现，虽然consistent，但是在高维度 régime中，使用惩罚函数 Huber loss 的最佳调整参数 $\delta$ 是不优化的，这显示了需要进一步的正则化以实现最佳性能。这也暴露了一个curious transition 在 $\delta$ 中，它与样本复杂度和污染的关系。此外，我们 derivethe decay rates for the excess risk of ridge regression。我们发现，当噪声分布有finite second moment时，它是最佳和universal的，但是其衰减率可以在 covariates 的 second moment不存在时显著快。最后，我们表明了我们的公式可以轻松扩展到一个更加Rich family of models and data distributions，例如通用线性估计器在 mixture models 上。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Program-Generation-for-Systematic-Generalization"><a href="#Compositional-Program-Generation-for-Systematic-Generalization" class="headerlink" title="Compositional Program Generation for Systematic Generalization"></a>Compositional Program Generation for Systematic Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16467">http://arxiv.org/abs/2309.16467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Klinger, Luke Liu, Soham Dan, Maxwell Crouse, Parikshit Ram, Alexander Gray</li>
<li>for: The paper is written to explore the ability of a neuro-symbolic architecture called the Compositional Program Generator (CPG) to generalize on new concepts in a few-shot manner and to achieve perfect generalization on sequence-to-sequence language tasks.</li>
<li>methods: The paper uses a grammar of the input domain and a parser to generate a type hierarchy, and learns parameters for the semantic modules incrementally.</li>
<li>results: The paper achieves perfect generalization on the SCAN and COGS benchmarks in both standard and extreme few-shot settings.Here is the text in Simplified Chinese:</li>
<li>for: 本文是用来探究一种叫做 Compositional Program Generator (CPG) 的 neuromorphic架构在几个示例下能够泛化到新的概念。</li>
<li>methods: CPG 使用输入语言的语法和解析器生成一个类型层次结构，并逐步学习 semantic module 的参数。</li>
<li>results: CPG 在 SCAN 和 COGS 测试集上在标准和极端几个示例下达到了完美泛化。<details>
<summary>Abstract</summary>
Compositional generalization is a key ability of humans that enables us to learn new concepts from only a handful examples. Machine learning models, including the now ubiquitous transformers, struggle to generalize in this way, and typically require thousands of examples of a concept during training in order to generalize meaningfully. This difference in ability between humans and artificial neural architectures, motivates this study on a neuro-symbolic architecture called the Compositional Program Generator (CPG). CPG has three key features: modularity, type abstraction, and recursive composition, that enable it to generalize both systematically to new concepts in a few-shot manner, as well as productively by length on various sequence-to-sequence language tasks. For each input, CPG uses a grammar of the input domain and a parser to generate a type hierarchy in which each grammar rule is assigned its own unique semantic module, a probabilistic copy or substitution program. Instances with the same hierarchy are processed with the same composed program, while those with different hierarchies may be processed with different programs. CPG learns parameters for the semantic modules and is able to learn the semantics for new types incrementally. Given a context-free grammar of the input language and a dictionary mapping each word in the source language to its interpretation in the output language, CPG can achieve perfect generalization on the SCAN and COGS benchmarks, in both standard and extreme few-shot settings.
</details>
<details>
<summary>摘要</summary>
人类 possess a crucial ability called "compositional generalization", which enables us to learn new concepts from just a few examples. However, current machine learning models, including transformers, struggle to generalize in this way and often require thousands of examples to generalize meaningfully. This difference in ability motivates the development of a neuro-symbolic architecture called the Compositional Program Generator (CPG).CPG has three key features: modularity, type abstraction, and recursive composition. These features enable CPG to generalize both systematically to new concepts in a few-shot manner and productively on various sequence-to-sequence language tasks. For each input, CPG uses a grammar of the input domain and a parser to generate a type hierarchy, where each grammar rule is assigned its own unique semantic module. Instances with the same hierarchy are processed with the same composed program, while those with different hierarchies may be processed with different programs.CPG learns parameters for the semantic modules and can learn the semantics for new types incrementally. With a context-free grammar of the input language and a dictionary mapping each word in the source language to its interpretation in the output language, CPG can achieve perfect generalization on the SCAN and COGS benchmarks, both in standard and extreme few-shot settings.
</details></li>
</ul>
<hr>
<h2 id="A-Metaheuristic-for-Amortized-Search-in-High-Dimensional-Parameter-Spaces"><a href="#A-Metaheuristic-for-Amortized-Search-in-High-Dimensional-Parameter-Spaces" class="headerlink" title="A Metaheuristic for Amortized Search in High-Dimensional Parameter Spaces"></a>A Metaheuristic for Amortized Search in High-Dimensional Parameter Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16465">http://arxiv.org/abs/2309.16465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neurolife77/drffit_paper">https://github.com/neurolife77/drffit_paper</a></li>
<li>paper_authors: Dominic Boutet, Sylvain Baillet</li>
<li>For: 这篇论文的目的是提出一种新的元HEURISTIC方法，用于适应 Dynamical models of (bio)physical systems 的参数推断问题，解决了难以求导数、高维空间和非线性模型函数等问题。* Methods: 这篇论文使用了 Bayesian inference methods，通过考虑参数在统计分布下的方法，不需要计算点优化参数值。具体来说，DR-FFIT 实现了一种有效的采样策略，通过 feature-informed transformations 来降低维度，并使用人工神经网络来获得模型特征的导数。* Results: 测试数据显示，DR-FFIT 可以提高 random-search 和 simulated-annealing 等metaheuristics的性能，同时保持计算成本在合理的范围内。此外，DR-FFIT 还可以提高模型的准确性。<details>
<summary>Abstract</summary>
Parameter inference for dynamical models of (bio)physical systems remains a challenging problem. Intractable gradients, high-dimensional spaces, and non-linear model functions are typically problematic without large computational budgets. A recent body of work in that area has focused on Bayesian inference methods, which consider parameters under their statistical distributions and therefore, do not derive point estimates of optimal parameter values. Here we propose a new metaheuristic that drives dimensionality reductions from feature-informed transformations (DR-FFIT) to address these bottlenecks. DR-FFIT implements an efficient sampling strategy that facilitates a gradient-free parameter search in high-dimensional spaces. We use artificial neural networks to obtain differentiable proxies for the model's features of interest. The resulting gradients enable the estimation of a local active subspace of the model within a defined sampling region. This approach enables efficient dimensionality reductions of highly non-linear search spaces at a low computational cost. Our test data show that DR-FFIT boosts the performances of random-search and simulated-annealing against well-established metaheuristics, and improves the goodness-of-fit of the model, all within contained run-time costs.
</details>
<details>
<summary>摘要</summary>
参数推断 для动力学模型（生物）物理系统仍然是一个挑战。不可追加的梯度、高维空间和非线性模型函数通常会带来困难，除非有大量计算预算。近些年，这个领域的研究都集中在 Bayesian推断方法上，它们考虑参数在统计分布中，因此不会得到优化参数值的点 estimate。我们提出了一种新的metaheuristic，它可以从特征 Informed Transformations（DR-FFIT）中得到维度减少。DR-FFIT实现了一种高效的采样策略，该策略可以在高维空间中进行梯度free的参数搜索。我们使用人工神经网络来获得模型特征的可导代理。得到的梯度可以计算出模型在采样区域内的当地活跃子空间。这种方法可以高效地减少非线性搜索空间，在低计算成本下。我们的测试数据显示，DR-FFIT可以在Random-Search和Simulated-Annealing等已有metaheuristics的基础上提高性能，同时保持模型的匹配度， все在包含的运行时间成本下。
</details></li>
</ul>
<hr>
<h2 id="Universal-Sleep-Decoder-Aligning-awake-and-sleep-neural-representation-across-subjects"><a href="#Universal-Sleep-Decoder-Aligning-awake-and-sleep-neural-representation-across-subjects" class="headerlink" title="Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects"></a>Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16457">http://arxiv.org/abs/2309.16457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Zheng, Zhongtao Chen, Haiteng Wang, Jianyang Zhou, Lin Zheng, Yunzhe Liu</li>
<li>for: 这项研究的目的是解码sleep中的记忆内容。</li>
<li>methods: 这项研究使用了一种新的认知神经科学实验和一个完整的电enzephalography（EEG）数据集，并开发了一个名为“Universal Sleep Decoder”（USD）的模型，以实现将 neural representations在睡眠和醒目之间进行对应。</li>
<li>results: 研究实现了Up to 16.6%的零尝试预测精度，与使用个体睡眠数据的表现相当。 fine-tuning USD on test subjects可以提高预测精度至25.9%，与基线的6.7%预测精度有显著差异。<details>
<summary>Abstract</summary>
Decoding memory content from brain activity during sleep has long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed the Universal Sleep Decoder (USD) to align neural representations between wakefulness and sleep across subjects. Our model achieves up to 16.6% top-1 zero-shot accuracy on unseen subjects, comparable to decoding performances using individual sleep data. Furthermore, fine-tuning USD on test subjects enhances decoding accuracy to 25.9% top-1 accuracy, a substantial improvement over the baseline chance of 6.7%. Model comparison and ablation analyses reveal that our design choices, including the use of (i) an additional contrastive objective to integrate awake and sleep neural signals and (ii) the pretrain-finetune paradigm to incorporate different subjects, significantly contribute to these performances. Collectively, our findings and methodologies represent a significant advancement in the field of sleep decoding.
</details>
<details>
<summary>摘要</summary>
“decode”brain activity during sleep的内容long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. 积极应用这个benchmark dataset，我们开发了Universal Sleep Decoder (USD)，用于对 neural representations between wakefulness and sleep across subjects进行对应。我们的模型在未看过数据的情况下可以达到16.6%的top-1零投票精度，与使用个体睡眠数据的decoding性能相当。此外，对测试 subjects进行 fine-tuning可以提高decoding精度到25.9%的top-1精度，与基线的6.7%精度有所提高。模型比较和简洁分析表明，我们的设计选择，包括（i）使用额外的对比性目标来整合睡眠和醒目的神经信号，以及（ii）使用pretrain-finetune paradigm来 incorporate different subjects，对这些性能做出了重要贡献。总的来说，我们的发现和方法ология在睡眠decoding领域 represent a significant advancement.
</details></li>
</ul>
<hr>
<h2 id="Resisting-Backdoor-Attacks-in-Federated-Learning-via-Bidirectional-Elections-and-Individual-Perspective"><a href="#Resisting-Backdoor-Attacks-in-Federated-Learning-via-Bidirectional-Elections-and-Individual-Perspective" class="headerlink" title="Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective"></a>Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16456">http://arxiv.org/abs/2309.16456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng</li>
<li>for: 本研究旨在防止 Federated Learning (FL) 中的后门攻击，通过 exclude 恶意更新而不 negatively impact 模型精度。</li>
<li>methods: 本文提出了一种新的 anti-backdoor FL 框架，即 Snowball，基于竞选机制。它包括 bottom-up 选举和 top-down 选举两部分，通过选举和拒绝恶意更新来防止后门攻击。</li>
<li>results: 对于五个真实的 dataset，Snowball 与当前的防御技术进行比较，显示其在防止后门攻击方面更高效，并且对全球模型精度的影响较小。<details>
<summary>Abstract</summary>
Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, model updates are easy to be mixed and scattered throughout in reality due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation; and b) top-down election, where selectees progressively enlarge themselves through picking up from the candidates. We compare Snowball with state-of-the-art defenses to backdoor attacks in FL on five real-world datasets, demonstrating its superior resistance to backdoor attacks and slight impact on the accuracy of the global model.
</details>
<details>
<summary>摘要</summary>
现有方法在 federated learning (FL) 中防止后门攻击主要通过：一、减轻感染模型的影响，或二、排除感染模型。前者会影响模型精度，而后者通常基于全局清晰的边界来分开干扰和不干扰模型更新。然而，模型更新在实际情况中容易杂mix和散布，这使得前两种方法具有局限性。这种工作关注于排除感染模型在 FL 中。与前一种全球视图不同，我们提出了 Snowball，一个新的反后门 FL 框架，基于个体视图而 inspirited 由我们所采用的一个原理和 FL 和深度学习中的两个原理。它的特点包括：a）底层选举，每个候选模型更新可以向几个同等 peer 模型更新投票，以选出一些模型更新作为选择者进行聚合;b）顶层选举，选择者逐渐扩大自己通过挑选候选模型更新。我们与现有的防御技术进行比较，在五个真实的数据集上，展示了 Snowball 对后门攻击的高度抵抗力和模型全球精度的轻微影响。
</details></li>
</ul>
<hr>
<h2 id="On-the-Trade-offs-between-Adversarial-Robustness-and-Actionable-Explanations"><a href="#On-the-Trade-offs-between-Adversarial-Robustness-and-Actionable-Explanations" class="headerlink" title="On the Trade-offs between Adversarial Robustness and Actionable Explanations"></a>On the Trade-offs between Adversarial Robustness and Actionable Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16452">http://arxiv.org/abs/2309.16452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyapriya Krishna, Chirag Agarwal, Himabindu Lakkaraju</li>
<li>for: 这 paper 的目的是研究机器学习模型在不同情况下的可解释性和抗击攻击性的关系。</li>
<li>methods: 这 paper 使用了现有的 state-of-the-art 算法来生成可解释性和抗击攻击性的模型。</li>
<li>results: 这 paper 的结果表明，逐渐增加模型的抗击攻击性会导致可解释性减退，而且在某些情况下，可能导致可解释性和抗击攻击性之间存在负相关性。<details>
<summary>Abstract</summary>
As machine learning models are increasingly being employed in various high-stakes settings, it becomes important to ensure that predictions of these models are not only adversarially robust, but also readily explainable to relevant stakeholders. However, it is unclear if these two notions can be simultaneously achieved or if there exist trade-offs between them. In this work, we make one of the first attempts at studying the impact of adversarially robust models on actionable explanations which provide end users with a means for recourse. We theoretically and empirically analyze the cost (ease of implementation) and validity (probability of obtaining a positive model prediction) of recourses output by state-of-the-art algorithms when the underlying models are adversarially robust vs. non-robust. More specifically, we derive theoretical bounds on the differences between the cost and the validity of the recourses generated by state-of-the-art algorithms for adversarially robust vs. non-robust linear and non-linear models. Our empirical results with multiple real-world datasets validate our theoretical results and show the impact of varying degrees of model robustness on the cost and validity of the resulting recourses. Our analyses demonstrate that adversarially robust models significantly increase the cost and reduce the validity of the resulting recourses, thus shedding light on the inherent trade-offs between adversarial robustness and actionable explanations
</details>
<details>
<summary>摘要</summary>
machine learning模型在不同的高风险场景中越来越常被应用，因此确保这些模型的预测结果不仅抗击性强，而且可以快速地解释给关键参与者变得非常重要。然而，是否可以同时实现这两个目标，或者存在这两个目标之间的负担，这是一个未知的问题。在这种情况下，我们在研究抗击性模型对可行的解释的影响，这些解释可以提供结束用户一种纠正的机会。我们在理论和实际上分析了使用当前的算法生成的纠正措施的成本（实施的容易度）和有效性（获得正确模型预测结果的概率）。我们在理论上 deriv了对抗性模型和非抗击性模型的线性和非线性模型的分析结果，并通过多个实际数据集的实验 validate我们的理论结果，以验证模型的抗击性对纠正措施的影响。我们的分析结果表明，抗击性模型会增加纠正措施的成本和降低纠正措施的有效性，从而揭示了这两个目标之间的内在负担。
</details></li>
</ul>
<hr>
<h2 id="A-parsimonious-computationally-efficient-machine-learning-method-for-spatial-regression"><a href="#A-parsimonious-computationally-efficient-machine-learning-method-for-spatial-regression" class="headerlink" title="A parsimonious, computationally efficient machine learning method for spatial regression"></a>A parsimonious, computationally efficient machine learning method for spatial regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16448">http://arxiv.org/abs/2309.16448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milan Žukovič, Dionissios T. Hristopulos</li>
<li>For: The paper is written for researchers and practitioners who are interested in spatial&#x2F;temporal regression and machine learning methods.* Methods: The paper introduces the modified planar rotator method (MPRS), a non-parametric machine learning method that incorporates spatial or temporal correlations via short-range, distance-dependent “interactions” without assuming a specific form for the underlying probability distribution. The method uses equilibrium conditional Monte Carlo simulations to make predictions.* Results: The paper reports tests on various synthetic and real-world data in one, two, and three dimensions that demonstrate the competitiveness of MPRS prediction performance with standard interpolation methods such as ordinary kriging and inverse distance weighting, especially for rough and non-Gaussian data. The method also shows superior computational efficiency and scalability for large samples, allowing for the processing of massive data sets involving millions of nodes in a few seconds on a standard personal computer.<details>
<summary>Abstract</summary>
We introduce the modified planar rotator method (MPRS), a physically inspired machine learning method for spatial/temporal regression. MPRS is a non-parametric model which incorporates spatial or temporal correlations via short-range, distance-dependent ``interactions'' without assuming a specific form for the underlying probability distribution. Predictions are obtained by means of a fully autonomous learning algorithm which employs equilibrium conditional Monte Carlo simulations. MPRS is able to handle scattered data and arbitrary spatial dimensions. We report tests on various synthetic and real-word data in one, two and three dimensions which demonstrate that the MPRS prediction performance (without parameter tuning) is competitive with standard interpolation methods such as ordinary kriging and inverse distance weighting. In particular, MPRS is a particularly effective gap-filling method for rough and non-Gaussian data (e.g., daily precipitation time series). MPRS shows superior computational efficiency and scalability for large samples. Massive data sets involving millions of nodes can be processed in a few seconds on a standard personal computer.
</details>
<details>
<summary>摘要</summary>
我团队引入修改的平面旋转方法（MPRS），这是一种物理启发的机器学习方法，用于空间/时间回归。MPRS 是一种非参数型模型，通过短距离、距离相互作用来包含空间或时间相关性，不需要假设特定的概率分布。预测通过完全自主学习算法，使用平衡conditional Monte Carlo仿真。MPRS 可以处理散列数据和任意空间维度。我们在一、二、三维 Synthetic 和实际数据上进行了多种测试，显示 MPRS 的预测性能（无需参数调整）与标准 interpolate 方法相匹配，如ordinary kriging 和 inverse distance weighting。尤其是在非欧几何数据（如日降雨时间序列）中，MPRS 表现出色，可以快速处理大量数据，只需几秒钟在标准个人电脑上处理百万个节点。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-MPC-design-for-incrementally-ISS-systems-with-application-to-GRU-networks"><a href="#Nonlinear-MPC-design-for-incrementally-ISS-systems-with-application-to-GRU-networks" class="headerlink" title="Nonlinear MPC design for incrementally ISS systems with application to GRU networks"></a>Nonlinear MPC design for incrementally ISS systems with application to GRU networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16428">http://arxiv.org/abs/2309.16428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Bonassi, Alessio La Bella, Marcello Farina, Riccardo Scattolini</li>
<li>for: 这篇论文旨在设计一种非线性预测控制策略，用于 exponentially incremental Input-to-State Stable（ISS）系统。</li>
<li>methods: 这种方法不需要耗费庞大的计算终端成分，而是基于显式定义的最小预测时间框，以保证关闭循环稳定性。</li>
<li>results: 这种控制方法在 GRU 网络上应用，并提供了一种适应性较高的状态观察器，并且有确定的收敛保证。  tested on a benchmark system, demonstrating its good control performances and efficient applicability.<details>
<summary>Abstract</summary>
This brief addresses the design of a Nonlinear Model Predictive Control (NMPC) strategy for exponentially incremental Input-to-State Stable (ISS) systems. In particular, a novel formulation is devised, which does not necessitate the onerous computation of terminal ingredients, but rather relies on the explicit definition of a minimum prediction horizon ensuring closed-loop stability. The designed methodology is particularly suited for the control of systems learned by Recurrent Neural Networks (RNNs), which are known for their enhanced modeling capabilities and for which the incremental ISS properties can be studied thanks to simple algebraic conditions. The approach is applied to Gated Recurrent Unit (GRU) networks, providing also a method for the design of a tailored state observer with convergence guarantees. The resulting control architecture is tested on a benchmark system, demonstrating its good control performances and efficient applicability.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Nonlinear Model Predictive Control" is translated as "非线性预测控制" (fēi xiàn xìng yù jí kòng zhì)* "Input-to-State Stable" is translated as "输入到状态稳定" (yù xīn dào zhèng dìng)* "Gated Recurrent Unit" is translated as "闭合回归单元" (bì hé huí qù dān yuán)* "Recurrent Neural Networks" is translated as "循环神经网络" (xún huán shēn zhì wǎng wǎn)
</details></li>
</ul>
<hr>
<h2 id="Selective-Nonparametric-Regression-via-Testing"><a href="#Selective-Nonparametric-Regression-via-Testing" class="headerlink" title="Selective Nonparametric Regression via Testing"></a>Selective Nonparametric Regression via Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16412">http://arxiv.org/abs/2309.16412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fedor Noskov, Alexander Fishkov, Maxim Panov</li>
<li>for: 本研究targets the nonparametric heteroskedastic regression problem, and develops an abstention procedure for selective prediction.</li>
<li>methods: 该方法通过测试 conditional variance 的值来实现选择性预测。与现有方法不同的是，该方法可以考虑 conditional variance 预测器的uncertainty。</li>
<li>results: 研究提供了非 asymptotic 的 risk bound，并证明了不同的收敛 режи。实验结果在 simulated 和实际数据上都有示例。<details>
<summary>Abstract</summary>
Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.
</details>
<details>
<summary>摘要</summary>
预测（或选择性预测）是重要的错误敏感机器学学问。在分类设置下，选择性预测已经受到了广泛的研究。但在回归设置下，选择性预测还是很少研究。在这种工作中，我们考虑了非参数化不同方差回归问题，并开发了一种投票过程，通过测试对给定点的 conditional variance 的假设来实现选择性预测。与现有方法不同的是，我们的方法不仅考虑了 conditional variance 的值本身，还考虑了相关的 variance 预测器的uncertainty。我们证明了非 asymptotic 的风险 bound，并证明了不同的整合 regime 的存在。理论分析通过了一系列的 simulate 和实际数据实验来进行了说明。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Synthetic-Treatment-Groups-without-the-Mean-Exchangeability-Assumption"><a href="#Constructing-Synthetic-Treatment-Groups-without-the-Mean-Exchangeability-Assumption" class="headerlink" title="Constructing Synthetic Treatment Groups without the Mean Exchangeability Assumption"></a>Constructing Synthetic Treatment Groups without the Mean Exchangeability Assumption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16409">http://arxiv.org/abs/2309.16409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhang, Yue Liu, Zhihua Zhang</li>
<li>for: 将多个随机控制试验的信息传输到目标人口，其中只有控制组数据 available。</li>
<li>methods: 使用模拟控制方法构建目标人口的合成治疗组，通过对源人口治疗组的权重进行最小化 conditional maximum mean discrepancy 来Estimate weights。</li>
<li>results: 我们建立了合成治疗组估计器的非 Parametric 性质，并通过实验证明了我们的方法可以作为 mean exchangeability assumption 被违反时的新的 complementary approach。<details>
<summary>Abstract</summary>
The purpose of this work is to transport the information from multiple randomized controlled trials to the target population where we only have the control group data. Previous works rely critically on the mean exchangeability assumption. However, as pointed out by many current studies, the mean exchangeability assumption might be violated. Motivated by the synthetic control method, we construct a synthetic treatment group for the target population by a weighted mixture of treatment groups of source populations. We estimate the weights by minimizing the conditional maximum mean discrepancy between the weighted control groups of source populations and the target population. We establish the asymptotic normality of the synthetic treatment group estimator based on the sieve semiparametric theory. Our method can serve as a novel complementary approach when the mean exchangeability assumption is violated. Experiments are conducted on synthetic and real-world datasets to demonstrate the effectiveness of our methods.
</details>
<details>
<summary>摘要</summary>
本研究的目的是将多个随机控制试验中的信息传输到目标人口，其中只有控制组数据 available。先前的研究几乎完全依赖于mean exchangeability假设。然而，根据当前的研究所指出，mean exchangeability假设可能被违反。我们被Synthetic control方法所驱动，通过对源人口中的治疗组组合一个权重的混合来构建目标人口中的 sintetic treatment组。我们对这个权重进行最小化条件的最大均值差来确定。我们建立了这种 sintetic treatment组估计器的几何分布正态性，基于 Sieving 半 Parametric 理论。我们的方法可以作为mean exchangeability假设不成立时的一种新的补充方法。我们在synthetic和实际世界数据上进行了实验，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="VAE-based-latent-space-classification-of-RNO-G-data"><a href="#VAE-based-latent-space-classification-of-RNO-G-data" class="headerlink" title="VAE-based latent-space classification of RNO-G data"></a>VAE-based latent-space classification of RNO-G data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16401">http://arxiv.org/abs/2309.16401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thorsten Glüsenkamp</li>
<li>for: 这个论文是为了描述一种用变量自适应Encoder（VAE）Latent Space来分类不同的噪声类型的方法。</li>
<li>methods: 该方法使用变量自适应Encoder（VAE）来生成一个含有噪声信息的笛割空间，然后使用这个笛割空间来分类不同的噪声类型。</li>
<li>results: 该方法可以自动检测和分类不同的噪声类型，包括物理风吹引起的信号和人工噪声。这些结果可以用来识别和分类不同的事件类型。<details>
<summary>Abstract</summary>
The Radio Neutrino Observatory in Greenland (RNO-G) is a radio-based ultra-high energy neutrino detector located at Summit Station, Greenland. It is still being constructed, with 7 stations currently operational. Neutrino detection works by measuring Askaryan radiation produced by neutrino-nucleon interactions. A neutrino candidate must be found amidst other backgrounds which are recorded at much higher rates -- including cosmic-rays and anthropogenic noise -- the origins of which are sometimes unknown. Here we describe a method to classify different noise classes using the latent space of a variational autoencoder. The latent space forms a compact representation that makes classification tractable. We analyze data from a noisy and a silent station. The method automatically detects and allows us to qualitatively separate multiple event classes, including physical wind-induced signals, for both the noisy and the quiet station.
</details>
<details>
<summary>摘要</summary>
<<SYS>>格陵兰的电台中微子观测站（RNO-G）是一个位于格陵兰的电子基本高能中微子探测器。它目前正在建设，现有7个站点已经运行。中微子探测是通过测量阿斯卡莱涅发生的中微子-原子间相互作用产生的探测。中微子候选者需要在其他背景中被发现，其中包括宇宙射线和人类噪声，它们的起源有时未知。我们介绍了一种使用变量自动编码器的方法来分类不同的噪声类型。变量空间形成了一个紧凑的表示，使得分类变得可追踪。我们分析了具有噪声和无噪声的站点的数据。该方法自动检测并允许我们质量地分开多个事件类型，包括物理风引起的信号，对于两个站点都是如此。>>>
</details></li>
</ul>
<hr>
<h2 id="Recent-Advances-of-Differential-Privacy-in-Centralized-Deep-Learning-A-Systematic-Survey"><a href="#Recent-Advances-of-Differential-Privacy-in-Centralized-Deep-Learning-A-Systematic-Survey" class="headerlink" title="Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey"></a>Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16398">http://arxiv.org/abs/2309.16398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lea Demelius, Roman Kern, Andreas Trügler</li>
<li>for: 这篇论文主要探讨了在机器学习领域中的差分隐私技术，尤其是在中央化深度学习中实现数据隐私保护的方法。</li>
<li>methods: 该论文使用了系统性的文献回顾，对最新的进展和开放问题进行了深入的分析，并对隐私Utility贸易的最新进展、抵御广泛的威胁和攻击、差分隐私生成模型以及emerging应用领域进行了讨论。</li>
<li>results: 该论文提供了一个全面的国家概述，概括了隐私中央化深度学习领域的最新进展和挑战，并对未来发展预测提出了一些建议。<details>
<summary>Abstract</summary>
Differential Privacy has become a widely popular method for data protection in machine learning, especially since it allows formulating strict mathematical privacy guarantees. This survey provides an overview of the state-of-the-art of differentially private centralized deep learning, thorough analyses of recent advances and open problems, as well as a discussion of potential future developments in the field. Based on a systematic literature review, the following topics are addressed: auditing and evaluation methods for private models, improvements of privacy-utility trade-offs, protection against a broad range of threats and attacks, differentially private generative models, and emerging application domains.
</details>
<details>
<summary>摘要</summary>
Diffential Privacy 已经成为机器学习中数据保护的广泛使用方法，特别是它允许提出严格的数学隐私保证。本调查提供了关于批处理隐私中心深度学习的现状报告，包括近期进展和开问题的系统性分析，以及未来发展领域的讨论。根据系统Literature Review，以下话题被讨论：隐私评估和评估方法 для私人模型，隐私利用 trait的改进，对各种威胁和攻击的保护，隐私生成模型，和emerging应用领域。
</details></li>
</ul>
<hr>
<h2 id="Multi-Swap-k-Means"><a href="#Multi-Swap-k-Means" class="headerlink" title="Multi-Swap $k$-Means++"></a>Multi-Swap $k$-Means++</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16384">http://arxiv.org/abs/2309.16384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Beretta, Vincent Cohen-Addad, Silvio Lattanzi, Nikos Parotsidis<br>for: 提高$k$-means clustering问题的解决方案质量methods: 使用$k$-means++搜索分布进行$O(k \log \log k)$次本地搜索，并通过多中心同时交换来提高解决方案质量results: 实现了$9 + \varepsilon$的近似比率，并在多个数据集上显示了重要的实践改进。<details>
<summary>Abstract</summary>
The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \log \log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler (ICML 2019) on several datasets.
</details>
<details>
<summary>摘要</summary>
“Arthur和Vassilvitskii（SODA 2007）的$k$-means++算法是优化流行的$k$-means减少对象的偏好算法，并且知道可以在期望下提供$O(\log k)$-approximation。为了获得更高质量的解决方案，Lattanzi和Sohler（ICML 2019）提出了在$k$-means++采样分布中进行$O(k \log \log k)$次本地搜索，以便实现$c$-approximation的$k$-means减少问题，其中$c$是一个大的绝对常数。在这里，我们扩展和推广他们的本地搜索算法，考虑更大和更复杂的本地搜索邻域，因此可以同时交换多个中心。我们的算法实现了$9 + \varepsilon$的approximation比率，这是本地搜索中的最佳可能性。重要的是，我们示出了我们的方法在许多数据集上实现了重要的实践改进。”
</details></li>
</ul>
<hr>
<h2 id="MHG-GNN-Combination-of-Molecular-Hypergraph-Grammar-with-Graph-Neural-Network"><a href="#MHG-GNN-Combination-of-Molecular-Hypergraph-Grammar-with-Graph-Neural-Network" class="headerlink" title="MHG-GNN: Combination of Molecular Hypergraph Grammar with Graph Neural Network"></a>MHG-GNN: Combination of Molecular Hypergraph Grammar with Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16374">http://arxiv.org/abs/2309.16374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akihiro Kishimoto, Hiroshi Kajino, Masataka Hirose, Junta Fuchiwaki, Indra Priyadarsini, Lisa Hamada, Hajime Shinohara, Daiju Nakano, Seiji Takeda</li>
<li>for: 物料发现中的性质预测</li>
<li>methods:  combine graph neural network (GNN) with Molecular Hypergraph Grammar (MHG)</li>
<li>results: 在多种物料性质预测任务中显示出搭配MHG-GNN的扩展性和可靠性<details>
<summary>Abstract</summary>
Property prediction plays an important role in material discovery. As an initial step to eventually develop a foundation model for material science, we introduce a new autoencoder called the MHG-GNN, which combines graph neural network (GNN) with Molecular Hypergraph Grammar (MHG). Results on a variety of property prediction tasks with diverse materials show that MHG-GNN is promising.
</details>
<details>
<summary>摘要</summary>
物理预测在材料发现中扮演着重要的角色。作为材料科学基础模型的初步阶段，我们介绍了一种新的自适应神经网络（GNN），即MHG-GNN，它将分子 гиперграмmar（MHG）与神经网络相结合。对于多种不同材料的性能预测任务，MHG-GNN表现了良好的承诺。
</details></li>
</ul>
<hr>
<h2 id="Bringing-the-Discussion-of-Minima-Sharpness-to-the-Audio-Domain-a-Filter-Normalised-Evaluation-for-Acoustic-Scene-Classification"><a href="#Bringing-the-Discussion-of-Minima-Sharpness-to-the-Audio-Domain-a-Filter-Normalised-Evaluation-for-Acoustic-Scene-Classification" class="headerlink" title="Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification"></a>Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16369">http://arxiv.org/abs/2309.16369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Milling, Andreas Triantafyllopoulos, Iosif Tsangko, Simon David Noel Rampp, Björn Wolfgang Schuller</li>
<li>for: 这篇论文探讨了深度神经网络中损失函数的锐度与泛化性之间的关系，特别是在音频Scene分类任务上。</li>
<li>methods: 该研究基于二维滤波器Normalized visualizations和一种 derive sharpness measure，对loss函数的不同部分进行了分析。</li>
<li>results: 结果表明，锐度较高的损失函数极 minimum tend to have better generalization performance，尤其是对于来自新设备的Out-of-domain data。此外，选择优化器的选择是主要驱动锐度的变化。<details>
<summary>Abstract</summary>
The correlation between the sharpness of loss minima and generalisation in the context of deep neural networks has been subject to discussion for a long time. Whilst mostly investigated in the context of selected benchmark data sets in the area of computer vision, we explore this aspect for the audio scene classification task of the DCASE2020 challenge data. Our analysis is based on twodimensional filter-normalised visualisations and a derived sharpness measure. Our exploratory analysis shows that sharper minima tend to show better generalisation than flat minima -even more so for out-of-domain data, recorded from previously unseen devices-, thus adding to the dispute about better generalisation capabilities of flat minima. We further find that, in particular, the choice of optimisers is a main driver of the sharpness of minima and we discuss resulting limitations with respect to comparability. Our code, trained model states and loss landscape visualisations are publicly available.
</details>
<details>
<summary>摘要</summary>
TEXT深度神经网络中loss minimum的锐度和泛化关系已经引起了长时间的讨论。我们在DCASE2020挑战数据集上进行了对音频Scene classification任务的分析，使用两维filter-normalized visualization和派生的锐度度量。我们的探索分析表明，锐度较高的 minimum 往往具有更好的泛化性，尤其是在外部数据集和从前未知设备上录制的数据集中。此外，我们发现了优化器的选择是loss minimum的锐度的主要驱动器，并讨论了相关的局限性。我们的代码、训练模型状态和损失地图Visualization都是公共可用的。Here's the translation in Traditional Chinese:TEXT深度神经网络中loss minimum的锯度和泛化关系已经引起了长时间的讨论。我们在DCASE2020挑战数据集上进行了对音频Scene classification任务的分析，使用两维filter-normalized visualization和派生的锯度度量。我们的探索分析显示，锯度较高的 minimum 往往具有更好的泛化性，尤其是在外部数据集和从前未知设备上录制的数据集中。此外，我们发现了优化器的选择是loss minimum的锯度的主要驱动器，并讨论了相关的局限性。我们的代码、训练模型状态和损失地图Visualization都是公共可用的。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Pre-trained-Language-Models-for-Time-Interval-Prediction-in-Text-Enhanced-Temporal-Knowledge-Graphs"><a href="#Leveraging-Pre-trained-Language-Models-for-Time-Interval-Prediction-in-Text-Enhanced-Temporal-Knowledge-Graphs" class="headerlink" title="Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs"></a>Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16357">http://arxiv.org/abs/2309.16357</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duyguislakoglu/temt">https://github.com/duyguislakoglu/temt</a></li>
<li>paper_authors: Duygu Sezen Islakoglu, Mel Chekol, Yannis Velegrakis</li>
<li>for: 本研究旨在提出一种基于语言模型的文本强化时间知识Graph completion（KGC）框架，以利用知识图中的文本描述和时间信息来提高知识图 completion的效果。</li>
<li>methods: 本研究使用了语言模型的参数知识，将文本和时间信息分别处理，并将其相互融合以生成可能性分数。与之前的方法不同，本研究能够 Capture time dependencies and perform inductive inference on unseen entities.</li>
<li>results: 在不同的时间间隔预测和 triple classification 任务中，TEMT 与当前状态域的方法相当竞争，并且在 inductive  Settings中表现出 excel。<details>
<summary>Abstract</summary>
Most knowledge graph completion (KGC) methods learn latent representations of entities and relations of a given graph by mapping them into a vector space. Although the majority of these methods focus on static knowledge graphs, a large number of publicly available KGs contain temporal information stating the time instant/period over which a certain fact has been true. Such graphs are often known as temporal knowledge graphs. Furthermore, knowledge graphs may also contain textual descriptions of entities and relations. Both temporal information and textual descriptions are not taken into account during representation learning by static KGC methods, and only structural information of the graph is leveraged. Recently, some studies have used temporal information to improve link prediction, yet they do not exploit textual descriptions and do not support inductive inference (prediction on entities that have not been seen in training).   We propose a novel framework called TEMT that exploits the power of pre-trained language models (PLMs) for text-enhanced temporal knowledge graph completion. The knowledge stored in the parameters of a PLM allows TEMT to produce rich semantic representations of facts and to generalize on previously unseen entities. TEMT leverages textual and temporal information available in a KG, treats them separately, and fuses them to get plausibility scores of facts. Unlike previous approaches, TEMT effectively captures dependencies across different time points and enables predictions on unseen entities. To assess the performance of TEMT, we carried out several experiments including time interval prediction, both in transductive and inductive settings, and triple classification. The experimental results show that TEMT is competitive with the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
大多数知识图完成（KGC）方法将实体和关系映射到一个维度空间中，以便学习缓存的表示。尽管大多数这些方法专注于静态知识图，但许多公共可用的KG具有时间信息，表示一个特定事实在某个时间点/时间段内是真。这些图被称为temporal知识图。此外，知识图也可能包含实体和关系的文本描述。 static KGC方法不会考虑时间信息和文本描述，只是利用结构信息来学习表示。最近，一些研究使用了时间信息来改进链接预测，但是它们不会利用文本描述，并且不支持推导推理（预测已经在训练中没有看到的实体）。我们提出了一种新的框架called TEMT，它利用预训练语言模型（PLM）来提高文本扩展 temporal knowledge graph completion。TEMT可以利用知识图中的文本和时间信息，将它们分 separetely处理，并将它们融合以获得可能性分数。与先前的方法不同，TEMT可以有效地捕捉不同时间点之间的依赖关系，并允许预测未经训练的实体。为评估TEMT的性能，我们进行了多个实验，包括时间间隔预测、满意度预测和 triple classification。实验结果表明，TEMT与状态之前的方法竞争。
</details></li>
</ul>
<hr>
<h2 id="ShapeDBA-Generating-Effective-Time-Series-Prototypes-using-ShapeDTW-Barycenter-Averaging"><a href="#ShapeDBA-Generating-Effective-Time-Series-Prototypes-using-ShapeDTW-Barycenter-Averaging" class="headerlink" title="ShapeDBA: Generating Effective Time Series Prototypes using ShapeDTW Barycenter Averaging"></a>ShapeDBA: Generating Effective Time Series Prototypes using ShapeDTW Barycenter Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16353">http://arxiv.org/abs/2309.16353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/msd-irimas/shapedba">https://github.com/msd-irimas/shapedba</a></li>
<li>paper_authors: Ali Ismail-Fawaz, Hassan Ismail Fawaz, François Petitjean, Maxime Devanne, Jonathan Weber, Stefano Berretti, Geoffrey I. Webb, Germain Forestier</li>
<li>for: 本研究目的是生成有用的时序数据 exemplars 和 prototypes，并提出了一种新的时序数据平均方法，ShapeDTW Barycentric Average。</li>
<li>methods: 本文使用了ShapeDTW Barycentric Average，一种基于时序相似度度量DTW的新型平均方法，以生成更加有用的时序数据 exemplars 和 prototypes。</li>
<li>results: 根据UCR数据集库中的123个数据集，与k-means减少算法相结合，ShapeDTW Barycentric Average可以达到新的州OF-THE-ARTResultsin Adjusted Rand Index。<details>
<summary>Abstract</summary>
Time series data can be found in almost every domain, ranging from the medical field to manufacturing and wireless communication. Generating realistic and useful exemplars and prototypes is a fundamental data analysis task. In this paper, we investigate a novel approach to generating realistic and useful exemplars and prototypes for time series data. Our approach uses a new form of time series average, the ShapeDTW Barycentric Average. We therefore turn our attention to accurately generating time series prototypes with a novel approach. The existing time series prototyping approaches rely on the Dynamic Time Warping (DTW) similarity measure such as DTW Barycentering Average (DBA) and SoftDBA. These last approaches suffer from a common problem of generating out-of-distribution artifacts in their prototypes. This is mostly caused by the DTW variant used and its incapability of detecting neighborhood similarities, instead it detects absolute similarities. Our proposed method, ShapeDBA, uses the ShapeDTW variant of DTW, that overcomes this issue. We chose time series clustering, a popular form of time series analysis to evaluate the outcome of ShapeDBA compared to the other prototyping approaches. Coupled with the k-means clustering algorithm, and evaluated on a total of 123 datasets from the UCR archive, our proposed averaging approach is able to achieve new state-of-the-art results in terms of Adjusted Rand Index.
</details>
<details>
<summary>摘要</summary>
时序数据可以在各个领域中找到，从医疗领域到制造和无线通信。生成实用和真实的示例和原型是时序数据分析的基本任务。在这篇论文中，我们研究了一种新的方法来生成实用和真实的示例和原型 для时序数据。我们的方法使用了一种新的时序数据平均方法，即ShapeDTW矩阵平均。因此，我们转移我们的注意力于准确地生成时序示例原型的新方法。现有的时序示例原型生成方法基于动态时间戳匹配（DTW）相似度度量，如DTW矩阵平均（DBA）和SoftDBA。这些方法都受到了生成不符合分布的缺陷，主要是因为DTW变体使用的匹配方法无法检测邻域相似性，而是检测绝对相似性。我们提议的方法，ShapeDBA，使用ShapeDTW变体，解决了这个问题。我们选择了时序分组，一种流行的时序分析方法来评估ShapeDBA的结果与其他原型生成方法相比。与k-means分 clustering算法结合，我们在UCAR存档中的总共123个数据集上进行了评估，并实现了新的状态之册 Rand Index的最佳成绩。
</details></li>
</ul>
<hr>
<h2 id="LagrangeBench-A-Lagrangian-Fluid-Mechanics-Benchmarking-Suite"><a href="#LagrangeBench-A-Lagrangian-Fluid-Mechanics-Benchmarking-Suite" class="headerlink" title="LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite"></a>LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16342">http://arxiv.org/abs/2309.16342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tumaer/lagrangebench">https://github.com/tumaer/lagrangebench</a></li>
<li>paper_authors: Artur P. Toshev, Gianluca Galletti, Fabian Fritz, Stefan Adami, Nikolaus A. Adams</li>
<li>for: 这个论文主要是为了探讨 Lagrange 粒子方法在 grid-based PDE 模型中的应用，以及如何使用机器学习来学习 PDE 解。</li>
<li>methods: 论文使用了 Lagrangian 粒子方法生成了七个新的 fluid mechanics 数据集，并提供了一个基于 JAX 的高效 API，以及一些现代训练策略和三种邻居搜索算法。</li>
<li>results: 论文 introduces physical metrics like kinetic energy MSE and Sinkhorn distance to measure the performance of learned surrogates, and provides baseline results using GNNs like GNS and SEGNN.<details>
<summary>Abstract</summary>
Machine learning has been successfully applied to grid-based PDE modeling in various scientific applications. However, learned PDE solvers based on Lagrangian particle discretizations, which are the preferred approach to problems with free surfaces or complex physics, remain largely unexplored. We present LagrangeBench, the first benchmarking suite for Lagrangian particle problems, focusing on temporal coarse-graining. In particular, our contribution is: (a) seven new fluid mechanics datasets (four in 2D and three in 3D) generated with the Smoothed Particle Hydrodynamics (SPH) method including the Taylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break, each of which includes different physics like solid wall interactions or free surface, (b) efficient JAX-based API with various recent training strategies and three neighbor search routines, and (c) JAX implementation of established Graph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally, to measure the performance of learned surrogates we go beyond established position errors and introduce physical metrics like kinetic energy MSE and Sinkhorn distance for the particle distribution. Our codebase is available at https://github.com/tumaer/lagrangebench .
</details>
<details>
<summary>摘要</summary>
机器学习已成功应用于网格基的 Partiall differential equation 模型化在不同科学领域。然而，基于拉格朗日 particulate 方法的学习 PDE 解决方案，这些解决方案更适用于具有自由表面或复杂物理的问题，仍然未得到充分探索。我们介绍了 LagrangeBench，首个为 Lagrangian particulate 问题提供了benchmarking 集成。具体来说，我们的贡献包括：（a） seven 个新的 fluid mechanics 数据集（四个在 2D 和三个在 3D），通过 Smoothed Particle Hydrodynamics (SPH) 方法生成，包括泰勒-绿 Vortex、封闭 Cavity、反 Poiseuille 流和溢流，每个数据集都包含不同的物理学如固体壁面交互或自由表面。（b）高效的 JAX-based API，包括各种最新的训练策略和三种邻居搜索算法。（c） JAX 实现了一些Established Graph Neural Networks (GNNs)，如 GNS 和 SEGNN，以及基线结果。最后，为了评估学习的表现，我们不仅使用了传统的位置错误，还引入了物理指标如动能差分和 Sinkhorn 距离，用于测试 particulate 分布的性能。我们的代码库可以在 https://github.com/tumaer/lagrangebench 上下载。
</details></li>
</ul>
<hr>
<h2 id="EFFL-Egalitarian-Fairness-in-Federated-Learning-for-Mitigating-Matthew-Effect"><a href="#EFFL-Egalitarian-Fairness-in-Federated-Learning-for-Mitigating-Matthew-Effect" class="headerlink" title="EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect"></a>EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16338">http://arxiv.org/abs/2309.16338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiashi Gao, Changwu Huang, Ming Tang, Shin Hwei Tan, Xin Yao, Xuetao Wei</li>
<li>for: The paper aims to address the issue of unequal representation and bias in federated learning (FL) when dealing with heterogeneous datasets from multiple clients.</li>
<li>methods: The proposed Egalitarian Fairness Federated Learning (EFFL) method uses a constrained multi-objective optimization approach to optimize the egalitarian fairness and performance of the global model.</li>
<li>results: The proposed EFFL algorithm outperforms other state-of-the-art FL algorithms in achieving a high-performance global model with enhanced egalitarian fairness among all clients.Here’s the simplified Chinese text for the three information points:</li>
<li>for: 本研究旨在解决 Federated Learning（FL）中处理多客户端数据的不均衡和偏见问题。</li>
<li>methods: 提议的 Egalitarian Fairness Federated Learning（EFFL）方法使用受限多目标优化方法来优化全局模型的 egalitarian fairness 和性能。</li>
<li>results: 提议的 EFFL 算法在实现高性能全局模型的同时，也能够提高所有客户端的 egalitarian fairness。<details>
<summary>Abstract</summary>
Recent advances in federated learning (FL) enable collaborative training of machine learning (ML) models from large-scale and widely dispersed clients while protecting their privacy. However, when different clients' datasets are heterogeneous, traditional FL mechanisms produce a global model that does not adequately represent the poorer clients with limited data resources, resulting in lower accuracy and higher bias on their local data. According to the Matthew effect, which describes how the advantaged gain more advantage and the disadvantaged lose more over time, deploying such a global model in client applications may worsen the resource disparity among the clients and harm the principles of social welfare and fairness. To mitigate the Matthew effect, we propose Egalitarian Fairness Federated Learning (EFFL), where egalitarian fairness refers to the global model learned from FL has: (1) equal accuracy among clients; (2) equal decision bias among clients. Besides achieving egalitarian fairness among the clients, EFFL also aims for performance optimality, minimizing the empirical risk loss and the bias for each client; both are essential for any ML model training, whether centralized or decentralized. We formulate EFFL as a constrained multi-constrained multi-objectives optimization (MCMOO) problem, with the decision bias and egalitarian fairness as constraints and the minimization of the empirical risk losses on all clients as multiple objectives to be optimized. We propose a gradient-based three-stage algorithm to obtain the Pareto optimal solutions within the constraint space. Extensive experiments demonstrate that EFFL outperforms other state-of-the-art FL algorithms in achieving a high-performance global model with enhanced egalitarian fairness among all clients.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose Egalitarian Fairness Federated Learning (EFFL), which aims to achieve egalitarian fairness among clients in two aspects:1. Equal accuracy among clients.2. Equal decision bias among clients.Besides egalitarian fairness, EFFL also pursues performance optimality by minimizing the empirical risk loss and bias for each client. This is essential for any ML model training, whether centralized or decentralized.We formulate EFFL as a constrained multi-constrained multi-objectives optimization (MCMOO) problem, with the decision bias and egalitarian fairness as constraints and the minimization of the empirical risk losses on all clients as multiple objectives to be optimized. To obtain the Pareto optimal solutions within the constraint space, we propose a gradient-based three-stage algorithm.Extensive experiments demonstrate that EFFL outperforms other state-of-the-art FL algorithms in achieving a high-performance global model with enhanced egalitarian fairness among all clients.
</details></li>
</ul>
<hr>
<h2 id="DeepPCR-Parallelizing-Sequential-Operations-in-Neural-Networks"><a href="#DeepPCR-Parallelizing-Sequential-Operations-in-Neural-Networks" class="headerlink" title="DeepPCR: Parallelizing Sequential Operations in Neural Networks"></a>DeepPCR: Parallelizing Sequential Operations in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16318">http://arxiv.org/abs/2309.16318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Federico Danieli, Miguel Sarabia, Xavier Suau, Pau Rodríguez, Luca Zappella</li>
<li>for: 加速深度神经网络的推理和训练</li>
<li>methods: 使用平行化技术以并行执行层次结构中的操作</li>
<li>results: 在多层感知器中实现了高达30倍的前向传播速度增加和高达200倍的反向传播速度增加，以及在 diffusion 模型中实现了更快的训练和生成速度<details>
<summary>Abstract</summary>
Parallelization techniques have become ubiquitous for accelerating inference and training of deep neural networks. Despite this, several operations are still performed in a sequential manner. For instance, the forward and backward passes are executed layer-by-layer, and the output of diffusion models is produced by applying a sequence of denoising steps. This sequential approach results in a computational cost proportional to the number of steps involved, presenting a potential bottleneck as the number of steps increases. In this work, we introduce DeepPCR, a novel algorithm which parallelizes typically sequential operations in order to speed up inference and training of neural networks. DeepPCR is based on interpreting a sequence of $L$ steps as the solution of a specific system of equations, which we recover using the Parallel Cyclic Reduction algorithm. This reduces the complexity of computing the sequential operations from $\mathcal{O}(L)$ to $\mathcal{O}(\log_2L)$, thus yielding a speedup for large $L$. To verify the theoretical lower complexity of the algorithm, and to identify regimes for speedup, we test the effectiveness of DeepPCR in parallelizing the forward and backward pass in multi-layer perceptrons, and reach speedups of up to $30\times$ for the forward and $200\times$ for the backward pass. We additionally showcase the flexibility of DeepPCR by parallelizing training of ResNets with as many as 1024 layers, and generation in diffusion models, enabling up to $7\times$ faster training and $11\times$ faster generation, respectively, when compared to the sequential approach.
</details>
<details>
<summary>摘要</summary>
深度学习模型的推理和训练速度加速技术已经广泛应用。然而，许多操作仍然以序列方式进行，例如层次推理和反向传播。这种序列方式会导致计算成本与操作步骤数直线关系，从而带来计算成本增加的潜在瓶颈。在这项工作中，我们介绍了深度PCR算法，它可以将通常以序列方式进行的操作并行化，以加速深度学习模型的推理和训练。深度PCR基于解释一系列$L$步骤为特定系统方程的解，我们使用并行循环减少算法来解决这些方程。这将计算序列操作的复杂度从 $\mathcal{O}(L)$ 降低到 $\mathcal{O}(\log_2L)$，从而实现大量$L$的速度增加。为了证明算法的理论下界复杂度，以及哪些情况下可以获得加速，我们在多层感知器的前向和反向传播中测试了深度PCR的效果，并达到了最多$30\times$的加速。此外，我们还示cases了深度PCR的灵活性，可以并行训练具有1024层的ResNet模型，以及在Diffusion模型中的生成过程，各自实现了$7\times$快的训练和$11\times$快的生成。
</details></li>
</ul>
<hr>
<h2 id="Astroconformer-The-Prospects-of-Analyzing-Stellar-Light-Curves-with-Transformer-Based-Deep-Learning-Models"><a href="#Astroconformer-The-Prospects-of-Analyzing-Stellar-Light-Curves-with-Transformer-Based-Deep-Learning-Models" class="headerlink" title="Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models"></a>Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16316">http://arxiv.org/abs/2309.16316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Shu Pan, Yuan-Sen Ting, Jie Yu</li>
<li>for: 这个研究是为了使用深度学习方法来探索恒星的振荡和粒子运动，并从light curve中提取有用的信息。</li>
<li>methods: 这个研究使用了一种名为“Astroconformer”的Transformer-based深度学习框架，可以从light curve中捕捉到恒星内部结构和进化状态的信息。</li>
<li>results: 研究发现，在训练数据丰富的情况下，Astroconformer可以实现Surface gravity（log g）的估计，其RMSE为0.017 dex，而在训练数据稀缺的情况下，RMSE可以达0.1 dex。此外，这个模型也超越了传统的K-nearest neighbor-based模型和现有的CNN模型。<details>
<summary>Abstract</summary>
Light curves of stars encapsulate a wealth of information about stellar oscillations and granulation, thereby offering key insights into the internal structure and evolutionary state of stars. Conventional asteroseismic techniques have been largely confined to power spectral analysis, neglecting the valuable phase information contained within light curves. While recent machine learning applications in asteroseismology utilizing Convolutional Neural Networks (CNNs) have successfully inferred stellar attributes from light curves, they are often limited by the local feature extraction inherent in convolutional operations. To circumvent these constraints, we present $\textit{Astroconformer}$, a Transformer-based deep learning framework designed to capture long-range dependencies in stellar light curves. Our empirical analysis, which focuses on estimating surface gravity ($\log g$), is grounded in a carefully curated dataset derived from $\textit{Kepler}$ light curves. These light curves feature asteroseismic $\log g$ values spanning from 0.2 to 4.4. Our results underscore that, in the regime where the training data is abundant, $\textit{Astroconformer}$ attains a root-mean-square-error (RMSE) of 0.017 dex around $\log g \approx 3 $. Even in regions where training data are sparse, the RMSE can reach 0.1 dex. It outperforms not only the K-nearest neighbor-based model ($\textit{The SWAN}$) but also state-of-the-art CNNs. Ablation studies confirm that the efficacy of the models in this particular task is strongly influenced by the size of their receptive fields, with larger receptive fields correlating with enhanced performance. Moreover, we find that the attention mechanisms within $\textit{Astroconformer}$ are well-aligned with the inherent characteristics of stellar oscillations and granulation present in the light curves.
</details>
<details>
<summary>摘要</summary>
星星的光谱Curve包含了许多关于恒星振荡和 granulation的信息，因此可以提供关键的内部结构和演化状态信息。传统的asteroseismic技术主要是通过功率 spectral analysis来分析，忽略了光谱Curve中的价值得 phase information。而最近的机器学习应用在asteroseismology中使用Convolutional Neural Networks (CNNs) 已经成功地从光谱Curve中推断出了星宿属性，但它们frequently受到了 convolutional operation中的本地特征提取的限制。为了缺乏这些限制，我们提出了 $\textit{Astroconformer}$，一种基于Transformer的深度学习框架，可以捕捉stellar light curves中的长距离依赖关系。我们的实验，关注于estersurface gravity（ $\log g$ ）的估算，基于 $\textit{Kepler}$ 光谱Curve的精心划分 datasets。这些光谱Curve的asteroseismic $\log g$ 值覆盖了0.2至4.4之间。我们的结果表明，当训练数据充足时， $\textit{Astroconformer}$ 在 $\log g \approx 3 $ 的 regime内具有根圆弧误差（RMSE）为0.017 dex。甚至在训练数据稀缺的地方，RMSE可以达到0.1 dex。它不仅超过了基于K-nearest neighbor（ $\textit{The SWAN}$）的模型，还超过了当前的 state-of-the-art CNNs。归并学习表明，模型在这个特定任务中的 efficacy 强烈受到了其 reception field 的大小的影响，大 reception field 与更高的表现相关。此外，我们发现 $\textit{Astroconformer}$ 中的注意机制与stellar oscillations和 granulation在光谱Curve中的特点相吻合。
</details></li>
</ul>
<hr>
<h2 id="A-Primer-on-Bayesian-Neural-Networks-Review-and-Debates"><a href="#A-Primer-on-Bayesian-Neural-Networks-Review-and-Debates" class="headerlink" title="A Primer on Bayesian Neural Networks: Review and Debates"></a>A Primer on Bayesian Neural Networks: Review and Debates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16314">http://arxiv.org/abs/2309.16314</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/konstantinos-p/Bayesian-Neural-Networks-Reading-List">https://github.com/konstantinos-p/Bayesian-Neural-Networks-Reading-List</a></li>
<li>paper_authors: Julyan Arbel, Konstantinos Pitas, Mariia Vladimirova, Vincent Fortuin</li>
<li>for: 本文旨在介绍权重神经网络（BNN）的基本概念和 bayesian 统计学的结合，以提高神经网络的预测性能和可解性。</li>
<li>methods: 本文使用 bayesian 统计学和神经网络的组合来解决神经网络的困难问题，如预测过度自信、可解性和攻击难免性等。</li>
<li>results: 本文提供了一种系统性的介绍，涵盖了 bayesian 统计学和神经网络之间的相互作用，以及在实际应用中的考虑因素。  additionally, the paper explores advanced topics in BNN research and acknowledges ongoing debates and controversies in the field.<details>
<summary>Abstract</summary>
Neural networks have achieved remarkable performance across various problem domains, but their widespread applicability is hindered by inherent limitations such as overconfidence in predictions, lack of interpretability, and vulnerability to adversarial attacks. To address these challenges, Bayesian neural networks (BNNs) have emerged as a compelling extension of conventional neural networks, integrating uncertainty estimation into their predictive capabilities.   This comprehensive primer presents a systematic introduction to the fundamental concepts of neural networks and Bayesian inference, elucidating their synergistic integration for the development of BNNs. The target audience comprises statisticians with a potential background in Bayesian methods but lacking deep learning expertise, as well as machine learners proficient in deep neural networks but with limited exposure to Bayesian statistics. We provide an overview of commonly employed priors, examining their impact on model behavior and performance. Additionally, we delve into the practical considerations associated with training and inference in BNNs.   Furthermore, we explore advanced topics within the realm of BNN research, acknowledging the existence of ongoing debates and controversies. By offering insights into cutting-edge developments, this primer not only equips researchers and practitioners with a solid foundation in BNNs, but also illuminates the potential applications of this dynamic field. As a valuable resource, it fosters an understanding of BNNs and their promising prospects, facilitating further advancements in the pursuit of knowledge and innovation.
</details>
<details>
<summary>摘要</summary>
neural networks 已经在不同的问题领域 достичь了很高的表现，但它们的广泛应用受到了内在的限制，如预测时的过度自信、难以解释性和针对攻击的敏感性。为了解决这些挑战， Bayesian neural networks（BNNs）作为传统神经网络的吸收性扩展，将不确定性估计integrated into its predictive capabilities。这个全面的指南将为拥有bayesian方法背景的统计学家提供系统性的引入，以及擅长深度学习的机器学习专家，尽管有限的bayesian统计知识。我们将详细介绍常用的 prior，并 analyze its impact on model behavior and performance。此外，我们还会讨论BNNs在训练和推理中的实际问题。此外，我们还会探讨BNN的高级主题，包括正在进行的辩论和争议。通过这个指南，您将获得BNN的坚实基础知识，并了解这个动态领域的潜在应用。作为一个有价值的资源，这个指南将促进BNN的理解和推动知识和创新的进步。
</details></li>
</ul>
<hr>
<h2 id="3D-Mol-A-Novel-Contrastive-Learning-Framework-for-Molecular-Property-Prediction-with-3D-Information"><a href="#3D-Mol-A-Novel-Contrastive-Learning-Framework-for-Molecular-Property-Prediction-with-3D-Information" class="headerlink" title="3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information"></a>3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.17366">http://arxiv.org/abs/2309.17366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taojie Kuang, Yiming Ren, Zhixiang Ren</li>
<li>for: 预测药物的物理性质，提高早期屏选和优化药物候选者。</li>
<li>methods: 提出了一种基于深度学习的3D结构基本模型方法，使用三个几何图来提取3D特征，并使用对比学习来预训练模型。</li>
<li>results: 在7个标准测试 benchmark 上比较了3D-Mol 与多种现有基eline（SOTA），在5个标准测试 benchmark 上表现出色。<details>
<summary>Abstract</summary>
Molecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art (SOTA) baselines on 7 benchmarks and demonstrate our outstanding performance in 5 benchmarks.
</details>
<details>
<summary>摘要</summary>
молекулярная свойство предсказание предлагает эффективный и эффективный подход для раннего скрининга и оптимизации кандидатов на лекарства. хотя методы на основе глубокого обучения сделали заметный прогресс, большинство существующих работ еще не полностью используют информацию о 3D-пространстве. это может привести к ситуации, когда один молекулярный представление отображает несколько реальных молекул. для решения этих проблем мы предлагаем новый метод 3D-Mol, который использует структурную моделирование молекул на основе трехмерных графиков. кроме того, мы используем 20M немаркированных данных для предварительного обучения нашего модели с помощью обучения с contraste. мы считаем конфигурации с одинаковой топологической структурой положительными парами, а противоположные конфигурации - отрицательными парами, а вес определяется с помощью разницы между конфигурациями. мы сравниваем 3D-Mol с различными стандартными базами на 7 benchmarks и демонстрируем нашу выдающуюся производительность на 5 benchmarks.
</details></li>
</ul>
<hr>
<h2 id="CasIL-Cognizing-and-Imitating-Skills-via-a-Dual-Cognition-Action-Architecture"><a href="#CasIL-Cognizing-and-Imitating-Skills-via-a-Dual-Cognition-Action-Architecture" class="headerlink" title="CasIL: Cognizing and Imitating Skills via a Dual Cognition-Action Architecture"></a>CasIL: Cognizing and Imitating Skills via a Dual Cognition-Action Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16299">http://arxiv.org/abs/2309.16299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Chen, Ze Ji, Shuyang Liu, Jing Huo, Yiyu Chen, Yang Gao</li>
<li>for: 本研究旨在提高机器人的长期任务完成能力，使其能够有效地模仿人类专家技能。</li>
<li>methods: 该研究提出了一种基于人类认知优先的技能学习框架（CasIL），通过人机交互来帮助机器人从视觉示例中学习重要的技能。</li>
<li>results: 实验结果表明， compared to其他方法， CasIL在多种长期任务中的机器人技能模仿能力具有竞争力和可靠性。<details>
<summary>Abstract</summary>
Enabling robots to effectively imitate expert skills in longhorizon tasks such as locomotion, manipulation, and more, poses a long-standing challenge. Existing imitation learning (IL) approaches for robots still grapple with sub-optimal performance in complex tasks. In this paper, we consider how this challenge can be addressed within the human cognitive priors. Heuristically, we extend the usual notion of action to a dual Cognition (high-level)-Action (low-level) architecture by introducing intuitive human cognitive priors, and propose a novel skill IL framework through human-robot interaction, called Cognition-Action-based Skill Imitation Learning (CasIL), for the robotic agent to effectively cognize and imitate the critical skills from raw visual demonstrations. CasIL enables both cognition and action imitation, while high-level skill cognition explicitly guides low-level primitive actions, providing robustness and reliability to the entire skill IL process. We evaluated our method on MuJoCo and RLBench benchmarks, as well as on the obstacle avoidance and point-goal navigation tasks for quadrupedal robot locomotion. Experimental results show that our CasIL consistently achieves competitive and robust skill imitation capability compared to other counterparts in a variety of long-horizon robotic tasks.
</details>
<details>
<summary>摘要</summary>
启用机器人效果地模仿专家技能，如行走、抓取和更多的任务，是长期挑战。现有的机器人学习（IL）方法仍然在复杂任务中表现不佳。在这篇论文中，我们考虑了如何通过人类认知优先级来解决这个挑战。我们准确地将行为扩展到高级认知（高水平）和低级动作（低水平）的双核心架构中，并提出了一种基于人类认知优先级的新型技能IL框架，称为认知动作基于技能学习（CasIL）。这种框架使得机器人代理人能够有效地认识和模仿从原始视觉示例中的关键技能。在高级认知指导低级动作的情况下，CasIL实现了both cognition和action imitation，提供了robustness和可靠性 для整个技能IL过程。我们在MuJoCo和RLBench标准吨量上进行了测试，以及 quadrupedal robot locomotion的障碍物避免和点目标导航任务。实验结果表明，我们的CasIL在多种长期机器人任务中具有竞争力和可靠的技能模仿能力。
</details></li>
</ul>
<hr>
<h2 id="A-framework-for-paired-sample-hypothesis-testing-for-high-dimensional-data"><a href="#A-framework-for-paired-sample-hypothesis-testing-for-high-dimensional-data" class="headerlink" title="A framework for paired-sample hypothesis testing for high-dimensional data"></a>A framework for paired-sample hypothesis testing for high-dimensional data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16274">http://arxiv.org/abs/2309.16274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Bargiotas, Argyris Kalogeratos, Nicolas Vayatis</li>
<li>for: This paper proposes a new approach to multidimensional paired-sample testing, which can handle numerous features and provide accurate results.</li>
<li>methods: The proposed approach uses scoring functions produced by decision rules defined by the perpendicular bisecting hyperplanes of the line segments connecting each pair of instances. The optimal scoring function is obtained by the pseudomedian of those rules, which is estimated using the Hodges-Lehmann estimator.</li>
<li>results: The proposed approach is shown to have substantial performance gains in testing accuracy compared to traditional multivariate and multiple testing methods, while also providing estimates of each feature’s contribution to the final result.<details>
<summary>Abstract</summary>
The standard paired-sample testing approach in the multidimensional setting applies multiple univariate tests on the individual features, followed by p-value adjustments. Such an approach suffers when the data carry numerous features. A number of studies have shown that classification accuracy can be seen as a proxy for two-sample testing. However, neither theoretical foundations nor practical recipes have been proposed so far on how this strategy could be extended to multidimensional paired-sample testing. In this work, we put forward the idea that scoring functions can be produced by the decision rules defined by the perpendicular bisecting hyperplanes of the line segments connecting each pair of instances. Then, the optimal scoring function can be obtained by the pseudomedian of those rules, which we estimate by extending naturally the Hodges-Lehmann estimator. We accordingly propose a framework of a two-step testing procedure. First, we estimate the bisecting hyperplanes for each pair of instances and an aggregated rule derived through the Hodges-Lehmann estimator. The paired samples are scored by this aggregated rule to produce a unidimensional representation. Second, we perform a Wilcoxon signed-rank test on the obtained representation. Our experiments indicate that our approach has substantial performance gains in testing accuracy compared to the traditional multivariate and multiple testing, while at the same time estimates each feature's contribution to the final result.
</details>
<details>
<summary>摘要</summary>
traditional multivariate and multiple testing方法在多维度设定下存在一些缺陷，特别是当数据集具有大量特征时。一些研究表明，分类准确率可以作为两个样本测试的代理。然而，这种策略的理论基础和实践方法尚未得到过详细的探讨。在这项工作中，我们提出了一种思路，即可以通过定义每对实例之间的垂线段的截距线性函数来生成评分函数。然后，我们可以通过拓展自然的方式来获得最佳评分函数，这里我们使用拓展了HODGES-LEHMANN estimator来进行估计。我们因此提出了一种两步测试方法。第一步是估计每对实例之间的截距线性函数，并使用这些函数 derive一个总评分函数。然后，我们使用这个总评分函数对paired samples进行分类，并生成一个一维ensional的表示。第二步是在 obtained representation上perform Wilcoxon signed-rank test。我们的实验表明，我们的方法在测试准确率方面具有substantial的性能提升，同时能够计算每个特征对最终结果的贡献。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Network-Data-Analytics-Framework-for-B5G-Network-Automation-Design-and-Implementation"><a href="#Hierarchical-Network-Data-Analytics-Framework-for-B5G-Network-Automation-Design-and-Implementation" class="headerlink" title="Hierarchical Network Data Analytics Framework for B5G Network Automation: Design and Implementation"></a>Hierarchical Network Data Analytics Framework for B5G Network Automation: Design and Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16269">http://arxiv.org/abs/2309.16269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youbin Jeon, Sangheon Pack</li>
<li>for: 支持新服务的更flexible和elastic方式，帮助解决5G模块化网络函数管理中的复杂性</li>
<li>methods: 提出了一个分层网络数据分析框架（H-NDAF），将推理任务分配给多个叶子网络数据分析函数（Leaf NWDAF），训练任务进行根网络数据分析函数（Root NWDAF）进行</li>
<li>results: 通过使用开源软件（i.e., free5GC）进行广泛的 simulate结果表明，H-NDAF可以提供充分准确的分析结果，并且比 convential NWDAF更快地提供分析结果Here is the same information in Simplified Chinese:</li>
<li>for: 支持新服务的更flexible和elastic方式，帮助解决5G模块化网络函数管理中的复杂性</li>
<li>methods: 提出了一个分层网络数据分析框架（H-NDAF），将推理任务分配给多个叶子网络数据分析函数（Leaf NWDAF），训练任务进行根网络数据分析函数（Root NWDAF）进行</li>
<li>results: 通过使用开源软件（i.e., free5GC）进行广泛的 simulate结果表明，H-NDAF可以提供充分准确的分析结果，并且比 convential NWDAF更快地提供分析结果<details>
<summary>Abstract</summary>
5G introduced modularized network functions (NFs) to support emerging services in a more flexible and elastic manner. To mitigate the complexity in such modularized NF management, automated network operation and management are indispensable, and thus the 3rd generation partnership project (3GPP) has introduced a network data analytics function (NWDAF). However, a conventional NWDAF needs to conduct both inference and training tasks, and thus it is difficult to provide the analytics results to NFs in a timely manner for an increased number of analytics requests. In this article, we propose a hierarchical network data analytics framework (H-NDAF) where inference tasks are distributed to multiple leaf NWDAFs and training tasks are conducted at the root NWDAF. Extensive simulation results using open-source software (i.e., free5GC) demonstrate that H-NDAF can provide sufficiently accurate analytics and faster analytics provision time compared to the conventional NWDAF.
</details>
<details>
<summary>摘要</summary>
5G 引入模块化网络功能（NF）以支持出现的服务更加灵活和弹性。为了减少这些模块化 NF 的管理复杂性，自动化网络运维和管理是必要的，因此3GPP 引入了网络数据分析功能（NWDAF）。然而，传统的 NWDAF 需要同时进行推理和训练任务，因此难以在增加数据分析请求后提供分析结果。在本文中，我们提议一种层次网络数据分析框架（H-NDAF），其中推理任务被分配到多个叶 NWDAF，而训练任务则在根 NWDAF 中进行。经过大量的 simulations 结果，我们发现 H-NDAF 可以提供充分的准确性和更快的分析结果提供时间，相比传统的 NWDAF。
</details></li>
</ul>
<hr>
<h2 id="Context-Based-Tweet-Engagement-Prediction"><a href="#Context-Based-Tweet-Engagement-Prediction" class="headerlink" title="Context-Based Tweet Engagement Prediction"></a>Context-Based Tweet Engagement Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03147">http://arxiv.org/abs/2310.03147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/jovan_ns/2020recsystwitter">https://gitlab.com/jovan_ns/2020recsystwitter</a></li>
<li>paper_authors: Jovan Jeromela</li>
<li>for:  investigate how well context alone may be used to predict tweet engagement likelihood</li>
<li>methods: employ the Spark engine on TU Wien’s Little Big Data Cluster to create scalable data preprocessing, feature engineering, feature selection, and machine learning pipelines, and manually create just under 200 additional features to describe tweet context</li>
<li>results: features describing users’ prior engagement history and the popularity of hashtags and links in the tweet were the most informative, and factors such as the prediction algorithm, training dataset size, training dataset sampling method, and feature selection significantly affect the results, with context-based models underperforming in terms of the RCE score compared to content-only models and models developed by the Challenge winners.<details>
<summary>Abstract</summary>
Twitter is currently one of the biggest social media platforms. Its users may share, read, and engage with short posts called tweets. For the ACM Recommender Systems Conference 2020, Twitter published a dataset around 70 GB in size for the annual RecSys Challenge. In 2020, the RecSys Challenge invited participating teams to create models that would predict engagement likelihoods for given user-tweet combinations. The submitted models predicting like, reply, retweet, and quote engagements were evaluated based on two metrics: area under the precision-recall curve (PRAUC) and relative cross-entropy (RCE).   In this diploma thesis, we used the RecSys 2020 Challenge dataset and evaluation procedure to investigate how well context alone may be used to predict tweet engagement likelihood. In doing so, we employed the Spark engine on TU Wien's Little Big Data Cluster to create scalable data preprocessing, feature engineering, feature selection, and machine learning pipelines. We manually created just under 200 additional features to describe tweet context.   The results indicate that features describing users' prior engagement history and the popularity of hashtags and links in the tweet were the most informative. We also found that factors such as the prediction algorithm, training dataset size, training dataset sampling method, and feature selection significantly affect the results. After comparing the best results of our context-only prediction models with content-only models and with models developed by the Challenge winners, we identified that the context-based models underperformed in terms of the RCE score. This work thus concludes by situating this discrepancy and proposing potential improvements to our implementation, which is shared in a public git repository.
</details>
<details>
<summary>摘要</summary>
推特是目前最大的社交媒体平台之一，其用户可以分享、阅读和参与短消息 called tweets。2020年ACM推荐系统会议上，推特发布了约70GB的数据集，并邀请参与者们创建模型，以预测给定用户-消息组合的参与可能性。提交的模型包括like、回复、转推和引用参与的预测都会被评估基于两个指标：精度-回归曲线面积（PRAUC）和相对杂化率（RCE）。在本毕业论文中，我们使用2020年RecSys挑战的数据集和评估方法，以 investigate how well context alone may be used to predict tweet engagement likelihood。我们使用TU Wien的Little Big Data Cluster上的Spark引擎，创建了可扩展的数据处理、工程、特征选择和机器学习管道。我们手动创建了约200个特征来描述消息上下文。结果显示，用户的前一次参与历史和消息中的话题和链接的流行程度是最有用的特征。我们还发现，预测算法、训练数据集大小、训练数据集采样方法和特征选择会影响结果。在与挑战赛得奖者的模型进行比较后，我们发现context-only模型在RCE指标上表现较差。这项工作因此结束，并提出了可能的改进方案，并在公共Git存储库中分享。
</details></li>
</ul>
<hr>
<h2 id="Max-Sliced-Mutual-Information"><a href="#Max-Sliced-Mutual-Information" class="headerlink" title="Max-Sliced Mutual Information"></a>Max-Sliced Mutual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16200">http://arxiv.org/abs/2309.16200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dor Tsur, Ziv Goldfeld, Kristjan Greenewald</li>
<li>for: 这篇论文的目的是量化高维Random Variable之间的依赖关系，以便进行统计学学习和推断。</li>
<li>methods: 该论文使用了一种可扩展的信息理论方法，称为最大剖分协同信息（mSMI），它等于高维变量的低维投影上的最大共同信息。mSMI在Gaussian情况下退化为CCA。这种方法可以快速计算和可扩展地估算高维变量之间的依赖关系，同时也可以捕捉数据中复杂的依赖关系。</li>
<li>results: 该论文的实验结果表明，mSMI在独立测试、多视图学习、公平性检测和生成模型中具有优异表现，并且在计算量方面具有明显的优势。<details>
<summary>Abstract</summary>
Quantifying the dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast computation and scalable estimation from samples. We show that mSMI retains favorable structural properties of Shannon's mutual information, like variational forms and identification of independence. We then study statistical estimation of mSMI, propose an efficiently computable neural estimator, and couple it with formal non-asymptotic error bounds. We present experiments that demonstrate the utility of mSMI for several tasks, encompassing independence testing, multi-view representation learning, algorithmic fairness, and generative modeling. We observe that mSMI consistently outperforms competing methods with little-to-no computational overhead.
</details>
<details>
<summary>摘要</summary>
高维 Random variable 之间的关系衡量是统计学学习和推断中的中心问题。两种经典方法是Canonical correlation analysis (CCA)，它可以找到最大相关的投影后的原始变量，以及Shannon的共同信息 (mutual information)，它是一种通用的关系度量，同时也能捕捉高阶关系。但是CCA只能考虑线性关系，可能不够 для某些应用，而共同信息则在高维时常不可计算或估计。这项工作提出了一种可扩展的信息理论基于CCA的方法，称为最大剖分共同信息 (mSMI)。mSMI等于高维变量的低维投影中的最大共同信息，在Gaussian情况下降到了CCA。它同时具有了两种方法的优点：能够捕捉数据中的复杂关系，并且可以快速计算和可扩展地估计。我们证明了mSMI保持了共同信息的有利结构性质，如变量形式和独立性识别。然后，我们研究了mSMI的统计估计，提出了一种高效计算的神经网络估计器，并与非假顺序 bound 相结合。我们在几个任务上进行了实验，包括独立性测试、多视图学习、算法公平和生成模型。我们发现mSMI在这些任务上一般性能更高，而且具有微不足的计算开销。
</details></li>
</ul>
<hr>
<h2 id="Stackelberg-Batch-Policy-Learning"><a href="#Stackelberg-Batch-Policy-Learning" class="headerlink" title="Stackelberg Batch Policy Learning"></a>Stackelberg Batch Policy Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16188">http://arxiv.org/abs/2309.16188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhuo Zhou, Annie Qu</li>
<li>for: 批处理学习（Batch Reinforcement Learning）任务，lacking exhaustive exploration。</li>
<li>methods: 采用游戏理论视角，将策略学习图表化为一个两player总和游戏，采用StackelbergLearner算法，其中领导者player更新基于总导数据，而追随者player进行个体更新和保证转移逻辑一致。</li>
<li>results: 提供实例 dependent regret bound，证明StackelbergLearner算法可以学习一个最佳尝试策略，可以与任何比较器策略进行竞争，无需数据覆盖和强 функ数据近似条件。通过广泛的实验，发现StackelbergLearner算法在批处理RL benchmark和实际数据上表现良好或更好。<details>
<summary>Abstract</summary>
Batch reinforcement learning (RL) defines the task of learning from a fixed batch of data lacking exhaustive exploration. Worst-case optimality algorithms, which calibrate a value-function model class from logged experience and perform some type of pessimistic evaluation under the learned model, have emerged as a promising paradigm for batch RL. However, contemporary works on this stream have commonly overlooked the hierarchical decision-making structure hidden in the optimization landscape. In this paper, we adopt a game-theoretical viewpoint and model the policy learning diagram as a two-player general-sum game with a leader-follower structure. We propose a novel stochastic gradient-based learning algorithm: StackelbergLearner, in which the leader player updates according to the total derivative of its objective instead of the usual individual gradient, and the follower player makes individual updates and ensures transition-consistent pessimistic reasoning. The derived learning dynamic naturally lends StackelbergLearner to a game-theoretic interpretation and provides a convergence guarantee to differentiable Stackelberg equilibria. From a theoretical standpoint, we provide instance-dependent regret bounds with general function approximation, which shows that our algorithm can learn a best-effort policy that is able to compete against any comparator policy that is covered by batch data. Notably, our theoretical regret guarantees only require realizability without any data coverage and strong function approximation conditions, e.g., Bellman closedness, which is in contrast to prior works lacking such guarantees. Through comprehensive experiments, we find that our algorithm consistently performs as well or better as compared to state-of-the-art methods in batch RL benchmark and real-world datasets.
</details>
<details>
<summary>摘要</summary>
批量强化学习（RL）定义为从固定批量数据中学习，缺乏完整的探索。最坏情况优化算法，它们从日志体验中拟合值函数模型类型，并在学习后进行一种类型的悲观评估。在这篇论文中，我们采用了游戏论视角，将策略学习图表作为两个玩家的通用和总和游戏，其中一个是领导者，另一个是追随者。我们提出了一种新的随机梯度学习算法：StackelbergLearner，其中领导者玩家更新根据总对象的梯度而不是各个梯度，而追随者玩家进行个人更新，并确保转移逻辑一致。这种学习动态自然地具有游戏论视角，并提供了对分解 Stackelberg 平衡的启发性证明。从理论上看，我们提供了实例特定的 regret bound，证明我们的算法可以学习一个最大努力策略，可以与任何比较器策略进行竞争，这些策略只需要涵盖批量数据中的一部分。值得注意的是，我们的理论 regret 保证只需要 realizability 和 strong function approximation 条件，而不需要数据覆盖和强函数approximation 条件，这与先前的方法不同。通过对比periment，我们发现我们的算法在批量 RL 标准测试数据集和实际数据中表现了良好的性能。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Sampling-and-Validation-of-Machine-Learning-Parameterizations-in-Climate-Models"><a href="#Systematic-Sampling-and-Validation-of-Machine-Learning-Parameterizations-in-Climate-Models" class="headerlink" title="Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models"></a>Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16177">http://arxiv.org/abs/2309.16177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jerrylin96/ClimScale">https://github.com/jerrylin96/ClimScale</a></li>
<li>paper_authors: Jerry Lin, Sungduk Yu, Tom Beucler, Pierre Gentine, David Walling, Mike Pritchard</li>
<li>for: 这个论文主要写于 hybrid physics-machine learning（ML）气象模拟中进展的限制，特别是在同时获得可行性和精度的 coupled（在线）模拟方面。</li>
<li>methods: 论文使用的方法包括评估多种机器学习（ML）参数化方法的在线模拟效果，以及对这些方法的评估和优化。</li>
<li>results: 研究发现，在线模拟中的模型性能可以通过添加内存、温度湿度输入变换和额外输入变量进行改进。同时，研究还发现了在线模拟错误的大量变化和在线vs. 离线错误统计的不一致现象。这些结果表明，需要评估数百个候选机器学习模型，以检测参数设计选择的效果。<details>
<summary>Abstract</summary>
Progress in hybrid physics-machine learning (ML) climate simulations has been limited by the difficulty of obtaining performant coupled (i.e. online) simulations. While evaluating hundreds of ML parameterizations of subgrid closures (here of convection and radiation) offline is straightforward, online evaluation at the same scale is technically challenging. Our software automation achieves an order-of-magnitude larger sampling of online modeling errors than has previously been examined. Using this, we evaluate the hybrid climate model performance and define strategies to improve it. We show that model online performance improves when incorporating memory, a relative humidity input feature transformation, and additional input variables. We also reveal substantial variation in online error and inconsistencies between offline vs. online error statistics. The implication is that hundreds of candidate ML models should be evaluated online to detect the effects of parameterization design choices. This is considerably more sampling than tends to be reported in the current literature.
</details>
<details>
<summary>摘要</summary>
“ hybrid physics-machine learning（ML）气象模拟的进步受到了在线（同时）模拟的困难所限制。虽然可以轻松地在离线环境中评估数百个ML参数化的子网格闭合（如风化和辐射），但在同样的大小级别上进行在线评估是技术上困难的。我们的软件自动化实现了在线模型评估中的样本增加，相比之前的评估，样本数量增加了一个数量级。使用这些样本，我们评估了混合气象模型的性能，并定义了改进策略。我们发现，在线模型性能会提高，当将记忆、湿度输入特征变换和额外输入变量添加到模型中时。我们还发现了在线错误的重大变化和在离线vs在线错误统计之间的不一致。这表明需要评估数百个候选机器学习模型，这比现有文献报道的评估范围更多。”
</details></li>
</ul>
<hr>
<h2 id="Distill-to-Delete-Unlearning-in-Graph-Networks-with-Knowledge-Distillation"><a href="#Distill-to-Delete-Unlearning-in-Graph-Networks-with-Knowledge-Distillation" class="headerlink" title="Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation"></a>Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16173">http://arxiv.org/abs/2309.16173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Sinha, Murari Mandal, Mohan Kankanhalli</li>
<li>for:  delete information from pre-trained graph neural network (GNN) to comply with data protection regulations and reduce carbon footprint</li>
<li>methods:  knowledge distillation, model-agnostic approach, dividing and marking complete graph knowledge for retention and deletion, using response-based soft targets and feature-based node embedding, minimizing KL divergence</li>
<li>results:  surpasses existing methods in various real-world graph datasets by up to $43.1%$ (AUC) in edge and node unlearning tasks, better efficiency, better performance in removing target elements, preservation of performance for the retained elements, zero overhead costs, surpasses state-of-the-art GNNDelete in AUC by $2.4%$, improves membership inference ratio by $+1.3$, requires $10.2\times10^6$ fewer FLOPs per forward pass and up to $\mathbf{3.2}\times$ faster.Here’s the Chinese version:</li>
<li>for:  delete信息从预训练的图 neural network (GNN) 以遵循数据保护法规和减少碳脚印</li>
<li>methods:  knowledge distillation, 模型无关方法, 将完整的图知识分成并标记为保留和删除, 使用响应基于软目标和特征基于节点嵌入, 最小化KL偏差</li>
<li>results:  surpasses 现有方法在各种实际图数据集中 by up to $43.1%$ (AUC) 的边和节点解启 зада务, 更好的效率, 更好地 removing 目标元素, 保留元素性能的 preserved, 零开销成本, surpasses 状态艺术 GNNDelete 的 AUC by $2.4%$, improves membership inference ratio by $+1.3$, requires $10.2\times10^6$  fewer FLOPs per forward pass and up to $\mathbf{3.2}\times$ faster.<details>
<summary>Abstract</summary>
Graph unlearning has emerged as a pivotal method to delete information from a pre-trained graph neural network (GNN). One may delete nodes, a class of nodes, edges, or a class of edges. An unlearning method enables the GNN model to comply with data protection regulations (i.e., the right to be forgotten), adapt to evolving data distributions, and reduce the GPU-hours carbon footprint by avoiding repetitive retraining. Existing partitioning and aggregation-based methods have limitations due to their poor handling of local graph dependencies and additional overhead costs. More recently, GNNDelete offered a model-agnostic approach that alleviates some of these issues. Our work takes a novel approach to address these challenges in graph unlearning through knowledge distillation, as it distills to delete in GNN (D2DGN). It is a model-agnostic distillation framework where the complete graph knowledge is divided and marked for retention and deletion. It performs distillation with response-based soft targets and feature-based node embedding while minimizing KL divergence. The unlearned model effectively removes the influence of deleted graph elements while preserving knowledge about the retained graph elements. D2DGN surpasses the performance of existing methods when evaluated on various real-world graph datasets by up to $43.1\%$ (AUC) in edge and node unlearning tasks. Other notable advantages include better efficiency, better performance in removing target elements, preservation of performance for the retained elements, and zero overhead costs. Notably, our D2DGN surpasses the state-of-the-art GNNDelete in AUC by $2.4\%$, improves membership inference ratio by $+1.3$, requires $10.2\times10^6$ fewer FLOPs per forward pass and up to $\mathbf{3.2}\times$ faster.
</details>
<details>
<summary>摘要</summary>
“图гра推断学（Graph Neural Network，GNN）的忘记（unlearning）技术已经成为了练网图模型（pre-trained graph neural network）中删除信息的重要方法。可以删除节点、类型节点、边或类型边。忘记方法可以让GNN模型遵循数据保护法规（如“忘记权”），适应数据分布的变化，并减少GPU时间的碳脚印迹（避免重复 retraining）。现有的分区和聚合方法有限制，因为它们对本地图像依赖不善，并且带有额外成本。更新的GNNDelete提供了一种模型无关的方法，解决了一些这些问题。我们的工作采用了一种新的方法来解决图гра忘记的挑战，通过知识储存（knowledge distillation）来忘记（D2DGN）。这是一种模型无关的储存框架，Complete graph knowledge 被分解并标记为保留和删除。它通过响应式软目标和特征基于节点嵌入进行储存，并最小化KL散度。被忘记的模型可以有效地减少被删除图像元素的影响，保留保留图像元素的知识。D2DGN在多种实际图像Dataset上评估得到了比GNNDelete更高的表现，最高达到$43.1\%$（AUC）的边和节点忘记任务。其他优点包括更高的效率、更好的目标元素删除、保留元素性能的保留、零额外成本。值得一提的是，我们的D2DGN在AUC方面比GNNDelete提高了$2.4\%$, 提高了成员推断率$+1.3$, 需要$10.2\times10^6$ fewer FLOPs per forward pass和${\mathbf{3.2}\times$快。”
</details></li>
</ul>
<hr>
<h2 id="A-Spectral-Approach-for-Learning-Spatiotemporal-Neural-Differential-Equations"><a href="#A-Spectral-Approach-for-Learning-Spatiotemporal-Neural-Differential-Equations" class="headerlink" title="A Spectral Approach for Learning Spatiotemporal Neural Differential Equations"></a>A Spectral Approach for Learning Spatiotemporal Neural Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16131">http://arxiv.org/abs/2309.16131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingtao Xia, Xiangting Li, Qijing Shen, Tom Chou</li>
<li>for:  Computationally reconstructing differential equations (DEs) from observational data to gain insights into underlying causative mechanisms.</li>
<li>methods:  Using spectral expansions in space to learn spatiotemporal DEs, without relying on spatial discretization, allowing for long-range, nonlocal spatial interactions on unbounded domains.</li>
<li>results:  The proposed spectral neural DE learning approach is shown to be as accurate as some of the latest machine learning approaches for learning PDEs operating on bounded domains, and can be applied to a larger class of problems including unbounded DEs and integro-differential equations.<details>
<summary>Abstract</summary>
Rapidly developing machine learning methods has stimulated research interest in computationally reconstructing differential equations (DEs) from observational data which may provide additional insight into underlying causative mechanisms. In this paper, we propose a novel neural-ODE based method that uses spectral expansions in space to learn spatiotemporal DEs. The major advantage of our spectral neural DE learning approach is that it does not rely on spatial discretization, thus allowing the target spatiotemporal equations to contain long range, nonlocal spatial interactions that act on unbounded spatial domains. Our spectral approach is shown to be as accurate as some of the latest machine learning approaches for learning PDEs operating on bounded domains. By developing a spectral framework for learning both PDEs and integro-differential equations, we extend machine learning methods to apply to unbounded DEs and a larger class of problems.
</details>
<details>
<summary>摘要</summary>
“快速发展的机器学习方法已经刺激了观察数据中的对应运动方程式（DEs）的计算重建的研究兴趣。在这篇论文中，我们提出了一种新的神经网络-ODE基于方法，使用特征展开来学习空间时间的对应运动方程式。我们的spectral neural DE学习方法不依赖空间维度化，因此允许目标空间时间方程式包含无限距离的非本地空间互动， acting on unbounded spatial domains。我们的spectral方法与latest machine learning方法相比，具有相同的精度。通过开发一个spectral框架来学习PDE和 integro-differential方程式，我们延伸了机器学习方法，让它适用于无限DE和更多的问题。”Note that Simplified Chinese is a written form of Chinese that uses shorter words and sentences, and is more commonly used in informal writing and online communication. Traditional Chinese is a more formal written form that is used in more formal writing, such as newspapers and books.
</details></li>
</ul>
<hr>
<h2 id="Compositional-Sculpting-of-Iterative-Generative-Processes"><a href="#Compositional-Sculpting-of-Iterative-Generative-Processes" class="headerlink" title="Compositional Sculpting of Iterative Generative Processes"></a>Compositional Sculpting of Iterative Generative Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16115">http://arxiv.org/abs/2309.16115</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timgaripov/compositional-sculpting">https://github.com/timgaripov/compositional-sculpting</a></li>
<li>paper_authors: Timur Garipov, Sebastiaan De Peuter, Ge Yang, Vikas Garg, Samuel Kaski, Tommi Jaakkola</li>
<li>for: 本研究旨在提出一种通用的拟合过程定义方法，以便将多个迭代生成过程组合成更复杂的生成模型。</li>
<li>methods: 本研究使用的方法包括：1）定义迭代生成过程的组合方法，2）基于分类器导航的采样方法，3）在GFlowNets和扩散模型中实现compositional sculpting。</li>
<li>results: 本研究通过实验表明，通过使用compositional sculpting可以在图像和分子生成任务中实现更高的生成质量和更好的扩散性。<details>
<summary>Abstract</summary>
High training costs of generative models and the need to fine-tune them for specific tasks have created a strong interest in model reuse and composition. A key challenge in composing iterative generative processes, such as GFlowNets and diffusion models, is that to realize the desired target distribution, all steps of the generative process need to be coordinated, and satisfy delicate balance conditions. In this work, we propose Compositional Sculpting: a general approach for defining compositions of iterative generative processes. We then introduce a method for sampling from these compositions built on classifier guidance. We showcase ways to accomplish compositional sculpting in both GFlowNets and diffusion models. We highlight two binary operations $\unicode{x2014}$ the harmonic mean ($p_1 \otimes p_2$) and the contrast ($p_1 \unicode{x25D1}\,p_2$) between pairs, and the generalization of these operations to multiple component distributions. We offer empirical results on image and molecular generation tasks.
</details>
<details>
<summary>摘要</summary>
高训练成本的生成模型和特定任务的 fine-tuning 已经创造了对模型再利用和组合的强大兴趣。一个关键挑战在组合迭代生成过程，如 GFlowNets 和 diffusion models，是确保所有生成过程步骤协调，并满足细腻的平衡条件。在这项工作中，我们提出了组合雕塑：一种通用的方法来定义生成过程的组合。然后，我们介绍了基于分类指导的抽样方法。我们在 GFlowNets 和 diffusion models 中实现了 compositional sculpting，并提供了对多组件分布的总体化。我们介绍了两种二元操作：harmonic mean ($p_1 \otimes p_2$) 和 contrast ($p_1 \unicode{x25D1}\,p_2$) 之间的对比，以及这些操作的普遍化到多个组件分布。我们提供了对图像和分子生成任务的实验结果。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Active-Learning-Performance-Driven-by-Gaussian-Processes-or-Bayesian-Neural-Networks-for-Constrained-Trajectory-Exploration"><a href="#Comparing-Active-Learning-Performance-Driven-by-Gaussian-Processes-or-Bayesian-Neural-Networks-for-Constrained-Trajectory-Exploration" class="headerlink" title="Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration"></a>Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16114">http://arxiv.org/abs/2309.16114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xfyna/al-bnn-gp">https://github.com/xfyna/al-bnn-gp</a></li>
<li>paper_authors: Sapphira Akins, Frances Zhu</li>
<li>for: 这篇论文旨在提高空间探索能力，尤其是在地面探测和采样方面。</li>
<li>methods: 论文使用了活动学习算法，其中包括 Gaussian processes 和 Bayesian neural networks。</li>
<li>results: 结果表明，使用 Gaussian processes 的活动学习策略可以更快地 converges 到一个准确的模型，并且可以采取更短的轨迹。 Bayesian neural networks 在大数据 regime 下可以更准确地模型环境，但是需要更多的计算资源。<details>
<summary>Abstract</summary>
Robots with increasing autonomy progress our space exploration capabilities, particularly for in-situ exploration and sampling to stand in for human explorers. Currently, humans drive robots to meet scientific objectives, but depending on the robot's location, the exchange of information and driving commands between the human operator and robot may cause undue delays in mission fulfillment. An autonomous robot encoded with a scientific objective and an exploration strategy incurs no communication delays and can fulfill missions more quickly. Active learning algorithms offer this capability of intelligent exploration, but the underlying model structure varies the performance of the active learning algorithm in accurately forming an understanding of the environment. In this paper, we investigate the performance differences between active learning algorithms driven by Gaussian processes or Bayesian neural networks for exploration strategies encoded on agents that are constrained in their trajectories, like planetary surface rovers. These two active learning strategies were tested in a simulation environment against science-blind strategies to predict the spatial distribution of a variable of interest along multiple datasets. The performance metrics of interest are model accuracy in root mean squared (RMS) error, training time, model convergence, total distance traveled until convergence, and total samples until convergence. Active learning strategies encoded with Gaussian processes require less computation to train, converge to an accurate model more quickly, and propose trajectories of shorter distance, except in a few complex environments in which Bayesian neural networks achieve a more accurate model in the large data regime due to their more expressive functional bases. The paper concludes with advice on when and how to implement either exploration strategy for future space missions.
</details>
<details>
<summary>摘要</summary>
Robots with increasing autonomy are advancing our space exploration capabilities, particularly for in-situ exploration and sampling to replace human explorers. Currently, humans control robots to achieve scientific objectives, but the delay in information exchange and driving commands between the human operator and robot can hinder mission success. An autonomous robot with a scientific objective and exploration strategy encoded does not incur communication delays and can complete missions more quickly. Active learning algorithms offer this capability of intelligent exploration, but the underlying model structure affects the performance of the active learning algorithm in understanding the environment. In this paper, we compare the performance differences between active learning algorithms driven by Gaussian processes or Bayesian neural networks for exploration strategies encoded on agents with constrained trajectories, such as planetary surface rovers. These two active learning strategies were tested in a simulation environment against science-blind strategies to predict the spatial distribution of a variable of interest along multiple datasets. The performance metrics of interest are model accuracy in root mean squared (RMS) error, training time, model convergence, total distance traveled until convergence, and total samples until convergence. Active learning strategies encoded with Gaussian processes require less computation to train, converge to an accurate model more quickly, and propose trajectories of shorter distance, except in a few complex environments where Bayesian neural networks achieve a more accurate model in the large data regime due to their more expressive functional bases. The paper concludes with advice on when and how to implement either exploration strategy for future space missions.
</details></li>
</ul>
<hr>
<h2 id="Feature-Normalization-Prevents-Collapse-of-Non-contrastive-Learning-Dynamics"><a href="#Feature-Normalization-Prevents-Collapse-of-Non-contrastive-Learning-Dynamics" class="headerlink" title="Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics"></a>Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16109">http://arxiv.org/abs/2309.16109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Bao</li>
<li>for: 本文研究了异形学习框架中的对比学习方法，包括对比学习和非对比学习两种方法。</li>
<li>methods: 本文使用了对比学习和非对比学习两种方法，并对这两种方法的学习过程进行了动力学分析。</li>
<li>results: 研究发现，通过增强数据增强而生成的两个正例会在数据表示空间中受到吸引力，而负例会受到排斥力。但是，通过对比学习和非对比学习两种方法的比较，发现 feature normalization 对学习过程的稳定性具有重要的影响。<details>
<summary>Abstract</summary>
Contrastive learning is a self-supervised representation learning framework, where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Non-contrastive learning, represented by BYOL and SimSiam, further gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. (2021) revealed through the learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly-used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may collapse the dynamics, which is an unnatural behavior under the presence of feature normalization. Therefore, we extend the previous theory based on the L2 loss by considering the cosine loss, which involves feature normalization. We show that the cosine loss induces sixth-order dynamics (while the L2 loss induces a third-order one), in which a stable equilibrium dynamically emerges even if there are only collapsed solutions with given initial parameters. Thus, we offer a new understanding that feature normalization plays an important role in robustly preventing the dynamics collapse.
</details>
<details>
<summary>摘要</summary>
“对照式学习是一种自我指导学习框架，其中两个正例通过数据增强生成的观察者通过吸引力在数据表示空间中变相似，而负例则通过排斥力让它们远离负例。非对照式学习，例如BYOL和SimSiam，进一步删除负例，并提高计算效率。然而，学习的表示可能会崩溃到单一点，因为缺乏排斥力。但是，这个问题可以通过调整数据增强的强度来解决。”“然而，这些分析不考虑通常使用的特征Normalizer，即在计算表示之间的相似度时，将特征转换为相同的尺度。因此，过度强制正规化可能会导致动态崩溃，这是一种不自然的行为。因此，我们从L2损失中推广的理论，考虑cosine损失，这个损失函数包含特征Normalizer。我们显示，cosine损失导致第六种动态（而L2损失导致第三种动态），其中稳定的平衡 dynamically emerges，即使只有崩溃的初始参数。因此，我们提出了一个新的理解，即特征Normalizer在避免动态崩溃中扮演了重要的角色。”
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Secure-Multiplication-Hiding-Information-in-the-Rubble-of-Noise"><a href="#Differentially-Private-Secure-Multiplication-Hiding-Information-in-the-Rubble-of-Noise" class="headerlink" title="Differentially Private Secure Multiplication: Hiding Information in the Rubble of Noise"></a>Differentially Private Secure Multiplication: Hiding Information in the Rubble of Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16105">http://arxiv.org/abs/2309.16105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viveck R. Cadambe, Ateet Devulapalli, Haewon Jeong, Flavio P. Calmon</li>
<li>for: 这个论文研究了分布式多方计算中的隐私问题，具体来说是使用谢米尔秘密分享编码策略来实现信息理论上的完美隐私。</li>
<li>methods: 该论文使用了谢米尔秘密分享编码策略，但是它允许一定的信息泄露和approximate multiplication，从而在部分诚实节点的情况下保证隐私和准确性。</li>
<li>results: 该论文提出了一种紧张性 privacy-accuracy 质量的衡量方法，并在不同层次上具有层次结构的隐私泄露分布，从而实现了在诚实节点少于2t+1的情况下的隐私和准确性。<details>
<summary>Abstract</summary>
We consider the problem of private distributed multi-party multiplication. It is well-established that Shamir secret-sharing coding strategies can enable perfect information-theoretic privacy in distributed computation via the celebrated algorithm of Ben Or, Goldwasser and Wigderson (the "BGW algorithm"). However, perfect privacy and accuracy require an honest majority, that is, $N \geq 2t+1$ compute nodes are required to ensure privacy against any $t$ colluding adversarial nodes. By allowing for some controlled amount of information leakage and approximate multiplication instead of exact multiplication, we study coding schemes for the setting where the number of honest nodes can be a minority, that is $N< 2t+1.$ We develop a tight characterization privacy-accuracy trade-off for cases where $N < 2t+1$ by measuring information leakage using {differential} privacy instead of perfect privacy, and using the mean squared error metric for accuracy. A novel technical aspect is an intricately layered noise distribution that merges ideas from differential privacy and Shamir secret-sharing at different layers.
</details>
<details>
<summary>摘要</summary>
我团队考虑了分布式多方计算中的私人分享 multiply 问题。已经证明了Shamir的秘密分享编码策略可以在分布式计算中实现完美的信息理论隐私，通过著名的Ben Or、Goldwasser和Wigderson算法（BGW算法）。然而，完美隐私和精度需要一个诚实的多数，即 $N \geq 2t+1$ 计算节点。我们允许一定的控制的信息泄露和approximate multiply instead of exact multiply，研究在计算节点少于 $2t+1$ 的情况下的编码方案。我们开发了一个紧张的隐私准确度质量负担，通过使用 {differential} privacy 而不是完美隐私来度量信息泄露，并使用 mean squared error  метри来度量准确性。一个新的技术方面是一种复杂层次的噪声分布，这将 Shamir secret-sharing 和分布式隐私的想法 merge 在不同层次。
</details></li>
</ul>
<hr>
<h2 id="Task-Oriented-Koopman-Based-Control-with-Contrastive-Encoder"><a href="#Task-Oriented-Koopman-Based-Control-with-Contrastive-Encoder" class="headerlink" title="Task-Oriented Koopman-Based Control with Contrastive Encoder"></a>Task-Oriented Koopman-Based Control with Contrastive Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16077">http://arxiv.org/abs/2309.16077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xubo Lyu, Hanyang Hu, Seth Siriya, Ye Pu, Mo Chen</li>
<li>for: 这 paper 的目的是 simultaneously learn Koopman latent embedding, operator and associated linear controller within an iterative loop, 以便在高维、复杂非线性系统中进行控制。</li>
<li>methods: 这 paper 使用 end-to-end reinforcement learning 和 contrastive encoder 来学习 Koopman latent embedding, operator and associated linear controller。</li>
<li>results: 通过优先级 task cost 作为控制器学习的主要目标，这 paper 可以减少控制器设计对于准确模型的依赖，从而扩展 Koopman control 到高维、复杂非线性系统，包括像素化enario。<details>
<summary>Abstract</summary>
We present task-oriented Koopman-based control that utilizes end-to-end reinforcement learning and contrastive encoder to simultaneously learn the Koopman latent embedding, operator and associated linear controller within an iterative loop. By prioritizing the task cost as main objective for controller learning, we reduce the reliance of controller design on a well-identified model, which extends Koopman control beyond low-dimensional systems to high-dimensional, complex nonlinear systems, including pixel-based scenarios.
</details>
<details>
<summary>摘要</summary>
我们提出了任务导向的库曼控制方法，该方法利用端到端学习和对比编码器同时学习库曼嵌入、运算和相关的直线控制器。我们将任务成本作为控制器学习的主要目标，从而减少控制器设计依赖于良好识别模型的需求，因此扩展了库曼控制到高维、复杂非线性系统，包括像素化场景。
</details></li>
</ul>
<hr>
<h2 id="Infer-and-Adapt-Bipedal-Locomotion-Reward-Learning-from-Demonstrations-via-Inverse-Reinforcement-Learning"><a href="#Infer-and-Adapt-Bipedal-Locomotion-Reward-Learning-from-Demonstrations-via-Inverse-Reinforcement-Learning" class="headerlink" title="Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning"></a>Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16074">http://arxiv.org/abs/2309.16074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feiyang Wu, Zhaoyuan Gu, Hanran Wu, Anqi Wu, Ye Zhao</li>
<li>for: 将步行机器人学习在高度不对称、动态变化的地形上行走是一个具有复杂性的挑战，因为机器人动力学和环境互动的复杂性。</li>
<li>methods: 本研究使用了学习从示例（Learning from Demonstrations，LfD）技术，将专家策略传染到机器人上，并运用了最新的反馈学习（Inverse Reinforcement Learning，IRL）技术来解决步行机器人的行走问题。</li>
<li>results: 研究发现，透过对专家策略进行学习，可以将机器人的行走性能提高，并且在未见过的地形上保持稳定的行走。这显示了对 reward 学习的适应性和机器人行走的可控性。<details>
<summary>Abstract</summary>
Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert's locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on unseen terrains, highlighting the adaptability offered by reward learning.
</details>
<details>
<summary>摘要</summary>
enable bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert's locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on unseen terrains, highlighting the adaptability offered by reward learning.Here's the translation in Traditional Chinese:问题是，具有访问高度不均匀、动态变化的地形的双脚行走机器人学习是具有机器人动力学和互动环境的复杂性，导致学习问题的挑战。现有的学习从示例探索已经展示了在复杂环境中机器人学习的可能性。然而，对于双脚行走中的专家奖励函数学习，尚未充分探索。本文将使用现代倒推奖励学技术（IRL）解决双脚行走问题。我们提出了学习专家奖励函数的算法，并且分析学习到的函数。通过非线性函数推对，我们获得了专家行走策略的深入理解。此外，我们还证明了将专家奖励函数训练到双脚行走策略上，可以增强其在未见地形上的行走性能，强调了奖励学习的适应性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/28/cs.LG_2023_09_28/" data-id="cloqtaetw00q9gh8829230bo7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/29/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><a class="page-number" href="/page/29/">29</a><span class="page-number current">30</span><a class="page-number" href="/page/31/">31</a><a class="page-number" href="/page/32/">32</a><span class="space">&hellip;</span><a class="page-number" href="/page/88/">88</a><a class="extend next" rel="next" href="/page/31/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">68</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
